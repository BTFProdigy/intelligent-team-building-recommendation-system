Automat ic  Lexical Acquisit ion Based on Statistical Distributions* 
Suzanne Stevenson  
Depar l ;ment of Coml)uter Science 
Uldvcrsity of Toronto  
6 King's College l(oa,d 
'.l.'oronto, ON Ca, lla, d~t M5S 311115 
suzanne~cs, toronto, edu 
Abstract 
We a, ui;omatically cla,ssi(y verbs into lexica,1 se- 
mantic classes, 1)ased on distributions of indices- 
tots of verb a.lterna.tions, extra.cCed froln a very 
la.rge a.nnota.ted corpus. We ~ddress a. prol)lem 
which is pa.rticuhtrly difficult 1)eca.use the verl) 
classes, a.lthough sema.ntica.lly different, show sim- 
ila.r surface syntactic 1)eha.vior, Five gra.m,na.tica.1 
fea.l;tlres ~u:e su\[\[icient to reduce error i:ate by more 
tha.n 50% over cha.nc(,: we a.chieve almost 70% 
a.ceura.cy in a 1;ask whose baseline perl'ornmn(:e is 
34%, and whose exl)ert-I)ased Ul)l)er bound we ('aJ- 
culated a.t 86.5%. We conclude l;ha.1; corl)us-driven 
exl;racl;ion of gramma.1;ical \['eaJ;ures i a. promising 
lnethodology for line-grained verb classilica.tion. 
1 I n t roduct ion  
\])eta.ileal hfforma.tion a.I)out verbs is critical to a. 
broad ra.nge of NI,I ) and l i t  1;asks, yet; ils mau 
us.1 (lel;('rmina.tion for la.rge numl)ers o\[' verl)s is 
difficult aml resource intensive. I{.esea.rch o,I tim 
a.ul,()matic, a(-quisil;ion o\[' verb-I)ased k,owl(~(Ig(, 
has succeded in gleaning sylH.a.('l;ic l)rol)erties o\[' 
verl)s such as sul)ca.tegoriza.tion frames from el> 
line resources (I}rent, \]9!)3; lh'iscoe a.nd C,a.rroll, 
1997; \])err, 1997; Ma.nning, \]993), ll,ecently, 
researchers have investiga?ed statistica.l corpus- 
ba.sed methods for lexica.l sema.ntic classitica.tion 
from synta.ctic prol)erties of verl) usage (Aone a.nd 
McKee, \]996; l,a.pa.ta, and Brew, \]999; Schulte im 
Wa.lde, :1998; Stevenson a.nd Merle. 1999; Steve.n- 
son et a l., \] 999; McCarthy, 2000). 
C, orl)us-based al)pro~mhes to lexica.l sema.ntic 
classitic~tion in pa,rticular ha.ve dra.wn on Levin's 
hypothesis (I,evin, 1993) that verbs can be classi- 
lied according to the dia.thesis aJterna.tions (a.lter- 
nations in the syntactic expressions o\[" a.rguments) 
ill which they f)articil)a.te l'or exa.mple, whether a. 
* This research was partly sponsored 1)y US NSI" grants 
#9702331 and #9818322, Swiss NSI" Mlowshlp 8210- 
d65(;9, Information Sciences (.Ollll(il of Hurters University 
and IllCS, U. of Peimsylwmia. 'l'his research was con- 
ducted wldle the tirst author was at l lutgers University. 
Pao la  Mer le  
LAT\ ] , -  \])cpa.rtment of l,inguisCics 
Univcrsil;y of ( leneva 
2 rue de C'~mdolle 
121\] Gent;re d -. Suisse 
merZo?let t res ,  un?ge, ch 
verb occurs in the dative/prepositiona.l phrase al- 
terna.tion in l';nglish. One diagnostic for dis.thesis 
a.lternations i the sulx'a.tegorization aJternatives 
of a. verb. ltowew~,r, some classes exhibit the same 
subca.tegoriza.tkm possibilities but differ in their 
a.rgument structures, i.e. tim content el' the the- 
real;it roles assigned to the arguments of l;he verb. 
rFhis gyl)e of situation consl;itutes a. pa.rticula.rly 
difficult case R)r corpus-based classification meth- 
ods. 
In this paper, we apply corpus-based lexica.l 
a.cquisition methodology 1;o distinguish classes of 
verbs which allow the same subca.tegoriza?ions, 
but differ in tlteJna.tic roles. We first a ssu me tha.t 
one ca.n a.ut;oma.tiea.lly restrict l;he choice o\[' ('lasses 
to those that; paxl;icil)a.1;e in the relewu~t subcate- 
gorizations (c\['. (l,a.p~ta. and Brew, \]999)). Our 
prOl)OSa.1 is lhen to use st.a.tistics ovel: di~Ll;hesis 
a lterna.nl,s as a, wa.y to \['urther distinguish those 
verl)s wldch allow 1;he same sul)ca.tegoriza.tions; 
achievi,g fine-grained cla.ssifica.tion within that 
S('l,. ()UI' work \['O(tllSeS oll determining tile 1)esl se- 
ma.nl, ic class for a verl) lgpc - the set of usages o1" 
a. verl) across a. document or corpus rather t;ha.n 
fl)r a single verb I, okc n in {~ single local context. 
In this way, we c~u/ exploit the broad beha.vior o1' 
the verb a.cross 1;he corl)uS to determine its most 
likely class overall. 
We investiga.te the proposed a.l)l)rOaCh in an in- 
del)th case study o\[' the three major classes of o1> 
tiona.lly inlra,nsitive w, rl)s in English: ullergative, 
unaccusa,tive, and ol)ject-drop. More specifically, 
according to l,evin's classificaJ;ion (l,evin, 1993), 
the unerga.tives are ma.nner of motion verbs, such 
as jump and march; the una.ccusa.tives are verl)s 
of cha.nge of state,, such a.s open and explode; the 
object-drop verbs a.re unexpressed object a.lCerna.- 
Lion verl)s, such as played a.nd painted. These 
classes a.ll supl)ort 1)oth tr~msitive and intra.nsi- 
1,ire sul)cal,egoriza.tions, I)ut a.re distinguished by 
the pal;tern of thema.tic role assignments i,o sub- 
jecC a.nd object position. We a.utomatica.lly cla.s- 
si(y these verbs on the basis of sta.tistical a,p- 
815 
proxilnations to syntactic indicators of the under- 
lying argunlent structures, using numerical fea- 
tures collected from a large syntactically anno- 
tated (tagged or parsed) corpus. We apply ma- 
chine learning techniques to determine whether 
the fi'equency distribntions of the features, in- 
dividually or in combination, support automatic 
classification of the verbs. To preview our re- 
sults, we demonstrate that combining only five 
numerical indicators is sufficient o reduce the er- 
ro r  rate in this classification task by more than 
50% over chance. Specifically, we achieve ahnost 
7(1% accuracy in a task whose baseline (chance) 
per\[brmance is 34%, and whose expert-based up- 
per bound is calculated at 86.5%. We conclude 
that a distribution-based method for lexical se- 
mantic verb classification is a promising avenue of 
research. 
2 The  Argument  S t ruc tures  
Our approach rests on tile hypothesis that, even in 
cases where verb classes cannot be distinguished 
by subcategorizations, the frequency distributions 
of syntactic indicators can hold clues to the under- 
lying thematic role differences. We start here then 
with a description of the subca.tegorizations and 
thematic role assignments for each of l.he three 
verb classes under investigation. 
As optionally intransitive verbs, each of 
the three classes participates in the transi- 
tive/intransitive Mternation: 
Uuergative 
(la) The horse raced past the barn. 
(1 b) The jockey raced the horse past tile barn. 
Unaccnsative 
(2a) The butter melted in the pan. 
(2b) The cook melted the butter in the pan. 
Object-drop 
(3a) The boy washed the hall. 
(3b) The boy washed. 
Unergatives are intransitive action verbs, as in (1), 
whose transitive form can be the causative coun- 
terpart of the intransitive form. In the causative 
use, the semantic argument hat appears as the 
subject of the intransitive, as in (la), surfaces 
as the object of the transitive, as in (lb) (Ilale 
and Keyser, 1993). Unaccusatives are intransitive 
change of state verbs, as in (2a); the transitive 
counterpart for these verbs exhibits the causative 
alternation, as in (2b). Object-drop verbs, as in 
(3), have a non-causative transitive/intransitive 
alternation, in which the object is simply optional. 
Subj of 
Classes Trans 
Unergative Causal Agent 
Unaccusative Causal Agent 
Object-drop Agent 
Obj of Subj of' 
rPrans Intrans 
Agent Agent 
'?heme rI'heme 
Theme Agent 
Table 1: Summary of Thematic Alternations. 
Each class is distinguished by the content of tile 
thematic roles assigned by tile verb. For object- 
drop verbs, tile subject is all Agent and the op- 
tional object is a Theme, yielding tile thematic 
assignments (Agent, Tlmme) and (Agent) for the 
transitive and intransitive alternants respectively. 
Unergatives and uuaccusatives differ \['1"o111 object- 
drop verbs in participating in the causative alter- 
nation, and also differ from each other in their core 
thematic argument. In an intransitive unerga- 
live, the subject is an Agent, and in an intran- 
sitive unaccusative, the subject is a Theme. In 
the causative transitive form of each, this core se- 
mantic argument is expressed as the direct object, 
with the addition of a Causal Agent (the causer of 
the action) as subject in bol;h cases. The thematic 
roles assigned, and their mapping to syntactic po- 
sition, are summarized in Ta.ble 1. 
3 The  features  for  C lass i f i ca t ion  
The key to any automatic lassification task is to 
determine a set of' useful fea.tures for discriminat- 
ing the itenls to be classitied. In what follows, we 
refer to the cohnnns of Table 1 to explain \]tow we 
expect the thematic distinctions to yield distri- 
butional features whose frequencies discriminate 
among the classes ~t hand. 
Considering column one of Table 1, only 
unergative and unaccusa.tive rbs assign ~ Causal 
Agent to the subject of the transitive. We hy- 
1)othesize that the causative construction is lin- 
guistically more complex than the simple argu- 
ment optionality of object-drop verbs (Stevenson 
and Merlo, 1.997). We expect then that object- 
drop verbs will be more fi:equent in the transi- 
tive than the other two classes. Furthernmre, the 
object of an unergative verb receives the Agent 
role (see the second column of Table 1.), a linguis- 
tically marked transitive construction (Stevenson 
and Merlo, 1997). We therefore xpect unerga- 
tives to be quite rare in the transitive, leading to 
a three-way distinction in transitive usage among 
the three classes. 
Second, due to the causative alternation of 
816 
'l'able 2: The l.i'ea.Cures and 'l'heir F;xpected 13ehavior 
TrmlsitiviLy Unaccusativcs and unergativcs have ~, causative transit ive, hence lower transit ive use. Fur- 
l;hc'rlnorc, unerga.tivcs ha.re a.n agent.ire object, hence very low transit ive use. 
Pa.ssivc Voice Passive implies transit ive use, hence correlated with transit ive feature. 
VBN Tag Passive implies past pa.rt;iciple use (VBN), hence correlated with transit ive (and passive). 
Causat iv i ty  () l ) ject-drop verbs do not have a. causal agent, hence low "ca.usative" use. Unergatives are 
rare in the transit ive, hence low cmlsative use. 
An imacy  Unaccusatives have a Theme subject in the intransit ive, hence lower use of animal, esubjects. 
unergatives and nnaccusatives, the l, hematic role 
of the subjec~ of the intransitiw,~ is identical to 
that of the objecl of the transitiw;, as shown in 
columns two and three of Table 1. C, iven the 
identity of thematic role mal)ped to subject and 
object positions, we expect to observe the sa.me 
noun occurring at times a.s subject of the verb, 
and at other times as object of the verb. In con- 
t ras t ,  for object-tirol) verbs, Cite thenm.tic role o\[' 
the sul)ject o17 the intransitive is identical to l;ha{, 
of the sul)ject of the transitive, not the object of 
the transitive. Thus, we expect that it will be less 
common for the same noun to occur in subject and 
object position of the same object-drop verb. We 
hypothesize that this pattern of thematic role as- 
signments will be retlected in difl'erential amount 
of u~'~age across the classes of the same nouns as 
subjects and ol)jects for a given verb. Further- 
more, since the causative is a transitive use, a.nd 
the 1,ra.nsitive use of u nerga.gives i oxpocl;ed to be 
rare., this overlap o(' subjects and ob.iects should 
primarily distinguish unaccusatives (predicted to 
have high overlap of subjects and objects) from 
the other two classes. 
Finally, considering columns one and three of 
Tal)le 1, we note that unergative and objecl;-drop 
verbs assign all agentive role to their subject in 
both the transitive and intra.nsitive, while unac- 
cusatives assign an agentive role to their subject 
only in the tr~msil, ive. Under the assutnpL ion that 
the intransitive use of' unaccusatives i  not rare, 1 
we then expect thai, unaccusatives will occur less 
often overall with an agentive subject than the 
other two verb classes. On the flu:ther assump- 
tion that Agents tend to be animate entities more 
so than Themes, we expect that unaccusatives 
will occur less freqnently with an animate subject 
compared to unergative and object-drop verbs. 
Note the importance of our use of frequency dis- 
tributions: the claim is not that only Agents can 
~This assumpl, ion is based on the linguistic conlplexity 
of the causative, and borne out in our corpus analysis. 
be animate, but rather that nouns that receive an 
Agent role will more often be animate than nouns 
that receive a Theme ,'ole. 
The above interactions between thematic roles 
and the syntactic expressions of arguments thus 
lead to three features whose distrit)utional proper- 
ties appear promising for distinguishing the verb 
classes: transitivity, causativity, and animaey of 
subject. We also investigate two additionM syn- 
tactic l'ea.l, ures, the passive voice and tile past pa.r- 
ticiple POS tag (VI3N). These features are related 
to the transitive/intransitive Mternal;ion, since a 
passive use implies a transitive use of the verb, 
and the nse of passive in turn implies the use of 
the past participle. Our hyl)ol;hesis is that these 
five features will exhibit distributional differences 
in the observed usages of the verbs, which can be 
used for classifica.tion. The features and their ex- 
pected relevance are summarized in '13ble 2. 
4 Da~a Col lec t ion  and  Ana lys i s  
We chose a set of 20 verbs from each of three 
classes. The complete list of verbs is reported in 
Appendix A. Recall that our goal is to achieve a 
fine-grained classification of verbs that exhibit the 
same subcategorization frames; thus, tile verbs 
were chosen because they do not generally show 
massive del)artures from the intended verb sense 
(and usage) in the corpus. 2 In order to simplify 
tile counting procedure, we included only tile reg- 
ular ("-ed") simple past/past participle form of 
tile verb, assuming that this would approximate 
the distribution of tile features across all forms of 
the verb. Finally, as far as we were able given 
the preceding constraints, we selected verbs that 
could occur in the transitive and in the passive. 
We counted the occurrences of each verb token 
in a transitive or intransitive use (3'RANS), ill a 
2~l~hough note that there are only 19 unaccusatives be- 
cause ripped was excluded fl'om the analysis as it occurred 
mostly in a very different use (ripped off) in the corpus 
from the intended diange of state usage. 
817 
passive or active use (PASS), in a past participle 
or simple past use (VBN), in a causative or non- 
causative use (tAgS), and with an animate subject 
or not (ANIM), as described below. The first three 
counts  (TRANS, I'ASS~ VBN) were performed on 
the LDC's 65-million word tagged ACL/DCI  cor- 
pus (Brown, and Wall Street Journal 1987-1989). 
The last two counts (CAUS and ANIM) were per- 
formed on a 29-million word parsed corpus (\gall 
Street Journal 1988, provided by Michael Collins 
(Collins, 1997)). The features were counted as 
follows: 
TaANS: The closest noun following a verb was 
considered a potential object. A verb immedi- 
ately \[bllowed by a potential object was counted 
as transitive, otherwise as intransitive. 
pass: A token tagged VBD (the tag for simple 
past) was counted as active. A token tagged VBN 
(the tag for past participle) was counted as active 
if the closest preceding auxiliary was have, and as 
passive if the closest preceding auxiliary was be. 
VBN: The counts tbr VBN/VBI )  were based on 
the POS label in the tagged corl)us. 
Each of the above counts was normalized over 
all occurrences of tim "-ed" form of the verb, yield- 
ing a single relative fi:equency measure \['or each 
verb for that feature. 
tags :  For each verl) token, the subject and ob- 
ject (it' there was one) were extracted from the 
parsed corpus, and the proportion of overlap be- 
tween subject and object nouns across all tokens 
of a verb was calculated. 
ANIM: To approximate animacy without refer- 
ence to a resource external to the corpus (such 
as WordNet), we count pronouns (other than it) 
in subject position (cf. (Aone and McKee, 1996)). 
The aSSUlnption is that the words I, we, you, .~'tze, 
he, and theft most often refer to animate entities. 
We automatically extracted all subject/verb tu- 
ples, and computed the ratio of occurrences of 
pronoun subjects to all subjects for each verb. 
The aggregate means by class resulting from the 
counts above are shown in Table 3. The distri- 
butions of each feature are indeed roughly as ex- 
pected according to the description in Section 3. 
Unergatives how a very low relative fi'equency 
of the TRANS feature, followed by unaccusatives, 
then object-drop verbs. Unaccusative verbs show 
a high frequency of the CAUS feature and a low 
frequency of the ANIM feature compared to the 
other classes. Although expected to be a redun- 
dant indicator of transitivity, pass and VBN do 
Ta.ble 3: Aggregated Relative Frequency Data \['or 
tile Five Features. E = unergatives, A = unac- 
cusatives, O = object-drol)s. 
Class 
E 
A 
O 
N MEAN I~ELATIVE FREQUENCY 
TR, ANS PASS VBN CAUS ANIM 
20 0.23 0.07 0.21 0.00 0.25 
19 0.40 0.33 0.65 0.12 0.07 
20 0.62 0.31 0.65 0.04 0.15 
not distinguish t)etween unaccusative and object- 
drop verbs, indicating that their distributions are 
sensitive to factors we have not yet investigated, a 
5 Exper iments  in C lass i f icat ion 
The frequency distributions of our  features yield 
a vector for each verb that represents the relative 
frequency wdues for the verb on eacln dimension: 
\[verb, TRANS, PASS, VBN~ CAUS, ANIM, class\] 
Example: \[opened, .69, .09, .21, .16, .36, unaec\] 
\?e use the resulting 59 vectors to train an au- 
tomatic classifier to determine, given a verb that 
exhibits transitive~intransitive sttbcategorization 
frames, which of the three major lexical semantic 
classes of English optionally intransitive verbs it 
belongs to. Note that the baseline (chance) per- 
Ibrmance in this task is 33.9%, since there are 59 
vectors and 3 possible classes, with the most coin- 
men class having 20 verbs. 
We used the C5.0 machine learning system 
(tnttp://www.rulequest.com), a newer version of 
C4.5 (Quinlan, 1992), which generates decision 
trees and corresponding rule sets from a training 
set of known classifications. We found little to no 
difference in performance between the trees and 
rule sets, and report only the rule set results. \?e 
report here on experiments using a single hold- 
out training and testing methodology. In this ap- 
proach, we hold out a single verb vector as the 
test case, and train the system on the remaining 
58 cases. We then test the resulting classifier on 
tile single hold-out case, recording tile assigned 
class for that verb. This is then repeated for each 
of the 59 verbs. This technique has the benefit 
of yielding both an overall accuracy rate (when 
the results are averaged across all 59 trials), as 
well as providing tile data necessary tbr determin- 
ing accuracy for each verb class (because we have 
the classification of each verb when it is the test 
case). This allows us to evaluate tile contribution 
aThese observations have been confirmed by t-test.s be- 
tween feature values for each pair of classes. 
818 
%d)le <1:: Percent Accuracy of Verb Clas- 
sifica,l,ion Task Using \],'eatures in Combina- 
tion. T=Tl lANS;  \])=PASS; \ /=VBN;  C=CAUS;  
An=ANIM. E=unergatives, A=unaccusatives, 
O =:ol)ject-drops 
Percent Accuracy by Class 
All l E I a I 0 \],'ca.I,1llJes 
1. '.I'P VCAn 69.5 85.0 
2. P V C An 64:.4: 80.0 
3. TV  C An 71.2 80.0 
4:. 51' P C An 61.0 65.0 
5. TPVAn 62.7 70.0 
6. 51'PV C 61.0 80.0 
63.2 
dT.d 
73.7 
68.4 
63.2 
42.1 
60.0 
65.0 
60.0 
50.0 
55.0 
60.0 
of individual feal:ures with respect to their effect 
on the perfornlance of individual classes. 
We performed experiments on the \['ull sel, of fea- 
tures, as well a.s each subsel, of fea.l,ures wil,h a. sin- 
gle f~ture  remow;d, as reported in Table d. Con- 
sider l;he first column in the ta.ble. The first line 
shows that the overall ~ccuracy for all live features 
is 69.5%, a reduction in tile error ra.te of more 
than 50% above the baseline. The removal o\[" the 
PASS lea.lure appears to improve performance (row 
3 of Ta.ble 4). However, it should be noted that 
this increase in performance results h:oln a single 
additionaJ verb being classified correctly. The re- 
ma.ining rows show thal no feal,ure is superflous 
or hm'mfld as l,he removal of any I'ealure has a. 5 
8% negative elfect on l)erR)rmance. Coral)arable. 
accuracies have been demonsl;rated vsing a more 
thorough cross-validation methodology a.nd using 
reel;hods that are, in principle, better a,t taking 
adva.nl,age of correlated lea,lures (Stevenson and 
Merle, 1999; Stevenson el. al., 1999). 
q'he single hold-out prol,ocol provides new data, 
fbr analysing the performalme on individual verbs 
and classes. The class-by-class accuracies a.re 
shown in the remaining columns of Ta.ble 4. \?e 
can see clearly thal, using all five features, l,he 
unergatives are classified with much greater ac- 
curacy (85%) than l,he UlmCCUsatives and object- 
drop verbs (63.2% and 60.0% respectively), as 
shown in the first row. The rema.ining rows show 
that this l)al,tern generally holds \['or l,he subsel,s 
of features as well, with tire excel)lion of line d. 
\?hile ful,ure work on our verb classificalion 
task will need lo focus on deterlnining features 
thal bel,ter discriminate unaccusative a.nd object- 
drop verbs, we can ah:eady exclude an explana- 
tion of the resull,s based simply on l,he wwbs' or 
tile classes' frequency. Unergatives have tile low- 
est average (log) frequency (1.3), but are the best 
classified, while unaccusatives and object-drops 
are comparable (a.verage log fi'equency = 2). If we 
group verbs by frequency, the proportion of errors 
to lhe total number of verbs remains fairly simi- 
lar (freq 1:7  errors/23 verbs; fi:eq. 2 :6  errors/24 
verbs; freq. 3 :4  errors/10 verbs). The only verb 
of frequency 0 is correctly classified, while lhe only 
one with log frequency 4 is not . In sum, we do 
not find that more frequent classes or verbs are 
more accurately classitied. 
lmlmrtantly, the experiments also enable us to 
see whether the fealures indeed contribute to dis- 
criminating the classes in the manner predicted in 
Seclion 3. The single hold-out results allow us to 
do 1;his, by comparing the individual class labels 
assigned using the full sol, of five features (TIIANS, 
PASS, VBN, CAUS, ANIM) to the class labels as- 
signed using each size four subset of features. This 
comparison indicates 1;he changes in class labels 
l,hat we can a.l,tribul,e to l,he added feature in going 
fi'om a size four subset to the full set of features. 
(The individual class labels supporling our a.naly- 
sis below a.re a.vailable from the authors.) \?e con- 
cent;rate on tile three main features: CAUS, ANIM, 
TRANS. \?e filial thai, the behaviour of lhese fea- 
l,ures generaJly does conform to our predicl;ions. 
We expected that TRANS would help make a. three- 
way distinction among the verb classes. While 
unergatives are ah:eady accurately classified with- 
Ollt TRANS, inspection of lhe change in class la.- 
bels reveals that the addition of TRANS tO tire sel; 
improves performance on unaccusatives by help- 
ing to distinguish 1;hem from object-drol)s, llow- 
ever, in this case, we also observe a loss in pre- 
cision of unerga.lives, ince some object-drops are 
now classitied a.s unergatives. Moreover, we ex- 
pected CAUS and ANIM tO be parl,icularly helpfid 
in identi\['ying unaccus~l,ives, and this is also borne 
out in our analysis of individual la.bels. We note 
that the increased accuracy from CAUS is primar- 
ily due to bel,ter disl,inguishing unergatives from 
unaccusatives, and l,he increased accura.cy from 
AN1M is primarily due go better distinguishing un- 
accusatives from objecl,-drops. \?e conclude tha.t 
the feal,ures we have devised are successful in clas- 
siting optionally 1,ra.nsil;ive verbs because they ca.p- 
lure predicted ifl'erences in underlying argument 
st ruct l r re .  4 
4 Matters are more cmnplex with the other two features 
and we arc still interpreting tile results. Our prediction 
819 
Table 5: Pair-wise Agreement (Calculated t)3' the 
Kappa Statistics) of Three Experts (El, E2, h;3) 
Compared to a Gold Standard (Levin) and to the 
Classifier (Prog). Numbers in parentheses are per- 
centage of verbs on which judges agree. 
PltOG F.I. E2 E3 
El .36 (59) 
E2 .50 (68) .59 (75) 
E3 .49 (66) .53 (70) .66 (77) 
LEWN .54 (69.5) .56 (71.) .80 (86.5) .74 (83) 
6 Compar i son  to Exper ts  
In order to evaluate the performance of the al- 
gorithm in practice, we need to compare it to the 
accuracy of classification performed by an expert, 
which gives a realistic upper bound for the task. 
In (Merle and Stevenson, 2000) we report the re- 
suits of an experiment that measures experts per- 
tbrmance and agreement on a classification task 
very similar: to the program we have described 
here. The results summarised in Table 5 illus- 
trate the performance of the progra, m. On the 
one hand, the algorithm does not perform at ex- 
pert level, as indicated by the fact that, for all ex- 
perts, the lowest agreement score is with the pro- 
gram. On the other: hand, the accuracy achieved 
by the program of 69.5% is only 1.5% less than 
one of the human experts in comparison to tire 
gold standard. In fact, if we take the best per- 
formance achieved by an expert in this task 
86.5%--as the maximum achievable accuracy in 
classification, our algorithm then reduces the er- 
ror rate over; chance by approximately 68%, a very 
respectable result. 
7 D iscuss ion  
The work here contributes both to general and 
technical issues in automatic lexical acquisition. 
Firstly, our results confirm the primary role of 
argument structure in verb classification. Our ex- 
perimental focus is particularly clear in this re- 
gard because we deM with verbs that are ~Illilli- 
was that VBN and PASS would behave similarly to TRANS. 
In fact, PASS is at best unhelpful in classification. VBN 
does appear to make the expected I.hree-way distinction. 
The change ill class labels shows that the improvement in
performance with VBN results from better distinguishing 
unergatives fi'om object-drops, and object-drops from un- 
accusatives. The latter is surprising, since analysis of the 
data found that the VnN feature values are statistically in- 
distinc~ for the object-drop and unaccusative classes as a 
whole. 
mal pairs" with respect o argument structure. 13y 
classif~ying verbs that show the same subcatego- 
rizations into different classes, we are able to elim- 
inate one of the confounds in classification work 
created by the fact that subcategorization a d ar- 
gument structure :M'e largely co-variant. We can 
infer that the accuracy in our classification is due 
to argument structure information, a.s subcatego- 
rization is the same for: all verbs, confirming that 
the con, tent of thematic roles is crucial for clas- 
sification. Secondly, our results further support 
the assumption that thematic differences such as 
these are apparent not only in differences in sub- 
categorization frames, but also in differences in 
(;heir frequencies. We thus join the many recent 
results that all seem to converge in SUl)porting 
the view that the relation between lexical syntax 
and semantics can be usefully exploited (Aone and 
McKee, 1996; l)orr, 1997; Dorr and Jones, 1996; 
Lapata and Brew, 1999; Schulte im Walde, 1998; 
Siegel, 1998), especially in a statistical franmwork. 
Finally, we observe that this information is de- 
tectable in a corpus and can be learned automat- 
ically. Thus we view corpora, especially if an- 
notated wil;h currently available tools, a.s useful 
repositories of implicit grammars. 
Technically, our N)proach extends existing 
corpus-based learning techniques 1;o a more com- 
plex lea.ruing problem, in severaJ dimensions. Our 
statistical apl)roach , which does not require ex- 
plicit negative xamples, extends ai)l)roaehes that 
encode l~evin's alternations directly, as symbolic 
properties of a verb (Dorr et al, 1995; l)orr and 
Jones, 1996; l)orr, 1997). We also extend work 
using surface indicators to approximate underly- 
ing properties. (Oishi and Matsumoto, 1997) use 
case marking particles to approximate graimnat- 
ical functions, such as subject and object. We 
improve on this approach by learning argument 
structure properties, which, unlike grammatical 
functions, are not marked lnorphologically. Oth- 
ers have tackled the problem of lexical semantic 
classification, as we have, but using only snbeate- 
gorization frequencies as input data (Lapata and 
Brew, 1.999; Sehulte im Walde, 1998). By con- 
trast, we explicitly address the definition of fea- 
tures that can tap directly into thematic role dif- 
ferences that are not reflected in "subcategoriza- 
tion distinctions. Finally, when learning of the- 
matic role assignment has been the explicit goal, 
the text has been semantically annotated (Web- 
ster and Marcus, 1989), or external semantic re- 
820 
sources ha.re I)een consulted (Ache and McI(ee, 
19!)6). We extend these results by showing that  
them;~tic informa,tion can 1)e inducexl from corpus 
COtllltS. 
The exl)er imental  results show that  our method 
is l)owerful, and suited to Cite classitica.l;i(m of lex- 
ica.1 items. However, we have not yet addressed 
the problem of verbs that  can h;~ve mult iple clas- 
sifications. We think tha.t many eases of am- 
1)iguous classification of verb types can 1)e ad- 
dressed with the notion of intersective sets in- 
t roduced by (Da.ng et al, 71998). This is an im- 
t)ortant concept tha,t l)rOl)OSes tha,t "i'egula, r" a.m- 
biguity in classifica.tion -i.e., sets of v(;rbs that  
ha.ve the same mult i -way classitications a~ccording 
to (l,evin, 1993) can be captured with a. liner- 
grained notion of lexical semant ic  classes. I~x- 
tending our work to exploit this idea. requires 
only to define the classes a.pl)ropriately; the ba- 
sic a.1)t)roac\]l will remain the same. When we turn 
to consider ambiguity,  we must a.lso address the 
l)roblem l;ha.t individual insta.nces of verl)s may 
come from diffel:ent classes. In future research we 
t)lan to extend our method to the ('\]a.ssificagion f 
a.mbiguous tokens, by exper iment ing with a. func- 
t ics  that  combines severaJ sources of information: 
a bias tbr the verb type (using the cross-corpus 
sta.l;istics we collect), as well as \[~a.tures o\[" the us- 
age of the insta.nce being classiiiod (cf. (l,apa.ta 
a,n<l I~rew, t999; Siegel, 199,q)). 
Re ferences  
Cllinatsu Aolm and Dot@as Mcl(ec. 1990. Acquiring 
predicate-argument mapping information in multilingual 
texts. In Branimir l\]oguraev and James l)ustetjovsky, ed- 
itors, Cou)us \])rocessinq.\[or l, cxieal Acq~tisition, pages 
191-202. MIq' Press. 
Michac'\] lh'ent. 1993. l"rom grammar to lexicon: UnSUl)er- 
vised learning o\[ lexical syntax. Compatational Linguis- 
tics, 1912):243 262. 
'lk:d Briscoe and ./ohn Cm'roll. 1997. Automatic extraction 
of subcategorization from corpora. In IS"ocs of the I"~Hh 
ANLP Co,@rence, pages 356-363. 
Michael John Collins. 1997. Three generative, lexicallsed 
models for statistical pro'sing. In lS"ocs of ACL '97, pages 
16-23. Madrid, Spain. 
Hot Trang Dang, Karin Kipper, Mm'tha l)almer, and 
.loseph l{osenzweig. 1998. Investigating regular sense 
exl, ensions 1oased on intersective 1,evin classes. \[n 
Procs of COI, ING-ACL '98, pages 293 299, Montreal, 
Canada. 
BOltnie 1)orr attd l)(n,g Jones. 1996. Role of word sense 
disambiguatlon i  lexical acquisition: lhedieting seman- 
tics from syntactic ues. In 15"oc. of UOL1NG'96, pages 
3;22 327, Col)enhage,l, )enmark. 
Boluiie l)orr, Joe t~larlliall, and Amy VVeinberg. 1995. 
\]~l:Oln syntactic encodillgs to thematic ro|es: l}uilding 
lexical entries for interlingual MT. Journal o\[ Machine 
!l'ran.?lation, 9(3):71- 100. 
\]\]onnie l)orr. 1997. I,m-ge-scale dictionary construction 
for foreign language tutoring and inter\]ingual machine. 
translation. Machine Translation, 12:1 55. 
K('.n l\[ale and Jay \](eyser. 1993. On argument structure 
and the lexical representation f syntactic relations. In 
I(. \]tale and .\]. l(eyser, editors, The View fl'om lJuilding 
20, pages 53-110. MIT Press. 
Maria Lapata and Chris Brew. 11999. Using subcategoriza- 
tion t,o resolve verb class ambiguity. In Frocs of Joint 
,5'IGDAT Con\[erence on Empirical Mett, ods in Natural 
Langaage, College Park, M\]). 
Beth Lcvin. 1993. English Verb Classes and Alternations. 
University of Chicago Press, Chicago, I\],. 
Christopher 1). Manning. :1993. Automatic acquisition of 
a large subcategorlzation dictionary l>om corpora. In 
l)rocs of A UL'93, pages 235 -242. Ohio State University. 
Diana McCarthy. 2000. Using semantic l)referenee Lo idcn- 
tit'y verb participation in role switchitlg alternations. In 
15"oes of NAA C1,-2000, Seattle, Washington. 
l)aola Merlo and Sllzantle Stevenson. 2000:-F, stablishing 
the upper-bound and inter-judge agremcnt in a verb 
classification task. In l)rocs of LI~EC-2000, pages 1659 
1G64. Athens, Greece. 
Akira Oishi and Yuji Matsumoto. 1997. l)etecting the or- 
ganization of semantic subclasses of Japanese verbs. In- 
ternational Journal of Corpus Linguistics, 2(1):65 89. 
,/. \]toss Quinlan. 1992. (?4.5 : l~rograms .for Maehi,ze 
Learning. Morgan l(aufmamt, San Mateo, CA. 
Sabine Sehulte im Walde. 1998. Automatic semanl.ic las- 
sification of verbs according to their alternation be- 
haviour. AIMS l\]eport 4(3), IMS, UniversitSt Stuttgart. 
I",eic V. Siegel. 1998. Linguistic Indicators .for L(mouaffc 
Undcrslandin 9 l)h.I), thesis, I)ept. of Comput(w Sci- 
ence, (Johm|l)ia University. 
Suzalllle SteveHsoll alld l)aola Merlo. 1997. l~exica\] st.ru(:- 
turc and \])ro(:essing complexity. Lanuagc and (7o\[pzilivc 
\])rocc:,~se.,% 12(1-2):3.'19 399. 
Suzanne Stevenson and \])aola Mer\]o. 1999. Verb classifi- 
cation using distributions of gt'ammatical features. In 
l)rocs of 1?,4 CL'99. Bergen, Norway. 
Suzmme Stevenson, l)aola Merlo, Natalia Kariaeva, and 
l(amin Whitehouse. 1999. Supervised learning of lexical 
semantic verb classes using fl'equency distributions. \[n 
I)rocs of Si.qLex '99, College Park, Maryland. 
Mort Webster and Mirth Mm'cus. 1989. Automatic ac- 
quisition of the lexical semanl, ics of verbs fl'om sentence 
frames. In Procs of A CL'89, pages 177-184, Vancouver, 
Canada. 
Append ix  A 
Unergatives: floated, galloped, glided, hiked, hopped, hur- 
ried, jogged , jumped, leaped, marched, paraded, raced, 
rushcd, seootcd, scurricd, skipped, tiptoed, t~vttcd, va,dtcd, 
wandered. Unaccusativcs: boiled, changcd, cleared, 
collapsed, cooled, cracked, dissolved, divided, exploded, 
flooded, .folded, fractured, hardcned, melted, opcncd, sim- 
mcred, solidified, stabilized, widened. Object-drops: bof 
rowed, eallcd, earvcd, clca~cd, danced, inheritcd, kiekcd, 
knittcd, oraniscd, packcd, paintcd, playcd, reaped, rcnted, 
skclehe.d, studied, swallowed, typed, washcd, ycllcd. 
821 
 
		
	
163
164
165
166
167
168
169
170
Automatically Constructing a Lexicon of
Verb Phrase Idiomatic Combinations
Afsaneh Fazly
Department of Computer Science
University of Toronto
Toronto, ON M5S 3H5
Canada
afsaneh@cs.toronto.edu
Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, ON M5S 3H5
Canada
suzanne@cs.toronto.edu
Abstract
We investigate the lexical and syntactic
flexibility of a class of idiomatic expres-
sions. We develop measures that draw
on such linguistic properties, and demon-
strate that these statistical, corpus-based
measures can be successfully used for dis-
tinguishing idiomatic combinations from
non-idiomatic ones. We also propose
a means for automatically determining
which syntactic forms a particular idiom
can appear in, and hence should be in-
cluded in its lexical representation.
1 Introduction
The term idiom has been applied to a fuzzy cat-
egory with prototypical examples such as by and
large, kick the bucket, and let the cat out of the
bag. Providing a definitive answer for what idioms
are, and determining how they are learned and un-
derstood, are still subject to debate (Glucksberg,
1993; Nunberg et al, 1994). Nonetheless, they are
often defined as phrases or sentences that involve
some degree of lexical, syntactic, and/or semantic
idiosyncrasy.
Idiomatic expressions, as a part of the vast fam-
ily of figurative language, are widely used both in
colloquial speech and in written language. More-
over, a phrase develops its idiomaticity over time
(Cacciari, 1993); consequently, new idioms come
into existence on a daily basis (Cowie et al, 1983;
Seaton and Macaulay, 2002). Idioms thus pose a
serious challenge, both for the creation of wide-
coverage computational lexicons, and for the de-
velopment of large-scale, linguistically plausible
natural language processing (NLP) systems (Sag
et al, 2002).
One problem is due to the range of syntactic
idiosyncrasy of idiomatic expressions. Some id-
ioms, such as by and large, contain syntactic vio-
lations; these are often completely fixed and hence
can be listed in a lexicon as ?words with spaces?
(Sag et al, 2002). However, among those idioms
that are syntactically well-formed, some exhibit
limited morphosyntactic flexibility, while others
may be more syntactically flexible. For example,
the idiom shoot the breeze undergoes verbal inflec-
tion (shot the breeze), but not internal modification
or passivization (?shoot the fun breeze, ?the breeze
was shot). In contrast, the idiom spill the beans
undergoes verbal inflection, internal modification,
and even passivization. Clearly, a words-with-
spaces approach does not capture the full range of
behaviour of such idiomatic expressions.
Another barrier to the appropriate handling of
idioms in a computational system is their seman-
tic idiosyncrasy. This is a particular issue for those
idioms that conform to the grammar rules of the
language. Such idiomatic expressions are indistin-
guishable on the surface from compositional (non-
idiomatic) phrases, but a computational system
must be capable of distinguishing the two. For ex-
ample, a machine translation system should trans-
late the idiom shoot the breeze as a single unit of
meaning (?to chat?), whereas this is not the case
for the literal phrase shoot the bird.
In this study, we focus on a particular class of
English phrasal idioms, i.e., those that involve the
combination of a verb plus a noun in its direct ob-
ject position. Examples include shoot the breeze,
pull strings, and push one?s luck. We refer to these
as verb+noun idiomatic combinations (VNICs).
The class of VNICs accommodates a large num-
ber of idiomatic expressions (Cowie et al, 1983;
Nunberg et al, 1994). Moreover, their peculiar be-
337
haviour signifies the need for a distinct treatment
in a computational lexicon (Fellbaum, 2005). De-
spite this, VNICs have been granted relatively lit-
tle attention within the computational linguistics
community.
We look into two closely related problems
confronting the appropriate treatment of VNICs:
(i) the problem of determining their degree of flex-
ibility; and (ii) the problem of determining their
level of idiomaticity. Section 2 elaborates on the
lexicosyntactic flexibility of VNICs, and how this
relates to their idiomaticity. In Section 3, we pro-
pose two linguistically-motivated statistical mea-
sures for quantifying the degree of lexical and
syntactic inflexibility (or fixedness) of verb+noun
combinations. Section 4 presents an evaluation
of the proposed measures. In Section 5, we put
forward a technique for determining the syntac-
tic variations that a VNIC can undergo, and that
should be included in its lexical representation.
Section 6 summarizes our contributions.
2 Flexibility and Idiomaticity of VNICs
Although syntactically well-formed, VNICs in-
volve a certain degree of semantic idiosyncrasy.
Unlike compositional verb+noun combinations,
the meaning of VNICs cannot be solely predicted
from the meaning of their parts. There is much ev-
idence in the linguistic literature that the seman-
tic idiosyncrasy of idiomatic combinations is re-
flected in their lexical and/or syntactic behaviour.
2.1 Lexical and Syntactic Flexibility
A limited number of idioms have one (or more)
lexical variants, e.g., blow one?s own trumpet and
toot one?s own horn (examples from Cowie et al
1983). However, most are lexically fixed (non-
productive) to a large extent. Neither shoot the
wind nor fling the breeze are typically recognized
as variations of the idiom shoot the breeze. Simi-
larly, spill the beans has an idiomatic meaning (?to
reveal a secret?), while spill the peas and spread
the beans have only literal interpretations.
Idiomatic combinations are also syntactically
peculiar: most VNICs cannot undergo syntactic
variations and at the same time retain their id-
iomatic interpretations. It is important, however,
to note that VNICs differ with respect to the degree
of syntactic flexibility they exhibit. Some are syn-
tactically inflexible for the most part, while others
are more versatile; as illustrated in 1 and 2:
1. (a) Tim and Joy shot the breeze.
(b) ?? Tim and Joy shot a breeze.
(c) ?? Tim and Joy shot the breezes.
(d) ?? Tim and Joy shot the fun breeze.
(e) ?? The breeze was shot by Tim and Joy.
(f) ?? The breeze that Tim and Joy kicked was fun.
2. (a) Tim spilled the beans.
(b) ? Tim spilled some beans.
(c) ?? Tim spilled the bean.
(d) Tim spilled the official beans.
(e) The beans were spilled by Tim.
(f) The beans that Tim spilled troubled Joe.
Linguists have explained the lexical and syntac-
tic flexibility of idiomatic combinations in terms
of their semantic analyzability (e.g., Glucksberg
1993; Fellbaum 1993; Nunberg et al 1994). Se-
mantic analyzability is inversely related to id-
iomaticity. For example, the meaning of shoot the
breeze, a highly idiomatic expression, has nothing
to do with either shoot or breeze. In contrast, a less
idiomatic expression, such as spill the beans, can
be analyzed as spill corresponding to ?reveal? and
beans referring to ?secret(s)?. Generally, the con-
stituents of a semantically analyzable idiom can be
mapped onto their corresponding referents in the
idiomatic interpretation. Hence analyzable (less
idiomatic) expressions are often more open to lex-
ical substitution and syntactic variation.
2.2 Our Proposal
We use the observed connection between id-
iomaticity and (in)flexibility to devise statisti-
cal measures for automatically distinguishing id-
iomatic from literal verb+noun combinations.
While VNICs vary in their degree of flexibility
(cf. 1 and 2 above; see also Moon 1998), on the
whole they contrast with compositional phrases,
which are more lexically productive and appear in
a wider range of syntactic forms. We thus propose
to use the degree of lexical and syntactic flexibil-
ity of a given verb+noun combination to determine
the level of idiomaticity of the expression.
It is important to note that semantic analyzabil-
ity is neither a necessary nor a sufficient condi-
tion for an idiomatic combination to be lexically
or syntactically flexible. Other factors, such as
the communicative intentions and pragmatic con-
straints, can motivate a speaker to use a variant
in place of a canonical form (Glucksberg, 1993).
Nevertheless, lexical and syntactic flexibility may
well be used as partial indicators of semantic ana-
lyzability, and hence idiomaticity.
338
3 Automatic Recognition of VNICs
Here we describe our measures for idiomaticity,
which quantify the degree of lexical, syntactic, and
overall fixedness of a given verb+noun combina-
tion, represented as a verb?noun pair. (Note that
our measures quantify fixedness, not flexibility.)
3.1 Measuring Lexical Fixedness
AVNIC is lexically fixed if the replacement of any
of its constituents by a semantically (and syntac-
tically) similar word generally does not result in
another VNIC, but in an invalid or a literal expres-
sion. One way of measuring lexical fixedness of
a given verb+noun combination is thus to exam-
ine the idiomaticity of its variants, i.e., expressions
generated by replacing one of the constituents by
a similar word. This approach has two main chal-
lenges: (i) it requires prior knowledge about the
idiomaticity of expressions (which is what we are
developing our measure to determine); (ii) it needs
information on ?similarity? among words.
Inspired by Lin (1999), we examine the strength
of association between the verb and noun con-
stituents of the target combination and its variants,
as an indirect cue to their idiomaticity. We use the
automatically-built thesaurus of Lin (1998) to find
similar words to the noun of the target expression,
in order to automatically generate variants. Only
the noun constituent is varied, since replacing the
verb constituent of a VNIC with a semantically re-
lated verb is more likely to yield another VNIC, as
in keep/lose one?s cool (Nunberg et al, 1994).
Let
 
		Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 883?890, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Exploiting a Verb Lexicon in Automatic Semantic Role Labelling
Robert S. Swier and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Ontario, Canada M5S 3G4
{swier,suzanne}@cs.toronto.edu
Abstract
We develop an unsupervised semantic role
labelling system that relies on the direct
application of information in a predicate
lexicon combined with a simple probabil-
ity model. We demonstrate the usefulness
of predicate lexicons for role labelling,
as well as the feasibility of modifying an
existing role-labelled corpus for evaluat-
ing a different set of semantic roles. We
achieve a substantial improvement over an
informed baseline.
1 Introduction
Intelligent language technologies capable of full
semantic interpretation of domain-general text re-
main an elusive goal. However, statistical advances
have made it possible to address core pieces of
the problem. Recent years have seen a wealth of
research on one important component of seman-
tic interpretation?automatic role labelling (e.g.,
Gildea and Jurafsky, 2002; Pradhan et al, 2004; Ha-
cioglu et al, 2004, and additional papers from Car-
reras and Marquez, 2004). Such work aims to an-
notate each constituent in a clause with a semantic
tag indicating the role that the constituent plays with
respect to the target predicate, as in (1):
(1) [Yuka]Agent [whispered]Pred to [Dar]Recipient
Semantic role labelling systems address a crucial
first step in the automatic extraction of semantic re-
lations from domain-general text, taking us closer to
the goal of comprehensive semantic mark-up.
Most work thus far on domain-general role la-
belling depends on supervised learning over statis-
tical features extracted from a hand-labelled corpus.
The reliance on such a resource?one in which the
arguments of each predicate are manually identified
and assigned a semantic role?limits the portability
of such methods to other languages or even to other
genres of corpora.
In this study, we explore the possibility of using a
verb lexicon, rather than a hand-labelled corpus, as
the primary resource in the semantic role labelling
task. Perhaps because of the focus on what can
be gleaned from labelled data, existing supervised
approaches have made little use of the additional
knowledge available in the predicate lexicon asso-
ciated with the labelled corpus. By contrast, we ex-
ploit the explicit knowledge of the role assignment
possibilities for each verb within an existing lexi-
con. Moreover, we utilize a very simple probability
model within a highly efficient algorithm.
We use VerbNet (Kipper et al, 2000), a computa-
tional lexicon which lists the possible semantic role
assignments for each of its verbs. Our algorithm
extracts automatically parsed arguments from a cor-
pus, and assigns to each a list of the compatible roles
according to VerbNet. Arguments which are given
only a single role possibility are considered to have
been assigned an unambiguous role label. This set
of arguments constitutes our primary-labelled data,
which serves as the noisy training data for a simple
probability model which is then used to label the re-
maining (role ambiguous) arguments.
This method has several advantages, the foremost
of which is that it eliminates the dependence on a
role labelled corpus, a very expensive resource to
produce. Of course, a verb lexicon is also an expen-
sive resource, but one that is highly reusable across a
range of NLP tasks. Moreover, the approach points
at some potentially useful information that current
883
supervised methods have failed to exploit. Even if
one has access to an annotated corpus for training,
our work shows that directly calling on additional
information from the lexicon itself may prove useful
in restricting the possible labels for an argument.
The method has disadvantages as well. The in-
formation available in a predicate lexicon is less di-
rectly applicable to building a learning model. In-
evitably, our results are noisier than in a super-
vised approach which has access to a labelled sam-
ple of what it must produce. Still, the method shows
promise: on unseen test data, the system yields an
F-measure of .83 on labelling of correctly extracted
arguments, compared to an informed baseline of .74,
and an F-measure of .65 (compared to .52) on the
overall identification and labelling task. The latter is
well below the best supervised performance of about
.80 on similar tasks, but it must be emphasized that
it is achieved with a simple probability model and
without the use of hand-labelled data. We view this
as a starting point by which to demonstrate the util-
ity of deriving more explicit knowledge from a pred-
icate lexicon, which can be later extended through
the use of additional probabilistic features.
We face a methodological challenge arising from
the particular choice of VerbNet for the prototyp-
ing of our method: the lexicon has no associated
semantic role labelled corpus. While this under-
scores the need for approaches which do not rely
on such a resource, it also means that we lack a
labelled sample of data against which to evaluate
our results. To address this, we use the existing
labelled corpus of FrameNet (Baker et al, 1998),
and develop a mapping for converting the FrameNet
roles to corresponding VerbNet roles. Our mapping
method demonstrates the possibility of leveraging
existing resources to support the development of role
labelling systems based on verb lexicons that do not
have an associated hand-labelled corpus.
2 VerbNet Roles and the Role Mapping
Before describing our labelling algorithm, we first
briefly introduce the semantic role information
available in VerbNet, and describe how we map
FrameNet roles to VerbNet roles.
whisper
Frames:
Agent V
Agent V Prep(+dest) Recipient
Agent V Topic
Verbs in same (sub)class:
[bark, croon, drone, grunt, holler, ...]
Figure 1: A portion of a VerbNet entry.
2.1 The VerbNet Lexicon
VerbNet is a manually developed hierarchical lexi-
con based on the verb classification of Levin (1993).
For each of almost 200 classes containing a total of
3000 verbs, VerbNet specifies the syntactic frames
along with the semantic role assigned to each argu-
ment position of a frame.1 Figure 1 shows an exam-
ple VerbNet entry. The thematic roles used in Verb-
Net are more general than the situation-specific roles
of FrameNet. For example, the roles Speaker, Mes-
sage, and Addressee of a Communication verb such
as whisper in FrameNet would be termed Agent,
Topic, and Recipient in VerbNet. These coarser-
grained roles are often assumed in linguistic the-
ory, and have some advantages in terms of capturing
commonalities of argument relations across a wide
range of predicates.
2.2 Mapping FrameNet to VerbNet Roles
As noted, VerbNet lacks a corpus of example role as-
signments against which to evaluate a role labelling
based upon it. We create such a resource by adapting
the existing FrameNet corpus. We formulate a map-
ping between FrameNet?s larger role set and Verb-
Net?s much smaller one, and create a new corpus
with our mapped roles substituted for the original
roles in the FrameNet corpus.
We perform the mapping in three steps. First we
use an existing mapping between the semantically-
specific roles in FrameNet and a much smaller inter-
mediate set of 39 semantic roles which subsume all
FrameNet roles.2 The associations in this mapping
are straightforward?e.g., the Place role for Abusing
verbs and the Area role for Operate-vehicle verbs are
both mapped to Location.
1Throughout the paper we use the term ?frame? to refer to
a syntactic frame?a configuration of syntactic arguments of a
verb?possibly labelled with roles, as in Figure 1.
2This mapping was provided by Roxana Girju, UIUC.
884
Second, from this intermediate set we create a
simple mapping to the set of 22 VerbNet roles. Some
roles are unaffected by the mapping (e.g., Cause
alone in the intermediate set maps to Cause in the
VerbNet set). Other roles are merged (e.g., Degree
and Measure both map to Amount). Moreover, some
roles in FrameNet (and the intermediate set) must be
mapped to more than one VerbNet role. For exam-
ple, an Experiencer role in FrameNet is considered
Experiencer by some VerbNet classes, but Agent by
others. In such cases, our mappings in this step must
be specific to the VerbNet class.
In this second step, some roles have no subsum-
ing VerbNet role, because FrameNet provides roles
for a wider variety of relations. For example, both
FrameNet and the intermediate role set contain a
Manner role, which VerbNet does not have. We
create a catch-all label, ?NoRole,? to which we
map eight such intermediate roles: Condition, Man-
ner, Means, Medium, Part-Whole, Property, Pur-
pose, and Result. These phrases labelled NoRole are
adjuncts?constituents not labelled by VerbNet.
In the third step of our mapping, some of the roles
in VerbNet?such as Theme and Topic, Asset and
Amount?which appear to be too-fine grained for us
to distinguish reliably, are mapped to a more coarse-
grained set of VerbNet roles. The final set consists
of 16 roles: Agent, Amount, Attribute, Beneficiary,
Cause, Destination, Experiencer, Instrument, Loca-
tion, Material, Predicate, Recipient, Source, Stimu-
lus, Theme and Time; plus the NoRole label.
3 The Frame Matching Process
A main goal of our system is to demonstrate the
usefulness of predicate lexicons for the role la-
belling task. The primary way that we apply the
knowledge in our lexicon is via a process we call
frame matching, adapted from Swier and Steven-
son (2004). The automatic frame matcher aligns
arguments extracted from an automatically parsed
sentence with the frames in VerbNet for the target
verb in the sentence. The output of this process is
a highly constrained set of candidate roles (possi-
bly of size one) for each potential argument. The
resulting singleton sets constitute a (noisy) role as-
signment for their corresponding arguments, form-
ing our primary-labelled data. This data is then used
to train a probability model, described in Section 4,
which we employ to label the remaining arguments
(those having more than one candidate role).
3.1 Initialization of Candidate Roles
The frame matcher construes extracted arguments
from the parsed sentence as being in one of the
four main types of syntactic positions (or slots) used
by VerbNet frames: subject, object, indirect object,
and PP-object.3 Additionally, we specialize the lat-
ter by the individual preposition, such as ?object of
for.? For the first three slot types, alignment be-
tween the extracted arguments and the frames is rel-
atively straightforward. An extracted subject would
be aligned with the subject position in a VerbNet
frame, for instance, and the subject role from the
frame would be listed as a possible label for the ex-
tracted subject.
The alignment of PP-objects is similar to that
of the other slot types, except that we add an ad-
ditional constraint that the associated prepositions
must match. For PP-object slots, VerbNet frames of-
ten provide an explicit list of allowable prepositions.
Alternatively, the frame may specify a required se-
mantic feature such as +path or +loc. In order
for an extracted PP-object to align with one of these
frame slots, its associated preposition must be in-
cluded in the list provided by the frame, or have the
specified feature. To determine the latter, we manu-
ally create lists of prepositions that we judge to have
each of the possible semantic features.
In general, this matching procedure assumes that
frames describing a syntactic argument structure
similar to that of the parsed sentence are more likely
to correctly describe the semantic roles of the ex-
tracted arguments. Thus, the frame matcher only
chooses roles from frames that are the best syntac-
tic matches with the extracted argument set. This
is achieved by adopting the scoring method of Swier
and Stevenson (2004), in which we compute the por-
tion %Frame of frame slots that can be mapped to
an extracted argument, and the portion %Sent of
extracted arguments from the sentence that can be
mapped to the frame. The score for each frame is
given by %Frame+%Sent, and only frames having
the highest score contribute candidate roles to the
3Since VerbNet has very few verbs with sentential comple-
ments, we do not consider them for now.
885
Extracted Slots
Possible Frames for Verb V SUBJ OBJ %Frame %Sent Score
Agent V Agent 100 50 150
Agent V Theme Agent Theme 100 100 200
Instrument V Theme Instrument Theme 100 100 200
Agent V Recipient Theme Agent Theme 67 100 167
Table 1: An example of frame matching.
extracted arguments. An example scoring is shown
in Table 1. Note that two of the frames are tied for
the highest score of 200, resulting in two possible
roles for the subject (Agent and Instrument), and
Theme as the only possible role for the object.
As mentioned, this frame matching step is very
restrictive, and it greatly reduces role ambiguity.
Many potential arguments receive only a single can-
didate role, providing the primary-labelled data we
use to train our probability model. Some slots re-
ceive no candidate roles, which is an error for argu-
ment slots but which is correct for adjuncts. The re-
duction of candidate roles in general is very helpful
in lightening the subsequent load on the probability
model to be applied next, but note that it may also
cause the correct role to be omitted. We experiment
with choosing roles from the frames that are the best
syntactic matches, and from all possible frames.
3.2 Adjustments to the Role Mapping
We further extend the frame matcher, which has ex-
tensive knowledge of VerbNet, for the separate task
of helping to eliminate some of the inconsistencies
that are introduced by our role mapping procedure.
This is a process that applies concurrently with the
initialization of candidate roles described above, but
only affects the gold standard labelling of evaluation
data.4
For instance, FrameNet assigns the role Side2 to
the object of the preposition with occurring with the
verb brawl. Side2 is mapped to Theme by our role
mapping; however, in VerbNet, brawl does not ac-
cept Theme as the object of with. Our mapping thus
creates a target (i.e., gold standard) label in the eval-
uation data that is inconsistent with VerbNet. Since
there is no possibility of the role labeller assigning a
label that matches such a target, this unfairly raises
4Of course, the fact that the frame matcher ?sees? the evalu-
ation set as part of its dual duties is not allowed to influence its
assignment of candidate roles.
the task difficulty. However, since brawl does ac-
cept Theme in another slot, it is not an option to
entirely eliminate this role in the mapping for the
verb. Instead, we use our frame matcher to verify
that each target role generated by our mapping from
FrameNet is allowed by VerbNet in the relevant slot.
If the target role is not allowed, then it is converted to
NoRole in the evaluation set. Constituents labelled
as NoRole are not considered target arguments, and
it is correct for the system to not assign labels in
these cases.
The NoRole conversions help to ensure that our
gold standard evaluation data is consistent with our
lexicon, but the method does have limitations. For
instance, some of the arguments which the sys-
tem fails to extract might have had their target role
changed to NoRole if they were properly extracted.
Additionally, in some cases a target role is converted
to NoRole when there is an actual role that VerbNet
would have assigned instead.
4 The Probability Model
Once argument slots are initialized with sets of pos-
sible roles, the algorithm uses a probability model
to label slots having two or more possibilities. Since
our primary goal is to demonstrate how much can be
accomplished through the frame matcher, we com-
pare a number of very simple probability models:
? P(r|v, s): the probability of a role given the
target verb and the slot; the latter includes sub-
ject, object, indirect object, and prepositional
object, where each PP slot is specialized by the
identity of the preposition;
? P(r|s): the probability of a role given the slot;
? P(r|sc): the probability of a role given the slot
class, in which all prepositional slots are treated
together.
886
Each probability model predicts a role given certain
conditioning information, with maximum likelihood
estimates determined by the primary-labelled data
directly resulting from the frame matching step.5
We also compare one non-probabilistic model to
resolve the same set of ambiguous cases:
? Default assignment: candidate roles for am-
biguous slots are ignored; the four slot classes
of subject, object, indirect object and PP-object
are assigned the roles Agent, Theme, Recipi-
ent, and Location, respectively.
These are the most likely roles assigned by the frame
matcher over our development data.
For comparison, we also apply the iterative algo-
rithm developed by Swier and Stevenson (2004), us-
ing the same bootstrapping parameters. The method
uses backoff over three levels of specificity of prob-
abilities.
5 Materials and Methods
5.1 The Target Verbs
For ease of comparison, we use the same verbs as in
Swier and Stevenson (2004), except that we measure
performance over a much larger superset of verbs. In
that work, a core set of 54 target verbs are selected
to represent a variety of classes with interesting role
ambiguities, and the system is evaluated against only
those verbs. An additional 1105 verbs?all verbs
sharing at least one class with the target verbs?are
also labelled, in order to provide more data for the
probability estimations. Here, we consider our sys-
tem?s performance over the 1159 target verbs that
consist of the union of these two sets of verbs.
5.2 The Corpus and Preprocessing
The majority of sentences in FrameNet II are taken
from the British National Corpus (BNC Reference
Guide, 2000). Our development and test data con-
sists of a percentage of these sentences. For some
experiments, these sentences are then merged with
a random selection of additional sentences from the
BNC in order to provide more training data for the
probability estimations. We evaluate performance
5Note that we assume the probability of a role for a slot is in-
dependent of other slots?that is, we do not ensure a consistent
role assignment to all arguments across an instance of a verb.
only on FrameNet sentences that include our target
verbs.
All of our corpus data was parsed using the
Collins parser (Collins, 1999). Next, we use TGrep2
(Rohde, 2004) to automatically extract from the
parse trees the constituents forming potential argu-
ments of the target verbs. For each verb, we label as
the subject the lowest NP node, if it exists, that is im-
mediately to the left of a VP node which dominates
the verb. Other arguments are identified by finding
sister NP or PP nodes to the right of the verb. Heads
of noun phrases are identified using the method of
Collins (1999), which primarily chooses the right-
most noun in the phrase that is not inside a preposi-
tional phrase or subordinate clause. Error may be in-
troduced at each step of this preprocessing?the sen-
tence may be misparsed, some arguments (such as
distant subjects) may not be extracted, or the wrong
word may be found as the phrase head.
5.3 Validation and Test Data
A random selection of 30% of the preprocessed
FrameNet data is set aside for testing, and another
random 30% is used for development and valida-
tion. For experiments involving additional BNC
data, each 30% of the FrameNet sentences is em-
bedded in a random selection of 20% of the BNC.
We selected these percentages to yield a sufficient
amount of data for experimentation, while reserving
some unseen data for future work. The FrameNet
portion of the validation set includes 515 types of
our target verbs (across 161 VerbNet classes) in
4300 sentences, and contains a total of 6636 target
constituents?i.e., constituents that receive a valid
VerbNet role as their gold standard label, not No-
Role. The test set includes 517 of the target verbs
(from 163 classes) in 4308 sentences, yielding 6705
target constituents.6
To create an evaluation set, we map the manually
annotated FrameNet roles in the corpus to VerbNet
roles (or NoRole), as described in Sections 2.2 and
3.2. We use this role information to calculate perfor-
mance: the system should assign roles matching the
target VerbNet roles, and make no assignment when
the target is NoRole.
6The verbs appearing in the validation and test sets occur
respectively across 161 and 165 FrameNet classes (what in
FrameNet are called ?frames?).
887
5.4 Methods of Argument Identification
One of the decisions we face is how to evaluate the
identification of extracted arguments generated by
the system against the manually annotated target ar-
guments provided by FrameNet. We try two meth-
ods, the most strict of which is to require full-phrase
agreement: an extracted argument and a target ar-
gument must cover exactly the same words in the
sentence in order for the argument to be considered
correctly extracted. This means, for instance, that
a prepositional phrase incorrectly attached to an ex-
tracted object would render the object incompatible
with the target argument, and any system label on
it would be counted as incorrect. This evaluation
method is commonly used in other work (e.g., Car-
reras and Marquez, 2004).
The other method we use is to require that only
the head of an extracted argument and a target argu-
ment match. This latter method helps to provide a
fuller picture of the range of arguments found by the
system, since there are fewer near-misses caused by
attachment errors. Since heads of phrases are often
the most semantically relevant part of an argument,
labels on heads provide much of the same informa-
tion as labels on whole phrases. For these reasons,
we use head matching for most of our experiments
below. For comparison, however, we provide results
based on full-phrase matching as well.
6 Experimental Results
6.1 Experimental Setup
We evaluate our system?s performance on several as-
pects of the overall role labelling task; all results are
given in terms of F-measure, 2PR/(P + R).7 The
first task is argument identification, in which con-
stituents considered by our system to be arguments
(i.e., those that are extracted and labelled) are eval-
uated against actual target arguments. The second
task is labelling extracted arguments, which evalu-
ates the labelling of only those arguments that were
correctly extracted. Last is the overall role labelling
task, which evaluates the system on the combined
tasks of identification and labelling of all target ar-
guments.
We compare our results to an informed baseline
that has access to the same set of extracted argu-
7In each case, P and R are close in value.
ments as does the frame matcher. The baseline la-
bels all extracted arguments using the default role
assignments described in Section 4.
In addition to experiments in which we employ
various methods of resolving ambiguous assign-
ments, we also evaluate the system with varying
types and amounts of training data, and with two al-
ternate methods for choosing frames from which to
draw candidate roles.
6.2 Evaluation of Probability Models
We first evaluate our system with the three very
simple probability models, as well as the non-
probabilistic default assignment, to determine roles
for the extracted arguments that the frame matcher
considers to be ambiguous. We also report results
after only the frame matcher has been applied, to
indicate how much work is being done by it alone.
Because we have constructed the frame matcher to
be highly restrictive in assigning candidate roles to
extracted arguments, a large number (about 62%)
become primary-labelled data and so do not require
resolution of ambiguous roles. Only about 16% of
our extracted arguments have role ambiguities, and
about 22% (many of which are adjuncts) do not re-
ceive any candidates and remain unlabelled.
Task: Id. Lab. Id. + Lab.
Baseline .80 .74 .52
FM + P (r|sc) .83 .83 .65
FM + P (r|s) .83 .84 .65
FM + P (r|v, s) .83 .78 .61
FM + Dflt. Assgnmt. .83 .82 .64
FM only .83 .76 .60
As shown in the table, all models perform equally
well on identification, which is determined by the
frame matcher (FM); i.e., any extracted argument
receiving one or more candidate roles is ?identi-
fied? as an argument. Performance is somewhat
above the baseline, which must label all extracted
arguments. For the task of labelling correctly ex-
tracted arguments and for the combined task, the
simplest probability models, P (r|sc) and P (r|s),
perform about the same. On the combined task, they
achieve .13 above the informed baseline, indicating
the effectiveness of such simple models when com-
bined with the frame matcher. The more specific
model, P (r|v, s), performs less well, and may be
over-fitting on this relatively small amount of train-
ing data.
888
Two observations indicate the power of the frame
matcher. First, even using the non-probabilistic de-
fault assignments to resolve ambiguous roles sub-
stantially outperforms the baseline (and indeed per-
forms quite close to the best results, since the default
role assignment is often the same as that chosen by
the probability models). Importantly, the baseline
uses the same default assignments, but without the
benefit of the frame matcher to further narrow down
the possible arguments. Second, the frame matcher
alone achieves .60 F-measure on the combined task,
not far below the performance of the best models.
These results show that once arguments have been
extracted, much of the labelling work is performed
by the frame matcher?s careful application of lexical
information.
Henceforth we consider the use of the frame
matcher plus P (r|sc) as our basic system, since this
is our simplest model, and no other outperforms it.
6.3 Evaluation of Training Methods
In our above experiments, the probabilistic mod-
els are trained only on primary-labelled data from
the frame matcher run on the FrameNet data. We
would like to determine whether using either more
data or less noisy data may improve results. To pro-
vide more data, we ran the frame matcher on the
additional 20% of the BNC. This provides almost
600K more sentences containing our target verbs,
yielding a much higher amount of primary-labelled
data. To provide less-noisy data, we trained the
probability models on manually annotated target la-
bels from system-identified arguments in 1000 sen-
tences. While fewer sentences are used, all argu-
ments in the training data are guaranteed to have a
correct role assignment, in contrast to the primary-
labelled data output by the frame matcher. (We
chose 1000 sentences as an upper bound on an
amount of data that could be relatively easily anno-
tated by human judges.)
Training Prim.-lab. Prim.-lab. 1K sents
Data: FN BNC annot?d
Baseline .52
FM + P (r|sc) .65 .65 .65
FM + P (r|v, s) .61 .62 .63
For our basic model, P (r|sc), these variations in
training data do not affect performance. Only the
most specific model, P (r|v, s), shows improvement
when trained on more data or on manually annotated
data, although it still does not perform as well as the
simplest model. Because the models only choose
from among candidate roles selected by the frame
matcher, differences in the learned probability esti-
mations must be quite large to have an effect. At
least for the simplest model, these estimations do
not vary with a larger corpus or one lacking in noise.
However, the increase in performance seen here for
the more specific model, albeit small, may indicate
that richer probability models may require more or
cleaner training data.
6.4 Evaluation of Frame Choice
?Best? frames All Frames
Baseline .52
FM + P (r|sc) .65 .63
The frame matcher has been shown to shoulder
much of the responsibility in our system, and it is
worth considering variations in its operation. For
example, by having the frame matcher only choose
roles from the frames that are the best syntactic
matches to the sentence, role ambiguity is mini-
mized at the cost of possibly excluding the correct
role. To determine whether we may do better by re-
lying more on the probability model and less on the
frame matcher, we instead include role candidates
from all frames in a verb?s lexical entry. The effect
of this choice is more role ambiguity, decreasing the
number of primary-labelled slots by roughly 30%.
We see that performance using P (r|sc) is slightly
worse with the greater ambiguity admitted by using
all frames, indicating the benefit of precise selection
of candidate roles.
6.5 Differing Argument Evaluation Methods
Heads Full Phrase
Baseline .52 .49
FM + P (r|sc) .65 .61
As mentioned, for most of our evaluations we match
the arguments extracted by the system to the tar-
get arguments via a match on phrase heads, since
head labels provide much useful semantic informa-
tion. When we instead require that the extracted
arguments match the targets exactly, the number of
correctly extracted arguments falls from about 80%
of the roughly 6700 targets to about 74%, due to in-
creased parsing difficulty. As expected, this results
889
in both the system and the baseline having perfor-
mance decreases on the overall task.
7 Related Work
Most role labelling systems have required hand-
labelled training data. Two exceptions are the sub-
categorization frame based work of Atserias et al
(2001) and the bootstrapping labeller of Swier and
Stevenson (2004), but both are evaluated on only a
small number of verbs and arguments. In related un-
supervised tasks, Riloff and colleagues have learned
?case frames? for verbs (e.g., Riloff and Schmelzen-
bach, 1998), while Gildea (2002) has learned role-
slot mappings (but does not apply the knowledge for
the labelling task).
Other role labelling systems have also relied on
the extraction of much more complex features or
probability models than we adopt here. As a point
of comparison, we apply the iterative backoff model
from Swier and Stevenson (2004), trained on 20% of
the BNC, with our frame matcher and test data. The
backoff model achieves an F-measure of .63, slightly
below the performance of .65 for our simplest proba-
bility model, which uses less training data and takes
far less time to run (minutes rather than hours).
In general, it is not possible to make direct com-
parisons between our work and most other role la-
bellers because of differences in corpora and role
sets, and, perhaps more significantly, differences in
the selection of target arguments. However, the
best supervised systems, using automatic parses to
identify full argument phrases in PropBank, achieve
about .82 on the task of identifying and labelling
arguments (Pradhan et al, 2004). Though this is
higher than our performance of .61 on full phrase ar-
guments, our system does not require manually an-
notated data.
8 Conclusion
In this work, we employ an expensive but highly
reusable resource?a verb lexicon?to perform role
labelling with a simple probability model and a
small amount of unsupervised training data. We out-
perform similar work that uses much more data and
a more complex model, showing the benefit of ex-
ploiting lexical information directly. To achieve per-
formance comparable to that of supervised methods
may require human filtering or augmentation of the
initial labelling. However, given the expense of pro-
ducing a large semantically annotated corpus, even
such ?human in the loop? approaches may lead to
a decrease in overall resource demands. We use
such a corpus for evaluation purposes only, modi-
fying it with a role mapping to correspond to our
lexicon. We thus demonstrate that such existing re-
sources can be bootstrapped for lexicons lacking an
associated annotated corpus.
Acknowledgments
We gratefully acknowledge the support of NSERC
of Canada. We also thank Afsaneh Fazly, who as-
sisted with much of our corpus pre-processing.
References
J. Atserias, L. Padro?, and G. Rigau. 2001. Integrating multiple
knowledge sources for robust semantic parsing. In Proc. of
the International Conf. on Recent Advances in NLP.
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berkeley
FrameNet Project. In Proc. of COLING-ACL, p. 86?90.
BNC Reference Guide. 2000. Reference Guide for the British
National Corpus (World Edition), second edition.
X. Carreras and L. Marquez, editors. 2004. CoNLL-04 Shared
Task.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
D. Gildea. 2002. Probabilistic models of verb-argument struc-
ture. In Proc. of the 19th International CoNLL, p. 308?314.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of seman-
tic roles. Computational Linguistics, 23(3):245?288.
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and D. Ju-
rafsky. 2004. Semantic role labeling by tagging syntactic
chunks. In Proc. of the 8th CoNLL, p. 110?113.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class based con-
struction of a verb lexicon. In Proc. of the 17th AAAI Conf.
B. Levin. 1993. English Verb Classes and Alternations: A Pre-
liminary Investigation. University of Chicago Press.
S. Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Jurafsky.
2004. Shallow semantic parsing using support vector ma-
chines. In Proc. of HLT/NAACL.
E. Riloff and M. Schmelzenbach. 1998. An empirical approach
to conceptual case frame acquisition. In Proc. of the 6th
WVLC.
D. L. T. Rohde. 2004. TGrep2 user manual ver. 1.11.
http://tedlab.mit.edu/ d?r/Tgrep2.
R. Swier and S. Stevenson. 2004. Unsupervised semantic role
labelling. In Proc. of the 2004 Conf. on EMNLP, p. 95?102.
890
Automatic Verb Classification 
Based on Statistical Distributions of 
Argument Structure 
Paola Merlo* 
University of Geneva 
Suzanne Stevenson  t 
University of Toronto 
Automatic acquisition of lexical knowledge is critical to a wide range of natural language pro- 
cessing tasks. Especially important is knowledge about verbs, which are the primary source of 
relational information in a sentence--the predicate-argument structure that relates an action 
or state to its participants (i.e., who did what to whom). In this work, we report on super- 
vised learning experiments o automatically classify three major types of English verbs, based 
on their argument structure--specifically, the thematic roles they assign to participants. We use 
linguistically-motivated statistical indicators extracted from large annotated corpora to train the 
classifier, achieving 69.8% accuracy for a task whose baseline is 34%, and whose expert-based 
upper bound we calculate at 86.5%. A detailed analysis of the performance ofthe algorithm and 
of its errors con~'rms that the proposed features capture properties related to the argument struc- 
ture of the verbs. Our results validate our hypotheses that knowledge about thematic relations 
is crucial for verb classification, and that it can be gleaned from a corpus by automatic means. 
We thus demonstrate aneffective combination of deeper linguistic knowledge with the robustness 
and scalability of statistical techniques. 
1. Introduction 
Automatic acquisition of lexical knowledge is critical to a wide range of natural an- 
guage processing (NLP) tasks (Boguraev and Pustejovsky 1996). Especially important 
is knowledge about verbs, which are the primary source of relational information in 
a sentence--the predicate-argument structure that relates an action or state to its par- 
ticipants (i.e., who did what to whom). In facing the task of automatic acquisition of 
knowledge about verbs, two basic questions must be addressed: 
What information about verbs and their relational properties needs to be 
learned? 
What information can in practice be learned through automatic means? 
In answering these questions, some approaches to lexical acquisition have focused on 
learning syntactic information about verbs, by automatically extracting subcategoriza- 
tion frames from a corpus or machine-readable dictionary (Brent 1993; Briscoe and 
Carroll 1997; Dorr 1997; Lapata 1999; Manning 1993; McCarthy and Korhonen 1998). 
* Linguistics Department; University of Geneva; 2 rue de Candolle; 1211 Geneva 4, Switzerland; merlo@ 
lettres.unige.ch 
t Department of Computer Science; University of Toronto; 6 King's College Road; Toronto, ON M5S 3H5 
Canada; suzanne@cs.toronto.edu 
Q 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 3 
Table 1 
Examples of verbs from the three optionally intransitive classes. 
Unergative The horse raced past the barn. 
The jockey raced the horse past the barn. 
Unaccusative The butter melted in the pan. 
The cook melted the butter in the pan. 
Object-Drop The boy played. 
The boy played soccer. 
Other work has attempted to learn deeper semantic properties uch as selectional re- 
strictions (Resnik 1996; Riloff and Schmelzenbach 1998), verbal aspect (Klavans and 
Chodorow 1992; Siegel 1999), or lexical-semantic verb classes uch as those proposed 
by Levin (1993) (Aone and McKee 1996; McCarthy 2000; Lapata and Brew 1999; Schulte 
im Walde 2000). In this paper, we focus on argument structure--the thematic roles as- 
signed by a verb to its arguments--as the way in which the relational semantics of 
the verb is represented at the syntactic level. 
Specifically, our proposal is to automatically classify verbs based on argument 
structure properties, using statistical corpus-based methods. We address the prob- 
lem of classification because it provides a means for lexical organization which can 
effectively capture generalizations over verbs (Palmer 2000). Within the context of 
classification, the use of argument structure provides a finer discrimination among 
verbs than that induced by subcategorization frames (as we see below in our example 
classes, which allow the same subcategorizations but differ in thematic assigmnent), 
but a coarser classification than that proposed by Levin (in which classes such as 
ours are further subdivided according to more detailed semantic properties). This 
level of classification granularity appears to be appropriate for numerous language 
engineering tasks. Because knowledge of argument structure captures fundamental 
participant/event relations, it is crucial in parsing and generation (e.g., Srinivas and 
Joshi \[1999\]; Stede \[1998\]), in machine translation (Dorr 1997), and in information re- 
trieval (Klavans and Kan 1998) and extraction (Riloff and Schmelzenbach 1998). Our 
use of statistical corpus-based methods to achieve this level of classification is moti- 
vated by our hypothesis that class-based differences in argument structure are reflected 
in statistics over the usages of the component verbs, and that those statistics can be 
automatically extracted from a large annotated corpus. 
The particular classification problem within which we investigate this hypothesis 
is the task of learning the three major classes of optionally intransitive verbs in English: 
unergative, unaccusative, and object-drop verbs. (For the unergative/unaccusative dis- 
tinction, see Perlmutter \[1978\]; Burzio \[1986\]; Levin and Rappaport Hovav \[1995\]). 
Table 1 shows an example of a verb from each class in its transitive and intransitive 
usages. These three classes are motivated by theoretical linguistic properties (see dis- 
cussion and references below, and in Stevenson and Merlo \[1997b\]; Merlo and Steven- 
son \[2000b\]). Furthermore, it appears that the classes capture typological distinctions 
that are useful for machine translation (for example, causative unergatives are un- 
grammatical in many languages), as well as processing distinctions that are useful for 
generating naturally occurring language (for example, reduced relatives with unerga- 
tive verbs are awkward, but they are acceptable, and in fact often preferred to full 
relatives for unaccusative and object-drop verbs) (Stevenson and Merlo 1997b; Merlo 
and Stevenson 1998). 
374 
Merlo and Stevenson Statistical Verb Classification 
Table 2 
Summary of thematic role assignments by class. 
Transitive Intransitive 
Classes Subject Object Subject 
Unergative Agent (of Causation) Agent Agent 
Unaccusative Agent (of Causation) Theme Theme 
Object-Drop Agent Theme Agent 
The question then is what underlies these distinctions. We identify the property 
that precisely distinguishes among these three classes as that of argument structure-- 
i.e., the thematic roles assigned by the verbs. The thematic roles for each class, and 
their mapping to subject and object positions, are summarized in Table 2. Note that 
verbs across these three classes allow the same subcategorization frames (taking an NP 
object or occurring intransitively); thus, classification based on subcategorization alone 
would not distinguish them. On the other hand, each of the three classes is comprised 
of multiple Levin classes, because the latter eflect more detailed semantic distinctions 
among the verbs (Levin 1993); thus, classification based on Levin's labeling would 
miss generalizations across the three broader classes. By contrast, as shown in Table 2, 
each class has a unique pattern of thematic assignments, which categorize the verbs 
precisely into the three classes of interest. 
Although the granularity of our classification differs from Levin's, we draw on her 
hypothesis that semantic properties of verbs are reflected in their syntactic behavior. 
The behavior that Levin focuses on is the notion of diathesis alternation--an alter- 
nation in the expression of the arguments of a verb, such as the different mappings 
between transitive and intransitive that our verbs undergo. Whether a verb partici- 
pates in a particular diathesis alternation or not is a key factor in Levin's approach to 
classification. We, like others in a computational framework, have extended this idea 
by showing that statistics over the alternants of a verb effectively capture information 
about its class (Lapata 1999; McCarthy 2000; Lapata and Brew 1999). 
In our specific task, we analyze the pattern of thematic assignments given in 
Table 2 to develop statistical indicators that are able to determine the class of an op- 
tionally intransitive verb by capturing information across its transitive and intransitive 
alternants. These indicators erve as input to a machine learning algorithm, under a 
supervised training methodology, which produces an automatic lassification system 
for our three verb classes. Since we rely on patterns of behavior across multiple occur- 
rences of a verb, we begin with the problem of assigning a single class to the entire 
set of usages of a verb within the corpus. For example, we measure properties across 
all occurrences of a word, such as raced, in order to assign a single classification to 
the lexical entry for the verb race. This contrasts with work classifying individual oc- 
currences of a verb in each local context, which have typically relied on training that 
includes instances of the verbs to be classified--essentially developing a bias that is 
used in conjunction with the local context o determine the best classification for new 
instances of previously seen verbs. By contrast, our method assigns a classification 
to verbs that have not previously been seen in the training data. Thus, while we do 
not as yet assign different classes to the instances of a verb, we can assign a single 
predominant class to new verbs that have never been encountered. 
To preview our results, we demonstrate hat combining just five numerical indi- 
cators, automatically extracted from large text corpora, is sufficient to reduce the error 
375 
Computational Linguistics Volume 27, Number 3 
rate in this classification task by more than 50% over chance. Specifically, we achieve 
almost 70% accuracy in a task whose baseline (chance) performance is 34%, and whose 
expert-based upper bound is calculated at 86.5%. 
Beyond the interest for the particular classification task at hand, this work ad- 
dresses more general issues concerning verb class distinctions based in argument 
structure. We evaluate our hypothesis that such distinctions are reflected in statis- 
tics over corpora through a computational experimental methodology in which we 
investigate as indicated each of the subhypotheses below, in the context of the three 
verb classes under study: 
? Lexical features capture argument structure differences between verb 
classes. 1 
? The linguistically distinctive features exhibit distributional differences 
across the verb classes that are apparent within linguistic experience (i.e., 
they can be collected from text). 
? The statistical distributions of (some of) the features contribute to 
learning the classifications of the verbs. 
In the following sections, we show that all three hypotheses above are borne out. In 
Section 2, we describe the argument structure distinctions of our three verb classes 
in more detail. In support of the first hypothesis above, we discuss lexical correlates 
of the underlying differences in thematic assignments hat distinguish the three verb 
classes under investigation. In Section 3, we show how to approximate these features 
by simple syntactic ounts, and how to perform these counts on available corpora. We 
confirm the second hypothesis above, by showing that the differences in distribution 
predicted by the underlying argument structures are largely found in the data. In 
Section 4, in a series of machine learning experiments and a detailed analysis of errors, 
we confirm the third hypothesis by showing that the differences in the distribution of 
the extracted features are successfully used for verb classification. Section 5 evaluates 
the significance of these results by comparing the program's accuracy to an expert- 
based upper bound. We conclude the paper with a discussion of its contributions, 
comparison to related work, and suggestions for future extensions. 
2. Deriving Classification Features from Argument Structure 
Our task is to automatically build a classifier that can distinguish the three major 
classes of optionally intransitive verbs in English. As described above, these classes 
are differentiated by their argument structures. In the first subsection below, we elab- 
orate on our description of the thematic role assignments for each of the verb classes 
under investigation--unergative, unaccusative, and object-drop. This analysis yields a 
distinctive pattern of thematic assignment for each class. (For more detailed iscussion 
concerning the linguistic properties of these classes, and the behavior of their compo- 
nent verbs, please see Stevenson and Merlo \[1997b\]; Merlo and Stevenson \[2000b\].) 
Of course, the key to any automatic classification task is to determine a set of useful 
features for discriminating the items to be classified. In the second subsection below, 
1 By lexical we mean features that we think are likely stored in the lexicon, because they are properties 
of words and not of phrases or sentences. Note, however, that some lexical features may not 
necessarily be stored with individual words--indeed, the motivation for classifying verbs to capture 
generalizations within each class uggests otherwise. 
376 
Merlo and Stevenson Statistical Verb Classification 
we show how the analysis of thematic distinctions enables us to determine lexical 
properties that we hypothesize will exhibit useful, detectable frequency differences 
in our corpora, and thus serve as the machine learning features for our classification 
experiments. 
2.1 The Argument Structure Distinctions 
The verb classes are exemplified below, in sentences repeated from Table 1 for ease of 
exposition. 
Unergative: (la) The horse raced past the barn. 
(lb) The jockey raced the horse past the barn. 
Unaccusative: (2a) The butter melted in the pan. 
(2b) The cook melted the butter in the pan. 
Object-Drop: (3a) The boy played. 
(3b) The boy played soccer. 
The example sentences illustrate that all three classes participate in a diathesis alter- 
nation that relates a transitive and intransitive form of the verb. However, according 
to Levin (1993), each class exhibits a different ype of diathesis alternation, which is 
determined by the particular semantic relations of the arguments othe verb. We make 
these distinctions explicit by drawing on a standard notion of thematic role, as each 
class has a distinct pattern of thematic assignments (i.e., different argument structures). 
We assume here that a thematic role is a label taken from a fixed inventory of 
grammaticalized semantic relations; for example, an Agent is the doer of an action, 
and a Theme is the entity undergoing an event (Gruber 1965). While admitting that 
such notions as Agent and Theme lack formal definitions (in our work and in the 
literature more widely), the distinctions are clear enough to discriminate our three 
verb classes. For our purposes, these roles can simply be thought of as semantic labels 
which are non-decomposable, but there is nothing in our approach that rests on this 
assumption. Thus, our approach would also be compatible with a feature-based deft- 
nition of participant roles, as long as the features capture such general distinctions as, 
for example, the doer of an action and the entity acted upon (Dowty 1991). 
Note that in our focus on verb class distinctions we have not considered finer- 
grained features that rely on more specific semantic features, such as, for example, 
that the subject of the intransitive melt must be something that can change from solid 
to liquid. While this type of feature may be important for semantic distinctions among 
individual verbs, it thus far seems irrelevant o the level of verb classification that 
we adopt, which groups verbs more broadly according to syntactic and (somewhat 
coarser-grained) semantic properties. 
Our analysis of thematic assignment--which was summarized in Table 2, repeated 
here as Table 3--is elaborated here for each verb class. The sentences in (1) above 
illustrate the relevant alternants ofan unergative verb, race. Unergatives are intransitive 
action verbs whose transitive form, as in (lb), can be the causative counterpart of the 
intransitive form (la). The type of causative alternation that unergatives participate in 
is the "induced action alternation" according to Levin (1993). For our thematic analysis, 
we note that the subject of an intransitive activity verb is specified to be an Agent. 
The subject of the transitive form is indicated by the label Agent of Causation, which 
indicates that the thematic role assigned to the subject is marked as the role which is 
377 
Computational Linguistics Volume 27, Number 3 
Table 3 
Summary of thematic assignments. 
Transitive Intransitive 
Classes Subject Object Subject 
Unergative Agent (of Causation) Agent Agent 
Unaccusative Agent (of Causation) Theme Theme 
Object-Drop Agent Theme Agent 
introduced with the causing event. In a causative alternation, the semantic argument 
of the subject of the intransitive surfaces as the object of the transitive (Brousseau and 
Ritter 1991; Hale and Keyser 1993; Levin 1993; Levin and Rappaport Hovav 1995). For 
unergatives, this argument is an Agent and thus the alternation yields an object in 
the transitive form that receives an Agent thematic role (Cruse 1972). These thematic 
assignments are shown in the first row of Table 3. 
The sentences in (2) illustrate the corresponding forms of an unaccusative verb, 
melt. Unaccusatives are intransitive change-of-state v rbs, as in (2a); the transitive 
counterpart for these verbs also exhibits a causative alternation, as in (2b). This is 
the "causative/inchoative alternation" (Levin, 1993). Like unergatives, the subject of 
a transitive unaccusative is marked as the Agent of Causation. Unlike unergatives, 
though, the alternating argument of an unaccusative (the subject of the intransitive 
form that becomes the object of the transitive) is an entity undergoing a change of 
state, without active participation, and is therefore a Theme. The resulting pattern of 
thematic assignments i indicated in the second row of Table 3. 
The sentences in (3) use an object-drop verb, play. These are activity verbs that 
exhibit a non-causative diathesis alternation, in which the object is simply optional. 
This is dubbed "the unexpressed object alternation" (Levin 1993), and has several 
subtypes that we do not distinguish ere. The thematic assignment for these verbs is 
simply Agent for the subject (in both transitive and intransitive forms), and Theme 
for the optional object; see the last row of Table 3. 
For further details and support of this analysis, please see the discussion in Steven- 
son and Merlo (1997b) and Merlo and Stevenson (2000b). For our purposes here, the 
important fact to note is that each of the three classes can be uniquely identified by 
the pattern of thematic assignments across the two alternants of the verbs. 
2.2 Features for Automatic Classification 
Our next task then is to derive, from these thematic patterns, useful features for au- 
tomatically classifying the verbs. In what follows, we refer to the columns of Table 3 
to explain how we expect he thematic distinctions to give rise to distributional prop- 
erties, which, when appropriately approximated through corpus counts, will discrim- 
inate across the three classes. 
Transitivity Consider the first two columns of thematic roles in Table 3, which illus- 
trate the role assignment in the transitive construction. The Prague school's notion 
of linguistic markedness (Jakobson 1971; Trubetzkoy 1939) enables us to establish a 
scale of markedness of these thematic assignments and make a principled prediction 
about their frequency of occurrence. Typical tests to determine the unmarked element 
of a pair or scale are simplicity--the unmarked element is simpler, distr ibution--the 
unmarked member is more widely attested across languages, and f requency--the un- 
378 
Merlo and Stevenson Statistical Verb Classification 
marked member is more frequent (Greenberg 1966; Moravcsik and Wirth 1983). The 
claim of markedness theory is that, once an element has been identified by one test 
as the unmarked element of a scale, then all other tests will be correlated. The three 
thematic assignments appear to be ranked on a scale by the simplicity and distribu- 
tion tests, as we describe below. From this, we can conclude that frequency, as a third 
correlated test, should also be ranked by the same scale, and we can therefore make 
predictions about the expected frequencies of the three thematic assignments. 
First, we note that the specification ofan Agent of Causation for transitive unerga- 
tives (such as race) and unaccusatives (such as melt) indicates a causative construction. 
Causative constructions relate two events, the causing event and the core event de- 
scribed by the intransitive verb; the Agent of Causation is the Agent of the causing 
event. This double event structure can be considered as more complex than the sin- 
gle event hat is found in a transitive object-drop verb (such as play) (Stevenson and 
Merlo 1997b). The simplicity test thus indicates that the causative unergatives and 
unaccusatives are marked in comparison to the transitive object-drop verbs. 
We further observe that the causative transitive of an unergative verb has an Agent 
thematic role in object position which is subordinated to the Agent of Causation in 
subject position, yielding an unusual "double agentive" thematic structure. This lex- 
ical causativization f unergatives (in contrast to analytic ausativization) is a distri- 
butionally rarer phenomenon--found i  fewer languages--than lexical causatives of 
unaccusatives. In asking native speakers about our verbs, we have found that lexical 
causatives of unergative verbs are not attested in Italian, French, German, Portuguese, 
Gungbe (Kwa family), and Czech. On the other hand, the lexical causatives are pos- 
sible for unaccusative rbs (i.e., where the object is a Theme) in all these languages. 
Vietnamese appears to allow a very restricted form of causativization f unergatives 
limited to only those cases that have a comitative reading. The typological distribu- 
tion test thus indicates that unergatives are more marked than unaccusatives in the 
transitive form. 
From these observations, we can conclude that unergatives ( uch as race) have 
the most marked transitive argument structure, unaccusatives ( uch as melt) have 
an intermediately marked transitive argument structure, and object-drops (such as 
play) have the least marked transitive argument structure of the three. Under the 
assumptions of markedness theory outlined above, we then predict hat unergatives 
are the least frequent in the transitive, that unaccusatives have intermediate frequency 
in the transitive, and that object-drop verbs are the most frequent in the transitive. 
Causativity Due to the causative alternation of unergatives and unaccusatives, the 
thematic role of the subject of the intransitive is identical to that of the object of the 
transitive, as shown in the second and third columns of thematic roles in Table 3. 
Given the identity of thematic role mapped to subject and object positions across the 
two alternants, we expect o observe the same noun occurring at times as subject of 
the verb, and at other times as object of the verb. In contrast, for object-drop verbs, 
the thematic role of the subject of the intransitive is identical to that of the subject of 
the transitive, not the object of the transitive. We therefore xpect hat it will be less 
common for the same noun to occur in subject and object position across instances of 
the same object-drop verb. 
Thus, we hypothesize that this pattern of thematic role assignments will be re- 
flected in a differential amount of usage across the classes of the same nouns as sub- 
jects and objects for a given verb. Generally, we would expect hat causative verbs 
(in our case, the unergative and unaccusative v rbs) would have a greater degree of 
overlap of nouns in subject and object position than non-causative transitive verbs (in 
379 
Computational Linguistics Volume 27, Number 3 
our case, the object-drop verbs). However, since the causative is a transitive use, and 
the transitive use of unergatives i expected to be rare (see above), we do not expect 
unergatives to exhibit a high degree of detectable overlap in a corpus. Thus, this over- 
lap of subjects and objects should primarily distinguish unaccusatives (predicted to 
have high overlap of subjects and objects) from the other two classes (each of which 
is predicted to have low \[detectable\] overlap of subjects and objects). 
Animacy Finally, considering the roles in the first and last columns of thematic assign- 
ments in Table 3, we observe that unergative and object-drop verbs assign an agentive 
role to their subject in both the transitive and intransitive, while unaccusatives a sign 
an agentive role to their subject only in the transitive. Under the assumption that the 
intransitive use of unaccusatives i  not rare, we then expect that unaccusatives will 
occur less often overall with an agentive subject han will the other two verb classes. 
(The assumption that unaccusatives are not rare in the intransitive is based on the 
linguistic complexity of the causative transitive alternant, and is borne out in our cor- 
pus analysis.) On the further assumption that Agents tend to be animate ntities more 
so than Themes are, we expect hat unaccusatives will occur less frequently with an 
animate subject compared to unergative and object-drop verbs. Note the importance 
of our use of frequency distributions: the claim is not that only Agents can be animate, 
but rather that nouns that receive an Agent role will more often be animate than nouns 
that receive a Theme role. 
Additional Features The above interactions between thematic roles and the syntactic 
expressions of arguments thus lead to three features whose distributional properties 
appear promising for distinguishing unergative, unaccusative and object-drop verbs: 
transitivity, causativity, and animacy of subject. We also investigate two additional syn- 
tactic features: the use of the passive or active voice, and the use of the past participle or 
simple past part-of-speech (POS) tag (VBN or VBD, in the Penn Treebank style). These 
features are related to the transitive/intransitive alternation, since a passive use implies 
a transitive use of the verb, as well as to the use of a past participle form of the verb .  2 
Table 4 summarizes the features we derive from the thematic properties, and our 
expectations concerning their frequency of use. We hypothesize that these five features 
will exhibit distributional differences in the observed usages of the verbs that can be 
used for classification. In the next section, we describe the actual corpus counts that we 
develop to approximate the features we have identified. (Notice that the counts will 
be imperfect approximations to the thematic knowledge, beyond the inevitable rrors 
due to automatic extraction from large automatically annotated corpora. Even when 
the counts are precise, they only constitute an approximation to the actual thematic 
notions, since the features we are using are not logically implied by the knowledge 
we want to capture, but only statistically correlated.) 
3. Data Collection and Analysis 
Clearly, some of the features we've proposed are difficult (e.g., the passive use) or im- 
possible (e.g., animate subject use) to automatically extract with high accuracy from a 
2 For our sample verbs, the statistical correlation between the transitive and passive features i highly 
significant (N ----- 59, R = .44, p =- .001), as is the correlation between the transitive and past participle 
features (N = 59, R = .36, p = .005). (Since, as explained in the next section, our features are expressed 
as proportions---e.g., percent transitive use out of detected transitive and intransitive use----correlations 
of intransitivity with passive or past participle use have the same magnitude but are negative.) 
380 
Merlo and Stevenson Statistical Verb Classification 
Table 4 
The features and expected behavior. 
Expected Frequency 
Feature Pattern Explanation 
Transitivity Unerg < Unacc < ObjDrop Unaccusatives and unergatives have a causative 
transitive, hence lower transitive use. Further- 
more, unergatives have an agentive object, hence 
very low transitive use. 
Causativity Unerg, ObjDrop < Unacc Object-drop verbs do not have a causal agent, 
hence low "causative" use. Unergatives are rare 
in the transitive, hence low causative use. 
Animacy Unacc < Unerg, ObjDrop Unaccusatives have a Theme subject in the in- 
transitive, hence lower use of animate subjects. 
Passive Voice Unerg K Unacc K ObjDrop Passive implies transitive use, hence correlated 
with transitive feature. 
VBN Tag Unerg < Unacc < ObjDrop Passive implies past participle use (VBN), hence 
correlated with transitive (and passive). 
large corpus, given the current state of annotation. However, we do assume that cur- 
rently available corpora, such as the Wall Street Journal (WSJ), provide a representative, 
and large enough, sample of language from which to gather corpus counts that can ap- 
proximate the distributional patterns of the verb class alternations. Our work draws on 
two text corpora--one an automatically tagged combined corpus of 65 million words 
(primarily WSJ), the second an automatically parsed corpus of 29 million words (a sub- 
set of the WSJ text from the first corpus). Using these corpora, we develop counting 
procedures that yield relative frequency distributions for approximations to the five 
linguistic features we have determined, over a sample of verbs from our three classes. 
3.1 Materials and Method 
We chose a set of 20 verbs from each class based primarily on the classification i  Levin 
(1993). 3 The complete list of verbs appears in Table 5; the group 1/group 2 designation 
is explained below in the section on counting. As indicated in the table, unergatives 
are manner-of-motion verbs (from the "run" class in Levin), unaccusatives are change- 
of-state verbs (from several of the classes in Levin's change-of-state super-class), while 
object-drop verbs were taken from a variety of classes in Levin's classification, all of 
which undergo the unexpressed object alternation. The most frequently used classes 
are verbs of change of possession, image-creation verbs, and verbs of creation and 
transformation. The selection of verbs was based partly on our intuitive judgment 
that the verbs were likely to be used with sufficient frequency in the WSJ. Also, each 
3 We used an equal number of verbs from each class in order to have a balanced group of items. One 
potential disadvantage of this decision is that each verb class is represented qually, even though they 
may not be equally frequent in the corpora. Although we lose the relative frequency information 
among the classes that could provide a better bias for assigning a default classification (i.e., the most 
frequent one), we have the advantage that our classifier will be equally informed (in terms of number 
of exemplars) about each class. 
Note that there are only 19 unaccusative rbs because ripped, which was initially counted in the 
unaccusatives, was then excluded from the analysis as it occurred mostly in a very different usage in 
the corpus (as verb+particle, in ripped off) from the intended optionally intransitive usage. 
381 
Computational Linguistics Volume 27, Number 3 
Table 5 
Verbs used in the experiments. 
Class Name Description Selected Verbs 
Unergative manner of motion jumped, rushed, marched, leaped, floated, raced, hurried, wan- 
dered, vaulted, paraded (group 1); galloped, glided, hiked, 
hopped, jogged, scooted, scurried, skipped, tiptoed, trotted 
(group 2). 
Unaccusative change of state opened, exploded, flooded, dissolved, cracked, hardened, boiled, 
melted, fractured, solidified (group 1); collapsed, cooled, 
folded, widened, changed, cleared, divided, simmered, stabi- 
lized (group 2). 
Object-Drop unexpressed 
object alternation 
played, painted, kicked, carved, reaped, washed, danced, 
yelled, typed, knitted (group 1); borrowed, inherited, orga- 
nized, rented, sketched, cleaned, packed, studied, swallowed, 
called (group 2). 
verb presents the same form in the simple past and in the past participle (the reg- 
ular "-ed" form). In order to simplify the counting procedure, we included only the 
"-ed" form of the verb, on the assumption that counts on this single verb form would 
approximate the distribution of the features across all forms of the verb. Additionally, 
as far as we were able given the preceding constraints, we selected verbs that could 
occur in the transitive and in the passive. Finally, we aimed for a frequency cut-off 
of 10 occurrences or more for each verb, although for unergatives we had to use one 
verb (jogged) that only occurred 8 times in order to have 20 verbs that satisfied the 
other criteria above. 
In performing this kind of corpus analysis, one has to recognize the fact that 
current corpus annotations do not distinguish verb senses. In these counts, we did 
not distinguish a core sense of the verb from an extended use of the verb. So, for 
instance, the sentence Consumer spending jumped 1.7% in February after a sharp drop the 
month before (WSJ 1987) is counted as an occurrence of the manner-of-motion verb jump 
in its intransitive form. This particular sense extension has a transitive alternant, but 
not a causative transitive (i.e., Consumer spending jumped the barrier . . . .  but not Low taxes 
jumped consumer spending... ). Thus, while the possible subcategorizations remain the 
same, rates of transitivity and causativity may be different han for the literal manner- 
of-motion sense. This is an unavoidable result of using simple, automatic extraction 
methods given the current state of annotation of corpora. 
For each occurrence of each verb, we counted whether it was in a transitive or 
intransitive use (TRANS), in a passive or active use (PASS), in a past participle or simple 
past use (VBN), in a causative or non-causative use (CAUS), and with an animate subject 
or not (ANIM). 4 Note that, except for the VBN feature, for which we simply extract he 
POS tag from the corpus, all other counts are approximations to the actual linguistic 
behaviour of the verb, as we describe in detail below. 
4 One additional feature was recorded--the log frequency ofthe verb in the 65 million word 
corpus--motivated by the conjecture that the frequency ofa verb may help in predicting its class. In 
our machine l arning experiments, however, this conjecture was not borne out, as the frequency feature 
did not improve performance. This is the case for experiments onall of the verbs, as well as for 
separate experiments onthe group 1 verbs (which were matched across the classes for frequency) and 
the group 2 verbs (which were not). We therefore limit discussion here to the thematically-motivated 
features. 
382 
Merlo and Stevenson Statistical Verb Classification 
The first three counts (TRANS, PASS, VBN) were performed on the tagged ACL/DCI  
corpus available from the Linguistic Data Consortium, which includes the Brown Cor- 
pus (of one million words) and years 1987-1989 of the Wall Street Journal, a combined 
corpus in excess of 65 million words. The counts for these features proceeded as fol- 
lows: 
? TRANS: A number, a pronoun, a determiner, an adjective, or a noun were 
considered to be indication of a potential object of the verb. A verb 
occurrence preceded by forms of the verb be, or immediately followed by 
a potential object was counted as transitive; otherwise, the occurrence 
was counted as intransitive (specifically, if the verb was followed by a 
punctuation s ign- -commas,  colons, full s tops- -or  by a conjunction, a
particle, a date, or a preposition.) 
* PASS: A main verb (i.e., tagged VBD) was counted as active. A token 
with tag VBN was also counted as active if the closest preceding 
auxiliary was have, while it was counted as passive if the closest 
preceding auxil iary was be. 
? VBN: The counts for VBN/VBD were simply done based on the POS 
label within the tagged corpus. 
Each of the above three counts was normalized over all occurrences of the "-ed" form 
of the verb, yielding a single relative frequency measure for each verb for that feature; 
i.e., percent ransitive (versus intransitive) use, percent active (versus passive) use, and 
percent VBN (versus VBD) use, respectively. 
The last two counts (CAUS and ANIM) were performed on a parsed version of the 
1988 year of the Wall Street Journal, so that we could extract subjects and objects of 
the verbs more accurately. This corpus of 29 million words was provided to us by 
Michael Collins, and was automatically parsed with the parser described in Collins 
(1997). 5The counts, and their justification, are described here: 
CAUS: As discussed above, the object of a causative transitive is the same 
semantic argument of the verb as the subject of the intransitive. The 
causative feature was approximated by the following steps, intended to 
capture the degree to which the subject of a verb can also occur as its 
object. Specifically, for each verb occurrence, the subject and object (if 
there was one) were extracted from the parsed corpus. The observed 
subjects across all occurrences of the verb were placed into one multiset 
of nouns, and the observed objects into a second multiset of nouns. (A 
multiset, or bag, was used so that our representation i dicated the 
number  of times each noun was used as either subject or object.) Then, 
the proport ion of overlap between the two multisets was calculated. We 
define overlap as the largest multiset of elements belonging to both the 
5 Readers might be concerned about he portability of this method to languages for which no large 
parsed corpus is available. It is possible that using a fully parsed corpus is not necessary. Our results 
were replicated in English without he need for a fully parsed corpus (Anoop Sarkar, p.c., citing a 
project report by Wootiporn Tripasai). Our method was applied to 23 million words of the WSJ that 
were automatically tagged with Ratnaparkhi's maximum entropy tagger (Ratnaparkhi 1996) and 
chunked with the partial parser CASS (Abney 1996). The results are very similar to ours (best accuracy 
66.6%), suggesting that a more accurate tagger than the one used on our corpus might in fact be 
sufficient to overcome the fact that no full parse is available. 
383 
Computational Linguistics Volume 27, Number 3 
subject and the object multisets; e.g., the overlap between (a, a, a, b} and 
{a} is {a,a,a}. The proportion is the ratio between the cardinality of the 
overlap multiset, and the sum of the cardinality of the subject and object 
multisets. For example, for the simple sets of characters above, the ratio 
would be 3/5, yielding a value of .60 for the CAUS feature. 
ANIM: A problem with a feature like animacy is that it requires either 
manual determination f the animacy of extracted subjects, or reference 
to an on-line resource such as WordNet for determining animacy. To 
approximate animacy with a feature that can be extracted automatically, 
and without reference to a resource xternal to the corpus, we take 
advantage of the well-attested animacy hierarchy, according to which 
pronouns are the most animate (Silverstein 1976; Dixon 1994). The 
hypothesis that the words I, we, you, she, he, and they most often refer 
to animate ntities. This hypothesis was confirmed by extracting 
100 occurrences of the pronoun they, which can be either animate or 
inanimate, from our 65 million word corpus. The occurrences 
immediately preceded a verb. After eliminating repetitions, 
94 occurrences were left, which were classified by hand, yielding 71 
animate pronouns, 11 inanimate pronouns and 12 unclassified 
occurrences (for lack of sufficient context o recover the antecedent of the 
pronoun with certainty). Thus, at least 76% of usages of they were 
animate; we assume the percentage of animate usages of the other 
pronouns to be even higher. Since the hypothesis was confirmed, we 
count pronouns (other than it) in subject position (Kariaeva \[1999\]; cf. 
Aone and McKee \[1996\]). The values for the feature were determined by 
automatically extracting all subject/verb tuples including our 59 example 
verbs from the parsed corpus, and computing the ratio of occurrences of
pronoun subjects to all subjects for each verb. 
Finally, as indicated in Table 5, the verbs are designated as belonging to "group 1" 
or "group 2". All the verbs are treated equally in our data analysis and in the machine 
learning experiments, but this designation does indicate a difference in details of the 
counting procedures described above. The verbs in group I had been used in an earlier 
study in which it was important to minimize noisy data (Stevenson and Merlo 1997a), 
so they generally underwent greater manual intervention i  the counts. In adding 
group 2 for the classification experiment, we chose to minimize the intervention i  
order to demonstrate hat the classification process is robust enough to withstand the 
resulting noise in the data. 
For group 2, the transitivity, voice, and VBN counts were done automatically with- 
out any manual intervention. For group 1, these three counts were done automatically 
by regular expression patterns, and then subjected to correction, partly by hand and 
partly automatically, by one of the authors. For transitivity, the adjustments vary for 
the individual verbs. Most of the reassignments from a transitive to an intransitive 
labelling occurred when the following noun was not the direct object but rather a 
measure phrase or a date. Most of the reassignments from intransitive to transitive 
occurred when a particle or a preposition following the verb did not introduce a prepo- 
sitional phrase, but instead indicated apassive form (by) or was part of a phrasal verb. 
Some verbs were mostly used adjectivally, in which case they were excluded from the 
transitivity counts. For voice, the required adjustments included cases of coordination 
of the past participle when the verb was preceded by a conjunction, or a comma. 
384 
Merlo and Stevenson Statistical Verb Classification 
Table 6 
Aggregated relative frequency data for the five features. E = unergatives, A = unaccusatives, 
O = object-drops. 
TRANS PASS VBN CAUS ANIM 
Class N Mean SD Mean SD Mean SD Mean SD Mean SD 
E 20 0.23 0.23 0.07 0.12 0.21 0.26 0.00 0.00 0.25 0.24 
A 19 0.40 0.24 0.33 0.27 0.65 0.27 0.12 0.14 0.07 0.09 
O 20 0.62 0.25 0.31 0.26 0.65 0.23 0.04 0.07 0.15 0.14 
These were collected and classified by hand as passive or active based on intuition. 
Similarly, partial adjustments to the VBN counts were made by hand. 
For the causativity feature, subjects and objects were determined by manual  in- 
spection of the corpus for verbs belonging to group 1, while they were extracted 
automatically from the parsed corpus for group 2. The group 1 verbs were sampled 
in three ways, depending on total frequency. For verbs with less than 150 occurrences, 
all instances of the verbs were used for subject/object extraction. For verbs whose 
total frequency was greater than 150, but whose VBD frequency was in the range 
100-200, we extracted subjects and objects of the VBD occurrences only. For higher 
frequency verbs, we used only the first 100 VBD occurrences. 6 The same script for 
computing the overlap of the extracted subjects and objects was then used on the 
resulting subject/verb and verb/object  tuples for both group 1 and group 2 verbs. 
The animacy feature was calculated over subject/verb tuples extracted automati-  
cally for both groups of verbs from the parsed corpus. 
3.2 Data Analysis 
The data collection described above yields the following data points in total: TRANS: 
27403; PASS: 20481; VBN: 36297; CAt;S: 11307; ANIM: 7542. (Different features yield differ- 
ent totals because they were sampled independently, and the search patterns to extract 
some features are more imprecise than others.) The aggregate means by class of the 
normalized frequencies for all verbs are shown in Table 6; item by item distributions 
are provided in Appendix A, and raw counts are available from the authors. Note 
that aggregate means are shown for illustration purposes only--al l  machine learning 
experiments are performed on the individual normalized frequencies for each verb, as 
given in Appendix A. 
The observed istributions of each feature are indeed roughly as expected accord- 
ing to the description in Section 2. Unergatives how a very low relative frequency of 
the TRANS feature, fol lowed by unaccusatives, then object-drop verbs. Unaccusative 
verbs show a high frequency of the CAUS feature and a low frequency of the ANIM fea- 
ture compared to the other classes. Somewhat unexpectedly, object-drop verbs exhibit 
a non-zero mean CAUS value (almost half the verbs have a CAUS value greater than 
zero), leading to a three-way causative distinction among the verb classes. We suspect 
that the approximation that we used for causative use- - the overlap between subjects 
6 For this last set of high-frequency verbs (exploded, jumped, opened, played, rushed), we used the first 
100 occurrences a the simplest way to collect he sample. In response to an anonymous reviewer's 
concern, we later verified that these counts were not different from counts obtained by random 
sampling of 100 VBD occurrences. A paired t-test of the two sets of counts (first 100 sampling and 
random sampling) indicates that the two sets of counts are not statistically different (t = 1.283, DF = 4, 
p = 0.2687). 
385 
Computational Linguistics Volume 27, Number 3 
Table 7 
Manually (Man) and automatically (Aut) calculated features for a random sample of verbs. 
T -~- TRANS,  P = PASS,  V ---- VBN,  C -~ CAUS,  A = ANIM.  
Unergative Unaccusative Object-Drop 
hopped scurried folded stabilized inherited swallowed 
Man Aut Man Aut Man Aut Man Aut Man Aut Man Aut 
T 0.21 0 .21  0.00 0.00 0 .71  0.23 0.24 0.18 1.00 0.64 0.96 0.35 
P 0.00 0.00 0.00 0.00 0.44 0.33 0.19 0.13 0.39 0.13 0.54 0.44 
V 0.03 0.00 0.10 0.00 0.56 0.73 0 .71  0.92 0.56 0.60 0.64 0.79 
C 0.00 0.00 0.00 0.00 0.54 0.00 0.24 0.35 0.00 0.06 0.00 0.04 
A 0.93 1.00 0.90 0.14 0.23 0.00 0.02 0.00 0.58 0.32 0.35 0.22 
and objects for a verb--also captures a "reciprocity" effect for some object-drop verbs 
(such as call), in which subjects and objects can be similar types of entities. Finally, 
although expected to be a redundant indicator of transitivity, PASS and VBN, unlike 
TRANS, have very similar values for unaccusative and object-drop verbs, indicating 
that their distributions are sensitive to factors we have not yet investigated. 
One issue we must address is how precisely the automatic ounts reflect he actual 
linguistic behaviour of the verbs. That is, we must be assured that the patterns we note 
in the data in Table 6 are accurate reflections of the differential behaviour of the verb 
classes, and not an artifact of the way in which we estimate the features, or a result of 
inaccuracies in the counts. In order to evaluate the accuracy of our feature counts, we 
selected two verbs from each class, and determined the "true" value of each feature 
for each of those six verbs through manual counting. The six verbs were randomly 
selected from the group 2 subset of the verbs, since counts for group 2 verbs (as 
explained above) had not undergone manual correction. This allows us to determine 
the accuracy of the fully automatic ounting procedures. The selected verbs (and their 
frequencies) are: hopped (29), scurried (21), folded (189), stabilized (286), inherited (357), 
swallowed (152). For verbs that had a frequency of over 100 in the "-ed" form, we 
performed the manual counts on the first 100 occurrences. 
Table 7 shows the results of the manual counts, reported as proportions to facil- 
itate comparison to the normalized automatic ounts, shown in adjoining columns. 
We observe first that, overall, most errors in the automatic ounts occur in the unac- 
cusative and object-drop verbs. While tagging errors affect the VBN feature for all of 
the verbs somewhat, we note that TP~ANS and PaSS are consistently underestimated for 
unaccusative and object-drop verbs. These errors make the unaccusative and object- 
drop feature values more similar to each other, and therefore potentially harder to 
distinguish. Furthermore, because the TRANS and PASS values are underestimated by
the automatic ounts, and therefore lower in value, they are also closer to the values 
for the unergative verbs. For the CAUS feature, we predict the highest values for the 
unaccusative verbs, and while that prediction is confirmed, the automatic ounts for 
that class also show the most errors. Finally, although the general pattern of higher 
values for the ANIM feature of unergatives and object-drop verbs is preserved in the 
automatic ounts, the feature is underestimated for almost all the verbs, again making 
the values for that feature closer across the classes than they are in reality. 
We conclude that, although there are inaccuracies in all the counts, the general 
patterns expected based on our analysis of the verb classes hold in both the manual 
and automatic ounts. Errors in the estimating and counting procedures are therefore 
386 
Merlo and Stevenson Statistical Verb Classification 
not likely to be responsible for the pattern of data in Table 6 above, which generally 
matches our predictions. Furthermore, the errors, at least for this random sample of 
verbs, occur in a direction that makes our task of distinguishing the classes more 
difficult, and indicates that developing more accurate search patterns may possibly 
sharpen the class distinctions, and improve the classification performance. 
4. Experiments in Classification 
In this section, we turn to our computational experiments that investigate whether the 
statistical indicators of thematic properties that we have developed can in fact be used 
to classify verbs. Recall that the task we have set ourselves is that of automatically 
learning the best class for a set of usages of a verb, as opposed to classifying individual 
occurrences of the verb. The frequency distributions of our features yield a vector for 
each verb that represents the estimated values for the verb on each dimension across 
the entire corpus: 
Vector template: \[verb-name, TRANS, PASS, VBN, CAUS, ANIM, class\] 
Example: \[opened, .69, .09, .21, .16, .36, unacc\] 
The resulting set of 59 vectors constitutes the data for our machine learning experi- 
ments. We use this data to train an automatic lassifier to determine, given the feature 
values for a new verb (not from the training set), which of the three major classes of 
English optionally intransitive verbs it belongs to. 
4.1 Experimental Methodology 
In pilot experiments on a subset of the features, we investigated a number  of su- 
pervised machine learning methods that produce automatic lassifiers (decision tree 
induction, rule learning, and two types of neural networks), as well as hierarchi- 
cal clustering; see Stevenson et al (1999) for more detail. Because we achieved ap- 
proximately the same level of performance in all cases, we narrowed our further 
experimentation to the publicly available version of the C5.0 machine learning system 
(http: / /www.rulequest.com),  a newer version of C4.5 (Quinlan 1992), due to its ease 
of use and wide availability. The C5.0 system generates both decision trees and cor- 
responding rule sets from a training set of known classifications. In our experiments, 
we found little to no difference in performance between the trees and rule sets, and 
report only the rule set results. 
In the experiments below, we follow two methodologies in training and testing, 
each of which tests a subset of cases held out from the training data. Thus, in all cases, 
the results we report are on test data that was never seen in training. 7
The first training and testing methodology we follow is 10-fold cross-validation. In
this approach, the system randomly divides the data into ten parts, and runs ten times 
on a different 90%-training-data/10%-test-data split,yielding an average accuracy and 
standard error across the ten test sets. This training methodology is very useful for 
7 One anonymous reviewer aised the concern that we do not test on verbs that were unseen by the 
authors prior to finalizing the specific features to count. However, this does not reduce the generality 
of our results. The features we use are motivated by linguistic theory, and derived from the set of 
thematic properties that discriminate the verb classes. It is therefore very unlikely that they are skewed 
to the particular verbs we have chosen. Furthermore, our cross-validation experiments, described in the 
next subsection, show that our results hold across a very large number of randomly selected subsets of 
this sample of verbs. 
387 
Computational Linguistics Volume 27, Number 3 
our application, as it yields performance measures across a large number of training 
data/test data sets, avoiding the problems of outliers in a single random selection 
from a relatively small data set such as ours. 
The second methodology is a single hold-out raining and testing approach. Here, 
the system is run N times, where N is the size of the data set (i.e., the 59 verbs in 
our case), each time holding out a single data vector as the test case and using the 
remaining N-1 vectors as the training set. The single hold-out methodology yields an 
overall accuracy rate (when the results are averaged across all N trials), but also-- 
unlike cross-validation--gives us classification results on each individual data vector. 
This property enables us to analyze differential performance on the individual verbs 
and across the different verb classes. 
Under both training and testing methodologies, the baseline (chance) performance 
in this task--a three-way classification--is 33.9%. In the single hold-out methodology, 
there are 59 test cases, with 20, 19, and 20 verbs each from the unergative, unaccusative, 
and object-drop classes, respectively. Chance performance ofpicking a single class label 
as a default and assigning it to all cases would yield at most 20 out of the 59 cases 
correct, or 33.9%. For the cross-validation methodology, the determination f a baseline 
is slightly more complex, as we are testing on a random selection of 10% of the full 
data set in each run. The 33.9% figure represents he expected relative proportion of a 
test set that would be labelled correctly by assignment of a default class label to the 
entire test set. Although the precise make-up of the test cases vary, on average the test 
set will represent the class membership proportions of the entire set of verbs. Thus, 
as with the single hold-out approach, chance accuracy corresponds to a maximum of 
20/59, or 33.9%, of the test set being labelled correctly. 
The theoretical maximum accuracy for the task is, of course, 100%, although in 
Section 5 we discuss ome classification results from human experts that indicate that 
a more realistic expectation is much lower (around 87%). 
4.2 Results Using 10-Fold Cross-Validation 
We first report he results of experiments u ing a training methodology of 10-fold cross- 
validation repeated 50 times. This means that the 10-fold cross-validation procedure is
repeated for 50 different random divisions of the data. The numbers reported are the 
averages of the results over all the trials. That is, the average accuracy and standard 
error from each random division of the data (a single cross-validation run including 
10 training and test sets) are averaged across the 50 different random divisions. This 
large number of experimental trials gives us a very tight bound on the mean accuracy 
reported, enabling us to determine with high confidence the statistical significance of 
differences in results. 
Table 8 shows that performance of classification using individual features varies 
greatly, from little above the baseline to almost 22% above the baseline, or a reduction 
of a third of the error rate, a very good result for a single feature. (All reported 
accuracies in Table 8 are statistically distinct, at the p < .01 level, using an ANOVA 
\[dr = 249, F = 334.72\], with a Tukey-Kramer post test.) 
The first line of Table 9 shows that the combination of all features achieves an 
accuracy of 69.8%, which is 35.9% over the baseline, for a reduction in the error rate of 
54%. This is a rather considerable r sult, given the very low baseline (33.9%). Moreover, 
recall that our training and testing sets are always disjoint (cf., Lapata and Brew \[1999\]; 
Siegel \[1999\]); in other words, we are predicting the classification of verbs that were 
never seen in the training corpus, the hardest situation for a classification algorithm. 
The second through sixth lines of Table 9 show the accuracy achieved on each 
subset of features that results from removing asingle feature. This allows us to evaluate 
388 
Merlo and Stevenson Statistical Verb Classification 
Table 8 
Percent accuracy and standard error of the verb classification task using each feature 
individually, under a training methodology of 10-fold cross-validation repeated 50 times. 
Feature %Accuracy %SE 
CAUS 55.7 .1 
VBN 52.5 .5 
PASS 50.2 .5 
TRANS 47.1 .4 
ANIM 35.3 .5 
Table 9 
Percent accuracy and standard error of the verb classification task using features in 
combination, under a training methodology of 10-fold cross-validation repeated 50 times. 
Feature 
Features Used Not Used %Accuracy %SE 
1. TRANS PASS VBN CAUS ANIM 69.8 .5 
2. TRANS VBN CAUS ANIM PASS 69.8 .5 
3. TRANS PASS VBN ANIM CAUS 67.3 .6 
4. TRANS PASS CAUS ANIM VBN 66.5 .5 
5. TRANS PASS VBN CAUS ANIM 63.2 .6 
6. PASS VBN CAUS ANIM TRANS 61 .6  .6 
the contribution of each feature to the performance of the classification process, by 
comparing the performance of the subset without it, to the performance using the full 
set of features. We see that the removal of PASS (second line) has no effect on the results, 
while removal of the remaining features yields a 2-8% decrease in performance. (In 
Table 9, the differences between all reported accuracies are statistically significant, at 
the p < .05 level, except for between lines 1 and 2, lines 3 and 4, and lines 5 and 6, 
using an ANOVA \[dr = 299, F = 37.52\], with a Tukey-Kramer post test.) We observe 
that the behavior of the features in combination cannot be predicted by the individual 
feature behavior. For example, CAUS, which is the best individually, does not greatly 
affect accuracy when combined with the other features (compare line 3 to line 1). 
Conversely, ANIM and TRANS, which do not classify verbs accurately when used alone, 
are the most relevant in a combination of features (compare lines 5 and 6 to line 1). We 
conclude that experimentation with combinations of features is required to determine 
the relevance of individual features to the classification task. 
The general behaviour in classification based on individual features and on size 
4 and size 5 subsets of features is confirmed for all subsets. Appendix B reports the 
results for all subsets of feature combinations, in order of decreasing performance. 
Table 10 summarizes this information. In the first data column, the table illustrates 
the average accuracy across all subsets of each size. The second through sixth data 
columns report the average accuracy of all the size n subsets in which each feature 
occurs. For example, the second data cell in the second row (54.9) indicates the average 
accuracy of all subsets of size 2 that contain the feature VBN. The last row of the 
table indicates the average accuracy for each feature of all subsets containing that 
feature. 
389 
Computational Linguistics Volume 27, Number 3 
Table 10 
Average percent accuracy of feature subsets, by subset size and by sets of each size including 
each feature. 
Mean Accuracy of Subsets 
Subset Mean Accuracy that Include Each Feature 
Size by Subset Size VBN PASS TRANS ANIM CAUS 
1 48.2 52.5 50.2 47.1 35.3 55.7 
2 55.1 54.9 52.8 56.4 58.0 57.6 
3 60.5 60.1 58.5 62.3 61.1 60.5 
4 65.7 65.5 64.7 66.7 66.3 65.3 
5 69.8 69.8 69.8 69.8 69.8 69.8 
Mean Acc/Feature: 60.6 59.2 60.5 58.1 61.8 
The first observation--that more features perform better-- is confirmed overall, 
in all subsets. Looking at the first data column of Table 10, we can observe that, on 
average, larger sets of features perform better than smaller sets. Furthermore, as can be 
seen in the following individual feature columns, individual features perform better in 
a bigger set than in a smaller set, without exception. The second observation--that the 
performance of individual features is not always a predictor of their performance in 
combination-- is confirmed by comparing the average performance of each feature in 
subsets of different sizes to the average across all subsets of each size. We can observe, 
for instance, that the feature CAUS, which performs very well alone, is average in 
feature combinations of size 3 or 4. By contrast, the feature ANIM, which is the worst 
if used alone, is very effective in combination, with above average performance for all 
subsets of size 2 or greater. 
4.3 Results Using Single Hold-Out Methodology 
One of the disadvantages of the cross-validation training methodology, which aver- 
ages performance across a large number of random test sets, is that we do not have 
performance data for each verb, nor for each class of verbs. In another set of experi- 
ments, we used the same C5.0 system, but employed a single hold-out training and 
testing methodology. In this approach, we hold out a single verb vector as the test case, 
and train the system on the remaining 58 cases. We then test the resulting classifier on 
the single hold-out case, and record the assigned class for that verb. This procedure 
is repeated for each of the 59 verbs. As noted above, the single hold-out methodology 
has the benefit of yielding both classification results on each individual verb, and an 
overall accuracy rate (the average results across all 59 trials). Moreover, the results on 
individual verbs provide the data necessary for determining accuracy for each verb 
class. This allows us to determine the contribution of individual features as above, but 
with reference to their effect on the performance of individual classes. This is impor- 
tant, as it enables us to evaluate our hypotheses concerning the relation between the 
thematic features and verb class distinctions, which we turn to in Section 4.4. 
We performed single hold-out experiments on the full set of features, as well as on 
each subset of features with a single feature removed. The first line of Table 11 shows 
that the overall accuracy for all five features is almost exactly the same as that achieved 
with the 10-fold cross-validation methodology (69.5% versus 69.8%). As with the cross- 
validation results, the removal of PASS does not degrade performance-- in fact, here its 
removal appears to improve performance (see line 2 of Table 11). However, it should 
be noted that this increase in performance results from one additional verb being 
390 
Merlo and Stevenson Statistical Verb Classification 
Table 11 
Percent accuracy of the verb classification task using features in combination, under a single 
hold-out raining methodology. 
Feature %Accuracy 
Features Used Not Used on All Verbs 
1. TRANS PASS VBN CAUS ANIM 69.5 
2. TRANS VBN CAUS ANIM PASS 71.2 
3. TRANS PASS VBN ANIM CAUS 62.7 
4. TRANS PASS CAUS AN1M VBN 61.0 
5. TRANS PASS VBN CAUS ANIM 61.0 
6. PASS VBN CAUS ANIM TRANS 64.4 
Table 12 
F score of classification within each class, under a single hold-out raining methodology. 
Feature F score (%) F score (%) F score (%) 
Features Used Not Used for Unergs for Unaccs for Objdrops 
1. TRANS PASS VBN CAUS ANIM 73.9 68.6 
2. TRANS VBN CAUS ANIM PASS 76.2 75.7 
3. TRANS PASS VBN ANIM CAUS 65.1 60 .0  
4. TRANS PASS CAUS ANIM VBN 66 .7  65 .0  
5. TRANS PASS VBN CAUS AN1M 72.7 47.0 
6. PASS VBN CAUS ANIM TRANS 78.1 51.5  
64.9 
61.6 
62.8 
51.3 
60.0 
61.9 
classified correctly. The remaining lines of Table 11 show that the removal of any other 
feature has a 5-8% negative ffect on performance, again similar to the cross-validation 
results. (Although note that the precise accuracy achieved is not the same in each case 
as with 10-fold cross-validation, indicating that there is some sensitivity to the precise 
make-up of the training set when using a subset of the features.) 
Table 12 presents the results of the single hold-out experiments in terms of per- 
formance within each class, using an F measure with balanced precision and recall. 8 
The first line of the table shows clearly that, using all five features, the unergatives 
are classified with greater accuracy (F = 73.9%) than the unaccusative and object-drop 
verbs (F scores of 68.6% and 64.9%, respectively). The features appear to be better 
at distinguishing unergatives than the other two verb classes. The remaining lines of 
Table 12 show that this pattern holds for all of the subsets of features as well. Clearly, 
future work on our verb classification task will need to focus on determining features 
that better discriminate unaccusative and object-drop verbs. 
One potential explanation that we can exclude is that the pattern of results is due 
simply to the frequencies of the verbs--that is, that more frequent verbs are more ac- 
curately classified. We examined the relation between classification accuracy and log 
8 For all previous results, we reported an accuracy measure (the percentage of correct classifications out 
of all classifications). Using the terminology oftrue or false positives/negatives, thisis the same as 
truePositives/(truePositives + fal eNegafives). In the earlier esults, there are no falsePositives or 
trueNegatives, since we are only considering for each verb whether it is correctly classified 
(truePositive) ornot (falseNegative). However, when we turn to analyzing the data for each class, the 
possibility arises of having falsePositives and trueNegatives for that class. Hence, here we use the 
balanced F score, which calculates an overall measure of performance as2PR/(P + R), in which P 
(precision) is truePositives/(truePositives + fal ePositives), and R (recall) is 
truePositives/(truePositives + fal eNegatives). 
391 
Computational Linguistics Volume 27, Number 3 
frequencies of the verbs, both by class and individually. By class, unergatives have 
the lowest average log frequency (1.8), but are the best classified, while unaccusatives 
and object-drops are comparable (average log frequency = 2.4). If we group individ- 
ual verbs by frequency, the proportion of errors to the total number of verbs is not 
linearly related to frequency (log frequency K 2:7  errors/24 verbs, or 29% error; log 
frequency between 2 and 3 :7  errors/25 verbs, or 28% error; log frequency > 3 :4  
errors/10 verbs, or 40% error). Moreover, it seems that the highest-frequency verbs 
pose the most problems to the program. In addition, the only verb of log frequency 
K 1 is correctly classified, while the only one with log frequency > 4 is not. In con- 
clusion, we do not find that there is a simple mapping from frequency to accuracy. In 
particular, it is not the case that more frequent classes or verbs are more accurately 
classified. 
One factor possibly contributing to the poorer performance on unaccusatives and 
object-drops i  the greater degree of error in the automatic ounting procedures for 
these verbs, which we discussed in Section 3.2. In addition to exploration of other 
linguistic features, another area of future work is to develop better search patterns, for 
transitivity and passive in particular. Unfortunately, one limiting factor in automatic 
counting is that we inherit the inevitable rrors in POS tags in an automatically tagged 
corpus. For example, while the unergative verbs are classified highly accurately, we 
note that two of the three errors in misclassifying unergatives (galloped and paraded) are 
due to a high degree of error in tagging. 9 The verb galloped is incorrectly tagged VBN 
instead of VBD in all 12 of its uses in the corpus, and the verb paraded is incorrectly 
tagged VBN instead of VBD in 13 of its 33 uses in the corpus. After correcting only 
the VBN feature of these two verbs to reflect he actual part of speech, overall accuracy 
in classification increases by almost 10%, illustrating the importance of both accurate 
counts and accurate annotation of the corpora. 
4.4 Contribution of the Features to Classification 
We can further use the single hold-out results to determine the contribution of each 
feature to accuracy within each class. We do this by comparing the class labels as- 
signed using the full set of five features (TRANS, PASS, VBN, CAUS, ANIM) with the class 
labels assigned using each size 4 subset of features. The difference in classifications 
between each four-feature subset and the full set of features indicates the changes in 
class labels that we can attribute to the added feature in going from the four-feature 
to five-feature set. Thus, we can see whether the features indeed contribute to dis- 
criminating the classes in the manner predicted in Section 2.2, and summarized here 
in Table 13. 
We illustrate the data with a set of confusion matrices, in Tables 14 and 15, which 
show the pattern of errors according to class label for each set of features. In each 
confusion matrix, the rows indicate the actual class of incorrectly classified verbs, and 
the columns indicate the assigned class. For example, the first row of the first panel 
of Table 14 shows that one unergative was incorrectly labelled as unaccusative, and 
two unergatives as object-drop. To determine the confusability of any two classes (the 
9 The third error in classification f unergatives is the verb floated, which we conjecture is due not to 
counting errors, but to the linguistic properties of the verb itself. The verb is unusual for a 
manner-of-motion verb in that the action is inherently "uncontrolled", and thus the subject of the 
intransitive/object of he transitive isa more passive ntity than with the other unergatives (perhaps 
indicating that the inventory of thematic roles should be refined to distinguish activity verbs with less 
agentive subjects). We think that this property relates to the notion of internal and external causation 
that is an important factor in distinguishing unergative and unaccusative rbs. We refer the interested 
reader to Stevenson and Merlo (1997b), which discusses the latter issue in more detail. 
392 
Merlo and Stevenson Statistical Verb Classification 
Table 13 
Expected class discriminations for each feature. 
Feature Expected Frequency Pattern 
Transitivity Unerg < Unacc < ObjDrop 
Causativity Unerg, ObjDrop < Unacc 
Animacy Unacc < Unerg, ObjDrop 
Passive Voice Unerg < Unacc < ObjDrop 
VBN Tag Unerg < Unacc < ObjDrop 
Table 14 
Confusion matrix indicating number of errors in classification by verb class, for the full set of 
five features, compared to two of the four-feature sets. E = unergatives, A = unaccusatives, 
O = object-drops. 
Assigned Class 
All features w/o CAUS W/O ANIM 
E A O E A O E A O 
Actual E 1 2 4 2 2 2 
Class A 4 3 5 2 5 6 
O 5 3 4 5 3 5 
Table 15 
Confusion matrix indicating number of errors in classification by verb class, for the full set of 
five features and for three of the four-feature sets. E = unergatives, A = unaccusatives, 
O = object-drops. 
Assigned Class 
All features w/o TRANS W/O PASS w/o  VBN 
E A O E A O E A O E A O 
Actual E 1 2 2 2 1 3 1 5 
Class A 4 3 3 7 1 4 2 4 
O 5 3 2 5 5 3 4 6 
opposite of discriminability), we look at two cells in the matrix: the one in which 
verbs of the first class were assigned the label of the second class, and the one in 
which verbs of the second class were assigned the label of the first class. (These pairs 
of cells are those opposite the diagonal of the confusion matrix.) By examining the 
decrease (or increase) in confusability of each pair of classes in going from a four- 
feature experiment to the five-feature xperiment, we gain insight into how well (or 
how poorly) the added feature helps to discriminate ach pair of classes. 
An analysis of the confusion matrices reveals that the behavior of the features 
largely conforms to our linguistic predictions, leading us to conclude that the features 
393 
Computational Linguistics Volume 27, Number 3 
we counted worked largely for the reasons we had hypothesized. We expected CAUS 
and ANIM to be particularly helpful in identifying unaccusatives, and these predictions 
are confirmed. Compare the second to the first panel of Table 14 (the errors without 
the CAUS feature compared to the errors with the ?AUS feature added to the set). 
We see that, without the CAUS feature, the confusability between unaecusatives and 
unergatives, and between unaccusatives and object-drops, is 9 and 7 errors, respec- 
tively; but when CAUS is added to the set of features, the confusability between these 
pairs of classes drops substantially, to5 and 6 errors, respectively. On the other hand, 
the confusability between unergatives and object-drops becomes lightly worse (errors 
increasing from 6 to 7). The latter indicates that the improvement in unaccusatives is 
not simply due to an across-the-board improvement in accuracy as a result of having 
more features. We see a similar pattern with the ANIM feature. Comparing the third 
to the first panel of Table 14 (the errors without the ANIM feature compared to the 
errors with the ANIM feature added to the set), we see an even larger improvement in
discriminability of unaccusatives when the ANIM feature is added. The confusability 
of unaccusatives and unergatives drops from 7 errors to 5 errors, and of unaccusatives 
and object-drops from 11 errors to 6 errors. Again, confusability of unergatives and 
object-drops i worse, with an increase in errors of 5 to 7. 
We had predicted that the TRANS feature would make a three-way distinction 
among the verb classes, based on its predicted linear relationship between the classes 
(see the inequalities in Table 13). We had further expected that PASS and VBN would 
behave similarly, since these features are correlated to TRANS. To make a three-way dis- 
tinction among the verb classes, we would expect confusability between all three pairs 
of verb classes to decrease (i.e., discriminability would improve) with the addition of 
TRANS, PASS, or VBN. We find that these predictions are confirmed in part. 
First consider the TRANS feature. Comparing the second to the first panel of Ta- 
ble 15, we find that unergatives are already accurately classified, and the addition of 
TRANS to the set does indeed greatly reduce the confusability of unaccusatives and 
object-drops, with the number of errors dropping from 12 to 6. However, we also 
observe that the confusability of unergatives and unaccusatives is not improved, and 
the confusability of unergatives and object-drops i worsened with the addition of 
the TRANS feature, with errors in the latter case increasing from 4 to 7. We conclude 
that the expected three-way discriminability of TRANS is most apparent in the reduced 
confusion of unaccusative and object-drop verbs. 
Our initial prediction was that PASS and VBN would behave similarly to TRANS-- 
that is, also making a three-way distinction among the classes--although the aggregate 
data revealed little difference in these feature values between unaccusatives and object- 
drops. Comparing the third to the first panel of Table 15, we observe that the addition 
of the PAss feature hinders the discriminability of unergatives and unaccusatives (in- 
creasing errors from 2 to 5); it does help in discriminating the other pairs of classes, 
but only slightly (reducing the number of errors by 1 in each case). The VBN fea- 
ture shows a similar pattern, but is much more helpful at distinguishing unergatives 
from object-drops, and object-drops from unaccusatives. In comparing the fourth to 
the first panel of Table 15, we find that the confusability of unergatives and object- 
drops is reduced from 9 errors to 7, and of unaccusatives and object-drops from 10 
errors to 6. The latter result is somewhat surprising, since the aggregate VBN data 
for the unaccusative and object-drop classes are virtually identical. We conclude that 
contribution of a feature to classification is not predictable from the apparent dis- 
criminability of its numeric values across the classes. This observation emphasizes the 
importance of an experimental method to evaluating our approach to verb classifica- 
tion. 
394 
Merlo and Stevenson Statistical Verb Classification 
Table 16 
Percent agreement (%Agr) and pair-wise agreement (K) of three experts (El, E2, E3) and the 
program compared to each other and to a gold standard (Levin). 
PROGRAM E1 E2 E3 
%Agr K %Agr K %Agr K %Agr K 
E1 59% .36 
E2 68% .50 75% .59 
E3 66% .49 70% .53 77% .66 
LEVIN 69.5% .54 71% .56 86.5% .80 83% .74 
5. Establishing the Upper Bound for the Task 
In order to evaluate the performance of the algorithm in practice, we need to compare 
it to the accuracy of classification performed by an expert, which gives a realistic upper 
bound for the task. The lively theoretical debate on class membership of verbs, and the 
complex nature of the linguistic information ecessary to accomplish this task, led us to 
believe that the task is difficult and not likely to be performed at 100% accuracy even 
by experts, and is also likely to show differences in classification between experts. 
We report here the results of two experiments which measure expert accuracy in 
classifying our verbs (compared to Levin's classification as the gold standard), as well 
as inter-expert agreement. (See also Merlo and Stevenson \[2000a\] for more details.) 
To enable comparison of responses, we performed a closed-form questionnaire study, 
where the number  and types of the target classes are defined in advance, for which 
we prepared a forced-choice and a non-forced-choice variant. The forced-choice study 
provides data for a maximal ly restricted experimental situation, which corresponds 
most closely to the automatic verb classification task. However,  we are also interested 
in slightly more natural results- -provided by the non-forced-choice task- -where the 
experts can assign the verbs to an "others" category. 
We asked three experts in lexical semantics (all native speakers of English) to 
complete the forced-choice lectronic questionnaire study. Neither author was among 
the three experts, who were all professionals in computational or theoretical linguistics 
with a specialty in lexical semantics. Materials consisted of individually randomized 
lists of the same 59 verbs used for the machine learning experiments, using Levin's 
(1993) electronic index, available from Chicago University Press. The verbs were to 
be classified into the three target classes--unergative, unaccusative, and object-drop--  
which were described in the instructions. 1?(All materials and instructions are available 
at URL http: / /www.lat l .unige.ch/ lat l /personal /paola.html. )  
Table 16 shows an analysis of the results, reporting both percent agreement and 
pairwise agreement (according to the Kappa statistic) among the experts and the 
program. ~1 Assessing the percentage of verbs on which the experts agree gives us 
10 The definitions of the classes were as follows. Unergative: A verb that assigns an agent heta role to the 
subject in the intransitive. If it is able to occur transitively, it can have a causative meaning. 
Unaccusative: A verb that assigns apatient/theme theta role to the subject in the intransitive. When it 
occurs transitively, it has a causative meaning. Object-Drop: A verb that assigns an agent role to the 
subject and patient/theme role to the object, which is optional. When it occurs transitively, it does not 
have a causative meaning. 
11 In the comparison of the program to the experts, we use the results of the classifier under single 
hold-out raining--which yields an accuracy of 69.5%--because those results provide the classification 
for each of the individual verbs. 
395 
Computational Linguistics Volume 27, Number 3 
an intuitive measure. However, this measure does not take into account how much 
the experts agree over  the expected agreement by chance. The latter is provided by 
the Kappa statistic, which we calculated following Klauer (1987, 55-57) (using the z 
distribution to determine significance; p ~ 0.001 for all reported results). The Kappa 
value measures the experts', and our classifier's, degree of agreement over chance, 
with the gold standard and with each other. Expected chance agreement varies with 
the number and the relative proportions of categories used by the experts. This means 
that two given pairs of experts might reach the same percent agreement on a given 
task, but not have the same expected chance agreement, if they assigned verbs to 
classes in different proportions. The Kappa statistic ranges from 0, for no agreement 
above chance, to 1, for perfect agreement. The interpretation of the scale of agreement 
depends on the domain, like all correlations. Carletta (1996) cites the convention from 
the domain of content analysis indicating that .67 K K < .8 indicates marginal agree- 
ment, while K > .8 is an indication of good agreement. We can observe that only one 
of our agreement figures comes close to reaching what would be considered "good" 
under this interpretation. Given the very high level of expertise of our human experts, 
we suspect hen that this is too stringent a scale for our task, which is qualitatively 
quite different from content analysis. 
Evaluating the experts' performance summarized in Table 16, we can remark two 
things, which confirm our expectations. First, the task is difficult--i.e., not performed 
at 100% (or close) even by trained experts, when compared to the gold standard, with 
the highest percent agreement with Levin at 86.5%. Second, with respect to comparison 
of the experts among themselves, the rate of agreement is never very high, and the 
variability in agreement is considerable, ranging from .53 to .66. This evaluation is 
also supported by a 3-way agreement measure (Siegel and Castellan 1988). Applying 
this calculation, we find that the percentage of verbs to which the three experts gave 
the same classification (60%, K = 0.6) is smaller than any of the pairwise agreements, 
indicating that the experts do not all agree on the same subset of verbs. 
The observation that the experts often disagree on this difficult task suggests that 
a combination of expert judgments might increase the upper bound. We tried the 
simplest combination, by creating a new classification using a majority vote: each 
verb was assigned the label given by at least two experts. Only three cases did not 
have any majority label; in these cases we used the classification of the most accurate 
expert. This new classification does not improve the upper bound, reaching only 86.4% 
(K = .80) compared to the gold standard. 
The evaluation is also informative with respect to the performance of the program. 
On the one hand, we observe that if we take the best performance achieved by an 
expert in this task---86.5%--as the maximum achievable accuracy in classification, 
our algorithm then reduces the error rate over chance by approximately 68%, a very 
respectable r sult. In fact, the accuracy of 69.5% achieved by the program is only 1.5% 
less than one of the human experts in comparison to the gold standard. On the other 
hand, the algorithm still does not perform at expert level, as indicated by the fact that, 
for all experts, the lowest agreement score is with the program. 
One interesting question is whether experts and program disagree on the same 
verbs, and show similar patterns of errors. The program makes 18 errors, in total, com- 
pared to the gold standard. However, in 9 cases, at least one expert agrees with the 
classification given by the program. The program makes fewer errors on unergatives 
(3) and comparably many on unaccusatives and object-drops (7 and 8 respectively), in- 
dicating that members of the latter two classes are quite difficult to classify. This differs 
from the pattern of average agreement between the experts and Levin, who agree on 
17.7 (of 20) unergatives, 16.7 (of 19) unaccusatives, and 11.3 (of 20) object-drops. This 
396 
Merlo and Stevenson Statistical Verb Classification 
clearly indicates that the object-drop class is the most difficult for the human experts 
to define. This class is the most heterogeneous in our verb list, consisting of verbs 
from several subclasses of the "unexpressed object alternation" class in (Levin, 1993). 
We conclude that the verb classification task is likely easier for very homogeneous 
classes, and more difficult for more broadly defined classes, even when the exemplars 
share the critical syntactic behaviors. 
On the other hand, frequency does not appear to be a simple factor in explaining 
patterns of agreement between experts, or increases in accuracy. As in Section 4.3, 
we again analyze the relation between log frequency of the verbs and classification 
performance, here considering the performance of the experts. We grouped verbs in 
three log frequency classes: verbs with log frequency less than 2 (i.e., frequency less 
than 100), those with log frequency between 2 and 3 (i.e., frequency between 100 
and 1000), and those with log frequency over 3 (i.e., frequency over 1000). The low- 
frequency group had 24 verbs (14 unergatives, 5 unaccusatives, and 5 object-drop), 
the intermediate-frequency group had 25 verbs (5 unergatives, 9 unaccusatives, and 
11 object-drops), and the high-frequency group had 10 verbs (1 unergative, 5 unac- 
cusatives, and 4 object-drops). We found that verbs with high and low frequency ield 
better accuracy and agreement among the experts than the verbs with mid frequency. 
Neither the accuracy of the majority classification, or the accuracy of the expert hat 
had the best agreement with Levin, were linearly affected by frequency. For the ma- 
jority vote, verbs with frequency less than 100 yield an accuracy of 92%, K = .84; 
verbs with frequency between 100 and 1000, accuracy 80%, K = .69; and for verbs 
with frequency over 1000, accuracy 90%, K = .82. For the "best" expert, the pattern 
is similar: verbs with frequency less than 100 yield an accuracy of 87.5%, K = .74; 
verbs with frequency between 100 and 1000, accuracy 84%, K = .76; and verbs with 
frequency over 1000, accuracy 90%, K = .82. 
We can see here that different frequency groups yield different classification be- 
havior. However, the relation is not simple, and it is clearly affected by the composition 
of the frequency group: the middle group contains mostly unaccusative and object- 
drop verbs, which are the verbs with which our experts have the most difficulty. This 
confirms that the class of the verb is the predominant factor in their pattern of errors. 
Note also that the pattern of accuracy across frequency groupings is not the same as 
that of the program (see Section 4.3, which revealed the most errors by the program on 
the highest frequency verbs), again indicating qualitative differences in performance 
between the program and the experts. 
Finally, one possible shortcoming of the above analysis is that the forced-choice 
task, while maximally comparable to our computational experiments, may not be a 
natural one for human experts. To explore this issue, we asked two different experts 
in lexical semantics (one native speaker of English and one bilingual) to complete the 
non-forced-choice electronic questionnaire study; again, neither author served as one 
of the experts. In this task, in addition to the three verb classes of interest, an answer 
of "other" was allowed. Materials consisted of individually randomized lists of 119 
target and filler verbs taken from Levin's (1993) electronic index, as above. The targets 
were again the same 59 verbs used for the machine learning experiments. To avoid 
unwanted priming of target items, the 60 fillers were automatically selected from the 
set of verbs that do not share any class with any of the senses of the 59 target verbs 
in Levin's index. In this task, if we take only the target items into account, the experts 
agreed 74.6% of the time (K = 0.64) with each other, and 86% (K = 0.80) and 69% 
(K = 0.57) with the gold standard. (If we take all the verbs into consideration, they 
agreed in 67% of the cases \[K = 0.56\] with each other, and 68% \[K = 0.55\] and 60.5% 
\[K = 0.46\] with the gold standard, respectively.) These results show that the forced- 
397 
Computational Linguistics Volume 27, Number 3 
choice and non-forced-choice task are comparable in accuracy of classification and 
inter-judge agreement on the target classes, giving us confidence that the forced-choice 
results provide a reasonably stable upper bound for computational experiments. 
6. Discussion 
The work presented here contributes to some central issues in computational linguis- 
tics, by providing novel insights, data, and methodology in some cases, and by rein- 
forcing some previously established results in others. Our research stems from three 
main hypotheses: 
. 
. 
. 
Argument structure is the relevant level of representation for verb 
classification. 
Argument structure is manifested distributionally in syntactic 
alternations, giving rise to differences in subcategorization frames or the 
distributions of their usage, or in the properties of the NP arguments to 
a verb. 
This information is detectable in a corpus and can be learned 
automatically. 
We discuss the relevant debate on each of these hypotheses, and the contribution of 
our results to each, in the following subsections. 
6.1 Argument Structure and Verb Classification 
Argument structure has previously been recognized as one of the most promising 
candidates for accurate classification. For example, Basili, Pazienza, and Velardi (1996) 
argue that relational properties of verbs--their argument structure--are more infor- 
mative for classification than their definitional properties (e.g., the fact that a verb 
describes a manner of motion or a way of cooking). Their arguments rest on linguistic 
and psycholinguistic results on classification and language acquisition (in particular, 
Pinker, \[1989\]; Rosch \[1978\]). 
Our results confirm the primary role of argument structure in verb classification. 
Our experimental focus is particularly clear in this regard because we deal with verbs 
that are "minimal pairs" with respect o argument structure. By classifying verbs that 
show the same subcategorizations (transitive and intransitive) into different classes, 
we are able to eliminate one of the confounds in classification work created by the 
fact that subcategorization and argument structure are often co-variant. We can infer 
that the accuracy in our classification is due to argument structure information, as 
subcategorization is the same for all verbs. Thus, we observe that the content of the 
thematic roles assigned by a verb is crucial for classification. 
6.2 Argument Structure and Distributional Statistics 
Our results further support the assumption that thematic differences across verb 
classes are apparent not only in differences in subcategorization frames, but also in 
differences in their frequencies. This connection relies heavily on the hypothesis that 
lexical semantics and lexical syntax are correlated, following Levin (1985; 1993). How- 
ever, this position has been challenged by Basili, Pazienza, and Velardi (1996) and 
Boguraev and Briscoe (1989), among others. For example, in an attempt o assess 
the actual completeness and usefulness of the Longman Dictionary of Contemporary 
English (LDOCE) entries, Boguraev and Briscoe (1989) found that people assigned a
398 
Merlo and Stevenson Statistical Verb Classification 
"change of possession" meaning both to verbs that had dative-related subcategoriza- 
tion frames (as indicated in the LDOCE) and to verbs that did not. Conversely, they 
also found that both verbs that have a change-of-possession c mponent in their mean- 
ing and those that do not could have a dative code. They conclude that the thesis put 
forth by Levin (1985) is only partially supported. Basili, Pazienza, and Velardi (1996) 
show further isolated examples meant to illustrate that lexical syntax and semantics 
are not in a one-to-one relation. 
Many recent results, however, seem to converge in supporting the view that the 
relation between lexical syntax and semantics can be usefully exploited (Aone and 
McKee 1996; Dorr 1997; Dorr, Garman, and Weinberg 1995; Dorr and Jones 1996; La- 
pata and Brew 1999; Schulte im Walde 2000; Siegel 1998; Siegel 1999). Our work in 
particular underscores the relation between the syntactic manifestations of argument 
structure, and lexical semantic lass. In light of these recent successes, the conclusions 
in Boguraev and Briscoe (1989) are clearly too pessimistic. In fact, their results do not 
contradict he more recent ones. First of all, it is not the case that if an implication 
holds from argument structure to subcategorization (change of possession implies da- 
tive shift), the converse also holds. It comes as no surprise that verbs that do not 
have any change-of-possession component in their meaning may also show dative 
shift syntactically. Secondly, as Boguraev and Briscoe themselves note, Levin's state- 
ment should be interpreted as a statistical trend, and as such, Boguraev and Briscoe's 
results also confirm it. They claim however, that in adopting a statistical point of view, 
predictive power is lost. Our work shows that this conclusion is not appropriate ither: 
the correlation is strong enough to be useful to predict semantic lassification, at least 
for the argument structures that have been investigated. 
6.3 Detection of Argument Structure in Corpora 
Given the manifestation of argument structure in statistical distributions, we view cor- 
pora, especially if annotated with currently available tools, as repositories of implicit 
grammars, which can be exploited in automatic verb-classification tasks. Besides es- 
tablishing a relationship between syntactic alternations and underlying semantic prop- 
erties of verbs, our approach extends existing corpus-based learning techniques to the 
detection and automatic acquisition of argument structure. To date, most work in this 
area has focused on learning of subcategorization from unannotated or syntactically 
annotated text (e.g., Brent \[1993\]; Sanfilippo and Poznanski \[1992\]; Manning \[1993\]; 
Collins \[1997\]). Others have tackled the problem of lexical semantic lassification, but 
using only subcategorization frequencies as input data (Lapata and Brew 1999; Schulte 
im Walde 2000). Specifically, these researchers have not explicitly addressed the def- 
inition of features to tap directly into thematic role differences that are not reflected 
in subcategorization distinctions. On the other hand, when learning of thematic role 
assignment has been the explicit goal, the text has been semantically annotated (Web- 
ster and Marcus 1989), or external semantic resources have been consulted (Aone and 
McKee 1996; McCarthy 2000). We extend these results by showing that thematic in- 
formation can be induced from linguistically-guided counts in a corpus, without the 
use of thematic role tagging or external resources uch as WordNet. 
Finally, our results converge with the increasing agreement that corpus-based tech- 
niques are fruitful in the automatic onstruction of computational lexicons, providing 
machine readable dictionaries with complementary, reusable resources, such as fre- 
quencies of argument structures. Moreover, these techniques produce data that is eas- 
ily updated, as the information contained in corpora changes all the time, allowing 
for adaptability to new domains or usage patterns. This dynamic aspect could be ex- 
ploited if techniques uch as the one presented here are developed, which can work 
399 
Computational Linguistics Volume 27, Number 3 
on a rough collection of texts, and do not require a carefully balanced corpus or time- 
consuming semantic tagging. 
7. Related Work 
We conclude from the discussion above that our own work and work of others upport 
our hypotheses concerning the importance of the relation between classes of verbs and 
the syntactic expression of argument structure in corpora. In light of this, it is instruc- 
tive to evaluate our results in the context of other work that shares this view. Some 
related work requires either exact exemplars for acquisition, or external pre-compiled 
resources. For example, Dorr (1997) summarizes a number of automatic lassification 
experiments based on encoding Levin's alternations directly, as symbolic properties 
of a verb (Dorr, Garman, and Weinberg 1995; Dorr and Jones 1996). Each verb is rep- 
resented as the binary settings of a vector of possible alternations, acquired through 
a large corpus analysis yielding exemplars of the alternation. To cope with sparse 
data, the corpus information is supplemented by syntactic information obtained from 
the LDOCE and semantic information obtained from WordNet. This procedure clas- 
sifies 95 unknown verbs with 61% accuracy. Dorr also remarks that this result could 
be improved to 83% if missing LDOCE codes were added. While Dorr's work re- 
quires finding exact exemplars of the alternation, Oishi and Matsumoto (1997) present 
a method that, like ours, uses surface indicators to approximate underlying proper- 
ties. From a dictionary of dependency relations, they extract case-marking particles as 
indicators of the grammatical function properties of the verbs (which they call the- 
matic properties), such as subject and object. Adverbials indicate aspectual properties. 
The combination of these two orthogonal dimensions gives rise to a classification of 
Japanese verbs. 
Other work has sought o combine corpus-based extraction of verbal properties 
with statistical methods for classifying verbs. Siegel's work on automatic aspectual 
classification (1998, 1999) also reveals a close relationship between verb-related syn- 
tactic and semantic information. In this work, experiments o learn aspectual classifi- 
cation from linguistically-based numerical indicators are reported. Using combinations 
of seven statistical indicators (some morphological nd some reflecting syntactic o- 
occurrences), it is possible to learn the distinction between events and states for 739 
verb tokens with an improvement of 10% over the baseline (error rate reduction of 
74%), and to learn the distinction between culminated and non-culminated vents for 
308 verb tokens with an improvement of 11% (error rate reduction of 29%) (Siegel 
1999). 
In work on lexical semantic verb classification, Lapata and Brew (1999) further 
support he thesis of a predictive correlation between syntax and semantics in a statis- 
tical framework, showing that the frequency distributions of subcategorization frames 
within and across classes can disambiguate he usages of a verb with more than one 
known lexical semantic lass. On 306 verbs that are disambiguated by subcategoriza- 
tion frame, they achieve 91.8% accuracy on a task with a 65.7% baseline, for a 76% 
reduction in error rate. On 31 verbs that can take the same subcategorization(s) in 
different classes--more similar to our situation in that subcategorization alone cannot 
distinguish the classes--they achieve 83.9% accuracy compared to a 61.3% baseline, 
for a 58% reduction in error. Aone and McKee (1996), working with a much coarser- 
grained classification of verbs, present a technique for predicate-argument xtraction 
from multi-lingual texts. Like ours, their work goes beyond statistics over subcate- 
gorizations to include counts over the more directly semantic feature of animacy. No 
numerical evaluation of their results is provided. 
400 
Merlo and Stevenson Statistical Verb Classification 
Schulte im Walde (2000) applies two clustering methods to two types of frequency 
data for 153 verbs from 30 Levin (1993) classes. One set of experiments uses verb 
subcategorization frequencies, and the other uses subcategorization frequencies plus 
selectional preferences (a numerical measure based on an adaptation of the relative 
entropy method of Resnik \[1996\]). The best results achieved are a correct classification 
of 58 verbs out of 153, with a precision of 61% and recall of 36%, obtained using 
only subcategorization frequencies. We calculate that this corresponds to an F-score of 
45% with balanced precision and recall, n The use of selectional preference information 
decreases classification performance under either clustering algorithm. The results are 
somewhat difficult o evaluate further, as there is no description of the classes included. 
Also, the method of counting correctness entails that some "correct" classes may be 
split across distant clusters (this level of detail is not reported), so it is unclear how 
coherent the class behaviour actually is. 
McCarthy (2000) proposes a method to identify diathesis alternations. After learn- 
ing subcategorization frames, based on a parsed corpus, selectional preferences are 
acquired for slots of the subcategorization frames, using probability distributions over 
Wordnet classes. Alternations are detected by testing the hypothesis that, given any 
verb, the selectional preferences for arguments occurring in alternating slots will be 
more similar to each other than those for slots that do not alternate. For instance, given 
a verb participating in the causative alternation, its selectional preferences for the sub- 
ject in an intransitive use, and for the object in a transitive use, will be more similar 
to each other than the selectional preferences for these two slots of a verb that does 
not participate in the causative alternation. This method achieves the best accuracy 
for the causative and the conative alternations (73% and 83%, respectively), despite 
sparseness of data. McCarthy reports that a simpler measure of selectional preferences 
based simply on head words yields a lower 63% accuracy. Since this latter measure 
is very similar to our CAUS feature, we think that our results would also improve by 
adopting a similar method of abstracting from head words to classes. 
Our work extends each of these approaches in some dimension, thereby provid- 
ing additional support for the hypothesis that syntax and semantics are correlated in 
a systematic and predictive way. We extend Dorr's alternation-based automatic lassi- 
fication to a statistical setting. By using distributional pproximations of indicators of 
alternations, we solve the sparse data problem without recourse to external sources of 
knowledge, such as the LDOCE, and in addition, we are able to learn argument struc- 
ture alternations using exclusively positive examples. We improve on the approach of 
Oishi and Matsumoto (1997) by learning argument structure properties, which, unlike 
grammatical functions, are not marked morphologically, and by not relying on exter- 
nal sources of knowledge. Furthermore, in contrast o Siegel (1998) and Lapata and 
Brew (1999) our method applies successfully to previously unseen words--i.e., test 
cases that were not represented in the training set .  13 This is a very important property 
of lexical acquisition algorithms to be used for lexicon organization, as their main 
interest lies in being applied to unknown words. 
On the other hand, our approach is similar to the approaches of Siegel, and La- 
pata and Brew (1999), in attempting to learn semantic notions from distributions of 
12 A baseline of 5% is reported, based on a closest-neighbor pairing of verbs, but it is not straightforward 
to compare this task to the proposed clustering algorithm. Determining a meaningful baseline for 
unsupervised clustering isclearly achallenge, but this gives an indication that the clustering task is 
indeed ifficult. 
13 Siegel (1998) reports two experiments over verb types with disjoint raining and test sets, but the 
results were not significantly different from the baseline. 
401 
Computational Linguistics Volume 27, Number 3 
indicators that can be gleaned from a text. In our case, we are trying to learn argu- 
ment structure, a finer-grained classification than the dichotomic distinctions studied 
by Siegel. Like Lapata and Brew, three of our indicators--TRANS, VBN, PASS--are based 
on the assumption that distributional differences in subcategorization frames are re- 
lated to underlying verb class distinctions. However, we also show that other syntactic 
indicators--cAUS and ANIM--can be devised that tap directly into the argument struc- 
ture of a verb. Unlike Schulte im Walde (2000), we find the use of these semantic 
features helpful in classification--using only TRANS and its related features, VBN and 
PASS, we achieve only 55% accuracy, in comparison to 69.8% using the full set of fea- 
tures. This can perhaps be seen as support for our hypothesis that argument structure 
is the right level of representation forverb class distinctions, ince it appears that our 
features that capture thematic differences are useful in classification, while Schulte im 
Walde's electional restriction features were not. 
Aone and McKee (1996) also use features that are intended to tap into both sub- 
categorization and thematic role distinctions--frequencies of the transitive use and 
animate subject use. In our task, we show that subject animacy can be profitably ap- 
proximated solely with pronoun counts, avoiding the need for reference to external 
sources of semantic information used by Aone and McKee. In addition, our work ex- 
tends theirs in investigating much finer-grained verb classes, and in classifying verbs 
that have multiple argument structures. While Aone and McKee define ach of their 
classes according to a single argument structure, we demonstrate he usefulness of 
syntactic features that capture relations across different argument structures of a sin- 
gle verb. Furthermore, while Aone and McKee, and others, look at relative frequency 
of subcategorization frames (as with our TRANS feature), or relative frequency of a 
property of NPs within a particular grammatical function (as with our ANIM feature), 
we also look at the paradigmatic relations across a text between thematic arguments 
in different alternations (with our CAUS feature). 
McCarthy (2000) shows that a method very similar to ours can be used for identi- 
fying alternations. Her qualitative results confirm, however, what was argued in Sec- 
tion 2 above: counts that tap directly into the thematic assignments are necessary to 
fully identify a diathesis alternation. In fact, on close inspection, McCarthy's method 
does not distinguish between the induced-action alternation (which the unergatives 
exhibit) and the causative/inchoative alternation (which the unaccusatives xhibit); 
thus, her method oes not discriminate two of our classes. It is likely that a combina- 
tion of our method, which makes the necessary thematic distinctions, and her more 
sophisticated method of detecting alternations would give very good results. 
8. Limitations and Future Work 
The classification results show that our method is powerful, and suited to the clas- 
sification of unknown verbs. However, we have not yet addressed the problem of 
verbs that can have multiple classifications. We think that many cases of ambigu- 
ous classification of the lexical entry for a verb can be addressed with the notion 
of intersective sets introduced by Dang et al (1998). This is an important concept, 
which proposes that "regular" ambiguity in classification--i.e., sets of verbs that have 
the same multi-way classifications according to Levin (1993)--can be captured with 
a finer-grained notion of lexical semantic lasses. Thus, subsets of verbs that occur 
in the intersection of two or more Levin classes form in themselves a coherent se- 
mantic (sub)class. Extending our work to exploit this idea requires only defining 
the classes appropriately; the basic approach will remain the same. Given the cur- 
rent demonstration f our method on fine-grained classes that share subcategoriza- 
402 
Merlo and Stevenson Statistical Verb Classification 
tion alternations, we are optimistic regarding its future performance on intersective 
sets. 
Because we assume that thematic properties are reflected in alternations of argu- 
ment structure, our features require searching for relations across occurrences of each 
verb. This motivated our initial experimental focus on verb types. However, when 
we turn to consider ambiguity, we must also address the problem that individual in- 
stances of verbs may come from different classes, and we may (like Lapata and Brew 
\[1999\]) want to classify the individual tokens of a verb. In future research we plan 
to extend our method to the case of ambiguous tokens, by experimenting with the 
combination of several sources of information: the classification of each instance will 
be a function of a bias for the verb type (using the cross-corpus statistics we collect), 
but also of features of the usage of the instance being classified (cf., Lapata and Brew 
\[1999\]; Siegel \[1998\]). 
Finally, corpus-based learning techniques collect statistical information related to 
language use, and are a good starting point for studying human linguistic perfor- 
mance. This opens the way to investigating the relation of linguistic data in text to 
people's linguistic behaviour and use. For example, Merlo and Stevenson (1998) show 
that, contrary to the naive assumption, speakers' preferences in syntactic disambigua- 
tion are not simply directly related to frequency (i.e., a speaker's preference for one 
construction over another is not simply modelled by the frequency of the construc- 
tion, or of the words in the construction). Thus, the kind of corpus investigation we 
are advocating--founded on in-depth linguistic analysis--holds promise for building 
more natural NLP systems which go beyond the simplest assumptions, and tie to- 
gether statistical computational linguistic results with experimental psycholinguistic 
data. 
9. Conclusions 
In this paper, we have presented an in-depth case study, in which we investigate 
machine learning techniques for automatically classifying a set of verbs into classes 
determined by their argument structures. We focus on the three major classes of op- 
tionally intransitive verbs in English, which cannot be discriminated by their subcate- 
gorizations, and therefore require distinctive features that are sensitive to the thematic 
properties of the verbs. We develop such features and automatically extract hem from 
very large, syntactically annotated corpora. Results show that a small number of lin- 
guistically motivated lexical features are sufficient o achieve a 69.8% accuracy rate 
in a three-way classification task with a baseline (chance) performance of 33.9%, for 
which the best performance achieved by a human expert is 86.5%. 
Returning to our original questions of what can and need be learned about the 
relational properties of verbs, we conclude that argument structure is both a highly 
useful and learnable aspect of verb knowledge. We observe that relevant semantic 
properties of verb classes (such as causativity, or animacy of subject) may be suc- 
cessfully approximated through countable syntactic features. In spite of noisy data 
(arising from diverse sources uch as tagging errors, or limitations of our extraction 
patterns), the lexical properties of interest are reflected in the corpora robustly enough 
to positively contribute to classification. 
We remark, however, that deep linguistic analysis cannot be eliminated--in our 
approach it is embedded in the selection of the features to count. Specifically, our 
features are derived through a detailed analysis of the differences in thematic role as- 
signments across the verb classes under investigation. Thus, an important contribution 
of the work is the proposed mapping between the thematic assignment properties of 
403 
Computational Linguistics Volume 27, Number 3 
the verb classes, and the statistical distributions of their surface syntactic properties. 
We think that using such linguistically motivated features makes the approach very 
effective and easily scalable: we report a 54% reduction in error rate (a 68% reduction, 
when the human expert-based upper bound is considered), using only five features 
that are readily extractable from automatically annotated corpora. 
Acknowledgments 
We gratefully acknowledge the financial 
support of the following organizations: the 
Swiss NSF (fellowship 8210-46569 to PM); 
the United States NSF (grants #9702331 and 
#9818322 to SS); the Canadian NSERC 
(grant o SS); the University of Toronto; and 
the Information Sciences Council of Rutgers 
University. Much of this research was 
carried out while PM was a visiting scientist 
at IRCS, University of Pennsylvania, nd 
while SS was a faculty member at Rutgers 
University, both of whose generous and 
supportive nvironments were of great 
benefit o us. We thank Martha Palmer, 
Michael Collins, Natalia Kariaeva, Kamin 
Whitehouse, Julie Boland, Kiva Dickinson, 
and three anonymous reviewers, for their 
helpful comments and suggestions, and for 
their contributions to this research. We also 
greatly thank our experts for the gracious 
contribution of their time in answering our 
electronic questionnaire. 
References 
Abney, Steven. 1996. Partial parsing via 
finite-state cascades. In John Carroll, 
editor, Proceedings ofthe Workshop on Robust 
Parsing at the Eighth Summer School on 
Logic, Language and Information, umber 
435 in CSRP, pages 8-15. University of 
Sussex, Brighton. 
Aone, Chinatsu and Douglas McKee. 1996. 
Acquiring predicate-argument mapping 
information i  multilingual texts. In 
Branimir Boguraev and James 
Pustejovsky, editors, Corpus Processing for 
Lexical Acquisition. MIT Press, 
pages 191-202. 
Basili, Roberto, Maria-Teresa Pazienza, and 
Paola Velardi. 1996. A context-driven 
conceptual c ustering method for verb 
classification. In Branimir Boguraev and 
James Pustejovsky, editors, Corpus 
Processing for Lexical Acquisition. MIT 
Press, pages 117-142. 
Boguraev, Branimir and Ted Briscoe. 1989. 
Utilising the LDOCE grammar codes. In 
Branimir Boguraev and Ted Briscoe, 
editors, Computational Lexicography for 
Natural Language Processing. Longman, 
London, pages 85-116. 
Boguraev, Branimir and James Pustejovsky. 
1996. Issues in text-based lexicon 
acquisition. In Branimir Boguraev and 
James Pustejovsky, editors, Corpus 
Processing for Lexical Acquisition. MIT 
Press, pages 3-20. 
Brent, Michael. 1993. From grammar to 
lexicon: Unsupervised learning of lexical 
syntax. Computational Linguistics, 
19(2):243-262. 
Briscoe, Ted and John Carroll. 1997. 
Automatic extraction of subcategorization 
from corpora. In Proceedings ofthe Fifth 
Applied Natural Language Processing 
Conference, pages 356-363. 
Brousseau, Anne-Marie and Elizabeth Ritter. 
1991. A non-unified analysis of agentive 
verbs. In West Coast Conference on Formal 
Linguistics, number 20, pages 53-64. 
Burzio, Luigi. 1986. Italian Syntax: A 
Government-Binding Approach. Reidel: 
Dordrecht. 
Carletta, Jean. 1996. Assessing agreement on 
classification tasks: the Kappa statistics. 
Computational Linguistics, 22(2):249-254. 
Collins, Michael John. 1997. Three 
generative, lexicalised models for 
statistical parsing. In Proceedings ofthe 35th 
Annual Meeting of the ACL, pages 16-23, 
Madrid, Spain. 
Cruse, D. A. 1972. A note on English 
causatives. Linguistic Inquiry, 3(4):520-528. 
Dang, Hoa Trang, Karin Kipper, Martha 
Palmer, and Joseph Rosenzweig. 1998. 
Investigating regular sense extensions 
based on intersective Levin classes. In 
Proceedings ofthe 36th Annual Meeting of the 
ACL and the 17th International Conference on 
Computational Linguistics (COLING-ACL 
"98), pages 293-299, Montreal. Universit~ 
de Montreal. 
Dixon, Robert M. W. 1994. Ergativity. 
Cambridge University Press, Cambridge. 
Dorr, Bonnie. 1997. Large-scale dictionary 
construction for foreign language tutoring 
and interlingual machine translation. 
Machine Translation, 12(4):1-55. 
Dorr, Bonnie, Joe Garman, and Amy 
Weinberg. 1995. From syntactic encodings 
to thematic roles: Building lexical entries 
for interlingual MT. Journal of Machine 
Translation, 9(3):71-100. 
404 
Merlo and Stevenson Statistical Verb Classification 
Dorr, Bonnie and Doug Jones. 1996. Role of 
word sense disambiguation i  lexical 
acquisition: Predicting semantics from 
syntactic ues. In Proceedings ofthe 16th 
International Conference on Computational 
Linguistics, pages 322-327, Copenhagen. 
Dowty, David. 1991. Thematic proto-roles 
and argument selection. Language, 
67(3):547-619. 
Greenberg, Joseph H. 1966. Language 
Universals. Mouton, The Hague, Paris. 
Gruber, Jeffrey. 1965. Studies in Lexical 
Relation. MIT Press, Cambridge, MA. 
Hale, Ken and Jay Keyser. 1993. On 
argument structure and the lexical 
representation f syntactic relations. In K. 
Hale and J. Keyser, editors, The View from 
Building 20. MIT Press, pages 53-110. 
Jakobson, Roman. 1971. Signe Z4ro. In 
Selected Writings, volume 2, 2d ed. 
Mouton, The Hague, pages 211-219. 
Kariaeva, Natalia. 1999. Discriminating 
between unaccusative and object-drop 
verbs: Animacy factor. Ms., Rutgers 
University. New Brunswick, NJ. 
Klavans, Judith and Martin Chodorow. 
1992. Degrees of stativity: The lexical 
representation f verb aspect. In 
Proceedings ofthe Fourteenth International 
Conference on Computational Linguistics 
(COLING "92), pages 1126-1131, Nantes, 
France. 
Klavans, Judith and Min-Yen Kan. 1998. 
Role of verbs in document analysis. In 
Proceedings ofthe 36th Annual Meeting of the 
ACL and the 17th International Conference on 
Computational Linguistics (COLING-ACL 
'98), pages 680-686, Montreal. Universit4 
de Montreal. 
Lapata, Maria. 1999. Acquiring lexical 
generalizations from corpora: A case 
study for diathesis alternations. In
Proceedings ofthe 37th Annual Meeting of the 
Association for Computational Linguistics 
(ACL'99), pages 397-404, College Park, 
MD. 
Lapata, Maria and Chris Brew. 1999. Using 
subcategorization to resolve verb class 
ambiguity. In Proceedings ofJoint SIGDAT 
Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora, 
pages 266-274, College Park, MD. 
Levin, Beth. 1985. Introduction. In Beth 
Levin, editor, Lexical Semantics in Review, 
number 1 in Lexicon Project Working 
Papers. Centre for Cognitive Science, MIT, 
Cambridge, MA, pages 1-62. 
Levin, Beth. 1993. English Verb Classes and 
Alternations. University of Chicago Press, 
Chicago, IL. 
Levin, Beth and Malka Rappaport Hovav. 
1995. Unaccusativity. MIT Press, 
Cambridge, MA. 
Manning, Christopher D. 1993. Automatic 
acquisition of a large subcategorization 
dictionary from corpora. In Proceedings of
the 31st Annual Meeting of the Association for 
Computational Linguistics, pages 235-242. 
Ohio State University. 
McCarthy, Diana. 2000. Using semantic 
preferences to identify verbal 
participation i  role switching 
alternations. In Proceedings of
ANLP-NAACL 2000, pages 256-263, 
Seattle, WA. 
McCarthy, Diana and Anna Korhonen. 1998. 
Detecting verbal participation i  diathesis 
alternations. In Proceedings ofthe 36th 
Annual Meeting of the ACL and the 17th 
International Conference on Computational 
Linguistics (COLING-ACL "98), 
pages 1493-1495, Montreal, Universit4 de 
Montreal. 
Merlo, Paola and Suzanne Stevenson. 1998. 
What grammars tell us about corpora: the 
case of reduced relative clauses. In 
Proceedings ofthe Sixth Workshop on Very 
Large Corpora, pages 134-142, Montreal. 
Merlo, Paola and Suzanne Stevenson. 2000a. 
Establishing the upper-bound and 
inter-judge agreement in a verb 
classification task. In Second International 
Conference on Language Resources and 
Evaluation (LREC-2000), volume 3, 
pages 1659-1664. 
Merlo, Paola and Suzanne Stevenson. 2000b. 
Lexical syntax and parsing architecture. 
In Matthew Crocker, Martin Pickering, 
and Charles Clifton, editors, Architectures 
and Mechanisms for Language Processing. 
Cambridge University Press, Cambridge, 
pages 161-188. 
Moravcsik, Edith and Jessica Wirth. 1983. 
Markedness--an Overview. In Fred 
Eckman, Edith Moravcsik, and Jessica 
Wirth, editors, Markedness. Plenum Press, 
New York, NY, pages 1-13. 
Oishi, Akira and Yuji Matsumoto. 1997. 
Detecting the organization of semantic 
subclasses of Japanese verbs. International 
Journal of Corpus Linguistics, 2(1):65-89. 
Palmer, Martha. 2000. Consistent criteria for 
sense distinctions. Special Issue of 
Computers and the Humanities, 
SENSEVAL98: Evaluating Word Sense 
Disambiguation Systems, 34(1-2):217-222. 
Perlmutter, David. 1978. Impersonal 
passives and the unaccusative hypothesis. 
In Proceedings ofthe Annual Meeting of the 
Berkeley Linguistics Society, volume 4, 
pages 157-189. 
405 
Computational Linguistics Volume 27, Number 3 
Pinker, Steven. 1989. Learnability and 
Cognition: the Acquisition of Argument 
Structure. MIT Press, Cambridge, MA. 
Quinlan, J. Ross. 1992. C4.5: Programs for 
Machine Learning. Series in Machine 
Learning. Morgan Kaufmann, San Mateo, 
CA. 
Ratnaparkhi, Adwait. 1996. A maximum 
entropy parbof-speech tagger. In 
Proceedings of the Empirical Methods in 
Natural Language Processing Conference, 
pages 133-142, Philadelphia, PA. 
Resnik, Philip. 1996. Selectional constraints: 
an information-theoretic model and its 
computational realization. Cognition, 
61(1-2):127-160. 
Riloff, Ellen and Mark Schmelzenbach. 1998. 
An empirical approach to conceptual case 
frame acquisition. In Proceedings of the Sixth 
Workshop on Very Large Corpora, 
pages 49-56. 
Rosch, Eleanor. 1978. Principles of 
categorization. I  Cognition and 
Categorization. Lawrence Erlbaum Assoc, 
Hillsdale, NJ. 
Sanfilippo, Antonio and Victor Poznanski. 
1992. The acquisition of lexical knowledge 
from combined machine-readable 
dictionary sources. In Proceedings of the 
Third Applied Natural Language Processing 
Conference, pages 80-87, Trento, Italy. 
Schulte im Walde, Sabine. 2000. Clustering 
verbs semantically according to their 
alternation behaviour. In Proceedings of 
COLING 2000, pages 747-753, 
Saarbruecken, Germany. 
Siegel, Eric 1998. Linguistic Indicators for 
Language Understanding: Using machine 
learning methods to combine corpus-based 
indicators for aspectual c assification of clauses. 
Ph.D. thesis, Dept. of Computer Science, 
Columbia University. 
Siegel, Eric. 1999. Corpus-based linguistic 
indicators for aspectual classification. In
Proceedings of ACL'99, pages 112-119, 
College Park, MD. University of 
Maryland. 
Siegel, Sidney and John Castellan. 1988. 
Nonparametric statistics for the Behavioral 
Sciences. McGraw-Hill, New York. 
Silverstein, Michael. 1976. Hierarchy of 
features and ergativity. In Robert Dixon, 
editor, Grammatical Categories in Australian 
Languages. Australian Institute of 
Aboriginal Studies, Canberra, 
pages 112-171. 
Srinivas, Bangalore and Aravind K. Joshi. 
1999. Supertagging: An approach to 
almost parsing. Computational Linguistics, 
25(2):237-265. 
Stede, Manfred. 1998. A generative 
perspective on verb alternations. 
Computational Linguistics, 24(3):401--430. 
Stevenson, Suzanne and Paola Merlo. 1997a. 
Architecture and experience in sentence 
processing. In Proceedings of the 19th 
Annual Conference of the Cognitive Science 
Society, pages 715-720. 
Stevenson, Suzanne and Paola Merlo. 1997b. 
Lexical structure and processing 
complexity. Language and Cognitive 
Processes, 12(1-2):349-399. 
Stevenson, Suzanne, Paola Merlo, Natalia 
Kariaeva, and Kamin Whitehouse. 1999. 
Supervised learning of lexical semantic 
verb classes using frequency 
distributions. In Proceedings of SigLex99: 
Standardizing Lexical Resources (SigLex'99), 
pages 15-21, College Park, MD. 
Trubetzkoy, Nicolaj S. 1939. Grundzage der 
Phonologie. Travaux du Cercle 
Linguistique de Prague, Prague. 
Webster, Mort and Mitch Marcus. 1989. 
Automatic acquisition of the lexical 
semantics of verbs from sentence frames. 
In Proceedings of the 27th Annual Meeting of 
the Association for Computational Linguistics, 
pages 177-184, Vancouver, Canada. 
406 
Merlo and Stevenson Statistical Verb Classification 
Appendix A 
The fo l lowing three tables conta in the overall  f requency and the normal i zed  feature 
values for each of the 59 verbs in our exper imental  set. 
Unergative Verbs 
Freq VBN PASS TRANS CAUS ANIM 
Min Value 8 0.00 0.00 0.00 0.00 0.00 
Max Value 4088 1.00 0.39 0.74 0.00 1.00 
floated 176 0.43 0.26 0.74 0.00 0.17 
hurried 86 0.40 0.31 0.50 0.00 0.37 
jumped 4088 0.09 0.00 0.03 0.00 0.20 
leaped 225 0.09 0.00 0.05 0.00 0.13 
marched 238 0.10 0.01 0.09 0.00 0.12 
paraded 33 0.73 0.39 0.46 0.00 0.50 
raced 123 0.01 0.00 0.06 0.00 0.15 
rushed 467 0.22 0.12 0.20 0.00 0.10 
vaulted 54 0.00 0.00 0.41 0.00 0.03 
wandered 67 0.02 0.00 0.03 0.00 0.32 
galloped 12 1.00 0.00 0.00 0.00 0.00 
glided 14 0.00 0.00 0.08 0.00 0.50 
hiked 25 0.28 0.12 0.29 0.00 0.40 
hopped 29 0.00 0.00 0.21 0.00 1.00 
jogged 8 0.29 0.00 0.29 0.00 0.33 
scooted 10 0.00 0.00 0.43 0.00 0.00 
scurried 21 0.00 0.00 0.00 0.00 0.14 
skipped 82 0.22 0.02 0.64 0.00 0.16 
tiptoed 12 0.17 0.00 0.00 0.00 0.00 
trotted 37 0.19 0.17 0.07 0.00 0.18 
Unaccusative Verbs 
Freq VBN PASS TRANS CAUS ANIM 
Min Value 13 0.16 0.00 0.02 0.00 0.00 
Max Value 5543 0.95 0.80 0.76 0.41 0.36 
boiled 58 0.92 0.70 0.42 0.00 0.00 
cracked 175 0.61 0.19 0.76 0.02 0.14 
dissolved 226 0.51 0.58 0.71 0.05 0.11 
exploded 409 0.34 0.02 0.66 0.37 0.04 
flooded 235 0.47 0.57 0.44 0.04 0.03 
fractured 55 0.95 0.76 0.51 0.00 0.00 
hardened 123 0.92 0.55 0.56 0.12 0.00 
melted 70 0.80 0.44 0.02 0.00 0.19 
opened 3412 0.21 0.09 0.69 0.16 0.36 
solidified 34 0.65 0.21 0.68 0.00 0.12 
collapsed 950 0.16 0.00 0.16 0.01 0.02 
cooled 232 0.85 0.21 0.29 0.13 0.11 
folded 189 0.73 0.33 0.23 0.00 0.00 
widened 1155 0.18 0.02 0.13 0.41 0.01 
changed 5543 0.73 0.23 0.47 0.22 0.08 
cleared 1145 0.58 0.40 0.50 0.31 0.06 
divided 1539 0.93 0.80 0.17 0.10 0.05 
simmered 13 0.83 0.00 0.09 0.00 0.00 
stabilized 286 0.92 0.13 0.18 0.35 0.00 
407 
Computational Linguistics Volume 27, Number 3 
Object-Drop Verbs 
Freq VBN PASS TRANS CAUS ANIM 
Min Value 39 0.10 0.04 0.21 0.00 0.00 
Max Value 15063 0.95 0.99 1.00 0.24 0.42 
carved 185 0.85 0.66 0.98 0.00 0.00 
danced 88 0.22 0.14 0.37 0.00 0.00 
kicked 308 0.30 0.18 0.97 0.00 0.33 
knitted 39 0.95 0.99 0.93 0.00 0.00 
painted 506 0.72 0.18 0.71 0.00 0.38 
played 2689 0.38 0.16 0.24 0.00 0.00 
reaped 172 0.56 0.05 0.90 0.00 0.22 
typed 57 0.81 0.74 0.81 0.00 0.00 
washed 137 0.79 0.60 1.00 0.00 0.00 
yelled 74 0.10 0.04 0.38 0.00 0.00 
borrowed 1188 0.77 0.15 0.60 0.13 0.19 
inherited 357 0.60 0.13 0.64 0.06 0.32 
organized 1504 0.85 0.38 0.65 0.18 0.07 
rented 232 0.72 0.22 0.61 0.00 0.42 
sketched 44 0.67 0.17 0.44 0.00 0.20 
cleaned 160 0.83 0.47 0.21 0.05 0.21 
packed 376 0.84 0.12 0.40 0.05 0.19 
studied 901 0.66 0.17 0.57 0.05 0.11 
swallowed 152 0.79 0.44 0.35 0.04 0.22 
called 15063 0.56 0.22 0.72 0.24 0.16 
Appendix B 
Performance of all the subsets of features, in  order of decreasing accuracy. To determine 
whether  the difference between any two results is statistically signif icant, the 95% 
confidence interval  can be calculated for each of the two results, and  the two ranges 
checked to see whether  they overlap. To do this, take each accuracy p lus and  minus  
2.01 t imes its associated s tandard  error to get the 95% confidence range (dr = 49, 
t = 2.01). If the two ranges overlap, then the difference in accuracy is not  signif icant 
at the p < .05 level. 
Accuracy SE Features Accuracy SE Features 
69.8 0.5 TRANS PASS VBN CAUS ANIM 57.3 0.5 TRANS CAUS 
69.8 0.5 TRANS VBN CAUS ANIM 57.3 0.5 PASS VBN ANIM 
67.3 0.6 TRANS PASS VBN ANIM 56.7 0.5 PASS CAUS ANIM 
66.7 0.5 TRANS VBN ANIM 55.7 0.5 VBN CAUS 
66.5 0.5 TRANS PASS CAUS ANIM 55.7 0.1 CAUS 
64.4 0.5 TRANS VBN CAUS 55.4 0.4 PASS CAUS 
63.2 0.6 TRANS PASS VBN CAUS 55.0 0.6 TIKANS PASS VBN 
63.0 0.5 TRANS PASS ANIM 54.7 0.4 TRANS PASS 
62.9 0.4 TRANS CAUS AN~M 54.2 0.5 TRANS VBN 
62.1 0.5 CAUS ANIM 52.5 0.5 VBN 
61.7 0.5 TRANS PASS CAUS 50.9 0.5 PASS ANIM 
61.6 0.6 PASS VBN CAUS ANIM 50.2 0.6 PASS VBN 
60.1 0.4 VBN CAUS ANIM 50.2 0.5 PASS 
59.5 0.6 TRANS ANIM 47.1 0.4 TKANS 
59.4 0.5 VBN ANIM 35.3 0.5 ANIM 
57.4 0.6 PASS VBN CAUS 
408 
Semantic Role Labeling: An Introduction to
the Special Issue
Llu??s Ma`rquez?
Universitat Polite`cnica de Catalunya
Xavier Carreras??
Massachusetts Institute of Technology
Kenneth C. Litkowski?
CL Research
Suzanne Stevenson?
University of Toronto
Semantic role labeling, the computational identification and labeling of arguments in text,
has become a leading task in computational linguistics today. Although the issues for this
task have been studied for decades, the availability of large resources and the development of
statistical machine learning methods have heightened the amount of effort in this field. This
special issue presents selected and representative work in the field. This overview describes
linguistic background of the problem, the movement from linguistic theories to computational
practice, the major resources that are being used, an overview of steps taken in computational
systems, and a description of the key issues and results in semantic role labeling (as revealed in
several international evaluations). We assess weaknesses in semantic role labeling and identify
important challenges facing the field. Overall, the opportunities and the potential for useful
further research in semantic role labeling are considerable.
1. Introduction
The sentence-level semantic analysis of text is concerned with the characterization of
events, such as determining ?who? did ?what? to ?whom,? ?where,? ?when,? and
?how.? The predicate of a clause (typically a verb) establishes ?what? took place,
and other sentence constituents express the participants in the event (such as ?who? and
?where?), as well as further event properties (such as ?when? and ?how?). The primary
task of semantic role labeling (SRL) is to indicate exactly what semantic relations hold
among a predicate and its associated participants and properties, with these relations
? Departament de Llenguatges i Sistemes Informa`tics, Universitat Polite`cnica de Catalunya, Jordi Girona
Salgado 1?3, 08034 Barcelona, Spain. E-mail: lluism@lsi.upc.edu.
?? Computer Science and Artificial Intelligence Laboratory (CSAIL), MIT, 32 Vassar St., Cambridge, MA
02139, USA. E-mail: carreras@csail.mit.edu.
? CL Research, 9208 Gue Road, Damascus, MD 20872 USA. E-mail: ken@clres.com.
? Department of Computer Science, 6 King?s College Road, Toronto, ON M5S 3G4, Canada.
E-mail: suzanne@cs.toronto.edu.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 2
drawn from a pre-specified list of possible semantic roles for that predicate (or class of
predicates). In order to accomplish this, the role-bearing constituents in a clause must
be identified and their correct semantic role labels assigned, as in:
[The girl on the swing]Agent [whispered]Pred to [the boy beside her]Recipient
Typical roles used in SRL are labels such as Agent, Patient, and Location for the entities
participating in an event, and Temporal and Manner for the characterization of other
aspects of the event or participant relations. This type of role labeling thus yields a first-
level semantic representation of the text that indicates the basic event properties and
relations among relevant entities that are expressed in the sentence.
Research has proceeded for decades on manually created lexicons, grammars, and
other semantic resources (Hirst 1987; Pustejovsky 1995; Copestake and Flickinger 2000)
in support of deep semantic analysis of language input, but such approaches have been
labor-intensive and often restricted to narrow domains. The 1990s saw a growth in
the development of statistical machine learning methods across the field of computa-
tional linguistics, enabling systems to learn complex linguistic knowledge rather than
requiring manual encoding. These methods were shown to be effective in acquiring
knowledge necessary for semantic interpretation, such as the properties of predicates
and the relations to their arguments?for example, learning subcategorization frames
(Briscoe and Carroll 1997) or classifying verbs according to argument structure prop-
erties (Merlo and Stevenson 2001; Schulte im Walde 2006). Recently, medium-to-large
corpora have been manually annotated with semantic roles in FrameNet (Fillmore,
Ruppenhofer, and Baker 2004), PropBank (Palmer, Gildea, and Kingsbury 2005), and
NomBank (Meyers et al 2004), enabling the development of statistical approaches
specifically for SRL.
With the advent of supporting resources, SRL has become a well-defined task with
a substantial body of work and comparative evaluation (see, among others, Gildea and
Jurafsky [2002], Surdeanu et al [2003], Xue and Palmer [2004], Pradhan et al [2005a],
the CoNLL Shared Task in 2004 and 2005, and Senseval-3 and SemEval-2007). The
identification of event frames may potentially benefit many natural language processing
(NLP) applications, such as information extraction (Surdeanu et al 2003), question
answering (Narayanan and Harabagiu 2004), summarization (Melli et al 2005), and
machine translation (Boas 2002). Related work on classifying the semantic relations in
noun phrases has also been encouraging for NLP tasks (Moldovan et al 2004; Rosario
and Hearst 2004).
Although the use of SRL systems in real-world applications has thus far been
limited, the outlook is promising for extending this type of analysis to many appli-
cations requiring some level of semantic interpretation. SRL represents an excellent
framework with which to perform research on computational techniques for acquiring
and exploiting semantic relations among the different components of a text.
This special issue of Computational Linguistics presents several articles represent-
ing the state-of-the-art in SRL, and this overview is intended to provide a broader
context for that work. First, we briefly discuss some of the linguistic views on se-
mantic roles that have had the most influence on computational approaches to SRL
and related NLP tasks. Next, we show how the linguistic notions have influenced
the development of resources that support SRL. We then provide an overview of
SRL methods and describe the state-of-the-art as well as current open problems in the
field.
146
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
2. Semantic Roles in Linguistics
Since the foundational work of Fillmore (1968), considerable linguistic research has been
devoted to the nature of semantic roles. Although there is substantial agreement on
major semantic roles, such as Agent and Theme, there is no consensus on a definitive
list of semantic roles, or even whether such a list exists. Proposed lists range from a
large set of situation-specific roles, such as Suspect, Authorities, and Offense (Fillmore,
Ruppenhofer, and Baker 2004), to a relatively small set of general roles, such as Agent,
Theme, Location, and Goal (typically referred to as thematic roles, as in Jackendoff
[1990]), to the set of two core roles, Proto-Agent and Proto-Theme, whose entailments
determine the precise relation expressed (Dowty 1991). This uncertainty within linguis-
tic theory carries over into computational work on SRL, where there is much variability
on the roles assumed in different resources.
A major focus of work in the linguistics community is on the mapping between the
predicate?argument structure that determines the roles, and the syntactic realization of
the recipients of those roles (Grimshaw 1990; Levin 1993; Levin and Rappaport Hovav
2005). Semantic role lists are generally viewed as inadequate for explaining the mor-
phosyntactic behavior of argument expression, with argument realization dependent
on a deeper lexical semantic representation of the components of the event that the
predicate describes. Although much of the mapping from argument structure to syntax
is predictable, this mapping is not completely regular, nor entirely understood. An
important question for SRL, therefore, is the extent to which performance is degraded
by the irregularities noted in linguistic studies of semantic roles.
Nonetheless, sufficient regularity exists to provide the foundation for meaningful
generalizations. Much research has focused on explaining the varied expression of verb
arguments within syntactic positions (Levin 1993). A major conclusion of that work is
that the patterns of syntactic alternation exhibit regularity that reflects an underlying
semantic similarity among verbs, forming the basis for verb classes. Such classes, and
the argument structure specifications for them, have proven useful in a number of NLP
tasks (Habash, Dorr, and Traum 2003; Shi and Mihalcea 2005), including SRL (Swier and
Stevenson 2004), and have provided the foundation for the computational verb lexicon
VerbNet (Kipper, Dang, and Palmer 2000).
This approach to argument realization focuses on the relation of morphosyntactic
behavior to argument semantics, and typically leads to a general conceptualization of
semantic roles. In frame semantics (Fillmore 1976), on the other hand, a word activates
a frame of semantic knowledge that relates linguistic semantics to encyclopedic knowl-
edge. This effort has tended to focus on the delineation of situation-specific frames (e.g.,
an Arrest frame) and correspondingly more specific semantic roles (e.g., Suspect and
Authorities) that codify the conceptual structure associated with lexical items (Fillmore,
Ruppenhofer, and Baker 2004). With a recognition that many lexical items could activate
any such frame, this approach leads to lexical classes of a somewhat different nature
than those of Levin (1993). Whereas lexical items in a Levin class are syntactically
homogeneous and share coarse semantic properties, items in a frame may syntactically
vary somewhat but share fine-grained, real-world semantic properties.
A further difference in these perspectives is the view of the roles themselves. In
defining verb classes that capture argument structure similarities, Levin (1993) does not
explicitly draw on the notion of semantic role, instead basing the classes on behavior
that is hypothesized to reflect the properties of those roles. Other work also eschews
the notion of a simple list of roles, instead postulating underlying semantic structure
that captures the relevant properties (Levin and Rappaport Hovav 1998). Interestingly,
147
Computational Linguistics Volume 34, Number 2
as described in Fillmore, Ruppenhofer, and Baker (2004), frame semantics also avoids a
predefined list of roles, but for different reasons. The set of semantic roles, called frame
elements, are chosen for each frame, rather than being selected from a predefined list
that may not capture the relevant distinctions in that particular situation. Clearly, to
the extent that disagreement persists on semantic role lists and the nature of the roles
themselves, SRL may be working on a shifting target.
These approaches also differ in the broad characterization of event participants
(and their roles) as more or less essential to the predicate. In the more syntactic-oriented
approaches, roles are typically divided into two categories: arguments, which cap-
ture a core relation, and adjuncts, which are less central. In frame semantics, the roles
are divided into core frame elements (e.g., Suspect, Authorities, Offense) and periph-
eral or extra-thematic elements (e.g., Manner, Time, Place). These distinctions carry
over into SRL, where we see that systems generally perform better on the more central
arguments.
Finally, although predicates are typically expressed as verbs, and thus much work
in both linguistics and SRL focuses on them, some nouns and adjectives may be used
predicatively, assigning their own roles to entities (as in the adjective phrase proud that
we finished the paper, where the subordinate clause is a Theme argument of the adjective
proud). Frame semantics tends to include in a frame relevant non-verb lexical items,
due to the emphasis on a common situation semantics. In contrast, the morphosyntactic
approaches have focused on defining classes of verbs only, because they depend on
common syntactic behavior that may not be apparent across syntactic categories.
Interestingly, prepositions have a somewhat dual status with regard to role labeling.
In languages like English, prepositions serve an important function in signaling the rela-
tion of a participant to a verb. For example, it is widely accepted that to in give the book to
Mary serves as a grammatical indicator of the Recipient role assigned by the verb, rather
than as a role assigner itself. In other situations, however, a preposition can be viewed
as a role-assigning predicate in its own right. Although some work in computational
linguistics is tackling the issue of the appropriate characterization of prepositions and
their contribution to semantic role assignment (as we see subsequently), much work
remains in order to fully integrate linguistic theories of prepositional function and
semantics into SRL.
3. From Linguistic Theory to Computational Resources
The linguistic approaches to semantic roles discussed previously have greatly influ-
enced current work on SRL, leading to the creation of significant computational lexicons
capturing the foundational properties of predicate?argument relations.
In the FrameNet project (Fillmore, Ruppenhofer, and Baker 2004), lexicographers
define a frame to capture some semantic situation (e.g., Arrest), identify lexical items
as belonging to the frame (e.g., apprehend and bust), and devise appropriate roles for
the frame (e.g., Suspect, Authorities, Offense). They then select and annotate example
sentences from the British National Corpus and other sources to illustrate the range of
possible assignments of roles to sentence constituents for each lexical item (at present,
over 141,000 sentences have been annotated).
FrameNet thus consists of both a computational lexicon and a role-annotated cor-
pus. The existence of such a corpus enabled Gildea and Jurafsky (2002) to develop the
first statistical machine learning approach to SRL, using various lexical and syntactic
features such as phrase type and grammatical function calculated over the annotated
constituents. Although this research spurred the current wave of SRL work that has
148
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
refined and extended Gildea and Jurafsky?s approach, the FrameNet data has not been
used extensively. One issue is that the corpus is not a representative sample of the
language, but rather consists of sentences chosen manually to illustrate the possible
role assignments for a given lexical item. Another issue is that the semantic roles are
situation-specific, rather than general roles like Agent, Theme, and Location that can be
used across many situations and genres.
The computational verb lexicon, VerbNet (Kipper, Dang, and Palmer 2000), instead
builds on Levin?s (1993) work on defining verb classes according to shared argument re-
alization patterns. VerbNet regularizes and extends the original Levin classes; moreover,
each class is explicitly associated with argument realization specifications that state the
constituents that a verb can occur with and the role assigned to each. The roles are
mostly drawn from a small set (around 25) of general roles widely used in linguistic
theory. This lexicon has been an important resource in computational linguistics, but
because of the lack of an associated role-annotated corpus, it has only been used directly
in SRL in an unsupervised setting (Swier and Stevenson 2004).
Research on VerbNet inspired the development of the Proposition Bank (PropBank;
Palmer, Gildea, and Kingsbury 2005), which has emerged as a primary resource for
research in SRL (and used in four of the articles in this special issue). PropBank ad-
dresses some of the issues for SRL posed by the FrameNet data. First, the PropBank
project has annotated the semantic roles for all verbs in the Penn Treebank corpus (the
Wall Street Journal [WSJ] news corpus). This provides a representative sample of text
with role-annotations, in contrast to FrameNet?s reliance on manually selected, illus-
trative sentences. Importantly, PropBank?s composition allows for consideration of the
statistical patterns across natural text. Although there is some concern about the limited
genre of its newspaper text, this aspect has the advantage of allowing SRL systems to
benefit from the state-of-the-art syntactic parsers and other resources developed with
the WSJ TreeBank data. Moreover, current work is extending the PropBank annotation
to balanced corpora such as the Brown corpus.
The lexical information associated with verbs in PropBank also differs significantly
from the situation-specific roles of FrameNet. At the same time, the PropBank designers
recognize the difficulty of providing a small, predefined list of semantic roles that is suf-
ficient for all verbs and predicate?argument relations, as in VerbNet. PropBank instead
takes a ?theory-neutral? approach to the designation of core semantic roles. Each verb
has a frameset listing its allowed role labelings in which the arguments are designated
by number (starting from 0). Each numbered argument is provided with an English-
language description specific to that verb. Participants typically considered as adjuncts
are given named argument roles, because there is more general agreement on such
modifiers as Temporal or Manner applying consistently across verbs. Different senses
for a polysemous verb have different framesets; however, syntactic alternations which
preserve meaning (as identified in Levin [1993]) are considered to be a single frameset.
While the designations of Arg0 and Arg1 are intended to indicate the general roles of
Agent and Theme/Patient across verbs, other argument numbers do not consistently
correspond to general (non-verb-specific) semantic roles.
Given the variability in the sets of roles used across the computational resources,
an important issue is the extent to which different role sets affect the SRL task, as well
as subsequent use of the output in other NLP applications. Gildea and Jurafsky (2002)
initiated this type of investigation by exploring whether their results were dependent
on the set of semantic roles they used. To this end, they mapped the FrameNet frame
elements into a set of abstract thematic roles (i.e., more general roles such as Agent,
Theme, Location), and concluded that their system could use these thematic roles
149
Computational Linguistics Volume 34, Number 2
without degradation. Similar questions must be investigated in the context of PropBank,
where the framesets for the verbs may have significant domain-specific meanings and
arguments due to the dependence of the project on WSJ data. Given the uncertainty in
the linguistic status of semantic role lists, and the lack of evidence about which types
of roles would be most useful in various NLP tasks, an important ongoing focus of
attention is the value of mapping between the role sets of the different resources (Swier
and Stevenson 2005; Loper, Yi, and Palmer 2007; Yi, Loper, and Palmer 2007).
We noted previously the somewhat special part that prepositions play in marking
semantic relations, in some sense mediating the role assignment of a verb to an argu-
ment. The resources noted earlier differ in their treatment of prepositions. In VerbNet,
for example, prepositions are listed explicitly as part of the syntactic context in which
a role is assigned (e.g., Agent V Prep(for) Recipient), but it is the NP object of the prep-
osition that receives the semantic role. In FrameNet and PropBank, on the other hand,
the full prepositional phrase is considered as the frame element (the constituent re-
ceiving the role). Clearly, further work needs to proceed on how to best capture the in-
teraction between verbs and prepositions in SRL. This is especially complex given
the high polysemy of prepositions, and work has proceeded on relating preposition
disambiguation to role assignment (e.g., O?Hara and Wiebe 2003). For such approaches
to make meaningful progress, resources are needed that elaborate the senses of prepo-
sitions and relate those senses to semantic roles. In The Preposition Project (TPP;
Litkowski and Hargraves 2005), a comprehensive, hierarchical characterization of the
semantic roles for all preposition senses in English is being developed. TPP has sense-
tagged more than 25,000 preposition instances in FrameNet sentences, allowing for
comprehensive investigation of the linking between preposition sense and semantic role
assignment.
4. Approaches to Automatic SRL
The work on SRL has included a broad spectrum of probabilistic and machine-learning
approaches to the task. We focus here on supervised systems, because most SRL research
takes an approach requiring training on role-annotated data. We briefly survey the main
approaches to automatic SRL, and the types of learning features used.
4.1 SRL Step by Step
Given a sentence and a designated verb, the SRL task consists of identifying the bound-
aries of the arguments of the verb predicate (argument identification) and labeling
them with semantic roles (argument classification). The most common architecture for
automatic SRL consists of the following steps to achieve these subtasks.
The first step in SRL typically consists of filtering (or pruning) the set of argu-
ment candidates for a given predicate. Because arguments may be a continuous or
discontinuous sequence of words, any subsequence of words in the sentence is an
argument candidate. Exhaustive exploration of this space of candidates is not feasible,
because it is both very large and imbalanced (i.e., the vast majority of candidates are
not actual arguments of the verb). The simple heuristic rules of Xue and Palmer (2004)
are commonly used to perform filtering because they greatly reduce the set of candidate
arguments, while maintaining a very high recall.
The second step consists of a local scoring of argument candidates by means of
a function that outputs probabilities (or confidence scores) for each of the possible
150
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
role labels, plus an extra ?no-argument? label indicating that the candidate should
not be considered an argument in the solution. In this step, candidates are usually
treated independently of each other. A crucial aspect in local scoring (see Section 4.2)
is the representation of candidates with features, rather than the particular choice of
classification algorithm.
Argument identification and classification may be treated jointly or separately in
the local scoring step. In the latter case, a pipeline of two subprocesses is typically
applied, first scoring between ?argument? and ?no-argument? labels, and then scoring
the particular argument labels. Because argument identification is closely related to
syntax and argument classification is more a semantic issue, useful features for the two
subtasks may be very different?that is, a good feature for addressing recognition may
hurt classification and vice versa (Pradhan et al 2005a).
The third step in SRL is to apply a joint scoring (or global scoring) in order to
combine the predictions of local scorers to produce a good structure of labeled argu-
ments for the predicate. In this step, dependencies among several arguments of the same
predicate can be exploited. For instance, Punyakanok, Roth, and Yih (this issue) ensure
that a labeling satisfies a set of structural and SRL-dependent constraints (arguments
do not overlap, core arguments do not repeat, etc.). Also in this issue, Toutanova,
Haghighi, and Manning apply re-ranking to select the best among a set of candidate
complete solutions produced by a base SRL system. Finally, probabilistic models have
also been applied to produce the structured output, for example, generative models
(Thompson, Levy, and Manning 2003), sequence tagging with classifiers (Ma`rquez et al
2005; Pradhan et al 2005b), and Conditional Random Fields on tree structures (Cohn
and Blunsom 2005). These approaches at a global level may demand considerable extra
computation, but current optimization techniques help solve them quite efficiently.
Some variations in the three-step architecture are found. Systems may bypass one
of the steps, by doing only local scoring, or skipping directly to joint scoring. A fourth
step may consist of fixing common errors or enforcing coherence in the final solution.
This postprocess usually consists of a set of hand-developed heuristic rules that are
dependent on a particular architecture and corpus of application.
An important consideration within this general SRL architecture is the combination
of systems and input annotations. Most SRL systems include some kind of combi-
nation to increase robustness, gain coverage, and reduce effects of parse errors. One
may combine: (1) the output of several independent SRL basic systems (Surdeanu
et al 2007; Pradhan et al 2005b), or (2) several outputs from the same SRL system
obtained by changing input annotations or other internal parameters (Koomen et al
2005; Toutanova, Haghighi, and Manning 2005). The combination can be as simple as
selecting the best among the set of complete candidate solutions, but usually consists of
combining fragments of alternative solutions to construct the final output. Finally, the
combination component may involve machine learning or not. The gain in performance
from the combination step is consistently between two and three F1 points. However, a
combination approach increases system complexity and penalizes efficiency.
Several exceptions to this described architecture for SRL can be found in the lit-
erature. One approach entails joint labeling of all predicates of the sentence, instead
of proceeding one by one. This opens the possibility of exploiting dependencies among
the different verbs in the sentence. However, the complexity may grow significantly, and
results so far are inconclusive (Carreras, Ma`rquez, and Chrupa?a 2004; Surdeanu et al
2007). Other promising approaches draw on dependency parsing rather than traditional
phrase structure parsing (Johansson and Nugues 2007), or combine parsing and SRL
into a single step of semantic parsing (Musillo and Merlo 2006).
151
Computational Linguistics Volume 34, Number 2
4.2 Feature Engineering
As previously noted, devising the features with which to encode candidate arguments
is crucial for obtaining good results in the SRL task. Given a verb and a candidate argu-
ment (a syntactic phrase) to be classified in the local scoring step, three types of features
are typically used: (1) features that characterize the candidate argument and its context;
(2) features that characterize the verb predicate and its context; and (3) features that cap-
ture the relation (either syntactic or semantic) between the candidate and the predicate.
Gildea and Jurafsky (2002) presented a compact set of features across these three
types, which has served as the core of most of the subsequent SRL work: (1) the phrase
type, headword, and governing category of the constituent; (2) the lemma, voice, and
subcategorization pattern of the verb; and (3) the left/right position of the constituent
with respect to the verb, and the category path between them. Extensions to these fea-
tures have been proposed in various directions. Exploiting the ability of some machine
learning algorithms to work with very large feature spaces, some authors have largely
extended the representation of the constituent and its context, including among others:
first and last words (and part-of-speech) in the constituent, bag-of-words, n-grams of
part of speech, and sequence of top syntactic elements in the constituent. Parent and
sibling constituents in the tree may also be codified with all the previous structural and
lexical features (Pradhan et al 2005a; Surdeanu et al 2007). Other authors have designed
new features with specific linguistic motivations. For instance, Surdeanu et al (2003)
generalized the concept of headword with the content word feature. They also used
named entity labels as features. Xue and Palmer (2004) presented the syntactic frame
feature, which captures the overall sentence structure using the verb predicate and the
constituent as pivots. All these features resulted in a significant increase in performance.
Finally, regarding the relation between the constituent and the predicate, several
variants of Gildea and Jurafsky?s syntactic path have been proposed in the literature
(e.g., generalizations to avoid sparsity, and adaptations to partial parsing). Also, some
attempts have been made at characterizing the semantic relation between the predicate
and the constituent. In Zapirain, Agirre, and Ma`rquez (2007) and Erk (2007), selectional
preferences between predicate and headword of the constituent are explored to generate
semantic compatibility features. Using conjunctions of several of the basic features is
also common practice. This may be very relevant when the machine learning method
used is linear in the space of features.
Joint scoring and combination components open the door to richer types of fea-
tures, which may take into account global properties of the candidate solution plus de-
pendencies among the different arguments. The most remarkable work in this direction
is the reranking approach by Toutanova, Haghighi, and Manning in this issue. When
training the ranker to select the best candidate solution they codify pattern features as
strings containing the whole argument structure of the candidate. Several variations
of this type of feature (with different degrees of generalization to avoid sparseness)
allow them to significantly increase the performance of the base system. Also related,
Pradhan et al (2005b) and Surdeanu et al (2007) convert the confidence scores of several
base SRL systems into features for training a final machine learning?based combination
system. Surdeanu et al (2007) develop a broad spectrum of features, with sentence-
based information, describing the role played by the candidate argument in every
solution proposed by the different base SRL systems.
A completely different approach to feature engineering is the use of kernel meth-
ods to implicitly exploit all kinds of substructures in the syntactic representation of
the candidates. This knowledge poor approach intends to take advantage of a massive
152
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
quantity of features without the need for manual engineering of specialized features.
This motivation might be relevant for fast system development and porting, especially
when specialized linguistic knowledge of the language of application is not available.
The most studied approach consists of using some variants of the ?all subtrees kernel?
applied to the sentence parse trees. The work by Moschitti, Pighin, and Basili in this
issue is the main representative of this family.
5. Empirical Evaluations of SRL Systems
Many experimental studies have been conducted since the work of Gildea and Jurafsky
(2002), including seven international evaluation tasks in ACL-related conferences and
workshops: the SIGNLL CoNLL shared tasks in 2004 and 2005 (Carreras and Ma`rquez
2004, 2005), the SIGLEX Senseval-3 in 2004 (Litkowski 2004), and four tasks in the
SIGLEX SemEval in 2007 (Pradhan et al 2007; Ma`rquez et al 2007; Baker, Ellsworth, and
Erk 2007; Litkowski and Hargraves 2007). In the subsequent sections, we summarize
their main features, results, and conclusions, although note that the scores are not
directly comparable across different exercises, due to differences in scoring and in the
experimental methodologies.
5.1 Task Definition and Evaluation Metrics
The standard experiment in automatic SRL can be defined as follows: Given a sentence
and a target predicate appearing in it, find the arguments of the predicate and label
them with semantic roles. A system is evaluated in terms of precision, recall, and F1 of
the labeled arguments. In evaluating a system, an argument is considered correct when
both its boundaries and the semantic role label match a gold standard. Performance
can be divided into two components: (1) the precision, recall, and F1 of unlabeled
arguments, measuring the accuracy of the system at segmenting the sentence; and (2)
the classification accuracy of assigning semantic roles to the arguments that have been
correctly identified. In calculating the metrics, the de facto standard is to give credit only
when a proposed argument perfectly matches an argument in the reference solution;
nonetheless, variants that give some credit for partial matching also exist.
5.2 Shared Task Experiments Using FrameNet, PropBank, and VerbNet
To date, most experimental work has made use of English data annotated either with
PropBank or FrameNet semantic roles.
The CoNLL shared tasks in 2004 and 2005 were based on PropBank (Carreras and
Ma`rquez 2004, 2005), which is the largest evaluation benchmark available today, and
also the most used by researchers?all articles in this special issue dealing with English
use this benchmark. In the evaluation, the best systems obtained an F1 score of ?80%,
and have achieved only minimal improvements since then. The articles in this issue by
Punyakanok, Roth, and Yih; Toutanova, Haghighi, and Manning; and Pradhan, Ward,
and Martin describe such efforts. An analysis of the outputs in CoNLL-2005 showed
that argument identification accounts for most of the errors: a system will recall ?81%
of the correct unlabeled arguments, and ?95% of those will be assigned the correct
semantic role. The analysis also showed that systems recognized core arguments better
than adjuncts (with F1 scores from the high 60s to the high 80s for the former, but below
60% for the latter). Finally, it was also observed that, although systems performed better
153
Computational Linguistics Volume 34, Number 2
on verbs appearing frequently in training, the best systems could recognize arguments
of unseen verbs with an F1 in the low 70s, not far from the overall performance.
1
SemEval-2007 included a task on semantic evaluation for English, combining word
sense disambiguation and SRL based on PropBank (Pradhan et al 2007). Unlike the
CoNLL tasks, this task concentrated on 50 selected verbs. Interestingly, the data was
annotated using verb-independent roles using the PropBank/VerbNet mapping from
Yi, Loper, and Palmer (2007). The two participating systems could predict VerbNet roles
as accurately as PropBank verb-dependent roles.
Experiments based on FrameNet usually concentrate on a selected list of frames.
In Senseval-3, 40 frames were selected for an SRL task with the goal of replicating
Gildea and Jurafsky (2002) and improving on them (Litkowski 2004). Participants were
evaluated on assigning semantic roles to given arguments, with best F1 of 92%, and on
the task of segmenting and labeling arguments, with best F1 of 83%.
SemEval-2007 also included an SRL task based on FrameNet (Baker, Ellsworth, and
Erk 2007). It was much more complete, realistic, and difficult than its predecessor in
Senseval-3. The goal was to perform complete analysis of semantic roles on unseen
texts, first determining the appropriate frames of predicates, and then determining their
arguments labeled with semantic roles. It also involved creating a graph of the sentence
representing part of its semantics, by means of frames and labeled arguments. The
test data of this task consisted of novel manually-annotated documents, containing a
number of frames and roles not in the FrameNet lexicon. Three teams submitted results,
with precision percentages in the 60s, but recall percentages only in the 30s.
To our knowledge, there is no evidence to date on the relative difficulty of assigning
FrameNet or PropBank roles.
5.3 Impact of Syntactic Processing in SRL
Semantic roles are closely related to syntax, and, therefore, automatic SRL heavily relies
on the syntactic structure of the sentence. In PropBank, over 95% of the arguments
match with a single constituent of the parse tree. If the output produced by a statistical
parser is used (e.g., Collins?s or Charniak?s) the exact matching is still over 90%. More-
over, some simple rules can be used to join constituents and fix a considerable portion
of the mismatches (Toutanova, Haghighi, and Manning 2005). Thus, it has become a
common practice to use full parse trees as the main source for solving SRL.
The joint model presented in this issue by Toutanova, Haghighi, and Manning
obtains an F1 at ?90% on the WSJ test of the CoNLL-2005 evaluation when using gold-
standard trees; but with automatic syntactic analysis, its best result falls to ?80%. This
and other work consistently show that the drop in performance occurs in identifying
argument boundaries; when arguments are identified correctly with predicted parses,
the accuracy of assigning semantic roles is similar to that with correct parses.
A relevant question that has been addressed in experimental work concerns the
use of a partial parser instead of a parser that produces full WSJ trees. In the CoNLL-
2004 task, systems were restricted to the use of base syntactic phrases (i.e., chunks)
and clauses, and the best results that could be obtained were just below 70%. But the
training set in that evaluation was about five times smaller than that of the 2005 task.
Punyakanok, Roth, and Yih (this issue) and Surdeanu et al (2007) have shown that, in
1 The analysis summarized here was presented in the oral session at CoNLL-2005. The slides of the session,
containing the results supporting this analysis, are available in the CoNLL-2005 shared task Web site.
154
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
fact, a system working with partial parsing can do almost as well as a system working
with full parses, with differences in F1 of only ?2?3 points.
Currently, the top-performing systems on the CoNLL data make use of several
outputs of syntactic parsers, as discussed in Section 4. It is clear that many errors in
SRL are caused by having incorrect syntactic constituents, as reported by Punyakanok,
Roth, and Yih in this issue. By using many parses, the recognition of semantic roles is
more robust to parsing errors. Yet, it remains unanswered what is the most appropriate
level of syntactic analysis needed in SRL.
5.4 Generalization of SRL Systems to New Domains
Porting a system to a new domain, different than the domain used to develop and train
the system, is a challenging question in NLP. SRL is no exception, with the particular
difficulty that a predicate in a new domain may exhibit a behavior not contemplated
in the dictionary of frames at training time. This difficulty was identified as a major
challenge in the FrameNet-based task in SemEval-2007 (Baker, Ellsworth, and Erk 2007).
In the CoNLL-2005 task, WSJ-trained systems were tested on three sections of
the Brown corpus annotated by the PropBank team. The performance of all systems
dropped dramatically: The best systems scored F1 below 70%, as opposed to figures at
?80% when testing on WSJ data. This is perhaps not surprising, taking into account that
the pre-processing systems involved in the analysis (tagger and parser) also experienced
a significant drop in performance. The article in this issue by Pradhan, Ward, and
Martin further investigates the robustness across text genres when porting a system
from WSJ to Brown. Importantly, the authors claim that the loss in accuracy takes place
in assigning the semantic roles, rather than in the identification of argument boundaries.
5.5 SRL on Languages Other Than English
SemEval-2007 featured the first evaluation exercise of SRL systems for languages other
than English, namely for Spanish and Catalan (Ma`rquez et al 2007). The data was part
of the CESS-ECE corpus, consisting of ?100K tokens for each language. The semantic
role annotations are similar to PropBank, in that role labels are specific to each verb,
but also include a verb-independent thematic role label similar to the scheme proposed
in VerbNet. The task consisted of assigning semantic class labels to target verbs, and
identifying and labeling arguments of such verbs, in both cases using gold-standard
syntax. Only two teams participated, with best results at ?86% for disambiguating
predicates, and at ?83% for labeling arguments.
The work by Xue in this issue studies semantic role labeling for Chinese, using the
Chinese PropBank and NomBank corpora. Apart from working also with nominalized
predicates, this work constitutes the first comprehensive study on SRL for a language
different from English.
5.6 SRL with Other Parts-of-Speech
The SemEval-2007 task on disambiguating prepositions (Litkowski and Hargraves 2007)
used FrameNet sentences as the training and test data, with over 25,000 sentences for
the 34 most common English prepositions. Although not overtly defined as semantic
role labeling, each instance was characterized with a semantic role name and also had
an associated FrameNet frame element. Almost 80% of the prepositional phrases in the
instances were identified as core frame elements, and are likely to be closely associated
155
Computational Linguistics Volume 34, Number 2
with arguments of the words to which they are attached. The three participants used a
variety of methods, with the top performing team using machine learning techniques
similar to those in other semantic role labeling tasks.
6. Final Remarks
To date, SRL systems have been shown to perform reasonably well in some controlled
experiments, with F1 measures in the low 80s on standard test collections for English.
Still, a number of important challenges exist for future research on SRL. It remains
unclear what is the appropriate level of syntax needed to support robust analysis of
semantic roles, and to what degree improved performance in SRL is constrained by the
state-of-the-art in tagging and parsing. Beyond syntax, the relation of semantic roles to
other semantic knowledge (such as WordNet, named entities, or even a catalogue of
frames) has scarcely been addressed in the design of current SRL models. A deeper
understanding of these questions could help in developing methods that yield im-
proved generalization, and that are less dependent on large quantities of role-annotated
training data.
Indeed, the requirement of most SRL approaches for such training data, which is
both difficult and highly expensive to produce, is the major obstacle to the widespread
application of SRL across different genres and different languages. Given the degrada-
tion of performance when a supervised system is faced with unseen events or a testing
corpus different from training, this is a major impediment to increasing the application
of SRL even within English, a language for which two major annotated corpora are
available. It is critical for the future of SRL that research broadens to include wider
investigation of unsupervised and minimally supervised learning methods.
In addition to these open research problems, there are also methodological issues
that need to be addressed regarding how research is conducted and evaluated. Shared
task frameworks have been crucial in SRL development by supporting explicit compar-
isons of approaches, but such benchmark testing can also overly focus research efforts
on small improvements in particular evaluation measures. Improving the entire SRL
approach in a significant way may require more open-ended investigation and more
qualitative analysis.
Acknowledgments
We are grateful for the insightful comments
of two anonymous reviewers whose input
helped us to improve the article. This work
was supported by the Spanish Ministry of
Education and Science (Ma`rquez); the
Catalan Ministry of Innovation, Universities
and Enterprise; and a grant from NTT, Agmt.
Dtd. 6/21/1998 (Carreras); and NSERC of
Canada (Stevenson).
References
Baker, C., M. Ellsworth, and K. Erk. 2007.
SemEval-2007 Task 19: Frame semantic
structure extraction. In Proceedings of the
4th International Workshop on Semantic
Evaluations (SemEval-2007), pages 99?104,
Prague, Czech Republic.
Boas, H. C. 2002. Bilingual framenet
dictionaries for machine translation.
In Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC), pages 1364?1371,
Las Palmas de Gran Canaria, Spain.
Briscoe, T. and J. Carroll. 1997. Automatic
extraction of subcategorization from
corpora. In Proceedings of the 5th ACL
Conference on Applied Natural Language
Processing (ANLP), pages 356?363,
Washington, DC.
Carreras, X. and L. Ma`rquez. 2004.
Introduction to the CoNLL-2004 Shared
Task: Semantic role labeling. In Proceedings
of the Eighth Conference on Computational
Natural Language Learning (CoNLL-2004),
pages 89?97, Boston, MA.
Carreras, X. and L. Ma`rquez. 2005.
Introduction to the CoNLL-2005 Shared
156
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
Task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 152?164, Ann Arbor, MI.
Carreras, X., L. Ma`rquez, and G. Chrupa?a.
2004. Hierarchical recognition of
propositional arguments with perceptrons.
In Proceedings of the Eighth Conference on
Computational Natural Language Learning
(CoNLL-2004), pages 106?109, Boston, MA.
Cohn, T. and P. Blunsom. 2005. Semantic role
labelling with tree conditional random
fields. In Proceedings of the Ninth Conference
on Computational Natural Language
Learning (CoNLL-2005), pages 169?172,
Ann Arbor, MI.
Copestake, A. and D. Flickinger. 2000. An
open-source grammar development
environment and broad-coverage English
grammar using HPSG. In Proceedings
of the Second International Conference on
Language Resources and Evaluation (LREC),
pages 591?600, Athens, Greece.
Dowty, D. 1991. Thematic proto-roles and
argument selection. Language, 67:547?619.
Erk, K. 2007. A simple, similarity-based
model for selectional preferences. In
Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics,
pages 216?223, Prague, Czech Republic.
Fillmore, C. 1968. The case for case. In
E. Bach and R. T. Harms, editors, Universals
in Linguistic Theory. Holt, Rinehart &
Winston, New York, pages 1?88.
Fillmore, C. J. 1976. Frame semantics and the
nature of language. Annals of the New York
Academy of Sciences: Conference on the Origin
and Development of Language and Speech,
280:20?32.
Fillmore, C. J., J. Ruppenhofer, and C. F.
Baker. 2004. Framenet and representing
the link between semantic and syntactic
relations. In Churen Huang and Winfried
Lenders, editors, Frontiers in Linguistics,
volume I of Language and Linguistics
Monograph Series B. Institute of Linguistics,
Academia Sinica, Taipei, pages 19?59.
Gildea, D. and D. Jurafsky. 2002. Automatic
labeling of semantic roles. Computational
Linguistics, 28(3):245?288.
Grimshaw, J. 1990. Argument Structure. MIT
Press, Cambridge, MA.
Habash, N., B. J. Dorr, and D. Traum. 2003.
Hybrid natural language generation from
lexical conceptual structures. Machine
Translation, 18(2):81?128.
Hirst, G. 1987. Semantic Interpretation and
the Resolution of Ambiguity. Cambridge
University Press, Cambridge.
Jackendoff, R. 1990. Semantic Structures. MIT
Press, Cambridge, MA.
Johansson, R. and P. Nugues. 2007. LTH:
Semantic structure extraction using
nonprojective dependency trees. In
Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 227?230, Prague,
Czech Republic.
Kipper, K., H. T. Dang, and M. Palmer. 2000.
Class-based construction of a verb lexicon.
In Proceedings of the 17th National Conference
on Artificial Intelligence (AAAI-2000),
Austin, TX.
Koomen, P., V. Punyakanok, D. Roth, and
W. Yih. 2005. Generalized inference
with multiple semantic role labeling
systems. In Proceedings of the Ninth
Conference on Computational Natural
Language Learning (CoNLL-2005),
pages 181?184, Ann Arbor, MI.
Levin, B. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
The University of Chicago Press,
Chicago, IL.
Levin, B. and M. Rappaport Hovav. 1998.
Building verb meanings. In M. Butt and
W. Geuder, editors, The Projection of
Arguments: Lexical and Compositional
Factors. CSLI Publications, Stanford, CA,
pages 97?134.
Levin, B. and M. Rappaport Hovav. 2005.
Argument Realization. Cambridge
University Press, Cambridge.
Litkowski, K. C. 2004. Senseval-3 task:
Automatic labeling of semantic roles. In
Proceedings of the 3rd International Workshop
on the Evaluation of Systems for the Semantic
Analysis of Text (Senseval-3), pages 9?12,
Barcelona, Spain.
Litkowski, K. C. and O. Hargraves. 2005.
The preposition project. In Proceedings
of the ACL-SIGSEM Workshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistic
Formalisms and Applications, pages 171?179,
Colchester, UK.
Litkowski, K. C. and O. Hargraves. 2007.
SemEval-2007 Task 06: Word-sense
disambiguation of prepositions. In
Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007),
pages 24?29, Prague, Czech Republic.
Loper, E., S. Yi, and M. Palmer. 2007.
Combining lexical resources: Mapping
between PropBank and VerbNet. In
Proceedings of the 7th International Workshop
on Computational Semantics, pages 118?128,
Tilburg, The Netherlands.
157
Computational Linguistics Volume 34, Number 2
Ma`rquez, L., P. R. Comas, J. Gime?nez, and
N. Catala`. 2005. Semantic role labeling
as sequential tagging. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 193?196, Ann Arbor, MI.
Ma`rquez, L., L. Villarejo, M.A. Mart??, and
M. Taule?. 2007. SemEval-2007 Task 09:
Multilevel semantic annotation of Catalan
and Spanish. In Proceedings of the 4th
International Workshop on Semantic
Evaluations (SemEval-2007), pages 42?47,
Prague, Czech Republic.
Melli, G., Y. Wang, Y. Liu, M. M. Kashani,
Z. Shi, B. Gu, A. Sarkar, and F. Popowich.
2005. Description of SQUASH, the SFU
question answering summary handler
for the DUC-2005 Summarization Task.
In Proceedings of the HLT/EMNLP
Document Understanding Workshop (DUC),
Vancouver, Canada, available at
http://duc.nist.gov/pubs/2005papers/
simonfraseru.sarkar.pdf.
Merlo, P. and S. Stevenson. 2001. Automatic
verb classification based on statistical
distributions of argument structure.
Computational Linguistics, 27(3):373?408.
Meyers, A., R. Reeves, C. Macleod,
R. Szekely, V. Zielinska, B. Young, and
R. Grishman. 2004. The NomBank Project:
An interim report. In Proceedings of
the HLT-NAACL 2004 Workshop: Frontiers
in Corpus Annotation, pages 24?31,
Boston, MA.
Moldovan, D., A. Badulescu, M. Tatu,
D. Antohe, and R. Girju. 2004. Models for
the semantic classification of noun
phrases. In Proceedings of the HLT-NAACL
2004 Workshop on Computational Lexical
Semantics, pages 60?67, Boston, MA.
Musillo, G. and P. Merlo. 2006. Accurate
parsing of the proposition bank. In
Proceedings of the Human Language
Technology Conference of the NAACL,
pages 101?104, New York, NY.
Narayanan, S. and S. Harabagiu. 2004.
Question answering based on semantic
structures. In Proceedings of the 20th
International Conference on Computational
Linguistics (COLING), pages 693?701,
Geneva, Switzerland.
O?Hara, T. and J. Wiebe. 2003. Preposition
semantic classification via Penn Treebank
and FrameNet. In Proceedings of the
Seventh Conference on Computational
Natural Language Learning (CoNLL-2003),
pages 79?86, Edmonton, Canada.
Palmer, M., D. Gildea, and P. Kingsbury.
2005. The Proposition Bank: An annotated
corpus of semantic roles. Computational
Linguistics, 31(1):71?105.
Pradhan, S., K. Hacioglu, V. Krugler,
W. Ward, J. Martin, and D. Jurafsky. 2005a.
Support vector learning for semantic
argument classification. Machine Learning,
60(1):11?39.
Pradhan, S., K. Hacioglu, W. Ward, J. H.
Martin, and D. Jurafsky. 2005b. Semantic
role chunking combining complementary
syntactic views. In Proceedings of the
Ninth Conference on Computational
Natural Language Learning (CoNLL-2005),
pages 217?220, Ann Arbor, MI.
Pradhan, S., E. Loper, D. Dligach, and
M. Palmer. 2007. SemEval-2007 Task 17:
English lexical sample, SRL and all words.
In Proceedings of the 4th International
Workshop on Semantic Evaluations
(SemEval-2007), pages 87?92, Prague,
Czech Republic.
Pustejovsky, J. 1995. The Generative Lexicon.
MIT Press, Cambridge, MA.
Rosario, B. and M. Hearst. 2004. Classifying
semantic relations in bioscience text. In
Proceedings of the 42nd Annual Meeting of the
Association for Computational Linguistics,
pages 430?437, Barcelona, Spain.
Schulte im Walde, S. 2006. Experiments
on the automatic induction of German
semantic verb classes. Computational
Linguistics, 32(2):159?194.
Shi, L. and R. Mihalcea. 2005. Putting pieces
together: Combining FrameNet, VerbNet
and WordNet for robust semantic parsing.
In Computational Linguistics and Intelligent
Text Processing; Sixth International
Conference, CICLing 2005, Proceedings,
LNCS, vol 3406, pages 100?111, Mexico
City, Mexico.
Surdeanu, M., S. Harabagiu, J. Williams,
and P. Aarseth. 2003. Using
predicate-argument structures for
information extraction. In Proceedings of the
41st Annual Meeting of the Association for
Computational Linguistics, pages 8?15,
Sapporo, Japan.
Surdeanu, M., L. Ma`rquez, X. Carreras, and
P. R. Comas. 2007. Combination strategies
for semantic role labeling. Journal of
Artificial Intelligence Research (JAIR),
29:105?151.
Swier, R. and S. Stevenson. 2004.
Unsupervised semantic role labelling. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 95?102, Barcelona, Spain.
Swier, R. and S. Stevenson. 2005. Exploiting
a verb lexicon in automatic semantic role
158
Ma`rquez, Carreras, Litkowski, and Stevenson Semantic Role Labeling
labelling. In Proceedings of the Human
Language Technology Conference and
Conference on Empirical Methods in Natural
Language Processing (HLT/EMLNP), pages
883?890, Vancouver, B.C., Canada.
Thompson, C. A., R. Levy, and C. Manning.
2003. A generative model for semantic role
labeling. In Proceedings of the 14th European
Conference on Machine Learning (ECML),
pages 397?408, Dubrovnik, Croatia.
Toutanova, K., A. Haghighi, and C. Manning.
2005. Joint learning improves semantic role
labeling. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics, pages 589?596, Ann Arbor, MI.
Xue, N. and M. Palmer. 2004. Calibrating
features for semantic role labeling. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing
(EMNLP), pages 88?94, Barcelona, Spain.
Yi, S., E. Loper, and M. Palmer. 2007. Can
semantic roles generalize across corpora?
In Human Language Technologies 2007:
The Conference of the North American
Chapter of the Association for Computational
Linguistics; Proceedings of the Main
Conference, pages 548?555, Rochester, NY.
Zapirain, B., E. Agirre, and L. Ma`rquez. 2007.
UBC-UPC: Sequential SRL using
selectional preferences: an approach with
maximum entropy Markov models. In
Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007),
pages 354?357, Prague, Czech Republic.
159

Unsupervised Type and Token Identification
of Idiomatic Expressions
Afsaneh Fazly?
University of Toronto
Paul Cook??
University of Toronto
Suzanne Stevenson?
University of Toronto
Idiomatic expressions are plentiful in everyday language, yet they remain mysterious, as it
is not clear exactly how people learn and understand them. They are of special interest to
linguists, psycholinguists, and lexicographers, mainly because of their syntactic and semantic
idiosyncrasies as well as their unclear lexical status. Despite a great deal of research on the
properties of idioms in the linguistics literature, there is not much agreement on which properties
are characteristic of these expressions. Because of their peculiarities, idiomatic expressions have
mostly been overlooked by researchers in computational linguistics. In this article, we look
into the usefulness of some of the identified linguistic properties of idioms for their automatic
recognition. Specifically, we develop statistical measures that each model a specific property
of idiomatic expressions by looking at their actual usage patterns in text. We use these sta-
tistical measures in a type-based classification task where we automatically separate idiomatic
expressions (expressions with a possible idiomatic interpretation) from similar-on-the-surface
literal phrases (for which no idiomatic interpretation is possible). In addition, we use some of
the measures in a token identification task where we distinguish idiomatic and literal usages of
potentially idiomatic expressions in context.
1. Introduction
Idioms form a heterogeneous class, with prototypical examples such as by and large, kick
the bucket, and let the cat out of the bag. It is hard to find a single agreed-upon definition
that covers all members of this class (Glucksberg 1993; Cacciari 1993; Nunberg, Sag,
and Wasow 1994), but they are often defined as sequences of words involving some de-
gree of semantic idiosyncrasy or non-compositionality. That is, an idiom has a different
? Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,
Canada. E-mail: afsaneh@cs.toronto.edu.
?? Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,
Canada. E-mail: pcook@cs.toronto.edu.
? Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,
Canada. E-mail: suzanne@cs.toronto.edu.
Submission received: 12 September 2007; revised submission received: 29 February 2008; accepted for
publication: 6 May 2008.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 1
meaning from the simple composition of the meaning of its component words. Idioms
are widely and creatively used by speakers of a language to express ideas cleverly, eco-
nomically, or implicitly, and thus appear in all languages and in all text genres (Sag et al
2002). Many expressions acquire an idiomatic meaning over time (Cacciari 1993); conse-
quently, new idioms come into existence on a daily basis (Cowie, Mackin, and McCaig
1983; Seaton and Macaulay 2002). Automatic tools are therefore necessary for assisting
lexicographers in keeping lexical resources up to date, as well as for creating and ex-
tending computational lexicons for use in natural language processing (NLP) systems.
Though completely frozen idioms, such as by and large, can be represented as
words with spaces (Sag et al 2002), most idioms are syntactically well-formed phrases
that allow some variability in expression, such as shoot the breeze and hold fire (Gibbs
and Nayak 1989; d?Arcais 1993; Fellbaum 2007). Such idioms allow a varying degree
of morphosyntactic flexibility?for example, held fire and hold one?s fire allow for an
idiomatic reading, whereas typically only a literal interpretation is available for fire was
held and held fires. Clearly, a words-with-spaces approach does not work for phrasal
idioms. Hence, in addition to requiring NLP tools for recognizing idiomatic expressions
(types) to include in a lexicon, methods for determining the allowable and preferred
usages (a.k.a. canonical forms) of such expressions are also needed. Moreover, in many
situations, an NLP system will need to distinguish a usage (token) of a potentially
idiomatic expression as either idiomatic or literal in order to handle a given sequence of
words appropriately. For example, a machine translation system must translate held fire
differently in The army held their fire and The worshippers held the fire up to the idol.
Previous studies focusing on the automatic identification of idiom types have often
recognized the importance of drawing on their linguistic properties, such as their se-
mantic idiosyncrasy or their restricted flexibility, pointed out earlier. Some researchers
have relied on a manual encoding of idiom-specific knowledge in a lexicon (Copestake
et al 2002; Odijk 2004; Villavicencio et al 2004), whereas others have presented ap-
proaches for the automatic acquisition of more general (hence less distinctive) knowl-
edge from corpora (Smadja 1993; McCarthy, Keller, and Carroll 2003). Recent work
that looks into the acquisition of the distinctive properties of idioms has been limited,
both in scope and in the evaluation of the methods proposed (Lin 1999; Evert, Heid,
and Spranger 2004). Our goal is to develop unsupervised means for the automatic
acquisition of lexical, syntactic, and semantic knowledge about a broadly documented
class of idiomatic expressions.
Specifically, we focus on a cross-linguistically prominent class of phrasal idioms
which are commonly and productively formed from the combination of a frequent verb
and a noun in its direct object position (Cowie, Mackin, and McCaig 1983; Nunberg,
Sag, and Wasow 1994; Fellbaum 2002), for example, shoot the breeze, make a face, and
push one?s luck. We refer to these as verb+noun idiomatic combinations or VNICs.1
We present a comprehensive analysis of the distinctive linguistic properties of phrasal
idioms, including VNICs (Section 2), and propose statistical measures that capture each
property (Section 3). We provide a multi-faceted evaluation of the measures (Section 4),
showing their effectiveness in the recognition of idiomatic expressions (types)?that is,
separating them from similar-on-the-surface literal phrases?as well as their superiority
to existing state-of-the-art techniques. Drawing on these statistical measures, we also
propose an unsupervised method for the automatic acquisition of an idiom?s canonical
1 We use the abbreviation VNIC and the term expression to refer to a verb+noun type with a potential
idiomatic meaning. We use the terms instance and usage to refer to a token occurrence of an expression.
62
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
forms (e.g., shoot the breeze as opposed to shoot a breeze), and show that it can successfully
accomplish the task (Section 5).
It is possible for a single VNIC to have both idiomatic and non-idiomatic (literal)
meanings. For example, make a face is ambiguous between an idiom, as in The little girl
made a funny face at her mother, and a literal combination, as in She made a face on the
snowman using a carrot and two buttons. Despite the common perception that phrases
that can be idioms are mainly used in their idiomatic sense, our analysis of 60 idioms
has shown otherwise. We found that close to half of these also have a clear literal
meaning; and of those with a literal meaning, on average around 40% of their usages
are literal. Distinguishing token phrases as idiomatic or literal combinations of words is
thus essential for NLP tasks, such as semantic parsing and machine translation, which
require the identification of multiword semantic units.
Most recent studies focusing on the identification of idiomatic and non-idiomatic
tokens either assume the existence of manually annotated data for a supervised clas-
sification (Patrick and Fletcher 2005; Katz and Giesbrecht 2006), or rely on manually
encoded linguistic knowledge about idioms (Uchiyama, Baldwin, and Ishizaki 2005;
Hashimoto, Sato, and Utsuro 2006), or even ignore the specific properties of non-
literal language and rely mainly on general purpose methods for the task (Birke and
Sarkar 2006). We propose unsupervised methods that rely on automatically acquired
knowledge about idiom types to identify their token occurrences as idiomatic or literal
(Section 6). More specifically, we explore the hypothesis that the type-based knowledge
we automatically acquire about an idiomatic expression can be used to determine
whether an instance of the expression is used literally or idiomatically (token-based
knowledge). Our experimental results show that the performance of the token-based
idiom identificationmethods proposed here is comparable to that of existing supervised
techniques (Section 7).
2. Idiomaticity, Semantic Analyzability, and Flexibility
Although syntactically well-formed, phrasal idioms (including VNICs) involve a certain
degree of semantic idiosyncrasy. This means that phrasal idioms are to some extent
nontransparent; that is, even knowing the meaning of the individual component words,
the meaning of the idiom is hard to determine without special context or previous ex-
posure. There is much evidence in the linguistics literature that idiomatic combinations
also have idiosyncratic lexical and syntactic behavior. Here, we first define semantic
analyzability and elaborate on its relation to semantic idiosyncrasy or idiomaticity. We
then expound on the lexical and syntactic behavior of VNICs, pointing out a suggestive
relation between the degree of idiomaticity of a VNIC and the degree of its lexicosyn-
tactic flexibility.
2.1 Semantic Analyzability
Idioms have been traditionally believed to be completely non-compositional (Fraser
1970; Katz 1973). This means that unlike compositional combinations, the meaning
of an idiom cannot be solely predicted from the meaning of its parts. Nonetheless,
many linguists and psycholinguists argue against such a view, providing evidence
from idioms that show some degree of semantic compositionality (Nunberg, Sag, and
Wasow 1994; Gibbs 1995). The alternative view suggests that many idioms in fact do
63
Computational Linguistics Volume 35, Number 1
have internal semantic structure, while recognizing that they are not compositional in a
simplistic or traditional sense. To explain the semantic behavior of idioms, researchers
who take this alternative view thus use new terms such as semantic decomposability
and/or semantic analyzability in place of compositionality.
To say that an idiom is semantically analyzable to some extent means that the
constituents contribute some sort of independent meaning?not necessarily their literal
semantics?to the overall idiomatic interpretation. Generally, the more semantically
analyzable an idiom is, the easier it is to map the idiom constituents onto their cor-
responding idiomatic referents. In other words, the more semantically analyzable an
idiom is, the easier it is to make predictions about the idiomatic meaning from the
meaning of the idiom parts. Semantic analyzability is thus inversely related to semantic
idiosyncrasy.
Many linguists and psycholinguists conclude that idioms clearly form a heteroge-
neous class, not all of them being truly non-compositional or unanalyzable (Abeille?
1995; Moon 1998; Grant 2005). Rather, semantic analyzability in idioms is a matter of
degree. For example, the meaning of shoot the breeze (?to chat idly?), a highly idiomatic
expression, has nothing to do with either shoot or breeze. A less idiomatic expression,
such as spill the beans (?to reveal a secret?), may be analyzed as spill metaphorically
corresponding to ?reveal? and beans referring to ?secret(s).? An idiom such as pop the
question is even less idiomatic because the relations between the idiom parts and their
idiomatic referents are more directly established, namely, pop corresponds to ?suddenly
ask? and question refers to ?marriage proposal.? As we will explain in the following
section, there is evidence that the difference in the degree of semantic analyzability of
idiomatic expressions is also reflected in their lexical and syntactic behavior.
2.2 Lexical and Syntactic Flexibility
Most idioms are known to be lexically fixed, meaning that the substitution of a near syn-
onym (or a closely related word) for a constituent part does not preserve the idiomatic
meaning of the expression. For example, neither shoot the wind nor hit the breeze are valid
variations of the idiom shoot the breeze. Similarly, spill the beans has an idiomatic meaning,
while spill the peas and spread the beans have only literal interpretations. There are, how-
ever, idiomatic expressions that have one (or more) lexical variants. For example, blow
one?s own trumpet and toot one?s own horn have the same idiomatic interpretation (Cowie,
Mackin, and McCaig 1983); also keep one?s cool and lose one?s cool have closely related
meanings (Nunberg, Sag, and Wasow 1994). Nonetheless, it is not the norm for idioms
to have lexical variants; when they do, there are usually unpredictable restrictions on
the substitutions they allow.
Idiomatic combinations are also syntactically distinct from compositional combi-
nations. Many VNICs cannot undergo syntactic variations and at the same time retain
their idiomatic interpretations. It is important, however, to note that VNICs differ with
respect to the extent to which they can tolerate syntactic operations, that is, the degree
of syntactic flexibility they exhibit. Some are syntactically inflexible for the most part,
whereas others are more versatile, as illustrated in the sentences in Examples (1) and (2):
1. (a) Sam and Azin shot the breeze.
(b) ?? Sam and Azin shot a breeze.
(c) ?? Sam and Azin shot the breezes.
(d) ?? Sam and Azin shot the casual breeze.
64
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
(e) ?? The breeze was shot by Sam and Azin.
(f) ?? The breeze that Sam and Azin shot was quite refreshing.
(g) ?? Which breeze did Sam and Azin shoot?
2. (a) Azin spilled the beans.
(b) ? Azin spilled some beans.
(c) ?? Azin spilled the bean.
(d) Azin spilled the Enron beans.
(e) The beans were spilled by Azin.
(f) The beans that Azin spilled caused Sam a lot of trouble.
(g) Which beans did Azin spill?
Linguists have often explained the lexical and syntactic flexibility of idiomatic
combinations in terms of their semantic analyzability (Fellbaum 1993; Gibbs 1993;
Glucksberg 1993; Nunberg, Sag, and Wasow 1994; Schenk 1995). The common belief
is that because the constituents of a semantically analyzable idiom can be mapped onto
their corresponding referents in the idiomatic interpretation, analyzable (less idiomatic)
expressions are often more open to lexical substitution and syntactic variation. Psy-
cholinguistic studies also support this hypothesis: Gibbs and Nayak (1989) and Gibbs
et al (1989), through a series of psychological experiments, demonstrate that there is
variation in the degree of lexicosyntactic flexibility of idiomatic combinations. (Both
studies narrow their focus to verb phrase idiomatic combinations, mainly of the form
verb+noun.) Moreover, their findings provide evidence that the lexical and syntactic
flexibility of VNICs is not arbitrary, but rather correlates with the semantic analyzability
of these idioms as perceived by the speakers participating in the experiments.
Corpus-based studies such as those by Moon (1998), Riehemann (2001), and Grant
(2005) conclude that idioms are not as fixed as most have assumed. These claims are
often based on observing certain idiomatic combinations in a form other than their so-
called canonical forms. For example, Moon mentions that she has observed both kick
the pail and kick the can as variations of kick the bucket. Also, Grant finds evidence of
variations such as eat one?s heart (out) and eat one?s hearts (out) in the BNC. Riehemann
concludes that in contrast to non-idiomatic combinations of words, ?idioms have a
strongly preferred canonical form, but at the same time the occurrence of lexical and
syntactic variations of idioms is too common to be ignored? (page 67). Our understand-
ing of such findings is that idiomatic combinations are not inherently frozen and that it
is possible for them to appear in forms other than their agreed-upon canonical forms.
However, it is important to note that most such observed variations are constrained,
often with unpredictable restrictions.
We are well aware that semantic analyzability is neither a necessary nor a sufficient
condition for an idiomatic combination to be lexically or syntactically flexible. Other
factors, such as communicative intentions and pragmatic constraints, can motivate a
speaker to use a variant in place of a canonical form (Glucksberg 1993). For exam-
ple, journalism is well known for manipulating idiomatic expressions for humor or
cleverness (Grant 2005). The age and the degree of familiarity of an idiom have also
been shown to be important factors that affect its flexibility (Gibbs and Nayak 1989).
Nonetheless, linguists often use observations about lexical and syntactic flexibility of
VNICs in order to make judgments about their degree of idiomaticity (Kyto? 1999;
Tanabe 1999). We thus conclude that lexicosyntactic behavior of a VNIC, although
affected by historical and pragmatic factors, can be at least partially explained in terms
of semantic analyzability or idiomaticity.
65
Computational Linguistics Volume 35, Number 1
3. Automatic Acquisition of Type-Based Knowledge about VNICs
We use the observed connection between idiomaticity and (in)flexibility to devise sta-
tistical measures for automatically distinguishing idiomatic verb+noun combinations
(types) from literal phrases. More specifically, we aim to identify verb?noun pairs such
as ?keep, word? as having an associated idiomatic expression (keep one?s word), and
also distinguish these from verb?noun pairs such as ?keep, fish? which do not have
an idiomatic interpretation. Although VNICs vary in their degree of flexibility (cf.
Examples (1) and (2)), on the whole they contrast with fully compositional phrases,
which are more lexically productive and appear in a wider range of syntactic forms. We
thus propose to use the degree of lexical and syntactic flexibility of a given verb+noun
combination to determine the level of idiomaticity of the expression.
Note that our assumption here is in line with corpus-linguistic studies on idioms:
we do not claim that it is inherently impossible for VNICs to undergo lexical sub-
stitution or syntactic variation. In fact, for each given idiomatic combination, it may
well be possible to find a specific situation in which a lexical or a syntactic variant of
the canonical form is perfectly plausible. However, the main point of the assumption
here is that VNICs are more likely to appear in fixed forms (known as their canonical
forms), more so than non-idiomatic phrases. Therefore, the overall distribution of a
VNIC in different lexical and syntactic forms is expected to be notably different from
the corresponding distribution of a typical verb+noun combination.
The following subsections describe our proposed statistical measures for idiomatic-
ity, which quantify the degree of lexical, syntactic, and overall fixedness of a given
verb+noun combination (represented as a verb?noun pair).
3.1 Measuring Lexical Fixedness
A VNIC is lexically fixed if the replacement of any of its constituents by a semantically
(and syntactically) similar word does not generally result in another VNIC, but in
an invalid or a literal expression. One way of measuring lexical fixedness of a given
verb+noun combination is thus to examine the idiomaticity of its variants, that is,
expressions generated by replacing one of the constituents by a similar word. This
approach has twomain challenges: (i) it requires prior knowledge about the idiomaticity
of expressions (which is what we are developing our measure to determine); (ii) it can
only measure the lexical fixedness of idiomatic combinations, and so could not apply to
literal combinations. We thus interpret this property statistically in the following way:
We expect a lexically fixed verb+noun combination to appear much more frequently
than its variants in general.
Specifically, we examine the strength of association between the verb and the
noun constituent of a combination (the target expression or its lexical variants) as
an indirect cue to its idiomaticity, an approach inspired by Lin (1999). We use the
automatically built thesaurus of Lin (1998) to find words similar to each constituent,
in order to automatically generate variants.2 Variants are generated by replacing either
2 We also replicated our experiments with an automatically built thesaurus created from the British
National Corpus (BNC) in a similar fashion, and kindly provided to us by Diana McCarthy. Results
were similar, hence we do not report them here.
66
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
the noun or the verb constituent of a pair with a semantically (and syntactically) similar
word.3
Examples of automatically generated variants for the pair ?spill, bean? are ?pour,
bean?, ?stream, bean?, ?spill, corn?, and ?spill, rice?.
Let Ssim(v) = {vi | 1 ? i ? Kv} be the set of the Kv most similar verbs to the verb v
of the target pair ?v,n?, and Ssim(n) =
{
nj | 1 ? j ? Kn
}
be the set of the Kn most similar
nouns to the noun n (according to Lin?s thesaurus). The set of variants for the target pair
is thus:
Ssim(v,n) = {?vi,n?| 1 ? i ? Kv} ?
{
?v,nj?| 1 ? j ? Kn
}
.
We calculate the association strength for the target pair and for each of its variants
using an information-theoretic measure called pointwise mutual information or PMI
(Church et al 1991):
PMI(vr, nt) = log
P(vr, nt)
P(vr)P(nt)
= log
Nv+n f (vr, nt)
f (vr, ?) f (?, nt)
(1)
where ?vr, nt? ? {?v, n?} ? Ssim(v,n); Nv+n is the total number of verb?object pairs in the
corpus; f (vr, nt) is the frequency of vr and nt co-occurring as a verb?object pair; f (vr, ?)
is the total frequency of the target (transitive) verb with any noun as its direct object;
and f (?, nt) is the total frequency of the noun nt in the direct object position of any verb
in the corpus.
In his work, Lin (1999) assumes that a target expression is non-compositional if and
only if its PMI value is significantly different from that of all the variants. Instead, we
propose a novel technique that brings together the association strengths (PMI values)
of the target and the variant expressions into a single measure reflecting the degree of
lexical fixedness for the target pair. We assume that the target pair is lexically fixed to
the extent that its PMI deviates from the average PMI of its variants. By ourmeasure, the
target pair is considered lexically fixed (i.e., is given a high fixedness score) only if the
difference between its PMI value and that of most of its variants?not necessarily all, as
in themethod of Lin (1999)?is high.4 Ourmeasure calculates this deviation, normalized
using the sample?s standard deviation:
Fixednesslex(v, n)
.
=
PMI(v, n)? PMI
s (2)
3 In an early version of this work (Fazly and Stevenson 2006), only the noun constituent was varied
because we expected replacing the verb constituent with a related verb to be more likely to yield another
VNIC, as in keep/lose one?s cool, give/get the bird, crack/break the ice (Nunberg, Sag, and Wasow 1994; Grant
2005). Later experiments on the development data showed that variants generated by replacing both
constituents, one at a time, produce better results.
4 This way, even if an idiom has a few frequently used variants (e.g., break the ice and crack the ice), it may
still be assigned a high fixedness score if most other variants are uncommon. Note also that it is possible
that some variants of a given idiom are frequently used literal expressions (e.g., make biscuit for take
biscuit). It is thus important to use a flexible formulation that relies on the collective evidence (e.g.,
average PMI) and hence is less sensitive to individual cases.
67
Computational Linguistics Volume 35, Number 1
where PMI is the mean and s the standard deviation of the following sample:
{
PMI(vr, nt) | ?vr, nt? ? {?v, n?} ? Ssim(v,n)
}
PMI can be negative, zero, or positive; thus Fixednesslex(v, n) ? [??,+?], where high
positive values indicate higher degrees of lexical fixedness.
3.2 Measuring Syntactic Fixedness
Compared to literal (non-idiomatic) verb+noun combinations, VNICs are expected to
appear in more restricted syntactic forms. To quantify the syntactic fixedness of a target
verb?noun pair, we thus need to: (i) identify relevant syntactic patterns, namely, those
that help distinguish VNICs from literal verb+noun combinations; and (ii) translate the
frequency distribution of the target pair in the identified patterns into a measure of
syntactic fixedness.
3.2.1 Identifying Relevant Patterns. Determining a unique set of syntactic patterns appro-
priate for the recognition of all idiomatic combinations is difficult indeed: Exactly which
forms an idiomatic combination can occur in is not entirely predictable (Sag et al 2002).
Nonetheless, there are hypotheses about the difference in behavior of VNICs and literal
verb+noun combinations with respect to particular syntactic variations (Nunberg, Sag,
and Wasow 1994). Linguists note that semantic analyzability of VNICs is related to
the referential status of the noun constituent (i.e., the process of idiomatization of a
verb+noun combination is believed to be accompanied by a change from concreteness
to abstractness for the noun). The referential status of the noun is in turn assumed to
be related to the participation of the combination in certain morpho-syntactic forms.
In what follows, we describe three types of syntactic variation that are assumed to be
mostly tolerated by literal combinations, but less tolerated by many VNICs.
Passivization. There is much evidence in the linguistics literature that VNICs often do
not undergo passivization. Linguists mainly attribute this to the fact that in most cases,
only referential nouns appear as the surface subject of a passive construction (Gibbs
and Nayak 1989). Due to the non-referential status of the noun constituent in most
VNICs, we expect that they do not undergo passivization as often as literal verb+noun
combinations do. Another explanation for this assumption is that passives are mainly
used to put focus on the object of a clause or sentence. For most VNICs, no such
communicative purpose can be served by topicalizing the noun constituent through
passivization (Jackendoff 1997). The passive construction is thus considered as one of
the syntactic patterns relevant to measuring syntactic flexibility.5
Determiner type. A strong correlation has been observed between the flexibility of the
determiner preceding the noun in a verb+noun combination and the overall flexibility
of the phrase (Fellbaum 1993; Kearns 2002; Desbiens and Simon 2003). It is however
5 Note that there are idioms that appear primarily in a passivized form, for example, the die is cast (?the
decision is made and will not change?). Our measure can in principle recognize such idioms because we
do not require that an idiom appears mainly in active form; rather, we include voice (passive or active) as
an important part of the syntactic pattern of an idiomatic combination.
68
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
important to note that the nature of the determiner is also affected by other factors,
such as the semantic properties of the noun. For this reason, determiner flexibility is
sometimes argued not to be a good predictor of the overall syntactic flexibility of an ex-
pression. Nonetheless, many researchers consider it as an important part in the process
of idiomatization of a verb+noun combination (Akimoto 1999; Kyto? 1999; Tanabe 1999).
We thus expect a VNIC to mainly appear with one type of determiner.
Pluralization. Although the verb constituent of a VNIC is morphologically flexible, the
morphological flexibility of the noun relates to its referential status (Grant 2005). Again,
one should note that the use of a singular or plural noun in a VNICmay also be affected
by the semantic properties of the noun. Recall that during the idiomatization process,
the noun constituent may become more abstract in meaning. In this process, the noun
may lose some of its nominal features, including number (Akimoto 1999). The non-
referential noun constituent of a VNIC is thus expected to mainly appear in just one of
the singular or plural forms.
Merging the three types of variation results in a pattern set, P , of 11 distinct syntac-
tic patterns that are displayed in Table 1 along with examples for each pattern. When
developing this set of patterns, we have taken into account the linguistic theories about
the syntactic constraints on idiomatic expressions; for example, our choice of patterns
is consistent with the idiom typology developed by Nicolas (1995). Note that we merge
some of the individual patterns into one; for example, we include only one passive
pattern independently of the choice of the determiner or the number of the noun. The
motivation here is to merge low frequency patterns (i.e., those that are expected to
be less common) in order to acquire more reliable evidence on the distribution of a
particular verb?noun pair over the resulting pattern set. In principle, however, the set
can be expanded to include more patterns; it can also be modified to contain different
patterns for different classes of idiomatic combinations.
3.2.2 Devising a Statistical Measure. The second step is to devise a statistical measure
that quantifies the degree of syntactic fixedness of a verb?noun pair, with respect to
Table 1
Patterns used in the syntactic fixedness measure, along with examples for each. A pattern
signature is composed of a verb v in active (vact) or passive (vpass) voice; a determiner (det) that
can be NULL, indefinite (a/an), definite (the), demonstrative (DEM), or possessive (POSS); and a
noun n that can be singular (nsg) or plural (npl).
Pattern No. Pattern Signature Example
1 vact det:NULL nsg give money
2 vact det:a/an nsg give a book
3 vact det:the nsg give the book
4 vact det:DEM nsg give this book
5 vact det:POSS nsg give my book
6 vact det:NULL npl give books
7 vact det:the npl give the books
8 vact det:DEM npl give those books
9 vact det:POSS npl give my books
10 vact det:OTHER nsg,pl give many books
11 vpass det:ANY nsg,pl a/the/this/my book/books was/were given
69
Computational Linguistics Volume 35, Number 1
the selected set of patterns, P . We propose a measure that compares the syntactic
behavior of the target pair with that of a ?typical? verb?noun pair. Syntactic behav-
ior of a typical pair is defined as the prior probability distribution over the patterns in
P . The maximum likelihood estimate for the prior probability of an individual pattern
pt ? P is calculated as
P(pt) =
?
vi?V
?
nj?N
f (vi, nj, pt)
?
vi?V
?
nj?N
?
ptk?P
f (vi, nj, ptk)
=
f (?, ?, pt)
f (?, ?, ?)
(3)
where V is the set of all instances of transitive verbs in the corpus, andN is the set of all
instances of nouns appearing as the direct object of some verb.
The syntactic behavior of the target verb?noun pair ?v,n? is defined as the posterior
probability distribution over the patterns, given the particular pair. The maximum like-
lihood estimate for the posterior probability of an individual pattern pt is calculated as
P(pt | v, n) =
f (v, n, pt)
?
ptk?P
f (v, n, ptk)
=
f (v, n, pt)
f (v, n, ?)
. (4)
The degree of syntactic fixedness of the target verb?noun pair is estimated as
the divergence of its syntactic behavior (the posterior distribution over the patterns)
from the typical syntactic behavior (the prior distribution). The divergence of the two
probability distributions is calculated using a standard information-theoretic measure,
the Kullback Leibler (KL-) divergence (Cover and Thomas 1991):
Fixednesssyn (v, n)
.
= D(P(pt | v,n) ||P(pt))
=
?
ptk?P
P(ptk | v, n) log
P(ptk | v, n)
P(ptk)
(5)
KL-divergence has proven useful in many NLP applications (Resnik 1999; Dagan,
Pereira, and Lee 1994). KL-divergence is always non-negative and is zero if and only
if the two distributions are exactly the same. Thus, Fixednesssyn(v, n) ? [0,+?], where
large values indicate higher degrees of syntactic fixedness.
3.3 A Unified Measure of Fixedness
VNICs are hypothesized to be, in most cases, both lexically and syntactically more fixed
than literal verb+noun combinations (see Section 2). We thus propose a new measure
70
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
of idiomaticity to be a measure of the overall fixedness of a given pair. We define
Fixednessoverall (v, n) as a weighted combination of Fixednesslex and Fixednesssyn:
Fixednessoverall (v, n)
.
= ? Fixednesssyn (v, n) + (1? ?) Fixednesslex (v, n) (6)
where ?weights the relative contribution of the measures in predicting idiomaticity.
Recall that Fixednesslex(v, n) ? [??,+?], and Fixednesssyn(v, n) ? [0,+?]. To
combine them in the overall fixedness measure, we rescale them, so that they fall in
the range [0, 1]. Thus, Fixednessoverall(v, n) ? [0, 1], where values closer to 1 indicate a
higher degree of overall fixedness.
4. VNIC Type Recognition: Evaluation
To evaluate our proposed fixedness measures, we analyze their appropriateness for
determining the degree of idiomaticity of a set of experimental expressions (in the form
of verb?noun pairs, extracted as described in Section 4.1). More specifically, we first use
each measure to assign scores to the experimental pairs. We then use the scores assigned
by each measure to perform two different tasks, and assess the overall goodness of the
measure by looking at its performance in both.
First, we look into the classification performance of each measure by using the
scores to separate idiomatic verb?noun pairs from literal ones in a mixed list. This is
done by setting a threshold, here the median score, where all pairs with scores higher
than the threshold are labeled as idiomatic and the rest as literal.6 For classification, we
report accuracy (Acc), as well as the relative error rate reduction (ERR) over a random
(chance) baseline, referred to as Rand. Second, we examine the retrieval performance
of our fixedness measures by using the scores to rank verb?noun pairs according to
their degree of idiomaticity. For retrieval, we present the precision?recall curves, as
well as the interpolated three-point average precision or IAP?that is, the average of
the interpolated precisions at the recall levels of 20%, 50%, and 80%. The interpolated
average precision and precision?recall curves are commonly used for the evaluation of
information retrieval systems (Manning and Schu?tze 1999), and reflect the goodness of
a measure in placing the relevant items (here, idioms) before the irrelevant ones (here,
literals).
Idioms are often assumed to exhibit collocational behavior to some extent, that is,
the components of an idiom are expected to appear together more often than expected
by chance. Hence, someNLP systems have used collocational measures to identify them
(Smadja 1993; Evert and Krenn 2001). However, as discussed in Section 2, idioms have
distinctive syntactic and semantic properties that separate them from simple colloca-
tions. For example, although collocations involve some degree of semantic idiosyncrasy
(strong tea vs. ?powerful tea), compared to idioms, they typically have a more transparent
meaning, and their syntactic behavior is more similar to that of literal expressions. We
thus expect our fixedness measures that draw on the distinctive linguistic properties
of idioms to be more appropriate than measures of collocation for the identification of
idioms. To verify this hypothesis, in both the classification and retrieval tasks, we com-
pare the performance of the fixedness measures with that of two collocation extraction
measures: an informed baseline, PMI, and a position-based fixedness measure proposed
6 We adopt the median for this particular (balanced) data set, understanding that in practice a suitable
threshold would need to be determined, e.g., based on development data.
71
Computational Linguistics Volume 35, Number 1
by Smadja (1993), which we refer to as Smadja. Next, we provide more details on PMI
and Smadja.
PMI is a widely used measure for extracting statistically significant combinations
of words or collocations. It has also been used for the recognition of idioms (Evert and
Krenn 2001), warranting its use as an informed baseline here for comparison.7 As in
Equation (1), our calculation of PMI here restricts the counts of the verb?noun pair to
the direct object relation. Smadja (1993) proposes a collocation extraction method which
measures the fixedness of a word sequence (e.g., a verb?noun pair) by examining the
relative position of the component words across their occurrences together. We replicate
Smadja?s method, where we measure fixedness of a target verb?noun pair as the spread
(variance) of the co-occurrence frequency of the verb and the noun over 10 relative
positions within a five-word window.8
Recall from Section 3.1 that our Fixednesslex measure is intended as an improve-
ment over the non-compositionalitymeasure of Lin (1999). For the sake of completeness,
we also compare the classification performance of our Fixednesslex with that of Lin?s
(1999) measure, which we refer to as Lin.9
We first elaborate on the methodological aspects of our experiments in Section 4.1,
and then present a discussion of the experimental results in Section 4.2.
4.1 Experimental Setup
4.1.1 Corpus and Data Extraction. We use the British National Corpus (BNC; Burnard
2000); to extract verb?noun pairs, along with information on the syntactic patterns they
appear in. We automatically parse the BNC using the Collins parser (Collins 1999), and
augment it with information about verb and noun lemmas, automatically generated
using WordNet (Fellbaum 1998). We further process the corpus using TGrep2 (Rohde
2004) in order to extract syntactic dependencies. For each instance of a transitive verb,
we use heuristics to extract the noun phrase (NP) in either the direct object position
(if the sentence is active), or the subject position (if the sentence is passive). We then
automatically find the head noun of the extracted NP, its number (singular or plural),
and the determiner introducing it.
4.1.2 Experimental Expressions. We select our development and test expressions from
verb?noun pairs that involve a member of a predefined list of transitive verbs, referred
to as basic verbs. Basic verbs, in their literal use, refer to states or acts that are central
to human experience. They are thus frequent, highly polysemous, and tend to combine
with other words to form idiomatic combinations (Cacciari 1993; Claridge 2000; Gentner
and France 2004). An initial list of such verbs was selected from several linguistic and
psycholinguistic studies on basic vocabulary (Ogden 1968; Clark 1978; Nunberg, Sag,
andWasow 1994; Goldberg 1995; Pauwels 2000; Claridge 2000; Newman and Rice 2004).
We further augmented this initial list with verbs that are semantically related to another
7 PMI has been shown to perform better than or comparable to many other association measures (Inkpen
2003; Mohammad and Hirst, submitted). In our experiments, we also found that PMI consistently
performs better than two other association measures, the Dice coefficient and the log-likelihood measure.
Experiments by Krenn and Evert (2001) showed contradicting results for PMI; however, these
experiments were performed on small-sized corpora, and on data which contained items with very low
frequency.
8 We implement the method as explained in Smadja (1993), taking into account the part-of-speech tags of
the target component words.
9 We implement the method as explained in Lin (1999), using 95% confidence intervals. We thus need to
ignore variants with frequency lower than 4 for which no confidence interval can be formed.
72
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
verb already in the list; for example, lose is added in analogy with find. Here is the final
list of the 28 verbs in alphabetical order:
blow, bring, catch, cut, find, get, give, have, hear, hit, hold, keep, kick, lay, lose, make, move,
place, pull, push, put, see, set, shoot, smell, take, throw, touch
From the corpus, we extract all the verb?noun pairs (lemmas) that contain any
of these listed basic verbs, and that appear at least 10 times in the corpus in a direct
object relation (irrespective of any intervening determiners or adjectives). From these,
we select a subset that are idiomatic, and another subset that are literal, as follows: A
verb?noun pair is considered idiomatic if it appears in an idiom listed in a credible
dictionary such as the Oxford Dictionary of Current Idiomatic English (ODCIE; Cowie,
Mackin, and McCaig 1983), or the Collins COBUILD Idioms Dictionary (CCID; Seaton
and Macaulay 2002).10 To decide whether a verb?noun pair has appeared in an idiom,
we look for all idioms containing the verb and the noun in a direct-object relation,
irrespective of any intervening determiners or adjectives, and/or any other arguments.
The pair is considered literal if it involves a physical act or state (i.e., the basic semantics
of the verb) and does not appear in any of the mentioned dictionaries as an idiom (or
part of an idiom). From the set of idiomatic pairs, we then randomly pull out 80 de-
velopment pairs and 100 test pairs, ensuring that we have items of both low and high
frequency. We then double the size of each data set (development and test) by adding
equal numbers of literal pairs, with similar frequency distributions. Some of the idioms
corresponding to the experimental idiomatic pairs are: kick the habit,move mountains, lose
face, and keep one?s word. Examples of literal pairs include: move carriage, lose ticket, and
keep fish.
Development expressions are used in devising the fixedness measures, as well as
in determining the values of their parameters as explained in the next subsection. Test
expressions are saved as unseen data for the final evaluation.
4.1.3 Parameter Settings. Our lexical fixedness measure in Equation (2) involves two
parameters, Kv and Kn, which determine the number of lexical variants considered in
measuring the lexical fixedness of a given verb?noun pair. We make the least-biased
assumption on the proportion of variants generated by replacing the verb (Kv) and
those generated by replacing the noun (Kn)?that is, we assume Kv = Kn.
11 We perform
experiments on the development data, where we set the total number of variants (i.e.,
Kv + Kn) from 10 to 100 by steps of 10. (For simplicity, we refer to the total number
of variants as K). Figure 1(a) shows the change in performance of Fixednesslex as a
function of K. Recall that Acc is the classification accuracy, and IAP reflects the average
precision of a measure in ranking idiomatic pairs before non-idiomatic ones. According
to these results, there is not much variation in the performance of the measure for
10 Our development data also contains items from several other dictionaries, such as Chambers Idioms
(Kirkpatrick and Schwarz 1982). However, our test data, which is also used in the token-based
experiments, however, only contains idioms from the two dictionaries ODCIE and CCID. Results
reported in this article are all on test pairs; development pairs are mainly used for the development of the
methods.
11 We also performed experiments on the development data in which we did not restrict the number of
variants, and hence did not enforce the condition Kv = Kn. Instead, we tried using a variety of thresholds
on the similarity scores (from the thesaurus) in order to find the set of most similar words to a given verb
or noun. We found that fixing the number of most similar words is more effective than using a similarity
threshold, perhaps because the actual scores can be very different for different words.
73
Computational Linguistics Volume 35, Number 1
Figure 1
%IAP and %Acc of Fixednesslex and Fixednessoverall over development data.
K ? 20. We thus choose an intermediate value for K that yields the highest accuracy
and a reasonably high precision; specifically, we set K to 50.
The overall fixedness measure defined in Equation (6) also uses a parameter, ?,
which determines the relative weights given to the individual fixedness measures in
the linear combination. We experiment on the development data with different values
of ? ranging from 0 to 1 by steps of .02; results are shown in Figure 1(b). As can be seen
in the figure, the accuracy of Fixednessoverall is not affected much by the change in the
value of ?. The average precision (IAP), however, shows that the combined measure
performs best when somewhat equal weights are given to the two individual measures,
and performs worst when the lexical fixedness component is completely ignored (i.e.,
? is close to 1). These results also reinforce that a complete evaluation of our fixedness
measures should include both metrics, accuracy, and average precision, as they reveal
different aspects of performance. Here, for example, Fixednesssyn (? = 1) has compa-
rable accuracy to Fixednesslex (? = 0), reflecting that the two measures generally give
higher scores to idioms. However, the ranking precision of the latter is much higher
than that of the former, showing that Fixednesslex ranks many of the idioms at the very
top of the list. In all our experiments reported here, we set ? to .6, a value for which
Fixednessoverall shows reasonably good performance according to both Acc and IAP.
4.2 Experimental Results and Analysis
In this section, we report the results of evaluating our measures on unseen test expres-
sions, with parameters set to the values determined in Section 4.1.3. (Results on devel-
opment data have similar trends to those on test data.) We analyze the classification
performance of the individual lexical and syntactic fixedness measures in Section 4.2.1,
and discuss their effectiveness for retrieval in Section 4.2.2. Section 4.2.3 then looks into
the performance of the overall fixedness measure, and Section 4.2.4 presents a summary
and discussion of the results.
4.2.1 Classification Performance. Here, we look into the performance of the individual
fixedness measures, Fixednesslex and Fixednesssyn, in classifying a mixed set of verb?
noun pairs into idiomatic and literal classes. We compare their performance against the
74
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 2
Accuracy and relative error reduction for the two fixedness measures, the two baseline
measures, and Smadja, over all test pairs (TESTall), and test pairs divided by frequency
(TESTflow and TESTfhigh ).
TESTall TESTflow
TESTfhigh
Measure %Acc (%ERR) %Acc (%ERR) %Acc (%ERR)
Rand 50 50 50
PMI 63 (26) 56 (12) 70 (40)
Smadja 54 (8) 64 (28) 62 (24)
Fixednesslex 68 (36) 70 (40) 70 (40)
Fixednesssyn 71 (42) 72 (44) 82 (64)
two baselines, Rand and PMI, as well as the two state-of-the-art methods, Smadja and
Lin. For analytical purposes, we further divide the set of all test expressions, TESTall,
into two sets corresponding to two frequency bands: TESTflow contains 50 idiomatic
and 50 literal pairs, each with total frequency (across all syntactic patterns under
consideration) between 10 and 40; TESTfhigh consists of 50 idiomatic and 50 literal pairs,
each with total frequency of 40 or greater. Classification performances of all measures
except Lin are given in Table 2. Lin does not assign scores to the test verb?noun pairs,
hence we cannot calculate its classification accuracy the same way we do for the other
methods (i.e., using median as the threshold). A separate comparison between Lin and
Fixednesslex is provided at the end of this section.
As can be seen in the first two columns of Table 2, the informed baseline, PMI, shows
a large improvement over the random baseline (26% error reduction) on TESTall. This
shows that many VNICs have turned into institutionalized (i.e., statistically significant)
co-occurrences. Hence, one can get relatively good performance by treating verb+noun
idiomatic combinations as collocations. Fixednesslex performs considerably better than
the informed baseline (36% vs. 26% error reduction on TESTall). Fixednesssyn has the best
performance (shown in boldface), with 42% error reduction over the random baseline,
and 21.6% error reduction over PMI. These results demonstrate that lexical and syntactic
fixedness are good indicators of idiomaticity, better than a simple measure of colloca-
tion such as PMI. On TESTall, Smadja performs only slightly better than the random
baseline (8% error reduction), reflecting that a position-based fixedness measure is not
sufficient for identifying idiomatic combinations. These results suggest that looking into
deep linguistic properties of VNICs is necessary for the appropriate treatment of these
expressions.12
PMI is known to perform poorly on low frequency items. To examine the effect of
frequency on the measures, we analyze their performance on the two divisions of the
12 Performing the ?2 test of statistical significance, we find that the differences between Smadja and our
lexical and syntactic fixedness measures are statistically significant at p < 0.05. However, the differences
in performance between fixedness measures and PMI are not statistically significant. Note that this does
not imply that the differences are not substantial, rather that there is not enough evidence in the observed
data to reject the null hypothesis (that two methods perform the same in general) with high confidence.
Moreover, ?2 is a non-parametric (distribution free) test and hence it has less power to reject a null
hypothesis. Later, when we take into account the actual scores assigned by the measures, we find that all
differences are statistically significant (see Sections 4.2.2?4.2.3 for more details). All significance tests are
performed using the R (2004) package.
75
Computational Linguistics Volume 35, Number 1
test data, corresponding to the two frequency bands, TESTflow and TESTfhigh . Results are
given in the four rightmost columns of Table 2, with the best performance shown in
boldface. As expected, the performance of PMI drops substantially for low frequency
items. Interestingly, although it is a PMI-based measure, Fixednesslex has comparable
performance on all data sets. The performance of Fixednesssyn improves quite a bit
when it is applied to high frequency items, while maintaining similar performance on
the low frequency items. These results show that the lexical and syntactic fixedness
measures perform reasonably well on both low and high frequency items.13 Hence they
can be used with a higher degree of confidence, especially when applied to data that is
heterogeneous with regard to frequency. This is important because, while some VNICs
are very common, others have very low frequency, as noted by Grant (2005). Smadja
shows a notable improvement in performance when data is divided by frequency. This
effect is likely due to the fact that fixedness is measured as the spread of the position-
based (raw) co-occurrence frequencies. Nonetheless, on both data sets the performance
of Smadja remains substantially worse than that of our two fixedness measures (the
differences are statistically significant in three out of the four comparisons at p < .05).
Collectively, these results show that our linguisticallymotivated fixednessmeasures
are particularly suited for identifying idiomatic combinations, especially in comparison
with more general collocation extraction techniques, such as PMI or the position-based
fixedness measure of Smadja (1993). Especially, our measures tend to perform well on
low frequency items, perhaps due to their reliance on distinctive linguistic properties of
idioms.
We now compare the classification performance of Fixednesslex to that of Lin. Unlike
Fixednesslex, Lin does not assign continuous scores to the verb?noun pairs, but rather
classifies them as idiomatic or non-idiomatic. Thus, we cannot use the same threshold
(e.g., median) for the two methods to calculate their classification accuracies in a com-
parable way. Recall also from Section 3.1 that the performance of both these methods
depends on the value of K (the number of variants). We thus measure the classification
precision of the methods at equivalent levels of recall, using the same number of
variants K at each recall level for the two measures. Varying K from 2 to 100 by steps of
4, Lin and Fixednesslex achieve an average classification precision of 81.5% and 85.8%,
respectively. Performing a t-test on the precisions of the two methods confirms that
the difference between the two is statistically significant at p < .001. In addition, our
method has the advantage of assigning a score to a target verb?noun reflecting its degree
of lexical fixedness. Such information can help a lexicographer decide whether a given
verb?noun should be placed in a lexicon.
4.2.2 Retrieval Performance. The classification results suggest that the individual fixed-
ness measures are overall better than a simple measure of collocation at separating
idiomatic pairs from literal ones. Here, we have a closer look at their performance
by examining their goodness in ranking verb?noun pairs according to their degree
of idiomaticity. Recall that the fixedness measures are devised to reflect the degree of
fixedness and hence the degree of idiomaticity of a target verb?noun pair. Thus, the
result of applying eachmeasure to a list of mixed pairs is a list that is ranked in the order
13 In fact, the results show that the performance of both fixedness measures is better when data is divided
by frequency. Although we expect better performance over high frequency items, more investigation is
needed to verify whether the improvement in performance over low frequency items is a meaningful
effect or merely an accident of the data at hand.
76
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
of idiomaticity. For a measure to be considered good at retrieval, we expect idiomatic
pairs to be very frequent near the top of the ranked list, and to become less frequent
towards the bottom. Precision?recall curves are very indicative of this trend: The ideal
measure will have a precision of 100% for all values of recall, namely, themeasure places
all idiomatic pairs at the very top of the ranked list. In reality, although the precision
drops as recall increases, we expect a good measure to keep high precision at most
levels of recall.
Figure 2 depicts the interpolated precision?recall curves for PMI and Smadja, and
for the lexical, syntactic, and overall fixedness measures, over TESTall. Note that the
minimum interpolated precision is 50% due to the equal number of idiomatic and literal
pairs in the test data. In this section, we discuss the retrieval performance of the two
individual fixedness measures; the next section analyzes the performance of the overall
fixedness measure.
The precision?recall curves of Smadja and PMI are nearly flat (with PMI consis-
tently higher than Smadja), showing that the distribution of idiomatic pairs in the lists
ranked by these two measures is only slightly better than random. A close look at the
precision?recall curve of Fixednesslex reveals that, up to the recall level of 50%, the
precision of this measure is substantially higher than that of PMI. This means that,
compared to PMI, Fixednesslex places more idiomatic pairs at the very top of the list.
At higher recall levels (50% and higher), Fixednesslex still consistently outperforms PMI.
Nonetheless, at these recall values, the twomeasures have relatively low precision (com-
pared to the other measures), suggesting that both measures also put many idiomatic
pairs near the bottom of the list. In contrast, the precision?recall curve of Fixednesssyn
shows that its performance is consistently much better than that of PMI: Even at the
recall level of 90%, its precision is close to 70% (cf. 55% precision of PMI).
A comparison of the precision?recall curves of the two individual fixedness mea-
sures reveals their complementary nature. Compared to Fixednesslex, Fixednesssyn
maintains higher precision at very high levels of recall, suggesting that the syntactic
fixedness measure places fewer idiomatic pairs at the bottom of the ranked list. In con-
trast, Fixednesslex has notably higher precision than Fixednesssyn at recall levels of up to
40%, suggesting that the former puts more idiomatic pairs at the top of the ranked list.
Statistical significance tests confirm these observations: Using the Wilcoxon Signed
Rank test (1945), we find that both Fixednesslex and Fixednesssyn produce significantly
different rankings from PMI and Smadja (p  .001). Also, the rankings of the items
Figure 2
Precision?recall curves for PMI, Smadja, and for the fixedness measures, over TESTall.
77
Computational Linguistics Volume 35, Number 1
Table 3
Classification and retrieval performance of the overall fixedness measure over TESTall.
Measure %Acc (%ERR) %IAP
PMI 63 (26) 63.5
Smadja 54 (8) 57.2
Fixednesslex 68 (36) 75.3
Fixednesssyn 71 (42) 75.9
Fixednessoverall 74 (48) 84.7
produced by the two individual fixedness measures are found to be significantly differ-
ent at p < .01.
4.2.3 Performance of the Overall Fixedness Measure. We now look at the classification
and retrieval performance of the overall fixedness measure. Table 3 presents %Acc,
%ERR, and %IAP of Fixednessoverall, repeating that of PMI, Smadja, Fixednesslex, and
Fixednesssyn, for comparison. Here again the error reductions are relative to the random
baseline of 50%. Looking at classification performance (expressed in terms of %Acc
and %ERR), we can see that Fixednessoverall notably outperforms all other measures,
including lexical and syntactic fixedness (18.8% error reduction relative to Fixednesslex,
and 10% error reduction relative to Fixednesssyn). According to the classification
results, each of the lexical and syntactic fixedness measures are good at separating
idiomatic from literal combinations, with syntactic fixedness performing better. Here
we demonstrate that combining them into a single measure of fixedness, while giving
more weight to the better measure, results in a more effective classifier.14 The overall
behavior of this measure as a function of ? is displayed in Figure 3.
As can be seen in Table 3, Fixednesslex and Fixednesssyn have comparable IAP:
75.3% and 75.9%, respectively. In comparison, Fixednessoverall has a much higher IAP
of 84.7%, reinforcing the claim that combining evidence from both lexical and syntac-
tic fixedness is beneficial. Recall from Section 4.2.2 that the two individual fixedness
measures exhibit complementary behavior, as observed in their precision?recall curves
shown in Figure 2. The precision?recall curve of the overall fixedness measure shows
that this measure in fact combines advantages of the two individual measures: At most
recall levels, Fixednessoverall has a higher precision than both individual measures. Sta-
tistical significance tests that look at the actual scores assigned by the measures confirm
that the observed differences in performance are significant. The Wilcoxon Signed Rank
test shows that the Fixednessoverall measure produces a ranking that is significantly
different from those of the individual fixedness measures, the baseline PMI, and Smadja
(at p .001).
4.2.4 Summary and Discussion. Overall, the worst performance belongs to the two collo-
cation extraction methods, PMI and Smadja, both in classifying test pairs as idiomatic or
14 Using a ?2 test, we find a statistically significant difference between the classification performance of
Fixednessoverall and that of Smadja (p < 0.01), and also a marginally significant difference between the
performance of Fixednessoverall and that of PMI (p < .1). Recall from footnote 12 (page 15) that none
of the individual measures? performances significantly differed from that of PMI. Nonetheless, no
significant differences are found between the classification performance of Fixednessoverall and that
of the individual fixedness measures.
78
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Figure 3
Classification performance of Fixednessoverall on test data as a function of ?.
literal, and in ranking the pairs according to their degree of idiomaticity. This suggests
that although some VNICs are institutionalized, many do not appear with markedly
high frequency, and hence only looking at their frequency is not sufficient for their
recognition. Moreover, a position-based fixedness measure does not seem to sufficiently
capture the syntactic fixedness of VNICs in contrast to the flexibility of literal phrases.
Fixednessoverall is the best performer of all, supporting the hypothesis that many VNICs
are both lexically and syntactically fixed, more so than literal verb+noun combinations.
In addition, these results demonstrate that incorporating such linguistic properties into
statistical measures is beneficial for the recognition of VNICs.
Althoughwe focus on experimental expressionswith frequency higher than 10, PMI
still shows great sensitivity to frequency differences, performing especially poorly on
items with frequency between 10 and 40. In contrast, none of the fixedness measures
are as sensitive to such frequency differences. Especially interesting is the consistent
performance of Fixednesslex, which is a PMI-based measure, on low and high frequency
items. These observations put further emphasis on the importance of devising new
methods for extracting multiword expressions with particular syntactic and semantic
properties, such as VNICs.
To further analyze the performance of the fixedness measures, we look at the top
and bottom 20 pairs (10%) in the lists ranked by each fixedness measure. Interestingly,
the list ranked by Fixednessoverall contains no false positives ( fp) in the top 20 items,
and no false negatives ( fn) in the bottom 20 items, once again reinforcing the usefulness
of combining evidence from the individual lexical and syntactic fixedness measures.
False positive and false negative errors found in the top and bottom 20 ranked pairs,
respectively, for the syntactic and lexical fixedness measures are given in Table 4. (Note
that fp errors are the non-idiomatic pairs ranked at the top, whereas fn errors are the
idiomatic pairs ranked at the bottom.)
We first look at the errors made by Fixednesssyn. The first fp error, throw hat, is
an interesting one: even though the pair is not an idiomatic expression on its own,
it is part of the larger idiomatic phrase throw one?s hat in the ring, and hence exhibits
syntactic fixedness. This shows that our methods can be easily extended to identify
other types of verb phrase idiomatic combinations which exhibit syntactic behavior
similar to VNICs. Looking at the frequency distribution of the occurrence of the other
two fp errors, touch finger and lose home, in the 11 patterns from Table 1, we observe that
both pairs tend to appear mainly in the patterns ?vact det:POSS nsg? (touch one?s finger,
lose one?s home) and/or ?vact det:POSS npl? (touch one?s fingers). These examples show
79
Computational Linguistics Volume 35, Number 1
Table 4
Errors found in the top and bottom 20 pairs in the lists ranked by the two individual fixedness
measures; fp stands for false positive, fn stands for false negative.
Measure: Fixednesssyn Fixednesslex
Error Type: fp fn fp fn
throw hat make pile push barrow have moment
touch finger keep secret blow bridge give way
lose home keep hand
that syntactic fixedness is not a sufficient condition for idiomaticity. In other words,
it is possible for non-idiomatic expressions to be syntactically fixed for reasons other
than semantic idiosyncrasy. In these examples, the nouns finger and home tend to be
introduced by a possessive determiner, because they often belong to someone. It is also
important to note that these two patterns have a low prior (i.e., verb?noun pairs do
not typically appear in these patterns). Hence, an expression with a strong tendency to
appear in such patterns will be given a high syntactic fixedness score.
The frequency distribution of the two fn errors for Fixednesssyn reveals that they are
given low scores mainly because their distributions are similar to the prior. Even though
make pile preferably appears in the two patterns ?vact det:a/an nsg? and ?vact det:NULL
npl,? both patterns have reasonably high prior probabilities. Moreover, because of the
low frequency of make pile (< 40), the evidence is not sufficient to distinguish it from a
typical verb?noun pair. The pair keep secret has a high frequency, but its occurrences are
scattered across all 11 patterns, closely matching the prior distribution. The latter exam-
ple shows that syntactic fixedness is not a necessary condition for idiomaticity either.15
Analyzing the errors made by Fixednesslex is more difficult as many factors may
affect scores given by this measure. Most important is the quality of the automatically
generated variants. We find that in one case, push barrow, the first 25 distributionally
similar nouns (taken from the automatically built thesaurus) are proper nouns, perhaps
because Barrow is a common last name. In general, it seems that the similar verbs and
nouns for a target verb?noun pair are not necessarily related to the same sense of the
target word. Another possible source of error is that in this measure we use PMI as an
indirect clue to idiomaticity. In the case of give way and keep hand, many of the variants
are plausible combinations with very high frequency of occurrence, for example, give
opportunity, give order, find way for the former, and hold hand, put hand, keep eye for the
latter. Whereas some of these high-frequency variants are literal (e.g., hold hand) or
idiomatic (e.g., keep eye), many have metaphorical interpretations (e.g., give opportunity,
find way). In our ongoing work, we use lexical and syntactic fixedness measures, in com-
bination with other linguistically motivated features, to distinguish such metaphori-
cal combinations from both literal and idiomatic expressions (Fazly and Stevenson,
to appear).
One way to decrease the likelihood of making any of these errors is to combine
evidence from the lexical and syntactic fixedness of idioms. As can be seen in Table 4, the
two fixedness measures make different errors, and combining them results in a measure
15 One might argue that keep secret is more semantically analyzable and hence less idiomatic than an
expression such as shoot the breeze. Nonetheless, it is still semantically more idiosyncratic than a fully
literal combination such as keep a pen, and hence should not be ranked at the very bottom of the list.
80
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
(the overall fixedness) that makes fewer errors. In the future, we intend to also look into
other properties of idioms, such as their semantic non-compositionality, as extra sources
of information.
5. Determining the Canonical Forms of VNICs
Our evaluation of the fixedness measures demonstrates their usefulness for the au-
tomatic recognition of VNICs. Recall from Section 2 that idioms appear in restricted
syntactic forms, often referred to as their canonical forms (Glucksberg 1993; Riehemann
2001; Grant 2005). For example, the idiom pull one?s weight mainly appears in this form
(when used idiomatically). The lexical representation of an idiomatic combination thus
must contain information about its canonical forms. Such information is necessary both
for automatically generating appropriate forms (e.g., in a natural language generation
system or a machine translation system), and for inclusion in dictionaries for learners
(e.g., in the context of computational lexicography).
Because VNICs are syntactically fixed, they are mostly expected to have a small
number of canonical forms. For example, shoot the breeze is listed in many idiom dictio-
naries as the canonical form for ?shoot, breeze?. Also, hold fire and hold one?s fire are listed
in CCID as canonical forms for ?hold, fire?. We expect a VNIC to occur in its canonical
form(s) with substantially higher frequency than in any other syntactic patterns. We
thus devise an unsupervised method that discovers the canonical form(s) of a given
idiomatic verb?noun pair by examining its frequency of occurrence in each syntactic
pattern under consideration. Specifically, the set of the canonical form(s) of the target
pair ?v, n? is defined as
C(v, n) = {ptk ? P | z(v, n, ptk) > Tz} (7)
Here, P is the set of patterns (see Table 1), and the condition z(v, n, ptk) > Tz determines
whether the frequency of the target pair ?v,n? in ptk is substantially higher than its
frequency in other patterns; z(v, n, ptk) is calculated using the statistic z-score as in
Equation (8), and Tz is a predefined threshold.
z(v, n, ptk) =
f (v, n, ptk)? f
s (8)
where f is the sample mean and s the sample standard deviation.
The statistic z(v, n, ptk) indicates how far and in which direction the frequency of
occurrence of the target pair ?v, n? in a particular pattern ptk deviates from the sample
mean, expressed in units of the sample standard deviation. To decide whether ptk is a
canonical pattern for the target pair, we check whether its z-score, z(v, n, ptk), is greater
than a threshold Tz. Here, we set Tz to 1, based on the distribution of z and through
examining the development data.
We evaluate our unsupervised canonical form identification method by verifying
its predicted forms against ODCIE and CCID. Specifically, for each of the 100 idiomatic
pairs in TESTall, we calculate the precision and recall of its predicted canonical forms
(those whose z-scores are above Tz), compared to the canonical forms listed in the two
dictionaries. The average precision across the 100 test pairs is 81.2%, and the average
recall is 88% (with 68 of the pairs having 100% precision and 100% recall). Moreover, we
81
Computational Linguistics Volume 35, Number 1
find that for the overwhelming majority of the pairs, 86%, the predicted canonical form
with the highest z-score appears in the dictionary entry of the pair.
According to the entries in ODCIE and CCID, 93 out of the 100 idiomatic pairs in
TESTall have one canonical form. Our canonical form extractionmethod on average finds
1.2 canonical forms for these 100 pairs (one canonical form for 79 of them, two for 18,
and three for 3 of these). Generally, our method tends to extract more canonical forms
than listed in the dictionaries. This is a desired property, because idiom dictionaries
often do not exhaustively list all canonical forms, but themost dominant ones. Examples
of such cases include: see the sights for which our method also finds see sights as a canon-
ical form, and catch one?s attention for which our method also finds catch the attention.
There are also cases where our method finds canonical forms for a given expression due
to noise resulting from the use of the expression in a non-idiomatic sense. For example,
for hold one?s horses, our method also finds hold the horse and hold the horses as canonical
forms. Similarly, for get the bird, our method also finds get a bird.
In a few cases (4 out of 100), our method finds fewer canonical forms than listed
in the dictionaries. These are catch the/one?s imagination, have a/one?s fling, make a/one?s
mark, and have a/the nerve. For the first two of these, the z-score of the missed pattern
is only slightly lower than our predefined threshold. In other cases (8 out of 100), none
of the canonical forms extracted by our method match those in a dictionary. Some of
these expressions also have a non-idiomatic sense which might be more dominant than
the idiomatic usage. For example, for give the push and give the flick, our method finds
give a push and give a flick, respectively, perhaps due to the common use of the latter
forms as light verb constructions. Formake one?s peace, our method finds a different form,
make peace, which seems a plausible canonical form; and moreover, the canonical form
listed in the dictionaries (make one?s peace) has a z-score which is only slightly lower
than our threshold. There is also one case where our method finds a canonical form
that corresponds to a different idiom using the same verb+noun: we find lose touch as
a canonical form, whereas the dictionaries list an idiom with a different canonical form
(lose one?s touch) as the idiom with lose and touch.
In general, canonical forms extracted by our method are reasonably accurate, but
may need to be further analyzed by a lexicographer to filter out incorrectly found
patterns. Moreover, our method extracts new canonical forms for some expressions,
which could be used to augment dictionaries.
6. Automatic Identification of VNIC Tokens
In previous sections, we have provided an analysis of the lexical and syntactic behavior
of idiomatic expressions. We have shown that our proposed techniques that draw on
such properties can successfully distinguish an idiomatic verb+noun combination (a
VNIC type) such as get the sack from a non-idiomatic (literal) one such as get the bag. It is
important, however, to note that a potentially idiomatic expression such as get the sack
can also have a literal interpretation in a given context, as in Joe got the sack from the top
shelf . This is true of many potential idioms, although the relative proportion of literal
usages may differ from one expression to another. For example, an expression such as
see stars is much more likely to have a literal interpretation than get the sack (according to
our findings in the BNC). Identification of idiomatic tokens in context is thus necessary
for a full understanding of text, and this will be the focus of Sections 6 and 7.
Recent studies addressing token identification for idiomatic expressions mainly
perform the task as one of word sense disambiguation, and draw on the local context of
82
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
a token to disambiguate it. Such techniques either do not use any information regarding
the linguistic properties of idioms (Birke and Sarkar 2006), or mainly focus on the
property of non-compositionality (Katz and Giesbrecht 2006). Studies that do make
use of deep linguistic information often handcode the knowledge into the systems
(Uchiyama, Baldwin, and Ishizaki 2005; Hashimoto, Sato, and Utsuro 2006). Our goal is
to develop techniques that draw on the specific linguistic properties of idioms for their
identification, without the need for handcoded knowledge or manually labelled train-
ing data. Such unsupervised techniques can also help provide automatically labelled
(noisy) training data to bootstrap (semi-)supervised methods.
In Sections 3 and 4, we showed that the lexical and syntactic fixedness of idioms
is especially relevant to their type-based recognition. We expect such properties to also
be relevant for their token identification. Moreover, we have shown that it is possible to
learn about the fixedness of idioms in an unsupervised manner. Here, we propose unsu-
pervised techniques that draw on the syntactic fixedness of idioms to classify individual
tokens of a potentially idiomatic phrase as literal or idiomatic. We also put forward a
classification technique that combines such information (in the form of noisy training
data) with evidence from the local context of usages of an expression. In Section 6.1,
we elaborate on the underlying assumptions of our token identification techniques.
Section 6.2 then describes our proposed methods that draw on these assumptions to
perform the task.
6.1 Underlying Assumptions
Although there may be fine-grained differences in meaning across the idiomatic us-
ages of an expression, as well as across its literal usages, we assume that the idiomatic
and literal usages correspond to two coarse-grained senses of the expression. We will
refer then to each of the literal and idiomatic designations as a (coarse-grained) mean-
ing of the expression, while acknowledging that each may have multiple fine-grained
senses.
Recall from Section 2 that idioms tend to be somewhat fixed with respect to the
syntactic configurations in which they occur. For example, pull one?s weight tends to
mainly appear in this form when used idiomatically. Other forms of the expression,
such as pull the weights, typically are only used with a literal meaning. In other words,
an idiom tends to have one (or a small number of) canonical form(s), which are its most
preferred syntactic patterns.16 Here we assume that, in most cases, idiomatic usages of
an expression tend to occur in its canonical form(s). We also assume that, in contrast,
the literal usages of an expression are less syntactically restricted, and are expressed
in a greater variety of patterns. Because of their relative unrestrictedness, literal usages
may occur in a canonical form for that expression, but usages in a canonical form are
more likely to be idiomatic. Usages in alternative syntactic patterns for the expression,
which we refer to as the non-canonical forms of the expression, are more likely to be
literal.
Drawing on these assumptions, we develop unsupervised methods that deter-
mine, for each verb+noun token in context, whether it has an idiomatic or a literal
16 As noted previously, 93 out of the 100 idiomatic pairs in TESTall have one canonical form, according to the
entries in ODCIE and CCID. Also, our canonical form extraction method on average finds 1.2 canonical
forms for the 100 test idioms.
83
Computational Linguistics Volume 35, Number 1
interpretation. Clearly, the success of our methods depends on the extent to which these
assumptions hold (we will return to these assumptions in Section 7.2.3).
6.2 Proposed Methods
This section elaborates on our proposed methods for identifying the idiomatic and
literal usages of a verb+noun combination: the CFORM method that uses knowledge
of canonical forms only, and the CONTEXT method that also incorporates distributional
evidence about the local context of a token. Both methods draw on our assumptions
described herein, that usages in the canonical form(s) for a potential idiom are more
likely to be idiomatic, and those in other forms are more likely to be literal. Because
our methods need information about canonical forms of an expression, we use the
unsupervisedmethod described in Section 5 to find these automatically. In the following
discussion, we describe each method in more detail.
CFORM. This method classifies an instance (token) of an expression as idiomatic if it
occurs in one of the automatically determined canonical form(s) for that expression
(e.g., pull one?s weight), and as literal otherwise (e.g., pull a weight, pull the weights). The
underlying assumption of this method is that information about the canonical form(s) of
an idiom type can provide a reasonably accurate classification of its individual instances
as literal or idiomatic.
CONTEXT. Recall our assumption that the idiomatic and literal usages of an idiom corre-
spond to two coarse-grained meanings of the expression. It is natural to further assume
that the literal and idiomatic usages have more in common semantically within each
group than between the two groups. Adopting a distributional approach to meaning?
where the meaning of an expression is approximated by the words with which it co-
occurs (Firth 1957)?we would expect the literal and idiomatic usages of an expression
to typically occur with different sets of words.
Indeed, in a supervised setting, Katz and Giesbrecht (2006) show that the local
context of an idiom usage is useful in identifying its sense. Inspired by this work, we
propose an unsupervisedmethod that incorporates distributional information about the
local context of the usages of an idiom, in addition to the (syntactic) knowledge about
its canonical forms, in order to determine if its token usages are literal or idiomatic.
To achieve this, the method compares the context surrounding a test instance of an
expression to ?gold-standard? contexts for the idiomatic and literal usages of the expres-
sion, which are taken from noisy training data automatically labelled using canonical
forms.17
For each test instance of an expression, the CONTEXT method thus compares its
co-occurring words to two sets of gold-standard co-occurring words: one typical of
idiomatic usages and one typical of literal usages of the expression (we will shortly
explain precisely how we find these). If the test token is determined to be (on aver-
age) more similar to the idiomatic usages, then it is labelled as idiomatic. Other-
wise, it is labelled as literal. To measure similarity between two sets of words, we use
17 The two CONTEXT methods in our earlier work (Cook, Fazly, and Stevenson 2007) were biased because
they used information about the canonical form of a test token (in addition to context information).
We found that when the bias was removed, the similarity measure used in those techniques was not
as effective, and hence we have developed a different method here.
84
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
a standard distributional similarity measure, Jaccard, defined subsequently.18 In the
following equation A and B represent the two sets of words to be compared:
Jaccard(A,B) = A ? B
A ? B (9)
Nowwe explain how the CONTEXT method finds typically co-occurring words for each
of the idiomatic and literal meanings of an expression. Note that unlike in a supervised
setting, here we do not assume access to manually annotated training data. We thus use
knowledge of automatically acquired canonical forms to find these.
The CONTEXT method labels usages of an expression in a leave-one-out strategy,
where each test token is labelled by using the other tokens as noisy training (gold-
standard) data. Specifically, to provide gold-standard data for each instance of an
expression, we first divide the other instances (of the same expression) into likely-
idiomatic and likely-literal groups, where the former group contains usages in canonical
form(s) and the latter contains usages in non-canonical form(s). We then pick represen-
tative usages from each group by selecting the K instances that are most similar to the
instance being labelled (the test token) according to the Jaccard similarity score.
Recall that we assume canonical form(s) are predictive of the idiomatic usages and
non-canonical form(s) are indicative of the literal usages of an expression. We thus
expect the co-occurrence sets of the selected canonical and non-canonical instances to
reflect the idiomatic and literal meanings of the expression, respectively. We take the
average similarity of the test token to the K nearest canonical instances (likely idiomatic)
and the K nearest non-canonical instances (likely literal), and label the test token accord-
ingly.19 In the event that there are less than K canonical or non-canonical form usages
of an expression, we take the average similarity over however many instances there are
of this form. If we have no instances of one of these forms, we classify each token as
idiomatic, the label we expect to be more frequent.
7. VNIC Token Identification: Evaluation
To evaluate the performance of our proposed token identification methods, we use
each in a classification task, in which the method indicates for each instance of a given
expression whether it has an idiomatic or a literal interpretation. Section 7.1 explains
the details of our experimental setup. Section 7.2 then presents the experimental results
as well as some discussion and analysis.
7.1 Experimental Setup
7.1.1 Experimental Expressions and Annotation. In our token classification experiments,
we use a subset of the 180 idiomatic expressions in the development and test data sets
used in the type-based experiments of Section 4. From the original 180 expressions, we
discard those whose frequency in the BNC is lower than 20, to increase the likelihood
that there are both literal and idiomatic usages of each expression. We also discard any
18 It is possible to incorporate extra knowledge sources, such as WordNet, for measuring similarity
between two sets of words. However, our intention is to focus on purely unsupervised, knowledge-lean
approaches.
19 We also tried using the average similarity of the test token to all instances in each group. However,
we found that focusing on the most similar instances from each group performs better.
85
Computational Linguistics Volume 35, Number 1
expression that is not from the two dictionaries ODCIE and CCID (see Section 4.1.2
for more details on the original data sets). This process results in the selection of
60 candidate verb?noun pairs.
For each of the selected pairs, 100 sentences containing its usage were randomly ex-
tracted from the automatically parsed BNC, using themethod described in Section 4.1.1.
For a pair which occurs less than 100 times in the BNC, all of its usages were extracted.
Two judges were asked to independently label each use of each candidate expression as
literal, idiomatic, or unknown. When annotating a token, the judges had access to only
the sentence in which it occurred, and not the surrounding sentences. If this context was
insufficient to determine the class of the expression, the judge assigned the unknown
label. In an effort to assure high agreement between the judges? annotations, the judges
were also provided with the dictionary definitions of the idiomatic meanings of the
expressions.
Idiomaticity is not a binary property; rather it is known to fall on a continuum
from completely semantically transparent, or literal, to entirely opaque, or idiomatic.
The human annotators were required to pick the label, literal or idiomatic, that best fit
the usage in their judgment; they were not to use the unknown label for intermediate
cases. Figurative extensions of literal meanings were classified as literal if their overall
meaning was judged to be fairly transparent, as in You turn right when we hit the road
at the end of this track (taken from the BNC). Sometimes an idiomatic usage, such as have
word in At the moment they only had the word of Nicola?s husband for what had happened
(also taken from the BNC), is somewhat directly related to its literal meaning, which
is not the case for more semantically opaque idioms such as hit the roof. This sentence
was classified as idiomatic because the idiomatic meaning is muchmore salient than the
literal meaning.
First, our primary judge, a native English speaker and an author of this paper,
annotated each use of each candidate expression. Based on this judge?s annotations, we
removed the 25 expressions with fewer than 5 instances of either of their literal or idi-
omatic meanings, leaving 28 expressions.20 (We will revisit the 25 removed expressions
in Section 7.2.4.) The remaining expressions were then split into development (DEV) and
test (TEST) sets of 14 expressions each. The data was divided such that DEV and TEST
would be approximately equal with respect to the frequency of their expressions, as
well as their proportion of idiomatic-to-literal usages (according to the primary judge?s
annotations). At this stage, DEV and TEST contained a total of 813 and 743 tokens,
respectively.
Our second judge, also a native English-speaking author of this paper, then anno-
tated DEV and TEST sentences. The observed agreement and unweighted kappa score
(Cohen 1960) on TEST were 76% and 0.62, respectively. The judges discussed tokens on
which they disagreed to achieve a consensus annotation. Final annotations were gener-
ated by removing tokens that received the unknown label as the consensus annotation,
leaving DEV and TEST with a total of 573 and 607 tokens, and an average of 41 and 43 to-
kens per expression, respectively. Table 5 shows the DEV and the TEST verb?noun pairs
used in our experiments. The table also contains information on the number of tokens
considered for each pair, as well as the percentage of its usages which are idiomatic.
20 From the original set of 60 expressions, seven were excluded because our primary annotator did not
provide any annotations for them. These include catch one?s breath, cut one?s losses, and push one?s luck (for
which our annotator did not have access to a literal interpretation); and blow one?s (own) horn, pull one?s
hair, give a lift, and get the bird (for which our annotator did not have access to an idiomatic meaning).
86
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 5
Experimental DEV and TEST verb?noun pairs, their token frequency (FRQ), and the percentage of
their usages that are idiomatic (%IDM), ordered in decreasing %IDM.
DEV TEST
verb?noun FRQ %IDM verb?noun FRQ %IDM
find foot 52 90 have word 89 90
make face 30 90 lose thread 20 90
get nod 26 89 get sack 50 86
pull weight 33 82 make mark 85 85
kick heel 38 79 cut figure 43 84
hit road 31 77 pull punch 22 82
take heart 79 73 blow top 28 82
pull plug 65 69 make scene 48 58
blow trumpet 29 66 make hay 17 53
hit roof 17 65 get wind 29 45
lose head 38 55 make hit 14 36
make pile 25 32 blow whistle 78 35
pull leg 51 22 hold fire 23 30
see star 61 8 hit wall 61 11
7.1.2 Baselines, Parameters, and Performance Measures. We compare the performance of
our proposed methods, CFORM and CONTEXT, with the baseline of always predicting
an idiomatic interpretation, the most frequent meaning in our development data. We
also compare the unsupervised methods against a supervised method, SUP, which is
similar to CONTEXT, except that it forms the idiomatic and literal co-occurrence sets
from manually annotated data (instead of automatically labelled data using canonical
forms). Like CONTEXT, SUP also classifies tokens in a leave-one-out methodology using
the K idiomatic and literal instances which are most similar to a test token. For both
CONTEXT and SUP, we set the value of K (the number of similar instances used as
gold-standard) to 5, since experiments on DEV indicated that performance did not vary
substantially using a range of values of K.
For all methods, we report the accuracy macro-averaged over all expressions in
TEST. We use the individual accuracies (accuracies for the individual expressions) to
perform t-tests for verifying whether different methods have significantly different
performance. To further analyze the performance of the methods, we also report their
recall and precision on identifying usages from each of the idiomatic and literal classes.
7.2 Experimental Results and Analysis
We first discuss the overall performance of our proposed unsupervised methods in
Section 7.2.1. Results reported in Section 7.2.1 are on TEST (results on DEV have similar
trends, unless noted otherwise). Next, we look into the performance of our methods
on expressions with different proportions of idiomatic-to-literal usages in Section 7.2.2,
which presents results on TEST and DEV combined, as explained subsequently. Sec-
tion 7.2.3 provides an analysis of the errors made because of using canonical forms, and
identifies some possible directions for future work. In Section 7.2.4, we present results
on a new data set containing expressions with highly skewed proportion of idiomatic-
to-literal usages.
87
Computational Linguistics Volume 35, Number 1
Table 6
Macro-averaged accuracy (%Acc) and relative error rate reduction (%ERR) on TEST expressions.
Method %Acc (%ERR)
Baseline 61.9
Unsupervised CONTEXT 65.8 (10.2)
CFORM 72.4 (27.6)
Supervised SUP 82.7 (54.6)
7.2.1 Overall Performance. Table 6 shows the macro-averaged accuracy on TEST of our
two unsupervised methods, as well as that of the baseline and the supervised method
for comparison. The best unsupervised performance is indicated in boldface.
As the table shows, both of our unsupervised methods as well as the supervised
method outperform the baseline, confirming that the canonical forms of an expression,
and local context, are both informative in distinguishing literal and idiomatic instances
of the expression.21 Moreover, CFORM outperforms CONTEXT (difference is marginally
significant at p < .06), which is somewhat unexpected, as CONTEXT was proposed
as an improvement over CFORM in that it combines contextual information along
with the syntactic information provided by CFORM. We return to these results later
(Section 7.2.3) to offer some reasons as to why this might be the case. However, the
results using CFORM confirm our hypothesis that canonical forms?which reflect the
overall behavior of a verb+noun type?are strongly informative about the class of a
token. Importantly, this is the case even though the canonical forms that we use are
imperfect knowledge obtained automatically through an unsupervised method.
Comparing CFORM with SUP, we observe that even though on average the latter
outperforms the former, the difference is not statistically significant (p > .1). A close
look at the performance of these methods on the individual expressions reveals that
neither consistently outperforms the other on all (or even most) expressions. Moreover,
as we will see in Section 7.2.2, SUP seems to gain most of its advantage over CFORM on
expressions with a low proportion of idiomatic usages, for which canonical forms tend
to have less predictive value (see Section 7.2.3 for details).
Recall that both CONTEXT and SUP label each token by comparing its local context
to those of its K nearest ?idiomatic? and its K nearest ?literal? usages. The difference is
that CONTEXT uses noisy (automatically) labelled data to identify these nearest usages
for each token, whereas SUP uses manually labelled data. One possible direction for fu-
ture work is thus to investigate whether providing substantially larger amounts of data
alleviates the effect of noise, as is often found to be the case by researchers in the field.
7.2.2 Performance Based on Class Distribution. Recall from Section 6 that both of our un-
supervised techniques for token identification depend on how accurately the canonical
forms of an expression can be acquired. The canonical form acquisition technique which
we use here works well if the idiomatic meaning of an expression is sufficiently frequent
compared to its literal usage. In this section, we thus examine the performance of the
21 Performing a paired t-test, we find that the difference between the baseline and CFORM is marginally
significant, p < .06, whereas the difference between baseline and CONTEXT is not statistically significant.
The difference between the baseline and SUP is significant at p < .01. The trend on DEV is somewhat
similar: baseline and CFORM are significantly different at p < .05; SUP is marginally different from
baseline at p < .06.
88
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 7
Macro-averaged accuracy (%Acc) and relative error rate reduction (%ERR) on the 28 expressions
in DT (DEV and TEST combined), divided according to the proportion of idiomatic-to-literal
usages (high and low).
DTIhigh DTIlow
Method %Acc (%ERR) %Acc (%ERR)
Baseline 81.4 35.0
Unsupervised CONTEXT 80.6 (?4.3) 44.6 (14.8)
CFORM 84.7 (17.7) 53.4 (28.3)
Supervised SUP 84.4 (16.1) 76.8 (64.3)
token identification methods for expressions with different proportions of idiomatic-to-
literal usages.
We merge DEV and TEST (referring to the new set as DT), and then divide the re-
sulting set of 28 expressions according to their proportion of idiomatic-to-literal usages
(as determined by the human annotations) as follows.22 Looking at the proportion of
idiomatic usages of our expressions in Table 5, we can see that there are gaps between
55% and 65% in DEV, and between 58% and 82% in TEST, in terms of proportion
of idiomatic usages. The value of 65% thus serves as a natural lower bound for dominant
idiomatic usage, and the value of 58% as a natural upper bound for non-dominant
idiomatic usage. We therefore split DT into two sets: DTIhigh contains 17 expressions with
65?90% of their usages being idiomatic (i.e., their idiomatic usage is dominant), whereas
DTIlow contains 11 expressions with 8?58% of their occurrences being idiomatic (i.e., their
idiomatic usage is not dominant).
Table 7 shows the average accuracy of all the methods on these two groups of
expressions, with the best performance on each group shown in boldface. We first look
at the performance of our methods on DTIhigh . On these expressions, CFORM outperforms
both the baseline (difference is not statistically significant) and CONTEXT (difference is
statistically significant at p < .05). CFORM also has a comparable performance to the su-
pervised method, reinforcing that for these expressions accurate canonical forms can be
acquired and that such knowledge can be used with high confidence for distinguishing
idiomatic and literal usages in context.
We now look into the performance on expressions in DTIlow . On these, both CFORM
and CONTEXT outperform the baseline, showing that even for expressions whose idi-
omatic meaning is not dominant, automatically acquired canonical forms can help with
their token classification. Nonetheless, both these methods perform substantially worse
than the supervised method, reinforcing that the automatically acquired canonical
forms are noisier, and hence less predictive, than they are for expressions in DTIhigh .
The poor performance of the unsupervised methods on expressions in DTIlow (com-
pared to the supervised performance) is likely to be mostly due to the less predictive
canonical forms extracted for these expressions. In general, we can conclude that when
canonical forms can be extracted with a high accuracy, the performance of the CFORM
method is comparable to that of a supervised method. One possible way of improving
the performance of unsupervised methods is thus to develop more accurate techniques
for the automatic acquisition of canonical forms.
22 We combine the two sets in order to have a sufficient number of expressions in each group after division.
89
Computational Linguistics Volume 35, Number 1
Table 8
Confusion matrix for CFORM on expression blow trumpet. idm = idiomatic class; lit = literal class;
tp = true positive; fp = false positive; fn = false negative; tn = true negative.
True Class
idm lit
Predicted idm 17 = tp 6 = fp
Class lit 2 = fn 4 = tn
Table 9
Formulas for calculating Sens and PPV (recall and precision for the idiomatic class), and Spec
and NPV (recall and precision for the literal class) from a confusion matrix.
recall (R) precision (P)
idm Sens =
tp
tp+ fn
PPV =
tp
tp+ fp
lit Spec = tn
tn+ fp
NPV = tn
tn+ fn
Accuracy is often not a sufficient measure for the evaluation of a binary (two-class)
classifier, especially when the number of items in the two classes (here, idiomatic and
literal) differ. Instead, one can have a closer look at the performance of a classifier by
examining its confusion matrix, which compares the labels predicted by the classifier
for each item with its true label. As an example, the confusion matrix of the CFORM
method for the expression blow trumpet is given in Table 8.
Note that the choice of idiomatic as the positive class (and literal as the negative
class) is arbitrary; however, because our ultimate goal is to identify idiomatic usages,
there is a natural reason for this choice. To summarize a confusion matrix, four standard
measures are often used, which are calculated from the cells in the matrix. The measures
are sensitivity (Sens), positive predictive value (PPV), specificity (Spec), and negative
predictive value (NPV), and are calculated as in Table 9. As stated in the table, Sens
and PPV are equivalents of recall and precision for the positive (idiomatic) class, also
referred to as Ridm and Pidm later in the article. Similarly, Spec and NPV are equivalents
of recall and precision for the negative (literal) class, also referred to as Rlit and Plit.
23
Table 10 gives the trimmed mean values of these four performance measures over
expressions in DTIhigh and DTIlow for the baseline, the two unsupervised methods, and the
supervised method.24 (The performance measures on individual expressions are given
in Tables 12, 13, and 14 in the Appendix.) Table 10 shows that, as expected, the baseline
has very high Sens (100% recall on identifying idiomatic usages), but very low Spec (0%
23 We mainly refer to these measures using their standard names in the literature: Sens, PPV, Spec, and
NPV. Alongside the standard names, we use the more expressive names Ridm, Pidm, Rlit, and Plit, to
remind the reader about the semantics of the measures.
24 When averaging interdependent measures, such as precision and recall, one needs to make sure that
the observed trend in the averages is consistent with that in the individual values. Trimmed mean is a
standard statistic used in such cases, which is equivalent to the mean after discarding a percentage (often
between 5 and 25) of the sample data at the high and low ends. Here, we report a 14%-trimmed mean,
which involves removing two data points from each end. The analysis presented here is based on the
trimmed means, as well as the individual values of the performance measures.
90
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 10
Detailed classification performance of all methods over DTIhigh and DTIlow . Performance is given
using four measures: Sens or Ridm, PPV or Pidm, Spec or Rlit, and NPV or Plit, macro-averaged
using 14%-trimmed mean.
Data Set Method Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
Baseline 1.00 .82 0.00 0.00
DTIhigh CONTEXT .97 .84 .11 .18
CFORM .95 .92 .61 .71
SUP .99 .86 .22 .53
Data Set Method Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
Baseline 1.00 .36 0.00 0.00
DTIlow CONTEXT .89 .37 .22 .63
CFORM .86 .43 .36 .86
SUP .44 .62 .88 .80
recall on identifying literal usages). We thus expect a well-performing method to have
lower Sens than the baseline, but higher Spec and also higher PPV and NPV (i.e., higher
precision on both idiomatic and literal usages).
Looking at performance on DTIhigh , we find that all three methods have reasonably
high Sens and PPV, revealing that the methods are good at labeling idiomatic usages.
Performance on literal usages, however, differs across the three methods. CONTEXT has
very low Spec andNPV, showing that it tends to label most tokens?including the literal
ones?as idiomatic. A close look at the performance of this method on the individual
expressions also confirms this tendency: on many expressions (10 out of 17) the Spec
and NPV of CONTEXT are both zero (see Table 13 in the Appendix). As we will see in
Section 7.2.3, this tendency is partly due to the distribution of the idiomatic and literal
usages in canonical and non-canonical forms; because literal usages can also appear in
a canonical form, for many expressions there are often not many non-canonical form
instances. (Recall that, for training, CONTEXT uses instances in canonical form as being
idiomatic and those in non-canonical form as being literal.) Thus, in many cases, it
is a priori more likely that a token is more similar to the K most similar canonical
form instances. Interestingly, CFORM is the method with the highest Spec and NPV,
even higher than those of the supervised method. Nonetheless, even CFORM is overall
much better at identifying idiomatic tokens than literal ones (see Section 7.2.3 for more
discussion on this).
We now turn to performance on DTIlow . CFORM has a high Sens, but a low PPV,
indicating that most idiomatic usages are identified correctly, but many literal usages
are also misclassified as idiomatic (hence a low Spec). CONTEXT shows the same trend
as CFORM, though overall it has poorer performance. Performance of SUP varies across
the expressions in this group: SUP is very good at identifying literal usages of these
expressions (high Spec and NPV for all expressions). Nonetheless, SUP has a low recall
in identifying idiomatic usages (low Sens) for many of these expressions.
7.2.3 Discussion and Error Analysis. In this section, we examine twomain issues. First, we
look into the plausibility of our original assumptions regarding the predictive value of
canonical forms (and non-canonical forms). Second, we investigate the appropriateness
of our automatically extracted canonical forms.
91
Computational Linguistics Volume 35, Number 1
To learn more about the predictive value of canonical forms, we examine the per-
formance of CFORM on the 28 expressions under study. More specifically, we look at
the values of Sens, PPV, Spec, and NPV on these expressions, as shown in Table 12
in the Appendix. On expressions in DTIhigh , CFORM has both high Sens and high PPV.
The formulas in Table 9 indicate that if both Sens and PPV are high, then tp fn and
tp fp. Thus, most idiomatic usages of expressions in DTIhigh appear in a canonical form,
and most usages in a canonical form are idiomatic. The values of Spec and NPV on the
same expressions are in general lower (compared to Sens and PPV), showing that tn is
not much higher than fp or fn.
On expressions in DTIlow , CFORM generally has high Sens but low-to-medium PPV.
This indicates that for these expressions, most idiomatic usages appear in a canonical
form, but not all usages in a canonical form are idiomatic. On these expressions, CFORM
has generally high NPV, but mostly low Spec. These indicate that tn fn, that is, most
usages in a non-canonical form are literal, and that tn is often lower than fp, that is, many
literal usages also appear in a canonical form. For example, almost all usages of hit wall
in a non-canonical form are literal, but most of its literal usages appear in a canonical
form.
Generally, it seems that, as we expected, literal usages are less restricted in terms
of the syntactic form they appear in; they can appear in both canonical form(s) and
in non-canonical form(s). For an expression with a low proportion of literal usages,
we can thus acquire canonical forms that are both accurate and have high predictive
value for identifying idiomatic usages in context. On the contrary, for expressions
with a relatively high proportion of literal usages, automatically acquired canonical
forms are less accurate and also have low predictive value (i.e., they are not specific
to idiomatic usages). We expected that using contextual information would help in
such cases. However, our CONTEXT method relies on noisy training data automatically
labelled using information about canonical forms. Given these findings, it is not sur-
prising that this method performs substantially worse than a corresponding supervised
method that uses similar contextual information, but manually labelled training data. It
remains to be tested in the future whether providing more noisy data will help. Another
possible future direction is to develop context methods that can better exploit noisy
labelled data.
Now we look at a few cases where our automatically extracted canonical forms are
not sufficiently accurate. For a verb+noun such as make pile (i.e., make a pile of money),
we correctly identify only some of the canonical forms. The automatically determined
canonical forms for make pile are make a pile and make piles. However, we find that idi-
omatic usages of this expression are sometimes of the formmake one?s pile. Furthermore,
we find that the frequency of this form is much higher than that of the non-canonical
forms, and not substantially lower than the frequency cut-off for selection as a canonical
form. This indicates that our heuristic for selecting patterns as canonical forms could be
fine-tuned to yield an improvement in performance.
For the expression pull plug, we identify its canonical form as pull the plug, but find a
mixture of literal and idiomatic usages in this form. However, many of the literal usages
are verb-particle constructions using out (pull the plug out), while many of the idiomatic
usages occur with a prepositional phrase headed by on (pull the plug on). This indi-
cates that incorporating information about particles and prepositions could improve
the quality of the canonical forms. Other syntactic categories, such as adjectives, may
also be informative in determining canonical forms for expressions which are typically
used idiomatically with words of a particular syntactic category, as in blow one?s own
trumpet.
92
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 11
Macro-averaged accuracy (%Acc) and relative error rate reduction (%ERR) on the 23 expressions
in SKEWED-IDM and on the 37 expressions in the combination of TEST and SKEWED-IDM (ALL).
SKEWED-IDM ALL
Method %Acc (%ERR) %Acc (%ERR)
Baseline 97.9 84.3
Unsupervised CONTEXT 94.2 (?176.2) 83.3 (?6.4)
CFORM 86.7 (?533.3) 81.3 (?19.1)
Supervised SUP 97.9 (0.0) 92.1 (49.7)
7.2.4 Performance on Expressions with Skewed Distribution. Recall from Section 7.1.1 that,
from the original set of 60 candidate expressions, we excluded those that had fewer than
5 instances of either of their literal or idiomatic meanings. It is nonetheless important to
see how well our methods perform on such expressions. In this section, we thus report
the performance of our measures on the set of 23 expressions with mostly idiomatic
usages, referred to as SKEWED-IDM. Table 11 presents the macro-averaged accuracy of
our methods on these expressions. This table also shows the accuracy on all unseen test
expressions, that is, the combination of SKEWED-IDM and TEST, referred to as ALL, for
comparison.25
On SKEWED-IDM, the supervised method performs as well as the baseline, whereas
both unsupervised methods perform worse.26 Note that for 19 out of the 23 expressions
in SKEWED-IDM, all instances are idiomatic, and the baseline accuracy is thus 100%. On
these, SUP also has 100% accuracy because no literal instances are available, and thus
SUP labels every token as idiomatic (same as the baseline). As for the unsupervised
methods, we can see that, unlike on TEST, the CONTEXT method outperforms CFORM
(the difference is statistically significant at p < .001). We saw previously that CONTEXT
tends to label usages as idiomatic. This bias might be partially responsible for the
better performance of CONTEXT on this data set. Moreover, we find that many of these
expressions tend to appear in a highly frequent canonical form, but also in less frequent
syntactic forms which we (perhaps incorrectly) consider as non-canonical forms. When
considering the performance on all unseen test expressions (ALL), neither unsupervised
method performs as well as the baseline, but the supervised method offers a substantial
improvement over the baseline.27
Our annotators pointed out that for many of the expressions in SKEWED-IDM,
either a literal interpretation was almost impossible (as for catch one?s imagination),
or extremely implausible (as for kick the habit). Hence, the annotators could predict
beforehand that the expression would be mainly used with an idiomatic meaning. A
semi-supervised approach that combines expert human knowledge with automatically
extracted corpus-drawn information can thus be beneficial for the task of identifying
25 The results obtained on the two excluded expressions which are predominantly used literally in terms
of percent accuracy using the various methods are as follows. Baseline: 4.2, Unsupervised CONTEXT: 6.5,
Unsupervised CFORM: 16.2, Supervised: 43.5. However, because there are only two such expressions,
it is difficult to draw conclusions from these results, and we do not further consider these expressions.
26 According to a paired t-test, on SKEWED-IDM, all the observed differences are statistically significant at
p < .05.
27 According to a paired t-test, on ALL, the differences between the supervised method and the three other
methods are statistically significant at p < .01; none of the other differences are statistically significant.
93
Computational Linguistics Volume 35, Number 1
idiomatic expressions in context. A human expert (e.g., a lexicographer) could first
filter out expressions for which a literal interpretation is highly unlikely. For the rest
of the expressions, a simple unsupervised method such as CFORM?that relies only on
automatically extracted information?can be used with reasonable accuracy.
8. Related Work
8.1 Type-Based Recognition of Idioms and Other Multiword Expressions
Our work relates to previous studies on determining the compositionality (the inverse
of idiomaticity) of idioms and other multiword expressions (MWEs). Most previous
work on the compositionality of MWEs either treats them as collocations (Smadja 1993),
or examines the distributional similarity between the expression and its constituents
(Baldwin et al 2003; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, and
Carroll 2003). Others have identified MWEs by looking into specific linguistic cues,
such as the lexical fixedness of non-compositional MWEs (Lin 1999; Wermter and Hahn
2005), or the lexical flexibility of productive noun compounds (Lapata and Lascarides
2003). Venkatapathy and Joshi (2005) combine aspects of this work, by incorporating
lexical fixedness, distributional similarity, and collocation-based measures into a
set of features which are used to rank verb+noun combinations according to their
compositionality. Our work differs from such studies in that it considers various kinds
of fixedness as surface behaviors that are tightly related to the underlying semantic
idiosyncrasy (idiomaticity) of expressions. Accordingly, we propose novel methods
for measuring the degree of lexical, syntactic, and overall fixedness of verb+noun
combinations, and use these as indirect ways of measuring degree of idiomaticity.
Earlier research on the lexical encoding of idiom types mainly relied on the exis-
tence of human annotations, especially for detecting which syntactic variations (e.g.,
passivization) an idiom can undergo (Odijk 2004; Villavicencio et al 2004). Evert, Heid,
and Spranger (2004) and Ritz and Heid (2006) propose methods for automatically
determining morphosyntactic preferences of idiomatic expressions. However, they treat
individual morphosyntactic markers (e.g., the number of the noun in a verb+noun
combination) as independent features, and rely mainly on the relative frequency of
each possible value for a feature (e.g., plural for number) as an indicator of a preference
for that value. If the relative frequency of a particular value of a feature for a given
combination (or the lower bound of the confidence interval, in the case of Evert, Heid,
and Spranger?s approach) is higher than a certain threshold, then the expression is
said to have a preference for that value. These studies recognize that morphosyntactic
preferences can be employed as clues to the identification of idiomatic combinations;
however, none proposes a systematic approach for such a task. Moreover, only subjec-
tive evaluations of the proposed methods are presented.
Others have also drawn on the notion of syntactic fixedness for the detection
of idioms and other MWEs. Widdows and Dorow (2005), for example, look into the
fixedness of a highly constrained type of idiom, namely, those of the form ?X conj X?
where X is a noun or an adjective, and conj is a conjunction such as and, or, but. Smadja
(1993) also notes the importance of syntactic fixedness in identifying strongly associated
multiword sequences, including collocations and idioms. Nonetheless, in both these
studies, the notion of syntactic fixedness is limited to the relative position of words
within the sequence. Such a general notion of fixedness does not take into account some
of the important syntactic properties of idioms (e.g., the choice of the determiner), and
hence cannot distinguish among different subtypes of MWEs which may differ on such
94
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
grounds. Our syntactic fixedness measure looks into a set of linguistically informed
patterns associated with a coherent, though large, class of idiomatic expressions. Results
presented in this article show that the fixedness measures can successfully separate
idioms from literal phrases. Corpus analysis of the measures proves that they can also
be used to distinguish idioms from other MWEs, such as light verb constructions and
collocations (Fazly and Stevenson 2007; Fazly and Stevenson, to appear). Bannard (2007)
proposes an extension of our syntactic fixedness measure?which first appeared in
Fazly and Stevenson (2006)?where he uses different prior distributions for different
syntactic variations.
Work on the identification of MWE types has also looked at evidence from another
language. For example, Melamed (1997a) assumes that non-compositional compounds
(NCCs) are usually not translated word-for-word to another language. He thus pro-
poses to discover NCCs by maximizing the information-theoretic predictive value of
a translation model between two languages. The sample extracted NCCs reveal an
important drawback of the proposed method: It relies on a translation model only,
without taking into account any prior linguistic knowledge about possible NCCswithin
a language. Nonetheless, such a technique is capable of identifying many NCCs that are
relevant for a translation task. Villada Moiro?n and Tiedemann (2006) propose measures
for distinguishing idiomatic expressions from literal ones (in Dutch), by examining
their automatically generated translations into a second language, such as English or
Spanish. Their approach is based on the assumptions that idiomatic expressions tend
to have fewer predictable translations and fewer compositional meanings, compared
to the literal ones. The first property is measured as the diversity in the translations
for the expression, estimated using an entropy-based measure proposed by Melamed
(1997b). The non-compositionality of an expression is measured as the overlap between
the meaning of an expression (i.e., its translations) and those of its component words.
General approaches (such as those explained in the previous paragraph) may be
more easily extended to different domains and languages. Our measures incorporate
language-specific information about idiomatic expressions, thus extra work may be
required to extend and apply them to other languages and other expressions. (Though
see Van de Cruys and Villada Moiro?n [2007] for an extension of our measures to Dutch
idioms of the form verb plus prepositional phrase.) Nonetheless, because our measures
capture deep linguistic information, they are also expected to acquire more detailed
knowledge?for example, they can be used for identifying other classes of MWEs (Fazly
and Stevenson 2007).
8.2 Token-Based Identification of Idioms and Other Multiword Expressions
A handful of studies have focused on identifying idiomatic and non-idiomatic usages
(tokens) of words or MWEs. Birke and Sarkar (2006) propose a minimally supervised
algorithm for distinguishing between literal and non-literal usages of verbs in context.
Their algorithm uses seed sets of literal and non-literal usages that are automatically
extracted from online resources such as WordNet. The similarity between the context of
a target token and that of each seed set determines the class of the token. The approach is
general in that it uses a slightly modified version of an existing word sense disambigua-
tion algorithm. This is both an advantage and a drawback: The algorithm can be easily
extended to other parts of speech and other languages; however, such a general method
ignores the specific properties of non-literal (metaphorical and/or idiomatic) language.
Similarly, the supervised token classification method of Katz and Giesbrecht (2006)
relies primarily on the local context of a token, and fails to exploit specific linguistic
95
Computational Linguistics Volume 35, Number 1
properties of non-literal language. Our results suggest that such properties are often
more informative than the local context, in determining the class of an MWE token.
The supervised classifier of Patrick and Fletcher (2005) distinguishes between com-
positional and non-compositional usages of English verb-particle constructions. Their
classifier incorporates linguistically motivated features, such as the degree of separation
between the verb and particle. Here, we focus on a different class of English MWEs,
namely, the class of idiomatic verb+noun combinations. Moreover, by making a more
direct use of their syntactic behavior, we develop unsupervised token classification
methods that perform well. The unsupervised token classifier of Hashimoto, Sato, and
Utsuro (2006) uses manually encoded information about allowable and non-allowable
syntactic transformations of Japanese idioms, which are roughly equivalent to our
notions of canonical and non-canonical forms. The rule-based classifier of Uchiyama,
Baldwin, and Ishizaki (2005) incorporates syntactic information about Japanese com-
pound verbs (JCVs), a type of MWE composed of two verbs. In both cases, although the
classifiers incorporate syntactic information about MWEs, their manual development
limits the scalability of the approaches.
Uchiyama, Baldwin, and Ishizaki (2005) also propose a statistical token classifica-
tion method for JCVs. This method is similar to ours, in that it also uses type-based
knowledge to determine the class of each token in context. However, their method is
supervised, whereas our methods are unsupervised. Moreover, Uchiyama, Baldwin,
and Ishizaki only evaluate their methods on a set of JCVs that are mostly monosemous.
Our main focus here is on MWEs that are harder to disambiguate, that is, those that
have two clear idiomatic and literal meanings, and that are frequently used with either
meaning.
9. Conclusions
The significance of the role idioms play in language has long been recognized; however,
due to their peculiar behavior, they have been mostly overlooked by researchers in
computational linguistics. In this work, we focus on a broadly documented and cross-
linguistically frequent class of idiomatic MWEs: those that involve the combination
of a verb and a noun in its direct object position, which we refer to as verb+noun
idiomatic combinations or VNICs. Although a great deal of research has focused on
non-compositionality of MWEs, less attention has been paid to other properties relevant
to their semantic idiosyncrasy, such as lexical and syntactic fixedness. Drawing on such
properties, we have developed techniques for the automatic recognition of VNIC types,
as well as methods for their token identification in context.
We propose techniques for the automatic acquisition and encoding of knowledge
about the lexicosyntactic behavior of idiomatic combinations. More specifically, we
propose novel statistical measures that quantify the degree of lexical, syntactic, and
overall fixedness of a verb+noun combination. We demonstrate that these measures
can be successfully applied to the task of automatically distinguishing idiomatic ex-
pressions (types) from non-idiomatic ones. Our results show that the syntactic and
overall fixedness measures substantially outperform existing measures of collocation
extraction, even when they incorporate some syntactic information. We put forward
an unsupervised means for automatically discovering the set of syntactic variations
that are preferred by a VNIC type (its canonical forms) and that should be included
in its lexical representation. In addition, we show that the canonical form extraction
method can effectively be used in identifying idiomatic and literal usages (tokens) of an
expression in context.
96
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
We have annotated a total of 2, 465 tokens for 51 VNIC types according to whether
they are a literal or idiomatic usage. We found that for 28 expressions (1, 180 tokens),
approximately 40% of the usages were literal. For the remaining 23 expressions (1, 285
tokens), almost all usages were idiomatic. These figures indicate that automatically
determining whether a particular instance of an expression is used idiomatically or lit-
erally is of great importance for NLP applications. We have proposed two unsupervised
methods that perform such a task.
Our proposed methods incorporate automatically acquired knowledge about the
overall syntactic behavior of a VNIC type, in order to do token classification. More
specifically, our methods draw on the syntactic fixedness of VNICs?a property which
has been largely ignored in previous studies of MWE tokens. Our results confirm the
usefulness of this property as incorporated into our methods. On the 23 expressions
whose usages are predominantly idiomatic, because the baseline is very high none
of the methods outperform it. Nonetheless, as pointed out by our human annotators,
for many of these expressions it can be predicted beforehand that they are mainly
idiomatic and that a literal interpretation is impossible or highly implausible. On the
28 expressions with frequent literal usages, all our methods outperform the baseline of
always predicting themost dominant class (idiomatic). Moreover, on these, the accuracy
of our best unsupervised method is not substantially lower than the accuracy of a
standard supervised approach.
Appendix: Performance on the Individual Expressions
This Appendix contains the values of the four performance measures, Sens, PPV, Spec,
and NPV, for our two unsupervised methods (i.e., CFORM and CONTEXT) as well as for
the supervised method, SUP, on individual expressions in DTIhigh and DTIlow . Expressions
(verb?noun pairs) in each data set are ordered alphabetically.
Table 12
Performance of CFORM on individual expressions in DTIhigh and DTIlow .
Data Set verb?noun Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
blow top 1.00 0.92 0.60 1.00
blow trumpet 0.89 0.89 0.80 0.80
cut figure 0.97 0.97 0.86 0.86
find foot 0.98 0.92 0.20 0.50
get nod 0.96 1.00 1.00 0.75
get sack 1.00 0.96 0.71 1.00
have word 0.56 0.96 0.78 0.17
hit road 1.00 0.80 0.14 1.00
DTIhigh hit roof 1.00 0.65 0.00 0.00
kick heel 1.00 0.81 0.12 1.00
lose thread 0.94 0.94 0.50 0.50
make face 0.74 0.95 0.67 0.22
make mark 0.85 1.00 1.00 0.54
pull plug 0.89 0.77 0.40 0.62
pull punch 0.83 0.94 0.75 0.50
pull weight 1.00 0.93 0.67 1.00
take heart 1.00 0.97 0.88 1.00
97
Computational Linguistics Volume 35, Number 1
Table 12
(continued)
Data Set verb?noun Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
blow whistle 0.93 0.44 0.37 0.90
get wind 0.85 0.73 0.75 0.86
hit wall 0.86 0.11 0.09 0.83
hold fire 1.00 0.37 0.25 1.00
lose head 0.76 0.62 0.41 0.58
DTIlow make hay 1.00 0.56 0.12 1.00
make hit 1.00 0.71 0.78 1.00
make pile 0.25 0.14 0.29 0.45
make scene 0.82 0.68 0.45 0.64
pull leg 0.64 0.23 0.40 0.80
see star 0.80 0.10 0.38 0.95
Table 13
Performance of CONTEXT on individual expressions in DTIhigh and DTIlow .
Data Set verb?noun Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
blow top 1.00 0.85 0.20 1.00
blow trumpet 0.89 0.74 0.40 0.67
cut figure 1.00 0.84 0.00 0.00
find foot 1.00 0.90 0.00 0.00
get nod 1.00 0.88 0.00 0.00
get sack 1.00 0.86 0.00 0.00
have word 0.70 0.95 0.67 0.20
hit road 1.00 0.77 0.00 0.00
DTIhigh hit roof 1.00 0.65 0.00 0.00
kick heel 0.97 0.78 0.00 0.00
lose thread 1.00 0.90 0.00 0.00
make face 0.85 0.88 0.00 0.00
make mark 1.00 0.91 0.46 1.00
pull plug 0.96 0.69 0.05 0.33
pull punch 0.94 0.89 0.50 0.67
pull weight 1.00 0.82 0.00 0.00
take heart 0.90 0.85 0.38 0.50
blow whistle 0.89 0.36 0.18 0.75
get wind 0.85 0.65 0.62 0.83
hit wall 1.00 0.11 0.00 0.00
hold fire 1.00 0.30 0.00 0.00
lose head 0.90 0.56 0.12 0.50
DTIlow make hay 0.78 0.50 0.12 0.33
make hit 0.60 0.38 0.44 0.67
make pile 0.50 0.25 0.29 0.56
make scene 0.96 0.66 0.30 0.86
pull leg 0.82 0.22 0.20 0.80
see star 1.00 0.12 0.32 1.00
98
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Table 14
Performance of SUP on individual expressions in DTIhigh and DTIlow .
Data Set verb?noun Sens (Ridm) PPV (Pidm) Spec (Rlit) NPV (Plit)
blow top 1.00 0.85 0.20 1.00
blow trumpet 0.95 0.72 0.30 0.75
cut figure 1.00 0.84 0.00 0.00
find foot 1.00 0.90 0.00 0.00
get nod 0.91 0.91 0.33 0.33
get sack 1.00 0.86 0.00 0.00
have word 1.00 0.90 0.00 0.00
hit road 1.00 0.80 0.14 1.00
DTIhigh hit roof 0.82 0.64 0.17 0.33
kick heel 0.97 0.78 0.00 0.00
lose thread 1.00 0.95 0.50 1.00
make face 1.00 0.96 0.67 1.00
make mark 1.00 0.91 0.46 1.00
pull plug 0.98 0.90 0.75 0.94
pull punch 1.00 0.90 0.50 1.00
pull weight 1.00 0.82 0.00 0.00
take heart 0.93 0.83 0.25 0.50
blow whistle 0.52 0.78 0.92 0.78
get wind 0.77 0.71 0.75 0.80
hit wall 0.00 0.00 1.00 0.89
hold fire 0.00 0.00 0.88 0.67
lose head 0.48 0.62 0.65 0.50
DTIlow make hay 0.89 0.80 0.75 0.86
make hit 0.40 1.00 1.00 0.75
make pile 0.38 0.75 0.94 0.76
make scene 0.89 0.69 0.45 0.75
pull leg 0.55 0.75 0.95 0.88
see star 0.00 0.00 1.00 0.92
99
Computational Linguistics Volume 35, Number 1
Acknowledgments
This article is an extended and updated
combination of two papers that appeared,
respectively, in the proceedings of EACL
2006 and the proceedings of the ACL 2007
Workshop on A Broader Perspective on
Multiword Expressions. We wish to thank
the anonymous reviewers of those papers
for their helpful recommendations. We also
thank the anonymous reviewers of this
article for their insightful comments which
we believe have helped us improve the
quality of the work. We are grateful to Eric
Joanis for providing us with the NP-head
extraction software, and to Afra Alishahi
and Vivian Tsang for proofreading the
manuscript. Our work is financially
supported by the Natural Sciences and
Engineering Research Council of Canada,
the Ontario Graduate Scholarship program,
and the University of Toronto.
References
Abeille?, Anne. 1995. The flexibility of French
idioms: A representation with lexicalized
Tree Adjoining Grammar. In Everaert
et al, editors, Idioms: Structural and
Psychological Perspectives. LEA, Mahwah,
NJ, pages 15?42.
Akimoto, Minoji. 1999. Collocations and
idioms in Late Modern English. In L. J.
Brinton and M. Akimoto. Collocational and
Idiomatic Aspects of Composite Predicates in
the History of English. John Benjamins
Publishing Company, Amsterdam,
pages 207?238.
Baldwin, Timothy, Colin Bannard, Takaaki
Tanaka, and Dominic Widdows. 2003. An
empirical model of multiword expression
decomposability. In Proceedings of the
ACL-SIGLEX Workshop on Multiword
Expressions: Analysis, Acquisition and
Treatment, pages 89?96, Sapporo.
Bannard, Colin. 2007. A measure of syntactic
flexibility for automatically identifying
multiword expressions in corpora. In
Proceedings of the ACL?07 Workshop on a
Broader Perspective on Multiword
Expressions, pages 1?8, Prague.
Bannard, Colin, Timothy Baldwin, and
Alex Lascarides. 2003. A statistical
approach to the semantics of
verb-particles. In Proceedings of the
ACL-SIGLEX Workshop on Multiword
Expressions: Analysis, Acquisition and
Treatment, pages 65?72, Sapporo.
Birke, Julia and Anoop Sarkar. 2006. A
clustering approach for the nearly
unsupervised recognition of nonliteral
language. In Proceedings of the 11th
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL?06), pages 329?336, Trento.
Burnard, Lou. 2000. Reference Guide for the
British National Corpus (World Edition),
second edition. Available at www.natcorp.
ox.ac.uk.
Cacciari, Cristina. 1993. The place of idioms
in a literal and metaphorical world. In C.
Cacciari and P. Tabossi, Idioms: Processing,
Structure, and Interpretation. LEA, Mahwah,
NJ, pages 27?53.
Church, Kenneth, William Gale, Patrick
Hanks, and Donald Hindle. 1991. Using
statistics in lexical analysis. In Uri Zernik,
editor, Lexical Acquisition: Exploiting
On-Line Resources to Build a Lexicon. LEA,
Mahwah, NJ, pages 115?164.
Claridge, Claudia. 2000.Multi-word Verbs in
Early Modern English: A Corpus-based Study.
Editions Rodopi B. V., Amsterdam.
Clark, Eve V. 1978. Discovering what words
can do. Papers from the Parasession on the
Lexicon, 14:34?57.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20:37?46.
Collins, Michael. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D.
thesis, University of Pennsylvania.
Cook, Paul, Afsaneh Fazly, and Suzanne
Stevenson. 2007. Pulling their weight:
Exploiting syntactic forms for the
automatic identification of idiomatic
expressions in context. In Proceedings of the
ACL?07 Workshop on a Broader Perspective on
Multiword Expressions, pages 41?48,
Prague.
Copestake, Ann, Fabre Lambeau, Aline
Villavicencio, Francis Bond, Timothy
Baldwin, Ivan A. Sag, and Dan Flickinger.
2002. Multiword expressions: Linguistic
precision and reusability. In Proceedings of
the 4th International Conference on Language
Resources and Evaluation (LREC?02),
pages 1941?47, Las Palmas.
Cover, Thomas M. and Joy A. Thomas. 1991.
Elements of Information Theory. John Wiley
and Sons, Inc., New York.
Cowie, Anthony P., Ronald Mackin, and
Isabel R. McCaig. 1983. Oxford Dictionary of
Current Idiomatic English, volume 2. Oxford
University Press.
Dagan, Ido, Fernando Pereira, and Lillian
Lee. 1994. Similarity-based estimation of
word co-occurrence probabilities. In
Proceedings of the 32nd Anuual Meeting of the
100
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
Association for Computational Linguistics
(ACL?94), pages 272?278, Las Cruces, NM.
d?Arcais, Giovanni B. Flores. 1993. The
comprehension and semantic
interpretation of idioms. In C. Cacciari and
P. Tabossi, Idioms: Processing, Structure, and
Interpretation. LEA, Mahwah, NJ,
pages 79?98.
Desbiens, Marguerite Champagne and Mara
Simon. 2003. De?terminants et locutions
verbales. Manuscript. Available at
www.er.uqam.ca/nobel/scilang/cesla02/
mara margue.pdf.
Evert, Stefan, Ulrich Heid, and Kristina
Spranger. 2004. Identifying
morphosyntactic preferences in
collocations. In Proceedings of the 4th
International Conference on Language
Resources and Evaluation (LREC?04),
pages 907?910, Lisbon.
Evert, Stefan and Brigitte Krenn. 2001.
Methods for the qualitative evaluation of
lexical association measures. In Proceedings
of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL?01),
pages 188?195, Toulouse.
Fazly, Afsaneh and Suzanne Stevenson. 2006.
Automatically constructing a lexicon of
verb phrase idiomatic combinations. In
Proceedings of the 11th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL?06),
pages 337?344, Trento.
Fazly, Afsaneh and Suzanne Stevenson. 2007.
Distinguishing subtypes of multiword
expressions using linguistically-motivated
statistical measures. In Proceedings of the
ACL?07 Workshop on a Broader Perspective
on Multiword Expressions, pages 9?16,
Prague.
Fazly, Afsaneh and Suzanne Stevenson. A
distributional account of the semantics of
multiword expressions. To appear in the
Italian Journal of Linguistics.
Fellbaum, Christiane. 1993. The determiner
in English idioms. In C. Cacciari and
P. Tabossi, Idioms: Processing, Structure,
and Interpretation. LEA, Mahwah, NJ,
pages 271?295.
Fellbaum, Christiane, editor. 1998.WordNet,
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fellbaum, Christiane. 2002. VP idioms in the
lexicon: Topics for research using a very
large corpus. In Proceedings of the
KONVENS 2002 Conference, pages 7?11,
Saarbruecken, Germany.
Fellbaum, Christiane. 2007. The ontological
loneliness of idioms. In Andrea Schalley
and Dietmar Zaefferer, editors,
Ontolinguistics. Mouton de Gruyter, Berlin,
pages 419?434.
Firth, John R. 1957. A synopsis of linguistic
theory 1930?1955. In Studies in Linguistic
Analysis (special volume of the Philological
Society). The Philological Society, Oxford,
pages 1?32.
Fraser, Bruce. 1970. Idioms within a
transformational grammar. Foundations of
Language, 6:22?42.
Gentner, Dedre and Ilene M. France. 2004.
The verb mutability effect: Studies of the
combinatorial semantics of nouns and
verbs. In Steven L. Small, Garrison W.
Cottrell, and Michael K. Tanenhaus,
editors, Lexical Ambiguity Resolution:
Perspectives from Psycholinguistics,
Neuropsychology, and Artificial Intelligence.
Kaufmann, San Mateo, CA, pages 343?382.
Gibbs, Raymond W. Jr. 1993. Why idioms are
not dead metaphors. In C. Cacciari and
P. Tabossi, Idioms: Processing, Structure, and
Interpretation. LEA, Mahwah, NJ,
pages 57?77.
Gibbs, Raymond W. Jr. 1995. Idiomaticity
and human cognition. In Everaert et al,
editors, Idioms: Structural and Psychological
Perspectives. LEA, Mahwah, NJ,
pages 97?116.
Gibbs, Raymond W. Jr. and Nandini P.
Nayak. 1989. Psychololinguistic studies on
the syntactic behavior of idioms. Cognitive
Psychology, 21:100?138.
Gibbs, Raymond W. Jr., Nandini P. Nayak,
J. Bolton, and M. Keppel. 1989. Speaker?s
assumptions about the lexical flexibility
of idioms.Memory and Cognition,
17:58?68.
Glucksberg, Sam. 1993. Idiom meanings and
allusional content. In C. Cacciari and P.
Tabossi, Idioms: Processing, Structure, and
Interpretation. LEA, Mahwah, NJ,
pages 3?26.
Goldberg, Adele E. 1995. Constructions: A
Construction Grammar Approach to
Argument Structure. The University of
Chicago Press.
Grant, Lynn E. 2005. Frequency of ?core
idioms? in the British National Corpus
(BNC). International Journal of Corpus
Linguistics, 10(4):429?451.
Hashimoto, Chikara, Satoshi Sato, and
Takehito Utsuro. 2006. Japanese idiom
recognition: Drawing a line between
literal and idiomatic meanings. In
Proceedings of the 17th International
Conference on Computational Linguistics
and the 36th Annual Meeting of the
101
Computational Linguistics Volume 35, Number 1
Association for Computational Linguistics
(COLING-ACL?06), pages 353?360, Sydney.
Inkpen, Diana. 2003. Building a Lexical
Knowledge-Base of Near-Synonym Differences.
Ph.D. thesis, University of Toronto.
Jackendoff, Ray. 1997. The Architecture of the
Language Faculty. MIT Press, Cambridge,
MA.
Katz, Graham and Eugenie Giesbrecht. 2006.
Automatic identification of
non-compositional multi-word
expressions using Latent Semantic
Analysis. In Proceedings of the ACL?06
Workshop on Multiword Expressions:
Identifying and Exploiting Underlying
Properties, pages 12?19, Sydney.
Katz, Jerrold J. 1973. Compositionality,
idiomaticity, and lexical substitution. In
S. Anderson and P. Kiparsky, editors, A
Festschrift for Morris Halle. Holt, Rinehart
and Winston, New York, pages 357?376.
Kearns, Kate. 2002. Light verbs in English.
Manuscript. Available at www.ling.
canterbury.ac.nz/people/kearns.html.
Kirkpatrick, E. M. and C. M. Schwarz,
editors. 1982. Chambers Idioms. W & R
Chambers Ltd, Edinburgh.
Krenn, Brigitte and Stefan Evert. 2001. Can
we do better than frequency? A case study
on extracting PP-verb collocations. In
Proceedings of the ACL?01 Workshop on
Collocations, pages 39?46, Toulouse.
Kyto?, Merja. 1999. Collocational and
idiomatic aspects of verbs in Early Modern
English. In L. J. Brinton and M. Akimoto.
Collocational and Idiomatic Aspects of
Composite Predicates in the History of
English. John Benjamins Publishing
Company, Amsterdam, pages 167?206.
Lapata, Mirella and Alex Lascarides. 2003.
Detecting novel compounds: The role of
distributional evidence. In Proceedings of
the 11th Conference of the European Chapter of
the Association for Computational Linguistics
(EACL?03), pages 235?242, Budapest.
Lin, Dekang. 1998. Automatic retrieval and
clustering of similar words. In Proceedings
of the 17th International Conference on
Computational Linguistics and the 36th
Annual Meeting of the Association for
Computational Linguistics
(COLING-ACL?98), pages 768?774,
Montreal.
Lin, Dekang. 1999. Automatic identification
of non-compositional phrases. In
Proceedings of the 37th Annual Meeting of the
Association for Computational Linguistics
(ACL?99), pages 317?324, College Park,
Maryland.
Manning, Christopher D. and Hinrich
Schu?tze. 1999. Foundations of Statistical
Natural Language Processing. The MIT
Press, Cambridge, MA.
McCarthy, Diana, Bill Keller, and John
Carroll. 2003. Detecting a continuum
of compositionality in phrasal verbs.
In Proceedings of the ACL-SIGLEX
Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment,
pages 73?80, Sapporo.
Melamed, I. Dan. 1997a. Automatic
discovery of non-compositional
compounds in parallel data. In Proceedings
of the 2nd Conference on Empirical Methods in
Natural Language Processing (EMNLP?97),
pages 97?108, Providence, RI.
Melamed, I. Dan. 1997b. Measuring semantic
entropy. In Proceedings of the ACL-SIGLEX
Workshop on Tagging Text with Lexical
Semantics: Why, What and How,
pages 41?46, Washington, DC.
Mohammad, Saif and Graeme Hirst.
Distributional measures as proxies for
semantic relatedness. Submitted.
Moon, Rosamund. 1998. Fixed Expressions and
Idioms in English: A Corpus-Based Approach.
Oxford University Press.
Newman, John and Sally Rice. 2004. Patterns
of usage for English SIT, STAND, and LIE:
A cognitively inspired exploration in
corpus linguistics. Cognitive Linguistics,
15(3):351?396.
Nicolas, Tim. 1995. Semantics of idiom
modification. In Everaert et al, editors,
Idioms: Structural and Psychological
Perspectives. LEA, Mahwah, NJ,
pages 233?252.
Nunberg, Geoffrey, Ivan A. Sag, and Thomas
Wasow. 1994. Idioms. Language,
70(3):491?538.
Odijk, Jan. 2004. A proposed standard for the
lexical representations of idioms. In
Proceedings of Euralex?04, pages 153?164,
Lorient.
Ogden, Charles Kay. 1968. Basic English,
International Second Language. Harcourt,
Brace, and World, New York.
Patrick, Jon and Jeremy Fletcher. 2005.
Classifying verb-particle constructions
by verb arguments. In Proceedings of
the Second ACL-SIGSEMWorkshop on the
Linguistic Dimensions of Prepositions and
their Use in Computational Linguistics
Formalisms and Applications, pages 200?209,
Colcheter.
Pauwels, Paul. 2000. Put, Set, Lay and Place: A
Cognitive Linguistic Approach to Verbal
Meaning. LINCOM EUROPA, Munich.
102
Fazly, Cook, and Stevenson Unsupervised Idiom Identification
R 2004. Notes on R: A Programming
Environment for Data Analysis and Graphics.
Available at www.r-project.org.
Resnik, Philip. 1999. Semantic similarity in a
taxonomy: An information-based measure
and its application to problems of
ambiguity in natural language. Journal of
Artificial Intelligence Research (JAIR),
(11):95?130.
Riehemann, Susanne. 2001. A Constructional
Approach to Idioms and Word Formation.
Ph.D. thesis, Stanford University.
Ritz, Julia and Ulrich Heid. 2006. Extraction
tools for collocations and their
morphosyntactic specificities. In
Proceedings of the 5th International
Conference on Language Resources and
Evaluation (LREC?06), pages 1925?30,
Genoa.
Rohde, Douglas L. T. 2004. TGrep2 User
Manual. Available at http://tedlab.mit.
edu/?dr/Tgrep2.
Sag, Ivan A., Timothy Baldwin, Francis
Bond, Ann Copestake, and Dan Flickinger.
2002. Multiword expressions: A pain in the
neck for NLP. In Proceedings of the 3rd
International Conference on Intelligent Text
Processing and Computational Linguistics
(CICLing?02), pages 1?15, Mexico City.
Schenk, Andre?. 1995. The syntactic behavior
of idioms. In Everaert et al, editors, Idioms:
Structural and Psychological Perspectives.
LEA, Mahwah, NJ, chapter 10,
pages 253?271.
Seaton, Maggie and Alison Macaulay,
editors. 2002. Collins COBUILD Idioms
Dictionary. HarperCollins Publishers,
second edition, New York.
Smadja, Frank. 1993. Retrieving collocations
from text: Xtract. Computational Linguistics,
19(1):143?177.
Tanabe, Harumi. 1999. Composite predicates
and phrasal verbs in The Paston Letters. In
L. J. Brinton and M. Akimoto. Collocational
and Idiomatic Aspects of Composite Predicates
in the History of English. John Benjamins
Publishing Company, Amsterdam,
pages 97?132.
Uchiyama, Kiyoko, Timothy Baldwin, and
Shun Ishizaki. 2005. Disambiguating
Japanese compound verbs. Computer
Speech and Language, 19:497?512.
Van de Cruys, Tim and Begon?a
Villada Moiro?n. 2007. Semantics-based
multiword expression extraction. In
Proceedings of the ACL?07 Workshop on a
Broader Perspective on Multiword
Expressions, pages 25?32, Prague.
Venkatapathy, Sriram and Aravid Joshi. 2005.
Measuring the relative compositionality of
verb-noun (V-N) collocations by
integrating features. In Proceedings of Joint
Conference on Human Language Technology
and Empirical Methods in Natural Language
Processing (HLT-EMNLP?05),
pages 899?906, Vancouver.
Villada Moiro?n, Begon?a and Jo?rg Tiedemann.
2006. Identifying idiomatic expressions
using automatic word-alignment. In
Proceedings of the EACL?06 Workshop on
Multiword Expressions in a Multilingual
Context, pages 33?40, Trento.
Villavicencio, Aline, Ann Copestake,
Benjamin Waldron, and Fabre Lambeau.
2004. Lexical encoding of multiword
expressions. In Proceedings of the 2nd ACL
Workshop on Multiword Expressions:
Integrating Processing, pages 80?87,
Barcelona.
Wermter, Joachim and Udo Hahn. 2005.
Paradigmatic modifiability statistics for
the extraction of complex multi-word
terms. In Proceedings of Joint Conference on
Human Language Technology and Empirical
Methods in Natural Language Processing
(HLT-EMNLP?05), pages 843?850,
Vancouver.
Widdows, Dominic and Beate Dorow. 2005.
Automatic extraction of idioms using
graph analysis and asymmetric
lexicosyntactic patterns. In Proceedings of
ACL?05 Workshop on Deep Lexical
Acquisition, pages 48?56, Ann Arbor, MI.
Wilcoxon, Frank. 1945. Individual
comparisons by ranking methods.
Biometrics Bulletin, 1(6):80?83.
103

   	

 
	ffSemi-supervised Verb Class Discovery Using Noisy Features
Suzanne Stevenson and Eric Joanis
Department of Computer Science
University of Toronto
 
suzanne,joanis  @cs.toronto.edu
Abstract
We cluster verbs into lexical semantic classes,
using a general set of noisy features that cap-
ture syntactic and semantic properties of the
verbs. The feature set was previously shown to
work well in a supervised learning setting, us-
ing known English verb classes. In moving to a
scenario of verb class discovery, using cluster-
ing, we face the problem of having a large num-
ber of irrelevant features for a particular cluster-
ing task. We investigate various approaches to
feature selection, using both unsupervised and
semi-supervised methods, comparing the results
to subsets of features manually chosen accord-
ing to linguistic properties. We find that the un-
supervised method we tried cannot be consis-
tently applied to our data. However, the semi-
supervised approach (using a seed set of sam-
ple verbs) overall outperforms not only the full
set of features, but the hand-selected features as
well.
1 Introduction
Computational linguists face a lexical acquisition bot-
tleneck, as vast amounts of knowledge about individual
words are required for language technologies. Learn-
ing the argument structure properties of verbs?the se-
mantic roles they assign and their mapping to syntac-
tic positions?is both particularly important and difficult.
A number of supervised learning approaches have ex-
tracted such informationabout verbs from corpora, includ-
ing their argument roles (Gildea and Jurafsky, 2002), se-
lectional preferences (Resnik, 1996), and lexical semantic
classification (i.e., grouping verbs according to their argu-
ment structure properties) (Dorr and Jones, 1996; Lapata
and Brew, 1999; Merlo and Stevenson, 2001; Joanis and
Stevenson, 2003). Unsupervised or semi-supervised ap-
proaches have been successful as well, but have tended to
be more restrictive, in relying on human filtering of the
results (Riloff and Schmelzenbach, 1998), on the hand-
selection of features (Stevenson and Merlo, 1999), or on
the use of an extensive grammar (Schulte im Walde and
Brew, 2002).
We focus here on extending the applicability of un-
supervised methods, as in (Schulte im Walde and Brew,
2002; Stevenson and Merlo, 1999), to the lexical seman-
tic classification of verbs. Such classes group together
verbs that share both a common semantics (such as trans-
fer of possession or change of state), and a set of syntactic
frames for expressing the arguments of the verb (Levin,
1993; FrameNet, 2003). As such, they serve as a means
for organizing complex knowledge about verbs in a com-
putational lexicon (Kipper et al, 2000). However, cre-
ating a verb classification is highly resource intensive, in
terms of both required time and linguistic expertise. De-
velopment of minimally supervised methods is of particu-
lar importance if we are to automatically classify verbs for
languages other than English, where substantial amounts
of labelled data are not available for training classifiers. It
is also necessary to consider the probable lack of sophisti-
cated grammars or text processing tools for extracting ac-
curate features.
We have previously shown that a broad set of 220
noisy features performs well in supervised verb classifi-
cation (Joanis and Stevenson, 2003). In contrast to Merlo
and Stevenson (2001), we confirmed that a set of gen-
eral features can be successfully used, without the need
for manually determining the relevant features for dis-
tinguishing particular classes (cf. Dorr and Jones, 1996;
Schulte im Walde and Brew, 2002). On the other hand, in
contrast to Schulte im Walde and Brew (2002), we demon-
strated that accurate subcategorizationstatistics are unnec-
essary (see also Sarkar and Tripasai, 2002).
By avoiding the dependence on precise feature extrac-
tion, our approach should be more portable to new lan-
guages. However, a general feature space means that most
features will be irrelevant to any given verb discrimination
task. In an unsupervised (clustering) scenario of verb class
discovery, can we maintain the benefit of only needing
noisy features, without the generality of the feature space
leading to ?the curse of dimensionality?? In supervised
experiments, the learner uses class labels during the train-
ing stage to determine which features are relevant to the
task at hand. In the unsupervised setting, the large number
of potentially irrelevant features becomes a serious prob-
lem, since those features may mislead the learner.
Thus, the problem of dimensionality reduction is a key
issue to be addressed in verb class discovery. In this paper,
we report results on several feature selection approaches to
the problem: manual selection (based on linguistic knowl-
edge), unsupervised selection (based on an entropy mea-
sure among the features, Dash et al, 1997), and a semi-
supervised approach (in which seed verbs are used to train
a supervised learner, from which we extract the useful fea-
tures). Although our motivation is verb class discovery,
we perform our experiments on English, for which we
have an accepted classification to serve as a gold standard
(Levin, 1993). To preview our results, we find that, over-
all, the semi-supervised method not only outperforms the
entire feature space, but also the manually selected subset
of features. The unsupervised feature selection method,
on the other hand, was not usable for our data.
In the remainder of the paper, we first briefly review
our feature space and present our experimental classes and
verbs. We then describe our clustering methodology, the
measures we use to evaluate a clustering, and our experi-
mental results. We conclude with a discussion of related
work, our contributions, and future directions.
2 The Feature Space
Like others, we have assumed lexical semantic classes of
verbs as defined in Levin (1993) (hereafter Levin), which
have served as a gold standard in computational linguis-
tics research (Dorr and Jones, 1996; Kipper et al, 2000;
Merlo and Stevenson, 2001; Schulte im Walde and Brew,
2002). Levin?s classes form a hierarchy of verb groupings
with shared meaning and syntax. Our feature space was
designed to reflect these classes by capturing properties
of the semantic arguments of verbs and their mapping to
syntactic positions. It is important to emphasize, however,
that our features are extracted from part-of-speech (POS)
tagged and chunked text only: there are no semantic tags
of any kind. Thus, the features serve as approximations to
the underlying distinctions among classes.
Here we briefly describe the features that comprise our
feature space, and refer the interested reader to Joanis and
Stevenson (2003) for details.
Features over Syntactic Slots (120 features)
One set of features encodes the frequency of the syntac-
tic slots occurring with a verb (subject, direct and indirect
object, and prepositional phrases (PPs) indexed by prepo-
sition), which collectively serve as rough approximations
to the allowable syntactic frames for a verb. We also count
fixed elements in certain slots (it and there, as in It rains
or There appeared a ship), since these are part of the syn-
tactic frame specifications for a verb.
In addition to approximating the syntactic frames them-
selves, we also want to capture regularities in the mapping
of arguments to particular slots. For example, the location
argument, the truck, is direct object in I loaded the truck
with hay, and object of a preposition in I loaded hay onto
the truck. These allowable alternations in the expressions
of arguments vary according to the class of a verb. We
measure this behaviour using features that encode the de-
gree to which two slots contain the same entities?that is,
we calculate the overlap in noun (lemma) usage between
pairs of syntactic slots.
Tense, Voice, and Aspect Features (24 features)
Verb meaning, and therefore class membership, inter-
acts in interesting ways with voice, tense, and aspect
(Levin, 1993; Merlo and Stevenson, 2001). In addition
to verb POS (which often indicates tense) and voice (pas-
sive/active), we also include counts of modals, auxiliaries,
and adverbs, which are partial indicators of these factors.
The Animacy Features (76 features)
Semantic properties of the arguments that fill certain roles,
such as animacy or motion, are more challenging to de-
tect automatically. Currently, our only such feature is an
extension of the animacy feature of Merlo and Stevenson
(2001). We approximate the animacy of each of the 76
syntactic slots by counting both pronouns and proper noun
phrases (NPs) labelled as ?person? by our chunker (Ab-
ney, 1991).
3 Experimental Classes and Verbs
We use the same classes and example verbs as in the su-
pervised experiments of Joanis and Stevenson (2003) to
enable a comparison between the performance of the un-
supervised and supervised methods. Here we describe the
selection of the experimental classes and verbs, and the es-
timation of the feature values.
3.1 The Verb Classes
Pairs or triples of verb classes from Levin were selected to
form the test pairs/triples for each of a number of separate
classification tasks. These sets exhibit different contrasts
between verb classes in terms of their semantic argument
assignments, allowing us to evaluate our approach under
a range of conditions. For example, some classes differ
in both their semantic roles and frames, while others have
the same roles in different frames, or different roles in the
same frames.1 Here we summarize the argument structure
distinctions between the classes; Table 1 below lists the
classes with their Levin class numbers.
Benefactive versus Recipient verbs.
Mary baked... a cake for Joan/Joan a cake.
Mary gave... a cake to Joan/Joan a cake.
These dative alternation verbs differ in the preposition and
the semantic role of its object.
1For practical reasons, as well as for enabling us to draw more
general conclusions from the results, the classes also could nei-
ther be too small nor contain mostly infrequent verbs.
Admire versus Amuse verbs.
I admire Jane. Jane amuses me.
These psychological state verbs differ in that the Experi-
encer argument is the subject of Admire verbs, and the ob-
ject of Amuse verbs.
Run versus Sound Emission verbs.
The kids ran in the room./*The room ran with kids.
The birds sang in the trees./The trees sang with birds.
These activity verbs both have an Agent subject in the in-
transitive, but differ in the prepositional alternations they
allow.
Cheat versus Steal and Remove verbs.
I cheated... Jane of her money/*the money from Jane.
I stole... *Jane of her money/the money from Jane.
These classes also assign the same semantic arguments,
but differ in their prepositional alternants.
Wipe versus Steal and Remove verbs.
Wipe... the dust/the dust from the table/the table.
Steal... the money/the money from the bank/*the bank.
These classes generally allow the same syntactic frames,
but differ in the possible semantic role assignment. (Loca-
tion can be the direct object of Wipe verbs but not of Steal
and Remove verbs, as shown.)
Spray/Load versus Fill versus Other Verbs of
Putting (several related Levin classes).
I loaded... hay on the wagon/the wagon with hay.
I filled... *hay on the wagon/the wagon with hay.
I put... hay on the wagon/*the wagon with hay.
These three classes also assign the same semantic roles
but differ in prepositional alternants. Note, however, that
the options for Spray/Load verbs overlap with those of the
other two types of verbs.
Optionally Intransitive: Run versus Change
of State versus ?Object Drop?.
The horse raced./The jockey raced the horse.
The butter melted./The cook melted the butter.
The boy played./The boy played soccer.
These three classes are all optionally intransitive but as-
sign different semantic roles to their arguments (Merlo and
Stevenson, 2001). (Note that the Object Drop verbs are a
superset of the Benefactives above.)
For many tasks, knowing exactly what PP arguments
each verb takes may be sufficient to perform the classifica-
tion (cf. Dorr and Jones, 1996). However, our features do
not give us such perfect knowledge, since PP arguments
and adjuncts cannot be distinguished with high accuracy.
Using our simple extraction tools, for example, the PP  
argument in I admired Jane for her honesty is not distin-
guished from the PP  adjunct in I amused Jane for the
money. Furthermore, PP arguments differ in frequency, so
that a highly distinguishing but rarely used alternant will
likely not be useful. Indicators of PP usage are thus useful
but not definitive.
Verb Class Class Number # Verbs
Benefactive 26.1, 26.3 35
Recipient 13.1, 13.3 27
Admire 31.2 35
Amuse 31.1 134
Run 51.3.2 79
Sound Emission 43.2 56
Cheat 10.6 29
Steal and Remove 10.5, 10.1 45
Wipe 10.4.1, 10.4.2 35
Spray/Load 9.7 36
Fill 9.8 63
Other V. of Putting 9.1?6 48
Change of State 45.1?4 169
Object Drop 26.1, 26.3, 26.7 50
Table 1: Verb classes (see Section 3.1), their Levin class
numbers, and the number of experimental verbs in each
(see Section 3.2).
3.2 Verb Selection
Our experimental verbs were selected as follows. We
started with a list of all the verbs in the given classes from
Levin, removing any verb that did not occur at least 100
times in our corpus (the BNC, described below). Because
we make the simplifying assumption of a single correct
classification for each verb, we also removed any verb:
that was deemed excessively polysemous; that belonged
to another class under consideration in our study; or for
which the class did not correspond to the main sense.
Table 1 above shows the number of verbs in each class
at the end of this process. Of these verbs, 20 from each
class were randomly selected to use as trainingdata for our
supervised experiments in Joanis and Stevenson (2003).
We began with this same set of 20 verbs per class for
our current work. We then replaced 10 of the 260 verbs
(4%) to enable us to have representative seed verbs for
certain classes in our semi-supervised experiments (e.g.,
so that we could include wipe as a seed verb for the Wipe
verbs, and fill for the Fill verbs). All experiments reported
here were run on this same final set of 20 verbs per class
(including a replication of our earlier supervised experi-
ments).
3.3 Feature Extraction
All features were estimated from counts over the British
National Corpus (BNC), a 100M word corpus of text sam-
ples of recent British English ranging over a wide spec-
trum of domains. Since it is a general corpus, we do not
expect any strong overall domain bias in verb usage.
We used the chunker (partial parser) of Abney (1991)
to preprocess the corpus, which (noisily) determines the
NP subject and direct object of a verb, as well as the PPs
potentially associated with it. Indirect objects are identi-
fied by a less sophisticated (and even noisier) method, sim-
ply assuming that two consecutive NPs after the verb con-
stitute a double object frame. From these extracted slots,
we calculate the features described in Section 2, yielding
a vector of 220 normalized counts for each verb, which
forms the input to our machine learning experiments.
4 Clustering and Evaluation Methods
4.1 Clustering Parameters
We used the hierarchical clustering command in Matlab,
which implements bottom-up agglomerative clustering,
for all our unsupervised experiments. In performing hi-
erarchical clustering, both a vector distance measure and
a cluster distance (?linkage?) measure are specified. We
used the simple Euclidean distance for the former, and
Ward linkage for the latter. Ward linkage essentially mini-
mizes the distances of all cluster points to the centroid, and
thus is less sensitive to outliers than some other methods.
We chose hierarchical clustering because it may be pos-
sible to find coherent subclusters of verbs even when there
are not exactly   good clusters, where   is the number
of classes. To explore this, we can induce any number
of clusters  by making a cut at a particular level in the
clustering hierarchy. In the experiments here, however,
we report only results for   , since we found no
principled way of automatically determining a good cut-
off. However, we did experiment with   (as in
Strehl et al, 2000), and found that performance was gen-
erally better (even on our 	
 measure, described below,
that discounts oversplitting). This supports our intuition
that the approach may enable us to find more consistent
clusters at a finer grain, without too much fragmentation.
4.2 Evaluation Measures
We use three separate evaluation measures, that tap into
very different properties of the clusterings.
4.2.1 Accuracy
We can assign each cluster the class label of the ma-
jority of its members. Then for all verbs  , consider  to
be classified correctly if Class(  )=ClusterLabel(  ), where
Class(  ) is the actual class of  and ClusterLabel(  ) is the
label assigned to the cluster in which  is placed. Then ac-
curacy has the standard definition:2
2  is equivalent to the weighted mean precision of the clus-
ters, weighted according to cluster size.
As we have defined it,  necessarily generally increases as
the number of clusters increases, with the extreme being at the
number of clusters equal to the number of verbs. However, since
we fix our number of clusters to the number of classes, the mea-
sure remains informative.


#verbs correctly classified
#verbs total

thus provides a measure of the usefulness in prac-
tice of a clustering?that is, if one were to use the clus-
tering as a classification, this measure tells how accurate
overall the class assignments would be. The theoretical
maximum is, of course, 1. To calculate a random baseline,
we evaluated 10,000 random clusterings with the same
number of verbs and classes as in each of our experimen-
tal tasks. Because the
Towards a Framework for Learning Structured Shape Models from
Text-Annotated Images
Sven Wachsmuth
 
, Suzanne Stevenson
 
, Sven Dickinson
 

Bielefeld University, Faculty of Technology, 33594 Bielefeld, Germany
 
University of Toronto, Dept. of Computer Science, Toronto, ON, Canada

swachsmu,suzanne,sven  @cs.toronto.edu
Abstract
We present on-going work on the topic of learn-
ing translation models between image data and
text (English) captions. Most approaches to
this problem assume a one-to-one or a flat, one-
to-many mapping between a segmented image
region and a word. However, this assump-
tion is very restrictive from the computer vi-
sion standpoint, and fails to account for two
important properties of image segmentation: 1)
objects often consist of multiple parts, each
captured by an individual region; and 2) indi-
vidual regions are often over-segmented into
multiple subregions. Moreover, this assump-
tion also fails to capture the structural rela-
tions among words, e.g., part/whole relations.
We outline a general framework that accommo-
dates a many-to-many mapping between im-
age regions and words, allowing for struc-
tured descriptions on both sides. In this paper,
we describe our extensions to the probabilis-
tic translation model of Brown et al (1993) (as
in Duygulu et al (2002)) that enable the cre-
ation of structured models of image objects.
We demonstrate our work in progress, in which
a set of annotated images is used to derive a set
of labeled, structured descriptions in the pres-
ence of oversegmentation.
1 Introduction
Researchers in computer vision and computational lin-
guistics have similar goals in their desire to automati-
cally associate semantic information with the visual or
linguistic representations they extract from an image or
text. Given paired image and text data, one approach
0Wachsmuth is supported by the German Research Founda-
tion (DFG). Stevenson and Dickinson gratefully acknowledge
the support of NSERC of Canada.
is to use the visual and linguistic representations as im-
plicit semantics for each other?that is, using the words
as names for the visual features, and using the image ob-
jects as referents for the words in the text (cf. Roy, 2002).
The goal of our work is to automatically acquire struc-
tured object models from image data associated with text,
at the same time learning an assignment of text labels for
objects as well as for their subparts (and, in the long run,
also for collections of objects).
Multimodal datasets that contain both images and text
are ubiquitous, including annotated medical images and
the Corel dataset, not to mention the World Wide Web,
allowing the possibility of associating textual and visual
information in this way. For example, if a web crawler
encountered many images containing a particular shape,
and also found that the word chair was contained in the
captions of those images, it might associate the shape
with the word chair, simultaneously indicating a name
for the shape and a visual ?definition? for the word. Such
a framework could then learn the class names for a set
of shape classes, effectively yielding a translation model
between image shapes (or more generally, features) and
words (Duygulu et al, 2002). This translation model
could then be used to answer many types of queries, in-
cluding labeling a new image in terms of its visible ob-
jects, or generating a visual prototype for a given class
name. Furthermore, since figure captions (or, in general,
image annotations) may contain words for entire objects,
as well as words for their component parts, a natural se-
mantic hierarchy may emerge from the words. For exam-
ple, just as tables in the image may be composed of ?leg?
image parts, the word leg can be associated with the word
table in a part-whole relation.
Others have explored the problem of learning
associations between image regions (or features)
and text, including Barnard and Forsyth (2001),
Duygulu et al (2002), Blei and Jordan (2002), and
Cascia et al (1998). As impressive as the results are,
these approaches make limiting assumptions that prevent
them from being appropriate to our goals of a structured
object model. On the vision side, each segmented region
is mapped one-to-one or one-to-many to words. Concep-
tually, associating a word with only one region prevents
an appropriate treatment of objects with parts, since such
objects may consistently be region-segmented into a
collection of regions corresponding to those components.
Practically, even putting aside the goal of part-whole
processing, any given region may be (incorrectly)
oversegmented into a set of subregions (that are not
component parts) in real images. Barnard et al (2003)
propose a ranking scheme for potential merges of regions
based on a model of word-region association, but do
not address the creation of a structured object model
from sequences of merges. To address these issues, we
propose a more elaborate translation/association model
in which we use the text of the image captions to guide
us in structuring the regions.
On the language side of this task, words have typi-
cally been treated individually with no semantic struc-
ture among them (though see Roy, 2002, which induces
syntactic structure among the words). Multiple words
may be assigned as the label to a region, but there?s
no knowledge of the relations among the words (and
in fact they may be treated as interchangeable labels,
Duygulu et al, 2002). The more restrictive goal of image
labeling has put the focus on the image as the (structured)
object. But we take an approach in principle of build-
ing a structured hierarchy for both the image objects and
their text labels. In this way, we aim not only to use the
words to help guide us in how to interpret image regions,
but also to use the image structure to help us induce a
part/whole hierarchy among the words. For example, as-
sume we find consistently associated leg and top regions
together referred to as a table. Then instead of treating
leg and table, e.g., as two labels for the same object, we
could capture the image part-whole structure as word re-
lations in our lexicon.
Our goal of inducing associated structured hierarchies
of visual and linguistic descriptions is a long-term one,
and this paper reports on our work thus far. We start with
the probabilistic translation model of Brown et al (1993)
(as in Duygulu et al, 2002), and extend it to structured
shape descriptions of visual data. As alluded to earlier,
we distinguish between two types of structured shape de-
scriptions: collections of regions that should be merged
due to oversegmentation versus collections of regions that
represent components of an object. To handle both types,
we incorporate into our algorithm several region merge
operations that iteratively evaluate potential merges in
terms of their improvement to the translation model.
These operations can exploit probabilities over region
adjacency, thus constraining the potential combinatorial
explosion of possible region merges. We also permit a
many-to-many mapping between regions and words, in
support of our goal of inducing structured text as well,
although here we report only on the structured image
model, assuming similar mechanisms will be useful on
the text side.
We are currently developing a system to demonstrate
our proposal. The input to the system is a set of images
segmented into regions organized into a region adjacency
graph. Nodes in the graph encode the qualitative shape of
a region using a shock graph (Siddiqi et al, 1999), while
undirected edges represent region adjacency (used to con-
strain possible merges). On the text side, each image has
an associated caption which is processed by a part-of-
speech tagger (Brill, 1994) and chunker (Abney, 1991).
The result is a set of noun phrases (nouns with associated
modifiers) which may or may not pertain to image con-
tent. The output of the system is a set of many-to-many
(possibly structured) associations between image regions
and text words.
This paper represents work in progress, and not all the
components have been fully integrated. Initially, we have
focused on the issues of building the structured image
models. We demonstrate the ideas on a set of annotated
synthetic scenes with both multi-part objects and over-
segmented objects/parts. The results show that at least
on simple scenes, the model can cope with oversegmen-
tation and converge to a set of meaningful many-to-many
(regions to words) mappings.
2 Visual Shape Description
In order to learn structured visual representations, we
must be able to make meaningful generalizations over
image regions that are sufficiently similar to be treated
as equivalent. The key lies in determining categorical
shape classes whose definitions are invariant to within-
class shape deformation, color, texture, and part articula-
tion. In previous work, we have explored various generic
shape representations, and their application to generic ob-
ject recognition (Siddiqi et al, 1999; Shokoufandeh et al,
2002) and content-based image retrieval (Dickinson et
al., 1998). Here we draw on our previous work, and adopt
a view-based 3-D shape representation, called a shock
graph, that is invariant to minor shape deformation, part
articulation, translation, rotation, and scale, along with
minor rotation in depth.
The vision component consists of a number of
steps. First, the image is segmented into regions, us-
ing the mean-shift region segmentation algorithm of
Comaniciu and Meer (1997).1 The result is a region ad-
jacency graph, in which nodes represent homogeneous
1The results presented in Section 4.2 are based on a syn-
thetic region segmentation. When working with real images,
we plan to use the mean-shift algorithm, although any region
segmentation algorithm could conceivably be used.
Type 3 
Type 1 Type 2
Type 4 
(a) (b)
#
3
?
3
3 3001
002
003 004
1 001 1 1 1 1
1 1 1
002 003 004 005
007 008 009
??? ?
(c)
Figure 1: The Shock Graph Qualitative Shape Represen-
tation: (a) the taxonomy of qualitative shape parts; (b) the
computed shock points of a 2-D closed contour; and (c)
the resulting shock graph.
regions, and edges capture region adjacency. The param-
eters of the segmentation algorithm can be set so that it
typically errs on the side of oversegmentation (regions
may be broken into fragments), although undersegmen-
tation is still possible (regions may be merged incorrectly
with their neighbors). Next, the qualitative shape of each
region is encoded by its shock graph (Siddiqi et al, 1999),
in which nodes represent clusters of skeleton points that
share the same qualitative radius function, and edges rep-
resent adjacent clusters (directed from larger to smaller
average radii). As shown in Figure 1(a), the radius func-
tion may be: 1) monotonically increasing, reflecting a
bump or protrusion; 2) a local minimum, monotonically
increasing on either side of the minimum, reflecting a
neck-like structure; 3) constant, reflecting an elongated
structure; or 4) a local maximum, reflecting a disk-like or
blob-like structure. An example of a 2-D shape, along
with its corresponding shock graph, is shown in Fig-
ures 1(b) and (c).
The set of all regions from all training images are clus-
tered according to a distance function that measures the
similarity of two shock graphs in terms of their structure
and their node attributes. As mentioned above, the key
requirement of our shape representation and distance is
that it be invariant to both within-class shape deforma-
tion as well as image transformation. We have developed
Figure 2: Generic Shape Matching
a matching algorithm for 2-D shape recognition. As illus-
trated in Figure 2, the matcher can compute shock graph
correspondence between different exemplars belonging
to the same class.
During training, regions are compared to region
(shape) class prototypes. If the distance to a prototype is
small, the region is added to the class, and the prototype
recomputed as that region whose sum distance to all other
class members is minimum. However, if the distance to
the nearest prototype is large, a new class and prototype
are created from the region. Using the region adjacency
graph, we can also calculate the probability that two pro-
totypes are adjacent in an image. This is typically a very
large, yet sparse, matrix.
3 Learning of Translation Models
The learning of translation models from a corpus of bilin-
gual text has been extensively studied in computational
linguistics. Probabilistic translation models generally
seek to find the translation string e that maximizes the
probability Pr  e  f  , given the source string f (where f re-
ferred to French and e to English in the original work,
Brown et al, 1993). Using Bayes rule and maximizing
the numerator, the following equation is obtained:
e?  argmax
e
Pr  f  e  Pr  e 
	 (1)
The application of Bayes rule incorporates Pr  e  into the
formula, which takes into account the probability that e? is
a correct English string.
Pr  f  e  is known as the translation model (prediction
of f from e), and Pr  e  as the language model (probabil-
ities over e independent of f). Like others (Duygulu et
al., 2002), we will concentrate on the translation model;
taking f as the words in the text and e as the regions in the
images, we thus predict words from image regions. How-
ever, we see the omission of the language model compo-
nent, Pr  e  (in our case, probabilities over the ?language?
of images?i.e., over ?good? region associations), as a
shortcoming. Indeed, as we see below, we insert some
simple aspects of a ?language model? into our current
formulation, i.e. using the region adjacency graph to re-
strict possible merges, and using the a priori probability
of a region Pr  r  if translating from words to regions. In
future work, we plan to elaborate the Pr  e  component
more thoroughly.
Data sparseness prevents the direct estimation of
Pr  f  e  (which predicts one complete sequence of sym-
bols from another), so practical translation models must
make independence assumptions to reduce the number of
parameters needed to be estimated. The first model of
Brown et al (1993), which will be used and expanded in
our initial formulation, uses the following approximation
to Pr  f  e  :
Pr  f  e  ?
a
Pr  M  ?
j  1    M
Pr  a j  L  Pr  f j  a j  ea j  (2)
where M is the number of French words in f, L is the
number of English words in e, and a is an alignment
that maps each French word to one of the English words,
or to the ?null? word e0. Pr  M  ? is constant and
Pr  a j  L  1  L  1  depends only on the number of En-
glish words. The conditional probability of f j depends
only on its own alignment to an English word, and not on
the translation of other words fi. These assumptions lead
to the following formulation, in which t  f j  ea j  defines a
translation table from English words to French words:
Pr  f  e  ?
 L  1  M ?j  1   M ?a j  0    L t  f j  ea j  (3)
To learn such a translation between image objects and
text passages, it is necessary to: 1) Define the vocabu-
lary of image objects; 2) Extract this vocabulary from
an image; 3) Extract text that describes an image ob-
ject; 4) Deal with multiple word descriptions of ob-
jects; and 5) Deal with compound objects consisting of
parts. Duygulu et al (2002) assume that all words (more
specifically, all nouns) are possible names of objects.
Each segmented region in an image is characterized by
a 33-dimensional feature vector. The vocabulary of im-
age objects is defined by a vector quantization of this
feature space. In the translation model of Brown et al,
Duygulu et al (2002) substitute the French string f by the
sequence w of caption words, and the English string e
by the sequence r of regions extracted from the image
(which they refer to as blobs, b). They do not consider
multiple word sequences describing an image object, nor
image objects that consist of multiple regions (overseg-
mentations or component parts).
In section 2 we argued that many object categories are
better characterized by generic shape descriptions rather
than finite sets of appearance-based features. However,
in moving to a shape-based representation, we need to
deal with image objects consisting of multiple regions
(cf. Barnard et al, 2003). We distinguish three different
types of multiple region sets:
1. Type A (accidental): Region over-segmentation due
to illumination effects or exemplar-specific mark-
ings on the object that results in a collection of sub-
regions that is not generic to the object?s class.
2. Type P (parts): Region over-segmentation common
to many exemplars of a given class that results in
a collection of subregions that may represent mean-
ingful parts of the object class. In this case, it is
assumed that on some occasions, the object is seen
as a silhouette, with no over-segmentation into parts.
3. Type C (compound): Objects that are always seg-
mented into their parts (e.g., due to differently col-
ored or textured parts). This type is similar to Type
P, except that these objects never appear as a whole
silhouette. (Our mechanism for dealing with these
objects will also allow us, in the future, to handle
conventional collections of objects, such as a set of
chairs with a table.)
We can extend the one-to-one translation model in
Eqn. (3) above by grouping or merging symbols (in this
case, regions) and then treating the group as a new sym-
bol to be aligned. Theoretically, then, multiple regions
can be handled in the same translation framework, by
adding to the sequence of regions in each image, the re-
gions resulting from all possible merges of image regions:
Pr  w  r 
?

?L  1  M ?j  1   M ?a j  0    ?L
t  w j  ra j  (4)
where ?L denotes the total number of segmented and
merged regions in an image. However, in practice this
causes complexity and stability problems; the number of
possible merges may be intractable, while the number of
semantically meaningful merges is quite small.
Motivated by the three types of multiple region sets
described above, we have instead developed an iterative
bootstrapping strategy that filters hypothetically mean-
ingful merges and adds these to the data set. Our method
proceeds as follows:
1. As in Dyugulu et al, we calculate a translation
model t0  w  r  between words and regions, using a
data set of N image/caption pairs D  wd  rd  d 
1 		Statistical Measures of the Semi-Productivity of Light Verb Constructions
Suzanne Stevenson and Afsaneh Fazly and Ryan North
Department of Computer Science
University of Toronto
Toronto, Ontario M5S 3G4
Canada
 
suzanne,afsaneh,ryan  @cs.toronto.edu
Abstract
We propose a statistical measure for the degree of
acceptability of light verb constructions, such as
take a walk, based on their linguistic properties. Our
measure shows good correlations with human rat-
ings on unseen test data. Moreover, we find that our
measure correlates more strongly when the poten-
tial complements of the construction (such as walk,
stroll, or run) are separated into semantically similar
classes. Our analysis demonstrates the systematic
nature of the semi-productivity of these construc-
tions.
1 Light Verb Constructions
Much research on multiword expressions involv-
ing verbs has focused on verb-particle constructions
(VPCs), such as scale up or put down (e.g., Bannard
et al, 2003; McCarthy et al, 2003; Villavicencio,
2003). Another kind of verb-based multiword ex-
pression is light verb constructions (LVCs), such as
the examples in (1).
(1) a. Sara took a stroll along the beach.
b. Paul gave a knock on the door.
c. Jamie made a pass to her teammate.
These constructions, like VPCs, may extend the
meaning of the component words in interesting
ways, may be (semi-)productive, and may or may
not be compositional. Interestingly, despite these
shared properties, LVCs are in some sense the oppo-
site of VPCs. Where VPCs involve a wide range of
verbs in combination with a small number of parti-
cles, LVCs involve a small number of verbs in com-
bination with a wide range of co-verbal elements.
An LVC occurs when a light verb, such as take,
give, or make in (1), is used in conjunction with
a complement to form a multiword expression. A
verb used as a light verb can be viewed as drawing
on a subset of its more general semantic features
(Butt, 2003). This entails that most of the distinc-
tive meaning of a (non-idiomatic) LVC comes from
the complement to the light verb. This property can
be seen clearly in the paraphrases of (1) given below
in (2): in each, the complement of the light verb in
(1a?c) contributes the main verb of the correspond-
ing paraphrase.1
(2) a. Sara strolled along the beach.
b. Paul knocked on the door.
c. Jamie passed to her teammate.
The linguistic importance and crosslinguistic fre-
quency of LVCs is well attested (e.g., Butt, 2003;
Folli et al, 2003). Furthermore, LVCs have partic-
ular properties that require special attention within
a computational system. For example, many LVCs
(such as those in (1) above) exhibit composi-
tional and semi-productive patterns, while others
(such as take charge) may be more fixed. Thus,
LVCs present the well-known problem with multi-
word expressions of determining whether and how
they should be listed in a computational lexicon.
Moreover, LVCs are divided into different classes
of constructions, which have distinctive syntactic
and semantic properties (Wierzbicka, 1982; Kearns,
2002). In general, there is no one ?light verb con-
struction? that can be dealt with uniformly in a com-
putational system, as is suggested by Sag et al
(2002), and generally assumed by earlier compu-
tational work on these constructions (Fontenelle,
1993; Grefenstette and Teufel, 1995; Dras and John-
son, 1996). Rather there are different types of
LVCs, each with unique properties.
In our initial computational investigation of light
verb phenomena, we have chosen to focus on a par-
ticular class of semi-productive LVCs in English,
exemplified by such expressions as take a stroll,
take a run, take a walk, etc. Specifically, we in-
vestigate the degree to which we can determine, on
the basis of corpus statistics, which words form a
valid complement to a given light verb in this type
of construction.
1The two expressions differ in aspectual properties. It has
been argued that the usage of a light verb adds a telic compo-
nent to the event in most cases (Wierzbicka, 1982; Butt, 2003);
though see Folli et al (2003) for telicity in Persian LVCs.
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 1-8
Our approach draws on a linguistic analysis, pre-
sented in Section 2, in which the complement of
this type of LVC (e.g., a walk in take a walk) is?in
spite of the presence of the determiner a?actually
a verbal element (Wierzbicka, 1982; Kearns, 2002).
Section 3 describes how this analysis motivates both
a method for generalizing over verb classes to find
potential valid complements for a light verb, and a
mutual information measure that takes the linguis-
tic properties of this type of LVC into account. In
Section 4, we outline how we collect the corpus
statistics on which we base our measures intended
to distinguish ?good? LVCs from poor ones. Sec-
tion 5 describes the experiments in which we deter-
mine human ratings of potential LVCs, and correlate
those with our mutual information measures. As
predicted, the correlations reveal interesting class-
based behaviour among the LVCs. Section 6 ana-
lyzes the relation of our approach to the earlier com-
putational work on LVCs cited above. Our investi-
gation is preliminary, and Section 7 discusses our
current and future research on LVCs.
2 Linguistic Properties of LVCs
An LVC is a multiword expression that combines
a light verb with a complement of type noun, ad-
jective, preposition or verb, as in, respectively, give
a speech, make good (on), take (NP) into account,
or take a walk. The light verb itself is drawn from
a limited set of semantically general verbs; among
the commonly used light verbs in English are take,
give, make, have, and do. LVCs are highly pro-
ductive in some languages, such as Persian, Urdu,
and Japanese (Karimi, 1997; Butt, 2003; Miyamoto,
2000). In languages such as French, Italian, Spanish
and English, LVCs are semi-productive construc-
tions (Wierzbicka, 1982; Alba-Salas, 2002; Kearns,
2002).
The syntactic and semantic properties of the com-
plement of an LVC determine distinct types of con-
structions. Kearns (2002) distinguishes between
two usages of light verbs in LVCs: what she calls
a true light verb (TLV), as in give a groan, and
what she calls a vague action verb (VAV), as in
give a speech. The main difference between these
two types of light verb usages is that the comple-
ment of a TLV is claimed to be headed by a verb.
Wierzbicka (1982) argues that although the com-
plement in such constructions might appear to be
a zero-derived nominal, its syntactic category when
used in an LVC is actually a verb, as indicated by
the properties of such TLV constructions. For exam-
ple, Kearns (2002) shows that, in contrast to VAVs,
the complement of a TLV usually cannot be definite
(3), nor can it be the surface subject of a passive
construction (4) or a fronted wh-element (5).
(3) a. Jan gave the speech just now.
b. * Jan gave the groan just now.
(4) a. A speech was given by Jan.
b. * A groan was given by Jan.
(5) a. Which speech did Jan give?
b. * Which groan did Jan give?
Because of their interesting and distinctive prop-
erties, we have restricted our initial investigation to
light verb constructions with TLVs, i.e. ?LV a V?
constructions, as in give a groan. For simplicity,
we will continue to refer to them here generally as
LVCs. The meaning of an LVC of this type is almost
equivalent to the meaning of the verbal complement
(cf. (1) and (2) in Section 1). However, the light
verb does contribute to the meaning of the construc-
tion, as can be seen by the fact that there are con-
straints on which light verb can occur with which
complement (Wierzbicka, 1982). For example, one
can give a cry but not *take a cry. The acceptability
depends on semantic properties of the complement,
and, as we explore below, may generalize in consis-
tent ways across semantically similar (complement)
verbs, as in give a cry, give a moan, give a howl;
*take a cry, *take a moan, *take a howl.
Many interesting questions pertaining to the syn-
tactic and semantic properties of LVCs have been
examined in the linguistic literature: How does the
semantics of an LVC relate to the semantics of its
parts? How does the type of the complement affect
the meaning of an LVC? Why do certain light verbs
select for certain complements? What underlies the
(semi-)productivity of the creation of LVCs?
Given the crosslinguistic frequency of LVCs,
work on computational lexicons will depend heav-
ily on the answers to these questions. We also be-
lieve that computational investigation can help to
precisely answer the questions as well, by using sta-
tistical corpus-based analysis to explore the range
and properties of these constructions. While details
of the underlying semantic representation of LVCs
are beyond the scope of this paper, we address the
questions of their semi-productivity.
3 Our Proposal
The initial goal in our investigation of semi-
productivity is to find a means for determining how
well particular light verbs and complements go to-
gether. We focus on the ?LV a V? constructions be-
cause we are interested in the hypothesis that the
complement to the LV is a verb, and think that the
properties of this construction may place interesting
restrictions on what forms a valid LVC.
3.1 Generalizing over Verb Classes
As noted above, there are constraints in an ?LV a
V? construction on which complements can occur
with particular light verbs. Moreover, similar po-
tential complements pattern alike in this regard?
that is, semantically similar complements may have
the same pattern of co-occurrence across different
light verbs. Since the complement is hypothesized
to be a verbal element, we look to verb classes to
capture the relevant semantic similarity. The lexical
semantic classes of Levin (1993) have been used as
a standard verb classification within the computa-
tional linguistics community. We thus propose us-
ing these classes as the semantically similar groups
over which to compare acceptability of potential
complements with a given light verb.2
Our approach is related to the idea of substi-
tutability in multiword expressions. Substituting
pieces of a multiword expression with semantically
similar words from a thesaurus can be used to deter-
mine productivity?higher degree of substitutabil-
ity indicating higher productivity (Lin, 1999; Mc-
Carthy et al, 2003).3 Instead of using a thesaurus-
based measure, Villavicencio (2003) uses substi-
tutability over semantic verb classes to determine
potential verb-particle combinations.
Our method is somewhat different from these ear-
lier approaches, not only in focusing on LVCs, but
in the precise goal. While Villavicencio (2003) uses
verb classes to generalize over verbs and then con-
firms whether an expression is attested, we seek to
determine how good an expression is. Specifically,
we aim to develop a computational approach not
only for characterizing the set of complements that
can occur with a given light verb in these LVCs, but
also to quantify the acceptability.
In investigating light verbs and their combina-
tion with complements from various verb semantic
classes, we expect that these LVCs are not fully id-
iosyncratic, but exhibit systematic behaviour. Most
importantly, we hypothesize that they show class-
based behaviour?i.e., that the same light verb will
show distinct patterns of acceptability with comple-
ments across different verb classes. We also ex-
2We also need to compare generalizability over semantic
noun classes to further test the linguistic hypothesis. We ini-
tially performed such experiments on noun classes in Word-
Net, but, due to the difficulty of deciding an appropriate level
of generalization in the hierarchy, we left this as future work.
3Note that although Lin characterizes his work as detecting
non-compositionality, we agree with Bannard et al (2003) that
it is better thought of as tapping into productivity.
plore whether the light verbs themselves show dif-
ferent patterns in terms of how they are used semi-
productively in these constructions.
We choose to focus on the light verbs take, give,
and make. We choose take and give because they
seem similar in their ability to occur in a range of
LVCs, and yet they have almost the opposite se-
mantics. We hope that the latter will reveal inter-
esting patterns in occurrence with the different verb
classes. On the other hand, make seems very dif-
ferent from both take and give. It seems much less
restrictive in its combinations, and also seems diffi-
cult to distinguish in terms of light versus ?heavy?
uses. We expect it to show different generalization
behaviour from the other two light verbs.
3.2 Devising an Acceptability Measure
Given the experimental focus, we must devise a
method for determining acceptability of LVCs. One
possibility is to use a standard measure for detect-
ing collocations, such as pointwise mutual informa-
tion (Church et al, 1991). ?LV a V? constructions
are well-suited to collocational analysis, as the light
verb can be seen as the first component of a colloca-
tion, and the string ?a V? as the second component.
Applying this idea to potential LVCs, we calculate
pointwise mutual information, I(lv; aV).
In addition, we use the linguistic properties of
the ?LV a V? construction to develop a more in-
formed measure. As noted in Section 2, generally
only the indefinite determiner a (or an) is allowed
in this type of LVC. We hypothesize then that for a
?good? LVC, we should find a much higher mutual
information value for ?LV a V? than for ?LV [det]
V?, where [det] is any determiner other than the in-
definite. While I(lv; aV) should tell us whether ?LV
a V? is a good collocation (Church et al, 1991), the
difference between the two, I(lv; aV) - I(lv; detV),
should tell us whether the collocation is an LVC.
To summarize, we assume that:
  if I(lv; aV)  0 then
?LV a V? is likely not a good collocation;
  if I(lv; aV) - I(lv; detV)  0 then
?LV a V? is likely not a true LVC.
In order to capture these two conditions in a sin-
gle measure, we combine them by using a linear ap-
proximation to the two lines given by I(lv; aV)  0
and I(lv; aV) - I(lv; detV)  0. The most straight-
forward line approximating the combined effect of
these two conditions is:
2  I(lv; aV) - I(lv; detV)  0
We hypothesize that this combined measure?
i.e., 2  I(lv; aV) - I(lv; detV)?will correlate bet-
Development Classes
Levin # Name Count
10.4.1* Wipe Verbs, Manner 30
17.1 Throw Verbs 30
51.3.2* Run Verbs 30
Test Classes
Levin # Name Count
18.1,2 Hit and Swat Verbs 35
30.3 Peer Verbs 18
43.2* Sound Emission 35
51.4.2 Motion (non-vehicle) 10
Table 1: Levin classes used in our experiments. A
?*? indicates a random subset of verbs in the class.
ter with human ratings of the LVCs than the mutual
information of the ?LV a V? construction alone.
For I(lv; detV), we explore several possible sets
of determiners standing in for ?det?, including the,
this, that, and the possessive determiners. We find,
contrary to the linguistic claim, that the is not al-
ways rare in ?LV a V? constructions, and the mea-
sures excluding the perform best on development
data.4
4 Materials and Methods
4.1 Experimental Classes
Three Levin classes are used for the development
set, and four classes for the test set, as shown in Ta-
ble 1. Each set of classes covers a range of LVC pro-
ductivity with the light verbs take, give, and make,
from classes in which we felt no LVCs were possi-
ble with a given LV, to classes in which many verbs
listed seemed to form valid LVCs with a given LV.
4.2 Corpora
Even the 100M words of the British National Cor-
pus (BNC Reference Guide, 2000) do not give an
acceptable level of LVC coverage: a very common
LVC such as take a stroll, for instance, is attested
only 23 times. To ensure sufficient data to detect
less common LVCs, we instead use the Web as our
corpus (in particular, the subsection indexed by the
Google search engine, http://www.google.com).
Using the Web to overcome data sparseness has
been attempted before (Keller et al, 2002); how-
ever, there are issues: misspellings, typographic er-
rors, and pages in other languages all contribute to
noise in the results. Moreover, punctuation is ig-
4Cf. I took the hike that was recommended. This finding
supports a statistical corpus-based approach to LVCs, as their
usage may be more nuanced than linguistic theory suggests.
Determiner Search Strings
Indefinite give/gives/gave a cry
Definite give/gives/gave the cry
Demons. give/gives/gave this/that cry
Possessive give/gives/gave my/.../their cry
Table 2: Searches for light verb give and verb cry.
nored in Google searches, meaning that search re-
sults can cross phrase or sentence boundaries. For
instance, an exact phrase search for ?take a cry?
would return a web page which had the text It was
too much to take. A cry escaped his lips. When
searching for an unattested LVC, these noisy results
can begin to dominate. In ongoing work, we are
devising some automatic clean-up methods to elim-
inate some of the false positives.
On the other hand, it should be pointed out that
not all ?good? LVCs will appear in our corpus, de-
spite its size. In this view we differ from Villavi-
cencio (2003), who assumes that if a multiword ex-
pression is not found in the Google index, then it is
not a good construction. As an example, consider
The clown took a cavort across the stage. The LVC
seems plausible; however, Google returns no results
for ?took a cavort?. This underlines the need for de-
termining plausible (as opposed to attested) LVCs,
which class-based generalization has the potential
to support.
4.3 Extraction
To measure mutual information, we gather several
counts for each potential LVC: the frequency of the
LVC (e.g., give a cry), the frequency of the light
verb (e.g., give), and the frequency of the comple-
ment of the LVC (e.g., a cry). To achieve broader
coverage, counts of the light verbs and the LVCs
are collapsed across three tenses: the base form, the
present, and the simple past. Since we are interested
in the differences across determiners, we search for
both the LVC (?give [det] cry?) and the complement
alone (?[det] cry?) using all singular determiners.
Thus, for each LVC, we require a number of LVC
searches, as exemplified in Table 2, and analogous
searches for ?[det] V?.
All searches were performed using an exact string
search in Google, during a 24-hour period in March,
2004. The number of results returned is used as the
frequency count. Note that this is an underestimate,
since an LVC may occur than once in a single web
page; however, examining each document to count
the actual occurrences is infeasible, given the num-
ber of possible results. The size of the corpus (also
needed in calculating our measures) is estimated at
5.6 billion, the number of hits returned in a search
for ?the?. This is also surely an underestimate, but
is consistent with our other frequency counts.
NSP is used to calculate pointwise mutual in-
formation over the counts (Banerjee and Pedersen,
2003).
5 Experimental Results
In these initial experiments, we compare human rat-
ings of the target LVCs to several mutual informa-
tion measures over our corpus counts, using Spear-
man rank correlation. We have two goals: to see
whether these LVCs show differing behaviour ac-
cording to the light verb and/or the verb class of
the complement, and to determine whether we can
indeed predict acceptability from corpus statistics.
We first describe the human ratings, then the corre-
lation results on our development and test data.
5.1 Human Ratings
We use pilot results in which two native speakers
of English rated each combination of ?LV a V? in
terms of acceptability. For the development classes,
we used integer ratings of 1 (unacceptable) to 5
(completely natural), allowing for ?in-between? rat-
ings as well, such as 2.5. For the test classes, we set
the top rating at 4, since we found that ratings up to
5 covered a larger range than seemed natural. The
test ratings yielded linearly weighted Kappa values
of .72, .39, and .44, for take, give, and make, respec-
tively, and .53 overall.5
To determine a consensus rating, the human raters
first discussed disagreements of more than one rat-
ing point. In the test data, this led to 6% of the rat-
ings being changed. (Note that this is 6% of ratings,
not 6% of verbs; fewer verbs were changed, since
for some verbs both raters changed their rating after
discussion.) We then simply averaged each pair of
ratings to yield a single consensus rating for each
item.
In order to see differences in human ratings
across the light verbs and the semantic classes of
their complements, we put the (consensus) human
ratings in bins of low (ratings   2) , medium (rat-
ings  2,   3), and high (ratings  3). (Even a
score of 2 meant that an LVC was ?ok?.) Table 3
shows the distribution of medium and high scores
for each of the light verbs and test classes. We can
see that some classes generally allow more LVCs
5Agreement on the development set was much lower (lin-
early weighted Kappa values of .37, .23, and .56, for take, give,
and make, respectively, and .38 overall), due to differences in
interpretation of the ratings. Discussion of these issues by the
raters led to more consistency in test data ratings.
Class # N take give make
18.1,2 35 8 (23%) 15 (43%) 8 (23%)
30.3 18 5 (28%) 5 (28%) 3 (17%)
43.2 35 1 (3%) 11 (31%) 9 (26%)
51.4.2 10 7 (70%) 2 (20%) 1 (10%)
Table 3: Number of medium and high scores for
each LV and class. N is the number of test verbs.
across the light verbs (e.g., 18.1,2) than others (e.g,
43.2). Furthermore, the light verbs show very differ-
ent patterns of acceptability for different classes?
e.g., give is fairly good with 43.2, while take is very
bad, and the pattern is reversed for 51.4.2. In gen-
eral, give allows more LVCs on the test classes than
do the other two light verbs.
5.2 Correlations with Statistical Measures
Our next step is to see whether the ratings, and the
patterns across light verbs and classes, are reflected
in the statistical measures over corpus data. Because
our human ratings are not normally distributed (gen-
erally having a high proportion of values less than
2), we use the Spearman rank correlation coefficient
 to compare the consensus ratings to the mutual in-
formation measures.6
As described in Section 3.2, we use pointwise
mutual information over the ?LV a V? string, as well
as measures we developed that incorporate the lin-
guistic observation that these LVCs typically do not
occur with definite determiners. On our develop-
ment set, we tested several of these measures and
found that the following had the best correlations
with human ratings:
  MI: I(lv; aV)
  DiffAll: 2  I(lv; aV) - I(lv; detV)
where I(lv; detV) is the mutual information over
strings ?LV [det] V?, and det is any determiner other
than a, an, or the. Note that DiffAll is the most
general of our combined measures; however, some
verbs are not detected with other determiners, and
thus DiffAll may apply to a smaller number of items
than MI.
We focus on the analysis of these two measures
on test data, but the general patterns are the same
6Experiments on the development set to determine a thresh-
old on the different measures to classify LVCs as good or not
showed promise in their coarse match with human judgments.
However, we set this work aside for now, since the correlation
coefficients are more informative regarding the fine-grained
match of the measures to human ratings, which cover a fairly
wide range of acceptability.
MI DiffAll
LV Class #  (  ) N  (  ) N
18.1,2 .52 (   .01) 34 .51 (   .01) 33
30.3 .53 (.02) 18 .59 (.02) 15
take 43.2 .24 (.20) 31 .32 (.10) 27
51.4.2 .68 (.03) 10 .65 (.04) 10
all .53 (   .01) 93 .52 (   .01) 85
18.1,2 .26 (.14) 33 .30 (.10) 32
30.3 .33 (.20) 17 .27 (.33) 15
give 43.2 .38 (.03) 33 .58 (   .01) 25
51.4.2 .09 (.79) 10 -.13 (.71) 10
all .28 (.01) 93 .33 (   .01) 82
18.1,2 .51 (   .01) 34 .49 (   .01) 34
30.3 .16 (.52) 18 -.11 (.68) 17
make 43.2 -.12 (.52) 34 -.19 (.29) 33
51.4.2 -.08 (.81) 10 -.20 (.58) 10
all .36 (   .01) 96 .26 (.01) 94
Table 4: Spearman rank correlation coefficents  , with   values and number of items N, between the mutual
information measures and the consensus human ratings, on unseen test data.
on the development set. Table 4 shows the correla-
tion results on our unseen test LVCs. We get rea-
sonably good correlations with the human ratings
across a number of the light verbs and classes, indi-
cating that these measures may be helpful in deter-
mining which light verb plus complement combina-
tions form valid LVCs. In what follows, we examine
more detailed patterns, to better analyze the data.
First, comparing the test correlations to Table 3,
we find that the classes with a low number of ?good?
LVCs have poor correlations. When we examine the
correlation graphs, we see that, in general, there is
a good correlation between the ratings greater than
1 and the corresponding measure, but when the rat-
ing is 1, there is often a wide range of values for the
corpus-based measure. One cause could be noise
in the data, as mentioned earlier?that is, for bad
LVCs, we are picking up too many ?false hits?, due
to the limitations of using Google searches on the
web. To confirm this, we examine one develop-
ment class (10.4.1, the Wipe manner verbs), which
was expected to be bad with take. We find a large
number of hits for ?take a V? that are not good
LVCs, such as ?take a strip [of tape/of paper]?, ?take
a pluck[-and-play approach]?. On the other hand,
some examples with unexpectedly high corpus mea-
sures are LVCs the human raters were simply not
aware of (?take a skim through the manual?), which
underscores the difficulty of human rating of a semi-
productive construction.
Second, we note that we get very good cor-
relations with take, somewhat less good correla-
tions with give, and generally poor correlations with
make. We had predicted that take and give would
behave similarly (and the difference between take
and give is less pronounced in the development
data). We think one reason give has poorer correla-
tions is that it was harder to rate (it had the highest
proportion of disagreements), and so the human rat-
ings may not be as consistent as for take. Also, for a
class like 30.3, which we expected to be good with
give (e.g., give a look, give a glance), we found that
the LVCs were mostly good only in the dative form
(e.g., give her a look, give it a glance). Since we
only looked for exact matches to ?LV a V?, we did
not detect this kind of construction.
We had predicted that make would behave dif-
ferently from take and give, and indeed, except in
one case, the correlations for make are poorer on
the individual classes. Interestingly, the correlation
overall attains a much better value using the mutual
information of ?LV a V? alone (i.e., the MI mea-
sure). We think that the pattern of correlations with
make may be because it is not necessarily a ?true
light verb? construction in many cases, but rather a
?vague action verb? (see Section 2). If so, its be-
haviour across the complements may be somewhat
more arbitrary, combining different uses.
Finally, we compare the combined measure Diff-
All to the mutual information, MI, alone. We hy-
pothesized that while the latter should indicate a
collocation, the combined measure should help to
focus on LVCs in particular, because of their lin-
guistic property of occurring primarily with an in-
definite determiner. On the individual classes, when
considering correlations that are statistically signif-
icant or marginally so (i.e., at the confidence level
of 90%), the DiffAll measure overall has somewhat
stronger correlations than MI. Over all complement
verbs together, DiffAll is roughly the same as MI
for take; is somewhat better for give, and is worse
for make.7
Better performance over the individual classes in-
dicates that when applying the measures, at least to
take and give, it is helpful to separate the data ac-
cording to semantic verb class. For make, the ap-
propriate approach is not as clear, since the results
on the individual classes are so skewed. In gen-
eral, the results confirm our hypothesis that seman-
tic verb classes are highly relevant to measuring the
acceptability of LVCs of this type. The results also
indicate the need to look in more detail at the prop-
erties of different light verbs.
6 Related Work
Other computational research on LVCs differs from
ours in two key aspects. First, the work has looked
at any nominalizations as complements of poten-
tial light verbs (what they term ?support verbs?)
(Fontenelle, 1993; Grefenstette and Teufel, 1995;
Dras and Johnson, 1996). Our work differs in fo-
cusing on verbal nouns that form the complement
of a particular type of LVC, allowing us to explore
the role of class information in restricting the com-
plements of these constructions. Second, this earlier
work has viewed all verbs as possible light verbs,
while we look at only the class of potential light
verbs identified by linguistic theory.
The difference in focus on these two aspects of
the problem leads to the basic differences in ap-
proach: while they attempt to find probable light
verbs for nominalization complements, we try to
find possible (verbal) noun complements for given
light verbs. Our work differs both practically, in the
type of measure used, and conceptually, in the for-
mulation of the problem. For example, Grefenstette
and Teufel (1995) used some linguistic properties to
weed out potential light verbs from lists sorted by
raw frequency, while Dras and Johnson (1996) used
frequency of the verb weighted by a weak predictor
of its prior probability as a light verb. We instead
use a standard collocation detection measure (mu-
tual information), the terms of which we modify to
7The development data is similar to the test data in favour-
ing DiffAll over MI across the individual classes. Over all de-
velopment verbs together, DiffAll is somewhat better than MI
for take, is roughly the same for give, and is somewhat worse
for make.
capture linguistic properties of the construction.
More fundamentally, our proposal differs in its
emphasis on possible class-based generalizations
in LVCs that have heretofore been unexplored. It
would be interesting to apply this idea to the broader
classes of nominalizations investigated in earlier
work. Moreover, our approach could draw on ideas
from the earlier proposals to detect the light verbs
automatically, since the precise set of LVs differs
crosslinguistically?and LV status may indeed be a
continuum rather than a discrete distinction.
7 Conclusions and Future Work
Our results demonstrate the benefit of treating LVCs
as more than just a simple collocation. We exploit
linguistic knowledge particular to the ?LV a V? con-
struction to devise an acceptability measure that cor-
relates reasonably well with human judgments. By
comparing the mutual information with indefinite
and definite determiners, we use syntactic patterns
to tap into the distinctive underlying properties of
the construction.
Furthermore, we hypothesized that, because the
complement in these constructions is a verb, we
would see systematic behaviour across the light
verbs in terms of their ability to combine with com-
plements from different verb classes. Our human
ratings indeed showed class-based tendencies for
the light verbs. Moreover, our acceptability measure
showed higher correlations when the verbs were di-
vided by class. This indicates that there is greater
consistency within a verb class between the cor-
pus statistics and the ability to combine with a light
verb. Thus, the semantic classes provide a useful
way to increase the performance of the acceptabil-
ity measure.
The correlations are far from perfect, however. In
addition to noise in the data, one problem may be
that these classes are too coarse-grained. Explo-
ration is needed of other possible verb (and noun)
classes as the basis for generalizing the comple-
ments of these constructions. However, we must
also look to the measures themselves for improv-
ing our techniques. Several linguistic properties dis-
tinguish these constructions, but our measures only
drew on one. In ongoing work, we are explor-
ing methods for incorporating other linguistic be-
haviours into a measure for these constructions, as
well as for LVCs more generally.
We are widening this investigation in other direc-
tions as well. Our results reveal interesting differ-
ences among the light verbs, indicating that the set
of light verbs is itself heterogeneous. More research
is needed to determine the properties of a broader
range of light verbs, and how they influence the
valid combinations they form with semantic classes.
Finally, we plan to collect more extensive rating
data, but are concerned with the difficulty found in
judging these constructions. Gathering solid human
ratings is a challenge in this line of investigation, but
this only serves to underscore the importance of de-
vising corpus-based acceptability measures in order
to better support development of accurate computa-
tional lexicons.
Acknowledgments
We thank Ted Pedersen (U. of Minnesota), Diana
Inkpen (U. of Ottawa), and Diane Massam (U. of
Toronto) for helpful advice and discussion, as well
as three anonymous reviewers for their useful feed-
back. We gratefully acknowledge the support of
NSERC of Canada.
References
J. Alba-Salas. 2002. Light Verb Constructions in
Romance: A Syntactic Analysis. Ph.D. thesis,
Cornell University.
S. Banerjee and T. Pedersen. 2003. The design,
implementation, and use of the Ngram Statistic
Package. In Proceedings of the Fourth Interna-
tional Conference on Intelligent Text Processing
and Computational Linguistics.
C. Bannard, T. Baldwin, and A. Lascarides. 2003.
A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL-2003 Work-
shop on Multiword Expressions: Analysis, Acqui-
sition and Treatment, p. 65?72.
BNC Reference Guide. 2000. Reference Guide
for the British National Corpus (World Edition).
http://www.hcu.ox.ac.uk/BNC, second edition.
M. Butt. 2003. The light verb jungle.
http://www.ai.mit.edu/people/jimmylin/papers
/Butt03.pdf.
K. Church, W. Gale, P. Hanks, and D. Hindle. 1991.
Using Statistics in Lexical Analysis, p. 115?164.
Lawrence Erlbaum.
M. Dras and M. Johnson. 1996. Death and light-
ness: Using a demographic model to find support
verbs. In Proceedings of the Fifth International
Conference on the Cognitive Science of Natural
Language Processing, Dublin, Ireland.
R. Folli, H. Harley, and S. Karimi. 2003. Determi-
nants of event type in Persian complex predicates.
Cambridge Working Papers in Linguistics.
T. Fontenelle. 1993. Using a bilingual computerized
dictionary to retrieve support verbs and combina-
torial information. Acta Linguistica Hungarica,
41(1?4):109?121.
G. Grefenstette and S. Teufel. 1995. A corpus-
based method for automatic identification of sup-
port verbs for nominalisations. In Proceedings of
EACL, p. 98?103, Dublin, Ireland.
S. Karimi. 1997. Persian complex verbs: Idiomatic
or compositional? Lexicology, 3(1):273?318.
K. Kearns. 2002. Light verbs in En-
glish. http://www.ling.canterbury.ac.nz/kate
/lightverbs.pdf.
F. Keller, M. Lapata, and O. Ourioupina. 2002. Us-
ing the Web to overcome data sparseness. In
Proceedings of the 2002 Conference on Empiri-
cal Methods in Natural Language Processing, p.
230?237, Philadelphia, USA.
B. Levin. 1993. English Verb Classes and Alterna-
tions, A Preliminary Investigation. University of
Chicago Press.
D. Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL-
99, p. 317?324.
D. McCarthy, B. Keller, and J. Carroll. 2003.
Detecting a continuum of compositionality in
phrasal verbs. In Proceedings of the ACL-
SIGLEX Workshop on Multiword Expressions:
Analysis, Acquisition and Treatment.
T. Miyamoto. 2000. The Light Verb Construction
in Japanese: the role of the verbal noun. John
Benjamins.
I. Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2002. Multiword expressions: A
pain in the neck for NLP. In Proceedings of
the Third International Conference on Intelligent
Text Processing and Computational Linguistics
(CICLING), p. 1?15.
A. Villavicencio. 2003. Verb-particle constructions
in the world wide web. In Proceedings of the
ACL-SIGSEM Workshop on the Linguistic Di-
mensions of Prepositions and their use in Com-
putational Linguistics Formalisms and Applica-
tions.
A. Wierzbicka. 1982. Why can you Have a Drink
when you can?t *Have an Eat? Language,
58(4):753?799.
Calculating Semantic Distance between Word Sense Probability
Distributions
Vivian Tsang and Suzanne Stevenson
Department of Computer Science
University of Toronto
 
vyctsang,suzanne  @cs.toronto.edu
Abstract
Semantic similarity measures have focused on
individual word senses. However, in many ap-
plications, it may be informative to compare
the overall sense distributions for two differ-
ent contexts. We propose a new method for
comparing two probability distributions over
WordNet, which captures in a single measure
the aggregate semantic distance of the com-
ponent nodes, weighted by their probability.
Previous such measures compute only the dis-
tributional distance, and do not take into ac-
count the semantic similarity between Word-
Net senses across the distributions. To in-
corporate semantic similarity, we calculate the
(dis)similarity between two probability distri-
butions as a weighted distance ?travelled? from
one to the other through the WordNet hierar-
chy. We evaluate the measure by applying it
to the acquisition of verb argument alternation
knowledge, and find that overall it outperforms
existing distance measures.
1 Introduction
Much attention has recently been given to calculating the
similarity of word senses, in support of various natu-
ral language learning and processing tasks. Such tech-
niques apply within a semantic hierarchy, or ontology,
such as WordNet. Typical methods comprise an edge-
distance measurement over the two sense nodes being
compared within the hierarchy (Leacock and Chodorow,
1998; Rada et al, 1989; Wu and Palmer, 1994). Other
approaches instead assume a probability distribution over
the entire sense hierarchy; similarity is captured between
individual senses by a formula over the information con-
tent (negative log probabilities) of relevant nodes (e.g.,
Jiang and Conrath, 1997; Lin, 1998).
The latter case assumes that there is a single WordNet
probability distribution of interest, which is estimated by
populating the hierarchy with word frequencies from an
appropriate corpus (e.g, Jiang and Conrath, 1997). But
some problems more naturally give rise to multiple con-
ditional probability distributions estimated from counts
that are conditioned on various contexts, such as differ-
ent corpora or differing word usage within a single cor-
pus. Each of these contexts would yield a distinct Word-
Net probability distribution, or what we will call a sense
profile. In this situation, instead of asking how similar are
two senses within a single sense profile, one may want to
know how similar are two sense profiles?i.e., two (con-
ditional) distributions across the entire set of nodes.
This question could be important to a number of ap-
plications. When two sets of WordNet frequency counts
are conditioned on differing contexts, a comparison of the
resulting probability distributions can give us a measure
of the degree of semantic similarity of the conditioning
contexts themselves. These conditioning contexts may be
any relevant ones defined by the application, such as dif-
fering sets of documents (to support asking how similar
various document collections are), or differing usages of
words within or across document collections (to support
asking questions about the similarity of various words in
their usages). For example, we foresee comparing the
sense profile of the objects of some verb in a particular
set of documents to that of its objects in another set of
documents, as an indicator of differing senses of the verb
across the collections.
We have developed a general method for answering
such questions, formulating a measure of the distance
between probability distributions defined over an onto-
logical hierarchy, which we call ?sense profile distance,?
or SPD. SPD is calculated as a tree distance that aggre-
gates the individual semantic distances between nodes
in the hierarchy, weighted by their probability in the
two sense profiles. SPD can be calculated between two
probability distributions over any hierarchy that supports
a user-supplied semantic distance function. (In fact,
the two sense profiles need not strictly be probability
distributions?the measure is well-defined as long as the
sum of the values of the two sense profiles is equal.)
We demonstrate our method on a problem that arises
in lexical acquisition, of determining whether two differ-
ent argument positions across syntactic usages of a verb
are assigned the same semantic role. For example, even
though the truck shows up in two different syntactic po-
sitions, it is the Destination of the action in both of the
sentences I loaded the truck with hay and I loaded hay
onto the truck. Automatic detection of such argument
alternations is important to acquisition of verb lexical se-
mantics (Dang et al, 2000; Dorr and Jones, 2000; Merlo
and Stevenson, 2001; Schulte im Walde and Brew, 2002;
Tsang et al, 2002), and moreover, may play a role in au-
tomatic processing of language for applied tasks, such as
question-answering (Katz et al, 2001), information ex-
traction (Riloff and Schmelzenbach, 1998), detection of
text relations (Teufel, 1999), and determination of verb-
particle constructions (Bannard, 2002). We focus on this
problem to illustrate how our general method works, and
how it aids in a particular natural language learning task.
As in McCarthy (2000), we cast argument alternation
detection as a comparison of sense profiles across two dif-
ferent argument positions of a verb. Our method differs,
however, in two important respects. First, our measure
can be used on any probability distribution, while Mc-
Carthy?s approach applies only to a very narrow form of
sense profile known as a tree cut.1 The dependence on
tree cuts greatly limits the applicability of her measure
in both this and other problems, since only a particular
method can be used for populating the WordNet hierar-
chy with probability estimates. Second, our approach
provides a much finer-grained measure of the distance
between the two profiles. McCarthy?s method rewards
probability mass that occurs in the same subtree across
two distributions, but does not take into account the dis-
tance between the classes that carry the probability mass.
Our new SPD method integrates a comparison of prob-
ability distributions over WordNet with a node distance
measure. SPD thus enables us to calculate a more de-
tailed comparison over the probability patterns of Word-
Net classes. As our results indicate, this has advantages
for argument alternation detection, but more importantly,
we think it is crucial for generalizing the method to a
wider range of problems.
1A tree cut for tree T is a set of nodes C in T such that every
leaf node of T has exactly one member of C on a path between
it and the root (Li and Abe, 1998). As a sense profile, a tree cut
will have a non-zero probability associated with every node in
C, and a zero probability for all other nodes in T. Figure 1 in
Section 3 has examples of two tree cuts.
In the next section, we present background work on
comparing sense profiles, and on using them to detect
alternations. In Section 3, we describe our new SPD
measure, and show how it captures both the general dif-
ferences between WordNet probability distributions, as
well as the fine-grained semantic distances between the
nodes that comprise them. Section 4 presents our corpus
methodology and experimental set-up. In Section 5, we
evaluate SPD against other distance measures, and evalu-
ate the different effects of our experimental factors, such
as the precise distance functions we use in SPD and the
division of our verbs into frequency bands. By classify-
ing the frequency bands separately, our method achieves
a combined accuracy of 70% overall on unseen test verbs,
in a task with a baseline of 50%. We summarize our find-
ings in Section 6 and point to directions in our on-going
work.
2 Related Work
Our method draws on, and extends, earlier work in verb
lexical semantics (Resnik, 1993; McCarthy, 2000). For
example, Resnik (1993) uses relative entropy (KL diver-
gence) to compare the sense profile over the objects of
a verb to the profile over the objects of all verbs, to de-
termine how much that verb differs from ?average? in its
strength of selection for an object. A drawback of this
approach for generalizing to other sense profile compar-
isons is the assumption in relative entropy of an asymme-
try between the two probability distributions.
Similarly, McCarthy (2000) uses skew divergence (a
variant of KL divergence proposed by Lee, 1999) to com-
pare the sense profile of one argument of a verb (e.g., the
subject position of the intransitive) to another argument
of the same verb (e.g., the object position of the transi-
tive), to determine if the verb participates in an argument
alternation involving the two positions. For example, the
causative alternation in sentences (1) and (2) illustrates
how the subject of the intransitive is the same underly-
ing semantic argument (i.e., the Theme?the argument
undergoing the action) as the object of the transitive:
(1) The snow melted.
(2) The sun melted the snow.
Because we demonstrate our new SPD measure on the
same problem as McCarthy (2000), we provide more de-
tail of her method here, for comparison. The first step
is to create the sense profiles for the relevant verb/slot
pairs (e.g., the intransitive subject of melt, and the tran-
sitive object of melt, if determining whether melt under-
goes the causative alternation, as illustrated above). The
head nouns are extracted from the syntactic slots to be
compared for each verb, yielding the frequency of each
noun for a verb/slot pair, which is then used to populate
the WordNet hierarchy. McCarthy determines the sense
profile of a verb/slot pair using a minimum description
length tree cut model over the frequency-populated hier-
archy (Li and Abe, 1998). The two profiles for a verb are
?aligned? to permit comparison using skew divergence as
a probability distance measure Lee (1999). (This step is
explained in more detail in the next section, with an ex-
ample.) The value of the distance measure is compared to
a threshold, which determines classification of a verb as
causative (the two profiles are similar) or non-causative
(the two profiles are dissimilar), leading to best perfor-
mance of 73% accuracy, on a set of hand-selected verbs.
In McCarthy (2000), an error analysis reveals that
the best method has more false positives than false
negatives?some slots are considered overly similar be-
cause the sense profiles are compared at a coarse-grained
level, losing fine semantic distinctions. Moreover, as
mentioned above, the method can only apply to tree-cuts,
which restricts its use to a very narrow range of sense
profile comparisons.
In the next section, we propose an alternative method
of comparing sense profiles, which addresses each of the
shortcomings of these previous measures.
3 Sense Profile Distance
Our measure of sense profile distance (SPD) is designed
to meet three criteria. First, it should capture fine-grained
semantic similarity between profiles. Second, it should
allow easy comparison between any sense profiles as
probability scores spread throughout a hierarchical on-
tology (such as WordNet), not just between a particular
format such as tree cuts. Third, it should be a symmet-
ric measure, making it more appropriate for a wide range
of applications of sense profile comparison. To achieve
these goals, we measure the distance as a tree distance
between the two profiles within the hierarchy, weighted
by the probability scores.
(Note that we formulate a distance measure, while re-
ferring to a component of semantic similarity. We assume
throughout the paper that WordNet node distance is the
inverse of WordNet similarity, and indeed the similarity
measures we use are directly invertible.)
We illustrate with an example the differences between
our measure and both McCarthy?s (2000) method and
general vector distance measures. Consider the two sense
profiles in Figure 1, with  
	 in square boxes, and
 	 in ovals.2 To calculate the vector distance be-
tween  
	 and  	 , we need two vectors of equal
dimension. In McCarthy (2000), the distributions are
propagated to the lowest common subsumers (i.e., the
nodes labelled B, C, and D). The vectors representing the
two profiles become:
2Note that these are both tree cuts, so that we can compare
McCarthy?s method, but keep in mind that our method?as well
as traditional vector distances?will apply to any probability
distribution over a tree.
Using Selectional Profile Distance to Detect Verb Alternations
Vivian Tsang and Suzanne Stevenson
Department of Computer Science
University of Toronto
 
vyctsang,suzanne  @cs.toronto.edu
Abstract
We propose a new method for detecting verb al-
ternations, by comparing the probability distri-
butions over WordNet classes occurring in two
potentially alternating argument positions. Ex-
isting distance measures compute only the dis-
tributional distance, and do not take into ac-
count the semantic similarity between Word-
Net senses across the distributions. Our method
compares two probability distributions over
WordNet by measuring the semantic distance
of the component nodes, weighted by their
probability. To incorporate semantic similarity,
we calculate the (dis)similarity between two
probability distributions as a weighted distance
?travelled? from one to the other through the
WordNet hierarchy. We evaluate the measure
on the causative alternation, and find that over-
all it outperforms existing distance measures.
1 Detecting Verb Alternations
Although patterns of verb alternations, as in (1) and (2),
may appear to be ?mere? syntactic variation, the ability
of a verb to alternate has been shown to be highly related
to its semantic properties.
1. The sun melted the snow./The snow melted.
2. Kiva ate his lunch./Kiva ate./*His lunch ate.
For example, melt in (1) undergoes a causative alterna-
tion in which the transitive form is related to the intransi-
tive by the introduction of a Causal Agent (the sun) into
the event structure. The verb eat in (2), like melt, allows
both transitive and intransitive forms, but these are re-
lated by the unspecified object alternation, as opposed to
causativization.
Based largely on the influence of Levin (1993), it has
become widely accepted that alternations such as these
can serve as a basis for the formation of semantic classes
of verbs. Correspondingly, the relation between alter-
nation patterns and meaning is a key focus in the com-
putational study of the lexical semantics of verbs (e.g.,
Allen, 1997; Dang et al, 2000; Dorr and Jones, 2000;
Merlo and Stevenson, 2001; Schulte im Walde and Brew,
2002; Tsang et al, 2002). Furthermore, we note that re-
cent work indicates that verb alternations may also play
a role in automatic processing of language for applied
tasks, such as question-answering (Katz et al, 2001), de-
tection of text relations (Teufel, 1999), and determination
of verb-particle constructions (Bannard, 2002).
The theoretical and practical implications of alterna-
tions mean that it is important to identify verbs which
undergo an alternation, and to discover the range of al-
ternations. Manual annotation of verbs is labour inten-
sive, and new verbs (or new uses of known verbs) may
be encountered in any given domain. In response, some
researchers have begun to investigate ways to detect alter-
nations automatically in a corpus. Some of this work has
focused on subcategorization patterns as the clear syn-
tactic cue to an alternation (Lapata, 1999; Lapata and
Brew, 1999; Schulte im Walde and Brew, 2002). Other
work has observed, however, that detecting an alterna-
tion involves more than observing the use of particular
subcategorizations?it must also be determined whether
the semantic arguments are mapped to the appropriate po-
sitions.1
To address this issue, it has been suggested that, if a
verb participates in an alternation, then there should be
similarity in the kinds of nouns that show up in the syn-
1For example, melt (as in (1) above) undergoes a causative
alternation because the Theme argument that surfaces as subject
of the intransitive surfaces as object of the transitive, with the
addition of a Causal Agent as the subject of the latter. It is
not the case that any optionally intransitive verb undergoes this
alternation, as shown by eat in (2).
tactic positions (or slots) that alternate?such as snow oc-
curring as intransitive subject and transitive object in the
causative alternation in (1) (Merlo and Stevenson, 2001;
McCarthy, 2000). As a cue to this alternation, Merlo and
Stevenson (2001) create a bag of head nouns for each of
the two potentially alternating slots, and compare them.
In contrast to comparing head nouns directly, McCarthy
(2000) instead compares the selectional preferences for
each of the two slots (captured by a probability distribu-
tion over WordNet). This approach thereby generalizes
over the compared nouns, increasing performance over a
method similar to that of Merlo and Stevenson.
In our work, we have developed a new method for
comparing WordNet probability distributions, called ?se-
lectional profile distance? (SPD), which combines the
benefits of each of the above approaches for detecting
alternations. The method used by Merlo and Steven-
son (2001) has the advantage of directly capturing sim-
ilarity between slots (in terms of use of identical nouns
[lemmas]), but fails to generalize over the nouns, lend-
ing itself to sparse data problems. The approach of Mc-
Carthy (2000), on the other hand, addresses the gener-
alization problem by comparing probability distributions
over WordNet. However, her comparison measure ab-
stracts over distances between nodes (classes of nouns)
in WordNet: it rewards probability mass that occurs in
the same subtree across two distributions, but does not
take into account the distance between the classes that
carry the probability mass. Thus, this approach only cap-
tures similarity among the noun arguments across slots
at a very coarse level. Our new SPD method integrates
a comparison of probability distributions over WordNet
with a node similarity measure, successfully capturing
both of the advantageous properties of generalization and
word (class) similarity. SPD thus enables us to calcu-
late a meaningful similarity measure over the patterns of
classes of nouns across two syntactic slots.
Our evaluation of the SPD measure for alternation de-
tection also covers some interesting experimental condi-
tions that have not been explored previously. For com-
parison to previous methods, we investigate these issues
in the context of classifying verbs according to whether
they undergo the causative alternation. We experiment
with randomly selected verbs, for both our alternating and
non-alternating (filler) classes, and use both relatively ho-
mogeneous and heterogeneous sets of filler verbs. We
find that our method performs about the same on each
set, indicating that it is insensitive to variation in the filler
verbs. Moreover, we experiment with equal numbers of
verbs in different frequency bands, and show that split-
ting verbs into high and low frequency (of slot occur-
rence) can improve performance. By classifying the high
and low frequency verbs separately, our method achieves
an accuracy of 70% overall on unseen test verbs, in a
task with a baseline of 50%. (For comparison, McCarthy
(2000) achieves 73% on her set of hand-selected verbs,
but our implementation of her method yields much lower
performance on our randomly selected test verbs.)
In the next section, we present background work on
capturing selectional preferences in WordNet, and on us-
ing them to detect alternations. In Section 3, we describe
our new SPD measure, and show how it captures both
the general differences between WordNet probability dis-
tributions, as well as the fine-grained semantic distances
between the nodes that comprise them. Section 4 presents
our corpus methodology and experimental set-up. In Sec-
tion 5, we compare SPD to a range of distance measures,
and evaluate the different effects of our experimental fac-
tors, such as the precise distance functions we use in SPD
and the division of our verbs into frequency bands. We
summarize our findings in Section 6 and point to direc-
tions in our on-going work.
2 The Use of Selectional Preferences
Selectional preference refers to the general notion of how
much a verb favours (or disfavours) a particular noun as
a semantic argument. For example, informally we would
say that eat has a strong selectional preference for nouns
of type food as its Theme argument. Formalization of
this notion has been difficult, but several computational
methods have now been proposed that capture selectional
preference of a verb as a probability distribution over
the WordNet hierarchy (Resnik, 1993; Li and Abe, 1998;
Clark and Weir, 2002).2 The key task that each of these
proposals address is how to generalize appropriately from
counts of observed nouns in the relevant verb argument
position (in a corpus), to a probabilistic representation of
selectional strength over classes. We will refer in the re-
mainder of the paper to such a probability distribution
over WordNet as a ?selectional profile.?
As mentioned above, McCarthy (2000) suggested the
use of selectional profiles to capture generalizations over
argument slots, so that two argument slots could be ef-
fectively compared for detecting alternations. After ex-
tracting the argument heads of the target slots of each
verb (e.g., the intransitive subject and the transitive object
for the causative alternation), she then determined their
selectional profiles using a minimum description length
tree cut model (Li and Abe, 1998).3 The two slot pro-
files were compared using skew divergence (a variant of
2Resnik?s proposed measure is not actually a probability dis-
tribution, but a difference between probability distributions.
3A tree cut for tree T is a set of nodes C in T such that every
leaf node of T has exactly one member of C on a path between
it and the root. As a selectional profile, a tree cut will have
a non-zero probability associated with every node in C, and a
zero probability for all other nodes in T. Figure 1 below has
examples of two tree cuts.
A0.5 B C 0.2 D 0.3
E F G H I
0.3 0.2 0.2 0.2 0.1
Figure 1: An example of two selectional profiles;  
	
in square boxes, and  	 in ovals. Probability values
of zero are not shown.
KL divergence, proposed by Lee, 2001) as a probability
distance measure. The value of the distance measure was
compared to a threshold, which determined classification
of a verb as causative (the two profiles were similar) or
non-causative (the two profiles were dissimilar), leading
to best performance of 73% accuracy.
In McCarthy (2000), an error analysis reveals that
the best method has more false positives than false
negatives?some slots are considered overly similar be-
cause the selectional profiles are compared at a coarse-
grained level, losing fine semantic distinctions.
In the next section, we propose an alternative method
of comparing selectional profiles, which addresses the
problem of insufficient discrimination of profile similar-
ity in WordNet. Furthermore, the approach applies gener-
ally to any probability distribution over WordNet, unlike
McCarthy?s method which is specific to profiles that are
tree cuts.
3 Selectional Profile Distance
Our measure of selectional profile distance (SPD) is de-
signed to meet two criteria. First, it should allow easy
comparison between selectional profiles as probability
scores spread throughout a hierarchical ontology (such as
WordNet), not just between tree cuts. Second, it should
capture fine-grained semantic similarity between profiles.
To achieve these two goals, we measure the distance as a
tree distance between the two profiles within the hierar-
chy, weighted by the probability scores.
(Note that we formulate a distance measure, while re-
ferring to a component of semantic similarity. We assume
throughout the paper that WordNet distance is the inverse
of WordNet similarity, and indeed the similarity measures
we use are directly invertible.)
We illustrate with an example the differences between
our measure and both McCarthy?s (2000) method and
general vector distance measures. Consider the two selec-
tional profiles in Figure 1, with  	  in square boxes,
and  	 in ovals.4 To calculate the vector distance
between  	 and  	  , we need two vectors of
equal dimension. In this example, one can propagate the
distributions to the lowest common subsumers (i.e., B, C,
and D) as in McCarthy (2000). The vectors representing
the two profiles become:
Unsupervised Semantic Role Labelling
Robert S. Swier and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Ontario, Canada M5S 3G4
 
swier,suzanne  @cs.toronto.edu
Abstract
We present an unsupervised method for labelling
the arguments of verbs with their semantic roles.
Our bootstrapping algorithm makes initial unam-
biguous role assignments, and then iteratively up-
dates the probability model on which future assign-
ments are based. A novel aspect of our approach
is the use of verb, slot, and noun class informa-
tion as the basis for backing off in our probability
model. We achieve 50?65% reduction in the error
rate over an informed baseline, indicating the po-
tential of our approach for a task that has heretofore
relied on large amounts of manually generated train-
ing data.
1 Introduction
Semantic annotation of text corpora is needed to
support tasks such as information extraction and
question-answering (e.g., Riloff and Schmelzen-
bach, 1998; Niu and Hirst, 2004). In particular, la-
belling the semantic roles of the arguments of a verb
(or any predicate), as in (1) and (2), provides crucial
information about the relations among event partic-
ipants.
1. Kiva 
			 admires Mats ffProceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 82?90,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
 
	Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 38?47,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Automatically Distinguishing Literal and Figurative Usages
of Highly Polysemous Verbs
Afsaneh Fazly and Ryan North and Suzanne Stevenson
Department of Computer Science
University of Toronto
 
afsaneh,ryan,suzanne  @cs.toronto.edu
Abstract
We investigate the meaning extensions
of very frequent and highly polysemous
verbs, both in terms of their compositional
contribution to a light verb construction
(LVC), and the patterns of acceptability of
the resulting LVC. We develop composi-
tionality and acceptability measures that
draw on linguistic properties specific to
LVCs, and demonstrate that these statisti-
cal, corpus-based measures correlate well
with human judgments of each property.
1 Introduction
Due to a cognitive priority for concrete, easily visu-
alizable entities, abstract notions are often expressed
in terms of more familiar and concrete things and
situations (Newman, 1996; Nunberg et al, 1994).
This gives rise to a widespread use of metaphor
in language. In particular, certain verbs easily un-
dergo a process of metaphorization and meaning
extension (e.g., Pauwels, 2000; Newman and Rice,
2004). Many such verbs refer to states or acts that
are central to human experience (e.g., sit, put, give);
hence, they are often both highly polysemous and
highly frequent. An important class of verbs prone
to metaphorization are light verbs, on which we fo-
cus in this paper.
A light verb, such as give, take, or make, com-
bines with a wide range of complements from differ-
ent syntactic categories (including nouns, adjectives,
and prepositions) to form a new predicate called a
light verb construction (LVC). Examples of LVCs
include:
1. (a) Azin took a walk along the river.
(b) Sam gave a speech to a few students.
(c) Joan takes care of him when I am away.
(d) They made good on their promise to win.
(e) You should always take this into account.
The light verb component of an LVC is ?seman-
tically bleached? to some degree; consequently, the
semantic content of an LVC is assumed to be de-
termined primarily by the complement (Butt, 2003).
Nevertheless, light verbs exhibit meaning variations
when combined with different complements. For ex-
ample, give in give (someone) a present has a literal
meaning, i.e., ?transfer of possession? of a THING
to a RECIPIENT. In give a speech, give has a figura-
tive meaning: an abstract entity (a speech) is ?trans-
ferred? to the audience, but no ?possession? is in-
volved. In give a groan, the notion of transfer is
even further diminished.
Verbs exhibiting such meaning variations are
widespread in many languages. Hence, successful
NLP applications?especially those requiring some
degree of semantic interpretation?need to identify
and treat them appropriately. While figurative uses
of a light verb are indistinguishable on the surface
from a literal use, this distinction is essential to a
machine translation system, as Table 1 illustrates. It
is therefore important to determine automatic mech-
anisms for distinguishing literal and figurative uses
of light verbs.
Moreover, in their figurative usages, light verbs
tend to have similar patterns of cooccurrence with
semantically similar complements (e.g., Newman,
1996). Each similar group of complement nouns can
even be viewed as a possible meaning extension for
a light verb. For example, in give advice, give or-
ders, give a speech, etc., give contributes a notion of
38
Sentence in English Intermediate semantics Translation in French
Azin gave Sam a book. (e1/give Azin a donne? un livre a` Sam.
:agent (a1/?Azin?) Azin gave a book to Sam.
:theme (b1/?book?)
:recepient (s1/?Sam?))
Azin gave the lasagna a try. (e2/give-a-try   try Azin a essaye? le lasagne.
:agent (a1/?Azin?) Azin tried the lasagna.
:theme (l1/?lasagna?))
Table 1: Sample sentences with literal and figurative usages of give.
?abstract transfer?, while in give a groan, give a cry,
give a moan, etc., give contributes a notion of ?emis-
sion?. There is much debate on whether light verbs
have one highly abstract (underspecified) meaning,
further determined by the context, or a number of
identifiable (related) subsenses (Pustejovsky, 1995;
Newman, 1996). Under either view, it is important
to elucidate the relation between possible interpreta-
tions of a light verb and the sets of complements it
can occur with.
This study is an initial investigation of techniques
for the automatic discovery of meaning extensions
of light verbs in English. As alluded to above, we
focus on two issues: (i) the distinction of literal ver-
sus figurative usages, and (ii) the role of semanti-
cally similar classes of complements in refining the
figurative meanings.
In addressing the first task, we note the connection
between the literal/figurative distinction and the de-
gree to which a light verb contributes composition-
ally to the semantics of an expression. In Section 2,
we elaborate on the syntactic properties that relate
to the compositionality of light verbs, and propose
a statistical measure incorporating these properties,
which places light verb usages on a continuum of
meaning from literal to figurative. Figure 1(a) de-
picts such a continuum in the semantic space of give,
with the literal usages represented as the core.
The second issue above relates to our long-term
goal of dividing the space of figurative uses of a
light verb into semantically coherent segments, as
shown in Figure 1(b). Section 3 describes our hy-
pothesis on the class-based nature of the ability of
potential complements to combine with a light verb.
At this point we cannot spell out the different figura-
tive meanings of the light verb associated with such
classes. We take a preliminary step in proposing a
statistical measure of the acceptability of a combi-
nation of a light verb and a class of complements,
and explore the extent to which this measure can re-
veal class-based behaviour.
Subsequent sections of the paper present the cor-
pus extraction methods for estimating our composi-
tionality and acceptability measures, the collection
of human judgments to which the measures will be
compared, experimental results, and discussion.
2 Compositionality of Light Verbs
2.1 Linguistic Properties: Syntactic Flexibility
We focus on a broadly-documented subclass of light
verb constructions, in which the complement is an
activity noun that is often the main source of seman-
tic predication (Wierzbicka, 1982). Such comple-
ments are assumed to be indefinite, non-referential
predicative nominals (PNs) that are often morpho-
logically related to a verb (see the complements in
examples (1a?c) above). We refer to this class of
light verb constructions as ?LV+PN? constructions,
or simply LVCs.
There is much linguistic evidence that semantic
properties of a lexical item determine, to a large ex-
tent, its syntactic behaviour (e.g., Rappaport Hovav
and Levin, 1998). In particular, the degree of com-
positionality (decomposability) of a multiword ex-
pression has been known to affect its participation
in syntactic transformations, i.e., its syntactic flexi-
bility (e.g., Nunberg et al, 1994). English ?LV+PN?
constructions enforce certain restrictions on the syn-
tactic freedom of their noun components (Kearns,
2002). In some, the noun may be introduced by a
definite article, pluralized, passivized, relativized, or
even wh-questioned:
39
give a book
give a present
give money
give rightgive advice
give opportunity
give orders
give permission
give a speech
give a smile
give a laugh give a yell
give a groan
give a sweep
give a push
give a dust
give a wipe
give a pull
give a kick
more figurative
give a book
give a present
give money
give a wipe
give a sweep
give a dust
give a push
give a kick
give a pull
give orders
give a speech
give advice
give permission
give right
give opportunity
give a yell
give a laugh
give a groan
give a smile
(a) (b)
Figure 1: Two possible partitionings of the semantic space of give.
2. (a) Azin gave a speech to a few students.
(b) Azin gave the speech just now.
(c) Azin gave a couple of speeches last night.
(d) A speech was given by Azin just now.
(e) Which speech did Azin give?
Others have little or no syntactic freedom:
3. (a) Azin gave a groan just now.
(b) * Azin gave the groan just now.
(c) ? Azin gave a couple of groans last night.
(d) * A groan was given by Azin just now.
(e) * Which groan did Azin give?
Recall that give in give a groan is presumed to be
a more abstract usage than give in give a speech. In
general, the degree to which the light verb retains
aspects of its literal meaning?and contributes them
compositionally to the LVC?is reflected in the de-
gree of syntactic freedom exhibited by the LVC. We
exploit this insight to devise a statistical measure of
compositionality, which uses evidence of syntactic
(in)flexibility of a potential LVC to situate it on a
scale of literal to figurative usage of the light verb:
i.e., the more inflexible the expression, the more fig-
urative (less compositional) the meaning.
2.2 A Statistical Measure of Compositionality
Our proposed measure quantifies the degree of syn-
tactic flexibility of a light verb usage by looking
at its frequency of occurrence in any of a set of
relevant syntactic patterns, such as those in exam-
ples (2) and (3). The measure, COMP   LV  N  , as-
signs a score to a given combination of a light verb
(LV) and a noun (N):
COMP
 
LV  N 
ASSOC
 
LV;N 
DIFF
 
ASSOC
 
LV;N  PSpos  ASSOC
 
LV;N  PSneg 
That is, the greater the association between LV and
N, and the greater the difference between their asso-
ciation with positive syntactic patterns and negative
syntactic patterns, the more figurative the meaning
of the light verb, and the higher the score.
The strength of the association between the light
verb and the complement noun is measured using
pointwise mutual information (PMI) whose standard
formula is given here:1
ASSOC
 
LV;N 	 log Pr
 
LV  N 
Pr
 
LV  Pr
 
N 

 log n f
 
LV  N 
f   LV  f   N 
where n is an estimate of the total number of verb
and object noun pairs in the corpus.
1PMI is subject to overestimation for low frequency items
(Dunning, 1993), thus we require a minimum frequency of oc-
currence for the expressions under study.
40
PSpos represents the set of syntactic patterns pre-
ferred by less-compositional (more figurative) LVCs
(e.g., as in (3a)), and PSneg represents less preferred
patterns (e.g., those in (3b?e)). Typically, these pat-
terns most affect the expression of the complement
noun. Thus, to measure the strength of association
between an expression and a set of patterns, we use
the PMI of the light verb, and the complement noun
appearing in all of the patterns in the set, as in:
ASSOC
 
LV;N  PSpos   PMI
 
LV;N  PSpos 
 log
Pr
 
LV  N  PSpos 
Pr
 
LV  Pr
 
N  PSpos 

 log
n f   LV  N  PSpos 
f   LV  f   N  PSpos 
in which counts of occurrences of N in syntactic
contexts represented by PSpos are summed over all
patterns in the set. ASSOC(LV;N  PSneg) is defined
analogously using PSneg in place of PSpos.
DIFF measures the difference between the asso-
ciation strengths of the positive and negative pat-
tern sets, referred to as ASSOC pos and ASSOCneg ,
respectively. Our calculation of ASSOC uses max-
imum likelihood estimates of the true probabilities.
To account for resulting errors, we compare the two
confidence intervals,

ASSOC pos  ?ASSOC pos  and

ASSOCneg  ?ASSOCneg  , as in Lin (1999). We take
the minimum distance between the two as a conser-
vative estimate of the true difference:
DIFF
 
ASSOC
 
LV;N  PSpos  ASSOC
 
LV;N  PSneg  

 
ASSOC pos  ?ASSOCpos 

 
ASSOCneg  ?ASSOCneg 
Taking the difference between confidence intervals
lessens the effect of differences that are not statisti-
cally significant. (The confidence level, 1

?, is set
to 95% in all experiments.)
3 Acceptability Across Semantic Classes
3.1 Linguistic Properties: Class Behaviour
In this aspect of our work, we narrow our focus onto
a subclass of ?LV+PN? constructions that have a PN
complement in a stem form identical to a verb, pre-
ceded (typically) by an indefinite determiner (as in
(1a?b) above). Kearns (2002), Wierzbicka (1982),
and others have noted that the way in which LVs
combine with such PNs to form acceptable LVCs
is semantically patterned?that is, PNs with similar
semantics appear to have the same trends of cooc-
currence with an LV.
Our hypothesis is that semantically similar
LVCs?i.e., those formed from an LV plus any of
a set of semantically similar PNs?distinguish a fig-
urative subsense of the LV. In the long run, if this is
true, it could be exploited by using class information
to extend our knowledge of acceptable LVCs and
their likely meaning (cf. such an approach to verb
particle constructions by Villavicencio, 2003).
As steps to achieving this long-term goal, we must
first devise an acceptability measure which deter-
mines, for a given LV, which PNs it successfully
combines with. We can even use this measure to
provide evidence on whether the hypothesized class-
based behaviour holds, by seeing if the measure ex-
hibits differing behaviour across semantic classes of
potential complements.
3.2 A Statistical Measure of Acceptability
We develop a probability formula that captures the
likelihood of a given LV and PN forming an accept-
able LVC. The probability depends on both the LV
and the PN, and on these elements being used in an
LVC:
ACPT
 
LV  PN 
 Pr
 
LV  PN  LVC 
 Pr
 
PN  Pr
 
LVC  PN  Pr
 
LV  PN  LVC 
The first factor, Pr
 
PN  , reflects the linguistic
observation that higher frequency words are more
likely to be used as LVC complements (Wierzbicka,
1982). We estimate this factor by f   PN  n, where n
is the number of words in the corpus.
The probability that a given LV and PN form an
acceptable LVC further depends on how likely it is
that the PN combines with any light verbs to form an
LVC. The frequency with which a PN forms LVCs is
estimated as the number of times we observe it in the
prototypical ?LV a/an PN? pattern across LVs. (Note
that such counts are an overestimate, since we can-
not determine which usages are indeed LVCs vs. lit-
eral uses of the LV.) Since these counts consider the
PN only in the context of an indefinite determiner,
41
we normalize over counts of ?a/an PN? (noted as
aPN) to form the conditional probability estimate of
the second factor:
Pr
 
LVC  PN  

v
?
i   1
f   LV i  aPN 
f   aPN 
where v is the number of light verbs considered.
The third factor, Pr
 
LV  PN  LVC  , reflects that
different LVs have varying degrees of acceptability
when used with a given PN in an LVC. We similarly
estimate this factor with counts of the given LV and
PN in the typical LVC pattern: f   LV  aPN  f   aPN  .
Combining the estimates of the three factors
yields:
ACPT
 
LV  PN  

f   PN 
n

v
?
i   1
f   LV i  aPN 
f   aPN 

f   LV  aPN 
f   aPN 
4 Materials and Methods
4.1 Light Verbs
Common light verbs in English include give, take,
make, get, have, and do, among others. We focus
here on two of them, i.e., give and take, that are
frequently and productively used in light verb con-
structions, and are highly polysemous. The Word-
Net polysemy count (number of different senses) of
give and take are 44 and 42, respectively.
4.2 Experimental Expressions
Experimental expressions?i.e., potential LVCs us-
ing give and take?are drawn from two sources.
The development and test data used in experiments
of compositionality (bncD and bncT, respectively)
are randomly extracted from the BNC (BNC Ref-
erence Guide, 2000), yielding expressions cover-
ing a wide range of figurative usages of give and
take, with complements from different semantic cat-
egories. In contrast, in experiments that involve ac-
ceptability, we need figurative usages of ?the same
type?, i.e., with semantically similar complement
nouns, to further examine our hypothesis on the
class-based behaviour of light verb combinations.
Since in these LVCs the complement is a predica-
tive noun in stem form identical to a verb, we form
development and test expressions by combining give
or take with verbs from selected semantic classes of
Levin (1993), taken from Stevenson et al (2004).
4.3 Corpora
We gather estimates for our COMP measure from the
BNC, processed using the Collins parser (Collins,
1999) and TGrep2 (Rohde, 2004). Because some
LVCs can be rare in classical corpora, our ACPT es-
timates are drawn from the World Wide Web (the
subsection indexed by AltaVista). In our compari-
son of the two measures, we use web data for both,
using a simplified version of COMP. The high level
of noise on the web will influence the performance
of both measures, but COMP more severely, due to
its reliance on comparisons of syntactic patterns.
Web counts are based on an exact-phrase query to
AltaVista, with the number of pages containing the
search phrase recorded as its frequency.2 The size
of the corpus is estimated at 3.7 billion, the number
of hits returned in a search for the. These counts are
underestimates of the true frequencies, as a phrase
may appear more than once in a web page, but we
assume all counts to be similarly affected.
4.4 Extraction
Most required frequencies are simple counts of a
word or string of words, but the syntactic patterns
used in the compositionality measure present some
complexity. Recall that PSpos and PSneg are pattern
sets representing the syntactic contexts of interest.
Each pattern encodes several syntactic attributes: v,
the voice of the extracted expression (active or pas-
sive); d, the type of the determiner introducing N
(definite or indefinite); and n, the number of N (sin-
gular or plural). In our experiments, the set of pat-
terns associated with less-compositional use, PSpos,
consists of the single pattern with values active, in-
definite, and singular, for these attributes. PSneg con-
sists of all patterns with at least one of these at-
tributes having the alternative value.
While our counts on the BNC can use syntac-
tic mark-up, it is not feasible to collect counts on
the web for some of the pattern attributes, such as
voice. We develop two different variations of the
measure, one for BNC counts, and a simpler one for
2All searches were performed March 15?30, 2005.
42
give take
Human Ratings bncD bncT bncD bncT
?low? 20 10 36 19
?medium? 35 16 9 5
?high? 24 10 27 10
Total 79 36 72 34
Table 2: Distribution of development and test expressions with
respect to human compositionality ratings.
web counts. We thus subscript COMP with abbre-
viations standing for each attribute in the measure:
COMPvdn for a measure involving all three attributes
(used on BNC data), and COMPd for a measure in-
volving determiner type only (used on web data).
5 Human Judgments
5.1 Judgments of Compositionality
To determine how well our proposed measure
of compositionality captures the degree of lit-
eral/figurative use of a light verb, we compare its
scores to human judgments on compositionality.
Three judges (native speakers of English with suf-
ficient linguistic knowledge) answered yes/no ques-
tions related to the contribution of the literal mean-
ing of the light verb within each experimental ex-
pression. The combination of answers to these ques-
tions is transformed to numerical ratings, ranging
from 0 (fully non-compositional) to 4 (largely com-
positional). The three sets of ratings yield linearly
weighted Kappa values of .34 and .70 for give and
take, respectively. The ratings are averaged to form
a consensus set to be used for evaluation.3
The lists of rated expressions were biased toward
figurative usages of give and take. To achieve a spec-
trum of literal to figurative usages, we augment the
lists with literal expressions having an average rating
of 5 (fully compositional). Table 2 shows the distri-
bution of the experimental expressions across three
intervals of compositionality degree, ?low? (ratings
  1), ?medium? (1  ratings  3), and ?high? (rat-
ings  3). Table 3 presents sample expressions with
different levels of compositionality ratings.
3We asked the judges to provide short paraphrases for each
expression, and only use those expressions for which the major-
ity of judges expressed the same sense.
Sample Expressions
Human Ratings give take
?low? give a squeeze take a shower
?medium? give help take a course
?high? give a dose take an amount
Table 3: Sample expressions with different levels of composi-
tionality ratings.
5.2 Judgments of Acceptability
Our acceptability measure is compared to the hu-
man judgments gathered by Stevenson et al (2004).
Two expert native speakers of English rated the ac-
ceptability of each potential ?LV+PN? construction
generated by combining give and take with candi-
date complements from the development and test
Levin classes. Ratings were from 1 (unacceptable)
to 5 (completely natural; this was capped at 4 for
test data), allowing for ?in-between? ratings as well,
such as 2.5. On test data, the two sets of ratings
yielded linearly weighted Kappa values of .39 and
.72 for give and take, respectively. (Interestingly,
a similar agreement pattern is found in our human
compositionality judgments above.) The consensus
set of ratings was formed from an average of the two
sets of ratings, once disagreements of more than one
point were discussed.
6 Experimental Results
To evaluate our compositionality and acceptability
measures, we compare them to the relevant con-
sensus human ratings using the Spearman rank cor-
relation coefficient, rs. For simplicity, we report
the absolute value of rs for all experiments. Since
in most cases, correlations are statistically signifi-
cant (p  01), we omit p values; those rs values
for which p is marginal (i.e.,  01   p    10) are
subscripted with an ?m? in the tables. Correlation
scores in boldface are those that show an improve-
ment over the baseline, PMILVC .
The PMILVC measure is an informed baseline, since
it draws on properties of LVCs. Specifically, PMILVC
measures the strength of the association between a
light verb and a noun appearing in syntactic patterns
preferred by LVCs, i.e., PMILVC  PMI
 
LV;N  PSpos  .
Assuming that an acceptable LVC forms a detectable
collocation, PMILVC can be interpreted as an informed
baseline for degree of acceptability. PMILVC can also
43
PMILVC COMPvdn
LV Data Set n rs rs
bncT 36 .62 .57
give bncDT 114 .68 .70
bncDT/a 79 .68 .75
bncT 34 .51 .59
take bncDT 106 .52 .61
bncDT/a 68 .63 .72
Table 4: Correlations (rs; n = # of items) between human com-
positionality ratings and COMP measure (counts from BNC).
be considered as a baseline for the degree of compo-
sitionality of an expression (with respect to the light
verb component), under the assumption that the less
compositional an expression, the more its compo-
nents appear as a fixed collocation.
6.1 Compositionality Results
Table 4 displays the correlation scores of the human
compositionality ratings with COMPvdn, our com-
positionality measure estimated with counts from
the BNC. Given the variety of light verb usages
in expressions used in the compositionality data,
we report correlations not only on test data (bncT),
but also on development and test data combined
(bncDT) to get more data points and hence more re-
liable correlation scores. Compared to the baseline,
COMPvdn has generally higher correlations with hu-
man ratings of compositionality.
There are two different types of expressions
among those used in compositionality experiments:
expressions with an indefinite determiner a (e.g.,
give a kick) and those without a determiner (e.g.,
give guidance). Despite shared properties, the two
types of expressions may differ with respect to syn-
tactic flexibility, due to differing semantic proper-
ties of the noun complements in the two cases. We
thus calculate correlation scores for expressions with
the indefinite determiner only, from both develop-
ment and test data (bncDT/a). We find that COMPvdn
has higher correlations (and larger improvements
over the baseline) on this subset of expressions.
(Note that there are comparable numbers of items
in bncDT and bncDT/a, and the correlation scores
are highly significant?very small p values?in both
cases.)
To explore the effect of using a larger but noisier
corpus, we compare the performance of COMPvdn
Levin class: 18.1,2 30.3 43.2
LV n=35 n=18 n=35
give % fair/good ratings 51 44 54
log of mean ACPT -6 -4 -5
take % fair/good ratings 23 28 3
log of mean ACPT -4 -3 -6
Table 5: Comparison of the proportion of human ratings consid-
ered ?fair? or ?good? in each class, and the log10 of the mean
ACPT score for that class.
with COMPd , the compositionality measure using
web data. The correlation scores for COMPd on
bncDT are .41 and .35, for give and take, respec-
tively, compared to a baseline (using web counts) of
.37 and .32. We find that COMPvdn has significantly
higher correlation scores (larger rs and much smaller
p values), as well as larger improvements over the
baseline. This is a confirmation that using more syn-
tactic information, from less noisy data, improves
the performance of our compositionality measure.4
6.2 Acceptability Results
We have two goals in assessing our ACPT measure:
one is to demonstrate that the measure is indeed in-
dicative of the level of acceptability of an LVC, and
the other is to explore whether it helps to indicate
class-based patterns of acceptability.
Regarding the latter, Stevenson et al (2004) found
differing overall levels of (human) acceptability for
different Levin classes combined with give and take.
This indicates a strong influence of semantic simi-
larity on the possible LV and complement combina-
tions. Our ACPT measure also yields differing pat-
terns across the semantic classes. Table 5 shows,
for each light verb and test class, the proportion of
acceptable LVCs according to human ratings, and
the log of the mean ACPT score for that LV and
class combination. For take, the ACPT score gener-
ally reflects the difference in proportion of accepted
expressions according to the human ratings, while
for give, the measure is less consistent. (The three
development classes show the same pattern.) The
ACPT measure thus appears to reflect the differing
patterns of acceptability across the classes, at least
4Using the automatically parsed BNC as a source of less
noisy data improves performance. However, since these con-
structions may be infrequent with any particular complement,
we do not expect the use of cleaner but more plentiful text (such
as existing treebanks) to improve the performance any further.
44
Levin PMILVC ACPT
LV Class n rs rs
18.1,2 35 .39m .55
give 30.3 18 .38m .73
43.2 35 .30m .34m
18.1.2 35 .57 .61
take 30.3 18 .55 .64
43.2 35 .43 .47
Table 6: Correlations (rs; n = # of items) between acceptability
measures and consensus human ratings (counts from web).
Human PMILVC ACPT COMPd
Ratings LV n rs rs rs
accept. give 88 .31 .42 .40
(Levin) take 88 .58 .61 .56
compos. give 114 .37 .21m .41
(bncDT) take 106 .32 .30 .35
Table 7: Correlations (rs; n = # of items) between each measure
and each set of human ratings (counts from web).
for take.
To get a finer-grained notion of the degree to
which ACPT conforms with human ratings, we
present correlation scores between the two, in
Table 6. The results show that ACPT has higher
correlation scores than the baseline?substantially
higher in the case of give. The correlations for give
also vary more widely across the classes.
These results together indicate that the accept-
ability measure may be useful, and indeed taps into
some of the differing levels of acceptability across
the classes. However, we need to look more closely
at other linguistic properties which, if taken into ac-
count, may improve the consistency of the measure.
6.3 Comparing the Two Measures
Our two measures are intended for different pur-
poses, and indeed incorporate differing linguistic in-
formation about LVCs. However, we also noted that
PMILVC can be viewed as a baseline for both, indicat-
ing some underlying commonality. It is worth ex-
ploring whether each measure taps into the differ-
ent phenomena as intended. To do so, we correlate
COMP with the human ratings of acceptability, and
ACPT with the human ratings of compositionality,
as shown in Table 7. (The formulation of the ACPT
measure here is adapted for use with determiner-less
LVCs.) For comparability, both measures use counts
from the web. The results confirm that COMPd cor-
relates better than does ACPT with compositionality
ratings, while ACPT correlates best with acceptabil-
ity ratings.
7 Discussion and Concluding Remarks
Recently, there has been increasing awareness of the
need for appropriate handling of multiword expres-
sions (MWEs) in NLP tasks (Sag et al, 2002). Some
research has concentrated on the automatic acqui-
sition of semantic knowledge about certain classes
of MWEs, such as compound nouns or verb parti-
cle constructions (VPCs) (e.g., Lin, 1999; McCarthy
et al, 2003; Villavicencio, 2003). Previous research
on LVCs, on the other hand, has primarily focused
on their automatic extraction (e.g., Grefenstette and
Teufel 1995; Dras and Johnson 1996; Moiro?n 2004;
though see Stevenson et al 2004).
Like most previous studies that focus on seman-
tic properties of MWEs, we are interested in the is-
sue of compositionality. Our COMP measure aims to
identify a continuum along which a light verb con-
tributes to the semantics of an expression. In this
way, our work combines aspects of earlier work on
VPC semantics. McCarthy et al (2003) determine a
continuum of compositionality of VPCs, but do not
distinguish the contribution of the individual compo-
nents. Bannard et al (2003), on the other hand, look
at the separate contribution of the verb and particle,
but assume that a binary decision on the composi-
tionality of each is sufficient.
Previous studies determine compositionality by
looking at the degree of distributional similarity be-
tween an expression and its component words (e.g.,
McCarthy et al, 2003; Bannard et al, 2003; Bald-
win et al, 2003). Because light verbs are highly pol-
ysemous and frequently used in LVCs, such an ap-
proach is not appropriate for determining their con-
tribution to the semantics of an expression. We in-
stead examine the degree to which a light verb usage
is ?similar? to the prototypical LVC, through a sta-
tistical comparison of its behaviour within different
syntactic patterns. Syntactic flexibility and semantic
compositionality are known to be strongly correlated
for many types of MWEs (Nunberg et al, 1994). We
thus intend to extend our approach to include other
polysemous verbs with metaphorical extensions.
Our compositionality measure correlates well
with the literal/figurative spectrum represented in
45
human judgments. We also aim to determine finer-
grained distinctions among the identified figurative
usages of a light verb, which appear to relate to the
semantic class of its complement. Semantic class
knowledge may enable us to elucidate the types of
relations between a light verb and its complement
such as those determined in the work of Wanner
(2004), but without the need for the manually la-
belled training data which his approach requires.
Villavicencio (2003) used class-based knowledge to
extend a VPC lexicon, but assumed that an unob-
served VPC is not acceptable. We instead believe
that more robust application of class-based knowl-
edge can be achieved with a better estimate of the
acceptability of various expressions.
Work indicating acceptability of MWEs is largely
limited to collocational analysis using PMI-based
measures (Lin, 1999; Stevenson et al, 2004). We
instead use a probability formula that enables flex-
ible integration of LVC-specific linguistic proper-
ties. Our ACPT measure yields good correlations
with human acceptability judgments; indeed, the av-
erage increase over the baseline is about twice as
high as that of the acceptability measure proposed
by Stevenson et al (2004). Although ACPT also
somewhat reflects different patterns across seman-
tic classes, the results clearly indicate the need for
incorporating more knowledge into the measure to
capture class-based behaviour more consistently.
The work presented here is preliminary, but is the
first we are aware of to tie together the two issues of
compositionality and acceptability, and relate them
to the notion of class-based meaning extensions of
highly polysemous verbs. Our on-going work is fo-
cusing on the role of the noun component of LVCs,
to determine the compositional contribution of the
noun to the semantics of the expression, and the role
of noun classes in influencing the meaning exten-
sions of light verbs.
References
Baldwin, T., Bannard, C., Tanaka, T., and Wid-
dows, D. (2003). An empirical model of multi-
word expression decomposability. In Proceedings
of the ACL-SIGLEX Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment,
pages 89?96.
Bannard, C., Baldwin, T., and Lascarides, A. (2003).
A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL-SIGLEX
Workshop on Multiword Expressions: Analysis,
Acquisition and Treatment, pages 65?72.
BNC Reference Guide (2000). Reference Guide for
the British National Corpus (World Edition), sec-
ond edition.
Butt, M. (2003). The light verb jungle. Workshop
on Multi-Verb Constructions.
Collins, M. (1999). Head-Driven Statistical Models
for Natural Language Parsing. PhD thesis, Uni-
versity of Pennsylvania.
Dras, M. and Johnson, M. (1996). Death and light-
ness: Using a demographic model to find support
verbs. In Proceedings of the Fifth International
Conference on the Cognitive Science of Natural
Language Processing.
Dunning, T. (1993). Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61?74.
Grefenstette, G. and Teufel, S. (1995). Corpus-
based method for automatic identification of sup-
port verbs for nominalization. In Proceedings of
the 7th Meeting of the EACL.
Kearns, K. (2002). Light verbs in English.
manuscript.
Levin, B. (1993). English Verb Classes and Alterna-
tions: A Preliminary Investigation. The Univer-
sity of Chicago Press.
Lin, D. (1999). Automatic identification of non-
compositional phrases. In Proceedings of the 37th
Annual Meeting of the ACL, pages 317?324.
McCarthy, D., Keller, B., and Carroll, J. (2003).
Detecting a continuum of compositionality in
phrasal verbs. In Proceedings of the ACL-SIGLEX
Workshop on Multiword Expressions: Analysis,
Acquisition and Treatment.
Moiro?n, M. B. V. (2004). Discarding noise in an au-
tomatically acquired lexicon of support verb con-
structions. In Proceedings of the 4th International
Conference on Language Resources and Evalua-
tion (LREC).
Newman, J. (1996). Give: A Cognitive Linguistic
Study. Mouton de Gruyter.
46
Newman, J. and Rice, S. (2004). Patterns of usage
for English SIT, STAND, and LIE: A cognitively
inspired exploration in corpus linguistics. Cogni-
tive Linguistics, 15(3):351?396.
Nunberg, G., Sag, I. A., and Wasow, T. (1994). Id-
ioms. Language, 70(3):491?538.
Pauwels, P. (2000). Put, Set, Lay and Place: A
Cognitive Linguistic Approach to Verbal Mean-
ing. LINCOM EUROPA.
Pustejovsky, J. (1995). The Generative Lexicon.
MIT Press.
Rappaport Hovav, M. and Levin, B. (1998). Build-
ing verb meanings. In Butt and Geuder, editors,
The Projection of Arguments: Lexical and Com-
putational Factors, pages 97?134. CSLI Publica-
tions.
Rohde, D. L. T. (2004). TGrep2 User Manual.
Sag, I. A., Baldwin, T., Bond, F., Copestake, A., and
Flickinger, D. (2002). Multiword expressions: A
pain in the neck for NLP. In Proceedings of the
3rd International Conference on Intelligent Text
Processing and Computational Linguistics (CI-
CLING?02), pages 1?15.
Stevenson, S., Fazly, A., and North, R. (2004). Sta-
tistical measures of the semi-productivity of light
verb constructions. In Proceedings of the ACL-04
Workshop on Multiword Expressions: Integrating
Processing, pages 1?8.
Villavicencio, A. (2003). Verb-particle construc-
tions and lexical resources. In Proceedings of
the ACL-SIGLEX Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment,
pages 57?64.
Wanner, L. (2004). Towards automatic fine-grained
semantic classification of verb-noun collocations.
Natural Language Engineering, 10(2):95?143.
Wierzbicka, A. (1982). Why can you Have a Drink
when you can?t *Have an Eat? Language,
58(4):753?799.
47
Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 45?53,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Classifying Particle Semantics in English Verb-Particle Constructions
Paul Cook
Department of Computer Science
University of Toronto
Toronto, ON M5S 3G4
Canada
pcook@cs.toronto.edu
Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, ON M5S 3G4
Canada
suzanne@cs.toronto.edu
Abstract
Previous computational work on learning
the semantic properties of verb-particle
constructions (VPCs) has focused on their
compositionality, and has left unaddressed
the issue of which meaning of the compo-
nent words is being used in a given VPC.
We develop a feature space for use in clas-
sification of the sense contributed by the
particle in a VPC, and test this on VPCs
using the particle up. The features that
capture linguistic properties of VPCs that
are relevant to the semantics of the par-
ticle outperform linguistically uninformed
word co-occurrence features in our exper-
iments on unseen test VPCs.
1 Introduction
A challenge in learning the semantics of mul-
tiword expressions (MWEs) is their varying de-
grees of compositionality?the contribution of
each component word to the overall semantics
of the expression. MWEs fall on a range from
fully compositional (i.e., each component con-
tributes its meaning, as in frying pan) to non-
compositional or idiomatic (as in hit the roof ). Be-
cause of this variation, researchers have explored
automatic methods for learning whether, or the de-
gree to which, an MWE is compositional (e.g.,
Lin, 1999; Bannard et al, 2003; McCarthy et al,
2003; Fazly et al, 2005).
However, such work leaves unaddressed the ba-
sic issue of which of the possible meanings of a
component word is contributed when the MWE is
(at least partly) compositional. Words are notori-
ously ambiguous, so that even if it can be deter-
mined that an MWE is compositional, its meaning
is still unknown, since the actual semantic contri-
bution of the components is yet to be determined.
We address this problem in the domain of verb-
particle constructions (VPCs) in English, a rich
source of MWEs.
VPCs combine a verb with any of a finite set
of particles, as in jump up, figure out, or give in.
Particles such as up, out, or in, with their literal
meaning based in physical spatial relations, show
a variety of metaphorical and aspectual meaning
extensions, as exemplified here for the particle up:
(1a) The sun just came up. [vertical spatial movement]
(1b) She walked up to him. [movement toward a goal]
(1c) Drink up your juice! [completion]
(1d) He curled up into a ball. [reflexive movement]
Cognitive linguistic analysis, as in Lindner (1981),
can provide the basis for elaborating this type of
semantic variation.
Given such a sense inventory for a particle,
our goal is to automatically determine its mean-
ing when used with a given verb in a VPC. We
classify VPCs according to their particle sense,
using statistical features that capture the seman-
tic and syntactic properties of verbs and particles.
We contrast these with simple word co-occurrence
features, which are often used to indicate the se-
mantics of a target word. In our experiments, we
focus on VPCs using the particle up because it is
highly frequent and has a wide range of meanings.
However, it is worth emphasizing that our feature
space draws on general properties of VPCs, and is
not specific to this particle.
A VPC may be ambiguous, with its particle oc-
curring in more than one sense; in contrast to (1a),
come up may use up in a goal-oriented sense as in
45
The deadline is coming up. While our long-term
goal is token classification (disambiguation) of a
VPC in context, following other work on VPCs
(e.g., Bannard et al, 2003; McCarthy et al, 2003),
we begin here with the task of type classification.
Given our use of features which capture the statis-
tical behaviour relevant to a VPC across a corpus,
we assume that the outcome of type classification
yields the predominant sense of the particle in the
VPC. Predominant sense identification is a useful
component of sense disambiguation of word to-
kens (McCarthy et al, 2004), and we presume our
VPC type classification work will form the basis
for later token disambiguation.
Section 2 continues the paper with a discussion
of the features we developed for particle sense
classification. Section 3 first presents some brief
cognitive linguistic background, followed by the
sense classes of up used in our experiments. Sec-
tions 4 and 5 discuss our experimental set-up and
results, Section 6 related work, and Section 7 our
conclusions.
2 Features Used in Classification
The following subsections describe the two sets of
features we investigated. The linguistic features
are motivated by specific semantic and syntactic
properties of verbs and VPCs, while the word co-
occurrence features are more general.
2.1 Linguistically Motivated Features
2.1.1 Slot Features
We hypothesize that the semantic contribution
of a particle when combined with a given verb is
related to the semantics of that verb. That is, the
particle contributes the same meaning when com-
bining with any of a semantic class of verbs.1 For
example, the VPCs drink up, eat up and gobble up
all draw on the completion sense of up; the VPCs
puff out, spread out and stretch out all draw on the
extension sense of out. The prevalence of these
patterns suggests that features which have been
shown to be effective for the semantic classifica-
tion of verbs may be useful for our task.
We adopt simple syntactic ?slot? features which
have been successfully used in automatic seman-
tic classification of verbs (Joanis and Stevenson,
1Villavicencio (2005) observes that verbs from a seman-
tic class will form VPCs with similar sets of particles. Here
we are hypothesizing further that VPCs formed from verbs
of a semantic class draw on the same meaning of the given
particle.
2003). The features are motivated by the fact
that semantic properties of a verb are reflected
in the syntactic expression of the participants in
the event the verb describes. The slot features
encode the relative frequencies of the syntactic
slots?subject, direct and indirect object, object of
a preposition?that the arguments and adjuncts of
a verb appear in. We calculate the slot features
over three contexts: all uses of a verb; all uses of
the verb in a VPC with the target particle (up in our
experiments); all uses of the verb in a VPC with
any of a set of high frequency particles (to capture
its semantics when used in VPCs in general).
2.1.2 Particle Features
Two types of features are motivated by proper-
ties specific to the semantics and syntax of par-
ticles and VPCs. First, Wurmbrand (2000) notes
that compositional particle verbs in German (a
somewhat related phenomenon to English VPCs)
allow the replacement of their particle with seman-
tically similar particles. We extend this idea, hy-
pothesizing that when a verb combines with a par-
ticle such as up in a particular sense, the pattern
of usage of that verb in VPCs using all other par-
ticles may be indicative of the sense of the target
particle (in this case up) when combined with that
verb. To reflect this observation, we count the rel-
ative frequency of any occurrence of the verb used
in a VPC with each of a set of high frequency par-
ticles.
Second, one of the striking syntactic properties
of VPCs is that they can often occur in either the
joined configuration (2a) or the split configuration
(2b):
(2a) Drink up your milk! He walked out quickly.
(2b) Drink your milk up! He walked quickly out.
Bolinger (1971) notes that the joined construction
may be more favoured when the sense of the par-
ticle is not literal. To encode this, we calculate the
relative frequency of the verb co-occurring with
the particle up with each of   ?  words between
the verb and up, reflecting varying degrees of verb-
particle separation.
2.2 Word Co-occurrence Features
We also explore the use of general context fea-
tures, in the form of word co-occurrence frequency
vectors, which have been used in numerous ap-
proaches to determining the semantics of a target
46
word. Note, however, that unlike the task of word
sense disambiguation, which examines the context
of a target word token to be disambiguated, here
we are looking at aggregate contexts across all in-
stances of a target VPC, in order to perform type
classification.
We adopt very simple word co-occurrence fea-
tures (WCFs), calculated as the frequency of any
(non-stoplist) word within a certain window left
and right of the target. We noted above that the
target particle semantics is related both to the se-
mantics of the verb it co-occurs with, and to the
occurrence of the verb across VPCs with different
particles. Thus we not only calculate the WCFs of
the target VPC (a given verb used with the parti-
cle up), but also the WCFs of the verb itself, and
the verb used in a VPC with any of the high fre-
quency particles. These WCFs give us a very gen-
eral means for determining semantics, whose per-
formance we can contrast with our linguistic fea-
tures.
3 Particle Semantics and Sense Classes
We give some brief background on cognitive
grammar and its relation to particle semantics, and
then turn to the semantic analysis of up that we
draw on as the basis for the sense classes in our
experiments.
3.1 Cognitive Grammar and Schemas
Some linguistic studies consider many VPCs to be
idiomatic, but do not give a detailed account of
the semantic similarities between them (Bolinger,
1971; Fraser, 1976; Jackendoff, 2002). In con-
trast, work in cognitive linguistics has claimed that
many so-called idiomatic expressions draw on the
compositional contribution of (at least some of)
their components (Lindner, 1981; Morgan, 1997;
Hampe, 2000). In cognitive grammar (Langacker,
1987), non-spatial concepts are represented as spa-
tial relations. Key terms from this framework are:
Trajector (TR) The object which is conceptually
foregrounded.
Landmark (LM) The object against which the
TR is foregrounded.
Schema An abstract conceptualization of an ex-
perience. Here we focus on schemas depict-
ing a TR, LM and their relationship in both
the initial configuration and the final config-
uration communicated by some expression.
TR
TR
LM LM
Initial Final
Figure 1: Schema for Vertical up.
The semantic contribution of a particle in a VPC
corresponds to a schema. For example, in sen-
tence (3), the TR is the balloon and the LM is the
ground the balloon is moving away from.
(3) The balloon floated up.
The schema describing the semantic contribution
of the particle in the above sentence is shown
in Figure 1, which illustrates the relationship be-
tween the TR and LM in the initial and final con-
figurations.
3.2 The Senses of up
Lindner (1981) identifies a set of schemas for each
of the particles up and out, and groups VPCs ac-
cording to which schema is contributed by their
particle. Here we describe the four senses of up
identified by Lindner.
3.2.1 Vertical up (Vert-up)
In this schema (shown above in Figure 1), the
TR moves away from the LM in the direction of
increase along a vertically oriented axis. This in-
cludes prototypical spatial upward movement such
as that in sentence (3), as well as upward move-
ment along an abstract vertical axis as in sen-
tence (4).
(4) The price of gas jumped up.
In Lindner?s analysis, this sense also includes ex-
tensions of upward movement where a vertical
path or posture is still salient. Note that in some of
these senses, the notion of verticality is metaphor-
ical; the contribution of such senses to a VPC may
not be considered compositional in a traditional
analysis. Some of the most common sense exten-
sions are given below, with a brief justification as
to why verticality is still salient.
47
Initial
TR
LM = goal LM = goal
TR
Final
Figure 2: Schema for Goal-Oriented up.
Up as a path into perceptual field. Spatially
high objects are generally easier to perceive.
Examples: show up, spring up, whip up.
Up as a path into mental field. Here up encodes
a path for mental as opposed to physical objects.
Examples: dream up, dredge up, think up.
Up as a path into a state of activity. Activity is
prototypically associated with an erect posture.
Examples: get up, set up, start up.
3.2.2 Goal-Oriented up (Goal-up)
Here the TR approaches a goal LM; movement
is not necessarily vertical (see Figure 2). Proto-
typical examples are walk up and march up. This
category also includes extensions into the social
domain (kiss up and suck up), as well as exten-
sions into the domain of time (come up and move
up), as in:
(5a) The intern kissed up to his boss.
(5b) The deadline is coming up quickly.
3.2.3 Completive up (Cmpl-up)
Cmpl-up is a sub-sense of Goal-up in which the
goal represents an action being done to comple-
tion. This sense shares its schema with Goal-up
(Figure 2), but it is considered as a separate sense
since it corresponds to uses of up as an aspectual
marker. Examples of Cmpl-up are: clean up, drink
up, eat up, finish up and study up.
3.2.4 Reflexive up (Refl-up)
Reflexive up is a sub-sense of Goal-up in which
the sub-parts of the TR are approaching each other.
The schema for Refl-up is shown in Figure 3; it is
unique in that the TR and LM are the same object.
Examples of Refl-up are: bottle up, connect up,
couple up, curl up and roll up.
LM = TR LM = TR
Initial Final
Figure 3: Schema for Reflexive up.
Vertical up Goal-Oriented up
Completive up
Reflexive up
Figure 4: Simplified schematic network for up.
3.3 The Sense Classes for Our Study
Adopting a cognitive linguistic perspective, we as-
sume that all uses of a particle make some compo-
sitional contribution of meaning to a VPC. In this
work, we classify target VPCs according to which
of the above senses of up is contributed to the ex-
pression. For example, the expressions jump up
and pick up are designated as being in the class
Vert-up since up in these VPCs has the vertical
sense, while clean up and drink up are designated
as being in the class Cmpl-up since up here has
the completive sense. The relations among the
senses of up can be shown in a ?schematic net-
work? (Langacker, 1987). Figure 4 shows a sim-
plification of such a network in which we connect
more similar senses with shorter edges. This type
of analysis allows us to alter the granularity of our
classification in a linguistically motivated fashion
by combining closely related senses. Thus we can
explore the effect of different sense granularities
on classification.
4 Materials and Methods
4.1 Experimental Expressions
We created a list of English VPCs using up, based
on a list of VPCs made available by McIntyre
(2001) and a list of VPCs compiled by two human
judges. The judges then filtered this list to include
only VPCs which they both agreed were valid, re-
sulting in a final list of 389 VPCs. From this list,
training, verification and test sets of sixty VPCs
each are randomly selected. Note that the expense
of manually annotating the data (as described be-
low) prevents us from using larger datasets in this
initial investigation. The experimental sets are
48
chosen such that each includes the same propor-
tion of verbs across three frequency bands, so that
the sets do not differ in frequency distribution of
the verbs. (We use frequency of the verbs, rather
than the VPCs, since many of our features are
based on the verb of the expression, and moreover,
VPC frequency is approximate.) The verification
data is used in exploration of the feature space and
selection of final features to use in testing; the test
set is held out for final testing of the classifiers.
Each VPC in each dataset is annotated by the
two human judges according to which of the four
senses of up identified in Section 3.2 is contributed
to the VPC. As noted in Section 1, VPCs may
be ambiguous with respect to their particle sense.
Since our task here is type classification, the
judges identify the particle sense of a VPC in its
predominant usage, in their assessment. The ob-
served inter-annotator agreement is      for each
dataset. The unweighted observed kappa scores
are
  
,
  
	
and      , for the training, verifica-
tion and test sets respectively.
4.2 Calculation of the Features
We extract our features from the 100M word
British National Corpus (BNC, Burnard, 2000).
VPCs are identified using a simple heuristic based
on part-of-speech tags, similar to one technique
used by Baldwin (2005). A use of a verb is con-
sidered a VPC if it occurs with a particle (tagged
AVP) within a six word window to the right. Over
a random sample of 113 VPCs thus extracted, we
found 88% to be true VPCs, somewhat below the
performance of Baldwin?s (2005) best extraction
method, indicating potential room for improve-
ment.
The slot and particle features are calculated us-
ing a modified version of the ExtractVerb software
provided by Joanis and Stevenson (2003), which
runs over the BNC pre-processed using Abney?s
(1991) Cass chunker.
To compute the word co-occurrence features
(WCFs), we first determine the relative frequency
of all words which occur within a five word win-
dow left and right of any of the target expressions
in the training data. From this list we eliminate
the most frequent 1% of words as a stoplist and
then use the next  most frequent words as ?fea-
ture words?. For each ?feature word?, we then cal-
culate its relative frequency of occurrence within
the same five word window of the target expres-
#VPCs in Sense Class
Sense Class Train Verification Test
Vert-up 24 33 27
Goal-up 1 1 3
Cmpl-up 20 23 22
Refl-up 15 3 8
Table 1: Frequency of items in each sense class.
#VPCs in Sense Class
Sense Class Train Verification Test
Vert-up 24 33 27
Goal-up  21 24 25
Cmpl-up
Refl-up 15 3 8
Table 2: Frequency of items in each class for the
3-way task.
sions in all datasets. We use      and      
to create feature sets WCF  and WCF  respec-
tively.
4.3 Experimental Classes
Table 1 shows the distribution of senses in each
dataset. Each of the training and verification sets
has only one VPC corresponding to Goal-up. Re-
call that Goal-up shares a schema with Cmpl-up,
and is therefore very close to it in meaning, as in-
dicated spatially in Figure 4. We therefore merge
Goal-up and Cmpl-up into a single sense, to pro-
vide more balanced classes.
Since we want to see how our features per-
form on differing granularities of sense classes, we
run each experiment as both a 3-way and 2-way
classification task. In the 3-way task, the sense
classes correspond to the meanings Vert-up, Goal-
up merged with Cmpl-up (as noted above), and
Refl-up, as shown in Table 2. In the 2-way task, we
further merge the classes corresponding to Goal-
#VPCs in Sense Class
Sense Class Train Verification Test
Vert-up 24 33 27
Goal-up  36 27 33
Cmpl-up 
Refl-up
Table 3: Frequency of items in each class for the
2-way task.
49
up/Cmpl-up with that of Refl-up, as shown in Ta-
ble 3. We choose to merge these classes because
(as illustrated in Figure 4) Refl-up is a sub-sense of
Goal-up, and moreover, all three of these senses
contrast with Vert-up, in which increase along a
vertical axis is the salient property. It is worth em-
phasizing that the 2-way task is not simply a clas-
sification between literal and non-literal up?Vert-
up includes extensions of up in which the increase
along a vertical axis is metaphorical.
4.4 Evaluation Metrics and Classifier
Software
The variation in the frequency of the sense classes
of up across the datasets makes the true distri-
bution of the classes difficult to estimate. Fur-
thermore, there is no obvious informed baseline
for this task. Therefore, we make the assumption
that the true distribution of the classes is uniform,
and use the chance accuracy   as the baseline
(where  is the number of classes?in our exper-
iments, either  or  ). Accordingly, our measure
of classification accuracy should weight each class
evenly. Therefore, we report the average per class
accuracy, which gives equal weight to each class.
For classification we use LIBSVM (Chang and
Lin, 2001), an implementation of a support-vector
machine. We set the input parameters, cost
and gamma, using 10-fold cross-validation on the
training data. In addition, we assign a weight of
 
	 

 
Workshop on TextGraphs, at HLT-NAACL 2006, pages 97?104,
New York City, June 2006. c?2006 Association for Computational Linguistics
Context Comparison as a Minimum Cost Flow Problem
Vivian Tsang and Suzanne Stevenson
Department of Computer Science
University of Toronto
Canada
 
vyctsang,suzanne  @cs.utoronto.ca
Abstract
Comparing word contexts is a key compo-
nent of many NLP tasks, but rarely is it
used in conjunction with additional onto-
logical knowledge. One problem is that
the amount of overhead required can be
high. In this paper, we provide a graphi-
cal method which easily combines an on-
tology with contextual information. We
take advantage of the intrinsic graphical
structure of an ontology for representing
a context. In addition, we turn the on-
tology into a metric space, such that sub-
graphs within it, which represent contexts,
can be compared. We develop two vari-
ants of our graphical method for compar-
ing contexts. Our analysis indicates that
our method performs the comparison effi-
ciently and offers a competitive alternative
to non-graphical methods.
1 Introduction
Many natural language problems can be cast as a
problem of comparing ?contexts? (units of text). For
example, the local context of a word can be used to
resolve its ambiguity (e.g., Schu?tze, 1998), assum-
ing that words used in similar contexts are closely
related semantically (Miller and Charles, 1991). Ex-
tending the meaning of context, the content of a
document may reveal which document class(es) it
belongs to (e.g., Xu et al, 2003). In any appli-
cation, once a sensible view of context is formu-
lated, the next step is to choose a representation that
makes comparisons possible. For example, in word
sense disambiguation, a context of an ambiguous
instance can be represented as a vector of the fre-
quencies of words surrounding it. Until recently, the
dominant approach has been a non-graphical one?
context comparison is reduced to a task of measuring
distributional distance between context vectors. The
difference in the frequency characteristics of con-
texts is used as an indicator of the semantic distance
between them.
We present a graphical alternative that combines
both distributional and ontological knowledge. We
begin with the use of a different context represen-
tation that allows easy incorporation of ontological
information. Treating an ontology as a network, we
can represent a context as a set of nodes in the net-
work (i.e., concepts in the ontology), each with a
weight (i.e., frequency). To contrast our work with
that of Navigli and Velardi (2005) and Mihalcea
(2006), the goal is not merely to provide a graph-
ical representation for a context in which the rele-
vant concepts are connected. Rather, contexts are
treated as weighted subgraphs within a larger graph
in which they are connected via a set of paths. By in-
corporating the semantic distance between individ-
ual concepts, the graph (representing the ontology)
becomes a metric space in which we can measure the
distance between subgraphs (representing the con-
texts to be compared).
More specifically, measuring the distance be-
tween two contexts can be viewed as solving a min-
imum cost flow (MCF) problem by calculating the
amount of ?effort? required for transporting the flow
from one context to the other. Our method has
the advantage of including semantic information (by
making use of the graphical structure of an ontol-
ogy) without losing distributional information (by
97
using the concept frequencies derived from corpus
data).
This network flow formulation, though support-
ing the inclusion of an ontology in context compari-
son, is not flexible enough. The problem is rooted in
the choice of concept-to-concept distance (i.e., the
distance between two concepts, to contrast it from
the overall semantic distance between two contexts).
Certain concept-to-concept distances may result in a
difficult-to-process network which severely compro-
mises efficiency. To remedy this, we propose a novel
network transformation method for constructing a
pared-down network which mimics the structure of
the more precise network, but without the expensive
processing or any significant information loss as a
result of the transformation.
In the remainder of this paper, we first present the
underlying network flow framework, and develop a
more efficient variant of it. We then evaluate the
robustness of our methods on a context comparison
task. Finally, we conclude with an analysis and some
future directions.
2 The Network Flow Method
2.1 Minimum Cost Flow
As a standard example of an MCF problem, consider
the graphical representation of a route map for deliv-
ering fresh produce from grocers (supply nodes) to
homes (demand nodes). The remaining nodes (e.g.,
intersections, gas stations) have neither a supply nor
a demand. Assuming there are sufficient supplies,
the optimal solution is to find the cheapest set of
routes from grocers to homes such that all demands
are satisfied.
Mathematically, let 
	 be a connected
network, where  is the set of nodes, and  is the
set of edges.1 Each edge has a cost   ,
which is the distance of the edge. Each node Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 41?48,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
A Cognitive Model for the Representation and Acquisition
of Verb Selectional Preferences
Afra Alishahi
Department of Computer Science
University of Toronto
afra@cs.toronto.edu
Suzanne Stevenson
Department of Computer Science
University of Toronto
suzanne@cs.toronto.edu
Abstract
We present a cognitive model of inducing
verb selectional preferences from individ-
ual verb usages. The selectional preferences
for each verb argument are represented as
a probability distribution over the set of
semantic properties that the argument can
possess?a semantic profile. The seman-
tic profiles yield verb-specific conceptual-
izations of the arguments associated with a
syntactic position. The proposed model can
learn appropriate verb profiles from a small
set of noisy training data, and can use them
in simulating human plausibility judgments
and analyzing implicit object alternation.
1 Introduction
Verbs have preferences for the semantic properties
of the arguments filling a particular role. For ex-
ample, the verb eat expects that the object receiving
its theme role will have the property of being edi-
ble, among others. Learning verb selectional pref-
erences is an important aspect of human language
acquisition, and the acquired preferences have been
shown to guide children?s expectations about miss-
ing or upcoming arguments in language comprehen-
sion (Nation et al, 2003).
Resnik (1996) introduced a statistical approach
to learning and use of verb selectional preferences.
In this framework, a semantic class hierarchy for
words is used, together with statistical tools, to in-
duce a verb?s selectional preferences for a particu-
lar argument position in the form of a distribution
over all the classes that can occur in that position.
Resnik?s model was proposed as a model of human
learning of selectional preferences that made min-
imal representational assumptions; it showed how
such preferences could be acquired from usage data
and an existing conceptual hierarchy. However, his
and later computational models (see Section 2) have
properties that do not match with certain cognitive
plausibility criteria for a child language acquisition
model. All these models use the training data in
?batch mode?, and most of them use information
theoretic measures that rely on total counts from a
corpus. Therefore, it is not clear how the representa-
tion of selectional preferences could be updated in-
crementally in these models as the person receives
more data. Moreover, the assumption that children
have access to a full hierarchical representation of
semantic classes may be too strict. We propose an
alternative view in this paper which is more plausi-
ble in the context of child language acquisition.
In previous work (Alishahi and Stevenson, 2005),
we have proposed a usage-based computational
model of early verb learning that uses Bayesian clus-
tering and prediction to model language acquisition
and use. Individual verb usages are incrementally
grouped to form emergent classes of linguistic con-
structions that share semantic and syntactic proper-
ties. We have shown that our Bayesian model can
incrementally acquire a general conception of the
semantic roles of predicates based only on expo-
sure to individual verb usages (Alishahi and Steven-
son, 2007). The model forms probabilistic associa-
tions between the semantic properties of arguments,
their syntactic positions, and the semantic primitives
41
of verbs. Our previous experiments demonstrated
that, initially, this probability distribution for an ar-
gument position yields verb-specific conceptualiza-
tions of the role associated with that position. As the
model is exposed to more input, the verb-based roles
gradually transform into more abstract representa-
tions that reflect the general properties of arguments
across the observed verbs.
A shortcoming of the model was that, because
the prediction of the semantic roles was based only
on the groupings of verbs, it could not make use of
verb-specific knowledge in generating expectations
about a particular verb?s arguments. That is, once
it was exposed to a range of verbs, it no longer had
access to the verb-specific information, only to gen-
eralizations over clusters of verbs.
In this paper, we propose a new version of our
model that, in addition to learning general seman-
tic roles for constructions, can use its verb-specific
knowledge to predict intuitive selectional prefer-
ences for each verb argument position. We introduce
a new notion, a verb semantic profile, as a prob-
ability distribution over the semantic properties of
an argument for each verb. A verb semantic pro-
file is predicted from both the verb-based and the
construction-based knowledge that the model has
learned through clustering, and reflects the prop-
erties of the arguments that are observed for that
verb. Our proposed prediction model makes appro-
priate generalizations over the observed properties,
and captures expectations about previously unseen
arguments.
As in other work on selectional preferences, the
semantic properties that we use in our representa-
tion of arguments are drawn from a standard lex-
ical ontology (WordNet; Miller, 1990), but we do
not require knowledge of the hierarchical structure
of the WordNet concepts. From the computational
point of view, this makes use of an available re-
source, while from the cognitive view, this avoids
ad hoc assumptions about the representation of a
conceptual hierarchy. However, we do require some
properties to be more general (i.e., shared by more
words) than others, which eventually enables the
model to make appropriate generalizations. Other-
wise, the selected semantic properties are not fun-
damental to the model, and could in the future be
replaced with an approach that is deemed more ap-
propriate to child language acquisition. Each argu-
ment contributes to the semantic profile of the verb
through its (potentially large) set of semantic prop-
erties instead of its membership in a single class. As
input to our model, we use an automatically parsed
corpus, which is very noisy. However, as a result of
our novel representation, the model can induce and
use selectional preferences using a relatively small
set of noisy training data.
2 Related Computational Models
A variety of computational models for verb selec-
tional preferences have been proposed, which use
different statistical models to induce the preferences
of each verb from corpus data. Most of these
models, however, use the same representation for
verb selectional preferences: the preference can be
thought of as a mapping, with respect to an argument
position for a verb, of each class to a real number
(Light and Greiff, 2002). The induction of a verb?s
preferences is, therefore, modeled as using a set of
training data to estimate that number.
Resnik (1996) defines the selectional preference
strength of a verb as the divergence between two
probability distributions: the prior probabilities of
the classes, and the posterior probabilities of the
classes given that verb. The selectional association
of a verb with a class is also defined as the contribu-
tion of that class to the total selectional preference
strength. Resnik estimates the prior and posterior
probabilities based on the frequencies of each verb
and its relevant argument in a corpus.
Li and Abe (1998) model selectional preferences
of a verb (for an argument position) as a set of nodes
in the semantic class hierarchy with a probability
distribution over them. They use the Minimum De-
scription Length (MDL) principle to find the best set
for each verb and argument based on the usages of
that verb in the training data. Clark and Weir (2002)
also find an appropriate set of concept nodes to rep-
resent the selectional preferences for a verb, but do
so using a ?2 test over corpus frequencies mapped
to concepts to determine when to generalize from a
node to its parent. Ciaramita and Johnson (2000)
use a Bayesian network with the same topology as
WordNet to estimate the probability distribution of
the relevant set of nodes in the hierarchy. Abney
42
and Light (1999) use a different representational ap-
proach: they train a separate hidden Markov model
for each verb, and the selectional preference is rep-
resented as a probability distribution over words in-
stead of semantic classes.
3 The Bayesian Verb-Learning Model
3.1 Overview of the Model
Our model learns the set of argument structure
frames for each verb, and their grouping across verbs
into constructions. An argument structure frame is
a set of features of a verb usage that are both syn-
tactic (the number of arguments, the syntactic pat-
tern of the usage) and semantic (the semantic prop-
erties of the verb, the semantic properties of each
argument). The syntactic pattern indicates the word
order of the verb and arguments. A construction is
a grouping of individual frames which probabilisti-
cally share syntactic and semantic features, and form
probabilistic associations across verb semantic prop-
erties, argument semantic properties, and syntactic
pattern. These groupings typically correspond to
general constructions in the language such as tran-
sitive, intransitive, and ditransitive.
For each verb, the model associates an argument
position with a probability distribution over a set of
semantic properties?a semantic profile. In doing
so, the model uses the knowledge that it has learned
for that verb, as well as the grouping of frames for
that verb into constructions.
The semantic properties of words are taken from
WordNet (version 2.0) as follows. We extract all the
hypernyms (ancestors) for all the senses of the word,
and add all the words in the hypernym synsets to the
list of the semantic properties. Figure 1 shows an ex-
ample of the hypernyms for dinner, and its resulting
set of semantic properties.1
The following sections review basic properties
of the model from Alishahi and Stevenson (2005,
2007), and introduce extensions that give the model
its ability to make verb-based predictions.
3.2 Learning as Bayesian Clustering
Each argument structure frame for an observed verb
usage is input to an incremental Bayesian clustering
1We do not remove alternate spellings of a term in WordNet;
this will be seen in the profiles in the results section.
Sense 1
dinner
=> meal, repast
=> nutriment, nourishment, nutrition, sustenance,
aliment, alimentation, victuals
=> food, nutrient
=> substance, matter
=> entity
Sense 2
dinner, dinner party
=> party
=> social gathering, social affair
=> gathering, assemblage
=> social group
=> group, grouping
dinner: {meal, repast, nutriment, nourishment, nutrition, substance, aliment, alimentation,
victuals, food, nutrient, substance, matter, entity, party, social gathering,
social affair, gathering, assemblage, social group, group, grouping }
Figure 1: Semantic properties for dinner from Word-
Net
process. This process groups the new frame together
with an existing group of frames?a construction?
that probabilistically has the most similar semantic
and syntactic properties to it. If no construction has
sufficiently high probability for the new frame, then
a new construction is created for it. We use the prob-
abilistic model of Alishahi and Stevenson (2007) for
learning constructions, which is itself an adaptation
of a Bayesian model of human categorization pro-
posed by Anderson (1991). It is important to note
that the categories (i.e., constructions) are not prede-
fined, but rather are created according to the patterns
of similarity over observed frames.
Grouping a frame F with other frames participat-
ing in construction k is formulated as finding the k
with the maximum probability given F :
BestConstruction(F ) = argmax
k
P (k|F ) (1)
where k ranges over the indices of all constructions,
with index 0 representing recognition of a new con-
struction.
Using Bayes rule, and dropping P (F ) which is
constant for all k:
P (k|F ) = P (k)P (F |k)P (F ) ? P (k)P (F |k) (2)
The prior probability, P (k), indicates the degree of
entrenchment of construction k, and is given by the
relative frequency of its frames over all observed
frames. The posterior probability of a frame F is
expressed in terms of the individual probabilities of
its features, which we assume are independent, thus
yielding a simple product of feature probabilities:
43
P (F |k) =
?
i?FrameFeatures
Pi(j|k) (3)
where j is the value of the ith feature of F , and
Pi(j|k) is the probability of displaying value j on
feature i within construction k. Given the focus here
on semantic profiles, we next focus on the calcula-
tion of the probabilities of semantic properties.
3.3 Probabilities of Semantic Properties
The probability in equation (3) of value j for feature
i in construction k is estimated using a smoothed
version of this maximum likelihood formula:
Pi(j|k) =
countki (j)
nk
(4)
where nk is the number of frames participating in
construction k, and countki (j) is the number of
those with value j for feature i.
For most features, countki (j) is calculated by
simply counting those members of construction k
whose value for feature i exactly matches j. How-
ever, for the semantic properties of words, counting
only the number of exact matches between the sets
is too strict, since even highly similar words very
rarely have the exact same set of properties. We
instead use the following Jaccard similarity score
to measure the overlap between the set of semantic
properties, SF , of a particular argument in the frame
to be clustered, and the set of semantic properties,
Sk, of the same argument in a member frame of a
construction:
sem score(SF , Sk) =
|SF ? Sk|
|SF ? Sk|
(5)
For example, assume that the new frame F repre-
sents a usage of John ate cake. In the construction
that we are considering for inclusion of F , one of
the member frames represents a usage of Mom got
water. We must compare the semantic properties of
the corresponding arguments cake and water:
cake: {baked goods,food,solid,substance,matter,entity}
water: {liquid,fluid,food,nutrient,substance,matter,entity}
The intersection of the two sets is {food, substance,
matter, entity}, yielding a sem score of 49 .
In general, to calculate the conditional probability
for the set of semantic properties, we set countki (j)
in equation (4) to the sum of the sem score?s for
the new frame and every member of construction k,
and normalize the resulting probability over all pos-
sible sets of semantic properties in our lexicon.
3.4 Predicting Semantic Profiles for Verbs
We represent the selectional preferences of a verb
for an argument position as a semantic profile, which
is a probability distribution over all the semantic
properties. To predict the profile of a verb v for
an argument position arg , we need to estimate the
probability of each semantic property j separately:
Parg (j|v) =
?
k
Parg(j, k|v) (6)
?
?
k
P (k, v)Parg (j|k, v)
Here, j ranges over all the possible semantic proper-
ties that an argument can have, and k ranges over all
constructions. The prior probability of having verb v
in construction k, or P (k, v), takes into account two
important factors: the relative entrenchment of the
construction k, and the (smoothed) frequency with
which v participates in k.
The posterior probability Parg (j|k, v) is calcu-
lated analogously to Pi(j|k) in equation (4), but lim-
iting the count of matching features to those frames
in k that contain v:
Parg (j|k, v) =
verb countkarg (j, v)
nkv
(7)
where nkv is the number of frames for v participat-
ing in construction k, and verb countkarg(j, v) is
the number of those with semantic property j for
argument arg . We use a smoothed version of the
above formula, where the relative frequency of each
property j among all nouns is used as the smoothing
factor.
3.5 Verb-Argument Compatibility
In one of our experiments, we need to measure the
compatibility of a particular noun n for an argument
position arg of some verb v. That is, we need to es-
timate how much the semantic properties of n con-
form to the acquired semantic profile of v for arg .
We formulate the compatibility as the conditional
probability of observing n as an argument arg of v:
compatibility(v, n) = log(Parg (jn|v)) (8)
44
where jn is the set of the semantic properties for
word n, and Parg (jn|v) is estimated as in equa-
tion (7). However, since jn here is a set of prop-
erties (as opposed to j in equation (7) being a
single property), verb countkarg in equation (7)
should be modified as described in Section 3.3:
we set verb countkarg (jn, v) to the sum of the
sem score?s (equation (5)) for jn and every frame
of v that participates in construction k.
4 Experimental Results
In the following sections, we first describe the train-
ing data for our model. In accordance with other
computational models, we focus here on the verb
preferences for the direct object position.2 Next, we
provide a qualitative analysis of our model through
examination of the semantic profiles for a number
of verbs. We then evaluate our model through two
tasks of simulating verb-argument plausibility judg-
ment, and analyzing the implicit object alternation,
following Resnik (1996).3
4.1 The Training Data
In earlier work (Alishahi and Stevenson, 2005,
2007), we used a method to automatically generate
training data with the same distributional properties
as the input children receive. However, this relies on
manually-compiled data about verbs and their argu-
ment structure frames from the CHILDES database
(MacWhinney, 1995). To evaluate the new version
of our model for the task of learning selectional pref-
erences, we need a wide selection of verbs and their
arguments that is impractical to compile by hand.
The training data for our experiments here are
generated as follows. We use 20,000 sentences
randomly selected from the British National Cor-
pus (BNC),4 automatically parsed using the Collins
parser (Collins, 1999), and further processed with
TGrep2,5 and an NP-head extraction software.6 For
2To our knowledge, the only work that considers selectional
preferences of subjects and prepositional phrases as well as di-
rect objects is Brockmann and Lapata (2003).
3Computational models of verb selectional preference have
been evaluated through disambiguation tasks (Li and Abe,
1998; Abney and Light, 1999; Ciaramita and Johnson, 2000;
Clark and Weir, 2002), but for to evaluate our cognitive model,
the experiments from Resnik (1996) are the most interesting.
4http://www.natcorp.ox.ac.uk
5http://tedlab.mit.edu/?dr/Tgrep2
6The software was provided to us by Eric Joanis, and Af-
each verb usage in a sentence, we construct a frame
by recording the verb in root form, the number of
the arguments for that verb, and the syntactic pattern
of the verb usage (i.e., the word order of the verb
and the arguments). We also record in the frame the
semantic properties of the verb and each of the ar-
gument heads (each noun is also converted to root
form); these properties are extracted from WordNet
(as discussed in Section 3.1 and illustrated in Fig-
ure 1). This process results in 16,300 frames which
serve as input data to our learning model.
4.2 Formation of Semantic Profiles for Verbs
After training our model on the above data, we use
equation (7) to predict the semantic profile of the di-
rect object position for a range of verbs. Some of
these verbs, such as write and sing, have strong se-
lectional preferences, whereas others, such as want
and put, can take a wide range of nouns as direct
object (as confirmed by Resnik?s (1996) estimated
strength of selectional preference for these verbs).
The semantic profiles for write and sing are dis-
played in Figure 2, and the profiles for want and put
are displayed in Figure 3. (Due to limited space, we
only include the 25 properties that have the highest
probability in each profile.)
Because we extract the semantic properties of
words from WordNet, which has a hierarchical
structure, the properties that come from nodes in
the higher levels of the hierarchy (such as entity and
abstraction) appear as the semantic property for a
very large set of words, whereas the properties that
come from the leaves in the hierarchy are specific to
a small set of words. Therefore, the general prop-
erties are more likely to be associated with a higher
probability in the semantic profiles for most verbs.
In fact, a closer look at the semantic profiles for want
and put reveals that the top portion of the semantic
profile for these verbs consists solely of such gen-
eral properties that are shared among a large group
of words. However, this is not the case for the more
restrictive verbs. The semantic profiles for write and
sing show that the specific properties that these verbs
demand from their direct object appear amongst the
highest-ranked properties, even though only a small
set of words share these properties (e.g., content,
saneh Fazly helped us in using the above-mentioned tools for
generating our input corpora.
45
write
(0.024) abstraction
(0.022) entity
(0.021) location
(0.020) substance
(0.019) destination
(0.018) relation
(0.015) communication
(0.015) social relation
(0.013) content
(0.011) message
(0.011) subject matter
(0.011) written
communication
(0.011) written
language
(0.010) object
(0.010) physical object
(0.010) writing
(0.010) goal
(0.010) unit
(0.009) whole
(0.009) whole thing
(0.009) artifact
(0.009) artefact
(0.009) state
(0.009) amount
(0.009) measure
sing
(0.020) abstraction
(0.015) relation
(0.015) communication
(0.015) social relation
(0.013) act
(0.013) human action
(0.013) human activity
(0.013) auditory
communication
(0.012) music
(0.010) entity
(0.010) piece
(0.009) composition
(0.009) musical
composition
(0.009) opus
(0.009) piece of music
(0.009) psychological
feature
(0.008) cognition
(0.008) knowledge
(0.008) noesis
(0.008) activity
(0.008) content
(0.008) grouping
(0.008) group
(0.008) amount
(0.008) measure
Figure 2: Semantic profiles of write and sing for the
direct object position.
message, written communication, written language,
... for write, and auditory communication, music,
musical composition, opus, ... for sing).
The examination of the semantic profiles for fairly
frequent verbs in the training data shows that our
model can use the verb usages to predict an appro-
priate semantic profile for each verb. When pre-
sented with a novel verb (for which no verb-based
information is available), equation (7) predicts a se-
mantic profile which reflects the relative frequencies
of the semantic properties among all words (due to
the smoothing factor added to equation (7)), modu-
lated by the prior probability of each construction.
The predicted profile is displayed in Figure 4. It
shows similarities with the profiles for want and put
in Figure 3, but the general properties in this profile
have an even higher probability. Since the profile for
the novel verb is predicted in the absence of any evi-
dence (i.e., verb usage) in the training data, we later
use it as the base for estimating other verbs? strength
of selectional preference.
want
(0.016) entity
(0.015) object
(0.015) physical object
(0.014) abstraction
(0.013) act
(0.012) human action
(0.012) human activity
(0.012) relation
(0.011) unit
(0.011) whole
(0.011) whole thing
(0.011) artifact
(0.011) artefact
(0.008) communication
(0.008) social relation
(0.008) activity
(0.007) cause
(0.007) state
(0.007) instrumentality
(0.007) instrumentation
(0.007) event
(0.006) being
(0.006) living thing
(0.006) animate thing
(0.006) organism
put
(0.015) entity
(0.015) object
(0.013) physical object
(0.013) abstraction
(0.011) unit
(0.011) whole
(0.011) whole thing
(0.011) artifact
(0.011) artefact
(0.010) act
(0.009) relation
(0.008) human action
(0.008) human activity
(0.008) communication
(0.008) social relation
(0.007) substance
(0.007) content
(0.007) instrumentality
(0.007) instrumentation
(0.007) measure
(0.006) amount
(0.006) quantity
(0.006) cause
(0.006) causal agent
(0.006) causal agency
Figure 3: Semantic profiles of want and put for the
direct object position.
4.3 Verb-Argument Plausibility Judgments
Holmes et al (1989) evaluate verb argument plau-
sibility by asking human subjects to rate sentences
like The mechanic warned the driver and The me-
chanic warned the engine. Resnik (1996) used this
data to assess the performance of his model by com-
paring its judgments of selectional fit against the
plausibility ratings elicited from human subjects. He
showed that his selectional association measure for
a verb and its direct object can be used to select the
more plausible verb-noun pair among the two (e.g.,
<warn,driver> vs. <warn,engine> in the previous
example). That is, a higher selectional association
between the verb and one of the nouns compared to
the other noun indicates that the former is the more
plausible pair. Resnik (1996) used the Brown corpus
as training data, and showed that his model arrives
at the correct ordering of more and less plausible ar-
guments in 11 of the 16 cases.
We repeated this experiment, using the same 16
pairs of verb-noun combinations. For each pair of
<v, n1> and <v, n2>, we calculate the compati-
bility measure using equation (8); these values are
shown in Figure 5. (Note that because these are
46
A novel verb
(0.021) entity
(0.017) object
(0.017) physical object
(0.015) abstraction
(0.010) act
(0.010) human action
(0.010) human activity
(0.010) unit
(0.009) whole
(0.009) whole thing
(0.009) artifact
(0.009) artefact
(0.009) being
(0.009) living thing
(0.009) animate thing
(0.009) organism
(0.008) cause
(0.008) causal agent
(0.008) causal agency
(0.008) relation
(0.008) person
(0.008) individual
(0.008) someone
(0.008) somebody
(0.008) mortal
Figure 4: Semantic profile of a novel verb for the
direct object position.
log-probabilities and therefore negative numbers,
a lower absolute value of compatibility(v, n)
shows a better compatibility between the verb v
and the argument n.) For example, <see,friend>
has a higher compatibility score (-30.50) than
<see,method> (-32.14). Similar to Resnik, our
model detects 11 plausible pairs out of 16. How-
ever, these results are reached with a much smaller
training corpus (around 500,000 words), compared
to the Brown corpus used by Resnik (1996) which
contains one million words. Moreover, whereas the
Brown corpus is tagged and parsed manually, the
portion of the BNC that we use is parsed automat-
ically, and as a result our training data is very noisy.
Nonetheless, the model achieves the same level of
accuracy in distinguishing plausible verb-argument
pairs from implausible ones.
4.4 Implicit Object Alternations
In English, some inherently transitive verbs can ap-
pear with or without their direct objects (e.g., John
ate his dinner as well as John ate), but others can-
not (e.g., Mary made a cake but not *Mary made).
It is argued that implicit object alternations involve a
Verb Plausible Implausible
see friend -30.50 method -32.14
read article -32.76 fashion -33.33
find label -32.05 fever -33.30
hear story -32.11 issue -32.40
write letter -31.37 market -32.46
urge daughter -36.73 contrast -35.64
warn driver -33.68 engine -34.42
judge contest -39.05 climate -38.23
teach language -45.64 distance -45.11
show sample -31.75 travel -31.42
expect visit -33.88 mouth -32.87
answer request -31.89 tragedy -33.95
recognize author -32.53 pocket -32.62
repeat comment -33.80 journal -33.97
understand concept -32.25 session -32.93
remember reply -33.79 smoke -34.29
Figure 5: Compatibility scores for plausible vs. im-
plausible verb-noun pairs.
particular relationship between the verb and its argu-
ment. In particular, for verbs that participate in the
implicit object alternation, the omitted object must
be in some sense inferable or typical for that verb
(Levin, 1993, among others).
Resnik (1996) used his model of selectional pref-
erences to analyze implicit object alternations, and
showed a relationship between his measure of se-
lectional preference strength and the notion of typ-
icality of an object. He calculated this measure
for two groups of Alternating and Non-alternating
verbs, and showed that, on average, the Alternating
verbs have a higher strength of selectional prefer-
ence for the direct object than the Non-alternating
verbs. However, there was no threshold separating
the two groups of verbs.
To repeat Resnik?s experiment, we need a mea-
sure of how ?strongly constraining? a semantic pro-
file is. We can do this by measuring the similarity
between the semantic profile we generate for the ob-
ject of a particular verb and some ?default? notion of
the argument for that position across all verbs. We
use the semantic profile predicted for the object po-
sition of a novel verb, shown earlier in Figure 4, as
the default profile for that argument position. Be-
cause this profile is predicted in the absence of any
evidence in the training data, it makes the minimum
assumptions about the properties of the argument
and thus serves as a suitable default. We then assume
that verbs with weaker selectional preferences have
semantic profiles more similar to the default profile
47
Alternating verbs Non-alternating verbs
write 0.61 hang 0.56
sing 0.67 wear 0.71
drink 0.67 say 0.75
eat 0.74 catch 0.76
play 0.74 show 0.77
pour 0.76 make 0.78
watch 0.77 hit 0.78
pack 0.78 open 0.81
steal 0.80 take 0.83
push 0.80 see 0.87
call 0.80 like 0.87
pull 0.80 get 0.87
explain 0.81 find 0.87
read 0.82 give 0.88
hear 0.87 bring 0.89
want 0.89
put 0.90
Mean: 0.76 Mean: 0.81
Figure 6: Similarity with the base profile for Alter-
nating and Non-alternating verbs.
than verbs with stronger preferences. We use the
cosine measure to estimate the similarity between
two profiles p and q:
cosine(p, q) = p? q||p|| ? ||q|| (9)
The similarity values for the Alternating and Non-
alternating verbs are shown in Figure 6. The larger
values represent more similarity with the base pro-
file, which means a weaker selectional preference.
The means for the Alternating and Non-alternating
verbs were respectively 0.76 and 0.81, which con-
firm the hypothesis that verbs participating in im-
plicit object alternations select more strongly for the
direct objects than verbs that do not. However, like
Resnik (1996), we find that it is not possible to set a
threshold that will distinguish the two sets of verbs.
5 Conclusions
We have proposed a cognitively plausible model for
learning selectional preferences from instances of
verb usage. The model represents verb selectional
preferences as a semantic profile, which is a prob-
ability distribution over the semantic properties that
an argument can take. One of the strengths of our
model is the incremental nature of its learning mech-
anism, in contrast to other approaches which learn
selectional preferences in batch mode. Here we have
only reported the results for the final stage of learn-
ing, but the model allows us to monitor the semantic
profiles during the course of learning, and compare
it with child data for different age groups, as we do
with semantic roles (Alishahi and Stevenson, 2007).
We have shown that the model can predict appropri-
ate semantic profiles for a variety of verbs, and use
these profiles to simulate human judgments of verb-
argument plausibility, using a small and highly noisy
set of training data. The model can also use the pro-
files to measure verb-argument compatibility, which
was used in analyzing the implicit object alternation.
References
Abney, S. and Light, M. (1999). Hiding a semantic hierarchy
in a Markov model. In Proc. of the ACL Workshop on Unsu-
pervised Learning in Natural Language Processing.
Alishahi, A. and Stevenson, S. (2005). A probabilistic model of
early argument structure acquisition. In Proc. of the CogSci
2005.
Alishahi, A. and Stevenson, S. (2007). A computational usage-
based model for learning general properties of semantic
roles. In Proc. of the EuroCogSci 2007.
Anderson, J. R. (1991). The adaptive nature of human catego-
rization. Psychological Review, 98(3):409?429.
Brockmann, C. and Lapata, M. (2003). Evaluating and com-
bining approaches to selectional preference acquisition. In
Proc. of the EACL 2003.
Ciaramita, M. and Johnson, M. (2000). Explaining away am-
biguity: Learning verb selectional preference with Bayesian
networks. In Proc. of the COLING 2000.
Clark, S. and Weir, D. (2002). Class-based probability estima-
tion using a semantic hierarchy. Computational Linguistics,
28(2):187?206.
Collins, M. (1999). Head-Driven Statistical Models for Natural
Language Parsing. PhD thesis, University of Pennsylvania.
Holmes, V. M., Stowe, L., and Cupples, L. (1989). Lexical
expectations in parsing complement-verb sentences. Journal
of Memory and Language, 28:668?689.
Levin, B. (1993). English verb classes and alternations: A pre-
liminary investigation. The University of Chicago Press.
Li, H. and Abe, N. (1998). Generalizing case frames using a
thesaurus and the MDL principle. Computational Linguis-
tics, 24(2):217?244.
Light, M. and Greiff, W. (2002). Statistical models for the in-
duction and use of selectional preferences. Cognitive Sci-
ence, 26(3):269?281.
MacWhinney, B. (1995). The CHILDES project: Tools for an-
alyzing talk. Lawrence Erlbaum.
Miller, G. (1990). WordNet: An on-line lexical database. Inter-
national Journal of Lexicography, 17(3).
Nation, K., Marshall, C. M., and Altmann, G. T. M. (2003). In-
vestigating individual differences in children?s real-time sen-
tence comprehension using language-mediated eye move-
ments. J. of Experimental Child Psych., 86:314?329.
Resnik, P. (1996). Selectional constraints: An information-
theoretic model and its computational realization. Cognition,
61:127?199.
48
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 9?16,
Prague, June 2007. c?2007 Association for Computational Linguistics
Distinguishing Subtypes of Multiword Expressions Using
Linguistically-Motivated Statistical Measures
Afsaneh Fazly
Department of Computer Science
University of Toronto
Toronto, Canada
afsaneh@cs.toronto.edu
Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
suzanne@cs.toronto.edu
Abstract
We identify several classes of multiword ex-
pressions that each require a different encod-
ing in a (computational) lexicon, as well as
a different treatment within a computational
system. We examine linguistic properties
pertaining to the degree of semantic idiosyn-
crasy of these classes of expressions. Ac-
cordingly, we propose statistical measures to
quantify each property, and use the measures
to automatically distinguish the classes.
1 Motivation
Multiword expressions (MWEs) are widely used in
written language as well as in colloquial speech. An
MWE is composed of two or more words that to-
gether form a single unit of meaning, e.g., frying pan,
take a stroll, and kick the bucket. Most MWEs behave
like any phrase composed of multiple words, e.g.,
their components may be separated, as in She took a
relaxing stroll along the beach. Nonetheless, MWEs
are distinct from multiword phrases because they in-
volve some degree of semantic idiosyncrasy, i.e., the
overall meaning of an MWE diverges from the com-
bined contribution of its constituent parts. Because of
their frequency and their peculiar behaviour, MWEs
pose a great challenge to the creation of natural lan-
guage processing (NLP) systems (Sag et al, 2002).
NLP applications, such as semantic parsing and ma-
chine translation should not only identify MWEs, but
also should know how to treat them when they are
encountered.
Semantic idiosyncrasy is a matter of degree (Nun-
berg et al, 1994). The idiom shoot the breeze is
largely idiosyncratic, because its meaning (?to chat?)
does not have much to do with the meaning of shoot
or breeze. MWEs such as give a try (?try?) and make
a decision (?decide?) are semantically less idiosyn-
cratic (more predictable). These are MWEs because
the overall meaning of the expression diverges from
the combined meanings of the constituents. Nonethe-
less, there is some degree of predictability in their
meanings that makes them distinct from idioms. In
these, the complement of the verb (here, a noun) de-
termines the primary meaning of the overall expres-
sion. This class of expressions is referred to as light
verb constructions (LVCs) in the linguistics literature
(Miyamoto, 2000; Butt, 2003).
Clearly, a computational system should distinguish
idioms and LVCs, both from each other, and from
similar-on-the-surface (literal) phrases such as shoot
the bird and give a present. Idioms are largely id-
iosyncratic; a computational lexicographer thus may
decide to list idioms such as shoot the breeze in a lex-
icon along with their idiomatic meanings. In contrast,
the meaning of MWEs such as make a decision can
be largely predicted, given that they are LVCs. Ta-
ble 1 shows the different underlying semantic struc-
ture of a sentence containing an idiom (shoot the
breeze) and a sentence containing an LVC (give a
try). As can be seen, such MWEs should also be
treated differently when translated into another lan-
guage. Note that in contrast to a literal combination,
such as shoot the bird, for idioms and LVCs, the num-
ber of arguments expressed syntactically may differ
from the number of the semantic participants.
Many NLP applications also need to distinguish
another group of MWEs that are less idiosyncratic
9
Class English sentence Semantic representation French translation
Literal Jill and Tim shot the bird. (event/SHOOT Jill et Tim ont abattu l?oiseau.
:agent (?Jill ? Tim?) Jill and Tim shot down the bird.
:theme (?bird?))
Abstract Jill makes a living singing in pubs. (event/EARN-MONEY Jill gagne sa vie en chantant dans des bars.
:agent (?Jill?)) Jill makes a living by singing in the pubs.
LVC Jill gave the lasagna a try. (event/TRY Jill a essaye? le lasagne.
:agent (?Jill?) Jill tried the lasagna.
:theme (?lasagna?))
Idiom Jill and Tim shot the breeze. (event/CHAT Jill et Tim ont bavarde?.
:agent (?Jill ? Tim?)) Jill and Tim chatted.
Table 1: Sample English MWEs and their translation in French.
than idioms and LVCs, but more so than literal com-
binations. Examples include give confidence and
make a living. These are idiosyncratic because the
meaning of the verb is a metaphorical (abstract)
extension of its basic physical semantics. More-
over, they often take on certain connotations be-
yond the compositional combination of their con-
stituent meanings. They thus exhibit behaviour of-
ten attributed to collocations, e.g., they appear with
greater frequency than semantically similar combina-
tions. For example, searching on Google, we found
much higher frequency for give confidence compared
to grant confidence. As can be seen in Table 1, an ab-
stract combination such as make a living, although
largely compositional, may not translate word-for-
word. Rather, it should be translated taking into ac-
count that the verb has a metaphorical meaning, dif-
ferent from its basic semantics.
Here, we focus on a particular class of English
MWEs that are formed from the combination of a
verb with a noun in its direct object position, re-
ferred to as verb+noun combinations. Specifically,
we provide a framework for identifying members of
the following semantic classes of verb+noun combi-
nations: (i) literal phrases (LIT), (ii) abstract combi-
nations (ABS), (iii) light verb constructions (LVC),
and (iv) idiomatic combinations (IDM). Section 2
elaborates on the linguistic properties related to the
differences in the degree of semantic idiosyncrasy
observed in members of the above four classes. In
Section 3, we propose statistical measures for quan-
tifying each of these properties, and use them as fea-
tures for type classification of verb+noun combina-
tions. Section 4 and Section 5 present an evaluation
of our proposed measures. Section 6 discusses the
related studies, and Section 7 concludes the paper.
2 Semantic Idiosyncrasy: Linguistic
Properties
Linguists and lexicographers often attribute certain
characteristics to semantically idiosyncratic expres-
sions. Some of the widely-known properties are in-
stitutionalization, lexicosyntactic fixedness, and non-
compositionality (Cowie, 1981; Gibbs and Nayak,
1989; Moon, 1998). The following paragraphs elab-
orate on each property, as well as on its relevance to
the identification of the classes under study.
Institutionalization is the process through which a
combination of words becomes recognized and ac-
cepted as a semantic unit involving some degree of
semantic idiosyncrasy. IDMs, LVCs, and ABS com-
binations are institutionalized to some extent.
Lexicosyntactic fixedness refers to some degree of
lexical and syntactic restrictiveness in a semantically
idiosyncratic expression. An expression is lexically
fixed if the substitution of a semantically similar
word for any of its constituents does not preserve its
original meaning (e.g., compare spill the beans and
spread the beans). In contrast to LIT and ABS com-
binations, IDMs and LVCs are expected to exhibit
lexical fixedness to some extent.
An expression is syntactically fixed if it cannot un-
dergo syntactic variations and at the same time retain
its original semantic interpretation. IDMs and LVCs
are known to show strong preferences for the syn-
tactic patterns they appear in (Cacciari and Tabossi,
1993; Brinton and Akimoto, 1999). E.g., compare
10
Joe gave a groan with ?A groan was given by Joe,
and Tim kicked the bucket with *Tim kicked the buck-
ets (in the idiom reading). Nonetheless, the type and
degree of syntactic fixedness in LVCs and IDMs are
different. For example, most LVCs prefer the pattern
in which the noun is introduced by the indefinite arti-
cle a (as in give a try and make a decision), whereas
this is not the case with IDMs (e.g., shoot the breeze
and kick the bucket). IDMs and LVCs may also ex-
hibit preferences with respect to adjectival modifica-
tion of their noun constituent. LVCs are expected to
appear both with and without an adjectival modifier,
as in give a (loud) groan and make a (wise) decision.
IDMs, on the other hand, mostly appear either with
an adjective, as in keep an open mind (cf. ?keep a
mind), or without, as in shoot the breeze (cf. ?shoot
the fun breeze).
Non-compositionality refers to the situation where
the meaning of a word combination deviates from
the meaning emerging from a word-by-word inter-
pretation of it. IDMs are largely non-compositional,
whereas LVCs are semi-compositional since their
meaning can be mainly predicted from the noun con-
stituent. ABS and LIT combinations are expected to
be largely compositional.
None of the above-mentioned properties are suffi-
cient criteria by themselves for determining which
semantic class a given verb+noun combination be-
longs to. Moreover, semantic properties of the con-
stituents of a combination are also known to be rele-
vant for determining its class (Uchiyama et al, 2005).
Verbs may exhibit strong preferences for appearing
in MWEs from a particular class, e.g., give, take and
make commonly form LVCs. The semantic category
of the noun is also relevant to the type of MWE, e.g.,
the noun constituent of an LVC is often a predicative
one. We hypothesize that if we look at evidence from
all these different sources, we will find members of
the same class to be reasonably similar, and members
of different classes to be notably different.
3 Statistical Measures of Semantic
Idiosyncrasy
This section introduces measures for quantifying the
properties of idiosyncratic MWEs, mentioned in the
previous section. The measures will be used as fea-
tures in a classification task (see Sections 4?5).
3.1 Measuring Institutionalization
Corpus-based approaches often assess the degree of
institutionalization of an expression by the frequency
with which it occurs. Raw frequencies drawn from
a corpus are not reliable on their own, hence asso-
ciation measures such as pointwise mutual informa-
tion (PMI) are also used in many NLP applications
(Church et al, 1991). PMI of a verb+noun combina-
tion ?v , n? is defined as:
PMI (v , n) .= log P (v , n)P (v)P (n)
? log f (?, ?)f (v , n)f (v , ?) f (?, n) (1)
where all frequency counts are calculated over
verb?object pairs in a corpus. We use both frequency
and PMI of a verb+noun combination to measure its
degree of institutionalization. We refer to this group
of measures as INST.
3.2 Measuring Fixedness
To measure fixedness, we use statistical measures of
lexical, syntactic, and overall fixedness that we have
developed in a previous study (Fazly and Stevenson,
2006), as well as some new measures we introduce
here. The following paragraphs give a brief descrip-
tion of each.
Fixednesslex quantifies the degree of lexical fixed-
ness of the target combination, ?v ,n?, by compar-
ing its strength of association (measured by PMI)
with those of its lexical variants. Like Lin (1999),
we generate lexical variants of the target automati-
cally by replacing either the verb or the noun con-
stituent by a semantically similar word from the
automatically-built thesaurus of Lin (1998). We then
use a standard statistic, the z -score, to calculate
Fixednesslex:
Fixednesslex(v , n) .=
PMI(v , n) ? PMI
std (2)
where PMI is the mean and std the standard devia-
tion over the PMI of the target and all its variants.
Fixednesssyn quantifies the degree of syntactic
fixedness of the target combination, by comparing
its behaviour in text with the behaviour of a typical
verb?object, both defined as probability distributions
over a predefined set of patterns. We use a stan-
dard information-theoretic measure, relative entropy,
11
v det:NULL nsg v det:NULL npl
v det:a/an nsg
v det:the nsg v det:the npl
v det:DEM nsg v det:DEM npl
v det:POSS nsg v det:POSS npl
v det:OTHER nsg,pl det:ANY nsg,pl be vpassive
Table 2: Patterns for syntactic fixedness measure.
to calculate the divergence between the two distribu-
tions as follows:
Fixednesssyn (v , n)
.= D(P(pt |v ,n) ||P(pt))
=
?
ptk?P
P(ptk | v , n) log
P(ptk | v , n)
P(ptk )
(3)
where P is the set of patterns (shown in Table 2)
known to be relevant to syntactic fixedness in LVCs
and IDMs. P(pt | v , n) represents the syntactic be-
haviour of the target, and P(pt) represents the typical
syntactic behaviour over all verb?object pairs.
Fixednesssyn does not show which syntactic pat-
tern the target prefers the most. We thus use an addi-
tional measure, Patterndom, to determine the domi-
nant pattern for the target:
Patterndom(v , n) .= argmax
ptk?P
f (v , n, ptk ) (4)
In addition to the individual measures of fixedness,
we use Fixednessoverall, which quantifies the degree
of overall fixedness of the target:
Fixednessoverall (v , n)
.= ? Fixednesssyn (v , n)
+ (1 ? ?) Fixednesslex (v , n) (5)
where ? weights the relative contribution of lexi-
cal and syntactic fixedness in predicting semantic id-
iosyncrasy.
Fixednessadj quantifies the degree of fixedness
of the target combination with respect to adjectival
modification of the noun constituent. It is similar to
the syntactic fixedness measure, except here there are
only two patterns that mark the presence or absence
of an adjectival modifier preceding the noun:
Fixednessadj(v , n) .= D(P(ai |v ,n) ||P(ai )) (6)
where ai ? {present, absent}. Fixednessadj does
not determine which pattern of modification the tar-
get combination prefers most. We thus add another
measure?the odds of modification?to capture this:
Oddsadj(v , n) .=
P(ai = present|v ,n)
P(ai = absent|v ,n)
(7)
Overall, we use six measures related to fixedness;
we refer to the group as FIXD.
3.3 Measuring Compositionality
Compositionality of an expression is often approxi-
mated by comparing the ?context? of the expression
with the contexts of its constituents. We measure
the degree of compositionality of a target verb+noun
combination, t =?v ,n?, in a similar fashion.
We take the context of the target (t) and each of its
constituents (v and n) to be a vector of the frequency
of nouns cooccurring with it within a window of ?5
words. We then measure the ?similarity? between the
target and each of its constituents, Simdist (t , v) and
Simdist (t , n), using the cosine measure.1
Recall that an LVC can be roughly paraphrased by
a verb that is morphologically related to its noun con-
stituent, e.g., to make a decision nearly means to de-
cide. For each target t , we thus add a third measure,
Simdist (t , rv), where rv is a verb morphologically
related to the noun constituent of t , and is automati-
cally extracted from WordNet (Fellbaum, 1998).2
We use abbreviation COMP to refer to the group of
measures related to compositionality.
3.4 The Constituents
Recall that semantic properties of the constituents of
a verb+noun combination are expected to be relevant
to its semantic class. We thus add two simple fea-
ture groups: (i) the verb itself (VERB); and (ii) the
semantic category of the noun according to WordNet
(NSEM). We take the semantic category of a noun to
be the ancestor of its first sense in the hypernym hier-
archy of WordNet 2.1, cut at the level of the children
1Our preliminary experiments on development data from Fa-
zly and Stevenson (2006) revealed that the cosine measure and a
window size of ?5 words resulted in the best performance.
2If no such verb exists, Simdist (t , rv) is set to zero. If more
than one verb exist, we choose the one that is identical to the
noun or the one that is shorter in length.
12
of ENTITY (which will include PHYSICAL ENTITY
and ABSTRACT ENTITY).3
4 Experimental Setup
4.1 Corpus and Experimental Expressions
We use the British National Corpus (BNC),4 auto-
matically parsed using the Collins parser (Collins,
1999), and further processed with TGrep2.5 We
select our potential experimental expressions from
pairs of verb and direct object that have a minimum
frequency of 25 in the BNC and that involve one
of a predefined list of basic (transitive) verbs. Ba-
sic verbs, which in their literal uses refer to states or
acts central to human experience (e.g., give and put),
commonly form MWEs in combination with their di-
rect object argument (Cowie et al, 1983). We use 12
such verbs ranked highly according to the number of
different nouns they appear with in the BNC. Here
are the verbs in alphabetical order:
bring, find, get, give, hold, keep, lose, make, put, see, set, take
To guarantee that the final set of expressions con-
tains pairs from all four classes, we pseudo-randomly
select them from the initial list of pairs extracted from
the BNC as explained above. To ensure the inclusion
of IDMs, we consult two idioms dictionaries (Cowie
et al, 1983; Seaton and Macaulay, 2002). To en-
sure we include LVCs, we select pairs in which the
noun has a morphologically related verb according
to WordNet. We also select pairs whose noun is not
morphologically related to any verb to ensure the in-
clusion of LIT combinations.
This selection process resulted in 632 pairs, re-
duced to 563 after annotation (see Section 4.2 for
details on annotation). Out of these, 148 are LIT,
196 are ABS, 102 are LVC, and 117 are IDM. We
randomly choose 102 pairs from each class as our
final experimental expressions. We then pseudo-
randomly divide these into training (TRAIN), devel-
opment (DEV), and test (TEST) data sets, so that each
set has an equal number of pairs from each class. In
addition, we ensure that pairs with the same verb that
belong to the same class are divided equally among
the three sets. Our final TRAIN, DEV, and TEST sets
3Experiments on development data show that looking at all
senses of a noun degrades performance.
4http://www.natcorp.ox.ac.uk.
5http://tedlab.mit.edu/?dr/Tgrep2.
contain 240, 84, and 84 pairs, respectively.
4.2 Human Judgments
We asked four native speakers of English with suf-
ficient linguistic background to annotate our exper-
imental expressions. The annotation task was ex-
pected to be time-consuming, hence it was not feasi-
ble for all the judges to annotate all the expressions.
Instead, we asked one judge to be our primary anno-
tator, PA henceforth. (PA is an author of this paper,
but the other three judges are not.)
First, PA annotated all the 632 expressions selected
as described in Section 4.1, and removed 69 of them
that could be potential sources of disagreement for
various reasons (e.g., if an expression was unfamil-
iar or was likely to be part of a larger phrase). Next,
we divided the remaining 563 pairs into three equal-
sized sets, and gave each set to one of the other
judges to annotate. The judges were given a com-
prehensive guide for the task, in which the classes
were defined solely in terms of their semantic prop-
erties. Since expressions were annotated out of con-
text (type-based), we asked the judges to annotate the
predominant meaning of each expression.
We use the annotations of PA as our gold standard
for evaluation, but use the annotations of the others
to measure inter-annotator agreement. The observed
agreement (po) between PA and each of the other
three annotators are 79.8%, 72.2%, and 67%, respec-
tively. The kappa (?) scores are .72, .62, and .56.
The reasonably high agreement scores confirm that
the classes are coherent and linguistically plausible.
4.3 Classification Strategy and Features
We use the decision tree induction system C5.0 as
our machine learning software, and the measures pro-
posed in Section 3 as features in our classification ex-
periments.6 We explore the relevance of each feature
group in the overall classification, as well as in iden-
tifying members of each individual class.
5 Experimental Results
We performed experiments on DEV to find features
most relevant for classification. These experiments
6Experiments on DEV using a Support Vector Machine algo-
rithm produced poorer results; we thus do not report them.
13
revealed that removing Simdist (t , v) resulted in bet-
ter performance. This is not surprising given that ba-
sic verbs are highly polysemous, and hence the distri-
butional context of a basic verb may not correspond
to any particular sense of it. We thus remove this
feature (from COMP) in experiments on TEST. Re-
sults presented here are on the TEST set; those on the
DEV set have similar trends. Here, we first look at the
overall performance of classification in Section 5.1.
Section 5.2 presents the results of classification for
the individual classes.
5.1 Overall Classification Performance
Table 3 presents the results of classification?in
terms of average accuracy (%Acc) and relative er-
ror reduction (%RER)?for the individual feature
groups, as well as for all groups combined. The base-
line (chance) accuracy is 25% since we have four
equal-sized classes in TEST. As can be seen, INST
features yield the lowest overall accuracy, around
36%, with a relative error reduction of only 14%
over the baseline. This shows that institutionaliza-
tion, although relevant, is not sufficient for distin-
guishing among different levels of semantic idiosyn-
crasy. Interestingly, FIXD features achieve the high-
est accuracy, 50%, with a relative error reduction of
33%, showing that fixedness is a salient aspect of se-
mantic idiosyncrasy. COMP features achieve reason-
ably good accuracy, around 40%, though still notably
lower than the accuracy of FIXD features. This is es-
pecially interesting since much previous research has
focused solely on the non-compositionality of MWEs
to identify them (McCarthy et al, 2003; Baldwin et
al., 2003; Bannard et al, 2003). Our results confirm
the relevance of this property, while at the same time
revealing its insufficiency. Interestingly, features re-
lated to the semantic properties of the constituents,
VERB and NSEM, overall perform comparably to the
compositionality features. However, a closer look at
their performance on the individual classes (see Sec-
tion 5.2) reveals that, unlike COMP, they are mainly
good at identifying items from certain classes. As
hypothesized, we achieve the highest performance,
an accuracy of 58% and a relative error reduction of
44%, when we combine all features.
Table 4 displays classification performance, when
we use all the feature groups except one. These re-
sults are more or less consistent with those in Ta-
Only the features in group %Acc (%RER)
INST 35.7 (14.3)
FIXD 50 (33.3)
COMP 40.5 (20.7)
VERB 42.9 (23.9)
NSEM 39.3 (19.1)
ALL 58.3 (44.4)
Table 3: Accuracy (%Acc) and relative error reduction
(%RER) over TEST pairs, for the individual feature groups, and
for all features combined.
All features except those in group %Acc (%RER)
INST 53.6 (38.1)
FIXD 47.6 (30.1)
COMP 56 (41.3)
VERB 48.8 (31.7)
NSEM 46.4 (28.5)
ALL 58.3 (44.4)
Table 4: Accuracy (%Acc) and relative error reduction
(%RER) over TEST pairs, removing one feature group at a time.
ble 3 above, except some differences which we dis-
cuss below. Removing FIXD features results in a
drastic decrease in performance (10.7%), while the
removal of INST and COMP features cause much
smaller drops in performance (4.7% and 2.3%, re-
spectively). Here again, we can see that features re-
lated to the semantics of the verb and the noun are
salient features. Removing either of these results
in a substantial decrease in performance?9.5% and
11.9%, respectively?which is comparable to the de-
crease resulting from removing FIXD features. This
is an interesting observation, since VERB and NSEM
features, on their own, do not perform nearly as well
as FIXD features. It is thus necessary to futher in-
vestigate the performance of these groups on larger
data sets with more variability in the verb and noun
constituents of the expressions.
5.2 Performance on Individual Classes
We now look at the performance of the feature
groups, both separately and combined, on the indi-
vidual classes. For each combination of class and
feature group, the F -measures of classification are
given in Table 5, with the two highest F -measures
for each class shown in boldface.7 These results
show that the combination of all feature groups yields
the best or the second-best performance on all four
classes. (In fact, in only one case is the performance
7Our F -measure gives equal weights to precision and recall.
14
Only the features in group
Class INST FIXD COMP VERB NSEM ALL
LIT .48 .42 .51 .54 .57 .60
ABS .40 .32 .17 .27 .49 .46
LVC .21 .58 .47 .55 - .68
IDM .33 .67 .42 0 - .56
Table 5: F -measures on TEST pairs, for individual feature
groups and all features combined.
ANNOTATOR1 ANNOTATOR2 ANNOTATOR3
Class %po ? %po ? %po ?
LIT 93.6 .83 88.3 .67 91.4 .78
ABS 83 .63 76.6 .46 78 .52
LVC 91 .71 83 .54 87.7 .61
IDM 92 .73 87.2 .63 87.2 .59
Table 6: Per-class observed agreement and kappa score be-
tween PA and each of the three annotators.
of ALL features notably smaller than the best perfor-
mance achieved by a single feature group.)
Looking at the performance of ALL features, we
can see that we get reasonably high F -measure for
all classes, except for ABS. The relatively low values
of po and ? on this class, as shown in Table 6, suggest
that this class was also the hardest to annotate. It is
possible that members of this class share properties
with other classes. The extremely poor performance
of the COMP features on ABS also reflects that per-
haps members of this class are not coherent in terms
of their degree of compositionality (e.g, compare give
confidence and make a living). In the future, we need
to incorporate more coherent membership criteria for
this class into our annotation procedure.
According to Table 5, the most relevant feature
group for identifying members of the LIT and ABS
classes is NSEM. This is expected since NSEM is a bi-
nary feature determining whether the noun is a PHYS-
ICAL ENTITY or an ABSTRACT ENTITY.8 Among
other feature groups, INST features also perform rea-
sonably well on both these classes. The most relevant
feature group for LVC and IDM is FIXD. (Note that
for IDM, the performance of this group is notably
higher than ALL). On the other hand, INST features
have a very poor performance on these classes, rein-
forcing that IDMs and LVCs may not necessarily ap-
pear with significantly high frequency of occurrence
in a given corpus. Fixedness features thus prove to be
8Since this is a binary feature, it can only distinguish two
classes. In the future, we need to include more semantic classes.
particularly important for the identification of highly
idiosyncratic MWEs, such as LVCs and IDMs.
6 Related Work
Much recent work on classifying MWEs focuses on
determining different levels of compositionality in
verb+particle combinations using a measure of distri-
butional similarity (McCarthy et al, 2003; Baldwin
et al, 2003; Bannard et al, 2003). Another group of
research attempts to classify a particular MWE sub-
type, such as verb-particle constructions (VPCs) or
LVCs, according to some fine-grained semantic crite-
ria (Wanner, 2004; Uchiyama et al, 2005; Cook and
Stevenson, 2006). Here, we distinguish subtypes of
MWEs that are defined according to coarse-grained
distinctions in their degree of semantic idiosyncrasy.
Wermter and Hahn (2004) recognize the impor-
tance of distinguishing MWE subtypes that are sim-
ilar to our four classes, but only focus on separat-
ing MWEs as one single class from literal combina-
tions. For this, they use a measure that draws on the
limited modifiability of MWEs, in addition to their
expected high frequency. Krenn and Evert (2001)
attempt to separate German idioms, LVCs, and lit-
eral phrases (of the form verb+prepositional phrase).
They treat LVCs and idioms as institutionalized ex-
pressions, and use frequency and several association
measures, such as PMI, for the task. The main goal
of their work is to find which association measures
are particularly suited for identifying which of these
classes. Here, we look at properties of MWEs other
than their institutionalization (the latter we quantify
using an association measure).
The work most similar to ours is that of Venkata-
pathy and Joshi (2005). They propose a minimally-
supervised classification schema that incorporates a
variety of features to group verb+noun combinations
according to their level of compositionality. Their
work has the advantage of requiring only a small
amount of manually-labeled training data. However,
their classes are defined on the basis of composition-
ality only. Here, we consider classes that are linguis-
tically salient, and moreover need special treatment
within a computational system. Our work is also dif-
ferent in that it brings in a new group of features, the
fixedness measures, which prove to be very effective
in identifying particular classes of MWEs.
15
7 Conclusions
We have provided an analysis of the important char-
acteristics pertaining to the semantic idiosyncrasy of
MWEs. We have also elaborated on the relation-
ship between these properties and four linguistically-
motivated classes of verb+noun combinations, falling
on a continuum from less to more semantically id-
iosyncratic. On the basis of such analysis, we
have developed statistical, corpus-based measures
that quantify each of these properties. Our results
confirm that these measures are effective in type clas-
sification of the MWEs under study. Our class-
based results look into the interaction between the
measures (each capturing a property of MWEs) and
the classes (which are defined in terms of seman-
tic idiosyncrasy). Based on this, we can see which
measures?or properties they relate to?are most or
least relevant for identifying each particular class of
verb+noun combinations. We are currently expand-
ing this work to investigate the use of similar mea-
sures in token classification of verb+noun combina-
tions in context.
Acknowledgements
We thank Eric Joanis for providing us with NP-head extraction
software. We thank Saif Mohammad for the CooccurrenceMa-
trix and the DistributionalDistance packages.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of mul-
tiword expression decomposability. In Proc. of ACL-
SIGLEX Wkshp. on Multiword Expressions, 89?96.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proc. of ACL-SIGLEX Wkshp. on Multi-
word Expressions, 65?72.
Laurel J. Brinton and Minoji Akimoto, eds. 1999. Col-
locational and Idiomatic Aspects of Composite Predi-
cates in the History of English. John Benjamins.
Miriam Butt. 2003. The light verb jungle. Workshop on
Multi-Verb Constructions.
Cristina Cacciari and Patrizia Tabossi, eds. 1993. Idioms:
Processing, Structure, and Interpretation. Lawrence
Erlbaum Associates, Publishers.
Kenneth Church, William Gale, Patrick Hanks, and Don-
ald Hindle. 1991. Using statistics in lexical analysis.
In Uri Zernik, editor, Lexical Acquisition: Exploiting
On-Line Resources to Build a Lexicon, 115?164.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, UPenn.
Paul Cook and Suzanne Stevenson. 2006. Classifying
particle semantics in English verb-particle construc-
tions. In Proc. of COLING-ACL?06 Wkshp. on Multi-
word Expressions, 45?53.
Anthony P. Cowie, Ronald Mackin, and Isabel R. McCaig.
1983. Oxford Dictionary of Current Idiomatic English,
volume 2. OUP.
Anthony P. Cowie. 1981. The treatment of collocations
and idioms in learner?s dictionaries. Applied Linguis-
tics, II(3):223?235.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-
ically constructing a lexicon of verb phrase idiomatic
combinations. In Proc. of EACL?06, 337?344.
Christiane Fellbaum, editor. 1998. WordNet, An Elec-
tronic Lexical Database. MIT Press.
Raymond W., Jr. Gibbs and Nandini P. Nayak. 1989. Psy-
chololinguistic studies on the syntactic behaviour of id-
ioms. Cognitive Psychology, 21:100?138.
Brigitte Krenn and Stefan Evert. 2001. Can we do bet-
ter than frequency? A case study on extracting PP-verb
collocations. In Proc. of ACL?01 Wkshp. on Colloca-
tions, 39?46.
Dekang Lin. 1998. Automatic retrieval and clustering of
similar words. In Proc. of COLING-ACL?98, 768?774.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proc. of ACL?99, 317?324.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proc. of ACL-SIGLEX Wkshp. on Multiword
Expressions, 73?80.
Tadao Miyamoto. 2000. The Light Verb Construction
in Japanese: the Role of the Verbal Noun. John Ben-
jamins.
Rosamund Moon. 1998. Fixed Expressions and Idioms in
English: A Corpus-Based Approach. Oxford Univer-
sity Press.
Geoffrey Nunberg, Ivan A. Sag, and Thomas Wasow.
1994. Idioms. Language, 70(3):491?538.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. In Proc. of CI-
CLing?02, 1?15.
Maggie Seaton and Alison Macaulay, eds. 2002. Collins
COBUILD Idioms Dictionary. HarperCollins.
Kiyoko Uchiyama, Timothy Baldwin, and Shun Ishizaki.
2005. Disambiguating Japanese compound verbs.
Computer Speech and Language, 19:497?512.
Sriram Venkatapathy and Aravind Joshi. 2005. Measur-
ing the relative compositionality of verb-noun (V-N)
collocations by integrating features. In Proc. of HLT-
EMNLP?05, 899?906.
Leo Wanner. 2004. Towards automatic fine-grained se-
mantic classification of verb-noun collocations. Natu-
ral Language Engineering, 10(2):95?143.
Joachim Wermter and Udo Hahn. 2004. Collocation ex-
traction based on modifiability statistics. In Proc. of
COLING?04, 980?986.
16
Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 41?48,
Prague, June 2007. c?2007 Association for Computational Linguistics
Pulling their Weight: Exploiting Syntactic Forms for the Automatic
Identification of Idiomatic Expressions in Context
Paul Cook and Afsaneh Fazly and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
fpcook,afsaneh,suzanneg@cs.toronto.edu
Abstract
Much work on idioms has focused on type
identification, i.e., determining whether a se-
quence of words can form an idiomatic ex-
pression. Since an idiom type often has a
literal interpretation as well, token classifi-
cation of potential idioms in context is criti-
cal for NLP. We explore the use of informa-
tive prior knowledge about the overall syn-
tactic behaviour of a potentially-idiomatic
expression (type-based knowledge) to de-
termine whether an instance of the expres-
sion is used idiomatically or literally (token-
based knowledge). We develop unsuper-
vised methods for the task, and show that
their performance is comparable to that of
state-of-the-art supervised techniques.
1 Introduction
Identification of multiword expressions (MWEs),
such as car park, make a decision, and kick the
bucket, is extremely important for accurate natural
language processing (NLP) (Sag et al, 2002). Most
MWEs need to be treated as single units of mean-
ing, e.g., make a decision roughly means ?decide?.
Nonetheless, the components of an MWE can be
separated, making it hard for an NLP system to iden-
tify the expression as a whole. Many researchers
have recently developed methods for the automatic
acquisition of various properties of MWEs from cor-
pora (Lin, 1999; Krenn and Evert, 2001; Baldwin et
al., 2003; McCarthy et al, 2003; Venkatapathy and
Joshi, 2005; Villada Moiro?n and Tiedemann, 2006;
Fazly and Stevenson, 2006). These studies look
into properties, such as the collocational behaviour
of MWEs, their semantic non-compositionality, and
their lexicosyntactic fixedness, in order to distin-
guish them from similar-on-the-surface literal com-
binations.
Most of these methods have been aimed at rec-
ognizing MWE types; less attention has been paid
to the identification of instances (tokens) of MWEs
in context. For example, most such techniques (if
successful) would identify make a face as a poten-
tial MWE. This expression is, however, ambiguous
between an idiom, as in The little girl made a funny
face at her mother, and a literal combination, as in
She made a face on the snowman using a carrot and
two buttons. Despite the common perception that
phrases that can be idioms are mainly used in their
idiomatic sense, our analysis of 60 idioms has shown
otherwise. We found that close to half of these id-
ioms also have a clear literal meaning; and of the ex-
pressions with a literal meaning, on average around
40% of their usages are literal. Distinguishing token
phrases as MWEs or literal combinations of words is
thus essential for NLP applications that require the
identification of multiword semantic units, such as
semantic parsing and machine translation.
Recent studies addressing MWE token classifi-
cation mainly perform the task as one of word
sense disambiguation, and draw on the local con-
text of an expression to disambiguate it. Such
techniques either do not use any information re-
garding the linguistic properties of MWEs (Birke
and Sarkar, 2006), or mainly focus on their non-
compositionality (Katz and Giesbrecht, 2006). Pre-
41
vious work on the identification of MWE types,
however, has found other properties of MWEs, such
as their syntactic fixedness, to be relevant to their
identification (Evert et al, 2004; Fazly and Steven-
son, 2006). In this paper, we propose techniques that
draw on this property to classify individual tokens of
a potentially idiomatic phrase as literal or idiomatic.
We also put forward classification techniques that
combine such information with evidence from the
local context of an MWE.
We explore the hypothesis that informative prior
knowledge about the overall syntactic behaviour of
an idiomatic expression (type-based knowledge) can
be used to determine whether an instance of the
expression is used literally or idiomatically (token-
based knowledge). Based on this hypothesis, we de-
velop unsupervised methods for token classification,
and show that their performance is comparable to
that of a standard supervised method.
Many verbs can be combined with one or more of
their arguments to form MWEs (Cowie et al, 1983;
Fellbaum, 2002). Here, we focus on a broadly doc-
umented class of idiomatic MWEs that are formed
from the combination of a verb with a noun in its di-
rect object position, as in make a face. In the rest
of the paper, we refer to these verb+noun combi-
nations, which are potentially idiomatic, as VNCs.
In Section 2, we propose unsupervised methods that
classify a VNC token as an idiomatic or literal usage.
Section 3 describes our experimental setup, includ-
ing experimental expressions and their annotation.
In Section 4, we present a detailed discussion of our
results. Section 5 compares our work with similar
previous studies, and Section 6 concludes the paper.
2 Unsupervised Idiom Identification
We first explain an important linguistic property at-
tributed to idioms?that is, their syntactic fixedness
(Section 2.1). We then propose unsupervised meth-
ods that draw on this property to automatically dis-
tinguish between idiomatic and literal usages of an
expression (Section 2.2).
2.1 Syntactic Fixedness and Canonical Forms
Idioms tend to be somewhat fixed with respect to
the syntactic configurations in which they occur
(Nunberg et al, 1994). For example, pull one?s
weight tends to mainly appear in this form when
used idiomatically. Other forms of the expression,
such as pull the weights, typically are only used
with a literal meaning. In their work on automati-
cally identifying idiom types, Fazly and Stevenson
(2006)?henceforth FS06?show that an idiomatic
VNC tends to have one (or at most a small number
of) canonical form(s), which are its most preferred
syntactic patterns. The preferred patterns can vary
across different idiom types, and can involve a num-
ber of syntactic properties: the voice of the verb (ac-
tive or passive), the determiner introducing the noun
(the, one?s, etc.), and the number of the noun (singu-
lar or plural). For example, while pull one?s weight
has only one canonical form, hold fire and hold one?s
fire are two canonical forms of the same idiom, as
listed in an idiom dictionary (Seaton and Macaulay,
2002).
In our work, we assume that in most cases, id-
iomatic usages of an expression tend to occur in a
small number of canonical form(s) for that idiom.
We also assume that, in contrast, the literal usages
of an expression are less syntactically restricted, and
are expressed in a greater variety of patterns. Be-
cause of their relative unrestrictiveness, literal us-
ages may occur in a canonical idiomatic form for
that expression, but usages in a canonical form are
more likely to be idiomatic. Usages in alternative
syntactic patterns for the expression, which we refer
to as the non-canonical forms of the idiom, are more
likely to be literal. Drawing on these assumptions,
we develop three unsupervised methods that deter-
mine, for each VNC token in context, whether it has
an idiomatic or a literal interpretation.
2.2 Statistical Methods
The following paragraphs elaborate on our proposed
methods for identifying the idiomatic and literal us-
ages of a VNC: the CForm method that uses knowl-
edge of canonical forms only, and two Diff methods
that draw on further contextual evidence as well. All
three methods draw on our assumptions described
above, that usages in the canonical form for an id-
iom are more likely to be idiomatic, and those in
other forms are more likely to be literal. Thus, for
all three methods, we need access to the canonical
form of the idiom. Since we want our token iden-
tification methods to be unsupervised, we adopt the
42
unsupervised statistical method of FS06 for finding
canonical forms for an idiomatic VNC. This method
determines the canonical forms of an expression to
be those forms whose frequency is much higher than
the average frequency of all its forms.
CForm: The underlying assumption of this
method is that information about the canonical
form(s) of an idiom type is extremely informative
in classifying the meaning of its individual instances
(tokens) as literal or idiomatic. Our CForm classi-
fies a token as idiomatic if it occurs in the automat-
ically determined canonical form(s) for that expres-
sion, and as literal otherwise.
Di : Our two Di methods combine local con-
text information with knowledge about the canon-
ical forms of an idiom type to determine if its to-
ken usages are literal or idiomatic. In developing
these methods, we adopt a distributional approach
to meaning, where the meaning of an expression is
approximated by the words with which it co-occurs
(Firth, 1957). Although there may be fine-grained
differences in meaning across the idiomatic usages
of an expression, as well as across its literal usages,
we assume that the idiomatic and literal usages cor-
respond to two coarse-grained senses of the expres-
sion. Since we further assume these two groups
of usages will have more in common semantically
within each group than between the two groups, we
expect that literal and idiomatic usages of an ex-
pression will typically occur with different sets of
words. We will refer then to each of the literal and
idiomatic designations as a (coarse-grained) mean-
ing of the expression, while acknowledging that
each may have multiple fine-grained senses. Clearly,
the success of our method depends on the extent to
which these assumptions hold.
We estimate the meaning of a set of usages of an
expression e as a word frequency vector ~v
e
where
each dimension i of ~v
e
is the frequency with which
e co-occurs with word i across the usages of e. We
similarly estimate the meaning of a single token of
an expression t as a vector ~v
t
capturing that usage.
To determine if an instance of an expression is literal
or idiomatic, we compare its co-occurrence vector to
the co-occurrence vectors representing each of the
literal and idiomatic meanings of the expression. We
use a standard measure of distributional similarity,
cosine, to compare co-occurrence vectors.
In supervised approaches, such as that of Katz and
Giesbrecht (2006), co-occurrence vectors for literal
and idiomatic meanings are formed from manually-
annotated training data. Here, we propose unsuper-
vised methods for estimating these vectors. We use
one way of estimating the idiomatic meaning of an
expression, and two ways for estimating its literal
meaning, yielding two methods for token classifica-
tion.
Our first Diff method draws further on our expec-
tation that canonical forms are more likely idiomatic
usages, and non-canonical forms are more likely lit-
eral usages. We estimate the idiomatic meaning of
an expression by building a co-occurrence vector,
~v
I -CF
, for all uses of the expression in its auto-
matically determined canonical form(s). Since we
hypothesize that idiomatic usages of an expression
tend to occur in its canonical form, we expect these
co-occurrence vectors to be largely representative of
the idiomatic usage of the expression. We similarly
estimate the literal meaning by constructing a co-
occurrence vector, ~v
L-NCF
, of all uses of the expres-
sion in its non-canonical forms. We use the term
Di
I-CF;L-NCF
to refer to this method.
Our second Diff method also uses the vector
~v
I -CF
to estimate the idiomatic meaning of an ex-
pression. However, this approach follows that of
Katz and Giesbrecht (2006) in assuming that literal
meanings are compositional. The literal meaning of
an expression is thus estimated by composing (sum-
ming and then normalizing) the co-occurrence vec-
tors for its component words. The resulting vec-
tor is referred to as ~v
L-Comp
, and this method as
Di
I-CF;L-Comp
.
For both Diff methods, if the meaning of
an instance of an expression is determined to
be more similar to its idiomatic meaning (e.g.,
cosine (~v
t
; ~v
I-CF
) > cosine (~v
t
; ~v
L-NCF
)), then
we label it as an idiomatic usage. Otherwise, it is
labeled as literal.1
1We also performed experiments using a KNN classifier
in which the co-occurrence vector for a token was compared
against the co-occurrence vectors for the canonical and non-
canonical forms of that expression, which were assumed to
be idiomatic and literal usages respectively. However, perfor-
mance was generally worse using this method.
43
Note that all three of our proposed techniques for
token identification depend on how accurately the
canonical forms of an expression can be acquired.
FS06?s canonical form acquisition technique, which
we use here, works well if the idiomatic usage of
a VNC is sufficiently frequent compared to its lit-
eral usage. In our experiments, we examine the
performance of our proposed classification methods
for VNCs with different proportions of idiomatic-to-
literal usages.
3 Experimental Setup
3.1 Experimental Expressions and Annotation
We use data provided by FS06, which consists of a
list of VNCs and their canonical forms. From this
data, we discarded expressions whose frequency in
the British National Corpus2 (BNC) is lower than
20, in an effort to make sure that there would be lit-
eral and idiomatic usages of each expression. The
frequency cut-off further ensures an accurate esti-
mate of the vectors representing each of the lit-
eral and idiomatic meanings of the expression. We
also discarded expressions that were not found in at
least one of two dictionaries of idioms (Seaton and
Macaulay, 2002; Cowie et al, 1983). This process
resulted in the selection of 60 candidate expressions.
For each of these 60 expressions, 100 sentences
containing its usage were randomly selected from
the automatically parsed BNC (Collins, 1999), using
the automatic VNC identification method described
by FS06. For an expression which occurs less than
100 times in the BNC, all of its usages were ex-
tracted. Our primary judge, a native English speaker
and an author of this paper, then annotated each use
of each candidate expression as one of literal, id-
iomatic, or unknown. When annotating a token, the
judge had access to only the sentence in which it oc-
curred, and not the surrounding sentences. If this
context was insufficient to determine the class of the
expression, the judge assigned the unknown label.
Idiomaticity is not a binary property, rather it is
known to fall on a continuum from completely se-
mantically transparent, or literal, to entirely opaque,
or idiomatic. The human annotators were required
to pick the label, literal or idiomatic, that best fit the
2http://www.natcorp.ox.ac.uk
usage in their judgment; they were not to use the un-
known label for intermediate cases. Figurative ex-
tensions of literal meanings were classified as literal
if their overall meaning was judged to be fairly trans-
parent, as in You turn right when we hit the road at
the end of this track (taken from the BNC). Some-
times an idiomatic usage, such as had words in I
was in a bad mood, and he kept pestering me, so
we had words, is somewhat directly related to its
literal meaning, which is not the case for more se-
mantically opaque idioms such as hit the roof. The
above sentence was classified as idiomatic since the
idiomatic meaning is much more salient than the lit-
eral meaning.
Based on the primary judge?s annotations, we re-
moved expressions with fewer than 5 instances of
either of their literal or idiomatic meanings, leav-
ing 28 expressions. The remaining expressions were
then split into development (DEV) and test (TEST)
sets of 14 expressions each. The data was divided
such that DEV and TEST would be approximately
equal with respect to the frequency, and proportion
of idiomatic-to-literal usages, of their expressions.
Before consensus annotation, DEV and TEST con-
tained a total of 813 and 743 tokens, respectively.
A second human judge, also a native English-
speaking author of this paper, then annotated DEV
and TEST. The observed agreement and unweighted
kappa score on TEST were 76% and 0:62 respec-
tively. The judges discussed tokens on which they
disagreed to achieve a consensus annotation. Final
annotations were generated by removing tokens that
received the unknown label as the consensus anno-
tation, leaving DEV and TEST with a total of 573 and
607 tokens, and an average of 41 and 43 tokens per
expression, respectively.
3.2 Creation of Co-occurrence Vectors
We create co-occurrence vectors for each expression
in our study from counts in the BNC. We form co-
occurrence vectors for the following items.
 Each token instance of the target expression
 The target expression in its automatically deter-
mined canonical form(s)
 The target expression in its non-canonical
form(s)
44
 The verb in the target expression
 The noun in the target expression
The co-occurrence vectors measure the frequency
with which the above items co-occur with each of
1000 content bearing words in the same sentence.3
The content bearing words were chosen to be the
most frequent words in the BNC which are used as
a noun, verb, adjective, adverb, or determiner. Al-
though determiners are often in a typical stoplist, we
felt it would be beneficial to use them here. Deter-
miners have been shown to be very informative in
recognizing the idiomaticity of MWE types, as they
are incorporated in the patterns used to automati-
cally determine canonical forms (Fazly and Steven-
son, 2006).4
3.3 Evaluation and Baseline
Our baseline for comparison is that of always pre-
dicting an idiomatic label, the most frequent class
in our development data. We also compare our un-
supervised methods against the supervised method
proposed by Katz and Giesbrecht (2006). In this
study, co-occurrence vectors for the tokens were
formed from uses of a German idiom manually an-
notated as literal or idiomatic. Tokens were classi-
fied in a leave-one-out methodology using k-nearest
neighbours, with k = 1. We report results using this
method (1NN) as well as one which considers a to-
ken?s 5 nearest neighbours (5NN). In all cases, we
report the accuracy macro-averaged across the ex-
perimental expressions.
4 Experimental Results and Analysis
In Section 4.1, we discuss the overall performance
of our proposed unsupervised methods. Section 4.2
explores possible causes of the differences observed
in the performance of the methods. We examine
our estimated idiomatic and literal vectors, and com-
pare them with the actual vectors calculated from
3We also considered 10 and 20 word windows on either side
of the target expression, but experiments on development data
indicated that using the sentence as a window performed better.
4We employed singular value decomposition (Deerwester et
al., 1990) to reduce the dimensionality of the co-occurrence
vectors. This had a negative effect on the results, likely be-
cause information about determiners, which occur frequently
with many expressions, is lost in the dimensionality reduction.
Method %Acc (%RER)
Baseline 61.9 -
Unsupervised Di
I -CF ; L-Comp
67.8 (15.5)
Di
I -CF ; L-NCF
70.1 (21.5)
CForm 72.4 (27.6)
Supervised 1NN 72.4 (27.6)
5NN 76.2 (37.5)
Table 1: Macro-averaged accuracy (%Acc) and relative error
reduction (%RER) over TEST.
manually-annotated data. Results reported in Sec-
tions 4.1 and 4.2 are on TEST (results on DEV have
very similar trends). Section 4.3 then examines the
performance of the unsupervised methods on ex-
pressions with different proportions of idiomatic-to-
literal usages. This section presents results on TEST
and DEV combined, as explained below.
4.1 Overall Performance
Table 4.1 shows the macro-averaged accuracy on
TEST of our three unsupervised methods, as well as
that of the baseline and the two supervised methods
for comparison (see Section 3.3). The best super-
vised performance and the best unsupervised perfor-
mance are indicated in boldface. As the table shows,
all three unsupervised methods outperform the base-
line, confirming that the canonical forms of an ex-
pression, and local context, are both informative in
distinguishing literal and idiomatic instances of the
expression.
The table also shows that Di
I -CF ;L-NCF
per-
forms better than Di
I -CF ;L-Comp
. This suggests
that estimating the literal meaning of an expression
using the non-canonical forms is more accurate than
using the composed vector, ~v
L-Comp
. In Section 4.2
we find more evidence for this. Another interesting
observation is that CForm has the highest perfor-
mance (among unsupervised methods), very closely
followed by Di
I -CF ;L-NCF
. These results confirm
our hypothesis that canonical forms?which reflect
the overall behaviour of a VNC type?are strongly
informative about the class of a token, perhaps even
more so than the local context of the token. Im-
portantly, this is the case even though the canonical
forms that we use are imperfect knowledge obtained
automatically through an unsupervised method.
Our results using 1NN, 72:4%, are comparable
45
Vectors cosine Vectors cosine
~a
idm
and ~a
lit
.55
~v
I -CF
and ~a
lit
.70 ~v
I -CF
and ~a
idm
.90
~v
L-NCF
and ~a
lit
.80 ~v
L-NCF
and ~a
idm
.60
~v
L-Comp
and ~a
lit
.72 ~v
L-Comp
and ~a
idm
.76
Table 2: Average similarity between the actual vectors (~a) and
the estimated vectors (~v), for the idiomatic and literal meanings.
to those of Katz and Giesbrecht (2006) using this
method on their German data (72%). However, their
baseline is slightly lower than ours at 58%, and
they only report results for 1 expression with 67 in-
stances. Interestingly, our best unsupervised results
are in line with the results using 1NN and not sub-
stantially lower than the results using 5NN.
4.2 A Closer Look into the Estimated Vectors
In this section, we compare our estimated idiomatic
and literal vectors with the actual vectors for these
usages calculated from manually-annotated data.
Such a comparison helps explain some of the differ-
ences we observed in the performance of the meth-
ods. Table 4.2 shows the similarity between the esti-
mated and actual vectors representing the idiomatic
and literal meanings, averaged over the 14 TEST ex-
pressions. Actual vectors, referred to as ~a
idm
and
~a
lit
, are calculated over idiomatic and literal usages
of the expressions as determined by the human an-
notations. Estimated vectors, ~v
I -CF
, ~v
L-CF
, and
~v
L-Comp
, are calculated using our methods described
in Section 2.2.
For comparison purposes, the first row of Ta-
ble 4.2 shows the average similarity between the
actual idiomatic and literal vectors, ~a
idm
and ~a
lit
.
These vectors are expected to be very dissimilar,
hence the low average cosine between them serves
as a baseline for comparison. We now look into the
relative similarity of each estimated vector, ~v
I -CF
,
~v
L-CF
, ~v
L-Comp
, with these two vectors.
The second row of the table shows that, as de-
sired, our estimated idiomatic vector, ~v
I -CF
, is no-
tably more similar to the actual idiomatic vector than
to the actual literal vector. Also, ~v
L-NCF
is more
similar to the actual literal vector than to the actual
idiomatic vector (third row). Surprisingly, however,
~v
L-Comp
is somewhat similar to both actual literal
and idiomatic vectors (in fact it is slightly more simi-
lar to the latter). These results suggest that the vector
composed of the context vectors for the constituents
of an expression may not always be the best estimate
of the literal meaning of the expression.5 Given this
observation, the overall better-than-baseline perfor-
mance of Di
I-CF;L-Comp
might seem unjustified at
a first glance. However, we believe this performance
is mainly due to an accurate estimate of ~v
I -CF
.
4.3 Performance Based on Class Distribution
We further divide our 28 DEV and TEST expres-
sions according to their proportion of idiomatic-to-
literal usages, as determined by the human annota-
tors. In order to have a sufficient number of expres-
sions in each group, here we merge DEV and TEST
(we refer to the new set as DT). DT
I
high
contains
17 expressions with 65%?90% of their usages be-
ing idiomatic?i.e., their idiomatic usage is domi-
nant. DT
I
low
contains 11 expressions with 8%?58%
of their occurrences being idiomatic?i.e., their id-
iomatic usage is not dominant.
Table 4.3 shows the average accuracy of all the
methods on these two groups of expressions, with
the best performance on each group shown in bold-
face. On DT
I
high
, both Di
I -CF ;L-NCF
and CForm
outperform the baseline, with CForm having the
highest reduction in error rate. The two methods per-
form similarly to each other on DT
I
low
, though note
that the error reduction of CForm is more in line
with its performance on DT
I
high
. These results show
that even for VNCs whose idiomatic meaning is
not dominant?i.e., those in DT
I
low
?automatically-
acquired canonical forms can help with their token
classification.
An interesting observation in Table 4.3 is the
inconsistent performance of Di
I -CF ;L-Comp
: the
method has a very poor performance on DT
I
high
, but
outperforms the other two unsupervised methods on
DT
I
low
. As we noted earlier in Section 2.2, the more
frequent the idiomatic meaning of an expression,
the more reliable the acquired canonical forms for
that expression. Since the performance of CForm
and Di
I -CF ;L-NCF
depends highly on the accu-
racy of the automatically acquired canonical forms,
it is not surprising that these two methods perform
5This was also noted by Katz and Giesbrecht (2006) in their
second experiment.
46
Method DT
I
high
DT
I
low
Baseline 81.4 (-) 35.0 (-)
Unsuper- Di
I -CF ; L-Comp
73.1 (-44.6) 58.6 (36.3)
vised Di
I -CF ; L-NCF
82.3 (4.8) 52.7 (27.2)
CForm 84.7 (17.7) 53.4 (28.3)
Super- 1NN 78.3 (-16.7) 65.8 (47.4)
vised 5NN 82.3 (4.8) 72.4 (57.5)
Table 3: Macro-averaged accuracy over DEV and TEST, di-
vided according to the proportion of idiomatic-to-literal usages.
worse than Di
I -CF ;L-Comp
on VNCs whose id-
iomatic usage is not dominant.
The high performance of the supervised meth-
ods on DT
I
low
also confirms that the poorer perfor-
mance of the unsupervised methods on these VNCs
is likely due to the inaccuracy of the canonical forms
extracted for them. Interestingly, when canonical
forms can be extracted with a high accuracy (i.e.,
for VNCs in DT
I
high
) the performance of the unsu-
pervised methods is comparable to (or even slightly
better than) that of the best supervised method. One
possible way of improving the performance of unsu-
pervised methods is thus to develop more accurate
techniques for the automatic acquisition of canoni-
cal forms.
5 Related Work
Various properties of MWEs have been exploited
in developing automatic identification methods for
MWE types (Lin, 1999; Krenn and Evert, 2001; Fa-
zly and Stevenson, 2006). Much research has ad-
dressed the non-compositionality of MWEs as an
important property related to their idiomaticity, and
has used it in the classification of both MWE types
and tokens (Baldwin et al, 2003; McCarthy et al,
2003; Katz and Giesbrecht, 2006). We also make
use of this property in an MWE token classification
task, but in addition, we draw on other salient char-
acteristics of MWEs which have been previously
shown to be useful for their type classification (Evert
et al, 2004; Fazly and Stevenson, 2006).
The idiomatic/literal token classification methods
of Birke and Sarkar (2006) and Katz and Giesbrecht
(2006) rely primarily on the local context of a to-
ken, and fail to exploit specific linguistic properties
of non-literal language. Our results suggest that such
properties are often more informative than the local
context, in determining the class of an MWE token.
The supervised classifier of Patrick and Fletcher
(2005) distinguishes between compositional and
non-compositional English verb-particle con-
struction tokens. Their classifier incorporates
linguistically-motivated features, such as the degree
of separation between the verb and particle. Here,
we focus on a different class of English MWEs,
verb+noun combinations. Moreover, by making
a more direct use of their syntactic behaviour, we
develop unsupervised token classification methods
that perform well. The unsupervised token classifier
of Hashimoto et al (2006) uses manually-encoded
information about allowable and non-allowable
syntactic transformations of Japanese idioms?that
are roughly equivalent to our notions of canonical
and non-canonical forms. The rule-based classifier
of Uchiyama et al (2005) incorporates syntac-
tic information about Japanese compound verbs
(JCVs), a type of MWE composed of two verbs.
In both cases, although the classifiers incorporate
syntactic information about MWEs, their manual
development limits the scalability of the approaches.
Uchiyama et al (2005) also propose a statistical
token classification method for JCVs. This method
is similar to ours, in that it also uses type-based
knowledge to determine the class of each token
in context. However, their method is supervised,
whereas our methods are unsupervised. Moreover,
Uchiyama et al (2005) evaluate their methods on a
set of JCVs that are mostly monosemous. Here, we
intentionally exclude such cases from consideration,
and focus on those MWEs that have two clear id-
iomatic and literal meanings, and that are frequently
used with either meaning.
6 Conclusions
While a great deal of research has focused on prop-
erties of MWE types, such as their compositional-
ity, less attention has been paid to issues surround-
ing MWE tokens. In this study, we have developed
techniques for a semantic classification of tokens of
a potential MWE in context. We focus on a broadly
documented class of English MWEs that are formed
from the combination of a verb and a noun in its
direct object position, referred to as VNCs. We an-
notated a total of 1180 tokens for 28 VNCs accord-
47
ing to whether they are a literal or idiomatic usage,
and we found that approximately 40% of the to-
kens were literal usages. These figures indicate that
automatically determining whether a VNC token is
used idiomatically or literally is of great importance
for NLP applications. In this work, we have pro-
posed three unsupervised methods that perform such
a task. Our proposed methods incorporate automati-
cally acquired knowledge about the overall syntactic
behaviour of a VNC type, in order to do token classi-
fication. More specifically, our methods draw on the
syntactic fixedness of VNCs?a property which has
been largely ignored in previous studies of MWE
tokens. Our results confirm the usefulness of this
property as incorporated into our methods. All our
methods outperform the baseline of always predict-
ing the most frequent class. Moreover, considering
our approach is unsupervised, our best accuracy of
72:4% is not substantially lower than the accuracy
of a standard supervised approach at 76:2%.
References
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL-SIGLEX Workshop on Multiword Ex-
pressions: Analysis, Acquisition and Treatment.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of nonlit-
eral language. In Proceedings of EACL-06, 329?336.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Anthony P. Cowie, Ronald Mackin, and Isabel R. Mc-
Caig. 1983. Oxford Dictionary of Current Idiomatic
English, volume 2. Oxford University Press.
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Stefan Evert, Ulrich Heid, and Kristina Spranger. 2004.
Identifying morphosyntactic preferences in colloca-
tions. In Proceedings LREC-04.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-
ically constructing a lexicon of verb phrase idiomatic
combinations. In Proceedings of EACL-06, 337?344.
Christiane Fellbaum. 2002. VP idioms in the lexicon:
Topics for research using a very large corpus. In
S. Busemann, editor, Proceedings of the KONVENS-
02 Conference.
John R. Firth. 1957. A synopsis of linguistic theory
1930?1955. In Studies in Linguistic Analysis (special
volume of the Philological Society), 1?32. The Philo-
logical Society, Oxford.
Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro.
2006. Japanese idiom recognition: Drawing a line be-
tween literal and idiomatic meanings. In Proceedings
of the COLING/ACL 2006 Main Conference Poster
Sessions, 353?360.
Graham Katz and Eugenie Giesbrecht. 2006. Auto-
matic identification of non-compositional multi-word
expressions using latent semantic analysis. In Pro-
ceedings of the ACL/COLING-06 Workshop on Multi-
word Expressions: Identifying and Exploiting Under-
lying Properties, 12?19.
Brigitte Krenn and Stefan Evert. 2001. Can we do better
than frequency? A case study on extracting PP-verb
collocations. In Proceedings of the ACL-01 Workshop
on Collocations.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL-99,
317?324.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL-SIGLEX Workshop
on Multiword Expressions: Analysis, Acquisition and
Treatment.
Geoffrey Nunberg, Ivan A. Sag, and Thomas Wasow.
1994. Idioms. Language, 70(3):491?538.
Jon Patrick and Jeremy Fletcher. 2005. Classifying verb-
particle constructions by verb arguments. In Proceed-
ings of the Second ACL-SIGSEM Workshop on the Lin-
guistic Dimensions of Prepositions and their use in
Computational Linguistics Formalisms and Applica-
tions, 200?209.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Proceedings
of CICLing-02, 1?15.
Maggie Seaton and Alison Macaulay, editors. 2002.
Collins COBUILD Idioms Dictionary. HarperCollins
Publishers, second edition.
Kiyoko Uchiyama, Timothy Baldwin, and Shun Ishizaki.
2005. Disambiguating Japanese compound verbs.
Computer Speech and Language, Special Issue on
Multiword Expressions, 19(4):497?512.
Sriram Venkatapathy and Aravid Joshi. 2005. Measur-
ing the relative compositionality of verb-noun (V-N)
collocations by integrating features. In Proceedings of
HLT/EMNLP-05, 899?906.
Begon?a Villada Moiro?n and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the EACL-06
Workshop on Multiword Expressions in a Multilingual
Context, 33?40.
48
Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 71?78,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
An Unsupervised Model for Text Message Normalization
Paul Cook
Department of Computer Science
University of Toronto
Toronto, Canada
pcook@cs.toronto.edu
Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
suzanne@cs.toronto.edu
Abstract
Cell phone text messaging users express them-
selves briefly and colloquially using a variety
of creative forms. We analyze a sample of cre-
ative, non-standard text message word forms
to determine frequent word formation pro-
cesses in texting language. Drawing on these
observations, we construct an unsupervised
noisy-channel model for text message normal-
ization. On a test set of 303 text message
forms that differ from their standard form, our
model achieves 59% accuracy, which is on par
with the best supervised results reported on
this dataset.
1 Text Messaging
Cell phone text messages?or SMS?contain many
shortened and non-standard forms due to a variety
of factors, particularly the desire for rapid text entry
(Grinter and Eldridge, 2001; Thurlow, 2003).1 Fur-
thermore, text messages are written in an informal
register; non-standard forms are used to reflect this,
and even for personal style (Thurlow, 2003). These
factors result in tremendous linguistic creativity, and
hence many novel lexical items, in the language of
text messaging, or texting language.
Normalization of non-standard forms?
converting non-standard forms to their standard
forms?is a challenge that must be tackled before
other types of natural language processing can
take place (Sproat et al, 2001). In the case of
text messages, text-to-speech synthesis may be
1The number of characters in a text message may also be
limited to 160 characters, although this is not always the case.
particularly useful for the visually impaired; au-
tomatic translation has also been considered (e.g.,
Aw et al, 2006). For texting language, given the
abundance of creative forms, and the wide-ranging
possibilities for creating new forms, normalization
is a particularly important problem, and has indeed
received some attention in computational linguistics
(e.g., Aw et al, 2006; Choudhury et al, 2007;
Kobus et al, 2008).
In this paper we propose an unsupervised noisy
channel method for texting language normalization,
that gives performance on par with that of a super-
vised system. We pursue unsupervised approaches
to this problem, as large collections of text mes-
sages, and their corresponding standard forms, are
not readily available.2 Furthermore, other forms of
computer-mediated communication, such as Inter-
net messaging, exhibit creative phenomena similar
to text messaging, although at a lower frequency
(Ling and Baron, 2007). Moreover, technological
changes, such as new input devices, are likely to
have an impact on the language of such media (Thur-
low, 2003).3 An unsupervised approach, drawing
on linguistic properties of creative word formations,
has the potential to be adapted for normalization of
text in other similar genres?such as Internet dis-
cussion forums?without the cost of developing a
large training corpus. Moreover, normalization may
be particularly important for such genres, given the
2One notable exception is Fairon and Paumier (2006), al-
though this resource is in French. The resource used in our
study, Choudhury et al (2007), is quite small in comparison.
3The rise of other technology, such as word prediction, could
reduce the use of abbreviations, although it?s not clear such
technology is widely used (Grinter and Eldridge, 2001).
71
Formation type Freq. Example
Stylistic variation 152 betta (better)
Subseq. abbrev. 111 dng (doing)
Prefix clipping 24 hol (holiday)
Syll. letter/digit 19 neway (anyway)
G-clipping 14 talkin (talking)
Phonetic abbrev. 12 cuz (because)
H-clipping 10 ello (hello)
Spelling error 5 darliog (darling)
Suffix clipping 4 morrow (tomorrow)
Punctuation 3 b/day (birthday)
Unclear 34 mobs (mobile)
Error 12 gal (*girl)
Total 400
Table 1: Frequency of texting forms in the development
set by formation type.
need for applications such as translation and ques-
tion answering.
We observe that many creative texting forms are
the result of a small number of specific word for-
mation processes. Rather than using a generic er-
ror model to capture all of them, we propose a mix-
ture model in which each word formation process is
modeled explicitly according to linguistic observa-
tions specific to that formation.
2 Analysis of Texting Forms
To better understand the creative processes present
in texting language, we categorize the word forma-
tion process of each texting form in our development
data, which consists of 400 texting forms paired with
their standard forms.4 Several iterations of catego-
rization were done in order to determine sensible
categories, and ensure categories were used consis-
tently. Since this data is only to be used to guide
the construction of our system, and not for formal
evaluation, only one judge (a native English speak-
ing author of this paper) categorized the expressions.
The findings are presented in Table 1.
Stylistic variations, by far the most frequent cat-
egory, exhibit non-standard spelling, such as repre-
4Most texting forms have a unique standard form; however,
some have multiple standard forms, e.g., will and well can both
be shortened to wl. In such cases we choose the category of the
most frequent standard form; in the case of frequency ties we
choose arbitrarily among the categories of the standard forms.
senting sounds phonetically. Subsequence abbrevi-
ations, also very frequent, are composed of a sub-
sequence of the graphemes in a standard form, of-
ten omitting vowels. These two formation types ac-
count for approximately 66% of our development
data; the remaining formation types are much less
frequent. Prefix clippings and suffix clippings con-
sist of a prefix or suffix, respectively, of a standard
form, and in some cases a diminutive ending; we
also consider clippings which omit just a g or h from
a standard form as they are rather frequent.5 A sin-
gle letter or digit can be used to represent a syllable;
we refer to these as syllabic (syll.) letter/digit. Pho-
netic abbreviations are variants of clippings and sub-
sequence abbreviations where some sounds in the
standard form are represented phonetically. Several
texting forms appear to be spelling errors; we took
the layout of letters on cell phone keypads into ac-
count when making this judgement. The items that
did not fit within the above texting form categories
were marked as unclear. Finally, for some expres-
sions the given standard form did not appear to be
appropriate. For example, girl is not the standard
form for the texting form gal; rather, gal is an En-
glish word that is a colloquial form of girl. Such
cases were marked as errors.
No texting forms in our development data corre-
spond to multiple standard form words, e.g., wanna
for want to.6 Since such forms are not present in our
development data, we assume that a texting form al-
ways corresponds to a single standard form word.
It is important to note that some text forms have
properties of multiple categories, e.g., bak (back)
could be considered a stylistic variation or a subse-
quence abbreviation. In such cases, we simply at-
tempt to assign the most appropriate category.
The design of our model for text message normal-
ization, presented below, uses properties of the ob-
served formation processes.
3 An Unsupervised Noisy Channel Model
for Text Message Normalization
Let S be a sentence consisting of standard forms
s1s2...sn; in this study the standard forms are reg-
5Thurlow (2003) also observes an abundance of g-clippings.
6A small number of similar forms, however, appear with a
single standard form word, and are therefore marked as errors.
72
ular English words. Let T be a sequence of texting
forms t1t2...tn, which are the texting language real-
ization of the standard forms, and may differ from
the standard forms. Given a sequence of texting
forms T , the challenge is then to determine the cor-
responding standard forms S.
Following Choudhury et al (2007)?and vari-
ous approaches to spelling error correction, such
as, e.g., Mays et al (1991)?we model text mes-
sage normalization using a noisy channel. We
want to find argmaxSP (S|T ). We apply Bayes
rule and ignore the constant term P (T ), giving
argmaxSP (T |S)P (S). Making the independence
assumption that each ti depends only on si, and not
on the context in which it occurs, as in Choudhury
et al, we express P (T |S) as a product of probabili-
ties: argmaxS (
?
i P (ti|si))P (S).
We note in Section 2 that many texting forms are
created through a small number of specific word for-
mation processes. Rather than model each of these
processes at once using a generic model for P (ti|si),
as in Choudhury et al, we instead create several such
models, each corresponding to one of the observed
common word formation processes. We therefore
rewrite P (ti|si) as ?wf P (ti|si,wf )P (wf) wherewf is a word formation process, e.g., subsequence
abbreviation. Since, like Choudhury et al, we focus
on the word model, we simplify our model as below.
argmaxsi
?
wf
P (ti|si,wf )P (wf )P (si)
We next explain the components of the model,
P (ti|si,wf ), P (wf ), and P (si), referred to as the
word model, word formation prior, and language
model, respectively.
3.1 Word Models
We now consider which of the word formation pro-
cesses discussed in Section 2 should be captured
with a word model P (ti|si,wf ). We model stylis-
tic variations and subsequence abbreviations simply
due to their frequency. We also choose to model
prefix clippings since this word formation process is
common outside of text messaging (Kreidler, 1979;
Algeo, 1991) and fairly frequent in our data. Al-
though g-clippings and h-clippings are moderately
frequent, we do not model them, as these very spe-
cific word formations are also (non-prototypical)
graphemes w i th ou t
phonemes w I T au t
Table 2: Grapheme?phoneme alignment for without.
subsequence abbreviations. We do not model syl-
labic letters and digits, or punctuation, explicitly; in-
stead, we simply substitute digits with a graphemic
representation (e.g., 4 is replaced by for), and re-
move punctuation, before applying the model. The
other less frequent formations?phonetic abbrevia-
tions, spelling errors, and suffix clippings?are not
modeled; we hypothesize that the similarity of these
formation processes to those we do model will allow
the system to perform reasonably well on them.
3.1.1 Stylistic Variations
We propose a probabilistic version of edit-
distance?referred to here as edit-probability?
inspired by Brill and Moore (2000) to model
P (ti|si, stylistic variation). To compute edit-
probability, we consider the probability of each edit
operation?substitution, insertion, and deletion?
instead of its cost, as in edit-distance. We then sim-
ply multiply the probabilities of edits as opposed to
summing their costs.
In this version of edit-probability, we allow two-
character edits. Ideally, we would compute the edit-
probability of two strings as the sum of the edit-
probability of each partitioning of those strings into
one or two character segments. However, following
Brill and Moore, we approximate this by the prob-
ability of the partition with maximum probability.
This allows us to compute edit-probability using a
simple adaptation of edit-distance, in which we con-
sider edit operations spanning two characters at each
cell in the chart maintained by the algorithm.
We then estimate two probabilities: P (gt|gs, pos)
is the probability of texting form grapheme gt given
standard form grapheme gs at position pos , where
pos is the beginning, middle, or end of the word;
P (ht|ps, hs, pos) is the probability of texting form
graphemes ht given the standard form phonemes ps
and graphemes hs at position pos . ht, ps, and hs can
be a single grapheme or phoneme, or a bigram.
We compute edit-probability between the
graphemes of si and ti. When filling each cell
in the chart, we consider edit operations between
73
segments of si and ti of length 0?2, referred to as a
and b, respectively. If a aligns with phonemes in si,
we also consider those phonemes, p. In our lexicon,
the graphemes and phonemes of each word are
aligned according to the method of Jiampojamarn
et al (2007). For example, the alignment for
without is given in Table 2. The probability of
each edit operation is then determined by three
properties?the length of a, whether a aligns with
any phonemes in si, and if so, p?as shown below:
|a|= 0 or 1, not aligned w/ si phonemes: P (b|a, pos)
|a|= 2, not aligned w/ si phonemes: 0
|a|= 1 or 2, aligned w/ si phonemes: P (b|p, a, pos)
3.1.2 Subsequence Abbreviations
We model subsequence abbreviations according
to the equation below:
P (ti|si, subseq abrv) =
{
c if ti is a subseq of si
0 otherwise
where c is a constant.
Note that this is similar to the error model for
spelling correction presented by Mays et al (1991),
in which all words (in our terms, all si) within
a specified edit-distance of the out-of-vocabulary
word (ti in our model) are given equal probability.
The key difference is that in our formulation, we
only consider standard forms for which the texting
form is potentially a subsequence abbreviation.
In combination with the language model,
P (ti|si, subseq abbrev) assigns a non-zero prob-
ability to each standard form si for which ti is
a subsequence, according to the likelihood of si
(under the language model). The models interact
in this way since we expect a standard form to be
recognizable relative to the other words for which ti
could be a subsequence abbreviation
3.1.3 Prefix Clippings
We model prefix clippings similarly to subse-
quence abbreviations.
P (ti|si, prefix clipping) =
?
??
??
c if ti is possible
pre. clip. of si
0 otherwise
Kreidler (1979) observes that clippings tend to be
mono-syllabic and end in a consonant. Further-
more, when they do end in a vowel, it is often
of a regular form, such as telly for television and
breaky for breakfast. We therefore only consider
P (ti|si, prefix clipping) if ti is a prefix clipping ac-
cording to the following heuristics: ti is mono-
syllabic after stripping any word-final vowels, and
subsequently removing duplicated word-final con-
sonants (e.g, telly becomes tel, which is a candidate
prefix clipping). If ti is not a prefix clipping accord-
ing to these criteria, P (ti|si) simply sums over all
models except prefix clipping.
3.2 Word Formation Prior
Keeping with our goal of an unsupervised method,
we estimate P (wf ) with a uniform distribution. We
also consider estimating P (wf ) using maximum
likelihood estimates (MLEs) from our observations
in Section 2. This gives a model that is not fully
unsupervised, since it relies on labelled training
data. However, we consider this a lightly-supervised
method, since it only requires an estimate of the fre-
quency of the relevant word formation types, and not
labelled texting form?standard form pairs.
3.3 Language Model
Choudhury et al (2007) find that using a bigram lan-
guage model estimated over a balanced corpus of
English had a negative effect on their results com-
pared with a unigram language model, which they
attribute to the unique characteristics of text messag-
ing that were not reflected in the corpus. We there-
fore use a unigram language model for P (si), which
also enables comparison with their results. Never-
theless, alternative language models, such as higher
order ngram models, could easily be used in place of
our unigram language model.
4 Materials and Methods
4.1 Datasets
We use the data provided by Choudhury et al (2007)
which consists of texting forms?extracted from a
collection of 900 text messages?and their manu-
ally determined standard forms. Our development
data?used for model development and discussed in
Section 2?consists of the 400 texting form types
that are not in Choudhury et al?s held-out test set,
and that are not the same as one of their standard
74
forms. The test data consists of 1213 texting forms
and their corresponding standard forms. A subset of
303 of these texting forms differ from their standard
form.7 This subset is the focus of this study, but we
also report results on the full dataset.
4.2 Lexicon
We construct a lexicon of potential standard forms
such that it contains most words that we expect to
encounter in text messages, yet is not so large as
to make it difficult to identify the correct standard
form. Our subjective analysis of the standard forms
in the development data is that they are frequent,
non-specialized, words. To reflect this observation,
we create a lexicon consisting of all single-word en-
tries containing only alphabetic characters found in
both the CELEX Lexical Database (Baayen et al,
1995) and the CMU Pronouncing Dictionary.8 We
remove all words of length one (except a and I) to
avoid choosing, e.g., the letter r as the standard form
for the texting form r. We further limit the lexicon
to words in the 20K most frequent alphabetic uni-
grams, ignoring case, in the Web 1T 5-gram Corpus
(Brants and Franz, 2006). The resulting lexicon con-
tains approximately 14K words, and excludes only
three of the standard forms?cannot, email, and on-
line?for the 400 development texting forms.
4.3 Model Parameter Estimation
MLEs for P (gt|gs, pos)?needed to estimate
P (ti|si, stylistic variation)?could be estimated
from texting form?standard form pairs. However,
since our system is unsupervised, no such data is
available. We therefore assume that many texting
forms, and other similar creative shortenings, occur
on the web. We develop a number of character
substitution rules, e.g., s? z, and use them to create
hypothetical texting forms from standard words.
We then compute MLEs for P (gt|gs, pos) using the
frequencies of these derived forms on the web.
7Choudhury et al report that this dataset contains 1228 tex-
ting forms. We found it to contain 1213 texting forms cor-
responding to 1228 standard forms (recall that a texting form
may have multiple standard forms). There were similar incon-
sistencies with the subset of texting forms that differ from their
standard forms. Nevertheless, we do not expect these small dif-
ferences to have an appreciable effect on the results.
8http://www.speech.cs.cmu.edu/cgi-bin/
cmudict
We create the substitution rules by examining ex-
amples in the development data, considering fast
speech variants and dialectal differences (e.g., voic-
ing), and drawing on our intuition. The derived
forms are produced by applying the substitution
rules to the words in our lexicon. To avoid con-
sidering forms that are themselves words, we elimi-
nate any form found in a list of approximately 480K
words taken from SOWPODS9 and the Moby Word
Lists.10 Finally, we obtain the frequency of the de-
rived forms from the Web 1T 5-gram Corpus.
To estimate P (ht|ps, hs, pos), we first esti-
mate two simpler distributions: P (ht|hs, pos) and
P (ht|ps, pos). P (ht|hs, pos) is estimated in the
same manner as P (gt|gs, pos), except that two char-
acter substitutions are allowed. P (ht|ps, pos) is es-
timated from the frequency of ps, and its align-
ment with ht, in a version of CELEX in which
the graphemic and phonemic representation of each
word is many?many aligned using the method of
Jiampojamarn et al (2007).11 P (ht|ps, hs, pos)
is then an evenly-weighted linear combination of
P (ht|hs, pos) and P (ht|ps, pos). Finally, we
smooth each of P (gt|gs, pos) and P (ht|ps, hs, pos)
using add-alpha smoothing.
We set the constant c in our word models for
subsequence abbreviations and prefix clippings such
that
?
si P (ti|si,wf )P (si) = 1. We similarly nor-
malize P (ti|si, stylistic variation)P (si).
We use the frequency of unigrams (ignoring case)
in the Web 1T 5-gram Corpus to estimate our lan-
guage model. We expect the language of text mes-
saging to be more similar to that found on the web
than that in a balanced corpus of English.
4.4 Evaluation Metrics
To evaluate our system, we consider three accuracy
metrics: in-top-1, in-top-10, and in-top-20.12 In-
top-n considers the system correct if a correct stan-
dard form is in the n most probable standard forms.
The in-top-1 accuracy shows how well the system
determines the correct standard form; the in-top-10
9http://en.wikipedia.org/wiki/SOWPODS
10http://icon.shef.ac.uk/Moby/
11We are very grateful to Sittichai Jiampojamarn for provid-
ing this alignment.
12These are the same metrics used by Choudhury et al
(2007), although we refer to them by different names.
75
Model % accuracy
Top-1 Top-10 Top-20
Uniform 59.4 83.8 87.8
MLE 55.4 84.2 86.5
Choudhury et al 59.9 84.3 88.7
Table 3: % in-top-1, in-top-10, and in-top-20 accuracy
on test data using both estimates for P (wf ). The results
reported by Choudhury et al (2007) are also shown.
and in-top-20 accuracies may be indicative of the
usefulness of the output of our system in other tasks
which could exploit a ranked list of standard forms,
such as machine translation.
5 Results and Discussion
In Table 3 we report the results of our system using
both the uniform estimate and the MLE of P (wf ).
Note that there is no meaningful random baseline
to compare against here; randomly ordering the
14K words in our lexicon gives very low accuracy.
The results using the uniform estimate of P (wf )?
a fully unsupervised system?are very similar to
the supervised results of Choudhury et al (2007).
Surprisingly, when we estimate P (wf ) using MLEs
from the development data?resulting in a lightly-
supervised system?the results are slightly worse
than when using the uniform estimate of this proba-
bility. Moreover, we observe the same trend on de-
velopment data where we expect to have an accurate
estimate of P (wf ) (results not shown). We hypothe-
size that the ambiguity of the categories of text forms
(see Section 2) results in poor MLEs for P (wf ),
thus making a uniform distribution, and hence fully-
unsupervised approach, more appropriate.
Results by Formation Type We now consider in-
top-1 accuracy for each word formation type, in Ta-
ble 4. We show results for the same word forma-
tion processes as in Table 1, except for h-clippings
and punctuation, as no words of these categories are
present in the test data. We present results using the
same experimental setup as before with a uniform
estimate of P (wf ) (All), and using just the model
corresponding to the word formation process (Spe-
cific), where applicable.13
13In this case our model then becomes, for each word forma-
tion process wf , argmaxsiP (ti|si,wf )P (si).
Formation type Freq. % in-top-1 acc.
n = 303 Specific All
Stylistic variation 121 62.8 67.8
Subseq. abbrev. 65 56.9 46.2
Prefix clipping 25 44.0 20.0
G-clipping 56 - 91.1
Syll. letter/digit 16 - 50.0
Unclear 12 - 0.0
Spelling error 5 - 80.0
Suffix clipping 1 - 0.0
Phonetic abbrev. 1 - 0.0
Error 1 - 0.0
Table 4: Frequency (Freq.), and % in-top-1 accuracy us-
ing the formation-specific model where applicable (Spe-
cific) and all models (All) with a uniform estimate for
P (wf ), presented by formation type.
We first examine the top panel of Table 3 where
we compare the performance on each word forma-
tion type for both experimental conditions (Specific
and All). We first note that the performance using
the formation-specific model on subsequence abbre-
viations and prefix clippings is better than that of
the overall model. This is unsurprising since we ex-
pect that when we know a texting form?s formation
process, and invoke a corresponding specific model,
our system should outperform a model designed to
handle a range of formation types. However, this is
not the case for stylistic variations; here the over-
all model performs better than the specific model.
We observed in Section 2 that some texting forms
do not fit neatly into our categorization scheme; in-
deed, many stylistic variations are also analyzable
as subsequence abbreviations. Therefore, the subse-
quence abbreviation model may benefit normaliza-
tion of stylistic variations. This model, used in iso-
lation on stylistic variations, gives an in-top-1 accu-
racy of 33.1%, indicating that this may be the case.
Comparing the performance of the individual
word models on only word types that they were de-
signed for (column Specific in Table 4), we see that
the prefix clipping model is by far the lowest, in-
dicating that in the future we should consider ways
of improving this word model. One possibility is
to incorporate phonemic knowledge. For example,
both friday and friend have the same probability un-
76
der P (ti|si, prefix clipping) for the texting form fri,
which has the standard form friday in our data. (The
language model, however, does distinguish between
these forms.) However, if we consider the phonemic
representations of these words, friday might emerge
as more likely. Syllable structure information may
also be useful, as we hypothesize that clippings will
tend to be formed by truncating a word at a syllable
boundary. We may similarly be able to improve our
estimate of P (ti|si, subseq. abrrev.). For example,
both text and taxation have the same probability un-
der this distribution, but intuitively text, the correct
standard form in our data, seems more likely. We
could incorporate knowledge about the likelihood of
omitting specific characters, as in Choudhury et al
(2007), to improve this estimate.
We now examine the lower panel of Table 4, in
which we consider the performance of the overall
model on the word formation types that are not ex-
plicitly modeled. The very high accuracy on g-
clippings indicates that since these forms are also a
type of subsequence abbreviation, we do not need to
construct a separate model for them. We in fact also
conducted experiments in which g-clippings and h-
clippings were modeled explicitly, but found these
extra models to have little effect on the results.
Recall from Section 3.1 our hypothesis that suf-
fix clippings, spelling errors, and phonetic abbrevia-
tions have common properties with formation types
that we do model, and therefore the system will per-
form reasonably well on them. Here we find pre-
liminary evidence to support this hypothesis as the
accuracy on these three word formation types (com-
bined) is 57.1%. However, we must interpret this
result cautiously as it only considers seven expres-
sions. On the syllabic letter and digit texting forms
the accuracy is 50.0%, indicating that our heuris-
tic to replace digits in texting forms with an ortho-
graphic representation is reasonable.
The performance on types of expressions that
we did not consider when designing the system?
unclear and error?is very poor. However, this has
little impact on the overall performance as these ex-
pressions are rather infrequent.
Results by Model We now consider in-top-1 ac-
curacy using each model on the 303 test expres-
sions; results are shown in Table 5. No model on its
Model % in-top-1 accuracy
Stylistic variation 51.8
Subseq. Abbrev. 44.2
Prefix clipping 10.6
Table 5: % in-top-1 accuracy on the 303 test expressions
using each model individually.
own gives results comparable to those of the over-
all model (59.4%, see Table 3). This indicates that
the overall model successfully combines informa-
tion from the specific word formation models.
Each model used on its own gives an accuracy
greater than the proportion of expressions of the
word formation type for which the model was de-
signed (compare accuracies in Table 5 to the num-
ber of expressions of the corresponding word forma-
tion type in the test data in Table 4). As we note in
Section 2, the distinctions between the word forma-
tion types are not sharp; these results show that the
shared properties of word formation types enable a
model for a specific formation type to infer the stan-
dard form of texting forms of other formation types.
All Unseen Data Until now we have discussed re-
sults on our test data of 303 texting forms which dif-
fer from their standard forms. We now consider the
performance of our system on all 1213 unseen tex-
ting forms, 910 of which are identical to their stan-
dard form. Since our model was not designed with
such expressions in mind, we slightly adapt it for
this new task; if ti is in our lexicon, we return that
form as si, otherwise we apply our model as usual,
using the uniform estimate of P (wf ). This gives
an in-top-1 accuracy of 88.2%, which is very sim-
ilar to the results of Choudhury et al (2007) on this
data of 89.1%. Note, however, that Choudhury et al
only report results on this dataset using a uniform
language model;14 since we use a unigram language
model, it is difficult to draw firm conclusions about
the performance of our system relative to theirs.
6 Related Work
Aw et al (2006) model text message normaliza-
tion as translation from the texting language into the
14Choudhury et al do use a unigram language model for their
experiments on the 303 texting forms which differ from their
standard forms (see Section 3.3).
77
standard language. Kobus et al (2008) incorporate
ideas from both machine translation and automatic
speech recognition for text message normalization.
However, both of these approaches are supervised,
and have only limited means for normalizing texting
forms that do not occur in the training data.
Our work, like that of Choudhury et al (2007),
can be viewed as a noisy-channel model for spelling
error correction (e.g., Mays et al, 1991; Brill and
Moore, 2000), in which texting forms are seen as
a kind of spelling error. Furthermore, like our ap-
proach to text message normalization, approaches to
spelling correction have incorporated phonemic in-
formation (Toutanova and Moore, 2002).
The word model of the supervised approach of
Choudhury et al consists of hidden Markov models,
which capture properties of texting language similar
to those of our stylistic variation model. We pro-
pose multiple word models?corresponding to fre-
quent texting language formation processes?and an
unsupervised method for parameter estimation.
7 Conclusions
We analyze a sample of texting forms to determine
frequent word formation processes in creative tex-
ting language. Drawing on these observations, we
construct an unsupervised noisy-channel model for
text message normalization. On an unseen test set
of 303 texting forms that differ from their standard
form, our model achieves 59% accuracy, which is on
par with that obtained by the supervised approach of
Choudhury et al (2007) on the same data.
More research is required to determine the impact
of our normalization method on the performance of
a system that further processes the resulting text. In
the future, we intend to improve our word models by
incorporating additional linguistic knowledge, such
as information about syllable structure. Since con-
text likely plays a role in human interpretation of
texting forms, we also intend to examine the perfor-
mance of higher order ngram language models.
Acknowledgements
This work is financially supported by the Natu-
ral Sciences and Engineering Research Council of
Canada, the University of Toronto, and the Dictio-
nary Society of North America.
References
John Algeo, editor. 1991. Fifty Years Among the New
Words. Cambridge University Press, Cambridge.
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for SMS text normaliza-
tion. In Proc. of the COLING/ACL 2006 Main Confer-
ence Poster Sessions, pages 33?40. Sydney.
R.H. Baayen, R. Piepenbrock, and L. Gulikers. 1995. The
CELEX Lexical Database (release 2). Linguistic Data
Consortium, University of Pennsylvania.
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Corpus version 1.1.
Eric Brill and Robert C. Moore. 2000. An improved error
model for noisy channel spelling correction. In Pro-
ceedings of ACL 2000, pages 286?293. Hong Kong.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure of
texting language. International Journal of Document
Analysis and Recognition, 10(3/4):157?174.
Ce?drick Fairon and Se?bastien Paumier. 2006. A trans-
lated corpus of 30,000 French SMS. In Proceedings of
LREC 2006. Genoa, Italy.
Rebecca E. Grinter and Margery A. Eldridge. 2001. y do
tngrs luv 2 txt msg. In Proceedings of the 7th Euro-
pean Conf. on Computer-Supported Cooperative Work
(ECSCW ?01), pages 219?238. Bonn, Germany.
Sittichai Jiampojamarn, Gregorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments and
hidden markov models to letter-to-phoneme conver-
sion. In Proc. of NAACL-HLT 2007, pages 372?379.
Rochester, NY.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing SMS: are two metaphors
better than one? In Proc. of the 22nd Int. Conf. on
Computational Linguistics, pp. 441?448. Manchester.
Charles W. Kreidler. 1979. Creating new words by short-
ening. English Linguistics, 13:24?36.
Rich Ling and Naomi S. Baron. 2007. Text messaging
and IM: Linguistic comparison of American college
data. Journal of Language and Social Psychology,
26:291?98.
Eric Mays, Fred J. Damerau, and Robert L. Mercer. 1991.
Context based spelling correction. Information Pro-
cessing and Management, 27(5):517?522.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
2001. Normalization of non-standard words. Com-
puter Speech and Language, 15:287?333.
Crispin Thurlow. 2003. Generation txt? The sociolin-
guistics of young people?s text-messaging. Discourse
Analysis Online, 1(1).
Kristina Toutanova and Robert C. Moore. 2002. Pronun-
ciation modeling for improved spelling correction. In
Proc. of ACL 2002, pages 144?151. Philadelphia.
78
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 244?254,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
A Cognitive Model of Semantic Network Learning
Aida Nematzadeh, Afsaneh Fazly, and Suzanne Stevenson
Department of Computer Science
University of Toronto
{aida,afsaneh,suzanne}@cs.toronto.edu
Abstract
Child semantic development includes
learning the meaning of words as well as
the semantic relations among words. A
presumed outcome of semantic develop-
ment is the formation of a semantic net-
work that reflects this knowledge. We
present an algorithm for simultaneously
learning word meanings and gradually
growing a semantic network, which ad-
heres to the cognitive plausibility require-
ments of incrementality and limited com-
putations. We demonstrate that the seman-
tic connections among words in addition
to their context is necessary in forming a
semantic network that resembles an adult?s
semantic knowledge.
1 Introduction
Child semantic development includes the acquisi-
tion of word-to-concept mappings (part of word
learning), and the formation of semantic connec-
tions among words/concepts. There is consid-
erable evidence that understanding the semantic
properties of words improves child vocabulary ac-
quisition. In particular, children are sensitive to
commonalities of semantic categories, and this
abstract knowledge facilitates subsequent word
learning (Jones et al., 1991; Colunga and Smith,
2005). Furthermore, representation of semantic
knowledge is significant as it impacts how word
meanings are stored in, searched for, and retrieved
from memory (Steyvers and Tenenbaum, 2005;
Griffiths et al., 2007).
Semantic knowledge is often represented as a
graph (a semantic network) in which nodes cor-
respond to words/concepts
1
, and edges specify
1
Here we assume that the nodes of a semantic network
are word forms and its edges are determined by the semantic
features of those words.
the semantic relations (Collins and Loftus, 1975;
Steyvers and Tenenbaum, 2005). Steyvers and
Tenenbaum (2005) demonstrated that a seman-
tic network that encodes adult-level knowledge of
words exhibits a small-world and scale-free struc-
ture. That is, it is an overall sparse network with
highly-connected local sub-networks, where these
sub-networks are connected through high-degree
hubs (nodes with many neighbours).
Much experimental research has investigated
the underlying mechanisms of vocabulary learn-
ing and characteristics of semantic knowledge
(Quine, 1960; Bloom, 1973; Carey and Bartlett,
1978; Gleitman, 1990; Samuelson and Smith,
1999; Jones et al., 1991; Jones and Smith,
2005). However, existing computational models
focus on certain aspects of semantic acquisition:
Some researchers develop computational models
of word learning without considering the acqui-
sition of semantic connections that hold among
words, or how this semantic knowledge is struc-
tured (Siskind, 1996; Regier, 2005; Yu and Bal-
lard, 2007; Frank et al., 2009; Fazly et al., 2010).
Another line of work is to model formation of
semantic categories but this work does not take
into account how word meanings/concepts are ac-
quired (Anderson and Matessa, 1992; Griffiths et
al., 2007; Fountain and Lapata, 2011).
Our goal in this work is to provide a cognitively-
plausible and unified account for both acquiring
and representing semantic knowledge. The re-
quirements for cognitive plausibility enforce some
constraints on a model to ensure that it is compa-
rable with the cognitive process it is formulating
(Poibeau et al., 2013). As we model semantic ac-
quisition, the first requirement is incrementality,
which means that the model learns gradually as
it processes the input. Also, there is a limit on
the number of computations the model performs
at each step.
In this paper, we present an algorithm for si-
244
multaneously learning word meanings and grow-
ing a semantic network, which adheres to the cog-
nitive plausibility requirements of incrementality
and limited computations. We examine networks
created by our model under various conditions,
and explore what is required to obtain a structure
that has appropriate semantic connections and has
a small-world and scale-free structure.
2 Related Work
Models of Word Learning. Given a word learn-
ing scenario, there are potentially many possible
mappings between words in a sentence and their
meanings (real-world referents), from which only
some mappings are correct (the mapping prob-
lem). One of the most dominant mechanisms
proposed for vocabulary acquisition is cross-
situational learning: people learn word mean-
ings by recognizing and tracking statistical reg-
ularities among the contexts of a word?s usage
across various situations, enabling them to nar-
row in on the meaning of a word that holds across
its usages (Siskind, 1996; Yu and Smith, 2007;
Smith and Yu, 2008). A number of computa-
tional models attempt to solve the mapping prob-
lem by implementing this mechanism, and have
successfully replicated different patterns observed
in child word learning (Siskind, 1996; Yu and Bal-
lard, 2007; Fazly et al., 2010). These models have
provided insight about underlying mechanisms of
word learning, but none of them consider the se-
mantic relations that hold among words, or how
the semantic knowledge is structured. Recently,
we have investigated properties of the semantic
structure of the resulting (final) acquired knowl-
edge of such a learner (Nematzadeh et al., 2014).
However, that work did not address how such
structural knowledge might develop and evolve in-
crementally within the learning model.
Models of Categorization. Computational mod-
els of categorization focus on the problem of form-
ing semantic clusters given a defined set of fea-
tures for words (Anderson and Matessa, 1992;
Griffiths et al., 2007; Sanborn et al., 2010). An-
derson and Matessa (1992) note that a cognitively
plausible categorization algorithm needs to be in-
cremental and only keep track of one potential
partitioning; they propose a Bayesian framework
(the Rational Model of Categorization or RMC)
that specifies the joint distribution on features and
category labels, and allows an unbounded number
of clusters. Sanborn et al. (2010) examine differ-
ent categorization models based on RMC. In par-
ticular, they compare the performance of the ap-
proximation algorithm of Anderson and Matessa
(1992) (local MAP) with two other approximation
algorithms (Gibbs Sampling and Particle Filters)
in various human categorization paradigms. San-
born et al. (2010) find that in most of the simula-
tions the local MAP algorithm performs as well as
the two other algorithms in matching human be-
havior.
The Representation of Semantic Knowledge.
There is limited work on computational models
of semantic acquisition that examine the represen-
tation of the semantic knowledge. Steyvers and
Tenenbaum (2005) propose an algorithm for build-
ing a network with small-world and scale-free
structure. The algorithm starts with a small com-
plete graph, incrementally adds new nodes to the
graph, and for each new node uses a probabilistic
mechanism for selecting a subset of current nodes
to connect to. However, their approach does not
address the problem of learning word meanings or
the semantic connections among them. Fountain
and Lapata (2011) propose an algorithm for learn-
ing categories that also creates a semantic network
by comparing all the possible word pairs. How-
ever, they too do not address the word learning
problem, and do not investigate the structure of the
learned semantic network to see whether it has the
properties observed in adult knowledge.
3 The Incremental Network Model
We propose here a model that unifies the incre-
mental acquisition of word meanings and forma-
tion of a semantic network structure that reflects
the similarities among those meanings. We use
an existing model to learn the meanings of words
(Section 3.1), and use those incrementally devel-
oping meanings as the input to the algorithm pro-
posed here for gradually growing a semantic net-
work (Section 3.2).
3.1 The Word Learner
We use the model of Fazly et al. (2010); this learn-
ing algorithm is incremental and involves limited
calculations, thus satisfying basic cognitive plausi-
bility requirements. A naturalistic language learn-
ing scenario consists of linguistic data in the con-
text of non-linguistic data, such as the objects,
245
Utterance: {let, find, a, picture, to, color }
Scene: {LET, PRONOUN, HAS POSSESSION, CAUSE,
ARTIFACT, WHOLE, CHANGE, . . .}
Table 1: A sample utterance-scene pair.
events, and social interactions that a child per-
ceives. This kind of input is modeled here as
a pair of an utterance (the words a child hears)
and a scene (the semantic features representing the
meaning of those words), as shown in Table 1 (and
described in more detail in Section 5.1). The word
learner is an instance of cross-situational learn-
ing applied to a sequence of such input pairs: for
each pair of a word w and a semantic feature f ,
the model incrementally learns P (f |w) from co-
occurrences of w and f across all the utterance-
scene pairs.
For each word, the probability distribution
over all semantic features, P (.|w), represents the
word?s meaning. The estimation of P (.|w) is
made possible by introducing a set of latent vari-
ables, alignments, that correspond to the possible
mappings between words and features in a given
utterance?scene pair. The learning problem is then
to find the mappings that best explain the data,
which is solved by using an incremental version
of the expectation?maximization (EM) algorithm
(Neal and Hinton, 1998). We skip the details of
the derivations and only report the resulting for-
mulas.
The model processes one utterance-scene pair at
a time. For the input pair processed at time t, first
the probability of each possible alignment (align-
ment probability) is calculated as:
2
P (a
ij
|u, f
i
) =
P
t?1
(f
i
|w
j
)
?
w
?
?u
P
t?1
(f
i
|w
?
)
(1)
where u is the utterance, and a
ij
is the alignment
variable specifying the word w
j
that is mapped
to the feature f
i
. P
t?1
(f
i
|w
j
) is taken from the
model?s current learned meaning of word w
j
. Ini-
tially, P
0
(f
i
|w
j
) is uniformly distributed. After
calculating the alignment probabilities, the learned
meanings are updated as:
P
t
(f
i
|w
j
) =
?
u?U
t
P (a
ij
|u, f
i
)
?
f
?
?M
?
u?U
t
P (a
ij
|u, f
?
)
(2)
where U
t
is the set of utterances processed so far,
andM is the set of features that the model has ob-
served. Note that for each w?f pair, the value of
the summations in this formula can be incremen-
tally updated after processing any utterance that
2
This corresponds to the expectation step of EM.
containsw; the summation does not have to be cal-
culated at every step.
3.2 Growing a Semantic Network
In our extended model, as we learn words incre-
mentally (as above), we also structure those words
into a semantic network based on the (partially)
learned meanings. At any given point in time, the
network will include as its nodes all the word types
the word learner has been exposed to. Weighted
edges (capturing semantic distance) will connect
those pairs of word types whose learned meanings
at that point are sufficiently semantically similar
(according to a threshold). Since the probabilis-
tic meaning of a word is adjusted each time it is
observed, a word may either lose or gain connec-
tions in the network after each input is processed.
Thus, to incrementally develop the network, at
each time step, our algorithm must both examine
existing connections (to see which edges should be
removed) and consider potential new connections
(to see which edges should be added).
A simple approach to achieve this is to examine
the current semantic similarity between a word w
in the input and all the current words in the net-
work, and include edges between only those word
pairs that are sufficiently similar. However, com-
paring w to all the words in the network each time
it is observed is computationally intensive (and not
cognitively plausible).
We present an approach for incrementally grow-
ing a semantic network that limits the computa-
tions when processing each input word w; see Al-
gorithm 1. After the meaning of w is updated, we
first check all the words that w is currently (di-
rectly) connected to, to see if any of those edges
need to be removed, or have their weight adjusted.
Next, to look for new connections forw, the idea is
to select only a small subset of words, S , to which
w will be compared. The challenge then is to se-
lect S in a way that will yield a network whose se-
mantic structure reasonably approximates the net-
work that would result from full knowledge of
comparing w to all the words.
Previous work has suggested picking ?impor-
tant? words (e.g., high-degree words) indepen-
dently of the target word w ? assuming these
might be words for which a learner might need
to understand their relationship to w in the future
(Steyvers and Tenenbaum, 2005). Our proposal
is instead to consider for S those words that are
246
Algorithm 1 Growing a network after each in-
put u.
for all w in u do
update P (.|w) using Eqn. (2)
update current connections of w
select S(w), a subset of words in the network
for all w
?
in S(w) do
if w and w
?
are sufficiently similar then
connect w and w
?
with an edge
end if
end for
end for
likely to be similar to w. That is, since the net-
work only needs to connect similar words to w, if
we can guess what (some of) those words are, then
we will do best at approximating the situation of
comparing w to all words.
The question now is how to find semantically
similar words to w that are not already connected
to w in the network. To do so, we incrementally
track semantic similarity among words usages as
their meanings are developing. Specifically we
cluster word tokens (not types) according to their
current word meanings. Since the probabilistic
meanings of words are continually evolving, in-
cremental clusters of word tokens can capture de-
veloping similarities among the various usages of
a word type, and be a clue to which words (types)
w might be similar to. In the next section, we de-
scribe the Bayesian clustering process we use to
identify potentially similar words.
3.3 Semantic Clustering of Word Tokens
We use the Bayesian framework of Anderson and
Matessa (1992) to form semantic clusters.
3
Recall
that for each word w, the model learns its mean-
ings as a probability distribution over all seman-
tic features, P (.|w). We represent this probability
distribution as a vector F whose length is the num-
ber of possible semantic features. Each element of
the vector holds the value P (f |w) (which is con-
tinuous). Given a word w and its vector F , we
need to calculate the probability that w belongs to
each existing cluster, and also allow for the pos-
sibility of it forming a new cluster. Using Bayes
rule we have:
P (k|F ) =
P (k)P (F |k)
?
k
?
P (k
?
)P (F |k
?
)
(3)
3
The distribution specified by this model is equivalent to
that of a Dirichlet Process Mixture Model (Neal, 2000).
where k is a given cluster. We thus need to calcu-
late the prior probability, P (k), and the likelihood
of each cluster, P (F |k).
Calculation of Prior. The prior probability that
word n + 1 is assigned to cluster k is calculated
as:
P (k) =
{
n
k
n+?
n
k
> 0
?
n+?
n
k
= 0 (new cluster)
(4)
where n
k
is the number of words in cluster k, n
is the number of words observed so far, and ? is a
parameter that determines how likely the creation
of a new cluster is. The prior favors larger clusters,
and also discourages the creation of new clusters
in later stages of learning.
Calculation of Likelihood. To calculate the like-
lihood P (F |k) in Eqn. (3), we assume that the fea-
tures are independent:
P (F |k) =
?
f
i
?F
P (f
i
= v|k) (5)
where P (f
i
= v|k) is the probability that the value
of the feature in dimension i is equal to v given
the cluster k. To derive P (f
i
|k), following An-
derson and Matessa (1992), we assume that each
feature given a cluster follows a Gaussian distri-
bution with an unknown variance ?
2
and mean ?.
(In the absence of any prior information about a
variable, it is often assumed to have a Gaussian
distribution.) The mean and variance of this dis-
tribution are inferred using Bayesian analysis: We
assume the variance has an inverse ?
2
prior, where
?
2
0
is the prior variance and a
0
is the confidence in
the prior variance:
?
2
? Inv-?
2
(a
0
, ?
2
0
) (6)
The mean given the variance has a Gaussian dis-
tribution with ?
0
as the prior mean and ?
0
as the
confidence in the prior mean.
?|? ? N(?
0
,
?
2
?
0
) (7)
Given the above conjugate priors, P (f
i
|k) can
be calculated analytically and is a Student?s t dis-
tribution with the following parameters:
P (f
i
|k) ? t
a
i
(?
i
, ?
2
i
(1 +
1
?
i
)) (8)
?
i
= ?
0
+ n
k
(9)
a
i
= a
0
+ n
k
(10)
247
?i
=
?
0
?
0
+ n
k
?
f
?
0
+ n
k
(11)
?
2
i
=
a
0
?
2
0
+ (n
k
? 1)s
2
+
?
0
n
k
?
0
+n
k
(?
0
+
?
f)
2
a
0
+ n
k
(12)
where
?
f and s
2
are the sample mean and variance
of the values of f
i
in k.
Note that in the above equations, the mean and
variance of the distribution are simply derived by
combining the sample mean and variance with
the prior mean and variance while considering the
confidence in the prior mean (?
0
) and variance
(a
0
). This means that the number of computations
to calculate P (F |K) is limited as w is only com-
pared to the ?prototype? of each cluster, which is
represented by ?
i
and ?
i
of different features.
Adding a word w to a cluster. We add w to
the cluster k with highest posterior probability,
P (k|F ), as calculated in Eqn. (3).
4
The parame-
ters of the selected cluster (k, ?
i
, ?
i
, ?
i
, and a
i
for
each feature f
i
) are then updated incrementally.
Using the Clusters to Select the Words in S(w).
We can now form S(w) in Algorithm 1 by select-
ing a given number of words n
s
whose tokens are
probabilistically chosen from the clusters accord-
ing to how likely each cluster k is given w: the
number of word tokens picked from each k is pro-
portional to P (k|F ) and is equal to P (k|F )?n
s
.
4 Evaluation
We evaluate a semantic network in two regards:
The semantic connectivity of the network ? to
what extent the semantically-related words are
connected in the network; and the structure of the
network ? whether it exhibits a small-world and
scale-free structure or not.
4.1 Evaluating Semantic Connectivity
The distance between the words in the network in-
dicates their semantic similarity: the more similar
a word pair, the smaller their distance. For word
pairs that are connected via a path in the network,
this distance is the weighted shortest path length
between the two words. If there is no path be-
tween a word pair, their distance is considered to
be? (which is represented with a large number).
We refer to this distance as the ?learned? semantic
similarity.
4
This approach is referred to as local MAP (Sanborn et al.,
2010); because of the incremental nature of the algorithm, it
maximizes the current posterior distribution as opposed to the
?global? posterior.
To evaluate the semantic connectivity of the
learned network, we compare these learned sim-
ilarity scores to ?gold-standard? similarity scores
that are calculated using the WordNet similarity
measure of Wu and Palmer (1994) (also known as
the WUP measure). We choose this measure since
it captures the same type of similarity as in our
model: words are considered similar if they belong
to the same semantic category. Moreover, this
measure does not incorporate information about
other types of similarities, for example, words are
not considered similar if they occur in similar con-
texts. Thus, the scores calculated with this mea-
sure are comparable with those of our learned net-
work.
Given the gold-standard similarity scores for
each word pair, we evaluate the semantic con-
nectivity of the network based on two perfor-
mance measures: coefficient of correlation and
the median rank of the first five gold-standard as-
sociates. Correlation is a standard way to com-
pare two lists of similarity scores (Budanitsky
and Hirst, 2006). We create two lists, one con-
taining the gold-standard similarity scores for all
word pairs, and the other containing their corre-
sponding learned similarity scores. We calculate
the Spearman?s rank correlation coefficient, ?, be-
tween these two lists of similarity scores. Note
that the learned similarity scores reflect the seman-
tic distance among words whereas the WordNet
scores reflect semantic closeness. Thus, a nega-
tive correlation is best in our evaluation, where the
value of -1 corresponds to the maximum correla-
tion.
Following Griffiths et al. (2007), we also cal-
culate the median learned rank of the first five
gold-standard associates for all words: For each
word w, we first create a ?gold-standard? asso-
ciates list: we sort all other words based on their
gold-standard similarity to w, and pick the five
most similar words (associates) to w. Similarly,
we create a ?learned associate list? for w by sort-
ing all words based on their learned semantic simi-
larity tow. For all words, we find the ranks of their
first five gold-standard associates in their learned
associate list. For each associate, we calculate the
median of these ranks for all words. We only re-
port the results for the first three gold-standard as-
sociates since the pattern of results is similar for
the fourth and fifth associates; we refer to the me-
dian rank of first three gold-standard associates as
248
1st
, 2
nd
, and 3
rd
.
4.2 Evaluating the Structure of the Network
A network exhibits a small-world structure when
it is characterized by short path length between
most nodes and highly-connected neighborhoods
(Watts and Strogatz, 1998). We first explain how
these properties are measured for a graph with N
nodes and E edges. Then we discuss how these
properties are used in assessing the small-world
structure of a graph.
5
.
Short path lengths. Most of the nodes of
a small-world network are reachable from other
nodes via relatively short paths. For a connected
network (i.e., all the node pairs are reachable from
each other), this can be measured as the average
distance between all node pairs (Watts and Stro-
gatz, 1998). Since our networks are not connected,
we instead measure this property using the median
of the distances (d
median
) between all node pairs
(Robins et al., 2005), which is well-defined even
when some node pairs have a distance of?.
Highly-connected neighborhoods. The neigh-
borhood of a node n in a graph consists of n and
all of the nodes that are connected to it. A neigh-
borhood is maximally connected if it forms a com-
plete graph ?i.e., there is an edge between all
node pairs. Thus, the maximum number of edges
in the neighborhood of n is k
n
(k
n
? 1)/2, where
k
n
is the number of neighbors. A standard metric
for measuring the connectedness of neighbors of
a node n is called the local clustering coefficient
(C) (Watts and Strogatz, 1998), which calculates
the ratio of edges in the neighborhood of n (E
n
)
to the maximum number of edges possible for that
neighborhood:
C =
E
n
k
n
(k
n
? 1)/2
(13)
The local clustering coefficient C ranges between
0 and 1. To estimate the connectedness of all
neighborhoods in a network, we take the average
of C over all nodes, i.e., C
avg
.
Small-world structure. A graph exhibits a
small-world structure if d
median
is relatively small
and C
avg
is relatively high. To assess this for
a graph g, these values are typically compared
to those of a random graph with the same num-
ber of nodes and edges as g (Watts and Strogatz,
5
We take the description of these measures from Ne-
matzadeh et al. (2014)
1998; Humphries and Gurney, 2008). The ran-
dom graph is generated by randomly rearranging
the edges of the network under consideration (Er-
dos and R?enyi, 1960). Because any pair of nodes
is equally likely to be connected as any other, the
median of distances between nodes is expected to
be low for a random graph. In a small-world net-
work, this value d
median
is expected to be as small
as that of a random graph: even though the random
graph has edges more uniformly distributed, the
small-world network has many locally-connected
components which are connected via hubs. On the
other hand, C
avg
is expected to be much higher
in a small-world network compared to its corre-
sponding random graph, because the edges of a
random graph typically do not fall into clusters
forming highly connected neighborhoods.
Given these two properties, the ?small-
worldness? of a graph g is measured as follows
(Humphries and Gurney, 2008):
?
g
=
C
avg
(g)
C
avg
(random)
d
median
(g)
d
median
(random)
(14)
where random is the random graph correspond-
ing to g. In a small-world network, it is ex-
pected that C
avg
(g)  C
avg
(random) and
d
median
(g) ? d
median
(random), and thus ?
g
>
1.
Note that Steyvers and Tenenbaum (2005) made
the empirical observation that small-world net-
works of semantic knowledge had a single con-
nected component that contained the majority of
nodes in the network. Thus, in addition to ?
g
,
we also measure the relative size of a network?s
largest connected component having size N
lcc
:
size
lcc
=
N
lcc
N
(15)
Scale-free structure. A scale-free network has
a relatively small number of high-degree nodes
that have a large number of connections to other
nodes, while most of its nodes have a small de-
gree, as they are only connected to a few nodes.
Thus, if a network has a scale-free structure, its de-
gree distribution (i.e., the probability distribution
of degrees over the whole network) will follow a
power-law distribution (which is said to be ?scale-
free?). We evaluate this property of a network by
plotting its degree distribution in the logarithmic
scale, which (if a power-law distribution) should
appear as a straight line. None of our networks ex-
249
hibit a scale-free structure; thus, we do not report
the results of this evaluation, and leave it to future
work for further investigation.
5 Experimental Set-up
5.1 Input Representation
Recall that the input to the model consists of a
sequence of utterance?scene pairs intended to re-
flect the linguistic data a child is exposed to, along
with the associated meaning a child might grasp.
As in much previous work (Yu and Ballard, 2007;
Fazly et al., 2010), we take child-directed utter-
ances from the CHILDES database (MacWhinney,
2000) in order to have naturalistic data. In partic-
ular, we use the Manchester corpus (Theakston et
al., 2001), which consists of transcripts of conver-
sations with 12 British children between the ages
of 1; 8 and 3; 0. We represent each utterance as
a bag of lemmatized words (see Utterance in Ta-
ble 1).
For the scene representation, we have no large
corpus to draw on that encodes the semantic por-
tion of language acquisition data.
6
We thus auto-
matically generate the semantics associated with
an utterance, using a scheme first introduced in
Fazly et al. (2010). The idea is to first create an
input generation lexicon that provides a mapping
between all the words in the input data and their
associated meanings. A scene is then represented
as a set that contains the meanings of all the words
in the utterance. We use the input generation lexi-
con of Nematzadeh et al. (2012) because the word
meanings reflect information about their semantic
categories, which is crucial to forming the seman-
tic clusters as in Section 3.3.
In this lexicon, the ?true? meaning for each
word w is a vector over a set of possible seman-
tic features for each part of speech; in the vec-
tor, each feature is associated with a score for that
word (see Figure 1). Depending on the word?s part
of speech, the features are extracted from various
6
Yu and Ballard (2007) created a corpus by hand-coding
the objects and cues that were present in the environment,
but that corpus is very small. Frank et al. (2013) provide a
larger manually annotated corpus (5000 utterances), but it is
still very small for longitudinal simulations of word learn-
ing. (Our corpus contains more than 100,000 utterances.)
Moreover, the corpus of Frank et al. (2013) is limited be-
cause a considerable number of words are not semantically
coded. (Only a subset of concrete objects in the environment
are coded.)
apple: { FOOD:1, SOLID:.72, ? ? ? , PLANT-PART:.22,
PHYSICAL-ENTITY:.17, WHOLE:.06, ? ? ? }
Figure 1: Sample true meaning features & their scores for
apple from Nematzadeh et al. (2012).
lexical resources such as WordNet
7
, VerbNet
8
, and
Harm (2002). The score for each feature is calcu-
lated using a measure similar to tf-idf that reflects
the association of the feature with the word and
with its semantic category: term frequency indi-
cates the strength of association of the feature with
the word, and inverse document frequency (where
the documents are the categories) indicates how
informative a feature is for that category. The se-
mantic categories of nouns (which we focus on in
our networks) are given by WordNet lex-names
9
,
a set of 25 general categories of entities. (We use
only nouns in our semantic networks because the
semantic similarity of words with different parts
of speech cannot be compared, since their seman-
tic features are drawn from different resources.)
The input generation lexicon is used to generate
a scene representation for an utterance as follows:
For each word w in the utterance, we probabilisti-
cally sample features, in proportion to their score,
from the full set of features in its true meaning.
The probabilistic sampling allows us to simulate
the noise and uncertainty in the input a child per-
ceives by omitting some meaning features from
the scene. The scene representation is the union
of all the features sampled for all the words in the
utterance (see Scene in Table 1).
5.2 Methods
We experiment with our network-growth method
that draws on the incremental clustering, and cre-
ate ?upper-bound? and baseline networks for com-
parison. Note that all the networks are created
using our Algorithm 1 (page 4) to grow networks
incrementally, drawing on the learned meanings of
words and updating their connections on the basis
of this evolving knowledge. The only difference
in creating the networks resides in how the com-
parison set S(w) is chosen for each target word w
that is being added to the growing network at each
time step. We provide more details in the para-
graphs below.
7
http://wordnet.princeton.edu
8
http://verbs.colorado.edu/
?
mpalmer/
projects/verbnet.html
9
http://wordnet.princeton.edu/wordnet/
man/lexnames.5WN.html
250
Upper-bound. Recall that one of our main goals
is to substantially reduce the number of similar-
ity comparisons needed to grow a semantic net-
work, in contrast to the straightforward method of
comparing each w to all current words. At the
same time, we need to understand the impact of
the increased efficiency on the quality of the re-
sulting networks. We thus need to compare the
target properties of our networks that are learned
using a small comparison set S , to those of an
?upper-bound? network that takes into account all
the pair-wise comparisons among words. We cre-
ate this upper-bound network by setting S(w) to
contain all words currently in the network.
Baselines. On the other hand, we need to evalu-
ate the (potential) benefit of our cluster-driven se-
lection process over a more simplistic approach to
selecting S(w). To do so, we consider three base-
lines, each using a different criteria for choosing
the comparison set S(w): The Random baseline
chooses the members of this set randomly from
the set of all observed words. The Context base-
line can be seen as an ?informed? baseline that at-
tempts to incorporate some semantic knowledge:
Here, we select words that are in the recent context
prior to w in the input, assuming that such words
are likely to be semantically related to w. We also
include a third baseline, Random+Context, that
picks half of the members of S randomly and half
of them from the prior context.
Cluster-based Methods. We report results for
three cluster-based networks that differ in their
choice of S(w) as follows: The Clusters-only net-
work chooses words in S(w) from the set of clus-
ters, proportional to the probability of each clus-
ter k given word w (as explained in Section 3.3).
In order to incorporate different types of semantic
information in selecting S, we also create a Clus-
ters+Context network that picks half of the mem-
bers of S from clusters (as above), and half from
the prior context. For completeness, we include a
Clusters+Random network that similarly chooses
half of words in S from clusters and half randomly
from all observed words.
We have experimented with several other meth-
ods, but they all performed substantially worse
than the baselines, and hence we do not report
them here. E.g., we tried picking words in S from
the best cluster. We also tried a few methods in-
spired by (Steyvers and Tenenbaum, 2005): E.g.,
we examined a method where if a member of S(w)
was sufficiently similar to w, we added the direct
neighbors of that word to S. We also tried to grow
networks by choosing the members of S according
to the degree or frequency of nodes in the network.
5.3 Experimental Parameters
We use 20, 000 utterance?scene pairs as our train-
ing data. Recall that we use clustering to help
guide our semantic network growth algorithm.
Given the clustering algorithm in Section 3.3, we
are interested to find the set of clusters that best
explain the data. (Other clustering algorithms can
be used instead of this algorithm.) We perform
a search on the parameter space, and select the
parameter values that result in the best clustering,
based on the number of clusters and their average
F-score. The value of the clustering parameters
are as follows: ? = 49, ?
0
= 1.0, a
0
= 2.0,
?
0
= 0.0, and ?
0
= 0.05. Two nouns with fea-
ture vectors F
1
and F
2
are connected in the net-
work if cosine(F1, F2) is greater than or equal to
0.6. (This threshold was selected following em-
pirical examination of the similarity values we ob-
serve among the ?true? meaning in our input gen-
eration lexicon.) The weight on the edge that con-
nects these nouns specifies their semantic distance,
which is calculated as 1? cosine(F1, F2).
Because we aim for a network creation method
that is cognitively plausible in performing a lim-
ited number of word-to-word comparisons, we
need to ensure that all the different methods of
selecting the comparison set S(w) yield roughly
similar numbers of such comparisons. Keeping
the size of S constant does not guarantee this,
because each method can yield differing num-
bers of connections of the target word w to other
words. We thus parameterize the size of S for
each method to keep the number of computations
similar, based on experiments on the development
data. In development work we also found that hav-
ing an increasing size of S over time improved
the results, as more words were compared as the
knowledge of learned meanings improved. To
achieve this, we use a percentage of the words
in the network as the size of S. In practice, the
setting of this parameter yields a number of com-
parisons across all methods that is about 8% of
the maximum possible word-to-word comparisons
that would be performed in the naive (computa-
tionally intensive) approach.
251
Note that all the Cluster-based, Random and
Random+Context methods include a random se-
lection mechanism; thus, we run each of these
methods 50 times and report the average ?, me-
dian ranks and size
lcc
(see Section 4). For the net-
works (out of 50 runs) that exhibit a small-world
structure (small-worldness greater than one), we
report the average small-worldness. We also re-
port the percentage of runs whose resulting net-
work exhibit a small-world structure.
6 Experimental Results and Discussion
Table 2 presents our results, including the eval-
uation measures explained above, for the Upper-
bound, Baseline, and Cluster-based networks cre-
ated by the various methods described in Sec-
tion 5.2.
10
Recall that the Upper-bound network is formed
from examining a word?s similarity to all other
(observed) words when it is added to the network.
We can see that this network is highly connected
(0.85) and has a small-world structure (5.5). There
is a statistically significant correlation of the net-
work?s similarity measures with the gold standard
ones (?0.38). For this Upper-bound structure, the
median ranks of the first three associates are be-
tween 31 and 42. These latter two measures on
the Upper-bound network give an indication of the
difficulty of learning a semantic network whose
knowledge matches gold-standard similarities.
Considering the baseline networks, we note that
the Random network is actually somewhat bet-
ter (in connectivity and median ranks) than the
Context network that we thought would provide
a more informed baseline. Interestingly, the cor-
relation value for both networks is no worse than
for the Upper-bound. The combination of Ran-
dom+Context yields a slightly lower correlation,
and no better ranks or connectivity than Random.
Note that none of the baseline networks exhibit a
small world structure (?
g
 1 for all three, except
for one out of 50 runs for the Random method).
Recall that the Random network is not a net-
work resulting from randomly connecting word
pairs, but one that incrementally compares each
target word with a set of randomly chosen words
when considering possible new connections. We
suspect that this approach performs reasonably
well because it enables the model to find a broad
10
All the reported co-efficients of correlation (?) are statis-
tically significant at p < 0.01.
range of similar words to the target; this might be
effective especially because the learned meanings
of words are changing over time.
Turning to the Cluster-based methods, we see
that indeed some diversity in the comparison set
for a target word might be necessary to good
performance. We find that the measures on the
Clusters-only network are roughly the same as on
the Random one, but when we combine the two in
Clusters+Random we see an improvement in the
ranks achieved. It is possible that the selection
from clusters does not have sufficient diversity to
find some of the valid new connections for a word.
We note that the best results overall occur with
the Clusters+Context network, which combines
two approaches to selecting words that have good
potential to be similar to the target word. The
correlation coefficient for this network is at a re-
spectable 0.36, and the median ranks are the sec-
ond best of all the network-growth methods. Im-
portantly, this network shows the desired small-
world structure in most of the runs (77%), with
the highest connectivity and a small-world mea-
sure well over 1.
The fact that the Clusters+Context network is
better overall than the networks of the Clusters-
only and Context methods indicates that both clus-
ters and context are important in making ?in-
formed guesses? about which words are likely
to be similar to a target word. Given the small
number of similarity comparisons used in our ex-
periments (only around 8% of all possible word-
to-word comparisons), these observations suggest
that both the linguistic context and the evolving
relations among word usages (captured by the in-
cremental clustering of learned meanings) contain
information crucial to the process of growing a se-
mantic network in a cognitively plausible way.
7 Conclusions
We propose a unified model of word learning and
semantic network formation, which creates a net-
work of words in which connections reflect struc-
tured knowledge of semantic similarity between
words. The model adheres to the cognitive plau-
sibility requirements of incrementality and use of
limited computations. That is, when incremen-
tally adding or updating a word?s connections in
the network, the model only looks at a subset of
words rather than comparing the target word to all
the nodes in the network. We demonstrate that
252
Comparing all Pairs
Semantic Connectivity Small World
Method ? 1
st
2
nd
3
rd
size
lcc
?
g
(%)
Upper-bound ?0.38 31 41 42 0.85 5.5
Baselines
Random ?0.38 56 76.9 68.9 0.6 5.2 (2)
Context ?0.39 97 115 89 0.5 0
Random+Context ?0.36 63.3 87.2 79.1 0.6 0 (0)
Cluster-based Methods
Clusters-only ?0.32 58.6 72.0 71.6 0.7 5.5 (43)
Clusters+Context ?0.36 53.9 67.6 64.8 0.7 7.2 (77)
Clusters+Random ?0.35 48.1 61.2 58.1 0.7 6.9 (48)
Table 2: Connectivity and small-worldness measures for the Upper-bound, Baseline, and Cluster-based
network-growth methods; best performances across the Baseline and Cluster-based methods are shown
in bold. ?: co-efficient of correlation between similarities of word pairs in network and in gold-standard;
1
st
, 2
nd
, 3
rd
: median ranks of corresponding gold-standard associates given network similarities; size
lcc
:
proportion of network in the largest connected component; ?
g
: overall ?small-worldness?, should be
greater than 1; %: the percentage of runs whose resulting networks exhibit a small-world structure. Note
there are 1074 nouns in each network.
using the evolving knowledge of semantic con-
nections among words as well as their context of
usage enables the model to create a network that
shows the properties of adult semantic knowledge.
This suggests that the information in the semantic
relations among words and their context can effi-
ciently guide semantic network growth.
Acknowledgments
We would like to thank Varada Kolhatkar for valu-
able discussion and feedback. We are also grateful
for the financial support from NSERC of Canada,
and University of Toronto.
References
John R. Anderson and Michael Matessa. 1992. Ex-
plorations of an incremental, bayesian algorithm for
categorization. Machine Learning, 9(4):275?308.
Lois Bloom. 1973. One word at a time: The use of
single word utterances before syntax, volume 154.
Mouton The Hague.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13?
47.
Susan Carey and Elsa Bartlett. 1978. Acquiring a sin-
gle new word.
Allan M. Collins and Elizabeth F. Loftus. 1975. A
spreading-activation theory of semantic processing.
Psychological review, 82(6):407.
Eliana Colunga and Linda B. Smith. 2005. From the
lexicon to expectations about kinds: A role for asso-
ciative learning. Psychological Review, 112(2):347?
382.
Paul Erdos and Alfr?ed R?enyi. 1960. On the evolution
of random graphs. Publ. Math. Inst. Hungar. Acad.
Sci, 5:17?61.
Afsaneh Fazly, Afra Alishahi, and Suzanne Steven-
son. 2010. A probabilistic computational model of
cross-situational word learning. Cognitive Science,
34(6):1017?1063.
Trevor Fountain and Mirella Lapata. 2011. Incremen-
tal models of natural language category acquisition.
In Proceedings of the 32st Annual Conference of the
Cognitive Science Society.
Michael C. Frank, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009. Using speakers referential inten-
tions to model early cross-situational word learning.
Psychological Science.
Michael C. Frank, Joshua B. Tenenbaum, and Anne
Fernald. 2013. Social and discourse contributions
to the determination of reference in cross-situational
word learning. Language Learning and Develop-
ment, 9(1):1?24.
Lila Gleitman. 1990. The structural sources of verb
meanings. Language Acquisition, 1(1):3?55.
Thomas L. Griffiths, Mark Steyvers, and Joshua B.
Tenenbaum. 2007. Topics in semantic representa-
tion. Psychological review, 114(2):211.
Michael W. Harm. 2002. Building large scale dis-
tributed semantic feature sets with WordNet. Tech-
nical Report PDP.CNS.02.1, Carnegie Mellon Uni-
versity.
253
Mark D. Humphries and Kevin Gurney. 2008. Net-
work small-world-ness: a quantitative method for
determining canonical network equivalence. PLoS
One, 3(4):e0002051.
Susan S. Jones and Linda B. Smith. 2005. Object name
learning and object perception: a deficit in late talk-
ers. J. of Child Language, 32:223?240.
Susan S. Jones, Linda B. Smith, and Barbara Landau.
1991. Object properties and knowledge in early lex-
ical learning. Child Development, 62(3):499?516.
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk, volume 2: The Database.
Erlbaum, 3rd edition.
Radford M. Neal and Geoffrey E. Hinton. 1998. A
view of the em algorithm that justifies incremental,
sparse, and other variants. In Learning in graphical
models, pages 355?368. Springer.
Radford M. Neal. 2000. Markov chain sampling meth-
ods for dirichlet process mixture models. Journal
of computational and graphical statistics, 9(2):249?
265.
Aida Nematzadeh, Afsaneh Fazly, and Suzanne
Stevenson. 2012. Interaction of word learning and
semantic category formation in late talking. In Proc.
of CogSci?12.
Aida Nematzadeh, Afsaneh Fazly, and Suzanne
Stevenson. 2014. Structural differences in the se-
mantic networks of simulated word learners.
Thierry Poibeau, Aline Villavicencio, Anna Korhonen,
and Afra Alishahi, 2013. Computational Modeling
as a Methodology for Studying Human Language
Learning. Springer.
Willard Van Orman Quine. 1960. Word and Object.
MIT Press.
Terry Regier. 2005. The emergence of words: Atten-
tional learning in form and meaning. Cognitive Sci-
ence, 29:819?865.
Garry Robins, Philippa Pattison, and Jodie Woolcock.
2005. Small and other worlds: Global network
structures from local processes1. American Journal
of Sociology, 110(4):894?936.
Larissa K. Samuelson and Linda B. Smith. 1999. Early
noun vocabularies: do ontology, category structure
and syntax correspond? Cognition, 73(1):1 ? 33.
Adam N. Sanborn, Thomas L. Griffiths, and Daniel J.
Navarro. 2010. Rational approximations to rational
models: alternative algorithms for category learning.
Jeffery Mark Siskind. 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61:39?91.
Linda B. Smith and Chen Yu. 2008. Infants rapidly
learn word-referent mappings via cross-situational
statistics. Cognition, 106(3):1558?1568.
Mark Steyvers and Joshua B. Tenenbaum. 2005. The
large-scale structure of semantic networks: Statisti-
cal analyses and a model of semantic growth. Cog-
nitive science, 29(1):41?78.
Anna L. Theakston, Elena V. Lieven, Julian M. Pine,
and Caroline F. Rowland. 2001. The role of
performance limitations in the acquisition of verb?
argument structure: An alternative account. J. of
Child Language, 28:127?152.
Duncan J. Watts and Steven H. Strogatz. 1998. Col-
lective dynamics of small-worldnetworks. nature,
393(6684):440?442.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, pages 133?138. Association for Com-
putational Linguistics.
Chen Yu and Dana H. Ballard. 2007. A unified
model of early word learning: Integrating statistical
and social cues. Neurocomputing, 70(1315):2149
? 2165. Selected papers from the 3rd Interna-
tional Conference on Development and Learning
(ICDL 2004), Time series prediction competition:
the CATS benchmark.
Chen Yu and Linda B. Smith. 2007. Rapid word learn-
ing under uncertainty via cross-situational statistics.
Psychological Science, 18(5):414?420.
254
A Graph-Theoretic Framework for
Semantic Distance
Vivian Tsang?
University of Toronto
Suzanne Stevenson??
University of Toronto
Many NLP applications entail that texts are classified based on their semantic distance (how
similar or different the texts are). For example, comparing the text of a new document to that of
documents of known topics can help identify the topic of the new text. Typically, a distributional
distance is used to capture the implicit semantic distance between two pieces of text. However,
such approaches do not take into account the semantic relations between words. In this article, we
introduce an alternative method of measuring the semantic distance between texts that integrates
distributional information and ontological knowledge within a network flow formalism. We first
represent each text as a collection of frequency-weighted concepts within an ontology. We then
make use of a network flow method which provides an efficient way of explicitly measuring the
frequency-weighted ontological distance between the concepts across two texts. We evaluate our
method in a variety of NLP tasks, and find that it performs well on two of three tasks. We develop
a new measure of semantic coherence that enables us to account for the performance difference
across the three data sets, shedding light on the properties of a data set that lends itself well to
our method.
1. Introduction
Many natural language tasks can be cast as a problem of comparing texts in terms of
their semantic distance. For example, given a suitable text distance measure, document
classification can be performed by comparing the text of a new document to the text of
various documents whose topics are known. The new document is then labelled with
the topic of the document whose text is most similar to it. In general, the texts to be
compared may be full documents, as in this example, or may be portions of documents,
or even collections of documents. Using text comparison to perform semantic classifi-
cation has been adopted in a variety of natural language processing (NLP) tasks, from
document classification (Scott and Matwin 1998; Rennie 2001; Al-Mubaid and Umair
2006), to prepositional phrase attachment (Pantel and Lin 2000), to spelling correction
(Budanitsky and Hirst 2001).
? Department of Computer Science, University of Toronto, 6 King?s College Road, Toronto, Ontario M5S
3G4, Canada. E-mail: vyctsang@cs.toronto.edu.
?? Department of Computer Science, University of Toronto, 6 King?s College Road, Toronto, Ontario M5S
3G4, Canada. E-mail: suzanne@cs.toronto.edu.
Submission received: 16 December 2007; revised submission received: 18 June 2008; accepted for publication:
20 August 2008.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 1
Distributional methods for semantic distance are widely used and highly successful
in comparing texts that are represented as bags of words with associated frequencies
of occurrence (Lee 2001; Weeds, Weir, and McCarthy 2004; Pedersen, Banerjee, and
Patwardhan 2005). In document classification, for example, the text of a document may
be represented as a word frequency vector, which is compared using a distributional
distance measure to each of the word frequency vectors of the texts of the documents
of known topics. In this way, distributional distance between word vectors captures the
semantic distance between two texts that is implicitly encoded in the set of words used
in each.
Semantic distance can also be measured more explicitly, by using the relations in
an ontology as the direct encoding of semantic association. However, such approaches
have generally been limited to calculating the distance between two individual con-
cepts, rather than capturing the distance between two sets of concepts corresponding
to two texts. Numerous measures have been proposed, for example, for capturing
the distance between two concepts in WordNet, typically relying on the synonymy
(synset) and hyponymy (is-a) relations (Wu and Palmer 1994; Resnik 1995; Jiang and
Conrath 1997, among others). Using such an ontological measure to compare two texts
(collections of words instead of single words) might involve mapping each word of a
text to its appropriate concept(s) in the ontology, and then calculating the aggregate
distance between the two resulting sets of concepts across the ontological relations.
For example, one might calculate the semantic distance between the two texts as the
average, minimum, maximum, or summed ontological distance between the individual
elements of the two sets of concepts (Corley and Mihalcea 2005).
Observe that each of these approaches to text comparison?distributional and
ontological?encodes information not contained in the other. Distributional distance
captures important information about frequency of occurrence of the words that consti-
tute the target text, whereas ontological distance captures essential semantic knowledge
that has been encoded in the relations of an ontology. In response, previous work has
attempted to combine distributional and ontological information in computing seman-
tic distance. For example, researchers have developed measures of semantic distance
between texts that apply distributional distances to concept vectors of frequencies rather
than to word vectors (McCarthy 2000; Mohammad and Hirst 2006). However, these ap-
proaches onlymake pairwise comparisions between the elements of the concept vectors,
and do not take into account the important ontological relations among the concepts. In
order to capture such relations, other methods have instead integrated distributional
information into an ontological method. However, such approaches have heretofore
been limited to measuring distance between two individual concepts. For example,
some ontological measures use corpus frequencies of words to yield concept weights
that are taken into account in measuring the distance between two concepts (Resnik
1995; Jiang and Conrath 1997). What has been missing is an approach to semantic
distance between two texts?two sets of words?that can truly integrate distributional
and ontological (relational) information, drawing more fully on their complementary
advantages for text comparison.
In this article, we describe a new graph-based distance measure that achieves the
desired integration of distributional and ontological factors in measuring semantic
distance between two sets of concepts (mapped from two texts). An ontology is treated
as a graph in the usual manner, in which the concepts are nodes and the relations are
edges. A text is represented as a subgraph of the ontology, by mapping the words in
the text into their corresponding concepts, which are weighted according to the word
frequencies. We call the resulting set of frequency-weighted concepts a semantic profile.
32
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
By exploiting the relational structure of the ontology, we can explicitly measure the
ontological distance over the paths between two profiles. Using the frequencies on the
concept nodes, we weight these paths according to the frequency distribution of words
in the two texts. The resulting calculation yields a frequency-weighted ontological
distance between the two sets of concepts. Thus, we view a text not as a set of items to
be compared individually to those in another set (with those individual distances then
somehow combined, e.g., as in Corley and Mihalcea [2005]), but rather as a distribution
of ?mass? within a graph that encodes the semantic relations across the two sets, and
use a weighted graph-based approach that captures the aggregate distance between the
two frequency masses.
To our knowledge, this is the first method to integrate ontological and distributional
information in the graphical calculation of text distance. This article describes the use
of the new measure in several different types of NLP text comparison tasks, in order to
explore the situations in which such an approach can be effective. Given the novelty
of the approach, the task-based evaluation is not intended as the last word on the
usefulness of the method, but rather as a first suite of experiments across different types
of text comparison tasks to illuminate some of the strengths and weaknesses of such
an approach to text distance. We thus analyze the results in detail to identify future
directions for further illuminating when and to what extent the methodmight be useful.
The analysis reveals that our method is not consistently successful across our
sample tasks. We hypothesize that, because ontological relations play an integral role
in our semantic distance measure, the measure is less effective when the semantic
profile for a text (the set of corresponding concepts) lacks semantic coherence. Other
work has explored ways to measure the semantic coherence of a set of concepts in
terms of their connectedness within an ontology (Gurevych et al 2003). Because a
semantic profile in our work includes both ontological (relational) and distributional
(frequency) knowledge, we require a measure of semantic coherence that takes both
into account. We develop a novel measure of semantic coherence called profile density
that captures both the ontological and distributional coherence of a set of frequency-
weighted concepts, and apply it to the data sets used in the different tasks to better
understand the performance of our semantic distance measure.
Our distance measure is cast as a graphical text comparison task within a network
flow framework as described in Section 2. In Section 3, we give an overview of our
exploration of the method on three types of text comparison problems. The following
three sections present experimental results and analysis of applying our method to the
various tasks: verb alternation detection (Section 4), name disambiguation (Section 5),
and document classification (Section 6). In Section 7, we describe our profile density
measure and use it to analyze the properties of the data sets that lead to the performance
differential across the tasks. We conclude the paper with a description of related work
in text comparison and graph-theoretic NLP approaches (Section 8) and a discussion of
some future directions for our research (Section 9).
2. The Network Flow Method
As noted previously, we treat an ontology as a graph and represent a text as a semantic
profile?a collection of nodes in the graph (concepts in the ontology), each having a
weight (its frequency). For example, in Figure 1, a small text consisting of the words
{cheese, wheat}, with frequencies of 4 and 10, respectively, is represented as a small
weighted subgraph in an ontology by uniformly distributing the word frequencies
among the associated concepts. In this way, a text is a weighted subgraph within a
33
Computational Linguistics Volume 36, Number 1
Figure 1
A small text represented as a collection of weighted nodes in a fragment of WordNet.
larger graph (with the thickness of the boxes in the figure indicating weight), and two
such weighted subgraphs are connected via a set of paths in the graph.
Our goal is to measure the distance between two subgraphs (representing two
texts to be compared), taking into account both the ontological distance between the
component concepts and their frequency distributions. To achieve this, we measure the
amount of ?effort? required to transform one profile to match the other graphically:
The more similar they are, the less effort it takes to transform one into the other. (This
view is similar to that motivating the use of ?earth mover?s distance? in computer
vision [Levina and Bickel 2001].) In Section 2.1, we first give the intuitive motivation
for the approach in terms of the properties of semantic distance that we want to cap-
ture by considering transport effort. We then present the mathematical formulation of
our graph-based method as a minimum cost flow (MCF) problem in Section 2.2, and
describe the formulation of our task within this network flow framework in Section 2.3.
In Section 2.4, we return to the properties we identify in Section 2.1 to explain how they
are reflected in the MCF formulation.
2.1 An Intuitive Overview
In Figure 2(a), we show a diagrammatic representation of an ontology (the large open
triangle) with two profiles, one indicated with filled squares and the other with filled
triangles. The location of a filled shape indicates the location of a profile concept in the
ontology, and its size indicates its frequency within the profile. We omit edges between
the nodes to simplify the diagram, but note that we assume we have a hierarchical, con-
nected ontology; hyponymy links are sufficient. Our goal is to calculate the similarity
between the two profiles by determining howmuch effort is required to transport, along
the ontological links, the frequency mass from all of the squares to ?fill? the available
space in the triangles. The amount of mass to move and the amount of space available
are indicated by the sizes of the squares and triangles, respectively. The degree of effort
required to transport one to the other indicates the degree of semantic distance.
34
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
Figure 2
Two subgraphs (one represented by squares, the other, triangles) with varying degrees of
overlap and, therefore, similarity within an ontology. Figure (b) differs from Figure (a) in terms
of the ontological distance between the square and the triangle clusters. Figure (c) differs from
Figure (a) in terms of the size of the individual squares.
The transport effort is determined by both the amount of mass to move and the
graphical distance over which it must travel. First consider graphical (ontological)
distance between the profiles. Assume the calculated distance between the two profiles
in Figure 2(a) is d. In Figure 2(b), the triangle profile is exactly the same. By contrast,
although the square profile has the same internal properties (same frequency distribu-
tion and graphical structure), its location is further from the triangles. Because the two
profiles occupymore distant portions of the ontological space, they are less semantically
similar than in Figure 2(a). As desired, the extra ontological distance over which the
square frequency mass must be transported to the triangles will cause the calculated
distance in Figure 2(b) to be larger than d.
Next consider the effect of varying the frequency distribution over the profile nodes.
Again, in Figure 2(c), the triangle profile is exactly the same as in Figure 2(a). However,
whereas the nodes of the square profile in Figure 2(c) are in the same locations as in
Figure 2(a), their distributional properties are different. The bulk of the frequency distri-
bution is now shifted closer to the nodes of the triangle profile. Because the two profiles
have more distributional weight located closer within the ontology, this indicates that
the semantic space they occupy is more similar than in Figure 2(a). Correspondingly,
because much of the mass of the square profile needs to travel less far to fill the space of
the triangle nodes, the calculated distance in Figure 2(c) will be less than d.
35
Computational Linguistics Volume 36, Number 1
It is worth noting explicitly that this notion of semantic distance as transport effort
of concept frequency over the relations (edges) of an ontology differs significantly from
an approach to semantic distance that utilizes concept vectors of frequency. By crucially
utilizing the relations between concepts in calculating semantic distance, our approach
can determine the distance between texts that use related but non-equivalent concepts.
For example, our measure will find greater similarity between a text that discusses
milk and one that discusses cheese than between one that discusses milk and one that
discusses bread. A vector distance would find each of these equally dissimilar, because
there are no concepts in common, and there is no way to relate milk to cheese.1
The intuitive examples in Figure 2 show that calculating semantic distance as
transport effort captures in a well-motivated way both the ontological distance between
the profiles and their weighting by the distributional amounts of the concept nodes. In
the next subsection, we describe a mathematical formulation that captures the relevant
properties of our problem in a network flow framework. Network flow methods are
often used in computer science for modelling such transport effort, for example, in
communication or transportation networks.
2.2 Minimum Cost Flow
Our intuitive transport effort examples above can be viewed as a supply?demand
problem, in which we find the minimum cost flow (MCF) from the supply profile to the
demand profile to meet the requirements of the latter. Mathematically, let G = (N,E) be
a connected graph representing an ontology, where N is the set of nodes representing
the individual concepts, and E is the set of edges representing the relations between
the concepts. (Most ontologies are connected; in the case of a forest, adding an arbi-
trary root node yields a connected graph.) Each edge has a cost c : E ? R, which is
the ontological distance of the edge. Each node i ? N is associated with a value b(i)
such that b : N ? R indicates its available supply (b(i) > 0), its demand (b(i) < 0), or
neither (b(i) = 0). The goal is to find a flow from supply nodes to demand nodes that
satisfies the supply/demand constraints of each node and minimizes the overall ?trans-
port cost.?
First, we have to define a function to describe the flow entering i via an incoming
edge (h, i) and exiting i via an outgoing edge (i, j). Let INi be the set of edges (h, i)
with a flow entering node i; similarly, let OUTi be the set of edges (i, j) with a flow
exiting node i. Then, the flow entering and exiting node i is captured by x : E ? R such
that we can observe the combined incoming flow,
?
(h,i)?INi x(h, i), from the entering
edges INi, as well as the combined outgoing flow,
?
(i,j)?OUTi x(i, j), via the exiting edges
OUTi (see Figure 3). A valid flow, x, must be found such that the net flow at each
node?the difference between its exiting flow and its entering flow?equals its specified
supply or demand constraints. For example, in Figure 2 where the squares represent
the supply and the triangles represent the demand, a solution for x would allow us to
transport all the weight at the squares to fill the triangles, via a set of routes connecting
them.
1 Techniques such as SVD or LSA could be applied to the concept vectors, as with word vectors, yielding
potential relations through unnamed concepts (e.g., Landauer and Dumias 1997). Note, however, that
such methods are dependent on the usages of the concepts implicitly encoding such connections,
whereas an ontology-based method draws on a knowledge base that explicitly encodes the relations
regardless of the particular usages of the concepts.
36
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
Figure 3
An illustration of flow entering and exiting node i.
Formally, the MCF problem can be stated as follows (from Chva?tal 1983):
Minimize z(x ) =
?
(i,j)?E
c(i, j) ? x(i, j) (1)
subject to
?
(i,j)?OUTi
x(i, j)?
?
(h,i)?INi
x(h, i) = b(i),?i ? N (2)
and x(i, j) ? 0,?(i, j) ? E (3)
The constraint specified by Equation (2) ensures that the difference between the flow
entering and exiting each node i matches its supply or demand b(i) exactly. The next
constraint, Equation (3), ensures that the flow is transported from the supply to the
demand but not in the opposite direction. The calculation of z in Equation (1) (which is
subject to these constraints) multiplies the amount of flow travelling along each edge,
x(i, j), by the transportation cost of using that edge, c(i, j). Taking the summation over all
edges of the product c(i, j) ? x(i, j) yields the desired transport effort of using the supply
to fill the demand.2
2.3 Semantic Distance as MCF
To cast our text comparison task into this framework, we first represent each text as a
semantic profile in an ontology. The profile of one text is chosen as the supply (S) and the
other as the demand (D); our distance measure is symmetric, so this choice is arbitrary.
In our examples in Section 2.1, the square profile was seen as the supply and the triangle
2 We cast our text comparison problem as an uncapacitated minimum-cost flow problem, i.e., there is no
upperbound constraint placed on the amount of flow along each edge (see Equation (3)). Unlike a
capacitated version of MCF, which is NP-complete (Garey and Johnson 1979), our problem is tractable
and can be solved in polynomial time.
37
Computational Linguistics Volume 36, Number 1
profile as the demand. The concept frequencies of the profiles are normalized, so that
the total supply equals the total demand.
The cost of the routes between nodes is determined by a semantic distance measure
defined over the nodes in the ontology?that is, a measure of individual concept-
to-concept distance. A relation (such as hyponymy) between two concepts i and j is
represented by an edge (i, j), and the cost c on the edge (i, j) can be defined as the
concept-to-concept distance between i and j. For simplicity in this article, we use edge
distance as our concept-to-concept distance measure c; that is, each edge (i, j) has a cost
of 1, and the distance between any two concepts is the number of edges separating
them.3
Next, we must determine the value of b(i) at each concept node i. In the simple
case, i occurs in only one profile or the other. If i ? S, b(i) is set to the normalized sup-
ply frequency, fS(i). If i ? D, b(i) is set to the negative of the normalized demand
frequency, ?fD(i), since demand is indicated by a value less than zero. However, i may
be part of both the supply and demand profiles, and then b(i) must be set to the net
supply/demand at node i. Thus we have:
b(i) = fS(i)? fD(i) (4)
For example, if the supply profile contains a node car with frequency of 0.25, and the
same node in the demand profile has a frequency of 0.7, then b(car) is ?0.45. In other
words, the node car has a net demand of 0.45.
Recall that our goal is to transport all the supply to meet the demand; the key
step is to determine the optimal routes between S and D such that the constraints in
Equation (2) and Equation (3) are satisfied. The total distance of the routes, or theMCF?
z(x ) in Equation (1)?is the distance between the two semantic profiles.
2.4 Ontological and Distributional Factors in MCF
To see how the factors of ontological distance and frequency distribution play out in
the MCF formulation, let?s return to our square and triangle profile example. Consider
a hypothetical zoomed-in area of the earlier diagram in Figure 2(a), shown in Figure 4.
Here we assume that the square nodes have a net supply (b(i) > 0) and the triangle
nodes have a net demand (b(i) < 0).4 The size of the square and triangle nodes in
the figure indicates |b(i)|?i.e., the relative supply/demand, respectively. The circles
indicate nodes with neither supply nor demand constraints?i.e., b(i) = 0. Each arrow
from node i to node j indicates the source and destination for transported flow from a
square node to a triangle. The length of an arrow represents the ontological distance,
c(i, j), and the width indicates the amount of flow, x(i, j). Note that the mass at the
rightmost square in the figure has to be distributed over the two triangles, and the
mass at the leftmost square is transported over a path with one edge (as indicated by
3 Some semantic distances, such as those of Lin (1998) and Resnik (1995), do not directly use the
underlying graph structure of the ontology in calculating the distance between two concepts. Using this
type of distance in our MCF framework requires an extra graph transformation step; see Tsang and
Stevenson (2006) for more details.
4 Earlier we made the simplifying assumption that square nodes were the supply profile and triangle
nodes the demand profile. We have now seen that a node can belong to both profiles, and its
characterization more accurately is stated in terms of net supply/demand. Thus, for example, a square
node may belong to just the supply profile or to both the supply and demand profile; the defining factor
is that it has a net supply.
38
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
Figure 4
An example of transporting the weights at the square nodes (supply nodes) to the triangle nodes
(demand nodes). The circle nodes have zero supply/demand requirement.
the arrow nearby) instead of a path with three edges (with two circle nodes on the path).
The aggregated length and width of the three arrows corresponds to the minimum cost
flow, i.e., the semantic distance between the profiles represented by the squares and
triangles.
Both the ontological distance between nodes and the node weights are important
in determining the minimum cost flow. The role of ontological information in the MCF
formulation is clear. If the squares were further away from the triangles in the ontology
in Figure 4?that is, if more edges separated the squares and the triangles?the sets of
concepts they represent would be less semantically similar. The length of the arrows
(representing c(i, j)) would be greater, and the resulting MCF would be larger, reflecting
the greater semantic distance between the profiles. Distributional information in this
method is equally critical to the distance calculation, because it determines the amount
of supply/demand at each node. If the squares in Figure 4 were more uniformly sized,
the two profiles would be more semantically similar because the weight would be
distributed more similarly across the ontological space. In this case, less flow would
have to travel from the rightmost square to the leftmost triangle (i.e., the corresponding
arrow would be thinner, representing x(i, j)), and the resulting MCF would therefore be
smaller. In short, our MCF method captures the desired property that both ontological
distance between profile nodes and their frequency distributions determine the overall
semantic distance between two profiles.
3. Evaluation: Experimental Tasks and Methodology
We select three different NLP tasks that can be formulated as text classification problems
based on semantic distance between the texts. In each case, the texts to be compared
are treated as bags of words with associated frequencies. The tasks are chosen to reflect
different types of relations used to extract the relevant words, to see if a varying amount
of constraint on the words comprising a text influences the performance of our method.
In verb alternation detection (Section 4), we identify which verbs, out of a set of
target and filler verbs, allow a certain variation in the syntactic expression of their
39
Computational Linguistics Volume 36, Number 1
underlying argument structure. The task is achieved by comparing the set of head
words that occur with the verb in each of two different syntactic positions (e.g., subject
of intransitive and object of transitive). In this task, the words that make up the texts
to be compared have a particular syntactic relation to the verb under consideration. In
proper name disambiguation (Section 5), we classify the sense of an ambiguous name
according to its local context. This task is similar to word sense disambiguation (WSD),
in picking the intended sense of a term, but also has similarities to topic identification,
since the proper name delineates a particular domain of discourse. In this task, we
compare the text constituting the ambiguous instance to texts representing each of the
known referents of the name. Here, the words of a text are extracted from a small
window of occurrence around the target name token (25 words on each side), regardless
of the syntactic relations among the words. For the known referents, the words from
these windows are aggregated across a small set of labelled instances. In document
classification (Section 6), a text is classified into one of a restricted number of topic
categories. The text to be classified consists of all the words in a document; for each
topic, it is compared to a set of words corresponding to a small set of known documents
for that topic. The extracted words are not constrained by syntactic relation (as in verb
alternation) or even by distance to a target element (as in name disambiguation).
In each case, the resulting bag of words for a text must be mapped into a semantic
profile?a frequency-weighted set of concepts in an ontology. Because all three of our
tasks involve general domain text, we use WordNet as our ontology (Fellbaum 1998).5
(A domain-restricted task may motivate the use of a domain-specific ontology, such
as UMLS for comparing medical texts as in Bodenreider [2004].) Because the noun
hierarchy of the WordNet ontology is most developed, we restrict our semantic profiles
to use only the nouns from the bag of words corresponding to a text: Any word in
the text that appears in the noun hierarchy of WordNet is included in the bag of
nouns.
The bag of nouns with their associated frequencies must be mapped to the appro-
priate concepts in WordNet. Given the current state of unsupervised WSD, there is
generally no attempt to disambiguate the words of a text when performing this kind
of mapping?that is, there is no selection of the most appropriate concept or set of
concepts to map the words to, given the context of their use. The simplest method
is to distribute the frequency of each word uniformly to its corresponding concepts.
For example, Ribas (1995) maps the word frequency to the most specific concept(s) for
the word, including all of the possible synsets for the word, but not their hypernyms.
Resnik (1993) also distributes the word frequency uniformly, but does so across the most
specific concept(s) and all of their hypernyms. Other approaches, although still avoiding
the difficulties of WSD, do try to capture the overall semantic ?tendencies? of the set of
words. Such methods estimate the appropriate probability distribution over a set of
concepts to represent a given bag of nouns as a whole (Li and Abe 1998; Clark and Weir
2002). However, such techniques still start with a mapping of each word to all of its
immediate concepts.
5 There is disagreement over the suitability of treating WordNet as an ontology, rather than as a lexical
network (Gangemi, Guarino, and Oltramari 2001; Hirst 2009). However, the intention of the creators of
WordNet is apparently that its synsets correspond to concepts, and the relations between them include
both ?conceptual-semantic and lexical relations? (http://wordnet.princeton.edu/), qualifying it, under
some views, as a general domain ontology. Although recognizing the limitations and difficulties of using
a primarily lexical resource as an ontology, we note that WordNet is standardly used as such in
computational linguistics, and so we adopt this use here.
40
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
For all three of our tasks, we take the simple approach of mapping each noun
individually to its most specific concepts (not their hypernyms), uniformly dividing
the word frequency among them. In verb alternation, we also experiment with the
possibility of finding the best set of frequency-weighted concepts for the full bag of
nouns (using the techniques of Li and Abe [1998] and Clark and Weir [2002]), to see if
this affects the performance of our method.
The precise classification experiment performed using these semantic profiles is
described in detail subsequently in the section for each task. In each case, we compare
the performance of our MCF method on the semantic profiles to one or more purely
distributional methods using the original word frequency vectors.
4. Task 1: Verb Alternation Detection
Verb alternation refers to variations in the syntactic expression of verbal arguments. If a
verb participates in an alternation, the same underlying semantic argument may appear
in varying positions (slots) of the verb?s subcategorization frames. For example, the
following sentences show that the argument undergoing the melting action can appear
as the subject of an intransitive use of melt (1a) or as the object of a transitive use (1b).
1a. The chocolate melted.
1b. The cook melted the chocolate.
This type of intranstive/transitive pairing is known as the causative alternation because
of the explicit expression of the causer (the cook) in the transitive alternant.
It has long been hypothesized that the semantics of a verb and its relations to its
arguments at least partially determine the syntactic expression of those arguments (see
Pinker [1989], among others). Influential work by Levin (1993) showed that this rela-
tionship could be exploited ?in reverse? by using alternation behavior as an indicator
of the underlying semantics of a verb?specifically, that verbs undergoing the same sets
of alternations form classes with similar semantics. Computational linguists have built
on this work by demonstrating that statistical cues to alternation behavior can be used
to automatically place verbs into semantic classes (Merlo and Stevenson 2001; Schulte
im Walde 2006).
Detection of verb alternation behavior can be cast as a text comparison problem
(McCarthy 2000; Merlo and Stevenson 2001). Consider an alternation such as the
causative illustrated in Example (1). The set of nouns appearing as the subject of the
intransitive (such as chocolate) have the same relation to the verb as the set of nouns
appearing as the object of the transitive. Because the verb places constraints on what
kinds of entities can be in that relation (here, things that are meltable), the two sets of
nouns should be similar. Hence, to identify a particular alternation for a verb, the set
of nouns in a certain slot of one of its subcategorization frames is compared to the set
of nouns in the alternating slot for that semantic argument in another subcategorization
frame.
For example, Merlo and Stevenson (2001) devise a simple lemma overlap score
that counts the number of tokens appearing in both of the relevant syntactic slots.
McCarthy (2000) instead compares two semantic profiles in WordNet that contain the
concepts corresponding to the nouns from the two argument positions. In McCarthy?s
method, the profiles are first generalized to a set of higher level nodes in the hierarchy
(starting with the method of Li and Abe [1998]); next, skew divergence is used to find
41
Computational Linguistics Volume 36, Number 1
the distance between the resulting vectors of concepts. Here we use our network flow
method to directly compare the semantic profiles corresponding to the noun sets. Our
method allows us to compare sets of weighted concepts as in McCarthy?s, but using
a distance method that applies within the ontology graph, rather than simply using a
distributional distance measure over concept vectors.
4.1 Experimental Set-up
We adopt the data set from an investigation of a semantic distance measure that was
a precursor to our network flow method (Tsang and Stevenson 2004). The selection
of these verbs and extraction of their arguments are discussed in the following two
sections; we then describe our evaluation methodology.
4.1.1 Experimental Verbs.We evaluate our method on the causative alternation. As noted
previously, in this alternation the target syntactic slots for comparison are the subject
of the intransitive (Subj-Intrans) and the object of the transitive (Obj-Trans). (These are
the positions of the chocolate in Examples (1a) and (1b), respectively.) To identify verbs
undergoing this alternation, we randomly selected verbs from among Levin classes that
are indicated to allow the causative alternation. This allows us to test the ability of a
distance measure to detect alternation behavior among verbs from a range of semantic
classes which may differ in other respects.
We refer to the verbs that are expected to undergo the causative alternation as
causative verbs. For comparison, we randomly selected an equal number of filler verbs,
subject to the constraint that their Levin classes do not allow a causative alternation.
(Specifically, none of the classes containing a filler verb allows an alternation in which
the same underlying argument appears in the Subj-Intrans slot as well as the Obj-Trans
slot.) The full set of potential causative and filler verbs were filtered according to corpus
counts, as described next.
4.1.2 Corpus Data and Argument Extraction. We used a randomly selected 35M-word
portion of the British National Corpus (BNC; Burnard 2000). The text was parsed using
the RASP parser of Briscoe and Carroll (2002), and subcategorization frames were
extracted using the system of Briscoe and Carroll (1997). Each subcategorization frame
entry for a verb includes a list of the observed argument heads per slot along with their
frequencies. For each verb/slot pair, we thus extracted the set of nouns used in that slot
along with their frequency of occurrence.
Verbs were filtered from the potential list of experimental items if they occurred
less than 10 times in our corpus in either the transitive or intransitive frame. The verbs
were then divided into multiple frequency bands: high (at least 450 instances), medium
(between 150 and 400 instances), and low (between 10 and 100 instances). An equal
number of verbs of each type (causative and filler) were randomly selected within each
band, yielding a total of 120 experimental verbs in balanced data sets of 60 items for
development and 60 items for testing. The development data was used in our earlier
work to select a profile-generation method for the test data (Tsang and Stevenson 2004).
In our current work, we did not make any adjustments to our method based on results
on the development set (i.e., it was not used to set any parameters or select a particular
implementation approach). Hence, we report the evaluation of our method on the full
set of 60 verbs in each of the data sets, as well as individually on the three frequency
bands of 20 verbs each. We refer here to the original ?development? and ?test? data sets
as ?dataset1? and ?dataset2?.
42
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
4.1.3 Evaluation Methodology. For each verb, we create a semantic profile for each of
the Subj-Intrans and Obj-Trans slots. We first take the argument heads with their fre-
quencies from the appropriate slots in the extracted subcategorization frame for the
verb. We then map these words with their frequencies to the corresponding nodes in
WordNet, as described in Section 3. (We also consider here different profile generation
methods, discussed later in Section 4.2.2.) We then calculate the network flow distance
between the two semantic profiles for each verb, yielding a distance calculation for
that verb. Recall that we expect verbs that participate in the alternation to have more
similar semantic profiles corresponding to the Subj-Intrans and Obj-Trans nouns. For
example, a causative verb like melt, as in Examples (1a) and (1b), may have words
like chocolate, sherbet, and glacier in the Subj-Intrans slot, and words like chocolate, butter,
and bronze in the Obj-Trans slot. In contrast, a non-causative verb like fry will typically
have more dissimilar sets of words that contribute to the two profiles (e.g., cook, wife,
and chef in the Subj-Intrans slot, and egg, noodle, and onion in the Obj-Trans slot). We
thus rank all the verbs by the distance calculation, and (as in McCarthy 2000) set a
threshold to divide the verbs into causative (smaller distance values) and non-causative
(larger distance values). FollowingMcCarthy, we experimented with both the mean and
median values as the threshold, but found little difference. We report the results using
themedian distance as the threshold, because this providedmore consistent results with
our method.
Because we label all verbs in our experiments as causative or non-causative, we use
accuracy as the performance measure. Since we have balanced data sets, the random
baseline is 50%. We compare our results as well to a number of distributional methods
(as enumerated in the next section). Given the small size of our data sets, a simple
statistical test on the resulting accuracies is not powerful enough to reveal differences
when the accuracies are close. However, because the difference in methods is due to
variation in how they rank the experimental items, we perform a Wilcoxon signed
rank test (Wilcoxon 1945) to determine when the rankings between two methods are
significantly different, using a p value of .05.
4.2 Results and Analysis
As noted herein, we present results on two sets of data, and also examine the effect of us-
ing alternative profile generation methods. We compare our network flow distance (NF)
to a number of other distance measures including probability distributional distances
given by Jensen-Shannon divergence (JS) and skew divergence (skew div) (Lee 2001),
as well as the general vector distances of cosine, Manhattan distance, and Euclidean
distance.
4.2.1 Experimental Results. On dataset1, our network flow distance performs better than
or as well as all other measures on the individual frequency bands, as shown in Table 1.
On all verbs combined (the ?All? column) the performance of our method is not the
best, although the Wilcoxon test shows no significant difference between the rankings
of NF and the best measure (Manhattan). (The difference in rankings between NF and
all other measures is significant.)
Interestingly, we find that the ?All Verbs? performance of NF (and that of several
other methods) is indeed worse than the performance on the individual frequency
bands. We examined the distance values across the frequency bands to determine the
cause for this pattern. We found that low frequency verbs tend to have smaller distances
43
Computational Linguistics Volume 36, Number 1
Table 1
Accuracies on dataset1 by the network flow method (NF), cosine, Manhattan distance, Euclidean
distance, skew divergence (skew div), and Jensen-Shannon divergence (JS). Best accuracies in
each condition are shown in boldface.
All Frequency Bands Avg of
Verbs High Medium Low Bands
NF 0.60 0.70 0.70 0.70 0.70
cosine 0.57 0.60 0.60 0.60 0.60
Manhattan 0.63 0.70 0.70 0.70 0.70
Euclidean 0.47 0.40 0.50 0.40 0.43
skew div 0.57 0.60 0.60 0.50 0.57
JS 0.60 0.70 0.60 0.70 0.67
Table 2
Accuracies on dataset2 by the network flow method (NF), cosine, Manhattan distance, Euclidean
distance, skew divergence (skew div), and Jensen-Shannon divergence (JS). Best accuracies in
each condition are shown in boldface.
All Frequency Bands Avg of
Verbs High Medium Low Bands
NF 0.67 0.60 0.80 0.60 0.67
cosine 0.50 0.60 0.50 0.50 0.53
Manhattan 0.63 0.60 0.80 0.60 0.67
Euclidean 0.60 0.50 0.70 0.50 0.57
skew div 0.63 0.60 0.80 0.60 0.67
JS 0.70 0.60 0.80 0.60 0.67
between the two slots and high frequency verbs tend to have larger distances. This is
due to the fact that higher frequency verbs typically occur with a wider range of nouns,
leading to a more dispersed semantic profile (i.e., a larger number of concepts). As a
result, the best threshold for separating the alternating and non-alternating verbs differs
across the frequency bands, and the threshold for all verbs together lies in between
the thresholds for the high and low frequency bands. When classifying all verbs, the
frequency effect may result in more false positives for low frequency verbs (which have
generally smaller distance values), and more false negatives for high frequency verbs
(which have generally larger distance values). The column labelled ?Avg? in Table 1
shows the performance when averaging the results across the individual frequency
bands. For most methods, including ours, the ?Avg? results are much better than when
considering all verbs together (the ?All? column).
Table 2 reports the performance on dataset2, which is similar to that on dataset1.
Again, we find that our method is tied for the best performance in every condition
except for all verbs combined. (Here we find that all four methods over .60 accuracy
in the ?All? condition have statistically indistinguishable rankings of the experimental
items.) On this data set, taking the average of the frequency bands does not help
performance of our method compared to ?All,? but neither does it hurt (and for most
methods ?Avg? does better or the same as ?All?). We conclude that separating items by
frequency may be required to achieve robust results in this type of task.
Although our method is tied for best in every condition except ?All,? neither
is our method distinguished from several of the other distance measures. Given the
44
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
Table 3
Average accuracies by the network flow method (NF), Manhattan distance (Man), skew
divergence (skew div), and Jensen-Shannon divergence (JS) on different profiles: original
(?raw?), Li and Abe, and Clark and Weir profiles. Best accuracies in each condition are shown in
boldface.
raw Li and Abe Clark and Weir
Dataset1 Dataset2 Dataset1 Dataset2 Dataset1 Dataset2
NF 0.70 0.67 0.50 0.67 0.73 0.70
Manhattan 0.70 0.67 0.57 0.67 0.60 0.57
skew div 0.57 0.67 0.53 0.67 0.68 0.60
JS 0.67 0.67 0.63 0.67 0.63 0.53
relatively small amounts of data per verb (with profiles averaging about 900 nodes
in size), it is possible that the raw profiles suffer from a sparse data problem and are
not sufficiently capturing the conceptual similarities among alternating slots. McCarthy
(2000) addressed this issue by using a technique for generalizing concept nodes prior to
comparing profiles. We explore this issue next.
4.2.2 Comparing Different Profile Generation Methods. Our experiments use semantic pro-
files created directly from the word frequencies, as described earlier. However, research
has explored the possibility of generalizing this kind of ?raw? data to a semantic profile
that more appropriately reflects the coherent concepts expressed in the original set of
weighted concept nodes. This can be especially useful when creating semantic profiles
from small amounts of data, given the noise introduced in the mapping of words to
concepts.6 To explore the effect of different profile generation methods on this task, we
consider here two approaches, that of Li and Abe (1998) and Clark and Weir (2002).
Both these methods start with a semantic profile generated as described in Section 3
and attempt to find the set of nodes in the ontology that appropriately generalize the
concepts in the ?raw? profile.
Table 3 compares the performance of the network flow distance with that of several
other measures on the original (?raw?) profiles, the Li and Abe profiles, and the Clark
and Weir profiles. Results are reported for the average of the individual frequency
bands, since that produced the best results overall in our earlier experiments. The results
for cosine and Euclidean distance are omitted, because they perform worse overall than
the other measures.7
The best results across both data sets are achieved by our network flow method on
the Clark andWeir profiles. Considering the results across all profile types, the network
flow approach is most consistent, achieving the best (or tied for best) performance
in but one condition (dataset1 with Li and Abe profiles). The distributional methods
6 Because we divide the frequency of a word uniformly among all the word?s concepts, with no attempt at
disambiguation or informed weighting, much noise is introduced. Given the small amounts of data, the
noise may be sufficient to mislead our network flow method.
7 Because these results use the approach of averaging results across the frequency bands, we cannot apply
the Wilcoxon signed rank test to the rankings. (The individual frequency bands have too few items for
the test to detect differences.) On All Verbs combined (results not reported in this table), the rankings of
NF are different from all other methods on each combination of data set and profile generation approach,
except in the single case of Manhattan and JS on dataset2 using Li and Abe to create the profiles.
45
Computational Linguistics Volume 36, Number 1
(Manhattan, skew div, JS) in almost all cases perform worse on the generalized profiles
than on the ?raw? profiles. (The one exception is that skew divergence does better on
dataset1 on the Clark and Weir profiles.)
Overall, then, it seems that raw data is likely best for a purely distributional method,
but the Clark andWeir profiles enable the network flowmethod to outperform them by
exploiting the graph structure of the ontology. Indeed, when comparing our method
to the others on the Clark and Weir profiles for the individual frequency bands (not
shown in the table), we find that much of our performance advantage comes on the
low frequency verbs. This indicates that the combination of our method with a suitable
generalization technique is especially important when dealing with sparse data.
We examine the data further to discover why the Li and Abe profiles yield poorer
performance inmost cases on dataset1.We find that Li andAbe?s (1998) method tends to
generate profiles with more general concepts. For example, when given an original set
of concepts such as Edam, Brie, Sockeye, and Chinook, the method may produce a single
general concept such as food instead of the two concepts cheese and salmon that capture
the two kinds of food that are indicated. The loss of semantic information from using
overly general concepts may produce the decrease in performance.
For comparison, we also apply McCarthy?s (2000) method to our dataset2, and
find that it achieves only 0.60 on all verbs and 0.53 averaged over the three frequency
bands. Her method is especially poor on low frequency verbs (below chance at 0.40).
We hypothesize that her method is less robust to low frequency counts because it
may overgeneralize the data by first applying Li and Abe?s (1998) method, and then
generalizing the nodes even further.
We see that although some amount of generalization of the semantic profiles is
useful in this task, overgeneralization may be harmful. We leave it to future work
to explore the interaction of our network flow method with different types of profile
generation across various tasks. Because the next two tasks we consider use larger
amounts of data, we only experiment with raw profiles in these cases.
5. Task 2: Name Disambiguation
Interest in the NLP problem of name disambiguation has increased as the growth of
the World Wide Web has led to large numbers of ambiguous name references in on-
line text. For example, Web sites or documents containing the name John Edwards may
refer to the U.S. presidential candidate for 2008, an NBA basketball player, or a British
medical geneticist. An ambiguous name may be resolved by comparing its local textual
context?the set of words it co-occurs with?with the local textual contexts of the name
when its reference is known. For example, the text surrounding the name John Edwards
in its various uses are very likely to include distinguishing words such as politician vs.
game vs. research. Many approaches have been proposed for resolving name ambiguity
by using distributional methods over contextual information (Xu, Liu, and Gong 2003;
Han, Zha, and Giles 2005; Pedersen, Purandare, and Kulkarni 2005).
In this section, we present the application of our network flow distance measure to a
name disambiguation task, and demonstrate the benefits of combining ontological and
distributional knowledge in this task. The particular task we examine is one of ?pseudo
name disambiguation,? in which the texts containing matched pairs of different names
are extracted, and then the two different names are replaced by a single symbol, leading
to an ambiguous ?name? across the two sets of texts. The goal is to recover the correct
target name in each instance. For example, the names of two soccer players (Ronaldo
and David Beckham) form one disambiguation task, and the names of an ethnic group
46
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
and a diplomat (Tajik and Rolf Ekeus) form another. This task was established by
Pedersen, Purandare, and Kulkarni (2005) to provide ?annotated? experimental data
(with each text indicating the correct name) without the need for expensive manual
annotation.
In Pedersen, Purandare, and Kulkarni (2005), an unsupervised method of name
discrimination through text clustering was used to address this task. This is infeasible
for a method like ours, in which each distance calculation requires access to an ontology.
(The worst-case complexity of clustering with our method is quadratic in the size of the
ontology used; a detailed discussion can be found in Tsang and Stevenson [2006].) In-
stead, we use a supervisedmethodology, but experiment with varying small amounts of
data in a minimally supervised approach. Although our method requires extra manual
effort in the form of data annotation for training, we find that the amount of annotated
data required is modest.
5.1 Experimental Methodology
5.1.1 Corpus Data. We use Pedersen, Purandare, and Kulkarni?s (2005) data set, which
was taken from the Agence France-Press English Service portion of the GigaWord
English corpus distributed by the Linguistic Data Consortium. They extracted the local
context of six pairs of names of varying confusability, including: the names of two
soccer players (Ronaldo and David Beckham); an ethnic group and a diplomat (Tajik
and Rolf Ekeus); two companies (Microsoft and IBM); two politicians (Shimon Peres
and Slobodan Milos?evic?); a nation and a nationality (Jordan and Egyptian); and two
countries (France and Japan). For each name instance, the extracted text consists of
50 words (25 words to the left and to the right of the target name), with the target
name obfuscated. For example, for the task of distinguishingDavid Beckham andRonaldo,
the target name in each instance becomes David BeckhamRonaldo. The original name in
each instance is retained only for evaluating the results (and for training, in the case
of our method, as described subsequently). (Note that this approach to data creation
avoids the use of manually annotated data for this experimental task, but in an actual
application, manual annotation of truly ambiguous names would be necessary.) Each
pair of names thus serves as one of six name disambiguation tasks. Table 4 shows the
number of instances per task (name pair). The ?Majority? column also indicates the
relative frequency of the majority name in each pair, which we adopt as the baseline
accuracy.
5.1.2 Classification Using the Network Flow Method. As mentioned previously, we take a
supervised approach, in which name instances are classified with the use of training
Table 4
The pairs to be identified, the raw frequencies, and the relative frequency of the majority name.
Name 1 Count Name 2 Count Total Majority
Ronaldo 1,700 David Beckham 752 2,452 0.69
Tajik 3,002 Rolf Ekeus 1,071 4,073 0.74
Microsoft 3,401 IBM 2,406 5,807 0.59
Shimon Peres 7,686 Slobodan Milos?evic? 6,048 13,734 0.56
Jordan 25,039 Egyptian 21,392 46,431 0.54
Japan 116,379 France 110,435 226,814 0.51
47
Computational Linguistics Volume 36, Number 1
data annotated by the original name in the instance. To generate our training data, we
randomly select a portion of the instances for each of the 12 names. All the training
instances for a name are used to form a single aggregate semantic profile, which serves
as the gold-standard for that name. The remaining instances serve as test data; for
each of these, we build an individual semantic profile. All profiles are generated as
described in Section 3, namely, each frequency count for a word is distributed uniformly
among the corresponding concepts in WordNet. A gold-standard profile is constructed
in exactly the same way except that its word frequency vector is created by aggregating
the word counts from all the relevant training instances. Note that there is nothing
special about such a profile or how it is formed; it simply aggregates counts from
multiple contexts.8
To classify a name instance, we measure the network-flow distance between the
individual profile of the ambiguous instance and each of the two gold-standard profiles
for that task. The name whose gold-standard profile has the shortest distance to the
instance profile is the name assigned to the ambiguous instance. For example, assume
we have a ?David BeckhamRonaldo? instance to be classified. We compare its profile to
each of the gold standard profiles for ?David Beckham? and ?Ronaldo? by measuring
the distance between each of the two pairs of profiles. If the instance profile has a
shorter distance to the profile for ?David Beckham? than to that of ?Ronaldo,? then
it is classified as ?David Beckham,? otherwise as ?Ronaldo.?
5.1.3 Evaluation Methodology. We use the accuracy of labelling all instances as our eval-
uation measure. To compare to prior results using F-measure, we report that in some
tables. Because we label all instances, accuracy and F-measure are equivalent in our
method, using 2rp/(r+ p) as the definition of F-measure.
The random baseline for our task is the accuracy of labelling all instances with the
predominant name, as shown in the ?Majority? column of Table 4. Because we use the
data set of Pedersen, Purandare, and Kulkarni (2005), we compare our performance to
their distributional method (reporting their best results both with and without singular
value decomposition). Because their method is an unsupervised one, we also train and
test a supervised learner using distributional data (LIBSVM by Chang and Lin [2001]).
For each set of training data, we remove stopwords and use the remaining words (with
their frequencies) as input features for the SVM. We then obtain the optimal parameters
(i.e., optimal values for cost and gamma in LIBSVM) by using 10-fold cross-validation
over the training data. Finally, we perform classification on the test data using those
parameters. This enables us to compare our results to a purely distributional method
with access to the same training data.
Because our method is supervised, it is important to minimize the amount of
annotated data required to build the gold-standard profiles. (Lengthy training time can
also be an issue for a supervised method, but here ?training? is the straightforward
task of building an aggregate semantic profile.) Because it is unclear a priori what
amount of training data is sufficient, we experiment with several quantities. We initially
select 200 random instances per pair of names, respecting the relative proportions of
the two names overall. (Two hundred instances constitute about 0.1?10% of the data
per pair of names.) Subsequently, we decrease the quantity further, to one-half and one-
quarter the original amount (100 and 50 instances, respectively) to observe how the
8 In our later experiment in document classification, on a subset of our data, we tried a nearest neighbor
approach to all training instances rather than aggregating them, but this did not perform as well.
48
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
Table 5
Network flow results using 200 training instances on the random samples and their average
performance.
Name Pair Random Samples Average of
1 2 3 4 5 Samples
Ronaldo/Beckham 0.78 0.83 0.76 0.79 0.84 0.80
Tajik/Ekeus 0.98 0.98 0.97 0.96 0.98 0.97
Microsoft/IBM 0.73 0.72 0.73 0.74 0.73 0.73
Peres/Milos?evic? 0.96 0.96 0.97 0.96 0.97 0.96
Jordan/Egyptian 0.79 0.78 0.78 0.77 0.76 0.77
Japan/France 0.79 0.73 0.77 0.70 0.73 0.75
Table 6
Average results for the network flow (NF) results using 200 instances per gold-standard profile,
SVM using 200 training vectors, and Ped05 and Ped05SVD (the best results without and with
SVD, respectively). All results are F-measure (the same as accuracy for our method and SVM).
The weighted average is calculated based on the number of instances in each pair of names. The
best result for each name pair is indicated in boldface.
Name Pair Majority Ped05 Ped05SVD SVM200 NF200
Ronaldo/Beckham 0.69 0.73 0.65 0.85 0.80
Tajik/Ekeus 0.74 0.96 0.89 0.90 0.97
Microsoft/IBM 0.59 0.51 0.59 0.62 0.73
Peres/Milos?evic? 0.56 0.97 0.94 0.90 0.96
Jordan/Egyptian 0.54 0.59 0.62 0.72 0.77
Japan/France 0.51 0.51 0.50 0.48 0.75
Unweighted Average 0.61 0.71 0.70 0.75 0.84
Weighted Average 0.53 0.55 0.55 0.55 0.77
performance is influenced by the amount of data used to construct the gold standard
profiles.9 To reduce the impact of possible skewed sampling of training data, we repeat
the random sampling five times, with no overlap between the random samples. We
report the performance of each sample set as well as the average over the five samples.
5.2 Results and Analysis
5.2.1 Initial Experiments. Table 5 shows the performance of our method over five random
samples of 200 training instances per task. Observe that the performance over the five
rounds varies very little (a maximum difference of 0.08, and most are much closer).
This shows the robustness of our method to different make-ups of training data. Table 6
shows the average performance of our method, in comparison to the chance (majority)
baseline, as well as the results produced by the unsupervised method of Pedersen,
Purandare, and Kulkarni (2005) (with singular value decomposition [SVD] reported as
9 We also experiment with 400 training instances to see whether increasing the amount of training data
helps. The performance benefit is minimal: two tasks have the same average performance, three improve
by 1%, and one by 2%, with an improvement in the average over all the tasks of 1.25%. A paired t-test
between the results on 400 and 200 training instances yields a high p value (p = 0.73), indicating that the
differences between the two are statistically insignificant.
49
Computational Linguistics Volume 36, Number 1
Ped05SVD, and without SVD as Ped05), and the supervised SVM on the same training
data as our method. Observe that our method not only significantly outperforms the
random baseline, it is moreover the best performer among all the methods (paired
t-test, p < 0.05).
There are cases for which Pedersen, Purandare, and Kulkarni?s (2005) methods
have at best chance performance (Microsoft/IBM and Japan/France). The authors suggest
that these pairs of names arise in the context of news text in which there are ?no consis-
tently strong discriminating features? useful in the clustering algorithm. (Interestingly,
this is the case even with SVD, where words are grouped into a small number of un-
named concepts.) Even the SVM has difficulty with these pairs, also performing at just
around chance. Yet our method performs well above chance for these pairs. In general,
SVM produces results that are little better on average than the unsupervised results in
Pedersen, Purandare, and Kulkarni (2005) (with some tasks performing better, and some
worse). This shows that the performance improvement from the network flow method
does not depend solely on access to training data. Instead, it seems that the use of
ontological relations in calculating distance can significantly enhance the discriminatory
power over simply using words.
Note that there is one difference between the data used in the SVM and the network
flow experiments: The SVM is trained using all words as features, while only WordNet
noun concepts are used in the network flow experiments. It is possible that using just
nouns or a mapping of nouns to WordNet concepts could bring the performance of the
SVM into line with our network flow measure. We thus perform two replications of the
SVM experiments, one using only nouns as features and one using noun concepts as
features (with the relevant frequencies as the feature values in both cases). However,
both of these approaches produce little to no improvement over the all-words results
reported in Table 6. We conclude that our network flow method is superior to, and
more consistent than, the purely distributional methods, and that this difference is
attributable to the integration of distributional and ontological (relational) information
in our measure.
5.2.2 Reducing the Amount of Training Data. Because, in contrast to Pedersen, Purandare,
and Kulkarni (2005), we use a supervised approach, we want to determine whether we
can reduce our dependence on training data. Here, we report experiments using one-
half (100 instances) and one-quarter (50 instances) of the training data used earlier. As
before, we repeat the random sampling of the training instances five times in each case,
and report the average performance here.
Table 7 shows the network flow performance for 200, 100, and 50 training instances.
Numerically, the results do not differ by much when the training data is reduced from
200 to 100 instances, and a paired t-test finds the difference to be non-significant. The
performance drop is more pronounced in the 50-instance experiment, where every pair
of names shows some drop in performance compared to 100 instances. Here, a paired
t-test shows that the performance drop in the 50-instance experiment is statistically
significant (p = 0.04). Despite this, we still outperform the other methods: Our results
using 50 training instances are much better than those of Pedersen, Purandare, and
Kulkarni (2005) in all but one task, and even better overall than the SVM in 200 training
instances (compare the SVM column of Table 6).
For comparison, we also train the SVM in 100 training instances, and find a decrease
of 3% on average from using 200 training instances. We conclude that our method is
more robust to minimal training conditions. To explore the least amount of training data
needed for our measure, we further reduce the amount for producing gold-standard
50
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
Table 7
Average classification results of the network flow method using 200, 100, and 50 training data
per classification task. The weighted average is calculated based on the number of test instances
per task.
Name Pair Number of Training Instances
200 100 50
Ronaldo/Beckham 0.80 0.79 0.76
Tajik/Ekeus 0.97 0.98 0.96
Microsoft/IBM 0.73 0.73 0.72
Peres/Milos?evic? 0.96 0.97 0.94
Jordan/Egyptian 0.77 0.74 0.70
Japan/France 0.75 0.75 0.70
Unweighted Average 0.83 0.83 0.80
Weighted Average 0.77 0.76 0.72
profiles to 20 and 5 instances per task, and observe a continual drop in performance. The
performance of one task (Ronaldo/David Beckham) drops below chance with 20 training
instances and another (Microsoft/IBM) drops below chance with 5. For this set of data,
we conclude that 50 instances per task are required to provide enough discriminatory
power for our method.
Although unsupervised methods have the advantage of requiring no training data,
in our case, 50 to 100 training instances constitute only a very small portion of the
data, as well as a small amount of annotation effort in absolute terms. We conclude
that the (small) labelling effort is justified by the performance gain achieved using our
minimally supervised approach.
6. Task 3: Document Classification
Document classification is an NLP task in which a previously unseen document is given
a topic label (or a set of such labels) based on its subject matter. For example, a financial
document discussing the fluctuation of crude oil prices may be labelled ?commerce? or
?crude oil? in the Reuters Corpus (Lewis et al 2004). In our version of the task, each
document has a single topic label. Document classification is typically performed by
comparing the text of an unlabelled document to the text of documents whose topics
(labels) are known, and assigning the label of the closest such document (Joachims 2002;
Iwayama et al 2003; Esuli, Fagni, and Sebastiani 2006; Nigam, McCallum, and Mitchell
2006). This task is thus similar to the name disambiguation task in the previous section,
and our approach is similar as well: Here again, we form gold-standard profiles from a
small collection of texts of known classes, and then compare each test instance to each
of the gold-standard profiles. As in name disambiguation, we experiment with different
amounts of training data for creating the gold-standard profiles.
There are two differences of note in comparison to name disambiguation. First, in
document classification we use the entire set of words constituting the document to
create a semantic profile, rather than a smaller window around a target word. Second,
whereas each ambiguous name instance in the earlier task had exactly two potential
labels (and thus there were two gold-standard profiles for comparison), the number of
labels in the document classification task is much larger, leading to more ambiguity in
the task.
51
Computational Linguistics Volume 36, Number 1
6.1 Experimental Set-up
6.1.1 Corpus Data. Our data is a corpus of articles from 20 different Usenet newsgroups
released by Mitchell (1999). Because each newsgroup corresponds to a topic, the articles
can be classified using the (single) newsgroup label. We use the collectionmaintained by
Rennie (2001), in which all the duplicates (cross-posts) are removed, resulting in 18,828
articles. The articles are approximately evenly distributed among the 20 newsgroups.
Stopwords and article headers are removed before processing each text.
Work that relies on word frequency vectors to represent the texts in document
classification has revealed the importance of preprocessing the word frequency data to
emphasize those terms that are likely to be most meaningful. For example, word fre-
quencies have typically been weighted by inverse document frequencies (tf ? idf ) to
lessen the impact of very common but less distinguishing words. According to
Rennie (2001), their best system on the same corpus uses the
log tf+1
log idf weighting scheme.
In order to compare our system to theirs, we use this same word weighting scheme
in the creation of the word vectors that are used to produce our semantic profiles.
(We have experimented with using raw word frequencies as well as tf ? idf to pro-
duce profiles. Both methods yield approximately the same results as the
log tf+1
log idf
frequency weighting scheme.)
6.1.2 Training and Evaluation. As mentioned before, we treat the classification task simi-
larly to name disambiguation, taking a minimally supervised approach. We randomly
select a small number of documents as training data for creating the gold-standard
semantic profiles. We use 10 or 30 documents per newsgroup, or approximately 1?3%
of the documents. The remaining documents are used as testing data. Again, we use
a random sample of documents for each gold-standard profile, repeated five times to
minimize the impact of a possible skewed sampling. We report the average accuracy
over the five samples.
Because there are 20 possible topic labels, the random baseline is very low, at 5%.
(Using the predominant label raises this only slightly.) A more informative evaluation
of our method is to compare to a state-of-the-art approach that is purely distributional.
A comparison to Rennie (2001) is natural, since we use the same data set. However,
they trained an SVM on 30 documents per class and tested on 10% of the documents,
repeated 10 times. Because our training approach differs somewhat (training on 10 or
30 documents per class, testing on all remaining documents, repeated 5 times), we
also replicate their SVM experiment using our training and test sets. As in the name
disambiguation task, we use the LIBSVM software package (Chang and Lin 2001) and
tune the classifier in the training phase for the best SVM parameters prior to the testing.
Also as in our name disambiguation task, we additionally train and test the SVM on
just the nouns in a document (rather than all words), and also on the nouns mapped
to concepts (with the relevant frequencies as the feature values in both cases). Thus we
report results of the SVM on three different types of input frequency vectors: all words,
nouns, and concepts.
6.2 Results and Analysis
6.2.1 Initial Results. Table 8 presents the classification results using 10 and 30 training
documents per class for our network flow and SVMmethods. Our network flowmethod
performs well above the random baseline, but is far from achieving state-of-the-art
results. The SVM experiments using all words in the document perform much better
than our network flow method, and are consistent with the accuracy of 68.7% achieved
52
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
Table 8
Average classification results using 10 and 30 training documents per newsgroup.
Training Size / Class SVM SVM SVM NF
All words Nouns (Words) Noun Concepts
10 47.1 47.8 42.7 31.2
30 66.2 66.4 61.4 32.0
by Rennie (2001) using an SVM. One possible reason is that the SVM is trained on
all words (minus stopwords and article headers), whereas our network flow method
applies to noun concepts only. The SVM performance on noun-only data is similar to
that of all words. Although there is a marked decrease in performance on the concept
frequency vectors, SVM still outperforms our method.
The poorer SVM performance on concept frequencies suggests that concept fre-
quency vectors are less easily distinguishable than word frequency vectors. Recall,
however, that we found no difference with these various training approaches for SVM
in name disambiguation. It is possible that the mapping from words to concepts is a
problem here because the full text is used, rather than a relatively small window around
a target word. Because each word can map to multiple (potentially unrelated) concepts,
the use of a larger, unconstrained bag of words may lead to a high degree of ambiguity,
introducing more noise in the semantic profile than our method can handle. This may
also explain why the network flow method does not improve with additional training
data, showing virtually no improvement between 10 and 30 training instances (0.8%
difference). We speculate that the amount of noise in a semantic profile based on the
larger amount of text may increase along with the increase in the training size, offsetting
any potential gain from having additional data.
If this hypothesis is correct, it is natural to ask why the SVM result using concepts
shows a substantial increase in accuracy from 10 to 30 training documents. If larger texts
yield nosier semantic profiles, why does this not negatively affect the SVM as well? This
highlights a fundamental distinction of our approach: our method is novel because it
finds the distance between concepts as embedded in a graph (the ontology), not just between
concept vectors. Generally, our thesis is that this is an advantage of our model: It entails
that all concepts generated from a text play a role in determining the distance of that
text from another. As we noted earlier, this allows us to find similarity between texts
that use related but not equivalent concepts. However, the performance of our method
in this document classification task reveals a potential drawback of this property of our
method. Because it takes all concepts into account in determining distance, it is more
susceptible to noise. Figure 5 illustrates the problem.We see that the square and triangle
profiles are noisy?that is, they each have a number of nodes that are not part of their
coherent semantic content. These noisy aspects of the two profiles are less separated
in ontological space, making the two profiles more similar according to our measure
than their ?true? semantic content would indicate. Because a vector representation of
concepts does not form connections between differing concepts, it is not led astray in
the way our method is.
6.2.2 Removing Noise from the Profiles. Our conjecture is that the poor performance of our
network flowmethod is due to noise caused by ambiguity in the mapping of each word
to all of its concepts (i.e., not just the relevant ones to the topic). This effect could also be
53
Computational Linguistics Volume 36, Number 1
Figure 5
Two noisy profiles, one represented by squares, the other by triangles.
exacerbated by the fact that, in using the full document, we may have a higher number
of less relevant words than when a profile is formed from a more constrained set of
words (as in verb alternation detection and name disambiguation). If this hypothesis
is true, then the noisy (irrelevant) concepts should be distributed within each profile
according to some prior probability distribution. If we knew that distribution, then we
could ?subtract out? the noise and form more semantically coherent profiles. Referring
to Figure 5, the idea is that we would like to remove the small, dispersed squares and
triangles, leaving only the larger ones that form a semantically more coherent set.
We test this idea, experimenting with two possible noise distributions. The first is
simply the uniform distribution, and the second is a distribution determined empiri-
cally using frequency counts from a domain-general corpus. For the latter, we determine
a distribution over concepts based on the nouns in the BNC. Because the BNC is a
balanced corpus, the distribution of its nouns can be considered a prior that is treatable
as noise compared to the distribution in a newsgroup posting that is specific to a
particular topic. In each case, we create a semantic profile representing the expected
noise, and then ?subtract? the resulting noise profile from each of our gold-standard
semantic profiles in the document classification task. The ?subtraction? is actually a
process of setting to zero all of the semantic profile frequencies that are less than the
noise value for that concept. Any node with a value higher than the noise value for
that node is expected to be a potentially relevant concept. We leave such nodes at their
original value so that they are more distinguished from the remaining values (now set
to zero). Figure 6 illustrates the result of applying this kind of noise reduction to the
profiles in Figure 5. We can see that low-frequency concept nodes are zeroed out, with
higher frequency nodes maintaining their concept weight.
Table 9 presents the network flow results on the noise-subtracted data, showing a
3?5% increase in the performance using 30 training documents per class. The perfor-
mance decreases with noise-subtraction when we have only 10 training documents per
class, suggesting that there may not be enough data in this case to use this simplistic
subtractive method.
Interestingly, subtracting the uniform noise distribution from the profiles has amore
favorable effect than subtracting the BNC noise distribution. The BNC distribution is
perhaps inappropriate for our data. Newsgroup data includes a variety of subjects
54
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
Figure 6
The same two profiles in Figure 5. The profile masses that are ?subtracted? are shaded in gray.
which may make it more similar to a balanced corpus than we have originally antic-
ipated, thus what we are treating as a ?noise? distribution in this case may not actually
represent noise. That said, there is a small but notable increase even using the BNC
noise distribution when we have sufficient training data. The idea of subtracting out
noise seems promising, but we leave the appropriate representation of noise, and the
mechanism for removing it effectively, as an area of future research.
7. Profile Density: A Measure of Coherence of Semantic Profiles
We have seen a performance difference across the three tasks we used in evaluation:
the network flow method outperforms purely distributional measures on verb alterna-
tion detection and name disambiguation, but does poorly on document classification
compared to a distributional approach. (See Table 10 for a summary of the results.) We
use the same ontology (WordNet) and the same concept distance (number of edges) in
our network flow measure across all three tasks, hence there must be some difference
in the three data sets themselves that impacts the ability of our method to distinguish
the semantic profiles corresponding to one class of data (one usage of an ambiguous
name, for example) from the profiles of a different class of data (the other usage of the
name). In this section, we develop a measure that can capture this property and explain
the performance differential we have observed for our method.
Table 9
Average classification results using 30 and 10 training documents per newsgroup, using the
original profiles (NF), and using profiles after the ?noise subtraction? process described in the
text (?NF ? Uniform? and ?NF ? BNC? are results subtracting the uniform distribution and
the BNC noun frequency distribution, respectively).
Training Size / Class NF NF ? Uniform NF ? BNC
10 31.2 28.2 27.4
30 32.0 37.2 35.6
55
Computational Linguistics Volume 36, Number 1
Table 10
Summary of task-based results. The numbers in parentheses indicate the number of training
instances used. The best result for each task is shown in bold.
Verb Alternation Detection random Manhattan skew div JS NF
Dataset1 Avg 0.50 0.70 0.57 0.67 0.70
Dataset2 Avg 0.50 0.67 0.67 0.67 0.67
Name Disambiguation random SVM (100) SVM (200) NF (100) NF (200)
Unweighted Avg 0.61 0.72 0.75 0.83 0.83
Weighted Avg 0.53 0.52 0.55 0.76 0.77
Document Classification random SVM (10) SVM (30) NF (10) NF (30)
20 newsgroups 0.05 0.43 0.61 0.31 0.32
7.1 Profile Coherence
Our goal is to find a property of individual semantic profiles that, when averaged across
the profiles in a data set, indicates whether our method will be able to distinguish
profiles of different classes in that data set. That is, we aim to learn about the overall
separability of the classes in a data set by investigating the properties of individual
profiles that constitute the data set. Our hypothesis is that the important factor for our
method is what we refer to as profile coherence: the degree to which profile mass is
concentrated within a constrained space (or set of constrained spaces) of the ontology.
The more spatially coherent the sets of weighted concepts are for the profiles in a data
set, the more likely it is that our method will be able to distinguish contrasting profiles.
Conversely, less coherent profiles, whose frequency mass is more distributed across a
wider area of the ontology, will be more difficult to separate into classes. (Note that
profile coherence is not a sufficient condition for data separability, but we hypothesize
that it can be a useful indicator.)
For example, consider the square and triangle profiles in Figure 7. Coherent profiles
have their profile mass (the concept weights) focused within small, distinct regions of
the ontology, as in Figure 7(a). These types of profiles tend to be highly distinguishable
from each other. Less coherent profiles, whose mass is more dispersed through the on-
tology, such as those in Figure 7(b), are likely to be less distinguishable. Note, however,
that it is not simply occupying greater or fewer nodes in the hierarchy that determines
profile coherence (and distinguishability). The profiles in Figure 7(c) are ?spread out?
as in (b), but are more coherent (and distinguishable) due to having areas of high mass.
The considerations illustrated in Figure 7 suggest that both distributional and onto-
logical factors contribute to the coherence of a semantic profile, and that we must deter-
mine a suitable measure of coherence that captures both factors. A simpler, alternative
hypothesis is that either purely distributional or purely ontological factors may suffi-
ciently capture the coherence of a semantic profile. To explore these ideas, we examine
different ways to assess the coherence of the semantic profiles in our example data sets.
We develop various measures of coherence, and then evaluate whether the degree of
coherence as determined by each measure indeed corresponds to the performance of
56
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
Figure 7
Examples of two profiles (indicated by squares and triangles) of varying coherence. The profiles
in (a) are more distinguishable than those in (b) and (c); the profile in (c) is in turn more
distinguishable than that in (b). The degree of distinguishability of these profiles is reflected in
their degree of coherence.
our network flowmethod on the data sets in our three tasks. We expect a useful measure
of profile coherence to have a high average value across the data sets on which we
perform well (verb alternation and name disambiguation), and a low average value
across the data set on which we perform poorly (document classification).
In Section 7.2, we briefly review several measures intended to separately capture
the distributional or ontological coherence of a semantic profile. We show that such
measures are insufficient for accounting for the performance differences of our method
across the data sets. In Section 7.3, we develop a novel measure to capture the coherence
of our profiles in terms of both distributional and ontological information. Thismeasure,
called profile density, expresses the degree to which a semantic profile forms a coherent
clustering of weighted concepts in an ontology. We demonstrate that our profile density
measure can account for the performance differential across our data sets.
7.2 Separate Distributional and Ontological Approaches
We explored several (unsuccessful) means for capturing profile coherence with a purely
distributional or purely ontological measure. Although we could not exhaustively
investigate all possible measures of this kind, the underlying reasons for the lack of
success of these measures in explaining the differing performance of our method across
57
Computational Linguistics Volume 36, Number 1
the data sets convinced us of the need for a measure that integrates distributional and
ontological factors (which we present in the following section). We mention the single-
factor measures here for completeness.
7.2.1 Potential Distributional Coherence. Recall that Section 6.2.2 shows that removing
the ?noise? distribution from each profile improves the document classification per-
formance of our method. In other words, subtracting the noise distribution from a
profile can make it distributionally more distinct from other profiles. Based on this
observation, we hypothesize that the less a profile resembles a noise distribution over
the ontology, the more coherent it is?that is, the more likely the frequency mass is
situated in meaningful clusters of concepts. To test this hypothesis, we calculate the
average distance (using KL-divergence [Kullback and Leibler 1951]) of the profiles in
a data set from a profile created from a noise distribution (the uniform distribution
of words, or their distribution in the BNC, as in Section 6.2.2). Higher values of this
measure indicate further distance from the noise distribution.
7.2.2 Potential Ontological Coherence. Here we consider two observations. First, we
hypothesize that profiles with fewer concepts are more coherent, because a smaller
number of concepts is more likely to be less dispersed in the ontology. We simply
use average profile size to capture this property (here, smaller values of profile size
indicate greater coherence). Second, we hypothesize that profiles whose concepts have
greater specificity are more coherent, because use of less specific concepts is indicative
of vagueness and potential lack of coherence. Because specificity corresponds well to
depth in WordNet, we use a simple measure of average profile depth to indicate the
specificity of the set of concepts in a profile (here, greater values of depth should indicate
greater coherence).
7.2.3 Analysis of the Single-Factor Measures. For each task, we calculate the average of
each of the hypothesized distributional and ontological coherence measures over the
profiles in the data set, and find that there is no consistent correspondence with the
performance of our network flow method across the tasks. Despite the intuitions and
observations presented herein, these results are not surprising. For example, the profiles
of a data set may all be distributionally very similar overall to the noise profile, sup-
posedly indicating low coherence, but they may be quite coherent in the actual ontolog-
ical space they occupy. Similarly, the profiles in a data set may all have a small average
depth in the ontology or large size (again supposedly indicating low coherence), but
their distributional properties (the weights on the concepts that are occupied) may yield
coherent clusters of mass in the profile. This analysis then confirms our hypothesis that,
because distributional and ontological information are intertwined in the representation
of a semantic profile, a useful measure of profile coherence must take into account an
integration of these two information sources.
7.3 Integrating Distributional and Ontological Factors in a Coherence Measure
As noted earlier, and tentatively confirmed by the results herein, we assume that the
interaction of distributional and ontological factors determines the coherence of pro-
files (i.e., a coherent profile has its frequency mass concentrated within a reasonably
constrained space [or set of constrained spaces] of the ontology). We observe that this
is similar to the geographical notion of population density, which is determined by
the population mass divided by the area occupied. Here we extend the geographical
58
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
Figure 8
Two examples of profile density within an ontology. The hollow triangles are the common
ancestors of the filled triangles, which are concept nodes in the profile. The profile in (a) is fairly
dispersed, requiring a single but distant ancestor node. The profile in (b) is more clustered; two
ancestor nodes are required but each is close to its descendants.
definition of density within our network framework by relating population mass to
distributional weights on concepts, and occupied area to the spread of the weighted
concepts in the ontology. We call the resulting measure of profile coherence profile
density.
7.3.1 The Profile Density Measure. To adapt the definition of geographical density to our
problem, we first need to determine the analogs of population mass and occupied area
in a semantic profile. The profile mass at each concept node is directly analogous to
the population mass. Defining the occupied area within an ontology is not as straight-
forward, as there is no simple definition of area within a graph. For example, Agirre and
Rigau (1996) use the number of nodes within a subgraph as its area, but this fails to take
into account how dispersed the nodes are throughout the ontology. We instead develop
a definition of area that captures the actual spatial spread of the profile mass through
the ontology.
To begin, we note that any subgraph of the WordNet hypernym hierarchy is hi-
erarchical itself. Thus, any region of the ontology that contains some profile mass is
a hierarchy rooted at some common ancestor of those profile nodes.10 As shown in
Figure 8, themore dispersed (less closely clustered together) a set of nodes is, the further
away their common ancestor is. That is, a highly related (and spatially constrained) set
of concept nodes can be generalized to a more specific ancestor concept (i.e., near the
descendants, as in Figure 8(b)), whereas a semantically distant set of concepts will be
generalized to a semantically general ancestor concept (i.e., far from the descendants,
as in Figure 8(a)). The ontological distance between a set of nodes and their common
ancestor thus indicates how closely clustered the descendant nodes are.
Next note that any semantic profile can be represented by a set of ancestor nodes,
and these ancestor nodes capture the spatial clusterings of the profilemass. For example,
10 Although WordNet contains instances of multiple inheritance, the rate is low. As a result, the likelihood
of a set of profile nodes sharing multiple ancestors is low as well.
59
Computational Linguistics Volume 36, Number 1
Figure 9
These two profiles have equal density value given our original profile density formula in
Equation (5), but are suitable distinguished (with the profile in (b) having higher density than
that in (a)) by the norm density formula in Equation (6). See the text for discussion.
the profile in Figure 8(a) is represented by one ancestor node, and that in Figure 8(b) by
two such nodes. Combining these observations, we see that given a suitable manner for
identifying ancestor nodes to represent a profile, we can use the combined ontological
distance between each of those nodes and their descendants as an indication of how
closely clustered the concepts of the profile are. We can now complete our definition
of profile density by using the total distance between each identified ancestor and its
descendants as an indication of the occupied area of the ontology.
Formally, let P be a profile and A be a set of ancestor concept nodes such that
each profile node d ? P is guaranteed to have an ancestor a ? A. (We will explain in
Section 7.3.2 how to find the set A.) The profile density of P is then defined as follows:
profile density(P) =
?
a?A
?
d?P,
d?descendant(a)
mass(d)
distance(d, a)
(5)
where mass(d) is the profile mass (concept frequency) at node d, and distance(d, a) is the
distance in the ontology between node d and node a, as given by a suitable concept-to-
concept distancemeasure (such as the edge distance that we have used in our task-based
evaluations).
There is one more subtle detail we must address. Consider the two examples in
Figure 9, where the distance between each ancestor and all its descendants is the same
(here, say, a distance of 1), but the distribution of the profile mass differs. The first
diagram has ten equally weighted profile nodes, and the second has two. Our current
formulation in Equation (5) yields a density of 1 for both diagrams (i.e., (0.1/1)? 10 =
1 = (0.5/1)? 2). However, the profile mass in diagram (a) is distributed among more
nodes than that in diagram (b). Intuitively, the second profile is more densely clustered
and should have a higher density value.
Looking more closely at our density formula in Equation (5), observe that the
number of profile nodes has an impact on the calculation?that is, density increases
as the number of profile nodes increases due to the inner summation in the formula. To
60
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
achieve an appropriate density measure, then, we normalize the original density value
by the number of profile nodes, resulting in a normalized density for a profile:
norm density(P) =
density(P)
sizeof (P)
= 1
sizeof (P)
?
a?A
?
d?P,
d?descendant(a)
mass(d)
distance(d, a)
(6)
Returning to our example in Figure 9, Equation (6) assigns the first profile a normalized
density of 0.1, and the second profile a normalized density of 0.5. The modified measure
now appropriately distinguishes the two profile densities, indicating that the profile in
Figure 9(a) is less tightly clustered than the profile in Figure 9(b).
7.3.2 Finding the Ancestor Set for Profile Density. As noted earlier, our definition of profile
density depends on identifying a suitable set of ancestor nodes of the concept nodes
in the profile: the aggregate distance of the ancestors to the profile nodes indirectly
indicates the degree to which the profile nodes are spatially clustered close together.
Thus, given a profile P, we need to find A, the set of nodes that are ancestors of the
profile nodes d ? P. (The nodes a ? A correspond to the hollow triangles indicated in
Figure 8 and Figure 9.) Recall that these ancestor nodes are intended to be a set of
concepts that serve as an appropriate generalization of the nodes in the profile?each
ancestor in a sense represents a coherent cluster of profile nodes. However, we do not
know a priori what the appropriate level of generalization is?we simply want a level
that gives a useful assessment of how clustered together the profile nodes are.
For this purpose, we make use of Clark and Weir?s (2002) method for generalizing
a set of weighted concept nodes in an ontology. As we noted in Section 4, given a
frequency distribution over all concept nodes, Clark andWeir use a statistical method to
search for the set of nodes (i.e., our node setA) that best generalize the original weighted
concepts. This method is particularly appropriate for our purposes because it includes a
parameter,? ? (0, 1), that controls the level of generalization.We vary? over five values
(0.05, 0.25, 0.5, 0.75, and 0.95) to obtain five different (more to less generalized) sets of
ancestors. In our analysis, we calculate the density using each ancestor set in order to
evaluate the impact of the precise choice of ancestor nodes on our measure.
7.3.3 Results and Analysis. For each of the three tasks in our earlier task-based evaluation,
we calculate the profile density of the corresponding data set. We define the profile
density of a data set to be the average of the normalized density values over its profiles.
For the verb alternation detection task, we perform the analysis on all 240 profiles used
in the task (120 verbs, with 2 profiles per verb, one for the subject slot, one for the object
slot). In the remaining two tasks, because each instance profile is compared to a gold-
standard profile, we believe that the performance depends primarily on the coherence of
the gold-standard profiles. We thus perform our analysis on the gold-standard profiles
only. For name disambiguation, we have 60 profiles (5 samplings with 12 gold-standard
profiles each); for document classification, we have 100 profiles (5 samplings with 20
gold-standard profiles each). For each profile, we calculate the normalized density using
each of five ancestor sets (based on the ? value, as noted above). For the concept-to-
concept distance measure, distance(d, a) in Equation (6), we use edge distance, the same
measure used in the tasks in earlier sections of the paper.
61
Computational Linguistics Volume 36, Number 1
Table 11
The profile density scores for each data set at five different values of ?, as well as the average
scores across the ? values.
? value 0.05 0.25 0.5 0.75 0.95 Avg
Verb Alternation 5.59e-4 5.90e-4 6.32e-4 7.14e-4 8.87e-4 6.76e-4
Name Disambiguation (200) 8.93e-5 9.89e-5 1.08e-4 1.18e-4 1.35e-4 1.10e-4
Name Disambiguation (100) 1.11e-4 1.26e-4 1.38e-4 1.52e-4 1.78e-4 1.41e-4
Doc Classification (30) 5.25e-5 5.94e-5 6.59e-5 7.43e-5 8.78e-5 6.80e-5
Doc Classification (10) 8.03e-5 8.85e-5 9.87e-5 1.11e-5 1.33e-5 5.84e-5
We expect that, if our profile density measure does indeed reflect the coherence
of a data set, then we will see a correspondence between the density values and the
performance of our network flow method. Higher density values indicate a profile
whose weighted concepts form more coherent clusters in the ontology. Specifically,
then, we expect higher density values for the data sets from our verb alternation de-
tection and name disambiguation tasks (on which our method had better performance
than distributional methods), and lower density values for the document classification
data set (on which our method had worse performance than a purely distributional
method).11
Table 11 shows the profile densities of each data set. First note that the density
values are relatively stable across all values of ?, indicating that the precise level of
generalization is not critical to the usefulness of our density measure. Next, observe
that, as predicted, the document classification data set is shown to have the lowest
density for both training set sizes. This observation is in accord with our hypothesis
that the profile density measure indicates the coherence of the profiles in a data set and
is therefore informative about the network flow performance on that data set.
Interestingly, we also observe that, across all values of ? and training set sizes, the
verb alternation data set has the largest densities, followed by the name disambiguation
data set, then the document classification data. (The differences between all three data
sets are statistically significant, p 0.05.) This result might stem from the fact that
there are varying degrees of constraint placed upon the data in the three tasks. In
verb alternation, the nouns used to generate a profile appear either all in the subject
or all in the object position of the target verb. In name disambiguation, we loosen the
restriction to include all nouns in a small window surrounding the target word. Lastly,
in document classification, the only restriction on the nouns used to generate a profile
is that they appear in the same document. This suggests that the syntactic and semantic
constraints placed upon a set of nouns can have an impact on the coherence of the profile
created from them.
This latter observation suggests that our profile density measure may be useful
not only in indicating the ability of our network flow method to distinguish relevant
profiles. More generally, it may also reflect the varying degrees of syntactic and semantic
11 Note that because our method in each task is compared to different kinds of alternative distributional
methods, we do not expect to find a mathematical correlation between the performance improvement
and the density values; rather, good performance should be reflected in higher density values and poor
performance in lower density values.
62
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
constraints placed upon the set of words that generate a profile. Our profile density
measure may indeed be generally useful as a measure of the semantic coherence of a set
of concepts in an ontology (Gurevych et al 2003), a matter we plan to explore in future
work.
In summary, our analysis in this section has shown that both distributional and
ontological properties contribute to the coherence of a profile, but neither alone is
indicative of the network flow performance in a particular task. Our new measure of
profile density serves as a tool for analyzing profiles that integrates their distributional
and ontological coherence, and provides a post hoc means for explaining the perfor-
mance differential of our method across the different tasks we performed here. The
results also point to the possibility of devising a diagnostic tool for the suitability of the
network flow method on novel data. An analysis of the data and results across a larger
set of tasks will allow us to investigate the possibility of determining a density threshold
that would be indicative of expected positive results with our method.
8. Related Work
To the best of our knowledge, our method is the only work that measures text distance
by combining ontological knowledge and distributional information together via a
graph-based algorithm. Although there are existing methods that use either or both
types of information in measuring the semantic distance of texts (Corley and Mihalcea
2005; Mohammad and Hirst 2006), our work is unique in that it integrates the ontolog-
ical distance between individual words across two texts as well as the distributional
differences between the texts. Here we review existing work on both text comparison
and graph-based approaches in CL, given the relevance of these two areas to our
research.
8.1 Text Comparison
Our work stems from the studies on measuring the semantic distance between two
words or concepts using an ontological resource (which is extensively covered in
Pedersen, Banerjee, and Patwardhan [2005] and Budanitsky andHirst [2006]). To extend
these methods for the comparison of two texts, we incorporate ontological distance
between concepts and distributional information in a systematic and efficient manner.
Other research that attempts to include the two takes a more modular approach. For
example, Corley and Mihalcea (2005) consider the ontological distance between the
concepts representing the texts but ignore their distributional information. On the other
hand, Scott andMatwin (1998), McCarthy (2000), andMohammad and Hirst (2006) take
the distributional distance between concept vectors representing the texts but do not
consider the ontological relations among the concepts.
Most recent work on text comparison tends to be word-based and distributional
(Lee 2001; Weeds, Weir, and McCarthy 2004; Pedersen, Purandare, and Kulkarni 2005;
Al-Mubaid and Umair 2006). In the case of high dimensionality and data sparseness,
words are grouped into a smaller number of (unnamed) concepts using some matrix
factorization technique (e.g., SVD) or some clustering method (Pereira, Tishby, and
Lee 1993). In other words, words are grouped together based on their distributional
properties instead of their explicit semantic/ontological properties. Furthermore, unlike
in our method, once the words are collapsed into unnamed concepts, the individual
elements (i.e., the unnamed concepts) across data points cannot be compared. As shown
63
Computational Linguistics Volume 36, Number 1
in our experiments, taking into account this extra piece of information is beneficial for
some applications.
8.2 Graph Approaches
In recent years, we have seen an increasing use of graph-based methods in NLP (Pang
and Lee 2004; Mihalcea 2005; Navigli and Velardi 2005). The graph-theoretic approach
is popular due to the elegance of representing appropriate NLP problems and the
availability of a number of efficient algorithms. One of the most straightforward NLP
examples is the use of WordNet. Besides our work here, much prior research has taken
advantage of the graphical structure of WordNet. For example, Agirre and Rigau?s
(1996) conceptual density uses WordNet as a graph and calculates the density within
a subgraph (the number of relevant concepts within a subgraph), which was found to
be useful for WSD.
Graphs in general are the obvious mathematical formalism to encode the relation-
ships (represented as edges) between either words or longer units of text (represented
as nodes). (The reverse is possible, using nodes to represent relations and edges for
semantic entities. The choice of representation clearly depends on the NLP task itself.)
Once we formulate a problem into a standard graph problem, there are existing efficient
graph-based algorithms that we can use to find an optimal or near-optimal solution.
For example, both Pang and Lee (2004) and Barzilay and Lapata (2005) use a minimum-
cut algorithm for two vastly different applications, document polarity classification and
content selection, respectively. In these approaches, the sentences are represented as
nodes in a graph, and the edge connecting each pair of nodes is weighted with an
association score between the sentences, reflecting, for example, the distance (number
of sentences) between a pair of sentences. The minimum-cut method allows them to
classify the nodes, and thus the sentences, into different categories.
Another popular graphmethod is the randomwalk algorithm, which is successfully
employed by the PageRank approach for ranking Web pages (Brin and Page 1998).
Similar to the minimum-cut algorithm, here, nodes represent semantic entities (e.g.,
words), and edges represent associations between the nodes (e.g., word co-occurrence).
The random walk algorithm allows for the classification of each node based on the
relevance of its neighbors. For example, Mihalcea (2006) uses random walk for WSD
by constructing a graph in the following way. Each node represents an ambiguous (test)
word, or a (training) word labelled with one of its senses. Each edge indicates that the
corresponding two words co-occur in some context. The sense of an ambiguous word
is determined by the sense of its most relevant neighbor(s), by randomly traversing
the graph until an equilibrium state has been reached. Hughes and Ramage (2007)
also use a random walk method, with the goal of determining semantic relatedness
between individual words (not sets of words, as in our work). In their work, the random
walk method computes a probability distribution over WordNet concepts. Note that the
probability distributions resulting from random walks centered at different concepts
in WordNet are distinct. One can then measure the semantic relatedness between two
concepts by calculating the divergence between their probability distributions over
WordNet concepts as a result of the two random walks centered at them.
In comparison to other graph approaches to NLP, we choose to use a minimum-
cost flow algorithm based on our graph formulation. Because a profile is a collection
of frequency-weighted concepts, some concept nodes are weighted more heavily than
others, therefore the routes between such nodes across the two profiles are also
weighted more heavily. An algorithm solving a minimum-cost flow problem provides
64
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
an efficient mechanism to find these weighted routes as our solution, making MCF,
rather than the shortest paths or maximum-flow-minimum-cut, the best choice for
formalizing the constraints we define in the text comparison problem.
9. Conclusion
We have presented a graph-theoretic approach to calculating semantic distance between
two texts, which encompasses both ontological knowledge and distributional informa-
tion. We have developed a network flow method that takes advantage of the graphical
structure of an ontology. Given a suitable ontology, a word frequency vector for a text
can be transformed into a frequency distribution over concept nodes. Hence, we treat
texts as weighted subgraphs within a larger graph (the ontology). By incorporating the
semantic distance between individual concepts, the graphical structure representing
the ontology becomes a metric space in which we can measure the distance between
subgraphs, weighted by their frequencies.
In this article, we use edge distance exclusively for the individual distance between
concepts. Given that the distance between concepts is an integral part of our for-
mulation, and that other sophisticated concept-to-concept distances have been shown
to outperform edge distance for comparing concepts (Jarmasz and Szpakowicz 2003;
Weeds 2003), we also investigate the use of such distances. However, incorporating
them can lead to a quadratic growth in complexity. To remedy this, a pre-processing
step is required to reduce the complexity to reasonable computation time. In Tsang and
Stevenson (2006), we introduce one such method by performing a graph transformation
on the original network prior to the network flow calculation. The transformed network
is more efficient to process without compromising the performance accuracy. We refer
the reader to that paper for further information.
In the task-based evaluation presented here, our method has been shown to provide
superior performance on verb alternation detection and name disambiguation, in com-
parison to alternative distributional approaches?even in cases where the alternative
methods have attempted to incorporate additional semantic knowledge (McCarthy
2000; Pedersen, Purandare, and Kulkarni 2005). Unlike existing distributional distances
and clustering techniques, the use of our text representation as well as the integration
of ontological distance allows a systematic way of capturing appropriate semantic
distinctions between the texts in these tasks.
In contrast, our method does not perform as well on document classification as a
state-of-the-art machine learning algorithm using a purely distributional approach. In
order to examine the performance discrepancy across tasks, we explore measures of the
coherence of the profiles in a data set, as potential indicators of how easily semantic
profiles of different classes can be distinguished. The purely distributional and purely
ontological indicators we consider are not useful in explaining the relatively poor per-
formance of our method on document classification. In response, we develop a measure
of profile coherence, called profile density, that integrates these factors by determining
the degree to which a profile forms distributionally and ontologically coherent clusters
of concepts. As a result, we are able to explain the performance of our method on the
data sets in terms of their density values.
Recall also that we saw a performance difference in the verb alternation task de-
pending on the different method used to generate the semantic profiles from the bag of
words of the text (i.e., using ?raw? data, versus a method to generalize to the best set of
concepts for the bag of words). Given also that we found that the profiles in document
classification have a low density (i.e., their concepts are overly dispersed), one focus for
65
Computational Linguistics Volume 36, Number 1
future work will be to explore further means for generating profiles that best capture the
intended senses of the words within the text. One option may be to use Mohammad?s
(2008) unsupervised method for building concept vectors from word frequency data,
which focuses the frequencies onto the most likely senses of the words according to
coarse ontological categories.
Another strand of future work relates to our profile density measure. We suggest
that not only is our profile density useful in predicting the performance of our network
flowmethod on unseen data, it may also be useful formeasuring the semantic coherence
of a text. Note that a text that is semantically coherent tends to form profiles with
highly frequent and highly related concepts within an ontology. Coincidentally, our
profile density formulation measures the overall relatedness, and thus coherence, of a
collection of concepts by taking into account the distance between the concepts as well
as the frequency distribution. For example, if we relax the notion of a text to include
verbal arguments, semantic coherence of a text can be thought of as the selectional
preference strength a verb imposes on its arguments. As future work, we intend to
investigate profile density as an indicator of selectional preference strength. Generally,
we believe profile density may offer a quantitative measure for semantic coherence and
other related NLP applications.
Acknowledgments
We would like to thank our colleagues in
Toronto, in particular Afsaneh Fazly and
the CL research group at the University of
Toronto, for helpful discussions. We would
also thank the anonymous reviewers for
their detailed comments. We gratefully
acknowledge the financial support provided
by the Natural Sciences and Engineering
Research Council of Canada (NSERC),
Ontario Graduate Scholarship (OGS),
and the University of Toronto.
References
Agirre, Eneko and German Rigau. 1996.
Word sense disambiguation using
conceptual density. In Proceedings of the
12th International Conference of
Computational Linguistics (COLING-1996),
pages 16?22, Copenhagen.
Al-Mubaid, Hisham and Syed A. Umair.
2006. A new text categorization technique
using distributional clustering and
learning logic. IEEE Transaction on
Knowledge and Data Engineering,
18(9):1156?1165.
Barzilay, Regina and Mirella Lapata. 2005.
Collective content selection for
concept-to-text generation. In Proceedings
of the Joint Conference on Human Language
Technology / Empirial Methods in Natural
Language Processing (HLT/EMNLP),
pages 331?338, Vancouver, Canada.
Bodenreider, Olivier. 2004. The unified
medical language system (UMLS):
Integrating biomedical terminology.
Nucleic Acids Research, 32:D267?D270.
Brin, Sergey and Lawrence Page. 1998.
The anatomy of a large-scale
hypertextual Web search engine.
Computer Networks and ISDN Systems,
30(1?7):107?117.
Briscoe, Ted and John Carroll. 1997.
Automatic extraction of subcategorization
from corpora. In Proceedings of the 5th
Applied Natural Language Processing
Conference (ANLP), pages 356?363,
Washington, DC, USA.
Briscoe, Ted and John Carroll. 2002. Robust
accurate statistical annotation of general
text. In Proceedings of the Third International
Conference on Language Resources and
Evaluation (LREC 2002), pages 1499?1504,
Las Palmas, Spain.
Budanitsky, Alex and Graeme Hirst. 2001.
Semantic distance in Wordnet: An
experimental, application-oriented
evaluation of five measures. In Proceedings
of the Workshop on WordNet and Other
Lexical Resources, in the North American
Chapter of the Association for Computational
Linguistics (NAACL-2001), pages 29?34,
Pittsburgh, PA, USA.
Budanitsky, Alex and Graeme Hirst. 2006.
Evaluating WordNet-based measures of
semantic distance. Computational
Linguistics, 32(1):13?47.
Burnard, Lou. 2000. The British National
Corpus Users Reference Guide. Oxford
University Computing Services,
Oxford, UK.
66
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
Chang, Chih-Chung and Chih-Jen Lin, 2001.
LIBSVM: a library for support vector
machines. Software available at
www.csie.ntu.edu.tw/?cjlin/libsvm.
Chva?tal, Vas?ek. 1983. Linear Programming.
W.H. Freeman and Company, New York.
Clark, Stephen and David Weir. 2002.
Class-based probability estimation using a
semantic hierarchy. Computational
Linguistics, 28(2):187?206.
Corley, Courtney and Rada Mihalcea. 2005.
Measuring the semantic similarity of texts.
In Proceedings of the ACL Workshop on
Empirical Modeling of Semantic Equivalence
and Entailment, pages 13?18, Ann Arbor,
Michigan, USA.
Esuli, Andrea, Tiziano Fagni, and Fabrizio
Sebastiani. 2006. TreeBoost.MH: A
boosting algorithm for multi-label
hierarchical text categorization. In
Proceedings of the 13th International
Symposium on String Processing and
Information Retrieval (SPIRE?06),
pages 13?24, Glasgow.
Fellbaum, Christiane, editor. 1998.WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Gangemi, Aldo, Nicola Guarino, and
Alessandro Oltramari. 2001. Conceptual
analysis of lexical taxonomies: The case of
WordNet top-level. In Chris Welty and
Barry Smith, editors, Formal Ontology in
Information Systems: Collected papers from
the Second International Conference. ACM
Press, pages 285?296, New York, USA.
Garey, Michael R. and David S. Johnson.
1979. Computers and Intractability: A Guide
to the Theory of NP-Completeness. W.H.
Freeman and Co., New York.
Gurevych, Iryna, Rainer Malaka, Robert
Porzel, and Hans-Peter Zorn. 2003.
Semantic coherence scoring using an
ontology. In Proceedings of the Joint Human
Language Technology and Northern Chapter of
the Association for Computational Linguistics
Conference (HLT-NAACL), pages 88?95,
Edmonton.
Han, Hui, Hongyuan Zha, and C. Lee Giles.
2005. Name disambiguation in author
citations using a K-way spectral clustering
method. In Joint Conference on Digital
Libraries (JCDL?05), pages 334?343, Denver,
CO, USA.
Hirst, Graeme. 2009. Ontology and the
lexicon. In Steffen Staab and Rudi Studer,
editors, Handbook on Ontologies.
International Handbooks on Information
Systems. Springer, New York,
pages 269?292.
Hughes, Thad and Daniel Ramage. 2007.
Lexical semantic relatedness with
random graph walks. In Proceedings of
the Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 581?589, Prague.
Iwayama, Makoto, Atsushi Fujii, Noriko
Kando, and Yuzo Marukawa. 2003. An
empirical study on retrieval models for
different document genres: Patents and
newspaper articles. In Proceedings of the
26th ACM SIGIR International Conference on
Research and Development in Information
Retrieval, pages 251?258, Toronto, Canada.
Jarmasz, Mario and Stan Szpakowicz. 2003.
Roget?s thesaurus and semantic similarity.
In Proceedings of Conference on Recent
Advances in Natural Language Processing
(RANLP 2003), pages 212?219, Borovets.
Jiang, Jay and David Conrath. 1997. Semantic
similarity based on corpus statistics and
lexical taxonomy. In Proceedings on the
International Conference on Research in
Computational Linguistics, pages 19?33,
Taipei, Taiwan.
Joachims, T. 2002. Learning to Classify Text
Using Support Vector Machines?Methods,
Theory, and Algorithms. Kluwer/Springer,
New York.
Kullback, S. and R. A. Leibler. 1951. On
information and sufficiency. Annals of
Mathematical Statistics, 22:79?86.
Landauer, Thomas K. and Susan T. Dumais.
1997. A solution to Plato?s problem: The
Latent Semantic Analysis theory of the
acquisition, induction, and representation
of knowledge. Psychological Review,
(104):211?240.
Lee, Lillian. 2001. On the effectiveness of the
skew divergence for statistical language
analysis. In Artificial Intelligence and
Statistics, pages 65?72.
Levin, Beth. 1993. English Verb Classes and
Alternations: A Preliminary Investigation.
University of Chicago Press, Chicago.
Levina, Elizaveta and Peter Bickel. 2001.
The earth mover?s distance is the
mallows distance: Some insights from
statistics. In Proceedings of the Eighth
IEEE International Conference on Computer
Vision, volume 2, pages 251?256,
Vancouver, Canada.
Lewis, David D., Yiming Yang, Tony G. Rose,
and Fan Li. 2004. RCV1: A new benchmark
collection for text categorization research.
Journal of Machine Learning Research,
(5):361?397.
Li, Hang and Naoki Abe. 1998. Word
clustering and disambiguation based on
67
Computational Linguistics Volume 36, Number 1
co-occurrence data. In Proceedings of
COLING-ACL 1998, pages 749?755,
Montreal, Canada.
Lin, Dekang. 1998. An information-theoretic
definition of similarity. In Proceedings of
International Conference on Machine
Learning, pages 296?304, Madison,
Wisconsin, USA.
McCarthy, Diana. 2000. Using semantic
preferences to identify verbal
participation in role switching alternations.
In Proceedings of Applied Natural Language
Processing and North American Chapter of the
Association for Computational Linguistics
(ANLP-NAACL 2000), pages 256?263,
Seattle, Washington, USA.
Merlo, Paola and Suzanne Stevenson. 2001.
Automatic verb classification based on
statistical distributions of argument
structure. Computational Linguistics,
27(3):393?408.
Mihalcea, Rada. 2005. Unsupervised
large-vocabulary word sense
disambiguation with graph-based
algorithms for sequence data labeling.
In Proceedings of the Joint Conference on
Human Language Technology / Empirial
Methods in Natural Language Processing
(HLT/EMNLP), pages 411?418, Vancouver,
Canada.
Mihalcea, Rada. 2006. Random walks on text
structures. In Proceedings of Computational
Linguistics and Intelligent Text Processing
(CICLing) 2006, pages 249?262, Mexico
City, Mexico.
Mitchell, Tom. 1999. 20 newsgroups usenet
articles. http://kdd.ics.uci.edu/
/databases/20newsgroups/
20newsgroups.data.html.
Mohammad, Saif. 2008.Measuring Semantic
Distance using Distributional Profiles of
Concepts. Ph.D. thesis, University of
Toronto, Canada.
Mohammad, Saif and Graeme Hirst. 2006.
Distributional measures of
concept-distance: A task-oriented
evaluation. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing (EMNLP 2006),
pages 35?43, Sydney.
Navigli, Roberto and Paola Velardi. 2005.
Structural semantic interconnections: A
knowledge-based approach to word sense
disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence,
27(7), pages 1075?1086.
Nigam, Kamal, Andrew McCallum, and Tom
Mitchell. 2006. Semi-Supervised text
classification using EM. In Olivier
Chapelle, Bernhard Scho?lkopf, and
Alexander Zien, editors, Semi-Supervised
Learning. MIT Press, Cambridge, MA,
pages 33?56.
Pang, Bo and Lillian Lee. 2004. A
sentimental education: Sentiment
analysis using subjectivity summarization
based on minimum cuts. In Proceedings of
the 42nd ACL, pages 271?278, Barcelona,
Spain.
Pantel, Patrick and Dekang Lin. 2000. An
unsupervised approach to prepositional
phrase attachment using contextually
similar words. In Proceedings of Association
for Computational Linguistics (ACL-00),
pages 101?108, Hong Kong.
Pedersen, Ted, Satanjeev Banerjee, and
Siddharth Patwardhan. 2005. Maximizing
semantic relatedness to perform word
sense disambiguation. Technical Report
UMSI 2005/25, University of Minnesota,
Duluth.
Pedersen, Ted, Amruta Purandare, and
Anagha Kulkarni. 2005. Name
discrimination by clustering similar
context. In Proceedings of the Sixth
International Conference on Intelligent Text
Processing and Computational Linguistics,
pages 226?237, Mexico City, Mexico.
Pereira, Fernando, Naftali Tishby, and
Lillian Lee. 1993. Distributional
clustering of English words. In
Proceedings of the 31st Annual Meeting
of the Association for Computational
Linguistics, pages 183?190, Columbus,
Ohio, USA.
Pinker, Steven. 1989. Learnability and
Cognition: The Acquisition of Argument
Structure. MIT Press, Cambridge, MA.
Rennie, Jason. 2001. Improving Multi-class
Text Classification with Naive Bayes.
Master?s thesis, Massachusetts Institute of
Technology, Cambridge, MA.
Resnik, Philip. 1993. Selection and Information:
A Class-Based Approach to Lexical
Relationships. Ph.D. thesis, University of
Pennsylvania, Philadelphia, PA.
Resnik, Philip. 1995. Using information
content to evaluate semantic similarity in a
taxonomy. In Proceedings of the 14th
International Joint Conference on Artificial
Intelligence, pages 448?453, Montreal,
Canada.
Ribas, Francesc. 1995. On learning more
appropriate selectional restrictions. In
Proceedings of the Seventh Conference of the
European Chapter of the Association for
Computational Linguistics, pages 112?118,
Dublin.
68
Tsang and Stevenson A Graph-Theoretic Framework for Semantic Distance
Schulte im Walde, Sabine. 2006. Experiments
on the automatic induction of German
semantic verb classes. Computational
Linguistics, 32(2):159?194.
Scott, Sam and Stan Matwin. 1998. Text
classification using WordNet hypernyms.
In Proceedings of the COLING-ACL
Workshop on Usage of WordNet in Natural
Language Processing Systems, pages 45?51,
Montreal, Canada.
Tsang, Vivian and Suzanne Stevenson. 2004.
Calculating semantic distance between
word sense probability distributions. In
Proceedings of the Eighth Conference on
Computational Natural Language Learning
(CoNLL-2004), pages 81?88, Boston,
MA, USA.
Tsang, Vivian and Suzanne Stevenson.
2006. Context comparison as a minimum
cost flow problem. In Proceedings of
HLT-NAACL 2006 Workshop on
Textgraphs: Graph-based Algorithms for
Natural Language Processing, pages 97?104,
New York, NY.
Weeds, Julie. 2003.Measures and Applications
of Lexical Distributional Similarity. Ph.D.
thesis, University of Sussex, Sussex, UK.
Weeds, Julie, David Weir, and Diana
McCarthy. 2004. Characterising measures
of lexical distributional similarity. In
Proceedings of the 20th International
Conference of Computational Linguistics
(COLING-2004), pages 1015?1021, Geneva,
Switzerland.
Wilcoxon, Frank. 1945. Individual
comparisons by ranking methods.
Biometrics, 1:80?83.
Wu, Zhibiao and Martha Palmer. 1994. Verb
semantics and lexical selection. In
Proceedings of the 32nd Annual Meeting of the
Association for Computational Linguistics,
pages 133?138, Las Cruces, New Mexico,
USA.
Xu, Wei, Xin Liu, and Yihong Gong. 2003.
Document clustering based on
non-negative matrix factorization. In
Proceedings of the 26th ACM SIGIR
International Conference on Research and
Development in Information Retrieval,
pages 267?273, Toronto, Canada.
69

Automatically Identifying the Source Words
of Lexical Blends in English
Paul Cook?
University of Toronto
Suzanne Stevenson??
University of Toronto
Newly coined words pose problems for natural language processing systems because they are not
in a system?s lexicon, and therefore no lexical information is available for such words. A common
way to form new words is lexical blending, as in cosmeceutical, a blend of cosmetic and
pharmaceutical. We propose a statistical model for inferring a blend?s source words drawing on
observed linguistic properties of blends; these properties are largely based on the recognizability
of the source words in a blend. We annotate a set of 1,186 recently coined expressions which
includes 515 blends, and evaluate our methods on a 324-item subset. In this first study of
novel blends we achieve an accuracy of 40% on the task of inferring a blend?s source words,
which corresponds to a reduction in error rate of 39% over an informed baseline. We also
give preliminary results showing that our features for source word identification can be used
to distinguish blends from other kinds of novel words.
1. Lexical Blends
Neologisms?newly coined words or new senses of an existing word?are constantly
being introduced into a language (Algeo 1980; Lehrer 2003), often for the purpose
of naming a new concept. Domains that are culturally prominent or that are rapidly
advancing, such as electronic communication and the Internet, often contain many ne-
ologisms, although novel words arise throughout a language (Ayto 1990, 2006; Knowles
and Elliott 1997). Consequently, any natural language processing (NLP) system operat-
ing on recently produced text will encounter new words. Because lexical resources are
often a key component of an NLP system, performance of the entire system will likely
suffer due to missing lexical information for neologisms. Ideally, an NLP system could
identify neologisms as such, and then infer various aspects of their syntactic or seman-
tic properties necessary for the computational task at hand. Recent approaches to this
kind of lexical acquisition task typically infer the target lexical information from sta-
tistical distributional properties of the terms. However, this technique is generally not
? Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,
Canada, E-mail: pcook@cs.toronto.edu.
?? Department of Computer Science, University of Toronto, 6 King?s College Rd., Toronto, ON M5S 3G4,
Canada, E-mail: suzanne@cs.toronto.edu.
Submission received: 4 November 2008; revised submission received: 23 May 2009; accepted for publication:
24 June 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 1
applicable to neologisms, which are relatively infrequent due to their recent intro-
duction into the language. Fortunately, linguistic observations regarding neologisms?
namely, that they are formed through specific word formation processes?can give
insights for automatically learning their lexical properties.
New words come about through a variety of means, including derivational mor-
phology, compounding, and borrowing from another language (Algeo 1980; Bauer 1983;
Plag 2003). Computational work on neologisms has largely focused on particular word
formation processes, and has exploited information about the formation process to
learn aspects of the semantic properties of words (Means 1988; Nadeau and Turney
2005; Baker and Brew 2008, for example). Subtractive word formations?words formed
from partial orthographic or phonological content from existing words?have received
a fair amount of attention recently in computational linguistics, particularly under the
heading of inferring the long form of acronyms, especially in the bio-medical domain
(e.g., Schwartz and Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou
2006, for example).
Lexical blends?the focus of this study?also known as blends, are another common
type of subtractive word formation. Most blends are formed by combining a prefix
of one source word with a suffix of another source word, as in brunch (breakfast and
lunch). There may be overlap in the contribution of the source words, as in fantabulous
( fantastic and fabulous). It is also possible that one or both source words are entirely
present, for example, gaydar (gay radar) and jetiquette ( jet etiquette). We refer to blends
such as these as simple two-word sequential blends, and focus on this common type
of blend in this article. Blends in which (part of) a word is inserted within another
(e.g., entertoyment, a blend of entertainment and toy) and blends formed from more than
two source words (e.g., nofriendo from no, friend, and Nintendo) are rare. In Algeo?s
(1991) study of new words, approximately 5% were blends. However, in our analysis
of 1,186 words taken from a popular neologisms Web site, approximately 43% were
blends. Clearly, computational techniques are needed that can augment lexicons with
knowledge of novel blends.
The precise nature and intended use of a computational lexicon will determine
the degree of processing required of a novel blend. In some cases it may suffice for
the lexical entry for a blend to simply consist of its source words. For example, a
system that employs a measure of distributional similarity may benefit from replacing
occurrences of a blend?likely a recently coined and hence low frequency item?by its
source words, for which distributional information is likely available. In other cases,
further semantic reasoning about the blend and its source words may be required (e.g.,
determining the semantic relationship between the source words as an approximation
of the meaning of the blend). However, any approach to handling blends will need
to recognize that a novel word is a blend and identify its source words. These two
tasks are the focus of this article. Specifically, we draw on linguistic knowledge of how
blends are formed as the basis for automatically determining the source words of a
blend. Language users create blends that tend to be interpretable by others. Tapping into
properties of blends believed to contribute to the recognizability of their source words?
and hence the interpretability of the resulting blend?we develop statistical measures
that indicate whether a word pair is likely the source words for a given blend. Moreover,
the fact that a novel word is determined to have a ?good? source word pair may be
evidence that it is in fact a blend, because we are unlikely to find two words that are a
?good? source word pair for a non-blend. Thus, the statistical measures we develop
for source word identification may also be useful in recognizing a novel word as a
blend.
130
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
To our knowledge, the only computational treatment of blends is our earlier work
that presents preliminary statistical methods and results for the two tasks of recognizing
an unknown word as a blend and identifying its source words (Cook and Stevenson
2007). Here we extend that work in a number of important directions. We expand the
statistical features to better capture co-occurrence patterns of the source words that
can indicate the likelihood of their combination into a blend. We present experimental
results confirming that the extended features provide a substantial improvement over
the earlier work on source word identification. We further propose a means, based on
linguistic factors of the source words, for pruning the number of word pairs that are
considered for a blend. This filtering heuristic greatly reduces the number of candidate
source words processed, while giving modest gains in performance, even though this
method excludes the correct word pair from consideration for a number of blends. We
then consider the use of a much larger lexicon of candidate source words, which could
potentially improve performance greatly as it contains the correct source word pair for
many more blends than a smaller lexicon.
We also make improvements to the earlier experimental data set and methods.
In our earlier study, we use a data set consisting of a list of blends extracted from
a dictionary. In the current work, we annotate a set of 324 blends (with their source
words) from a recent database of neologisms, to enable a more legitimate testing of
our method, on truly novel blends. Experiments on this new data set show that the
recent blends differ from established blends in terms of their statistical properties,
and emphasize the need for further resources of neologisms. We also experiment
with a machine learning approach to combine the information from the statistical
features in a more sophisticated manner than in our previous work. Finally, we per-
form more extensive experiments on distinguishing blends from other kinds of novel
words.
2. A Statistical Model of Lexical Blends
We present statistical features that are used to automatically infer the source words of
a word known to be a lexical blend, and show that the same features can be used to
distinguish blends from other types of neologisms. First, given a blend, we generate
all word pairs that could have formed the blend. This set is termed the candidate set,
and the word pairs it contains are referred to as candidate pairs (Section 2.1). Next, we
extract a number of linguistically motivated statistical features for each candidate pair,
as well as filter from the candidate sets those pairs that are unlikely to be source words
due to their linguistic properties (Section 2.2). Later, we explain how we use the features
to rank the candidate pairs according to how likely they are the source words for that
blend. Interestingly, the ?goodness? of a candidate pair is also related to how likely the
word is actually a blend.
2.1 Candidate Sets
To create the candidate set for a blend, we first generate each prefix?suffix pair such that
the blend is composed of the prefix followed by the suffix. (In this work, prefix and suffix
refer to the beginning or ending of a string, regardless of whether those portions are
affixes.) We restrict the prefixes and suffixes to be of length two or more. This heuristic
reduces the size of the candidate sets, yet generally does not exclude a blend?s source
131
Computational Linguistics Volume 36, Number 1
words from its candidate set since it is uncommon for a source word to contribute less
than two letters. For example, for brunch (breakfast+lunch) we consider the following
prefix?suffix pairs: br, unch; bru, nch; brun, ch. For each prefix?suffix pair, we then find in
a lexicon all words beginning with the prefix and all words ending in the suffix, ignoring
hyphens and whitespace, and take the Cartesian product of the prefix words and suffix
words to form a list of candidate word pairs. The candidate set for the blend is the union
of the candidate word pairs for all its prefix?suffix pairs. Note that in this example, the
candidate pair brute crunch would be included twice: once for the prefix?suffix pair br,
unch; and once again for bru, nch. Unlike in our previous study, we remove all such
duplicate pairs from the final candidate set. A candidate set for architourist, a blend of
architecture and tourist, is given in Table 1.
2.2 Statistical Features
Our statistical features are motivated by properties of blends observed in corpus-based
studies, and by cognitive factors in human interpretation of blends, particularly relating
to how easily humans can recognize a blend?s source words. All the features are formu-
lated to give higher values for more likely candidate pairs. We organize the features into
four groups?frequency; length, contribution, and phonology; semantics; and syllable
structure?and describe each feature group in the following subsections.
2.2.1 Frequency. Various frequency properties of the source words influence how easily a
language user recognizes the words that form a blend. Because blends are most usefully
coined when the source words can be readily deduced, we hypothesize that frequency-
based features will be useful in identifying blends and their source words. We propose
ten features that draw on the frequency of candidate source words.
Lehrer (2003) presents a study in which humans are asked to give the source
words for blends. Among her findings are that frequent source words are more
easily recognizable. Our first two features?the frequency of each candidate word,
freq(w1) and freq(w2)?reflect this finding. Lehrer also finds that the recognizability of
a source word is further affected by both the number of words in its neighborhood?
the set of words which begin/end with the prefix/suffix which that source word
Table 1
A candidate set for architourist, a blend of architecture and tourist.
archimandrite tourist
archipelago tourist
architect behaviourist
architect tourist
architectural behaviourist
architectural tourist
architecturally behaviourist
architecturally tourist
architecture behaviourist
architecture tourist
archives tourist
archivist tourist
132
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
contributes?and the frequencies of those words. (Gries [2006] reports a similar finding.)
Our next two features capture this insight:
freq(w1)
freq(prefix)
freq(w2)
freq(suffix)
(1)
where freq(prefix) is the sum of the frequency of all words beginning with prefix, and
similarly for freq(suffix).
These four features were used in our previous study; the following six features are
new in this study.
Because we observe that blends are often formed from two words that co-occur
in language use, our previous study (Cook and Stevenson 2007) included a feature,
the pointwise mutual information of w1 and w2, to reflect this. However, this feature
provides only a weak indication that there is a semantic relation between two words
sufficient to lead to them being blended. Here we propose six new features that capture
various co-occurrence frequencies as follows.
A blend?s source words often correspond to a common sequence of words, for
example, camouflanguage is camouflaged language. We therefore include two features
based on Dice?s co-efficient to capture the frequency with which the source words occur
consecutively:
2 ? freq(w1 w2)
freq(w1)+ freq(w2)
2 ? freq(w2 w1)
freq(w1)+ freq(w2)
(2)
Because many blends can be paraphrased by a conjunctive phrase?for example, brocco-
flower is broccoli and cauliflower?we also use a feature that reflects how often the candi-
date words are used in this way:
2 ? ( freq(w1 and w2)+ freq(w2 and w1))
freq(w1 and)+ freq(and w1)+ freq(w2 and)+ freq(and w2)
(3)
Furthermore, some blends can be paraphrased by a noun modified by a prepositional
phrase, for example, a nicotini is a martini with nicotine. Lauer (1995) suggests eight
prepositional paraphrases for identifying the semantic relationship between the modi-
fier and head in a noun compound. Using the same paraphrases, the following feature
measures how often two candidate source words occur with any of the following
prepositions P between them: about, at, for, from, in, of, on, with:
2 ? ( freq(w1 P w2)+ freq(w2 P w1))
freq(w1 P)+ freq(P w1)+ freq(w2 P)+ freq(P w2)
(4)
where freq(w P v) is the sum of the frequency of w and v occurring with each of the eight
prepositions between w and v, and freq(w P) is the sum of the frequency of w occurring
with each of the eight prepositions immediately following w.
133
Computational Linguistics Volume 36, Number 1
Because the previous three features target the source words occurring in very spe-
cific patterns, we also count the candidate source words occurring in any of the patterns
in an effort to avoid data sparseness problems.
2 ? ( freq(w1 w2)+ freq(w2 w1)+ freq(w1 and w2)
+freq(w2 and w1)+ freq(w1 P w2)+ freq(w2 P w1))
freq(w1)+ freq(w2)
(5)
Finally, because the above patterns are very specific, and do not capture general co-
occurrence information which may also be useful in identifying a blend?s source words,
we include the following feature which counts the candidate source words co-occurring
within a five-word window.
2 ? freq(w1,w2 in a 5 word window)
freq(w1)+ freq(w2)
(6)
2.2.2 Length, Contribution, and Phonology. Ten features tap into properties of the ortho-
graphic or phonetic composition of the source words and blend. In our previous work
on blends, we found such features unhelpful in source word identification. Here, we
propose revised versions of our old features, and a few new ones. Note that although we
use information about the phonological and/or syllabic structure of the source words,
we do not assume such knowledge for the blend itself, since it is a neologism for which
such lexical information is typically unavailable.
The first word in a conjunct tends to be shorter than the second, and this also seems
to be the case for the source words in blends (Kelly 1998; Gries 2004). The first three
features therefore capture this tendency based on the graphemic, phonemic, and syllabic
length of w2 relative to w1, respectively:
lengraphemes(w2)
lengraphemes(w1)+ lengraphemes(w2)
(7)
lenphonemes(w2)
lenphonemes(w1)+ lenphonemes(w2)
(8)
lensyllables(w2)
lensyllables(w1)+ lensyllables(w2)
(9)
A blend and its second source word also tend to be similar in length, possibly because,
similar to compounds, the second source word of a blend is often the head; therefore
it is this word that determines the overall phonological structure of the resulting blend
(Kubozono 1990). The following feature captures this property using graphemic length
134
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
as an approximation to phonemic length, because as stated previously, we assume no
phonological information about the blend.
1 ?
|lengraphemes(blend) ? lengraphemes(w2)|
max(lengraphemes(blend), lengraphemes(w2))
(10)
We hypothesize that a candidate source word is more likely if it contributes more
graphemes to a blend. We use two ways to measure contribution in terms of graph-
emes: contseq(w, b) is the length of the longest prefix/suffix of word w which blend b
begins/ends with, and contlcs(w, b) is the longest common subsequence (LCS) of w and
b. This yields four features:
contseq(w1, b)
lengraphemes(w1)
contseq(w2, b)
lengraphemes(w2)
(11)
contlcs(w1, b)
lengraphemes(w1)
contlcs(w2, b)
lengraphemes(w2)
(12)
Note that for some blends, such as spamdex (spam index), contseq and contlcs will be equal;
however, this is not the case in general, as in the blend tomacco (tomato and tobacco) in
which tomato overlaps with the blend not only in its prefix toma, but also in the final o.
In order to be recognizable in a blend, the shorter source word will tend to con-
tribute more material, relative to its length, than the longer source word (Gries 2004).
We formulate the following feature which is positive only when this is the case:
(
contseq(w1, b)
lengraphemes(w1)
?
contseq(w2, b)
lengraphemes(w2)
)
?
(
lengraphemes(w2) ? lengraphemes(w1)
lengraphemes(w1)+ lengraphemes(w2)
)
(13)
For this feature we don?t have strong motivation for choosing one measure of contribu-
tion over the other, and therefore use contseq, the simpler version of contribution.
Finally, the source words in a blend are often phonologically similar, as in sheeple
(sheep people); the following feature captures this (Gries 2006):
LCSphonemes(w1,w2) (14)
2.2.3 Semantics. We include two semantic features from our previous study that are
based on Lehrer?s (2003) observation that people can more easily identify the source
words of a blend when there is a semantic relation between them.
As noted, blends are often composed of two semantically similar words, reflecting a
conjunction of their concepts. For example, a pug and a beagle are both a kind of dog, and
can be combined to form the blend puggle. Similarly an exergame is a blend of exercise and
game, both of which are types of activity. Our first semantic feature captures similarity
using an ontological similarity measure, which is calculated over an ontology populated
with word frequencies from a corpus.
The source words of some blends are not semantically similar (in the sense of their
relative positions within an ontology), but are semantically related. For example, the
source words of slanguist?slang and linguist?are related in that slang is a type of lan-
guage and a linguist studies language. Our second semantic feature is a measure of se-
mantic relatedness using distributional similarity between word co-occurrence vectors.
135
Computational Linguistics Volume 36, Number 1
2.2.4 Syllable Structure. Kubozono (1990) notes that the split of a source word?into the
prefix/suffix it contributes to the blend and the remainder of the word?occurs at a
syllable boundary or immediately after the onset of the syllable. Because this syllable
structure property holds sufficiently often, we use it as a filter over candidate pairs
(rather than as an additional statistical feature) in an effort to reduce the size of the
candidate sets. Candidate sets can be very large, and we expect that our features will
be more successful at selecting the correct source word pair from a smaller candidate
set. In our subsequent results, we analyze the reduction in candidate set size using this
syllable structure heuristic, and its impact on performance.
3. Creating a Data Set of Recent Blends
The data set used in our previous work on blends contains dictionary words whose
etymological entry indicates they were formed from a blend of two words. Using a
dictionary in this way provides an objective method for selecting experimental expres-
sions and indicating their gold standard source words. However, it results in a data set
of blends that are sufficiently established in the language to appear in a dictionary. Truly
novel blends?neologisms which have been recently added to the language?may have
differing properties from fully established forms in a dictionary. In particular, many
of our features are based on properties of the source words, both individually and in
relation to each other, that may not hold for expressions that entered the language some
time ago. For example, although meld is a blend of melt and weld, the current frequency
of the phrase melt and weld may not be as common as the source word co-occurrences
for newly coined expressions. Thus, an important step to support further research
on blends is to develop a data set of recent neologisms that are judged to be lexical
blends.
To develop a data set of recently coined blends we drew on www.wordspy.com, a
popular Web site documenting English neologisms (and a small number of rare or
specialized terms) that have been recently used in a recordable medium such as a
newspaper or book, and that (typically) are not found in currently available dictionaries.
A (partial) sample entry from Wordspy is given in Table 2. The words on this Web
site satisfy our goal of being new; however, they include many kinds of neologisms,
not just blends. We thus annotated the data set to identify the blends and their source
Table 2
The Wordspy definition, and first citation given, for the blend staycation.
staycation n. A stay-at-home vacation. Also: stay-cation.
?staycationer n.
Example Citation:
Amy and Adam Geurden of Hollandtown, Wis., had planned a long summer of short,
fun getaways with their kids, Eric, 6, Holly, 3, and Jake, 2. In the works were water-park
visits, roller-coaster rides, hiking adventures and a whirlwind weekend in Chicago.
Then Amy did the math: their Chevy Suburban gets 17 miles to the gallon and, with gas
prices topping $4, the family would have spent about $320 on fill-ups alone. They?ve
since scrapped their plans in favor of a ?staycation? around the backyard swimming
pool.
?Linda Stern, ?Try Freeloading Off Friends!,? Newsweek, May 26, 2008
136
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
words. (In cases where multiple source words were found to be equally acceptable, all
source words judged to be valid were included in the annotation.) Most expressions
in Wordspy include both a definition and an example usage, making the task fairly
straightforward.
As of 17 July 2008 Wordspy contained 1,186 single word entries. One author anno-
tated each of these words as a blend or not a blend, and indicated the source words
for each blend. To ensure validity of the annotation task, the other author similarly
annotated 100 words randomly sampled from the 1,186. On this subset of 100 words, ob-
served agreement on both the blend/non-blend annotation and the component source
word identification was 92%, with an unweighted Kappa score of .84. On four blends,
the judges gave different variants of the same source word; for example, fuzzy buzzword
and fuzz buzzword for the blend fuzzword. These items were counted as agreements, and
all variants were considered correct source words.
Given the high level of agreement between the annotators, only one person anno-
tated all 1,186 items. A total of 515 words were judged to be blends, with 351 being
simple two-word sequential blends whose source words are not proper nouns (this
latter type of blend being the focus of this study). Table 3 shows the variety of blends
encountered in the Wordspy data, organized according to a categorization scheme we
devised. Of the simple two-word sequential blends, we restrict our experimental data
set to the 324 items whose entries included a citation of their usage, as we have evidence
that they have in fact been used; moreover, such blends may be less likely to be nonce
formations?expressions which are used once but do not become part of the language.
The usage data in the citations can also be used in the future for semantic features based
on contextual information. We refer to this new data set of 324 items as WORDSPLEND
(a blend of Wordspy and blend).
4. Materials and Methods
4.1 Experimental Expressions
The data set used in our previous study of blends consisted of expressions from the
Macquarie Dictionary (Delbridge 1981) with an etymology entry indicating that they
are blends. All of our statistical features were devised using the development portion of
this data set, enabling us to use the full WORDSPLEND data set for testing. To compare
our results to those in our earlier study, we also perform experiments on a subset of
the previous data set. We are uncertain as to whether a number of the blends from the
Macquarie Dictionary are in fact blends. For example, it does not match our intuition
that clash is a blend of clap and dash. We created a second data set of confirmed blends,
MAC-CONF, consisting of only those blends from Macquarie that are found in at least
one of two additional dictionaries with an etymology entry indicating that they are
blends. We report results on the 30 expressions in the unseen test portion of MAC-CONF.
4.2 Experimental Resources
We generate candidate sets using two different lexicons: the CELEX lexicon (Baayen,
Piepenbrock, and Gulikers 1995),1 and a wordlist created from the Web 1T 5-gram
1 From CELEX, we use lemmas as potential source words, as it is uncommon for a source word to be an
inflected form?there are no such examples in our development data.
137
Computational Linguistics Volume 36, Number 1
Table 3
Types of blends and their frequency in Wordspy data.
Blend type Freq. Example
Simple two-word sequential blends 351 digifeiter
(digital counterfeiter)
Proper nouns 50 Japanimation
( Japanese animation)
Affixes 61 prevenge
(pre-revenge)
Common one-letter prefix 10 e-business
(electronic business)
Non-source word material 7 aireoke
(air guitar karaoke)
w2 contributes a prefix 10 theocon
(theological conservative)
Foreign word 4 sousveillance
(French sous, meaning under, and English
surveillance)
Non-sequential blends 6 entertoyment
(entertainment blended with toy)
w1 contributes a suffix 5 caponomics
(salary cap economics)
Multiple source words 6 MoSoSo
(mobile social software)
Other 5 CUV
(car blended with initialism SUV)
Corpus (Brants and Franz 2006). These are discussed further herein. The frequency
information needed to calculate the frequency features is extracted from the Web 1T 5-
gram Corpus. The length, contribution, and phonology features, as well as the syllable
structure filter, are calculated on the basis of the source words themselves, or are derived
from information in CELEX (when CELEX is the lexicon in use).2 We compute semantic
similarity between the source words using Jiang and Conrath?s (1997) measure in the
WordNet::Similarity package (Pedersen, Patwardhan, and Michelizzi 2004), and we
compute semantic relatedness of the pair using the cosine between word co-occurrence
vectors using software provided by Mohammad and Hirst (2006).
We conduct separate experiments with the two different lexicons for candidate set
creation. We began by using CELEX, because it contains rich phonological information
that some of our features draw on. However, in our analysis of the results, we noted
that for many expressions the correct candidate pair is not in the candidate set. Many
of the blends in WORDSPLEND are formed from words which are themselves new
words, often coined for concepts related to the Internet, such as download, for example;
such words are not listed in CELEX. This motivated us to create a lexicon from a
2 Note that it would be possible to automatically infer the phonological and syllabic information required
for our features using automatic approaches for text-to-phoneme conversion and syllabification (Bartlett,
Kondrak, and Cherry 2008, for example). Although such techniques currently provide noisy information,
phonological and syllabic information for the blend itself could also be inferred, allowing the
development of features that exploit this information. We leave exploring such possibilities for future
work.
138
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
recent data set (the Web 1T 5-gram Corpus) that would be expected to contain many
of these new coinages. To form a lexicon from this corpus, we extract the 100K most
frequent words, restricted to lowercase and all-alphabetic forms. Using this lexicon we
expect the correct source word pair to be in the candidate set for more expressions.
However, this comes at the expense of potentially larger candidate sets, due to the
larger lexicon size. Furthermore, since this lexicon does not contain phonological or
syllabic representations of each word, we cannot extract three features: the feature for
the syllable heuristic, and the two features that capture the tendency for the second
source word to be longer than the first in terms of phonemes and syllables. (We do cal-
culate the phonological similarity between the two candidate source words, in terms of
graphemes.)
4.3 Experimental Methods
Because each of our features is designed to have a high value for a correct source
word pair and a low value otherwise, we can simply sum the features for each candi-
date pair to get a score for each pair indicating its degree of goodness as a source word
pair for the blend under consideration. However, because our various features have
values falling on differing ranges, we first normalize the feature values by subtracting
the mean of that feature within that candidate set and dividing by the corresponding
standard deviation. We also take the arctan of each resulting feature value to reduce the
influence of outliers. We then sum the feature values for each candidate pair, and order
the pairs within each candidate set according to this sum. This ranks the pairs in terms
of decreasing degree of goodness as a source word pair. We refer to this method as the
feature ranking approach.
We also use a machine learning approach applied to the features in a training
regimen. Our task can be viewed as a classification problem in which each candidate
pair is either a positive instance (the correct source word pair) or a negative instance
(an incorrect source word pair). However, a standard machine learning algorithm does
not directly apply because of the structure of the problem space. In classification,
we typically look for a hyperplane that separates the positive and negative training
examples. In the context of our problem, this corresponds to separating all the correct
candidate pairs (for all blends in our data set) from all the incorrect candidate pairs.
However, such an approach is undesirable as it ignores the structure of the candidate
sets; it is only necessary to separate the correct source word pair for a given blend from
the corresponding incorrect candidate pairs (i.e., for the same blend). This is also in
line with the formulation of our features, which are designed to give relatively higher
values to correct candidate pairs than incorrect candidate pairs within the candidate set
for a given blend; it is not necessarily the case that the feature values for the correct
candidate pair for a given blend will be higher than those for an incorrect candidate
pair for another blend. In other words, the features are designed to give values that are
relative to the candidates for a particular blend.
To address this issue, we use a version of the perceptron algorithm similar to
that proposed by Shen and Joshi (2005). In this approach, the classifier is trained
by only adjusting the perceptron weight vector when the correct candidate pair is
not scored higher than the incorrect pairs for the target blend (not across all the can-
didate pairs for all blends). Furthermore, to accommodate for the large variation in
candidate set size we use an uneven margin?in this case the distance between the
weighted sum of the feature vector for a correct and incorrect candidate pair?of
139
Computational Linguistics Volume 36, Number 1
1
#correct cand. pairs ? #incorrect cand. pairs . We therefore learn a single weight vector such that,
within each candidate set, the correct candidate pairs are scored higher than the in-
correct candidate pairs by a factor of this margin. When updating the weight vector,
we multiply the update that we add to the weight vector by a factor of this margin
to prevent the classifier from being overly influenced by large candidate sets. During
testing, each candidate pair is ranked according to the weighted sum of its feature
vector. To evaluate this approach, on each of WORDSPLEND and MAC-CONF we per-
form 10-fold cross-validation with 10 random restarts. In these experiments, we use our
syllable heuristic as a feature, rather than as a filter, to allow the learner to weight it
appropriately.
4.4 Evaluation Metrics
We evaluate our methods according to two measures: accuracy and mean reciprocal
rank (MRR). Under the accuracy measure, the system is scored as correct if it ranks one
of the correct source word pairs for a given blend first, and as incorrect otherwise. The
MRR gives the mean of the rank of the highest ranked correct source word pair for each
blend. Although accuracy is more stringent than MRR, we are interested in MRR to see
where the system ranks the correct source word pair in the case that it is not ranked
first. We compare the accuracy of our system against a chance (random) baseline, and
an informed baseline in which the feature ranking approach is applied using just two of
our features, the frequency of each candidate source word.
5. Experimental Results
5.1 Candidate Sets
We begin by examining some properties of the candidate sets created using CELEX as
the lexicon, also referred to as the CELEX candidate set, in rows 2?4 of Table 4. First,
in the second row of this table, we observe that only 78?83% of expressions have both
source words in CELEX. For the other 17?22% of expressions, our system is always
incorrect, because the CELEX candidate set cannot contain the correct source words.
Table 4
Percent of expressions (% exps) with their source words in each lexical resource and candidate
set (CS), and after applying the syllable heuristic filter on the CELEX CS, as well as median CS
size, for both the WORDSPLEND and MAC-CONF data sets.
Lexical resource or CS WORDSPLEND MAC-CONF
% exps Med. CS size % exps Med. CS size
CELEX 78 - 83 -
CELEX CS 76 117 83 121
CELEX CS after syllable filter 71 71 77 92
Web 1T lexicon 92 - - -
Web 1T CS 89 442 - -
140
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
The percentages reported in this row thus serve as an upper bound on the task for each
data set.
The third row of Table 4 shows the percentage of expressions for which the CELEX
candidate set contains the correct source words. Note that in most cases, if the source
words are in CELEX, they are also in the CELEX candidate set. The only expressions
in WORDSPLEND for which that is not the case are those in which a source word
contributes a single letter to the blend. We could remove our restriction that each source
word contribute at least two letters; however, this would cause the candidate sets to be
much larger and likely reduce accuracy.
We now look at the effect of filtering the CELEX candidate sets to include only those
candidate pairs that are valid according to our syllable heuristic. This process results in
a 24?39% reduction in median candidate set size, but only excludes the source words
from the candidate set for a relatively small number of expressions (5?6%), as shown in
the fourth row of Table 4. We will further examine the effectiveness of this heuristic in
the following subsection.
Now we examine the candidate sets created using the lexicon derived from the
Web 1T 5-gram Corpus.3 In the final two rows of Table 4 we see that, as expected, many
more expressions have their source words in the Web 1T lexicon than in CELEX, and
furthermore, more expressions have their source words in the candidate sets created
using the Web 1T lexicon than in the candidate sets formed from CELEX. This means
that the upper bound for our task is much higher when using the Web 1T lexicon than
when using CELEX. However, this comes at the cost of creating much larger candidate
sets; we examine this trade-off more thoroughly herein.
5.2 Source Word Identification
In the following subsections we present results using the feature ranking approach
(Section 5.2.1), and analyze some of the errors the system makes in these experiments
(Section 5.2.2). We then consider results using the modified perceptron algorithm (Sec-
tion 5.2.3), and finally we compare our results to our previous study and human
performance (Section 5.2.4).
5.2.1 Feature Ranking. Table 5 gives the accuracy using the feature ranking approach for
both the random and informed baselines, each feature group, and the combination of
all features, on each data set, using both the CELEX and Web 1T lexicons in the case of
WORDSPLEND. Feature groups and combinations marked with an asterisk are signifi-
cantly better than the informed baseline at the 0.05 confidence level using McNemar?s
Test.4
We first note that the informed baseline is an improvement over the random base-
line in all cases, which points to the importance of word frequency in blend formation.
We also see that the informed baseline is quite a bit higher on WORDSPLEND than MAC-
CONF. Inspection of candidate sets?created from the CELEX lexicon?that include the
correct source words reveals that the average source word frequency for WORDSPLEND
3 Syllable structure information is not available for all words in the Web 1T lexicon; therefore, we do not
apply the syllable heuristic filter to the pairs in these candidate sets (see Section 4.2). We do not create
candidate sets for MAC-CONF using the Web 1T lexicon since this lexicon was constructed specifically in
response to the kinds of new words found in WORDSPLEND.
4 McNemar?s Test is a non-parametric test that can be applied to correlated, nominal data.
141
Computational Linguistics Volume 36, Number 1
Table 5
Percent accuracy on blends in WORDSPLEND and MAC-CONF using the feature ranking
approach. The size of each data set is given in parentheses. The lexicon employed (CELEX or
WEB 1T) is indicated. The best accuracy obtained using this approach for each data set and
lexicon is shown in boldface. * = results that are significantly better than the informed baseline.
Features WORDSPLEND MAC-CONF
(324) (30)
CELEX WEB 1T CELEX
Random Baseline 6 3 1
Informed Baseline 27 27 7
Frequency 32* 32* 30*
Len./Cont./Phono. 20 20 7
Semantic 15 13 20
All 38* 42* 37*
All+Syllable 40* - 37*
is much higher than for MAC-CONF (118M vs. 34M). On the other hand, the average for
non-source words in the candidate sets is similar across these data sets (11M vs. 9M).
Thus, although source words are more frequent than non-source words for both data
sets, frequency is a much more reliable indicator of being a source word for truly novel
blends than for established blends. This finding emphasizes the need for a data set such
as WORDSPLEND to evaluate methods for processing neologisms.
All of the individual feature groups outperform the random baseline. We also see
that our frequency features are better than the informed baseline. Although source
word frequency (the informed baseline) clearly plays an important role in forming inter-
pretable blends, this finding confirms that additional aspects of source word frequency
beyond their unigram counts also play an important role in blend formation. Also note
that the semantic features are substantially better than the informed baseline?although
not significantly so?on MAC-CONF, but not on WORDSPLEND. This result demon-
strates the importance of testing on true neologisms to have an accurate assessment
of a method. It also supports our future plan to explore alternative semantic features,
such as those that draw on the context of usage of a blend (as provided in our new
data set).
We expect using all the features to provide an improvement in performance over
any individual feature group, because they tap into very different types of information
about blends. Indeed, the combination of all features (All) does perform better than
the frequency features, supporting our hypothesis that the information provided by the
different feature groups is complementary.5
Looking at the results on WORDSPLEND using the Web 1T lexicon, we see that as
expected, due to the larger candidate sets, the random baseline is lower than when
using the CELEX lexicon. However, the informed baseline, and each feature group
used on its own, give very similar results, with only a small difference observed for
the semantic features. The combination of all features gives slightly higher performance
5 This difference is significant (p < 0.01) according to McNemar?s test for the WORDSPLEND data set using
both the CELEX and Web 1T lexicons. The difference was not significant for MAC-CONF.
142
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
using the Web 1T lexicon than the CELEX lexicon, although again this difference is
rather small.
Recall that we wanted to see if the use of our syllable heuristic filter to reduce
candidate set size would have a negative impact on performance. Table 5 shows that
the accuracy on all features when we apply our syllable heuristic filter (All+Syllable) is
at least as good as when we do not apply the filter (All). This is the case even though
the syllable heuristic filter removes the correct source word pairs for 5?6% of the blends
(see Table 4). It seems that the words this heuristic excludes from consideration are
not those that the features rank highly, indicating that it is a reasonable method for
pruning candidate sets. Moreover, reducing candidate set size will enable future work
to explore features that are more expensive to extract than those currently used. Given
the promising results using the Web1T lexicon, we also intend to examine ways to
automatically estimate the syllable filtering heuristic for words for which we do not
have syllable structure information.
5.2.2 Error Analysis. We now examine some cases where the system ranks an incorrect
candidate pair first, to try to determine why the system makes the errors it does. We
focus on the expressions in WORDSPLEND using the CELEX lexicon, as we are able to
extract all of our features for this experimental setup. First, we observe that when con-
sidering feature groups individually, the frequency features perform best; however, in
many cases, they also contribute to errors. This seems to be primarily due to (incorrect)
candidate pairs that occur very frequently together. For example, in the case of mathlete
(math athlete), the candidate pair male and athlete co-occurs much more frequently than
the correct source word pair, causing the system to rank the incorrect source word
pair first. We observe a similar situation for cutensil (cute utensil), where the candidate
pair cup and utensil often co-occur. In both these cases, phonological information for
the blend itself could help as, for example, cute ([kjut]) contributes more phonemes to
cutensil ([kjutEnsl
"
]) than cup ([k2p]).
Turning to the length, contribution, and phonology features, we see that although
many blends exhibit the properties on which these features are based, there are also
many blends which do not. For example, our first feature in this group captures the
property that the second source word tends to be longer than the first; however, this
is not the case for some blends, such as testilie (testify and lie). Furthermore, even for
blends for which the second source word is longer than the first, there may exist a
candidate pair that has a higher value for this feature than the correct source word
pair. In the case of banalysis?banal analysis?banal electrolysis is a better source word
pair according to this feature. These observations, and similar issues with other length,
contribution, and phonology features, likely contribute to the poor performance of
this feature group. Moreover, such findings motivate approaches such as our modified
perceptron algorithm?discussed in the following subsection?that learn a weighting
for the features.
Finally, for the semantic features, we find cases where a blend?s source words are
similar and related, but there is another (incorrect) candidate pair which is more similar
and related according to these features. For example, puggle, a blend of pug and beagle,
has the candidate source words push and struggle which are more semantically similar
and related than the correct source word pair. In this case, the part-of-speech of the
candidate source words, along with contextual knowledge indicating the part-of-speech
of the blend, may be useful; blending pug and beagle would result in a noun, while a
blend of push and struggle would likely be a verb. Another example is camikini, a blend
143
Computational Linguistics Volume 36, Number 1
of camisole and bikini. Both of these source words are women?s garments, so we would
expect them to have a moderately high similarity. However, the semantic similarity
feature assigns this candidate pair the lowest possible score, since these words do not
occur in the corpus from which this feature is estimated.
5.2.3 Modified Perceptron. Table 6 shows the average accuracy of the modified perceptron
algorithm for the informed baseline and the combination of all features plus the feature
corresponding to the syllable heuristic, on each data set, using both the CELEX and Web
1T lexicons in the case of WORDSPLEND. We don?t compare this method directly against
the results using the feature ranking approach because our perceptron experiments are
conducted using cross-validation, rather than a held-out test set methodology. Examin-
ing the results using the combination of All+Syllable, we see that for each data set and
lexicon the mean accuracy over the 10-fold cross-validation is significantly higher than
that obtained using the informed baseline, according to an unpaired t-test (p < 0.0001
in each case).
Interestingly, on WORDSPLEND using the combination of all features, we see higher
performance using the CELEX lexicon than the Web 1T lexicon. We hypothesize that
this is due to the training data in the latter case containing many more negative ex-
amples (incorrect candidate pairs?due to the larger candidate sets). It is worth noting
that, despite the differing experimental methodologies, the results are in fact not very
different from those obtained in the feature ranking approach. One limitation of this
perceptron algorithm is that it assumes that the training data is linearly separable.
In future work, we will try other machine learning techniques that do not make this
assumption.
5.2.4 Discussion. We now compare the feature ranking results on MAC-CONF here of
37% accuracy, to our previous best results on this data set of 27% accuracy, also using
feature ranking (Cook and Stevenson 2007). To make this comparison, we should con-
sider the differing baselines and upper bounds across the experiments. The informed
baseline in our previous study on MAC-CONF is 13%, substantially higher than the
7% in the current study. Recall that the first row of Table 4 shows the upper bound
using the CELEX lexicon on this data set to be 83%. By contrast, in our previous
work we only use blends whose source words appear in the lexicon we used there
(Macquarie), so the upper bound for that study is 100%. Taking these factors into
account, the best results in our previous study correspond to a reduction in error rate
(RER) over the informed baseline of 16%, while the feature ranking method here using
Table 6
Percent accuracy on blends in WORDSPLEND and MAC-CONF using the modified perceptron
algorithm. The size of each data set is given in parentheses. The lexicon employed (CELEX or
WEB 1T) is indicated. * = results that are significantly better than the informed baseline.
Features WORDSPLEND MAC-CONF
(324) (30)
CELEX WEB 1T CELEX
Informed Baseline 23 24 7
All+Syllable 40* 37* 35*
144
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
the combination of all features and the syllable heuristic filter achieves a much higher
RER of 39%.6
Lehrer (2003) finds human performance for determining the source words of blends
to be 34% to 79%?depending on the blends considered?which indicates the difficulty
of this task.7 Our best accuracy on each data set of 37%?42% is quite respectable
in comparison. These accuracies correspond to mean reciprocal ranks of 0.47?0.51,
and the random baseline on WORDSPLEND and MAC-CONF in terms of this mea-
sure is 0.03?0.07. This indicates that even when our system is incorrect, the correct
source word pair is still ranked fairly high. Such information about the best inter-
pretations of a blend could be useful in semi-automated methods, such as computer-
aided translation, where a human may not be familiar with a novel blend in the source
text.
6. Blend Identification
The statistical features we have developed may also be informative about whether
or not a word is in fact a blend?that is, we expect that if a novel word has ?good?
candidate source words, then the word is more likely to be a blend than the result of
another word formation process. Because our features are designed to be high for a
blend?s source words and low for other word pairs, we hypothesize that the highest
scoring candidate pairs for blends will be higher than those of non-blends.
To test this hypothesis, we first create a data set of non-blends from our earlier
annotation, which found 671 non-blends out of the 1,186 Wordspy expressions (see
Section 3). From these words, we eliminate all those beginning with a capital letter
(to exclude words formed from proper nouns) or containing a non-letter character (to
exclude acronyms and initialisms). This results in 663 non-blends.
We create candidate sets for the non-blends using the CELEX lexicon. Using the
CELEX lexicon allows us to extract?and consider the contribution of?all of our length,
contribution, and phonology features, some of which are not available when using the
Web 1T lexicon. The candidate sets resulting from using the CELEX lexicon were also
much smaller than when using the Web 1T lexicon. We calculate the features for the non-
blends as we did for the blends, and then order all expressions (both blends and non-
blends) according to the sum of the features for their highest-scoring candidate source
word pair. We use the same feature groups and combinations presented in Table 5.
Rather than set an arbitrary cut-off to distinguish blends from non-blends, we instead
give receiver operating characteristic (ROC) curves for some of these experiments.
ROC curves plot true positive rate versus false positive rate as the cut-off is varied
(see Figure 1). The top-left corner represents perfect classification, with points further
towards the top-left from the diagonal (a random classifier) being ?better.? We see that
the informed baseline is a substantial improvement over a random classifier, and the
combination All+Syllable is a further improvement over the informed baseline. The in-
dividual feature groups (not shown in Figure 1) do not perform as well as All+Syllable.
6 Reduction in error rate =
accuracy?baseline
upper bound?baseline .
7 Note that the high level of interannotator agreement achieved in our annotation task (Section 3) may
seem surprising in the context of Lehrer?s results. However, our task is much easier, because our
annotators were given a definition of the blend, whereas Lehrer?s subjects were not.
145
Computational Linguistics Volume 36, Number 1
Figure 1
ROC curves for blend identification.
In future work, we plan to re-examine this task and develop methods specifically for
identifying blends and other types of neologism.
7. Related Work
As discussed in Section 1, techniques generally used in the automatic acquisition of
syntactic and semantic properties of words are not applicable here, because they use
corpus statistics that cannot be accurately estimated for low frequency items, such as
the novel lexical blends considered in this study (Hindle 1990; Lapata and Brew 2004;
Joanis, Stevenson, and James 2008, for example). Other work has used the context
in which an unknown word occurs, along with domain-specific knowledge, to infer
aspects of its meaning and syntax (Granger 1977; Cardie 1993; Hastings and Lytinen
1994, for example). These studies have been able to learn properties of an unknown
word from just one usage, or a small number of usages; however, the domain-specific
resources that these studies rely on limit their applicability to general text.
Techniques for inferring lexical properties of neologisms can make use of infor-
mation that is typically not available in other lexical acquisition tasks?specifically,
knowledge of the processes through which neologisms are formed. Computational
work on neologisms has tended to focus on tasks pertaining to a specific type of
neologism, such as identifying and inferring the long form of acronyms (Schwartz and
Hearst 2003; Nadeau and Turney 2005; Okazaki and Ananiadou 2006, for example),
recognizing loanwords (Baker and Brew 2008; Alex 2008, for example), and identifying
and expanding clippings (Means 1988, for example). This study focuses on the tasks
of identifying, and inferring the source words of, lexical blends, a common type of
neologism, which have been previously unaddressed except for our preliminary work
in Cook and Stevenson (2007).
In addition to knowledge about a word?s formation process, for many types of
neologism, information about its phonological and orthographic content can be used to
146
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
infer aspects of its syntactic and semantic properties. This is the case for neologisms that
are composed of existing words or affixes (e.g., compounds and derivations) or partial
orthographic or phonological material from existing words or affixes (e.g., acronyms,
clippings, and blends). For example, in the case of part-of-speech tagging, information
about the suffix of an unknown word can be used to determine its part-of-speech
(Brill 1994; Ratnaparkhi 1996; Mikheev 1997, for example). For the task of inferring the
long form of an acronym, the letters which compose a given acronym can be used to
determine the most likely long form (Schwartz and Hearst 2003; Nadeau and Turney
2005; Okazaki and Ananiadou 2006, for example).
The latter approach to acronyms is somewhat similar to the way in which we use
knowledge of the letters that make up a blend to form candidate sets and determine
the most likely source words. However, in the case of acronyms, each word in a long
form typically contributes only one letter to the acronym, while for blends, a source
word usually contributes more than one letter. At first glance, it may appear that this
makes the task of source word identification easier for blends, since there is more source
word material available to work with. However, acronyms have two properties that
help in their identification. First, there is less uncertainty in the ?split? of an acronym,
because each letter is usually contributed by a separate word. By contrast, due to the
large variation in the amount of material contributed by the source words in blends,
one of the challenges in blend identification is to determine which material in the blend
belongs to each source word. Second, and more importantly, acronyms are typically
introduced in regular patterns (e.g., the long form followed by the acronym capitalized
and in parentheses) which can be exploited in acronym identification and long form
inference; in the case of blends there is no counterpart for this information.
8. Conclusions
We propose a statistical model for inferring the source words of lexical blends?a very
frequent class of new words?based largely on properties related to the recognizability
of their source words. We also introduce a method based on syllable structure for re-
ducing the number of words that are considered as possible source words. We evaluate
our methods on two data sets, one consisting of novel blends, the other containing
established blends; in both cases our features significantly outperform an informed
baseline. Moreover, the results in this study are substantially better than those reported
previously (Cook and Stevenson 2007). We further show that our methods for source
word identification can also be used to distinguish blends from other word types. In
addition, we annotate a data set of newly coined expressions which will support future
research not only on lexical blends, but on neologisms in general.
Our future plans include expanding our techniques for identifying blends to ad-
dress the more general problem of determining the formation process of a novel word.
We further intend to apply our source word identification methods to other types of
neologisms formed from material from existing words, such as clippings (e.g., lab for
laboratory).
Acknowledgments
This article is an extended and updated
version of a paper that appeared in the
Proceedings of the Tenth Conference of the
Pacific Association for Computational
Linguistics (PACLING-2007). We thank the
anonymous reviewers of this article for their
comments, which have helped us to improve
the quality of this work. We also thank the
members of the computational linguistics
group at the University of Toronto for their
comments and feedback on our research.
This work was financially supported by the
National Sciences and Engineering Research
147
Computational Linguistics Volume 36, Number 1
Council of Canada, the Ontario Graduate
Scholarship program, and the University
of Toronto.
References
Alex, Beatrice. 2008. Comparing
corpus-based to Web-based lookup
techniques for automatic English
inclusion detection. In Proceedings of
the Sixth International Language
Resources and Evaluation Conference
(LREC?08), pages 2693?2697,
Marrakech.
Algeo, John. 1980. Where do all the new
words come from? American Speech,
55(4):264?277.
Algeo, John, editor. 1991. Fifty Years Among
the New Words. Cambridge University
Press, Cambridge.
Ayto, John, editor. 1990. The Longman Register
of New Words, volume 2. Longman,
London.
Ayto, John. 2006. Movers and Shakers: A
Chronology of Words that Shaped our Age.
Oxford University Press, Oxford.
Baayen, R. Harald, Richard Piepenbrock, and
Leon Gulikers. 1995. The CELEX Lexical
Database (release 2) [CD-ROM].
Philadelphia, PA: Linguistic Data
Consortium, University of
Pennsylvania [distributor].
Baker, Kirk and Chris Brew. 2008.
Statistical identification of English
loanwords in Korean using automatically
generated training data. In Proceedings
of the Sixth International Language
Resources and Evaluation Conference
(LREC?08), pages 1159?1163, Marrakech.
Bartlett, Susan, Grzegorz Kondrak, and
Colin Cherry. 2008. Automatic
syllabification with structured SVMs
for letter-to-phoneme conversion. In
Proceedings of the 46th Annual Meeting
of the Association for Computational
Linguistics (ACL-08): Human Language
Technologies, pages 568?576,
Columbus, OH.
Bauer, Laurie. 1983. English Word-formation.
Cambridge University Press, Cambridge.
Brants, Thorsten and Alex Franz. 2006. Web
1T 5-gram Corpus version 1.1. Linguistic
Data Consortium, Philadelphia, PA.
Brill, Eric. 1994. Some advances in
transformation-based part of speech
tagging. In Proceedings of the Twelfth
National Conference on Artificial Intelligence,
pages 722?727, Seattle, WA.
Cardie, Claire. 1993. A case-based approach
to knowledge acquisition for
domain-specific sentence analysis. In
Proceedings of the Eleventh National
Conference on Artificial Intelligence,
pages 798?803, Washington, DC.
Cook, Paul and Suzanne Stevenson. 2007.
Automagically inferring the source words
of lexical blends. In Proceedings of the Tenth
Conference of the Pacific Association for
Computational Linguistics (PACLING-2007),
pages 289?297, Melbourne.
Delbridge, Arthur, editor. 1981. The Macquarie
Dictionary. Macquarie Library, Sydney.
Granger, Richard H. 1977. FOUL-UP: A
program that figures out the meanings of
words from context. In Proceedings of the
Fifth International Joint Conference on
Artificial Intelligence, pages 172?178,
Cambridge, MA.
Gries, Stefan Th. 2004. Shouldn?t it be
breakfunch? A quantitative analysis of the
structure of blends. Linguistics,
42(3):639?667.
Gries, Stefan Th. 2006. Cognitive
determinants of subtractive
word-formation processes: A corpus-based
perspective. Cognitive Linguistics,
17(4):535?558.
Hastings, Peter M. and Steven L. Lytinen.
1994. The ups and downs of lexical
acquisition. In Proceedings of the Twelfth
National Conference on Artificial
Intelligence, pages 754?759, Seattle, WA.
Hindle, Donald. 1990. Noun classification
from predicate-argument structures. In
Proceedings of the 28th Annual Meeting
of the Association for Computational
Linguistics, pages 268?275, Pittsburgh, PA.
Jiang, Jay J. and David W. Conrath. 1997.
Semantic similarity based on corpus
statistics and lexical taxonomy. In
Proceedings of the International Conference
on Research in Computational Linguistics
(ROCLING X), pages 19?33, Taiwan.
Joanis, Eric, Suzanne Stevenson, and
David James. 2008. A general feature space
for automatic verb classification. Natural
Language Engineering, 14(3):337?367.
Kelly, Michael H. 1998. To ?brunch? or to
?brench?: Some aspects of blend
structure. Linguistics, 36(3):579?590.
Knowles, Elizabeth and Julia Elliott, editors.
1997. The Oxford Dictionary of New Words.
Oxford University Press, New York.
Kubozono, Haruo. 1990. Phonological
constraints on blending in English as a
case for phonology-morphology interface.
Yearbook of Morphology, 3:1?20.
148
Cook and Stevenson Automatically Identifying the Source Words of Lexical Blends in English
Lapata, Mirella and Chris Brew. 2004. Verb
class disambiguation using informative
priors. Computational Linguistics,
30(1):45?73.
Lauer, Mark. 1995. Designing Statistical
Language Learners: Experiments on Noun
Compounds. Ph.D. thesis, Macquarie
University, Sydney.
Lehrer, Adrienne. 2003. Understanding
trendy neologisms. Italian Journal of
Linguistics, 15(2):369?382.
Means, Linda G. 1988. Cn yur cmputr raed
ths? In Proceedings of the Second Conference
on Applied Natural Language Processing,
pages 93?100, Austin, TX.
Mikheev, Andrei. 1997. Automatic rule
induction for unknown-word guessing.
Computational Linguistics, 23(3):405?423.
Mohammad, Saif and Graeme Hirst. 2006.
Distributional measures of
concept-distance: A task-oriented
evaluation. In Proceedings of the 2006
Conference on Empirical Methods in Natural
Language Processing (EMNLP-2006),
pages 35?43, Sydney.
Nadeau, David and Peter D. Turney. 2005. A
supervised learning approach to acronym
identification. In Proceedings of the
Eighteenth Canadian Conference on Artificial
Intelligence (AI?2005), pages 319?329,
Victoria.
Okazaki, Naoaki and Sophia Ananiadou.
2006. A term recognition approach to
acronym recognition. In Proceedings of the
21st International Conference on
Computational Linguistics and the 44th
Annual Meeting of the Association for
Computational Linguistics (Coling-ACL
2006), pages 643?650, Sydney.
Pedersen, Ted, Siddharth Patwardhan,
and Jason Michelizzi. 2004.
Wordnet::Similarity?Measuring the
relatedness of concepts. In Demonstration
Papers at the Human Language Technology
Conference of the North American Chapter
of the Association for Computational
Linguistics (HLT-NAACL 2004),
pages 38?41, Boston, MA.
Plag, Ingo. 2003. Word-formation in
English. Cambridge University Press,
Cambridge.
Ratnaparkhi, Adwait. 1996. A maximum
entropy model for part-of-speech
tagging. In Proceedings of the Conference
on Empirical Methods in Natural
Language Processing, pages 133?142,
Philadelphia, PA.
Schwartz, Ariel S. and Marti A. Hearst.
2003. A simple algorithm for identifying
abbreviation definitions in biomedical
texts. In Proceedings of the Pacific
Symposium on Biocomputing (PSB 2003),
pages 451?462, Lihue, HI.
Shen, Libin and Aravind K. Joshi. 2005.
Ranking and reranking with perceptron.
Machine Learning, 60(1):73?96.
149

First Joint Conference on Lexical and Computational Semantics (*SEM), pages 85?89,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Disambiguation of Image Captions
Wesley May, Sanja Fidler, Afsaneh Fazly, Sven Dickinson, and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Ontario, Canada, M5S 3G4
{wesley,fidler,afsaneh,sven,suzanne}@cs.toronto.edu
Abstract
Given a set of images with related captions,
our goal is to show how visual features can
improve the accuracy of unsupervised word
sense disambiguation when the textual con-
text is very small, as this sort of data is com-
mon in news and social media. We extend
previous work in unsupervised text-only dis-
ambiguation with methods that integrate text
and images. We construct a corpus by using
Amazon Mechanical Turk to caption sense-
tagged images gathered from ImageNet. Us-
ing a Yarowsky-inspired algorithm, we show
that gains can be made over text-only disam-
biguation, as well as multimodal approaches
such as Latent Dirichlet Allocation.
1 Introduction
We examine the problem of performing unsuper-
vised word sense disambiguation (WSD) in situa-
tions with little text, but where additional informa-
tion is available in the form of an image. Such situ-
ations include captioned newswire photos, and pic-
tures in social media where the textual context is of-
ten no larger than a tweet.
Unsupervised WSD has been shown to work very
well when the target word is embedded in a large
We thank NSERC and U. Toronto for financial support. Fi-
dler and Dickinson were sponsored by the Army Research Lab-
oratory and this research was accomplished in part under Co-
operative Agreement Number W911NF-10-2-0060. The views
and conclusions contained in this document are those of the au-
thors and should not be interpreted as representing the official
policies, either express or implied, of the Army Research Lab-
oratory or the U.S. Government.
Figure 1: ?The crane was so massive it blocked the sun.?
Which sense of crane? With images the answer is clear.
quantity of text (Yarowsky, 1995). However, if the
only available text is ?The crane was so massive it
blocked the sun? (see Fig. 1), then text-only dis-
ambiguation becomes much more difficult; a human
could do little more than guess. But if an image is
available, the intended sense is much clearer. We
develop an unsupervised WSD algorithm based on
Yarowsky?s that uses words in a short caption along
with ?visual words? from the captioned image to
choose the best of two possible senses of an ambigu-
ous keyword describing the content of the image.
Language-vision integration is a quickly develop-
ing field, and a number of researchers have explored
the possibility of combining text and visual features
in various multimodal tasks. Leong and Mihal-
cea (2011) explored semantic relatedness between
words and images to better exploit multimodal con-
tent. Jamieson et al (2009) and Feng and Lap-
ata (2010) combined text and vision to perform ef-
fective image annotation. Barnard and colleagues
(2003; 2005) showed that supervised WSD by could
be improved with visual features. Here we show that
unsupervised WSD can similarly be improved. Lo-
eff, Alm and Forsyth (2006) and Saenko and Darrell
(2008) combined visual and textual information to
solve a related task, image sense disambiguation, in
85
an unsupervised fashion. In Loeff et al?s work, little
gain was realized when visual features were added
to a great deal of text. We show that these features
have more utility with small textual contexts, and
that, when little text is available, our method is more
suitable than Saenko and Darrell?s.
2 Our Algorithm
We model our algorithm after Yarowsky?s (1995) al-
gorithm for unsupervised WSD: Given a set of doc-
uments that contain a certain ambiguous word, the
goal is to label each instance of that word as some
particular sense. A seed set of collocations that
strongly indicate one of the senses is initially used to
label a subset of the data. Yarowsky then finds new
collocations in the labelled data that are strongly as-
sociated with one of the current labels and applies
these to unlabelled data. This process repeats iter-
atively, building a decision list of collocations that
indicate a particular sense with a certain confidence.
In our algorithm (Algorithm 1), we have a docu-
ment collection D of images relevant to an ambigu-
ous keyword k with senses s1 and s2 (though the al-
gorithm is extensible to more than two senses). Such
a collection might result from an internet image
search using an ambiguous word such as ?mouse?.
Each Di is an image?caption pair repsented as a
bag-of-words that includes both lexical words from
the caption, and ?visual words? from the image. A
visual word is simply an abstract representation that
describes a small portion of an image, such that sim-
ilar portions in other images are represented by the
same visual word (see Section 3.2 for details). Our
seed sets consist of the words in the definitions of s1
and s2 from WordNet (Fellbaum, 1998). Any docu-
ment whose caption contains more words from one
sense definition than the other is initially labelled
with that sense. We then iterate between two steps
that (i) find additional words associated with s1 or
s2 in currently labelled data, and (ii) relabel all data
using the word sense associations discovered so far.
We let V be the entire vocabulary of words across
all documents. We run experiements both with and
without visual words, but when we use visual words,
they are included in V . In the first step, we com-
pute a confidence Ci for each word Vi. This con-
fidence is a log-ratio of the probability of seeing
Vi in documents labelled as s1 as opposed to doc-
uments labelled as s2. That is, a positive Ci indi-
cates greater association with s1, and vice versa. In
the second step we find, for each document Dj , the
word Vi ? Dj with the highest magnitude of Ci. If
the magnitude of Ci is above a labelling threshold
?c, then we label this document as s1 or s2 depend-
ing on the sign of Ci. Note that all old labels are dis-
carded before this step, so labelled documents may
become unlabelled, or even differently labelled, as
the algorithm progresses.
Algorithm 1 Proposed Algorithm
D: set of documents D1 ... Dd
V : set of lexical and visual words V1 ... Vv in D
Ci: log-confidence Vi is sense 1 vs. sense 2
S1 and S2: bag of dictionary words for each sense
L1 and L2: documents labelled as sense 1 or 2
for all Di do . Initial labelling using seed set
if |Di ? S1| > |Di ? S2| then
L1 ? L1 ? {Di}
else if |Di ? S1| < |Di ? S2| then
L2 ? L2 ? {Di}
end if
end for
repeat
for all i ? 1..v do . Update word conf.
Ci ? log
(
P (Vi|L1)
P (Vi|L2)
)
end for
L1 ? ?, L2 ? ? . Update document conf.
for all Di do
. Find word with highest confidence
m? argmax
j?1..v,Vj?Di
|Cj |
if Cm > ?c then
L1 ? L1 ? {Di}
else if Cm < ??c then
L2 ? L2 ? {Di}
end if
end for
until no change to L1 or L2
3 Creation of the Dataset
We require a collection of images with associated
captions. We also require sense annotations for
the keyword for each image to use for evalua-
tion. Barnard and Johnson (2005) developed the
86
?Music is an important
means of expression for
many teens.?
?Keeping your office sup-
plies organized is easy, with
the right tools.?
?The internet has opened up
the world to people of all
nationalities.?
?When there is no cheese I
will take over the world.?
Figure 2: Example image-caption pairs from our dataset,
for ?band? (top) and ?mouse? (bottom).
ImCor dataset by associating images from the Corel
database with text from the SemCor corpus (Miller
et al, 1993). Loeff et al (2006) and Saenko and
Darrell (2008) used Yahoo!?s image search to gather
images with their associated web pages. While these
datasets contain images paired with text, the textual
contexts are much larger than typical captions.
3.1 Captioning Images
To develop a large set of sense-annotated image?
caption pairs with a focus on caption-sized text, we
turned to ImageNet (Deng et al, 2009). ImageNet is
a database of images that are each associated with
a synset from WordNet. Hundreds of images are
available for each of a number of senses of a wide
variety of common nouns. To gather captions, we
used Amazon Mechanical Turk to collect five sen-
tences for each image. We chose two word senses
for each of 20 polysemous nouns and for each sense
we collected captions for 50 representative images.
For each image we gathered five captions, for a to-
tal of 10,000 captions. As we have five captions for
each image, we split our data into five sets. Each set
has the same images, but each image is paired with
a different caption in each set.
We specified to the Turkers that the sentences
should be relevant to, but should not talk directly
about, the image, as in ?In this picture there is a
blue fish?, as such captions are very unnatural. True
captions generally offer orthogonal information that
is not readily apparent from the image. The key-
word for each image (as specified by ImageNet) was
not presented to the Turkers, so the captions do not
necessarily contain it. Knowledge of the keyword is
presumed to be available to the algorithm in the form
of an image tag, or filename, or the like. We found
that forcing a certain word to be included in the cap-
tion also led to sentences that described the picture
very directly. Sentences were required to be a least
ten words long, and have acceptable grammar and
spelling. We remove stop words from the captions
and lemmatize the remaining words. See Figure 2
for some examples.
3.2 Computing the Visual Words
We compute visual words for each image with Ima-
geNet?s feature extractor. This extractor lays down
a grid of overlapping squares onto the image and
computes a SIFT descriptor (Lowe, 2004) for each
square. Each descriptor is a vector that encodes the
edge orientation information in a given square. The
descriptors are computed at three scales: 1x, 0.5x
and 0.25x the original side lengths. These vectors
are clustered with k-means into 1000 clusters, and
the labels of these clusters (arbitrary integers from 1
to 1000) serve as our visual words.
It is common for each image to have a ?vocab-
ulary? of over 300 distinct visual words, many of
which only occur once. To denoise the visual data,
we use only those visual words which account for at
least 1% of the total visual words for that image.
4 Experiments and Results
To show that the addition of visual features improves
the accuracy of sense disambiguation for image?
caption pairs, we run our algorithm both with and
without the visual features. We also compare our re-
sults to three different baseline methods: K-means
(K-M), Latent Dirichlet Allocation (LDA) (Blei et
al., 2003), and an unsupervised WSD algorithm
(PBP) explained below. We use accuracy to measure
performance as it is commonly used by the WSD
community (See Table 1).
For K-means, we set k = 2 as we have two senses,
and represent each document with a V -dimensional
87
Table 1: Results (Average accuracy across all five sets of
data). Bold indicates best performance for that word.
Ours Ours K-M K-M LDA LDA PBP
text w/vis text w/vis text w/vis text
band .80 .82 .66 .65 .64 .56 .73
bank .77 .78 .71 .59 .52 .67 .62
bass .94 .94 .90 .88 .61 .62 .49
chip .90 .90 .73 .58 .57 .66 .75
clip .70 .79 .65 .58 .48 .53 .65
club .80 .84 .80 .81 .61 .73 .63
court .79 .79 .61 .53 .62 .82 .57
crane .62 .67 .76 .76 .52 .54 .66
game .78 .78 .60 .66 .60 .66 .70
hood .74 .73 .73 .70 .51 .45 .55
jack .76 .74 .62 .53 .58 .66 .47
key .81 .92 .79 .54 .57 .70 .50
mold .67 .68 .59 .67 .57 .66 .54
mouse .84 .84 .71 .62 .62 .69 .68
plant .54 .54 .56 .53 .52 .50 .72
press .60 .59 .60 .54 .58 .62 .48
seal .70 .80 .61 .67 .55 .53 .62
speaker .70 .69 .57 .53 .55 .62 .63
squash .89 .95 .84 .92 .55 .67 .79
track .78 .85 .71 .66 .51 .54 .69
avg. .76 .78 .69 .65 .56 .63 .62
vector, where the ith element is the proportion of
word Vi in the document. We run K-means both with
and without visual features.
For LDA, we use the dictionary sense model from
Saenko and Darrell (2008). A topic model is learned
where the relatedness of a topic to a sense is based
on the probabilities of that topic generating the seed
words from its dictionary definitions. Analogously
to k-means, we learn a model for text alone, and a
model for text augmented with visual information.
For unsupervised WSD (applied to text only),
we use WordNet::SenseRelate::TargetWord, here-
after PBP (Patwardhan et al, 2007), the highest
scoring unsupervised lexical sample word sense dis-
ambiguation algorithm at SemEval07 (Pradhan et
al., 2007). PBP treats the nearby words around the
target word as a bag, and uses the WordNet hierar-
chy to assign a similarity score between the possible
senses of words in the context, and possible senses
of the target word. As our captions are fairly short,
we use the entire caption as context.
The most important result is the gain in accuracy
after adding visual features. While the average gain
across all words is slight, it is significant at p < 0.02
(using a paired t-test). For 12 of the 20 words, the
visual features improve performance, and in 6 of
those, the improvement is 5?11%.
For some words there is no significant improve-
ment in accuracy, or even a slight decrease. With
words like ?bass? or ?chip? there is little room to
improve upon the text-only result. For words like
?plant? or ?press? it seems the text-only result is not
strong enough to help bootstrap the visual features
in any useful way. In other cases where little im-
provement is seen, the problem may lie with high
intra-class variation, as our visual words are not very
robust features, or with a lack of orthogonality be-
tween the lexical and visual information.
Our algorithm also performs significantly better
than the baseline measurements. K-means performs
surprisingly well compared to the other baselines,
but seems unable to make much sense of the visual
information present. Saenko and Darrell?s (2008)
LDA model makes substansial gains by using vi-
sual features, but does not perform as well on this
task. We suspect that a strict adherence to the seed
words may be to blame: while both this LDA model
and our algorithm use the same seed definitions ini-
tially, our algorithm is free to change its mind about
the usefulness of the words in the definitions as it
progresses, whereas the LDA model has no such
capacity. Indeed, words that are intuitively non-
discriminative, such as ?carry?, ?lack?, or ?late?, are
not uncommon in the definitions we use.
5 Conclusion and Future Work
We present an approach to unsupervised WSD that
works jointly with the visual and textual domains.
We showed that this multimodal approach makes
gains over text-only disambiguation, and outper-
forms previous approaches for WSD (both text-only,
and multimodal), when textual contexts are limited.
This project is still in progress, and there are many
avenues for further study. We do not currently ex-
ploit collocations between lexical and visual infor-
mation. Also, the bag-of-SIFT visual features that
we use, while effective, have little semantic content.
More structured representations over segmented im-
age regions offer greater potential for encoding se-
mantic content (Duygulu et al, 2002).
88
References
Kobus Barnard and Matthew Johnson. 2005. Word
sense disambiguation with pictures. In Artificial In-
telligence, volume 167, pages 13?130.
Kobus Barnard, Matthew Johnson, and David Forsyth.
2003. Word sense disambiguation with pictures.
In Workshop on Learning Word Meaning from Non-
Linguistic Data, Edmonton, Canada.
David M. Blei, Andrew Ng, and Michael I. Jordan. 2003.
Latent dirichlet alocation. In JMLR, volume 3, pages
993?1022.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. Imagenet: A large-scale hierar-
chical image database. In IEEE Conference on Com-
puter Vision and Pattern Recognition.
Pinar Duygulu, Kobus Barnard, Nando de Freitas, and
David Forsyth. 2002. Object recognition as machine
translation: Learning a lexicon for a fixed image vo-
cabulary. In European Conference on Computer Vi-
sion, Copenhagen, Denmark.
Christiane Fellbaum. 1998. Wordnet: An electronic lex-
ical database. In Bradford Books.
Yansong Feng and Mirella Lapata. 2010. Topic models
for image annotation and text illustration. In Annual
Conference of the North American Chapter of the ACL,
pages 831?839, Los Angeles, California.
Michael Jamieson, Afsaneh Fazly, Suzanne Stevenson,
Sven Dickinson, and Sven Wachsmuth. 2009. Using
language to learn structured appearance models for im-
age annotation. IEEE Transactions on Pattern Analy-
sis and Machine Intelligence, 32(1):148?164.
Chee Wee Leong and Rada Mihalcea. 2011. Measuring
the semantic relatedness between words and images.
In International Conference on Semantic Computing,
Oxford, UK.
Nicolas Loeff, Cecilia Ovesdotter Alm, and David
Forsyth. 2006. Discriminating image senses by clus-
tering with multimodal features. In Proceedings of the
COLING/ACL 2006 Main Conference Poster Sessions,
pages 547?554, Sydney, Australia.
David Lowe. 2004. Distinctive image features from
scale-invariant keypoints. International Journal of
Computer Vision, 60(2):91?110.
George Miller, Claudia Leacock, Randee Tengi, and Ross
Bunker. 1993. A semantic concordance. In Proceed-
ings of the 3rd DARPA Workshop on Human Language
Technology, pages 303?308.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2007. UMND1: Unsupervised word sense
disambiguation using contextual semantic relatedness.
In Proceedings of SemEval-2007, pages 390?393,
Prague, Czech Republic.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. Task 17: English lexical sam-
ple, SRL and all words. In Proceedings of SemEval-
2007, pages 87?92, Prague, Czech Republic.
Kate Saenko and Trevor Darrell. 2008. Unsupervised
learning of visual sense models for polysemous words.
In Proceedings of Neural Information Processing Sys-
tems, Vancouver, Canada.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33rd Annual Meeting of the ACL, pages
189?196, Cambridge, Massachusetts.
89
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 57?64
Manchester, August 2008
Fast Mapping in Word Learning: What Probabilities Tell Us
Afra Alishahi and Afsaneh Fazly and Suzanne Stevenson
Department of Computer Science
University of Toronto
{afra,afsaneh,suzanne}@cs.toronto.edu
Abstract
Children can determine the meaning of a
new word from hearing it used in a familiar
context?an ability often referred to as fast
mapping. In this paper, we study fast map-
ping in the context of a general probabilistic
model of word learning. We use our model
to simulate fast mapping experiments on chil-
dren, such as referent selection and retention.
The word learning model can perform these
tasks through an inductive interpretation of
the acquired probabilities. Our results suggest
that fast mapping occurs as a natural conse-
quence of learning more words, and provides
explanations for the (occasionally contradic-
tory) child experimental data.
1 Fast Mapping
An average six-year-old child knows over 14, 000
words, most of which s/he has learned from hearing
other people use them in ambiguous contexts (Carey,
1978). Children are thus assumed to be equipped with
powerful mechanisms for performing such a complex
task so efficiently. One interesting ability children as
young as two years of age show is that of correctly and
immediately mapping a novel word to a novel object
in the presence of other familiar objects. The term
?fast mapping? was first used by Carey and Bartlett
(1978) to refer to this phenomenon.
Carey and Bartlett?s goal was to examine how much
children learn about a word when presented in an am-
biguous context, as opposed to concentrated teaching.
They used an unfamiliar name (chromium) to refer to
an unfamiliar color (olive green), and then asked
a group of four-year-old children to select an object
from among a set, upon hearing a sentence explicitly
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some
rights reserved.
asking for the object of the new color, as in: bring
the chromium tray, not the blue one. Children were
generally good at performing this ?referent selection?
task. In a production task performed six weeks later,
when children had to use the name of the new color,
they showed signs of having learned something about
the new color name, but were not successful at pro-
ducing it. On the basis of these findings, Carey and
Bartlett suggest that fast mapping and word learning
are two distinct, yet related, processes.
Extending Carey and Bartlett?s work, much re-
search has concentrated on providing an explanation
for fast mapping, and on examining its role in word
learning. These studies also show that children are
generally good at referent selection, given a novel tar-
get. However, there is not consistent evidence regard-
ing whether children actually learn the novel word
from one or a few such exposures (retention). For
example, whereas the children in the experiments of
Golinkoff et al (1992) and Halberda (2006) showed
signs of nearly-perfect retention of the fast-mapped
words, those in the studies reported by Horst and
Samuelson (2008) did not (all participating children
were close in age range).
There are also many speculations about the possible
causes of fast mapping. Some researchers consider
it as a sign of a specialized (innate) mechanism for
word learning. Markman and Wachtel (1988), for ex-
ample, argue that children fast map because they ex-
pect each object to have only one name (mutual exclu-
sivity). Golinkoff et al (1992) attribute fast mapping
to a (hard-coded) bias towards mapping novel names
to nameless object categories. Some even suggest a
change in children?s learning mechanisms, at around
the time they start to show evidence of fast mapping
(which coincides with a sudden burst in their vocab-
ulary), e.g., from associative to referential (Gopnik
and Meltzoff, 1987; Reznick and Goldfield, 1992). In
contrast, others see fast mapping as a phenomenon
that arises from more general processes of learning
57
and/or communication, which also underlie the im-
pressive rate of lexical acquisition in children (e.g.,
Clark, 1990; Diesendruck and Markson, 2001; Regier,
2005; Horst et al, 2006; Halberda, 2006).
In our previous work (Fazly et al, 2008), we pre-
sented a word learning model which proposes a prob-
abilistic interpretation of cross-situational learning,
and bootstraps its own partially-learned knowledge of
the word meanings to accelerate word learning over
time. We have shown that the model can learn reason-
able word?meaning associations from child-directed
data, and that it accounts for observed learning pat-
terns in children, such as vocabulary spurt, without
requiring a developmental change in the underlying
learning mechanism. Here, we use this computational
model to investigate fast mapping and its relation to
word learning. Specifically, we take a close look at
the onset of fast mapping in our model by simulat-
ing some of the psychological experiments mentioned
above. We examine the behaviour of the model in var-
ious referent selection and retention tasks, and pro-
vide explanations for the (occasionally contradictory)
experimental results reported in the literature. We also
study the effect of exposure to more input on the per-
formance of the model in fast mapping.
Our results suggest that fast mapping can be ex-
plained as an induction process over the acquired as-
sociations between words and meanings. Our model
learns these associations in the form of probabilities
within a unified framework; however, we argue that
different interpretations of such probabilities may be
involved in choosing the referent of a familiar as op-
posed to a novel target word (as noted by Halberda,
2006). Moreover, the overall behaviour of our model
confirms that the probabilistic bootstrapping approach
to word learning naturally leads to the onset of fast
mapping in the course of lexical development, with-
out hard-coding any specialized learning mechanism
into the model to account for this phenomenon.
2 Overview of the Computational Model
This section summarizes the model presented in Fa-
zly et al (2008). Our word learning algorithm is an
adaptation of the IBM translation model proposed by
Brown et al (1993). However, our model is incre-
mental, and does not require a batch process over the
entire data.
2.1 Utterance and Meaning Representations
The input to our word learning model consists of a set
of utterance?scene pairs that link an observed scene
(what the child perceives) to the utterance that de-
scribes it (what the child hears). We represent each
utterance as a sequence of words, and the correspond-
ing scene as a set of meaning symbols. To simulate
referential uncertainty (i.e., the case where the child
perceives aspects of the scene that are unrelated to the
perceived utterance), we include additional symbols
in the representation of the scene, e.g.:
Utterance: Joe rolled the ball
Scene: {joe, roll, the, ball, mommy, hand, talk}
In Section 3.1, we explain how the utterances and
the corresponding semantic symbols are selected, and
how we add referential uncertainty.
Given a corpus of such utterance?scene pairs, our
model learns the meaning of each word w as a prob-
ability distribution, p(.|w), over the semantic sym-
bols appearing in the corpus. In this representation,
p(m|w) is the probability of a symbol m being the
meaning of a word w. In the absence of any prior
knowledge, all symbols are equally likely to be the
meaning of a word. Hence, prior to receiving any us-
ages of a given word, the model assumes a uniform
distribution over semantic symbols as its meaning.
2.2 Meaning Probabilities
Our model combines probabilistic interpretations of
cross-situational learning (Quine, 1960) and of a
variation of the principle of contrast (Clark, 1990),
through an interaction between two types of prob-
abilistic knowledge acquired and refined over time.
Given an utterance?scene pair received at time t, i.e.,
(U
(t)
, S
(t)
), the model first calculates an alignment
probability a for each w ? U(t) and each m ? S(t),
using the meaning probabilities p(.|w) of all the
words in the utterance prior to this time. The model
then revises the meaning of the words in U(t) by in-
corporating the alignment probabilities for the current
input pair. This process is repeated for all the input
pairs, one at a time.
Step 1: Calculating the alignment probabilities.
We estimate the alignment probabilities of words
and meaning symbols based on a localized version
of the principle of contrast: that a meaning sym-
bol in a scene is likely to be highly associated with
only one of the words in the corresponding utter-
ance.1 For a symbol m ? S(t) and a word w ? U(t),
the higher the probability of m being the meaning
of w (according to p(m|w)), the more likely it is
that m is aligned with w in the current input. In
other words, a(w |m, U(t), S(t)) is proportional to
p
(t?1)
(m|w). In addition, if there is strong evidence
that m is the meaning of another word in U(t)?
i.e., if p(t?1)(m|w?) is high for some w? ? U(t) other
1Note that this differs from what is widely known as the prin-
ciple of contrast (Clark, 1990), in that the latter assumes contrast
across the entire vocabulary rather than within an utterance.
58
than w?the likelihood of aligning m to w should de-
crease. Combining these two requirements:
a(w |m, U
(t)
, S
(t)
) =
p
(t?1)
(m|w)
?
w
?
?U
(t)
p
(t?1)
(m|w
?
)
(1)
Due to referential uncertainty, some of the meaning
symbols in the scene might not have a counterpart
in the utterance. To accommodate for such cases, a
dummy word is added to each utterance before the
alignment probabilities are calculated, in order to let
a meaning symbol not be (strongly) aligned with any
of the words in the current utterance.
Step 2: Updating the word meanings. We need to
update the probabilities p(.|w) for all words w ? U(t),
based on the evidence from the current input pair re-
flected in the alignment probabilities. We thus add
the current alignment probabilities for w and the sym-
bols m ? S(t) to the accumulated evidence from prior
co-occurrences of w and m. We summarize this
cross-situational evidence in the form of an associa-
tion score, which is updated incrementally:
assoc
(t)
(w, m) = assoc
(t?1)
(w, m) +
a(w|m, U
(t)
, S
(t)
) (2)
where assoc(t?1)(w, m) is zero if w and m have not
co-occurred before. The association score of a word
and a symbol is basically a weighted sum of their co-
occurrence counts.
The model then uses these association scores to up-
date the meaning of the words in the current input:
p
(t)
(m|w) =
assoc
(t)
(m, w) + ?
?
m
j
?M
assoc
(t)
(m
j
, w) + ? ? ?
(3)
where M is the set of all symbols encountered prior to
or at time t, ? is the expected number of symbol types,
and ? is a small smoothing factor. The denominator is
a normalization factor to get valid probabilities. This
formulation results in a uniform probability of 1/?
over all m ? M for a novel word w, and a probability
smaller than ? for a meaning symbol m that has not
been previously seen with a familiar word w.
Our model updates the meaning of a word ev-
ery time it is heard in an utterance. The strength
of learning of a word at time t is reflected in
p
(t)
(m = m
w
|w), where m
w
is the ?correct? mean-
ing of w: for a learned word w, the probability dis-
tribution p(.|w) is highly skewed towards the correct
meaning m
w
, and therefore hearing w will trigger the
retrieval of the meaning m
w
.
2
2An input-generation lexicon contains the correct meaning for
each word, as described in Section 3.1. Note that the model does
not have access to this lexicon for learning; it is used only for
input generation and evaluation.
From this point on, we simply use p(m|w) (omit-
ting the superscript (t)) to refer to the meaning prob-
ability of m for w at the present time of learning.
2.3 Referent Probabilities
The meaning probability p(m|w) is used to retrieve
the most probable meaning for w among all the possi-
ble meaning symbols m. However, in the referent se-
lection tasks performed by children, the subject is of-
ten forced to select the referent of a target word from
among a limited set of objects, even when the mean-
ing of the target word has not been accurately learned
yet. For our model to perform such tasks, it has to de-
cide how likely it is for a target word w to refer to a
particular object m, based on its previous knowledge
about the mapping between m and w (i.e., p(m|w)),
as well as the mapping between m and other words in
the lexicon.3
The likelihood of using a particular name w to refer
to a given object m is calculated as:
rf (w|m) = p(w|m)
=
p(m|w) ? p(w)
p(m)
=
p(m|w) ? p(w)
?
w
?
?V
p(m|w
?
) ? p(w
?
)
(4)
where V is the set of all words that the model has seen
so far, and p(w) is the relative frequency of w:
p(w) =
freq(w)
?
w
?
?V
freq(w
?
)
(5)
The referent of a target word w among the present ob-
jects, therefore, will be the object m with the highest
referent probability rf (w|m).
3 Experimental Setup
3.1 The Input Corpora
We extract utterances from the Manchester corpus
(Theakston et al, 2001) in the CHILDES database
(MacWhinney, 2000). This corpus contains tran-
scripts of conversations with children between the
ages of 1; 8 and 3; 0 (years;months). We use the
mother?s speech from transcripts of 6 children, re-
move punctuation and lemmatize the words, and con-
catenate the corresponding sessions as input data.
There is no semantic representation of the corre-
sponding scenes available from CHILDES. There-
fore, we automatically construct a scene representa-
tion for each utterance, as a set containing the seman-
tic referents of the words in that utterance. We get
these from an input-generation lexicon that contains
a symbol associated with each word as its semantic
3All through the paper, we use m as both the meaning and the
referent of a word w.
59
referent. We use every other sentence from the orig-
inal corpus, preserving their chronological order. To
simulate referential uncertainty in the input, we then
pair each sentence with its own scene representation
as well as that of the following sentence in the origi-
nal corpus. (Note that the latter sentence is not used
as an utterance in our input.) The extra semantic sym-
bols that are added to each utterance thus correspond
to meaningful semantic representations, as opposed
to randomly selected symbols. In the resulting corpus
of 92, 239 input pairs, each utterance is, on average,
paired with 78% extra meaning symbols, reflecting a
high degree of referential uncertianty.
3.2 The Model Parameters
We set the parameters of our learning algorithm using
a development data set which is similar to our training
and test data, but is selected from a non-overlapping
portion of the Manchester corpus. The expected num-
ber of symbols, ? in Eq. (3), is set to 8500 based on
the total number of distinct symbols extracted for the
development data. Therefore, the default probability
of a symbol for a novel word will be 1/8500. A famil-
iar word, on the other hand, has been seen with some
symbols before. Therefore, the probability of a previ-
ously unseen symbol for it (which, based on Eq. (3),
has an upper bound of ?) must be less than the default
probability mentioned above. Accordingly, we set ?
to 10?5.
3.3 The Training Procedure
In the next section, we report results from the com-
putational simulation of our model for a number of
experiments. All of the simulations use the same pa-
rameter settings (as described in the previous section),
but different input: in each simulation, a random por-
tion of 1000 utterance?scene pairs is selected from
the input corpus, and incrementally processed by the
model. The size of the training corpus is chosen arbi-
trarily to reflect a sample point in learning, and further
experiments have shown that increasing this number
does not change the pattern observed in the results. In
order to avoid behaviour that is specific to a particu-
lar sequence of input items, the reported results in the
next section are averaged over 10 simulations.
4 Experimental Results and Analysis
4.1 Referent Selection
In a typical word learning scenario, the child faces
a scene where a number of familiar and unfamiliar
objects are present. The child then hears a sentence,
which describes (some part of) the scene, and is com-
posed of familiar and novel words (e.g., hearing Joe is
eating a cheem, where cheem is a previously unseen
fruit). In such a setting, our model aligns the objects
in the scene with the words in the utterance based on
its acquired knowledge of word meanings, and then
updates the meanings of the words accordingly. The
model can align a familiar word with its referent with
high confidence, since the previously learned mean-
ing probability of the familiar object given the famil-
iar word, or p(m|w), is much higher than the meaning
probability of the same object given any other word in
the sentence. In a similar fashion, the model can eas-
ily align a novel word in the sentence with a novel
object in the scene, because the meaning probability
of the novel object given the novel word (1/?, ac-
cording to Eq. (3)) is higher than the meaning proba-
bility of that object for any previously heard word in
the sentence (the latter probability is smaller than ? in
Eq. (3), as explained in Section 3.2).
Earlier fast mapping experiments on children as-
sumed that it is such a contrast between the familiar
and novel words in the same sentence that helps chil-
dren select the correct target object in a referent selec-
tion task. For example, in Carey and Bartlett?s (1978)
experiment, to introduce a novel word?meaning as-
sociation (e.g., chromium?olive), the authors use
both the familiar and the novel words in one sentence
(bring me the chromium tray, not the blue one.). How-
ever, further experiments show that children can suc-
cessfully select the correct referent even if such a con-
trast is not present in the sentence. Many researchers
have performed experiments where young subjects
are forced to choose between a novel and a familiar
object upon hearing a request, such as give me the
ball (familiar target), or give me the dax (novel tar-
get). In all of the reported experimental results, chil-
dren can readily pick the correct referent for a famil-
iar or a novel target word in such a setting (Golinkoff
et al, 1992; Halberda and Goldman, 2008; Halberda,
2006; Horst and Samuelson, 2008).
However, Halberda?s eye-tracking experiments on
both adults and pre-schoolers suggest that the pro-
cesses involved for referent selection in the familiar
target situation may be different from those in the
novel target situation. In the latter situation, subjects
appear to systematically reject the familiar object as
the referent of the novel name before mapping the
novel object to the novel name. In the familiar target
situation, however, there is no need to reject the novel
distractor object, because the subject already knows
the referent of the target.
The difference between these two conditions can be
explained in terms of the meaning and referent proba-
bilities of our model explained in Section 2. In a typi-
cal referent selection experiment, the child is asked to
60
?get the ball? while facing a ball and a novel object
(dax). We assume that the child knows the meaning
of verbs and determiners such as get and the, therefore
we simplify the familiar target condition in the form
of the following input item:
ball (FAMILIAR TARGET)
{ball, dax}
A familiar word such as ball has a meaning prob-
ability highly skewed towards its correct meaning.
That is, upon hearing ball, the model can confidently
retrieve its meaning ball, which is the one with
the highest probability p(m|ball) among all possible
meanings m. In such a case, if ball is present in the
scene, the model can easily pick it as the referent of
the familiar target name, without processing the other
objects in the scene.
Now consider the condition where a novel target
name is used in the presence of a familiar and a pre-
viously unseen object:
dax (NOVEL TARGET)
{ball, dax}
Since this is the first time the model has heard the
word dax, both meanings ball and dax are equally
likely because p(.|dax ) is uniform. Thus the mean-
ing probabilities cannot be solely used for selecting
the referent of dax, and the model has to perform
some kind of induction on the potential referents in
the scene based on what it has learned about each
of them. The model can infer the referent of dax
by comparing the referent probabilities rf (dax |ball)
and rf (dax |dax) from Eq. (4) after processing the in-
put item. Since ball has strong associations with an-
other word ball, its referent probability for the novel
name dax is much lower than the referent probability
of dax, which does not have strong associations with
any of the words in the learned lexicon.
We simulate the process of referent selection in our
model as follows. We train the model as described
in Section 3.3. We then present the model with one
more input item, which represents either the FAMIL-
IAR TARGET or the NOVEL TARGET condition. For
each condition, we compare the meaning probability
p(object|target) for both familiar and novel objects
in the scene (see Table 1, top panel). In the FA-
MILIAR TARGET condition, the model demonstrates
a strong preference towards choosing the familiar ob-
ject as the referent, whereas in the NOVEL TARGET
condition, the model shows no preference towards any
of the objects based on the meaning probabilities of
the target word. Therefore, for the NOVEL TARGET
condition, we also compare the referent probabilities
rf (target |object) for both objects after processing
Table 1: Referent selection in FAMILIAR and NOVEL
TARGET conditions.
UPON HEARING THE TARGET WORD
Condition p(ball|target ) p(dax|target )
FAMILIAR TARGET 0.843 ?0.056 ? 0.0001
NOVEL TARGET 0.0001 ?0.00 0.0001 ?0.00
AFTER PERFORMING INDUCTION
Condition rf (target |ball) rf (target |dax)
NOVEL TARGET 0.127 ?0.127 0.993 ?0.002
the input item as a training pair, simulating the in-
duction process that humans go through to select the
referent in such cases. This time, the model shows a
strong preference towards the novel object as the ref-
erent of the target word (see Table 1, bottom panel).
Our results confirm that in both conditions, the model
consistently selects the correct referent for the target
word across all the simulations.
4.2 Retention
As discussed in the previous section, results from
the human experiments as well as our computational
simulations show that the referent of a novel target
word can be selected based on the previous knowl-
edge about the present objects and their names. How-
ever, the success of a subject in a referent selection
task does not necessarily mean that the child/model
has learned the meaning of the novel word based on
that one trial. In order to better understand what and
how much children learn about a novel word from a
single ambiguous exposure, some studies have per-
formed retention trials after the referent selection ex-
periments. Often, various referent selection trials are
performed in one session, where in each trial a novel
object?name pair is introduced among familiar ob-
jects. Some of the recently introduced objects are
then put together in one last trial, and the subjects
are asked to choose the correct referent for one of the
(recently heard) novel target words. The majority of
the reported experiments show that children can suc-
cessfully perform the retention task (Golinkoff et al,
1992; Halberda and Goldman, 2008; Halberda, 2006).
We simulate a similar retention experiment by
training the model as usual. We further present the
model with two experimental training items similar to
the one used in the NOVEL TARGET condition in the
previous section, with different familiar and novel ob-
jects and words in each input:
dax (REFERENT SELECTION TRIAL 1)
{ball, dax}
cheem (REFERENT SELECTION TRIAL 2)
{pen, cheem}
61
Table 2: Retention of a novel target word from a set
of novel objects.
2-OBJECT RETENTION TRIAL
rf (dax |dax) rf (dax |cheem)
0.996 ?0.001 0.501 ?0.068
3-OBJECT RETENTION TRIAL
rf (dax |dax) rf (dax |cheem) rf (dax |lukk)
0.995 ?0.001 0.407 ?0.062 0.990 ?0.001
The training session is followed by a retention trial,
where the two novel objects used in the previous ex-
perimental inputs are paired with one of the novel tar-
get words:
dax (2-OBJECT RETENTION TRIAL)
{cheem, dax}
After processing the retention input, we com-
pare the referent probabilities rf (dax |cheem) and
rf (dax |dax) to see if the model can choose the cor-
rect novel object in response to the target word dax.
The top panel in Table 2 summarizes the results of this
experiment. The model consistently shows a strong
preference towards the correct novel object as the ref-
erent of the novel target word across all simulations.
Unlike studies on referent selection, experimental
results for retention have not been consistent across
various studies. Horst and Samuelson (2008) per-
form experiments with two-year-old children involv-
ing both referent selection and retention, and report
that their subjects perform very poorly at the retention
task. One factor that discriminates the experimental
setup of Horst and Samuelson from others (e.g., Hal-
berda, 2006) is that, in their retention trials, they put
together two recently observed novel objects with a
third novel object that has not been seen in any of the
experimental sessions before. The authors do not at-
tribute their contradictory results to the presence of
this third object, but this factor can in fact affect the
performance considerably. We simulate this condition
by using the same input items for referent selection
trials as in the previous simulation, but we replace the
retention trial with the following:
dax (3-OBJECT RETENTION TRIAL)
{cheem, dax, lukk}
The third object, lukk, has not been seen by the
model before. Results under the new condition are re-
ported in the bottom panel of Table 2. As can be seen,
the model shows a strong tendency towards the cor-
rect novel referent dax for the novel target dax, com-
pared to the other recently seen novel object cheem.
However, the probability of the unseen object lukk
is also very high for the target word dax. That is be-
cause the model cannot use any previously acquired
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
x 104
0
5
10
15
20
25
30
35
time of first exposure
nu
m
be
r o
f u
sa
ge
s n
ee
de
d 
to
 le
ar
n
Figure 1: Number of usages needed to learn a word,
as a function of the word?s age of exposure.
knowledge about lukk (i.e., associating it with an-
other word) to rule it out as a referent for dax. These
results show that introducing a new object for the first
time in a retention trial considerably increases the dif-
ficulty of the task. This can explain the contradictory
results reported in the literature: when the referent
probabilities are not informative, other factors may
influence the outcome of the experiment, such as the
amount of training received for a novel word?object,
or a possible delay between training and test sessions.
4.3 The Effect of Exposure to More Input
The fast mapping ability observed in children implies
that once children have learned a repository of words,
they can easily link novel words to novel objects in a
familiar context based only on a few exposures. We
examine this effect in our model: we train the model
on 20, 000 input pairs, looking at the relation between
the time of first exposure to a word, and the number
of usages that the model needs for learning that word.
Figure 1 plots this for words that have been learned at
some point during the training.4 We can see that the
model shows clear fast mapping behaviour?that is,
words received later in time, on average, require fewer
usages to be learned. These results show that our
model exhibits fast mapping patterns once it has been
exposed to enough word usages, and that no change
in the underlying learning mechanism is needed.5
The effect of exposure to more input on fast map-
ping can be described in terms of context familiarity:
the more input the model has processed so far, the
more likely it is that the context of the usage of a novel
word (the other words in the sentence and the objects
in the scene) is familiar to the model. This pattern
has been studied through a number of experiments on
4We consider a word w as learned if the meaning probability
p(m
w
|w) is higher than a certain threshold ?. For this experi-
ment, we set ? = 0.70.
5In Fazly et al (2008), we reported a variation of this exper-
iment, where we used a smaller training set, and also a different
semantic representation for word meanings.
62
children. For example, Gershkoff-Stowe and Hahn
(2007) taught 16- to 18-month-olds the names of 24
unfamiliar objects over 12 training sessions, where
unfamiliar objects were presented with varying fre-
quency. Data were compared to a control group of
children who were exposed to the same experimen-
tal words at the first and last sessions only. Their re-
sults show that for children in the experimental group,
extended practice with a novel set of words led to
the rapid acquisition of a second set of low-practice
words. Children in the control group did not show the
same lexical advantage.
Inspired by Gershkoff-Stowe and Hahn (2007), we
perform an experiment to study the effect of con-
text familiarity on fast mapping in our model. We
choose two sets of words, CONTEXT (containing 20
words) and TARGET (containing 10 words), to con-
duct a referent selection task as follows. First, we
train our model on a sequence of utterance?scene
pairs constructed from the set CONTEXT ? TARGET,
as follows: the unified set is randomly shuffled and
divided into two subsets, words in each subset are
put together to form an utterance, and the meanings
of the words in that utterance are put together to
form the corresponding scene. We repeat this process
twice, so that each word appears in exactly two input
pairs. We train our model on the constructed pairs.6
Next, we perform a referent selection task on each
word in the TARGET set: we pair each target word
w with the meaning of 10 randomly selected words
from CONTEXT ? TARGET, including the meaning of
the target word itself (m
w
), and have the model pro-
cess this test pair. We compare the referent probabil-
ity of w and each m ? CONTEXT ? TARGET to see
whether the model can correctly map the target word
to its referent. We call this setting the LOW TRAIN-
ING condition.
In the above setting, the context words in the ref-
erent selection trials are as new to the model as the
target words. We thus repeat this experiment with
a familiar context: we first train the model over in-
put pairs that are randomly constructed from words
in CONTEXT only, using the same training proce-
dure as described above. This context-familiarization
process is followed by a similar training session on
CONTEXT ? TARGET, and a test session on target
words, similar to the previous condition. Again, we
count the number of correct mappings between a tar-
get word and its referent based on the referent proba-
bilities. We call this setting the HIGH TRAINING con-
dition. Table 3 shows the results for both conditions.
It can be seen that the accuracy of finding the referent
6Unlike in previous experiments, here we do not use child-
directed data as we want to control the familiarity of the context.
Table 3: Average number of correct mappings and the
referent probabilities of target words for two condi-
tions, LOW and HIGH TRAINING.
Condition Correct mappings P (target |m
target
)
LOW TRAINING %54 0.216?0.04
HIGH TRAINING %90 0.494?0.79
for a target word, as well as the referent probability of
a target word for its correct meaning, increase as a re-
sult of more training on the context. In other words, a
more familiar context helps the model perform better
in a fast mapping task.
5 Related Computational Models
The rule-based model of Siskind (1996), and the con-
nectionist model proposed by Regier (2005), both
show that learning gets easier as the model is exposed
to more input?that is, words heard later are learned
faster. These findings confirm that fast mapping may
simply be a result of learning more words, and that
no explicit change in the underlying learning mech-
anism is needed. However, these studies do not ex-
amine various aspects of fast mapping, such as ref-
erent selection and retention. Horst et al (2006) ex-
plicitly test fast mapping in their connectionist model
of word learning by performing referent selection and
retention tasks. The behaviour of their model matches
the child experimental data reported in a study by the
same authors (Horst and Samuelson, 2008), but not
that of the contradictory findings of other similar ex-
periments. Moreover, the model?s learning capacity
is limited, and the fast mapping experiments are per-
formed on a very small vocabulary. Frank et al (2007)
examine fast mapping in their Bayesian model by test-
ing its performance in a novel target referent selection
task. However, the experiment is performed on an ar-
tifical corpus. Moreover, since the learning algorithm
is non-incremental, the success of the model in refer-
ent selection is determined implicitly: each possible
word?meaning mapping from the test input is added
to the current lexicon, and the consistency of the new
lexicon is checked against the training corpus.
6 Discussion and Concluding Remarks
We have used a general computational model of word
learning (first introduced in Fazly et al, 2008) to study
fast mapping. Our model learns a probabilistic asso-
ciation between a word and its meaning, from expo-
sure to word usages in naturalistic contexts. We have
shown that these probabilities can be used to simu-
late various fast mapping experiments performed on
children, such as referent selection and retention. Our
63
experimental results suggest that fast mapping can be
explained as an induction process over the acquired
associations between words and objects. In that sense,
fast mapping is a general cognitive ability, and not
a hard-coded, specialized mechanism of word learn-
ing.7 In addition, our results confirm that the onset
of fast mapping is a natural consequence of learning
more words, which in turn accelerates the learning of
new words. This bootstrapping approach results in a
rapid pace of vocabulary acquisition in children, with-
out requiring a developmental change in the underly-
ing learning mechanism.
Results of the referent selection experiments show
that our model can successfully find the referent of
a novel target word in a familiar context. Moreover,
our retention experiments show that the model can
map a recently heard novel word to its recently seen
novel referent (among other novel objects) after only
one exposure. However, the strength of the associa-
tion of a novel pair after one exposure shows a no-
table difference compared to the association between
a ?typical? familiar word and its meaning.8 This is
consistent with what is commonly assumed in the lit-
erature: even though children learn something about
a word from only one exposure, they often need more
exposure to reliably learn its meaning (Carey, 1978).
Various kinds of experiments have been performed to
examine how strongly children learn novel words in-
troduced to them in experimental settings. For exam-
ple, children are persuaded to produce a fast-mapped
word, or to use the novel word to refer to objects
that are from the same category as its original refer-
ent (e.g., Golinkoff et al, 1992; Horst and Samuelson,
2008). We intend to look at these new tasks in our fu-
ture research.
References
Behrend, Douglas A., Jason Scofield, and Erica E.
Kleinknecht 2001. Beyond fast mapping: Young chil-
dren?s extensions of novel words and novel facts. De-
velopmental Psychology, 37(5):698?705.
Brown, Peter F., Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer 1993. The mathematics
of statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Carey, Susan 1978. The child as word learner. In Halle, M.,
J. Bresnan, and G. A. Miller, editors, Linguistic Theory
and Psychological Reality. The MIT Press.
Carey, Susan and Elsa Bartlett 1978. Acquiring a single
new word. Papers and reports on Child Language De-
velopment, 15:17?29.
7In fact, similar fast mapping effects have been studied in con-
texts other than language. For example, Behrend et al (2001) re-
port on children?s fast mapping of novel facts about novel objects.
8After processing 1000 input pairs, the average meaning prob-
ability of familiar words (those with frequency higher than 10) is
0.77, whereas that of the novel word after one exposure is 0.64.
Clark, Eve 1990. On the pragmatics of contrast. Journal
of Child Language, 17:417?431.
Diesendruck, Gil and Lori Markson 2001. Children?s
avoidance of lexical overlap: A pragmatic account. De-
velopmental Psychology, 37(5):630?641.
Fazly, Afsaneh, Afra Alishahi, and Suzanne Steven-
son 2008. A probabilistic incremental model of word
learning in the presence of referential uncertainty. In
Proceedings of the 30th Annual Conference of the Cog-
nitive Science Society.
Frank, Michael C., Noah D. Goodman, and Joshua B.
Tenenbaum 2007. A bayesian framework for cross-
situational word-learning. In Advances in Neural Infor-
mation Processing Systems, volume 20.
Gershkoff-Stowe, Lisa and Erin R. Hahn 2007. Fast map-
ping skills in the developing lexicon. Journal of Speech,
Language, and Hearing Research, 50:682?697.
Golinkoff, Roberta Michnick, Kathy Hirsh-Pasek,
Leslie M. Bailey, and Neil R. Wegner 1992. Young
children and adults use lexical principles to learn new
nouns. Developmental Psychology, 28(1):99?108.
Gopnik, Alison and Andrew Meltzoff 1987. The develop-
ment of categorization in the second year and its relation
to other cognitive and linguistic developments. Child
Development, 58(6):1523?1531.
Halberda, Justin 2006. Is this a dax which I see before
me? use of the logical argument disjunctive syllogism
supports word-learning in children and adults. Cognitive
Psychology, 53:310?344.
Halberda, Justin and Julie Goldman 2008. One-trial learn-
ing in 2-year-olds: Children learn new nouns in 3 sec-
onds flat. (in submission).
Horst, Jessica S., Bob McMurray, and Larissa K. Samuel-
son 2006. Online processing is essential for learning:
Understanding fast mapping and word learning in a dy-
namic connectionist architecture. In Proc. of CogSci?06.
Horst, Jessica S. and Larissa K. Samuelson 2008. Fast
mapping but poor retention by 24-month-old infants. In-
fancy, 13(2):128?157.
MacWhinney, B. 2000. The CHILDES Project: Tools for
Analyzing Talk, volume 2: The Database. MahWah, NJ:
Lawrence Erlbaum Associates, third edition.
Markman, Ellen M. and Gwyn F. Wachtel 1988. Children?s
use of mutual exclusivity to constrain the meanings of
words. Cognitive Psychology, 20:121?157.
Quine, W.V.O. 1960. Word and Object. Cambridge, MA:
MIT Press.
Regier, Terry 2005. The emergence of words: Atten-
tional learning in form and meaning. Cognitive Science,
29:819?865.
Reznick, J. Steven and Beverly A. Goldfield 1992. Rapid
change in lexical development in comprehension and
production. Developmental Psychology, 28(3):406?413.
Siskind, Jeffery Mark 1996. A computational study
of cross-situational techniques for learning word-to-
meaning mappings. Cognition, 61:39?91.
Theakston, A. L., E. V. Lieven, J. M. Pine, and C. F. Row-
land 2001. The role of performance limitations in the
acquisition of verb-argument structure: An alternative
account. Journal of Child Language, 28:127?152.
64
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 89?96
Manchester, August 2008
An Incremental Bayesian Model for Learning Syntactic Categories
Christopher Parisien, Afsaneh Fazly and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, ON, Canada
[chris,afsaneh,suzanne]@cs.toronto.edu
Abstract
We present an incremental Bayesian model for
the unsupervised learning of syntactic cate-
gories from raw text. The model draws infor-
mation from the distributional cues of words
within an utterance, while explicitly bootstrap-
ping its development on its own partially-
learned knowledge of syntactic categories.
Testing our model on actual child-directed
data, we demonstrate that it is robust to noise,
learns reasonable categories, manages lexical
ambiguity, and in general shows learning be-
haviours similar to those observed in children.
1 Introduction
An important open problem in cognitive science and
artificial intelligence is how children successfully
learn their native language despite the lack of explicit
training. A key challenge in the early stages of lan-
guage acquisition is to learn the notion of abstract
syntactic categories (e.g., nouns, verbs, or determin-
ers), which is necessary for acquiring the syntactic
structure of language. Indeed, children as young as
two years old show evidence of having acquired a
good knowledge of some of these abstract categories
(Olguin and Tomasello, 1993); by around six years of
age, they have learned almost all syntactic categories
(Kemp et al, 2005). Computational models help to
elucidate the kinds of learning mechanisms that may
be capable of achieving this feat. Such studies shed
light on the possible cognitive mechanisms at work
in human language acquisition, and also on potential
means for unsupervised learning of complex linguis-
tic knowledge in a computational system.
Learning the syntactic categories of words has
been suggested to be based on the morphological and
phonological properties of individual words, as well
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
as on the distributional information about the con-
texts in which they appear. Several computational
models have been proposed that draw on one or more
of the above-mentioned properties in order to group
words into discrete unlabeled categories. Most ex-
isting models only intend to show the relevance of
such properties to the acquisition of adult-like syn-
tactic categories such as nouns and verbs; hence, they
do not necessarily incorporate the types of learning
mechanisms used by children (Schu?tze, 1993; Red-
ington et al, 1998; Clark, 2000; Mintz, 2003; Onnis
and Christiansen, 2005). For example, in contrast to
the above models, children acquire their knowledge
of syntactic categories incrementally, processing the
utterances they hear one at a time. Moreover, chil-
dren appear to be sensitive to the fact that syntactic
categories are partially defined in terms of other cat-
egories, e.g., nouns tend to follow determiners, and
can be modified by adjectives.
We thus argue that a computational model should
be incremental, and should use more abstract cate-
gory knowledge to help better identify syntactic cat-
egories. Incremental processing also allows a model
to incorporate its partially-learned knowledge of cat-
egories, letting the model bootstrap its development.
To our knowledge, the only incremental model of
category acquisition that also incorporates bootstrap-
ping is that of Cartwright and Brent (1997). Their
template-based model, however, draws on very spe-
cific linguistic constraints and rules to learn cate-
gories. Moreover, their model has difficulty with the
variability of natural language data.
We address these shortcomings by developing an
incremental probabilistic model of syntactic category
acquisition that uses a domain-general learning algo-
rithm. The model also incorporates a bootstrapping
mechanism, and learns syntactic categories by look-
ing only at the general patterns of distributional sim-
ilarity in the input. Experiments performed on actual
(noisy) child-directed data show that an explicit boot-
strapping component improves the model?s ability to
89
learn adult-like categories. The model?s learning tra-
jectory resembles some relevant behaviours seen in
children, and we also show that the categories that
our model learns can be successfully used in a lexical
disambiguation task.
2 Overview of the Computational Model
We adapt a probabilistic incremental model of un-
supervised categorization (i.e., clustering) proposed
by Anderson (1991). The original model has been
used to simulate human categorization in a variety
of domains, including the acquisition of verb argu-
ment structure (Alishahi and Stevenson, 2008). Our
adaptation of the model incorporates an explicit boot-
strapping mechanism and a periodic merge of clus-
ters, both facilitating generalization over input data.
Here, we explain the input to our model (Section 2.1),
the categorization model itself (Section 2.2), how we
estimate probabilities to facilitate bootstrapping (Sec-
tion 2.3), and our approach for merging similar clus-
ters (Section 2.4).
2.1 Input Frames
We aim to learn categories of words, and we do this
by looking for groups of similar word usages. Thus,
rather than categorizing a word alone, we categorize a
word token with its context from that usage. The ini-
tial input to our model is a sequence of unannotated
utterances, that is, words separated by spaces. Before
being categorized by the model, each word usage in
the input is processed to produce a frame that con-
tains the word itself (the head word of the frame) and
its distributional context (the two words before and
after it). For example, in the utterance ?I gave Josie
a present,? when processing the head word Josie, we
create the following frame for input to the categoriza-
tion system:
feature w
?2
w
?1
w
0
w
+1
w
+2
I gave Josie a present
where w
0
denotes the head word feature, and w
?2
,
w
?1
, w
+1
, w
+2
are the context word features. A con-
text word may be ?null? if there are fewer than two
preceding or following words in the utterance.
2.2 Categorization
Using Anderson?s (1991) incremental Bayesian cat-
egorization algorithm, we learn clusters of word us-
ages (i.e., the input frames) by drawing on the overall
similarity of their features (here, the head word and
the context words). The clusters themselves are not
predefined, but emerge from similarities in the input.
More formally, for each successive frame F in the
input, processed in the order of the input words, we
place F into the most likely cluster, either from the
K existing clusters, or a new one:
BestCluster(F ) = argmax
k
P (k|F ) (1)
where k = 0, 1, ..,K, including the new cluster
k = 0. Using Bayes? rule, and dropping P (F ) from
the denominator, which is constant for all k, we find:
P (k|F ) =
P (k)P (F |k)
P (F )
? P (k)P (F |k) (2)
The prior probability of k, P (k), is given by:
P (k) =
cn
k
(1? c) + cn
, 1 ? k ? K (3)
P (0) =
1? c
(1? c) + cn
(4)
where n
k
is the number of frames in k, and n is
the total number of frames observed at the time of
processing frame F . Intuitively, a well-entrenched
(large) cluster should be a more likely candidate for
categorization than a small one. We reserve a small
probability for creating a new cluster (Eq. 4). As the
model processes more input overall, it should become
less necessary to create new clusters to fit the data, so
P (0) decreases with large n. In our experiments, we
set c to a large value, 0.95, to further increase the
likelihood of using existing clusters.1
The probability of a frame F given a cluster k,
P (F |k), depends on the probabilities of the features
in F given k. We assume that the individual fea-
tures in a frame are conditionally independent given
k, hence:
P (F |k) = P
H
(w
0
|k)
?
i?{?2,?1,+1,+2}
P (w
i
|k) (5)
where P
H
is the head word probability, i.e., the like-
lihood of seeing w
0
as a head word among the frames
in cluster k. The context word probability P (w
i
|k) is
the likelihood of seeing w
i
in the ith context position
of the frames in cluster k. Next, we explain how we
estimate each of these probabilities from the input.
2.3 Probabilities and Bootstrapping
For the head word probability P
H
(w
0
|k), we use a
smoothed maximum likelihood estimate (i.e., the pro-
portion of frames in cluster k with head word w
0
).
For the context word probability P (w
i
|k), we can
form two estimates. The first is a simple maximum
likelihood estimate, which enforces a preference for
creating clusters of frames with the same context
words. That is, head words in the same cluster will
1The prior P (k) is equivalent to the prior in a Dirichlet pro-
cess mixture model (Sanborn et al, 2006), commonly used for
sampling clusters of objects.
90
tend to share the same adjacent words. We call this
word-based estimate P
word
.
Alternatively, we may consider the likelihood of
seeing not just the context word w
i
, but similar words
in that position. For example, if w
i
can be used as a
noun or a verb, then we want the likelihood of seeing
other nouns or verbs in position i of frames in cluster
k. Here, we use the partial knowledge of the learned
clusters. That is, we look over all existing clusters
k
?
, estimate the probability that w
i
is the head word
of frames in k?, then estimate the probability of using
the head words from those other clusters in position i
in cluster k. We refer to this category-based estimate
as P
cat
:
P
cat
(w
i
|k) =
?
k
?
P
H
(w
i
|k
?
)P
i
(k
?
|k) (6)
where P
i
(k
?
|k) is the probability of finding usages
from cluster k? in position i given cluster k. To sup-
port this we record the categorization decisions the
model has made. When we categorize the frames of
an utterance, we get a sequence of clusters for that
utterance, which gives additional information to sup-
plement the frame. We use this information to esti-
mate P
i
(k
?
|k) for future categorizations, again using
a smoothed maximum likelihood formula.
In contrast to the P
word
estimate, the estimate in
Eq. (6) prefers clusters of frames that use the same
categories as context. While some of the results of
these preferences will be the same, the latter approach
lets the model make second-order inferences about
categories. There may be no context words in com-
mon between the current frame and a potential clus-
ter, but if the context words in the cluster have been
found to be distributionally similar to those in the
frame, it may be a good cluster for that frame.
We equally weight the word-based and the
category-based estimates for P (w
i
|k) to get the like-
lihood of a context word; that is:
P (w
i
|k) ?
1
2
P
word
(w
i
|k) +
1
2
P
cat
(w
i
|k) (7)
This way, the model sees an input utterance simulta-
neously as a sequence of words and as a sequence of
categories. It is the P
cat
component, by using devel-
oping category knowledge, that yields the bootstrap-
ping abilities of our model.
2.4 Generalization
Our model relies heavily on the similarity of word
contexts in order to find category structure. In nat-
ural language, these context features are highly vari-
able, so it is difficult to draw consistent structure from
the input in the early stages of an incremental model.
When little information is available, there is a risk of
incorrectly generalizing, leading to clustering errors
which may be difficult to overcome. Children face
a similar problem in early learning, but there is ev-
idence that they may manage the problem by using
conservative strategies (see, e.g., Tomasello, 2000).
Children may form specific hypotheses about each
word type, only later generalizing their knowledge to
similar words. Drawing on this observation, we form
early small clusters specific to the head word type,
then later aid generalization by merging these smaller
clusters. By doing this, we ensure that the model only
groups words of different types when there is suffi-
cient evidence for their contextual similarity.
Thus, when a cluster has been newly created, we
require that all frames put into the cluster share the
same head word type.2 When clusters are small, this
prevents the model from making potentially incorrect
generalizations to different words. Periodically, we
evaluate a set of reasonably-sized clusters, and merge
pairs of clusters that have highly similar contexts (see
below for details). If the model decides to merge two
clusters with different head word types?e.g., one
cluster with all instances of dog, and another with
cat?it has in effect made a decision to generalize.
Intuitively, the model has learned that the contexts
in the newly merged cluster apply to more than one
word type. We now say that any word type could be
a member of this cluster, if its context is sufficiently
similar to that of the cluster. Thus, when categoriz-
ing a new word token (represented as a frame F ),
our model can choose from among the clusters with
a matching head word, and any of these ?generalized?
clusters that contain mixed head words.
Periodically, we look through a subset of the clus-
ters to find similar pairs to merge. In order to limit
the number of potential merges to consider, we only
examine pairs of clusters in which at least one cluster
has changed since the last check. Thus, after pro-
cessing every 100 frames of input, we consider the
clusters used to hold those recent 100 frames as can-
didates to be merged with another cluster. We only
consider clusters of reasonable size (here, at least 10
frames) as candidates for merging. For each candi-
date pair of clusters, k
1
and k
2
, we first evaluate a
heuristic merge score that determines if the pair is
appropriate to be merged, according to some local
criteria, i.e., the size and the contents of the candi-
date clusters. For each suggested merge (a pair whose
merge score exceeds a pre-determined threshold), we
then look at the set of all clusters, the global evidence,
to decide whether to accept the merge.
The merge score combines two factors: the en-
trenchment of the two clusters, and the similarity of
2However, a word type may exist in several clusters (e.g., for
distinct noun and verb usages), thus handling lexical ambiguity.
91
their context features. The entrenchment measure
identifies clusters that contain enough frames to show
a significant trend. We take a sigmoid function over
the number of frames in the clusters, giving a soft
threshold approaching 0 for small clusters and 1 for
large clusters. The similarity measure identifies pairs
of clusters with similar distributions of word and cat-
egory contexts. Given two clusters, we measure the
symmetric Kullback-Leibler divergence for each cor-
responding pair of context feature probabilities (in-
cluding the category contexts P
i
(k
?
|k), 8 pairs in to-
tal), then place the sum of those measures on another
sigmoid function. The merge score is the sum of the
entrenchment and similarity measures.
Since it is only a local measure, the merge score is
not sufficient on its own for determining if a merge
is appropriate. For each suggested merge, we thus
examine the likelihood of a sample of input frames
(here, the last 100 frames) under two states: the set
of clusters before the merge, and the set of clusters if
the merge is accepted. We only accept a merge if it
results in an increase in the likelihood of the sample
data. The likelihood of a sample set of frames, S ,
over a set of clusters, K, is calculated as in:
P (S) =
?
F?S
?
k?K
P (F |k)P (k) (8)
3 Evaluation Methodology
To test our proposed model, we train it on a sample of
language representative of what children would hear,
and evaluate its categorization abilities. We have
multiple goals in this evaluation. First, we determine
the model?s ability to discover adult-level syntactic
categories from the input. Since this is intended to be
a cognitively plausible learning model, we also com-
pare the model?s qualitative learning behaviours with
those of children. In the first experiment (Section 4),
we compare the model?s categorization with a gold
standard of adult-level syntactic categories and exam-
ine the effect of the bootstrapping component. The
second experiment (Section 5) examines the model?s
development of three specific parts of speech. De-
velopmental evidence suggests that children acquire
different syntactic categories at different ages, so we
compare the model?s learning rates of nouns, verbs,
and adjectives. Lastly, we examine our model?s abil-
ity to handle lexically ambiguous words (Section 6).
English word forms commonly belong to more than
one syntactic category, so we show how our model
uses context to disambiguate a word?s category.
In all experiments, we train and test the model us-
ing the Manchester corpus (Theakston et al, 2001)
from the CHILDES database (MacWhinney, 2000).
The corpus contains transcripts of mothers? conver-
sations with 12 British children between the ages of
1;8 (years;months) and 3;0. There are 34 one-hour
sessions per child over the course of a year. The age
range of the children roughly corresponds with the
ages at which children show the first evidence of syn-
tactic categories.
We extract the mothers? speech from each of the
transcripts, then concatenate the input of all 12 chil-
dren (all of Anne?s sessions, followed by all of Aran?s
sessions, and so on). We remove all punctuation. We
spell out contractions, so that each token in the input
corresponds to only one part-of-speech (PoS) label
(noun, verb, etc.). We also remove single-word ut-
terances and utterances with a single repeated word
type, since they contain no distributional informa-
tion. We randomly split the data into development
and evaluation sets, each containing approximately
683,000 tokens. We use the development set to fine-
tune the model parameters and develop the experi-
ments, then use the evaluation set as a final test of
the model. We further split the development set into
about 672,000 tokens (about 8,000 types) for training
and 11,000 tokens (1,300 types) for validation. We
split the evaluation set comparably, into training and
test subsets. All reported results are for the evaluation
set. A conservative estimate suggests that children
are exposed to at least 1.5 million words of child-
directed speech annually (Redington et al, 1998), so
this corpus represents only a small portion of a child?s
available input.
4 Experiment 1: Adult Categories
4.1 Methods
We use three separate versions of the categorization
model, in which we change the components used to
estimate the context word probability, P (w
i
|k) (as
used in Eq. (5), Section 2.2). In the word-based
model, we estimate the context probabilities using
only the words in the context window, by directly
using the maximum-likelihood P
word
estimate. The
bootstrap model uses only the existing clusters to es-
timate the probability, directly using the P
cat
esti-
mate from Eq. (6). The combination model uses an
equally-weighted combination of the two probabili-
ties, as presented in Eq. (7).
We run the model on the training set, categoriz-
ing each of the resulting frames in order. After every
10,000 words of input, we evaluate the model?s cate-
gorization performance on the test set. We categorize
each of the frames of the test set as usual, treating the
text as regular input. So that the test set remains un-
seen, the model does not record these categorizations.
4.2 Evaluation
The PoS tags in the Manchester corpus are too fine-
grained for our evaluation, so for our gold standard
92
we map them to the following 11 tags: noun, verb,
auxiliary, adjective, adverb, determiner, conjunction,
negation, preposition, infinitive to, and ?other.? When
we evaluate the model?s categorization performance,
we have two different sets of clusters of the words in
the test set: one set resulting from the gold standard,
and another as a result of the model?s categorization.
We compare these two clusterings, using the adjusted
Rand index (Hubert and Arabie, 1985), which mea-
sures the overall agreement between two clusterings
of a set of data points. The measure is ?corrected for
chance,? so that a random grouping has an expected
score of zero. This measure tends to be very con-
servative, giving values much lower than an intuitive
percentage score. However, it offers a useful relative
comparison of overall cluster similarity.
4.3 Results
Figure 1 gives the adjusted Rand scores of the three
model variants, word-based, bootstrap, and combi-
nation. Higher values indicate a better fit with the
gold-standard categorization scheme. The adjusted
Rand score is corrected for chance, thus providing a
built-in baseline measure. Since the expected score
for a random clustering is zero, all three model vari-
ants operate at above-baseline performance.
As seen in Figure 1, the word-based model gains
an early advantage in the comparison, but its per-
formance approaches a plateau at around 200,000
words of input. This suggests that while simple
word distributions provide a reliable source of infor-
mation early in the model?s development, the infor-
mation is not sufficient to sustain long-term learn-
ing. The bootstrap model learns much more slowly,
which is unsurprising, given that it depends on hav-
ing some reasonable category knowledge in order to
develop its clusters?leading to a chicken-and-egg
problem. However, once started, its performance im-
proves well beyond the word-based model?s plateau.
These results suggest that on its own, each compo-
nent of the model may be effectively throwing away
useful information. By combining the two models,
the combination model appears to gain complemen-
tary benefits from each component, outperforming
both. The word-based component helps to create a
base of reliable clusters, which the bootstrap compo-
nent uses to continue development.
After all of the training text, the combination
model uses 411 clusters to categorize the test tokens
(compared to over 2,000 at the first test point). While
this seems excessive, we note that 92.5% of the test
tokens are placed in the 25 most populated clusters.3
3See www.cs.toronto.edu/?chris/syncat for examples.
0 1 2 3 4 5 6
x 105
0
0.05
0.1
0.15
0.2
Training set size (words)
R a
dj
Combination
Word?based
Bootstrap
Figure 1: Adjusted Rand Index of each of three mod-
els? clusterings of the test set, as compared with the
PoS tags of the test data.
5 Experiment 2: Learning Trends
A common trend observed in children is that differ-
ent syntactic categories are learned at different rates.
Children appear to have learned the category of nouns
by 23 months of age, verbs shortly thereafter, and
adjectives relatively late (Kemp et al, 2005). Our
goal in this experiment is to look for these specific
trends in the behaviour of our model. We thus simu-
late an experiment where a child uses a novel word?s
linguistic context to infer its syntactic category (e.g.,
Tomasello et al, 1997). For our experiment, we ran-
domly generate input frames with novel head words
using contexts associated with nouns, verbs, and ad-
jectives, then examine the model?s categorization in
each case. We expect that our model should approxi-
mate the developmental trends of children, who tend
to learn the category of ?noun? before ?verb,? and both
of these before ?adjective.?
5.1 Methods
We generate new input frames using the most com-
mon syntactic patterns in the training data. For each
of the noun, verb, and adjective categories (from the
gold standard), we collect the five most frequent PoS
sequences in which these are used, bounded by the
usual four-word context window. For example, the
Adjective set includes the sequence ?V Det Adj N
null?, where the sentence ends after the N. For each
of the three categories, we generate each of 500 input
frames by sampling one of the five PoS sequences,
weighted by frequency, then sampling words of the
right PoS from the lexicon, also weighted by fre-
quency. We replace the head word with a novel word,
forcing the model to use only the context for cluster-
ing. Since the context words are chosen at random,
most of the word sequences generated will be novel.
This makes the task more difficult, rather than sim-
ply sampling utterances from the corpus, where rep-
93
etitions are common. While a few of the sequences
may exist in the training data, we expect the model
to mostly use the underlying category information to
cluster the frames.
We intend to show that the model uses context to
find the right category for a novel word. To evaluate
the model?s behaviour, we let it categorize each of
the randomly generated frames. We score each frame
as follows: if the frame gets put into a new cluster,
it earns score zero. Otherwise, its score is the pro-
portion of frames in the chosen cluster matching the
correct part of speech (we use a PoS-tagged version
of the training corpus; for example, a noun frame put
into a cluster with 60% nouns would get 0.6). We re-
port the mean score for each of the noun, verb, and
adjective sets. Intuitively, the matching score indi-
cates how well the model recognizes that the given
contexts are similar to input it has seen before. If the
model clusters the novel word frame with others of
the right type, then it has formed a category for the
contextual information in that frame.
We use the full combination model (Eq. (7)) to
evaluate the learning rates of individual parts of
speech. We run the model on the training subset of
the evaluation corpus. After every 10,000 words of
input, we use the model to categorize the 1,500 con-
text frames with novel words (500 frames each for
noun, verb, and adjective). As in experiment 1, the
model does not record these categorizations.
5.2 Results
Figure 2 shows the mean matching scores for each
of the tested parts of speech. Recall that since the
frames each use a novel head word, a higher match-
ing score indicates that the model has learned to cor-
rectly recognize the contexts in the frames. This does
not necessarily mean that the model has learned sin-
gle, complete categories of ?noun,? ?verb,? and ?ad-
jective,? but it does show that when the head word
gives no information, the model can generalize based
on the contextual patterns alone. The model learns
to categorize novel nouns better than verbs until late
in training, which matches the trends seen in children.
Adjectives progress slowly, and show nearly no learn-
ing ability by the end of the trial. Again, this appears
to reflect natural behaviour in children, although the
effect we see here may simply be a result of the over-
all frequency of the PoS types. Over the entire corpus
(development and evaluation), 35.4% of the word to-
kens are nouns and 24.3% are verbs, but only 2.9%
are tagged as adjectives. The model, and similarly a
child, may need much more data to learn adjectives
than is available at this stage.
The scores in Figure 2 tend to fluctuate, partic-
ularly for the noun contexts. This fluctuation cor-
responds to periods of overgeneralization, followed
0 1 2 3 4 5 6
x 105
0
0.05
0.1
0.15
0.2
0.25
Training set size (words)
M
at
ch
in
g 
sc
or
e
Nouns
Verbs
Adjectives
Figure 2: Comparative learning trends of noun, verb,
and adjective patterns.
by recovery (also observed in children; see, e.g.,
Tomasello, 2000). When the model merges two clus-
ters, the contents of the resulting cluster can initially
be quite heterogeneous. Furthermore, the new cluster
is much larger, so it becomes a magnet for new cate-
gorizations. This results in overgeneralization errors,
giving the periodic drops seen in Figure 2. While our
formulation in Section 2.4 aims to prevent such er-
rors, they are likely to occur on occasion. Eventually,
the model recovers from these errors, and it is worth
noting that the fluctuations diminish over time. As the
model gradually improves with more input, the dom-
inant clusters become heavily entrenched, and incon-
sistent merges are less likely to occur.
6 Experiment 3: Disambiguation
The category structure of our model allows a single
word type to be a member of multiple categories. For
example, kiss could belong to a category of predom-
inantly noun usages (Can I have a kiss?) and also
to a category of verb usages (Kiss me!). As a result,
the model easily represents lexical ambiguity. In this
experiment, inspired by disambiguation work in psy-
cholinguistics (see, e.g., MacDonald, 1993), we ex-
amine the model?s ability to correctly disambiguate
category memberships.
6.1 Methods
Given a word that the model has previously seen as
various different parts of speech, we examine how
well the model can use that ambiguous word?s con-
text to determine its category in the current usage.
For example, by presenting the word kiss in sepa-
rate noun and verb contexts, we expect that the model
should categorize kiss as a noun, then as a verb, re-
spectively. We also wish to examine the effect of the
target word?s lexical bias, that is, the predominance of
a word type to be used as one category over another.
As with adults, if kiss is mainly used as a noun, we
expect the model to more accurately categorize the
94
N V N V N V N V N V N V
0
0.1
0.2
0.3
0.4
Po
S 
pr
op
or
tio
n 
in 
ch
os
en
 cl
us
te
rs
Nouns
Verbs
Context:
Noun only Noun biased Equibiased Verb biased Verb only Novel wordWord bias:
Figure 3: Syntactic category disambiguation. Shown are the proportions of nouns and verbs in the chosen
clusters for ambiguous words used in either noun (N) or verb (V) contexts.
word in a noun context than in a verb context.
We focus on noun/verb ambiguities. We artificially
generate input frames for noun and verb contexts as
in experiment 2, with the following exceptions. To
make the most use of the context information, we al-
low no null words in the input frames. We also ensure
that the contexts are distinctive enough to guide dis-
ambiguation. For each PoS sequence surrounding a
noun (e.g., ?V Det head Prep Det?), we ensure that
over 80% of the instances of that pattern in the cor-
pus are for nouns, and likewise for verbs.
We test the model?s disambiguation in six con-
ditions, with varying degrees of lexical bias. Un-
ambiguous (?noun/verb only?) conditions test words
seen in the corpus only as nouns or verbs (10 words
each). ?Biased? conditions test words with a clear
bias (15 with average 93% noun bias; 15 with aver-
age 84% verb bias). An ?equibiased? condition uses 4
words of approximately equal bias, and a novel word
condition provides an unbiased case.
For the six sets of test words, we measure the ef-
fect of placing each of these words in both noun and
verb contexts. That is, each word in each condition
was used as the head word in each of the 500 noun
and 500 verb disambiguating frames. For example,
we create 500 frames where book is used as a noun,
and 500 frames where it is used as a verb. We then
use the fully-trained ?combination? model (Eq. (7)) to
categorize each frame. Unlike in the previous experi-
ment, we do not let the model create new clusters. For
each frame, we choose the best-fitting existing clus-
ter, then examine that cluster?s contents. As in ex-
periment 2, we measure the proportions of each PoS
of the frames in this cluster. We then average these
measures over all tested frames in each condition.
6.2 Results
Figure 3 presents the measured PoS proportions for
each of the six conditions. For both the equibias and
novel word conditions, we see that the clusters cho-
sen for the noun context frames (labeled N) contain
more nouns than verbs, and the clusters chosen for
the verb context frames (V) contain more verbs than
nouns. This suggests that although the model?s past
experience with the head word is not sufficiently in-
formative, the model can use the word?s context to
disambiguate its category. In the ?unambiguous? and
the ?biased? conditions, the head words? lexical biases
are too strong for the model to overcome.
However, the results show a realistic effect of the
lexical bias. Note the contrasts from the ?noun only?
condition, to the ?noun biased? condition, to ?equibi-
ased? (and likewise for the verb biases). As the lex-
ical bias weakens, the counter-bias contexts (e.g., a
noun bias with a verb context) show a stronger ef-
fect on the chosen clusters. This is a realistic effect
of disambiguation seen in adults (MacDonald, 1993).
Strongly biased words are more difficult to categorize
in conflict with their bias than weakly biased words.
7 Related Work
Several existing computational models use distribu-
tional cues to find syntactic categories. Schu?tze
(1993) employs co-occurrence statistics for common
words, while Redington et al (1998) build word dis-
tributional profiles using corpus bigram counts. Clark
(2000) also builds distributional profiles, introducing
an iterative clustering method to better handle am-
biguity and rare words. Mintz (2003) shows that
even very simple three-word templates can effec-
tively define syntactic categories. Each of these mod-
els demonstrates that by using the kinds of simple in-
formation to which children are known to be sensi-
tive, syntactic categories are learnable. However, the
specific learning mechanisms they use, such as the
hierarchical clustering methods of Redington et al
(1998), are not intended to be cognitively plausible.
In contrast, Cartwright and Brent (1997) propose
95
an incremental model of syntactic category acquisi-
tion that uses a series of linguistic preferences to find
common patterns across sentence-length templates.
Their model presents an important incremental al-
gorithm which is very effective for discovering cat-
egories in artificial languages. However, the model?s
reliance on templates limits its applicability to tran-
scripts of actual spoken language data, which contain
high variability and noise.
Recent models that apply Bayesian approaches
to PoS tagging are not incremental and assume a
fixed number of tags (Goldwater and Griffiths, 2007;
Toutanova and Johnson, 2008). In syntactic cate-
gory acquisition, the true number of categories is un-
known, and must be inferred from the input.
8 Conclusions and Future Directions
We have developed a computational model of syn-
tactic category acquisition in children, and demon-
strated its behaviour on a corpus of naturalistic child-
directed data. The model is based on domain-general
properties of feature similarity, in contrast to earlier,
more linguistically-specific methods. The incremen-
tal nature of the algorithm contributes to a substantial
improvement in psychological plausibility over pre-
vious models of syntactic category learning. Further-
more, due to its probabilistic framework, our model
is robust to noise and variability in natural language.
Our model successfully uses a syntactic bootstrap-
ping mechanism to build on the distributional proper-
ties of words. Using its existing partial knowledge
of categories, the model applies a second level of
analysis to learn patterns in the input. By making
few assumptions about prior linguistic knowledge,
the model develops realistic syntactic categories from
the input data alone. The explicit bootstrapping com-
ponent improves the model?s ability to learn adult cat-
egories, and its learning trajectory resembles relevant
behaviours seen in children. Using the contextual
patterns of individual parts of speech, we show dif-
ferential learning rates across nouns, verbs, and ad-
jectives that mimic child development. We also show
an effect of a lexical bias in category disambiguation.
The algorithm is currently only implemented as an
incremental process. However, comparison with a
batch version of the algorithm, such as by using a
Gibbs sampler (Sanborn et al, 2006), would help us
further understand the effect of incrementality on lan-
guage fidelity.
While we have only examined the effects of learn-
ing categories from simple distributional information,
the feature-based framework of our model could eas-
ily be extended to include other sources of informa-
tion, such as morphological and phonological cues.
Furthermore, it would also be possible to include se-
mantic features, thereby allowing the model to draw
on correlations between semantic and syntactic cate-
gories in learning.
Acknowledgments
We thank Afra Alishahi for valuable discussions,
and the anonymous reviewers for their comments.
We gratefully acknowledge the financial support of
NSERC of Canada and the University of Toronto.
References
Alishahi, A. and S. Stevenson 2008. A computational
model for early argument structure acquisition. Cog-
nitive Science, 32(5).
Anderson, J. R. 1991. The adaptive nature of human cate-
gorization. Psychological Review, 98(3):409?429.
Cartwright, T. A. and M. R. Brent 1997. Syntactic catego-
rization in early language acquisition: formalizing the
role of distributional analysis. Cognition, 63:121?170.
Clark, A. 2000. Inducing syntactic categories by context
distribution clustering. In CoNLL2000, pp. 91?94.
Goldwater, S. and T. L. Griffiths 2007. A fully bayesian
approach to unsupervised part-of-speech tagging. In
Proc. of ACL2007, pp. 744?751.
Hubert, L. and P. Arabie 1985. Comparing partitions.
Journal of Classification, 2:193?218.
Kemp, N., E. Lieven, and M. Tomasello 2005. Young chil-
dren?s knowledge of the ?determiner? and ?adjective?
categories. J. Speech Lang. Hear. R., 48:592?609.
MacDonald, M. C. 1993. The interaction of lexical and
syntactic ambiguity. J. Mem. Lang., 32:692?715.
MacWhinney, B. 2000. The CHILDES Project: Tools for
analyzing talk, volume 2: The Database. Lawrence Erl-
baum, Mahwah, NJ, 3 edition.
Mintz, T. H. 2003. Frequent frames as a cue for gram-
matical categories in child directed speech. Cognition,
90:91?117.
Olguin, R. and M. Tomasello 1993. Twenty-five-month-
old children do not have a grammatical category of verb.
Cognitive Development, 8:245?272.
Onnis, L. and M. H. Christiansen 2005. New beginnings
and happy endings: psychological plausibility in com-
putational models of language acquisition. CogSci2005.
Redington, M., N. Chater, and S. Finch 1998. Distribu-
tional information: A powerful cue for acquiring syn-
tactic categories. Cognitive Science, 22(4):425?469.
Sanborn, A. N., T. L. Griffiths, and D. J. Navarro 2006. A
more rational model of categorization. CogSci2006.
Schu?tze, H. 1993. Part of speech induction from scratch.
In Proc. of ACL1993, pp. 251?258.
Theakston, A. L., E. V. Lieven, J. M. Pine, and C. F. Row-
land 2001. The role of performance limitations in the
acquisition of verb-argument structure: an alternative
account. J. Child Lang., 28:127?152.
Tomasello, M. 2000. Do young children have adult syn-
tactic competence? Cognition, 74:209?253.
Tomasello, M., N. Akhtar, K. Dodson, and L. Rekau 1997.
Differential productivity in young children?s use of
nouns and verbs. J. Child Lang., 24:373?387.
Toutanova, K. and M. Johnson 2008. A Bayesian LDA-
based model for semi-supervised part-of-speech tag-
ging. In NIPS2008.
96
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 61?69,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
No sentence is too confusing to ignore
Paul Cook
Department of Computer Science
University of Toronto
Toronto, Canada
pcook@cs.toronto.edu
Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
suzanne@cs.toronto.edu
Abstract
We consider sentences of the form No
X is too Y to Z, in which X is a noun
phrase, Y is an adjective phrase, and Z
is a verb phrase. Such constructions are
ambiguous, with two possible (and oppo-
site!) interpretations, roughly meaning ei-
ther that ?Every X Zs?, or that ?No X Zs?.
The interpretations have been noted to de-
pend on semantic and pragmatic factors.
We show here that automatic disambigua-
tion of this pragmatically complex con-
struction can be largely achieved by us-
ing features of the lexical semantic prop-
erties of the verb (i.e., Z) participating in
the construction. We discuss our experi-
mental findings in the context of construc-
tion grammar, which suggests a possible
account of this phenomenon.
1 No noun is too adjective to verb
Consider the following two sentences:
(1) No interest is too narrow to deserve its own
newsletter.
(2) No item is too minor to escape his attention.
Each of these sentences has the form of No X is too
Y to Z, where X, Y, and Z are a noun phrase, ad-
jective phrase, and verb phrase, respectively. Sen-
tence (1) is generally taken to mean that every in-
terest deserves its own newsletter, regardless of
how narrow it is. On the other hand, (2) is typi-
cally interpreted as meaning that no item escapes
his attention, regardless of how minor it is. That
is, sentences with the identical form of No X is too
Y to Z either can mean that ?every X Zs?, or can
mean the opposite?that ?no X Zs?!1
1Note that in examples (1) and (2), the nouns interest and
item are the subjects of the verbs deserve and escape, respec-
This ?verbal illusion? (Wason and Reich, 1979),
so-called because there are two opposite inter-
pretations for the very same structure, is of in-
terest to us for two reasons. First, the con-
tradictory nature of the possible meanings has
been explained in terms of pragmatic factors con-
cerning the relevant presuppositions of the sen-
tences. According to Wason and Reich (1979)
(as explained in more detail below), sentences
such as (2) are actually nonsensical, but people
coerce them into a sensible reading by revers-
ing the interpretation. One of our goals in this
work is to explore whether computational lin-
guistic techniques?specifically automatic corpus
analysis drawing on lexical resources?can help
to elucidate the factors influencing interpretation
of such sentences across a collection of actual us-
ages.
The second reason for our interest in this con-
struction is that it illustrates a complex ambigu-
ity that can cause difficulty for natural language
processing applications that seek to semantically
interpret text. Faced with the above two sen-
tences, a parsing system (in the absence of spe-
cific knowledge of this construction) will presum-
ably find the exact same structure for each, giv-
ing no basis on which to determine the correct
meaning from the parse. (Unsurprisingly, when
we run the C&C Parser (Curran et al, 2007) on (1)
and (2) it assigns the same structure to each sen-
tence.) Our second goal in this work is thus to ex-
plore whether increased linguistic understanding
of this phenomenon could be used to disambiguate
such examples automatically. Specifically, we use
this construction as an example of the kind of
difficulties faced in semantic interpretation when
meaning may be determined by pragmatic or other
extra-syntactic factors, in order to explore whether
tively. In this construction the noun can also be the object of
the verb, as in the title of this paper which claims no sentence
can/should be ignored.
61
lexical semantic features can be used as cues to
resolving pragmatic ambiguity when a complex
semantico-pragmatic model is not feasible.
In the remainder of this paper, we present the
first computational study of the No X is too Y to
Z phenomenon, which attempts to automatically
determine the meaning of instances of this seman-
tically and pragmatically complex construction. In
Section 2 we present previous analyses of this
construction, and our hypothesis. In Section 3,
we describe the creation of a dataset of instances
that verifies that both interpretations (?every? and
?no?) indeed occur in corpora. We then analyze
the human annotations in this dataset in more de-
tail in Section 4. In Section 5, we present the fea-
ture model we use to describe the instances, which
taps into the lexical semantics and polarity of the
constituents. In Section 6, we describe machine
learning experiments and classification results that
support our hypothesis that the interpretation of
this construction largely depends on the semantics
of its component verb. In Section 7 we suggest that
our results support an analysis of this phenomenon
within construction grammar, and point to some
future directions in our research in Section 8.
2 Background and our proposal
The No X is too Y to Z construction was investi-
gated by Wason and Reich (1979), and discussed
more recently by Pullum (2004) and Liberman
(2009a,b). Here we highlight some of the most
important properties of this complex phenomenon.
Our presentation owes much to the lucid discus-
sion and clarification of this topic, and of the work
of Wason and Reich specifically, by Liberman.
Wason and Reich argue that the compositional
interpretation of sentences of the form of (1) and
(2) is ?every X Zs?. Intuitively, this can be under-
stood by considering a sentence identical to sen-
tence (1), but without a negative subject: This in-
terest is too narrow to deserve its own newslet-
ter, which means that ?this interest is so narrow
that it does not deserve a newsletter?. This ex-
ample indicates that the meaning of too narrow
to deserve its own newsletter is ?so narrow that
it does not deserve a newsletter?. When this neg-
ative ?too? assertion is compositionally combined
with the No interest subject of sentence (1), it re-
sults in a meaning with two negatives: ?No inter-
est is so narrow that it does not deserve a newslet-
ter?, or simply, ?Every interest deserves a newslet-
ter?. Wason and Reich note that in sentences such
as (1), the compositional ?every? interpretation is
consistent with common beliefs about the world,
and thus refer to such sentences as ?pragmatic?.
By contrast, the compositional interpretation of
sentences such as (2) does not correspond to our
common sense beliefs. Consider an analogous
(non-negative subject) sentence to sentence (2)?
i.e., This item is too minor to escape his attention.
It is nonsensical that ?This item is so minor that
it does not escape his attention?, since being more
?minor? entails more likelihood of escaping atten-
tion, not less. The compositional interpretation of
(2) is similarly nonsensical?i.e., that ?No item
is so minor that it does not escape his attention?;
Such sentences are thus termed ?non-pragmatic?
by Wason and Reich, who argue that the com-
plexity of the non-pragmatic sentences?arising in
part due to the number of negations they contain?
causes the listener or reader to misconstrue them.
According to their reasoning, listeners choose an
interpretation that is consistent with their beliefs
about the world?namely that ?no X Zs?, in this
case that ?No item escapes his attention??instead
of the compositional interpretation (?Every item
escapes his attention?).
While Wason and Reich focus on the compo-
sitional semantics and pragmatics of these sen-
tences, they also note that the non-pragmatic ex-
amples typically use a verb that itself has some
aspect of negation, such as ignore, miss, and over-
look. This property is also pointed out by Pullum
(2004), who notes that avoid in his example of
the construction means ?manage to not do? some-
thing. Building on this observation, we hypothe-
size that lexical properties of the component con-
stituents of this construction, particularly the verb,
can be important cues to its semantico-pragmatic
interpretation. Specifically, we hypothesize that
the pragmatic (?every? interpretation) and non-
pragmatic (?no? interpretation) sentences will tend
to involve verbs with different semantics. Given
that verbs of different semantic classes have differ-
ent selectional preferences, we also expect to see
the ?every? and ?no? sentences associated with se-
mantically different nouns and adjectives.
3 Dataset
3.1 Extraction
To create a dataset of usages of the construction
no NP is too AP to VP?referred to as the tar-
62
get construction?we use two corpora: the British
National Corpus (Burnard, 2000), an approxi-
mately one hundred million word corpus of late-
twentieth century British English, and The New
York Times Annotated Corpus (Sandhaus, 2008),
approximately one billion words of non-newswire
text from the New York Times from the years
1987?2006. We extract all sentences in these cor-
pora containing the sequence of strings no, is too,
and to separated by one or more words. We then
manually filter all sentences that do not have no
NP as the subject of is too, or that do not have to
VP as an argument of is too. After removing dupli-
cates, this results in 170 sentences. We randomly
select 20 of these sentences for development data,
leaving 150 sentences for testing.
Although we find only 170 examples of the
target construction in 1.1 billion words of text,
note that our extraction process is quite strict and
misses some relevant usages. For example, we do
not extract sentences of the form Nothing is too Y
to Z in which the subject NP does not contain the
word no. Nor do we extract usages of the related
construction No X is too Y for Z, where Z is an NP
related to a verb, as in No interest is too narrow
for attention. (We would only extract the latter if
there were an infinitive verb embedded in or fol-
lowing the NP.) In the present study we limit our
consideration to sentences of the form discussed
by Wason and Reich (1979), but intend to con-
sider related constructions such as these?which
appear to exhibit the same ambiguity as the target
construction?in the future.
We next manually identify the noun, adjective,
and verb that participate in the target construction
in each sentence. Although this could be done au-
tomatically using a parser (e.g., Collins, 2003) or
chunker (e.g., Abney, 1991), here we want to en-
sure error-free identification. We also note a num-
ber of sentences containing co-ordination, such as
in the following example.
(3) These days, no topic is too recent or
specialized to disqualify it from museum
apotheosis.
This sentence contains two instances of the tar-
get construction: one corresponding to the noun-
adjective-verb triple topic, recent, disqualify, and
the other to the triple topic, specialized, disqual-
ify. In general, we consider each unique noun-
adjective-verb triple participating in the target con-
struction as a separate instance.
3.2 Annotation
We used Amazon Mechanical Turk (AMT,
https://www.mturk.com/) to obtain judge-
ments as to the correct interpretation of each in-
stance of the target construction in both the devel-
opment and testing datasets. For each instance, we
generated two paraphrases, one corresponding to
each of the interpretations discussed in Section 1.
We then presented the given instance of the target
construction along with its two paraphrases to an-
notators through AMT, as shown in Table 1. In
generating the paraphrases, one of the authors se-
lected the most appropriate paraphrase, in their
judgement, where can in the paraphrases in Ta-
ble 1 was selected from can, should, will, and ?.
Note that the paraphrases do not contain the ad-
jective from the target construction. In the case of
multiple instances of the target construction with
differing adjectives but the same noun and verb,
we only solicited judgements for one instance, and
used these judgements for the other instances. In
our dataset we observe that all instances obtained
from the same sentence which differ only with re-
spect to their noun or verb have the same inter-
pretation. We therefore believe that instances with
the same noun and verb but a different adjective
are unlikely to differ in their interpretation.
Instructions:
? Read the sentence below.
? Based on your interpretation of that sen-
tence, select the answer that most closely
matches your interpretation.
? Select ?I don?t know? if neither answer is
close to your interpretation, or if you are
really unsure.
That success was accomplished in large part to
tight control on costs , and no cost is too small
to be scrutinized .
? Every cost can be scrutinized.
? No cost can be scrutinized.
? I don?t know.
Enter any feedback you have about this HIT. We
greatly appreciate you taking the time to do so.
Table 1: A sample of the Amazon Mechanical
Turk annotation task.
63
We also allowed the judges to optionally enter
any feedback about the annotation task which in
some cases?discussed in the following section?
was useful in determining whether the judges
found a particular instance difficult to annotate.2
For each instance of the target construction we
obtained three judgements from unique workers
on AMT. For approximately 80% of the items,
the judgements were unanimous. In the remaining
cases we solicited four additional judgements, and
used the majority judgement. We paid $0.05 per
judgement; the average time spent on each annota-
tion was approximately twenty seconds, resulting
in an average hourly wage of about $10.
The development data was also annotated by
three native English speaking experts (compu-
tational linguists with extensive linguistic back-
ground, two of whom are also authors of this pa-
per). The inter-annotator agreement among these
judges is very high, with pairwise observed agree-
ments of 1.00, 0.90, and 0.90, and corresponding
unweighted Kappa scores of 1.00, 0.79, and 0.79.
The majority judgements of these annotators are
the same as those obtained from AMT on the de-
velopment data, giving us confidence in the reli-
ability of the AMT judgements. These findings
are consistent with those of Snow et al (2008) in
showing that AMT judgements can be as reliable
as those of expert judges.
Finally, we remove a small number of items
from the testing dataset which were difficult to
paraphrase due to ellipsis of the verb participating
in the target construction, or an extra negation in
the verb phrase. We further remove one sentence
because we believe the paraphrases we provided
are in fact misleading. The number of sentences
and of instances (i.e., noun-verb-adjective triples)
of the target construction in the development and
testing datasets is given in Table 2. 160 of the 199
testing instances (80%) have the ?every? interpre-
tation, with the remainder having the ?no? inter-
pretation.
4 Analysis of annotation
We now more closely examine the annotations ob-
tained from AMT to better determine the extent to
2In other cases the comments were more humourous. In
response to the following sentence If you?ve ever yearned
to live on Sesame Street, where no problem is too big to be
solved by a not-too-big slice of strawberry-rhubarb pie, this
is the spot for you, one judge told us her preferred types of
pie.
Dataset # sentences # instances
Development 20 33
Test 140 199
Table 2: The number of sentences containing the
target construction, and the number of resulting in-
stances.
which they are reliable. We also consider specific
instances of the target construction that are judged
inconsistently to establish some of the causes of
disagreement.
One of the three experts who annotated the de-
velopment items (discussed in Section 3.2) also
annotated twenty items selected at random from
the testing data. In this case two instances are
judged differently than the majority judgement ob-
tained from AMT. These instances are given below
with the noun, adjective and verb in the target con-
struction underlined.
(4) When it comes to the clash of candidates on
national television, no detail, it seems, is too
minor for negotiation, no risk too small to
eliminate.
(5) Lectures by big-name Wall Street felons will
show why no swindler is too big to beat the
rap by peaching on small-timers.
For sentence (4), the AMT judgements were unan-
imously for the ?no? interpretation whereas the
expert annotator chose the ?every? interpretation.
We are uncertain as to the reason for this disagree-
ment, but are convinced that the ?every? interpre-
tation is the intended one.
In the case of sentence (5), the AMT judge-
ments were split four?three for the ?every? and
?no? interpretations, respectively, while the ex-
pert annotator chose the ?no? interpretation. For
this sentence the provided paraphrases were Ev-
ery swindler can beat the rap and No swindler
can beat the rap. If attention in the sentence
is restricted to the target construction?i.e., no
swindler is too big to beat the rap by peaching
on small-timers?either of the ?no? and ?every?
interpretations is possible. That is, this clause
alone can mean that ?no swindler is ?big? enough
to be able to beat the rap? (the ?no? interpreta-
tion), or that ?no swindler is ?big? enough that they
64
are above peaching on small-timers? (or in other
words, ?every swindler is able to beat the rap by
peaching on small-timers?, the ?every? interpreta-
tion). However, the intention of the sentence as the
?no? interpretation is clear from the referral in the
main clause to big-name Wall Street felons, which
implies that ?big? swindlers have not beaten the
rap. Since the AMT annotators may not be devot-
ing a large amount of attention to the task, they
may focus only on the target construction and not
the preliminary disambiguating material. In this
event, they may be choosing between the ?every?
and ?no? interpretations based on how cynical they
are of the ability (or lack thereof) of the American
legal system to punish Wall Street criminals.
We also examine a small number of examples
in the testing set which do not receive a clear
majority judgement from AMT. For this analysis
we consider items for which the difference in the
number of judgements for each of the ?every? and
the ?no? interpretations is one or less This gives
four instances of the target construction, one of
which we have already discussed above, example
(5); the others are presented below, again with the
noun, adjective, and verb participating in the target
construction underlined:
(6) Where are our priorities when we so
carefully weigh costs and medical efficacy in
deciding to offer a medical lifeline to the
elderly, yet no amount of money is too great
to spend on the debatable paths we?ve taken
in our war against terror?
(7) No neighborhood is too remote to diminish
Mr. Levine?s determination to discover and
announce some previously unheralded treat.
(8) No one is too remote anymore to be
concerned about style, Ms. Hansen
suggested.
In example (6) the author is using the target con-
struction to express somebody else?s viewpoint
that ?any amount should be spent on the war
against terror?. Therefore the literal reading of
the target construction appears to be the ?every?
interpretation. However, this construction is be-
ing used rhetorically (as part of the overall sen-
tence) to express the author?s belief that ?too much
money is being spent on the war against terror?,
which is close in meaning to the ?no? interpreta-
tion. It appears that the annotators are split be-
tween these two readings. For sentence (7) the
atypicality of neighbourhood as the subject of di-
minish may make this instance particularly diffi-
cult for the judges. Sentence (8) appears to us to be
a clear example of the ?every? interpretation. The
paraphrases for this usage are ?Everyone should
be concerned about style? and ?No one should be
concerned about style?. In this case it is possible
that the judges are biased by their beliefs about
whether one should be concerned about style, and
that this is giving rise to the lack of agreement.
These examples illustrate that some of these us-
ages are clearly complex for people to annotate.
Such complex examples may require more context
to be annotated with confidence.
5 Model
To test our hypothesis that the interaction of the se-
mantics of the noun, adjective, and verb in the tar-
get construction contributes to its pragmatic inter-
pretation, we represent each instance in our dataset
as a vector of features that capture aspects of the
semantics of its component words.
WordNet To tap into general lexical semantic
properties of the words in the construction, we
use features that draw on the semantic classes of
words in WordNet (Fellbaum, 1998). These bi-
nary features each represent a synset in WordNet,
and are turned on or off for the component words
(the noun, adjective, and verb) in each instance
of the target construction. A synset feature is on
for a word if the synset occurs on the path from
all senses of the word to the root, and off other-
wise. We use WordNet version 3.0 accessed using
NLTK version 2.0 (Bird et al, 2009).
Polarity Because of the observation that the
verb in the target construction, in particular, has
some property of negativity in the ?no? interpre-
tation, we also use features representing the se-
mantic polarity of the noun, adjective, and verb
in each instance. The features are tertiary, repre-
senting positive, neutral, or negative polarity. We
obtain polarity information from the subjectivity
lexicon provided by Wilson et al (2005), and con-
sider words to be neutral if they have both positive
and negative polarity, or are not in the lexicon.
6 Experimental results
6.1 Experimental setup
To evaluate our model we conduct a 5-fold cross-
validation experiment using the items in the test-
65
ing dataset. When partitioning the items in the
testing dataset into the five parts necessary for the
cross-validation experiment, we ensure that all the
instances of the target construction from a single
sentence are in the same part. This ensures that
no instance used for training is from the same sen-
tence as an instance used for testing. We further
ensure that the proportion of items in each class is
roughly the same in each split.
For each of the five runs, we linearly scale the
training data to be in the range [?1, 1], and ap-
ply the same transformation to the testing data.
We train a support vector machine (LIBSVM ver-
sion 2.9, Chang and Lin, 2001) with a radial ba-
sis function kernel on the training portion in each
run, setting the cost and gamma parameters using
cross-validation on just the training portion, and
then test the classifier on the testing portion for
that run using the same parameter settings. We
micro-average the accuracy obtained on each of
the five runs. Finally, we repeat each 5-fold cross-
validation experiment five times, with five random
splits, and report the average accuracy over these
trials.
6.2 Results
Results for experiments using various subsets of
the features are presented in Table 3. We re-
strict the component word?the noun, adjective, or
verb?for which we extract features to those listed
in column ?Word?, and extract only the features
given in column ?Features? (WordNet, polarity, or
all). The majority baseline is 80%, corresponding
to always selecting the ?every? interpretation. Ac-
curacies shown in boldface are significantly better
than the majority class baseline using a paired t-
test. (In all cases where the difference is signifi-
cant, we obtain p ? 0.01.)
We first consider the results using features ex-
tracted only for the noun, adjective, or verb indi-
vidually, using all features. The best accuracy in
this group of experiments, 87%, is achieved using
the verb features, and is significantly higher than
the majority baseline. On the other hand, the clas-
sifiers trained on the noun and adjective features
individually perform no better than the baseline.
These results support our hypothesis that lexical
semantic properties of the component verb in the
No X is too Y to Z construction do indeed play
an important role in determining its interpretation.
Although we proposed that selectional constraints
from the verb would also lead to differing seman-
tics of the nouns and adjectives in the two interpre-
tations, our WordNet features are likely too sim-
plistic to capture this effect, if it does hold. Before
ruling out the semantic contribution of these words
to the interpretation, we need to explore whether
a more sophisticated model of selectional prefer-
ences, as in Ciaramita and Johnson (2000) or Clark
and Weir (2002), yields more informative features
for the noun and adjective.
Experimental setup % accuracy
Word Features
Noun All 80
Adjective All 80
Verb All 87
All WordNet 88
All Polarity 80
All All 88
Majority baseline 80
Table 3: % accuracy on testing data for each exper-
imental condition and the majority baseline. Ac-
curacies in boldface are statistically significantly
different from the baseline.
We now consider the results using the WordNet
and polarity features individually, but extracted for
all three component words. The WordNet features
perform as well as the best results using all fea-
tures for all three words, which gives further sup-
port to our hypothesis that the semantics of the
components of the target construction are related
to its interpretation. The polarity features perform
poorly. This is perhaps unsurprising as polarity is
a poor approximation to the property of ?negativ-
ity? that we are attempting to capture. Moreover,
many of the nouns, adjectives, and verbs in our
dataset either have neutral polarity or are not in
the polarity lexicon, and therefore the polarity fea-
tures are not very discriminative. In future work,
we plan to examine the WordNet classes of the
verbs that occur in the ?no? interpretation to try to
more precisely characterize the property of nega-
tivity that these verbs tend to have.
6.3 Error analysis
To better understand the errors our classifier is
making, we examine the specific instances which
are classified incorrectly. Here we focus on the
experiment using all features for all three com-
ponent words. There are 23 instances which are
66
consistently mis-classified in all runs of the exper-
iment. According to the AMT judgements, each of
these instances corresponds to the ?no? interpreta-
tion. These errors reflect the bias of the classifier
towards the more frequent class, the ?every? inter-
pretation.
We further note that two of the instances dis-
cussed in Section 4?examples (4) and (6)?are
among those instances consistently classified in-
correctly. The majority judgement from AMT for
both of these instances is the ?no? interpretation,
while in our assessment they are in fact the ?ev-
ery? interpretation. We are therefore not surprised
to see these items ?mis-classified? as ?every?.
Example (8) was incorrectly classified in one
trial. In this case we agree with the gold-standard
label obtained from AMT in judging this instance
as the ?every? interpretation; nevertheless, this
does appear to be a difficult instance given the low
agreement observed for the AMT judgements.
It is interesting that no items with an ?every? in-
terpretation are consistently misclassified. In the
context of our overall results showing the impact
of the verb features on performance, we conclude
that the ?no? interpretation arises due to particular
lexical semantic properties of certain verbs. We
suspect then that the consistent errors on the 21
truly misclassified expressions (23 minus the 2 in-
stances discussed above that we believe to be an-
notated incorrectly) are due to sparse data. That
is, if it is indeed the verb that plays a major role in
leading to a ?no? interpretation, there may simply
be insufficient numbers of such verbs for training
a supervised model in a dataset with only 39 ex-
amples of those usages.
7 Discussion
We have presented the first computational study of
the semantically and pragmatically complex con-
struction No X is too Y to Z. We have developed
a computational model that automatically disam-
biguates the construction with an accuracy of 88%,
reducing the error-rate over the majority-baseline
by 40%. The model uses features that tap into the
lexical semantics of the component words partic-
ipating in the construction, particularly the verb.
These results demonstrate that lexical properties
can be successful in resolving an ambiguity pre-
viously thought to depend on complex pragmatic
inference over presuppositions (as in Wason and
Reich (1979)).
These results can be usefully situated within
the context of linguistic and psycholinguistic work
on semantic interpretation processing. Beginning
around 20 years ago, work in modeling of human
semantic preferences has focused on the extent to
which properties of lexical items influence the in-
terpretation of various linguistic ambiguities (e.g.,
Trueswell and Tanenhaus, 1994). While semantic
context and plausibility are also proposed to play
a role in human interpretation of ambiguous sen-
tences (e.g., Crain and Steedman, 1985; Altmann
and Steedman, 1988), it has been pointed out that
it would be difficult to ?operationalize? the com-
plex interactions of presuppositional factors with
real-world knowledge in a precise algorithm for
disambiguation (Jurafsky, 1996). Although not in-
tended as proposing a cognitive model, the work
here can be seen as connected to these lines of re-
search, in investigating the extent to which lexical
factors can be used as proxies to more ?hidden?
features that underlie the appropriate interpreta-
tion of a pragmatically complex construction.
Moreover, as in the approach of Jurafsky
(1996), the phenomenon we investigate here may
be best considered within a constructional analy-
sis (e.g., Langacker, 1987), in which both the syn-
tactic construction and the particular lexical items
contribute to the determination of the meaning of a
usage. We suggest that a clause of the form No X is
too Y to Z might be the (identical) surface expres-
sion of two underlying constructions?one with
the ?every? interpretation and one with the ?no?
interpretation?which place differing constraints
on the semantics of the verb. (E.g., in the ?no?
interpretation, the verb typically has some ?neg-
ative? semantic property, as noted in Section 2.)
Looked at from the other perspective, the lexical
semantic properties of the verb might determine
which No X is too Y to Z construction (and associ-
ated interpretation) it is compatible with. Our re-
sults support this view, by showing that semantic
classes of verbs have predictive value in selecting
the correct interpretation.
Note that such a constructional analysis of
this phenomenon assumes that both interpretations
of these sentences are linguistically valid, given
the appropriate lexical instantiation. This stands
in contrast to the analysis of Wason and Reich
(1979), which presumes that people are apply-
ing some higher-level reasoning to ?correct? an
ill-formed statement in the case of the ?no? in-
67
terpretation. While such extra-grammatical infer-
ence may play a role in support of language under-
standing when people are faced with noisy data, it
seems unlikely to us that a construction that is used
quite readily and with a predictable interpretation
is nonsensical according to rules of grammar. Our
results point to an alternative linguistic analysis,
one whose further development may also help to
improve automatic disambiguation of instances of
No X is too Y to Z. In the next section, we discuss
directions for future work that could elaborate on
these preliminary findings.
8 Future Work
One limitation of this study is that the dataset used
is rather small, consisting of just 199 instances
of the target construction. As discussed in Sec-
tion 3.1, the extraction process we use to obtain
our experimental items has low recall; in particular
it misses variants of the target construction such as
Nothing is too Y to Z and No X is too Y for Z. In
the future we intend to expand our dataset by ex-
tracting such usages. Furthermore, the data used
in the present study is primarily taken from news
text. While we do not adopt the view of some that
usages of the target construction having the ?no?
interpretation are errors, it could be the case that
such usages are more frequent in less formal text.
In the future we also intend to extract usages of
the target construction from datasets of less formal
text, such as blogs (e.g., Burton et al, 2009).
Constructions other than No X is too Y to Z ex-
hibit a similar ambiguity. For example, the con-
struction X didn?t wait to Y is ambiguous between
?X did Y right away? and ?X didn?t do Y at all?
(Karttunen, 2007). In the future we would like to
extend our study to consider more such construc-
tions which are ambiguous due to the interpreta-
tion of negation.
In Section 4 we note that for some instances the
complexity of the sentences containing the target
construction may make it difficult for the anno-
tators to judge the meaning of the target. In the
future we intend to present simplified versions of
these sentences?which retain the noun, adjective,
and verb from the target construction in the orig-
inal sentence?to the judges to avoid this issue.
Such an approach will also help us to focus more
clearly on observable lexical semantic effects.
We are particularly interested in further explor-
ing the hypothesis that it is the semantics of the
component verb that gives rise to the meaning of
the target construction. Recall Pullum?s (2004)
observation that the verb in the ?no? interpretation
involves explicitly not acting. Using this intuition,
we have informally observed that it is largely pos-
sible to (manually) predict the interpretation of the
target construction knowing only the component
verb. We are interested in establishing the extent to
which this observation holds, and precisely which
aspects of a verb?s meaning give rise to the inter-
pretation of the target construction.
Our current model of the semantics of the target
construction does not capture Wason and Reich?s
(1979) observation that the compositional mean-
ing of instances having the ?no? interpretation is
non-pragmatic. While we do not adopt their view
that these usages are somehow ?errors?, we do
think that their observation can indicate other pos-
sible lexical semantic properties that may help to
identify the correct interpretation. Taking the clas-
sic example from Wason and Reich, no head in-
jury is too trivial to ignore, one clue to the ?no?
interpretation is that generally a head injury is not
something that is ignored. On the other hand, con-
sidering Wason and Reich?s example no missile is
too small to ban, it is widely believed that missiles
should be banned. We would like to add features
that capture this knowledge to our model.
In preliminary experiments we have used co-
occurrence information as an approximation to
this knowledge. (For example, we would expect
that head injury would tend to co-occur less with
ignore than with antonymous verbs such as treat
or address.) Although our early results using
co-occurrence features do not indicate that they
are an improvement over the other features con-
sidered (WordNet and polarity), it may also be
the case that our present formulation of these co-
occurrence features does not effectively capture
the intended knowledge. In the future we plan
to further consider such features, especially those
that model the selectional preferences of the verb
participating in the target construction.
These several strands of future work?
increasing the size of the dataset, improving the
quality of annotation, and exploring additional
features in our computational model?will en-
able us to extend our linguistic analysis of this
interesting phenomenon, as well as to improve
performance on automatic disambiguation of this
complex construction.
68
Acknowledgments
We thank Magali Boizot-Roche and Timothy
Fowler for their help in preparing the data for this
study. This research was financially supported by
the Natural Sciences and Engineering Research
Council of Canada and the University of Toronto.
References
Steven Abney. 1991. Parsing by chunks. In Robert
Berwick, Steven Abney, and Carol Tenny, ed-
itors, Principle-Based Parsing: Computation
and Psycholinguistics, pages 257?278. Kluwer
Academic Publishers.
Gerry T. M. Altmann and Mark Steedman. 1988.
Interaction with context during human sentence
processing. Cognition, 30(3):191?238.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with
Python. O?Reilly Media Inc.
Lou Burnard. 2000. The British National Cor-
pus Users Reference Guide. Oxford University
Computing Services.
Kevin Burton, Akshay Java, and Ian Soboroff.
2009. The ICWSM 2009 Spinn3r Dataset. In
Proc. of the Third International Conference on
Weblogs and Social Media. San Jose, CA.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.
ntu.edu.tw/
?
cjlin/libsvm.
Massimiliano Ciaramita and Mark Johnson. 2000.
Explaining away ambiguity: Learning verb se-
lectional preference with Bayesian networks. In
Proceedings of the 18th International Confer-
ence on Computational Linguistics (COLING
2000), pages 187?193. Saarbru?cken, Germany.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hier-
archy. Computational Linguistics, 28(2):187?
206.
Michael Collins. 2003. Head-driven statistical
models for natural language parsing. Compu-
tational Linguistics, 29(4):589?637.
Stephen Crain and Mark Steedman. 1985. On
not being led up the garden path: The use
of context by the psychological syntax pro-
cessor. In David R. Dowty, Lauri Karttunen,
and Arnold M. Zwicky, editors, Natural lan-
guage parsing: Psychological, computational,
and theoretical perspectives, pages 320?358.
Cambridge University Press, Cambridge.
James Curran, Stephen Clark, and Johan Bos.
2007. Linguistically motivated large-scale NLP
with C&C and Boxer. In Proceedings of the
45th Annual Meeting of the Association for
Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions,
pages 33?36. Prague, Czech Republic.
Christiane Fellbaum, editor. 1998. Wordnet: An
Electronic Lexical Database. Bradford Books.
Daniel Jurafsky. 1996. A probabilistic model of
lexical and syntactic access and disambigua-
tion. Cognitive Science, 20(2):137?194.
Lauri Karttunen. 2007. Word play. Computational
Linguistics, 33(4):443?467.
Ronald W. Langacker. 1987. Foundations of
Cognitive Grammar: Theoretical Prerequisites,
volume 1. Stanford University Press, Stanford.
Mark Liberman. 2009a. No detail too small.
Retrieved 9 February 2010 from http://
languagelog.ldc.upenn.edu/nll/.
Mark Liberman. 2009b. No wug is too
dax to be zonged. Retrieved 9 February
2010 from http://languagelog.ldc.
upenn.edu/nll/.
Geoffrey K. Pullum. 2004. Too complex to
avoid judgment? Retrieved 7 April 2010 from
http://itre.cis.upenn.edu/
?
myl/
languagelog/.
Evan Sandhaus. 2008. The New York Times An-
notated Corpus. Linguistic Data Consortium,
Philadelphia, PA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky,
and Andrew Y. Ng. 2008. Cheap and fast ? But
is it good? Evaluating non-expert annotations
for natural language tasks. In Proceedings of
EMNLP-2008, pages 254?263. Honolulu, HI.
John Trueswell and Michael J. Tanenhaus. 1994.
Toward a lexicalist framework for constraint-
based syntactic ambiguity resolution. In
Charles Clifton, Lyn Frazier, and Keith Rayner,
editors, Perspectives on Sentence Processing,
pages 155?179. Lawrence Erlbaum, Hillsdale,
NJ.
Peter Wason and Shuli Reich. 1979. A verbal il-
lusion. The Quarterly Journal of Experimental
Psychology, 31(4):591?597.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings
of HLT/EMNLP-2005, pages 347?354. Vancou-
ver, Canada.
69
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 72?80,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Incorporating Coercive Constructions into a Verb Lexicon 
Claire Bonial*, Susan Windisch Brown*, Jena D. Hwang*, Christopher Parisien**, 
Martha Palmer* and Suzanne Stevenson** 
*Department of Linguistics, University of Colorado at Boulder 
**Department of Computer Science, University of Toronto 
{Claire.Bonial, Susan.Brown, hwangd, Martha.Palmer}@colorado.edu 
{chris, suzanne}@cs.toronto.edu 
 
 
Abstract 
We take the first steps towards augmenting a lexical 
resource, VerbNet, with probabilistic information 
about coercive constructions. We focus on CAUSED-
MOTION as an example construction occurring with 
verbs for which it is a typical usage or for which it 
must be interpreted as extending the event semantics 
through coercion, which occurs productively and adds 
substantially to the relational semantics of a verb. 
However, through annotation we find that VerbNet 
fails to accurately capture all usages of the 
construction. We use unsupervised methods to 
estimate  probabilistic measures from corpus data for 
predicting usage of the construction across verb 
classes in the lexicon and evaluate against VerbNet. 
We discuss how these methods will form the basis for 
enhancements for VerbNet supporting more accurate 
analysis of the relational semantics of a verb across 
productive usages. 
1 Introduction  
Automatic semantic analysis has been very successful 
when taking a supervised learning approach on data 
labeled with sense tags and semantic roles (e.g., see 
M?rquez et al, 2008). Underlying these recent successes 
are lexical resources, such as PropBank (Palmer et al, 
2005), VerbNet (Kipper et al, 2008), and FrameNet 
(Baker et al, 1998; Fillmore et al, 2002), which encode 
the relational semantics of numerous lexical items, 
especially verbs. However, because authors and speakers 
use verbs productively in previously unseen ways, 
semantic analysis systems must not be limited to direct 
extrapolation from previously seen usages licensed by 
static lexical resources (cf. Pustejovsky & Jezek, 2008). 
To achieve more accurate semantic analyses, we must 
augment such resources with knowledge of the 
extensibility of verbs. 
Central to verb extensibility is the process of semantic 
and syntactic coercion. Coercion allows a verb to be used 
in ?atypical? contexts that extend its relational semantics, 
thereby enabling expression of a novel concept, or simply 
more fluid expression of a complex concept. For 
example, consider a strictly intransitive action verb such 
as blink. This verb may instead be used in a construction 
with an object, as in She blinked the snow off her lashes, 
leading to an interpretation of the verb in which the object 
is causally affected and changes location (the CAUSED-
MOTION construction; Goldberg, 1995). This type of 
constructional coercion is common in language and 
underlies much extensibility of verb usages. 
Understanding such coercive processes thus has 
significant impact on how we should represent 
knowledge about verbs in a lexical resource. 
Importantly, constructional coercion is not an all-or-
nothing process ? a word must be semantically and 
syntactically compatible in some respects with a context 
in order for its use to be extended to that context, but the 
restrictions on compatibility are not hard-and-fast rules 
(Langacker, 1987; Kay & Fillmore, 1999; Goldberg, 
2006; Goldberg, to appear). Gradience of compatibility 
plays an important role in coercion, suggesting that a 
probabilistic approach may be necessary for encoding 
knowledge of constructional coercion in a verb lexicon 
(cf. Lapata & Lascarides, 2003). 
Our hypothesis here is that, due to this gradient process 
of productivity, existing verb lexicons do not adequately 
capture the actual patterns of use of extensible 
constructions. In this paper, we focus on the CAUSED-
MOTION (CM) construction as an initial test case. We first 
annotate the classes of an extensive verb lexicon, 
VerbNet, as to whether the CM construction is allowed 
for all, some, or none of the verbs in the class, noting 
additionally whether it is a typical or coerced usage. We 
find that many of the classes that allow the construction 
for at least some verbs do not include the CM frame in 
their definition, indicating a significant shortcoming in the 
relational knowledge encoded in the lexicon. Next, we 
72
develop probabilistic measures for determining to what 
degree a class is likely to admit the CM construction. We 
then test our measures over corpus data, manually 
annotated for use of the CM construction. Finally, we 
present preliminary work on automatic techniques for 
calculating the proposed measures in an unsupervised 
way, to avoid the need for expensive manual annotation. 
This work forms the preliminary steps toward empirically 
augmenting VerbNet?s predictive capabilities concerning 
the event semantics of verbs in coercible constructions. 
2 Extensible Constructions and VerbNet 
Construction grammar has much insight to offer on the 
topic of productivity and on the resulting statistical 
patterns and gradience of usages (e.g., Langacker, 1987; 
Kay & Fillmore, 1999; Goldberg, 2006). A construction 
is formally defined to be any pairing of linguistic form 
(e.g., a syntactic frame) and meaning. Words can be used 
in constructions to the extent that their lexical semantics is 
compatible with ? or can be coerced to be compatible 
with ? the semantic constraints on the construction. 
It is this notion of constructional coercion, and degree 
of coercibility, that accounts for the richness of usages 
that go beyond those thought of as typical or definitional 
for a verb: by coercing a verb not normally associated 
with a particular frame to occur in it, the meaning of the 
event can take on additional properties not considered a 
core part of the verb?s semantics. For example, in the case 
of the sentence discussed above, She blinked the snow off 
her lashes, it is not the verb but rather the CM 
construction itself that licenses the direct object and adds 
the notion of ?motion causally affecting the object? to the 
event semantics. Amongst other examples of well-known 
constructional coercions are: (1) The CAUSE-RECEIVE 
construction has the syntactic form of NP-V-NP-NP. For 
example, in Bob painted Sally a picture, the simple 
transitive verb paint gains the CAUSE TO RECEIVE sense, 
in which Sally is the recipient and the picture is the 
transferred item. (2) The WAY construction has the form 
of NP-V-[POSS way]-PP. For example, in Frank found 
his way to New York, the construction allows the verb 
find to gain a motion reading (i.e., ?Frank traveled to New 
York?) that would not otherwise be allowed (e.g., *Frank 
found to New York).  
Recognizing such extensions to the relational 
semantics of verbs is very important for accurate 
semantic interpretation in NLP. However, precise 
specifications for capturing the notion of coercible 
constructions, such as are needed for a computational 
resource, have heretofore been lacking. 
2.1 VerbNet & Knowledge of Constructions 
Computational verb lexicons are key to supporting NLP 
systems aimed at semantic interpretation. Verbs express 
the semantics of an event being described as well as the 
relational information among participants in that event, 
and project the syntactic structures that encode that 
information. Verbs are also highly variable, displaying a 
rich range of semantic and syntactic behavior. 
Verb classifications help NLP systems to deal with 
this complexity by organizing verbs into groups that 
share core semantic and syntactic properties. For 
example, VerbNet (derived from Levin?s [1993] work, 
Kipper et al, 2008) is widely used for a number of 
semantic processing tasks, including semantic role 
labeling (Swier and Stevenson, 2004), the creation of 
semantic parse trees (Shi and Mihalcea, 2005), and 
implicit argument resolution (Gerber and Chai, 2010). 
The detailed semantic predicates listed with each 
VerbNet class also have the potential to contribute to text-
specific semantic representations and, thereby, to tasks 
requiring inferencing (Zaenen et al, 2008; Palmer et al, 
2009). 
VerbNet identifies semantic roles and syntactic 
patterns characteristic of the verbs in each class makes 
explicit the connections between the syntactic patterns 
and the underlying semantic relations that can be inferred 
for all members of the class. Each syntactic frame in a 
class has a corresponding semantic representation that 
details the semantic relations between event participants 
across the course of the event. For example, one of the 
characteristic patterns listed for the Pour class is a 
CAUSED-MOTION pattern, which accounts for sentences 
like She poured water from the pitcher into the bowl. This 
is represented in VerbNet as follows: 
Syntactic representation: 
NP V NP PP PP 
Agent V Theme Source Location 
Semantic representation: 
MOTION (DURING(E), THEME)  
NOT (PREP (START(E), THEME, LOCATION)) 
PREP (START(E), THEME, SOURCE) 
PREP (END(E), THEME, LOCATION) 
CAUSE (AGENT, E) 
This representation details connections between the 
syntax and semantics using the semantic roles as links, 
indicating that the Agent is the Subject NP and has 
CAUSED the Event, and that the Theme is the Object NP 
and has a new LOCATION at the end of the event. These 
types of inferences provide the foundation for deep 
semantic analysis of text.  
73
However, the specifications in VerbNet (as in other 
predicate lexicons, such as FrameNet, Baker et al, 1998; 
Fillmore et al, 2002) are seen as definitional ? they are 
restricted to the core usages of the verbs that are valid for 
all verbs in the class. However, as noted above, people 
often use verbs productively, in ways that go beyond the 
boundaries of the verb class structure. It is important to 
correctly identify these productive usages when they 
occur, since they may be explicitly adding crucial 
inferences. If a construction is not recognized in the form 
of a syntactic frame in VerbNet, such inferences are not 
possible, greatly reducing VerbNet?s utility and coverage. 
For example, creative uses of a verb, such as She blinked 
the snow off her lashes, would have no corresponding 
frame in blink?s class, the Hiccup class.  It contains one 
intransitive frame: 
 NP V 
Agent V 
  
 
BODY_PROCESS (E, AGENT) 
INVOLUNTARY (E, AGENT) 
 
Sentences that coerce the meaning of blink to fit with a 
CM event would currently be misanalysed. One option 
might be to augment the Hiccup class with the CM frame 
from the Pour class, which would ensure that such 
sentences would be analyzed more accurately. However, 
given the productive nature of constructional coercion 
and its widespread applicability, the approach of adding 
any possible pattern to each class is not appropriate: this 
would undermine the definitional distinctions between 
classes and greatly lessen their usefulness.  
Complicating the issue is the phenomenon of regular 
sense extensions (Dang et al, 1998), where what once 
may have been coercion has become entrenched and is 
now seen as a different sense of the verb. For example, 
the verbs in the Push class express the general meaning of 
exerting force on an object, such as She pushed on the 
wall. Often, the exertion of force moves the object, which 
can be expressed in a CM construction such as She 
pushed the box across the room. VerbNet accounts for 
this regular sense extension by including most of the Push 
verbs in the Carry class as well, which has the CM 
construction as one of its frames. Deciding when to 
include a verb in another class based on regular sense 
extensions, when to add a frame for a construction to a 
class, or when to reject the frame as a defining part of a 
class, is made difficult by the graded nature of matches 
between verbs and a construction. Our goal is to maintain 
the advantages of the class structure of VerbNet while 
enhancing it with a graded view of the applicability of a 
construction for each class. Noting the applicability of a 
construction will enable the inclusion of its appropriate 
semantic predicates, and the inferencing over them, 
which are currently not supported. 
3 Our Proposal: Constructional Profiles 
We aim to augment VerbNet with knowledge of 
constructions that are likely to be used extensibly with a 
range of verbs. Such extensible constructions will be core 
usages for some classes (such as the CM for the Pour 
class, as noted above) but will be less characteristic of the 
fundamental semantics of other verb classes (such as CM 
for the Hiccup class). We propose to identify such a 
construction and its varying roles in the different classes 
by using relevant statistics over usages of verbs in a 
corpus ? what we call a constructional profile. 
A constructional profile is a probabilistic assessment 
of the usage of a particular construction by the verbs in a 
class. We developed the following three measures to 
capture the relevant behavior, with the goal of providing 
both type- and token-based views of the behavior of a 
verb class with respect to a target construction: 
P1 Ptype(X|C): probability that a verb type in class C is 
attested in construction X 
P1 gives a type-based assessment, indicating how 
widespread the use of the construction is across the 
verb types in the class. For example, if 8 out of 10 
members of a class appear with the construction, we 
might estimate P1 as 0.8. 
P2 Ptoken(X|C): probability that the instances of a typical 
verb in class C occur in construction X 
P2 gives a token-based assessment, indicating, for a 
typical verb in the class, the relative amount of usage of 
the construction among all usages of the verb. For 
example, to estimate this, we might average across all 
verbs in the class, the percentage of tokens in this 
construction. 
P3: Ptoken(X|X-verbs-in-C): same as P2 but considering 
only verbs that have been attested in construction X 
P3 is the same as P2, but looking only at those verbs in 
the class that have an attested usage of the construction, 
removing verbs without attested usages. 
We hypothesize that these measures will have high 
values for those classes for which the construction should 
be definitional; very low values for those classes that are 
not compatible with the construction; and varying values 
for those classes that allow coerced usages to a greater or 
lesser extent. 
Although these probabilities are intuitively very 
simple, estimating them from corpus data poses a 
significant challenge. Since a construction is a pairing of 
form with meaning, recognizing the use of a particular 
74
construction is not simply a matter of determining the 
syntactic pattern of the usage; rather, certain semantic 
properties and relations must co-occur with the syntactic 
pattern. Earlier work has shown that a supervised learning 
method was able to discriminate potential usages of the 
CM construction given training sentences manually 
labeled as either CM or not (Hwang et al, 2010). Here, 
we aim instead to identify usages of the CM construction, 
but without requiring an expensive manual annotation 
effort. That is, we seek an unsupervised method for 
estimating the probabilities in P1?P3 above. 
We approach this goal in steps as follows. First, we 
examine all the classes in VerbNet to see which allow the 
CM construction (Section 4). This anno-tation reveals 
shortcomings in VerbNet?s representa-tion (classes that 
allow the CM construction but do not list it) and also 
provides a gold standard with which to evaluate our 
method of identifying an exten-sible construction using 
our constructional profiles. Second, we use the manually 
annotated CM construction data from Hwang et al 
(2010) to estimate probabilities P1?P3 using maximum 
likelihood formulations (Section 5). An analysis of the 
predictive power of these constructional profile measures 
shows a good match with the distinctions made in the 
human annotation of the classes. Thus, our annotation 
based constructional profile measures show promise for 
identifying relevant behaviors of the construction across 
the classes. Third, we explore automatic methods for 
estimating the constructional profile measures without the 
need for manual annotations (Section 6). We use a 
hierarchical Bayesian model that learns verb classes from 
corpus data to provide unsupervised estimates of the 
constructional profiles, which also exhibit the relevant 
distinctions across the classes. 
4 Annotating the VerbNet Resource  
We begin with a manual examination of the resource and 
a thorough annotation of the status of each class with 
respect to the CM construction. This effort reveals a 
number of shortcomings in VerbNet, and the need for 
developing methods that can support the extension of 
VerbNet to better reflect the coercive uses of 
constructions across the classes. The annotation described 
here also forms the basis for the evaluation in the 
following sections of our new probabilistic measures, by 
motivating hypotheses about the expected patterns of use 
of the CM construction across the classes. 
4.1 Annotation Guidelines and Results 
The first goal of our manual annotation of VerbNet 
classes was to determine which classes currently 
represent CM in one of their frames. To this end, we 
identified which classes contain the following frame:  
NP [Agent/Cause]-V-NP [Patient/Theme]- 
PP [Source/Destination/Recipient/Location]  
These frames correspond to classes such as Slide, with its 
frame NP-V-NP-PP.Destination: Carla slid the books to 
the floor. We also examined classes with the patterns NP-
V-NP-PP.Oblique, NP-V-NP-PP. Theme2, and NP-V-
NP-PP.Patient2. In these classes, annotators had to judge 
whether the final PP was compatible with CM. For 
example, the Breathe class contains the frame NP-V-
NP.Theme-PP.Oblique, The dragon breathed fire on 
Mary, which is compatible with CM; whereas the same 
basic frame in the Other_cos class is not: NP V NP 
PP.Oblique, The summer sun tanned her skin to a golden 
bronze. 
In addition, we annotated which classes were 
potentially compatible with CM for either all verbs in the 
class or only some verbs. The "some" classification has 
the drawback that it may be applied to classes with very 
different proportions of compatible verbs; while suitable 
for our exploratory work here, we plan to make finer 
distinctions in the future. A secondary determination was 
whether or not the class was compatible with CM as part 
of its core semantics, or if it was compatible with CM 
because it was coercible into the construction. A verb was 
considered ?compatible with CM? and ?not coerced? if 
the verb could be used in the CM construction and its 
semantics, as reflected in VerbNet?s semantic predicates, 
involved a CAUSE predicate in combination with another 
predicate such as CONTACT, TRANSFER, (EN)FORCE, 
EMIT, TAKE_IN (predicates potentially involving 
movement along some path). For example, although CM 
is not already included as a frame for the Bend class 
containing the verb fold, the semantics of this class 
include CAUSE and CONTACT, and the verb can be used 
in a CM construction: She folded the note into her 
journal. Therefore, this class would have been considered 
?compatible with CM? but ?not coerced?. Conversely, a 
verb was considered ?compatible with CM? and 
?coerced? if the verb could be used in the CM 
construction, yet its semantics, again as reflected in 
VerbNet, did not involve CAUSE and MOVEMENT 
ALONG A PATH (e.g., the verb wiggle of the 
Body_internal_motion class: She wiggled her foot out of 
the boot). 
In summary, as presented in the table below, we 
annotated each class according to whether (1) the CM 
construction was already represented in VerbNet for this 
class, (2) the construction was possible for all, some, or 
75
none of the verbs in that class, and (3) the verbs of any 
class compatible with CM were coerced into the 
construction or not. The classification for (3) was made 
regardless of whether ?all? verbs or only ?some? were 
compatible with CM. This determination was made 
uniformly for a class: there were no classes in which only 
certain CM-compatible verbs were considered ?coerced?.  
VN class example  
[# of classes like this] 
CM in 
VN 
CM is 
possible 
CM is 
coerced 
Banish [50] Yes All No 
Nonverbal_Expression [2] Yes All Yes 
Cheat [6] Yes Some No 
Exhale [18] No All No 
Hiccup [30] No All Yes 
Fill [46] No Some No 
Wish [54] No Some Yes 
Matter [64] No None N/A 
Notably, we identified 206 classes where at least some of 
the verbs in that class are compatible with the CM 
construction; however, VerbNet currently only 
recognizes the CM construction in 58 classes. There were 
several classes of interest: First, although it may seem 
unusual that CM is represented in 6 classes where we 
found that only ?some? verbs were compatible with CM 
(e.g., Cheat class), these were cases where only more 
restricted subclasses are compatible with CM, and this 
syntactic frame is listed for that subclass. This suggests 
subclasses may provide a more precise characterization 
of which verbs are compatible with a construction.  
Secondly, we identified 18 classes in which all verbs 
were compatible with CM without coercion; thus, these 
classes could likely be improved by the addition of the 
CM syntactic frame. Additionally, we found 30 classes in 
which all verbs are coercible into the CM construction; 
however, the actual likelihood of a verb in those classes 
occurring in a CM construction remains to be 
investigated in the following sections. Like those classes 
where it was determined that only ?some? verbs are 
compatible with CM, usefully incorporating the CM 
construction into classes that require coercion relies on 
accurately determining the probability that verbs in those 
classes will actually appear in the CM construction.  
For those classes in which ?all? verbs are compatible 
with CM, our intuition was that some aspect of the verb?s 
semantics either inherently includes or allows the verb to 
be coerced into the CM construction. Conversely, for 
those classes in which no verbs are compatible with CM, 
presumably some aspect of the verb?s semantics is 
logically incompatible with CM. Although pinpointing 
precisely what aspect of a verb?s semantics makes it 
compatible with CM may not be possible, we can 
investigate whether or not our intuitions are supported by 
examining the actual frequencies of CM constructions for 
given verbs or a given class.  
4.2 Hypotheses  
Using these annotations, we were able to develop two 
simple hypotheses. 
Hypothesis 1: We expect the constructional profile 
measures for the CM construction in a given corpus to be 
highest for those classes in which all verbs were found to 
be compatible with CM; lower for classes in which only 
some verbs were found to be compatible; and lowest for 
classes in which no verbs were found to be compatible. 
Hypothesis 2: We expect the constructional profile 
measures for the CM construction in a given corpus to be 
highest for verbs that fall into classes where CM is not 
considered coerced (for either some or all of the verbs in 
the class); lower for verbs that fall into classes in which 
the CM construction only works through coercion (for 
either some or all of the verbs in the class); and lowest for 
verbs that fall into classes in which no verbs are 
compatible with CM.  
To investigate Hypothesis 1, we grouped the annotated 
classes according to whether all, some, or no verbs in the 
class are compatible with CM: 
 Class example # of classes 
Allowed by All Bring, Carry 106 
Allowed by Some Appoint, Lodge 100 
Allowed by None Try, Own 64 
To investigate Hypothesis 2, we did a second grouping 
of the classes according to whether CM is not coerced, 
CM is coerced, or CM is simply not compatible with the 
class. This second grouping did not distinguish whether 
CM was compatible with ?all? or ?some? of the verbs in 
a given class. 
 Class example # of classes 
Not Coerced Put, Throw 120 
Coerced Floss, Wink 86 
Not Compatible Differ 64 
5 Evaluation using Constructional Profiles 
5.1 Annotated data description 
Our research uses the data annotated for Hwang et al 
(2010), in which 1800 instances in the form NP-V-NP-
PP were identified in the Wall Street Journal portion of 
the Penn Treebank II (Marcus et al, 1994). Each instance 
76
of the data was single annotated with one of the two 
labels: CM or non-CM. The annotation guidelines were 
based on the CM analysis of Goldberg (1995). 
Our analysis began with the same data but adopted a 
slightly narrower definition of CM. We diverged from 
the Hwang et al (2010) study in the following two ways: 
(1) sentences where the object NP is an item that is 
created by the event denoted by the verb were not 
considered CM (e.g., Mr. Pilson scribbled a frighteningly 
large figure on a slip of paper, where the figure is created 
through the scribbling event); and (2) sentences in which 
movement is prevented were not considered CM (e.g., 
He kept her at arm?s length). In agreement with Hwang 
et al, our annotation included both metaphorical senses 
(e.g., [It] cast a shadow over world oil markets) and 
literal senses (e.g., The company moved the employees to 
New York) of CM. Our annotation using the narrower 
guidelines resulted in 85.8% agreement with the original 
annotation.1  The distribution of labels in our data is 
21.8% for CM and 78.2% for NON-CM. 
5.2 Annotated data description 
Using statistics over the manually annotated data, we 
calculate maximum likelihood estimates of the three 
constructional profile measures introduced in Section 3, 
as follows. First, let the probability that a verb v is used in 
the CM construction be estimated as: 
P(CM|v,C) = 
#(CM usages of     ) 
#(CM+non-CM usages of    ) 
That is, P(CM|v,C) is estimated as the relative frequency 
of the CM construction for v out of all annotated usages 
of v that are labeled as class C. Now let CCM be all verbs v 
in C with at least one usage annotated as CM; i.e.: 
    *      |  (  |   )    + 
Then we calculate estimates of P1?P3 as: 
P1: Ptype(CM|C) = |CCM |/|C| 
This measure indicates how widespread the use of CM is 
across the verb types in the class. 
P2: Ptoken(CM|C) =,?  (  |   )   - | |?  
The average over all verbs v in C of P(CM|v,C) 
This indicates the relative amount of usage of CM among 
all usages of the verbs in the class.  
P3: Ptoken(CM|v,C) = [?  (  |   ))- |   |       
The average over all verbs v in CCM of P(CM|v,C) 
P3 narrows the P2 measure to only those verbs in the 
                                                          
1We found that 34.0% of the disagreements were directly due to 
the changes in annotation resulting from our two new criteria. 
class for which there is an attested usage of CM. 
5.3 Analysis of the Constructional Profiles 
The tables below provide a summary of the profile 
measures P1-P3 for the groups of VerbNet classes as 
defined in section 4.2. For each group listed, we report 
the averages of P1-P3 over all classes in the group where 
at least one verb in the class occurred in the data 
manually annotated for CM usage. 
 P1 P2 P3 
CM Allowed by All 0.413 0.323 0.437 
CM Allowed by Some 0.087 0.078 0.224 
CM Not Allowed 0.055 0.055 0.083 
As seen here, the constructional profile measures over 
CM in the data corroborate our Hypothesis 1 (Section 
4.2). All three measures on average are highest for the 
classes that fall into the ?all allowed? group, next highest 
for those in the ?some allowed? group, and lowest for the 
?not allowed? classes.  
 P1 P2 P3 
CM Non-Coerced 0.354 0.274 0.418 
CM Coerced 0.091 0.091 0.185 
CM Not Allowed2 0.056 0.056 0.083 
Furthermore, the second table here confirms our 
expectations for Hypothesis 2 (Section 4.2). Again, all 
three measures on average are highest for classes that fall 
into the ?non-coerced? group, next highest for classes in 
the ?coerced? group (in which the construction is 
achievable only through coercion), and lowest for the 
?not allowed? group.  
Thus, our two hypotheses are borne out, showing that 
our constructional profile measures, when estimated over 
manually annotated data, can be useful in capturing 
important distinctions among classes of verbs with regard 
to their usage in an extensible construction such as CM. 
6 Automatic Creation of Constructional 
Profiles Using a Bayesian Model  
Manually annotating a corpus for usages of a con-
struction can be prohibitively expensive, so we also 
investigate the use of automatic methods to estimate 
constructional profile measures. By using a hierarchi-cal 
Bayesian model (HBM) that acquires latent prob-abilistic 
verb classes from corpus data, we provide unsupervised 
                                                          
2 Note the non-zero values result from actual CM verb usages in 
the data belonging to classes believed to be not compatible with 
CM by VerbNet expert annotators. 
77
estimates of the constructional profiles. 
6.1 Overview of Model and Data 
We use the HBM of Parisien & Stevenson (2011), a 
model that automatically acquires probabilistic 
knowledge about verb argument structure and verb 
classes from large-scale corpora. The model is based on a 
large body of research in nonparametric Bayesian topic 
modeling (e.g., Teh et al, 2004), a robust method of 
discovering syntactic and semantic structure in very large 
datasets. For each verb encountered in a corpus, the 
model provides an estimate of the verb?s expected overall 
pattern of usage. By using latent probabilistic verb classes 
to influence these expected usage patterns, the model can, 
for example, estimate the probability that a verb like blink 
might occur in a CM construction, even if no such 
attested usages appear in the corpus. 
In this preliminary study, we use the corpus data from 
Parisien & Stevenson (2011), since the model has been 
trained and evaluated on this data. As that study was 
aimed at modeling facts of child language acquisition, it 
uses child-directed speech from the Thomas corpus 
(Lieven et al, 2009), part of the CHILDES database 
(MacWhinney, 2000). In this preliminary study, we use 
their development dataset containing approx. 170,000 
verb usages, covering approx. 1,400 verb types. (We 
reserve the test set for future experiments.) For each verb 
usage in the input, a number of features are automatically 
extracted that indicate the number and type of syntactic 
arguments occurring with the verb and general semantic 
properties of the verb. The semantic features are drawn 
from the set of VerbNet semantic predicates, such as 
CAUSE, MOTION, and CONTACT. These are automatically 
extracted from all classes compatible with the verb (with 
no sense disambiguation). 
6.2 Measures for Constructional Profiles 
Using the argument structure constructions, verb usage 
patterns and classes learned by the model, we estimate 
the three constructional profile measures in Section 3, as 
follows. First, we note that since the constructions 
acquired by the model are probabilistic in nature, a 
particular CM instance may be a partial match to more 
than one of the model?s constructions.  
For each verb in the input, we consider the likelihood 
of use of the CM construction to be the likelihood of a 
contrived frame intended to capture the important 
properties of a CM usage. FCM is a usage taking a direct 
object and a prepositional phrase, and including the 
semantic features CAUSE and MOTION, with all other 
semantic features left unspecified. For a given verb v, we 
estimate the likelihood of this CM usage, over all 
constructions in the model, as follows: 
 (   | )  ? (   | ) (
 
 | ) 
Here, P(FCM |k) is the likelihood of the CM usage FCM 
being an instance of the probabilistic construction k, and 
P(k|v) is the likelihood that verb v occurs with 
construction k. These component probabilities are 
estimated using the probability distributions acquired by 
the model and averaged over 100 samples from the 
Markov Chain Monte Carlo simulation, as described in 
Parisien & Stevenson (2011). 
Now, we let CCM be the set of verbs in VerbNet class 
C where the expected likelihood of a CM usage is non-
negligible (akin to the set of verbs with attested usage in 
Section 5.2): 
CCM = {v C | P(FCM|v)>? } 
where ? is a small threshold, here 0.0001. Note that since 
v is not disambiguated for class in our data, all usages of v 
contribute to this estimate. 
The estimates of P1-P3 are comparable to those in 
Section 5.2. The difference is that since we are un-able to 
disambiguate individual usages of the verbs, each usage 
of v is considered to belong to all possible classes C of 
which v is a member. P1 is estimated as before; P2 and 
P3 are averages of P(FCM|v). 
6.3 Analysis of the Constructional Profiles 
The tables below provide a summary of the profile 
estimates P1-P3 for the groups of VerbNet classes as 
given in Section 4.2. For each group listed, we report the 
averages of P1-P3 over all classes in the group where at 
least one of the verbs in the class occurred in the training 
input to the model. 
 P1 P2 P3 
All allowed 0.569 0.0180 0.0250 
Some allowed 0.449 0.0106 0.0192 
Not allowed 0.363 0.0044 0.0079 
These profile measures align with the hypotheses in 
Section 4.2 and with the measures based on manually 
annotated data in Section 5.2. The estimates are high-est 
for classes where all verbs permit the CM con-struction, 
second highest for classes where only some permit it, and 
lowest for classes that do not permit it. 
 P1 P2 P3 
CM non-coerced 0.546 0.0178 0.0260 
CM coerced 0.458 0.0095 0.0167 
CM not allowed 0.363 0.0044 0.0079 
78
Again, the overall patterns of the profile measures align 
with Sections 4.2 and 5.2. The profile estimates are 
highest for classes annotated to be non-coerced usages of 
CM, second highest for coerced classes, and lowest for 
?not allowed?.  
The measures show the overall differences among 
classes in the different groups (for both groupings) ? i.e., 
the average behavior among classes in the different 
groups varies as we predicted.  This indicates that the 
measures are tapping into aspects of construction usage 
that are relevant to making the desired distinctions in 
VerbNet, and validates the use of automatic 
techniques.  However, there is a substantial amount of 
variability in these measures across the classes, so we also 
consider how well the estimates can predict the 
appropriate group for individual classes. That is, can we 
automatically predict whether the CM construction can 
be used by all, some, or none of the verbs in a given verb 
class, and can we predict whether such usages are 
coerced? 
We consider the P3 measure as it provides the best 
separation among the class groupings. The tables below 
report precision (P), recall (R) and F-measures (F) for 
each group, where ?all? and ?some? have been collapsed. 
For exploratory purposes, we pick P3 = 0.006 as the 
value that optimizes F-measures of this classification. 
Future work will explore more principled means for 
setting these thresholds. 
 P R F 
CM allowed 0.880 0.742 0.806 
CM not allowed 0.407 0.636 0.497 
Only a 2-way distinction can be made reliably for the 
allowed grouping. The F-score of over 80% for the 
?allowed? label is very promising. The low precision for 
the ?not allowed? case suggests that the model can?t 
generalize sufficiently due to sparse data. 
 P R F 
CM non-coerced 0.691 0.491 0.574 
CM coerced 0.461 0.417 0.438 
CM not allowed 0.406 0.709 0.517 
We use thresholds of P3 = 0.021 to separate non-coerced 
from coerced classes, and P3 = 0.007 to separate coerced 
from not allowed classes. The model estimates show 
moderate success in distinguishing classes with coerced 
vs. non-coerced usage of the CM construction. However, 
our measures simply cannot distinguish non-occurrence 
due to semantic incompatibility from non-occurrence due 
to chance, given the expected low frequency of a novel 
coerced use of a construction.  To separate the allowed 
cases into whether they are coerced or not requires a 
more detailed assessment of the semantic compatibility of 
the class, which means looking at finer-grained features 
of verb usages that are indicative of the semantic 
predicates compatible with the particular construction.  
Moreover, this kind of assessment likely needs to be 
applied on a verb-specific (and not just class-specific) 
level, in order to identify those verbs out of a potentially 
coercible class that are indeed coercible (i.e., identifying 
the coercible verbs in a class labeled as "some allowed"). 
7 Conclusion 
Our investigation demonstrates that VerbNet does not 
currently represent the CM construction for all verbs or 
verb classes that are compatible with this construction, 
and the existing static representation of verbs is 
inadequate for analyzing extensions of verb meaning 
brought about by coercion. The utility of VerbNet would 
be greatly enhanced by an improved representation of 
constructions: specifically, the incorporation of 
probabilities that verbs in a given (sub)class would occur 
in a particular construction, and whether this constitutes a 
regular sense extension. This addition to VerbNet would 
increase the resource?s coverage of syntactic frames that 
are compatible with a given verb, and therefore enable 
appropriate inferences when coercion occurs. We have 
made preliminary steps towards developing this 
probabilistic distribution over both verb instances and 
classes, based on a large corpus. Unsupervised methods 
for estimating the probabilities achieve an F-score of over 
80% in distinguishing the classes that allow the target 
construction. However, making distinctions among 
coerced and non-coerced cases will require us to go 
beyond these class-based probabilities to finer-grained, 
corpus-based assessments of a verb?s semantic 
compatibility with a coercible construction.  
To move beyond these preliminary findings, we must 
therefore shift our focus to the behavior of individual 
verbs. Additionally, to reduce the impact of errors 
resulting from low-frequency verbs and classes, we plan 
to expand our research to more data, specifically the 
OntoNotes TreeBank data (Weischedel et al, 2011). 
Finally, to achieve our ultimate goal of creating a lexicon 
that can flexibly account for a variety of constructions, we 
will examine other constructions as well. While 
determining the set of coercible constructions in a 
language is itself a topic of current research, we propose 
initially to include the widely recognized CAUSE-
RECEIVE and WAY constructions in addition to CM. 
79
References  
Baker, Collin F., Charles J. Fillmore, and John B. Lowe. 1998. 
The Berkeley FrameNet Project. Proceedings of the 17th 
International Conference on Computational Linguistics 
(COLING/ACL-98), pp. 86?90, Montreal. 
Dang, HoaTrang, Karin Kipper, Martha Palmer, and Joseph 
Rosenzweig. 1998. Investigating regular sense extensions 
based on intersective Levin classes. Proceedings of 
COLING-ACL98, pp. 293?299. 
Fillmore, Charles J., Christopher R. Johnson, and Miriam R.L. 
Petruck. 2002. Background to FrameNet. International 
Journal of Lexicography, 16(3):235-250.  
Gerber, Matthew, and Joyce Y. Chai. 2010. Beyond 
NomBank: A study of implicit arguments for nominal 
predicates. Proceedings of the 48th Annual Meeting of the 
Association of Computational Linguistics, pp. 1583?1592, 
Uppsala, Sweden, July. 
Goldberg, A. E. 1995. Constructions: A construction 
grammar approach to argument structure. Chicago: 
University of Chicago Press. 
Goldberg, A. E. 2006. Constructions at work: The nature of 
generalization in language. Oxford: Oxford University 
Press. 
Goldberg, A. E. To appear. Corpus evidence of the viability of 
statistical preemption. Cognitive Linguistics. 
Hwang Jena D., Rodney D. Nielsen and Martha Palmer. 2010. 
Towards a domain-independent semantics: Enhancing 
semantic representation with construction grammar. 
Proceedings of Extracting and Using Constructions in 
Computational Linguistic Workshop, held with NAACL 
HLT 2010, Los Angeles, June. 
Kay, P., and C. J. Fillmore. 1999. Grammatical constructions 
and linguistic generalizations: The What's X Doing Y? 
construction. Language, 75:1?33. 
Kipper, Karin, Anna Korhonen, Neville Ryant, and Martha 
Palmer. 2008. A large-scale classification of English verbs. 
Language Resources and Evaluation Journal, 42:21?40. 
Langacker, R. W. 1987. Foundations of cognitive grammar: 
Theoretical prerequisites. Stanford, CA: Stanford 
University Press. 
Lapata, M., and A. Lascarides. 2003. Detecting novel 
compounds: The role of distributional evidence. 
Proceedings of the 11th Conference of the European 
Chapter of the Association for Computational 
Linguistics(EACL03), pp.235?242. Budapest, Hungary. 
Levin, B. 1993.English Verb Classes and Alternations: A 
Preliminary Investigation. Chicago: Chicago University 
Press.  
 
 
Lieven, E., D. Salomo, and M. Tomasello. 2009. Two-year-
old children?s production of multiword utterances: A 
usage-based analysis. Cognitive Linguistics 20(3):481?507. 
MacWhinney, B. 2000.The CHILDES Project: Tools for 
analyzing talk (3rd ed., Vol. 2: The Database). Erlbaum. 
M?rquez, L., X. Carreras, K. Litkowski, and S. Stevenson. 
2008. Semantic role labeling: An introduction to the special 
issue. Computational Linguistics, 34(2): 145?159. 
Martha Palmer, Jena D. Hwang, Susan Windisch Brown, 
Karin Kipper Schuler and Arrick Lanfranchi. 2009. 
Leveraging lexical resources for the detection of event 
relations. Proceedings of the AAAI 2009 Spring 
Symposium on Learning by Reading, Stanford, CA, March. 
Palmer, Martha, Daniel Gildea, and Paul Kingsbury.2005. 
The Proposition Bank: An annotated corpus of semantic 
roles. Computational Linguistics, 31(1):71?106. 
Parisien, Christopher, and Suzanne Stevenson. 2011. To 
appear in Proceedings of the 33rd Annual Meeting of the 
Cognitive Science Society, Boston, MA, July. 
Pustejovsky, J., and E. Jezek. 2008. Semantic coercion in 
language: Beyond distributional analysis. Italian Journal of 
Linguistics/RivistaItaliana di Linguistica 20(1): 181?214. 
Shi, Lei, and Rada Mihalcea. 2005. Putting pieces together: 
Combining FrameNet, VerbNet and WordNet for robust 
semantic parsing. Proceedings of the 6th International 
Conference on Intelligent Text Processing and 
Computational Linguistics, Mexico City, Mexico. 
Swier, R., and S. Stevenson. 2004. Unsupervised semantic 
role labeling. Proceedings of the 2004 Conf. on Empirical 
Methods in Natural Language Processing, pp. 95?102, 
Barcelona, Spain. 
Teh, Y. W., M. I. Jordan, M. J.Beal, and D. M.Blei.2006. 
Hierarchical Dirichlet processes. Jrnl of the American 
Statistical Asscn, 101(476): 1566?1581. 
Weischedel, R., E. Hovy, M. Marcus, M. Palmer, .R. Belvin, 
S. Pradan, L. Ramshaw and N. Xue. 2011.OntoNotes: A 
Large Training Corpus for Enhanced Processing. In Part 1: 
Data Acquisition and Linguistic Resources of The 
Handbook of Natural Language Processing and Machine 
Translation: Global Automatic Language Exploitation, 
Eds.: Joseph Olive, Caitlin Christianson, John McCary. 
Springer Verlag, pp. 54-63. 
Zaenen, A., C. Condoravdi, and D. G. Bobrow. 2008. The 
encoding of lexical implications in VerbNet. Proceedings 
of LREC 2008, Morocco, May. 
80
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 1?10,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
Modeling the Acquisition of Mental State Verbs
Libby Barak, Afsaneh Fazly, and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
{libbyb,afsaneh,suzanne}@cs.toronto.edu
Abstract
Children acquire mental state verbs (MSVs)
much later than other, lower-frequency, words.
One factor proposed to contribute to this de-
lay is that children must learn various seman-
tic and syntactic cues that draw attention to the
difficult-to-observe mental content of a scene.
We develop a novel computational approach
that enables us to explore the role of such cues,
and show that our model can replicate aspects
of the developmental trajectory of MSV acqui-
sition.
1 Introduction
Mental State Verbs (MSVs), such as think, know,
and want, are very frequent in child-directed lan-
guage, yet children use them productively much
later than lower-frequency action verbs, such as fall
and throw (Johnson and Wellman, 1980; Shatz et al,
1983). Psycholinguistic theories have suggested that
there is a delay in the acquisition of MSVs because
they require certain cognitive and/or linguistic skills
that are not available during the early stages of lan-
guage development. For example, MSVs typically
occur with a sentential complement (SC) that refers
to the propositional content of the mental state, as in
He thinks Mom went home. Children have to reach a
stage of syntactic development that includes some
facility with SCs in order to fully acquire MSVs.
However, even at 3?5 years old, children are able to
process SCs only imperfectly (e.g., Asplin, 2002).
Even when children are able to produce SCs with
other verbs (such as verbs of communication, as in
He said Mom went home), there is a lag before they
productively use MSVs referring to actual mental
content (Diessel and Tomasello, 2001).1 Psycholin-
guists have suggested that young children lack the
conceptual ability to conceive that others have men-
tal states separate from their own (Bartsch and Well-
man, 1995; Gopnik and Meltzoff, 1997), further de-
laying the acquisition of MSVs.
Another factor suggested to contribute to the dif-
ficulty of acquiring MSVs is their informational re-
quirements (Gleitman et al, 2005; Papafragou et al,
2007). Children learn word meanings by figuring
out which aspects of an observed scene are referred
to by a particular word (Quine, 1960). MSVs of-
ten refer to aspects of the world that are not directly
observable (i.e., the beliefs and desires of another
entity). Thus, in addition to the above-mentioned
challenges posed by children?s developing linguis-
tic/conceptual abilities, children may simply have
difficulty in identifying the relevant mental content
necessary to learning MSVs.
In particular, Papafragou et al (2007) [PCG] have
shown that even given adequate conceptual and lin-
guistic abilities (as in adults) the mental events in a
scene (the actors? internal states) are not attended
to as much as the actions, unless there are cues
that heighten the salience of the mental content.
PCG further demonstrate that children?s sensitivity
to such cues lags behind that of adults, suggesting an
additional factor in the acquisition of MSVs which
1Researchers have noted that children use MSVs in fixed
phrases, in a performative use or as a pragmatic marker, well be-
fore they use them to refer to actual mental content (e.g., Diessel
and Tomasello, 2001; Shatz et al, 1983). Here by ?acquisition
of MSVs?, we are specifically referring to children learning us-
ages that genuinely refer to mental content.
1
is the developmental change in how strongly such
cues are associated with the relevant mental content.
We develop a computational model of MSV ac-
quisition (the first, to our knowledge) to further il-
luminate these issues. We extend an existing model
of verb argument structure acquisition (Alishahi and
Stevenson, 2008) to enable the representation and
processing of mental state semantics and syntax.
We simulate the developmental change proposed by
PCG through a gradually increasing ability in the
model to appropriately attend to the mental content
of a scene. In addition, we suggest that even when
the learner?s semantic representation is biased to-
wards the action content, the learner attends to the
observed SC syntax in an MSV utterance. This is
especially important to account for the pattern of er-
rors in child data. Our model thus extends the ac-
count of PCG to show that a probabilistic interplay
of the semantic and syntactic features of a partial and
somewhat erroneous perception of the input, com-
bined with a growing ability to attend to cues indica-
tive of mental content, can help to account for chil-
dren?s developmental trajectory in learning MSVs.
2 Background and Our Approach
To investigate the linguistic and contextual cues that
could help in learning MSVs, PCG use a procedure
called the Human Simulation Paradigm (originally
proposed by Gillette et al, 1999). In this paradigm,
subjects are put in situations intended to simulate
various word learning conditions of young children.
E.g., in one condition, adults watch silent videos of
caregivers interacting with children, and are asked
to predict the verb uttered by the caregiver. In an-
other condition, subjects hear a sentence containing
a nonce verb (e.g., gorp) after watching the video,
and are asked what gorp might mean.
We focus on two factors investigated by PCG in
the performance of adults and children in identifying
MSVs. The first factor they investigated involved
the syntactic frame used when subjects were given a
sentence with a nonce verb. PCG hypothesized that
an SC frame would be a cue to mental content (and
an MSV), since the SC refers to propositional con-
tent. The second factor PCG examined was whether
the video described a ?true belief? or a ?false be-
lief? scene: A true belief scene shows an ordinary
situation which unfolds as the character in the scene
expects ? e.g., a little boy takes food to his grand-
mother, and she is there in the house as expected.
The corresponding false belief scene has an unex-
pected outcome for the character ? in this case, an-
other character has replaced the grandmother in her
bed. Here the hypothesis was that such false belief
scenes would heighten the salience of mental activ-
ity in the scene and lead to greater belief verb re-
sponses in describing them.
PCG?s results showed that both adults and chil-
dren were sensitive to both the scene and syntax
cues, but children?s ability to draw on such cues was
inferior to that of adults. They thus propose that the
difference between children and adults is that chil-
dren have not yet formed as strong an association
as adults between the cues and the mental content
of a scene as required to match the performance of
adults. Nonetheless, their results suggest that the
participating children had the conceptual and lin-
guistic abilities required for MSVs, since they were
able to produce them under conditions with suffi-
ciently strong cues.
We simulate PCG?s experiments using a novel
computational approach. Following PCG, we as-
sume that even when a learner is able to perceive
the general semantic and syntactic properties of a
belief scene and associated utterance, they may not
attend to the mental content in every situation, and
that this ability improves over time. We model a de-
velopmental change in a learner?s attention to mental
content: At early stages, corresponding to the state
of young children, the learner largely focuses on the
action aspects of a belief scene, even in the presence
of an utterance using an MSV. Over time, the learner
gradually increases in the ability to attend appropri-
ately to the mental aspects of such a scene and ut-
terance, until adult-like competence is achieved in
associating the available cues with mental content.
Importantly, our work extends the proposal of
PCG by bringing in evidence from other relevant
studies on children?s ability to process SCs. More
specifically, we suggest that when children hear a
sentence like I think Mom went home, they recog-
nize (and record) the existence of an SC, while at
the same time they focus on the action semantics
as the main (most salient) event. In other words,
we assume that children?s imperfect syntactic abil-
2
ities are at least sufficient to recognize the SC us-
age (Nelson et al, 1989; Asplin, 2002). However,
their attention is mostly directed towards the action
expressed in the embedded complement, either be-
cause mental content is less easily observable than
action (Papafragou et al, 2007), or due to the lin-
guistic saliency of the embedded clause (Diessel and
Tomasello, 2001; Dehe and Wichmann, 2010). As
mentioned above, we model this misrepresentation
by considering the possibility of not attending to
mental content in a belief scene. Specifically, we
assume that (i) the model is very likely to overlook
the mental content at earlier stages (corresponding to
children?s observed behaviour); and (ii) as the model
?ages? (i.e., receives more input), its attentional abil-
ities improve and thus the model is more likely to
focus on the mental content as the main proposition.
Our results suggest that these changes to the model
lead to a match between our model?s behaviour and
PCG?s differential results for children and adults.
3 The Computational Model
A number of computational models have examined
the role of interacting syntactic and semantic cues
in the acquisition of verb argument structure (e.g.,
Niyogi, 2002; Buttery, 2006; Alishahi and Steven-
son, 2008; Perfors et al, 2010; Parisien and Steven-
son, 2011). However, to our knowledge no com-
putational model has addressed the developmental
trajectory in the acquisition of MSVs. Here we ex-
tend the verb argument structure acquisition model
of Alishahi and Stevenson (2008) to enable it to ac-
count for MSV acquisition. Specifically, we use
their core Bayesian learning algorithm, but modify
the input processing component to reflect a develop-
mental change in attention to the mental state con-
tent of an MSV usage and its consequent represen-
tation, as noted above.
We use this model for the following reasons: (i) it
focuses on argument structure learning, and the in-
terplay between syntax and semantics, which are key
to MSV acquisition; (ii) it is probabilistic and hence
can naturally capture gradient responses to different
cues; and (iii) it is incremental, which allows us to
investigate changes in behaviour over time. We first
give an overview of the original model, and then ex-
plain our extensions.
3.1 Model Overview
The input to the model is a sequence of utterances
(what the child hears), each paired with a scene
(what the child perceives); see Table 1 for an ex-
ample. First, the frame extraction component of
the model extracts from the input pair a frame?
a collection of features. We use features that in-
clude both semantic properties (?event primitives?
and ?event participants?) and syntactic properties
(?syntactic pattern? and ?verb count?). See Table 2
for examples of two possible frames extracted from
the pair in Table 1. Second, the learning component
of the model incrementally clusters the extracted
frames one by one. These clusters correspond to
constructions that reflect probabilistic associations
of semantic and syntactic features across similar us-
ages, such as an agentive intransitive or causative
transitive. The model can use these associations to
simulate various language tasks as the prediction of
a missing feature given others. For example, to sim-
ulate the human simulation paradigm setting, we can
use the model to predict a missing verb on the basis
of the available semantic and syntactic information
(as in Alishahi and Pyykkon?en, 2011).
3.2 Algorithm for Learning Constructions
The model clusters the input frames into construc-
tions on the basis of their overall similarity in the
values of their features. Importantly, the model
learns these constructions incrementally, consider-
ing the possibility of creating a new construction for
a given frame if the frame is not sufficiently similar
to any of the existing constructions. Formally, the
model finds the best construction (including a new
one) for a given frame F as in:
BestConstruction(F ) = argmax
k?Constructions
P (k|F )
(1)
where k ranges over all existing constructions and a
new one. Using Bayes rule:
P (k|F ) =
P (k)P (F |k)
P (F )
? P (k)P (F |k) (2)
The prior probability of each construction P (k) is
estimated as the proportion of observed frames that
are in k, assigning a higher prior to constructions
3
Think[state,consider,cogitate](I[experiencer,preceiver,considerer ],Go[physical,act,move](MOM[agent,actor,change],HOME[location,destination]))
I think Mom went home.
Table 1: A sample Scene?Utterance input pair.
(a) Interpretation#1 (mental event is attended to) (b) Interpretation#2 (mental event not attended to)
main predicate think main predicate go
other predicate go other predicate think
event primitives { state, consider , cogitate } event primitives { physical , act ,move}
event participants { experiencer , perceiver , considerer} event participants { agent , actor , change}
{ preposition, action, perceivable} { location, destination}
syntactic pattern arg1 verb arg-S syntactic pattern arg1 verb arg-S
verb count 2 verb count 2
Table 2: Two frames extracted from the scene?utterance pair in Table 1. The bottom left and right panels of the table
describe the two possible interpretations given the input pair. (a) Interpretation#1 assumes that the mental event is the
focus of attention. Here think is interpreted as the main predicate, which the event primitives and participants refer
to. (b) Interpretation#2 assumes that attention is mostly directed to the physical action in the scene, and thus go is
taken to be the main predicate, which also determines the extracted event primitives and participants. Note that for
both interpretations, the learner is assumed to perceive the utterance in full, thus both verbs are heard in the context
of the sentential complement syntax (i.e., syntactic pattern with SC and 2 verbs), without fully extracting the syntactic
relations between the clauses.
that are more entrenched (i.e., observed more fre-
quently). The likelihood P (F |k) is estimated based
on the values of features in F and the frames in k:
P (F |k) =
?
i?frameFeatures
Pi(j|k) (3)
where i refers to the ith feature of F and j refers
to its value. The conditional probability of a feature
i to have the value j in construction k, Pi(j|k), is
calculated with a smoothed version of:
Pi(j|k) =
counti(j, k)
nk
(4)
where counti(j, k) reflects the number of times fea-
ture i has the value j in construction k, and nk is the
number of frames in k. We have two types of fea-
tures: single-valued and set-valued. The result of the
counti operator for a single-valued feature is based
on exact match to the value j, while the result for a
set-valued feature is based on the degree of overlap
between the compared sets, as in the original model.
3.3 Modeling Developmental Changes in
Attending to Mental Content
We extend the model above to account for the in-
crease in the ability to attend to cues associated with
MSVs, as observed by PCG. In addition, we pro-
pose that children?s representation of this situation
includes the observed syntax of the MSV. That is,
children do not simply ignore the MSV usage, focus-
ing only on the action expressed in its complement
? they must also note that this action semantics oc-
curs in the context of an SC usage.
To adapt the model in these ways, we change
the frame extraction component to allow two pos-
sible interpretations for a mental event input. First,
to reflect PCG?s proposal, we incorporate a mecha-
nism into the model?s frame-extraction process that
takes into account the probability of attending to
mental content. Specifically, we assume that when
presented with an input pair containing an MSV,
as in Table 1, a learner attends to the perceptu-
ally salient action/state expressed in the comple-
ment (here Go) with probability p, and to the non-
perceptually salient mental event expressed in the
main verb (here Think) with probability 1? p. This
probability p is a function over time, correspond-
ing to the observed developmental progression. At
very early stages, p will be high (close to 1), sim-
ulating the much greater saliency of physical ac-
tions compared to mental events for younger chil-
dren. With subsequent input, p will decrease, giv-
ing more and more attention to the mental content
of a scene with a mental event, gradually approach-
ing adult-like abilities.
4
We adopt the following function for p:
p =
1
? ? t+ 1
, 0 < ?  1 (5)
where t is the current time, expressed as the total
number of scene?utterance pairs observed thus far
by the model, and the parameter ? is set to a small
value to assign a high probability to the physical ac-
tion interpretation of the scene in the initial stages of
learning (when t is small).
We must specify the precise make-up of the
frames that correspond to the two possible inter-
pretations considered with probability p and 1 ? p.
PCG state only that children and adults differen-
tially attend to the action vs. mental content of the
scene. We operationalize this by forming two pos-
sible frames in response to an MSV usage. We pro-
pose that one of the frames (with probability 1?p) is
the interpretation of the mental content usage, as in
Table 2(a). However, we extend the account of PCG
by proposing that the other frame considered is not
simply a standard representation of an action scene?
utterance pair. Rather, we suggest that the interpre-
tation of an MSV scene?utterance pair that focuses
on the action semantics does so within the context of
the SC syntax, given the assumed stage of linguistic
abilities of the learner. This leads to the frame (with
probability p) as in Table 2(b), which represents the
action semantics within a two-verb construction as-
sociated with the SC syntax.
4 Experimental Setup
4.1 Input Data
We generate artificial corpora for our simulations,
since we do not have access to sufficient data of ac-
tual utterances paired with scene representations. In
order to create naturalistic data that resembles what
children are exposed to, we follow the approach of
Alishahi and Stevenson (2008) to build an input-
generation lexicon that has the distributional prop-
erties of actual child-directed speech (CDS). Their
original lexicon contains only high-frequency phys-
ical action verbs that appear in limited syntactic pat-
terns. Our expanded lexicon also includes mental
state, perception, and communication verbs, all of
which can appear with SCs.
We extracted our verbs and their distributional
properties from the child-directed speech of 8
children in the CHILDES database (MacWhinney,
2000).2 We selected 28 verbs from different se-
mantic classes and different frequency ranges: 12
physical action verbs taken from the original model
(come, go, fall, eat, play, get, give, take, make, look,
put, sit), 6 perception and communication verbs
(see, hear, watch, say, tell, ask), 5 belief verbs (think,
know, guess, bet, believe), and 5 desire verbs (want,
wish, like, mind, need). For each verb, we manually
analyzed a random sample of 100 CDS usages (or
all usages if fewer than 100) to extract distributional
information about its argument structures.
We construct the input-generation lexicon by list-
ing each of the 28 verbs (i.e. the ?main predicate?),
along with its overall frequency, as well as the fre-
quency with which it appears with each argument
structure. Each entry contains values of the syn-
tactic and semantic features (see Table 2 for ex-
amples), including ?event primitives?, ?event partic-
ipants?, ?syntactic pattern?, and ?verb count?. By
including these features, we assume that a learner
is capable of understanding basic syntactic proper-
ties of an utterance, including word syntactic cat-
egories (e.g., noun and verb), word order, and the
appearance of SCs (e.g., Nelson et al, 1989). We
also assume that a learner has the ability to perceive
and conceptualize the general semantic properties
of events ? including mental, perceptual, commu-
nicative, and physical actions ? as well as those
of the event participants. Values for the semantic
features (the event primitives and event participants)
are taken from Alishahi and Stevenson (2008) for
the action verbs, and from several sources including
VerbNet (Kipper et al, 2008) and Dowty (1991) for
the additional verbs.
For each simulation in our experiments (explained
below), we use the input-generation lexicon to
automatically generate an input corpus of scene?
utterance pairs that reflects the observed frequency
distribution in CDS.3 For an input utterance that
contains an MSV, we randomly pick one of the ac-
tion verbs as the verb appearing within the sentential
complement (the ?other predicate?).
2Corpora of Brown (1973); Suppes (1974); Kuczaj (1977);
Bloom et al (1974); Sachs (1983); Lieven et al (2009).
3The model does not use the input-generation lexicon in
learning.
5
4.2 Setup of Simulations
We perform simulations by training the model on
a randomly generated input corpus, and examin-
ing changes in its performance over time with pe-
riodic tests. Specifically, we perform simulations of
the verb identification task in the human simulation
paradigm as follows: At each test point, we present
the model with a partial test frame with missing
predicate (verb) values, and different amounts of in-
formation for the other features. The tests corre-
spond to the scenarios in the original experiments of
PCG, where each scenario is represented by a partial
frame as follows:
1. scene-only scenario: Corresponds to subjects
watching a silent video depicting either an Ac-
tion or a Belief scene. Our test frame includes
values for the semantic features (event primi-
tives and event participants) corresponding to
the scene type, but no syntactic features.
2. syntax-only scenario: Corresponds to subjects
hearing either an SC or a non-SC utterance.
The test frame includes the corresponding syn-
tactic pattern and verb count of the utterance
type heard, but no semantic features.
3. syntax & scene scenario: Corresponds to sub-
jects watching a silent video (with Action or
Belief content), and hearing an associated (non-
SC or SC) utterance. The test frame includes all
the relevant syntactic and semantic features.
We perform 100 simulations, each on 15000
randomly-generated training frames, and examine
the type of verbs that the model predicts in response
to test frames for the three scenarios. For each
scenario and each simulation, we generate a test
frame by including the relevant feature values of a
randomly-selected physical action or belief verb us-
age from the input-generation lexicon.
PCG code the individual verb responses of their
human subjects into various verb classes. To analo-
gously code our model?s response to each test frame,
we estimate the likelihood of each of two verb
groups, Belief and Action,4 by summing over the
4The Action verbs include action, communication, and per-
ception verbs, as in PCG. Verbs from the desire group are not
considered here, also as in PCG.
Figure 1: Likelihood of Belief verb prediction given Ac-
tion or Belief input.
likelihood of all the verbs in that group. In the re-
sults below, these likelihood scores are averaged for
each test point over the 100 simulations.
When our model is presented with a test frame
containing a Belief scene, we assume that the model
(like a language learner) may not attend to the men-
tal content, resulting in one of the two interpreta-
tions described in Section 3.3 (see Table 2). We thus
calculate the verb class likelihoods using a weighted
average of the verbs predicted under the two inter-
pretations. Following PCG, we test our model with
two types of Belief scenes: True Belief and False
Belief, with the latter having a higher level of be-
lief saliency. We model the difference between these
two scene types as a difference in the probabilities
of perceiving the two interpretations, with a higher
probability for the belief interpretation given a False
Belief test frame. In the experiments presented here,
we set this probability to 80% for False Belief, and
to 60% (just above chance) for True Belief. (Un-
like in training, where we assume a change over time
in the probability of a belief interpretation, for each
presentation of the test frame we use the same prob-
abilities of the two interpretations.)
5 Experimental Results
We present two sets of results: In Section 5.1, we
examine the role of syntactic and semantic cues in
MSV identification, by comparing the likelihoods
of the model?s Belief verb predictions across the
three scenarios. Here we test the model after pro-
cessing 15000 input frames, simulating an adult-like
behaviour (as in PCG). At this stage, we present
the model with an Action test frame (Action scene
and/or Transitive syntax), or a Belief test frame
6
(False Belief scene and/or SC syntax). In Sec-
tion 5.2, we look into the role of semantic cues
that enhance belief saliency, by comparing the like-
lihoods of Belief vs. Action verb predictions in the
syntax & scene scenario. The test frames depict ei-
ther a True Belief or a False Belief scene, paired with
an SC utterance. Here, we test our model periodi-
cally to examine the developmental pattern of MSV
identification, comparing our results with the differ-
ence in the behaviour of children and adults in PCG.
5.1 Linguistic Cues for Belief Verb Prediction
The left side of Figure 1 presents the results of PCG
(for adult subjects); the right side shows the likeli-
hood of Belief verb prediction by our model. Simi-
lar to the results of PCG, our model?s likelihood of
Belief verb prediction is extremely low when given
an Action test frame (Action scene and/or Transi-
tive syntax), whereas it is much higher when the
model is presented with a Belief test frame (False
Belief scene and/or SC syntax). Moreover, as in
PCG, when the model is tested with Belief content,
the lowest likelihood is for the scene-only scenario
and the highest is for the syntax & scene scenario.
PCG found, somewhat surprisingly, that the
syntax-only scenario was more informative for MSV
prediction than the scene-only scenario. Our results
replicate this finding, which we believe is due to the
way our Bayesian clustering groups verb usages to-
gether. Non-SC usages of MSVs are often grouped
with action verbs that frequently appear with non-
SC syntax, and this results in constructions with
mixed (action and belief) semantics. When using
MSV semantic features to make the verb predic-
tion, the action verbs get a higher likelihood based
on such mixed constructions. However, the frequent
usage of MSVs with SC results in entrenched con-
structions of mostly MSVs. Although other verbs,
such as see and say, may also be used with SC syn-
tax, they are grouped with verbs such as watch and
tell into constructions with mixed (SC and non-SC)
syntax. When given SC syntax in verb prediction,
the more coherent MSV constructions result in a
high likelihood of predicting Belief verbs.
5.2 Belief Saliency in Verb Prediction
Figure 2(a) shows the PCG results, for children
and adults, and for True Belief and False Belief.
(a)
(b)
(c)
Figure 2: Verb class likelihood: (a) PCG results for
adults and children (aged 3;7?5;9); (b) Model?s results
given True Belief; (c) Model?s results given False Belief.
Figures 2(b) and (c) present the likelihoods of the
model?s Belief vs. Action verb prediction, over time,
for True and False Belief situations (True/False Be-
lief scene and SC syntax), respectively. We first
compare the responses of our model at the final stage
of training to those of adults in PCG. At this stage,
the model?s verb predictions (for both True and False
Belief) follow a similar trend to that of adult sub-
jects in PCG. The likelihood of Belief verbs is much
higher than the likelihood of Action verbs given a
False Belief situation. Moreover, the likelihood of
Belief verbs is higher given a False Belief situation,
compared to a True Belief situation.
Next, we compare the developmental pattern of
Belief/Action verb predictions in the model with the
difference in behaviour of children and adults in
PCG. We focus on the model?s responses after pro-
7
cessing about 3000 input pairs, as it corresponds to
the trends observed for the children in PCG. At this
stage, the likelihood of Belief verbs is lower than
that of Action verbs for the True Belief situation,
but the pattern is reversed for False Belief; a pattern
similar to children?s behaviour in PCG (see Figure
2(a)). As in PCG, the likelihood of Belief verb pre-
dictions in our model is higher than that of Action
verbs for the False Belief situation, in both ?child?
and ?adult? stages, with a larger difference as the
model ?ages? (i.e., processes more input). For the
True Belief situation also the pattern is similar to
that of PCG: Belief verbs are less likely than Action
verbs to be predicted at early stages, but as the model
receives more input, the likelihood of Belief verbs
becomes slightly higher than that of Action verbs.
PCG?s hypothesis of greater attention to the action
content of a scene implicitly implies that children
focus on the action semantics and syntax of the em-
bedded SC of a Belief verb. We have suggested in-
stead that the focus is on the action semantics within
the context of the SC syntax of the MSV. To directly
evaluate the necessity of our latter assumption, we
performed a simulation using both action syntax and
semantics to represent the physical interpretation of
the belief scene. Specifically, the syntactic features
in this representation were non-SC structure with
only one verb. Based on these settings, the model
predicted high likelihood for the Belief verbs from a
very early stage, not showing the same delayed ac-
quisition pattern exhibited by PCG?s results. This
result suggests that the SC syntax plays an impor-
tant role in MSV acquisition.
6 Discussion
Various studies have considered why mental state
verbs (MSVs) appear relatively late in children?s
productions (e.g., Shatz et al, 1983; Bartsch and
Wellman, 1995). The Human Simulation Paradigm
has revealed that adult participants tend to focus on
the physical action cues of a scene (Gleitman et al,
2005). PCG?s results further show that cues empha-
sizing mental content lead to a significant increase
in MSV responses in such tasks. Moreover, they
show that a sentential complement (SC) structure is
a stronger cue to an MSV than the semantic cues
emphasizing mental content.
In this paper we adapt a computational Bayesian
model to analyze such semantic and syntactic cues
in the ability of children to identify them. We sim-
ulate an attentional mechanism of the growing sen-
sitivity to mental content in a scene into the model.
We show that both the ability to observe the obscure
mental content and the ability to recognize the use of
an SC structure are essential to replicate PCG?s ob-
servations. Moreover, our results predict the strong
association of MSVs to the SC syntax, for the first
time (to our knowledge) in a computational model.
Children often use verbs other than MSVs in ex-
perimental settings in which MSVs would be the ap-
propriate or correct verb choice (Asplin, 2002; Kidd
et al, 2006; Papafragou et al, 2007). Our model
presents similar variability in verb choice. One un-
derlying cause of this behaviour in the model is its
association of action semantics to SC syntax, due to
the tendency to observe the physical cues in a scene
associated with an utterance using an MSV with an
SC. Preliminary results (not reported here) imply
that the association of perception and communica-
tion verbs that frequently appear with SC contribute
to this pattern of verb choice (see de Villiers, 2005,
for theoretical support). Our results require further
work to fully understand this behaviour.
Finally, our model will facilitate future work in re-
gards to the performative usage of MSVs, in which
MSVs do not indicate mental content, but rather di-
rect the conversation. Several studies (e.g., Diessel
and Tomasello, 2001; Howard et al, 2008), have re-
ferred to the role performative use likely plays in
MSV acquisition, since the first MSV usages by
children are performative. The semantic properties
MSVs take in performative usages is not currently
represented in our lexicon. However, the physical
interpretation of the mental scene that we have used
in our experiments here is similar to the performa-
tive usage: i.e., the main perceived action and the
observed syntactic structure are the same. At the
moment, our results imply that the association of
MSVs with their genuine mental meaning is delayed
by interpretations of the mental scene which over-
look the mental content. In the future, we aim to in-
corporate the semantic representation of performa-
tive usages to better analyze their effect on MSV ac-
quisition.
8
References
Afra Alishahi and Pirita Pyykkon?en. 2011. The on-
set of syntactic bootstrapping in word learning:
Evidence from a computational study. In Pro-
ceedings of the 33st Annual Conference of the
Cognitive Science Society.
Afra Alishahi and Suzanne Stevenson. 2008. A com-
putational model of early argument structure ac-
quisition. Cognitive Science, 32(5):789?834.
Kristen N. Asplin. 2002. Can complement frames
help children learn the meaning of abstract
verbs? Ph.D. thesis, UMass Amherst.
Karen Bartsch and Henry M. Wellman. 1995. Chil-
dren talk about the mind.
Lois Bloom, Lois Hood, and Patsy Lightbown.
1974. Imitation in language development: If,
when, and why. Cognitive Psychology, 6(3):380?
420.
Roger Brown. 1973. A first language: The early
stages. Harvard U. Press.
Paula J. Buttery. 2006. Computational models
for first language acquisition. Technical Report
UCAM-CL-TR-675, University of Cambridge,
Computer Laboratory.
Jill G. de Villiers. 2005. Can language acquisition
give children a point of view. In Why Language
Matters for Theory of Mind, pages 199?232. Ox-
ford University Press.
Nicole Dehe and Anne Wichmann. 2010. Sentence-
initial I think (that) and i believe (that): Prosodic
evidence for use as main clause, comment clause
and dicourse marker. Stuides in Language,
34(1):36?74.
Holger Diessel and Michael Tomasello. 2001. The
acquisition of finite complement clauses in en-
glish: A corpus-based analysis. Cognitive Lin-
guistics, 12(2):97?142.
David Dowty. 1991. Thematic Proto-Roles and Ar-
gument Selection. Language, 67(3):547?619.
Jane Gillette, Lila Gleitman, Henry Gleitman, and
Anne Lederer. 1999. Human simulations of lexi-
cal acquisition. Cognition, 73(2):135?176.
Lila R. Gleitman, Kimberly Cassidy, Rebecca
Nappa, Anna Papafragou, and John C. Trueswell.
2005. Hard words. Language Learning and De-
velopment, 1(1):23?64.
Alison Gopnik and Andrew N. Meltzoff. 1997.
Words, thoughts, and theories.
Alice A. Howard, Lara Mayeux, and Letitia R.
Naigles. 2008. Conversational correlates of chil-
dren?s acquisition of mental verbs and a theory of
mind. First Language, 28(4):375.
Carl Nils Johnson and Henry M. Wellman. 1980.
Children?s developing understanding of mental
verbs: Remember, know, and guess. Child De-
velopment, 51(4):1095?1102.
Evan Kidd, Elena Lieven, and Michael Tomasello.
2006. Examining the role of lexical frequency in
the acquisition and processing of sentential com-
plements. Cognitive Development, 21(2):93?107.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classification
of English verbs. Language Resources and Eval-
uation, 42(1):21?40?40.
A. Kuczaj, Stan. 1977. The acquisition of regular
and irregular past tense forms. Journal of Verbal
Learning and Verbal Behavior, 16(5):589?600.
Elena Lieven, Dorothe? Salomo, and Michael
Tomasello. 2009. Two-year-old children?s pro-
duction of multiword utterances: A usage-based
analysis. Cognitive Linguistics, 20(3):481?507.
Brian MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk, volume 2. Psychology
Press.
Deborah G. Kemler Nelson, Kathy Hirsh-Pasek, Pe-
ter W. Jusczyk, and Kimberly Wright Cassidy.
1989. How the prosodic cues in motherese might
assist language learning. Journal of child Lan-
guage, 16(1):55?68.
Sourabh Niyogi. 2002. Bayesian learning at the
syntax-semantics interface. In Proceedings of the
24th Annual Conference of the Cognitive Science
Society.
Anna Papafragou, Kimberly Cassidy, and Lila Gleit-
man. 2007. When we think about thinking:
The acquisition of belief verbs. Cognition,
105(1):125?165.
Christopher Parisien and Suzanne Stevenson. 2011.
Generalizing between form and meaning using
9
learned verb classes. In Proceedings of the 33rd
Annual Meeting of the Cognitive Science Society.
Amy Perfors, Joshua B. Tenenbaum, and Elizabeth
Wonnacott. 2010. Variability, negative evidence,
and the acquisition of verb argument construc-
tions. Journal of Child Language, 37(03):607?
642.
Willard .V.O. Quine. 1960. Word and object, vol-
ume 4. The MIT Press.
Jacqueline Sachs. 1983. Talking about the there and
then: The emergence of displaced reference in
parent-child discourse. Children?s Language, 4.
Marilyn Shatz, Henry M. Wellman, and Sharon Sil-
ber. 1983. The acquisition of mental verbs: A
systematic investigation of the first reference to
mental state. Cognition, 14(3):301?321.
Patrick Suppes. 1974. The semantics of children?s
language. American Psychologist, 29(2):103.
10
In: R. Levy & D. Reitter (Eds.), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 80?89,
Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics
A Computational Model of Memory, Attention, and Word Learning
Aida Nematzadeh, Afsaneh Fazly, and Suzanne Stevenson
Department of Computer Science
University of Toronto
{aida,afsaneh,suzanne}@cs.toronto.edu
Abstract
There is considerable evidence that people
generally learn items better when the presen-
tation of items is distributed over a period of
time (the spacing effect). We hypothesize that
both forgetting and attention to novelty play
a role in the spacing effect in word learning.
We build an incremental probabilistic compu-
tational model of word learning that incorpo-
rates a forgetting and attentional mechanism.
Our model accounts for experimental results
on children as well as several patterns ob-
served in adults.
1 Memory, Attention, and Word Learning
Learning the meaning of words is an important com-
ponent of language acquisition, and an extremely
challenging task faced by young children (e.g.,
Carey, 1978; Bloom, 2000). Much psycholinguis-
tic research has investigated the mechanisms under-
lying early word learning, and the factors that may
facilitate or hinder this process (e.g., Quine, 1960;
Markman and Wachtel, 1988; Golinkoff et al, 1992;
Carpenter et al, 1998). Computational modeling has
been critical in this endeavor, by giving precise ac-
counts of the possible processes and influences in-
volved (e.g., Siskind, 1996; Regier, 2005; Yu, 2005;
Fazly et al, 2010). However, computational models
of word learning have generally not given sufficient
attention to the broader interactions of language ac-
quisition with other aspects of cognition and cogni-
tive development.
Memory limitations and attentional mechanisms
are of particular interest, with recent computational
studies reconfirming their important role in aspects
of word learning. For example, Frank et al (2010)
show that memory limitations are key to matching
human performance in a model of word segmenta-
tion, while Smith et al (2010) further demonstrate
how attention plays a role in word learning by form-
ing the basis for abstracting over the input. But
much potential remains for computational modeling
to contribute to a better understanding of the role of
memory and attention in word learning.
One area where there is much experimental evi-
dence relevant to these interactions is in the investi-
gation of the spacing effect in learning (Ebbinghaus,
1885; Glenberg, 1979; Dempster, 1996; Cepeda
et al, 2006). The observation is that people gen-
erally show better learning when the presentations
of the target items to be learned are ?spaced? ? i.e.,
distributed over a period of time ? instead of be-
ing ?massed? ? i.e., presented together one after
the other. Investigations of the spacing effect often
use a word learning task as the target learning event,
and such studies have looked at the performance of
adults as well as children (Glenberg, 1976; Pavlik
and Anderson, 2005; Vlach et al, 2008). While
this work involves controlled laboratory conditions,
the spacing effect is very robust across domains and
tasks (Dempster, 1989), suggesting that the underly-
ing cognitive processes likely play a role in natural
conditions of word learning as well.
Hypothesized explanations for the spacing effect
have included both memory limitations and atten-
tion. For example, many researchers assume that the
process of forgetting is responsible for the improved
performance in the spaced presentation: Because
participants forget more of what they have learned
in the longer interval, they learn more from sub-
sequent presentations (Melton, 1967; Jacoby, 1978;
80
Cuddy and Jacoby, 1982). However, the precise re-
lation between forgetting and improved learning has
not been made clear. It has also been proposed that
subjects attend more to items in the spaced presen-
tation because accessing less recent (more novel)
items in memory requires more effort or attention
(Hintzman, 1974). However, the precise attentional
mechanism at work in the spacing experiments is not
completely understood.
While such proposals have been discussed for
many years, to our knowledge, there is as yet no de-
tailed computational model of the precise manner in
which forgetting and attention to novelty play a role
in the spacing effect. Moreover, while mathemat-
ical models of the effect help to clarify its proper-
ties (Pavlik and Anderson, 2005), it is very impor-
tant to situate these general cognitive mechanisms
within a model of word learning in order to under-
stand clearly how these various processes might in-
teract in the natural word learning setting.
We address this gap by considering memory con-
straints and attentional mechanisms in the context
of a computational model of word-meaning acquisi-
tion. Specifically, we change an existing probabilis-
tic incremental model of word learning (Fazly et al,
2010) by integrating two new factors: (i) a forgetting
mechanism that causes the learned associations be-
tween words and meanings to decay over time; and
(ii) a mechanism that simulates the effects of atten-
tion to novelty on in-the-moment learning. The re-
sult is a more cognitively plausible word learning
model that includes a precise formulation of both
forgetting and attention to novelty. In simulations
using this new model, we show that a possible ex-
planation for the spacing effect is the interplay of
these two mechanisms, neither of which on its own
can account for the effect.
2 The Computational Model
We extend the model of Fazly et al (2010) ? hence-
forth referred to as FAS10 ? by integrating new
functionality to capture forgetting and attention to
novelty. The model of FAS10 is an appropriate start-
ing point for our study because it is an incremen-
tal model of word learning that learns probabilis-
tic associations between words and their semantic
properties from naturalistic data. Nonetheless, the
model assumes equal attention to all words and ob-
jects present in the input, and, although incremental,
it has a perfect memory for the internal represen-
tation of each processed input. Hence, as we will
show, it is incapable of simulating the spacing ef-
fects observed in humans.
2.1 The FAS10 Model
The input to the model is a sequence of utterances (a
set of words), each paired with a scene representa-
tion (a set of semantic features, representing what is
perceived when the words are heard), as in:
Utterance: { she, drinks, milk }
Scene: { ANIMATE, PERSON, FEMALE, CONSUME,
DRINK, SUBSTANCE, FOOD, DAIRY-PRODUCT }
For each word, the model of FAS10 learns a proba-
bility distribution over all possible features, p(.|w),
called the meaning probability of the word. Before
processing any input, all features are equally likely
for a word, and the word?s meaning probability is
uniform over all features. At each time step t, an
input utterance?scene pair (similar to the above
example) is processed. For each word w and seman-
tic feature f in the input pair, an alignment score,
at(w| f ), is calculated that specifies how strongly
the w? f pair are associated at time t. The alignment
score in FAS10 uses the meaning probabilities of
all the words in the utterance, which reflect the
knowledge of the model of word meanings up to
that point, as in:
at(w|f ) =
pt?1(f |w)
?
w??Ut
pt?1(f |w
?)
(1)
where pt?1( f |w) is the probability of f being part of
the meaning of word w at time t?1.
In the FAS10 model, pt(.|w) is then updated for
all the words in the utterance, using the accumulated
evidence from all prior and current co-occurrences
of w? f pairs. Specifically, an association score is
defined between a word and a feature, assoct(w, f ),
which is a summation of all the alignments for that
w and f up to time t.1 This association score is then
normalized using a smoothed version of the follow-
1In FAS10, assoct(w, f ) = assoct?1(w, f )+at(w| f ).
81
ing to yield pt( f |w):
pt( f |w) =
assoct( f , w)
?
f ??M
assoct( f
?, w)
(2)
whereM is the set of all observed features.
There are two observations to make about the
FAS10 model in the context of our desire to explore
attention and forgetting mechanisms in word learn-
ing. First, the calculation of alignments at(w| f ) in
Eqn. (1) treats all words equally, without special at-
tention to any particular item(s) in the input. Sec-
ond, the assoct( f ,w) term in Eqn. (2) encodes per-
fect memory of all calculated alignments since it is a
simple accumulated sum. These properties motivate
the changes to the formulation of the model that we
describe next.
2.2 Adding Attention to Novelty to the Model
As noted just above, the FAS10 model lacks any
mechanism to focus attention on certain words, as is
suggested by theories on the spacing effect (Hintz-
man, 1974). One robust observation in studies on
attention is that people attend to new items in a
learning scenario more than other items, leading to
improved learning of the novel items (e.g., Snyder
et al, 2008; MacPherson and Moore, 2010; Horst
et al, 2011). We thus model the effect of attention
to novelty when calculating alignments in our new
model: attention to a more novel word increases the
strength of its alignment with a feature ? and con-
sequently the learned word?feature association ?
compared to the alignment of a less novel word.
We modify the original alignment formulation of
FAS10 to incorporate a multiplicative novelty term
as follows (cf. Eqn. (1)):
at(w, f ) =
pt(f |w)
?
w??Ut
pt(f |w
?)
?noveltyt(w) (3)
where noveltyt(w) specifies the degree of novelty of
a word as a simple inverse function of recency. That
is, we assume that the more recently a word has been
observed by the model, the less novel it appears to
the model. Given a word w at time t that was last
observed at time tlastw , we calculate noveltyt(w) as:
noveltyt(w) = 1? recency(t, tlastw) (4)
where recency(t, tlastw) is inversely proportional
to the difference between t and tlastw . We set
novelty(w) to be 1 for the first exposure of the word.
2.3 Adding a Forgetting Mechanism to the
Model
Given the observation above (see end of Section 2.1)
that assoct(w, f ) embeds perfect memory in the
FAS10 model, we add a forgetting mechanism by re-
formulating assoct(w, f ) to incorporate a decay over
time of the component alignments at(w| f ). In or-
der to take a cognitively plausible approach to calcu-
lating this function, we observe that assoct(w, f ) in
FAS10 serves a similar function to activation in the
ACT-R model of memory (Anderson and Lebiere,
1998). In ACT-R, activation of an item is the sum
of individual memory strengthenings for that item,
just as assoct(w, f ) is a sum of individual align-
ment strengths for the pair (w, f ). A crucial dif-
ference is that memory strengthenings in ACT-R
undergo decay. Specifically, activation of an item
m after t presentations is calculated as: act(m)t =
ln(?tt ?=1 1/(t?t
?)d), where t ? is the time of each pre-
sentation, and d is a constant decay parameter.
We adapt this formulation for assoct(w, f ) with
the following changes: First, in the act formula, the
constant 1 in the numerator is the basic strength of
each presentation to memory. In our model, this
is not a constant but rather the strength of align-
ment, at(w| f ). Second, we assume that stronger
alignments should be more entrenched in memory
and thus decay more slowly than weaker alignments.
Thus, each alignment undergoes a decay which is
dependent on the strength of the alignment rather
than a constant decay d. We thus define assoct(w, f )
to be:
assoct( f ,w) = ln(
t
?
t ?=1
at ?(w| f )
(t? t ?)dat?
) (5)
where the decay for each alignment dat? is:
dat? =
d
at ?(w| f )
(6)
where d is a constant parameter. Note that the dat?
decreases as at ?(w| f ) increases.
82
apple: { FOOD:1, SOLID:.72, PRODUCE:.63,
EDIBLE-FRUIT:.32, PLANT-PART:.22,
PHYSICAL-ENTITY:.17, WHOLE:.06, ? ? ? }
Figure 1: True meaning features & scores for apple.
3 Input Generation
The input data consists of a set of utterances paired
with their corresponding scene representations. The
utterances are taken from the child-directed speech
(CDS) portion of the Manchester corpus (Theakston
et al, 2001), from CHILDES (MacWhinney, 2000),
which includes transcripts of conversations with 12
British children, ages 1;8 to 3;0. Every utterance
is considered as a bag of lemmatized words. Half of
the data is used as the development set, and the other
half in the final experiments.
Because no manually-annotated semantic repre-
sentation is available for any such large corpus of
CDS, we use the approach of Nematzadeh et al
(2012) to generate scene representations. For each
utterance a scene representation is generated artifi-
cially, by first creating an input-generation lexicon
that contains the true meaning (t(w)) of all the words
(w) in our corpus. The true meaning is a vector
of semantic features and their assigned scores (Fig-
ure 1). The semantic features for a word, depend-
ing on its part of speech, are chosen from different
sources such as WordNet.2 The score of each feature
is calculated automatically to give a higher value to
the more specific features (such as FRUIT for apple),
rather than more general features (like PHYSICAL-
ENTITY for apple).
To generate the scene representation S of an utter-
ance U, we probabilistically sample a subset of fea-
tures from the features in t(w) for each word w ?U.
Thus, in each occurrence of w some of its features
are missing from the scene, resulting in an imper-
fect sampling. This imperfect sampling allows us to
simulate noise and uncertainty in the input, as well
as the uncertainty of a child in determining the rele-
vant meaning elements in a scene. The scene S is the
union of all the features sampled for all the words in
the utterance. We note that the input-generation lex-
icon is only used in creating input corpora that are
naturalistic (based on child-directed speech), and not
in the learning of the model.
2http://wordnet.princeton.edu
4 Experiments
First, we examine the overall word learning be-
haviour in our new model. Then we look at spacing
effects in the learning of novel words. In both these
experiments, we compare the behavior of our model
with the model of FAS10 to clearly illustrate the ef-
fects of forgetting and attention to novelty in the new
model. Next we turn to further experiments explor-
ing in more detail the interaction of forgetting and
attention to novelty in producing spacing effects.
4.1 Word Learning over Time
Generally, the model of FAS10 has increasing com-
prehension of words as it is exposed to more input
over time. In our model, we expect attention to nov-
elty to facilitate word learning, by focusing more
on newly observed words, whereas forgetting is ex-
pected to hinder learning. We need to see if the new
model is able to learn words effectively when sub-
ject to the combined effects of these two influences.
To measure how well a word w is learned in each
model, we compare its learned meaning l(w) (a vec-
tor holding the values of the meaning probability
p(.|w)) to its true meaning t(w) (see Section 3):
acq(w) = sim(l(w), t(w)) (7)
where sim is the cosine similarity between the two
meaning vectors, t(w) and l(w). The better the
model learns the meaning of w, the closer l(w)
would get to t(w), and the higher the value of sim
would become. To evaluate the overall behaviour of
a model, at each point in time, we average the acq
score of all the words that the model has seen.
We train each model on 10,000 input utterance?
scene pairs and compare their patterns of word learn-
ing over time (Figure 2).3 We can see that in the
original model, the average acq score is mostly in-
creasing over time before leveling off. Our model,
starts at a higher average acq score compared to
FAS10?s model, since the effect of attention to nov-
elty is stronger than the effect of forgetting in early
stages of training. There is a sharp decrease in the
acq scores after the early training stage, which then
levels off. The early decrease in acq scores oc-
curs because many of the words the model is ex-
3The constant decay parameter d in Eqn. (6) is set to 0.03 in
this experiment.
83
Figure 2: Average acq score of the words over time, for
our model and FAS10?s model.
posed to early on are not learned very well initially,
and so forgetting occurs at a higher rate during that
stage. The model subsequently stabilizes, and the
acq scores level off although at a lower absolute
level than the FAS10 model. Note that when com-
paring these two models, we are interested in the
pattern of learning; in particular, we need to en-
sure that our new word learning model will even-
tually stabilize as expected. Our model stabilizes
at a lower average acq score since unlike FAS10?s
model, it does not implement a perfect memory.
4.2 The Spacing Effect in Novel Word
Learning
Vlach et al (2008) performed an experiment to in-
vestigate the effect of presentation spacing in learn-
ing novel word?object pairs in three-year-old chil-
dren. Each pair was presented 3 times in each of
two settings, either consecutively (massed presenta-
tion), or with a short play interval between each pre-
sentation (spaced presentation). Children were then
asked to identify the correct object corresponding to
the novel word. The number of correct responses
was significantly higher when the pairs were in the
spaced presentation compared to the massed presen-
tation. This result clearly demonstrates the spacing
effect in novel word learning in children.
Experiments on the spacing effect in adults have
typically examined and compared different amounts
of time between the spaced presentations, which we
refer to as the spacing interval. Another important
parameter in such studies is the time period between
the last training trial and the test trial(s), which we
refer to as the retention interval (Glenberg, 1976;
Bahrick and Phelps, 1987; Pavlik and Anderson,
2005). Since the experiment of Vlach et al (2008)
was designed for very young children, the proce-
dures were kept simple and did not vary these two
parameters. We design an experiment similar to that
of Vlach et al (2008) to examine the effect of spac-
ing in our model, but extend it to also study the role
of various spacing and retention intervals, for com-
parison to earlier adult studies.
4.2.1 Experimental Setup
First, the model is trained on 100 utterance?scene
pairs to simulate the operation of normal word learn-
ing prior to the experiment.4 Then a randomly
picked novel word (nw) that did not appear in the
training trials is introduced to the model in 3 teach-
ing trials, similar to Vlach et al?s (2008) experiment.
For each teaching trial, nw is added to a different ut-
terance, and its probabilistically-generated meaning
representation (see Section 3) is added to the corre-
sponding scene. We add nw to an utterance?scene
pair from our corpus to simulate the presentation of
the novel word during the natural interaction with
the child in the experimental setting.
The spacing interval between each of these 3
teaching trials is varied from 0 to 29 utterances, re-
sulting in 30 different simulations for each nw. For
example, when the spacing interval is 5, there are
5 utterances between each presentation of nw. A
spacing of 0 utterances yields the massed presenta-
tion. We run the experiment for 20 randomly-chosen
novel words to ensure that the pattern of the results
is not related to the meaning representation of a spe-
cific word.
For each spacing interval, we look at the acq score
of the novel word at two points in time, to simu-
late two retention intervals: One immediately after
the last presentation of the novel word (imm condi-
tion) and one at a later point in time (lat condition).
By looking at these two conditions, we can further
observe the effect of forgetting in our model, since
the decay in the model?s memory would be more se-
vere in the lat condition, compared to the imm con-
dition.5 The results reported here for each spacing
4In the experiments of Section 4.2.2 and Section 4.3, the
constant decay parameter d is equal to 0.04.
5Recall that each point of time in our model corresponds to
84
Figure 3: Average acq score of novel words over spacing
intervals, in our model and FAS10?s model.
interval average the acq scores of all the novel words
at the corresponding points in time.
4.2.2 The Basic Spacing Effect Results
Figure 3 shows the results of the simulations in
our model and the FAS10 model. We assume that
very small spacing intervals (but greater than 0)
correspond to the spaced presentation in the Vlach
et al (2008) experiments, while a spacing of 0 cor-
responds to the massed presentation. In the FAS10
model, the average acq score of words does not
change with spacing, and there is no difference be-
tween the imm and lat conditions, confirming that
this model fails to mimic the observed spacing ef-
fects. By contrast, in our model the average acq
score is greater in the small spacing intervals (1-
3) than in the massed presentation, mimicking the
Vlach et al (2008) results on children. This happens
because a nw appears more novel with larger spacing
intervals between each of its presentations resulting
in stronger alignments.
We can see two other interesting patterns in our
model: First, the average acq score of words for all
spacing intervals is greater in the imm condition than
in the lat condition. This occurs because there is
more forgetting in the model over the longer reten-
tion interval of lat. Second, in both conditions the
average acq score initially increases from a massed
presentation to the smaller spacing intervals. How-
ever, at spacing intervals between about 3 and 5,
processing an input pair. The acq score in the imm condition is
calculated at time t, which is immediately after the last presen-
tation of nw. The lat condition corresponds to t +20.
the acq score begins to decrease as spacing intervals
grow larger. As explained earlier, the initial increase
in acq scores for small spacing intervals results from
novelty of the words in a spaced presentation. How-
ever, for bigger spacing intervals the effect of nov-
elty is swamped by the much greater degree of for-
getting after a bigger spacing interval.
Although Vlach et al (2008) did not vary their
spacing and retention intervals, other spacing effect
studies on adults have done so. For example, Glen-
berg (1976) presented adults with word pairs to learn
under varying spacing intervals, and tested them af-
ter several different retention intervals (his experi-
ment 1). Our pattern of results in Figure 3 is in line
with his results. In particular, he found a nonmono-
tonic pattern of spacing similar to the pattern in our
model: learning of pairs was improved with increas-
ing spacing intervals up to a point, but there was a
decrease in performance for larger spacing intervals.
Also, the proportion of recalled pairs decreased for
longer retention intervals, similar to our lower per-
formance in the lat condition.
4.3 The Role of Forgetting and Attention
To fully understand the role as well as the neces-
sity of, both forgetting and attention to novelty in
our results, we test two other models under the same
conditions as the previous spacing experiment: (a) a
model with our mechanism for attention to novelty
but not forgetting, and (b) a model with our forget-
ting mechanism but no attention to novelty; see Fig-
ure 4 and Figure 5, respectively.
In the model that attends to novelty but does not
incorporate a memory decay mechanism (Figure 4),
the average acq score consistently increases as spac-
ing intervals grow bigger. This occurs because the
novel words appear more novel following bigger
spacing intervals, and thus attract more alignment
strength. Since the model does not forget, there is
no difference between the immediate (imm) and later
(lat) retention intervals. This pattern does not match
the spacing effect patterns of people, suggesting that
forgetting is a necessary aspect of our model?s abil-
ity to do so in the previous section.
On the other hand, in the model with forgetting
but no attentional mechanism (Figure 5), we see two
different behaviors in the imm and lat conditions. In
the imm condition, the average acq score decreases
85
Figure 4: Average acq score of the novel words over spac-
ing intervals, for the model with novelty but without for-
getting.
Figure 5: Average acq score of the novel words over spac-
ing intervals, for the model with forgetting but without
novelty.
consistently over spacing intervals. This is as ex-
pected, because the greater time between presenta-
tions means a greater degree of forgetting. Specif-
ically, the alignment scores decay more between
presentations of the word to be learned, given the
greater passage of time in larger spacing intervals.
The weaker alignments then lead to lower acq scores
in this condition.
Paradoxically, although this effect on learning
also holds in the lat condition, another factor is at
play, leading to better performance than in the imm
condition at all spacing intervals. Here the greater
retention interval ? the time between the last learn-
ing presentation and the test time ? leads to greater
forgetting in a manner that instead improves the acq
scores. Consider that the meaning representation
for a word includes some probability mass assigned
to irrelevant features ? i.e., those features that oc-
curred in an utterance?scene pair with the word but
are not part of its true meaning. Because such fea-
tures generally have lower probability than relevant
features (which are observed more consistently with
a word), a longer retention interval leads to them de-
caying more than the relevant features. Thus the lat
condition enables the model to better focus on the
features relevant to a word.
In conclusion, neither attention to novelty nor for-
getting alone achieves the pattern typical of the spac-
ing effects in people that our model shows in the
lower two plots in Figure 3. Hence we conclude that
both factors are necessary to our account, suggesting
that it is an interaction between the two that accounts
for people?s behaviour.
4.4 The ?Spacing Crossover Interaction?
In our model with attention to novelty and forgetting
(see Section 4.2), the average acq score was always
better in the imm condition than the lat condition.
However, researchers have observed other patterns
in spacing experiments. A particularly interesting
pattern found in some studies is that the plots of the
results for earlier and later retention intervals cross
as the spacing intervals are increased. That is, with
smaller spacing intervals, a shorter retention inter-
val (such as our imm condition) leads to better re-
sults, but with larger spacing intervals, a longer re-
tention interval (such as our lat condition) leads to
better results (Bahrick, 1979; Pavlik and Anderson,
2005). This interaction of spacing and retention in-
tervals results in a pattern referred to as the spacing
crossover interaction (Pavlik and Anderson, 2005).
This pattern is different from Glenberg?s (1976) ex-
periment and from the pattern of results shown ear-
lier for our model (Figure 3).
We looked at an experiment in which the spac-
ing crossover pattern was observed: Pavlik and An-
derson (2005) taught Japanese?English pairs to sub-
jects, varying the spacing and retention intervals.
One difference we noticed between the experiment
of Pavlik and Anderson (2005) and Glenberg (1976)
was that in the former, the presentation period of the
stimulus was 5 seconds, whereas in the latter, it was
3 seconds. We hypothesize that the difference be-
tween the amount of time for the presentation peri-
86
Figure 6: Average acq score of the novel words over spac-
ing intervals
ods might explain the different spacing patterns in
these experiments.
We currently cannot model presentation time di-
rectly in our model, since having access to an in-
put longer does not change its computation of align-
ments between words and features. However, we
can indirectly model a difference in presentation
time by modifying the amount of memory decay:
We assume that when an item is presented longer, it
is learned better and therefore subject to less forget-
ting. We run the spacing experiment with a smaller
forgetting parameter to model the longer presenta-
tion period used in Pavlik and Anderson?s (2005)
versus Glenberg (1976).6
Our results using the decreased level of forgetting,
given in Figure 6, show the expected crossover inter-
action between the retention and spacing intervals:
for smaller spacing intervals, the acq scores are bet-
ter in the imm condition, whereas for larger spacing
intervals, they are better in the lat condition. Thus,
our model suggests an explanation for the observed
crossover: in tasks which strengthen the learning of
the target item ? and thus lessen the effect of forget-
ting ? we expect to see a benefit of later retention
trials in experiments with people.
5 General Discussion and Future Work
The spacing effect (where people learn items better
when multiple presentations are spread over time)
has been studied extensively and is found to be ro-
bust over different types of tasks and domains. Many
6Here, the decay parameter is set to 0.03.
experiments have examined the spacing effect in the
context of word learning and other similar tasks.
Particularly, in a recent study of Vlach et al (2008),
young children demonstrated a spacing effect in a
novel word learning task.
We use computational modeling to show that by
changing a probabilistic associative model of word
learning to include both a forgetting and attentional
mechanism, the new model can account not only for
the child data, but for various patterns of spacing ef-
fect data in adults. Specifically, our model shows the
nonmonotonic pattern of spacing observed in the ex-
perimental data, where learning improves in shorter
spacing intervals, but worsens in bigger spacing in-
tervals. Our model can also replicate the observed
cross-over interaction between spacing and retention
intervals: for smaller spacing intervals, performance
is better when tested after a shorter retention inter-
val, whereas for bigger spacing intervals, it is better
after longer retention intervals. Finally, our results
confirm that by modelling word learning as a stan-
dalone development process, we cannot account for
the spacing effect. Instead, it is important to con-
sider word learning in the context of fundamental
cognitive processes of memory and attention.
Much remains to be investigated in our model.
For example, most human experiments examine the
effect of frequency of presentations of target items.
Also, the range of retention intervals that has been
studied is greater than what we have considered
here. In the future, we plan to study the effect of
these two parameters. In addition, with our current
model, the amount of time an item is presented to
the learner does not play a role. We can also re-
formulate our alignment mechanism to incorporate
a notion of the amount of time to consider an item
to be learned. Another interesting future direction,
especially in the context of word learning, is to de-
velop a more complete attentional mechanism, that
considers different parameters such as social cues
and linguistic cues. Finally, we will study the role
of forgetting and attention in modelling other rele-
vant experimental data (e.g., Kachergis et al, 2009;
Vlach and Sandhofer, 2010).
87
References
John .R. Anderson and Christian Lebiere. 1998. The
atomic components of thought. Lawrence Erl-
baum Associates.
Harry P. Bahrick. 1979. Maintenance of knowl-
edge: Questions about memory we forgot to ask.
Journal of Experimental Psychology: General,
108(3):296?308.
Harry P. Bahrick and Elizabeth Phelps. 1987. Reten-
tion of Spanish vocabulary over 8 years. Journal
of Experimental Psychology: Learning, Memory,
and Cognition, 13(2):344?349.
Paul Bloom. 2000. How Children Learn the Mean-
ings of Words. MIT Press.
Susan Carey. 1978. The child as word learner. In
M. Halle, J. Bresnan, and G. A. Miller, editors,
Linguistic Theory and Psychological Reality. The
MIT Press.
Malinda Carpenter, Katherine Nagell, Michael
Tomasello, George Butterworth, and Chris
Moore. 1998. Social cognition, joint attention,
and communicative competence from 9 to 15
months of age. Monographs of the Society for
Research in Child Development, 63(4).
Nicholas J. Cepeda, Harold Pashler, Edward Vul,
John T. Wixted, and Doug Rohrer. 2006. Dis-
tributed practice in verbal recall tasks: A review
and quantitative synthesis. Psychological Bul-
letin, 132(3):354 ? 380.
Lauren J. Cuddy and Larry L. Jacoby. 1982. When
forgetting helps memory: an analysis of repetition
effects. Journal of Verbal Learning and Verbal
Behavior, 21(4):451 ? 467.
Frank Dempster. 1989. Spacing effects and their im-
plications for theory and practice. Educational
Psychology Review, 1:309?330.
Frank N. Dempster. 1996. Distributing and manag-
ing the conditions of encoding and practice. Mem-
ory, pages 317?344.
Hermann Ebbinghaus. 1885. Memory: A contri-
bution to experimental psychology. New York,
Teachers College, Columbia University.
Afsaneh Fazly, Afra Alishahi, and Suzanne Steven-
son. 2010. A probabilistic computational model
of cross-situational word learning. Cognitive Sci-
ence, 34(6):1017?1063.
Michael C. Frank, Sharon Goldwater, Thomas L.
Griffiths, and Joshua B. Tenenbaum. 2010. Mod-
eling human performance in statistical word seg-
menation. Cognition, 117:107?125.
Arthur Glenberg. 1979. Component-levels theory of
the effects of spacing of repetitions on recall and
recognition. Memory and Cognition, 7:95?112.
Arthur M. Glenberg. 1976. Monotonic and non-
monotonic lag effects in paired-associate and
recognition memory paradigms. Journal of Ver-
bal Learning & Verbal Behavior, 15(1).
Roberta M. Golinkoff, Kathy Hirsh-Pasek, Leslie M.
Bailey, and Neil R. Wegner. 1992. Young chil-
dren and adults use lexical principles to learn new
nouns. Developmental Psychology, 28(1):99?
108.
Douglas L. Hintzman. 1974. Theoretical implica-
tions of the spacing effect.
Jessica S. Horst, Larissa K. Samuelson, Sarah C.
Kucker, and Bob McMurray. 2011. Whats new?
children prefer novelty in referent selection. Cog-
nition, 118(2):234 ? 244.
Larry L. Jacoby. 1978. On interpreting the effects
of repetition: Solving a problem versus remem-
bering a solution. Journal of Verbal Learning and
Verbal Behavior, 17(6):649 ? 667.
George Kachergis, Chen Yu, and Richard Shiffrin.
2009. Temporal contiguity in cross-situational
statistical learning.
Amy C. MacPherson and Chris Moore. 2010. Un-
derstanding interest in the second year of life. In-
fancy, 15(3):324?335.
Brian MacWhinney. 2000. The CHILDES Project:
Tools for Analyzing Talk, volume 2: The
Database. Erlbaum, 3rd edition.
Ellen M. Markman and Gwyn F. Wachtel. 1988.
Children?s use of mutual exclusivity to constrain
the meanings of words. Cognitive Psychology,
20:121?157.
Arthur W. Melton. 1967. Repetition and retrieval
from memory. Science, 158:532.
88
Aida Nematzadeh, Afsaneh Fazly, and Suzanne
Stevenson. 2012. Interaction of word learning and
semantic category formation in late talking. In
Proc. of CogSci?12. To appear.
Philip I. Pavlik and John R. Anderson. 2005. Prac-
tice and forgetting effects on vocabulary memory:
An activationbased model of the spacing effect.
Cognitive Science, 29:559?586.
W.V.O. Quine. 1960. Word and Object. MIT Press.
Terry Regier. 2005. The emergence of words: At-
tentional learning in form and meaning. Cognitive
Science, 29:819?865.
Jeffery Mark Siskind. 1996. A computational study
of cross-situational techniques for learning word-
to-meaning mappings. Cognition, 61:39?91.
Linda B Smith, Eliana Colunga, and Hanako
Yoshida. 2010. Knowledge as process:
Contextually-cued attention and early word
learning. Cogn Sci, 34(7):1287?314.
Kelly A. Snyder, Michael P. BlanK, and chad J. Mar-
solek. 2008. What form of memory underlies nov-
elty preferences? Psychological Bulletin and Re-
view, 15(2):315 ? 321.
Anna L. Theakston, Elena V. Lieven, Julian M. Pine,
and Caroline F. Rowland. 2001. The role of per-
formance limitations in the acquisition of verb?
argument structure: An alternative account. J. of
Child Language, 28:127?152.
Haley A Vlach and Catherine M Sandhofer. 2010.
Desirable difficulties in cross-situational word
learning.
Haley A. Vlach, Catherine M. Sandhofer, and Nate
Kornell. 2008. The Spacing Effect in Children?s
Memory and Category Induction. Cognition,
109(1):163?167, October.
Chen Yu. 2005. The emergence of links between
lexical acquisition and object categorization: A
computational study. Connection Science, 17(3?
4):381?397.
89
Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 231?240,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Acquisition of Desires before Beliefs: A Computational Investigation
Libby Barak, Afsaneh Fazly, and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
{libbyb,afsaneh,suzanne}@cs.toronto.edu
Abstract
The acquisition of Belief verbs lags be-
hind the acquisition of Desire verbs in
children. Some psycholinguistic theo-
ries attribute this lag to conceptual differ-
ences between the two classes, while oth-
ers suggest that syntactic differences are
responsible. Through computational ex-
periments, we show that a probabilistic
verb learning model exhibits the pattern of
acquisition, even though there is no dif-
ference in the model in the difficulty of
the semantic or syntactic properties of Be-
lief vs. Desire verbs. Our results point
to the distributional properties of various
verb classes as a potentially important, and
heretofore unexplored, factor in the ob-
served developmental lag of Belief verbs.
1 Introduction
Psycholinguistic studies have shown great inter-
est in the learning of Mental State Verbs (MSVs),
such as think and want, given the various cogni-
tive and linguistic challenges in their acquisition.
MSVs refer to an entity?s inner states, such as
thoughts and wishes, which the language learner
must be able to perceive and conceptualize appro-
priately. Moreover, such verbs often appear in a
Sentential Complement (SC) construction, which
is complex for children because of the embedded
clause.
Despite some shared properties, MSVs are
a heterogeneous group, with different types of
verbs exhibiting different developmental patterns.
Specifically, a wealth of research shows that chil-
dren produce Desire verbs, such as want and
wish, earlier than Belief verbs, such as think and
know (Shatz et al, 1983; Bartsch and Wellman,
1995; Asplin, 2002; Perner et al, 2003; de Vil-
liers, 2005; Papafragou et al, 2007; Pascual et al,
2008). Some explanations for this pattern posit
that differences in the syntactic usages of Desire
and Belief verbs underlie the observed develop-
mental lag of the latter (de Villiers, 2005; Pas-
cual et al, 2008). In particular, Desire verbs oc-
cur mostly with an infinitival SC (as in I want
(her) to leave), while Belief verbs occur mostly
with a finite SC (a full tensed embedded clause,
as in I think (that) she left). Notably, infiniti-
vals appear earlier than finite SCs in the speech
of young children (Bloom et al, 1984, 1989).
Others suggest that Desire verbs are conceptu-
ally simpler (Bartsch and Wellman, 1995) or prag-
matically/communicatively more salient (Perner,
1988; Fodor, 1992; Perner et al, 2003). Propo-
nents of the conceptual and pragmatic accounts ar-
gue that syntax alone cannot explain the delay in
the acquisition of Belief verbs, because children
use finite SCs with verbs of Communication (e.g.,
say) and Perception (e.g., see) long before they
use them with Belief verbs (Bartsch and Wellman,
1995).
We use a computational model of verb argu-
ment structure acquisition to shed light on the fac-
tors that might be responsible for the developmen-
tal gap between Desire and Belief verbs. Impor-
tantly, our model exhibits the observed pattern of
learning Desire before Belief verbs, without hav-
ing to encode any differences in difficulty between
the two classes in terms of their syntactic or con-
ceptual/pragmatic requirements. The behaviour of
the model can thus be attributed to its probabilistic
learning mechanisms in conjunction with the dis-
tributional properties of the input. In particular, we
investigate how the model?s learning mechanism
interacts with the distributions of several classes
of verbs ? including Belief, Desire, Perception,
Communication, and Action ? in the finite and
infinitival SC syntax to produce the observed pat-
tern of acquisition of Desire and Belief verbs. Us-
ing a computational model can reveal the poten-
231
tial effects of interactions of verb classes in hu-
man language acquisition which would be difficult
to investigate experimentally. Our results suggest
that the distributional properties of relevant verb
classes are a potentially important, and heretofore
unexplored, factor in experimental studies of the
developmental lag of Belief verbs.
2 The Computational Model
We require an incremental model in which we
can examine developmental patterns as it gradu-
ally learns relevant aspects of argument structures.
This task calls for an ability to represent the se-
mantic and syntactic properties of verb usages, in-
cluding those containing MSVs and other kinds of
verbs taking sentential complements (SCs). Most
computational models of verb argument structure
acquisition have largely focused on physical ac-
tion verbs (Alishahi and Stevenson, 2008; Chang,
2009; Perfors et al, 2010; Parisien and Steven-
son, 2011). Recently, Barak et al (2012) ex-
tended the incremental Bayesian model of Al-
ishahi and Stevenson (2008) to include the syntac-
tic and semantic features required for the process-
ing of MSVs and other verbs that take SCs. While
Barak et al (2012) modeled some developmental
patterns of MSVs overall, their work did not ac-
count for the difference between Desire and Be-
lief verbs. In this section, we present their model,
which we adopt for our experiments. In Section 3,
we describe how we modify the representation of
the input in Barak et al (2012) to enable our inves-
tigation of the differences among the MSV classes.
2.1 Overview of the Model
The input to the Barak et al (2012) model is a
sequence of frames, where each frame is a col-
lection of syntactic and semantic features repre-
senting what the learner might extract from an ut-
terance s/he has heard paired with a scene s/he
has perceived. In particular, we consider syntactic
properties, including syntactic pattern, argument
count, and complement type, as well as seman-
tic properties, including event primitives and event
participants. Table 1 presents a sample frame il-
lustrating possible values for these features.
The model incrementally groups the input
frames into clusters that reflect probabilistic as-
sociations of the syntactic and semantic features
across similar verb usages. Each learned cluster
is a probabilistic (and possibly noisy) representa-
head predicate think
other predicate make
Syntactic Features:
syntactic pattern arg1 verb arg2 verb arg3
argument count 3
complement type SC-fin
Semantic Features:
event primitives { state, consider , cogitate, action }
event participants { experiencer , perceiver , considerer}
{ agent , animate}
{ theme, changed}
Table 1: An example input frame. The Syntactic features
reflect an utterance such as He thinks Mom made pancakes:
i.e., syntactic pattern ?arg1 verb arg2 verb arg3?, 3 arguments,
and finite SC. The Semantic features reflect a corresponding
conceptualized belief event with a physical action described
in the SC ({state, consider , cogitate, action}) whose
?arg1? participant ({experiencer , perceiver , considerer})
perceives the ?arg2? ({agent , animate}) acting on the ?arg3?
({theme, changed}).
tion of an argument structure construction: e.g.,
a cluster containing frames corresponding to us-
ages such as I eat apples, She took the ball, and
He got a book, etc., represents a Transitive Action
construction.1 Note that a cluster operates as more
than simply a set of similar frames: The model
can use the probabilistic associations among the
various features of the frames in a cluster to gen-
eralize over the individual verb usages that it has
seen. For example, if the model is presented with a
frame corresponding to a transitive utterance using
a verb it has not observed before, such as She gor-
ped the ball, the example cluster above would lead
the model to predict that gorp has semantic event
primitives in common with other Action verbs like
eat, take, and get. Such probabilistic reasoning is
especially powerful because clusters involve com-
plex interactions of features, and the model rea-
sons across all such clusters to make suitable gen-
eralizations over its learned knowledge.
2.2 Algorithm for Learning Clusters
The model groups input frames into clusters on
the basis of the overall similarity in the values of
their syntactic and semantic features. Importantly,
the model learns these clusters incrementally; the
number and type of clusters is not predetermined.
The model considers the creation of a new cluster
for a given frame if the frame is not sufficiently
similar to any of the existing clusters. Formally,
the model finds the best cluster for a given input
1Note that, because the associations are probabilistic, a
construction may be represented by more than one cluster.
232
frame F as in:
BestCluster(F ) = argmax
k?Clusters
P (k|F ) (1)
where k ranges over all existing clusters and a new
one. Using Bayes rule:
P (k|F ) = P (k)P (F |k)P (F ) ? P (k)P (F |k) (2)
The prior probability of a cluster P (k) is estimated
as the proportion of frames that are in k out of
all observed input frames, thus assigning a higher
prior to larger clusters, representing more frequent
constructions. The likelihood P (F |k) is estimated
based on the match of feature values in F and in
the frames of k (assuming independence of the
features):
P (F |k) =
?
i?Features
Pi(j|k) (3)
where i refers to the ith feature of F and j refers
to its value, and Pi(j|k) is calculated using a
smoothed version of:
Pi(j|k) =
counti(j, k)
nk
(4)
where counti(j, k) is the number of times feature
i has the value j in cluster k, and nk is the number
of frames in k.
2.3 Attention to Mental Content
One factor proposed to play an important role in
the acquisition of MSVs is the difficulty children
have in being aware of (or perceiving the salience
of) the mental content of a scene that an utterance
may be describing (Papafragou et al, 2007). This
difficulty arises because the aspects of a scene as-
sociated with an MSV ? the ?believing? or the
?wanting? ? are not directly observable, as they
involve the inner states of an event participant. In-
stead, younger children tend to focus on the phys-
ical (observable) parts of the scene, which gener-
ally correspond to the event described in the em-
bedded clause of an MSV utterance. For instance,
young children may focus on the ?making? action
in He thinks Mom made pancakes, rather than on
the ?thinking?.
A key component of the model of Barak
et al (2012) is a mechanism that simulates the
gradually-developing ability in children to attend
to the mental content rather than solely to the (em-
bedded) physical action. This mechanism basi-
cally entails that the model may ?misinterpret? an
input frame containing an MSV as focusing on the
semantics of the action in the sentential comple-
ment. Specifically, when receiving an input frame
with an MSV, as in Table 1, there is a probability p
that the frame is perceived with attention to the se-
mantics corresponding to the physical action verb
(here, make). In this case, the model correctly in-
cludes the syntactic features as in Table 1, on the
assumption that the child can accurately note the
number and pattern of arguments. However, the
model replaces the semantic features with those
that correspond to the physical action event and its
participants. At very early stages, p is very high
(close to 1), simulating the much greater saliency
of physical actions compared to mental events for
younger children. As the model ?ages? (i.e., re-
ceives more input), p decreases, giving more and
more attention to the mental content, gradually ap-
proaching adult-like abilities.
3 Experimental Setup
3.1 Generation of the Input Corpora
Because there are no readily available large cor-
pora of actual child-directed speech (CDS) associ-
ated with appropriate semantic representations, we
generate artificial corpora for our simulations that
mimic the relevant syntactic properties of CDS
along with automatically-produced semantic prop-
erties. Importantly, these artificial corpora have
the distributional properties of the argument struc-
tures for the verbs under investigation based on
an analysis of verb usages in CDS. To accomplish
this, we adopt and extend the input-generation lex-
icon of Barak et al (2012), which is used to au-
tomatically generate the syntactic and semantic
features of the frames that serve as input to the
model. Using this lexicon, each simulation cor-
pus is created through a probabilistic generation of
argument structure frames according to their rela-
tive frequencies of occurrence in CDS. Since the
corpora are probabilistically generated, all exper-
imental results are averaged over simulations on
100 different input corpora, to ensure the results
are not dependent on idiosyncratic properties of a
single generated corpus.
Our input-generation lexicon contains 31 verbs
from various semantic classes and different fre-
quency ranges; these verbs appear in a variety
233
Semantic Verb Frequency % Relative
class frequency with
SC-fin SC-inf
Belief think 13829 100 -
bet 391 100 -
guess 278 76 -
know 7189 61 -
believe 78 21 -
Desire wish 132 94 -
hope 290 86 -
want 8425 - 76
like 6944 - 51
need 1690 - 60
Communication tell 2953 64 -
say 8622 60 -
ask 818 29 10
speak 62 - -
talk 1322 - -
Perception hear 1370 21 25
see 9717 14 -
look 5856 9 -
watch 1045 - 27
listen 413 33 2
Action go 20364 - 5
get 16493 - 14
make 4165 - 10
put 8794 - -
come 6083 - -
eat 3894 - -
take 3239 - -
play 2565 - -
sit 2462 - -
give 2341 - -
fall 1555 - -
Table 2: The list of our 31 verbs from the five semantic
classes, along with their overall frequency, and their rela-
tive frequency with the finite SC (SC-fin) or the infinitival
SC (SC-inf).
of syntactic patterns including the sentential com-
plement (SC) construction. Our focus here is on
learning the Belief and Desire classes; however,
we include verbs from other classes to have a re-
alistic context of MSV acquisition in the presence
of other types of verbs. In particular, we include
(physical) Action verbs because of their frequent
usage in CDS, and we include Communication
and Perception groups because of their suggested
role in the acquisition of MSVs (Bloom et al,
1989; de Villiers, 2005). Table 2 lists the verbs of
each semantic class, along with their overall fre-
quency and their relative frequency with the finite
(SC-fin) and infinitival SC (SC-inf) in our data.
For each of these 31 verbs, the distributional in-
formation about its argument structure was manu-
ally extracted from a random sample of 100 CDS
usages (or all usages if fewer than 100) from eight
corpora from CHILDES (MacWhinney, 2000).2
The input-generation lexicon then contains the
overall frequency of each verb, as well as the rela-
tive frequency with which it appears with each of
its argument structures. Each argument structure
entry for a verb also contains the values for all the
syntactic and semantic features in a frame (see Ta-
ble 1 for an example), which are determined from
the manual inspection of the usages.
The values for syntactic features are based on
simple observation of the order and number of
verbs and arguments in the usage, and, if an ar-
gument is an SC, whether it is finite or infiniti-
val. We add this latter feature (the type of the
SC) to the syntactic representation used by Barak
et al (2012) to allow distinguishing the syntac-
tic properties associated with Desire and Belief
verbs. Note that this feature does not incorporate
any potential level of difficulty in processing an
infinitival vs. finite SC; the feature simply records
that there are three different types of embedded ar-
guments: SC-inf, SC-fin, or none. Thus, while
Desire and Belief verbs that typically occur with
an SC-inf or SC-fin have a distinguishing feature,
there is nothing in this representation that makes
Desire verbs inherently easier to process. This
syntactic representation reflects our assumptions
that a learner: (i) understands basic syntactic prop-
erties of an utterance, such as syntactic categories
(e.g., noun and verb) and word order; and (ii) dis-
tinguishes between a finite complement, as in He
thinks that Mom left, and an infinitival, as in He
wants Mom to leave.
The values for the semantic features of a verb
and its arguments are based on a simple taxonomy
of event and participant role properties adapted
from several resources, including Alishahi and
Stevenson (2008), Kipper et al (2008), and Dowty
(1991). In particular, we assume that the learner is
able to perceive and conceptualize the general se-
mantic properties of different kinds of events (e.g.,
state and action), as well as those of the event par-
ticipants (e.g., agent, experiencer, and theme). In
an adaptation of the lexicon of Barak et al, we
make minimal assumptions about shared seman-
tics across verb classes. Specifically, to encode
suitable semantic distinctions among MSVs, and
between MSVs and other verbs, we aimed for a
representation that would capture reasonable as-
2Brown (1973); Suppes (1974); Kuczaj (1977); Bloom
et al (1974); Sachs (1983); Lieven et al (2009).
234
sumptions about high-level similarities and differ-
ences among the verb classes. As with the syn-
tactic features, we ensured that we did not simply
encode the result we are investigating (that chil-
dren have facility with Desire verbs before Be-
lief verbs) by making the representation for Desire
verbs easier to learn.
In the results presented in Section 4, ?our
model? refers to the computational model of Barak
et al (2012) together with our modifications to the
input representation.
3.2 Simulations and Verb Prediction
Psycholinguistic studies have used variations of
a novel verb prediction task to examine how
strongly children (or adults) have learned to asso-
ciate the various syntactic and semantic properties
of a typical MSV usage. In particular, the typical
Desire verb usage combines desire semantics with
an infinitival SC syntax, while the typical Belief
verb usage combines belief semantics with a finite
SC syntax. In investigating the salience of these
associations in human experiments, participants
are presented with an utterance containing a nonce
verb with an SC (e.g., He gorped that his grand-
mother was in the bed), sometimes paired with a
corresponding scene representing a mental event
(e.g., a picture or a silent video depicting a think-
ing event with heightened saliency). An experi-
menter then asks each participant what the nonce
verb (gorp) ?means? ? i.e., what existing English
verb does it correspond to (see, e.g., Asplin, 2002;
Papafragou et al, 2007). The expectation is that,
e.g., if a participant has a well-entrenched Belief
construction, then they should have a strong as-
sociation between the finite-SC syntax and belief
semantics, and hence should produce more Belief
verbs as the meaning of a novel verb in an finite-
SC utterance (and analogously for infinitival SCs
and Desire verbs).
We perform simulations that are based on such
psycholinguistic experiments. After training the
model on some number of input frames, we then
present it with a test frame in which the main verb
(head predicate) is replaced by a nonce verb like
gorp (a verb that doesn?t occur in our lexicon).
Analogously to the human experiments, in order
to study the differences in the strength of associ-
ation between the syntax and semantics of Desire
and Belief verbs, we present the model with two
types of test frames: (i) a typical desire test frame,
with syntactic features corresponding to the infini-
tival SC syntax, optionally paired (depending on
the experiment) with semantic features associated
with a Desire verb in our lexicon; and (ii) a typi-
cal belief test frame, with syntactic features corre-
sponding to the finite SC syntax, optionally paired
with semantic features from a Belief verb.3
Given a test frame Ftest, we use the clusters
learned by the model to calculate the likelihood of
each of the 31 verbs v as the response of the model
indicating the meaning of the novel verb, as in:
P (v|Ftest) (5)
=
?
k?Clusters
Phead(v|k)P (k|Ftest)
?
?
k?Clusters
Phead(v|k)P (Ftest|k)P (k)
where Phead(v|k) is the probability of the head
feature having the value v in cluster k, calculated
as in Eqn. (4); P (Ftest|k) is the probability of the
test frame Ftest given cluster k, calculated as in
Eqn. (3); and P (k) is the prior probability of clus-
ter k, calculated as explained in Section 2.2.
What we really want to know is the likelihood
of the model producing a verb from each of the
semantic classes, rather than the likelihood of any
particular verb. For each test frame, we calculate
the likelihood of each semantic class by summing
the likelihoods of the verbs in that class:
P (Class|Ftest) =
?
vc?Class
P (vc|Ftest)
where vc is one of the verbs in Class, and Class
ranges over the 5 classes in Table 2. We average
the verb class likelihoods across the 100 simula-
tions.
4 Experimental Results
The novel verb prediction experiments described
above have found differences in the performance
of children across the two MSV classes (e.g., As-
plin, 2002; Papafragou et al, 2007). For exam-
ple, children performed better at predicting that a
novel verb is a Desire verb in a typical desire con-
text (infinitival-SC utterance paired with a desire
scene), compared to their performance at identify-
ing a novel verb as a Belief verb in a typical belief
3Table 2 shows that, in our data, Belief verbs occur ex-
clusively with finite clauses in an SC usage. Although Desire
verbs occur in both SC-inf and SC-fin usages, the former out-
number the latter by almost 30 to 1 over all Desire verbs.
235
context (finite-SC utterance accompanied by a be-
lief scene). In Section 4.1, we examine whether
the model exhibits this behaviour in our verb class
prediction task, thereby mimicking children?s lag
in facility with Belief verbs compared to Desire
verbs.
Recall that some researchers attribute the
above-mentioned developmental gap to the con-
ceptual and pragmatic differences between the two
MSV classes, whereas others suggest it is due to a
difference in the syntactic requirements of the two
classes. As noted in Section 3.1, we have tailored
our representation of Desire and Belief verbs to
not build in any differences in the ease or difficulty
of acquiring their syntactic or semantic properties.
Moreover, the possibility in the model for ?misin-
terpretation? of mental content as action semantics
(see Section 2.3) also applies equally to both types
of verbs. Thus, any observed performance gap in
the model reflects an interaction between its pro-
cessing approach and the distributional properties
of CDS. To better understand the role of the in-
put, in Section 4.2 we examine how the distribu-
tional pattern of appearances of various semantic
classes of verbs (including Belief, Desire, Com-
munication, Perception and Action verbs) with the
finite and infinitival SC constructions affects the
learning of the two types of MSVs.
4.1 Verb Prediction Simulations
Here we compare the verb prediction responses of
the participants in the experiments of Papafragou
et al (2007) (PCG), with those of the model when
presented with a novel verb in a typical desire or
belief test frame. (See Section 3.2 for how we con-
struct these frames.) PCG report verb responses
for the novel verb meaning as desire, belief, or ac-
tion, where the latter category contains all other
verb responses. Looking closely at the latter cat-
egory in PCG, we find that most verbs are what
we have termed (physical) Action verbs. We thus
report the verb class likelihoods of the model for
the Belief, Desire, and Action verbs in our lexi-
con. To compare the model?s responses with those
of the children and adults in PCG, we report the
responses of the model to the test frames at two
test points: after training the model with 500 in-
put frames, resembling the ?Child stage?, and after
presenting the model with 10, 000 input frames,
representing the ?Adult stage?.
Figure 1(a) gives the percent verb types from
(a) Human participants in Papafragou et al (2007)
(b) The model
Figure 1: (a) Percent verb types produced by adult and
child participants given a desire or belief utterance and scene.
(b) The model?s verb class likelihoods given a desire or be-
lief test frame. Child stage is represented by 500 input frames
compared to the 10, 000 input frames for Adult stage.
PCG;4 Figure 1(b) presents the results of the
model. Similarly to the children in PCG, the
model at earlier stages of learning (?Child stage?)
is better at predicting Desire verbs for a desire test
frame (.56) than it is at predicting Belief verbs for
a belief test frame (.42) ? cf. 59% Desire vs.
41% Belief prediction for PCG. In addition, as for
both the children and adult participants of PCG,
the model produces more Action verbs in a desire
context than in a belief context at both stages.
We note that although the adult participants of
PCG perform well at identifying both Desire and
Belief verbs, the model does not identify Belief
verbs with the same accuracy as it does Desire
verbs, even after processing 10, 000 input frames
(i.e., the ?Adult stage?). In Section 4.2, we will see
that this is due to the model forming strong asso-
ciations between the Communication and Percep-
tion verbs and the SC-fin usage (the typical syn-
tax of Belief verbs). These associations might be
4Based on results presented in Table 4, Page 149 in Pa-
pafragou et al (2007), for the utterance and scene condition.
236
overly strong in our model because of the limited
number of verbs and verb classes ? an issue we
will need to address in the future. We also note
that, unlike the results of PCG, the model only
rarely produces Desire verbs in a Belief context.
This also may be due to our choice of Desire verbs,
which have extremely few SC-fin usages overall.
To summarize, similarly to children (Asplin,
2002; Papafragou et al, 2007), the model per-
forms better at identifying Desire verbs compared
to Belief verbs. Moreover, we replicate the ex-
perimental results of PCG without encoding any
conceptual or syntactic differences in difficulty be-
tween the two types of verbs. Specifically, because
the representation of Desire and Belief classes in
our experiments does not build in a bias due to the
ease of processing Desire verbs, the differential
results in the model must be due to the interac-
tion of the different distributional patterns in CDS
(see Table 2) and the processing approach of the
model. Although this finding does not rule out the
role of conceptual or syntactic differences between
Desire and Belief verbs in delayed acquisition of
the latter, it points to the importance of the dis-
tributional patterns as a potentially important and
relevant factor worth further study in human ex-
periments. We further investigate this hypothesis
in the following section.
4.2 A Closer Look at the Role of Syntax
The goal of the experiments presented here is to
understand how an interaction among the 5 dif-
ferent semantic classes of verbs, in terms of their
distribution of appearance with the two types of
SC constructions, coupled with the probabilistic
?misinterpretation? of MSVs in the model, might
play a role in the acquisition of Desire before Be-
lief verbs. Because our focus is on the syntactic
properties of the verbs, we present the model with
partial test frames containing a novel verb and syn-
tactic features that correspond to either a finite SC
usage (the typical use of a Belief verb) or an infini-
tival SC usage (the typical use of a Desire verb).5
We refer to the partial test frames as SC-fin or SC-
inf test frames. We test the model periodically,
over the course of 10, 000 input frames, in order
to examine the progression of the verb class like-
5Verb prediction given an isolated utterance has been per-
formed with adult participants (e.g., Gleitman et al, 2005;
Papafragou et al, 2007). Here we simulate the settings of
such experiments, but do not compare our results with the
experimental data, since they have not included children.
(a) Model?s likelihoods given SC-inf test frame
(b) Model?s likelihoods given SC-fin test frame
Figure 2: The model?s verb class likelihoods for the indi-
vidual semantic classes.
lihoods over time.
First, we examine the verb class prediction like-
lihoods, given an SC-inf test frame; see Fig-
ure 2(a). We can see that all through training,
the likelihoods are mainly divided between Desire
and Action verbs, with the Desire likelihood im-
proving over time. Looking at Table 2, we note
that the Desire and Action verbs have the highest
frequency of occurrence with SC-inf (taking into
account both the overall frequency of verbs, and
their relative frequency with SC-inf), contributing
to their strength of association with the infinitival-
SC syntax. Note that the very high likelihood of
Action verbs given an SC-inf test frame, especially
at the earlier stages of training, cannot be solely
due to their occurrence with SC-inf, since these
verbs mostly occur with other syntactic patterns.
Recall that the model incorporates a mechanism
that simulates a higher probability of erroneously
attending to the physical action (as opposed to the
mental event) at earlier stages, simulating what has
been observed in young children (see Section 2.3
for details). We believe that this mechanism is re-
237
sponsible for some of the Action verb responses of
the model for an SC-inf test frame.
Next, we look at the pattern of verb class likeli-
hoods given an SC-fin test frame; see Figure 2(b).
We can see that the likelihoods here are divided
across a larger number of classes ? namely, Ac-
tion, Communication, and Perception ? com-
pared with Figure 2(a) for the SC-inf test frame.
Since Action verbs do not occur in our data with
SC-fin (see Table 2), their likelihood here comes
from the misinterpretation of mental events (ac-
companied with SC-fin) as action. The initially
high likelihoods of Communication and Percep-
tion verbs results from their high frequency of oc-
currence with SC-fin. Because at this stage Belief
verbs are not always correctly associated with SC-
fin due to the high probability of misinterpreting
them as action, we see a lower likelihood of pre-
dicting Belief verbs. Eventually, the model pro-
duces more Belief responses than any other verb
class, since Beliefs have the highest frequency of
occurrence with the finite-SC syntax.
To summarize, our results here confirm our hy-
pothesis that the distributional properties of the
verb classes with the finite and infinitival SC pat-
terns, coupled with the learning mechanisms of
the model, account for the observed developmen-
tal pattern of MSV acquisition in our model.
5 Discussion
We use a computational model of verb argument
structure learning to shed light on the factors that
might underlie the earlier acquisition of Desire
verbs (e.g., wish and want) than Belief verbs (e.g.,
think and know). Although this developmental gap
has been noted by many researchers, there are at
least two competing theories as to what might be
the important factors: differences in the concep-
tual/pragmatic requirements (e.g., Fodor, 1992;
Bartsch and Wellman, 1995; Perner et al, 2003),
or differences in the syntactic properties (e.g., de
Villiers, 2005; Pascual et al, 2008). Using a com-
putational model, we suggest other factors that
may play a role in an explanation of the observed
gap, and should be taken into account in experi-
mental studies on human subjects.
First, we show that the model exhibits a simi-
lar pattern to children, in that it performs better at
predicting Desire verbs compared to Belief verbs,
given a novel verb paired with typical Desire or
Belief syntax and semantics, respectively. This
difference in performance suggests that the model
forms a strong association between the desire se-
mantics and the infinitival-SC syntax ? one that
is formed earlier and is stronger than the associa-
tion it forms between the belief semantics and the
finite-SC syntax. Importantly, the replication of
this behaviour in the model does not require an
explicit encoding of conceptual/pragmatic differ-
ences between Desire and Belief verbs, nor of a
difference between the two types of SC syntax (fi-
nite and infinitival) with respect to their ease of
acquisition. Instead, we find that what is responsi-
ble for the model?s behaviour is the distribution of
the semantic verb classes (Desire, Belief, Percep-
tion, Communication, and Action) with the finite
and infinitival SC syntactic patterns in the input.
Children are also found to produce
semantically-concrete verbs, such as Com-
munication (e.g., say) and Perception verbs (e.g.,
see), with the finite SC before they produce
(more abstract) Belief verbs with the same syntax.
Psycholinguistic theories have different views
on what this observation tells us about the delay
in the acquisition of Belief verbs. For example,
Bartsch and Wellman (1995) suggest that the
earlier production of Communication verbs shows
that even when children have learned the finite-SC
syntax (and use it with more concrete verbs),
they lack the required conceptual development
to talk about the beliefs of others. Our results
suggest a different take on these same findings:
because Communication (and Perception) verbs
also frequently appear with the finite-SC syntax in
the input, the model learns a relatively strong as-
sociation between each of these semantic classes
and the finite SC. This in turn causes a delay in
the formation of a sufficiently-strong association
between the Belief verbs and that same syntax,
compared with the association between the Desire
verbs and the infinitival SC.
de Villiers (2005) suggests that associating
Communication verbs with the finite-SC syntax
has a facilitating effect on the acquisition of Be-
lief verbs. In our model, we observe a competi-
tion between Communication and Belief verbs, in
terms of their association with the finite-SC syn-
tax. To further explore the hypothesis of de Vil-
liers (2005) will require expanding our model with
enriched semantic representations that enable us to
investigate the bootstrapping role of Communica-
tion verbs in the acquisition of Beliefs.
238
References
Afra Alishahi and Suzanne Stevenson. 2008. A
computational model of early argument struc-
ture acquisition. Cognitive Science, 32(5):789?
834.
Kristen N. Asplin. 2002. Can complement frames
help children learn the meaning of abstract
verbs? Ph.D. thesis, UMass Amherst.
Libby Barak, Afsaneh Fazly, and Suzanne Steven-
son. 2012. Modeling the acquisition of mental
state verbs. NAACL-HLT 2012.
Karen Bartsch and Henry M. Wellman. 1995.
Children talk about the mind. New York: Ox-
ford Univ. Press.
Lois Bloom, Lois Hood, and Patsy Lightbown.
1974. Imitation in language development:
If, when, and why. Cognitive Psychology,
6(3):380?420.
Lois Bloom, Matthew Rispoli, Barbara Gartner,
and Jeremie Hafitz. 1989. Acquisition of com-
plementation. Journal of Child Language,
16(01):101?120.
Lois Bloom, Jo Tackeff, and Margaret Lahey.
1984. Learning to in complement constructions.
Journal of Child Language, 11(02):391?406.
Roger Brown. 1973. A first language: The early
stages. Harvard Univ. Press.
Nancy Chih-Lin Chang. 2009. Constructing
grammar: A computational model of the emer-
gence of early constructions. Ph.D. thesis, Uni-
versity of California, Berkeley.
Jill G. de Villiers. 2005. Can language acquisi-
tion give children a point of view. In Why Lan-
guage Matters for Theory of Mind, pages 199?
232. Oxford Univ. Press.
David Dowty. 1991. Thematic Proto-Roles and
Argument Selection. Language, 67(3):547?
619.
Jerry A Fodor. 1992. A theory of the child?s theory
of mind. Cognition, 44(3):283?296.
Lila R. Gleitman, Kimberly Cassidy, Rebecca
Nappa, Anna Papafragou, and John C.
Trueswell. 2005. Hard words. Language
Learning and Development, 1(1):23?64.
Karin Kipper, Anna Korhonen, Neville Ryant, and
Martha Palmer. 2008. A large-scale classifica-
tion of English verbs. Language Resources and
Evaluation, 42(1):21?40.
A. Kuczaj, Stan. 1977. The acquisition of regular
and irregular past tense forms. Journal of Verbal
Learning and Verbal Behavior, 16(5):589?600.
Elena Lieven, Dorothe? Salomo, and Michael
Tomasello. 2009. Two-year-old children?s pro-
duction of multiword utterances: A usage-based
analysis. Cognitive Linguistics, 20(3):481?507.
B. MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk, volume 2. Psychology
Press.
Anna Papafragou, Kimberly Cassidy, and Lila
Gleitman. 2007. When we think about think-
ing: The acquisition of belief verbs. Cognition,
105(1):125?165.
Christopher Parisien and Suzanne Stevenson.
2011. Generalizing between form and meaning
using learned verb classes. In Proceedings of
the 33rd Annual Meeting of the Cognitive Sci-
ence Society.
Bele?n Pascual, Gerardo Aguado, Mar??a Sotillo,
and Jose C Masdeu. 2008. Acquisition of men-
tal state language in Spanish children: a longitu-
dinal study of the relationship between the pro-
duction of mental verbs and linguistic develop-
ment. Developmental Science, 11(4):454?466.
Amy Perfors, Joshua B. Tenenbaum, and Eliz-
abeth Wonnacott. 2010. Variability, negative
evidence, and the acquisition of verb argu-
ment constructions. Journal of Child Language,
37(03):607?642.
Josef Perner. 1988. Developing semantics for the-
ories of mind: From propositional attitudes to
mental representation. Developing theories of
mind, pages 141?172.
Josef Perner, Manuel Sprung, Petra Zauner, and
Hubert Haider. 2003. Want That is understood
well before Say That, Think That, and False Be-
lief: A test of de Villiers?s linguistic determin-
ism on German?speaking children. Child devel-
opment, 74(1):179?188.
Jacqueline Sachs. 1983. Talking about the There
and Then: The emergence of displaced refer-
ence in parent?child discourse. Children?s lan-
guage, 4.
Marilyn Shatz, Henry M. Wellman, and Sharon
Silber. 1983. The acquisition of mental verbs:
A systematic investigation of the first reference
to mental state. Cognition, 14(3):301?321.
239
Patrick Suppes. 1974. The semantics of children?s
language. American psychologist, 29(2):103.
240
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 37?45,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Learning Verb Classes in an Incremental Model
Libby Barak, Afsaneh Fazly, and Suzanne Stevenson
Department of Computer Science
University of Toronto
Toronto, Canada
{libbyb,afsaneh,suzanne}@cs.toronto.edu
Abstract
The ability of children to generalize over
the linguistic input they receive is key to
acquiring productive knowledge of verbs.
Such generalizations help children extend
their learned knowledge of constructions
to a novel verb, and use it appropriately in
syntactic patterns previously unobserved
for that verb?a key factor in language
productivity. Computational models can
help shed light on the gradual development
of more abstract knowledge during verb
acquisition. We present an incremental
Bayesian model that simultaneously and
incrementally learns argument structure
constructions and verb classes given nat-
uralistic language input. We show how the
distributional properties in the input lan-
guage influence the formation of general-
izations over the constructions and classes.
1 Introduction
Usage-based accounts of language learning note
that young children rely on verb-specific knowl-
edge to produce their early utterances (e.g.,
Tomasello, 2003). However, evidence suggests
that even young children can generalize their
verb knowledge to novel verbs and syntactic
frames (e.g., Fisher, 2002), and that the abstract
knowledge gradually strengthens over time (e.g.,
Tomasello and Abbot-Smith, 2002). One area of
verb usage where more sophisticated abstraction
appears necessary for fully adult productivity in
language is the knowledge of verb alternations.
A verb alternation is a pairing of constructions
shared by a number of verbs, in which the two
constructions express related argument structures
(Levin, 1993): e.g., the dative alternation involves
the related forms of the prepositional dative (PD;
X gave Y to Z) and the double-object dative (DO; X
gave Z Y). Such alternations enable language users
to readily adapt new and low frequency verbs to
appropriate constructions of the language by gen-
eralizing the observed use of one such form to the
other.
1
For example, Conwell and Demuth (2007) show
that 3-year-old children understand that a novel
verb observed only in the DO dative (John gor-
ped Heather the book) can also be used in the PD
form (John gorped the book to Heather), though
the children can only generalize such knowledge
under certain experimental conditions. Wonnacott
et al. (2008) demonstrate the proficiency of adults
in making such generalizations within an artificial
language learning scenario, which enables the re-
searchers to explore the distributional properties
of the linguistic input that facilitate the acquisition
of such generalizations. The results suggest that
the overall frequency of the syntactic patterns as
well as the distribution of verbs across the patterns
play a facilitatory role in the formation of abstract
verb knowledge (in the form of verb alternations)
in adult language learners.
In this work, we propose a computational
model that extends an existing Bayesian model of
verb argument structure acquisition (Alishahi and
Stevenson, 2008)[AS08] to support the learning of
verb classes over the acquired constructions. Our
model is novel in its approach to verb class forma-
tion, because it clusters tokens of a verb that reflect
the distribution of the verb over the learned con-
structions each time the verb is used in an input.
That is, the model forms verb classes by cluster-
ing verb tokens that reflect the evolving usages of
the verbs in various constructions.
We use this new model to analyze the role of
the classes and the distributional properties of the
input in learning abstract verb knowledge, given
1
The generalization of an alternation refers to a speaker
using one variant of an alternation for a verb (e.g., PD) having
only observed the verb in the other variant (e.g., DO).
37
naturalistic input that contains many verbs and
many constructions. The model can form higher-
level generalizations such as learning verb alterna-
tions, which is not possible with the AS08 model
(cf. the findings of Parisien and Stevenson, 2010).
Moreover, because our model gradually forms its
representations of constructions and classes over
time (in contrast to other Bayesian models, such
as Parisien and Stevenson, 2010; Perfors et al.,
2010), it is possible to analyze the monotonically-
growing representations and show their compati-
bility with the developmental patterns seen in chil-
dren (Conwell and Demuth, 2007). We also repli-
cate some of the observations of Wonnacott et al.
(2008) on the role of distributional properties of
the language in influencing the degree of general-
ization over an alternation.
2 Related Work
To explore the properties of learning mechanisms
that are capable of mimicking child and adult psy-
cholinguistic observations, a number of cognitive
modeling studies have focused on learning ab-
stract verb knowledge from individual verb usages
(e.g., Alishahi and Stevenson, 2008; Perfors et al.,
2010; Parisien and Stevenson, 2010). Here we fo-
cus on such computational models that enable the
sort of higher-level generalization that people do
across verb alternations, unlike the AS08 model.
The hierarchical Bayesian models of Perfors
et al. (2010) and Parisien and Stevenson (2010)
focus on learning this kind of higher-level general-
ization. The model of Perfors et al. (2010) learns
verb alternations, i.e., pairs of syntactic patterns
shared by certain groups of verbs. By incorpo-
rating this sort of abstract knowledge into their
model, Perfors et al. are able to simulate the abil-
ity of adults to generalize across verb alternations
(as in Wonnacott et al., 2008). That is, Perfors
et al. predict the ability of a novel verb to occur
in a syntactic structure after exposure to it in the
alternative pattern of that alternation. However,
this model is trained on data that contains only a
limited number of verbs and syntactic patterns un-
like naturalistic Child-directed Speech (CDS) and
moreover incorporates built-in information about
verb constructions.
The hierarchical Dirichlet model of Parisien
and Stevenson (2010) addresses these limitations
by working with natural child-directed speech
(CDS) data. Moreover, the model of Parisien and
Stevenson simultaneously learns constructions as
in AS08 and verb classes based on verb alterna-
tion behaviour, showing that the latter level of ab-
straction is necessary to support effective learn-
ing of verb alternations. Still, the models of both
Parisien and Stevenson and Perfors et al. can only
be utilized as a batch process and hence are lim-
ited in the analysis of developmental trajectories.
Although it is possible to simulate development by
training such models on increasing portions of in-
put, such an approach does not ensure that the rep-
resentations given n + i inputs can be developed
from the representation given n inputs.
In this paper, we propose a significant extension
to the model of AS08, by adding an extra level of
abstraction that incrementally learns verb classes
by drawing on the distribution of verbs over the
learned constructions. The new model combines
the advantages of having a monotonic clustering
model that enables the analysis of developing clus-
ters, with the simultaneous learning of construc-
tions and verb classes.
3 The Computational Model
As mentioned above, our model is an extension
of the model of AS08 in which we add a level of
learned abstract knowledge about verbs. Specif-
ically, our model uses a Bayesian clustering pro-
cess to learn clusters of verb usages that occur in
similar argument structure constructions, as in the
original model of AS08. To this, we add another
level of abstraction that learns clusters of verbs
that exhibit similar distributional patterns of oc-
currence across the learned constructions?that is,
classes of verbs that occur in similar sets of con-
structions, and in similar proportions. To distin-
guish between the clusters of the two levels of ab-
straction in our new model, we refer to the clusters
of verb usages as constructions, and to the group-
ings of verbs given their distribution over those
constructions as verb classes.
3.1 Overview of the Model
The model learns from a sequence of frames,
where each frame is a collection of features rep-
resenting what the learner might extract from an
utterance s/he has heard. Similarly to previous
computational studies (e.g., Parisien and Steven-
son, 2010), here we focus on syntactic features
since our goal is to understand the acquisition of
acceptable syntactic structures of verbs indepen-
38
Figure 1: A visual representation of the two levels of ab-
straction in the model, with sample verb usages input (and
extracted input frames), constructions, and classes.
dently of their meaning, as in some relevant psy-
cholinguistic (Wonnacott et al., 2008) and com-
putational studies (Parisien and Stevenson, 2010).
We focus particularly on properties such as syn-
tactic slots and argument count. (These features,
as in Parisien and Stevenson (2010), provide a
more flexible and generalizable representation of a
syntactic structure than the syntactic pattern string
used by AS08.) See the bottom rows of boxes in
Figure 1 for sample input verb usages with their
extracted frames.
The model incrementally clusters the extracted
input frames into constructions that reflect prob-
abilistic associations of the features across simi-
lar verb usages; see the middle level of Figure 1.
Each learned cluster is a probabilistic (and possi-
bly noisy) representation of an argument structure
construction: e.g., a cluster containing frames cor-
responding to usages such as I eat apples, She took
the ball, and He got a book, etc., represents a Tran-
sitive Action construction.
2
Such constructions al-
low for some degree of generalization over the ob-
served input; e.g., when seeing a novel verb in a
Transitive utterance, the model predicts the simi-
larity of this verb to other Action verbs appearing
in that pattern (Alishahi and Stevenson, 2008).
Grouping of verb usages into constructions may
not be sufficient for making higher-level general-
izations across verb alternations. Knowledge of al-
ternations is only captured indirectly in construc-
tions (because usages of the same verb can oc-
cur in multiple clusters). Following Parisien and
Stevenson (2010), we hypothesize that true gen-
eralization behaviour requires explicit knowledge
that verbs have commonalities in their patterns of
occurrence across constructions; this is the basis
2
Because the associations are probabilistic, a linguistic
construction may be represented by more than one cluster.
for verb classes (Levin, 1993; Merlo and Steven-
son, 2000; Schulte im Walde and Brew, 2002).
To capture this, our model learns groupings of
verbs that have similar distributions across the
learned constructions. These groupings form verb
classes that provide a higher-level of abstraction
over the input; see the top level in Figure 1. Con-
sider the dative alternation: the classes capture the
fact that some verbs may occur only in preposi-
tional dative (PD) forms, such as sing, while oth-
ers occur only in double object (DO) forms (call),
while still others alternate ? i.e., they occur in both
(bring).
Our model simultaneously learns both of these
types of knowledge: constructions are clusters of
verb usages, and classes are clusters of verb dis-
tributions over those constructions. Importantly, it
does so incrementally, which allows us to exam-
ine the developmental trajectory of acquiring al-
ternations such as the dative as the learned clus-
ters grow over time. Moreover, both types of clus-
tering are monotonic, i.e., we do not re-structure
the groupings that our model learns. However, the
model in both levels is clustering verb tokens ? i.e.,
the features corresponding to the verb at that time
in the input, its usage or its current distribution ?
so that the same verb type may be added to various
clusters at different stages in the training.
3.2 Learning Constructions of Verb Usages
The model of AS08 groups input frames into clus-
ters on the basis of the overall similarity in the
values of their features. Importantly, the model
learns these clusters incrementally in response to
the input; the number and type of clusters is not
predetermined. The model considers the creation
of a new cluster for a given frame if the frame is
not sufficiently similar to any of the existing clus-
ters. Formally, the model finds the best cluster for
a given input frame F as in:
BestCluster(F ) = argmax
k?Clusters
P (k|F ) (1)
where k ranges over all existing clusters and a new
one. Using Bayes rule:
P (k|F ) =
P (k)P (F |k)
P (F )
? P (k)P (F |k) (2)
The prior probability of a cluster P (k) is estimated
as the proportion of frames that are in k out of
all observed input frames, thus assigning a higher
39
prior more frequent constructions. The likelihood
P (F |k) is estimated based on the match of fea-
ture values in F and in the frames of k (assuming
independence of the features):
P (F |k) =
?
i?Features
P
i
(j|k) (3)
where j is the value of the i
th
feature of F , and
P
i
(j|k) is calculated using a smoothed version of:
P
i
(j|k) =
count
i
(j, k)
n
k
(4)
where count
i
(j, k) is the number of times feature i
has the value j in cluster k, and n
k
is the number of
frames in k. We compare the slot features as sets to
capture similarities in overlapping syntactic slots
rather than enforcing an exact match. The model
uses the Jaccard similarity score to measure the
degree of overlap between two feature sets, instead
of the direct count of occurrence in Eqn. (4):
sim score(S
1
, S
2
) =
|S
1
? S
2
|
|S
1
? S
2
|
(5)
where S
1
and S
2
in our experiments here are the
sets of syntactic slot features.
3.3 Learning Verb Classes
Our new model extends the construction-
formation model of AS08 by grouping verbs into
classes on the basis of their distribution across
the learned constructions. That is, verbs that have
statistically-similar patterns of occurrence across
the learned constructions will be considered as
forming a verb class. For example, in Figure 1 we
see that bring and read may be put into the same
class because they both occur in a similar relative
frequency across the DO and PD constructions
(the leftmost and rightmost constructions in the
figure).
We use the same incremental Bayesian cluster-
ing algorithm for learning the verb classes as for
learning constructions. At the class level, the fea-
ture used for determining similarity of items in
clustering is the distribution of each verb across
the learned constructions. As for constructions,
the model learns the verb classes incrementally;
the number and type is not predetermined. More-
over, just as constructions are gradually formed
from successively processing a particular verb us-
age at each input step, the model forms verb
classes from a sequence of snapshots of the input
verb?s distribution over the constructions at each
input step. This means that our model is forming
classes of verb tokens rather than types; if a verb?s
behaviour changes over the duration of the input,
subsequent tokens (the distributions over construc-
tions at later points in time) may be clustered into
a different class (or classes) than earlier tokens,
even though prior decisions cannot be undone.
Formally, after clustering the input frame at
time t into a construction, as explained above, the
model extracts the current distribution d
v
t
of its
head verb v over the learned constructions; this is
estimated as a smoothed version of v?s relative fre-
quency in each construction:
P (k|v) =
count(v, k)
n
v
(6)
where count(v, k) is the number of times that in-
puts with verb v have been clustered into construc-
tion k, and n
v
is the number of times v has oc-
curred in the input thus far.
To cluster this snapshot of the verb?s distribu-
tion, d
v
t
, it is compared to the distributions en-
coded by the model?s classes. The distribution d
c
of an existing class c is the weighted average of
the distributions of its member verb tokens:
d
c
=
1
|c|
?
v?c
count(v, c)? d
v
(7)
where |c| is the size of class c, count(v, c) is the
number of occurrences of v that have been as-
signed to c, and d
v
is the distribution of the verb v
given by the tokens of v (the ?snapshots? of distri-
butions of v assigned to class c). That is, d
v
in c is
an average of the distributions of all d
v
t
for verb v
that have been clustered into c.
The model finds the best class for a given verb
distribution d
v
t
based on its similarity to the dis-
tributions of all existing classes and a new one:
BestClass(d
v
t
) = argmax
c?Classes
(1?D
JS
(d
c
?d
v
t
))
(8)
where c ranges over all existing classes as well as
a new class that is represented as a uniform dis-
tribution over the existing constructions. Jensen?
Shannon divergence, D
JS
, is a popular method for
measuring the distance between two distributions:
It is based on the KL?divergence, but it is symmet-
ric and has a finite value between 0 and 1:
D
JS
(p?q) =
1
2
D
KL
(p?
1
2
(p+ q)) +
1
2
D
KL
(q?
1
2
(p+ q)) (9)
40
non-ALT ALT
DO-only PD-only DO PD
Number of verbs 12 5 6
Relative frequency 14% 2% 2% 1%
Table 1: Number of non-alternating (non-ALT) and alter-
nating (ALT) verbs in our lexicon, as well as the relative fre-
quency of each construction in our generated input corpora.
4 Experimental Setup
4.1 Generation of the Input Corpora
We follow the input generation method of AS08
to create naturalistic corpora that are based on the
distributional properties of verbs over various con-
structions, as observed in child-directed speech
(CDS). Our input-generation lexicon contains 71
verbs drawn from AS08 (11 action verbs) and
Barak et al. (2013) (31 verbs of varying syntac-
tic patterns), plus an additional 40 of the most fre-
quent verbs in CDS, in order to have a range of
verbs that occur with the PD and DO construc-
tions. Table 4.1 shows the number of verbs that
appear in the DO or PD construction only (non-
alternating), as well as those that alternate across
the two. (The table also gives the relative fre-
quency of each dative construction in our gener-
ated input corpora.) Each verb lexical entry in-
cludes its overall frequency, and its relative fre-
quency with each of a number of observed syn-
tactic constructions. The frequencies are extracted
from a manual annotation of a sample of 100
child-directed utterances per verb from a collec-
tion of eight corpora from CHILDES (MacWhin-
ney, 2000).
3
An input corpus is generated by it-
eratively selecting a random verb and a syntactic
construction based on their frequencies according
to the lexicon, so that all input corpora used in our
simulations have the distributional properties ob-
served in CDS, but show some variation in precise
make-up and ordering of verb usages. The gener-
ated input consists of frames (a set of features) that
correspond to verb usages in CDS.
4.2 Simulations
Because the generation of the input data is prob-
abilistic, we conduct 100 simulations for each
experiment (each using a different input cor-
pus) to avoid any dependency on specific id-
iosyncratic properties of a single generated cor-
pus. For each simulation, we train our model
3
Brown (1973); Suppes (1974); Kuczaj (1977); Bloom
et al. (1974); Sachs (1983); Lieven et al. (2009).
on an automatically-generated corpus of 15, 000
frames, from which the model learns construc-
tions and verb classes. At specified points in
the input, we present the model with usages of
a novel verb in a DO and/or PD frame, and
then test the model?s generalization ability by
predicting DO and PD frames given that verb.
Since we are interested in the relative likeli-
hoods of the two frames, we report the differ-
ence between the log-likelihood of the DO frame
and the log-likelihood of the PD frame, i.e.,
log-likelihood(DO)? log-likelihood(PD).
Specifically, we form a partial frame F
test
(con-
taining all usage features except for the verb) that
reflects either the PD or the DO syntax, and assess
the probability P (F
test
|v) for each of these, as in:
P (F
test
|v) =
?
k?Constructions
P (F
test
|k)P (k|v)
(10)
where P (F
test
|k) is calculated as in Eqn. (3).
We can calculate P (k|v) in two different ways:
using only the knowledge in the constructions of
the model, and using the knowledge that takes into
account the verb classes over the constructions.
For model predictions based on the construction
level only, we calculate P (k|v) as in Eqn. (6),
which is the smoothed relative frequency of the
verb v over construction k.
Predictions using knowledge of the verb classes
will instead determine P (k|v) drawing on the fit
of verb v to the various classes (specifically, the
similarity of v?s distribution over constructions to
the distribution encoded in each class), and the
likelihood of each construction k for each class c
(specifically, the likelihood of k given the distribu-
tion over constructions encoded in c), as in:
P (k|v) ?
?
c?Classes
P (k|c)P (c|v) (11)
where P (k|c) is the probability of construction
k given class c?s distribution over constructions
(d
c
); and P (c|v) is the probability of c given verb
v?s distribution d
v
over the constructions (using
Jensen-Shannon divergence as in Eqn. (9)).
Due to the different number of clusters in each
of the construction and class layers of the model,
the likelihoods computed for each will differ in
the range of values. For this reason, specific val-
ues cannot be directly compared across the layers
of the model, rather we must analyze the general
trends of the construction-only and class-based re-
sults.
41
5 Evaluation
In this section we examine whether and how our
model generalizes across the two variants of the
dative alternation, the double-object dative (DO)
and the prepositional dative (PD). To do so, we
measure the tendency of the model to produce a
novel verb observed in one dative frame in that
same frame, or in the other dative frame (unob-
served for that verb). Our goal is to understand the
impact of the learned constructions and classes on
this generalization behaviour. Following Parisien
and Stevenson (2010), we examine three input
conditions in which the novel verb occurs: (i)
twice with the DO syntax (non-alternating); (ii)
twice with the PD syntax (non-alternating); or (iii)
once each with DO and PD syntax (alternating).
4
We then ask the model to predict the likelihood of
producing each dative frame with that verb. Our
focus here is on comparing the generalization abil-
ities of the two levels of abstract knowledge in our
model: the constructions versus the verb classes.
As a reminder, we use the dative alternation as
one example for considering this kind of higher-
level generalization behaviour observed in adults
and to a lesser extent in children. Moreover, we
perform the analysis in the context of naturalistic
input that contains many verbs (those that appear
in the dative and those that do not), and a variety of
constructions , to provide a realistic setting for the
task. Our settings differ from the psycholinguis-
tic studies in the variability of constructions com-
pared with the artificial language used by Won-
nacott et al., and in focusing only on the syntac-
tic properties unlike Conwell and Demuth. How-
ever, we follow the settings of these studies in an-
alyzing the syntactic properties of a generated ut-
terance given minimal exposure to a novel verb.
Therefore, we aim to replicate their general ob-
servations by showing that (i) children are limited
in their ability to generalize across verb alterna-
tions compared with adults, and (ii) the frequency
of a construction has a positive correlation with the
generalization rate of the construction.
5.1 Generalization of Learned Knowledge
We examine the generalization patterns of our
model when presented with a novel verb in DO/PD
forms after being trained on 15, 000 inputs, which
we compare to the performance of adults in such
4
For the alternating condition, half the simulations have
DO first, and half have PD first.
Figure 2: The difference between the log-likelihood values
of the DO and PD frames, given each of the three input con-
ditions: DO only, PD only, and Alternating. Values above
zero denote a higher likelihood for the DO frame, and values
below zero denote a higher likelihood for the PD frame.
language tasks. We first consider the case where
the model predictions are based solely on the
knowledge of constructions. Here we expect the
predictions to correspond to the syntactic proper-
ties of the two inputs observed for the novel verb,
with limited generalization. That is, we expect a
non-alternating verb to be much more likely in the
observed dative frame, and an alternating verb to
be equally likely in both frames. The left hand
side of Figure 2 presents the differences in log-
likelihoods of the predicted DO and PD frames for
the novel verb using the construction-based prob-
abilities. The results confirm our expectation that
the knowledge of constructions can support only
limited generalization across the variants of an al-
ternation. For the non-alternating conditions, the
observed frame is highly favoured, and for the
Alternating test scenario, the DO and PD frames
have nearly equal likelihoods.
We next turn to using the knowledge of verb
classes, which we expect to enable generaliza-
tions that correspond to verb alternation behaviour
? that is, we expect the model predictions here
to reflect the knowledge that verbs that occur in
one form of the alternation also often occur in
the other form of the alternation. This is possible
because the classes in the model encode the dis-
tributional patterns of verbs across constructions.
In the absence of other factors, we would expect
the Alternating condition to again show near equal
likelihoods for the two frames, and the two non-
alternating conditions to show a slight preference
for the observed frame (rather than the strong pref-
erence seen in the construction-based predictions),
because the unobserved frame is also likely due to
the knowledge here of the alternation.
The right hand side of Figure 2 presents the
42
difference in the log-likelihoods of the DO and
PD frames when using the knowledge encoded
in the verb classes. The results are not directly
in line with the simple prediction above: The
non-alternating (DO-only and PD-only) condi-
tions show a weak preference (as expected) for one
frame over another, but both favour the DO frame,
as does the Alternating condition. That is, the PD-
only and Alternating conditions show a preference
for the DO frame that does not follow simply from
the knowledge of alternations.
The DO preference in the PD-only and Alter-
nating conditions arises due to distributional fac-
tors in the input, related to the frequencies of the
constructions reported in Table 1. First, the DO
frame is overall much more likely than the PD
frame, causing generalization in the PD-only and
Alternating conditions to lean more to that frame.
Second, fully 1/3 of the uses of the PD frame in
the corpus are with verbs that alternate (i.e., 1%
of the corpus are PD frames of alternating PD-
DO verbs, out of a total of 3% of the corpus be-
ing PD frames), while only 1/8 of the uses of the
DO frame are with alternating rather than non-
alternating verbs. Recall that our classes encode
the distribution (roughly relative frequency) of the
verbs in the class occurring across the different
constructions. This means that in our class-based
predictions, greater weight will be given to con-
structions with DO when observing a PD frame
than to constructions with PD when observing a
DO frame. These results underline the importance
of using naturalistic input and considering the im-
pact of various distributional factors on general-
ization of verb knowledge.
In contrast to the construction-based results, our
class-based results conform with the experimental
findings of Wonnacott et al. (2008), who show that
adult (artificial) language learners robustly gener-
alize a newly-learned verb observed in a single
syntactic form by producing it in the alternating
syntactic form under certain language conditions.
Moreover, we show similar distributional effects
to theirs ? the overall frequency of the syntactic
patterns, as well as the distribution of verbs across
those patterns ? in the level of preference for one
form over another, within the context of our nat-
uralistic data with multiple verbs, constructions,
and alternations. These results show that the verb
classes in the model are able to capture useful ab-
stract knowledge that is key to understanding the
human ability to make high-level generalizations
across verb alternations.
5.2 Development of Generalizations
Next, we present the results of our model evalu-
ated throughout the course of training in order to
understand the developmental pattern of general-
ization. We perform the same construction-based
or class-based prediction tasks (the likelihoods of
a DO and PD frame), following the same input
conditions (a novel verb with two DO frames, two
PD frames, or one of each) at given points during
the 15, 000 inputs. As above, we present the dif-
ference in the log-likelihood values of the DO and
the PD frames in order to focus on the relative like-
lihoods of the two frames within each condition of
construction-based or class-based predictions.
Figure 3(a) presents the results for the DO-
only test scenario. As in Section 5.1, for
both construction-based and class-based predic-
tions there is a higher likelihood for the DO frame
throughout the course of training. In contrast, the
incremental results for the PD-only test scenario,
in Figure 3(b), display a developing level of gen-
eralization throughout the training stage for the
class-based predictions. While the construction-
based predictions reflect a much higher likelihood
for the PD frame, the results from the verb classes
are in favor of the PD frame only initially; after
training on 5000 input frames, the likelihood of
the DO frame becomes higher for this test sce-
nario. These results indicate that using construc-
tion knowledge alone does not enable generaliza-
tion from the PD frame to the DO frame; in con-
trast, the verb class knowledge enables the grad-
ual acquisition of generalization ability over the
course of training.
Finally, Figure 3(c) presents the results for the
Alternating test scenario for the two types of pre-
dictions. As in Section 5.1, both construction-
based and class-based predictions have a small
preference for the DO frame. In the construction-
based predictions, this preference lessens over
time to where the likelihoods for DO and PD are
almost equal, while the class-based predictions
stay relatively constant in their preference for the
DO frame. In some ways the construction-based
predictions are more expected in response to an
apparently alternating verb; however, the class-
based predictions show a higher degree of general-
ization, responding to the higher frequency of the
43
(a) DO only (b) PD only (c) Alternating
Figure 3: Difference of log-likelihood values of the DO and PD frames over the course of training for the constructions and
the verb classes for each of the 3 test scenarios. Values above zero denote a higher likelihood for the DO frame, and values
below zero denote a higher likelihood for the PD frame.
DO frame and the higher association of PD frames
with DO alternates. These results again empha-
size the importance of further exploring the role
of distributional factors on generalization of verb
knowledge in children.
The developmental results presented here are in
line with the suggestions of Tomasello (2003) that
the productions of younger children follow ob-
served patterns in the input, and only later reflect
robust generalizations of their knowledge across
verbs. Conwell and Demuth (2007) for example,
found evidence of generalization across verb al-
ternations in 3-year-old children, but their produc-
tion of unobserved forms for a novel verb was
very sensitive to the precise context of the ex-
periment and the distributional patterns across the
novel verbs. In accord with these observations, the
developmental trajectories in our model show that
our class-based predictions increase in their degree
of generalization over time, and are sensitive to
various distributional factors in the input, such as
the overall expectation for a frame and the expec-
tation that a verb will alternate.
6 Discussion
We present a novel computational model that
probabilistically learns two levels of abstractions
over individual verb usages: constructions that
are clusters of similar verb usages, and classes of
verbs with similar distributional behaviour across
the constructions. Specifically, we extend the
model of AS08 by incrementally learning token-
based verb classes that generalize over the con-
struction knowledge level. In contrast to the mod-
els of Parisien and Stevenson and Perfors et al.,
our model is incremental, and hence enables the
analysis of the monotonically developing classes
to show the relation to the development of gener-
alization ability in human learners.
We analyze how generalization is supported by
each level of learning in our model: constructions
and verb classes. Our results confirm (cf. Parisien
and Stevenson, 2010) that a higher-level knowl-
edge of the verb classes is required to replicate the
observed patterns of generalization, such as pro-
ducing a novel verb gorp in the in the prepositional
dative pattern after hearing it in the double object
dative pattern. In addition, our analysis of the in-
crementally developing verb classes shows that the
generalization knowledge gradually emerges over
time, similar to what is observed in children.
The flexibility of input representation of our
model enables us to further explore the properties
of the input in learning abstract knowledge, fol-
lowing psycholinguistic studies. Our results repli-
cate the findings of Wonnacott et al. on the role
of the distributional properties over the alternat-
ing syntactic forms, but in naturalistic settings of
many constructions. In future, we plan to extend
this analysis by manipulating the distributions of
our input data to replicate the exact settings of the
artificial language used by Wonnacott et al.. More-
over, in this study, we followed the settings of pre-
vious computational and psycholinguistic studies
that focused on the syntactic properties of the in-
put (Perfors et al., 2010; Parisien and Stevenson,
2010; Wonnacott et al., 2008; Conwell and De-
muth, 2007). However, we can further our anal-
ysis by incorporating semantic features in the in-
put to study syntactic bootstrapping effects (Scott
and Fisher, 2009) as well as the role of seman-
tic properties in constraining the generalizations
across the alternating forms.
Acknowledgments
The authors would like to thank Afra Alishahi for
providing us with the code and data for her model,
and to Chris Parisien for sharing his data with us.
44
References
Afra Alishahi and Suzanne Stevenson. 2008. A
computational model of early argument struc-
ture acquisition. Cognitive Science, 32(5):789?
834.
Libby Barak, Afsaneh Fazly, and Suzanne Steven-
son. 2013. Acquisition of desires before beliefs:
A computational investigation. In Proceedings
of CoNLL-2013.
Lois Bloom, Lois Hood, and Patsy Lightbown.
1974. Imitation in language development:
If, when, and why. Cognitive Psychology,
6(3):380?420.
Roger Brown. 1973. A first language: The early
stages. Harvard Univ. Press.
Erin Conwell and Katherine Demuth. 2007. Early
syntactic productivity: Evidence from dative
shift. Cognition, 103(2):163?179.
Cynthia Fisher. 2002. The role of abstract syntac-
tic knowledge in language acquisition: A reply
to Tomasello. Cognition, 82(3):259?278.
A. Kuczaj, Stan. 1977. The acquisition of regular
and irregular past tense forms. Journal of Verbal
Learning and Verbal Behavior, 16(5):589?600.
B. Levin. 1993. English verb classes and alterna-
tions: A preliminary investigation, volume 348.
University of Chicago press Chicago, IL.
Elena Lieven, Doroth?e Salomo, and Michael
Tomasello. 2009. Two-year-old children?s pro-
duction of multiword utterances: A usage-based
analysis. Cognitive Linguistics, 20(3):481?507.
B. MacWhinney. 2000. The CHILDES project:
Tools for analyzing talk, volume 2. Psychology
Press.
P. Merlo and S. Stevenson. 2000. Automatic verb
classification based on statistical distributions
of argument structure. Computational Linguis-
tics, 27(3):373?408.
Christopher Parisien and Suzanne Stevenson.
2010. Learning verb alternations in a usage-
based bayesian model. In Proceedings of the
32nd annual meeting of the Cognitive Science
Society.
Amy Perfors, Joshua B. Tenenbaum, and Eliz-
abeth Wonnacott. 2010. Variability, negative
evidence, and the acquisition of verb argu-
ment constructions. Journal of Child Language,
37(03):607?642.
Jacqueline Sachs. 1983. Talking about the There
and Then: The emergence of displaced refer-
ence in parent?child discourse. Children?s lan-
guage, 4.
Sabine Schulte im Walde and Chris Brew. 2002.
Inducing German semantic verb classes from
purely syntactic subcategorisation information.
In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics,
pages 223?230, Philadelphia, PA.
Rose M Scott and Cynthia Fisher. 2009. Two-
year-olds use distributional cues to interpret
transitivity-alternating verbs. Language and
cognitive processes, 24(6):777?803.
Patrick Suppes. 1974. The semantics of children?s
language. American psychologist, 29(2):103.
Michael Tomasello. 2003. Constructing a lan-
guage: A usage-based theory of language ac-
quisition.
Michael Tomasello and Kirsten Abbot-Smith.
2002. A tale of two theories: Response to
Fisher. Cognition, 83(2):207?214.
Elizabeth Wonnacott, Elissa L Newport, and
Michael K Tanenhaus. 2008. Acquiring and
processing verb argument structure: Distribu-
tional learning in a miniature language. Cog-
nitive psychology, 56(3):165?209.
45
Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 46?54,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
A Usage-Based Model of Early Grammatical Development
Barend Beekhuizen
LUCL
Leiden University
b.f.beekhuizen@hum.leidenuniv.nl
Rens Bod
ILLC
University of Amsterdam
l.w.m.bod@uva.nl
Afsaneh Fazly and Suzanne Stevenson
Department of Computer Science
University of Toronto
afsaneh,suzanne@cs.toronto.edu
Arie Verhagen
LUCL
Leiden University
a.verhagen@hum.leidenuniv.nl
Abstract
The representations and processes yield-
ing the limited length and telegraphic style
of language production early on in acqui-
sition have received little attention in ac-
quisitional modeling. In this paper, we
present a model, starting with minimal lin-
guistic representations, that incrementally
builds up an inventory of increasingly long
and abstract grammatical representations
(form+meaning pairings), in line with the
usage-based conception of language ac-
quisition. We explore its performance on
a comprehension and a generation task,
showing that, over time, the model bet-
ter understands the processed utterances,
generates longer utterances, and better ex-
presses the situation these utterances in-
tend to refer to.
1 Introduction
A striking aspect of language acquisition is the dif-
ference between children?s and adult?s utterances.
Simulating early grammatical production requires
a specification of the nature of the linguistic repre-
sentations underlying the short, telegraphic utter-
ances of children. In the usage-based view, young
children?s grammatical representions are thought
to be less abstract than adults?, e.g. by having
stricter constraints on what can be combined with
them (cf. Akhtar and Tomasello 1997; Bannard
et al. 2009; Ambridge et al. 2012). The represen-
tations and processes yielding the restricted length
of these early utterances, however, have received
little attention. Following Braine (1976), we adopt
the working hypothesis that the early learner?s
grammatical representations are more limited in
length (or: arity) than those of adults.
Similarly, in computational modeling of gram-
mar acquisition, comprehension has received more
attention than language generation. In this pa-
per we attempt to make the mechanisms underly-
ing early production explicit within a model that
can parse and generate utterances, and that in-
crementally learns constructions (Goldberg, 1995)
on the basis of its previous parses. The model?s
search through the hypothesis space of possible
grammatical patterns is highly restricted. Start-
ing from initially small and concrete representa-
tions, it learns incrementally long representations
(syntagmatic growth) as well as more abstract
ones (paradigmatic growth). Several models ad-
dress either paradigmatic (Alishahi and Stevenson,
2008; Chang, 2008; Bannard et al., 2009) or syn-
tagmatic (Freudenthal et al., 2010) growth. This
model aims to explain both, thereby contribut-
ing to the understanding of how different learning
mechanisms interact. As opposed to other models
involving grammars with semantic representations
(Alishahi and Stevenson, 2008; Chang, 2008), but
similar to Kwiatkowski et al. (2012), the model
starts without an inventory of mappings of single
words to meanings.
Based on motivation from usage-based and con-
struction grammar approaches, we define several
learning principles that allow the model to build
up an inventory of linguistic representations. The
model incrementally processes pairs of an utter-
ance U , consisting of a string of words w
1
. . . w
n
,
and a set of situations S, one of which is the situa-
tion the speaker intends to refer to. The other situ-
ations contribute to propositional uncertainty (the
uncertainty over which proposition the speaker is
trying to express; Siskind 1996). The model tries
to identify the intended situation and to understand
how parts of the utterance refer to certain parts of
that situation. To do so, the model uses its growing
inventory of linguistic representations (Section 2)
to analyze U , producing a set of structured seman-
tic analyses or parses (Fig. 1, arrow 1; Section 3).
46
The resulting best parse, U and the selected situa-
tion are then stored in a memory buffer (arrow 2),
which is used to learn new constructions (arrow
3) using several learning mechanisms (Section 4).
The learned constructions can then be used to gen-
erate utterances as well. We describe two experi-
ments: in the comprehension experiment (Section
5), we evaluate the model?s ability to parse the
stream of input items. In the generation experi-
ment (Section 6), the model generates utterances
on the basis of a given situation and its linguistic
knowledge. We evaluate the generated utterances
given different amounts of training items to con-
sider the development of the model over time.
2 Representations
We represent linguistic knowledge as construc-
tions: pairings of a signifying form and a signi-
fied (possibly incomplete) semantic representation
(Goldberg, 1995). The meaning is represented as
a graph with the nodes denoting entities, events,
and their relations, connected by directed unla-
beled edges. The conceptual content of each node
is given by a set of semantic features. We assume
that meaning representations are rooted trees. The
signifying form consists of a positive number of
constituents. Every constituent has two elements:
a phonological form, and a pointer to a node in the
signified meaning (in line with Verhagen 2009).
Both can be specified, or one can be left empty.
Constituents with unspecified phonological forms
are called open, denoted with  in the figures. The
head constituent of a construction is defined as
the constituent that has a pointer to the root node
of the signified meaning. We furthermore require
that no two constituents point to the same node of
the signified meaning.
This definition generalizes over lexical ele-
ments (one phonologically specified constituent)
as well as larger linguistic patterns. Fig. 2, for in-
stance, shows two larger constructions being com-
bined with each other. We call the set of construc-
tions the learner has at some moment in time the
constructicon C (cf. Goldberg 2003).
3 Parsing
3.1 Parsing operations
We first define a derivation d as an assembly
of constructions in C, using four parsing opera-
tions defined below. In parsing, derivations are
constrained by the utterance U and the situations
utterancesituation 1
situation n
situation 2...
situationsinput item
construction 1construction 2construction 3construction nconstructicon
analysis
(utterance, intended situation, analysis)
...
...
memory buffer
1 12
3(utterance, intended situation, analysis)(utterance, intended situation, analysis)
Figure 1: The global flow of the model
S, whereas in production, only a situation s con-
strains the derivation. The leaf nodes of a deriva-
tion must consist of phonological constraints of
constructions that (in parsing) are satisfied by U .
All constructions used in a derivation must map to
the same situation s ? S. A construction cmaps to
s iff the meaning of c constitutes a subgraph of s,
with the features on each of the nodes in the mean-
ing of c being a subset of the features on the corre-
sponding node of s. Moreover, each construction
must map to a different part of s. This constitutes
a mutual exclusivity effect in analyzing U : every
part of the analysis must contribute to the compos-
ite meaning. A derivation d thus gives us a map-
ping between the composed meaning of all con-
structions used in d and one situation s ? S. The
aggregate mapping specifies a subgraph of s that
constitutes the interpretation of that derivation.
The central parsing operation is the COMBINA-
TION operator ?. In c
i
? c
j
, the leftmost open con-
stituent of c
i
is combined with c
j
. Fig. 2 illus-
trates COMBINATION. COMBINATION succeeds if
both the semantic pointer of the leftmost open con-
stituent of c
i
and the semantic pointer of the head
constituent of c
j
map to the same semantic node
of a situation s
Initially, the model has few constructions to an-
alyze the utterance with. Therefore, we define
three other operations that allow the model to cre-
ate a derivation over the full utterance without
combining constructions. First, a known or un-
known word that cannot be fit into a derivation,
can be IGNOREd. Second, an unknown word can
be used to fill an open constituent slot of a con-
struction with the BOOTSTRAP operator. Boot-
strapping entails that the unknown word will be
associated with the semantics of the node. Finally,
the learner can CONCATENATE multiple deriva-
tions, by linearly sequencing them, thus creating a
more complex derivation without combining con-
47
uteranucsaio tera1n u2e.copei sec2nuciptcoa m3ctn uly1(c.an
,d)bf n???????f ,d)bf? ????f
uteranucsaio tera1n u2e.copei sec2nuciptcoa m3ctn
?? ? ?? ?
) ?
,d)bf n??????f ,d)bf? ????f
uteranu?copaio tera3n u2e.copei sec2nue??.o aiopo? uly1(c.an
u?copaio tera3nue??.o aiopo? ,d)bf n???????f ,d)bf? ????f
uteranucsaio tera1n u2e.copei sec2nuciptcoa m3ctn
,d)bf n??????f ,d)bf? ????f
uteranu?copaio tera3n u2e.copei sec2nue??.o aiopo? uly1(c.an
??????n????n????????????n??????? ?? ?n??????n?n??????????n????????
Figure 2: Combining constructions. The dashed lines represent semantic pointers, either from con-
stituents to the constructional meaning (black) or from the constructions to the situation (red and blue).
uttereanc
n s
io ta1
21n i o. p . m
21n i o. pn p n m n 3
. p
21n i o.t .ne1 ne1
Figure 3: The CONCATENATE, IGNORE and
BOOTSTRAP operators (internal details of the con-
structions left out).
structions. This allows the learner to interpret a
larger part of the situation than with COMBINA-
TION only. The resulting sequences may be ana-
lyzed in the learning process as constituting one
larger construction, consisting of the parts of the
concatenated derivations. Fig. 3 illustrates these
three operations.
3.2 Selecting the best analysis
Multiple derivations can be highly similar in the
way they map parts of U to parts of an s ? S. We
define a parse to be a set of derivations that have
the same internal structure and the same mappings
to a situation, but that use different constructions
in doing so (cf. multiple licensing; Kay 2002). We
take the most probable parse of U to be the best
analysis of U . The most probable parse points to a
situation, which the model then assumes to be the
identified situation or s
identified
. If no parse can be
made, s
identified
is selected at random from S.
The probability of a parse p is given by the sum
of the probabilities of the derivations d subsumed
under that parse, which in turn are defined as the
product of the probabilities of the constructions c
used in d.
P (p) =
?
d?p
P (d) (1)
P (d) =
?
c?d
P (c) (2)
The probability of a construction P (c) is given
by its relative frequency (count) in the construc-
ticon C, smoothed with Laplace smoothing. We
assume that the simple parsing operations of IG-
NORE, BOOTSTRAP, and CONCATENATION reflect
usages of an unseen construction with a count of
0.
P (c) =
c.count+ 1
?
c
?
?C
c
?
.count+ |C|+ 1
(3)
The most probable parse, U and s
identified
are
added to the memory buffer. The memory buffer
has a pre-set maximal length, discarding the oldest
exemplars upon reaching this length. In the future,
we plan to consider more realistic mechanisms for
the memory buffer, such as graceful degradation,
and attention effects.
48
4 Learning mechanisms
The model uses the best parse of the utterance to
develop its knowledge of the constructions in the
constructicon C. Two simple operations, UPDATE
and ASSOCIATION, are used to create initial con-
structions and reinforce existing ones respectively.
Two additional operations, PARADIGMATIZATION
and SYNTAGMATIZATION, are key to the model?s
ability to extend these initial representations by
inducing novel constructions that are richer and
more abstract than existing ones.
4.1 Direct learning from the best parse
The best parse is used to UPDATE C. For this
mechanism, the model uses the concrete mean-
ing of s
identified
rather than the (potentially more
abstract) meaning of the constructions in the best
parse.
1
Every construction in the parse is assigned
the subgraph of s
identified
it maps to as its new
meaning, and the count of the adjusted construc-
tion is incremented with 1, or added to C with a
count of 1, if it does not yet exist. This includes
applications of the BOOTSTRAP operation, creat-
ing a mapping of the previously unknown word to
a situational meaning.
ASSOCIATE constitutes a form of simple cross-
situational learning over the memory buffer. The
intuition is that co-occurring word sequences
and meaning components that remain unanalyzed
across multiple parses might themselves comprise
the form-meaning pairing of a construction. If the
unanalyzed parts of two situations contain an over-
lapping subgraph, and the unanalyzed parts of two
utterances an overlapping subsequence of words,
the two are mapped to each other and added to C
with a count of 0.
4.2 Qualitative extension of the best parse
Syntagmatization Some of the processes de-
scribed thus far yield analyses of the input in
which constructions are linearly associated but
lack appropriate relational structure among them.
The model requires a process, which we call SYN-
TAGMATIZATION, that enables it to induce further
hierarchical structure.
In order for the learner to acquire constructions
in which the different constituents point to differ-
ent parts of the construction?s meaning, the ASSO-
1
This follows Langacker?s (2009) claim that the processed
concrete usage events should leave traces in the learner?s
mind.
CIATE operation does not suffice. We assume that
the learner is able to learn such constructions by
using concatenated derivations. The process we
propose is SYNTAGMATIZATION. In this process,
the various concatenated derivations are taken as
constituents of a novel construction. This instanti-
ates the idea that joint processing of two (or more)
events gradually leads to a joint representation of
these, previously independent, events.
More precisely, the process starts by taking the
top nodes T of the derivations in the best parse,
where T consists of the single top node if no CON-
CATENATION has been applied, or the set of con-
catenated nodes of the parse tree if CONCATENA-
TION has been applied (e.g. for the derivation in
Fig. 3, |T | = 2). For each top node t ? T , we take
the root node of the construction?s meaning, and
define its semantic frame to consist of all children
(roles) and grandchildren (role-fillers) of the node
in the situation it maps to. The model then forms a
novel construction c
syn
by taking all the construc-
tions in the parse whose semantic root nodes point
to a node in this semantic frame, referring to those
as the set R of semantically related constructions.
As the novel meaning of c
syn
, the model takes the
subgraph of the situation mapped to by the joint
mapping of all constructional meanings of con-
structions in R.
R, as well as all phonologically specified con-
stituents of t itself, are then linearized as the con-
stituents of c
syn
. The novel construction thus con-
stitutes a construction with a higher arity, ?joining?
several previously independent constructions. Fig.
4 illustrates the syntagmatization mechanism.
Paradigmatization Due to our usage-driven ap-
proach, all learning mechanisms so far give us
maximally concrete constructions. In order for the
model to generalize beyond the observed input,
some degree of abstraction is needed. The model
does so with the PARADIGMATIZATION mecha-
nism. This mechanism recursively looks for min-
imal abstractions (cf. Tomasello 2003, 123) over
the constructions in C and adds those to C, thus
creating a full-inheritance network (cf. Langacker
1989, 63-76).
An abstraction over a set of constructions is
made if there is an overlapping subgraph between
the meanings of the constructions, where every
node of the subgraph is the non-empty feature
set intersection between two mapped nodes of the
constructional meanings. Furthermore, the con-
49
uterauncsiricots 111autoi2tr. p.tm.ma
uio3.l.o3.ory.(i,ra
d)b d)bf??? n? f??? ?
uter 2cn.auncsiricots 111a uio3.l.o3.ory.(i,rad)bf??? ??
ecoetr.otr.
uc?.er .orir? ?ssa
d)bf??? ??
uterauncsiricots 111autoi2tr. p.tm.ma
uio3.l.o3.ory.(i,ra
d)b d)bf??? n? f??? ??
uc?.er .orir? ?ssa
d)bf??? ??
????????????????? ??rt??r ? ??????????????????
Figure 4: The SYNTAGMATIZATION mechanism. The mechanism takes a derivation as its input and
reinterprets it as a novel construction of higher arity).
stituents must be mappable: both constructions
have the same number of constituents and the
paired constituents point to a mapped node of the
meaning. The meaning of the abstracted construc-
tion is then set to this overlapping subgraph, which
is the lowest possible semantic abstraction over
the constructions. The constituents of this new ab-
straction have a specified phonological form if the
more concrete constructions share the same word,
and an unspecified one otherwise. The count of an
abstracted construction is given by the cardinality
of the set of its direct descendants in the network.
This generalizes Bybee?s (1995) idea about type
frequency as a proxy for productivity to a network
structure. Fig. 5 illustrates the paradigmatization
mechanism.
5 Experimental set-up
The model is incrementally presented with U, S
pairings based on Alishahi & Stevenson?s (2010)
generation procedure. In this procedure, an utter-
ance and a semantic frame expressing its meaning
(a situation) are generated. The generation pro-
cedure follows distributions occurring in a corpus
of child-directed speech. As we are interested in
the performance of the model under propositional
uncertainty, we add a parametrized number of ran-
domly sampled situations, so that S consists of the
situation the speaker intends to refer to (s
correct
)
and a number of situations the speaker does not
intend to refer to.
2
Here, we set the number of ad-
2
We are currently researching the effects of sampling non-
correct situations that have a greater likelihood of overlap
ditional situations to be 1 or 5; the other parameter
of the model, the size of the memory buffer, is set
to 5 exemplars.
For the comprehension experiment, we eval-
uate the model?s performance parsing the input
items, averaging over every 50 U, S pairs. We
track the ability to identify the intended situation
from S. Identification succeeds if the best parse
maps to s
correct
, i.e. if s
identified
= s
correct
. Next,
situation coverage expresses what proportion of
s
identified
has been interpreted and thus how rich the
meanings of the used constructions are. It is de-
fined as the number of nodes of the interpretation
of the best parse, divided by the number of nodes
of s
identified
. Finally, utterance coverage tells us
what proportion of U has been parsed with con-
structions (excluding IGNORED; including BOOT-
STRAPPED words). The measure expresses the
proportion of the signal that the learner (correctly
or incorrectly) is able to interpret.
For exploring language production, the model
receives a situation, and (given the constructicon)
finds the most probable, maximally expressive,
fully lexicalized derivation expressing it. That is:
among all derivations terminating in phonologi-
cally specified constituents, it selects the deriva-
tions that cover the most semantic nodes of the
given situation. In the case of multiple such
derivations, it selects the most probable one, fol-
lowing the probability model in Section 3. We
only allow for the COMBINATION operator in the
derivations, as BOOTSTRAPPING and IGNORE re-
with the intended situation, to reflect more realistic input (cf.
Siskind 1996).
50
uterancsion uoi12.2ipe1cmmm uep2se.nc3nelnl uye.2np.cmmm u1ite.2ipcnp.2.(ct3e2l 
,d) ,d) ,d)bf?? n? bf?? ?? bf?? ???
uterancsion uoi12.2ipe1cmmm uep2se.nc3nelnl uye.2np.cmmm u1ite.2ipcnp.2.(c.e?n 
uterancsion uoi12.2ipe1cmmm uep2se.nc3nelnl uye.2np.cmmm u1ite.2ipcnp.2.( 
,d) ,d) ,d)bf?? n? bf?? ?? bf?? ?
,d) ,d) ,d)bf?? n? bf?? ?? bf?? ???
?? ???? ?? ???n???n????? ???? ? ??? ?? ??? ??? t3e2l??? .e?n ?
? ?????? ????? ????u1ite.2ipcnp.2.(ct3e2l ??? ?u1ite.2ipcnp.2.(c?.e?n 
Figure 5: The PARADIGMATIZATION mechanism. The construction on top is an abstraction obtained
over the two constructions at the bottom.
fer to words in a given U , and CONCATENATE is a
back-off method for analyzing more of U than the
constructicon allows for. The situations used in the
generation experiment do not occur in the training
items, so that we truly measure the model?s ability
to generate utterances for novel situations.
The phonologically specified leaf nodes of the
best derivation constitute the generated utterance
U
gen
. U
gen
is evaluated on the basis of its mean
length, in number of words, its situation cover-
age, as defined in the comprehension experiment,
and its utterance precision and utterance recall.
To calculate these, we take the maximally overlap-
ping subsequenceU
overlap
between the actual utter-
ance U
act
associated with the situation and U
gen
.
Utterance precision (how many words are gener-
ated correctly) and utterance recall (how many of
the correct words are generated) are defined as:
Utterance precision =
|U
overlap
|
|U
gen
|
(4)
Utterance recall =
|U
overlap
|
|U
act
|
(5)
Because the U, S-pairs on which the model was
trained, are generated randomly, we show results
for comprehension and production averaged over
5 simulations.
6 Experiments
A central motivation for the development of this
model is to account for early grammatical produc-
tion: can we simulate the developmental pattern
of the growth of utterance length and a growing
potential for generalization? The same construc-
tions underlying these productions should, at the
same time, also account for the learner?s increas-
ing grasp of the meaning of U . To explore the
model?s performance in both domains, we present
a comprehension and a generation experiment.
6.1 Comprehension results
Fig. 6a gives us the results over time of the com-
prehension measures given a propositional un-
certainty of 1, i.e. one situation besides s
correct
in S. Overall, the model understands the utter-
ances increasingly well. After 2000 input items,
the model identifies s
correct
in 95% of the cases.
With higher levels of propositional uncertainty
(not shown here), performance is still relatively
robust: given 5 incorrect situations in S, s
correct
is identified in 62% of all cases (random guess-
ing gives a score of 17%, or
1
6
). Similarly, the
proportion of the situation interpreted and the pro-
portion of the utterance analyzed go up over time.
This means that the model builds up an increasing
repertoire of constructions that allow it to analyze
larger parts of the utterance and the situations it
identifies. It is important to realize that these mea-
51
0 500 1000 1500 2000
0.0
0.2
0.4
0.6
0.8
1.0
time
prop
ortion
measures
situation coverageutterance coverageidentification
(a) Comprehension results over time
0 500 1000 1500 2000
0.0
0.5
1.0
1.5
2.0
2.5
3.0
time
utter
anc
e len
gth in
 word
s
(b) Length of U
gen
over time
0 500 1000 1500 2000
0.0
0.2
0.4
0.6
0.8
1.0
time
prop
ortion
measures
situation coverageutterance precisionutterance recall
(c) Generation results over time
Figure 6: Quantitative results for the comprehension and generation experiments
sures do not display what proportion of the utter-
ance or situation is analyzed correctly.
6.2 Generation results
Quantitative results Fig. 6b shows that the av-
erage utterance length increases over time. This
indicates that the number of constituents of the
used constructions grows. Next, Fig. 6c shows the
performance of the model on the generation task.
After 2000 input items, the model generates pro-
ductions expressing 93% of the situation, with an
utterance precision of 0.91, and an utterance recall
of 0.81. Given a propositional uncertainty of 5,
these go down to 79%, 0.76 and 0.59 respectively.
Comparing the utterance precision and recall
over time, we can see that the utterance preci-
sion is high from the start, whereas the recall
gradually increases. This is in line with the ob-
servation that children predominantly produce er-
rors of omission (leaving linguistic material out an
adult speaker would produce), and few errors of
comission (producing linguistic material an adult
speaker would not produce).
Qualitative results Tracking individual produc-
tions given specific situations over time allows us
to study in detail what the model is doing. Here,
we look at one case qualitatively. Given the sit-
uation for which the U
act
is she put them away,
the model generates, over time, the utterances in
Table 1. The brackets show the internal hierarchi-
cal structure of the derivation. This development
illustrates several interesting aspects of the model.
First, as discussed earlier, the model mostly makes
errors of omission: earlier productions leave out
more words found in the adult utterances. Only at
t = 550, the model makes an error of commission,
using the word in erroneously.
[
[
s
h
e
]
p
u
t
]
[
s
h
e
[
p
u
t
]
]
[
[
s
h
e
]
[
p
u
t
]
[
i
n
]
]
[
[
s
h
e
]
p
u
t
t
h
e
m
[
a
w
a
y
]
]
[
[
s
h
e
]
p
u
t
[
t
h
e
m
]
]
[
[
s
h
e
]
p
u
t
t
h
e
m
[
a
w
a
y
]
]
[
[
s
h
e
]
p
u
t
[
t
h
e
m
]
a
w
a
y
]
[
[
s
h
e
]
p
u
t
t
h
e
m
a
w
a
y
]
t 50 500 550 600 950 1000 1050 1400
Table 1: Generations over time t for one situation.
Starting from t = 600 (except at t = 950),
the model generates the correct utterance, but the
derivations leading to this production differ. At
t = 550, for instance, the learner combines a
completely non-phonologically specific construc-
tion for which the constituents refer to the agent,
action and goal location, with three ?lexical? con-
structions that fill in the words for those items..
The constructions used after t = 550 are all more
specific, combining 3, or even only 2 constructions
(t ? 1400) where the entire sequence of words
?put them away? arises from a single construction.
Using less abstract constructions over time
seems contrary to the usage-based idea that con-
structions become more abstract over the course of
acquisition. However, this result follows from the
way the probability model is defined. More spe-
cific constructions that are able to account for the
input will entail fewer combinations, and a deriva-
tion with fewer combination operations will often
be more likely than one with more such opera-
tions. Given equal expressivity of the situation,
the former derivation will be selected over the lat-
ter in generation.
The effect is indeed in line with another concept
hypothesized to play a role in language acquisition
on a usage-based account, viz. pre-emption (Gold-
52
uterancsion uoi12.2ipe1cmmmccteran31ite.2ip u1ite.2ipclna.2pe.2ip uep2se.ncmm ueyynt.nlcmmmca.e.2ipe(, uid)nt.ce(.nyet. uid)nt.cbe(.nyet. f?? f?? f?? f?? f?????b n?????b ? ???b ? ???b ? ???b ?
u(na.cet. uoi12.2ipe1cbmmm uep2se.ncmmm ulna.2pe.2ipc1ite.2ip u1ite.2ipcnp.2., f?? f?? f????? ? ??? ? ??? ?
uet. uoi12.2ipe1cmmm uep2se.nc?ne(n( u2pln?np3lnp.3n?a. uid)nt.ce(.nyet. f?? f?? f?????b ? ? ???b ????
(b) (c)
(a)
Figure 7: Some representations at t = 2000
berg, 2006, 94-95). Pre-emption is the effect that
a language user will select a more concrete rep-
resentation over the combination of more abstract
ones. The effect can be reconceptualized in this
model as an epiphenomenon of the way the prob-
ability model works: simply because combining
fewer constructions in a derivation is often more
probable than combining more constructions, the
former derivation will be selected over the lat-
ter. Pre-emption is typically invoked to explain the
blocking of overgeneralization patterns, and an in-
teresting future step will be to see if the model can
simulate developmental patterns for well-known
cases of overgeneralization errors.
The potential for abstraction The paradigma-
tization operation allows the model to go beyond
observed concrete instances of form-meaning
pairings: without it, unseen situations could never
be fully expressed. Despite this potential, we have
seen that the model relies on highly concrete con-
structions. The concreteness of the used patterns,
however, does not imply the absence of more ab-
stract representations. Fig. 7 gives three exam-
ples of constructions in C in one simulation. Con-
struction (a) could be seen as a verb-island con-
struction (Tomasello, 1992, 23-24). The second
constituent is phonologically specified with put,
and the other arguments are open, but mapped to
specific semantic functions. This pattern allows
for the expression of many caused-motion events.
Construction (b) is the inverse of (a): the argu-
ments are phonologically specified, but the verb-
slot is open. This would be a case of a pronominal
argument frame [you V it], which have been found
to be helpful in the bootstrapping of verbal mean-
ings (Tomasello, 2001). Finally, (c) presents a case
of full abstraction. This construction licenses ut-
terances such as I sit here, you stay there and er-
roneous ones like he sits on (which, again, will be
pre-empted in the generation of utterances if more
concrete constructions licence he sits on it).
Summarizing, abstract constructions are ac-
quired, but only used for those cases in which no
concrete construction is available. This is in line
with the usage-based hypotheses that abstract con-
structions do emerge, but that for much of lan-
guage production, a language user can rely on
highly concrete patterns. A next step will be
to measure the development of abstractness and
length over the constructions themselves, rather
than the parses and generations they allow.
7 Conclusion
This, admittedly complex, model forms an attempt
to model different learning mechanisms in interac-
tion from a usage-based constructionist perspec-
tive. Starting with an empty set of linguistic rep-
resentations, the model acquires words and gram-
matical constructions simultaneously. The learn-
ing mechanisms allow the model to build up in-
creasingly abstract, as well as increasingly long
constructions. With these developing representa-
tions, we showed how the model gets better over
time at understanding the input item, performing
relatively robustly under propositional uncertainty.
Moreover, in the generation experiment, the
model shows patterns of production (increasingly
long utterances) similar to those of children. An
important future step will be to look at these pro-
ductions more closely and investigate if they also
converge on more detailed patterns of develop-
ment in the production of children (e.g. item-
specificity, as hypothesized on the usage-based
view). Despite highly concrete constructions suf-
ficing for most of production, inspection of the ac-
quired representations tells us that more abstract
constructions are learned as well. Here, an inter-
esting next step would be to simulate patterns of
overgeneralization in children?s production.
Acknowledgements
We would like to thank three anonymous review-
ers for their valuable and thoughtful comments.
We gratefully acknowledge the funding of BB
through NWO of the Netherlands (322.70.001)
and AF and SS through NSERC of Canada.
53
References
Nameera Akhtar and Michael Tomasello. 1997.
Young Children?s Productivity With Word Or-
der and Verb Morphology. Developmental Psy-
chology, 33(6):952?965.
Afra Alishahi and Suzanne Stevenson. 2008 A
Computational Model of Early Argument Struc-
ture Acquisition. Cognitive Science, 32(5):789?
834.
Afra Alishahi and Suzanne Stevenson. 2010. A
computational model of learning semantic roles
from child-directed language. Language and
Cognitive Processes, 25(1):50?93.
Ben Ambridge, Julian M Pine, and Caroline F
Rowland. 2012. Semantics versus statistics in
the retreat from locative overgeneralization er-
rors. Cognition, 123(2):260?79.
Colin Bannard, Elena Lieven, and Michael
Tomasello. 2009. Modeling children?s early
grammatical knowledge. Proceedings of the
National Academy of Sciences of the United
States of America, 106(41):17284?9.
Martin D.S. Braine. 1976. Children?s first word
combinations. University of Chicago Press,
Chicago, IL.
Joan Bybee. 1995. Regular morphology and the
lexicon. Language and Cognitive Processes, 10
(5):425?455.
Nancy C.-L. Chang. 2008. Constructing Gram-
mar: A computational model of the emergence
of early constructions. Dissertation, University
of California, Berkeley.
Daniel Freudenthal, Julian Pine, and Fernand Go-
bet. 2010. Explaining quantitative variation in
the rate of Optional Infinitive errors across lan-
guages: a comparison of MOSAIC and the Vari-
ational Learning Model. Journal of Child Lan-
guage, 37(3):643?69.
Adele E. Goldberg. 1995. Constructions. A
Construction Grammar Approach to Argument
Structure. Chicago University Press, Chicago,
IL.
Adele E Goldberg. 2003. Constructions: a new
theoretical approach to language. Trends in
Cognitive Sciences, 7(5):219?224.
Adele E. Goldberg. 2006. Constructions at Work.
The Nature of Generalization in Language. Ox-
ford University Press, Oxford.
Paul Kay. 2002. An Informal Sketch of a Formal
Architecture for Construction Grammar. Gram-
mars, 5:1?19.
Tom Kwiatkowski, Sharon Goldwater, Luke
Zettlemoyer, and Mark Steedman. 2012. A
Probabilistic Model of Syntactic and Seman-
tic Acquisition from Child-Directed Utterances
and their Meanings. In Proceedings EACL.
Ronald W. Langacker. 1989. Foundations of Cog-
nitive Grammar, Volume I. Stanford University
Press.
Ronald W. Langacker. 2009. A dynamic view of
usage and language acquisition. Cognitive Lin-
guistics, 20(3):627?640.
Jeffrey M Siskind. 1996. A computational study of
cross-situational techniques for learning word-
to-meaning mappings. Cognition, 61(1-2):39?
91.
Michael Tomasello. 1992. First Verbs: A study
of early grammatical development. Cambridge
University Press, Cambridge, UK.
Michael Tomasello. 2001 Perceiving intentions
and learning words in the second year of life.
In Melissa Bowerman and Stephen C. Levinson,
editors, Language Acquisition and Conceptual
Development, chapter 5, pages 132?158. Cam-
bridge University Press, Cambridge, UK.
Michael Tomasello. 2003. Constructing a lan-
guage: A Usage-Based Theory of Language
Acquisition. Harvard University Press, Cam-
bridge, MA.
Arie Verhagen. 2009 The conception of construc-
tions as complex signs. Emergence of struc-
ture and reduction to usage. Constructions and
Frames, 1:119?152.
54
