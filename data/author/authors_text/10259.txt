Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 21?30,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Regular Expression Learning for Information Extraction
Yunyao Li, Rajasekar Krishnamurthy, Sriram Raghavan, Shivakumar Vaithyanathan
IBM Almaden Research Center
San Jose, CA 95120
{yunyaoli, rajase, rsriram}@us.ibm.com, shiv@almaden.ibm.com
H. V. Jagadish?
Department of EECS
University of Michigan
Ann Arbor, MI 48109
jag@umich.edu
Abstract
Regular expressions have served as the dom-
inant workhorse of practical information ex-
traction for several years. However, there has
been little work on reducing the manual ef-
fort involved in building high-quality, com-
plex regular expressions for information ex-
traction tasks. In this paper, we propose Re-
LIE, a novel transformation-based algorithm
for learning such complex regular expressions.
We evaluate the performance of our algorithm
on multiple datasets and compare it against the
CRF algorithm. We show that ReLIE, in ad-
dition to being an order of magnitude faster,
outperforms CRF under conditions of limited
training data and cross-domain data. Finally,
we show how the accuracy of CRF can be im-
proved by using features extracted by ReLIE.
1 Introduction
A large class of entity extraction tasks can be ac-
complished by the use of carefully constructed reg-
ular expressions (regexes). Examples of entities
amenable to such extractions include email ad-
dresses and software names (web collections), credit
card numbers and social security numbers (email
compliance), and gene and protein names (bioinfor-
matics), etc. These entities share the characteristic
that their key representative patterns (features) are
expressible in standard constructs of regular expres-
sions. At first glance, it may seem that constructing
?Supported in part by NSF 0438909 and NIH 1-U54-
DA021519.
a regex to extract such entities is fairly straightfor-
ward. In reality, robust extraction requires the use
of rather complex expressions, as illustrated by the
following example.
Example 1 (Phone number extraction). An obvious
pattern for identifying phone numbers is ?blocks of
digits separated by hyphens? represented as R1 =
(\d+\-)+\d+.1 While R1 matches valid phone numbers
like 800-865-1125 and 725-1234, it suffers from both
?precision? and ?recall? problems. Not only does R1
produce incorrect matches (e.g., social security numbers
like 123-45-6789), it also fails to identify valid phone
numbers such as 800.865.1125, and (800)865-CARE. An
improved regex that addresses these problems is R2 =
(\d{3}[-.\ ()]){1,2}[\dA-Z]{4}.
While multiple machine learning approaches have
been proposed for information extraction in recent
years (McCallum et al, 2000; Cohen and McCal-
lum, 2003; Klein et al, 2003; Krishnan and Man-
ning, 2006), manually created regexes remain a
widely adopted practical solution for information
extraction (Appelt and Onyshkevych, 1998; Fukuda
et al, 1998; Cunningham, 1999; Tanabe and Wilbur,
2002; Li et al, 2006; DeRose et al, 2007; Zhu et al,
2007). Yet, with a few notable exceptions, which we
discuss later in Section 1.1, there has been very little
work in reducing this human effort through the use
of automatic learning techniques. In this paper, we
propose a novel formulation of the problem of learn-
1Throughout this paper, we use the syntax of the standard
Java regex engine (Java, 2008).
21
ing regexes for information extraction tasks. We
demonstrate that high quality regex extractors can be
learned with significantly reduced manual effort. To
motivate our approach, we first discuss prior work
in the area of learning regexes and describe some of
the limitations of these techniques.
1.1 Learning Regular Expressions
The problem of inducing regular languages from
positive and negative examples has been studied in
the past, even outside the context of information
extraction (Alquezar and Sanfeliu, 1994; Dupont,
1996; Firoiu et al, 1998; Garofalakis et al, 2000;
Denis, 2001; Denis et al, 2004; Fernau, 2005;
Galassi and Giordana, 2005; Bex et al, 2006).
Much of this work assumes that the target regex
is small and compact thereby allowing the learn-
ing algorithm to exploit this information. Consider,
for example, the learning of patterns motivated by
DNA sequencing applications (Galassi and Gior-
dana, 2005). Here the input sequence is viewed
as multiple atomic events separated by gaps. Since
each atomic event is easily described by a small and
compact regex, the problem reduces to one of learn-
ing simple regexes. Similarly, in XML DTD infer-
ence (Garofalakis et al, 2000; Bex et al, 2006), it
is possible to exploit the fact that the XML docu-
ments of interest are often described using simple
DTDs. E.g., in an online books store, each book
has a title, one or more authors and price. This in-
formation can be described in a DTD as ?book? ?
?title??author? + ?price?. However, as shown in Ex-
ample 1, regexes for information extraction rely on
more complex constructs.
In the context of information extraction, prior
work has concentrated primarily on learning regexes
over relatively small alphabet sizes. A common
theme in (Soderland, 1999; Ciravegna, 2001; Wu
and Pottenger, 2005; Feldman et al, 2006) is the
problem of learning regexes over tagged tokens
produced by other text-processing steps such as
POS tagging, morphological analysis, and gazetteer
matching. Thus, the alphabet is defined by the space
of possible tags output by these analysis steps. A
similar approach has been proposed in (Brill, 2000)
for POS disambiguation. In contrast, our paper ad-
dresses extraction tasks that require ?fine-grained?
control to accurately capture the structural features
of the entity of interest. Consequently, the domain
of interest consists of all characters thereby dramat-
ically increasing the size of the alphabet. To enable
this scale-up, the techniques presented in this paper
exploit advanced syntactic constructs (such as char-
acter classes and quantifiers) supported by modern
regex languages.
Finally, we note that almost all of the above de-
scribed work define the learning problem over a
restricted class of regexes. Typically, the restric-
tions involve either disallowing or limiting the use of
Kleene disclosure and disjunction operations. How-
ever, our work imposes no such restrictions.
1.2 Contributions
In a key departure from prior formulations, the
learning algorithm presented in this work takes as
input not just labeled examples but also an initial
regular expression. The use of an initial regex has
two major advantages. First, this expression pro-
vides a natural mechanism for a domain expert to
provide domain knowledge about the structure of the
entity being extracted. Second, as we show in Sec-
tion 2, the space of output regular expressions un-
der consideration can be meaningfully restricted by
appropriately defining their relationship to the input
expression. Such a principled approach to restrict
the search space permits the learning algorithm to
consider complex regexes in a tractable manner. In
contrast, prior work defined a tractable search space
by placing restrictions on the target class of regular
expressions. Our specific contributions are:
? A novel regex learning problem consisting of learn-
ing an ?improved? regex given an initial regex and
labeled examples
? Formulation of this learning task as an optimization
problem over a search space of regexes
? ReLIE, a regex learning algorithm that employs
transformations to navigate the search space
? Extensive experimental results over multiple
datasets to show the effectiveness of ReLIE and
a comparison study with the Conditional Random
Field (CRF) algorithm
? Finally, experiments that demonstrate the benefits
of using ReLIE as a feature extractor for CRF and
possibly other machine learning algorithms.
22
2 The Regex Learning Problem
Consider the task of identifying instances of some
entity E . Let R0 denote the input regex provided by
the user and let M(R0 ,D) denote the set of matches
obtained by evaluating R0 over a document col-
lection D. Let Mp(R0 ,D) = {x ? M(R0 ,D) :
x instance of E} and Mn(R0 ,D) = {x ? M(R0 ,D) :
x not an instance of E} denote the set of positive and
negative matches for R0 . Note that a match is pos-
itive if it corresponds to an instance of the entity of
interest and is negative otherwise. The goal of our
learning task is to produce a regex that is ?better?
than R0 at identifying instances of E .
Given a candidate regex R, we need a mechanism
to judge whether R is indeed a better extractor for
E than R0 . To make this judgment even for just the
original document collection D, we must be able to
label each instance matched byR (i.e., each element
of M(R,D)) as positive or negative. Clearly, this
can be accomplished if the set of matches produced
byR are contained within the set of available labeled
examples, i.e., if M(R,D) ? M(R0 ,D). Based on
this observation, we make the following assumption:
Assumption 1. Given an input regex R0 over some al-
phabet ?, any other regexR over ? is a candidate for our
learning algorithm only if L(R) ? L(R0 ). (L(R) denotes
the language accepted by R).
Even with this assumption, we are left with a po-
tentially infinite set of candidate regexes from which
our learning algorithm must choose one. To explore
this set in a principled fashion, we need a mecha-
nism to move from one element in this space to an-
other, i.e., from one candidate regex to another. In
addition, we need an objective function to judge the
extraction quality of each candidate regex. We ad-
dress these two issues below.
Regex Transformations To systematically ex-
plore the search space, we introduce the concept of
regex transformations.
Definition 1 (Regex Transformation). LetR? denote
the set of all regular expressions over some alphabet ?. A
regex transformation is a function T : R? ? 2R? such
that ?R? ? T (R), L(R?) ? L(R).
For example, by replacing different occurrences
of the quantifier + in R1 from Example 1 with
specific ranges (such as {1,2} or {3}), we obtain
expressions such as R3 = (\d+\-){1,2}\d+ and
R4 = (\d{3}\-)+\d+. The operation of replacing
quantifiers with restricted ranges is an example of a
particular class of transformations that we describe
further in Section 3. For the present, it is sufficient
to view a transformation as a function applied to a
regexR that produces, as output, a set of regexes that
accept sublanguages of L(R). We now define the
search space of our learning algorithm as follows:
Definition 2 (Search Space). Given an input regex R0
and a set of transformations T , the search space of our
learning algorithm is T (R0 ), the set of all regexes ob-
tained by (repeatedly) applying the transformations in T
to R0 .
For instance, if the operation of restricting quanti-
fiers that we described above is part of the transfor-
mation set, then R3 and R4 are in the search space
of our algorithm, given R1 as input.
Objective Function We now define an objective
function, based on the well known F-measure, to
compare the extraction quality of different candidate
regexes in our search space. Using Mp(R,D) (resp.
Mn(R,D)) to denote the set of positive (resp. nega-
tive) matches of a regex R, we define
precision(R,D) =
Mp(R,D)
Mp(R,D) + Mn(R,D)
recall(R,D) =
Mp(R,D)
Mp(R0,D)
F(R,D) =
2 ? precision(R,D) ? recall(R,D)
precision(R,D) + recall(R,D)
The regex learning task addressed in this paper
can now be formally stated as the following opti-
mization problem:
Definition 3 (Regex Learning Problem). Given
an input regex R0 , a document collection D, labeled
sets of positive and negative examples Mp(R0 ,D) and
Mn(R0 ,D), and a set of transformations T , compute the
output regex Rf = argmaxR?T (R0 ) F(R,D).
3 Instantiating Regex Transformations
In this section, we describe how transformations
can be implemented by exploiting the syntactic con-
structs of modern regex engines. To help with our
description, we introduce the following task:
Example 2 (Software name extraction). Consider the
task of identifying names of software products in text.
A simple pattern for this task is: ?one or more capital-
ized words followed by a version number?, represented
as R5 = ([A-Z]\w*\s*)+[Vv]?(\d+\.?)+.
23
When applied to a collection of University web
pages, we discovered that R5 identified correct in-
stances such as Netscape 2.0, Windows 2000 and
Installation Designer v1.1. However, R5 also ex-
tracted incorrect instances such as course numbers
(e.g. ENGLISH 317), room numbers (e.g. Room
330), and section headings (e.g. Chapter 2.2). To
eliminate spurious matches such as ENGLISH 317,
let us enforce the condition that ?each word is a
single upper-case letter followed by one or more
lower-case letters?. To accomplish this, we focus
on the sub-expression of R5 that identifies capital-
ized words, R51 = ([A-Z]\w*\s*)+, and replace it
with R51a = ([A-Z][a-z]*\s*)+. The regex result-
ing from R5 by replacing R51 with R51a will avoid
matches such as ENGLISH 317.
An alternate way to improve R5 is by explicitly
disallowing matches against strings like ENGLISH,
Room and Chapter. To accomplish this, we can
exploit the negative lookahead operator supported
in modern regex engines. Lookaheads are special
constructs that allow a sequence of characters
to be checked for matches against a regex with-
out the characters themselves being part of the
match. As an example, (?!Ra)Rb (??!? being
the negative lookahead operator) returns matches
of regex Rb but only if they do not match Ra.
Thus, by replacing R51 in our original regex with
R51b =(?! ENGLISH|Room|Chapter)[A-Z]\w*\s*,
we produce an improved regex for software names.
The above examples illustrate the general prin-
ciple of our transformation technique. In essence,
we isolate a sub-expression of a given regex R and
modify it such that the resulting regex accepts a sub-
language of R. We consider two kinds of modifica-
tions ? drop-disjunct and include-intersect. In drop-
disjunct, we operate on a sub-expression that corre-
sponds to a disjunct and drop one or more operands
of that disjunct. In include-intersect, we restrict the
chosen sub-expression by intersecting it with some
other regex. Formally,
Definition 4 (Drop-disjunct Transformation). Let
R ? R? be a regex of the form R = Ra?(X)Rb,
where ?(X) denotes the disjunction R1|R2| . . . |Rn of
any non-empty set of regexes X = {R1, R2, . . . , Rn}.
The drop-disjunct transformation DD(R,X, Y ) for some
Y ? X, Y 6= ? results in the new regex Ra?(Y )Rb.
Definition 5 (Include-Intersect Transformation). Let
.\W \s \w[a-zA-Z] \d|[0-9] _[a-z] [A-Z]
Figure 1: Sample Character Classes in Regex
R ? R? be a regex of the form R = RaXRb for some
X ? R?, X 6= ?. The include-intersect transformation
II(R,X, Y ) for some Y ? R?, Y 6= ? results in the new
regex Ra(X ? Y )Rb.
We state the following proposition (proof omit-
ted in the interest of space) that guarantees that both
drop-disjunct and include-intersect restrict the lan-
guage of the resulting regex, and therefore are valid
transformations according to Definition 1.
Proposition 1. Given regexes R,X1, Y1, X2 and Y2
from R? such that DD(R,X1, Y1) and II(R,X2, Y2)
are applicable, L(DD(R,X1, Y1)) ? L(R) and
L(II(R,X2, Y2)) ? L(R).
We now proceed to describe how we use differ-
ent syntactic constructs to apply drop-disjunct and
include-intersect transformations.
Character Class Restrictions Character
classes are short-hand notations for denoting
the disjunction of a set of characters (\d is
equivalent to (0|1...|9); \w is equivalent to
(a|. . .|z|A|. . .|Z|0|1. . .|9| ); etc.).2 Figure 1
illustrates a character class hierarchy in which
each node is a stricter class than its parent (e.g.,
\d is stricter than \w). A replacement of any of
these character classes by one of its descendants
is an instance of the drop-disjunct transformation.
Notice that in Example 2, when replacing R51 with
R51a , we were in effect applying a character class
restriction.
Quantifier Restrictions Quantifiers are used to
define the range of valid counts of a repetitive se-
quence. For instance, a{m,n} looks for a sequence
of a?s of length at least m and at most n. Since
quantifiers are also disjuncts (e.g., a{1,3} is equiv-
alent to a|aa|aaa), the replacement of an expres-
sion R{m,n} with an expression R{m1, n1} (m ?
m1 ? n1 ? n) is an instance of the drop-disjunct
transformation. For example, given a subexpres-
sion of the form a{1,3}, we can replace it with
2Note that there are two distinct character classes \W and \w
24
one of a{1,1}, a{1,2}, a{2,2}, a{2,3}, or a{3,3}.
Note that, before applying this transformation, wild-
card expressions such as a+ and a* are replaced by
a{0,maxCount} and a{1,maxCount} respectively,
where maxCount is a user configured maximum
length for the entity being extracted.
Negative Dictionaries Observe that the include-
intersect transformation (Definition 5) is applicable
for every possible sub-expression of a given regex
R. Note that a valid sub-expression in R is any
portion of R where a capturing group can be intro-
duced.3 Consider a regex R = RaXRb with a sub-
expression X; the application of include-intersect
requires another regex Y to yieldRa(X?Y )Rb. We
would like to construct Y such thatRa(X ?Y )Rb is
?better? than R for the task at hand. Therefore, we
construct Y as ?Y ? where Y ? is a regex constructed
from negative matches ofR. Specifically, we look at
each negative match of R and identify the substring
of the match that corresponds to X . We then apply
a greedy heuristic (see below) to these substrings to
yield a negative dictionary Y ?. Finally, the trans-
formed regexRa(X??Y ?)Rb is implemented using
the negative lookahead expression Ra(?! Y?)XRb.
Greedy Heuristic for Negative Dictionaries Im-
plementation of the above procedure requires cer-
tain judicious choices in the construction of the neg-
ative dictionary to ensure tractability of this trans-
formation. Let S(X) denote the distinct strings
that correspond to the sub-expression X in the neg-
ative matches of R.4 Since any subset of S(X)
is a candidate negative dictionary, we are left with
an exponential number of possible transformations.
In our implementation, we used a greedy heuris-
tic to pick a single negative dictionary consisting
of all those elements of S(X) that individually
improve the F-measure. For instance, in Exam-
ple 2, if the independent substitution of R51 with
(?!ENGLISH)[A-Z]\w*\s*, (?!Room)[A-Z]
\w*\s*, and (?!Chapter)[A-Z]\w*\s* each im-
proves the F-measure, we produce a nega-
tive dictionary consisting of ENGLISH, Room, and
Chapter. This is precisely how the disjunct
ENGLISH|Room|Chapter is constructed in R51b .
3For instance, the sub-expressions of ab{1,2}c are a,
ab{1,2}, ab{1,2}c, b, b{1,2}, b{1,2}c, and c.
4S(X) can be obtained automatically by identifying the sub-
string corresponding to the group X in each entry in Mn(R,D)
Procedure ReLIE(Mtr ,Mval,R0 ,T )
//Mtr : set of labeled matches used as training data
//Mval: set of labeled matches used as validation data
// R0 : user-provided regular expression
// T : set of transformations
begin
1. Rnew = R0
2. do {
3. for each transformation ti ? T
4. Candidatei=ApplyTransformations(Rnew, ti)
5. let Candidates =
?
i Candidatei
6. let R? = argmaxR?Candidates F(R,Mtr)
7. if (F(R?,Mtr) <= F(Rnew,Mtr)) return Rnew
8. if (F(R?,Mval) < F(Rnew,Mval)) return Rnew
9. Rnew = R?
10. } while(true)
end
Figure 2: ReLIE Search Algorithm
4 ReLIE Search Algorithm
Figure 2 describes the ReLIE algorithm for the
Regex Learning Problem (Definition 3) based on the
transformations described in Section 3. ReLIE is a
greedy hill climbing search procedure that chooses,
at every iteration, the regex with the highest F-
measure. An iteration in ReLIE consists of:
? Applying every transformation on the current regex
Rnew to obtain a set of candidate regexes
? From the candidates, choosing the regex R? whose
F-measure over the training dataset is maximum
To avoid overfitting, ReLIE terminates when either
of the following conditions is true: (i) there is no
improvement in F-measure over the training set;
(ii) there is a drop in F-measure when applying R?
on the validation set.
The following proposition provides an upper
bound for the running time of the ReLIE algorithm.
Proposition 2. Given any valid set of inputs Mtr,
Mval, R0 , and T , the ReLIE algorithm terminates in at
most |Mn(R0 ,Mtr )| iterations. The running time of the
algorithm TTotal(R0 ,Mtr ,Mval) ? |Mn(R0 ,Mtr )| ?
t0 , where t0 is the time taken for the first iteration of the
algorithm.
Proof. With reference to Figure 2, in each iteration, the
F-measure of the ?best? regex R? is strictly better than
Rnew. Since L(R?) ? L(Rnew), R? eliminates at least
one additional negative match compared toRnew. Hence,
the maximum number of iterations is |Mn(R0 ,Mtr )|.
For a regular expression R, let ncc(R) and nq(R) de-
note, respectively, the number of character classes and
quantifiers in R. The maximum number of possible sub-
expressions in R is |R|2, where |R| is the length of R.
Let MaxQ(R) denote the maximum number of ways in
25
which a single quantifier appearing in R can be restricted
to a smaller range. Let Fcc denote the maximum fanout5
of the character class hierarchy. Let TReEval(D) denote
the average time taken to evaluate a regex over datasetD.
Let Ri denote the regex at the beginning of iteration
i. The number of candidate regexes obtained by applying
the three transformations is
NumRE(Ri,Mtr) ? ncc(Ri)?Fcc+nq(Ri)?MaxQ(Ri)+|Ri|
2
The time taken to enumerate the character class and
quantifier restriction transformations is proportional to
the resulting number of candidate regexes. The time
taken for the negative dictionaries transformation is given
by the running time of the greedy heuristic (Section 3).
The total time taken to enumerate all candidate regexes is
given by (for some constant c)
TEnum(Ri,Mtr) ? c ? (ncc(Ri) ? Fcc + nq(Ri) ?MaxQ(Ri)
+ |Ri|
2 ?Mn(Ri,Mtr) ? TReEval(Mtr))
Choosing the best transformation involves evaluating
each candidate regex over the training and validation cor-
pus and the time taken for this step is
TPickBest(Ri,Mtr,Mval) = NumRE(Ri,Mtr)
?(TReEval(Mtr) + TReEval(Mval))
The total time taken for an iteration can be written as
TI(Ri,Mtr,Mval) =TEnum(Ri,Mtr)
+ TPickBest(Ri,Mtr,Mval)
It can be shown that the time taken in each iteration
decreases monotonically (details omitted in the interest of
space). Therefore, the total running time of the algorithm
is given by
TTotal(R0 ,Mtr ,Mval) =
?
TI(Ri,Mtr,Mval)
? |Mn(R0 ,Mtr )| ? t0 .
where t0 = TI(R0 ,Mtr ,Mval) is the running time
of the first iteration of the algorithm.
5 Experiments
In this section, we present an empirical study of
the ReLIE algorithm using four extraction tasks over
three real-life data sets. The goal of this study is to
evaluate the effectiveness of ReLIE in learning com-
plex regexes and to investigate how it compares with
standard machine learning algorithms.
5.1 Experimental Setup
Data Set The datasets used in our experiments are:
? EWeb: A collection of 50,000 web pages crawled
from a corporate intranet.
5Fanout is the number of ways in which a character class
may be restricted as defined by the hierarchy (e.g. Figure 1).
? AWeb: A set of 50,000 web pages obtained from
the publicly available University of Michigan Web
page collection (Li et al, 2006), including a sub-
collection of 10,000 pages (AWeb-S).
? Email: A collection of 10,000 emails obtained
from the publicly available Enron email collec-
tion (Minkov et al, 2005).
Extraction Tasks SoftwareNameTask, CourseNum-
berTask and PhoneNumberTask were evaluated on
EWeb, AWeb and Email, respectively. Since web
pages have large number of URLs, to keep the la-
beling task manageable, URLTask was evaluated on
AWeb-S.
Gold Standard For each task, the gold standard
was created by manually labeling all matches for the
initial regex. Note that only exact matches with the
gold standard are considered correct in our evalua-
tions. 6
Comparison Study To evaluate ReLIE for entity
extraction vis-a-vis existing algorithms, we used the
popular conditional random field (CRF). Specifi-
cally, we used the MinorThird (Cohen, 2004) imple-
mentation of CRF to train models for all four extrac-
tion tasks. For training the CRF we provided it with
the set of positive and negative matches from the ini-
tial regex with a context of 200 characters on either
side of each match7. Since it is unlikely that useful
features are located far away from the entity, we be-
lieve that 200 characters on either side is sufficient
context. The CRF used the base features described
in (Cohen et al, 2005). To ensure fair compari-
son with ReLIE, we also included the matches corre-
sponding to the input regex as a feature to the CRF.
In practice, more complex features (e.g., dictionar-
ies, simple regexes) derived by domain experts are
often provided to CRFs. However, such features can
also be used to refine the initial regex given to ReLIE.
Hence, with a view to investigating the ?raw? learn-
ing capability of the two approaches, we chose to
run all our experiments without any additional man-
ually derived features. In fact, the patterns learned
by ReLIE through transformations are often similar
6The labeled data will be made publicly available at
http://www.eecs.umich.edu/db/regexLearning/.
7Ideally, we would have preferred to let MinorThird extract
appropriate features from complete documents in the training-
set but could not get it to load our large datasets.
26
(a) SoftwareNameTask
0.50.6
0.70.8
0.91
10% 40% 80%Percentage of Data Used for Training
F
-
M
e
a
s
u
r
e
ReLIE CRF
(b) CourseNumberTask
0.50.6
0.70.8
0.91
10% 40% 80%Percentage of Data Used for Training
F
-
M
e
a
s
u
r
e
ReLIE CRF
(c) URLTask
0.50.6
0.70.8
0.91
10% 40% 80%Percentage of Data Used for Training
F
-
M
e
a
s
u
r
e
ReLIE CRF
(d) PhoneNumberTask
0.50.6
0.70.8
0.91
10% 40% 80%Percentage of Data Used for Training
F
-
M
e
a
s
u
r
e
ReLIE CRF
Figure 3: Extraction Qualitya
aFor SoftwareNameTask, with 80% training data we could not obtain results for CRF as the program
failed repeatedly during the training phase.
to the features that domain experts may provide to
CRF. We will revisit this issue in Section 5.4.
Evaluation We used the standard F-measure to
evaluate the effectiveness of ReLIE and CRF. We di-
vided each dataset into 10 equal parts and used X%
of the dataset for training (X=10, 40 and 80), 10%
for validation, and remaining (90-X)% for testing.
All results are reported on the test set.
5.2 Results
Four extraction tasks were chosen to reflect the enti-
ties commonly present in the three datasets.
? SoftwareNameTask: Extracting software names such
as Lotus Notes 8.0, Open Office Suite 2007.
? CourseNumberTask: Extracting university course
numbers such as EECS 584, Pharm 101.
? PhoneNumberTask: Extracting phone numbers such
as 1-800-COMCAST, (425)123 5678.
? URLTask: Extracting URLs such as
http:\\www.abc.com and lsa.umich.edu/ foo/.8
This section summarizes the results of our empir-
ical evaluation comparing ReLIE and CRF.
8URLTask may appear to be simplistic. However, extracting
URLs without the leading protocol definitions (e.g. http) can
be challenging.
Raw Extraction Quality The cross-validated re-
sults across all four tasks are presented in Figure 3.
? With 10% training data, ReLIE outperforms CRF
on three out of four tasks with a difference in F-
measure ranging from 0.1 to 0.2.
? As training data increases, both algorithms perform
better with the gap between the two reducing for
all the four tasks. For CourseNumberTask and URL-
Task, CRF does slightly better than ReLIE for larger
training dataset. For the other two tasks, ReLIE re-
tains its advantage over CRF.9
The above results indicate that ReLIE performs
comparably with CRF with a slight edge in condi-
tions of limited training data. Indeed, the capability
to learn high-quality extractors using a small train-
ing set is important because labeled data is often ex-
pensive to obtain. For precisely this same reason, we
would ideally like to learn the extractors once and
then apply them to other datasets as needed. Since
these other datasets may be from a different domain,
we next performed a cross-domain test (i.e., training
9For SoftwareNameTask, with 80% training data we could
not obtain results for CRF as the program failed repeatedly dur-
ing the training phase.
27
and testing on different domains).
Task(Training, Testing)
Data for Training 10% 40% 80%
ReLIE CRF ReLIE CRF ReLIE CRF
SoftwareNameTask(EWeb,AWeb) 0.920 0.297 0.977 0.503 0.971 N/A
URLTask(AWeb-S,Email) 0.690 0.209 0.784 0.380 0.801 0.507
PhoneNumberTask(Email,AWeb) 0.357 0.130 0.475 0.125 0.513 0.120
Table 1: Cross Domain Test (F-measure).
Technique
SoftwareNameTask CourseNumberTask URLTask PhoneNumberTask
training testing training testing training testing training testing
ReLIE 511.7 20.6 69.3 18.4 73.8 7.7 39.4 1.1
CRF 7597.0 2315.8 482.5 75.4 438.7 53.8 434.8 57.7
t(ReLIE)
t(CRF) 0.067 0.009 0.144 0.244 0.168 0.143 0.091 0.019
Table 2: Average Training/Testing Time (sec)(with 40% data for training)
Task(Extra Feature)
Data for Training 10% 40% 80%
CRF C+RL CRF C+RL CRF C+RL
CourseNumberTask(Negative Dictionary) 0.553 0.624 0.644 0.764 0.854 0.845
PhoneNumberTask(Quantifier) 0.695 0.893 0.820 0.937 0.821 0.964
Table 3: ReLIE as Feature Extractor (C+RL is CRF enhanced with
features learned by ReLIE).
Cross-domain Evaluation Table 1
summarizes the results of training
the algorithms on one data set and
testing on another. The scenarios
chosen are: (i) SoftwareNameTask
trained on EWeb and tested on
AWeb, (ii) URLTask trained on AWeb
and tested on Email, and (iii) Pho-
neNumberTask trained on Email
and tested on AWeb.10 We can
see that ReLIE significantly out-
performs CRF for all three tasks,
even when provided with a large
training dataset. Compared to test-
ing on the same dataset, there is a
reduction in F-measure (less than
0.1 in many cases) when the regex
learned by ReLIE is applied to a dif-
ferent dataset, while the drop for
CRF is much more significant (over 0.5 in many
cases).11
Training Time Another issue of practical consid-
eration is the efficiency of the learning algorithm.
Table 2 reports the average training and testing time
for both algorithms on the four tasks. On average Re-
LIE is an order of magnitude faster than CRF in both
building the model and applying the learnt model.
Robustness to Variations in Input Regexes The
transformations done by ReLIE are based on the
structure of the input regex. Therefore given differ-
ent input regexes, the final regexes learned by ReLIE
will be different. To evaluate the impact of the struc-
ture of the input regex on the quality of the regex
learned by ReLIE, we started with different regexes12
for the same task. We found that ReLIE is robust
to variations in input regexes. For instance, on Soft-
wareNameTask, the standard deviation in F-measure
10We do not report results for CourseNumberTask as course
numbers are specific to academic webpages and do not appear
in the other two domains
11Similar cross-domain performance deterioration for a ma-
chine learning approach has been observed by (Guo et al,
2006).
12Recall that the search space of ReLIE is limited by L(R0)
(Assumption 1). Thus to ensure meaningful comparison, for
the same task any two given input regexes R0 and R?0 are cho-
sen in such a way that although their structures are different,
Mp(R0,D) = Mp(R?0,D) and Mn(R0,D) = Mn(R
?
0,D).
of the final regexes generated from six different in-
put regexes was less than 0.05. Further details of this
experiment are omitted in the interest of space.
5.3 Discussion
The results of our comparison study (Figure 3) in-
dicates that for raw extraction quality ReLIE has a
slight edge over CRF for small training data. How-
ever, in cross-domain performance (Table 1) ReLIE
is significantly better than CRF (by 0.41 on aver-
age) . To understand this discrepancy, we examined
the final regex learned by ReLIE and compared that
with the features learned by CRF. Examples of ini-
tial regexes with corresponding final regexes learnt
by ReLIE with 10% training data are listed in Ta-
ble 4. Recall, from Section 3, that ReLIE transfor-
mations include character class restrictions, quanti-
fier restrictions and addition of negative dictionar-
ies. For instance, in the SoftwareNameTask, the final
regex listed was obtained by restricting [a-zA-Z]
to [a-z], \w to [a-zA-Z], and adding the nega-
tive dictionary (Copyright|Fall| ? ? ? |Issue). Sim-
ilarly, for the PhoneNumberTask, the final regex
involved two negative dictionaries (expressed as
(?![,]) and (?![,:])) 13 and quantifier restric-
tions (e.g. the first [A-Z\d]{2,4} was transformed
13To obtain these negative dictionaries, ReLIE not only
needs to correctly identify the dictionary entries from negative
matches but also has to place the corresponding negative looka-
head expression at the appropriate place in the regex.
28
SoftwareNameTask
R0 \b([A-Z][a-zA-Z]{1,10}\s){1,5}\s*(\w{0,2}\d[\.]?){1,4}\b
Rfinal
\b((?!(Copyright|Page|Physics|Question| ? ? ? |Article|Issue))[A-Z][a-z]{1,10}
\s){1,5}\s*([a-zA-Z]{0,2}\d[\.]?){1,4}\b
PhoneNumberTask R0 \b(1\W+)?\W?\d{3,3}\W*\s*\W?[A-Z\d]{2,4}\s*\W?[A-Z\d]{2,4}\bRfinal \b(1\W+)?\W?\d{3,3}((?![,])\W*)\s*\W?[A-Z\d]{3,3}\s*((?![,:])\W?)[A-Z\d]{3,4}\b
CourseNumberTask R0 \b([A-Z][a-zA-Z]+)\s+\d{3,3}\bRfinal \b(((?!(At|Between| ? ? ?Contact|Some|Suite|Volume))[A-Z][a-zA-Z]+))\s+\d{3,3}\b
URLTask R0 \b(\w+://)?(\w+\.){0,2}\w+\.\w+(/[
?\s]+){0,20}\b
Rfinal
\b((?!(Response 20010702 1607.csv| ? ? ?))((\w+://)?(\w+\.){0,2}\w+\.(?!(ppt
| ? ? ?doc))[a-zA-Z]{2,3}))(/[?\s]+){0,20}\b
Table 4: Sample Regular Expressions Learned by ReLIE(R0: input regex; Rfinal: final regex learned; the parts of R0
modified by ReLIE and the corresponding parts in Rfinal are highlighted.)
into [A-Z\d]{3,3}).
After examining the features learnt by CRF, it was
clear that while CRF could learn features such as the
negative dictionary it is unable to learn character-
level features. This should not be surprising since
our CRF was trained with primarily tokens as fea-
tures (cf. Section 5.1). While this limitation was less
of a factor in experiments involving data from the
same domain (some effects were seen with smaller
training data), it does explain the significant differ-
ence between the two algorithms in cross-domain
tasks where the vocabulary can be significantly dif-
ferent. Indeed, in practical usage of CRF, the main
challenge is to come up with additional complex fea-
tures (often in the form of dictionary and regex pat-
terns) that need to be given to the CRF (Minkov et
al., 2005). Such complex features are largely hand-
crafted and thus expensive to obtain. Since the Re-
LIE transformations are operations over characters,
a natural question to ask is: ?Can the regex learned
by ReLIE be used to provide features to CRF?? We
answer this question below.
5.4 ReLIE as Feature Extractor for CRF
To understand the effect of incorporating ReLIE-
identified features into CRF, we chose the two tasks
(CourseNumberTask and PhoneNumberTask) with the
least F-measure in our experiments to determine raw
extraction quality. We examined the final regex pro-
duced by ReLIE and manually extracted portions
to serve as features. For example, the negative
dictionary learned by ReLIE for the CourseNumber-
Task (At|Between| ? ? ? |Volume) was incorporated as
a feature into CRF. To help isolate the effects, for
each task, we only incorporated features correspond-
ing to a single transformation: negative dictionar-
ies for CourseNumberTask and quantifier restrictions
for PhoneNumberTask. The results of these experi-
ments are shown in Table 3. The first point worthy of
note is that performance has improved in all but one
case. Second, despite the F-measure on CourseNum-
berTask being lower than PhoneNumberTask (presum-
ably more potential for improvement), the improve-
ments on PhoneNumberTask are significantly higher.
This observation is consistent with our conjecture
in Section 5.1 that CRF learns token-level features;
therefore incorporating negative dictionaries as extra
feature provides only limited improvement. Admit-
tedly more experiments are needed to understand the
full impact of incorporating ReLIE-identified fea-
tures into CRF. However, we do believe that this is
an exciting direction of future research.
6 Summary and Future Work
We proposed a novel formulation of the problem of
learning complex character-level regexes for entity
extraction tasks. We introduced the concept of regex
transformations and described how these could be
realized using the syntactic constructs of modern
regex languages. We presented ReLIE, a powerful
regex learning algorithm that exploits these ideas.
Our experiments demonstrate that ReLIE is very ef-
fective for certain classes of entity extraction, partic-
ularly under conditions of cross-domain and limited
training data. Our preliminary results also indicate
the possibility of using ReLIE as a powerful feature
extractor for CRF and other machine learning algo-
rithms. Further investigation of this aspect of ReLIE
presents an interesting avenue of future work.
Acknowledgments
We thank the anonymous reviewers for their insight-
ful and constructive comments and suggestions. We
are also grateful for comments from David Gondek
and Sebastian Blohm.
29
References
R. Alquezar and A. Sanfeliu. 1994. Incremental gram-
matical inference from positive and negative data using
unbiased finite state automata. In SSPR.
Douglas E. Appelt and Boyan Onyshkevych. 1998. The
common pattern specification language. In TIPSTER
TEXT PROGRAM.
Geert Jan Bex et al 2006. Inference of concise DTDs
from XML data. In VLDB.
Eric Brill. 2000. Pattern-based disambiguation for natu-
ral language processing. In SIGDAT.
William W. Cohen and Andrew McCallum. 2003. Infor-
mation Extraction from the World Wide Web. in KDD
William W. Cohen. 2004. Minorthird: Methods for
identifying names and ontological relations in text
using heuristics for inducing regularities from data.
http://minorthird.sourceforge.net.
William W. Cohen et al 2005. Learning to Understand
Web Site Update Requests. In IJCAI.
Fabio Ciravegna. 2001. Adaptive information extraction
from text by rule induction and generalization. In IJ-
CAI.
H. Cunningham. 1999. JAPE ? a java annotation patterns
engine.
Francois Denis et al 2004. Learning regular languages
using RFSAs. Theor. Comput. Sci., 313(2):267?294.
Francois Denis. 2001. Learning regular languages
from simple positive examples. Machine Learning,
44(1/2):37?66.
Pedro DeRose et al 2007. DBLife: A Community In-
formation Management Platform for the Database Re-
search Community In CIDR
Pierre Dupont. 1996. Incremental regular inference. In
ICGI.
Ronen Feldman et al. 2006. Self-supervised Relation
Extraction from the Web. In ISMIS.
Henning Fernau. 2005. Algorithms for learning regular
expressions. In ALT.
Laura Firoiu et al 1998. Learning regular languages
from positive evidence. In CogSci.
K. Fukuda et al 1998. Toward information extraction:
identifying protein names from biological papers. Pac
Symp Biocomput., 1998:707?718
Ugo Galassi and Attilio Giordana. 2005. Learning regu-
lar expressions from noisy sequences. In SARA.
Minos Garofalakis et al 2000. XTRACT: a system for
extracting document type descriptors from XML doc-
uments. In SIGMOD.
Hong Lei Guo et al 2006. Empirical Study on the
Performance Stability of Named Entity Recognition
Model across Domains In EMNLP.
Java Regular Expressions. 2008. http://java.sun.com
/javase/6/docs/api/java/util/regex/package-
summary.html.
Dan Klein et al 2003. Named Entity Recognition with
Character-Level Models. In HLT-NAACL.
Vijay Krishnan and Christopher D. Manning. 2006. An
Effective Two-Stage Model for Exploiting Non-Local
Dependencies in Named Entity Recognition. In ACL.
Yunyao Li et al 2006. Getting work done on the web:
Supporting transactional queries. In SIGIR.
Andrew McCallum et al 2000. Maximum Entropy
Markov Models for Information Extraction and Seg-
mentation. In ICML.
Einat Minkov et al 2005. Extracting personal names
from emails: Applying named entity recognition to in-
formal text. In HLT/EMNLP.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34:233?272.
Lorraine Tanabe and W. John Wilbur 2002. Tagging
gene and protein names in biomedical text. Bioinfor-
matics, 18:1124?1132.
Tianhao Wu and William M. Pottenger. 2005. A semi-
supervised active learning algorithm for information
extraction from textual data. JASIST, 56(3):258?271.
Huaiyu Zhu, Alexander Loeser, Sriram Raghavan, Shiv-
akumar Vaithyanathan 2007. Navigating the intranet
with high precision. In WWW.
30
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1002?1012,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Domain Adaptation of Rule-Based Annotators
for Named-Entity Recognition Tasks
Laura Chiticariu Rajasekar Krishnamurthy Yunyao Li Frederick Reiss Shivakumar Vaithyanathan
IBM Research ? Almaden
650 Harry Road, San Jose, CA 95120, USA
{chiti, rajase, yunyaoli, frreiss}@us.ibm.com, shiv@almaden.ibm.com
Abstract
Named-entity recognition (NER) is an impor-
tant task required in a wide variety of ap-
plications. While rule-based systems are ap-
pealing due to their well-known ?explainabil-
ity,? most, if not all, state-of-the-art results
for NER tasks are based on machine learning
techniques. Motivated by these results, we ex-
plore the following natural question in this pa-
per: Are rule-based systems still a viable ap-
proach to named-entity recognition? Specif-
ically, we have designed and implemented
a high-level language NERL on top of Sys-
temT, a general-purpose algebraic informa-
tion extraction system. NERL is tuned to the
needs of NER tasks and simplifies the pro-
cess of building, understanding, and customiz-
ing complex rule-based named-entity annota-
tors. We show that these customized annota-
tors match or outperform the best published
results achieved with machine learning tech-
niques. These results confirm that we can
reap the benefits of rule-based extractors? ex-
plainability without sacrificing accuracy. We
conclude by discussing lessons learned while
building and customizing complex rule-based
annotators and outlining several research di-
rections towards facilitating rule development.
1 Introduction
Named-entity recognition (NER) is the task of iden-
tifying mentions of rigid designators from text be-
longing to named-entity types such as persons, orga-
nizations and locations (Nadeau and Sekine, 2007).
While NER over formal text such as news articles
and webpages is a well-studied problem (Bikel et
al., 1999; McCallum and Li, 2003; Etzioni et al,
2005), there has been recent work on NER over in-
formal text such as emails and blogs (Huang et al,
2001; Poibeau and Kosseim, 2001; Jansche and Ab-
ney, 2002; Minkov et al, 2005; Gruhl et al, 2009).
The techniques proposed in the literature fall under
three categories: rule-based (Krupka and Hausman,
2001; Sekine and Nobata, 2004), machine learning-
based (O. Bender and Ney, 2003; Florian et al,
2003; McCallum and Li, 2003; Finkel and Manning,
2009; Singh et al, 2010) and hybrid solutions (Sri-
hari et al, 2001; Jansche and Abney, 2002).
1.1 Motivation
Although there are well-established rule-based sys-
tems to perform NER tasks, most, if not all, state-of-
the-art results for NER tasks are based on machine
learning techniques. However, the rule-based ap-
proach is still extremely appealing due to the associ-
ated transparency of the internal system state, which
leads to better explainability of errors (Siniakov,
2010). Ideally, one would like to benefit from the
transparency and explainability of rule-based tech-
niques, while achieving state-of-the-art accuracy.
A particularly challenging aspect of rule-based
NER in practice is domain customization ? cus-
tomizing existing annotators to produce accurate re-
sults in new domains. In machine learning-based
systems, adapting to a new domain has tradition-
ally involved acquiring additional labeled data and
learning a new model from scratch. However, recent
work has proposed more sophisticated approaches
that learn a domain-independent base model, which
can later be adapted to specific domains (Florian et
1002
BASEBALL - MAJOR LEAGUE STANDINGS AFTER TUESDAY 'S GAMES NEW YORK 1996-08-28?
?AMERICAN LEAGUE EASTERN DIVISION W L PCT GB NEW YORK 74 57 .565 -BALTIMORE 70 61 .534 4 BOSTON 68 65 .511 7 
?TEXAS AT KANSAS CITYBOSTON AT CALIFORNIANEW YORK AT SEATTLE
?
BASEBALL - ORIOLES WIN , YANKEESLOSE . BALTIMORE 1996-08-27
?In Seattle , Jay Buhner 's eighth-inning single snapped a tie as the Seattle Mariners edged the New York Yankees 2-1 in the opener of a three-game series .New York starter Jimmy Key left the game in the first inning after Seattle shortstop Alex Rodriguez lined a shot off his left elbow .
?
Document d1 Document d2
Customization Requirement : City, County or State names within sports articles may refer to a sports team  or to the location itself.Customization Solution (CS) :Within sports articles, Identify all occurrences of city/county/state as Organizations, Except when a contextual clue indicates that the reference is to the location
Organization Location
Figure 1: Example Customization Requirement
al., 2004; Blitzer et al, 2006; Jiang and Zhai, 2006;
Arnold et al, 2008; Wu et al, 2009). Implement-
ing a similar approach for rule-based NER typically
requires a significant amount of manual effort to (a)
identify the explicit semantic changes required for
the new domain (e.g., differences in entity type def-
inition), (b) identify the portions of the (complex)
core annotator that should be modified for each dif-
ference and (c) implement the required customiza-
tion rules without compromising the extraction qual-
ity of the core annotator. Domain customization of
rule-based NER has not received much attention in
the recent literature with a few exceptions (Petasis et
al., 2001; Maynard et al, 2003; Zhu et al, 2005).
1.2 Problem Statement
In this paper, we explore the following natural ques-
tion: Are rule-based systems still a viable approach
to named-entity recognition? Specifically, (a) Is it
possible to build, maintain and customize rule-based
NER annotators that match the state-of-the-art re-
sults obtained using machine-learning techniques?
and (b) Can this be achieved with a reasonable
amount of manual effort?
1.3 Contributions
In this paper, we address the challenges mentioned
above by (i) defining a taxonomy of the different
types of customizations that a rule developer may
perform when adapting to a new domain (Sec. 2), (ii)
identifying a set of high-level operations required
for building and customizing NER annotators, and
(iii) exposing these operations in a domain-specific
NER rule language, NERL, developed on top of Sys-
// Core rules identify Organization and Location candidates
// Begin customization// Identify articles covering sports event from article title CR1 <SportsArticle>  Evaluate Regular Expressions <R1>// Identify locations in sports articlesCR2 Retain <Location> As <LocationMaybeOrg> If ContainedWithin <SportsArticle>// City/County/State references (e.g., New York) may refer to the sports team in that cityCR3 Retain <LocationMaybeOrg> If Matches Dictionaries<?cities.dict?,?counties.dict?,?states.dict?>
// Some city references in sports articles may refer to the city (e.g., In Seattle )// These references should not be reclassified as OrganizationCR4 Discard <LocationMaybeOrg> If Matches Regular Expression <R2>on Left Context 2 Tokens
// City references to sports teams are added to Organization and removed from LocationCR5 Augment <Organization> With <LocationMaybeOrg>// End customization
// Continuation of core rules// Remove Locations that overlap with OrganizationsDiscard <Location> If Overlaps Concepts <Organization>Figure 2: Example Customization Rules in NERL
temT (Chiticariu et al, 2010), a general-purpose
algebraic information extraction system (Sec. 3).
NERL is specifically geared towards building and
customizing complex NER annotators and makes it
easy to understand a complex annotator that may
comprise hundreds of rules. It simplifies the iden-
tification of what portions need to be modified for
a given customization requirement. It also makes
individual customizations easier to implement, as il-
lustrated by the following example.
Suppose we have to customize a domain-
independent rule-based NER annotator for the
CoNLL corpus (Tjong et al, 2003). Consider the
two sports-related news articles in Fig. 1 from the
corpus, where city names such as ?New York? or
?Seattle? can refer to either a Location or an Orga-
nization (the sports team based in that city). In the
domain-independent annotator, city names were al-
ways identified as Location, as this subtle require-
ment was not considered during rule development.
A customization to address this issue is shown in
Fig. 1, which can be implemented in NERL with five
rules (Fig. 2). This customization (explained in de-
tail in Sec. 3) improved the F?=1 score for Organi-
zation and Location by approximately 9% and 3%,
respectively (Sec. 4).
We used NERL to customize a domain-
independent rule-based NER annotator for three
different domains ? CoNLL03 (Tjong et al, 2003),
Enron (Minkov et al, 2005) and ACE05 (NIST,
2005). Our experimental results (Sec. 4.3) demon-
strate that the customized annotators have extraction
quality better than the best-known results for
1003
Affects Single Affects Multiple
Entity Type Entity Types
Identify New Instances CS , CDD , CDSD BR
Modify Existing instances CEB , CDD CATA, CG
Table 1: Categorizing NER Customizations
individual domains, which were achieved with
machine learning techniques. The fact that we are
able to achieve such results across multiple domains
answers our earlier question and confirms that
we can reap the benefits of rule-based extractors?
explainability without sacrificing accuracy.
However, we found that even using NERL, the
amount of manual effort and expertise required in
rule-based NER may still be significant. In Sec. 5,
we report on the lessons learned and outline several
interesting research directions towards simplifying
rule development and facilitating the adoption of the
rule-based approach towards NER.
2 Domain Customization for NER
We consider NER tasks following the broad defini-
tion put forth by (Nadeau and Sekine, 2007), for-
mally defined as follows:
Definition 1 Named entity recognition is the task of
identifying and classifying mentions of entities with
one or more rigid designators, as defined by (Kripke,
1982).
For instance, the identification of proper nouns
representing persons, organizations, locations, prod-
uct names, proteins, drugs and chemicals are consid-
ered as NER tasks.
Based on our experience of customizing NER an-
notators for multiple domains, we categorize the
customizations involved into two main categories as
listed below. This categorization motivates the de-
sign of NERL (Sec. 3).
Data-driven (CDD): The most common NER cus-
tomization is data-driven, where the customizations
mostly involve the addition of new patterns and
dictionary entries, driven by observations from the
training data in the new domain. An example is
the addition of a new rule to identify locations from
the beginning of news articles (e.g., ?BALTIMORE
1995-08-27? and ?MURCIA , Spain 1996-09-10?).
Application-driven: What is considered a valid
named entity and its corresponding type can vary
across application domains. The most common di-
mensions on which the definition of a named entity
can vary are:
Entity Boundary (CEB): Different application do-
mains may have different definitions of where the
same entity starts or ends. For example, a Person
may (CoNLL03) or may not (Enron) include gener-
ational markers (e.g. ?Jr.? in ?Bush Jr.? or ?IV? in
?Henry IV?).
Ambiguous Type Assignment (CATA): The exact
type of a given named entity can be ambiguous.
Different applications may assign different types
for the same named entity. For instance, all in-
stances of ?White House? may be considered as Lo-
cation (CoNLL03), or be assigned as Facility or Or-
ganization based on their context (ACE05). In fact,
even within the same application domain, entities
typically considered as of the same type may be as-
signed differently. For example, given ?New York
beat Seattle? and ?Ethiopia beat Uganda?, both
?New York? and ?Ethiopia? are teams referred by their
locations. However, (Tjong et al, 2003) considers
the former, which corresponds to a city, as an Orga-
nization, and the latter, which corresponds to a coun-
try, as a Location.
Domain-Specific Definition (CDSD): Whether a
given term is even considered a named entity may
depend on the specific domain. As an example, con-
sider the text ?Commercialization Meeting - SBeck,
BHall, BSuperty, TBusby, SGandhi-Gupta?. Infor-
mal names such as ?SBeck? and ?BHall? may be con-
sidered as valid person names (Enron).
Scope(CS): Each type of named entity usually con-
tains several subtypes. For the same named en-
tity task, different applications may choose to in-
clude different sets of subtypes. For instance,
roads and buildings are considered part of Location
in CoNLL03, while they are not included in ACE05.
Granularity(CG): Name entity types are hierarchi-
cal. Different applications may define NER tasks
at different granularities. For instance, in ACE05,
Organization and Location entity types were split
into four entity types (Organization, Location, Geo-
Political Entity and Facility).
The different customizations are summarized as
shown in Tab. 1, based on the following criteria: (i)
whether the customization identifies new instances
or modifies existing instances; and (ii) whether the
1004
customization affects single or multiple entities. For
instance, CS identifies new instances for a single en-
tity type, as it adds instances of a new subtype for an
existing entity type. Note that BR in the table de-
notes the rules used to build the core annotator.
3 Named Entity Rule Language
3.1 Grammar vs. Algebraic NER
Traditionally, rule-based NER systems were based
on the popular CPSL cascading grammar specifi-
cation (Appelt and Onyshkevych, 1998). CPSL is
designed so that rules that adhere to the standard
can be executed efficiently with finite state transduc-
ers. Accordingly, the standard defines a rigid left-to-
right execution model where a region of text can be
matched by at most one rule according to a fixed rule
priority, and where overlapping annotations are dis-
allowed in the output of each grammar phase.
While it simplifies the design of CPSL engines,
the rigidity of the rule matching semantics makes
it difficult to express operations frequently used in
rule-based information extraction. These limitations
have been recognized in the literature, and several
extensions have been proposed to allow more flex-
ible matching semantics, and to allow overlapping
annotations (Cunningham et al, 2000; Boguraev,
2003; Drozdzynski et al, 2004). However, even
with these extensions, common operations such as
filtering annotations (e.g. CR4 in Fig. 2), are dif-
ficult to express in grammars and often require an
escape to custom procedural code.
Recently, several declarative algebraic languages
have been proposed for rule-based IE systems, no-
tably AQL (Chiticariu et al, 2010) and Xlog (Shen
et al, 2007). These languages are not constrained
by the requirement that all rules map onto finite state
transducers, and therefore can express a significantly
richer semantics than grammar-based languages. In
particular, the AQL rule language as implemented in
SystemT (Chiticariu et al, 2010) can express many
common operations used in rule-based information
extraction without requiring custom code. In addi-
tion, the separation of extraction semantics from ex-
ecution enables SystemT?s rule optimizer and effi-
cient runtime engine. Indeed, as shown in (Chiti-
cariu et al, 2010), SystemT can deliver an order of
magitude higher annotation throughput compared to
a state-of-the-art CPSL-based IE system.
Since AQL is a general purpose information ex-
traction rule language, similar to CPSL and JAPE,
it exposes an expressive set of capabilities that go
beyond what is required for NER tasks. These ad-
ditional capabilities can make AQL rules more ver-
bose than is necessary for implementing rules in the
NER domain. For example, Fig. 3 shows how the
same customization rule CR4 from Fig. 2 can be
implemented in JAPE or in AQL. Notice how im-
plementing even a single customization may lead to
defining complex rules (e.g. JAPE-R1, AQL-R1)
and sometimes even using custom code (e.g. JAPE-
R2). As illustrated by this example, the rules in AQL
and JAPE tend to be complex since some operations
? e.g., filtering the outputs of one rule based on the
outputs of another rule ? that are common in NER
rule sets require multiple rules in AQL or multiple
grammar phases in JAPE.
To make NER rules easier to develop and to
understand, we designed and implemented Named
Entity Rule Language (NERL) on top of SystemT.
NERL is a declarative rule language designed specif-
ically for named entity recognition. The design of
NERL draws on our experience with building and
customizing multiple complex NER annotators. In
particular, we have identified the operations required
in practice for such tasks, and expose these opera-
tions as built-in constructs in NERL. In doing so, we
ensure that frequently performed operations can be
expressed succinctly, so as not to complicate the rule
set unnecessarily. As a result, NERL rules for named
entity recognition tasks are significantly more com-
pact and easy to understand than the equivalent AQL
rules. At the same time, NERL rules can easily be
compiled to AQL, allowing our NER rule develop-
ment framework to take advantage of the capabilities
of the SystemT rule optimizer and efficient runtime
execution engine.
3.2 NERL
For the rest of this section, we focus on describ-
ing the types of rules supported in NERL. In Sec. 4,
we shall demonstrate empirically that NERL can be
successfully employed in building and customizing
complex NER annotators.
A NERL rule has the following form:
IntConcept ? RuleBody(IntConcept1, IntConcept2, . . .)
1005
// Some city references in sports articles may refer to the city (e.g., In Seattle )// These references should not be reclassified as OrganizationCR4 Discard <LocationMaybeOrg> If Matches Regular Expression <R2> on Left Context 2 Tokens
Rule in NERL
JAPE Phase 1Rule : AmbiguousLocationContext
({Token}[2]):context({AmbiguousLoc}): annot
  :annot.AmbiguousLoc = {lc = context.string}
JAPE Phase 2Rule : RetainValidLocation
({AmbiguousLoc.lc =~ R2}):ambiguousloc -->{  // rule to discard ambiguous locationsAnnotationSet loc = bindings.get(?ambiguousloc");
outputAS.removeAll(loc); }
Rule : RetainValidLocation({Token}[2]):context({AmbiguousLoc}):loc    -->{   // Action part in Java to test R2 on left context // and delete annotationAnnotationSet loc = bindings.get(?loc");AnnotationSet context = bindings.get(?context");int begOffset = context.firstNode().getOffset().intValue(); int endOffset = context.lastNode().getOffset().intValue(); String mydocContent = doc.getContent().toString(); String contextString =
mydocContent.substring(begOffset, endOffset);if (Pattern.matches(?R2?, contextString)) {
outputAS.removeAll(loc); }}
create view LocationMaybeOrgInvalid as
select LMO.value as valuefrom LocationMaybeOrg LMO 
where MatchesRegex(/R2/,LeftContextTok(LMO.value,2));
create view LocationMaybeOrgValid as(select LMO.value as value from LocationMaybeOrg LMO)
minus(select LMOI.value as value from LocationMaybeOrgInvalid LMOI);
Two Alternative Rule sets in JAPE Equivalent Rule set in AQL
JAPE-R1 JAPE-R2 AQL-R1
Figure 3: Single Customization Rule expressed in NERL, JAPE and AQL
Intuitively, a NERL rule creates an intermediate con-
cept or named entity (IntConcept for short) by ap-
plying a NERL rule on the input text and zero or
more previously defined intermediate concepts.
NERL Rule Types The types of rules supported in
NERL are summarized in Tab. 2. In what follows,
we illustrate these types by means of examples.
Feature definition (FD): FD rules identify basic
features from text (e.g., FirstName, LastName and
CapsWord features for identifying person names).
Candidate definition (CD): CD rules identify com-
plete occurrences of the target entity. For instance,
the Sequence rule ?LastName followed by ?,? fol-
lowed by FirstName? identifies person annotations
as a sequence of three tokens, where the first and
third tokens occur in dictionaries containing last and
first names.
Candidate Refinement (CR): CR rules are used to
refine candidates generated for different annotation
types. E.g., the Filter rule CR3 in Fig. 2 retains Loca-
tionMaybeOrg annotations that appear in one of sev-
eral dictionaries.
Consolidation (CO): CO rules are used to resolve
overlapping candidates generated by multiple CD
rules. For instance, consider the text ?Please see
the following request from Dr. Kenneth Lim of the
BAAQMD.?. A CD rule may identify ?Dr. Kenneth
Lim? as a person, while another CD rule may identify
?Kenneth Lim? as a candidate person. A consolidation
rule is then used to merge these two annotations to
produce a single annotation for ?Dr. Kenneth Lim?.
NERL Examples Within these categories, three
types of rules deserve special attention, as they cor-
respond to frequently used operations and are specif-
ically designed to ensure compactness of the rule-
set. In contrast, as discussed earlier (Fig. 3), each of
these operations require several rules and possibly
custom code in existing rule-based IE systems.
DynamicDict: The DynamicDict rule is used to create
customized gazetteers on the fly. The following ex-
ample shows the need for such a rule: While ?Clin-
ton? does not always refer to a person?s last name
(Clinton is the name of several cities in USA), in
documents containing a full person name with ?Clin-
ton? as a last name (e.g., ?Hillary Clinton?) it is rea-
sonable to annotate all references to the (possibly)
ambiguous word ?Clinton? as a person. This goal
can be accomplished using the rule <Create Dynamic
Dictionary using Person with length 1 to 2 tokens>,
which creates a gazetteer on a per-document basis.
Filter: The Filter rule is used to discard/retain cer-
tain intermediate annotations based on predicates on
the annotation text and its local context. Example
filtering predicates include
? Discard C If Matches Regular Expression R
? Retain C If Contains Dictionary D on Local Context LC
? Discard C If Overlaps Concepts C1, C2, . . .
ModifySpan: The ModifySpan rule is used to expand
or trim the span of a candidate annotation. For
instance, an Entity Boundary customization to in-
clude generational markers as part of a Person anno-
tation can be implemented using a ModifySpan rule
<Expand Person Using Dictionary ?generation.dict? on
RightContext 2 Tokens>.
Using NERL Tab. 2 shows how different types of
rules are used during rule building and customiza-
tions. Since BR and CS involve identifying one
1006
Rule Category Syntax BR CDD CG
CS CDSD CEB CATA
Dictionary FD Evaluate Dictionaries < D1, D2, . . . > with flags? X X
Regex FD Evaluate Regular Expressions < R1, R2, . . . > with flags? X X
PoS FD Evaluate Part of Speech < P1, P2, . . . > with language < L >? X X
DynamicDict FD Create Dynamic Dictionary using IntConcept with flags? X X
Sequence CD IntConceptorString multiplicity?
(followed by IntConceptorString multiplicity?)+ X X
Filter CR Discard/Retain IntConcept(As IntConcept)?
If SatisfiesPredicate on LocalContext X X X
ModifySpan CR Trim/Expand IntConcept Using Dictionary < D >
on LocalContext X X
Augment CO Augment IntConcept With IntConcept X X
Consolidate CO Consolidate IntConcept using ConsolidationPolicy X X
Table 2: Description of rules supported in NERL
or more entity (sub)types from scratch, all types
of rules are used. CDD and CDSD identify addi-
tional instances for an existing type and therefore
mainly rely on FD and CD rules. On the other hand,
the customizations that modify existing instances
(CEB ,CATA,CG) require CR and CO rules.
Revisiting the example in Fig. 2, CR rules were
used to implement a fairly sophisticated customiza-
tion in a compact fashion, as follows. Rule CR1
first identifies sports articles using a regular expres-
sion based on the article title. Rule CR2 marks
Locations within these articles as LocationMaybeOrg
and Rule CR3 only retains those occurrences that
match a city, county or state name (e.g., ?Seattle?).
Rule CR4 identifies occurrences that have a contex-
tual clue confirming that the mention was to a lo-
cation (e.g., ?In? or ?At?). These occurrences are al-
ready classified correctly as Location and do not need
to be changed. Finally, CR5 adds the remaining am-
biguous mentions to Organization, which would be
deleted from Location by a subsequent core rule.
4 Development and Customization of NER
extractors with NERL
Using NERL, we have developed CoreNER, a
domain-independent generic library for multiple
NER extraction tasks commonly encountered in
practice, including Person, Organization, Location,
EmailAddress, PhoneNumber, URL, and DateTime, but
we shall focus the discussion on the first three tasks
(see Tab. 3 for entity definitions), since they are the
most challenging. In this section, we first overview
the process of developing CoreNER (Sec. 4.1). We
then describe how we have customized CoreNER
for three different domains (Sec. 4.2), and present
a quality comparison with best published results ob-
tained with state-of-the-art machine learning tech-
niques (Sec. 4.3). The tasks we consider are not re-
stricted to documents in a particular language, but
due to limited availability of non-English corpora
and extractors for comparison, our evaluation uses
English-language text. In Sec. 5 we shall elaborate
on the difficulties encountered while building and
customizing CoreNER using NERL and the lessons
we learned in the process.
4.1 Developing CoreNER
We have built our domain independent CoreNER li-
brary using a variety of formal and informal text
(e.g. web pages, emails, blogs, etc.), and informa-
tion from public data sources such as the US Census
Bureau (Census, 2007) and Wikipedia.
The development process proceeded as follows.
We first collected dictionaries for each entity
type from different resources, followed by man-
ual cleanup when needed to categorize entries col-
lected into ?strong? and ?weak? dictionaries. For
instance, we used US Census data to create several
name dictionaries, placing ambiguous entries such
as ?White? and ?Price? in a dictionary of ambigu-
ous last names, while unambiguous entries such as
?Johnson? and ?Williams? went to the dictionary for
strict last names. Second, we developed FD and
CD rules to identify candidate entities based on the
way named entities generally occur in text. E.g.,
<Salutation CapsWord CapsWord> and <FirstName
1007
Type Subtypes
PER individual
LOC
Address, Boundary, Land-Region-Natural, Region-General,
Region-International, Airport, Buildings-Grounds, Path, Plant,
Subarea-Facility, Continent, Country-or-District, Nation,
Population-Center, State-or-Province
ORG Commercial, Educational, Government, Media, Medical-ScienceNon-Governmental
Table 3: NER Task Types and Subtypes
LastName> for Person, and <CapsWord{1,3} OrgSuf-
fix> and <CapsWord{1,2} Industry> for Organization.
We then added CR and CO rules to account for
contextual clues and overlapping annotations (e.g.,
Delete Person annotations appearing within an Orga-
nization annotation).
The final CoreNER library consists of 104 FD (in-
volving 68 dictionaries, 33 regexes and 3 dynamic
dictionaries), 74 CD, 123 CR and 102 CO rules.
4.2 Customizing CoreNER
In this section we describe the process of customiz-
ing our domain-independent CoreNER library for
several different datasets. We start by discussing our
choice of datasets to use for customization.
Datasets For a rigorous evaluation of CoreNER?s
customizability, we require multiple datasets satis-
fying the following criteria: First, the datasets must
cover diverse sources and styles of text. Second,
the set of the most challenging NER tasks Person,
Organization and Location (see Tab. 3) considered
in CoreNER should be applicable to them. Finally,
they should be publicly available and preferably
have associated published results, against which we
can compare our experimental results. Towards this
end, we chose the following public datasets.
? CoNLL03 (Tjong et al, 2003): a collection of
Reuters news stories. Consists of formal text.
? Enron (Minkov et al, 2005): a collection of
emails with meeting information from the Enron
dataset. Contains predominantly informal text.
? ACE05 (NIST, 2005)1 a collection of broadcast
news, broadcast conversations and newswire re-
ports. Consists of both formal and informal text.
Customization Process The goal of customization
1The evaluation test set is not publicly available. Thus, fol-
lowing the example of (Florian et al, 2006), the publicly avail-
able set is split into a 80%/20% data split, with the last 20% of
the data in chronological order selected as test data.
is to refine the original CoreNER (hence referred
to as CoreNERorig) in order to improve its extrac-
tion quality on the training set (in terms of F?=1)
for each dataset individually. In addition, a devel-
opment set is available for CoNLL03 (referred to as
CoNLL03dev), therefore we seek to improve F?=1 on
CoNLL03dev as well.
The customization process for each dataset pro-
ceeded as follows. First, we studied the entity defini-
tions and identified their differences when compared
with the definitions used for CoreNERorig (Tab. 3).
We then added rules to account for the differences.
For example, the definition of Organization in the
CoNLL03 dataset contained a sports organization
subtype, which was not considered when develop-
ing CoreNER. Therefore, we have used public data
sources (e.g., Wikipedia) to collect and curate dic-
tionaries of major sports associations and sport clubs
from around the world. The new dictionaries, along
with regular expressions identifying sports teams in
sports articles were used for defining FD and CD
rules such as CR1 (Fig. 2). Finally, CR and CO rules
were added to filter invalid candidates and augment
the Organization type with the new sports subtype
(similar in spirit to rules CR4 and CR5 in Fig. 2).
In addition to the train and development sets, the
customization process for CoNLL03 also involved
unlabeled data from the corpus as follows. 1) Since
data-driven rules (CDD) are often created based on a
few instances from the training data, testing them on
the unlabeled data helped fine tune the rules for pre-
cision. 2) CoNLL03 is largely dominated by sports
news, but only a subset of all sports were represented
in the train dataset. Using the unlabeled data, we
were able to add CDD rules for five additional types
of sports, resulting in 0.31% improvement in F?=1
score on CoNLL03dev. 3) Unlabeled data was also
useful in identifying domain-specific gazetteers by
using simple extraction rules followed by a man-
ual cleanup phase. For instance, for CoNLL03 we
collected five gazetteers of organization and person
names from the unlabeled data, resulting in 0.45%
improvement in recall for CoNLL03dev.
The quality of the customization on the train col-
lections is shown in Tab. 5. The total number of
rules added during customization for each of the
three domains is listed in Tab. 4. Notice how rules
of all four types are used both in the development
1008
FD CD CR CO
CoreNERorig 104 74 123 102
CoreNERconll 179 56 284 71
CoreNERenron 13 10 9 1
CoreNERace 83 35 117 26
Table 4: Rules added during customization
Precision Recall F?=1
CoreNERconll 97.64 95.60 96.61
CoreNERenron 91.15 92.58 91.86
CoreNERace 92.32 91.22 91.77
Table 5: Quality of customization on train datasets (%)
of the domain independent NER annotator, and dur-
ing customizations for different domains. A total of
8 person weeks were spent on customizations, and
we believe this effort is quite reasonable by rule-
based extraction standards. For example, (Maynard
et al, 2003) reports that customizing the ANNIE
domain independent NER annotator developed us-
ing the JAPE grammar-based rule language for the
ACE05 dataset required 6 weeks (and subsequent
tuning over the next 6 months), resulting in im-
proving the quality to 82% for this dataset. As we
shall discuss shortly, with similar manual effort, we
were able to achieve results outperforming state-of-
art published results on three different datasets, in-
cluding ACE05. However, one may rightfully ar-
gue that the process is still too lengthy impeding the
widespread deployment of rule-based NER extrac-
tion. We elaborate on the effort involved and the
lessons learned in the process in Sec. 5.
4.3 Evaluation of Customization
We now present an experimental evaluation of the
customizability of CoreNER. The main goals are
to investigate: (i) the feasibility of CoreNER cus-
tomization for different application domains; (ii)
the effectiveness of such customization compared to
state-of-the-art results; (iii) the impact of different
types of customization (Tab. 1); and (iv) how often
different categories of NERL rules (Tab. 2) are used
during customization.
We measured the effectiveness of customization
using the improvement in extraction quality of the
customized CoreNER over CoreNERorig. As shown
in Tab. 6, customization significantly improved
Precision Recall F?=1
CoNLL03dev
CoreNERorig 83.81 61.77 71.12
CoreNERconll 96.49 93.76 95.11
Improvement 12.68 31.99 13.99
CoNLL03test
CoreNERorig 77.21 54.87 64.15
CoreNERconll 93.89 89.75 91.77
Improvement 15.68 34.88 27.62
Enron
CoreNERorig 85.06 69.55 76.53
CoreNERenron 88.41 82.39 85.29
Improvement 3.35 12.84 8.76
ACE2005
CoreNERorig 57.23 57.41 57.32
CoreNERace 90.11 87.82 88.95
Improvement 32.88 30.41 31.63
Table 6: Overall Improvement due to Customization (%)
Precision Recall F?=1
LOC CoreNERconll 97.17 95.37 96.26
CoNLL03dev
Florian 96.59 95.65 96.12
ORG CoreNERconll 93.70 88.67 91.11Florian 90.85 89.63 90.24
PER CoreNERconll 97.79 95.87 96.82Florian 96.08 97.12 96.60
LOC CoreNERconll 93.11 91.61 92.35
CoNLL03test
Florian 90.59 91.73 91.15
ORG CoreNERconll 92.25 85.31 88.65Florian 85.93 83.44 84.67
PER CoreNERconll 96.32 92.39 94.32Florian 92.49 95.24 93.85
Enron PER CoreNERenron 87.27 81.82 84.46Minkov 81.1 74.9 77.9
Table 7: Comparison with state-of-the-art results(%)
F?=1 score for CoreNERorig across all datasets. 2
We note that the extraction quality of
CoreNERorig was low on CoNLL03 and ACE05
mainly due to differences in entity type definitions.
In particular, sports organizations, which occurred
frequently in the CoNLL03 collection, were not
considered during the development of CoreNERorig,
while in ACE05, ORG and LOC entity types were
split into four entity types (Organization, Location,
Geo-Political Entity and Facility). Customizations
such as CS and CG address the above changes
in named-entity type definition and substantially
improve the extraction quality of CoreNERorig.
Next, we compare the extraction quality of the
2CoNLL03dev and CoNLL03test correspond to the develop-
ment and test sets for CoNLL03 respectively.
1009
customized CoreNER for CoNLL03 and Enron3 with
the corresponding best published results by (Florian
et al, 2003) and (Minkov et al, 2005). Tab. 7 shows
that our customized CoreNER outperforms the cor-
responding state-of-the-art numbers for all the NER
tasks on both CoNLL03 and Enron. 4 These results
demonstrate that high-quality annotators can be built
by customizing CoreNERorig using NERL, with the
final extraction quality matching that of state-of-the-
art machine learning-based extractors.
It is worthwhile noting that the best pub-
lished results for CoNLL03 (Florian et al, 2003)
were obtained by using four different classifiers
(Robust Risk Minimization, Maximum Entropy,
Transformation-based learning, and Hidden Markov
Model) and trying six different classifier combi-
nation methods. Compared to the best published
result obtained by combining the four classifiers,
the individual classifiers performed between 2.5-
7.6% worse for Location, 5.6-15.2% for Organiza-
tion and 3.9-14.0% for Person5. Taking this into
account, the extraction quality advantage of cus-
tomized CoreNER is significant when compared
with the individual state-of-the-art classifiers.
Impact of Customizations by Type. While cus-
tomizing CoreNER for the three datasets, all types
of changes described in Sec. 2 were performed. We
measured the impact of each type of customization
by comparing the extraction quality of CoreNERorig
with CoreNERorig enhanced with all the customiza-
tions of that type. From the results for CoNLL03
(Tab. 8), we make the following observations.
? Customizations that identify additional subtypes
of entities (CS) or modify existing instances for
multiple types (CATA) have significant impact.
This effect can be especially high when the miss-
ing subtype appears very often in the new do-
main (E.g., over 50% of the organizations in
CoNLL03 are sports teams).
? Data-driven customizations (CDD) rely on the
aggregated impact of many rules. While individ-
ual rules may have considerable impact on their
3We cannot meaningfully compare our results against previ-
ously published results for ACE05, which is originally used for
mention detection while CoreNER considers only NER tasks.
4For Enron the comparison is reported only for Person, as
labeled data is available only for that type.
5Extended version obtained via private communication.
# rules added Precision Recall F?=1
CEB 3
CoNLL03dev
LOC ?0.21 ?0.22 ?0.22
ORG ?1.35 ?0.38 ?0.59
PER - - -
CoNLL03test
LOC ?0.30 ?0.36 ?0.33
ORG ?0.54 ?0.12 ?0.20
PER - - -
CATA 5
CoNLL03dev
LOC ?7.18 ?0.87 ?3.19
ORG ?1.37 ?10.67 ?9.04
PER ?0.04 - ?0.01
CoNLL03test
LOC ?7.73 ?1.20 ?3.77
ORG ?1.37 ?11.62 ?14.18
PER - - -
CDSD 2
CoNLL03dev
LOC ?0.85 - ?0.45
ORG ?1.00 ?0.07 ?0.01
PER - - -
CoNLL03test
LOC ?0.04 ?0.12 ?0.12
ORG ?0.64 - ?0.04
PER - - -
CS 149
CoNLL03dev
LOC ?1.63 ?0.21 ?0.85
ORG ?11.44 ?40.79 ?39.73
PER ?0.13 - ?0.05
CoNLL03test
LOC ?3.71 ?0.18 ?2.05
ORG ?9.2 ?36.24 ?37.96
PER ?0.58 - ?0.2
CDD 431
CoNLL03dev
LOC ?0.94 ?10.18 ?3.99
ORG ?9.63 ?11.93 ?14.71
PER ?6.12 ?28.5 ?18.84
CoNLL03test
LOC ?1.66 ?6.72 ?1.64
ORG ?8.84 ?12.40 ?15.90
PER ?9.15 ?31.48 ?22.21
Table 8: Impact by customization type on CoNLL03(%)
own (e.g., identifying all names that appear as
part of a player list increases the recall of PER by
over 6% on both CoNLL03dev and CoNLL03test),
the overall impact relies on the accumulative ef-
fect of many small improvements.
? Certain customizations (CEB and CDSD) pro-
vide smaller quality improvements, both per rule
and in aggregate.
5 Lessons Learned
Our experimental evaluation shows that rule-based
annotators can achieve quality comparable to that of
state-of-the-art machine learning techniques. In this
section we discuss three important lessons learned
regarding the human effort involved in developing
such rule-based extractors.
Usefulness of NERL We found NERL very helpful
in that it provided a higher-level abstraction catered
specifically towards NER tasks, thus hiding the com-
plexity inherent in a general-purpose IE rule lan-
guage. In doing so, NERL restricts the large space
of operations possible within a general-purpose lan-
guage to the small number of predefined ?templates?
1010
listed in Tab. 2. (We have shown empirically that our
choice of NERL rules is sufficient to achieve high
accuracy for NER tasks.) Therefore, NERL simpli-
fies development and maintenance of complex NER
extractors, since one does not need to worry about
multiple AQL statements or JAPE grammar phases
for implementing a single conceptual operation such
as filtering (see Fig. 3).
Is NERL Sufficient? Even using NERL, building
and customizing NER rules remains a labor inten-
sive process. Consider the example of designing the
filter rule CR4 from Fig. 3. First, one must exam-
ine multiple false positive Location entities to even
decide that a filter rule is appropriate. Second, one
must understand how those false positives were pro-
duced, and decide accordingly on the particular con-
cept to be used as filter (LocationMaybeOrg in this
case). Finally, one needs to decide how to build the
filter. Tab. 9 lists all the attributes that need to be
specified for a Filter rule, along with examples of
the search space for each rule attribute.
Rule Attributes Examples of Search Space
Location Intermediate Concept to filter
Predicate Type Matches Regex, Contains Dictionary, . . .
Predicate Parameter Regular Expressions, Dictionary Entries, . . .
Context Type Entity text, Left or Right context
Context Parameter k tokens, l characters
Table 9: Search space explored while adding a Filter rule
This search space problem is not unique to filter
rules. In fact, most rules in Tab. 2 have two or more
rule attributes. Therefore, designing an individual
NERL rule remains a time-consuming ?trial and er-
ror? process, in which multiple ?promising? combi-
nations are implemented and evaluated individually
before deciding on a satisfactory final rule.
Tooling for NERL The fact that NERL is a high-
level language exposing a restricted set of operators
can be exploited to reduce the human effort involved
in building NER annotators by enabling the follow-
ing tools:
Annotation Provenance Tools tracking prove-
nance (Cheney et al, 2009) for NERL rules can
help in explaining exactly which sequence of rules
is responsible for producing a given false positive,
thereby enabling one to quickly identify ?misbe-
haved? rules. For instance, one can quickly narrow
down the choices for the location where the filter
rule CR4 (Fig. 2) should be applied based on the
provenance of the false positives. Similarly, tools
for explaining false positives in the spirit of (Huang
et al, 2008), are also conceivable.
Automatic Parameter Learning The most time-
consuming part in building a rule often is to decide
the value of its parameters, especially for FD and
CR rules. For instance, while defining a CR rule,
one has to choose values for the Predicate parame-
ter and the Context parameter (see Tab. 9). Some
parameter values can be learned ? for example, dic-
tionaries (Riloff, 1993) and regular expressions (Li
et al, 2008).
Automatic Rule Refinement Tools automatically
suggesting entire customization rules to a complex
NERL program in the spirit of (Liu et al, 2010) can
further reduce human effort in building NER anno-
tators. With the help of such tools, one only needs
to consider good candidate NERL rules suggested
by the system without having to go through the
conventional manual ?trial and error? process.
6 Conclusion
In this paper, we described NERL, a high-level rule
language for building and customizing NER annota-
tors. We demonstrated that a complex NER annota-
tor built using NERL can be effectively customized
for different domains, achieving extraction quality
superior to the state-of-the-art numbers. However,
our experience also indicates that the process of de-
signing the rules themselves is still manual and time-
consuming. Finally, we discuss how NERL opens
up several interesting research directions towards the
development of sophisticated tooling for automating
some of the rule development tasks.
References
D. E. Appelt and B. Onyshkevych. 1998. The common
pattern specification language. In TIPSTER workshop.
A. Arnold, R. Nallapati, and W. W. Cohen. 2008.
Exploiting feature hierarchy for transfer learning in
named entity recognition. In ACL.
D. M. Bikel, R. L. Schwartz, and R. M. Weischedel.
1999. An algorithm that learns what?s in a name. In
Machine Learning, volume 34, pages 211?231.
J. Blitzer, R. Mcdonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
EMNLP.
1011
B. Boguraev. 2003. Annotation-based finite state pro-
cessing in a large-scale nlp arhitecture. In RANLP.
Census. 2007. U.S. Census Bureau.
http://www.census.gov.
J. Cheney, L. Chiticariu, and W. Tan. 2009. Provenance
in databases: Why, how, and where. Foundations and
Trends in Databases, 1(4):379?474.
L. Chiticariu, R. Krishnamurthy, Y. Li, S. Raghavan,
F. Reiss, and S. Vaithyanathan. 2010. SystemT: An
algebraic approach to declarative information extrac-
tion. In ACL.
H. Cunningham, D. Maynard, and V. Tablan. 2000.
JAPE: a Java Annotation Patterns Engine (Second Edi-
tion). Research Memorandum CS?00?10, Department
of Computer Science, University of Sheffield.
W. Drozdzynski, H. Krieger, J. Piskorski, U. Scha?fer, and
F. Xu. 2004. Shallow processing with unification and
typed feature structures ? foundations and applica-
tions. Ku?nstliche Intelligenz, 1:17?23.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005.
Unsupervised named-entity extraction from the web:
an experimental study. Artif. Intell., 165(1):91?134.
J. R. Finkel and C. D. Manning. 2009. Nested named
entity recognition. In EMNLP.
R. Florian, A. Ittycheriah, H. Jing, and T. Zhang. 2003.
Named entity recognition through classifier combina-
tion. In CoNLL.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N. Nicolov, and S. Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In NAACL-HLT.
R. Florian, H. Jing, N. Kambhatla, and I. Zitouni. 2006.
Factorizing complex models: A case study in mention
detection. In ACL.
D. Gruhl, M. Nagarajan, J. Pieper, C. Robson, and
A. Sheth. 2009. Context and domain knowledge en-
hanced entity spotting in informal text. In ISWC.
J. Huang, G. Zweig, and M. Padmanabhan. 2001. Infor-
mation extraction from voicemail. In ACL.
Jiansheng Huang, Ting Chen, AnHai Doan, and Jeffrey F.
Naughton. 2008. On the Provenance of Non-Answers
to Queries over Extracted Data. PVLDB, 1(1):736?
747.
M. Jansche and S. Abney. 2002. Information extraction
from voicemail transcripts. In EMNLP.
J. Jiang and C. Zhai. 2006. Exploiting domain structure
for named entity recognition. In NAACL-HLT.
Saul Kripke. 1982. Naming and Necessity. Harvard Uni-
versity Press.
G. R. Krupka and K. Hausman. 2001. IsoQuest Inc.: De-
scription of the NetOwlTM extractor system as used
for MUC-7. In MUC-7.
Y. Li, R. Krishnamurthy, S. Raghavan, S. Vaithyanathan,
and H. V. Jagadish. 2008. Regular expression learning
for information extraction. In EMNLP.
B. Liu, L. Chiticariu, V. Chu, H. V. Jagadish, and F. Reiss.
2010. Automatic Rule Refinement for Information
Extraction. PVLDB, 3.
D. Maynard, K. Bontcheva, and H. Cunningham. 2003.
Towards a semantic extraction of named entities. In
RANLP.
A. McCallum and W. Li. 2003. Early results for named
entity recognition with conditional random fields, fea-
ture induction and web-enhanced lexicons. In CoNLL.
E. Minkov, R. C. Wang, and W. W. Cohen. 2005. Ex-
tracting personal names from emails: Applying named
entity recognition to informal text. In HLT/EMNLP.
D. Nadeau and S. Sekine. 2007. A survey of named
entity recognition and classification. Linguisticae In-
vestigationes, 30(1):3?26.
NIST. 2005. The ace evaluation plan.
F. J. Och O. Bender and H. Ney. 2003. Maximum en-
tropy models for named entity recognition. In CoNLL.
G. Petasis, F. Vichot, F. Wolinski, G. Paliouras,
V. Karkaletsis, and C. Spyropoulos. 2001. Using
machine learning to maintain rule-based named-entity
recognition and classification systems. In ACL.
T. Poibeau and L. Kosseim. 2001. Proper name ex-
traction from non-journalistic texts. In Computational
Linguistics in the Netherlands, pages 144?157.
E. Riloff. 1993. Automatically constructing a dictionary
for information extraction tasks. In KDD.
S. Sekine and C. Nobata. 2004. Definition, dictionaries
and tagger for extended named entity hierarchy. In
Conference on Language Resources and Evaluation.
W. Shen, A. Doan, J. F. Naughton, and R. Ramakrishnan.
2007. Declarative information extraction using data-
log with embedded extraction predicates. In VLDB.
S. Singh, D. Hillard, and C. Leggeteer. 2010. Minimally-
supervised extraction of entities from text advertise-
ments. In NAACL-HLT.
P. Siniakov. 2010. GROPUS - an adaptive rule-based al-
gorithm for information extraction. Ph.D. thesis, Freie
Universitat Berlin.
R. Srihari, C. Niu, and W. Li. 2001. A hybrid approach
for named entity and sub-type tagging. In ANLP.
E. F. Tjong, K. Sang, and F. De Meulder. 2003. Intro-
duction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In CoNLL.
D. Wu, W. S. Lee, N. Ye, and H. L. Chieu. 2009. Domain
adaptive bootstrapping for named entity recognition.
In EMNLP.
J. Zhu, V. Uren, and E. Motta. 2005. Espotter: Adaptive
named entity recognition for web browsing. In WM.
1012
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 128?138, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Towards Efficient Named-Entity Rule Induction for Customizability
Ajay Nagesh1,2
1IITB-Monash Research Academy
ajaynagesh@cse.iitb.ac.in
Ganesh Ramakrishnan
2IIT Bombay
ganesh@cse.iitb.ac.in
Laura Chiticariu
IBM Research - Almaden
chiti@us.ibm.com
Rajasekar Krishnamurthy
IBM Research - Almaden
rajase@us.ibm.com
Ankush Dharkar
SASTRA University
ankushdharkar@cse.sastra.edu
Pushpak Bhattacharyya
IIT Bombay
pb@cse.iitb.ac.in
Abstract
Generic rule-based systems for Information
Extraction (IE) have been shown to work
reasonably well out-of-the-box, and achieve
state-of-the-art accuracy with further domain
customization. However, it is generally rec-
ognized that manually building and customiz-
ing rules is a complex and labor intensive pro-
cess. In this paper, we discuss an approach
that facilitates the process of building cus-
tomizable rules for Named-Entity Recognition
(NER) tasks via rule induction, in the Annota-
tion Query Language (AQL). Given a set of
basic features and an annotated document col-
lection, our goal is to generate an initial set
of rules with reasonable accuracy, that are in-
terpretable and thus can be easily refined by
a human developer. We present an efficient
rule induction process, modeled on a four-
stage manual rule development process and
present initial promising results with our sys-
tem. We also propose a simple notion of ex-
tractor complexity as a first step to quantify
the interpretability of an extractor, and study
the effect of induction bias and customization
of basic features on the accuracy and complex-
ity of induced rules. We demonstrate through
experiments that the induced rules have good
accuracy and low complexity according to our
complexity measure.
1 Introduction
Named-entity recognition (NER) is the task of iden-
tifying mentions of rigid designators from text be-
longing to named-entity types such as persons, orga-
nizations and locations (Nadeau and Sekine, 2007).
Generic NER rules have been shown to work reason-
ably well-out-of-the-box, and with further domain
customization (Chiticariu et al2010b), achieve
quality surpassing state-of-the-art results. Table 1
System Dataset F?=1
Generic Customized
GATE ACE2002 57.8 82.2
ACE 2005 57.32 88.95
SystemT CoNLL 2003 64.15 91.77
Enron 76.53 85.29
Table 1: Quality of generic vs. customized rules.
summarizes the quality of NER rules out-of-the-box
and after domain customization in the GATE (Cun-
ningham et al2011) and SystemT (Chiticariu et
al., 2010a) systems, as reported in (Maynard et al
2003) and (Chiticariu et al2010b) respectively.
Rule-based systems are widely used in enterprise
settings due to their explainability. Rules are trans-
parent, which leads to better explainability of errors.
One can easily identify the cause of a false positive
or negative, and refine the rules without affecting
other correct results identified by the system. Fur-
thermore, rules are typically easier to understand by
an IE developer and can be customized for a new
domain without requiring additional labeled data.
Typically, a rule-based NER system consists of a
combination of four categories of rules (Chiticariu et
al., 2010b): (1) Basic Feature (BF) rules to identify
components of an entity such as first name and last
name. (2) Candidate definition (CD) rules to iden-
tify complete occurrences of an entity by combining
the output of multiple BF rules, e.g., first name fol-
lowed by last name is a person candidate. (3) Candi-
date refinement (CR) rules to refine candidates gen-
erated by CD rules. E.g., discard candidate persons
contained within organizations. (4) Consolidation
rules (CO) to resolve overlapping candidates gener-
ated by multiple CD and CR rules.
A well-known drawback that influences the
adoptability of rule-based NER systems is the man-
128
ual effort required to build the rules. A common ap-
proach to address this problem is to build a generic
NER extractor and then customize it for specific do-
mains. While this approach partially alleviates the
problem, substantial manual effort (in the order of
several person weeks) is still required for the two
stages as reported in (Maynard et al2003; Chiti-
cariu et al2010b). In this paper, we present initial
work towards facilitating the process of building a
generic NER extractor using induction techniques.
Specifically, given as input an annotated docu-
ment corpus, a set of BF rules, and a default CO
rule for each entity type, our goal is to generate a
set of CD and CR rules such that the resulting ex-
tractor constitutes a good starting point for further
refinement by a developer. Since the generic NER
extractor has to be manually customized, a major
challenge is to ensure that the generated rules have
good accuracy, and, at the same time, that they are
not too complex, and consequently interpretable.
The main contributions in this paper are
1. An efficient system for NER rule induction, us-
ing a highly expressive rule language (AQL) as
the target language. The first phase of rule in-
duction uses a combination of clustering and
relative least general generalization (RLGG)
techniques to learn CD rules. The second phase
identifies CR rules using a propositional rule
learner like JRIP to learn accurate composi-
tions of CD rules.
2. Usage of induction biases to enhance the inter-
pretability of rules. These biases capture the
expertise gleaned from manual rule develop-
ment and constrain the search space in our in-
duction system.
3. Definition of an initial notion of extractor com-
plexity to quantify the interpretability of an ex-
tractor and to guide the process of adding in-
duction biases to favor learning less complex
extractors. This is to ensure that the rules are
easily customizable by the developer.
4. Scalable induction process through usage of
SystemT, a state-of-the-art IE system which
serves as a highly efficient theorem prover for
AQL, and performance optimizations such as
clustering of examples and parallelizing vari-
ous modules (E.g.: propositional rule learning).
Roadmap We first describe preliminaries on Sys-
temT and AQL (Section 3) and define the target lan-
guage for our induction algorithm and the notion of
rule complexity (Section 4). We then present our
approach for inducing CD and CR rules, and dis-
cuss induction biases that would favor interpretabil-
ity (Section 5), and discuss the results of an empir-
ical evaluation (Section 6). We conclude with av-
enues for improvement in the future (Section 7).
2 Related Work
Existing approaches to rule induction for IE focus
on rule-based systems based on the cascading gram-
mar formalism exemplified by the Common Pat-
tern Specification Language (CPSL) (Appelt and
Onyshkevych, 1998), where rules are specified as
a sequence of basic features that describe an en-
tity, with limited predicates in the context of an
entity mention. Patel et al2009) and Soderland
(1999) elaborate on top-down techniques for induc-
tion of IE rules, whereas (Califf and Mooney, 1997;
Califf and Mooney, 1999) discuss a bottom-up IE
rule induction system that uses the relative least gen-
eral generalization (RLGG) of examples1. However,
in all these systems, the expressivity of the rule-
representation language is restricted to that of cap-
turing sequence information. As discussed in Sec-
tion 3, contextual clues and higher level rule inter-
actions such as filtering and join are very difficult,
if not impossible to express in such representations
without resorting to custom code. Learning higher
level interactions between rules has received little
attention. Our technique for learning higher level in-
teractions is similar to the induction of ripple down
rules (Gaines and Compton, 1995), which, to the
best of our knowledge, has not been previously ap-
plied to IE. A framework for refining AQL extractors
based on an annotated document corpus described
in (Liu et al2010). We present complementary
techniques for inducing an initial extractor that can
be automatically refined in this framework.
3 Preliminaries
SystemT is a declarative IE system based on an al-
gebraic framework. In SystemT, developers write
rules in AQL. To represent annotations in a docu-
1Our work also makes use of RLGGs but computes these
generalizations for clusters of examples, instead of pairs.
129
Figure 1: Example Person extractor in AQL
ment, AQL uses a simple relational data model with
three types: a span is a region of text within a docu-
ment identified by its ?begin? and ?end? positions; a
tuple is a fixed-size list of spans; a relation, or view,
is a multi-set of tuples, where every tuple in the view
must be of the same size.
Figure 1 shows a portion of a Person extractor
written in AQL. The basic building block of AQL
is a view. A view is a logical description of a set of
tuples in terms of (i) the document text (denoted as a
special view called Document), and (ii) the contents
of other views, as specified in the from clauses of
each statement. Figure 1 also illustrates five of the
basic constructs that can be used to define a view,
and which we explain next. The complete specifica-
tion can be found in the AQL manual (IBM, 2012).
In the paper, we will use ?rules? and ?views? inter-
changeably.
The extract statement specifies basic character-
level extraction primitives such as regular expression
and dictionary matching over text, creating a tuple
for each match. As an example, rule R1 uses the ex-
tract statement to identify matches (Caps spans) of a
regular expression for capitalized words.
The select statement is similar to the SQL select
statement but it contains an additional consolidate
on clause (explained further), along with an exten-
sive collection of text-specific predicates. Rule R5
illustrates a complex example: it selects First spans
immediately followed within zero tokens by a Last
span, where the latter is also a Caps span. The
two conditions are specified using two join predi-
cates: FollowsTok and Equals respectively. For each
triplet of First, Last and Caps spans satisfying the two
predicates, the CombineSpans built-in scalar func-
tion in the select clause constructs larger PersonFirst-
Last spans that begin at the begin position of the First
span, and end at the end position of the Last (also
Caps) span.
The union all statement merges the outputs of two
or more statements. For example, rule R6 unions
person candidates identified by rules R4 and R5.
The minus statement subtracts the output of one
statement from the output of another. For example,
rule R8 defines a view PersonAll by filtering out Per-
sonInvalid tuples from the set of PersonCandidate tu-
ples. Notice that rule R7 used to define the view Per-
sonInvalid illustrates another join predicate of AQL
called Overlaps, which returns true if its two argu-
ment spans overlap in the input text. Therefore, at
a high level, rule R8 removes person candidates that
overlap with an Organization span. (The Organization
extractor is not depicted in the figure.)
The consolidate clause of a select statement re-
moves selected overlapping spans from the indicated
column of the input tuples, according to the spec-
ified policy (for instance, ?ContainedWithin?). For
example, rule R9 retains PersonAll spans that are not
contained in other PersonAll spans.
Internally, SystemT compiles an AQL extractor
into an executable plan in the form of a graph of
operators. The formal definition of these operators
takes the form of an algebra (Reiss et al2008), sim-
ilar to relational algebra, but with extensions for text
processing. The decoupling between AQL and the
operator algebra allows for greater rule expressiv-
ity because the rule language is not constrained by
the need to compile to a finite state transducer, as in
grammar systems based on the CPSL standard. In
fact, join predicates such as Overlaps, as well as fil-
ter operations (minus) are extremely difficult to ex-
130
press in CPSL systems such as GATE without an
escape to custom code (Chiticariu et al2010b). In
addition, the decoupling between the AQL specifi-
cation of ?what? to extract from ?how? to extract
it, allows greater flexibility in choosing an efficient
execution strategy among the many possible opera-
tor graphs that may exist for the same AQL extrac-
tor. Therefore, extractors written in AQL achieve
orders of magnitude higher throughput (Chiticariu
et al2010a).
4 Induction Target Language
Our goal is to automatically generate NER extrac-
tors with good quality, and at the same time, man-
ageable complexity, so that the extractors can be fur-
ther refined and customized by the developer. To this
end, we focus on inducing extractors using the sub-
set of AQL constructs described in Section 3. We
note that we have chosen a small subset of AQL con-
structs that are sufficient to implement several com-
mon operations required for NER. However, AQL is
a much more expressive language, and incorporating
additional constructs is subject to our future work.
In this section we describe the building blocks of
our target language, and propose a simple definition
for measuring the complexity of an extractor.
Target Language. The components of the target
language are as follows, and summarized in Table 2.
Basic features (BF): BF views are specified using the
extract statement, such as rulesR1 toR3 in Figure 1.
In this paper, we assume as input a set of basic fea-
tures, consisting of dictionaries and regular expres-
sions.
Candidate definition (CD): CD views are expressed
using the select statement to combine BF views with
join predicates (e.g., Equals, FollowsTok or Over-
laps), and the CombineSpans scalar function to con-
struct larger candidate spans from input spans. Rules
R4 and R5 in Figure 1 are example CD rules. In
general, a CD view is defined as: ?Select all spans
constructed from view1, view2, . . ., viewn, such that all
join predicates are satisfied?.
Candidate refinement (CR): CR views are used to
discard spans output by the CD views that may be
incorrect. In general, a CR view is defined as: ?From
the list of spans of viewvalid subtract all those spans that
belong to viewinvalid?. viewvalid is obtained by join-
ing all the positive CD clues on the Equals predicate
and viewinvalid is obtained by joining all the nega-
tive overlapping clues with the Overlaps predicate
and subsequently ?union?ing all the negative clues.
(e.g., similar in spirit to rules R6, R7 and R8 in Fig-
ure 1, except that the subtraction is done from a sin-
gle view and not the union of multiple views).
Consolidation (CO): Finally, a select statement with
a fixed consolidate clause is used for each entity type
to remove overlapping spans from CR views. An
example CO view is defined by rule R9 in Figure 1.
Extractor Complexity. Since our goal is to gener-
ate extractors with manageable complexity, we must
introduce a quantitative measure of extractor com-
plexity, in order to (1) judge the complexity of the
extractors generated by our system, and (2) reduce
the search space considered by the induction system.
To this end, we define a simple complexity score
that is a function of the number of rules, and the
number of input views to each rule of the extrac-
tor. In particular, we define the length of rule R,
denoted as L(R), as the number of input views in
the from clause(s) of the view. For example, in Fig-
ure 1, we have L(R4) = 2 and L(R5) = 3, since
R4 and R5 have two, and respectively three views
in the from clause. Furthermore, L(R8) = 2 since
each of the two inner statements of R8 has one from
clause with a single input view. The complexity of
BF rules (e.g., R1 to R3) and CO rules (e.g., R9) is
always 1, since these types of rules have a single in-
put view. We define the complexity of extractor E,
denoted as C(E) as the sum of lengths of all rules of
E. For example, the complexity of the Person extrac-
tor from Figure 1 is 15, plus the length of all rules
involved in defining Organization, which are omitted
from the figure.
Our simple notion of rule length is motivated
by existing literature in the area of database sys-
tems (Abiteboul et al1995), where the size of a
conjunctive query is determined only by the number
of atoms in the body of the query (e.g., items in the
FROM clause), and it is independent on the number
of join variables (i.e., items in the WHERE clause),
or the size of the head of the query (e.g., items in the
SELECT clause). As such, our notion of complexity
is rather coarse, and we shall discuss its shortcom-
ings in detail in Section 6.2. However, we shall show
that the complexity score significantly reduces the
search space of our induction techniques leading to
131
Phase name AQL statements Prescription Rule Type
Basic Features extract Off-the-shelf, Learning using prior
work (Riloff, 1993; Li et al2008)
Basic Features Definition
Phase 1 (Clustering and
RLGG)
select Bottom-up learning (LGG), Top-down refine-
ment
Development of Candidate
Rules
Phase 2 (Propositional Rule
Learning)
select, union
all, minus
RIPPER, Lightweight Rule Induction Candidate Rules Filtering
Consolidation consolidate,
union all
Manually identified consolidation rules, based
on domain knowledge
Consolidation rules
Table 2: Phases in induction, the language constructs invoked in each phase, the prescriptions for inducing rules in the
phase and the corresponding type of rule in manual rule development.
simpler and smaller extractors, and therefore consti-
tutes a good basis for more comprehensive measures
of interpretability in the future.
5 Induction of Rules
Since the goal is to generate rules that can be cus-
tomized by humans, the overall structure of the in-
duced rules must be similar in spirit to what a devel-
oper following best practices would write. Hence,
the induction process is divided into multiple phases.
Figure 2 shows the correspondence between the
phases of induction and the types of rules. In Ta-
ble 2, we summarize the phases of our induction al-
gorithm, along with the subset of AQL constructs
that comprise the language of the rules learnt in that
phase, the possible methods prescribed for inducing
the rules and their correspondence with the stages in
the manual rule development.
Our induction system generates rules for two of
the four categories, namely CD and CR rules as
highlighted in Figure 2. We assume that we are
given the BFs in the form of dictionaries and reg-
ular expressions. Prior work on learning dictionar-
ies (Riloff, 1993) and regular expressions (Li et al
2008) could be leveraged to semi-automate the pro-
cess of defining the basic features.
We represent each example, in conjunction with
relevant background knowledge in the form first
order horn clauses. This background knowledge
will serve as input to our induction system. The
first phase of induction uses a combination of
clustering and relative least general generalization
(RLGG) (Nienhuys-Cheng andWolf, 1997; Muggle-
ton and Feng, 1992) techniques. At the end of this
phase, we have a number of CD rules. In the sec-
ond phase, we begin by forming a structure called
the span-view table. Broadly speaking, this is an
attribute-value table formed by all the views induced
in the first phase along with the textual spans gener-
ated by them. The attribute-value table is used as
input to a propositional rule learner such as JRIP
to learn accurate compositions of a useful (as deter-
mined by the learning algorithm) subset of the CD
rules. This forms the second phase of our system.
The rules learnt from this phase are the CR rules.
At various phases, several induction biases are intro-
duced to enhance the interpretability of rules. These
biases capture the expertise gleaned from manual
rule development and constrain the search space in
our induction system.
The hypothesis language of our induction sys-
tem is Annotation Query Language (AQL) and we
use SystemT as the theorem prover. SystemT pro-
vides a very fast rule execution engine and is cru-
cial in our induction system as we test multiple hy-
potheses in the search for the more promising ones.
AQL provides a very expressive rule representation
language that is proven to be capable of encoding
all the paradigms that any rule-based representa-
tion can encode. The dual advantages of rich rule-
representation and execution efficiency are the main
motivation behind our choice.
We discuss our induction procedure in detail next.
5.1 Basic Features and Background Knowledge
We assume that we are provided with a set of dictio-
naries and regular expressions for defining all our
basic create view statements. R1, R2 and R3 in
Figure 1 are such basic view definitions. The ba-
sic views are compiled and executed in SystemT
over the training document collection and the re-
sulting spans are represented by equivalent predi-
cates in first order logic. Essentially, each train-
ing example is represented as a definite clause,
132
Figure 2: Correspondence between Manual Rule devel-
opment and Rule Induction.
that includes in its body, the basic view-types en-
coded as background knowledge predicates. To es-
tablish relationships between different background
knowledge predicates for each training example, we
define some additional ?glue predicates? such as
contains and before.
5.2 Induction of Candidate Definition Rules
Clustering Module. We obtain non-overlapping
clusters of examples within each type, by comput-
ing similarities between their representations as def-
inite clauses. We present the intuition behind this
approach in Figure 3 which illustrates the process
of taking two examples and finding their generaliza-
tion. It is worthwhile to look at generalizations of
instances that are similar. For instance, two token
person names such as Mark Waugh and Mark Twain
are part of a single cluster. However, we would not
be able to generalize a two-token name (e.g., Mark
Waugh) with another name consisting of initials fol-
lowed by a token (e.g., M. Waugh). Using a wrap-
per around the hierarchical agglomerative cluster-
ing implemented in LingPipe2, we cluster examples
and look at generalizations only within each cluster.
Clustering also helps improve efficiency by reduc-
ing the computational overhead, since otherwise, we
would have to consider generalizations of all pairs of
examples (Muggleton and Feng, 1992).
RLGG computation. We compute our CD
rules as the relative least general generalization
(RLGG) (Nienhuys-Cheng and Wolf, 1997; Mug-
gleton and Feng, 1992) of examples in each clus-
ter. Given a set of clauses in first order logic,
their RLGG is the least generalized clause in the
2http://alias-i.com/lingpipe/demos/tutorial/cluster/read-
me.html
Figure 3: Relative Least General Generalization
subsumption lattice of the clauses relative to the
background knowledge (Nienhuys-Cheng and Wolf,
1997). RLGG is associative, and we use this fact
to compute RLGGs of sets of examples in a clus-
ter. The RLGG of two bottom clauses as computed
in our system and its translation to an AQL view is
illustrated in Figure 3. We filter out noisy RLGGs
and convert the selected RLGGs into the equivalent
AQL views. Each such AQL view is treated as a
CD rule. We next discuss the process of filtering-
out noisy RLGGs. We interchangeably refer to the
RLGGs and the clusters they represent .
Iterative Clustering and RLGG filtering. Since
clustering is sub-optimal, we expected some clusters
in a single run of clustering to have poor RLGGs, ei-
ther in terms of complexity or precision. We there-
fore use an iterative clustering approach, based on
the separate-and-conquer (Fu?rnkranz, 1999) strat-
egy. In each iteration, we pick the clusters with the
best RLGGs and remove all examples covered by
those RLGGs. The best RLGGs must have preci-
sion and number of examples covered above a pre-
specified threshold.
5.3 Induction of Candidate Refinement Rules
Span-View Table. The CD views from phase 1
along with the textual spans they generate, yield the
span-view table. The rows of the table correspond
to the set of spans returned by all the CD views. The
columns correspond to the set of CD view names.
Each span either belongs to one of the named en-
tity types (PER, ORG or LOC) or is none of them
(NONE); the type information constitutes its class
label (see Figure 4 for an illustrated example). The
cells in the table correspond to either a match (M) or
a no-match (N) or partial/overlapping match (O) of
a span generated by a CD view. This attribute-value
table is used as input to a propositional rule learner
133
Figure 4: Span-View Table
like JRIP to learn compositions of CD views.
Propositional Rule Learning. Based on our study
of different propositional rule learners, we decided
to use RIPPER (Fu?rnkranz and Widmer, 1994) im-
plemented as the JRIP classifier in weka (Witten et
al., 2011). Some considerations that favor JRIP are
(i) absence of rule ordering, (ii) ease of conversion
to AQL and (iii) amenability to add induction biases
in the implementation.
A number of syntactic biases were introduced in
JRIP to aid in the interpretability of the induced
rules. We observed in our manually developed rules
that CR rules for a type involve interaction between
CDs for the same type and negations (not-overlaps,
not matches) of CDs of the other types. This bias
was incorporated by constraining a JRIP rule to con-
tain only positive features (CDs) of the same type
(say PER) and negative features (CDs) of only other
types (ORG and LOC, in this case).
The output of the JRIP algorithm is a set of
rules, one set for each of PER, ORG and LOC.
Here is an example rule: PER-CR-Rule ? (PerCD
= m) AND (LocCD != o) which is read as : ?If a
span matches PerCD and does not overlap with LocCD,
then that span denotes a PER named entity?. Here
PerCD is {[FirstName ? CapsPerson][LastName
? CapsPerson]} 3 and LocCD is {[CapsPlace ?
CitiesDict]}. This rule filters out wrong person
annotations like ?Prince William? in Prince William
Sound. (This is the name of a location but has over-
lapped with a person named entity.) In AQL, this
effect can be achieved most elegantly by the minus
(filter) construct. Such an AQL rule will filter all
those occurrences of Prince William from the list of
3Two consecutive spans where the 1st is FirstName and
CapsPerson and the 2nd is LastName and CapsPerson.
persons that overlap with a city name.
Steps such as clustering, computation of RLGGs,
JRIP, and theorem proving using SystemT were par-
allelized. Once the CR views for each type of
named entity are learnt, many forms of consolida-
tions (COs) are possible, both within and across
types. A simple consolidation policy that we have
incorporated in the system is as follows: union all
the rules of a particular type, then perform a con-
tained within consolidation, resulting in the final set
of consolidated views for each named entity type.
6 Experiments
We evaluate our system on CoNLL03 (Tjong
Kim Sang and De Meulder, 2003), a collection
of Reuters news stories. We used the CoNLL03
training set for induction and report results on the
CoNLL03 test collection.
The basic features (BFs) form the primary input to
our induction system. We experimented with three
sets of BFs:
Initial set(E1): The goal in this setup is to induce
an initial set of rules based on a small set of reason-
able BFs. We use a conservative initial set consisting
of 15 BFs (5 regular expressions and 10 dictionar-
ies).
Enhanced set (E2): Based on the results of E1,
we identify a set of additional domain independent
BFs4. Five views were added to the existing set in
E1 (1 regular expression and 4 dictionaries). The
goal is to observe whether our approach yields rea-
sonable accuracies compared to generic rules devel-
oped manually.
Domain customized set (E3): Based on the
knowledge of the domain of the training dataset
(CoNLL03), we introduced a set of features specific
to this dataset. These included sports-related person,
organization and location dictionaries5. These views
were added to the existing set in E2. The intended
goal is to observe what are the best possible accura-
cies that could be achieved with BFs customized to
a particular domain.
The set of parameters for iterative clustering on
which the accuracies reported are : the precision
threshold for the RLGGs of the clusters was 70%
4E.g., the feature preposition dictionary was added in E2 to
help identify organization names such as Bank of England.
5Half of the documents in CoNLL03 are sports-related.
134
Train Test
Type P R F P R F C(E)
E1 (Initial set)
PER 88.5 41.4 56.4 92.5 39.4 55.3 144
ORG 89.1 7.3 13.4 85.9 5.2 9.7 22
LOC 91.6 54.5 68.3 87.3 55.3 67.8 105
Overall 90.2 35.3 50.7 89.2 33.3 48.5 234
E2 (Enhanced set)
PER 84.7 52.9 65.1 87.5 49.9 63.5 233
ORG 88.2 7.8 14.3 85.8 5.9 11.0 99
LOC 92.1 58.6 71.7 88.6 59.1 70.9 257
Overall 88.6 40.7 55.8 88.0 38.2 53.3 457
E3 (Domain customized set)
PER 89.9 57.3 70.0 91.7 56.0 69.5 430
ORG 86.9 50.9 64.2 86.9 47.5 61.4 348
LOC 90.8 67.0 77.1 84.3 67.3 74.8 356
Overall 89.4 58.7 70.9 87.3 57.0 68.9 844
Table 3: Results on CoNLL03 dataset with different basic
feature sets
and the number of examples covered by each RLGG
was 5. We selected the top 5 clusters from each iter-
ation whose RLGGs crossed this threshold. If there
were no such clusters then we would lower the preci-
sion threshold to 35% (half of the threshold). When
no new clusters were formed, we ended the itera-
tions.
6.1 Experiments and Results
Effect of Augmenting Basic Features. Table 3
shows the accuracy and complexity of rules induced
with the three basic feature sets E1, E2 and E3,
respectively 6. The overall F-measure on the test
dataset is 48.5% with E1, it increases to around
53.3% with E2 and is highest at 68.9% with E3.
As we increase the number of BFs, the accuracies
of the induced extractors increases, at the cost of
an increase in complexity. In particular, the re-
call increases significantly across the board, and is
more prominent between E2 and E3, where the ad-
ditional domain specific features result in recall in-
crease from 5.9% to 47.5% for ORG. The precision
increases slightly for PER, but decreases slightly for
LOC and ORG with the addition of domain specific
features.
Comparison with manually developed rules. We
compared the induced extractors with the manually
developed extractors of (Chiticariu et al2010b),
heretofore referred to as manual extractors. (For a
detailed analysis, we obtained the extractors from
6These are the results for the configuration with bias.
the authors). Table 4 shows the accuracy and com-
plexity of the induced rules with E2 and E3 and the
manual extractors for the generic domain and, re-
spectively, customized for the CoNLL03 domain.
(In the table, ignore the column Induced (without
bias), which is discussed later). Our technique
compares reasonably with the manually constructed
generic extractor for two of the three entity types;
and on precision for all entity types, especially since
our system generated the rules in 1 hour, whereas the
development of manual rules took much longer 7.
Additional work is required to match the manual
customized extractor?s performance, primarily due
to shortcomings in our current target language. Re-
call that our framework is limited to a small subset
of AQL constructs for expressing CD and CR rules,
and there is a single consolidation rule. In particu-
lar, advanced constructs such as dynamic dictionar-
ies are not supported, and the set of predicates to the
Filter construct supported in our system is restricted
to predicates over other concepts, which is only a
subset of those used in (Chiticariu et al2010b).
The manual extractors also contain a larger number
of rules covering many different cases, improving
the accuracy, but also leading to a higher complex-
ity score. To better analyze the complexity, we also
computed the average rule length for each extrac-
tor by dividing the complexity score by the number
of AQL views of the extractor. The average rule
length is 1.78 and 1.87 for the induced extractors
with E2 and E3, respectively, and 1.9 and 2.1 for the
generic and customized extractors of (Chiticariu et
al., 2010b), respectively. The average rule length in-
creases from the generic extractor to the customized
extractor in both cases. On average, however, an in-
dividual induced rule is slightly smaller than a man-
ually developed rule.
Effect of Bias. The goal of this experiment is to
demonstrate the importance of biases in the induc-
tion process. The biases added to the system are
broadly of two types: (i) Partition of basic features
based on types (ii) Restriction on the type of CD
views that can appear in a CR view. 8 Without
7 (Chiticariu et al2010b) mentions that customization for 3
domains required 8 person weeks. It is reasonable to infer that
developing the generic rules took comparable effort.
8For e.g., person CR view can contain only person CD views
as positive clues and CD views of other types as negative clues.
135
(i) many semantically similar basic features (espe-
cially, regular expressions) would match a given to-
ken, leading to an increase in the length of a CD
a rule. For example, in the CD rule [FirstName-
Dict][CapsPerson ? CapsOrg]} (?A FirstNameDict
span followed by a CapsPerson span that is also a Cap-
sOrg span?), CapsPerson and CapsOrg are two very
similar regular expressions identifying capitalized
phrases that look like person, and respectively, orga-
nization names, with small variations (e.g., the for-
mer may allow special characters such as ?-?). In-
cluding both BFs in a CD rule leads to a larger rule
that is unintuitive for a developer. The former bias
excludes such CD rules from consideration.
The latter type of bias prevents CD rules of one
type to appear as positive clues for a CR rule of
a different type. For instance, without this bias,
one of the CR rules obtained was Per ? (OrgCD
= m) AND (LocCD != o) (?If a span matches OrgCD
and does not overlap with LocCD, then that span
denotes a PER named entity?. Here OrgCD was
{[CapsOrg][CapsOrg]} and LocCD was {[CapsLoc
? CitiesDict]}. The inclusion of an Organization
CD rule as a positive clue for a Person CR rule is
unintuitive for a developer.
Table 4, shows the effect (for E2 and E3) on the
test dataset of disabling and enabling bias during
the induction of CR rules using JRIP. Adding bias
improves the precision of the induced rules. With-
out bias, however, the system is less constrained in
its search for high recall rules, leading to slightly
higher overall F measure. This comes at the cost
of an increase in extractor complexity and average
rule length. For example, for E2, the average rule
length decreases from 2.17 to 1.78 after adding the
bias. Overall, our results show that biases lead to
less complex extractors with only a very minor ef-
fect on accuracy, thus biases are important factors
contributing to inducing rules that are understand-
able and may be refined by humans.
Comparison with other induction systems. We
also experimented with two other induction systems,
Aleph9 and ALP10, a package that implements one
of the reportedly good information extraction algo-
rithms (Ciravegna, 2001). While induction in Aleph
9A system for inductive logic programming. See
http://www.cs.ox.ac.uk/activities/machlearn/Aleph/aleph.html
10http://code.google.com/p/alpie/
was performed with the same target language as in
our approach, the target language of ALP is JAPE,
which has been shown (Chiticariu et al2010b) to
lack in some of the constructs (such as minus) that
AQL provides and which form a part of our tar-
get language (especially the rule refinement phase).
However, despite experimenting with all possible
parameter configurations for each of these (in each
of E1, E2 and E3 settings), the accuracies obtained
were substantially (30-50%) worse and the extrac-
tor complexity was much (around 60%) higher when
compared to our system (with or without bias). Ad-
ditionally, Aleph takes close to three days for induc-
tion, whereas both ALP and our system require less
than an hour.
6.2 Discussion
Weak and Strong CDs reflected in CRs. In
our experiments, we found that varying the pre-
cision and complexity thresholds while inducing
the CDs (c.f Section 5) affected the F1 of the fi-
nal extractor only minimally. But reducing the
precision threshold generally improved the preci-
sion of the final extractor, which seemed counter-
intuitive at first. We found that CR rules learned
by JRIP consist of a strong CD rule (high preci-
sion, typically involving a dictionary) and a weak
CD rule (low precision, typically involving only
regular expressions). The strong CD rule always
corresponded to a positive clue (match) and the
weak CD rule corresponded to the negative clue
(overlaps or not-matches). This is illustrated in
the following CR rule: PER ? (PerCD = m) AND
(OrgCD != o) where (PerCD is {[CapsPersonR]
[CapsPersonR ? LastNameDict]} and (OrgCD is
{[CapsOrgR][CapsOrgR][CapsOrgR]}. This is
posited to be the way the CR rule learner operates
? it tries to learn conjunctions of weak and strong
clues so as to filter one from the other. Therefore,
setting a precision threshold too high limited the
number of such weak clues and the ability of the CR
rule learner to find such rules.
Interpretability. Measuring interpretability of rules
is a difficult problem. In this work, we have taken
a first step towards measuring interpretability using
a coarse grain measure in the form of a simple no-
tion of complexity score. The complexity is very
helpful in comparing alternative rule sets based on
136
Chiticariu et al010b Induced (With Bias) Induced (Without Bias)
P R F C(E) P R F C(E) P R F C(E)
Generic (E2) PER 82.2 60.3 69.5 945 87.5 49.9 63.5 233 85.8 53.7 66.0 476
ORG 75.7 17.5 28.5 1015 85.8 5.9 11.0 99 74.1 15.7 25.9 327
LOC 72.2 86.1 78.6 921 88.6 59.1 70.9 257 85.9 61.5 71.7 303
Overall 75.9 54.6 63.5 1015 88.0 38.2 53.3 457 84.2 43.5 57.4 907
Customised (E3) PER 96.3 92.2 94.2 2154 91.7 56.0 69.5 430 90.7 60.3 72.4 359
ORG 91.1 85.1 88.0 2154 86.9 47.5 61.4 348 90.4 46.8 61.7 397
LOC 93.3 91.7 92.5 2154 84.3 67.3 74.8 356 83.9 69.1 75.8 486
Overall 93.5 89.6 91.5 2160 87.3 57.0 68.9 844 87.8 58.7 70.4 901
Table 4: Comparison of induced rules (with and without bias) and manually developed rules. (CoNLL03 test dataset)
the number of rules, and the size of each rule, but
exhibits a number of shortcomings described next.
First, it disregards other components of a rule be-
sides its from clause, for example, the number of
items in the select clause, or the where clause. Sec-
ond, rule developers use semantically meaningful
view names such as those shown in Figure 1 to help
them recall the semantics of a rule at a high-level, an
aspect that is not captured by the complexity mea-
sure. Automatic generation of meaningful names
for induced views is an interesting direction for fu-
ture work. Finally, the overall structure of an extrac-
tors is not considered. In simple terms, an extrac-
tor consisting of 5 rules of size 1 is indistinguish-
able from an extractor consisting of a single rule
of size 5, and it is arguable which of these extrac-
tors is more interpretable. More generally, the ex-
tent of this shortcoming is best explained using an
example. When informally examining the rules in-
duced by our system, we found that CD rules are
similar in spirit to those written by rule develop-
ers. On the other hand, the induced CR rules are
too fine-grained. In general, rule developers group
CD rules with similar semantics, then write refine-
ment rules at the higher level of the group, as op-
posed to the lower level of individual CD views. For
example, one may write multiple CD rules for can-
didate person names of the form ?First??Last?, and
multiple CD rules of the form ?Last?, ?First?. One
would then union together the candidates from each
of the two groups into two different views, e.g., Per-
FirstLast and PerLastCommaFirst, and write filter
rules at the higher level of these two views, e.g.,
?Remove PerLastCommaFirst spans that overlap with a
PerFirstLast span?. In contrast, our induction algo-
rithm considers CR rules consisting of combinations
of CD rules directly, leading to many semantically
similar CR rules, each operating over small parts of
a larger semantic group (see rule in Section 6.1).
This results in repetition, and qualitatively less in-
terpretable rules, since humans prefer higher levels
of abstraction and generalization. This nuance is not
captured by the complexity score which may deem
an extractor consisting of many rules, where many
of the rules operate at higher levels of groups of can-
didates to be more complex than a smaller extrac-
tor with many fine-grained rules. Indeed, as shown
before, the complexity of the induced extractors is
much smaller compared to that of manual extrac-
tors, although the latter follow the semantic group-
ing principle and are considered more interpretable.
7 Conclusion
We presented a system for efficiently inducing
named entity annotation rules in the AQL language.
The design of our approach is aimed at producing
accurate rules that can be understood and refined
by humans, by placing special emphasis on low
complexity and efficient computation of the induced
rules, while mimicking a four stage approach used
for manually constructing rules. The induced rules
have good accuracy and low complexity according
to our complexity measure. While our complexity
measure informs the biases in our system and leads
to simpler, smaller extractors, it captures extrac-
tor interpretability only to a certain extent. There-
fore, we believe more work is required to devise a
more comprehensive quantitative measure for inter-
pretability, and refine our techniques in order to in-
crease the interpretability of induced rules. Other
interesting directions for future work are introduc-
ing more constructs in our framework, and applying
our techniques to other languages.
137
References
S. Abiteboul, R. Hull, and V. Vianu. 1995. Foundations
of Databases. Addison Wesley Publishing Co.
Douglas E. Appelt and Boyan Onyshkevych. 1998. The
common pattern specification language. In TIPSTER
workshop.
Mary Elaine Califf and Raymond J. Mooney. 1997. Ap-
plying ilp-based techniques to natural language infor-
mation extraction: An experiment in relational learn-
ing. In IJCAI Workshop on Frontiers of Inductive
Logic Programming.
Mary Elaine Califf and Raymond J. Mooney. 1999. Re-
lational learning of pattern-match rules for information
extraction. In AAAI.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li,
Sriram Raghavan, Frederick R. Reiss, and Shivaku-
mar Vaithyanathan. 2010a. Systemt: an algebraic ap-
proach to declarative information extraction. In ACL.
Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao
Li, Frederick Reiss, and Shivakumar Vaithyanathan.
2010b. Domain adaptation of rule-based annotators
for named-entity recognition tasks. In EMNLP.
Fabio Ciravegna. 2001. (lp)2, an adaptive algorithm for
information extraction from web-related texts. In In
Proceedings of the IJCAI-2001 Workshop on Adaptive
Text Extraction and Mining.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Niraj Aswani, Ian
Roberts, Genevieve Gorrell, Adam Funk, An-
gus Roberts, Danica Damljanovic, Thomas Heitz,
Mark A. Greenwood, Horacio Saggion, Johann
Petrak, Yaoyong Li, and Wim Peters. 2011. Text
Processing with GATE (Version 6).
J. Fu?rnkranz and G. Widmer. 1994. Incremental reduced
error pruning. pages 70?77.
Johannes Fu?rnkranz. 1999. Separate-and-conquer rule
learning. Artif. Intell. Rev., 13(1):3?54, February.
B. R. Gaines and P. Compton. 1995. Induction of ripple-
down rules applied to modeling large databases. J. In-
tell. Inf. Syst., 5:211?228, November.
IBM, 2012. IBM InfoSphere BigInsights - An-
notation Query Language (AQL) reference.
http://publib.boulder.ibm.com/
infocenter/bigins/v1r3/topic/com.
ibm.swg.im.infosphere.biginsights.
doc/doc/biginsights_aqlref_con_
aql-overview.html.
Yunyao Li, Rajasekar Krishnamurthy, Sriram Raghavan,
Shivakumar Vaithyanathan, and H. V. Jagadish. 2008.
Regular expression learning for information extrac-
tion. In EMNLP.
Bin Liu, Laura Chiticariu, Vivian Chu, H. V. Jagadish,
and Frederick R. Reiss. 2010. Automatic rule refine-
ment for information extraction. Proc. VLDB Endow.,
3:588?597.
Diana Maynard, Kalina Bontcheva, and Hamish Cun-
ningham. 2003. Towards a semantic extraction of
named entities. In In Recent Advances in Natural Lan-
guage Processing.
Stephen Muggleton and C. Feng. 1992. Efficient induc-
tion in logic programs. In ILP.
D. Nadeau and S. Sekine. 2007. A survey of named
entity recognition and classification. Linguisticae In-
vestigationes, 30:3?26.
Shan-Hwei Nienhuys-Cheng and Ronald de Wolf. 1997.
Foundations of Inductive Logic Programming.
Anup Patel, Ganesh Ramakrishnan, and Pushpak Bhat-
tacharyya. 2009. Incorporating linguistic expertise
using ilp for named entity recognition in data hungry
indian languages. In ILP.
Frederick Reiss, Sriram Raghavan, Rajasekar Krishna-
murthy, Huaiyu Zhu, and Shivakumar Vaithyanathan.
2008. An algebraic approach to rule-based informa-
tion extraction. In ICDE.
Ellen Riloff. 1993. Automatically constructing a dictio-
nary for information extraction tasks. In AAAI.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Mach.
Learn., 34:233?272.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the conll-2003 shared task: language-
independent named entity recognition. In HLT-
NAACL.
Ian H.Witten, Eibe Frank, andMark A. Hall. 2011. Data
Mining: Practical Machine Learning Tools and Tech-
niques. Morgan Kaufmann, Amsterdam, 3rd edition.
138
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 128?137,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
SystemT: An Algebraic Approach to Declarative Information Extraction
Laura Chiticariu Rajasekar Krishnamurthy Yunyao Li
Sriram Raghavan Frederick R. Reiss Shivakumar Vaithyanathan
IBM Research ? Almaden
San Jose, CA, USA
{chiti,sekar,yunyaoli,rsriram,frreiss,vaithyan}@us.ibm.com
Abstract
As information extraction (IE) becomes
more central to enterprise applications,
rule-based IE engines have become in-
creasingly important. In this paper, we
describe SystemT, a rule-based IE sys-
tem whose basic design removes the ex-
pressivity and performance limitations of
current systems based on cascading gram-
mars. SystemT uses a declarative rule
language, AQL, and an optimizer that
generates high-performance algebraic ex-
ecution plans for AQL rules. We com-
pare SystemT?s approach against cascad-
ing grammars, both theoretically and with
a thorough experimental evaluation. Our
results show that SystemT can deliver re-
sult quality comparable to the state-of-the-
art and an order of magnitude higher an-
notation throughput.
1 Introduction
In recent years, enterprises have seen the emer-
gence of important text analytics applications like
compliance and data redaction. This increase,
combined with the inclusion of text into traditional
applications like Business Intelligence, has dra-
matically increased the use of information extrac-
tion (IE) within the enterprise. While the tradi-
tional requirement of extraction quality remains
critical, enterprise applications also demand ef-
ficiency, transparency, customizability and main-
tainability. In recent years, these systemic require-
ments have led to renewed interest in rule-based
IE systems (Doan et al, 2008; SAP, 2010; IBM,
2010; SAS, 2010).
Until recently, rule-based IE systems (Cunning-
ham et al, 2000; Boguraev, 2003; Drozdzynski
et al, 2004) were predominantly based on the
cascading grammar formalism exemplified by the
Common Pattern Specification Language (CPSL)
specification (Appelt and Onyshkevych, 1998). In
CPSL, the input text is viewed as a sequence of an-
notations, and extraction rules are written as pat-
tern/action rules over the lexical features of these
annotations. In a single phase of the grammar, a
set of rules are evaluated in a left-to-right fash-
ion over the input annotations. Multiple grammar
phases are cascaded together, with the evaluation
proceeding in a bottom-up fashion.
As demonstrated by prior work (Grishman and
Sundheim, 1996), grammar-based IE systems can
be effective in many scenarios. However, these
systems suffer from two severe drawbacks. First,
the expressivity of CPSL falls short when used
for complex IE tasks over increasingly pervasive
informal text (emails, blogs, discussion forums
etc.). To address this limitation, grammar-based
IE systems resort to significant amounts of user-
defined code in the rules, combined with pre-
and post-processing stages beyond the scope of
CPSL (Cunningham et al, 2010). Second, the
rigid evaluation order imposed in these systems
has significant performance implications.
Three decades ago, the database community
faced similar expressivity and efficiency chal-
lenges in accessing structured information. The
community addressed these problems by introduc-
ing a relational algebra formalism and an associ-
ated declarative query language SQL. The ground-
breaking work on System R (Chamberlin et al,
1981) demonstrated how the expressivity of SQL
can be efficiently realized in practice by means of
a query optimizer that translates an SQL query into
an optimized query execution plan.
Borrowing ideas from the database community,
we have developed SystemT, a declarative IE sys-
tem based on an algebraic framework, to address
both expressivity and performance issues. In Sys-
temT, extraction rules are expressed in a declar-
ative language called AQL. At compilation time,
128
({First} {Last} ) :full :full.Person
({Caps}  {Last} ) :full :full.Person
({Last} {Token.orth = comma} {Caps | First}) : reverse
:reverse.Person
({First}) : fn  :fn.Person
({Last}) : ln  :ln.Person
({Lookup.majorType = FirstGaz})  : fn  :fn.First
({Lookup.majorType = LastGaz}) : ln  :ln.Last
({Token.orth = upperInitial} | 
{Token.orth = mixedCaps } ) :cw  :cw.Caps
Rule Patterns
50
20
10
10
10
50
50
10
Priority
P2R1
P2R2
P2R3
P2R4
P2R5
P1R1
P1R2
P1R3
RuleId
Input
First
Last
Caps
Token
Output
Person
Input
Lookup
Token
Output
First
Last
Caps
TypesPhase
P2
P1
P2R3        ({Last} {Token.orth = comma} {Caps | First}) : reverse   :reverse.Person
Last followed by Token whose orth attribute has value 
comma followed by Caps or First
Rule part Action part
Create Person
annotation
Bind match 
to variables
Syntax:
Gazetteers containing first names and last names
Figure 1: Cascading grammar for identifying Person names
SystemT translates AQL statements into an al-
gebraic expression called an operator graph that
implements the semantics of the statements. The
SystemT optimizer then picks a fast execution
plan from many logically equivalent plans. Sys-
temT is currently deployed in a multitude of real-
world applications and commercial products1.
We formally demonstrate the superiority of
AQL and SystemT in terms of both expressivity
and efficiency (Section 4). Specifically, we show
that 1) the expressivity of AQL is a strict superset
of CPSL grammars not using external functions
and 2) the search space explored by the SystemT
optimizer includes operator graphs correspond-
ing to efficient finite state transducer implemen-
tations. Finally, we present an extensive experi-
mental evaluation that validates that high-quality
annotators can be developed with SystemT, and
that their runtime performance is an order of mag-
nitude better when compared to annotators devel-
oped with a state-of-the-art grammar-based IE sys-
tem (Section 5).
2 Grammar-based Systems and CPSL
A cascading grammar consists of a sequence of
phases, each of which consists of one or more
rules. Each phase applies its rules from left to
right over an input sequence of annotations and
generates an output sequence of annotations that
the next phase consumes. Most cascading gram-
mar systems today adhere to the CPSL standard.
Fig. 1 shows a sample CPSL grammar that iden-
tifies person names from text in two phases. The
first phase, P1, operates over the results of the tok-
1A trial version is available at
http://www.alphaworks.ibm.com/tech/systemt
Rule skipped 
due to priority 
semantics
CPSL
Phase P1
Last(P1R2) Last(P1R2)
? Mark        Scott     ,            Howard         Smith   ?
First(P1R1) First(P1R1) First(P1R1) Last(P1R2)
CPSL
Phase P2
? Mark        Scott     ,        Howard          Smith    ?
Person(P2R1)
Person (P2R4)
Person(P2R4)
Person (P2R5)
Person(P2R4)
? Mark        Scott     ,            Howard         Smith   ?
First(P1R1) First(P1R1) First(P1R1) Last(P1R2)
JAPE
Phase P1
(Brill) Caps(P1R3) Last(P1R2) Last(P1R2)
Caps(P1R3) Caps(P1R3)
Caps(P1R3)
? Mark        Scott     ,        Howard          Smith    ?
Person(P2R1)
Person (P2R4, P2R5)
JAPE
Phase P2
(Appelt)
Person(P2R1)
Person (P2R2)
Some discarded 
matches omitted
for clarity
? Tomorrow, we will meet Mark Scott, Howard Smith and ?Document d1
Rule fired
Legend
3 persons
identified
2 persons
identified
(a)
(b)
Figure 2: Sample output of CPSL and JAPE
enizer and gazetteer (input types Token and Lookup,
respectively) to identify words that may be part of
a person name. The second phase, P2, identifies
complete names using the results of phase P1.
Applying the above grammar to document d1
(Fig. 2), one would expect that to match ?Mark
Scott? and ?Howard Smith? as Person. However,
as shown in Fig. 2(a), the grammar actually finds
three Person annotations, instead of two. CPSL has
several limitations that lead to such discrepancies:
L1. Lossy sequencing. In a CPSL grammar,
each phase operates on a sequence of annotations
from left to right. If the input annotations to a
phase may overlap with each other, the CPSL en-
gine must drop some of them to create a non-
overlapping sequence. For instance, in phase P1
(Fig. 2(a)), ?Scott? has both a Lookup and a To-
ken annotation. The system has made an arbitrary
choice to retain the Lookup annotation and discard
the Token annotation. Consequently, no Caps anno-
tations are output by phase P1.
L2. Rigid matching priority. CPSL specifies
that, for each input annotation, only one rule can
actually match. When multiple rules match at the
same start position, the following tie-breaker con-
ditions are applied (in order): (a) the rule match-
ing the most annotations in the input stream; (b)
the rule with highest priority; and (c) the rule de-
clared earlier in the grammar. This rigid match-
ing priority can lead to mistakes. For instance,
as illustrated in Fig. 2(a), phase P1 only identi-
fies ?Scott? as a First. Matching priority causes
the grammar to skip the corresponding match for
?Scott? as a Last. Consequently, phase P2 fails to
identify ?Mark Scott? as one single Person.
L3. Limited expressivity in rule patterns. It is
not possible to express rules that compare annota-
tions overlapping with each other. E.g., ?Identify
129
[A-Z]{\w|-}+
DocumentInput Tuple
?
we will meet Mark 
Scott, ?
Output Tuple 2 Span 2Document
Span 1Output Tuple 1 Document
Regex
Caps
Figure 3: Regular Expression Extraction Operator
words that are both capitalized and present in the
FirstGaz gazetteer? or ?Identify Person annotations
that occur within an EmailAddress?.
Extensions to CPSL
In order to address the above limitations, several
extensions to CPSL have been proposed in JAPE,
AFst and XTDL (Cunningham et al, 2000; Bogu-
raev, 2003; Drozdzynski et al, 2004). The exten-
sions are summarized as below, where each solu-
tion Si corresponds to limitation Li.
? S1. Grammar rules are allowed to operate on
graphs of input annotations in JAPE and AFst.
? S2. JAPE introduces more matching regimes
besides the CPSL?s matching priority and thus
allows more flexibility when multiple rules
match at the same starting position.
? S3. The rule part of a pattern has been ex-
panded to allow more expressivity in JAPE,
AFst and XTDL.
Fig. 2(b) illustrates how the above extensions
help in identifying the correct matches ?Mark Scott?
and ?Howard Smith? in JAPE. Phase P1 uses a match-
ing regime (denoted by Brill) that allows multiple
rules to match at the same starting position, and
phase P2 uses CPSL?s matching priority, Appelt.
3 SystemT
SystemT is a declarative IE system based on an
algebraic framework. In SystemT, developers
write rules in a language called AQL. The system
then generates a graph of operators that imple-
ment the semantics of the AQL rules. This decou-
pling allows for greater rule expressivity, because
the rule language is not constrained by the need to
compile to a finite state transducer. Likewise, the
decoupled approach leads to greater flexibility in
choosing an efficient execution strategy, because
many possible operator graphs may exist for the
same AQL annotator.
In the rest of the section, we describe the parts
of SystemT, starting with the algebraic formalism
behind SystemT?s operators.
3.1 Algebraic Foundation of SystemT
SystemT executes IE rules using graphs of op-
erators. The formal definition of these operators
takes the form of an algebra that is similar to the
relational algebra, but with extensions for text pro-
cessing.
The algebra operates over a simple relational
data model with three data types: span, tuple, and
relation. In this data model, a span is a region of
text within a document identified by its ?begin?
and ?end? positions; a tuple is a fixed-size list of
spans. A relation is a multiset of tuples, where ev-
ery tuple in the relation must be of the same size.
Each operator in our algebra implements a single
basic atomic IE operation, producing and consum-
ing sets of tuples.
Fig. 3 illustrates the regular expression ex-
traction operator in the algebra, which per-
forms character-level regular expression match-
ing. Overall, the algebra contains 12 different op-
erators, a full description of which can be found
in (Reiss et al, 2008). The following four oper-
ators are necessary to understand the examples in
this paper:
? The Extract operator (E) performs character-
level operations such as regular expression and
dictionary matching over text, creating a tuple
for each match.
? The Select operator (?) takes as input a set of
tuples and a predicate to apply to the tuples. It
outputs all tuples that satisfy the predicate.
? The Join operator (??) takes as input two sets
of tuples and a predicate to apply to pairs of
tuples from the input sets. It outputs all pairs
of input tuples that satisfy the predicate.
? The consolidate operator (?) takes as input a
set of tuples and the index of a particular col-
umn in those tuples. It removes selected over-
lapping spans from the indicated column, ac-
cording to the specified policy.
3.2 AQL
Extraction rules in SystemT are written in AQL,
a declarative relational language similar in syn-
tax to the database language SQL. We chose SQL
as a basis for our language due to its expres-
sivity and its familiarity. The expressivity of
SQL, which consists of first-order logic predicates
130
Figure 4: Person annotator as AQL query
over sets of tuples, is well-documented and well-
understood (Codd, 1990). As SQL is the pri-
mary interface to most relational database sys-
tems, the language?s syntax and semantics are
common knowledge among enterprise application
programmers. Similar to SQL terminology, we
call a collection of AQL rules an AQL query.
Fig. 4 shows portions of an AQL query. As
can be seen, the basic building block of AQL is
a view: A logical description of a set of tuples in
terms of either the document text (denoted by a
special view called Document) or the contents of
other views. Every SystemT annotator consists
of at least one view. The output view statement in-
dicates that the tuples in a view are part of the final
results of the annotator.
Fig. 4 also illustrates three of the basic con-
structs that can be used to define a view.
? The extract statement specifies basic
character-level extraction primitives to be
applied directly to a tuple.
? The select statement is similar to the SQL
select statement but it contains an additional
consolidate on clause, along with an exten-
sive collection of text-specific predicates.
? The union all statement merges the outputs
of one or more select or extract statements.
To keep rules compact, AQL also provides a
shorthand sequence pattern notation similar to the
syntax of CPSL. For example, the CapsLast
view in Figure 4 could have been written as:
create view CapsLast as
extract pattern <C.name> <L.name>
from Caps C, Last L;
Internally, SystemT translates each of these ex-
tract pattern statements into one or more select
and extract statements.
AQL SystemT
Optimizer
SystemT
Runtime
Compiled
Operator
Graph
Figure 5: The compilation process in SystemT
Figure 6: Execution strategies for the CapsLast rule
in Fig. 4
SystemT has built-in multilingual support in-
cluding tokenization, part of speech and gazetteer
matching for over 20 languages using Language-
Ware (IBM, 2010). Rule developers can utilize
the multilingual support via AQL without hav-
ing to configure or manage any additional re-
sources. In addition, AQL allows user-defined
functions to be used in a restricted context in or-
der to support operations such as validation (e.g.
for extracted credit card numbers), or normaliza-
tion (e.g., compute abbreviations of multi-token
organization candidates that are useful in gener-
ating additional candidates). More details on AQL
can be found in the AQL manual (SystemT, 2010).
3.3 Optimizer and Operator Graph
Grammar-based IE engines place rigid restrictions
on the order in which rules can be executed. Due
to the semantics of the CPSL standard, systems
that implement the standard must use a finite state
transducer that evaluates each level of the cascade
with one or more left to right passes over the entire
token stream.
In contrast, SystemT places no explicit con-
straints on the order of rule evaluation, nor does
it require that intermediate results of an annota-
tor collapse to a fixed-size sequence. As shown in
Fig. 5, the SystemT engine does not execute AQL
directly; instead, the SystemT optimizer compiles
AQL into a graph of operators. By tying a collec-
tion of operators together by their inputs and out-
puts, the system can implement a wide variety of
different execution strategies. Different execution
strategies are associated with different evaluation
costs. The optimizer chooses the execution strat-
egy with the lowest estimated evaluation cost.
131
Fig. 6 presents three possible execution strate-
gies for the CapsLast rule in Fig. 4. If the opti-
mizer estimates that the evaluation cost of Last is
much lower than that of Caps, then it can deter-
mine that Plan C has the lowest evaluation cost
among the three, because Plan C only evaluates
Caps in the ?left? neighborhood for each instance
of Last. More details of our algorithms for enumer-
ating plans can be found in (Reiss et al, 2008).
The optimizer in SystemT chooses the best ex-
ecution plan from a large number of different al-
gebra graphs available to it. Many of these graphs
implement strategies that a transducer could not
express: such as evaluating rules from right to left,
sharing work across different rules, or selectively
skipping rule evaluations. Within this large search
space, there generally exists an execution strategy
that implements the rule semantics far more effi-
ciently than the fastest transducer could. We refer
the reader to (Reiss et al, 2008) for a detailed de-
scription of the types of plan the optimizer consid-
ers, as well as an experimental analysis of the per-
formance benefits of different parts of this search
space.
Several parallel efforts have been made recently
to improve the efficiency of IE tasks by optimiz-
ing low-level feature extraction (Ramakrishnan et
al., 2006; Ramakrishnan et al, 2008; Chandel et
al., 2006) or by reordering operations at a macro-
scopic level (Ipeirotis et al, 2006; Shen et al,
2007; Jain et al, 2009). However, to the best of
our knowledge, SystemT is the only IE system
in which the optimizer generates a full end-to-end
plan, beginning with low-level extraction primi-
tives and ending with the final output tuples.
3.4 Deployment Scenarios
SystemT is designed to be usable in various de-
ployment scenarios. It can be used as a stand-
alone system with its own development and run-
time environment. Furthermore, SystemT ex-
poses a generic Java API that enables the integra-
tion of its runtime environment with other applica-
tions. For example, a specific instantiation of this
API allows SystemT annotators to be seamlessly
embedded in applications using the UIMA analyt-
ics framework (UIMA, 2010).
4 Grammar vs. Algebra
Having described both the traditional cascading
grammar approach and the declarative approach
Figure 7: Supporting Complex Rule Interactions
used in SystemT, we now compare the two in
terms of expressivity and performance.
4.1 Expressivity
In Section 2, we described three expressivity lim-
itations of CPSL grammars: Lossy sequencing,
rigid matching priority, and limited expressivity in
rule patterns. As we noted, cascading grammar
systems extend the CPSL specification in various
ways to provide workarounds for these limitations.
In SystemT, the basic design of the AQL lan-
guage eliminates these three problems without the
need for any special workaround. The key design
difference is that AQL views operate over sets of
tuples, not sequences of tokens. The input or out-
put tuples of a view can contain spans that overlap
in arbitrary ways, so the lossy sequencing prob-
lem never occurs. The annotator will retain these
overlapping spans across any number of views un-
til a view definition explicitly removes the over-
lap. Likewise, the tuples that a given view pro-
duces are in no way constrained by the outputs of
other, unrelated views, so the rigid matching prior-
ity problem never occurs. Finally, the select state-
ment in AQL allows arbitrary predicates over the
cross-product of its input tuple sets, eliminating
the limited expressivity in rule patterns problem.
Beyond eliminating the major limitations of
CPSL grammars, AQL provides a number of other
information extraction operations that even ex-
tended CPSL cannot express without custom code.
Complex rule interactions. Consider an exam-
ple document from the Enron corpus (Minkov et
al., 2005), shown in Fig. 7, which contains a list
of person names. Because the first person in the
list (?Skilling?) is referred to by only a last name,
rule P2R3 in Fig. 1 incorrectly identifies ?Skilling,
Cindy? as a person. Consequently, the output of
phase P2 of the cascading grammar contains sev-
eral mistakes as shown in the figure. This problem
132
went to the Switchfoot concert at the Roxy. It was pretty fun,? The lead singer/guitarist 
was really good, and even though there was another guitarist  (an Asian guy), he ended up 
playing most of the guitar parts, which was really impressive. The biggest surprise though is 
that I actually liked the opening bands. ?I especially liked the first band
Consecutive review snippets are within 25 tokens
At least 4 occurrences of MusicReviewSnippet or GenericReviewSnippet
At least 3 of them should be MusicReviewSnippets
Review ends with one of these.
Start with 
ConcertMention
Complete review is
within 200 tokens
ConcertMention
MusicReviewSnippet
GenericReviewSnippet
Example Rule
Informal Band Review
Figure 8: Extracting informal band reviews from web logs
occurs because CPSL only evaluates rules over
the input sequence in a strict left-to-right fashion.
On the other hand, the AQL query Q1 shown in
the figure applies the following condition: ?Al-
ways discard matches to Rule P2R3 if they overlap
with matches to rules P2R1 or P2R2? (even if the
match to Rule P2R3 starts earlier). Applying this
rule ensures that the person names in the list are
identified correctly. Obtaining the same effect in
grammar-based systems would require the use of
custom code (as recommended by (Cunningham
et al, 2010)).
Counting and Aggregation. Complex extraction
tasks sometimes require operations such as count-
ing and aggregation that go beyond the expressiv-
ity of regular languages, and thus can be expressed
in CPSL only using external functions. One such
task is that of identifying informal concert reviews
embedded within blog entries. Fig. 8 describes, by
example, how these reviews consist of reference
to a live concert followed by several review snip-
pets, some specific to musical performances and
others that are more general review expressions.
An example rule to identify informal reviews is
also shown in the figure. Notice how implement-
ing this rule requires counting the number of Mu-
sicReviewSnippet and GenericReviewSnippet annotations
within a region of text and aggregating this occur-
rence count across the two review types. While
this rule can be written in AQL, it can only be ap-
proximated in CPSL grammars.
Character-Level Regular Expression CPSL
cannot specify character-level regular expressions
that span multiple tokens. In contrast, the extract
regex statement in AQL fully supports these ex-
pressions.
We have described above several cases where
AQL can express concepts that can only be ex-
pressed through external functions in a cascad-
ing grammar. These examples naturally raise the
question of whether similar cases exist where a
cascading grammar can express patterns that can-
not be expressed in AQL.
It turns out that we can make a strong statement
that such examples do not exist. In the absence
of an escape to arbitrary procedural code, AQL is
strictly more expressive than a CPSL grammar. To
state this relationship formally, we first introduce
the following definitions.
We refer to a grammar conforming to the CPSL
specification as a CPSL grammar. When a CPSL
grammar contains no external functions, we refer
to it as a Code-free CPSL grammar. Finally, we
refer to a grammar that conforms to one of the
CPSL, JAPE, AFst and XTDL specifications as an
expanded CPSL grammar.
Ambiguous Grammar Specification An ex-
panded CPSL grammar may be under-specified in
some cases. For example, a single rule contain-
ing the disjunction operator (|) may match a given
region of text in multiple ways. Consider the eval-
uation of Rule P2R3 over the text fragment ?Scott,
Howard? from document d1 (Fig. 1). If ?Howard?
is identified both as Caps and First, then there are
two evaluations for Rule P2R3 over this text frag-
ment. Since the system has to arbitrarily choose
one evaluation, the results of the grammar can be
non-deterministic (as pointed out in (Cunning-
ham et al, 2010)). We refer to a grammar G as
an ambiguous grammar specification for a docu-
ment collection D if the system makes an arbitrary
choice while evaluating G over D.
Definition 1 (UnambigEquiv) A query Q is Un-
ambigEquiv to a cascading grammar G if and only
if for every document collection D, where G is not
an ambiguous grammar specification for D, the
results of the grammar invocation and the query
evaluation are identical.
We now formally compare the expressivity of
AQL and expanded CPSL grammars. The detailed
proof is omitted due to space limitations.
Theorem 1 The class of extraction tasks express-
ible as AQL queries is a strict superset of that ex-
pressible through expanded code-free CPSL gram-
mars. Specifically,
(a) Every expanded code-free CPSL grammar can
be expressed as an UnambigEquiv AQL query.
(b) AQL supports information extraction opera-
tions that cannot be expressed in expanded code-
free CPSL grammars.
133
Proof Outline: (a) A single CPSL grammar can
be expressed in AQL as follows. First, each rule
r in the grammar is translated into a set of AQL
statements. If r does not contain the disjunct (|)
operator, then it is translated into a single AQL
select statement. Otherwise, a set of AQL state-
ments are generated, one for each disjunct opera-
tor in rule r, and the results merged using union
all statements. Then, a union all statement is used
to combine the results of individual rules in the
grammar phase. Finally, the AQL statements for
multiple phases are combined in the same order as
the cascading grammar specification.
The main extensions to CPSL supported by ex-
panded CPSL grammars (listed in Sec. 2) are han-
dled as follows. AQL queries operate on graphs
on annotations just like expanded CPSL gram-
mars. In addition, AQL supports different match-
ing regimes through consolidation operators, span
predicates through selection predicates and co-
references through join operators.
(b) Example operations supported in AQL that
cannot be expressed in expanded code-free CPSL
grammars include (i) character-level regular ex-
pressions spanning multiple tokens, (ii) count-
ing the number of annotations occurring within a
given bounded window and (iii) deleting annota-
tions if they overlap with other annotations start-
ing later in the document. 2
4.2 Performance
For the annotators we test in our experiments
(See Section 5), the SystemT optimizer is able to
choose algebraic plans that are faster than a com-
parable transducer-based implementation. The
question arises as to whether there are other an-
notators for which the traditional transducer ap-
proach is superior. That is, for a given annota-
tor, might there exist a finite state transducer that
is combinatorially faster than any possible algebra
graph? It turns out that this scenario is not possi-
ble, as the theorem below shows.
Definition 2 (Token-Based FST) A token-based
finite state transducer (FST) is a nondeterministic
finite state machine in which state transitions are
triggered by predicates on tokens. A token-based
FST is acyclic if its state graph does not contain
any cycles and has exactly one ?accept? state.
Definition 3 (Thompson?s Algorithm)
Thompson?s algorithm is a common strategy
for evaluating a token-based FST (based on
(Thompson, 1968)). This algorithm processes the
input tokens from left to right, keeping track of the
set of states that are currently active.
Theorem 2 For any acyclic token-based finite
state transducer T , there exists an UnambigEquiv
operator graph G, such that evaluating G has the
same computational complexity as evaluating T
with Thompson?s algorithm starting from each to-
ken position in the input document.
Proof Outline: The proof constructs G by struc-
tural induction over the transducer T . The base
case converts transitions out of the start state into
Extract operators. The inductive case adds a Se-
lect operator to G for each of the remaining state
transitions, with each selection predicate being the
same as the predicate that drives the corresponding
state transition. For each state transition predicate
that T would evaluate when processing a given
document, G performs a constant amount of work
on a single tuple. 2
5 Experimental Evaluation
In this section we present an extensive comparison
study between SystemT and implementations of
expanded CPSL grammar in terms of quality, run-
time performance and resource requirements.
TasksWe chose two tasks for our evaluation:
? NER : named-entity recognition for Person,
Organization, Location, Address, PhoneNumber,
EmailAddress, URL and DateTime.
? BandReview : identify informal reviews in
blogs (Fig. 8).
We chose NER primarily because named-entity
recognition is a well-studied problem and standard
datasets are available for evaluation. For this task
we use GATE and ANNIE for comparison3. We
chose BandReview to conduct performance evalu-
ation for a more complex extraction task.
Datasets. For quality evaluation, we use:
? EnronMeetings (Minkov et al, 2005): collec-
tion of emails with meeting information from
the Enron corpus4 with Person labeled data;
? ACE (NIST, 2005): collection of newswire re-
ports and broadcast news/conversations with
Person, Organization, Location labeled data5.
3To the best of our knowledge, ANNIE (Cunningham et
al., 2002) is the only publicly available NER library imple-
mented in a grammar-based system (JAPE in GATE).
4http://www.cs.cmu.edu/ enron/
5Only entities of type NAM have been considered.
134
Table 1: Datasets for performance evaluation.
Dataset Description of the Content Number of Document size
documents range average
Enronx Emails randomly sampled from the Enron corpus of average size xKB (0.5 < x < 100)2 1000 xKB +/? 10% xKB
WebCrawl Small to medium size web pages representing company news, with HTML tags removed 1931 68b - 388.6KB 8.8KB
FinanceM Medium size financial regulatory filings 100 240KB - 0.9MB 401KB
FinanceL Large size financial regulatory filings 30 1MB - 3.4MB 1.54MB
Table 2: Quality of Person on test datasets.
Precision (%) Recall (%) F1 measure (%)
(Exact/Partial) (Exact/Partial) (Exact/Partial)
EnronMeetings
ANNIE 57.05/76.84 48.59/65.46 52.48/70.69
T-NE 88.41/92.99 82.39/86.65 85.29/89.71
Minkov 81.1/NA 74.9/NA 77.9/NA
ACE
ANNIE 39.41/78.15 30.39/60.27 34.32/68.06
T-NE 93.90/95.82 90.90/92.76 92.38/94.27
Table 1 lists the datasets used for performance
evaluation. The size of FinanceLis purposely
small because GATE takes a significant amount of
time processing large documents (see Sec. 5.2).
Set Up. The experiments were run on a server
with two 2.4 GHz 4-core Intel Xeon CPUs and
64GB of memory. We use GATE 5.1 (build 3431)
and two configurations for ANNIE: 1) the default
configuration, and 2) an optimized configuration
where the Ontotext Japec Transducer6 replaces the
default NE transducer for optimized performance.
We refer to these configurations as ANNIE and
ANNIE-Optimized, respectively.
5.1 Quality Evaluation
The goal of our quality evaluation is two-fold:
to validate that annotators can be built in Sys-
temT with quality comparable to those built in
a grammar-based system; and to ensure a fair
performance comparison between SystemT and
GATE by verifying that the annotators used in the
study are comparable.
Table 2 shows results of our comparison study
for Person annotators. We report the classical
(exact) precision, recall, and F1 measures that
credit only exact matches, and corresponding par-
tial measures that credit partial matches in a fash-
ion similar to (NIST, 2005). As can be seen, T-
NE produced results of significantly higher quality
than ANNIE on both datasets, for the same Person
extraction task. In fact, on EnronMeetings, the F1
measure of T-NE is 7.4% higher than the best pub-
lished result (Minkov et al, 2005). Similar results
6http://www.ontotext.com/gate/japec.html
a) Throughput on Enron
0
100
200
300
400
500
600
700
0 20 40 60 80 100
Average document size (KB)
Th
ro
u
gh
pu
t (
K
B
/s
ec
)
ANNIE
ANNIE-Optimized
T-NE
x
b) Memory Utilization on Enron
0
200
400
600
0 20 40 60 80 100
Average document size (KB)
A
v
g 
H
ea
p 
si
ze
 
(M
B
) ANNIE
ANNIE-Optimized
T-NE
Error bars show
25th and 75th
percentile 
x
Figure 9: Throughput (a) and memory consump-
tion (b) comparisons on Enronx datasets.
can be observed for Organization and Location on
ACE (exact numbers omitted in interest of space).
Clearly, considering the large gap between
ANNIE?s F1 and partial F1 measures on both
datasets, ANNIE?s quality can be improved via
dataset-specific tuning as demonstrated in (May-
nard et al, 2003). However, dataset-specific tun-
ing for ANNIE is beyond the scope of this paper.
Based on the experimental results above and our
previous formal comparison in Sec. 4, we believe
it is reasonable to conclude that annotators can be
built in SystemT of quality at least comparable to
those built in a grammar-based system.
5.2 Performance Evaluation
We now focus our attention on the throughput and
memory behavior of SystemT, and draw a com-
parison with GATE. For this purpose, we have con-
figured both ANNIE and T-NE to identify only the
same eight types of entities listed for NER task.
Throughput. Fig. 9(a) plots the throughput of
the two systems on multiple Enronx datasets with
average document sizes of between 0.5KB and
100KB. For this experiment, both systems ran
with a maximum Java heap size of 1GB.
135
Table 3: Throughput and mean heap size.
ANNIE ANNIE-Optimized T-NE
Dataset ThroughputMemoryThroughput Memory ThroughputMemory
(KB/s) (MB) (KB/s) (MB) (KB/s) (MB)
WebCrawl 23.9 212.6 42.8 201.8 498.9 77.2
FinanceM 18.82 715.1 26.3 601.8 703.5 143.7
FinanceL 19.2 2586.2 21.1 2683.5 954.5 189.6
As shown in Fig. 9(a), even though the through-
put of ANNIE-Optimized (using the optimized trans-
ducer) increases two-fold compared to ANNIE un-
der default configuration, T-NE is between 8 and
24 times faster compared to ANNIE-Optimized. For
both systems, throughput varied with document
size. For T-NE, the relatively low throughput on
very small document sizes (less than 1KB) is due
to fixed overhead in setting up operators to pro-
cess a document. As document size increases, the
overhead becomes less noticeable.
We have observed similar trends on the rest
of the test collections. Table 3 shows that T-
NE is at least an order of magnitude faster than
ANNIE-Optimized across all datasets. In partic-
ular, on FinanceL T-NE?s throughput remains
high, whereas the performance of both ANNIE and
ANNIE-Optimized degraded significantly.
To ascertain whether the difference in perfor-
mance in the two systems is due to low-level com-
ponents such as dictionary evaluation, we per-
formed detailed profiling of the systems. The pro-
filing revealed that 8.2%, 16.2% and respectively
14.2% of the execution time was spent on aver-
age on low-level components in the case of ANNIE,
ANNIE-Optimized and T-NE, respectively, thus lead-
ing us to conclude that the observed differences
are due to SystemT?s efficient use of resources at
a macroscopic level.
Memory utilization. In theory, grammar based
systems can stream tuples through each stage
for minimal memory consumption, whereas Sys-
temT operator graphs may need to materialize in-
termediate results for the full document at certain
points to evaluate the constraints in the original
AQL. The goal of this study is to evaluate whether
this potential problem does occur in practice.
In this experiment we ran both systems with a
maximum heap size of 2GB, and used the Java
garbage collector?s built-in telemetry to measure
the total quantity of live objects in the heap over
time while annotating the different test corpora.
Fig. 9(b) plots the minimum, maximum, and mean
heap sizes with the Enronx datasets. On small doc-
uments of size up to 15KB, memory consumption
is dominated by the fixed size of the data struc-
tures used (e.g., dictionaries, FST/operator graph),
and is comparable for both systems. As docu-
ments get larger, memory consumption increases
for both systems. However, the increase is much
smaller for T-NE compared to that for both AN-
NIE and ANNIE-Optimized. A similar trend can be
observed on the other datasets as shown in Ta-
ble 3. In particular, for FinanceL, both ANNIE and
ANNIE-Optimized required 8GB of Java heap size to
achieve reasonable throughput7 , in contrast to T-
NE which utilized at most 300MB out of the 2GB
of maximum Java heap size allocation.
SystemT requires much less memory than
GATE in general due to its runtime, which monitors
data dependencies between operators and clears
out low-level results when they are no longer
needed. Although a streaming CPSL implemen-
tation is theoretically possible, in practice mecha-
nisms that allow an escape to custom code make it
difficult to decide when an intermediate result will
no longer be used, hence GATE keeps most inter-
mediate data in memory until it is done analyzing
the current document.
The BandReview Task. We conclude by briefly dis-
cussing our experience with the BandReview task
from Fig. 8. We built two versions of this anno-
tator, one in AQL, and the other using expanded
CPSL grammar. The grammar implementation
processed a 4.5GB collection of 1.05 million blogs
in 5.6 hours and output 280 reviews. In contrast,
the SystemT version (85 AQL statements) ex-
tracted 323 reviews in only 10 minutes!
6 Conclusion
In this paper, we described SystemT, a declar-
ative IE system based on an algebraic frame-
work. We presented both formal and empirical
arguments for the benefits of our approach to IE.
Our extensive experimental results show that high-
quality annotators can be built using SystemT,
with an order of magnitude throughput improve-
ment compared to state-of-the-art grammar-based
systems. Going forward, SystemT opens up sev-
eral new areas of research, including implement-
ing better optimization strategies and augmenting
the algebra with additional operators to support
advanced features such as coreference resolution.
7GATE ran out of memory when using less than 5GB of
Java heap size, and thrashed when run with 5GB to 7GB
136
References
Douglas E. Appelt and Boyan Onyshkevych. 1998.
The common pattern specification language. In TIP-
STER workshop.
Branimir Boguraev. 2003. Annotation-based finite
state processing in a large-scale nlp arhitecture. In
RANLP, pages 61?80.
D. D. Chamberlin, A. M. Gilbert, and Robert A. Yost.
1981. A history of System R and SQL/data system.
In vldb.
Amit Chandel, P. C. Nagesh, and Sunita Sarawagi.
2006. Efficient batch top-k search for dictionary-
based entity recognition. In ICDE.
E. F. Codd. 1990. The relational model for database
management: version 2. Addison-Wesley Longman
Publishing Co., Inc., Boston, MA, USA.
H. Cunningham, D. Maynard, and V. Tablan. 2000.
JAPE: a Java Annotation Patterns Engine (Sec-
ond Edition). Research Memorandum CS?00?10,
Department of Computer Science, University of
Sheffield, November.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphical
development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniver-
sary Meeting of the Association for Computational
Linguistics, pages 168 ? 175.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, Valentin Tablan, Marin Dimitrov, Mike
Dowman, Niraj Aswani, Ian Roberts, Yaoyong
Li, and Adam Funk. 2010. Developing language
processing components with gate version 5 (a user
guide).
AnHai Doan, Luis Gravano, Raghu Ramakrishnan, and
Shivakumar Vaithyanathan. 2008. Special issue on
managing information extraction. SIGMOD Record,
37(4).
Witold Drozdzynski, Hans-Ulrich Krieger, Jakub
Piskorski, Ulrich Scha?fer, and Feiyu Xu. 2004.
Shallow processing with unification and typed fea-
ture structures ? foundations and applications.
Ku?nstliche Intelligenz, 1:17?23.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference - 6: A brief history. In
COLING, pages 466?471.
IBM. 2010. IBM LanguageWare.
P. G. Ipeirotis, E. Agichtein, P. Jain, and L. Gravano.
2006. To search or to crawl?: towards a query opti-
mizer for text-centric tasks. In SIGMOD.
Alpa Jain, Panagiotis G. Ipeirotis, AnHai Doan, and
Luis Gravano. 2009. Join optimization of informa-
tion extraction output: Quality matters! In ICDE.
Diana Maynard, Kalina Bontcheva, and Hamish Cun-
ningham. 2003. Towards a semantic extraction of
named entities. In Recent Advances in Natural Lan-
guage Processing.
Einat Minkov, Richard C. Wang, and William W. Co-
hen. 2005. Extracting personal names from emails:
Applying named entity recognition to informal text.
In HLT/EMNLP.
NIST. 2005. The ACE evaluation plan.
Ganesh Ramakrishnan, Sreeram Balakrishnan, and
Sachindra Joshi. 2006. Entity annotation based on
inverse index operations. In EMNLP.
Ganesh Ramakrishnan, Sachindra Joshi, Sanjeet Khai-
tan, and Sreeram Balakrishnan. 2008. Optimization
issues in inverted index-based entity annotation. In
InfoScale.
Frederick Reiss, Sriram Raghavan, Rajasekar Kr-
ishnamurthy, Huaiyu Zhu, and Shivakumar
Vaithyanathan. 2008. An algebraic approach to
rule-based information extraction. In ICDE, pages
933?942.
SAP. 2010. Inxight ThingFinder.
SAS. 2010. Text Mining with SAS Text Miner.
Warren Shen, AnHai Doan, Jeffrey F. Naughton, and
Raghu Ramakrishnan. 2007. Declarative informa-
tion extraction using datalog with embedded extrac-
tion predicates. In vldb.
SystemT. 2010. AQL Manual.
http://www.alphaworks.ibm.com/tech/systemt.
Ken Thompson. 1968. Regular expression search al-
gorithm. pages 419?422.
UIMA. 2010. Unstructured Information Management
Architecture.
http://uima.apache.org.
137
