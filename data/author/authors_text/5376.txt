Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 396?404,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Reducing semantic drift with bagging and distributional similarity
Tara McIntosh and James R. Curran
School of Information Technologies
University of Sydney
NSW 2006, Australia
{tara,james}@it.usyd.edu.au
Abstract
Iterative bootstrapping algorithms are typ-
ically compared using a single set of hand-
picked seeds. However, we demonstrate
that performance varies greatly depend-
ing on these seeds, and favourable seeds
for one algorithm can perform very poorly
with others, making comparisons unreli-
able. We exploit this wide variation with
bagging, sampling from automatically ex-
tracted seeds to reduce semantic drift.
However, semantic drift still occurs in
later iterations. We propose an integrated
distributional similarity filter to identify
and censor potential semantic drifts, en-
suring over 10% higher precision when ex-
tracting large semantic lexicons.
1 Introduction
Iterative bootstrapping algorithms have been pro-
posed to extract semantic lexicons for NLP tasks
with limited linguistic resources. Bootstrapping
was initially proposed by Riloff and Jones (1999),
and has since been successfully applied to extract-
ing general semantic lexicons (Riloff and Jones,
1999; Thelen and Riloff, 2002), biomedical enti-
ties (Yu and Agichtein, 2003), facts (Pas?ca et al,
2006), and coreference data (Yang and Su, 2007).
Bootstrapping approaches are attractive because
they are domain and language independent, re-
quire minimal linguistic pre-processing and can be
applied to raw text, and are efficient enough for
tera-scale extraction (Pas?ca et al, 2006).
Bootstrapping is minimally supervised, as it is
initialised with a small number of seed instances
of the information to extract. For semantic lexi-
cons, these seeds are terms from the category of in-
terest. The seeds identify contextual patterns that
express a particular semantic category, which in
turn recognise new terms (Riloff and Jones, 1999).
Unfortunately, semantic drift often occurs when
ambiguous or erroneous terms and/or patterns are
introduced into and then dominate the iterative
process (Curran et al, 2007).
Bootstrapping algorithms are typically com-
pared using only a single set of hand-picked seeds.
We first show that different seeds cause these al-
gorithms to generate diverse lexicons which vary
greatly in precision. This makes evaluation un-
reliable ? seeds which perform well on one algo-
rithm can perform surprisingly poorly on another.
In fact, random gold-standard seeds often outper-
form seeds carefully chosen by domain experts.
Our second contribution exploits this diversity
we have identified. We present an unsupervised
bagging algorithm which samples from the ex-
tracted lexicon rather than relying on existing
gazetteers or hand-selected seeds. Each sample is
then fed back as seeds to the bootstrapper and the
results combined using voting. This both improves
the precision of the lexicon and the robustness of
the algorithms to the choice of initial seeds.
Unfortunately, semantic drift still dominates in
later iterations, since erroneous extracted terms
and/or patterns eventually shift the category?s di-
rection. Our third contribution focuses on detect-
ing and censoring the terms introduced by seman-
tic drift. We integrate a distributional similarity
filter directly into WMEB (McIntosh and Curran,
2008). This filter judges whether a new term is
more similar to the earlier or most recently ex-
tracted terms, a sign of potential semantic drift.
We demonstrate these methods for extracting
biomedical semantic lexicons using two bootstrap-
ping algorithms. Our unsupervised bagging ap-
proach outperforms carefully hand-picked seeds
by ? 10% in later iterations. Our distributional
similarity filter gives a similar performance im-
provement. This allows us to produce large lexi-
cons accurately and efficiently for domain-specific
language processing.
396
2 Background
Hearst (1992) exploited patterns for information
extraction, to acquire is-a relations using manually
devised patterns like such Z as X and/or Y where X
and Y are hyponyms of Z. Riloff and Jones (1999)
extended this with an automated bootstrapping al-
gorithm, Multi-level Bootstrapping (MLB), which
iteratively extracts semantic lexicons from text.
In MLB, bootstrapping alternates between two
stages: pattern extraction and selection, and term
extraction and selection. MB is seeded with a small
set of user selected seed terms. These seeds are
used to identify contextual patterns they appear in,
which in turn identify new lexicon entries. This
process is repeated with the new lexicon terms
identifying new patterns. In each iteration, the top-
n candidates are selected, based on a metric scor-
ing their membership in the category and suitabil-
ity for extracting additional terms and patterns.
Bootstrapping eventually extracts polysemous
terms and patterns which weakly constrain the
semantic class, causing the lexicon?s meaning to
shift, called semantic drift by Curran et al (2007).
For example, female firstnames may drift into
flowers when Iris and Rose are extracted. Many
variations on bootstrapping have been developed
to reduce semantic drift.1
One approach is to extract multiple semantic
categories simultaneously, where the individual
bootstrapping instances compete with one another
in an attempt to actively direct the categories away
from each other. Multi-category algorithms out-
perform MLB (Thelen and Riloff, 2002), and we
focus on these algorithms in our experiments.
In BASILISK, MEB, and WMEB, each compet-
ing category iterates simultaneously between the
term and pattern extraction and selection stages.
These algorithms differ in how terms and patterns
selected by multiple categories are handled, and
their scoring metrics. In BASILISK (Thelen and
Riloff, 2002), candidate terms are ranked highly if
they have strong evidence for a category and little
or no evidence for other categories. This typically
favours less frequent terms, as they will match far
fewer patterns and are thus more likely to belong
to one category. Patterns are selected similarly,
however patterns may also be selected by differ-
ent categories in later iterations.
Curran et al (2007) introduced Mutual Exclu-
1Komachi et al (2008) used graph-based algorithms to
reduce semantic drift for Word Sense Disambiguation.
sion Bootstrapping (MEB) which forces stricter
boundaries between the competing categories than
BASILISK. In MEB, the key assumptions are that
terms only belong to a category and that patterns
only extract terms of a single category. Semantic
drift is reduced by eliminating patterns that collide
with multiple categories in an iteration and by ig-
noring colliding candidate terms (for the current
iteration). This excludes generic patterns that can
occur frequently with multiple categories, and re-
duces the chance of assigning ambiguous terms to
their less dominant sense.
2.1 Weighted MEB
The scoring of candidate terms and patterns in
MEB is na??ve. Candidates which 1) match the most
input instances; and 2) have the potential to gen-
erate the most new candidates, are preferred (Cur-
ran et al, 2007). This second criterion aims to in-
crease recall. However, the selected instances are
highly likely to introduce drift.
Our Weighted MEB algorithm (McIntosh and
Curran, 2008), extends MEB by incorporating term
and pattern weighting, and a cumulative pattern
pool. WMEB uses the ?2 statistic to identify pat-
terns and terms that are strongly associated with
the growing lexicon terms and their patterns re-
spectively. The terms and patterns are then ranked
first by the number of input instances they match
(as in MEB), but then by their weighted score.
In MEB and BASILISK2, the top-k patterns for
each iteration are used to extract new candidate
terms. As the lexicons grow, general patterns can
drift into the top-k and as a result the earlier pre-
cise patterns lose their extracting influence. In
WMEB, the pattern pool accumulates all top-k pat-
terns from previous iterations, to ensure previous
patterns can contribute.
2.2 Distributional Similarity
Distributional similarity has been used to ex-
tract semantic lexicons (Grefenstette, 1994), based
on the distributional hypothesis that semantically
similar words appear in similar contexts (Harris,
1954). Words are represented by context vectors,
and words are considered similar if their context
vectors are similar.
Patterns and distributional methods have been
combined previously. Pantel and Ravichandran
2In BASILISK, k is increased by one in each iteration, to
ensure at least one new pattern is introduced.
397
TYPE (#) MEDLINE
Terms 1 347 002
Contexts 4 090 412
5-grams 72 796 760
Unfiltered tokens 6 642 802 776
Table 1: Filtered 5-gram dataset statistics.
(2004) used lexical-syntactic patterns to label
clusters of distributionally similar terms. Mirkin et
al. (2006) used 11 patterns, and the distributional
similarity score of each pair of terms, to construct
features for lexical entailment. Pas?ca et al (2006)
used distributional similarity to find similar terms
for verifying the names in date-of-birth facts for
their tera-scale bootstrapping system.
2.3 Selecting seeds
For the majority of bootstrapping tasks, there is
little or no guidance on how to select seeds which
will generate the most accurate lexicons. Most
previous works used seeds selected based on a
user?s or domain expert?s intuition (Curran et al,
2007), which may then have to meet a frequency
criterion (Riloff et al, 2003).
Eisner and Karakos (2005) focus on this issue
by considering an approach called strapping for
word sense disambiguation. In strapping, semi-
supervised bootstrapping instances are used to
train a meta-classifier, which given a bootstrap-
ping instance can predict the usefulness (fertility)
of its seeds. The most fertile seeds can then be
used in place of hand-picked seeds.
The design of a strapping algorithm is more
complex than that of a supervised learner (Eisner
and Karakos, 2005), and it is unclear how well
strapping will generalise to other bootstrapping
tasks. In our work, we build upon bootstrapping
using unsupervised approaches.
3 Experimental setup
In our experiments we consider the task of extract-
ing biomedical semantic lexicons from raw text
using BASILISK and WMEB.
3.1 Data
We compared the performance of BASILISK and
WMEB using 5-grams (t1, t2, t3, t4, t5) from raw
MEDLINE abstracts3. In our experiments, the can-
didate terms are the middle tokens (t3), and the
patterns are a tuple of the surrounding tokens (t1,
3The set contains all MEDLINE abstracts available up to
Oct 2007 (16 140 000 abstracts).
CAT DESCRIPTION
ANTI Antibodies: Immunoglobulin molecules that react
with a specific antigen that induced its synthesis
MAb IgG IgM rituximab infliximab (?1:0.89, ?2:1.0)
CELL Cells: A morphological or functional form of a cell
RBC HUVEC BAEC VSMC SMC (?1:0.91, ?2:1.0)
CLNE Cell lines: A population of cells that are totally de-
rived from a single common ancestor cell
PC12 CHO HeLa Jurkat COS (?1:0.93, ?2: 1.0)
DISE Diseases: A definite pathological process that affects
humans, animals and or plants
asthma hepatitis tuberculosis HIV malaria
(?1:0.98, ?2:1.0)
DRUG Drugs: A pharmaceutical preparation
acetylcholine carbachol heparin penicillin tetracy-
clin (?1:0.86, ?2:0.99)
FUNC Molecular functions and processes
kinase ligase acetyltransferase helicase binding
(?1:0.87, ?2:0.99)
MUTN Mutations: Gene and protein mutations, and mutants
Leiden C677T C282Y 35delG null (?1:0.89, ?2:1.0)
PROT Proteins and genes
p53 actin collagen albumin IL-6 (?1:0.99, ?2:1.0)
SIGN Signs and symptoms of diseases
anemia hypertension hyperglycemia fever cough
(?1:0.96, ?2:0.99)
TUMR Tumors: Types of tumors
lymphoma sarcoma melanoma neuroblastoma
osteosarcoma (?1:0.89, ?2:0.95)
Table 2: The MEDLINE semantic categories.
t2, t4, t5). Unlike Riloff and Jones (1999) and
Yangarber (2003), we do not use syntactic knowl-
edge, as we aim to take a language independent
approach.
The 5-grams were extracted from the MEDLINE
abstracts following McIntosh and Curran (2008).
The abstracts were tokenised and split into sen-
tences using bio-specific NLP tools (Grover et al,
2006). The 5-grams were filtered to remove pat-
terns appearing with less than 7 terms4. The statis-
tics of the resulting dataset are shown in Table 1.
3.2 Semantic Categories
The semantic categories we extract from MED-
LINE are shown in Table 2. These are a subset
of the TREC Genomics 2007 entities (Hersh et al,
2007). Categories which are predominately multi-
term entities, e.g. Pathways and Toxicities, were
excluded.5 Genes and Proteins were merged into
PROT as they have a high degree of metonymy,
particularly out of context. The Cell or Tissue Type
category was split into two fine grained classes,
CELL and CLNE (cell line).
4This frequency was selected as it resulted in the largest
number of patterns and terms loadable by BASILISK
5Note that polysemous terms in these categories may be
correctly extracted by another category. For example, all
Pathways also belong to FUNC.
398
The five hand-picked seeds used for each cat-
egory are shown in italics in Table 2. These were
carefully chosen based on the evaluators? intuition,
and are as unambiguous as possible with respect to
the other categories.
We also utilised terms in stop categories which
are known to cause semantic drift in specific
classes. These extra categories bound the lexi-
cal space and reduce ambiguity (Yangarber, 2003;
Curran et al, 2007). We used four stop cate-
gories introduced in McIntosh and Curran (2008):
AMINO ACID, ANIMAL, BODY and ORGANISM.
3.3 Lexicon evaluation
The evaluation involves manually inspecting each
extracted term and judging whether it was a mem-
ber of the semantic class. This manual evaluation
is extremely time consuming and is necessary due
to the limited coverage of biomedical resources.
To make later evaluations more efficient, all eval-
uators? decisions for each category are cached.
Unfamiliar terms were checked using online
resources including MEDLINE, Medical Subject
Headings (MeSH), Wikipedia. Each ambiguous
term was counted as correct if it was classified into
one of its correct categories, such as lymphoma
which is a TUMR and DISE. If a term was un-
ambiguously part of a multi-word term we consid-
ered it correct. Abbreviations, acronyms and typo-
graphical variations were included. We also con-
sidered obvious spelling mistakes to be correct,
such as nuetrophils instead of neutrophils (a type
of CELL). Non-specific modifiers are marked as
incorrect, for example, gastrointestinal may be in-
correctly extracted for TUMR, as part of the entity
gastrointestinal carcinoma. However, the modi-
fier may also be used for DISE (gastrointestinal
infection) and CELL.
The terms were evaluated by two domain ex-
perts. Inter-annotator agreement was measured
on the top-100 terms extracted by BASILISK and
WMEB with the hand-picked seeds for each cat-
egory. All disagreements were discussed, and the
kappa scores, before (?1) and after (?2) the discus-
sions, are shown in Table 2. Each score is above
0.8 which reflects an agreement strength of ?al-
most perfect? (Landis and Koch, 1977).
For comparing the accuracy of the systems we
evaluated the precision of samples of the lexicons
extracted for each category. We report average
precision over the 10 semantic categories on the
1-200, 401-600 and 801-1000 term samples, and
over the first 1000 terms. In each algorithm, each
category is initialised with 5 seed terms, and the
number of patterns, k, is set to 5. In each itera-
tion, 5 lexicon terms are extracted by each cate-
gory. Each algorithm is run for 200 iterations.
4 Seed diversity
The first step in bootstrapping is to select a set of
seeds by hand. These hand-picked seeds are typi-
cally chosen by a domain expert who selects a rea-
sonably unambiguous representative sample of the
category with high coverage by introspection.
To improve the seeds, the frequency of the po-
tential seeds in the corpora is often considered, on
the assumption that highly frequent seeds are bet-
ter (Thelen and Riloff, 2002). Unfortunately, these
seeds may be too general and extract many non-
specific patterns. Another approach is to identify
seeds using hyponym patterns like, * is a [NAMED
ENTITY] (Meij and Katrenko, 2007).
This leads us to our first investigation of seed
variability and the methodology used to compare
bootstrapping algorithms. Typically algorithms
are compared using one set of hand-picked seeds
for each category (Pennacchiotti and Pantel, 2006;
McIntosh and Curran, 2008). This approach does
not provide a fair comparison or any detailed anal-
ysis of the algorithms under investigation. As
we shall see, it is possible that the seeds achieve
the maximum precision for one algorithm and the
minimum for another, and thus the single compar-
ison is inappropriate. Even evaluating on multiple
categories does not ensure the robustness of the
evaluation. Secondly, it provides no insight into
the sensitivity of an algorithm to different seeds.
4.1 Analysis with random gold seeds
Our initial analysis investigated the sensitivity and
variability of the lexicons generated using differ-
ent seeds. We instantiated each algorithm 10 times
with different random gold seeds (Sgold) for each
category. We randomly sample Sgold from two
sets of correct terms extracted from the evalua-
tion cache. UNION: the correct terms extracted by
BASILISK and WMEB; and UNIQUE: the correct
terms uniquely identified by only one algorithm.
The degree of ambiguity of each seed is unknown
and term frequency is not considered during the
random selection.
Firstly, we investigated the variability of the
399
 50
 60
 70
 80
 90
 50  60  70  80  90  100
BA
SI
LI
SK
 (p
rec
isi
on
)
WMEB (precision)
Hand-picked
Average
Figure 1: Performance relationship between
WMEB and BASILISK on Sgold UNION
extracted lexicons using UNION. Each extracted
lexicon was compared with the other 9 lexicons
for each category and the term overlap calcu-
lated. For the top 100 terms, BASILISK had an
overlap of 18% and WMEB 44%. For the top
500 terms, BASILISK had an overlap of 39% and
WMEB 47%. Clearly BASILISK is far more sensi-
tive to the choice of seeds ? this also makes the
cache a lot less valuable for the manual evaluation
of BASILISK. These results match our annotators?
intuition that BASILISK retrieved far more of the
esoteric, rare and misspelt results. The overlap be-
tween algorithms was even worse: 6.3% for the
top 100 terms and 9.1% for the top 500 terms.
The plot in Figure 1 shows the variation in pre-
cision between WMEB and BASILISK with the 10
seed sets from UNION. Precision is measured on
the first 100 terms and averaged over the 10 cate-
gories. The Shand is marked with a square, as well
as each algorithms? average precision with 1 stan-
dard deviation (S.D.) error bars. The axes start
at 50% precision. Visually, the scatter is quite
obvious and the S.D. quite large. Note that on
our Shand evaluation, BASILISK performed signif-
icantly better than average.
We applied a linear regression analysis to iden-
tify any correlation between the algorithm?s per-
formances. The resulting regression line is shown
in Figure 1. The regression analysis identified no
correlation between WMEB and BASILISK (R2 =
0.13). It is almost impossible to predict the per-
formance of an algorithm with a given set of seeds
from another?s performance, and thus compar-
isons using only one seed set are unreliable.
Table 3 summarises the results on Sgold, in-
cluding the minimum and maximum averages over
the 10 categories. At only 100 terms, lexicon
Sgold Shand Avg. Min. Max. S.D.
UNION
BASILISK 80.5 68.3 58.3 78.8 7.31
WMEB 88.1 87.1 79.3 93.5 5.97
UNIQUE
BASILISK 80.5 67.1 56.7 83.5 9.75
WMEB 88.1 91.6 82.4 95.4 3.71
Table 3: Variation in precision with random gold
seed sets
variations are already obvious. As noted above,
Shand on BASILISK performed better than average,
whereas WMEB Sgold UNIQUE performed signifi-
cantly better on average than Shand. This clearly
indicates the difficulty of picking the best seeds
for an algorithm, and that comparing algorithms
with only one set has the potential to penalise an
algorithm. These results do show that WMEB is
significantly better than BASILISK.
In the UNIQUE experiments, we hypothesized
that each algorithm would perform well on its
own set, but BASILISK performs significantly
worse than WMEB, with a S.D. greater than 9.7.
BASILISK?s poor performance may be a direct re-
sult of it preferring low frequency terms, which are
unlikely to be good seeds.
These experiments have identified previously
unreported performance variations of these sys-
tems and their sensitivity to different seeds. The
standard evaluation paradigm, using one set of
hand-picked seeds over a few categories, does not
provide a robust and informative basis for compar-
ing bootstrapping algorithms.
5 Supervised Bagging
While the wide variation we reported in the pre-
vious section is an impediment to reliable evalua-
tion, it presents an opportunity to improve the per-
formance of bootstrapping algorithms. In the next
section, we present a novel unsupervised bagging
approach to reducing semantic drift. In this sec-
tion, we consider the standard bagging approach
introduced by Breiman (1996). Bagging was used
by Ng and Cardie (2003) to create committees of
classifiers for labelling unseen data for retraining.
Here, a bootstrapping algorithm is instantiated
n = 50 times with random seed sets selected from
the UNION evaluation cache. This generates n new
lexicons L1, L2, . . . , Ln for each category. The
next phase involves aggregating the predictions in
L1?n to form the final lexicon for each category,
using a weighted voting function.
400
1-200 401-600 801-1000 1-1000
Shand
BASILISK 76.3 67.8 58.3 66.7
WMEB 90.3 82.3 62.0 78.6
Sgold BAG
BASILISK 84.2 80.2 58.2 78.2
WMEB 95.1 79.7 65.0 78.6
Table 4: Bagging with 50 gold seed sets
Our weighting function is based on two related
hypotheses of terms in highly accurate lexicons: 1)
the more category lexicons in L1?n a term appears
in, the more likely the term is a member of the
category; 2) terms ranked higher in lexicons are
more reliable category members. Firstly, we rank
the aggregated terms by the number of lexicons
they appear in, and to break ties, we take the term
that was extracted in the earliest iteration across
the lexicons.
5.1 Supervised results
Table 4 compares the average precisions of the
lexicons for BASILISK and WMEB using just the
hand-picked seeds (Shand) and 50 sample super-
vised bagging (Sgold BAG).
Bagging with samples from Sgold successfully
increased the performance of both BASILISK and
WMEB in the top 200 terms. While the improve-
ment continued for BASILISK in later sections, it
had a more variable effect for WMEB. Overall,
BASILISK gets the greater improvement in perfor-
mance (a 12% gain), almost reaching the perfor-
mance of WMEB across the top 1000 terms, while
WMEB?s performance is the same for both Shand
and Sgold BAG. We believe the greater variability
in BASILISK meant it benefited from bagging with
gold seeds.
6 Unsupervised bagging
A significant problem for supervised bagging ap-
proaches is that they require a larger set of gold-
standard seed terms to sample from ? either an
existing gazetteer or a large hand-picked set. In
our case, we used the evaluation cache which took
considerable time to accumulate. This saddles
the major application of bootstrapping, the quick
construction of accurate semantic lexicons, with a
chicken-and-egg problem.
However, we propose a novel solution ? sam-
pling from the terms extracted with the hand-
picked seeds (Lhand). WMEB already has very
high precision for the top extracted terms (88.1%
BAGGING 1-200 401-600 801-1000 1-1000
Top-100
BASILISK 72.3 63.5 58.8 65.1
WMEB 90.2 78.5 66.3 78.5
Top-200
BASILISK 70.7 60.7 45.5 59.8
WMEB 91.0 78.4 62.2 77.0
Top-500
BASILISK 63.5 60.5 45.4 56.3
WMEB 92.5 80.9 59.1 77.2
PDF-500
BASILISK 69.6 68.3 49.6 62.3
WMEB 92.9 80.7 72.1 81.0
Table 5: Bagging with 50 unsupervised seed sets
for the top 100 terms) and may provide an accept-
able source of seed terms. This approach now
only requires the original 50 hand-picked seed
terms across the 10 categories, rather than the
2100 terms used above. The process now uses two
rounds of bootstrapping: first to create Lhand to
sample from and then another round with the 50
sets of randomly unsupervised seeds, Srand.
The next decision is how to sample Srand from
Lhand. One approach is to use uniform random
sampling from restricted sections of Lhand. We
performed random sampling from the top 100,
200 and 500 terms of Lhand. The seeds from the
smaller samples will have higher precision, but
less diversity.
In a truly unsupervised approach, it is impossi-
ble to know if and when semantic drift occurs and
thus using arbitrary cut-offs can reduce the diver-
sity of the selected seeds. To increase diversity we
also sampled from the top n=500 using a proba-
bility density function (PDF) using rejection sam-
pling, where r is the rank of the term in Lhand:
PDF(r) =
?n
i=r i
?1
?n
i=1
?n
j=i j?1
(1)
6.1 Unsupervised results
Table 5 shows the average precision of the lex-
icons after bagging on the unsupervised seeds,
sampled from the top 100 ? 500 terms from Lhand.
Using the top 100 seed sample is much less effec-
tive than Sgold BAG for BASILISK but nearly as ef-
fective for WMEB. As the sample size increases,
WMEB steadily improves with the increasing vari-
ability, however BASILISK is more effective when
the more precise seeds are sampled from higher
ranking terms in the lexicons.
Sampling with PDF-500 results in more accurate
lexicons over the first 1000 terms than the other
401
 0
 0.5
 1
 1.5
 2
 2.5
 3
 0  100  200  300  400  500  600  700  800  900  1000
D
rif
t
Number of terms
Correct
Incorrect
Figure 2: Semantic drift in CELL (n=20, m=20)
sampling methods for WMEB. In particular, WMEB
is more accurate with the unsupervised seeds than
the Sgold and Shand (81.0% vs 78.6% and 78.6%).
WMEB benefits from the larger variability intro-
duced by the more diverse sets of seeds, and the
greater variability available out-weighs the poten-
tial noise from incorrect seeds. The PDF-500 dis-
tribution allows some variability whilst still prefer-
ring the most reliable unsupervised seeds. In the
critical later iterations, WMEB PDF-500 improves
over supervised bagging (Sgold BAG) by 7% and
the original hand-picked seeds (Shand) by 10%.
7 Detecting semantic drift
As shown above, semantic drift still dominates the
later iterations of bootstrapping even after bag-
ging. In this section, we propose distributional
similarity measurements over the extracted lexi-
con to detect semantic drift during the bootstrap-
ping process. Our hypothesis is that semantic drift
has occurred when a candidate term is more sim-
ilar to recently added terms than to the seed and
high precision terms added in the earlier iterations.
We experiment with a range of values of both.
Given a growing lexicon of size N , LN , let
L1...n correspond to the first n terms extracted into
L, and L(N?m)...N correspond to the last m terms
added to LN . In an iteration, let t be the next can-
didate term to be added to the lexicon.
We calculate the average distributional similar-
ity (sim) of t with all terms in L1...n and those in
L(N?m)...N and call the ratio the drift for term t:
drift(t, n,m) =
sim(L1...n, t)
sim(L(N?m)...N , t)
(2)
Smaller values of drift(t, n,m) correspond to
the current term moving further away from the
first terms. A drift(t, n,m) of 0.2 corresponds
to a 20% difference in average similarity between
L1...n and L(N?m)...N for term t.
Drift can be used as a post-processing step to fil-
ter terms that are a possible consequence of drift.
However, our main proposal is to incorporate the
drift measure directly within the WMEB bootstrap-
ping algorithm, to detect and then prevent drift oc-
curing. In each iteration, the set of candidate terms
to be added to the lexicon are scored and ranked
for their suitability. We now additionally deter-
mine the drift of each candidate term before it is
added to the lexicon. If the term?s drift is below a
specified threshold, it is discarded from the extrac-
tion process. If the term has zero similarity with
the last m terms, but is similar to at least one of
the first n terms, the term is selected. Preventing
the drifted term from entering the lexicon during
the bootstrapping process, has a flow on effect as
it will not be able to extract additional divergent
patterns which would lead to accelerated drift.
For calculating drift we use the distributional
similarity approach described in Curran (2004).
We extracted window-based features from the
filtered 5-grams to form context vectors for
each term. We used the standard t-test weight
and weighted Jaccard measure functions (Curran,
2004). This system produces a distributional score
for each pair of terms presented by the bootstrap-
ping system.
7.1 Drift detection results
To evaluate our semantic drift detection we incor-
porate our process in WMEB. Candidate terms are
still weighted in WMEB using the ?2 statistic as de-
scribed in (McIntosh and Curran, 2008). Many of
the MEDLINE categories suffer from semantic drift
in WMEB in the later stages. Figure 2 shows the
distribution of correct and incorrect terms appear-
ing in the CELL lexicon extracted using Shand with
the term?s ranks plotted against their drift scores.
Firstly, it is evident that incorrect terms begin to
dominate in later iterations. Encouragingly, there
is a trend where low values of drift correspond to
incorrect terms being added. Drift also occurs in
ANTI and MUTN, with an average precision at 801-
1000 terms of 41.5% and 33.0% respectively.
We utilise drift in two ways with WMEB;
as a post-processing filter (WMEB+POST) and
internally during the term selection phase
(WMEB+DIST). Table 6 shows the performance
402
1-200 401-600 801-1000 1000
WMEB 90.3 82.3 62.0 78.6
WMEB+POST
n:20 m:5 90.3 82.3 62.1 78.6
n:20 m:20 90.3 81.5 62.0 76.9
n:100 m:5 90.2 82.3 62.1 78.6
n:100 m:20 90.3 82.1 62.1 78.1
WMEB+DIST
n:20 m:5 90.8 79.7 72.1 80.2
n:20 m:20 90.6 80.1 76.3 81.4
n:100 m:5 90.5 82.0 79.3 82.8
n:100 m:20 90.5 81.5 77.5 81.9
Table 6: Semantic drift detection results
of drift detection with WMEB, using Shand. We
use a drift threshold of 0.2 which was selected
empirically. A higher value substantially reduced
the lexicons? size, while a lower value resulted
in little improvements. We experimented with
various sizes of initial terms L1...n (n=20, n=100)
and L(N?m)...N (m=5, m=20).
There is little performance variation observed
in the various WMEB+POST experiments. Over-
all, WMEB+POST was outperformed slightly by
WMEB. The post-filtering removed many incor-
rect terms, but did not address the underlying drift
problem. This only allowed additional incorrect
terms to enter the top 1000, resulting in no appre-
ciable difference.
Slight variations in precision are obtained using
WMEB+DIST in the first 600 terms, but noticeable
gains are achieved in the 801-1000 range. This is
not surprising as drift in many categories does not
start until later (cf. Figure 2).
With respect to the drift parameters n and m, we
found values of n below 20 to be inadequate. We
experimented initially with n=5 terms, but this is
equivalent to comparing the new candidate terms
to the initial seeds. Setting m to 5 was also less
useful than a larger sample, unless n was also
large. The best performance gain of 4.2% over-
all for 1000 terms and 17.3% at 801-1000 terms
was obtained using n=100 and m=5. In different
phases of WMEB+DIST we reduce semantic drift
significantly. In particular, at 801-1000, ANTI in-
crease by 46% to 87.5% and MUTN by 59% to
92.0%.
For our final experiments, we report the perfor-
mance of our best performing WMEB+DIST sys-
tem (n=100 m=5) using the 10 random GOLD seed
sets from section 4.1, in Table 7. On average
WMEB+DIST performs above WMEB, especially in
the later iterations where the difference is 6.3%.
Shand Avg. Min. Max. S.D.
1-200
WMEB 90.3 82.2 73.3 91.5 6.43
WMEB+DIST 90.7 84.8 78.0 91.0 4.61
401-600
WMEB 82.3 66.8 61.4 74.5 4.67
WMEB+DIST 82.0 73.1 65.2 79.3 4.52
Table 7: Final accuracy with drift detection
8 Conclusion
In this paper, we have proposed unsupervised
bagging and integrated distributional similarity to
minimise the problem of semantic drift in itera-
tive bootstrapping algorithms, particularly when
extracting large semantic lexicons.
There are a number of avenues that require fur-
ther examination. Firstly, we would like to take
our two-round unsupervised bagging further by
performing another iteration of sampling and then
bootstrapping, to see if we can get a further im-
provement. Secondly, we also intend to experi-
ment with machine learning methods for identify-
ing the correct cutoff for the drift score. Finally,
we intend to combine the bagging and distribu-
tional approaches to further improve the lexicons.
Our initial analysis demonstrated that the output
and accuracy of bootstrapping systems can be very
sensitive to the choice of seed terms and therefore
robust evaluation requires results averaged across
randomised seed sets. We exploited this variability
to create both supervised and unsupervised bag-
ging algorithms. The latter requires no more seeds
than the original algorithm but performs signifi-
cantly better and more reliably in later iterations.
Finally, we incorporated distributional similarity
measurements directly into WMEB which detect
and censor terms which could lead to semantic
drift. This approach significantly outperformed
standard WMEB, with a 17.3% improvement over
the last 200 terms extracted (801-1000). The result
is an efficient, reliable and accurate system for ex-
tracting large-scale semantic lexicons.
Acknowledgments
We would like to thank Dr Cassie Thornley, our
second evaluator who also helped with the eval-
uation guidelines; and the anonymous reviewers
for their helpful feedback. This work was sup-
ported by the CSIRO ICT Centre and the Aus-
tralian Research Council under Discovery project
DP0665973.
403
References
Leo Breiman. 1996. Bagging predictors. Machine Learning,
26(2):123?140.
James R. Curran, Tara Murphy, and Bernhard Scholz. 2007.
Minimising semantic drift with mutual exclusion boot-
strapping. In Proceedings of the 10th Conference of the
Pacific Association for Computational Linguistics, pages
172?180, Melbourne, Australia.
James R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Jason Eisner and Damianos Karakos. 2005. Bootstrapping
without the boot. In Proceedings of the Conference on
Human Language Technology and Conference on Empiri-
cal Methods in Natural Language Processing, pages 395?
402, Vancouver, British Columbia, Canada.
Gregory Grefenstette. 1994. Explorations in Automatic The-
saurus Discovery. Kluwer Academic Publishers, USA.
Claire Grover, Michael Matthews, and Richard Tobin. 2006.
Tools to address the interdependence between tokeni-
sation and standoff annotation. In Proceedings of the
Multi-dimensional Markup in Natural Language Process-
ing Workshop, Trento, Italy.
Zellig Harris. 1954. Distributional structure. Word,
10(2/3):146?162.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th Inter-
national Conference on Computational Linguistics, pages
539?545, Nantes, France.
William Hersh, Aaron M. Cohen, Lynn Ruslen, and
Phoebe M. Roberts. 2007. TREC 2007 Genomics Track
Overview. In Proceedings of the 16th Text REtrieval Con-
ference, Gaithersburg, MD, USA.
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and Yuji
Matsumoto. 2008. Graph-based analysis of semantic drift
in Espresso-like bootstrapping algorithms. In Proceedings
of the Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1011?1020, Honolulu, USA.
J. Richard Landis and Gary G. Koch. 1977. The measure-
ment of observer agreement in categorical data. Biomet-
rics, 33(1):159?174.
Tara McIntosh and James R. Curran. 2008. Weighted mu-
tual exclusion bootstrapping for domain independent lex-
icon and template acquisition. In Proceedings of the Aus-
tralasian Language Technology Association Workshop,
pages 97?105, Hobart, Australia.
Edgar Meij and Sophia Katrenko. 2007. Bootstrapping lan-
guage associated with biomedical entities. The AID group
at TREC Genomics 2007. In Proceedings of The 16th Text
REtrieval Conference, Gaithersburg, MD, USA.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006. In-
tegrating pattern-based and distributional similarity meth-
ods for lexical entailment acquistion. In Proceedings of
the 21st International Conference on Computational Lin-
guisitics and the 44th Annual Meeting of the Association
for Computational Linguistics, pages 579?586, Sydney,
Australia.
Vincent Ng and Claire Cardie. 2003. Weakly supervised
natural language learning without redundant views. In
Proceedings of the Human Language Technology Confer-
ence of the North American Chapter of the Association
for Computational Linguistics, pages 94?101, Edmonton,
USA.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lifchits,
and Alpa Jain. 2006. Names and similarities on the web:
Fact extraction in the fast lane. In Proceedings of the 21st
International Conference on Computational Linguisitics
and the 44th Annual Meeting of the Association for Com-
putational Linguistics, pages 809?816, Sydney, Australia.
Patrick Pantel and Deepak Ravichandran. 2004. Automati-
cally labelling semantic classes. In Proceedings of the Hu-
man Language Technology Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics, pages 321?328, Boston, MA, USA.
Marco Pennacchiotti and Patrick Pantel. 2006. A bootstrap-
ping algorithm for automatically harvesting semantic re-
lations. In Proceedings of Inference in Computational Se-
mantics (ICoS-06), pages 87?96, Buxton, England.
Ellen Riloff and Rosie Jones. 1999. Learning dictionaries
for information extraction by multi-level bootstrapping. In
Proceedings of the 16th National Conference on Artificial
Intelligence and the 11th Innovative Applications of Ar-
tificial Intelligence Conference, pages 474?479, Orlando,
FL, USA.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern boot-
strapping. In Proceedings of the Seventh Conference on
Natural Language Learning (CoNLL-2003), pages 25?32.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing, pages
214?221, Philadelphia, USA.
Xiaofeng Yang and Jian Su. 2007. Coreference resolu-
tion using semantic relatedness information from automat-
ically discovered patterns. In Proceedings of the 45th An-
nual Meeting of the Association for Computational Lin-
guistics, pages 528?535, Prague, Czech Republic.
Roman Yangarber. 2003. Counter-training in discovery of
semantic patterns. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguistics,
pages 343?350, Sapporo, Japan.
Hong Yu and Eugene Agichtein. 2003. Extracting synony-
mous gene and protein terms from biological literature.
Bioinformatics, 19(1):i340?i349.
404
BioNLP 2007: Biological, translational, and clinical language processing, pages 171?178,
Prague, June 2007. c?2007 Association for Computational Linguistics
Challenges for extracting biomedical knowledge from full text
Tara McIntosh
School of IT
University of Sydney
NSW 2006, Australia
tara@it.usyd.edu.au
James R. Curran
School of IT
University of Sydney
NSW 2006, Australia
james@it.usyd.edu.au
Abstract
At present, most biomedical Information
Retrieval and Extraction tools process ab-
stracts rather than full-text articles. The in-
creasing availability of full text will allow
more knowledge to be extracted with greater
reliability. To investigate the challenges of
full-text processing, we manually annotated
a corpus of cited articles from a Molecular
Interaction Map (Kohn, 1999).
Our analysis demonstrates the necessity of
full-text processing; identifies the article
sections where interactions are most com-
monly stated; and quantifies both the amount
of external knowledge required and the pro-
portion of interactions requiring multiple or
deeper inference steps. Further, it identi-
fies a range of NLP tools required, including:
identifying synonyms, and resolving coref-
erence and negated expressions. This is im-
portant guidance for researchers engineering
biomedical text processing systems.
1 Introduction
It is no longer feasible for biologists to keep abreast
of the vast quantity of biomedical literature. Even
keyword-based Information Retrieval (IR) over ab-
stracts retrieves too many articles to be individually
inspected. There is considerable interest in NLP sys-
tems that overcome this information bottleneck.
Most bioNLP systems have been applied to ab-
stracts only, due to their availability (Hirschman et
al., 2002). Unfortunately, the information in ab-
stracts is dense but limited. Full-text articles have
the advantage of providing more information and
repeating facts in different contexts, increasing the
likelihood of an imperfect system identifying them.
Full text contains explicit structure, e.g. sections
and captions, which can be exploited to improve
Information Extraction (IE) (Regev et al, 2002).
Previous work has investigated the importance of
extracting information from specific sections, e.g.
Schuemie et al (2004), but there has been little anal-
ysis of when the entire document is needed for accu-
rate knowledge extraction. For instance, extracting
a fact from the Results may require a synonym to be
resolved that is only mentioned in the Introduction.
External domain knowledge may also be required.
We investigated these issues by manually anno-
tating full-text passages that describe the functional
relationships between bio-entities summarised in a
Molecular Interaction Map (MIM). Our corpus
tracks the process Kohn (1999) followed in sum-
marising interactions for the mammalian cell MIM,
by identifying information required to infer facts,
which we call dependencies. We replicate the pro-
cess of manual curation and demonstrate the neces-
sity of full-text processing for fact extraction.
In the same annotation process we have identi-
fied NLP problems in these passages which must be
solved to identify the facts correctly including: syn-
onym and hyponym substitution, coreference reso-
lution, negation handling, and the incorporation of
knowledge from within the full text and the domain.
This allows us to report on the relative importance
of anaphora resolution and other tasks to the prob-
lem of biomedical fact extraction.
As well as serving as a dataset for future tool de-
velopment, our corpus is an excellent case study pro-
viding valuable guidance to developers of biomedi-
cal text mining and retrieval systems.
171
Figure 1: Map A of the Molecular Interaction Map compiled by Kohn (1999)
2 Biomedical NLP
Full-text articles are becoming increasingly avail-
able to NLP researchers, who have begun inves-
tigating how specific sections and structures can
be mined in various information extraction tasks.
Regev et al (2002) developed the first bioIR sys-
tem specifically focusing on limited text sections.
Their performance in the KDD Cup Challenge, pri-
marily using Figure legends, showed the importance
of considering document structure. Yu et al (2002)
showed that the Introduction defines the majority of
synonyms, while Schuemie et al (2004) and Shah et
al. (2003) showed that the Results and Methods are
the most and least informative, respectively. In con-
trast, Sinclair and Webber (2004) found the Methods
useful in assigning Gene Ontology codes to articles.
These section specific results highlight the infor-
mation loss resulting from restricting searches to in-
dividual sections, as sections often provide unique
information. Furthermore, facts appearing in dif-
ferent contexts across various sections, will be lost.
This redundancy has been used for passage valida-
tion and ranking (Clarke et al, 2001).
There are limited training resources for biomedi-
cal full-text systems. The majority of corpora con-
sist of abstracts annotated for bio-entity recognition
and Relationship Extraction, such as the GENIA
(Kim et al, 2003) and the BioCreAtIvE corpora.
However, due to the lack of full-text corpora, many
current systems only process abstracts (Ohta et al,
2006). Few biomedical corpora exist for other tasks,
such as coreference resolution (Castan?o et al, 2004;
Vlachos et al, 2006), and these are very small. In
this paper, we estimate the importance of these tasks
in bioNLP systems, which will help determine which
tasks system developers should focus effort on first.
Despite limited full-text training corpora, compe-
titions such as the Genomics track of TREC, require
systems to retrieve and rank passages from full text
that are relevant to question style queries.
3 Molecular Interaction Maps
Kohn (1999) constructed a Molecular Interaction
Map (MIM) based on literature describing 203 dif-
ferent interactions between bio-entities, such as pro-
teins and genes, in mammalian cells (Figure 1). In-
teractions in the MIM are represented as links be-
tween nodes labelled with the bio-entities. Each link
is associated with a description that summarises the
evidence for the interaction from the literature, in-
cluding citations. For example, Table 1 contains
the description passage for interaction M4 (on the
right of the Myc Box at grid reference C10 in Fig-
ure 1). Although MIM interactions may be men-
tioned in other articles, the articles cited by Kohn
(1999) document the main biomedical research lead-
ing to the discovery of these interactions.
172
c-Myc and pRb enhance transcription from the E-cadherin promoter in an AP2-dependent manner in epithelial cells (mechanism
unknown) (Batsche et al, 1998). Activation by pRb and c-Myc is not additive, suggesting that they act upon the same site,
thereby perhaps blocking the binding of an unidentified inhibitor. No c-Myc recognition element is required for activation of
the E-cadherin promoter by c-Myc. Max blocks transcriptional activation from the E-cadherin promoter by c-Myc, presumably
because it blocks the binding between c-Myc and AP2.
Table 1: MIM annotation M4
1. M4 Subfact: Activation of E-cadherin by pRb and c-Myc is not additive, suggesting they act on the
same site
a) However, the precise molecular mechanisms by which RB, Myc, and AP-2 cooperate to effect transcriptional activation of
E-cadherin requires further study. . . . the positive effects of RB and c-Myc were not additive. (Discussion)
Synonym: pRb equivalent to RB ? undefined
Synonym: c-Myc equivalent to Myc
b) The c-myc proto-oncogene, which encodes two amino-terminally distinct Myc proteins, acts as a transcription factor. (Intro)
Table 2: Example instances depending on synonym facts
In creating our corpus we have attempted to re-
verse engineer and document the MIM creation pro-
cess for many of the interactions in Kohn (1999). We
exhaustively traced and documented the process of
identifying passages from the cited full-text articles
that substantiate the MIM interactions. This allows
us to identify and quantify the amount of informa-
tion that is unavailable when systems are restricted
to abstracts.
4 Corpus Creation
The first stage of corpus creation involved obtaining
the full text of the articles cited in the MIM descrip-
tions. There are 262 articles cited in Kohn (1999),
and we have manually extracted the text from 218 of
them; we have abstracts for the other 44 which have
not been included in the analysis presented here.
Currently, the annotated part of the corpus con-
sists of passages from 101 full-text articles, support-
ing 95 of the 203 MIM descriptions. A biomedi-
cal expert exhaustively identified these passages by
manually reading each article several times. 30% of
these articles support multiple MIM descriptions and
so passages from these articles may appear multiple
times. We restricted the corpus to the cited articles
only. This allows us to quantify the need for external
resources, e.g. synonym lists and ontologies. The
corpus collection involved the following:
1. Each sentence in a MIM description is a called
a main fact.
2. For each main fact we annotated every passage
(instance) that the fact can be derived from.
These include direct statements of the fact and
passages the fact can be implied from.
3. Main facts are often complex sentences, com-
bining numerous facts from the article. Pas-
sages from which part of a fact can be de-
rived are also annotated as instances. A subfact
is then created to represent these partial facts.
This may be repeated for subfacts.
4. Many instances cannot be directly linked to
their corresponding fact, as they depend on ad-
ditional passages within the full text or exter-
nal domain knowledge. New facts are formed
to represent the dependency information ? syn-
onym and extra facts. Instances of these are an-
notated, and a link is added between the origi-
nal and dependency facts.
5. Each instance is annotated with its location
within the article. Linguistic phenomena, in-
cluding anaphora, cataphora, and negated ex-
pressions which must be resolved to derive the
fact are identified.
Tables 1 and 2 show an example of this pro-
cess. One of the main facts of interaction M4 (Ta-
ble 1) is Activation by pRb and c-Myc is not additive
. . . blocking the binding of an unidentified inhibitor.
An instance supporting part of this fact, the subfact
in Table 2 Activation of E-cadherin by pRb and c-
Myc is not additive . . . , 1.a), was identified. This in-
stance requires the resolution of two synonymy de-
pendencies, only one of which appears in the article.
173
2. E13 Main Fact: HDAC1 binds to the pocket proteins pRb, p107 and p130 and in turn is recruited to
E2F complexes on promoters
a) The experiments described above indicate that p107 and p130 can interact with HDAC1. We thus reasoned that they could
repress E2F activity by recruiting histone deacetylase activity to E2F containing promoters. (Results)
Extra: HDAC1 is a histone deacetylase
b) We have previously shown that Rb, the founding member of the pocket proteins family, represses E2F1 activity by recruiting
the histone deacetylase HDAC1. (Abstract)
Table 3: Example instances depending on extra facts
3. N4 Main fact: RPA2 binds XPA via the C-terminal region of RPA2
Mutant RPA that lacked the p34 C terminus failed to interact with XPA, whereas RPA containing the p70 mutant (Delta RS)
interacted with XPA (Fig. 2). (Results)
4. C9 Subfact: Cyclin D1 degraded rapidly by phosphorylation at threonine-286
Although ?free? or CDK4-bound cyclin D1 molecules are intrinsically unstable (t1/2< 30 min), a cyclin D1 mutant (T286A)
containing an alanine for threonine-286 substitution fails to undergo efficient polyubiquitination in an in vitro system or in
vivo, and it is markedly stabilized (t1/2 approximately 3.5 hr) when inducibly expressed in either quiescent or proliferating
mouse fibroblasts. (Abstract)
Table 4: Example instances with negated expressions
5 Dependencies
In our corpus, an instance of a fact may depend on
additional facts (dependencies) to allow the fact to
be derived from the original instance. Dependencies
may occur elsewhere in the document or may not be
mentioned at all. We consider two types of depen-
dencies: synonym facts and extra facts.
5.1 Synonym Facts
The frequent use of synonyms, abbreviations and
acronyms in biomedical text is a common source
of ambiguity that is often hard to resolve (Sehgal
et al, 2004). Furthermore, synonym lists are dif-
ficult to maintain in rapidly moving fields like bi-
ology (Lussier et al, 2006). There has been recent
interest in developing systems to identify and extract
these (Ao and Takagi, 2005; Okazaki and Anani-
adou, 2006).
In our corpus we group all of these synonyms, ab-
breviations, acronyms and other orthographic varia-
tions as synonym facts. For example, the synonyms
(1) E2F4, (2) E2F-4 and (3) E2F1-4 in our cor-
pus refer to the same entity E2F4, however term (3)
also includes the entities E2F1, E2F2 and E2F3.
In Table 2, an instance supporting subfact 1. is
shown in 1.a). The bio-entity pRb mentioned in the
subfact does not appear in this instance. Thus 1.a)
depends on knowing that pRb is equivalent to RB,
and so we form a new synonym fact. This synonym
is undefined in the article and cannot be assumed as
RB is also a homograph for the gene ruby (rb), ru-
bidium (Rb) and Robertsonian (Rb) translocations.
Instance 1 also depends on a second synonym ?
c-Myc and Myc are used interchangeably, where
the protein Myc is referred to by its gene name,
c-Myc. Metonymy is common in biology, and an
instance supporting this synonym fact was found in
the article, 1.b).
5.2 Extra Facts
Extra facts include all assertions (excluding syn-
onym definitions) which are necessary to make a
valid inference from an instance to a fact or subfact.
These extra facts must be found within the same ar-
ticle. Many extra facts are descriptions or classes
of bio-entities and hyponym relationships. Accord-
ing to Ne?dellec et al (2006), a clearer distinc-
tion between entities and their classes/descriptions
is needed in bioNLP corpora.
Example 2 in Table 3 is an instance which de-
pends on an extra fact, 2.b), to derive the main fact.
The class of proteins histone deacetylase
in sentence 2 must be linked to the specific pro-
tein HDAC1 in sentence 1, since the sortal anaphor
they in sentence 2 refers to the antecedents p107
and p130, and does not include HDAC1. This extra
fact is identified in the apposition the histone
deacetylase HDAC1 in instance 2.b).
174
5. C11b Subfact: p19ARF induces cell cycle arrest in a p53-dependent manner
INK4a/ARF is perhaps the second most commonly disrupted locus in cancer cells. It encodes two distinct tumor
suppressor proteins: p16INK4a, which inhibits the phosphorylation of the retinoblastoma protein by cyclin D-
dependent kinases, and p19ARF, which stabilizes and activates p53 to promote either cell cycle arrest or apoptosis. (Intro)
6. C36 Main fact: Cdc25C is phosphorylated by Cyclin B-cdk1
In this work, we examine the effect of phosphorylation on the human cdc25-C protein (Sadhu et al,1990). We show that this
protein is phosphorylated during mitosis in human cells and that this requires active cdc2-cyclin B. (Intro)
Table 5: Example instances with cataphora and event anaphora
6 Negated Expressions
To quantify the importance of lexical and logical
negations we have annotated each instance involv-
ing one or more negated expressions that must be
resolved to derive the fact. In biomedical literature,
negated expressions are commonly used to describe
an abnormal condition, such as a mutation, and its
resulting abnormal outcome, such as cancer, from
which the normal condition and outcome can be in-
ferred. This typically requires two or more negated
expressions to be processed simultaneously.
Table 4 shows examples of instances with negated
expressions. In the subject NP of instance 3, the lex-
ical negative form of RPA (Mutant RPA) is fol-
lowed directly by a logical negative detailing the
function it failed to perform. These two negative ex-
pressions support the positive in the main fact. This
implicit reporting of results expressed in terms of
negative experimental outcomes is very common in
molecular biology and genetics.
Example 4 requires external domain knowl-
edge. Firstly, the amino acid alanine cannot
be phosphorylated like threonine. Secondly,
polyubiquitination triggers a signal for a
protein (cyclin D1) to be degraded. Therefore
from this negated pair the positive fact from interac-
tion C9 can be inferred.
The context surrounding potential negative ex-
pressions must be analysed to determine if it is in-
deed a negative. For example, not all mutations re-
sult in negative outcomes ? the mutation of p70 in
instance 3 did not have a negative outcome.
7 Coreference Expressions
In biomedical literature, coreference expressions are
used to make abbreviated or indirect references to
bio-entities or events, and to provide additional in-
formation, such as more detailed descriptions.
To quantify the importance of coreference expres-
sions, instances in our corpus are annotated with
pronominal, sortal and event anaphoric, and cat-
aphoric expressions, including those extending be-
yond one sentence. Instances 4?6 in Tables 4?
5, each contain annotated pronominal or sortal
anaphoric expressions. Instance 5 also involves
a cataphoric expression, where suppressor
proteins refers to p16INK4a and p19ARF
Event anaphora refer to processes and are quite
common in biomedical text. We have annotated
these separately to pronominal and sortal anaphora.
Our event anaphora annotations are different to
Humphreys et al (1997). They associate sequential
events, while we only refer to the same event.
An example is shown in instance 6 (Table 5)
where the additional sortal anaphor complicates re-
solving the event anaphor. The third this refers
to the phosphorylation event, phosphorylated,
and not the protein cdc25-C like the second this.
8 Locating Facts
The key facts and results are generally repeated and
reworded in various contexts within an article. This
redundancy can be used in two ways to improve sys-
tem precision and recall. Firstly, the redundancy in-
creases the chance of an imperfect system identify-
ing at least one instance. Secondly, the redundancy
can be used for fact validation. By annotating every
instance that supports a fact we are able to measure
the degree of factual redundancy in full-text articles.
We have also annotated each instance with its lo-
cation within the article: which section (or structure
such as a title, heading or caption) it was contained
within and the number of the paragraph. Using this
data, we can evaluate the informativeness of each
section and structure for identifying interactions.
Using our detailed dependency annotations we
can also determine how many instances need addi-
175
Location Main Fact Subfact Synonym Extra
Title 3.3 ( 0.2) 1.9 ( 0.7) 0.0 ( 0.0) 0.8 ( 0.8)
Abstract 19.1 (10.1) 9.3 ( 5.1) 36.2 (21.7) 25.8 (14.8)
Introduction 11.3 ( 5.2) 8.3 ( 3.4) 30.4 (17.4) 17.2 ( 7.8)
Results 31.0 (13.8) 37.6 (16.1) 20.3 (15.9) 32.0 (12.5)
Discussion 21.8 ( 7.3) 19.5 ( 6.6) 2.9 ( 1.4) 9.4 ( 3.1)
Figure Heading 5.0 ( 0.6) 10.7 ( 3.8) 1.4 ( 1.4) 2.3 ( 0.0)
Figure Legend 3.1 ( 1.3) 4.8 ( 2.0) 0.0 ( 0.0) 7.0 ( 4.7)
Table Data 0.0 ( 0.0) 0.2 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0)
Methods 0.2 ( 0.0) 0.1 ( 0.1) 0.0 ( 0.0) 4.7 ( 0.8)
Conclusion 0.6 ( 0.4) 0.1 ( 0.0) 0.0 ( 0.0) 0.0 ( 0.0)
Footnotes 0.0 ( 0.0) 0.0 ( 0.0) 5.8 ( 2.9) 0.0 ( 0.0)
Headings 4.8 ( 0.6) 7.5 ( 2.7) 2.9 ( 1.4) 0.8 ( 0.8)
Full-text 100.0 (39.4) 100.0 (40.6) 100.0 (62.3) 100.0 (45.3)
Table 6: Instances found excluding (including) all dependencies
Fact Type # Created # Found # Instances
Main Fact 170 156 523
Subfact 251 251 1196
Synonym 155 62 69
Extra 152 87 128
Total 728 556 1916
Table 7: Distribution of fact types in corpus
tional knowledge outside of the current section to
support a particular fact. This demonstrates how im-
portant full-text processing is.
9 Corpus Analysis
Having described the corpus annotation we can now
investigate various statistical properties of the data.
Table 7 shows the distribution of the various anno-
tated fact types within the corpus. There are a to-
tal of 728 different facts identified, with 556 (76%)
found within the documents. We have annotated
1916 individual passages as instances, totally 2429
sentences. There were 14 main facts that we found
no instances or subfact instances for.
The most redundancy occurs in main facts and
subfacts, with on average 3.35 and 4.76 instances
each respectively, whilst synonym facts have almost
no redundancy. Also, a large proportion of synonym
and extra facts, 60% and 43% respectively, do not
appear anywhere in the articles (Table 7).
This high level of redundancy in facts demon-
strates the significant advantages of processing full
text. However, the proportion of missing synonym
Instances Synonym Extra
Main Fact 46.8 (10.9) 26.2 (18.9)
Subfact 36.9 ( 8.2) 26.7 (15.4)
Synonym 8.7 ( 2.9) 7.2 ( 4.3)
Extra 25.0 ( 0.0) 13.3 (10.9)
Table 8: Instances with (all found) dependencies
and extra facts shows the importance of external re-
sources, such as synonym lists, and tools for recog-
nising orthographic variants.
9.1 Locating Facts
Table 6 shows the percentage of instances identified
in particular locations within the articles. The best
sections for finding instances of facts and subfacts
were the Results and Discussion sections, whereas
synonym and extra facts were best found in the Ab-
stract, Introduction and Results. The later sections
of each article rarely contributed any instances. In-
terestingly, we did not find the Figure headings or
legends to be that informative for main facts. Figure
headings are restricted in length and thus are rarely
able to express main facts as well as subfacts.
The proportion of main facts and subfact in-
stances found in the abstract is quite small, further
demonstrating the value of full-text processing.
If we take into account the additional dependency
information, and restrict the instances to those fully
supported within a given section, the results drop
dramatically (those in parentheses in Table 6). In
176
Depth Fact Subfact Synonym Extra
0 35.2 45.1 87.0 64.8
1 53.9 44.2 13.0 26.6
2 9.6 9.5 0.0 7.0
3 1.3 0.9 0.0 1.6
4 0.0 0.3 0.0 0.0
Table 9: Maximum depth of instance dependencies
Breadth Fact Subfact Synonym Extra
0 35.2 45.1 87.0 64.8
1 36.5 35.5 7.2 29.7
2 22.6 15.7 5.8 4.7
3 4.6 2.9 0.0 0.8
4 0.8 0.6 0.0 0.0
5 0.2 0.2 0.0 0.0
Table 10: Breadth of instance dependencies
total, the number of instances drops to 39.4% and
40.6%, for main facts and subfacts, respectively.
This again demonstrates the need for full-text pro-
cessing, including the dependencies between facts
found in different sections of the article.
9.2 Dependencies
Our corpus represents each of the facts and subfacts
as a dependency graph of instances, each which in
turn may require support from other facts, including
synonym and extra facts.
Table 8 shows the percentage of instances which
depend on synonym and extra facts in our corpus.
46.8% of main fact instances depend on at least one
synonym fact, but only 10.9% of main fact instances
which depend on at least one synonym were com-
pletely resolved (i.e. all of the synonyms were found
as well). Interestingly, synonym and extra facts of-
ten required other synonym and extra facts.
Our corpus contains more synonym than extra fact
dependencies, however more extra facts were de-
fined in the articles. The large proportion of main
facts and subfacts depending on synonyms and extra
facts demonstrates the importance of automatically
extracting this information from full text.
Since the inference from an instance to a fact may
depend on other facts, long chains of dependencies
may occur, all of which would need to be resolved
before a main fact could be derived from the text.
Expressions Instances
Negated 4.3
Anaphora 13.2
Event Anaphora 6.6
Cataphora 2.7
Table 11: Distribution of annotated expressions
Table 9 shows the distribution of maximum chain
depth in our dependency graphs. The maximum
depth is predominately less than 3. Table 10 shows
the distribution of the breadth of dependency graphs.
Again, most instances are supported by fewer than 3
dependency chains. Most instances depend on some
other information, but luckily, a large proportion of
those only require information from a small number
of other facts. However, given that these facts could
occur anywhere within the full text, extracting them
is still a very challenging task.
9.3 Negated & Coreference Expressions
Table 11 shows the percentage of instances anno-
tated with negated, anaphoric and cataphoric ex-
pressions in our corpus. We have separated event
anaphora from pronominal and sortal anaphora.
There are fewer cataphoric and negated expressions
than anaphoric expressions. Therefore, we would
expect the greatest improvement when systems in-
corporate anaphora resolution components, and lit-
tle improvement from cataphoric and negated ex-
pression analysis. However, negated expressions
provide valuable information regarding experimen-
tal conditions and outcomes, and thus may be ap-
propriate for specific extraction tasks.
10 Conclusion
This paper describes a corpus documenting the man-
ual identification of facts from full-text articles by
biomedical researchers. The corpus consists of arti-
cles cited in a Molecular Interaction Map developed
by Kohn (1999). Each fact can be derived from one
or more passages from the citations. Each of these
instances was annotated with their location in the
article and whether they contained coreference or
negated expressions. Each instance was also linked
with other information, including synonyms and ex-
tra knowledge, that was required to derive the partic-
ular interaction. The annotation task was quite com-
177
plex and as future work we will increase the relia-
bility of our corpus by including the annotations of
other domain experts using our guidelines, and use
this resource for tool development. The guidelines
and corpus will be made publicly available.
Our corpus analysis demonstrates that full-text
analysis is crucial for exploiting biomedical litera-
ture. Less than 20% of fact instances we identified
were contained in the abstract. Analysing sections
in isolation reduced the number of supported facts
by 60%. We also showed that many instances were
dependent on a significant amount of other informa-
tion, both within and outside the article. Finally, we
showed the potential impact of various NLP compo-
nents such as anaphora resolution systems.
This work provides important empirical guidance
for developers of biomedical text mining systems.
Acknowledgements
This work was supported by the CSIRO ICT Cen-
tre and ARC Discovery grants DP0453131 and
DP0665973.
References
Hiroko Ao and Toshihisa Takagi. 2005. ALICE: An algorithm
to extract abbreviations from Medline. Journal of the Amer-
ican Medical Informatics Association, 12(5):576?586.
J. Castan?o, J. Zhang, and J. Pustejovsky. 2004. Anaphora reso-
lution in biomedical literature. In International Symposium
on Reference Resolution in NLP, Alicante, Spain.
Charles L. A. Clarke, Gordon V. Cormack, and Thomas R. Ly-
nam. 2001. Exploiting redundancy in question answering.
In Proc. of the 24th Annual International ACM SIGIR Con-
ference on Research and Development in Information Re-
trieval, pages 358?365, New Orleans, LA.
Lynette Hirschman, Jong C. Park, Junichi Tsujii, Limsoon
Wong, and Cathy Wu. 2002. Accomplishments and chal-
lenges in literature data mining for biology. Bioinformatics
Review, (12):1553?1561.
Kevin Humphreys, Robert Gaizauskas, and Saliha Azzam.
1997. Event coreference for information extraction. In Proc.
of the ACL/EACL Workshop on Operational Factors in Prac-
tical, Robust Anaphora Resolution for Unrestricted Texts,
Madrid, Spain.
Jin-Dong Kim, Tomoko Ohta, Yuka Teteisi, and Jun?ichi Tsujii.
2003. GENIA corpus - a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(1):i180?i182.
Kurt W. Kohn. 1999. Molecular interaction map of the mam-
malian cell cycle and DNA repair systems. Molecular Biol-
ogy of the Cell, 10:2703?2734.
Yves Lussier, Tara Borlawsky, Daniel Rappaport, Yang Liu, and
Carol Friedman. 2006. PHENOGO: Assigning phenotypic
context to gene ontology annotations with natural language
processing. In Proc. of the Pacific Symposium on Biocom-
puting, volume 11, pages 64?75, Maui, HI.
Clair Ne?dellec, Philippe Bessie`res, Robert Bossy, Alain Kptou-
janksy, and Alain-Pierre Manine. 2006. Annotation guide-
lines for machine learning-based named entity recognition
in microbiology. In Proc. of the ACL Workshop on Data and
Text for Mining Integrative Biology, pages 40?54, Berlin.
Tomoko Ohta, Yusuke Miyao, Takashi Ninomiya, Yoshi-
masa Tsuruoka, Akane Yakushiji, Katsuya Masuda, Jumpei
Takeuchi, Kazuhiro Yoshida, Tadayoshi Hara, Jin-Dong
Kim, Yuka Tateisi, and Jun?ichi Tsujii. 2006. An intelli-
gent search engine and GUI-based efficient Medline search
tool based on deep syntactic parsing. In Proc. of the COL-
ING/ACL Interactive Presentation Sessions, pages 17?20,
Sydney, Australia.
Naoaki Okazaki and Sophia Ananiadou. 2006. A term recog-
nition approach to acronym recognition. In Proc. the 21st
International Conference on Computational Linguistics and
the 44th Annual Meeting of the ACL, pages 643?650, Syd-
ney, Australia.
Yizhar Regev, Michal Finkelstein-Langau, Ronen Feldman,
Mayo Gorodetsky, Xin Zheng, Samuel Levy, Rosane Char-
lab, Charles Lawrence, Ross A. Lippert, Qing Zhang, and
Hagit Shatkay. 2002. Rule-based extraction of experimen-
tal evidence in the biomedical domain - the KDD Cup 2002
(Task 1). ACM SIGKKD Explorations, 4(2):90?92.
M.J. Schuemie, M.Weeber, B.J.A. Schijvenaars, E.M. vanMul-
ligen, C.C. van der Eijk, R.Jelier, B.Mons, and J.A Kors.
2004. Distribution of information in biomedical abstracts
and full-text publications. Bioinformatics, 20(16):2597?
2604.
Aditya K. Sehgal, Padmini Srinivasan, and Olivier Bodenreider.
2004. Gene terms and english words: An ambiguous mix. In
Proc. of the ACM SIGIR Workshop on Search and Discovery
for Bioinformatics, Sheffield, UK.
Parantu K. Shah, Carolina Perez-Iratxeta, Peer Bork, and
Miguel A. Andrade. 2003. Information extraction from full
text scientific articles: where are the keywords? BMC Bioin-
formatics, 4(20).
Gail Sinclair and Bonnie Webber. 2004. Classification from
full text: A comparison of canonical sections of scientific
papers. In Proc. of the International Joint Workshop on Nat-
ural Language Processing in Biomedicine and its Applica-
tions, pages 66?69, Geneva, Switzerland.
Andreas Vlachos, Caroline Gasperin, Ian Lewin, and Ted
Briscoe. 2006. Bootstrapping the recognition and anaphoric
linking of named entities in drosophila articles. In Proc. of
the Pacific Symposium on Biocomputing, volume 11, pages
100?111, Maui, HI.
Hong Yu, Vasileios Hatzivassiloglou, Carol Friedman, Andrey
Rzhetsky, and W.John Wilbur. 2002. Automatic extraction
of gene and protein synonyms fromMedline and journal arti-
cles. In Proc. of the AMIA Symposium 2002, pages 919?923,
San Antonio, TX.
178
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 356?365,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Unsupervised discovery of negative categories in lexicon bootstrapping
Tara McIntosh
NICTA Victoria Research Lab
Dept of Computer Science and Software Engineering
University of Melbourne
nlp@taramcintosh.org
Abstract
Multi-category bootstrapping algorithms were
developed to reduce semantic drift. By ex-
tracting multiple semantic lexicons simultane-
ously, a category?s search space may be re-
stricted. The best results have been achieved
through reliance on manually crafted negative
categories. Unfortunately, identifying these
categories is non-trivial, and their use shifts
the unsupervised bootstrapping paradigm to-
wards a supervised framework.
We present NEG-FINDER, the first approach
for discovering negative categories automat-
ically. NEG-FINDER exploits unsupervised
term clustering to generate multiple nega-
tive categories during bootstrapping. Our al-
gorithm effectively removes the necessity of
manual intervention and formulation of nega-
tive categories, with performance closely ap-
proaching that obtained using negative cate-
gories defined by a domain expert.
1 Introduction
Automatically acquiring semantic lexicons from text
is essential for overcoming the knowledge bottle-
neck in many NLP tasks, e.g. question answer-
ing (Ravichandran and Hovy, 2002). Many of the
successful methods follow the unsupervised itera-
tive bootstrapping framework (Riloff and Shepherd,
1997). Bootstrapping has since been effectively ap-
plied to extracting general semantic lexicons (Riloff
and Jones, 1999), biomedical entities (Yu and
Agichtein, 2003) and facts (Carlson et al, 2010).
Bootstrapping is often considered to be minimally
supervised, as it is initialised with a small set of seed
terms of the target category to extract. These seeds
are used to identify patterns that can match the tar-
get category, which in turn can extract new lexicon
terms (Riloff and Jones, 1999). Unfortunately, se-
mantic drift often occurs when ambiguous or erro-
neous terms and/or patterns are introduced into the
iterative process (Curran et al, 2007).
In multi-category bootstrapping, semantic drift is
often reduced when the target categories compete
with each other for terms and/or patterns (Yangarber
et al, 2002). This process is most effective when
the categories bound each other?s search space. To
ensure this, manually crafted negative categories are
introduced (Lin et al, 2003; Curran et al, 2007).
Unfortunately, this makes these algorithms substan-
tially more supervised.
The design of negative categories is a very time
consuming task. It typically requires a domain ex-
pert to identify the semantic drift and its cause, fol-
lowed by a significant amount of trial and error in or-
der to select the most suitable combination of nega-
tive categories. This introduces a substantial amount
of supervised information into what was an unsuper-
vised framework, and in turn negates one of the main
advantages of bootstrapping ? the quick construc-
tion of accurate semantic lexicons.
We show that although excellent performance is
achieved using negative categories, it varies greatly
depending on the negative categories selected. This
highlights the difficulty of crafting negative cate-
gories and thus the necessity for tools that can au-
tomatically identify them.
Our second contribution is the first fully unsu-
pervised approach, NEG-FINDER, for discovering
356
negative categories automatically. During boot-
strapping, efficient clustering techniques are applied
to sets of drifted candidate terms to generate new
negative categories. Once a negative category is
identified it is incorporated into the subsequent it-
erations whereby it provides the necessary semantic
boundaries for the target categories.
We demonstrate the effectiveness of our ap-
proach for extracting biomedical semantic lexicons
by incorporating NEG-FINDER within the WMEB-
DRIFT bootstrapping algorithm (McIntosh and Cur-
ran, 2009). NEG-FINDER significantly outperforms
bootstrapping prior to the domain expert?s negative
categories. We show that by using our discovered
categories we can reach near expert-guided perfor-
mance. Our methods effectively remove the neces-
sity of manual intervention and formulation of neg-
ative categories in semantic lexicon bootstrapping.
2 Background
Various automated pattern-based bootstrapping al-
gorithms have been proposed to iteratively build se-
mantic lexicons. In multi-level bootstrapping, a lex-
icon is iteratively expanded from a small sample of
seed terms (Riloff and Jones, 1999). The seed terms
are used to identify contextual patterns they appear
in, which in turn may be used to extract new lexi-
con entries. This process is repeated with the new
expanded lexicon identifying new patterns.
When bootstrapping semantic lexicons, polyse-
mous or erroneous terms and/or patterns that weakly
constrain the semantic class are eventually extracted.
This often causes semantic drift ? when a lexicon?s
intended meaning shifts into another category dur-
ing bootstrapping (Curran et al, 2007). For exam-
ple, female names may drift into gemstones when
the terms Ruby and Pearl are extracted.
Multi-category bootstrapping algorithms, such as
BASILISK (Thelen and Riloff, 2002), NOMEN (Yan-
garber et al, 2002), and WMEB (McIntosh and
Curran, 2008), aim to reduce semantic drift by
extracting multiple semantic categories simultane-
ously. These algorithms utilise information about
other semantic categories in order to reduce the cate-
gories from drifting towards each other. This frame-
work has recently been extended to extract different
relations from text (Carlson et al, 2010).
2.1 Weighted MEB
In Weighted Mutual Exclusion Bootstrapping
(WMEB, McIntosh and Curran, 2008), multiple se-
mantic categories iterate simultaneously between
the term and pattern extraction phases, competing
with each other for terms and patterns. Semantic
drift is reduced by forcing the categories to be mu-
tually exclusive. That is, candidate terms can only
be extracted by a single category and patterns can
only extract terms for a single category.
In WMEB, multiple bootstrapping instances are
initiated for each competing target category. Each
category?s seed set forms its initial lexicon. For
each term in the category lexicon, WMEB identifies
all candidate contextual patterns that can match the
term in the text. To ensure mutual exclusion between
the categories, candidate patterns that are identified
by multiple categories in an iteration are excluded.
The remaining patterns are then ranked according to
the reliability measure and relevance weight.
The reliability of a pattern for a given category
is the number of extracted terms in the category?s
lexicon that match the pattern. A pattern?s relevance
weight is defined as the sum of the ?-squared values
between the pattern (p) and each of the lexicon terms
(t): weight(p) =
?
t?T ?
2(p, t). These metrics are
symmetrical for both candidate terms and patterns.
The top-m patterns are then added to the pool of
extracting patterns. If each of the top-m patterns al-
ready exists in the pool, the next unseen pattern is
added to the pool. This ensures at least one new pat-
tern is added to the pool in each iteration.
In the term selection phase, all patterns within the
pattern pool are used to identify candidate terms.
Like the candidate patterns, terms that are extracted
by multiple categories in the same iteration are also
excluded. The remaining candidate terms are ranked
with respect to their reliability and relevance weight,
and the top-n terms are added to the lexicon.
2.2 Detecting semantic drift in WMEB
In McIntosh and Curran (2009), we showed that
multi-category bootstrappers are still prone to se-
mantic drift in the later iterations. We proposed a
drift detection metric based on our hypothesis that
semantic drift occurs when a candidate term is more
similar to the recently added terms than to the seed
357
and high precision terms extracted in the earlier
iterations. Our metric is based on distributional sim-
ilarity measurements and can be directly incorpo-
rated into WMEB?s term selection phase to prevent
drifting terms from being extracted (WMEB-DRIFT).
The drift metric is defined as the ratio of the aver-
age distributional similarity of the candidate term to
the first n terms extracted into the lexicon L, and to
the last m terms extracted in the previous iterations:
drift(term, n,m) =
avgsim(L1...n, term)
avgsim(L(N?m+1)...N , term)
(1)
2.3 Negative categories
In multi-category bootstrapping, improvements in
precision arise when semantic boundaries between
multiple target categories are established. Thus, it is
beneficial to bootstrap categories that share similar
semantic spaces, such as female names and flowers.
Unfortunately, it is difficult to predict if a tar-
get category will suffer from semantic drift and/or
whether it will naturally compete with the other tar-
get categories. Once a domain expert establishes
semantic drift and its possible cause, a set of neg-
ative/stop categories that may be of no direct inter-
est are manually crafted to prevent semantic drift.
These additional categories are then exploited dur-
ing another round of bootstrapping to provide fur-
ther competition for the target categories (Lin et al,
2003; Curran et al, 2007).
Lin et al (2003) improved NOMEN?s perfor-
mance for extracting diseases and locations from
the ProMED corpus by incorporating negative cat-
egories into the bootstrapping process. They first
used one general negative category, seeded with the
10 most frequent nouns in the corpus that were un-
related to the target categories. This single nega-
tive category resulted in substantial improvements in
precision. In their final experiment, six negative cat-
egories that were notable sources of semantic drift
were identified, and the inclusion of these lead to
further performance improvements (?20%).
In similar experiments, both Curran et al (2007)
and McIntosh (2010) manually crafted negative
categories that were necessary to prevent semantic
drift. In particular, in McIntosh (2010), a biomedical
expert spent considerable time (?15 days) and effort
Initial Lexicon
Drift Cache
Clustered Terms Negative Lexicon
Figure 1: NEG-FINDER: Local negative discovery
identifying potential negative categories and subse-
quently optimising their associated seeds in trial and
error bootstrapping runs.
By introducing manually crafted negative cate-
gories, a significant amount of expert domain knowl-
edge is introduced. The use of this expert knowl-
edge undermines the principle advantages of un-
supervised bootstrapping, by making it difficult to
bootstrap lexicons for a large number of categories
across diverse domains or languages. In this pa-
per, we aim to push multi-category bootstrapping
back into its original minimally-supervised frame-
work, with as little performance loss as possible.
3 NEG-FINDER
Our approach, Negative Category Finder for Boot-
strapping (NEG-FINDER), can be easily incorporated
into bootstrapping algorithms that exclude candidate
terms or facts based on a selection criteria, includ-
ing WMEB-DRIFT and Pas?ca et al?s (2006) large-
scale fact extraction system. For simplicity, we de-
scribe our approach within the WMEB-DRIFT boot-
strapping algorithm. Figure 1 shows the framework
of our approach.
To discover negative categories during bootstrap-
ping, NEG-FINDER must identify a representative
cluster of the drifted terms. In this section, we
present the two types of clustering used (maximum
and outlier), and our three different levels of nega-
tive discovery (local, global and mixture).
3.1 Discovering negative categories
We have observed that semantic drift begins to dom-
inate when clusters of incorrect terms with similar
358
meanings are extracted. In the term selection phase
of WMEB-DRIFT, the top-n candidate terms that sat-
isfy the drift detection threshold are added to the ex-
panding lexicon. Those terms which are considered
but do not meet the threshold are excluded.
In NEG-FINDER, these drifted terms are cached as
they may provide adequate seed terms for new neg-
ative categories. However, the drifted terms can also
include scattered polysemous or correct terms that
share little similarity with the other drifted terms.
Therefore, simply using the first set of drifted terms
to establish a negative category is likely to introduce
noise rather than a cohesive competing category.
To discover negative categories, we exploit hi-
erarchical clustering to group similar terms within
the cache of drifted terms. In agglomerative hi-
erarchical clustering, a single term is assigned to
an individual cluster, and these clusters are itera-
tively merged until a final cluster is formed contain-
ing all terms (Kaufmann and Rousseeuw, 1990). In
our approach, the similarity between two clusters is
computed as the average distributional similarity be-
tween all pairs of terms across the clusters (average-
link clustering).
For calculating the similarity between two terms
we use the distributional similarity approach de-
scribed in Curran (2004). We extracted window-
based features from the set of candidate patterns to
form context vectors for each term. We use the
standard t-test weight and weighted Jaccard measure
functions (Curran, 2004).
To ensure adequate coverage of the possible drift-
ing topics, negative discovery and hence clustering
is only performed when the drift cache consists of at
least 20 terms.
3.2 Maximum and outlier clustering
Although hierarchical clustering is quadratic, we can
efficiently exploit the agglomerative process as the
most similar terms will merge into clusters first.
Therefore, to identify the k most similar terms, we
can exit the clustering process as soon as a cluster
of size k is established. We refer to this approach as
maximum clustering.
In our next clustering method, we aim to form a
negative category with as little similarity to the tar-
get seeds. We use an outlier clustering strategy, in
which the drifted term t with the least average distri-
butional similarity to the first n terms in the lexicon
must be contained in the cluster of seeds. We use
average similarity to the first n terms, as it is already
pre-computed for the drift detection metric. As with
maximum clustering, once a cluster of size k con-
taining the term t is formed, the clustering process
can be terminated.
3.3 Incorporating the negative category
After a cluster of negative seed terms is established,
the drift cache is cleared, and a new negative cate-
gory is created and introduced into the iterative boot-
strapping process in the next iteration. This means
that the negative category can only influence the
subsequent iterations of bootstrapping. The nega-
tive categories can compete with all other categories,
including any previously introduced negative cate-
gories, however the negative categories do not con-
tribute to the drift caches.
Before the new category is introduced, its first
set of extracting patterns must be identified. For
this, the complete set of extracting patterns match-
ing any of the negative seeds are considered and
ranked with respect to the seeds. The top scoring
patterns are considered sequentially until m patterns
are assigned to the new negative category. To ensure
mutual exclusion between the new category and the
target categories, a candidate pattern that has previ-
ously been selected by a target category cannot be
used to extract terms for either category in the sub-
sequent iterations.
3.4 Levels of negative discovery
Negative category discovery can be performed at a
local or global level, or as a mixture of both. In local
discovery, each target category has its own drifted
term cache and can generate negative categories ir-
respective of the other target categories. This is
shown in Figure 1. The drifted terms (shaded) are
extracted away from the lexicon into the local drift
cache, which is then clustered. A cluster is then used
to initiate a negative category?s lexicon. Target cate-
gories can also generate multiple negative categories
across different iterations.
In global discovery, all drifted terms are pooled
into a global cache, from which a single negative
category can be identified in an iteration. This is
based on our intuition that multiple target categories
359
TYPE MEDLINE
No. Terms 1 347 002
No. Patterns 4 090 412
No. 5-grams 72 796 760
No. Unfiltered tokens 6 642 802 776
Table 1: Filtered 5-gram dataset statistics.
may be drifting into similar semantic categories, and
enables these otherwise missed negative categories
to be established.
In the mixture discovery method, both global and
local negative categories can be formed. A cate-
gory?s drifted terms are collected into its local cache
as well as the global cache. Negative discovery is
then performed on each cache when they contain at
least 20 terms. Once a local negative category is
formed, the terms within the local cache are cleared
and also removed from the global cache. This pre-
vents multiple negative categories being instantiated
with overlapping seed terms.
4 Experimental setup
To compare the effectiveness of our negative discov-
ery approaches we consider the task of extracting
biomedical semantic lexicons from raw text.
4.1 Data
The algorithms take as input a set of candidate terms
to be extracted into semantic lexicons. The source
text collection consists of 5-grams (t1, t2, t3, t4, t5)
from approximately 16 million MEDLINE abstracts.1
The set of possible candidate terms correspond to
the middle tokens (t3), and the possible patterns are
formed from the surrounding tokens (t1, t2, t4, t5).
We do not use syntactic knowledge, as we did not
wish to rely on any tools that require supervised
training, to ensure our technique is as domain and
language independent as possible.
Limited preprocessing was required to extract the
5-grams from MEDLINE. The XML markup was
removed, and the collection was tokenised and split
into sentences using bio-specific NLP tools (Grover
et al, 2006). Filtering was applied to remove infre-
quent patterns and terms ? patterns appearing with
less than 7 different terms, and terms only appearing
1The set contains all MEDLINE titles and abstracts available
up to Oct 2007.
CAT DESCRIPTION
ANTI Antibodies: MAb IgG IgM rituximab infliximab
(?1:0.89, ?2:1.0)
CELL Cells: RBC HUVEC BAEC VSMC SMC (?1:0.91,
?2:1.0)
CLNE Cell lines: PC12 CHO HeLa Jurkat COS (?1:0.93,
?2: 1.0)
DISE Diseases: asthma hepatitis tuberculosis HIV malaria
(?1:0.98, ?2:1.0)
DRUG Drugs: acetylcholine carbachol heparin penicillin
tetracyclin (?1:0.86, ?2:0.99)
FUNC Molecular functions and processes: kinase ligase
acetyltransferase helicase binding (?1:0.87, ?2:0.99)
MUTN Protein and gene mutations: Leiden C677T C282Y
35delG null (?1:0.89, ?2:1.0)
PROT Proteins and genes: p53 actin collagen albumin IL-6
(?1:0.99, ?2:1.0)
SIGN Signs and symptoms: anemia fever hypertension
hyperglycemia cough (?1:0.96, ?2:0.99)
TUMR Tumors: lymphoma sarcoma melanoma osteosarcoma
neuroblastoma (?1:0.89, ?2:0.95)
Table 2: The MEDLINE semantic categories
with those patterns were removed. The statistics of
the resulting dataset are shown in Table 1.
4.2 Semantic categories
The semantic categories we extract from MEDLINE
were inspired by the TREC Genomics entities (Hersh
et al, 2007) and are described in detail in McIntosh
(2010). The hand-picked seeds selected by a domain
expert for each category are shown in italics in Table
2. These were carefully chosen to be as unambigu-
ous as possible with respect to the other categories.
4.3 Negative categories
In our experiments, we use two different sets of neg-
ative categories. These are shown in Table 3. The
first set corresponds to those used in McIntosh and
Curran (2008), and were identified by a domain ex-
pert as common sources of semantic drift in prelimi-
nary experiments with MEB and WMEB. The AMINO
ACID category was created in order to filter common
MUTN errors. The ANIMAL and BODY PART cate-
gories were formed with the intention of preventing
drift in the CELL, DISE and SIGN categories. The
ORGANISM category was then created to reduce the
new drift forming in the DISE category after the first
set of negative categories were introduced.
The second set of negative categories was identi-
fied by an independent domain expert with limited
360
CATEGORY SEED TERMS
1 AMINO ACID arginine cysteine glycine glutamate histamine
ANIMAL insect mammal mice mouse rats
BODY PART breast eye liver muscle spleen
ORGANISM Bartonella Borrelia Cryptosporidium
Salmonella toxoplasma
2 AMINO ACID Asn Gly His Leu Valine
ANIMAL animals dogs larvae rabbits rodents
ORGANISM Canidia Shigella Scedosporium Salmonella
Yersinia
GENERIC decrease effects events increase response
MODIFIERS acute deep intrauterine postoperative
secondary
PEOPLE children females men subjects women
SAMPLE biopsies CFU sample specimens tissues
Table 3: Manually crafted negative categories
knowledge of NLP and bootstrapping. This expert
identified three similar categories to the first expert,
however their seeds are very different. They also
identified three more categories than the first.
4.4 Lexicon evaluation
Our evaluation process follows that of McIntosh
and Curran (2009) and involved manually inspect-
ing each extracted term and judging whether it was
a member of the semantic class. This manual eval-
uation was performed by two domain experts and is
necessary due to the limited coverage of biomedical
resources. Inter-annotator agreement scores are pro-
vided in Table 2.2 To make later evaluations more
efficient, all evaluators? decisions for each category
are cached.
Unfamiliar terms were checked using online re-
sources including MEDLINE, MeSH, and Wikipedia.
Each ambiguous term was counted as correct if it
was classified into one of its correct categories, such
as lymphoma, which is a TUMR and DISE. If a term
was unambiguously part of a multi-word term we
considered it correct. Abbreviations, acronyms, and
obvious misspelled words were included.
For comparing the performance of the algorithms,
the average precision for the top-1000 terms over the
10 target categories is measured. To identify when
semantic drift has a significant impact, we report the
precision of specific sections of the lexicon, e.g. the
801-1000 sample corresponds to the last 200 terms.
2All disagreements were discussed, and the kappa scores ?1
and ?2 are those before and after the discussions, respectively.
1-500 1-1000
WMEB-DRIFT 74.3 68.6
+negative 1 87.7 82.8
+negative 2 83.8 77.8
Table 4: Influence of negative categories
4.5 System settings
All experiments were performed using the 10 tar-
get categories as input. Unless otherwise stated, no
hand-picked negative categories are used.
Each target category is initialised with the 5 hand-
picked seed terms (Table 2). In each iteration a max-
imum of 5 lexicon terms and 5 new patterns can
be extracted by a category. The bootstrapping al-
gorithms are run for 200 iterations.
The drift detection metric is calculated over the
first 100 terms and previous 5 terms extracted into
the lexicon, and the filter threshold is set to 0.2, as
in McIntosh and Curran (2009). To ensure infre-
quent terms are not used to seed negative categories,
drifted terms must occur at least 50 times to be re-
tained in the drift cache. Negative category discov-
ery is only initiated when the drifted cache contains
at least 20 terms, and a minimum of 5 terms are used
to seed a negative category.
4.6 Random seed experiments
Both McIntosh and Curran (2009) and Pantel et
al. (2009) have shown that a bootstrapper?s per-
formance can vary greatly depending on the input
seeds. To ensure our methods are compared reliably,
we also report the average precision of randomised
seed experiments. Each algorithm is instantiated 10
times with different random gold seeds for each tar-
get category. These gold seeds are randomly sam-
pled from the evaluation cache formed in McIntosh
and Curran (2009).
5 Results
5.1 Influence of negative categories
In our first experiments, we investigate the per-
formance variations and improvements gained us-
ing negative categories selected by two indepen-
dent domain experts. Table 4 shows WMEB-DRIFT?s
average precision over the 10 target categories with
and without the two negative category sets. Both
361
1-200 201-400 401-600 601-800 801-1000 1-1000
WMEB-DRIFT 79.5 74.8 64.7 61.9 62.1 68.6
NEG-FINDER
First discovered 79.5 74.3 64.8 67.8 66.6 70.7
Local discovery
+maximum 79.5 74.8 67.3 69.3 70.5 72.2
+outlier 79.5 73.9 64.8 67.8 71.0 71.5
Global discovery
+maximum 79.5 73.9 65.7 73.2 72.7 73.4
+outlier 79.5 74.7 65.6 71.4 68.2 72.1
Mixture discovery
+maximum 79.5 74.7 69.3 73.3 72.8 74.0
+outlier 79.5 75.2 69.7 72.0 69.4 73.2
Table 5: Performance comparison of WMEB-DRIFT and NEG-FINDER
sets significantly improve WMEB-DRIFT, however
there is a significant performance difference be-
tween them. This demonstrates the difficulty of se-
lecting appropriate negative categories and seeds for
the task, and in turn the necessity for tools to dis-
cover them automatically.
5.2 Negative category discovery
Table 5 compares the performance of NEG-FINDER
incorporated with WMEB-DRIFT. Each method has
equal average precision over the first 200 terms, as
semantic drift does not typically occur in the early
iterations. Each discovery method significantly out-
performs WMEB-DRIFT in the later stages, and over
the top 1000 terms.3
The first discovery approach corresponds to the
na??ve NEG-FINDER system that generates local neg-
ative categories from the first five drifted terms. Al-
though it outperforms WMEB-DRIFT, its advantage
is smaller than the clustering methods.
The outlier clustering approach, which we pre-
dicted to be the most effective, was surprisingly less
accurate than the maximum approach for selecting
negative seeds. This is because the seed cluster
formed around the outlier term is not guaranteed to
have high pair-wise similarity and thus it may repre-
sent multiple semantic categories.
Local discovery was the least effective discov-
ery approach. Compared to local discovery, global
discovery is capable of detecting new negative cate-
gories earlier, and the categories it detects are more
3Statistical significance was tested using computationally-
intensive randomisation tests (Cohen, 1995).
CATEGORY NEGATIVE SEEDS
CELL-NEG animals After Lambs Pigs Rabbits
TUMR-NEG inoperable multinodular nonresectable
operated unruptured
GLOBAL days Hz mM post Torr
GLOBAL aortas eyes legs mucosa retinas
GLOBAL men offspring parents persons relatives
GLOBAL Australian Belgian Dutch European Italian
GLOBAL Amblyospora Branhamella Phormodium
Pseudanabaena Rhodotorula
Table 6: Negative categories from mixture discovery
likely to compete with multiple target categories.
The NEG-FINDER mixture approach, which ben-
efits from both local and global discovery, identi-
fies the most useful negative categories. Table 6
shows the seven discovered categories ? two lo-
cal negative categories from CELL and TUMOUR,
and five global categories were formed. Many of
these categories are similar to those identified by
the domain experts. For example, clear categories
for ANIMAL, BODY PART, PEOPLE and ORGANISM
are created. By identifying and then including these
negative categories, NEG-FINDER significantly out-
performs WMEB-DRIFT by 5.4% over the top-1000
terms and by 10.7% over the last 200 terms, where
semantic drift is prominent. These results demon-
strate that suitable negative categories can be identi-
fied and exploited during bootstrapping.
5.3 Boosting hand-picked negative categories
In our next set of experiments, we investigate
whether NEG-FINDER can improve state-of-the-
art performance by identifying new negative cate-
gories in addition to the manually selected negative
362
1-200 201-400 401-600 601-800 801-1000 1-1000
WMEB-DRIFT
+negative 1 90.5 87.3 82.0 74.6 79.8 82.8
+negative 2 87.8 82.2 78.7 76.1 63.3 77.8
WMEB-DRIFT
+restart +local 85.5 82.6 76.5 75.7 68.5 78.4
+restart +global 84.0 83.8 79.1 74.8 69.5 79.7
+restart +mixture 85.2 85.0 82.3 72.5 72.7 81.4
Table 7: Performance of WMEB-DRIFT using negative categories discovered by NEG-FINDER
601-800 801-1000 1-1000
WMEB-DRIFT
+negative 1 74.6 79.8 82.8
NEG-FINDER
+negative 1 +local 76.4 80.1 83.2
+negative 1 +global 77.5 76.0 82.7
+negative 1 +mixture 76.7 79.9 83.2
Table 8: Performance of NEG-FINDER with manually
crafted negative categories
categories. Both NEG-FINDER and WMEB-DRIFT
are initialised with the 10 target categories and the
first set of negative categories.
Table 8 compares our best performing systems
(NEG-FINDER maximum clustering) with standard
WMEB-DRIFT, over the last 400 terms where seman-
tic drift dominates. NEG-FINDER effectively dis-
covers additional categories and significantly out-
performs WMEB-DRIFT. This further demonstrates
the utility of our approach.
5.4 Restarting with new negative categories
The performance improvements so far using NEG-
FINDER have been limited by the time at which new
negative categories are discovered and incorporated
into the bootstrapping process. That is, system im-
provements can only be gained from the negative
categories after they are generated. For example,
in Local NEG-FINDER, five negative categories are
discovered in iterations 83, 85, 126, 130 and 150.
On the other hand, in the WMEB-DRIFT +negative
experiments (Table 8 row 2), the hand-picked neg-
ative categories can start competing with the target
categories in the very first iteration of bootstrapping.
To test the full utility of NEG-FINDER, we use the
set of discovered categories as competing input for
WMEB-DRIFT. Table 7 shows the average precision
of WMEB-DRIFT over the 10 target categories when
it is restarted with the new negative categories dis-
covered from our three approaches (using maximum
clustering). Over the first 200 terms, significant im-
provements are gained using the new negative cate-
gories (+6%). However, the manually selected cat-
egories are far superior in preventing drift (+11%).
This may be attributed by the target categories not
strongly drifting into the new negative categories un-
til the later stages, whereas the hand-picked cate-
gories were selected on the basis of observed drift
in the early stages (over the first 500 terms).
Each NEG-FINDER approach significantly outper-
forms WMEB-DRIFT with no negative categories.
For example, using the NEG-FINDER mixture cat-
egories increases precision by 12.8%. These ap-
proaches also outperform their corresponding inline
discovery methods (e.g. +7.4% with mixture discov-
ery ? Table 5).
Table 7 shows that each of the discovered neg-
ative sets can significantly outperform the negative
categories selected by a domain expert (negative set
2) (+0.6 ? 3.9%). Our best system?s performance
(mixture: 81.4%) closely approaches that of the su-
perior negative set, trailing by only 1.4%.
5.5 Individual categories
In this section, we analyse the effect of NEG-FINDER
on the individual target categories. Table 9 shows
the average precision of the lexicons for some tar-
get categories. All categories, except TUMOUR, im-
prove significantly with the inclusion of the discov-
ered negative categories. In particular, the CELL
and SIGN categories, which are affected severely by
semantic drift, increase by up to 33.3% and 45.2%,
respectively. The discovered negative categories
are more effective than the manually crafted sets in
reducing semantic drift in the ANTIBODY, CELL and
DISEASE lexicons.
363
ANTI CELL DISE SIGN TUMR
WMEB-DRIFT 92.9 47.8 49.3 27.9 39.5
+negative 1 91.6 73.1 87.8 76.5 48.7
+negative 2 85.8 68.0 84.2 71.3 16.3
NEG-FINDER
+mixture 94.9 73.9 56.0 41.0 42.2
+mixture +negative 1 90.8 77.2 87.8 78.2 48.2
WMEB-DRIFT
+restart +local 89.9 78.8 71.6 73.1 32.2
+restart +global 94.6 79.0 81.9 62.6 35.2
+restart +mixture 92.6 81.1 91.1 63.6 47.5
Table 9: Individual category results (1-1000 terms)
5.6 Random seed experiments
In Table 10, we report the results of our randomised
experiments. Over the last 200 terms, WMEB-DRIFT
with the first set of negative categories (row 2) is out-
performed by NEG-FINDER (row 4). NEG-FINDER
also significantly boosts the performance of the orig-
inal negative categories by identifying additional
negative categories (row 5). Our final experiment,
where WMEB-DRIFT is re-initialised with the nega-
tive categories discovered by NEG-FINDER, further
demonstrates the utility of our method. On average,
the discovered negative categories significantly out-
perform the manually crafted negative categories.
6 Conclusion
In this paper, we have proposed the first completely
unsupervised approach to identifying the negative
categories that are necessary for bootstrapping large
yet precise semantic lexicons. Prior to this work,
negative categories were manually crafted by a do-
main expert, undermining the advantages of an un-
supervised bootstrapping paradigm.
There are numerous avenues for further examina-
tion. We intend to use sophisticated clustering meth-
ods, such as CBC (Pantel, 2003), to identify multiple
negative categories across the target categories in a
single iteration. We would also like to explore the
suitability of NEG-FINDER for relation extraction.
Our initial analysis demonstrated that although
excellent performance is achieved using negative
categories, large performance variations occur when
using categories crafted by different domain experts.
In NEG-FINDER, unsupervised clustering ap-
proaches are exploited to automatically discover
401-600 801-1000
WMEB-DRIFT 66.9 58.5
+negative 1 73.1 61.7
NEG-FINDER
+mixture 71.9 64.2
+mixture +negative 1 76.1 66.7
WMEB-DRIFT
+restart +mixture 78.0 70.8
Table 10: Random seed results
negative categories during bootstrapping. NEG-
FINDER identifies cohesive negative categories and
many of these are semantically similar to those iden-
tified by domain experts.
NEG-FINDER significantly outperforms the state-
of-the-art algorithm WMEB-DRIFT, before negative
categories are crafted, by up to 5.4% over the top-
1000 terms; and by 10.7% over the last 200 terms ex-
tracted, where semantic drift is extensive. The new
discovered categories can also be fully exploited in
bootstrapping, where they successfully outperform
a domain expert?s negative categories and approach
that of another expert.
The result is an effective approach that can be in-
corporated within any bootstrapper. NEG-FINDER
successfully removes the necessity of including
manually crafted supervised knowledge to boost a
bootstrapper?s performance. In doing so, we revert
the multi-category bootstrapping framework back to
its originally intended minimally supervised frame-
work, with little performance trade-off.
Acknowledgements
We would like to thank Dr Cassie Thornley, our
second evaluator; and the anonymous reviewers for
their helpful feedback. NICTA is funded by the Aus-
tralian Government as represented by the Depart-
ment of Broadband, Communications and the Dig-
ital Economy and the Australian Research Council
through the ICT Centre of Excellence program.
References
Andrew Carlson, Justin Betteridge, Richard C. Wang, Jr.
Estevam R. Hruschka, and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the third ACM interna-
tional conference on Web search and data mining,
pages 101?110, New York, NY, USA.
364
Paul R. Cohen. 1995. Empirical Methods for Artificial
Intelligence. MIT Press, Cambridge, MA, USA.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 172?180, Melbourne, Australia.
James R. Curran. 2004. From Distributional to Seman-
tic Similarity. Ph.D. thesis, University of Edinburgh,
Edinburgh, UK.
Claire Grover, Michael Matthews, and Richard Tobin.
2006. Tools to address the interdependence between
tokenisation and standoff annotation. In Proceed-
ings of the 5th Workshop on NLP and XML: Multi-
Dimensional Markup in Natural Language Process-
ing, pages 19?26, Trento, Italy.
William Hersh, Aaron M. Cohen, Lynn Ruslen, and
Phoebe M. Roberts. 2007. TREC 2007 Genomics
track overview. In Proceedings of the 16th Text RE-
trieval Conference, Gaithersburg, MD, USA.
Leonard Kaufmann and Peter J. Rousseeuw. 1990. Find-
ing Groups in Data: an Introdution to Cluster Analy-
sis. John Wiley and Sons.
Winston Lin, Roman Yangarber, and Ralph Grishman.
2003. Bootstrapped learning of semantic classes from
positive and negative examples. In Proceedings of
the ICML-2003 Workshop on The Continuum from La-
beled to Unlabeled Data, pages 103?111, Washington,
DC, USA.
Tara McIntosh and James R. Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceedings
of the Australasian Language Technology Association
Workshop, pages 97?105, Hobart, Australia.
Tara McIntosh and James R. Curran. 2009. Reducing
semantic drift with bagging and distributional similar-
ity. In Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics and the 4th
International Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing, pages 396?404, Suntec, Singapore.
Tara McIntosh. 2010. Reducing Semantic Drift in
Biomedical Lexicon Bootstrapping. Ph.D. thesis, Uni-
versity of Sydney.
Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-
chits, and Alpa Jain. 2006. Names and similarities on
the web: Fact extraction in the fast lane. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics, pages
809?816, Sydney, Australia.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 238?247, Sin-
gapore, Singapore.
Patrick Pantel. 2003. Clustering by Committee. Ph.D.
thesis, University of Alberta.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 41?47,
Philadelphia, PA, USA.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In Proceedings of the 16th National Conference
on Artificial Intelligence and the 11th Innovative Ap-
plications of Artificial Intelligence Conference, pages
474?479, Orlando, FL, USA.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Proceed-
ings of the Second Conference on Empirical Meth-
ods in Natural Language Processing, pages 117?124,
Providence, RI, USA.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 214?221, Philadelphia, PA, USA.
Roman Yangarber, Winston Lin, and Ralph Grishman.
2002. Unsupervised learning of generalized names. In
Proceedings of the 19th International Conference on
Computational Linguistics (COLING), pages 1135?
1141, San Francisco, CA, USA.
Hong Yu and Eugene Agichtein. 2003. Extracting syn-
onymous gene and protein terms from biological liter-
ature. Bioinformatics, 19(1):i340?i349.
365
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 266?270,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Relation Guided Bootstrapping of Semantic Lexicons
Tara McIntosh? Lars Yencken? James R. Curran? Timothy Baldwin?
? NICTA, Victoria Research Lab ? School of Information Technologies
Dept. of Computer Science and Software Engineering The University of Sydney
The University of Melbourne
nlp@taramcintosh.org james@it.usyd.edu.au
lars@yencken.org tb@ldwin.net
Abstract
State-of-the-art bootstrapping systems rely on
expert-crafted semantic constraints such as
negative categories to reduce semantic drift.
Unfortunately, their use introduces a substan-
tial amount of supervised knowledge. We
present the Relation Guided Bootstrapping
(RGB) algorithm, which simultaneously ex-
tracts lexicons and open relationships to guide
lexicon growth and reduce semantic drift.
This removes the necessity for manually craft-
ing category and relationship constraints, and
manually generating negative categories.
1 Introduction
Many approaches to extracting semantic lexicons
extend the unsupervised bootstrapping framework
(Riloff and Shepherd, 1997). These use a small set
of seed examples from the target lexicon to identify
contextual patterns which are then used to extract
new lexicon items (Riloff and Jones, 1999).
Bootstrappers are prone to semantic drift, caused
by selection of poor candidate terms or patterns
(Curran et al, 2007), which can be reduced by
semantically constraining the candidates. Multi-
category bootstrappers, such as NOMEN (Yangar-
ber et al, 2002) and WMEB (McIntosh and Curran,
2008), reduce semantic drift by extracting multiple
categories simultaneously in competition.
The inclusion of manually-crafted negative cate-
gories to multi-category bootstrappers achieves the
best results, by clarifying the boundaries between
categories (Yangarber et al, 2002). For exam-
ple, female names are often bootstrapped with
the negative categories flowers (e.g. Rose, Iris)
and gem stones (e.g. Ruby, Pearl) (Curran et al,
2007). Unfortunately, negative categories are dif-
ficult to design, introducing a substantial amount
of human expertise into an otherwise unsupervised
framework. McIntosh (2010) made some progress
towards automatically learning useful negative cate-
gories during bootstrapping.
In this work we identify an unsupervised source
of semantic constraints inspired by the Coupled Pat-
tern Learner (CPL, Carlson et al (2010)). In CPL,
relation bootstrapping is coupled with lexicon boot-
strapping in order to control semantic drift in the
target relation?s arguments. Semantic constraints
on categories and relations are manually crafted in
CPL. For example, a candidate of the relation IS-
CEOOF will only be extracted if its arguments can
be extracted into the ceo and company lexicons
and a ceo is constrained to not be a celebrity
or politician. Negative examples such as IS-
CEOOF(Sergey Brin, Google) are also introduced to
clarify boundary conditions. CPL employs a large
number of these manually-crafted constraints to im-
prove precision at the expense of recall (only 18 IS-
CEOOF instances were extracted). In our approach,
we exploit open relation bootstrapping to minimise
semantic drift, without any manual seeding of rela-
tions or pre-defined category lexicon combinations.
Orthogonal to these seeded and constraint-based
methods is the relation-independent Open Informa-
tion Extraction (OPENIE) paradigm. OPENIE sys-
tems, such as TEXTRUNNER (Banko et al, 2007),
define neither lexicon categories nor predefined re-
lationships. They extract relation tuples by exploit-
266
ing broad syntactic patterns that are likely to indi-
cate relations. This enables the extraction of inter-
esting and unanticipated relations from text. How-
ever these patterns are often too broad, resulting in
the extraction of tuples that do not represent rela-
tions at all. As a result, heavy (supervised) post-
processing or use of supervised information is nec-
essary. For example, Christensen et al (2010) im-
prove TEXTRUNNER precision by using deep pars-
ing information via semantic role labelling.
2 Relation Guided Bootstrapping
Rather than relying on manually-crafted category
and relation constraints, Relation Guided Bootstrap-
ping (RGB) automatically detects, seeds and boot-
straps open relations between the target categories.
These relations anchor categories together, e.g. IS-
CEOOF and ISFOUNDEROF anchor person and
company, preventing them from drifting into other
categories. Relations can also identify new terms.
We demonstrate that this relation guidance effec-
tively reduces semantic drift, with performance ap-
proaching manually-crafted constraints.
RGB can be applied to any multi-category boot-
strapper, and in these experiments we use WMEB
(McIntosh and Curran, 2008), as shown in Figure 1.
RGB alternates between two phases of WMEB, one
for terms and the other for relations, with a one-off
relation discovery phase in between.
Term Extraction
The first stage of RGB follows the term extraction
process of WMEB. Each category is initialised by a
set of hand-picked seed terms. In each iteration, a
category?s terms are used to identify candidate pat-
terns that can match the terms in the text. Seman-
tic drift is reduced by forcing the categories to be
mutually exclusive (i.e. patterns must be nominated
by only one category). The remaining patterns are
ranked according to reliability and relevance, and
the top-n patterns are then added to the pattern set.1
The reliability of a pattern for a given category is
the number of extracted terms in the category?s lex-
icon that match the pattern. A pattern?s relevance
weight is defined as the sum of the ?2 values be-
tween the pattern (p) and each of the lexicon terms
1In this work, n is set to 5.
WMEB
WMEB
lexicon
Person
get patterns
get terms
lexicon
Company
get patterns
get terms
relation
get patterns
get tuples
? ?
? ?
arg ? 
arg ? 
relation 
discovery
Lee Scott, Walmart
Sergey Brin, Google
Joe Bloggs, Walmart
T
e
r
m
 
e
x
t
r
a
c
t
i
o
n
R
e
l
a
t
i
o
n
 
e
x
t
r
a
c
t
i
o
n
Figure 1: Relation Guided Bootstrapping framework
(t): weight(p) =
?
t?T ?
2(p, t). These metrics are
symmetrical for both candidate terms and pattern.
In WMEB?s term selection phase, a category?s pat-
tern set is used to identify candidate terms. Like the
candidate patterns, terms matching multiple cate-
gories are excluded. The remaining terms are ranked
and the top-n terms are added to the lexicon.
Relation Discovery
In CPL (Carlson et al, 2010), a relation is instanti-
ated with manually-crafted seed tuples and patterns.
In RGB, the relations and their seeds are automati-
cally identified in relation discovery. Relation dis-
covery is only performed once after the first 20 iter-
ations of term extraction, which ensures the lexicons
have adequate coverage to form potential relations.
Each ordered pair of categories (C1, C2) = R1,2
is checked for open (not pre-defined) relations be-
tween C1 and C2. This check removes all pairs of
terms, tuples (t1, t2) ? C1 ? C2 with freq(t1, t2) <
5 and a cooccurrence score ?2(t1, t2) ? 0.2 If R1,2
has fewer than 10 remaining tuples, it is discarded.
The tuples for R1,2 are then used to find its ini-
tial set of relation patterns. Each pattern must match
more than one tuple and must be mutually exclusive
between the relations. If fewer than n relation pat-
terns are found forR1,2, it is discarded. At this stage
2This cut-off is used as the ?2 statistic is sensitive to low
frequencies.
267
TYPE 5gm 5gm + 4gm 5gm + DC
Terms 1 347 002
Patterns 4 090 412
Tuples 2 114 243 3 470 206 14 369 673
Relation Patterns 5 523 473 10 317 703 31 867 250
Table 1: Statistics of three filtered MEDLINE datasets
we have identified the open relations that link cate-
gories together and their initial extraction patterns.
Using the initial relation patterns, the top-n mu-
tually exclusive seed tuples are identified for the re-
lation R1,2. In CPL, these tuple seeds are manually
crafted. Note that R1,2 can represent multiple rela-
tions betweenC1 andC2, which may not apply to all
of the seeds, e.g. isCeoOf and isEmployedBy.
We discover two types of relations, inter-category
relations where C1 6= C2, and intra-category rela-
tions where C1 = C2.
Relation Extraction
The relation extraction phase involves running
WMEB over tuples rather than terms. If multiple re-
lations are found, e.g. R1,2 and R2,3, these are boot-
strapped simultaneously, competing with each other
for tuples and relation patterns. Mutual exclusion
constraints between the relations are also forced.
In each iteration, a relation?s set of tuples is used
to identify candidate relation patterns, as for term
extraction. The top-n non-overlapping patterns are
extracted for each relation, and are used to identify
the top-n candidate tuples. The tuples are scored
similarly to the relation patterns, and any tuple iden-
tified by multiple relations is excluded.
For tuple extraction, a relation R1,2 is constrained
to only consider candidates where either t1 or t2
has previously been extracted into C1 or C2, respec-
tively. To extract a candidate tuple with an unknown
term, the term must also be a valid candidate of its
associated category. That is, the term must match
at least one pattern assigned to the category and not
match patterns assigned to another category.
This type-checking anchors relations to the cat-
egories they link together, limiting their drift into
other relations. It also provides guided term growth
in the categories they link. The growth is ?guided?
because the relations define, semantically coher-
ent subregions of the category search spaces. For
example, ISCEOOF defines the subregion ceo
CAT DESCRIPTION
ANTI Antibodies: MAb IgG IgM rituximab infliximab
CELL Cells: RBC HUVEC BAEC VSMC SMC
CLNE Cell lines: PC12 CHO HeLa Jurkat COS
DISE Diseases: asthma hepatitis tuberculosis HIV malaria
DRUG Drugs: acetylcholine carbachol heparin penicillin
tetracyclin
FUNC Molecular functions and processes:
kinase ligase acetyltransferase helicase binding
MUTN Mutations: Leiden C677T C282Y 35delG null
PROT Proteins and genes: p53 actin collagen albumin IL-6
SIGN Signs and symptoms: anemia cough fever
hypertension hyperglycemia
TUMR Tumors: lymphoma sarcoma melanoma
neuroblastoma osteosarcoma
Table 2: The MEDLINE semantic categories
within person. This guidance reduces semantic
drift.
3 Experimental Setup
To compare the effectiveness of RGB we consider
the task of extracting biomedical semantic lexi-
cons, building on the work of McIntosh and Curran
(2008). Note however the method is equally appli-
cable to any corpus and set of semantic categories.
The corpus consists of approximately 18.5 mil-
lion MEDLINE abstracts (up to Nov 2009). The text
was tokenised and POS-tagged using bio-specific
NLP tools (Grover et al, 2006), and parsed using
the biomedical C&C CCG parser (Rimell and Clark,
2009; Clark and Curran, 2007).
The term extraction data is formed from the raw
5-grams (t1, t2, t3, t4, t5), where the set of candi-
date terms correspond to the middle tokens (t3) and
the patterns are formed from the surrounding tokens
(t1, t2, t4, t5). The relation extraction data is also
formed from the 5-grams. The candidate tuples cor-
respond to the tokens (t1, t5) and the patterns are
formed from the intervening tokens (t2, t3, t4).
The second relation dataset (5gm + 4gm), also in-
cludes length 2 patterns formed from 4-grams. The
final relation dataset (5gm + DC) includes depen-
dency chains up to length 5 as the patterns between
terms (Greenwood et al, 2005). These chains are
formed using the Stanford dependencies generated
by the Rimell and Clark (2009) parser. All candi-
dates occurring less than 10 times were filtered. The
sizes of the resulting datasets are shown in Table 1.
268
1-500 501-1000 1-1000
WMEB 76.1 56.4 66.3
+negative 86.9 68.7 77.8
intra-RGB 75.7 62.7 69.2
+negative 87.4 72.4 79.9
inter-RGB 80.5 69.9 75.1
+negative 87.7 76.4 82.0
mixed-RGB 74.7 69.9 72.3
+negative 87.9 73.5 80.7
Table 3: Performance comparison of WMEB and RGB
We follow McIntosh and Curran (2009) in us-
ing the 10 biomedical semantic categories and
their hand-picked seeds in Table 2, and manu-
ally crafted negative categories: amino acid,
animal, body part and organism. Our eval-
uation process involved manually judging each ex-
tracted term and we calculate the average precision
of the top-1000 terms over the 10 target categories.
We do not calculate recall, due to the open-ended
nature of the categories.
4 Results and Discussion
Table 3 compares the performance of WMEB and
RGB, with and without the negative categories. For
RGB, we compare intra-, inter- and mixed relation
types, and use the 5gm format of tuples and relation
patterns. In WMEB, drift dominates in the later iter-
ations with ?19% precision drop between the first
and last 500 terms. The manually-crafted negative
categories give a substantial boost in precision on
both the first and last 500 terms (+11.5% overall).
Over the top 1000 terms, RGB significantly out-
performs the corresponding WMEB with and with-
out negative categories (p < 0.05).3 In particu-
lar, inter-RGB significantly improves upon WMEB
with no negative categories (501-1000: +13.5%,
1-1000: +8.8%). In similar experiments, NEG-
FINDER, used during bootstrapping, was shown to
increase precision by ?5% (McIntosh, 2010). Inter-
RGB without negatives approaches the precision of
WMEB with the negatives, trailing only by 2.7%
overall. This demonstrates that RGB effectively re-
duces the reliance on manually-crafted negative cat-
egories for lexicon bootstrapping.
The use of intra-category relations was far less
3Significance was tested using intensive randomisation tests.
INTER-RGB 1-500 501-1000 1-1000
5gm 80.5 69.9 75.1
+negative 87.7 76.4 82.0
5gm + 4gm 79.6 71.5 75.5
+negative 87.7 76.1 81.9
5gm + DC 77.2 70.1 73.5
+negative 86.6 80.2 83.5
Table 4: Comparison of different relation pattern types
effective than inter-category relations, and the com-
bination of intra- and inter- was less effective than
just using inter-category relations. In intra-RGB the
categories are more susceptible to single-category
drift. The additional constraints provided by anchor-
ing two categories appear to make inter-RGB less
susceptible to drift. Many intra-category relations
represent listings commonly identified by conjunc-
tions. However, these patterns are identified by mul-
tiple intra-category relations and are excluded.
Through manual inspection of inter-RGB?s tuples
and patterns, we identified numerous meaningful re-
lations, such as isExpressedIn(prot, cell).
Relations like this helped to reduce semantic drift
within the CELL lexicon by up to 23%.
Table 4 compares the effect of different relation
pattern representations on the performance of inter-
RGB. The 5gm+4gm data, which doubles the num-
ber of possible candidate relation patterns, performs
similarly to the 5gm representation. Adding depen-
dency chains decreased and increased precision de-
pending on whether negative categories were used.
In Wu and Weld (2010), the performance of an
OPENIE system was significantly improved by us-
ing patterns formed from dependency parses. How-
ever in our DC experiments, the earlier bootstrap-
ping iterations were less precise than the simple
5gm+4gm and 5gm representations. Since the
chains can be as short as two dependencies, some
of these patterns may not be specific enough. These
results demonstrate that useful open relations can be
represented using only n-grams.
5 Conclusion
In this paper, we have proposed Relation Guided
Bootstrapping (RGB), an unsupervised approach to
discovering and seeding open relations to constrain
semantic lexicon bootstrapping.
269
Previous work used manually-crafted lexical and
relation constraints to improve relation extraction
(Carlson et al, 2010). We turn this idea on its head,
by using open relation extraction to provide con-
straints for lexicon bootstrapping, and automatically
discover the open relations and their seeds from the
expanding bootstrapped lexicons.
RGB effectively reduces semantic drift delivering
performance comparable to state-of-the-art systems
that rely on manually-crafted negative constraints.
Acknowledgements
We would like to thank Dr Cassie Thornley, our sec-
ond evaluator, and the reviewers for their helpful
feedback. NICTA is funded by the Australian Gov-
ernment as represented by the Department of Broad-
band, Communications and the Digital Economy
and the Australian Research Council through the
ICT Centre of Excellence program. This work has
been supported by the Australian Research Council
under Discovery Project DP1097291 and the Capital
Markets Cooperative Research Centre.
References
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence, pages 2670?2676, Hyderabad, India.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka, Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. In Proceedings of the Third ACM Interna-
tional Conference on Web Search and Data Mining,
pages 101?110, New York, USA.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2010. Semantic role labeling for
open information extraction. In Proceedings of the
NAACL HLT 2010 First International Workshop on
Formalisms and Methodology for Learning by Read-
ing, pages 52?60, Los Angeles, California, USA, June.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with ccg and log-
linear models. Computational Linguistics, 33(4):493?
552.
James R. Curran, Tara Murphy, and Bernhard Scholz.
2007. Minimising semantic drift with mutual exclu-
sion bootstrapping. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 172?180, Melbourne, Australia.
Mark A. Greenwood, Mark Stevenson, Yikun Guo, Henk
Harkema, and Angus Roberts. 2005. Automatically
acquiring a linguistically motivated genic interaction
extraction system. In Proceedings of the 4th Learn-
ing Language in Logic Workshop, pages 46?52, Bonn,
Germany.
Claire Grover, Michael Matthews, and Richard Tobin.
2006. Tools to address the interdependence between
tokenisation and standoff annotation. In Proceed-
ings of the 5th Workshop on NLP and XML: Multi-
Dimensional Markup in Natural Language Process-
ing, pages 19?26, Trento, Italy.
Tara McIntosh and James R. Curran. 2008. Weighted
mutual exclusion bootstrapping for domain indepen-
dent lexicon and template acquisition. In Proceedings
of the Australasian Language Technology Association
Workshop, pages 97?105, Hobart, Australia.
Tara McIntosh and James R. Curran. 2009. Reducing
semantic drift with bagging and distributional similar-
ity. In Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics and the 4th
International Conference on Natural Language Pro-
cessing of the Asian Federation of Natural Language
Processing, pages 396?404, Suntec, Singapore, Au-
gust.
Tara McIntosh. 2010. Unsupervised discovery of neg-
ative categories in lexicon bootstrapping. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 356?365,
Boston, USA.
Ellen Riloff and Rosie Jones. 1999. Learning dictionar-
ies for information extraction by multi-level bootstrap-
ping. In Proceedings of the 16th National Conference
on Artificial Intelligence and the 11th Innovative Ap-
plications of Artificial Intelligence Conference, pages
474?479, Orlando, USA.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Proceed-
ings of the Second Conference on Empirical Meth-
ods in Natural Language Processing, pages 117?124,
Providence, USA.
Laura Rimell and Stephen Clark. 2009. Porting a
lexicalized-grammar parser to the biomedical domain.
Journal of Biomedical Informatics, pages 852?865.
Fei Wu and Daniel S. Weld. 2010. Open information
extraction using wikipedia. In Proceedings of the 48th
Annual Meeting of the Association of Computational
Linguistics, pages 118?127, Uppsala, Sweden.
Roman Yangarber, Winston Lin, and Ralph Grishman.
2002. Unsupervised learning of generalized names. In
Proceedings of the 19th International Conference on
Computational Linguistics, pages 1135?1141, Taipei,
Taiwan.
270
