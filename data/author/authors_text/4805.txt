Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 169?172,
New York, June 2006. c?2006 Association for Computational Linguistics
Illuminating Trouble Tickets with Sublanguage Theory  
 
Svetlana Symonenko, Steven Rowe, Elizabeth D. Liddy 
Center for Natural Language Processing 
School Of Information Studies 
Syracuse University 
Syracuse, NY  13244 
{ssymonen, sarowe, liddy}@syr.edu 
  
Abstract 
A study was conducted to explore the poten-
tial of Natural Language Processing (NLP)-
based knowledge discovery approaches for 
the task of representing and exploiting the 
vital information contained in field service 
(trouble) tickets for a large utility provider. 
Analysis of a subset of tickets, guided by 
sublanguage theory, identified linguistic pat-
terns, which were translated into rule-based 
algorithms for automatic identification of 
tickets? discourse structure. The subsequent 
data mining experiments showed promising 
results, suggesting that sublanguage is an ef-
fective framework for the task of discovering 
the historical and predictive value of trouble 
ticket data. 
1 Introduction 
Corporate information systems that manage cus-
tomer reports of problems with products or ser-
vices have become common nowadays. Yet, the 
vast amount of data accumulated by these systems 
remains underutilized for the purposes of gaining 
proactive, adaptive insights into companies? busi-
ness operations. 
Unsurprising, then, is an increased interest by 
organizations in knowledge mining approaches to 
master this information for quality assurance or 
Customer Relationship Management (CRM) pur-
poses. Recent commercial developments include 
pattern-based extraction of important entities and 
relationships in the automotive domain (Attensity, 
2003) and text mining applications in the aviation 
domain (Provalis, 2005).    
This paper describes an exploratory feasibility 
study conducted for a large utility provider. The 
company was interested in knowledge discovery 
approaches applicable to the data aggregated by its 
Emergency Control System (ECS) in the form of 
field service tickets. When a ?problem? in the 
company?s electric, gas or steam distribution sys-
tem is reported to the corporate Call Center, a new 
ticket is created. A typical ticket contains the 
original report of the problem and steps taken to 
fix it. An operator also assigns a ticket an Original 
Trouble Type, which can be changed later, as addi-
tional information clarifies the nature of the prob-
lem. The last Trouble Type assigned to a ticket 
becomes its Actual Trouble Type. 
Each ticket combines structured and unstruc-
tured data. The structured portion comes from sev-
eral internal corporate information systems. The 
unstructured portion is entered by the operator who 
receives information over the phone from a person 
reporting a problem or a field worker fixing it. This 
free text constitutes the main material for the 
analysis, currently limited to known-item search 
using keywords and a few patterns. The company 
management grew dissatisfied with such an ap-
proach as time-consuming and, likely, missing out 
on emergent threats and opportunities or discover-
ing them too late. Furthermore, this approach lacks 
the ability to knit facts together across trouble 
tickets, except for grouping them by date or gross 
attributes, such as Trouble Types. The company 
management felt the need for a system, which, 
based on the semantic analysis of ticket texts, 
would not only identify items of interest at a more 
granular level, such as events, people, locations, 
dates, relationships, etc., but would also enable the 
discovery of unanticipated associations and trends. 
The feasibility study aimed to determine 
whether NLP-based approaches could deal with 
169
such homely, ungrammatical texts and then to ex-
plore various knowledge mining techniques that 
would meet the client?s needs. Initial analysis of a 
sample of data suggested that the goal could be 
effectively accomplished by looking at the data 
from the perspective of sublanguage theory.  
The novelty of our work is in combining sym-
bolic NLP and statistical approaches, guided by 
sublanguage theory, which results in an effective 
methodology and solution for such data. 
This paper describes analyses and experiments 
conducted and discusses the potential of the sub-
language approach for the task of tapping into the 
value of trouble ticket data. 
2 Related Research 
Sublanguage theory posits that texts produced 
within a certain discourse community exhibit 
shared, often unconventional, vocabulary and 
grammar (Grishman and Kittredge, 1986; Harris, 
1991). Sublanguage theory has been successfully 
applied in biomedicine (Friedman et al, 2002; 
Liddy et al, 1993), software development (Etzkorn 
et al, 1999), weather forecasting (Somers, 2003), 
and other domains. Trouble tickets exhibit a spe-
cial discourse structure, combining system-
generated, structured data and free-text sections; a 
special lexicon, full of acronyms, abbreviations 
and symbols; and consistent ?bending? of grammar 
rules in favor of speed writing (Johnson, 1992; 
Marlow, 2004). Our work has also been informed 
by the research on machine classification tech-
niques (Joachims, 2002; Yilmazel et al, 2005). 
3 Development of the sublanguage model  
The client provided us with a dataset of 162,105 
trouble tickets dating from 1995 to 2005. An im-
portant part of data preprocessing included token-
izing text strings. The tokenizer was adapted to fit 
the special features of the trouble tickets? vocabu-
lary and grammar: odd punctuation; name variants; 
domain-specific terms, phrases, and abbreviations.  
Development of a sublanguage model began 
with manual annotation and analysis of a sample of 
73 tickets, supplemented with n-gram analysis and 
contextual mining for particular terms and phrases. 
The analysis aimed to identify consistent linguistic 
patterns: domain-specific vocabulary (abbrevia-
tions, special terms); major ticket sections; and 
semantic components (people, organizations, loca-
tions, events, important concepts).  
The analysis resulted in compiling the core do-
main lexicon, which includes acronyms for Trou-
ble Types (SMH - smoking manhole); departments 
(EDS - Electric Distribution); locations (S/S/C - 
South of the South Curb); special terms (PACM - 
Possible Asbestos Containing Material); abbrevia-
tions (BSMNT - basement, F/UP - follow up); and 
fixed phrases (NO LIGHTS, WHITE HAT). Origi-
nally, the lexicon was intended to support the de-
velopment of the sublanguage grammar, but, since 
no such lexicon existed in the company, it can now 
enhance the corporate knowledge base. 
Review of the data revealed a consistent struc-
ture for trouble ticket discourse. A typical ticket 
(Fig.1) consists of several text blocks ending with 
an operator?s ID (12345 or JS). A ticket usually 
opens with a complaint (lines 001-002) that pro-
vides the original account of a problem and often 
contains: reporting entity (CONST MGMT), time-
stamp, short problem description, location. Field 
work (lines 009-010) normally includes the name 
of the assigned employee, new information about 
the problem, steps needed or taken, complications, 
etc. Lexical choices are limited and section-
specific; for instance, reporting a problem typically 
opens with REPORTS, CLAIMS, or CALLED.  
 
Figure 1. A sample trouble ticket 
The resulting typical structure of a trouble ticket 
(Table 1) includes sections distinct in their content 
and data format. 
Section Name Data 
Complaint Original report about the problem, Free-text 
Office Action 
Office Note 
Scheduling actions, Structured 
text 
Field Report Field work, Free-text  
Job Referral 
Job Completion 
Job Cancelled 
Referring actions, Closing actions, 
Structured text 
Table 1. Sample discourse structure of a ticket. 
170
Analysis also identified recurring semantic 
components: people, locations, problem, time-
stamp, equipment, urgency, etc. The annotation of 
tickets by sections (Fig.2) and semantic compo-
nents was validated with domain experts.  
 
Figure 2. Annotated ticket sections. 
The analysis became the basis for developing 
logical rules for automatic identification of ticket 
sections and selected semantic components. 
Evaluation of system performance on 70 manually 
annotated and 80 unseen tickets demonstrated high 
accuracy in automatic section identification, with 
an error rate of only 1.4%, and no significant dif-
ference between results on the annotated vs. un-
seen tickets. Next, the automatic annotator was run 
on the entire corpus of 162,105 tickets. The anno-
tated dataset was used in further experiments. 
Identification of semantic components brings 
together variations in names and spellings under a 
single ?normalized? term, thus streamlining and 
expanding coverage of subsequent data analysis. 
For example, strings UNSAFE LADDER, HAZ, 
(hazard) and PACM (Possible Asbestos Containing 
Material) are tagged and, thus, can be retrieved as 
hazard indicators. ?Normalization? is also applied 
to name variants for streets and departments.  
The primary value of the annotation is in effec-
tive extraction of structured information from these 
unstructured free texts. Such information can next 
be fed into a database and integrated with other 
data attributes for further analysis. This will sig-
nificantly expand the range and the coverage of 
data analysis techniques, currently employed by 
the company. 
The high accuracy in automatic identification of 
ticket sections and semantic components can, to a 
significant extent, be explained by the relatively 
limited number and high consistency of the identi-
fied linguistic constructions, which enabled their 
successful translation into a set of logical rules. 
This also supported our initial view of the ticket 
texts as exhibiting sublanguage characteristics, 
such as: distinct shared common vocabulary and 
constructions; extensive use of special symbols and 
abbreviations; and consistent bending of grammar 
in favor of shorthand. The sublanguage approach 
thus enables the system to recognize effectively a 
number of implicit semantic relationships in texts.  
4 Leveraging pattern-based approaches 
with statistical techniques 
Next, we assessed the potential of some knowledge 
discovery approaches to meet company needs and 
fit the nature of the data. 
4.1 Identifying Related Tickets 
When several reports relate to the same or recur-
ring trouble, or to multiple problems affecting the 
same area, a note is made in each ticket, e.g.:  
RELATED TO THE 21 ON E38ST TICKET 9999 
Each of these related tickets usually contains 
some aspects of the trouble (Figure 3), but current 
analytic approaches never brought them together to 
create a complete picture of the problem, which 
may provide for useful associations. Semantic 
component related-ticket is expressed through pre-
dictable linguistic patterns that can be used as lin-
guistic clues for automatic grouping of related 
tickets for further analysis. 
Ticket 1 
..REPORTS FDR-26M49 OPENED AUTO @ 16:54..  
OTHER TICKETS RELATED TO THIS JOB      
========= TICKET 2 =========== TICKET 3 = 
Ticket 2 
.. CEILING IS IN VERY BAD CONDITION AND IN 
DANGER OFCOLLAPSE. ? 
Ticket 3 
.. CONTRACTOR IS DOING FOUNDATION 
WATERPROOFINGWORK ...  
Figure 3. Related tickets 
4.2 Classification experiments 
The analysis of Trouble Type distribution revealed, 
much to the company?s surprise, that 18% of tick-
171
ets had the Miscellaneous (MSE) Type and, thus, 
remained out-of-scope for any analysis of associa-
tions between Trouble Types and semantic compo-
nents that would reveal trends. A number of 
reasons may account for this, including uniqueness 
of a problem or human error. Review of a sample 
of MSE tickets showed that some of them should 
have a more specific Trouble Type. For example 
(Figure 4), both tickets, each initially assigned the 
MSE type, describe the WL problem, but only one 
ticket later receives this code.  
Ticket 1 Original Code="MSE" Actual Code="WL" 
WATER LEAKING INTO TRANSFORMER BOX IN 
BASEMENT OF DORM; ?  
Ticket 2 Original Code ="MSE" Actual Code ="MSE" 
? WATER IS FLOWING INTO GRADING WHICH 
LEADS TO ELECTRICIAL VAULT.   
Figure 4. Complaint sections, WL-problem 
Results of n-gram analyses (Liddy et al, 2006), 
supported our hypothesis that different Trouble 
Types have distinct linguistic features. Next, we 
investigated if knowledge of these type-dependent 
linguistic patterns can help with assigning specific 
Types to MSE tickets. The task was conceptualized 
as a multi-label classification, where the system is 
trained on complaint sections of tickets belonging 
to specific Trouble Types and then tested on tickets 
belonging either to these Types or to the MSE 
Type. Experiments were run using the Extended 
LibSVM tool (Chang and Lin, 2001), modified for 
another project of ours (Yilmazel et al, 2005). 
Promising results of classification experiments, 
with precision and recall for known Trouble Types 
exceeding 95% (Liddy et al, 2006), can, to some 
extent, be attributed to the fairly stable and distinct 
language ? a sublanguage ? of the trouble tickets.  
5 Conclusion and Future Work 
Initial exploration of the Trouble Tickets revealed 
their strong sublanguage characteristics, such as: 
wide use of domain-specific terminology, abbre-
viations and phrases; odd grammar rules favoring 
shorthand; and special discourse structure reflec-
tive of the communicative purpose of the tickets. 
The identified linguistic patterns are sufficiently 
consistent across the data, so that they can be de-
scribed algorithmically to support effective auto-
mated identification of ticket sections and semantic 
components.  
Experimentation with classification algorithms 
shows that applying the sublanguage theoretical 
framework to the task of mining trouble ticket data 
appears to be a promising approach to the problem 
of reducing human error and, thus, expanding the 
scope of data amenable to data mining techniques 
that use Trouble Type information.  
Our directions for future research include ex-
perimenting with other machine learning tech-
niques, utilizing the newly-gained knowledge of 
the tickets? sublanguage grammar, as well as test-
ing sublanguage analysis technology on other types 
of field service reports. 
6 References 
Improving Product Quality Using Technician Com-
ments.2003. Attensity. 
Chang, C.-C. and Lin, C.-J. 2001. LIBSVM 
http://www.csie.ntu.edu.tw/~cjlin/libsvm. 
Etzkorn, L. H., Davis, C. G., and Bowen, L. L. 1999. 
The Language of Comments in Computer Software: 
A Sublanguage of English. Journal of Pragmatics, 
33(11): 1731-1756. 
Friedman, C., Kraa, P., and Rzhetskya, A. 2002. Two 
Biomedical Sublanguages:  a Description Based on 
the Theories of Zellig Harris. Journal of Biomedical 
Informatics, 35(4): 222-235. 
Grishman, R. and Kittredge, R. I. (Eds.). 1986. Analyz-
ing Language in Restricted Domains: Sublanguage 
Description and Processing. 
Harris, Z. A theory of language and information: a 
mathematical approach.  (1991). 
Joachims, T. Learning  to Classify Text using Support 
Vector Machines: Ph.D. Thesis  (2002). 
RFC 1297 - NOC Internal Integrated Trouble Ticket 
System Functional Specification Wishlist.1992. 
http://www.faqs.org/rfcs/rfc1297.html. 
Liddy, E. D., Jorgensen, C. L., Sibert, E. E., and Yu, E. 
S. 1993. A Sublanguage Approach to Natural Lan-
guage Processing for an Expert System. Information 
Processing & Management, 29(5): 633-645. 
Liddy, E. D., Symonenko, S., and Rowe, S. 2006. Sub-
language Analysis Applied to Trouble Tickets. 19th 
International FLAIRS Conference. 
Marlow, D. 2004. Investigating Technical Trouble Tick-
ets: An Analysis of a Homely CMC Genre. HICSS'37. 
Application of Statistical Content Analysis Text Mining 
to Airline Safety Reports.2005. Provalis. 
Somers, H. 2003. Sublanguage. In H. Somers (Ed.), 
Computers and Translation: A translator's guide. 
Yilmazel, O., Symonenko, S., Balasubramanian, N., and 
Liddy, E. D. 2005. Leveraging One-Class SVM and 
Semantic Analysis to Detect Anomalous Content. 
ISI/IEEE'05, Atlanta, GA. 
 
172
Evaluation of Restricted Domain Question-Answering Systems 
Anne R. Diekema, Ozgur Yilmazel, and Elizabeth D. Liddy 
Center for Natural Language Processing 
School of Information Studies 
Syracuse University 
4-206 Center for Science and Technology 
Syracuse, NY 13244 
{diekemar,liddy,oyilmaz}@syr.edu 
 
 
Abstract 
Question-Answering (QA) evaluation efforts 
have largely been tailored to open-domain 
systems. The TREC QA test collections contain 
newswire articles and the accompanying queries 
cover a wide variety of topics. While some 
apprehension about the limitations of restricted-
domain systems is no doubt justified, the strict 
promotion of unlimited domain QA evaluations 
may have some unintended consequences. 
Simply applying the open domain QA evaluation 
paradigm to a restricted-domain system poses 
problems in the areas of test question 
development, answer key creation, and test 
collection construction. This paper examines the 
evaluation requirements of restricted domain 
systems. It incorporates evaluation criteria 
identified by users of an operational QA system 
in the aerospace engineering domain. While the 
paper demonstrates that user-centered task-based 
evaluations are required for restricted domain 
systems, these evaluations are found to be 
equally applicable to open domain systems. 
1 Introduction 
The Text REtrieval Conference (TREC) 
organized the first QA evaluation (QA track) in 
1999 (Voorhees, 2000) and annual evaluations of 
this nature are ongoing (Voorhees, to appear). 
While the tasks and answer requirements have 
varied slightly from year to year, the purpose 
behind QA evaluations remains the same: to 
move from the traditional document retrieval to 
actual information retrieval by providing an 
answer to a question rather than a ranked list of 
relevant documents. The track was originally 
intended to bring together the fields of 
Information Extraction (IE) and Information 
Retrieval (IR). This legacy still continues in the 
factoid questions that require an IE type answer 
snippet in response, e.g.: ?What country is the 
Aswan High Dam located in?? This style of QA 
evaluation is spreading with very similar 
evaluations in Asia (Fukumoto, Kato, Masui, 
2003) and Europe (Magnini et al, 2003). 
Although these evaluations have a multilingual 
slant, they are strongly modeled after the TREC 
QA track. 
Typical QA systems that participate in these 
evaluations classify the questions into types 
which determine what kind of answer is required. 
After an initial retrieval of documents pertaining 
to the question, some form of text processing is 
then applied to identify possible answer 
sentences in the documents. Sentences that are 
near or contain keywords from the original 
question and contain the desired answer pattern 
are selected for answer extraction. Since it is 
difficult for systems to determine which part of 
the sentence is the correct answer, especially if it 
contains multiple extractions of the desired type, 
many systems have resorted to redundancy 
tactics (Banko et al, 2002; Buchholz, 2002). 
These systems use the Web as an answer 
verification tool by choosing the answer that 
appears most often together with the question 
keywords. While this technique is very 
successful in open domain evaluations, 
restricted-domain systems do not have the luxury 
of using redundancy, making these evaluations 
inappropriate for systems such as these. 
Our QA system participated in the three 
earlier TREC evaluations, e.g. (Diekema et al, 
2002). However, after starting work in the 
restricted-domain of re-usable launch vehicles, 
we found that the TREC evaluation no longer 
suited our system development needs and 
maintaining two different QA systems was too 
costly. 
 
 
2 Restricted-domain system 
characteristics  
The restricted-domain systems of today are 
different from the toy systems from the early 
years of QA (Voorhees and Tice, 2000), which 
might be what first comes to mind when reading 
the term ?restricted-domain?. Early systems like 
LUNAR (with a domain somewhat tangentially 
related to ours, namely lunar archeology) were 
developed by researchers in the field of natural 
language understanding. These early systems 
encoded large amounts of domain knowledge in 
databases. The restricted-domain systems of 
today are far less dependent on large knowledge 
bases and do not aim for language understanding 
per se. Rather, they use specialized extraction 
rules on a domain specific collection. The one 
thing that both types of restricted-domain 
systems have in common is that they are often 
developed with a certain goal or task in mind.  
As we will see later, this task orientation 
becomes equally important in the evaluation of 
these QA systems.  
An example of a modern-day restricted-
domain system is our Knowledge Acquisition 
and Access System (KAAS) QA system. The 
KAAS was developed for use in a collaborative 
learning environment (Advanced Interactive 
Discovery Environment for Engineering 
Education or AIDE) for undergraduate students 
from two universities majoring in aeronautical 
engineering. While students are working within 
the AIDE they can ask questions and quickly get 
answers. The collection against which the 
questions are searched consists of textbooks, 
technical papers, and websites that have been 
pre-selected for relevance and pedagogical value. 
The KAAS system uses a two-stage retrieval 
model to find answers in relevant passages. 
Relevant passages are processed by the Center 
for Natural Language Processing?s eQuery 
information extraction system using additional 
rules in the domain of reusable launch vehicles. 
Users are aided in their question formulations 
through domain specific query expansions. 
 
3 Initiating a restricted domain 
evaluation 
When it came time to evaluate the KAAS 
system, we initially defaulted to the TREC style 
QA evaluation with short, fact-based questions, 
adjudicated answers to these questions, and a test 
collection in which to find those answers. This 
choice of evaluation was not surprising since 
early versions of our system grew out of that 
environment. However, it quickly became 
apparent that this evaluation style posed 
problems for our restricted-domain, specific 
purpose system. 
Developing a set of test questions was easier 
said than done. Unlike the open domain 
evaluations, where test questions can be mined 
from question logs (Encarta, Excite, AskJeeves), 
no question sets are at the disposal of restricted-
domain evaluators. To build a set of test 
questions, we hired two sophomore aerospace 
engineering students. Based on class project 
papers of the previous semester and examples of 
TREC questions, the students were asked to 
create as many short factoid questions as they 
could, i.e ?What is APAS?? However, the real 
user questions that we collected later did not look 
anything like the short test questions in this 
initial evaluation set. The user questions were 
much more complex, e.g. ?How difficult is it to 
mold and shape graphite-epoxies compared with 
alloys or ceramics that may be used for thermal 
protective applications?? A more in depth 
analysis of KAAS question types can be found in 
Diekema et al (to appear). 
Establishing answers for the initial test 
questions proved difficult as well. The students 
did fine at collecting the questions that they had 
while reading the papers, but lacked sufficient 
domain expertise to establish answer correctness. 
Another issue was determining recall because it 
wasn?t always clear whether the (small) corpus 
simply did not contain the answer or whether the 
system was not able to find it. A third student, a 
doctoral student in aerospace engineering, was 
hired to help with these issues. To facilitate 
automatic evaluation we wanted to represent the 
answers in simple patterns but found that 
complex answers are not necessarily suitable for 
such a representation, even though patterns have 
proven feasible for TREC systems.  
While a newswire document collection for 
general domain evaluation is easy to find, a 
collection in our specialized domain needed to be 
created from scratch. Not only did the collection 
of documents take time, the conversion of most 
of these documents to text proved to be quite an 
unexpected hurdle as well. 
As is evident, the TREC style QA evaluation 
did not suit our restricted domain system. It also 
leaves out the user entirely. While information-
based evaluations are necessary to establish the 
ability of the system to answer questions 
correctly, we felt that they were not sufficient for 
evaluating a system with real users. 
4 User-based evaluation dimensions 
Restricted domain systems tend to be situated 
not only within a specific domain, but also within 
a certain user community and within a specific 
task domain. A generic evaluation is neither 
sufficient nor suitable for a restricted domain 
system. The environment in which KAAS is 
situated should drive the evaluation. Unlike 
many of the systems that participate in a TREC 
QA evaluation, the KAAS system has to function 
in real time with real users, not in batch mode 
with surrogate relevance assessors. This brings 
with it additional evaluation criteria such as 
utility and system speed (Nyberg and Mitamura, 
2003). 
KAAS users were asked in two separate 
surveys about their use and experiences with the 
system. The surveys were part of larger scale, 
cross-university course evaluations which looked 
at the students? perceptions of distance learning, 
collaboration at a distance, the collaborative 
software package, the KAAS, and each 
participating faculty member. While there was 
some structure and guidance in the user survey of 
the QA system, it was minimal and the survey is 
mainly characterized by the open nature of the 
responses. There were 25 to 30 students 
participating in each full course survey, but since 
we do not have the actual surveys that were 
turned in, we are not certain as to exactly how 
many students completed the survey section on 
the KAAS. However, it appears that most, if not 
all of the students provided feedback. 
Given the free text nature of the responses, it 
was decided that the three researchers would do a 
content analysis of the responses and 
independently derive a set of evaluation 
dimensions that they detected in the students? 
responses. Through content analysis of the user 
responses and follow-up discussion, we 
identified 5 main areas of importance to KAAS 
users when using the system: system 
performance, answers, database content, display, 
and expectations (see Figure 1). Each of the 
categories will be described in more detail 
below. 
4.1 System Performance 
System Performance is the category that deals 
with system speed and system availability. Users 
indicated that the speed with which answers were 
returned to them mattered. While they did not 
necessarily expect an immediate answer, they 
also did not want to wait, e.g. ?took so long, so I 
gave up?. Whenever users have a question, they 
want to find an answer immediately. If the 
system is down or not available to them at that 
moment, they will not come back later and try 
again. 
Possible system performance metrics are the 
?answer return rate?, and ?up time?. The answer 
return rate measures how long it takes (on 
average) to return an answer after the user has 
submitted a question.  ?Up-time? measures for a 
certain time period how often the system is 
available (system available time divided by the 
length of up-time time period). 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 Figure 1: User-based evaluation dimensions. 
 
 
1 System Performance 
1.1 Speed 
1.2 Availability / reliability / upness 
2 Answers  
2.1 Completeness 
2.2 Accuracy 
2.3 Relevance 
2.4 Applicability to task / utility / usefulness 
3 Database Content 
3.1 Authority / provenance / Source quality 
3.2 Scope /extensiveness / coverage 
3.3 Size 
3.4 Updatedness 
4 Display (UI) 
4.1 Input 
4.1.1 Question understanding / info need 
understanding 
4.1.2 Querying style 
4.1.2.1 Question 
4.1.2.1.1 NL query 
4.1.2.2 Keywords 
4.1.2.3 Browsing 
4.1.3 Question formulation assistance 
4.1.3.1 Spell Checker 
4.1.3.2 Abbreviation recognition 
4.2 Output 
4.2.1 Organization 
4.2.2 Feedback Solicitation 
5 Expectations 
5.1 Googleness 
 
4.2 Answers  
What users find important in an answer is 
captured in the Answers category. The users not 
only wanted answers to be accurate, they also 
wanted them to be complete and, something that 
is not tested at all in a regular evaluation, 
applicable to their task. e.g. ?in general what I 
received was helpful and accurate?, ?it [the 
system] was useful for the Columbia incident 
exercise??. 
Possible metrics concerning answers are 
?accuracy or correctness?, ?completeness?, 
?relevance?, or ?task suitability?. While the first 
three metrics are used in some shape or form in 
the TREC evaluations, ?task suitability? is not.  
Perhaps this measure requires a certain task 
description with a question to test whether the 
answer provided by the system allowed the user 
to complete the task. 
4.3 Database Content  
Users also shared thoughts about the Database 
Content or source documents that are searched 
for answers. They find it important that these 
documents are reputable. They also shared 
concerns about the size of the database, fearing 
that a limit in size would restrict the number of 
answerable questions, e.g. ?it needs more 
documents?. The same is true for the scope of the 
collection. Users desired extended coverage to 
ensure that a wide range of questions could be 
fielded by the collection, e.g. ?I found the data 
too limited in scope?. 
Possible database content metrics are 
?authority?, ?coverage?, ?size?, and ?up-to-
dateness?. To measure ?authority? one would 
first have to identify the core authors for a 
domain through citation analysis. Once that is 
established, one could measure the percentage of 
database content created by these core 
researchers. ?Coverage? could be measured in a 
similar way after the main research areas within 
a domain are identified.  ?Size? could simply be 
measured in megabytes or gigabytes. ?Up-to-
dateness? could be measured by calculating the 
number of articles per year or simply noting the 
date of the most recent article. 
 
4.4 User Interface 
The User Interface of a system was also found 
of importance. Users were critical about the way 
they were asked to input their questions. They 
did not always want to phrase their question as a 
question but sometimes preferred to use 
keywords, e.g. ?a keyword search would be more 
useful?. They also expected the system to prompt 
them with assistance in case they misspelled 
terms, or when the system did not understand the 
question, e.g ?sometimes very good at correcting 
you to what you need, other times not very 
good?. Users also care about the way in which 
the results are presented to them and whether the 
system desires any additional responses from 
them. They did not like being prompted for 
feedback on a document?s relevance for example, 
e.g. ??the ?was this useful? window was 
disruptive?. 
Measuring UI related aspects can be done 
through observation, questionnaires and 
interviews and does not typically result in actual 
metrics but rather a set of recommendations that 
can be implemented in the next version of the 
system. 
4.5 Expectations 
Another interesting aspect of user criteria is 
Expectations , e.g. ?the documents in the e-Query 
database were useful, but Google is much 
faster?. All users are familiar with Google and 
tend to have little patience with systems that 
have a different look and feel. 
Expectations can be captured by survey so 
that it can be established whether these 
expectations are reasonable and whether they can 
be met.   
5 Restricted domain QA Evaluation  
If we consider a restricted domain QA system 
as a system developed for a certain application, it 
is clear that these systems require a situated 
evaluation. The evaluation has to be situated in 
the task, domain, and user community for which 
the system is developed.  
How then can a restricted domain system best 
be evaluated? We believe that the evaluation 
should be driven by the dimensions identified by 
the users as important: system performance, 
answers, database content, display, and 
expectations. 
The system should be evaluated on its 
performance. How many seconds does it take to 
answer a question? Once the speed is known, one 
can determine how long users are willing to wait 
for an answer. It may very well be that the 
answer-finding capability of a system will need 
to be simplified in order to speed up the system 
and satisfy its users. Similarly, tests to determine 
robustness need to be part of the system 
performance evaluation. Users tend to shy away 
from systems that are periodically unavailable or 
slow to a crawl during peak usage hours.  
Systems should also be evaluated on their 
answer providing ability. This evaluation should 
include measures for answer completeness, 
accuracy, and relevancy. Test questions should 
be within the domain of the QA system in order 
to test the answer quality for that domain. 
Answers to certain questions require a more fine-
grained scoring procedure: answers that are 
explanations or summaries or biographies or 
comparative evaluations cannot be meaningfully 
rated as simply right or wrong. The answer 
providing capability should be evaluated in light 
of the task or purpose of the system. For 
example, users of the KAAS are learners in the 
field and are not well served with exact answer 
snippets. For their task, they need answer context 
information to be able to learn from the answer 
text.  
The evaluation should also include measures 
of the Database Content. Rather than assuming 
relevancy of a collection, it should be evaluated 
whether the content is regularly updated, whether 
the contents are of acceptable quality to the 
users, and whether the coverage of the restricted 
domain is extensive enough. 
Another system component that should be 
evaluated is the User Interface. Is the system 
easy to use? Does the interface provide clear 
guidance and/or assistance to the user? Does it 
allow users to search in multiple ways? 
Finally, it may be pertinent to evaluate how 
far the system goes in living up to user 
expectations. Although it is impossible to satisfy 
everybody, the system developers need to know 
whether there is a large discrepancy between user 
expectations and the actual system, since this 
may influence the use of the system. 
6 Cross-fertilization between evaluations 
How different are restricted-domain 
evaluations from open-domain evaluations? Are 
they so diametrically opposed that restricted-
domain systems require separate evaluations 
from open-domain systems and vice versa? As 
pointed out in Section 1, we stopped 
participating in the TREC QA evaluations 
because that evaluation was not well suited to 
our restricted-domain system. However, we 
regretted this as we believe we could, 
nevertheless, have gained valuable insights. 
Clearly, open-domain systems would benefit 
from the evaluation dimensions discussed in 
Section 4. The difference would be that the test 
questions used for evaluation would be general 
rather than tailored to a specific domain. 
Additionally, it may be harder to evaluate the 
database content (i.e. the collection) for a general 
domain system than would be the case for 
restricted-domain systems.  
To make open-domain evaluations more 
applicable to restricted-domain systems, they 
could be extended to include metrics about 
answer speed, and the ability of answering within 
a certain task. For example, the evaluation could 
include system performance to get an indication 
as to how much processing time, given certain 
hardware, is required in getting the answers. As 
for answer correctness itself, it may be 
interesting to require extensive use of task 
scenarios that would determine aspects such as 
answer length and level of detail. It may also be 
desirable to evaluate runs without redundancy 
techniques separately. Ideally, users would be 
incorporated into the evaluation to assess the user 
interface and the ability of the system to assist 
them in completion of a certain task. 
 
7 Summary 
Restricted-domain systems require a more 
situated evaluation than is generally provided in 
open-domain evaluations. A restricted-domain 
evaluation extends beyond domain specific test 
questions and collections to include the user and 
their task. Users of the restricted-domain KAAS 
system identified five areas that should be 
included in an evaluation: System Performance, 
Answers, Database Content, Display, and 
Expectations. Most of these evaluation 
dimensions could be applied to open-domain 
evaluations as well. Adding system performance 
metrics (such as answer speed) and specific task 
requirements may allow a convergence between 
open domain and restricted domain QA 
evaluations. 
 
Acknowledgements 
Funding for this research has been jointly provided by 
NASA,  NY State, and AT&T. 
 
References  
 
Banko, M., Brill, E., Dumais, S. and Lin, J. 2002.  
AskMSR: Question answering using the worldwide 
Web.  In Proceedings of the 2002 AAAI Spring 
Symposium on Mining Answers from Texts and 
Knowledge Bases, March 2002, Palo Alto, 
California.  
Buchholz, S. 2002. Using Grammatical Relations, 
Answer Frequencies and the World Wide Web for 
TREC Question Answering. In: E. M. Voorhees 
and D. K. Harman (Eds.), The Tenth Text REtrieval 
Conference (TREC 2001), volume 500-250 of 
NIST Special Publication, Gaithersburg, MD. 
National Institute of Standards and Technology, 
2002, pp. 502-509. 
Diekema, A.R., Chen, J., McCracken, N, Ozgencil, 
N.E., Taffet, M.D., Yilmazel, O. and Liddy, E.D. 
2002. Question Answering: CNLP at the TREC-
2002 Question Answering Track. In: Proceedings 
of the Eleventh Text Retrieval Conference (TREC-
2002). E.M. Voorhees and D.K. Harman (Eds.). 
Gaithersburg, MD: Department of Commerce, 
National Institute of Standards and Technology, 
2002. 
Fukumoto, J., Kato, T., and Masui, F. 2003. Question 
Answering Challenge (QAC-1): An Evaluation of 
Question Answering Tasks at the NTCIR 
Workshop 3. In Proceedings of the AAAI Spring 
Symposium: New Directions in Question 
Answering, p.122-133, 2003. 
Diekema, A.R., Yilmazel, O., Chen, J., Harwell, S., 
He, L., and Liddy, E.D. Finding Answers to 
Complex Questions. To appaer. In Maybury, M. 
(Ed.) New Directions in Question Answering. 
AAAI-MIT Press. 
Magnini, B., Romagnoli, S., Vallin, A., Herrera, J. 
Pe?as, A., Peinado, V., Verdejo, F., M. de Rijke, 
The Multiple Language Question Answering Track 
at CLEF 2003. In Carol Peters (Ed.), Working 
Notes for the CLEF 2003 Workshop, 21-22 August, 
Trondheim, Norway, 2003. 
Nyberg E. and T. Mitamura. 2002. Evaluating QA 
Systems on Multiple Dimensions. In Proceedings 
of LREC 2002 Workshop on QA Strategy and 
Resources, May 28th, Las Palmas, Gran Canaria. 
Voorhees, E.M. 2003. DRAFT Overview of the 
TREC 2003 Question Answering Track. To appear 
in Proceedings of TREC 2003. Gaithersburg, MD, 
NIST, to appear. 
Voorhees, E.M. Overview of the TREC-8 Question 
Answering Track Report. In Proceedings of TREC-
8, 77-82. Gaithersburg, MD, NIST, 2000. 
Voorhees, E.M. & Tice, D.M.  Implementing a 
Question Answering Evaluation. In Proceedings of 
LREC?2000 Workshop on Using Evaluation within 
HLT Programs: Results and Trends. 2000. 
Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 62?68,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Hands-On NLP for an Interdisciplinary Audience 
 
 
Elizabeth D. Liddy and Nancy J. McCracken 
Center for Natural Language Processing 
School of Information Studies 
Syracuse University 
liddy@syr.edu, njm@ecs.syr.edu 
 
 
 
 
Abstract 
The need for a single NLP offering for a 
diverse mix of graduate students (including 
computer scientists, information scientists, 
and linguists) has motivated us to develop a 
course that provides students with a breadth 
of understanding of the scope of real world 
applications, as well as depth of knowledge 
of the computational techniques on which 
to build in later experiences. We describe 
the three hands-on tasks for the course that 
have proven successful, namely: 1) in-class 
group simulations of computational proc-
esses;  2) team posters and public presenta-
tions on state-of-the-art commercial NLP 
applications, and; 3) team projects imple-
menting various levels of human language 
processing using open-source software on 
large textual collections. Methods of 
evaluation and indicators of success are 
also described. 
1 Introduction 
This paper presents both an overview and some of 
the details regarding audience, assignments, tech-
nology, and projects in an interdisciplinary course 
on Natural Language Processing that has evolved 
over time and been successful along multiple di-
mensions ? both from the students? and the fac-
ulty?s perspective in terms of accomplishments and 
enjoyment. This success has required us to meet 
the challenges of enabling students from a range of 
disciplines and diverse experience to each gain a 
real understanding of what is entailed in Natural 
Language Processing. 
2 A Course Within Multiple Curricula 
The course is entitled Natural Language Processing 
and is taught at the 600 graduate course level in a 
School of Information Studies in a mid to large-
size private university. While NLP is not core to 
any of the three graduate degree programs in the 
Information School, it is considered an important 
area within the Information School for both profes-
sional careers and advanced research, as well as in 
the Computer Science and Linguistic Programs on 
campus. The course has been taught every 1? to 2 
years for the last 18 years. While some aspects of 
the course have changed dramatically, particularly 
in regards to the nature of the student team pro-
jects, the basic structure ? the six levels of lan-
guage processing ? has remained essentially the 
same, with updates to topics within these levels 
reflecting recent research findings and new appli-
cations. 
3 Audience 
At the moment, this is the only course offering on 
NLP within the university, but a second-level, 
seminar course, entitled Content Analysis Research 
Using Natural Language Processing, geared to-
wards PhD students doing social science research 
on large textual data sets, will be offered for the 
first time in Fall 2005. Given that the current NLP 
course is the only one taught, it cannot, by neces-
sity, have the depth that could be achieved in cur-
ricula where there are multiple courses. In a more 
extensive curriculum, courses provide a greater 
depth than is possible in our single course.  Our 
goal is to provide students with a solid, broad basis 
on which to build in later experiences, and to en-
62
able real understanding of a complex topic for 
which students realize there is a much greater 
depth of understanding that could be reached. 
The disciplinary mix of students in the course is 
usually an even mix of information science and 
computer science students, with slightly fewer lin-
guistics majors. Recently the Linguistics Depart-
ment has established a concentration in 
Information Representation and Retrieval, for 
which the NLP course is a required course. Also, 
the course is cross-listed as an elective for com-
puter science graduate students. All of the above 
facts contribute to the widely diverse mix of stu-
dents in the NLP course, and has required us to 
develop a curriculum that enables all students to be 
successful in achieving solid competency in NLP. 
4 Topics Covered 
The topics in the course include typical ones cov-
ered in most NLP courses and are organized 
around the levels of language processing and the 
specific computational techniques within each of 
these. Discussions of more general theoretic no-
tions such as statistical vs. symbolic NLP, repre-
sentation theories, and language modeling are 
interspersed. A single example of topics that are 
taught within the levels of language processing 
include: 
 
Morphology - Finite state automata 
Lexicology - Part-of-speech tagging 
Syntax - Parsing with context free grammars 
Semantics - Word sense disambiguation 
Discourse - Sublanguage analysis 
Pragmatics - Gricean Maxims 
 
Each of the topics has assigned readings, from the 
course?s textbook, Speech and Language Process-
ing: An Introduction to Natural Language Process-
ing, Computational Linguistics, and Speech 
Recognition by Daniel Jurafsky & James H. Mar-
tin, as well as from recent and seminal papers. 
5 Methods 
What really enables the students to fully grasp the 
content of the course are the three important hands-
on features of the course, namely: 
1. Small, in-class group simulations of compu-
tational processes.   
2. Team posters and public presentations re-
porting on the state-of-the-art in commer-
cial NLP applications such as 
summarization, text mining, machine 
translation, question answering, speech 
recognition, and natural language genera-
tion.  
3. Team projects implementing various levels 
of human language processing using open-
source software on large collections. 
Each of these features of the course is described in 
some detail in the following sections.  
The course is designed around group projects, 
while the membership of the teams changes for 
each assignment. This is key to enabling a diverse 
group to learn to work with students from different 
disciplines and to value divergent experience. It 
has also proven extremely successful in forming a 
class that thinks of itself as a community and in 
encouraging sharing of best practices so that eve-
ryone advances their learning significantly further 
than if working alone or with the same team 
throughout the course. The way that teams are 
formed for the three types of projects varies, and 
will be described in each of the following three 
sections. 
Furthermore, constant, frequent presentations to 
the class of the group work, no matter how brief, 
enable students to own their newly-gained under-
standings. In fact, this course no longer requires 
any written papers, but instead focuses on applica-
tion of what is learned, first at the specific level of 
language processing, then to new data for new 
purposes, and then, to understanding real-world 
NLP systems performing various applications ? 
with the group constantly reporting their findings 
back to the class. 
5.1 In-class Group Simulations of Computa-
tional Processes 
During the first third of the course, lectures on 
each level of language processing are followed by 
a 30 to 45 minute exercise that enables the students 
who work in small groups to simulate the process 
they have just learned about, i.e. morphological 
analysis, part-of-speech tagging, or parsing some 
sample sentences with a small grammar. These 
groups are formed by the professor in an ad hoc 
manner by counting off by 4 in a different pattern 
each week to ensure that students work with stu-
63
dents on the other side of the room, given that 
friends or students from the same school tend to sit 
together. After the exercise, each group has 5 min-
utes to report back to the class on how they ap-
proached the task, with visuals.  
We?ve found that the formation of these small 
groups is pedagogically sound and enables learning 
in three ways. First, the groups break down social 
barriers and as the course advances the students 
find it much easier to work together and are more 
comfortable in sharing their work. Secondly, the 
students begin to understand and value what the 
students from different disciplines bring to bear on 
NLP problems. That is, the computer scientists 
recognize the value of the deeper understanding of 
language of the linguistic students, and the linguis-
tic students learn how the computer science stu-
dents approach the task computationally. Thirdly, 
while there were concerns on our part that these 
simulations might be too easy, the students have 
affirmed in mid-term course evaluations (which are 
not required, but do provide invaluable insight into 
a class?s engagement with and assimilation of the 
material) that these simulations really help them to 
understand conceptually what the task is and how 
it might be accomplished before they have to 
automate the processes. 
5.2 Real World Applications of NLP 
This year, two semester-long team projects were 
assigned ? the usual team-based computer imple-
mentation of NLP for a particular computational 
task ? and an investigation into how NLP is util-
ized in various state-of-the-art commercial NLP 
applications. The motivation for adding this second 
semester-long team project was that a number of 
the students in the course, particularly the masters 
students in Information Management, are most 
likely to encounter NLP in their work world when 
they need to advise on particular language-based 
applications. It has become clear, however, that as 
a result of this assignment, all of the students are 
quite pleased with their own improved ability to 
understand what a language-based technology is 
actually doing. Even if a student is more research-
focused, they are intrigued by what might be done 
to improve or add to a particular technology. 
Students are given two weeks to familiarize 
themselves outside of class with the suggested ap-
plications sufficiently to select a topic of real inter-
est to them. This year?s choices included Spell 
Correction, Machine Translation, Search Engines, 
Text Mining, Summarization, Question Answer-
ing, Speech Recognition, Cross-Language Infor-
mation Retrieval, Natural Language Generation, 
and Dialogue Agents.  
Students then sign up, on a first-come basis, for 
their preferred application. The teams are kept 
small (up to four) to ensure that each student con-
tributes. At times a single student is sufficiently 
interested in a topic that a team of one is formed. 
Students arrange their own division of labor. There 
are three 10 to 20 minute report-backs by each 
team over the course of the semester, the first two 
to the class and the final one during an open invita-
tion, school-wide Poster & Reception event. There 
are guidelines for each of the three presentations, 
as well as a stated expectation that the teams ac-
tively critique and comment on the presentations, 
both in terms of the information presented as well 
as presentational factors. Five minutes are allowed 
for class comments and students are graded on how 
actively they participate and provide feedback. 
The 1st presentation is a non-technical overview 
of what the particular NLP application does and 
includes examples of publicly available systems / 
products the class might know. The 2nd presenta-
tion covers technical details of the application, 
concentrating on the computational linguistic as-
pects, particularly how such an application typi-
cally works, and the levels of NL processing that 
are involved (e.g., lexical, syntactic, etc). The 3rd 
presentation involves a poster which incorporates 
the best of their first two presentations and sugges-
tions from the class, plus a laptop demo if possible. 
As stated above, the 3rd presentation is done in 
an open school-wide Poster and Reception event 
which is attended by faculty and students, mainly 
PhD students. The Poster Receptions have proven 
very successful along multiple dimensions ? first, 
the students take great pride in the work they are 
presenting;  second, posters are better than one-
time, in-class presentations as the multiple oppor-
tunities to explain their work and get feedback im-
prove the students? ability to create the best 
presentation of their work; third, the wider expo-
sure of the field and its applications builds an audi-
ence for future semesters and instills in the student 
body a sense of the reach and importance of NLP. 
5.3 Hands-On NL Processing of Text  
64
The second of the semester-long team projects is 
the computer implementation of NLP.  The goal of 
the project is for students to gain hands-on experi-
ence in utilizing NLP software in the context of 
accomplishing analysis of a large, real-world data 
set. The project comprises two tasks, each of which 
is reported back to the class by each team. These 
presentations were not initially in the syllabus, but 
interestingly, the students requested that each team 
present after each task so that they could all learn 
from the experiences of the other teams. 
The corpus chosen was the publicly available 
Enron email data set, which consists of about 
250,000 unique emails from 150 people. With du-
plication, the data has approximately 500,000 files 
and takes up 2.75 gigabytes. The data set was pre-
pared for public release by William Cohen at CMU 
and, available at http://www-2.cs.cmu.edu/~enron/. 
This data set is useful not only as real text of the 
email genre, but it can be easily divided into 
smaller subsets suitable for student projects. (And, 
of course, there is also the human interest factor in 
that the data set is available due to its use in the 
Enron court proceedings!) 
The goal of the project is to use increasing lev-
els of NLP to characterize a selected subset of En-
ron email texts. The project is designed to be 
carried out in two parts, involving two assigned 
levels of NLP. The first level, part-of-speech tag-
ging, is accomplished as Task 1 and the second, 
phrase-bracketing or chunk-parsing, is assigned as 
Task 2. However, the overall characterization of 
the text is left open-ended, and the student teams 
chose various dimensions for their analyses.  Pro-
jects included analyzing the topics of the emails of 
different people, social network analyses based on 
people and topics mentioned in the email text, and 
analyses based on author and recipient header in-
formation about each email. 
Teams are established for these projects by the 
professor based on the capabilities and interests of 
the individual students as reported in short self-
surveys. This resulted in teams on which there is a 
mix of computer science, linguistics and informa-
tion science expertise. The teams accomplished the 
tasks of choosing a data analysis method, process-
ing data subsets, designing NL processing to ac-
complish the analysis, programming the NL 
processing, conducting the data analysis, and pre-
paring the in-class reports. 
5.3.1 Tools Used in the Project  
For preliminary processing of the Enron email 
files, programs and data made available by Profes-
sor Andr?s Corrada-Emmanuel at the University of 
Massachusetts at Amherst, and available at 
http://ciir.cs.umass.edu/~corrada/ were used. The 
emails were assigned MD5-digest numbers in or-
der to identify them uniquely, and the data con-
sisted of mappings from the digest numbers to 
files, as well as to authors and recipients of the 
email. The programs contained filters that could be 
used to remove extraneous text such as headers and 
forwarded text. The teams adapted parts of these 
programs to convert the email files to files with 
text suitable for NL processing. 
For the NL processing, the Natural Language 
Toolkit (NL Toolkit or NLTK), developed at the 
University of Pennsylvania by Loper and Bird 
(2002), and available for download from Source-
Forge at http://nltk.sourceforge.net/ was used.  The 
NL Toolkit is a set of libraries written in the Py-
thon programming language that provides core 
data types for processing natural language text, 
support for statistical processing, and a number of 
standard processing algorithms used in NLP, in-
cluding tokenization, part of speech (POS) tagging, 
chunk parsing, and syntactic parsing. The toolkit 
provides demonstration packages, tutorials, exam-
ple corpora and documentation to support its use in 
educational classes.  Experience using the Toolkit 
shows that in order to use the NL Toolkit, one 
member of each team should have at least some 
programming background in order to write Python 
programs that use the NL Toolkit libraries.  The 
use of Python as the programming language was 
successful in that the level needed to use the NL 
Toolkit was manageable by the students with only 
a little programming background and in that the 
computer science students were able to adapt to the 
Python programming style and could easily utilize 
the classes and libraries. 
At the beginning of the term project, the stu-
dents were offered a lab session and lab materials 
to get them started. Since no one knew the Python 
programming language at the outset, there was an 
initial learning curve for the Python language as 
well as for the NL Toolkit. The lab materials pro-
vided to the students consisted of installation in-
structions for Python and NL Toolkit and a number 
of example programs that combined programming 
65
snippets from the NL Toolkit tutorials to process 
text through the NLP phases of tokenization, POS 
tagging and the construction of frequency distribu-
tions over the POS tagged text. During the lab ses-
sion, some of the example programs were worked 
through as a group with the goal of enabling the 
students to become competent in Python and to 
introduce them to the NL Toolkit tutorials that had 
additional materials. The NL Toolkit tutorials are 
extensive on the lower levels of NL processing 
(e.g. lexical and syntactic) and students with some 
programming background were able to utilize 
them. 
As part of their first task, the student teams were 
asked to select a subset of the Enron emails to 
work with. The entire Enron email directories were 
placed on a server for the teams to look at in mak-
ing their selections. The teams also used informa-
tion about the Enron employees as described in a 
paper by Corrada-Emmanuel (2005). Some student 
teams elected to work with different email topic 
folders for one person, while others chose a few 
email folders each from a small number of people 
(2-5). Their selected emails first needed to be 
processed to text using programs adapted from 
Corrada-Emmanuel. For the most part, the sub-
corpora choices of the student teams worked out 
well in terms of size and content. Several hundred 
emails turned out to be a good size, providing 
enough data to experience the challenges of long 
processing times and to appreciate why NLP is 
useful in processing large amounts of data, without 
being unduly overwhelmed. Initially, one team 
chose all the emails from several people. The 
number of email files involved was several thou-
sand and it took several hours to unzip those direc-
tories, let alne process them, and they 
subsequently reduced the number of files for their 
analysis. 
The first task was to analyze the chosen emails 
based solely on lexical level information, namely 
words with POS tags. NL Toolkit provides librar-
ies for tokenization where the user can define the 
tokens through regular expressions, and the stu-
dents used these to tailor the tokenization of their 
emails. The Toolkit also provides a regular expres-
sion POS tagger as well as n-gram taggers, and the 
students used these in combination for their POS 
tagging. Students experimented with the Brown 
corpus and a part of the Penn Treebank corpus, 
provided by NL Toolkit to train the POS taggers, 
and compared the results.  
Building on the first task, the second task ex-
tended the analysis of the chosen emails to phrases 
from the text. Again, NL Toolkit provides a library 
for chunk parsing where regular expressions can be 
used to specify patterns of words with POS tags 
either to be included or excluded from phrases. 
Since chunk parsing depends on POS tagging, 
there was a need for a larger training corpus. A 
research center within the Information School has 
a license for Penn Treebank, and  provided addi-
tional Penn Treebank files for the class to use for 
that purpose. Most teams used regular expressions 
to bracket proper names, minimal noun phrases, 
and verb phrases. One team used these to group 
maximal noun phrases, and another team used 
regular expressions to find patterns of communica-
tion verbs for use in social network analysis.   
In retrospect, it was found that the chunk pars-
ing did not take the teams far enough in NLP 
analysis of text. Experience in teaching using the 
NL Toolkit suggests that use of the syntactic pars-
ing libraries to find more complex structures in the 
text would have provided more depth of analysis. 
Students also suggested that they would have liked 
to incorporate semantic level capabilities, such as 
the use of WordNet to find conceptual groupings 
via synonym recognition. The next offering of the 
course will include these improvements. 
Using the NL Toolkit for NL processing worked 
out well overall and enabled the students to ob-
serve and appreciate details of the processing steps 
without having to write a program for every algo-
rithm themselves. The tutorials are good, both at 
explaining concepts and providing programming 
examples. There were a few places where some 
data structure details did not seem to be suffi-
ciently documented, either in the tutorials or in the 
API.  This was true for  the recently added Brill 
POS tagger, and is likely due to its recency of ad-
dition to the toolkit.  However for the most part, 
the coverage of the documentation is impressive. 
6 Evaluation  
Multiple types of evaluation are associated with 
the course. First, the typical evaluation of the stu-
dents by the professor (here, 2 professors) was 
done on multiple dimensions that contributed pro-
portionately to the student?s final grade as follows: 
66
 
? In-Class group exercises 20% 
? NLToolkit Team Assignments 35% 
? NLP Application Team Poster &  
         Presentations 
35% 
? Contributions to class discussion  
         (both quality and quantity) 
10% 
 
Additionally, each team member evaluated each of 
their fellow team members as well as themselves. 
This was done for both of the teams in which a 
student participated. For each team member, the 
questions covered:  the role or tasks of the student 
on the project; an overall performance rating from 
1 for POOR to 4 for EXCELLENT; the rationale 
for this score, and finally; what the student could 
have done to improve their contribution. Knowl-
edge of this end-of-semester team self-evaluation 
tended to ensure that students were active team 
contributors. 
The professor was also evaluated by the stu-
dents. And while there are quantitative scores that 
are used by the university for comparison across 
faculty and to track individual faculty improve-
ments over time, the most useful feature of the stu-
dent evaluations is the set of open-ended questions 
concerning what worked well in the course, what 
didn?t work well, and what could be done to im-
prove the course. Over the years of teaching this 
course, these comments (plus the mid-term evalua-
tions) have been most instructive in efforts to find 
ways to improve the course. Frequently the sugges-
tions are very practical and easy to implement, 
such as showing a chart with the distribution of 
grades on each assignment when they are returned 
so that the students know where they stand relative 
to the class as grading is on a scale of 1 to 10. 
 
7. Indicators of Success 
 
Finally, how is the success of this course measured 
in the longer term?  For this, success is measured 
by:  whether students elect to do continued work in 
NLP, either in the context of further courses in 
which NLP is utilized, such as Information Re-
trieval or Text Mining;  whether the masters (and 
undergraduate) students decide to pursue an ad-
vanced degree based on the excitement engendered 
and knowledge gained from the NLP course; or 
whether PhD students elect to do continued re-
search either in the school?s Center for Natural 
Language Processing or as part of their disserta-
tion. For students in a terminal degree program, 
success is reflected by their seeking and obtaining 
jobs that utilize the NLP they have learned in the 
course and that has provided them with a solid, 
broad basis on which to build. For several of the 
undergraduate computer science students in the 
course, their NLP experience has given them an 
added dimension of specialization and competitive 
advantage in a tight hiring market.  
An additional measure of success was the re-
quest by the doctoral students in the home school 
for a PhD level seminar course to build on the NLP 
course. This course is entitled Content Analysis 
Research Using Natural Language Processing and 
will enable PhD students doing social science re-
search on large textual data sets to explore and ap-
ply the NLP tools that are developed within the 
school, as well as to understand how these NLP 
tools can be successfully interleaved with commer-
cial content analysis tools to support rich explora-
tion of their data. As is the current course, this 
seminar will be open to PhD students from all 
schools across campus and already has enrollees 
from public policy, communications, and man-
agement, as well as information science. 
 
8. Summary 
 
While it might appear that a disproportionate 
amount of thought and attention is given to the 
more human and social aspects of designing and 
conducting this course, experience shows that such 
attention is the key to the success of this diverse 
body of students in learning and understanding the 
content of the course. Furthermore, given the great 
diversity in class-level and disciplinary back-
ground of students, this attention to structuring the 
course has paid off in the multiple ways exempli-
fied above. While it is obvious that a course for 
computer-science majors alone would be designed 
quite differently, it would not provide the enriched 
understanding of the field of NLP and its applica-
tion value that is possible with the contributions by 
the variety of disciplines brought together in this 
course. 
Acknowledgements 
67
We would like to acknowledge the contributions of 
the students in all the classes over the years whose 
efforts and suggestions have continually improved 
the course. We would to especially acknowledge 
this year?s class, who were especially contributory 
of ideas for improving and building on a currently 
successful course, namely Agnieszka Kwiat-
kowska, Anatoliy Gruzd, Carol Schwartz, Cun-
Fang Cheng, Freddie Wade, Joshua Legler, Kei-
suke Inoue, Matthew Wolf, Michael Fudge, Michel 
Tinuiri, Olga Azarova, Rebecca Gilbert, Shuyuan 
Ho, Tuncer Can, Xiaozhony Liu, and Xue Xiao. 
References  
Loper, E. & Bird, S., 2002.  NLTK, the Natural 
Language Toolkit. In Proceedings of the ACL 
Workshop on Effective Tools and Methodolo-
gies for Teaching Natural Language Processing 
and Computational Linguistics. Philadelphia: 
Association for Computational Linguistics. 
 
Corrada-Emmanuel, A. McCallum, A., Smyth, P., 
Steyvers, M. & Chemudugunta, C., 2005. Social 
Network Analysis and Topic Discovery for the 
Enron Email Dataset. In Proceedings of the 
Workshop on Link Analysis, Counterterrorism 
and Security at 2005 SIAM International Con-
ference in Data Mining. 
68
Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 17?24,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
Modeling Reference Interviews as a Basis for Improving Automatic QA 
Systems 
 
Nancy J. McCracken, Anne R. Diekema, Grant Ingersoll, Sarah C. 
Harwell, Eileen E. Allen, Ozgur Yilmazel, Elizabeth D. Liddy 
Center for Natural Language Processing 
Syracuse University 
Syracuse, NY 13244 
{ njmccrac, diekemar, gsingers, scharwel, eeallen, oyilmaz, liddy}@syr.edu 
 
 
 
 
 
Abstract 
 
The automatic QA system described in 
this paper uses a reference interview 
model to allow the user to guide and 
contribute to the QA process.  A set of 
system capabilities was designed and 
implemented that defines how the user?s 
contributions can help improve the 
system.  These include tools, called the 
Query Template Builder and the 
Knowledge Base Builder, that tailor the 
document processing and QA system to 
a particular domain by allowing a 
Subject Matter Expert to contribute to 
the query representation and to the 
domain knowledge.  During the QA 
process, the system can interact with the 
user to improve query terminology by 
using Spell Checking, Answer Type 
verification, Expansions and Acronym 
Clarifications.  The system also has 
capabilities that depend upon, and 
expand the user?s history of interaction 
with the system, including a User 
Profile, Reference Resolution, and 
Question Similarity modules 
 
 
1  Introduction 
 
Reference librarians have successfully fielded 
questions of all types for years using the Reference 
Interview to clarify an unfocused question, narrow 
a broad question, and suggest further information 
that the user might not have thought to ask for.  
The reference interview tries to elicit sufficient 
information about the user?s real need to enable a 
librarian to understand the question enough to 
begin searching.  The question is clarified, made 
more specific, and contextualized with relevant 
detail.  Real questions from real users are often 
?ill-formed? with respect to the information 
system; that is, they do not match the structure of 
?expectations? of the system (Ross et al, 2002). A 
reference interview translates the user?s question 
into a representation that the librarian and the 
library systems can interpret correctly. The human 
reference interview process provides an ideal, 
well-tested model of how questioner and answerer 
work together co-operatively and, we believe, can 
be successfully applied to the digital environment.  
The findings of researchers applying this model in 
online situations (Bates, 1989, Straw, 2004) have 
enabled us to understand how a system might work 
with the user to provide accurate and relevant 
answers to complex questions. 
 Our long term goal in developing Question-
Answering (QA) systems for various user groups is 
to permit, and encourage users to positively 
contribute to the QA process, to more nearly 
mirror what occurs in the reference interview, and 
to develop an automatic QA system that provides 
fuller, more appropriate, individually tailored 
responses than has been available to date. 
 Building on our Natural Language 
Processing (NLP) experience in a range of 
information access applications, we have focused 
our QA work in two areas:  1) modeling the subject 
domain of the collections of interest to a set of 
 
 
 
17
users for whom we are developing the QA system, 
and; 2) modeling the query clarification and 
negotiation interaction between the information 
seeker and the information provider. Examples of 
these implementation environments are: 
 
1. Undergraduate aerospace engineering students 
working in collaborative teams on course 
projects designing reusable launch vehicles, 
who use a QA system in their course-related 
research. 
2. Customers of online business sites who use a 
QA system to learn more about the products or 
services provided by the company, or who 
wish to resolve issues concerning products or 
service delivery. 
 
In this paper, we describe the capabilities we 
have developed for these specific projects in order 
to explicate a more general picture of how we 
model and utilize both the domains of inquiry and 
typical interaction processes observed in these 
diverse user groups. 
 
2 Background and related research 
 
Our work in this paper is based on two premises: 
1) user questions and responsive answers need to 
be understood within a larger model of the user?s 
information needs and requirements, and, 2) a 
good interactive QA system facilitates a dialogue 
with its users to ensure it understands and satisfies 
these information needs. The first premise is based 
on the long-tested and successful model of the 
reference interview (Bates, 1997, Straw, 2004), 
which was again validated by the findings of an 
ARDA-sponsored workshop to increase the 
research community?s understanding of the 
information seeking needs and cognitive processes 
of intelligence analysts (Liddy, 2003). The second 
premise instantiates this model within the digital 
and distributed information environment. 
 Interactive QA assumes an interaction 
between the human and the computer, typically 
through a combination of a clarification dialogue 
and user modeling to capture previous interactions 
of users with the system. De Boni et al (2005) 
view the clarification dialogue mainly as the 
presence or absence of a relationship between the 
question from the user and the answer provided by 
the system. For example, a user may ask a 
question, receive an answer and ask another 
question in order to clarify the meaning, or, the 
user may ask an additional question which expands 
on the previous answer. In their research De Boni 
et al (2005) try to determine automatically 
whether or not there exists a relationship between a 
current question and preceding questions, and if 
there is a relationship, they use this additional 
information in order to determine the correct 
answer.  
 We prefer to view the clarification dialogue 
as more two-sided, where the system and the user 
actually enter a dialogue, similar to the reference 
interview as carried out by reference librarians 
(Diekema et al, 2004). The traditional reference 
interview is a cyclical process in which the 
questioner poses their question, the librarian (or the 
system) questions the questioner, then locates the 
answer based on information provided by the 
questioner, and returns an answer to the user who 
then determines whether this has satisfied their 
information need or whether further clarification or 
further questions are needed.  The HITIQA 
system?s (Small et al, 2004) view of a clarification 
system is closely related to ours?their dialogue 
aligns the understanding of the question between 
system and user. Their research describes three 
types of dialogue strategies: 1) narrowing the 
dialogue, 2) broadening the dialogue, and 3) a fact 
seeking dialogue. 
 Similar research was carried out by Hori et 
al. (2003), although their system automatically 
determines whether there is a need for a dialogue, 
not the user. The system identifies ambiguous 
questions (i.e. questions to which the system could 
not find an answer). By gathering additional 
information, the researchers believe that the system 
can find answers to these questions. Clarifying 
questions are automatically generated based on the 
ambiguous question to solicit additional 
information from the user. This process is 
completely automated and based on templates that 
generate the questions. Still, removing the 
cognitive burden from the user through automation 
is not easy to implement and can be the cause of 
error or misunderstanding. Increasing user 
involvement may help to reduce this error. 
 As described above, it can be seen that 
interactive QA systems have various levels of 
dialogue automation ranging from fully automatic 
(De Boni et al, 2004, Hori et al, 2004) to a strong 
18
user involvement (Small et al, 2004, Diekema et 
al., 2004). Some research suggests that 
clarification dialogues in open-domain systems are 
more unpredictable than those in restricted domain 
systems, the latter lending itself better to 
automation (Hori et al, 2003, J?nsson et al, 2004).  
Incorporating the user?s inherent knowledge of the 
intention of their query is quite feasible in 
restricted domain systems and should improve the 
quality of answers returned, and make the 
experience of the user a less frustrating one. While 
many of the systems described above are 
promising in terms of IQA, we believe that 
incorporating knowledge of the user in the 
question negotiation dialogue is key to developing 
a more accurate and satisfying QA system.   
 
3 System Capabilities 
 
In order to increase the contribution of users to our 
question answering system, we expanded our 
traditional domain independent QA system by 
adding new capabilities that support system-user 
interaction. 
 
3.1  Domain Independent QA 
 
Our traditional domain-independent QA capability 
functions in two stages, the first information 
retrieval stage selecting a set of candidate 
documents, the second stage doing the answer 
finding within the filtered set.  The answer finding 
process draws on models of question types and 
document-based knowledge to seek answers 
without additional feedback from the user.  Again, 
drawing on the modeling of questions as they 
interact with the domain representation, the system 
returns answers of variable lengths on the fly in 
response to the nature of the question since factoid 
questions may be answered with a short answer, 
but complex questions often require longer 
answers.  In addition, since our QA projects were 
based on closed collections, and since closed 
collections may not provide enough redundancy to 
allow for short answers to be returned, the variable 
length answer capability assists in finding answers 
to factoid questions.  The QA system provides 
answers in the form of short answers, sentences, 
and answer-providing passages, as well as links to 
the full answer-providing documents. The user can 
provide relevance feedback by selecting the full 
documents that offer the best information.  Using 
this feedback, the system can reformulate the 
question and look for a better set of documents 
from which to find an answer to the question. 
Multiple answers can be returned, giving the user a 
more complete picture of the information held 
within the collection.   
 One of our first tactics to assist in both 
question and domain modeling for specific user 
needs was to develop tools for Subject Matter 
Experts (SMEs) to tailor our QA systems to a 
particular domain.  Of particular interest to the 
interactive QA community is the Query Template 
Builder (QTB) and the Knowledge Base Builder 
(KBB).  
 Both tools allow a priori alterations to 
question and domain modeling for a community, 
but are not sensitive to particular users.  Then the 
interactive QA system permits question- and user-
specific tailoring of system behavior simply 
because it allows subject matter experts to change 
the way the system understands their need at the 
time of the search. 
 Question Template Builder (QTB) allows 
a subject matter expert to fine tune a question 
representation by adding or removing stopwords 
on a question-by-question basis, adding or masking 
expansions, or changing the answer focus.  The 
QTB displays a list of Question-Answer types, 
allows the addition of new Answer Types, and 
allows users to select the expected answer type for 
specific questions.  For example, the subject matter 
expert may want to adjust particular ?who? 
questions as to whether the expected answer type is 
?person? or ?organization?.  The QTB enables 
organizations to identify questions for which they 
want human intervention and to build specialized 
term expansion sets for terms in the collection.  
They can also adjust the stop word list, and refine 
and build the Frequently or Previously Asked 
Question (FAQ/PAQ) collection. 
 Knowledge Base Builder (KBB) is a suite 
of tools developed for both commercial and 
government customers.  It allows the users to view 
and extract terminology that resides in their 
document collections.  It provides useful statistics 
about the corpus that may indicate portions that 
require attention in customization.  It collects 
frequent / important terms with categorizations to 
enable ontology building (semi-automatic, 
permitting human review), term collocation for use 
19
in identifying which sense of a word is used in the 
collection for use in term expansion and 
categorization review.  KBB allows companies to 
tailor the QA system to the domain vocabulary and 
important concept types for their market.  Users 
are able to customize their QA applications 
through human-assisted automatic procedures.  
The Knowledge Bases built with the tools are  
 
 
IR Answer Providers
Question 
Processing
Session 
Tracking
Reference 
Resolution
User Profile
Question 
Similarity
User
Answer
Spell 
checking
Answer 
Type 
Verification
Expansion 
Clarification
Domain Modeling
QTB KBB
 
Figure 1. System overview 
 
 
primarily lexical semantic taxonomic resources.  
These are used by the system in creating frame 
representations of the text.  Using automatically 
harvested data, customers can review and alter 
categorization of names and entities and expand 
the underlying category taxonomy to the domain of 
interest.  For example, in the NASA QA system, 
experts added categories like ?material?, ?fuel?, 
?spacecraft? and ?RLV?, (Reusable Launch 
Vehicles).  They also could specify that ?RLV? is a 
subcategory of ?spacecraft? and that space shuttles 
like ?Atlantis? have category ?RLV?.  The KBB 
works in tandem with the QTB, where the user can 
find terms in either documents or example queries 
 
3.2 Interactive QA Development 
 
In our current NASA phase, developed for 
undergraduate aerospace engineering students to 
quickly find information in the course of their 
studies on reusable launch vehicles, the user can 
view immediate results, thus bypassing the 
Reference Interviewer, or they may take the 
opportunity to utilize its increased functionality 
and interact with the QA system. The capabilities 
we have developed, represented by modules added 
to the system, fall into two groups. Group One 
includes capabilities that draw on direct interaction 
with the user to clarify what is being asked and that 
address terminological issues.  It includes Spell 
Checking, Expansion Clarification, and Answer 
Type Verification. Answers change dynamically as 
the user provides more input about what was 
meant. Group Two capabilities are dependent 
upon, and expand upon the user?s history of 
interaction with the system and include User 
Profile, Session Tracking, Reference Resolution, 
Question Similarity and User Frustration 
Recognition modules.  These gather knowledge 
about the user, help provide co-reference 
resolution within an extended dialogue, and 
monitor the level of frustration a user is 
experiencing.   
20
 The capabilities are explained in greater 
detail below.  Figure 1 captures the NASA system 
process and flow.  
 
Group One: 
  
In this group of interactive capabilities, after the 
user asks a query, answers are returned as in a 
typical system.  If the answers presented aren?t 
satisfactory, the system will embark on a series of 
interactive steps (described below) in which 
alternative spelling, answertypes, clarifications and 
expansions will be suggested.   The user can 
choose from the system?s suggestions or type in 
their own.  The system will then revise the query 
and return a new set of answers.  If those answers 
aren?t satisfactory, the user can continue 
interacting with the system until appropriate 
answers are found. 
Spell checking: Terms not found in the 
index of the document collection are displayed as 
potentially misspelled words.  In this preliminary 
phase, spelling is checked and users have the 
opportunity to select correct and/or alternative 
spellings.  
 AnswerType verification: The interactive 
QA system displays the type of answer that the 
system is looking for in order to answer the 
question.  For example for the question, Who 
piloted the first space shuttle?, the answer type is 
?person?, and the system will limit the search for 
candidate short answers in the collection to those 
that are a person?s name.  The user can either 
accept the system?s understanding of the question 
or reject the type it suggests.  This is particularly 
useful in semantically ambiguous questions such as 
?Who makes Mountain Dew?? where the system 
might interpret the question as needing a person, 
but the questioner actually wants the name of a 
company.  
Expansion:  This capability allows users to 
review the possible relevant terms (synonyms and 
group members) that could enhance the question-
answering process.  The user can either select or 
deselect terms of interest which do or do not 
express the intent of the question.  For example, if 
the user asks: How will aerobraking change the 
orbit size? then the system can bring back the 
following expansions for ?aerobraking?:  By 
aerobraking do you mean the following: 1) 
aeroassist, 2) aerocapture, 3) aeromaneuvering, 4) 
interplanetary transfer orbits, or 5) transfer orbits. 
Acronym Clarification: For abbreviations 
or acronyms within a query, the full explications 
known by the system for the term can be displayed 
back to the user.  The clarifications implemented 
are a priori limited to those that are relevant to the 
domain.  In the aerospace domain for example, if 
the question was What is used for the TPS of the 
RLV?, the clarifications of TPS would be thermal 
protection system, thermal protection subsystem, 
test preparation sheet, or twisted pair shielded, and 
the clarification of RLV would be reusable launch 
vehicle.  The appropriate clarifications can be 
selected to assist in improving the search.  For a 
more generic domain, the system would offer 
broader choices.  For example, if the user types in 
the question: What educational programs does the 
AIAA offer?, then the system might return: By 
AIAA, do you mean (a) American Institute of 
Aeronautics and Astronautics (b) Australia 
Indonesia Arts Alliance or (c) Americans for 
International Aid & Adoption?   
 
Group Two: 
 
User Profile: The User Profile keeps track of more 
permanent information about the user.  The profile 
includes a small standard set of user attributes, 
such as the user?s name and / or research interests.  
In our commercially funded work, selected 
information gleaned from the question about the 
user was also captured in the profile.  For example, 
if a user asks ?How much protein should my 
husband be getting every day??, the fact that the 
user is married can be added to their profile for 
future marketing, or for a new line of dialogue to 
ask his name or age.  This information is then 
made available as context information for the QA 
system to resolve references that the user makes to 
themselves and their own attributes.  
 For the NASA question-answering 
capability, to assist students in organizing their 
questions and results, there is an area for users to 
save their searches as standing queries, along with 
the results of searching (Davidson, 2006).  This 
information, representing topics and areas of 
interest, can help to focus answer finding for new 
questions the user asks. 
Not yet implemented, but of interest, is the 
ability to save information such as a user?s 
21
preferences (format, reliability, sources), that could 
be used as filters in the answer finding process. 
 Reference Resolution:  A basic feature of 
an interactive QA system is the requirement to 
understand the user?s questions and responsive 
answers as one session. The sequence of questions 
and answers forms a natural language dialogue 
between the user and the system.  This necessitates 
NLP processing at the discourse level, a primary 
task of which is to resolve references across the 
session.  Building on previous work in this area 
done for the Context Track of TREC 2001 
(Harabagiu et al 2001) and additional work (Chai 
and Jin, 2004) suggesting discourse structures are 
needed to understand the question/answer 
sequence, we have developed session-based 
reference resolution capability. In a dialogue, the 
user naturally includes referring phrases that 
require several types of resolution. 
 The simplest case is that of referring 
pronouns, where the user is asking a follow-up 
question, for example: 
 
Q1:  When did Madonna enter the music business? 
A1:  Madonna's first album, Madonna, came out in 
1983 and since then she's had a string of hits, been 
a major influence in the music industry and 
become an international icon. 
Q2:  When did she first move to NYC? 
 
In this question sequence, the second 
question contains a pronoun, ?she?, that refers to 
the person ?Madonna? mentioned both in the 
previous question and its answer.    Reference 
resolution would transform the question into 
?When did Madonna first move to NYC?? 
Another type of referring phrase is the 
definite common noun phrase, as seen in the next 
example: 
 
Q1: If my doctor wants me to take Acyclovir, is it 
expensive?  
A1:  Glaxo-Wellcome, Inc., the company that 
makes Acyclovir, has a program to assist 
individuals that have HIV and Herpes.  
Q2:  Does this company have other assistance 
programs? 
 
The second question has a definite noun 
phrase ?this company? that refers to ?Glaxo-
Wellcome, Inc.? in the previous answer, thus 
transforming the question to ?Does Glaxo-
Wellcome, Inc. have other assistance programs?? 
Currently, we capture a log of the 
question/answer interaction, and the reference 
resolution capability will resolve any references in 
the current question that it can by using linguistic 
techniques on the discourse of the current session.  
This is almost the same as the narrative 
coreference resolution used in documents, with the 
addition of the need to understand first and second 
person pronouns from the dialogue context.  The 
coreference resolution algorithm is based on 
standard linguistic discourse processing techniques 
where referring phrases and candidate resolvents 
are analyzed along a set of features that typically 
includes gender, animacy, number, person and the 
distance between the referring phrase and the 
candidate resolvent. 
Question Similarity: Question Similarity is 
the task of identifying when two or more questions 
are related.  Previous studies (Boydell et al, 2005, 
Balfe and Smyth, 2005) on information retrieval 
have shown that using previously asked questions 
to enhance the current question is often useful for 
improving results among like-minded users.  
Identifying related questions is useful for finding 
matches to Frequently Asked Questions (FAQs) 
and Previously Asked Questions (PAQs) as well as 
detecting when a user is failing to find adequate 
answers and may be getting frustrated.  
Furthermore, similar questions can be used during 
the reference interview process to present 
questions that other users with similar information 
needs have used and any answers that they 
considered useful.   
CNLP?s question similarity capability 
comprises a suite of algorithms designed to 
identify when two or more questions are related.  
The system works by analyzing each query using 
our Language-to-Logic (L2L) module to identify 
and weight keywords in the query, provide 
expansions and clarifications, as well as determine 
the focus of the question and the type of answer the 
user is expecting (Liddy et al, 2003).  We then 
compute a series of similarity measures on two or 
more L2L queries.  Our measures adopt a variety 
of approaches, including those that are based on 
keywords in the query: cosine similarity, keyword 
string matching, expansion analysis, and spelling 
variations.  In addition, two measures are based on 
the representation of the whole query:answer type 
22
and answer frame analysis. An answer frame is our 
representation of the meaningful extractions 
contained in the query, along with metadata about 
where they occur and any other extractions that 
relate to in the query. 
Our system will then combine the weighted 
scores of two or more of these measures to 
determine a composite score for the two queries, 
giving more weight to a measure that testing has 
determined to be more useful for a particular task. 
We have utilized our question similarity 
module for two main tasks.  For FAQ/PAQ (call it 
XAQ) matching, we use question similarity to 
compare the incoming question with our database 
of XAQs.  Through empirical testing, we 
determined a threshold above which we consider 
two questions to be similar. 
Our other use of question similarity is in the 
area of frustration detection.  The goal of 
frustration detection is to identify the signs a user 
may be giving that they are not finding relevant 
answers so that the system can intervene and offer 
alternatives before the user leaves the system, such 
as similar questions from other users that have 
been successful.    
 
4 Implementations:  
 
The refinements to our Question Answering 
system and the addition of interactive elements 
have been implemented in three different, but 
related working systems, one of which is strictly an 
enhanced IR system.  None of the three 
incorporates all of these capabilities.  In our work 
for MySentient, Ltd, we developed the session-
based reference resolution capability, implemented 
the variable length and multiple answer capability, 
modified our processing to facilitate the building 
of a user profile, added FAQ/PAQ capability, and 
our Question Similarity capability for both 
FAQ/PAQ matching and frustration detection.  A 
related project, funded by Syracuse Research 
Corporation, extended the user tools capability to 
include a User Interface for the KBB and basic 
processing technology.  Our NASA project has 
seen several phases.  As the project progressed, we 
added the relevant developed capabilities for 
improved performance.  In the current phase, we 
are implementing the capabilities which draw on 
user choice.  
 
5 Conclusions and Future Work 
 
 The reference interview has been 
implemented as an interactive dialogue between 
the system and the user, and the full system is near 
completion. We are currently working on two 
types of evaluation of our interactive QA 
capabilities. One is a system-based evaluation in 
the form of unit tests, the other is a user-based 
evaluation. The unit tests are designed to verify 
whether each module is working correctly and 
whether any changes to the system adversely affect 
results or performance. Crafting unit tests for 
complex questions has proved challenging, as no 
gold standard for this type of question has yet been 
created.  As the data becomes available, this type 
of evaluation will be ongoing and part of regular 
system development. 
 As appropriate for this evolutionary work 
within specific domains for which there are not 
gold standard test sets, our evaluation of the QA 
systems has focused on qualitative assessments. 
What has been a particularly interesting outcome is 
what we have learned in elicitation from graduate 
students using the NASA QA system, namely that 
they have multiple dimensions on which they 
evaluate a QA system, not just traditional recall 
and precision (Liddy et al 2004). The high level 
dimensions identified include system performance, 
answers, database content, display, and 
expectations. Therefore the evaluation criteria we 
believe appropriate for IQA systems are centered 
around the display (UI) category as described in 
Liddy et al (2004).  We will evaluate aspects of 
the UI input subcategory, including question 
understanding, information need understanding, 
querying style, and question formulation 
assistance. Based on this user evaluation the 
system will be improved and retested.   
 
 
References 
 
Evelyn Balfe and Barry Smyth. 2005. An Analysis 
of Query Similarity in Collaborative Web 
Search. In Proceedings of the 27th European 
Conference on Information Retrieval. Santiago 
de Compostela, Spain. 
 
23
Marcia J. Bates. 1989. The Design of Browsing 
and Berrypicking Techniques for the Online 
Search Interface.  Online Review, 13: 407-424. 
 
Mary Ellen Bates. 1997. The Art of the Reference 
Interview.  Online World. September 15. 
 
Ois?n Boydell, Barry Smyth, Cathal Gurrin, and 
Alan F. Smeaton. 2005. A Study of Selection 
Noise in Collaborative Web Search. In 
Proceedings of the 19th International Joint 
Conference on Artificial Intelligence. Edinburgh, 
Scotland. 
http://www.ijcai.org/papers/post-0214.pdf 
 
Joyce Y. Chai, and Rong Jin. 2004.  Discourse 
Structure for Context Question Answering.  In 
Proceedings of the Workshp on the Pragmatics 
of Quesiton Answering, HST-NAACL, Boston. 
http://www.cse.msu.edu/~rongjin/publications/H
LTQAWorkshop04.pdf   
 
Barry D. Davidson. 2006. An Advanced Interactive 
Discovery Learning Environment for 
Engineering Education: Final Report.  
Submitted to R. E. Gillian, National Aeronautics 
and Space Administration. 
 
Marco De Boni and Suresh Manandhar. 2005. 
Implementing Clarification Dialogues in Open 
Domain Question Answering. Natural Language 
Engineering 11(4): 343-361. 
 
Anne R. Diekema, Ozgur Yilmazel, Jiangping 
Chen, Sarah Harwell, Lan He, and Elizabeth D. 
Liddy. 2004. Finding Answers to Complex 
Questions. In New Directions in Question 
Answering. (Ed.) Mark T. Maybury. The MIT 
Press, 141-152. 
 
Sanda Harabagiu, Dan Moldovan, Marius Pa?ca, 
Mihai Surdeanu, Rada Mihalcea, Roxana G?rju, 
Vasile Rus, Finley L?c?tu?u, Paul Mor?rescu, 
R?zvan Bunescu. 2001. Answering Complex, 
List and Context Questions with LCC?s 
Question-Answering Server, TREC 2001. 
 
Chiori Hori, Takaaki Hori., Hideki Isozaki, Eisaku 
Maeda, Shigeru Katagiri, and Sadaoki Furui. 
2003. Deriving Disambiguous Queries in a 
Spoken Interactive ODQA System. In ICASSP. 
Hongkong, I: 624-627. 
 
Arne J?nsson, Frida And?n, Lars Degerstedt, 
Annika Flycht-Eriksson, Magnus Merkel, and 
Sara Norberg. 2004. Experiences from 
Combining Dialogue System Development With 
Information Extraction Techniques. In New 
Directions in Question Answering. (Ed.) Mark T. 
Maybury. The MIT Press, 153-164. 
 
Elizabeth D. Liddy. 2003. Question Answering in 
Contexts. Invited Keynote Speaker. ARDA 
AQUAINT Annual Meeting. Washington, DC. 
Dec 2-5, 2003. 
 
Elizabeth D. Liddy, Anne R. Diekema, Jiangping 
Chen, Sarah Harwell, Ozgur Yilmazel, and Lan 
He. 2003. What do You Mean? Finding Answers 
to Complex Questions. Proceedings of New 
Directions in Question Answering. AAAI Spring 
Symposium, March 24-26. 
 
Elizabeth D. Liddy, Anne R. Diekema, and Ozgur 
Yilmazel. 2004. Context-Based Question-
Answering Evaluation. In Proceedings of the 27th 
Annual ACM-SIGIR Conference. Sheffield, 
England 
 
Catherine S. Ross, Kirsti Nilsen, and Patricia 
Dewdney. 2002. Conducting the Reference 
Interview.  Neal-Schuman, New York, NY. 
 
Sharon Small, Tomek Strzalkowski, Ting Liu, 
Nobuyuki Shimizu, and Boris Yamrom. 2004. A 
Data Driven Approach to Interactive QA. In New 
Directions in Question Answering. (Ed.) Mark T. 
Maybury. The MIT Press, 129-140. 
 
Joseph E. Straw. 2004. Expecting the Stars but 
Getting the Moon: Negotiating around Patron 
Expectations in the Digital Reference 
Environment. In The Virtual Reference 
Experience: Integrating Theory into Practice. 
Eds. R. David Lankes, Joseph Janes, Linda C. 
Smith, and Christina M.  Finneran. Neal-
Schuman, New York, NY. 
 
24
Proceedings of the 8th International Conference on Computational Semantics, pages 326?332,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Towards a Cognitive Approach for the Automated
Detection of Connotative Meaning
Jaime Snyder*, Michael A. D?Eredita, Ozgur Yilmazel, and
Elizabeth D. Liddy
School of Information Studies, Syracuse University, Syracuse, NY, USA
* Corresponding author, jasnyd01@syr.edu
1 Introduction
The goal of the research described here is to automate the recognition of con-
notative meaning in text using a range of linguistic and non-linguistic fea-
tures. Pilot results are used to illustrate the potential of an integrated multi-
disciplinary approach to semantic text analysis that combines cognitive-
oriented human subject experimentation with Machine Learning (ML) based
Natural Language Processing (NLP). The research presented here was funded
through the Advanced Question and Answering for Intelligence (AQUAINT)
Project of the U.S. federal government?s Intelligence Advanced Research
Projects Activity (IARPA) Office. Funded as an exploratory ?Blue Sky?
project, this award enabled us to develop an extensible experimental setup
and to make progress towards training a machine learning system.
Automated understanding of connotative meaning of text requires an
understanding of both the mechanics of text and the human behaviors in-
volved in the disambiguation of text. We glean more from text than what
can be explicitly parsed from parts of speech or named entities. There are
other aspects of meaning that humans take away from text, such as a sincere
apology, an urgent request for help, a serious warning, or a perception of
personal threat. Merging cognitive and social cognitive psychology research
with sophisticated machine learning could extend current NLP systems to
account for these aspects. Building on current natural language processing
research [?], this pilot project encapsulates an end-to-end research method-
ology that begins by 1) establishing a human-understanding baseline for
the distinction between connotative and denotative meaning, 2) then ex-
tends the analysis of the mechanics of literal versus non-literal meaning by
326
applying NLP tools to the human-annotated text, and 3) uses these cu-
mulative results to feed a machine learning system that will be taught to
recognize the potential for connotative meaning at the sentence level, across
a much broader corpus. This paper describes the preliminary iteration of
this methodology and suggests ways that this approach could be improved
for future applications.
2 Analytic framework: A cognitive approach
We view an excerpt of text to be a stimulus, albeit much more complex than
most stimuli used in typical psychological experiments. The meaning of any
excerpt of text is tieds to a constructive cognitive process that is heavily
influenced by previous experience and cues, or features, embedded within
the text. Our goal is to gain a better understanding of (1) what features are
attended to when the text is being interpreted, (2) which of these features
are most salient and (3) how these features affect connotative meaning.
One?s ability to derive connotative meaning from text is behavior that
is learned, becoming intuitive in much the same way an individual learns
any skill or behavior. When this process of attending and learning is re-
peated across instances, specific skills become more automatic, or reliable
[?, ?]. This process is considered to be constructive and episodic in nature,
yet heavily dependent upon ?cues? that work to draw or focus one?s atten-
tion [?]. Further, research on communities suggests that the meaning of an
artifact (e.g., a specific excerpt of text) is heavily influenced by how it is
used in practice [?] The meaning of text is constructed in a similar manner.
Members of a speech community tend to make similar assumptions, or in-
ferences. The mechanics of making such inferences are scaled to the amount
of contextual information provided. Our preliminary research suggests that
when presented with a sentence that is out of context an individual seem-
ingly makes assumptions about one or all of the following: who created the
text, the context from which it was pulled and the intended meaning given
the features of the text.
3 Methods
3.1 Data
Blog text was used as the corpus for this research. Sentences were deemed
the most practical and fruitful unit of analysis because words were consid-
327
ered too restrictive and pieces of text spanning more than one sentence too
unwieldy. A single sentence presented enough context while still allowing
for a wide range of interpretation. Sentences were randomly selected from
a pool of texts automatically extracted from blogs, using a crawler set with
keywords such as ?oil?, ?Middle East? or ?Iraq.? Topics were selected with
the intention of narrowing the range of vocabulary used in order to aid the
machine learning experiments.
3.2 Preliminary phase
To start, we conducted a series of eight semi-structured, face-to-face inter-
views. Individuals were presented with 20 sentences selected to include some
texts that were expected to be perceived as highly connotative as well as
some expected to be perceived as highly denotative. Each interviewee was
asked to exhaustively share all possible meanings they could derive from
the stimulus text, while also pinpointing what it was about the text that
led them to make their conclusions. Based on these interviews, we modified
our probes slightly and moved the human evaluation process to an open-
ended, on-line instrument in order to increase the number of responses. We
presented a series of 20 sentences to participants (N=193) and, for each stim-
ulus text, asked: 1) ?What does this sentence suggest?? & ?What makes
you think this??; and 2) ?What else does this sentence suggest?? & ?What
makes you think this?? Upon analysis of the responses, we found that while
interpretations of the text were relatively idiosyncratic, how people allocated
their attention was more consistent. Most people tended to be making as-
sumptions about the (1) author (addressing who created the artifact), (2)
context (addressing from where the sentence was taken) and/or (3) intended
meaning of the words. We interpreted this to mean that these three areas
were potentially important for identifying inferred meanings of texts.
3.3 Design of pilot experiment
Next, our efforts focused on designing a reusable and scalable online evalu-
ation tool that would allow us to systematically gather multiple judgments
for each sentence using a much larger pool of stimulus text. Scaling up the
human evaluations also allowed us to decipher between responses that were
either systematically patterned or more idiosyncratic (or random). Accord-
ing to our forced-choice design, each online participant was presented with
a series of 32 pairs of sentences, one pair at a time, and asked to identify the
sentence that provided more of an opportunity to read between the lines.
328
Half the participants were presented with a positive prompt (which sentence
provides the most opportunity) and half were presented with a negative
prompt (which sentence provides the least opportunity). Positive/negative
assignment was determined randomly. The 16 sentences selected during
the first round were re-paired in a second round. This continued until 4
sentences remained, representing sentences that were more strongly conno-
tative or denotative, depending on the prompt. Final sentence scores were
averaged across all evaluations received.
The forced choice scenario requires a sample of only 13 participants to
evaluate 832 sentences. This was a significant improvement over previous
methods, increasing the number of sentences and the number of evaluations
per sentence and therefore increasing the reliability of our findings. For ex-
ample, using this scalable setup on a set of 832 sentences we need only 26
participants to generate two evaluations per sentence in the set, 39 partici-
pants to yield three evaluations per sentence, etc. We ran the system with
a randomly selected sample of both sentences and participants with the in-
tent to eventually make direct comparison among more controlled samples
of sentences and participants. This has direct implication for the evalua-
tion phase of our pilot. Because sentences were selected at random, without
guarantee of a certain number of each type of sentence, our goal was to
achieve results on a par with chance. Anything else would reveal systematic
bias in the experiment design or implementation. This also provides us with
a baseline for future investigations where the stimulus text would be more
wilfully controlled.
4 Results
4.1 Evaluation of text by human subjects
In the first iteration of the pilot setup, each of 832 sentences were viewed
by six different participants, three assigned to a positive group and three to
a negative group, as described above. The denotative condition ranged in
ratings from 0 to -3 while the connotative condition ranged in rating from 0
to 3. These were then averaged to achieve an overall score for each sentence.
Because they were randomly selected, each sentence had predictable chance
of ultimately being identified as connotative or denotative. In other words,
each sentence had an equal chance of being identified as connotative.
Having established a baseline based on chance, we can next control for
various features and evaluate the relative impact as systematic differences
from the baseline. We will be able to say with a relatively high degree of
329
certainty that ?x,? ?y? or ?z? feature, sentence structure, behavior, etc.
was responsible for skewing the odds in a reliable manner because we will
be able to control for these variables across various experimental scenarios.
This, combined with improved validity resulting from an increased number
of human judgments and an increased number of sentences viewed, marks
the strength of this methodology.
Additionally, we will be able to compare sentences within each scenario
even when an overall chance outcome occurs. For example, in the initial run
of our sentences, we achieved an overall chance outcome. However, ?anoma-
lies? emerged, sentences that were strongly skewed towards being assigned a
neutral evaluation score or towards an extreme score (either distinctly con-
notative or distinctly denotative). This allowed us to gather a reliable and
valid subset of data that can be utilized in ML experiments. See below for
a very short list of sample sentences grouped according to the overall scores
they received determine by the six human reviewers:
Denotative examples-
? The equipment was a radar system.
? Kosovo has been part of modern day Serbia since 1912.
? The projected figure for 2007 is about $ 3100.
Connotative examples-
? In fact, do what you bloody well like .
? But it?s pretty interesting , in a depressing sort of way .
? It?s no more a language than American English or Quebecois French
4.2 Experimental Machine Learning system
Our preliminary analysis suggests that humans are consistent in recogniz-
ing the extremes of connotative and denotative sentences and an automatic
recognition system could be built to identify when a text is likely to convey
connotative meaning. Machine Learning (ML) techniques could be used to
enable a system to first classify a text according to whether it conveys a
connotative or denotative level of meaning, and eventually, identify specific
connotations. ML techniques usually assume a feature space within which
the system learns the relative importance of features to use in classification.
Since humans process language at various levels (morphological, lexical, syn-
tactic, semantic, discourse and pragmatic), some multi-level combination of
features is helping them reach consistent conclusions. Hence, the initial ma-
chine learning classification decision will be made based on a class of critical
330
features, as cognitive and social-cognitive theory suggests happens in human
interpretation of text.
TextTagger, an Information Extraction System developed at Syracuse
University?s Center for Natural Language Processing, currently can identify
sentence boundaries, part-of-speech tag words, stem and lemmatize words,
identify various types of phrases, categorize named entities and common
nouns, recognize relations, and resolve co-references in text. We are in the
process of designing a ML framework that utilizes these tags and can learn
from a few examples provided by the human subject experiments described
above, then train on other sets of similar data marked by analysts as pos-
sessing the features illustrated by the sentences consistently identified as
conveying connotative meaning.
For preliminary ML-based analysis, the data collection included 266 sen-
tences (from the original 832 used in human subject experiments), 145
tagged as strongly connotative and 121 tagged as strongly denotative by
subjects. Fifty sentences from each set became a test collection and the
remaining 95 connotative and 71 denotative sentences were used for train-
ing. Our baseline results (without TextTagger annotations) were: Precision:
44.77 ; Recall: 60; F: 51.28. After tagging, when we only use proper names
and common nouns the results improved: Precision: 51.61 Recall: 92; F:
67.13. Although these results are not as high as some categorization results
reported in the literature for simpler categorization tasks such as document
labeling or spam identification, we believe that using higher level linguistic
features extracted by our NLP technology will significantly improve them.
More sophisticated analysis will be conducted during future applications of
this methodology.
5 Discussion and Future Work
By allowing the ML system to do time- and labor-intensive analysis, and
exploiting a natural human ability to ?know it when they see it? (in this case
?it? referring to connotative meaning), we feel that this pilot methodology
has great potential to deliver robust results. In addition to the significant
contribution this research will make in the area of natural language process-
ing, it will also provide a model for future work that seeks to create similar
bridges between psychological investigation and system building. Prelimi-
nary results suggest that our approach is viable and that a system composed
of multiple layers of analysis-with each level geared towards reducing the
variability of the next-holds promise.
331
Future work will concentrate efforts in two areas. First, the notion of
speech communities will be addressed. The pilot study looked at a very gen-
eralized speech community, expecting to achieve equally generalized results.
While this has merit, there is much to be learned by implementing this ap-
proach using a more targeted community. Second, the protocol used in this
pilot study was run using a relatively modest number of human evaluators
and a relatively small set of data. With the experience gained during the
pilot, the reliability of the data used to train the ML system can be easily
improved by increasing the size of both human subject samples and data
sets. With a more robust set of initial data, ML experiments can progress
beyond the basic proof-of-concept results reported here and produce action-
able feature sets tuned to specific speech communities.
References
[1] M. A. D?Eredita and C. Barreto. How does tacit knowledge proliferate? Orga-
nization Studies, 27(12):1821, 2006.
[2] E.D. Liddy, E. Hovy, J. Lin, J. Prager, D. Radev, L. Vanderwende, and
R. Weischedel. Natural Language Processing. Encyclopedia of Library and
Information Science, pages 2126?2136, 2003.
[3] G. D. Logan. Toward an instance theory of automatization. Psychological
Review, 95(4):492?527, 1988.
[4] E. Wenger. Communities of Practice: Learning, Meaning, and Identity. Cam-
bridge University Press, 1999.
[5] R.S. Wyer and J.A. Bargh. The Automaticity of Everyday Life. Lawrence
Erlbaum Associates, 1997.
332
