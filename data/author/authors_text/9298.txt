Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1533?1541,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Phrase Dependency Parsing for Opinion Mining
Yuanbin Wu, Qi Zhang, Xuanjing Huang, Lide Wu
Fudan University
School of Computer Science
{ybwu,qi zhang,xjhuang,ldwu}@fudan.edu.cn
Abstract
In this paper, we present a novel approach
for mining opinions from product reviews,
where it converts opinion mining task to
identify product features, expressions of
opinions and relations between them. By
taking advantage of the observation that a
lot of product features are phrases, a con-
cept of phrase dependency parsing is in-
troduced, which extends traditional depen-
dency parsing to phrase level. This con-
cept is then implemented for extracting re-
lations between product features and ex-
pressions of opinions. Experimental eval-
uations show that the mining task can ben-
efit from phrase dependency parsing.
1 Introduction
As millions of users contribute rich information
to the Internet everyday, an enormous number of
product reviews are freely written in blog pages,
Web forums and other consumer-generated medi-
ums (CGMs). This vast richness of content be-
comes increasingly important information source
for collecting and tracking customer opinions. Re-
trieving this information and analyzing this con-
tent are impossible tasks if they were to be manu-
ally done. However, advances in machine learning
and natural language processing present us with
a unique opportunity to automate the decoding of
consumers? opinions from online reviews.
Previous works on mining opinions can be di-
vided into two directions: sentiment classification
and sentiment related information extraction. The
former is a task of identifying positive and neg-
ative sentiments from a text which can be a pas-
sage, a sentence, a phrase and even a word (So-
masundaran et al, 2008; Pang et al, 2002; Dave
et al, 2003; Kim and Hovy, 2004; Takamura et
al., 2005). The latter focuses on extracting the el-
ements composing a sentiment text. The elements
include source of opinions who expresses an opin-
ion (Choi et al, 2005); target of opinions which
is a receptor of an opinion (Popescu and Etzioni,
2005); opinion expression which delivers an opin-
ion (Wilson et al, 2005b). Some researchers refer
this information extraction task as opinion extrac-
tion or opinion mining. Comparing with the for-
mer one, opinion mining usually produces richer
information.
In this paper, we define an opinion unit as a
triple consisting of a product feature, an expres-
sion of opinion, and an emotional attitude(positive
or negative). We use this definition as the basis for
our opinion mining task. Since a product review
may refer more than one product feature and ex-
press different opinions on each of them, the rela-
tion extraction is an important subtask of opinion
mining. Consider the following sentences:
1. I highly [recommend]
(1)
the Canon SD500
(1)
to
anybody looking for a compact camera that can take
[good]
(2)
pictures
(2)
.
2. This camera takes [amazing]
(3)
image qualities
(3)
and its size
(4)
[cannot be beat]
(4)
.
The phrases underlined are the product features,
marked with square brackets are opinion expres-
sions. Product features and opinion expressions
with identical superscript compose a relation. For
the first sentence, an opinion relation exists be-
tween ?the Canon SD500? and ?recommend?, but
not between ?picture? and ?recommend?. The ex-
ample shows that more than one relation may ap-
pear in a sentence, and the correct relations are not
simple Cartesian product of opinion expressions
and product features.
Simple inspection of the data reveals that prod-
uct features usually contain more than one word,
such as ?LCD screen?, ?image color?, ?Canon
PowerShot SD500?, and so on. An incomplete
product feature will confuse the successive anal-
ysis. For example, in passage ?Image color is dis-
1533
appointed?, the negative sentiment becomes ob-
scure if only ?image? or ?color? is picked out.
Since a product feature could not be represented
by a single word, dependency parsing might not be
the best approach here unfortunately, which pro-
vides dependency relations only between words.
Previous works on relation extraction usually use
the head word to represent the whole phrase and
extract features from the word level dependency
tree. This solution is problematic because the in-
formation provided by the phrase itself can not be
used by this kind of methods. And, experimental
results show that relation extraction task can ben-
efit from dependencies within a phrase.
To solve this issue, we introduce the concept
of phrase dependency parsing and propose an ap-
proach to construct it. Phrase dependency pars-
ing segments an input sentence into ?phrases? and
links segments with directed arcs. The parsing
focuses on the ?phrases? and the relations be-
tween them, rather than on the single words inside
each phrase. Because phrase dependency parsing
naturally divides the dependencies into local and
global, a novel tree kernel method has also been
proposed.
The remaining parts of this paper are organized
as follows: In Section 2 we discuss our phrase de-
pendency parsing and our approach. In Section 3,
experiments are given to show the improvements.
In Section 4, we present related work and Section
5 concludes the paper.
2 The Approach
Fig. 1 gives the architecture overview for our ap-
proach, which performs the opinion mining task
in three main steps: (1) constructing phrase de-
pendency tree from results of chunking and de-
pendency parsing; (2) extracting candidate prod-
uct features and candidate opinion expressions; (3)
extracting relations between product features and
opinion expressions.
2.1 Phrase Dependency Parsing
2.1.1 Overview of Dependency Grammar
Dependency grammar is a kind of syntactic the-
ories presented by Lucien Tesni`ere(1959). In de-
pendency grammar, structure is determined by the
relation between a head and its dependents. In
general, the dependent is a modifier or comple-
ment; the head plays a more important role in de-
termining the behaviors of the pair. Therefore, cri-
Phrase Dependency Parsing  
Review Crawler 
Review Database
 Chunking DependencyParsing
  
CandidateProduct FeaturesIdentification
CandidateOpinion ExpressionsExtraction
Relation ExtractionOpinionDatabase
Phrase Dependency Tree
Figure 1: The architecture of our approach.
teria of how to establish dependency relations and
how to distinguish the head and dependent in such
relations is central problem for dependency gram-
mar. Fig. 2(a) shows the dependency represen-
tation of an example sentence. The root of the
sentence is ?enjoyed?. There are seven pairs of
dependency relationships, depicted by seven arcs
from heads to dependents.
2.1.2 Phrase Dependency Parsing
Currently, the mainstream of dependency parsing
is conducted on lexical elements: relations are
built between single words. A major informa-
tion loss of this word level dependency tree com-
pared with constituent tree is that it doesn?t ex-
plicitly provide local structures and syntactic cat-
egories (i.e. NP, VP labels) of phrases (Xia and
Palmer, 2001). On the other hand, dependency
tree provides connections between distant words,
which are useful in extracting long distance rela-
tions. Therefore, compromising between the two,
we extend the dependency tree node with phrases.
That implies a noun phrase ?Cannon SD500 Pow-
erShot? can be a dependent that modifies a verb
phrase head ?really enjoy using? with relation type
?dobj?. The feasibility behind is that a phrase is a
syntactic unit regardless of the length or syntac-
tic category (Santorini and Kroch, 2007), and it is
acceptable to substitute a single word by a phrase
with same syntactic category in a sentence.
Formally, we define the dependency parsing
with phrase nodes as phrase dependency parsing.
A dependency relationship which is an asymmet-
ric binary relationship holds between two phrases.
One is called head, which is the central phrase in
the relation. The other phrase is called dependent,
which modifies the head. A label representing the
1534
enjoyed
We nsubj reallyadvmod using
partmod
SD500
thedet Canon PowerShotnn nn
dobj
enjoyed
nsubj really using
partmod
We 
VP
NP SD500
the
det
Canon PowerShot
nn nn
NP
advmod
dobj
(a)
(c)(b)
  NP SEGMENT:      [We] VP SEGMENT:      [really]      [enjoyed ]      [using] NP SEGMENT:      [the]      [Canon]      [PowerShot]      [SD500]
Figure 2: Example of Phrase Dependency Parsing.
relation type is assigned to each dependency rela-
tionship, such as subj (subject), obj (object), and
so on. Fig.2(c) shows an example of phrase de-
pendency parsing result.
By comparing the phrase dependency tree and
the word level dependency tree in Fig.2, the for-
mer delivers a more succinct tree structure. Local
words in same phrase are compacted into a sin-
gle node. These words provide local syntactic and
semantic effects which enrich the phrase they be-
long to. But they should have limited influences on
the global tree topology, especially in applications
which emphasis the whole tree structures, such as
tree kernels. Pruning away local dependency re-
lations by additional phrase structure information,
phrase dependency parsing accelerates following
processing of opinion relation extraction .
To construct phrase dependency tree, we pro-
pose a method which combines results from an
existing shallow parser and a lexical dependency
parser. A phrase dependency tree is defined as
T = (V ,E ), where V is the set of phrases,
E is the dependency relations among the phrases
in V representing by direct edges. To reserve
the word level dependencies inside a phrase, we
define a nested structure for a phrase T
i
in V :
T
i
= (V
i
, E
i
). V
i
= {v
1
, v
2
, ? ? ? , v
m
} is the inter-
nal words, E
i
is the internal dependency relations.
We conduct the phrase dependency parsing in
this way: traverses word level dependency tree
in preorder (visits root node first, then traverses
the children recursively). When visits a node R,
searches in its children and finds the node set D
which are in the same phrase with R according
Algorithm 1 Pseudo-Code for constructing the
phrase dependency tree
INPUT:
T
?
= (V
?
, E
?
) a word level dependency tree
P = phrases
OUTPUT:
phrase dependency tree T = (V , E ) where
V = {T
1
(V
1
, E
1
), T
2
(V
2
, E
2
), ? ? ? , T
n
(V
n
, E
n
)}
Initialize:
V ? {({v
?
}, {})|v
?
? V
?
}
E ? {(T
i
, T
j
)|(v
?
i
, v
?
j
) ? E
?
, v
?
i
? V
i
, v
?
j
? V
j
}
R = (V
r
, E
r
) root of T
PhraseDPTree(R, P )
1: Find p
i
? P where word[R] ? p
i
2: for each S = (V
s
, E
s
), (R,S) ? E do
3: if word[S] ? p
i
then
4: V
r
? V
r
? v
s
; v
s
? V
s
5: E
r
? E
r
? (v
r
, root[S]); v
r
? V
r
6: V ? V ? S
7: E ? E + (R, l); ?(S, l) ? E
8: E ? E ? (R,S)
9: end if
10: end for
11: for each (R,S) ? E do
12: PhraseDPTree(S,P )
13: end for
14: return (V , E )
to the shallow parsing result. Compacts D and R
into a single node. Then traverses all the remain-
ing children in the same way. The algorithm is
shown in Alg. 1.
The output of the algorithm is still a tree, for we
only cut edges which are compacted into a phrase,
the connectivity is keeped. Note that there will be
inevitable disagrees between shallow parser and
lexical dependency parser, the algorithm implies
that we simply follow the result of the latter one:
the phrases from shallow parser will not appear in
the final result if they cannot be found in the pro-
cedure.
Consider the following example:
?We really enjoyed using the Canon PowerShot SD500.?
Fig.2 shows the procedure of phrase depen-
dency parsing. Fig.2(a) is the result of the lex-
ical dependency parser. Shallow parsers result
is shown in Fig.2(b). Chunk phrases ?NP(We)?,
?VP(really enjoyed using)? and ?NP(the Canon
PowerShot SD500)? are nodes in the output phrase
dependency tree. When visiting node ?enjoyed? in
Fig.2(a), the shallow parser tells that ?really? and
?using? which are children of ?enjoy? are in the
same phrase with their parent, then the three nodes
are packed. The final phrase dependency parsing
tree is shown in the Fig. 2(c).
1535
2.2 Candidate Product Features and Opinion
Expressions Extraction
In this work, we define that product features
are products, product parts, properties of prod-
ucts, properties of parts, company names and re-
lated objects. For example,in consumer elec-
tronic domain, ?Canon PowerShot?, ?image qual-
ity?,?camera?, ?laptop? are all product features.
From analyzing the labeled corpus, we observe
that more than 98% of product features are in a
single phrase, which is either noun phrase (NP) or
verb phrase (VP). Based on it, all NPs and VPs
are selected as candidate product features. While
prepositional phrases (PPs) and adjectival phrases
(ADJPs) are excluded. Although it can cover
nearly all the true product features, the precision
is relatively low. The large amount of noise can-
didates may confuse the relation extraction clas-
sifier. To shrink the size of candidate set, we in-
troduce language model by an intuition that the
more likely a phrase to be a product feature, the
more closely it related to the product review. In
practice, for a certain domain of product reviews,
a language model is build on easily acquired unla-
beled data. Each candidate NP or VP chunk in the
output of shallow parser is scored by the model,
and cut off if its score is less than a threshold.
Opinion expressions are spans of text that ex-
press a comment or attitude of the opinion holder,
which are usually evaluative or subjective phrases.
We also analyze the labeled corpus for opinion ex-
pressions and observe that many opinion expres-
sions are used in multiple domains, which is iden-
tical with the conclusion presented by Kobayashi
et al (2007). They collected 5,550 opinion ex-
pressions from various sources . The coverage of
the dictionary is high in multiple domains. Moti-
vated by those observations, we use a dictionary
which contains 8221 opinion expressions to select
candidates (Wilson et al, 2005b). An assump-
tion we use to filter candidate opinion expressions
is that opinion expressions tend to appear closely
with product features, which is also used to extract
product features by Hu and Liu (2004). In our ex-
periments, the tree distance between product fea-
ture and opinion expression in a relation should be
less than 5 in the phrase dependency parsing tree.
2.3 Relation Extraction
This section describes our method on extracting
relations between opinion expressions and product
features using phrase dependency tree. Manually
built patterns were used in previous works which
have an obvious drawback that those patterns can
hardly cover all possible situations. By taking ad-
vantage of the kernel methods which can search a
feature space much larger than that could be repre-
sented by a feature extraction-based approach, we
define a new tree kernel over phrase dependency
trees and incorporate this kernel within an SVM to
extract relations between opinion expressions and
product features.
The potential relation set consists of the all
combinations between candidate product features
and candidate opinion expressions in a sentence.
Given a phrase dependency parsing tree, we
choose the subtree rooted at the lowest common
parent(LCP) of opinion expression and product
feature to represent the relation.
Dependency tree kernels has been proposed by
(Culotta and Sorensen, 2004). Their kernel is de-
fined on lexical dependency tree by the convolu-
tion of similarities between all possible subtrees.
However, if the convolution containing too many
irrelevant subtrees, over-fitting may occur and de-
creases the performance of the classifier. In phrase
dependency tree, local words in a same phrase are
compacted, therefore it provides a way to treat ?lo-
cal dependencies? and ?global dependencies? dif-
ferently (Fig. 3). As a consequence, these two
kinds of dependencies will not disturb each other
in measuring similarity. Later experiments prove
the validity of this statement.
B
A C
D
E
B
A
C
D E
Phrase Local dependencies
Global dependencies
Figure 3: Example of ?local dependencies? and
?global dependencies?.
We generalize the definition by (Culotta and
Sorensen, 2004) to fit the phrase dependency tree.
Use the symbols in Section 2.1.2, T
i
and T
j
are
two trees with root R
i
and R
j
, K(T
i
,T
j
) is the
kernel function for them. Firstly, each tree node
T
k
? T
i
is augmented with a set of features F ,
and an instance of F for T
k
is F
k
= {f
k
}. A
match function m(T
i
, T
j
) is defined on comparing
a subset of nodes? features M ? F . And in the
same way, a similarity function s(T
i
, T
j
) are de-
1536
fined on S ? F
m(T
i
, T
j
) =
{
1 if f
i
m
= f
j
m
?f
m
? M
0 otherwise
(1)
and
s(T
i
, T
j
) =
?
f
s
?S
C(f
i
s
, f
j
s
) (2)
where
C(f
i
s
, f
j
s
) =
{
1 if f
i
s
= f
j
s
0 otherwise
(3)
For the given phrase dependency parsing trees,
the kernel function K(T
i
,T
j
) is defined as fol-
low:
K(T
i
,T
j
) =
?
?
?
?
?
0 if m(R
i
, R
j
) = 0
s(R
i
, R
j
) +K
in
(R
i
, R
j
)
+K
c
(R
i
.C, R
j
.C) otherwise
(4)
where K
in
(R
i
, R
j
) is a kernel function over
R
i
= (V
i
r
, E
i
r
) and R
j
= (V
j
r
, E
j
r
)?s internal
phrase structures,
K
in
(R
i
, R
j
) = K(R
i
, R
j
) (5)
K
c
is the kernel function over R
i
and R
j
?s chil-
dren. Denote a is a continuous subsequence of in-
dices a, a+1, ? ? ? a+ l(a) for R
i
?s children where
l(a) is its length, a
s
is the s-th element in a. And
likewise b for R
j
.
K
c
(R
i
.C, R
j
.C) =
?
a,b,l(a)=l(b)
?
l(a)
K(R
i
.[a], R
j
.[b])
?
?
s=1..l(a)
m(R
i
.[a
s
], R
j
.[b
s
])
(6)
where the constant 0 < ? < 1 normalizes the ef-
fects of children subsequences? length.
Compared with the definitions in (Culotta and
Sorensen, 2004), we add term K
in
to handle the
internal nodes of a pharse, and make this exten-
sion still satisfy the kernel function requirements
(composition of kernels is still a kernel (Joachims
et al, 2001)). The consideration is that the local
words should have limited effects on whole tree
structures. So the kernel is defined on external
children (K
c
) and internal nodes (K
in
) separately,
Table 1: Statistics for the annotated corpus
Category # Products # Sentences
Cell Phone 2 1100
Diaper 1 375
Digital Camera 4 1470
DVD Player 1 740
MP3 Player 3 3258
as the result, the local words are not involved in
subsequences of external children for K
c
. After
the kernel computing through training instances,
support vector machine (SVM) is used for classi-
fication.
3 Experiments and Results
In this section, we describe the annotated corpus
and experiment configurations including baseline
methods and our results on in-domain and cross-
domain.
3.1 Corpus
We conducted experiments with labeled corpus
which are selected from Hu and Liu (2004), Jin-
dal and Liu (2008) have built. Their documents
are collected from Amazon.com and CNet.com,
where products have a large number of reviews.
They also manually labeled product features and
polarity orientations. Our corpus is selected
from them, which contains customer reviews of
11 products belong to 5 categories(Diaper, Cell
Phone, Digital Camera, DVD Player, and MP3
Player). Table 1 gives the detail statistics.
Since we need to evaluate not only the prod-
uct features but also the opinion expressions and
relations between them, we asked two annotators
to annotate them independently. The annotators
started from identifying product features. Then for
each product feature, they annotated the opinion
expression which has relation with it. Finally, one
annotator A
1
extracted 3595 relations, while the
other annotator A
2
extracted 3745 relations, and
3217 cases of them matched. In order to measure
the annotation quality, we use the following metric
to measure the inter-annotator agreement, which is
also used by Wiebe et al (2005).
agr(a||b) =
|A matches B|
|A|
1537
Table 2: Results for extracting product features
and opinion expressions
P R F
Product Feature 42.8% 85.5% 57.0%
Opinion Expression 52.5% 75.2% 61.8%
Table 3: Features used in SVM-1: o denotes an
opinion expression and t a product feature
1) Positions of o/t in sentence(start, end, other);
2) The distance between o and t (1, 2, 3, 4, other);
3) Whether o and t have direct dependency relation;
4) Whether o precedes t;
5) POS-Tags of o/t.
where agr(a||b) represents the inter-annotator
agreement between annotator a and b, A and B
are the sets of anchors annotated by annotators a
and b. agr(A
1
||A
2
) was 85.9% and agr(A
2
||A
1
)
was 89.5%. It indicates that the reliability of our
annotated corpus is satisfactory.
3.2 Preprocessing Results
Results of extracting product features and opin-
ion expressions are shown in Table 2. We use
precision, recall and F-measure to evaluate perfor-
mances. The candidate product features are ex-
tracted by the method described in Section 2.2,
whose result is in the first row. 6760 of 24414
candidate product features remained after the fil-
tering, which means we cut 72% of irrelevant can-
didates with a cost of 14.5%(1-85.5%) loss in true
answers. Similar to the product feature extraction,
the precision of extracting opinion expression is
relatively low, while the recall is 75.2%. Since
both product features and opinion expressions ex-
tractions are preprocessing steps, recall is more
important.
3.3 Relation Extraction Experiments
3.3.1 Experiments Settings
In order to compare with state-of-the-art results,
we also evaluated the following methods.
1. Adjacent method extracts relations between a
product feature and its nearest opinion expression,
which is also used in (Hu and Liu, 2004).
2. SVM-1. To compare with tree kernel based
Table 4: Features used in SVM-PTree
Features for match function
1) The syntactic category of the tree node
(e.g. NP, VP, PP, ADJP).
2) Whether it is an opinion expression node
3) Whether it is a product future node.
Features for similarity function
1) The syntactic category of the tree node
(e.g. NP, VP, PP, ADJP).
2) POS-Tag of the head word of node?s internal
phrases.
3) The type of phrase dependency edge linking
to node?s parent.
4) Feature 2) for the node?s parent
5) Feature 3) for the node?s parent
approaches, we evaluated an SVM
1
result with a
set of manually selected features(Table 3), which
are also used in (Kobayashi et al, 2007).
3. SVM-2 is designed to compare the effective-
ness of cross-domain performances. The features
used are simple bag of words and POS-Tags be-
tween opinion expressions and product features.
4. SVM-WTree uses head words of opinion ex-
pressions and product features in the word-level
dependency tree, as the previous works in infor-
mation extraction. Then conducts tree kernel pro-
posed by Culotta and Sorensen (2004).
5. SVM-PTree denotes the results of our tree-
kernel based SVM, which is described in the Sec-
tion 2.3. Stanford parser (Klein and Manning,
2002) and Sundance (Riloff and Phillips, 2004)
are used as lexical dependency parser and shallow
parser. The features in match function and simi-
larity function are shown in Table 4.
6. OERight is the result of SVM-PTree with
correct opinion expressions.
7. PFRight is the result of SVM-PTree with
correct product features.
Table 5 shows the performances of different
relation extraction methods with in-domain data.
For each domain, we conducted 5-fold cross val-
idation. Table 6 shows the performances of the
extraction methods on cross-domain data. We use
the digital camera and cell phone domain as train-
ing set. The other domains are used as testing set.
1
libsvm 2.88 is used in our experiments
1538
Table 5: Results of different methods
Cell Phone MP3 Player Digital Camera DVD Player Diaper
Methods P R F P R F P R F P R F P R F
Adjacent 40.3% 60.5% 48.4% 26.5% 59.3% 36.7% 32.7% 59.1% 42.1% 31.8% 68.4% 43.4% 23.4% 78.8% 36.1%
SVM-1 69.5% 42.3% 52.6% 60.7% 30.6% 40.7% 61.4% 32.4% 42.4% 56.0% 27.6% 37.0% 29.3% 14.1% 19.0%
SVM-2 60.7% 19.7% 29.7% 63.6% 23.8% 34.6% 66.9% 23.3% 34.6% 66.7% 13.2% 22.0% 79.2% 22.4% 34.9%
SVM-WTree 52.6% 52.7% 52.6% 46.4% 43.8% 45.1% 49.1% 46.0% 47.5% 35.9% 32.0% 33.8% 36.6% 31.7% 34.0%
SVM-PTree 55.6% 57.2% 56.4% 51.7% 50.7% 51.2% 54.0% 49.9% 51.9% 37.1% 35.4% 36.2% 37.3% 30.5% 33.6%
OERight 66.7% 69.5% 68.1% 65.6% 65.9% 65.7% 64.3% 61.0% 62.6% 59.9% 63.9% 61.8% 55.8% 58.5% 57.1%
PFRight 62.8% 62.1% 62.4% 61.3% 56.8% 59.0% 59.7% 56.2% 57.9% 46.9% 46.6% 46.7% 58.5% 51.3% 53.4%
Table 6: Results for total performance with cross domain training data
Diaper DVD Player MP3 Player
Methods P R F P R F P R F
Adjacent 23.4% 78.8% 36.1% 31.8% 68.4% 43.4% 26.5% 59.3% 36.7%
SVM-1 22.4% 30.6% 25.9% 52.8% 30.9% 39.0% 55.9% 36.8% 44.4%
SVM-2 71.9% 15.1% 25.0% 51.2% 13.2% 21.0% 63.1% 22.0% 32.6%
SVM-WTree 38.7% 52.4% 44.5% 30.7% 59.2% 40.4% 38.1% 47.2% 42.2%
SVM-PTree 37.3% 53.7% 44.0% 59.2% 48.3% 46.3% 43.0% 48.9% 45.8%
3.3.2 Results Discussion
Table 5 presents different methods? results in five
domains. We observe that the three learning based
methods(SVM-1, SVM-WTree, SVM-PTree) per-
form better than the Adjacent baseline in the first
three domains. However, in other domains, di-
rectly adjacent method is better than the learning
based methods. The main difference between the
first three domains and the last two domains is the
size of data(Table 1). It implies that the simple Ad-
jacent method is also competent when the training
set is small.
A further inspection into the result of first 3
domains, we can also conclude that: 1) Tree
kernels(SVM-WTree and SVM-PTree) are better
than Adjacent, SVM-1 and SVM-2 in all domains.
It proofs that the dependency tree is important
in the opinion relation extraction. The reason
for that is a connection between an opinion and
its target can be discovered with various syntac-
tic structures. 2) The kernel defined on phrase
dependency tree (SVM-PTree) outperforms ker-
nel defined on word level dependency tree(SVM-
WTree) by 4.8% in average. We believe the main
reason is that phrase dependency tree provides a
more succinct tree structure, and the separative
treatment of local dependencies and global depen-
dencies in kernel computation can indeed improve
the performance of relation extraction.
To analysis the results of preprocessing steps?
influences on the following relation extraction,
we provide 2 additional experiments which the
product features and opinion expressions are all
correctly extracted respectively: OERight and
PFRight. These two results show that given an
exactly extraction of opinion expression and prod-
uct feature, the results of opinion relation extrac-
tion will be much better. Further, opinion expres-
sions are more influential which naturally means
the opinion expressions are crucial in opinion re-
lation extraction.
For evaluations on cross domain, the Adjacent
method doesn?t need training data, its results are
the same as the in-domain experiments. Note
in Table 3 and Table 4, we don?t use domain
related features in SVM-1, SVM-WTree, SVM-
PTree, but SVM-2?s features are domain depen-
dent. Since the cross-domain training set is larger
than the original one in Diaper and DVD domain,
the models are trained more sufficiently. The fi-
nal results on cross-domain are even better than
in-domain experiments on SVM-1, SVM-WTree,
and SVM-PTree with percentage of 4.6%, 8.6%,
10.3% in average. And the cross-domain train-
ing set is smaller than in-domain in MP3, but
it also achieve competitive performance with the
1539
in-domain. On the other hand, SVM-2?s result
decreased compared with the in-domain experi-
ments because the test domain changed. At the
same time, SVM-PTree outperforms other meth-
ods which is similar in in-domain experiments.
4 Related Work
Opinion mining has recently received consider-
able attention. Amount of works have been
done on sentimental classification in different lev-
els (Zhang et al, 2009; Somasundaran et al, 2008;
Pang et al, 2002; Dave et al, 2003; Kim and
Hovy, 2004; Takamura et al, 2005). While we
focus on extracting product features, opinion ex-
pressions and mining relations in this paper.
Kobayashi et al (2007) presented their work on
extracting opinion units including: opinion holder,
subject, aspect and evaluation. Subject and aspect
belong to product features, while evaluation is the
opinion expression in our work. They converted
the task to two kinds of relation extraction tasks
and proposed a machine learning-based method
which combines contextual clues and statistical
clues. Their experimental results showed that the
model using contextual clues improved the perfor-
mance. However since the contextual information
in a domain is specific, the model got by their ap-
proach can not easily converted to other domains.
Choi et al (2006) used an integer linear pro-
gramming approach to jointly extract entities and
relations in the context of opinion oriented infor-
mation extraction. They identified expressions of
opinions, sources of opinions and the linking re-
lation that exists between them. The sources of
opinions denote to the person or entity that holds
the opinion.
Another area related to our work is opinion
expressions identification (Wilson et al, 2005a;
Breck et al, 2007). They worked on identify-
ing the words and phrases that express opinions
in text. According to Wiebe et al (2005), there are
two types of opinion expressions, direct subjective
expressions and expressive subjective elements.
5 Conclusions
In this paper, we described our work on min-
ing opinions from unstructured documents. We
focused on extracting relations between product
features and opinion expressions. The novelties
of our work included: 1) we defined the phrase
dependency parsing and proposed an approach
to construct the phrase dependency trees; 2) we
proposed a new tree kernel function to model
the phrase dependency trees. Experimental re-
sults show that our approach improved the perfor-
mances of the mining task.
6 Acknowledgement
This work was (partially) funded by Chinese
NSF 60673038, Doctoral Fund of Ministry of
Education of China 200802460066, and Shang-
hai Science and Technology Development Funds
08511500302. The authors would like to thank the
reviewers for their useful comments.
References
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Pro-
ceedings of IJCAI-2007.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction pat-
terns. In Proceedings of HLT/EMNLP.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In Proceedings EMNLP.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In In Proceed-
ings of ACL 2004.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In
Proceedings of WWW 2003.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the ACM
SIGKDD 2004.
Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of WSDM ?08.
Thorsten Joachims, Nello Cristianini, and John Shawe-
Taylor. 2001. Composite kernels for hypertext cate-
gorisation. In Proceedings of ICML ?01.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of Coling
2004. COLING.
Dan Klein and Christopher D. Manning. 2002. Fast
exact inference with a factored model for natural
language parsing. In In Advances in Neural Infor-
mation Processing Systems.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of
relations in opinion mining. In Proceedings of
EMNLP-CoNLL 2007.
1540
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proc. of EMNLP
2002.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of HLT/EMNLP.
E. Riloff and W. Phillips. 2004. An introduction to
the sundance and autoslog systems. In University of
Utah School of Computing Technical Report UUCS-
04-015.
Beatrice Santorini and Anthony Kroch. 2007.
The syntax of natural language: An on-
line introduction using the Trees program.
http://www.ling.upenn.edu/ beatrice/syntax-
textbook.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of COLING 2008.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words us-
ing spin model. In Proceedings of ACL?05.
L. Tesni`ere. 1959. El?ements de syntaxe structurale.
Editions Klincksieck.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2/3).
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: A system for subjectiv-
ity analysis. In Demonstration Description in Con-
ference on Empirical Methods in Natural Language
Processing.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of HLT-
EMNLP.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In HLT ?01:
Proceedings of the first international conference on
Human language technology research.
Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara,
Joseph Johnson, and Xuanjing Huang. 2009. Min-
ing product reviews based on shallow dependency
parsing. In Proceedings of SIGIR 2009.
1541
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 561?568,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Progressive Feature Selection Algorithm for Ultra  
Large Feature Spaces 
 
 
Qi Zhang 
Computer Science Department 
Fudan University 
Shanghai 200433, P.R. China 
qi_zhang@fudan.edu.cn  
Fuliang Weng 
Research and Technology Center 
Robert Bosch Corp. 
Palo Alto, CA 94304, USA 
fuliang.weng@rtc.bosch.com 
 
Zhe Feng 
Research and Technology Center 
Robert Bosch Corp. 
Palo Alto, CA 94304, USA 
zhe.feng@rtc.bosch.com  
 
Abstract 
Recent developments in statistical modeling 
of various linguistic phenomena have shown 
that additional features give consistent per-
formance improvements. Quite often, im-
provements are limited by the number of fea-
tures a system is able to explore. This paper 
describes a novel progressive training algo-
rithm that selects features from virtually 
unlimited feature spaces for conditional 
maximum entropy (CME) modeling. Experi-
mental results in edit region identification 
demonstrate the benefits of the progressive 
feature selection (PFS) algorithm: the PFS 
algorithm maintains the same accuracy per-
formance as previous CME feature selection 
algorithms (e.g., Zhou et al, 2003) when the 
same feature spaces are used. When addi-
tional features and their combinations are 
used, the PFS gives 17.66% relative im-
provement over the previously reported best 
result in edit region identification on 
Switchboard corpus (Kahn et al, 2005), 
which leads to a 20% relative error reduction 
in parsing the Switchboard corpus when gold 
edits are used as the upper bound. 
1 Introduction 
Conditional Maximum Entropy (CME) modeling 
has received a great amount of attention within 
natural language processing community for the 
past decade (e.g., Berger et al, 1996; Reynar and 
Ratnaparkhi, 1997; Koeling, 2000; Malouf, 2002; 
Zhou et al, 2003; Riezler and Vasserman, 2004). 
One of the main advantages of CME modeling is 
the ability to incorporate a variety of features in a 
uniform framework with a sound mathematical 
foundation. Recent improvements on the original 
incremental feature selection (IFS) algorithm, 
such as Malouf (2002) and Zhou et al (2003), 
greatly speed up the feature selection process. 
However, like many other statistical modeling 
algorithms, such as boosting (Schapire and 
Singer, 1999) and support vector machine (Vap-
nik 1995), the algorithm is limited by the size of 
the defined feature space. Past results show that 
larger feature spaces tend to give better results. 
However, finding a way to include an unlimited 
amount of features is still an open research prob-
lem. 
In this paper, we propose a novel progressive 
feature selection (PFS) algorithm that addresses 
the feature space size limitation. The algorithm is 
implemented on top of the Selective Gain Com-
putation (SGC) algorithm (Zhou et al, 2003), 
which offers fast training and high quality mod-
els. Theoretically, the new algorithm is able to 
explore an unlimited amount of features. Be-
cause of the improved capability of the CME 
algorithm, we are able to consider many new 
features and feature combinations during model 
construction. 
To demonstrate the effectiveness of our new 
algorithm, we conducted a number of experi-
ments on the task of identifying edit regions, a 
practical task in spoken language processing. 
Based on the convention from Shriberg (1994) 
and Charniak and Johnson (2001), a disfluent 
spoken utterance is divided into three parts: the 
reparandum, the part that is repaired; the inter-
561
regnum, which can be filler words or empty; and 
the repair/repeat, the part that replaces or repeats 
the reparandum. The first two parts combined are 
called an edit or edit region. An example is 
shown below: 
 
interregnum 
It is, you know, this is a tough problem.
reparandum repair 
 
In section 2, we briefly review the CME mod-
eling and SGC algorithm. Then, section 3 gives a 
detailed description of the PFS algorithm. In sec-
tion 4, we describe the Switchboard corpus, fea-
tures used in the experiments, and the effective-
ness of the PFS with different feature spaces. 
Section 5 concludes the paper. 
2 Background 
Before presenting the PFS algorithm, we first 
give a brief review of the conditional maximum 
entropy modeling, its training process, and the 
SGC algorithm. This is to provide the back-
ground and motivation for our PFS algorithm. 
2.1 Conditional Maximum Entropy Model 
The goal of CME is to find the most uniform 
conditional distribution of y given observation 
x, ( )xyp , subject to constraints specified by a set 
of features ( )yxf i , , where features typically take 
the value of either 0 or 1 (Berger et al, 1996). 
More precisely, we want to maximize 
 ( ) ( ) ( ) ( )( )xypxypxppH
yx
log~
,
??=           (1) 
given the constraints:  
                  ( ) ( )ii fEfE ~=                         (2) 
where  
( ) ( ) ( )?=
yx
ii yxfyxpfE
,
,,~~  
is the empirical expected feature count from the 
training data and 
   ( ) ( ) ( ) ( )?=
yx
ii yxfxypxpfE
,
,~  
is the feature expectation from  the conditional 
model ( )xyp . 
This results in the following exponential 
model: 
              ( ) ( ) ( )???
?
???
?= ?
j
jj yxfxZ
xyp ,exp1 ?          (3) 
where ?j  is the weight corresponding to the fea-
ture fj, and Z(x) is a normalization factor. 
A variety of different phenomena, including 
lexical, structural, and semantic aspects, in natu-
ral language processing tasks can be expressed in 
terms of features. For example, a feature can be 
whether the word in the current position is a verb, 
or the word is a particular lexical item. A feature 
can also be about a particular syntactic subtree, 
or a dependency relation (e.g., Charniak and 
Johnson, 2005). 
2.2 Selective Gain Computation Algorithm 
In real world applications, the number of possi-
ble features can be in the millions or beyond. 
Including all the features in a model may lead to 
data over-fitting, as well as poor efficiency and 
memory overflow. Good feature selection algo-
rithms are required to produce efficient and high 
quality models. This leads to a good amount of 
work in this area (Ratnaparkhi et al, 1994; Ber-
ger et al, 1996; Pietra et al 1997; Zhou et al, 
2003; Riezler and Vasserman, 2004) 
In the most basic approach, such as Ratna-
parkhi et al (1994) and Berger et al (1996), 
training starts with a uniform distribution over all 
values of y and an empty feature set. For each 
candidate feature in a predefined feature space, it 
computes the likelihood gain achieved by includ-
ing the feature in the model. The feature that 
maximizes the gain is selected and added to the 
current model. This process is repeated until the 
gain from the best candidate feature only gives 
marginal improvement. The process is very slow, 
because it has to re-compute the gain for every 
feature at each selection stage, and the computa-
tion of a parameter using Newton?s method be-
comes expensive, considering that it has to be 
repeated many times.  
The idea behind the SGC algorithm (Zhou et 
al., 2003) is to use the gains computed in the 
previous step as approximate upper bounds for 
the subsequent steps. The gain for a feature 
needs to be re-computed only when the feature 
reaches the top of a priority queue ordered by 
gain. In other words, this happens when the fea-
ture is the top candidate for inclusion in the 
model. If the re-computed gain is smaller than 
that of the next candidate in the list, the feature is 
re-ranked according to its newly computed gain, 
and the feature now at the top of the list goes 
through the same gain re-computing process.  
This heuristics comes from evidences that the 
gains become smaller and smaller as more and 
more good features are added to the model. This 
can be explained as follows: assume that the 
Maximum Likelihood (ML) estimation lead to 
the best model that reaches a ML value. The ML 
value is the upper bound. Since the gains need to 
be positive to proceed the process, the difference 
562
between the Likelihood of the current and the 
ML value becomes smaller and smaller. In other 
words, the possible gain each feature may add to 
the model gets smaller. Experiments in Zhou et 
al. (2003) also confirm the prediction that the 
gains become smaller when more and more fea-
tures are added to the model, and the gains do 
not get unexpectively bigger or smaller as the 
model grows. Furthermore, the experiments in 
Zhou et al (2003) show no significant advantage 
for looking ahead beyond the first element in the 
feature list. The SGC algorithm runs hundreds to 
thousands of times faster than the original IFS 
algorithm without degrading classification per-
formance. We used this algorithm for it enables 
us to find high quality CME models quickly. 
The original SGC algorithm uses a technique 
proposed by Darroch and Ratcliff (1972) and 
elaborated by Goodman (2002): when consider-
ing a feature fi, the algorithm only modifies those 
un-normalized conditional probabilities: ( )( )? j jj yxf ,exp ?   
for (x, y) that satisfy fi (x, y)=1, and subsequently 
adjusts the corresponding normalizing factors 
Z(x) in (3). An implementation often uses a map-
ping table, which maps features to the training 
instance pairs (x, y).  
3 Progressive Feature Selection Algo-
rithm 
In general, the more contextual information is 
used, the better a system performs. However, 
richer context can lead to combinatorial explo-
sion of the feature space. When the feature space 
is huge (e.g., in the order of tens of millions of 
features or even more), the SGC algorithm ex-
ceeds the memory limitation on commonly avail-
able computing platforms with gigabytes of 
memory.  
To address the limitation of the SGC algo-
rithm, we propose a progressive feature selection 
algorithm that selects features in multiple rounds. 
The main idea of the PFS algorithm is to split the 
feature space into tractable disjoint sub-spaces 
such that the SGC algorithm can be performed 
on each one of them. In the merge step, the fea-
tures that SGC selects from different sub-spaces 
are merged into groups. Instead of re-generating 
the feature-to-instance mapping table for each 
sub-space during the time of splitting and merg-
ing, we create the new mapping table from the 
previous round?s tables by collecting those en-
tries that correspond to the selected features. 
Then, the SGC algorithm is performed on each 
of the feature groups and new features are se-
lected from each of them. In other words, the 
feature space splitting and subspace merging are 
performed mainly on the feature-to-instance 
mapping tables. This is a key step that leads to 
this very efficient PFS algorithm.  
At the beginning of each round for feature se-
lection, a uniform prior distribution is always 
assumed for the new CME model. A more pre-
cise description of the PFS algorithm is given in 
Table 1, and it is also graphically illustrated in 
Figure 1. 
Given:  
    Feature space F(0) = {f1(0), f2(0), ?, fN(0)},
step_num = m,  select_factor = s 
1. Split the feature space into N1 parts 
    {F1(1), F2(1), ?, FN1(1)} = split(F(0)) 
2. for k=1 to m-1 do 
      //2.1 Feature selection 
      for each feature space Fi(k) do 
           FSi(k) = SGC(Fi(k), s) 
      //2.2 Combine selected features 
      {F1(k+1), ?, FNk+1(k+1)}  =  
                      merge(FS1(k), ?, FSNk(k)) 
3. Final feature selection & optimization
F(m) = merge(FS1(m-1), ?, FSNm-1(m-1)) 
FS(m) = SGC(F(m), s) 
Mfinal = Opt(FS(m)) 
 
Table 1. The PFS algorithm. 
 
 
M
)2(
1F
)1(
1FS
)1(
1i
FS
M
M
)1(
2i
FS
M
)1(
1N
FS
L
select 
Step 1 Step m 
)1(
1F
)1(
1i
F
M
M
)1(
2i
F
M
)1(
1N
F
)2(
1FS
)2(
2N
FS
)(mFM
merge 
Step 2 
)0(F
Split 
select merge 
select 
)2(
2N
F
Mfinal 
)(mFS
optimize
Figure 1. Graphic illustration of PFS algorithm. 
 
In Table 1, SGC() invokes the SGC algorithm, 
and Opt() optimizes feature weights. The func-
tions split() and merge() are used to split and 
merge the feature space respectively.  
Two variations of the split() function are in-
vestigated in the paper and they are described 
below: 
1. random-split: randomly split a feature 
space into n- disjoint subspaces, and select 
an equal amount of features for each fea-
ture subspace.  
2. dimension-based-split: split a feature 
space into disjoint subspaces based on fea-
563
ture dimensions/variables, and select the 
number of features for each feature sub-
space with a certain distribution.  
We use a simple method for merge() in the 
experiments reported here, i.e., adding together 
the features from a set of selected feature sub-
spaces. 
One may image other variations of the split() 
function, such as allowing overlapping sub-
spaces. Other alternatives for merge() are also 
possible, such as randomly grouping the selected 
feature subspaces in the dimension-based split. 
Due to the limitation of the space, they are not 
discussed here. 
This approach can in principle be applied to 
other machine learning algorithms as well.  
4 Experiments with PFS for Edit Re-
gion Identification 
In this section, we will demonstrate the benefits 
of the PFS algorithm for identifying edit regions. 
The main reason that we use this task is that the 
edit region detection task uses features from sev-
eral levels, including prosodic, lexical, and syn-
tactic ones. It presents a big challenge to find a 
set of good features from a huge feature space.  
First we will present the additional features 
that the PFS algorithm allows us to include. 
Then, we will briefly introduce the variant of the 
Switchboard corpus used in the experiments. Fi-
nally, we will compare results from two variants 
of the PFS algorithm. 
4.1 Edit Region Identification Task 
In spoken utterances, disfluencies, such as self-
editing, pauses and repairs, are common phe-
nomena. Charniak and Johnson (2001) and Kahn 
et al (2005) have shown that improved edit re-
gion identification leads to better parsing accu-
racy ? they observe a relative reduction in pars-
ing f-score error of 14% (2% absolute) between 
automatic and oracle edit removal.  
The focus of our work is to show that our new 
PFS algorithm enables the exploration of much 
larger feature spaces for edit identification ? in-
cluding prosodic features, their confidence 
scores, and various feature combinations ? and 
consequently, it further improves edit region 
identification. Memory limitation prevents us 
from including all of these features in experi-
ments using the boosting method described in 
Johnson and Charniak (2004) and Zhang and 
Weng (2005). We couldn?t use the new features 
with the SGC algorithm either for the same rea-
son. 
The features used here are grouped according 
to variables, which define feature sub-spaces as 
in Charniak and Johnson (2001) and Zhang and 
Weng (2005). In this work, we use a total of 62 
variables, which include 16 1  variables from 
Charniak and Johnson (2001) and Johnson and 
Charniak (2004), an additional 29 variables from 
Zhang and Weng (2005), 11 hierarchical POS tag 
variables, and 8 prosody variables (labels and 
their confidence scores). Furthermore, we ex-
plore 377 combinations of these 62 variables, 
which include 40 combinations from Zhang and 
Weng (2005). The complete list of the variables 
is given in Table 2, and the combinations used in 
the experiments are given in Table 3. One addi-
tional note is that some features are obtained af-
ter the rough copy procedure is performed, where 
we used the same procedure as the one by Zhang 
and Weng (2005). For a fair comparison with the 
work by Kahn et al (2005), word fragment in-
formation is retained. 
4.2 The Re-segmented Switchboard Data 
In order to include prosodic features and be able 
to compare with the state-oft-art, we use the 
University of Washington re-segmented 
Switchboard corpus, described in Kahn et al 
(2005). In this corpus, the Switchboard sentences 
were segmented into V5-style sentence-like units 
(SUs) (LDC, 2004). The resulting sentences fit 
more closely with the boundaries that can be de-
tected through automatic procedures (e.g., Liu et 
al., 2005). Because the edit region identification 
results on the original Switchboard are not di-
rectly comparable with the results on the newly 
segmented data, the state-of-art results reported 
by Charniak and Johnson (2001) and Johnson 
and Charniak (2004) are repeated on this new 
corpus by Kahn et al (2005).  
The re-segmented UW Switchboard corpus is 
labeled with a simplified subset of the ToBI pro-
sodic system (Ostendorf et al, 2001).  The three 
simplified labels in the subset are p, 1 and 4, 
where p refers to a general class of disfluent 
boundaries (e.g., word fragments, abruptly short-
ened words, and hesitation); 4 refers to break 
level 4, which describes a boundary that has a 
boundary tone and phrase-final lengthening;
                                                 
1 Among the original 18 variables, two variables, Pf and Tf 
are not used in our experiments, because they are mostly 
covered by the other variables. Partial word flags only con-
tribute to 3 features in the final selected feature list. 
564
Categories Variable Name Short Description 
Orthographic 
Words W-5, ? , W+5 
Words at the current position and the left and right 5 
positions. 
Partial Word Flags P-3, ?, P+3 
Partial word flags at the current position and the left 
and right 3 positions 
Words 
Distance DINTJ, DW, DBigram, DTrigram Distance features 
POS Tags T-5, ?, T+5 
POS tags at the current position and the left and 
right 5 positions. Tags 
Hierarchical  
POS Tags (HTag) HT-5, ?, HT+5 
Hierarchical POS tags at the current position and the 
left and right 5 positions. 
HTag Rough Copy Nm, Nn, Ni, Nl, Nr, Ti Hierarchical POS rough copy features. 
Rough Copy 
Word Rough Copy WNm, WNi, WNl, WNr Word rough copy features. 
Prosody Labels PL0, ?, PL3 
Prosody label with largest post possibility at the 
current position and the right 3 positions. Prosody 
Prosody Scores PC0, ?, PC3 
Prosody confidence at the current position and the 
right 3 positions. 
Table 2. A complete list of variables used in the experiments. 
 
Categories Short Description Number of  Combinations 
Tags HTagComb Combinations among Hierarchical POS Tags  55 
Words OrthWordComb Combinations among Orthographic Words 55 
Tags 
WTComb 
WTTComb Combinations of Orthographic Words and POS Tags; Combination among POS Tags 176 
Rough Copy RCComb Combinations of HTag Rough Copy and Word Rough Copy 55 
Prosody PComb Combinations among Prosody, and with Words 36 
Table 3. All the variable combinations used in the experiments. 
 
and 1 is used to include the break index levels 
BL 0, 1, 2, and 3. Since the majority of the cor-
pus is labeled via automatic methods, the f-
scores for the prosodic labels are not high. In 
particular, 4 and p have f-scores of about 70% 
and 60% respectively (Wong et al, 2005). There-
fore, in our experiments, we also take prosody 
confidence scores into consideration. 
Besides the symbolic prosody labels, the cor-
pus preserves the majority of the previously an-
notated syntactic information as well as edit re-
gion labels.  
In following experiments, to make the results 
comparable, the same data subsets described in 
Kahn et al (2005) are used for training, develop-
ing and testing. 
4.3 Experiments 
The best result on the UW Switchboard for edit 
region identification uses a TAG-based approach 
(Kahn et al, 2005). On the original Switchboard 
corpus, Zhang and Weng (2005) reported nearly 
20% better results using the boosting method 
with a much larger feature space 2 . To allow 
comparison with the best past results, we create a 
new CME baseline with the same set of features 
as that used in Zhang and Weng (2005).  
We design a number of experiments to test the 
following hypotheses: 
1. PFS can include a huge number of new 
features, which leads to an overall per-
formance improvement. 
2. Richer context, represented by the combi-
nations of different variables, has a posi-
tive impact on performance. 
3. When the same feature space is used, PFS 
performs equally well as the original SGC 
algorithm. 
The new models from the PFS algorithm are 
trained on the training data and tuned on the de-
velopment data. The results of our experiments 
on the test data are summarized in Table 4. The 
first three lines show that the TAG-based ap-
proach is outperformed by the new CME base-
line (line 3) using all the features in Zhang and 
Weng (2005). However, the improvement from 
                                                 
2 PFS is not applied to the boosting algorithm at this time 
because it would require significant changes to the available 
algorithm.  
565
Results on test data Feature Space Codes number of features Precision Recall F-Value 
TAG-based result on UW-SWBD reported in Kahn et al (2005)    78.20 
CME with all the variables from Zhang and Weng (2005) 2412382 89.42 71.22 79.29 
CME with all the variables from Zhang and Weng (2005) + post 2412382 87.15 73.78 79.91 
+HTag +HTagComb +WTComb +RCComb 17116957 90.44 72.53 80.50 
+HTag +HTagComb +WTComb +RCComb +PL0 ? PL3 17116981 88.69 74.01 80.69 
+HTag +HTagComb +WTComb +RCComb +PComb: without cut 20445375 89.43 73.78 80.86 
+HTag +HTagComb +WTComb +RCComb +PComb: cut2 19294583 88.95 74.66 81.18 
+HTag +HTagComb +WTComb +RCComb +PComb: cut2 +Gau 19294583 90.37 74.40 81.61 
+HTag +HTagComb +WTComb +RCComb +PComb: cut2 +post 19294583 86.88 77.29 81.80 
+HTag +HTagComb +WTComb +RCComb +PComb: cut2 +Gau 
+post  19294583 87.79 77.02 82.05 
Table 4. Summary of experimental results with PFS. 
 
CME is significantly smaller than the reported 
results using the boosting method. In other 
words, using CME instead of boosting incurs a 
performance hit. 
The next four lines in Table 4 show that addi-
tional combinations of the feature variables used 
in Zhang and Weng (2005) give an absolute im-
provement of more than 1%. This improvement 
is realized through increasing the search space to 
more than 20 million features, 8 times the maxi-
mum size that the original boosting and CME 
algorithms are able to handle.  
Table 4 shows that prosody labels alone make 
no difference in performance. Instead, for each 
position in the sentence, we compute the entropy 
of the distribution of the labels? confidence 
scores. We normalize the entropy to the range [0, 
1], according to the formula below: 
            ( ) ( )UniformHpHscore ?= 1        (4) 
Including this feature does result in a good 
improvement. In the table, cut2 means that we 
equally divide the feature scores into 10 buckets 
and any number below 0.2 is ignored. The total 
contribution from the combined feature variables 
leads to a 1.9% absolute improvement. This con-
firms the first two hypotheses. 
When Gaussian smoothing (Chen and 
Rosenfeld, 1999), labeled as +Gau, and post-
processing (Zhang and Weng, 2005), labeled as 
+post, are added, we observe 17.66% relative 
improvement (or 3.85% absolute) over the previ-
ous best f-score of 78.2 from Kahn et al (2005). 
To test hypothesis 3, we are constrained to the 
feature spaces that both PFS and SGC algorithms 
can process. Therefore, we take all the variables 
from Zhang and Weng (2005) as the feature 
space for the experiments. The results are listed 
in Table 5. We observed no f-score degradation 
with PFS. Surprisingly, the total amount of time 
PFS spends on selecting its best features is 
smaller than the time SGC uses in selecting its 
best features. This confirms our hypothesis 3. 
 
Results on test data Split / Non-split Precision Recall F-Value 
non-split 89.42 71.22 79.29 
split by 4 parts 89.67 71.68 79.67 
split by 10 parts 89.65 71.29 79.42 
Table 5. Comparison between PFS and SGC with 
all the variables from Zhang and Weng (2005). 
 
The last set of experiments for edit identifica-
tion is designed to find out what split strategies 
PFS algorithm should adopt in order to obtain 
good results. Two different split strategies are 
tested here. In all the experiments reported so far, 
we use 10 random splits, i.e., all the features are 
randomly assigned to 10 subsets of equal size. 
We may also envision a split strategy that divides 
the features based on feature variables (or dimen-
sions), such as word-based, tag-based, etc. The 
four dimensions used in the experiments are 
listed as the top categories in Tables 2 and 3, and 
the results are given in Table 6.  
 
Results on test data Split  
Criteria 
Allocation 
Criteria Precision Recall F-Value 
Random Uniform 88.95 74.66 81.18 
Dimension Uniform 89.78 73.42 80.78 
Dimension Prior 89.78 74.01 81.14 
Table 6. Comparison of split strategies using feature space 
+HTag+HTagComb+WTComb+RCComb+PComb: cut2 
 
In Table 6, the first two columns show criteria 
for splitting feature spaces and the number of 
features to be allocated for each group. Random 
and Dimension mean random-split and dimen-
sion-based-split, respectively. When the criterion 
566
is Random, the features are allocated to different 
groups randomly, and each group gets the same 
number of features. In the case of dimension-
based split, we determine the number of features 
allocated for each dimension in two ways. When 
the split is Uniform, the same number of features 
is allocated for each dimension. When the split is 
Prior, the number of features to be allocated in 
each dimension is determined in proportion to 
the importance of each dimension. To determine 
the importance, we use the distribution of the 
selected features from each dimension in the 
model ?+ HTag + HTagComb + WTComb + 
RCComb + PComb: cut2?, namely: Word-based 
15%, Tag-based 70%, RoughCopy-based 7.5% 
and Prosody-based 7.5%3. From the results, we 
can see no significant difference between the 
random-split and the dimension-based-split. 
To see whether the improvements are trans-
lated into parsing results, we have conducted one 
more set of experiments on the UW Switchboard 
corpus. We apply the latest version of Charniak?s 
parser (2005-08-16) and the same procedure as 
Charniak and Johnson (2001) and Kahn et al 
(2005) to the output from our best edit detector 
in this paper. To make it more comparable with 
the results in Kahn et al (2005), we repeat the 
same experiment with the gold edits, using the 
latest parser. Both results are listed in Table 7. 
The difference between our best detector and the 
gold edits in parsing (1.51%) is smaller than the 
difference between the TAG-based detector and 
the gold edits (1.9%). In other words, if we use 
the gold edits as the upper bound, we see a rela-
tive error reduction of 20.5%. 
 
Parsing F-score 
Methods Edit  F-score 
Reported 
in Kahn et 
al. (2005) 
Latest 
Charniak 
Parser 
Diff. 
with 
Oracle 
Oracle 100 86.9 87.92 -- 
Kahn et 
al. (2005) 78.2 85.0 -- 1.90 
PFS best 
results 82.05 -- 86.41 1.51 
Table 7. Parsing F-score various different edit 
region identification results. 
                                                 
3 It is a bit of cheating to use the distribution from the se-
lected model. However, even with this distribution, we do 
not see any improvement over the version with random-
split. 
5 Conclusion 
This paper presents our progressive feature selec-
tion algorithm that greatly extends the feature 
space for conditional maximum entropy model-
ing. The new algorithm is able to select features 
from feature space in the order of tens of mil-
lions in practice, i.e., 8 times the maximal size 
previous algorithms are able to process, and 
unlimited space size in theory. Experiments on 
edit region identification task have shown that 
the increased feature space leads to 17.66% rela-
tive improvement (or 3.85% absolute) over the 
best result reported by Kahn et al (2005), and 
10.65% relative improvement (or 2.14% abso-
lute) over the new baseline SGC algorithm with 
all the variables from Zhang and Weng (2005). 
We also show that symbolic prosody labels to-
gether with confidence scores are useful in edit 
region identification task. 
In addition, the improvements in the edit iden-
tification lead to a relative 20% error reduction in 
parsing disfluent sentences when gold edits are 
used as the upper bound.  
Acknowledgement 
This work is partly sponsored by a NIST ATP 
funding. The authors would like to express their 
many thanks to Mari Ostendorf and Jeremy Kahn 
for providing us with the re-segmented UW 
Switchboard Treebank and the corresponding 
prosodic labels. Our thanks also go to Jeff Rus-
sell for his careful proof reading, and the anony-
mous reviewers for their useful comments. All 
the remaining errors are ours.  
References 
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A Maximum Entropy 
Approach to Natural Language Processing. Com-
putational Linguistics, 22 (1): 39-71.  
Eugene Charniak and Mark Johnson. 2001. Edit De-
tection and Parsing for Transcribed Speech. In 
Proceedings of the 2nd Meeting of the North Ameri-
can Chapter of the Association for Computational 
Linguistics, 118-126, Pittsburgh, PA, USA. 
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best Parsing and MaxEnt Discriminative 
Reranking. In Proceedings of the 43rd Annual 
Meeting of Association for Computational Linguis-
tics, 173-180, Ann Arbor, MI, USA. 
Stanley Chen and Ronald Rosenfeld. 1999. A Gaus-
sian Prior for Smoothing Maximum Entropy Mod-
567
els. Technical Report CMUCS-99-108, Carnegie 
Mellon University. 
John N. Darroch and D. Ratcliff. 1972. Generalized 
Iterative Scaling for Log-Linear Models. In Annals 
of Mathematical Statistics, 43(5): 1470-1480. 
Stephen A. Della Pietra, Vincent J. Della Pietra, and 
John Lafferty. 1997. Inducing Features of Random 
Fields. In IEEE Transactions on Pattern Analysis 
and Machine Intelligence, 19(4): 380-393. 
Joshua Goodman. 2002. Sequential Conditional Gen-
eralized Iterative Scaling. In Proceedings of the 
40th Annual Meeting of Association for Computa-
tional Linguistics, 9-16, Philadelphia, PA, USA.  
Mark Johnson, and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. In 
Proceedings of the 42nd Annual Meeting of the As-
sociation for Computational Linguistics, 33-39, 
Barcelona, Spain. 
Jeremy G. Kahn, Matthew Lease, Eugene Charniak, 
Mark Johnson, and Mari Ostendorf. 2005.  Effec-
tive Use of Prosody in Parsing Conversational 
Speech. In Proceedings of the 2005 Conference on 
Empirical Methods in Natural Language Process-
ing, 233-240, Vancouver, Canada. 
Rob Koeling. 2000. Chunking with Maximum En-
tropy Models. In Proceedings of the CoNLL-2000 
and LLL-2000, 139-141, Lisbon, Portugal. 
LDC. 2004. Simple MetaData Annotation Specifica-
tion. Technical Report of Linguistic Data Consor-
tium. (http://www.ldc.upenn.edu/Projects/MDE). 
Yang Liu, Elizabeth Shriberg, Andreas Stolcke, Bar-
bara Peskin, Jeremy Ang, Dustin Hillard, Mari Os-
tendorf, Marcus Tomalin, Phil Woodland and Mary 
Harper. 2005. Structural Metadata Research in the 
EARS Program. In Proceedings of the 30th 
ICASSP, volume V, 957-960, Philadelphia, PA, 
USA. 
Robert Malouf. 2002. A Comparison of Algorithms 
for Maximum Entropy Parameter Estimation. In 
Proceedings of the 6th  Conference on Natural Lan-
guage Learning (CoNLL-2002), 49-55, Taibei, 
Taiwan. 
Mari Ostendorf, Izhak Shafran, Stefanie Shattuck-
Hufnagel, Leslie Charmichael, and William Byrne. 
2001. A Prosodically Labeled Database of Sponta-
neous Speech. In Proceedings of the ISCA Work-
shop of Prosody in Speech Recognition and Under-
standing, 119-121, Red Bank, NJ, USA. 
Adwait Ratnaparkhi, Jeff Reynar and Salim Roukos. 
1994. A Maximum Entropy Model for Preposi-
tional Phrase Attachment. In Proceedings of the 
ARPA Workshop on Human Language Technology, 
250-255, Plainsboro, NJ, USA. 
Jeffrey C. Reynar and Adwait Ratnaparkhi. 1997. A 
Maximum Entropy Approach to Identifying Sen-
tence Boundaries. In Proceedings of the 5th Con-
ference on Applied Natural Language Processing, 
16-19, Washington D.C., USA. 
Stefan Riezler and Alexander Vasserman. 2004. In-
cremental Feature Selection and L1 Regularization 
for Relaxed Maximum-entropy Modeling. In Pro-
ceedings of the 2004 Conference on Empirical 
Methods in Natural Language Processing, 174-
181, Barcelona, Spain. 
Robert E. Schapire and Yoram Singer, 1999. Im-
proved Boosting Algorithms Using Confidence-
rated Predictions. Machine Learning, 37(3): 297-
336. 
Elizabeth Shriberg. 1994. Preliminaries to a Theory 
of Speech Disfluencies. Ph.D. Thesis, University of 
California, Berkeley.  
Vladimir Vapnik. 1995. The Nature of Statistical 
Learning Theory. Springer, New York, NY, USA. 
Darby Wong, Mari Ostendorf, Jeremy G. Kahn. 2005. 
Using Weakly Supervised Learning to Improve 
Prosody Labeling. Technical Report UWEETR-
2005-0003, University of Washington.  
Qi Zhang and Fuliang Weng. 2005. Exploring Fea-
tures for Identifying Edited Regions in Disfluent 
Sentences. In Proc. of the 9th International Work-
shop on Parsing Technologies, 179-185, Vancou-
ver, Canada. 
Yaqian Zhou, Fuliang Weng, Lide Wu, and Hauke 
Schmidt. 2003. A Fast Algorithm for Feature Se-
lection in Conditional Maximum Entropy Model-
ing. In Proceedings of the 2003 Conference on 
Empirical Methods in Natural Language Process-
ing, 153-159, Sapporo, Japan. 
568
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 179?185,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Exploring Features for Identifying Edited Regions in Disfluent Sentences 
 
 
Qi Zhang Fuliang Weng 
Department of Computer Science Research and Technology Center 
Fudan University Robert Bosch Corp. 
Shanghai, P.R.China 200433 Palo Alto, CA 94304 
qi_zhang@fudan.edu.cn fuliang.weng@rtc.bosch.com 
 
 
 
 
Abstract 
This paper describes our effort on the task 
of edited region identification for parsing 
disfluent sentences in the Switchboard 
corpus. We focus our attention on 
exploring feature spaces and selecting 
good features and start with analyzing the 
distributions of the edited regions and 
their components in the targeted corpus. 
We explore new feature spaces of a part-
of-speech (POS) hierarchy and relaxed for 
rough copy in the experiments. These 
steps result in an improvement of 43.98% 
percent relative error reduction in F-score 
over an earlier best result in edited 
detection when punctuation is included in 
both training and testing data [Charniak 
and Johnson 2001], and 20.44% percent 
relative error reduction in F-score over the 
latest best result where punctuation is 
excluded from the training and testing 
data [Johnson and Charniak 2004]. 
1 Introduction 
Repairs, hesitations, and restarts are common in 
spoken language, and understanding spoken 
language requires accurate methods for identifying 
such disfluent phenomena. Processing speech 
repairs properly poses a challenge to spoken dialog 
systems. Early work in this field is primarily based 
on small and proprietary corpora, which makes the 
comparison of the proposed methods difficult 
[Young and Matessa 1991, Bear et al 1992, 
Heeman & Allen 1994]. Because of the availability 
of the Switchboard corpus [Godfrey et al 1992] 
and other conversational telephone speech (CTS) 
corpora, there has been an increasing interest in 
improving the performance of identifying the 
edited regions for parsing disfluent sentences 
[Charniak and Johnson 2001, Johnson and 
Charniak 2004, Ostendorf et al 2004, Liu et al 
2005].  
 
In this paper we describe our effort towards the 
task of edited region identification with the 
intention of parsing disfluent sentences in the 
Switchboard corpus. A clear benefit of having 
accurate edited regions for parsing has been 
demonstrated by a concurrent effort on parsing 
conversational speech [Kahn et al2005]. Since 
different machine learning methods provide similar 
performances on many NLP tasks, in this paper, 
we focus our attention on exploring feature spaces 
and selecting good features for identifying edited 
regions. We start by analyzing the distributions of 
the edited regions and their components in the 
targeted corpus. We then design several feature 
spaces to cover the disfluent regions in the training 
data. In addition, we also explore new feature 
spaces of a part-of-speech hierarchy and extend 
candidate pools in the experiments. These steps 
result in a significant improvement in F-score over 
the earlier best result reported in [Charniak and 
Johnson 2001], where punctuation is included in 
both the training and testing data of the 
Switchboard corpus, and a significant error 
reduction in F-score over the latest best result 
[Johnson and Charniak 2004], where punctuation 
is ignored in both the training and testing data of 
the Switchboard corpus.  
 
179
In this paper, we follow the definition of [Shriberg 
1994] and others for speech repairs: A speech 
repair is divided into three parts: the reparandum, 
the part that is repaired; the interregnum, the part 
that can be either empty or fillers; and the 
repair/repeat, the part that replaces or repeats the 
reparandum. The definition can also be 
exemplified via the following utterance: 
 
N
repeatreparanda int erregnum
 ,  , this is  a big problem.This is you know	
 	
  
 
This paper is organized as follows. In section 2, we 
examine the distributions of the editing regions in 
Switchboard data. Section 3, then, presents the 
Boosting method, the baseline system and the 
feature spaces we want to explore. Section 4 
describes, step by step, a set of experiments that 
lead to a large performance improvement. Section 
5 concludes with discussion and future work. 
2 Repair Distributions in Switchboard 
We start by analyzing the speech repairs in the 
Switchboard corpus. Switchboard has over one 
million words, with telephone conversations on 
prescribed topics [Godfrey et al 1992]. It is full of 
disfluent utterances, and [Shriberg 1994, Shriberg 
1996] gives a thorough analysis and categorization 
of them. [Engel et al 2002] also showed detailed 
distributions of the interregnum, including 
interjections and parentheticals. Since the majority 
of the disfluencies involve all the three parts 
(reparandum, interregnum, and repair/repeat), the 
distributions of all three parts will be very helpful 
in constructing patterns that are used to identify 
edited regions.  
 
For the reparandum and repair types, we include 
their distributions with and without punctuation. 
We include the distributions with punctuation is to 
match with the baseline system reported in 
[Charniak and Johnson 2001], where punctuation 
is included to identify the edited regions. Resent 
research showed that certain punctuation/prosody 
marks can be produced when speech signals are 
available [Liu et al 2003]. The interregnum type, 
by definition, does not include punctuation.  
 
The length distributions of the reparanda in the 
training part of the Switchboard data with and 
without punctuation are given in Fig. 1. The 
reparanda with lengths of less than 7 words make 
up 95.98% of such edited regions in the training 
data. When we remove the punctuation marks, 
those with lengths of less than 6 words reach 
roughly 96%. Thus, the patterns that consider only 
reparanda of length 6 or less will have very good 
coverage. 
 
Length distribution of reparanda
0%
10%
20%
30%
40%
50%
1 2 3 4 5 6 7 8 9 10
With punctation Without punctation
 
 
Figure 1. Length distribution of reparanda in 
Switchboard training data. 
 
Length distribution of 
repairs/repeats/restarts 
0%
10%
20%
30%
40%
50%
0 1 2 3 4 5 6 7 8 9
With punctation Without punctation
Figure 2. Length distribution of 
repairs/repeats/restarts in Switchboard training data. 
 
Length distribution of interregna
0%
20%
40%
60%
80%
100%
0 1 2 3 4 5 6 7 8 9 10
 
Figure 3. Length distribution of interregna in 
Switchboard training data. 
 
The two repair/repeat part distributions in the 
training part of the Switchboard are given in Fig. 2. 
The repairs/repeats with lengths less than 7 words 
180
make 98.86% of such instances in the training data. 
This gives us an excellent coverage if we use 7 as 
the threshold for constructing repair/repeat patterns. 
 
The length distribution of the interregna of the 
training part of the Switchboard corpus is shown in 
Fig. 3. We see that the overwhelming majority has 
the length of one, which are mostly words such as 
?uh?, ?yeah?, or ?uh-huh?. 
 
In examining the Switchboard data, we noticed that 
a large number of reparanda and repair/repeat pairs 
differ on less than two words, i.e. ?as to, you know, 
when to?1, and the amount of the pairs differing on 
less than two POS tags is even bigger. There are 
also cases where some of the pairs have different 
lengths. These findings provide a good base for our 
feature space. 
3 Feature Space Selection for Boosting 
We take as our baseline system the work by 
[Charniak and Johnson 2001]. In their approach, 
rough copy is defined to produce candidates for 
any potential pairs of reparanda and repairs. A 
boosting algorithm [Schapire and Singer 1999] is 
used to detect whether a word is edited. A total of 
18 variables are used in the algorithm. In the rest 
of the section, we first briefly introduce the 
boosting algorithm, then describe the method used 
in [Charniak and Johnson 2001], and finally we 
contrast our improvements with the baseline 
system. 
3.1 Boosting Algorithm 
Intuitively, the boosting algorithm is to combine a 
set of simple learners iteratively based on their 
classification results on a set of training data. 
Different parts of the training data are scaled at 
each iteration so that the parts of the data previous 
classifiers performed poorly on are weighted 
higher. The weighting factors of the learners are 
adjusted accordingly.  
 
We re-implement the boosting algorithm reported 
by [Charniak and Johnson 2001] as our baseline 
system in order to clearly identify contributing 
                                                          
1  ?as to?  is the edited region. Italicized words in the 
examples are edited words 
factors in performance.  Each word token is 
characterized by a finite tuple of random variables  
(Y, X1,..., Xm ). 
Y is  the conditioned variables and ranges from    
{-1,+1}, with Y = +1 indicating that the word is 
edited. X1,..., Xm  are the conditioning variables; 
each variable jX  ranges over a finite set j? . The 
goal of the classifer is to predict the value of Y 
given a value for X1,..., Xm .  
 
A boosting classifier is a linear combination of n 
features to define the prediction variable Z. 
                          ?
=
=
n
i
iiFZ
1
?                (1) 
where ?i is the weight to be estimated for feature ?i. 
?i is a set of variable-value pairs, and each Fi has 
the form of: 
                  Fi = (X j = x j )
<X j ,x j >?? i
?         (2) 
with X?s being conditioning variables and x?s being 
values.   
 
Each component in the production for Fi  is 
defined as: 
 
       (X j = x j ) =
1  < X j = x j >? ?i
0   otherwise
? ? ?      (3) 
 
In other words, Fi is 1 if and only if all the 
variable-value pairs for the current position belong 
to ?i.  
 
The prediction made by the classifier is 
|Z| Z/ sign(Z)= . Intuitively, our goal is to adjust 
the vector of feature weights 1( ,...., )n? ? ?=K  to 
minimize the expected misclassification rate 
]E[sign(Z) Y? . This function is difficult to 
minimize, so our boosting classifier minimizes the 
expected boost loss )][(exp(-YZE?t  as in [Collins 
2000], where ][E?t ?  is the expectation on the 
empirical training corpus distribution. In our 
implementation, each learner contains only one 
variable. The feature weights are adjusted 
iteratively, one weight per iteration. At each 
iteration, it reduces the boost loss on the training 
corpus. In our experiments, ?K is obtained after 
181
1500 iterations, and contains around 1350 non-zero 
feature weights. 
3.2 Charniak-Johnson approach 
In [Charniak and Johnson 2001], identifying edited 
regions is considered as a classification problem, 
where each word is classified either as edited or 
normal. The approach takes two steps. The first 
step is to find rough copy. Then, a number of 
variables are extracted for the boosting algorithm. 
In particular, a total of 18 different conditioning 
variables are used to predict whether the current 
word is an edited word or a non-edited word. The 
18 different variables listed in Table 1 correspond 
to the 18 different dimensions/factors for the 
current word position. Among the 18 variables, six 
of them, Nm, Nu, Ni, Nl, Nr and Tf , depend on the 
identification of a rough copy. 
 
For convenience, their definition of a rough copy is 
repeated here. A rough copy in a string of tagged 
words has the form of 21 ?? ?? , where: 
1. 1?  (the source) and 2?  (the copy) both 
begin   with    non-punctuation, 
2. the strings of non-punctuation POS tag 
of   1?  and 2?  are identical, 
3. ?  (the free final) consists of zero or 
more sequences of a free final word  (see 
below) followed by optional punctuation, 
4. ?  (the interregnum) consists of 
sequences of an interregnum string (see 
below) followed by optional punctuation. 
 
The set of free final words includes all partial 
words and a small set of conjunctions, adverbs and 
miscellanea. The set of interregnum strings 
consists of a small set of expressions such as uh, 
you know, I guess, I mean, etc.  
3.3 New Improvements 
Our improvements to the Charniak-Johnson 
method can be classified into three categories with 
the first two corresponding to the twp steps in their 
method. The three categories of improvements are 
described in details in the following subsections.  
3.3.1 Relaxing Rough Copy  
We relax the definition for rough copy, because 
more than 94% of all edits have both reparandum 
and repair, while the rough copy defined in 
[Charniak and Johnson 2001] only covers 77.66% 
of such instances.  
 
Two methods are used to relax the rough copy 
definition. The first one is to adopt a hierarchical 
POS tag set: all the Switchboard POS tags are 
further classified into four major categories: N 
(noun related), V (verb related), Adj (noun 
modifiers), Adv (verb modifiers). Instead of 
requiring the exact match of two POS tag 
sequences, we also consider two sequences as a 
Variables Name Short description 
X1 W0 The current orthographic word. 
X2 ? X5 P0,P1,P2,Pf Partial word flags for the current position, the next two to the right, and the first one 
in a sequence of free-final words (partial, conjunctions, etc.) to the right of the 
current position. 
X6 ? X10 T-1,T0,T1,T2,Tf Part of speech tags for the left position, the current position, the next two positions 
to the right, and the first free-final word position to the right of the current position.
X11 Nm Number of words in common in reparandum and repair 
X12 Nn Number of words in reparandum but not repair 
X13 Ni Number of words in interregnum 
X14 Nl Number of words to the left edge of reparandum 
X15 Nr Number of words to the right edge of reparandum 
X16 Ct The first non-punctuation tag to the right of the current position 
X17 Cw The first non-punctuation word to the right of the current position 
X18 Ti The tag of the first word right after the interregnum that is right after the current 
word.  
 
Table 1. Descriptions of the 18 conditioning variables from [Charniak and Johnson 2001] 
 
182
rough copy if their corresponding major categories 
match. This relaxation increases the rough copy 
coverage, (the percent of words in edited regions 
found through the definition of rough copy), from 
77.66% to 79.68%.  
 
The second is to allow one mismatch in the two 
POS sequences. The mismatches can be an 
addition, deletion, or substitution. This relaxation 
improves the coverage from 77.66% to 85.45%. 
Subsequently, the combination of the two 
relaxations leads to a significantly higher coverage 
of 87.70%. Additional relaxation leads to excessive 
candidates and worse performance in the 
development set. 
3.3.2 Adding New Features  
We also include new features in the feature set: 
one is the shortest distance (the number of words) 
between the current word and a word of the same 
orthographic form to the right, if that repeated 
word exists; another is the words around the 
current position. Based on the distributional 
analysis in section 2, we also increase the window 
sizes for POS tags ( 5 5,...,T T? ) and words 
( 5 5,...,W W? ) to ?5 and partial words ( 3 3,...,P P? ) 
to ?3, extending Ti and Pj.  
3.3.3 Post Processing Step 
In addition to the two categories, we try to use 
contextual patterns to address the independency of 
variables in the features. The patterns have been 
extracted from development and training data, to 
deal with certain sequence-related errors, e.g.,  
E N E ? E E E, 
which means that if the neighbors on both sides of 
a word are classified into EDITED, it should be 
classified into EDITED as well.  
4 Experimental Results  
We conducted a number of experiments to test the 
effectiveness of our feature space exploration. 
Since the original code from [Charniak and 
Johnson 2001] is not available, we conducted our 
first experiment to replicate the result of their 
baseline system described in section 3. We used 
the exactly same training and testing data from the 
Switchboard corpus as in [Charniak and Johnson 
2001]. The training subset consists of all files in 
the sections 2 and 3 of the Switchboard corpus. 
Section 4 is split into three approximately equal 
size subsets. The first of the three, i.e., files 
sw4004.mrg to sw4153.mrg, is the testing corpus. 
The files sw4519.mrg to sw4936.mrg are the 
development corpus. The rest files are reserved for 
other purposes.  When punctuation is included in 
both training and testing, the re-established 
baseline has the precision, recall, and F-score of 
94.73%, 68.71% and 79.65%, respectively. These 
results are comparable with the results from 
[Charniak & Johnson 2001], i.e., 95.2%, 67.8%, 
and 79.2% for precision, recall, and f-score, 
correspondingly. 
 
In the subsequent experiments, the set of additional 
feature spaces described in section 3 are added, 
step-by-step. The first addition includes the 
shortest distance to the same word and window 
size increases. This step gives a 2.27% 
improvement on F-score over the baseline. The 
next addition is the introduction of the POS 
hierarchy in finding rough copies. This also gives 
more than 3% absolute improvement over the 
baseline and 1.19% over the expanded feature set 
model. The addition of the feature spaces of 
relaxed matches for words, POS tags, and POS 
hierarchy tags all give additive improvements, 
which leads to an overall of 8.95% absolute 
improvement over the re-implemented baseline, or 
43.98% relative error reduction on F-score.  
 
When compared with the latest results from 
[Johnson and Charniak 2004], where no 
punctuations are used for either training or testing 
data, we also observe the same trend of the 
improved results. Our best result gives 4.15% 
absolute improvement over their best result, or 
20.44% relative error reduction in f-scores. As a 
sanity check, when evaluated on the training data 
as a cheating experiment, we show a remarkable 
consistency with the results for testing data.  
 
For error analysis, we randomly selected 100 
sentences with 1673 words total from the test 
sentences that have at least one mistake. Errors can 
be divided into two types, miss (should be edited) 
and false alarm (should be noraml). Among the 
207 misses, about 70% of them require some 
phrase level analysis or acoustic cues for phrases. 
183
For example, one miss is ?because of the friends 
because of many other things?, an error we would 
have a much better chance of correct identification, 
if we were able to identify prepositional phrases 
reliably. Another example is ?most of all my 
family?. Since it is grammatical by itself, certain 
prosodic information in between ?most of? and ?all 
my family? may help the identification. [Ostendorf 
et al 2004] reported that interruption point could 
help parsers to improve results.  [Kahn et al 2005] 
also showed that prosody information could help 
parse disfluent sentences. The second major class 
of the misses is certain short words that are not 
labeled consistently in the corpus. For example, 
?so?, ?and?, and ?or?, when they occur in the 
beginning of a sentence, are sometimes labeled as 
edited, and sometimes just as normal. The last 
category of the misses, about 5.3%, contains the 
ones where the distances between reparanda and 
repairs are often more than 10 words.  
 
Among the 95 false alarms, more than three 
quarters of misclassified ones are related to certain 
grammatical constructions. Examples include cases 
like, ?the more ? the more? and ?I think I 
should ??. These cases may be fixable if more 
elaborated grammar-based features are used.  
5 Conclusions  
This paper reports our work on identifying edited 
regions in the Switchboard corpus. In addition to a 
Results on testing data Results on training data 
with punctuation Punctuation on both  No punctuation on both 
Method codes 
Precision Recall f-score Precision Recall f-score Precision Recall f-score
CJ?01    95.2 67.8 79.2    
JC?04 p       82.0 77.8 79.7 
 R CJ?01 94.9 71.9 81.81 94.73 68.71 79.65 91.46 64.42 75.59 
+d 94.56 78.37 85.71 94.47 72.31 81.92 91.79 68.13 78.21 
+d+h 94.23 81.32 87.30 94.58 74.12 83.11 91.56 71.33 80.19 
+d+rh 94.12 82.61 87.99 92.61 77.15 84.18 89.92 72.68 80.39 
+d+rw 96.13 82.45 88.77 94.79 75.43 84.01 92.17 70.79 80.08 
+d+rw+rh 94.42 84.67 89.28 94.57 77.93 85.45 92.61 73.46 81.93 
+d+rw+rt+wt 94.43 84.79 89.35 94.65 76.61 84.68 92.08 72.61 81.19 
+d+rw+rh+wt 94.58 85.21 89.65 94.72 79.22 86.28 92.69 75.30 83.09 
+d+rw+rh+wt+ps 93.69 88.62 91.08 93.81 83.94 88.60 89.70 78.71 83.85 
 
Table 2. Result summary for various feature spaces. 
 
Method codes Method description 
CJ?01 Charniak and Johnson 2001 
JC?04 p Johnson and Charniak 2004, parser results 
R CJ?01 Duplicated results for Charniak and Johnson 2001 
+d Distance + window sizes 
+d+h Distance + window sizes + POS hierarchy in rough copy 
+d+rh Distance + window sizes + relaxed POS hierarchy in rough copy 
+d+rw Distance + window sizes + relaxed word in rough copy 
+d+rw+rh Distance + window sizes + relaxed word and POS hierarchy in rough copy 
+d+rw+rt+wt Distance + window sizes + word & tag pairs + relaxed word and POS in rough copy 
+d+rw+rh+wt Distance + window sizes + word & tag pairs + relaxed word and POS hierarchy in 
rough copy 
+d+rw+rh+wt+ps Distance + window sizes + word & tag pairs + relaxed word and POS hierarchy in 
rough copy + pattern substitution 
 
Table 3. Description of method codes used in the result table. 
184
distributional analysis for the edited regions, a 
number of feature spaces have been explored and 
tested to show their effectiveness. We observed a 
43.98% relative error reduction on F-scores for the 
baseline with punctuation in both training and 
testing [Charniak and Johnson 2001]. Compared 
with the reported best result, the same approach 
produced a 20.44% of relative error reduction on 
F-scores when punctuation is ignored in training 
and testing data [Johnson and Charniak 2004]. The 
inclusion of both hierarchical POS tags and the 
relaxation for rough copy definition gives large 
additive improvements, and their combination has 
contributed to nearly half of the gain for the test 
set with punctuation and about 60% of the gain for 
the data without punctuation.  
 
Future research would include the use of other 
features, such as prosody, and the integration of 
the edited region identification with parsing.   
6 Acknowledgement 
This work has been done while the first author is 
working at the Research and Technology Center of 
Robert Bosch Corp. The research is partly 
supported by the NIST ATP program. The authors 
would also like to express their thanks to Tess 
Hand-Bender for her proof-reading and Jeremy G. 
Kahn for many useful comments. Nevertheless, all 
the remaining errors are ours. 
References  
John Bear, John Dowding and Elizabeth Shriberg. 1992. 
Integrating Multiple Knowledge Sources for Detection 
and Correction of Repairs in Human-Computer Dialog. 
Proc. Annual Meeting of the Association for 
Computational Linguistics. 1992. 
 
Charniak, Eugene and Mark Johnson. 2001. Edit 
Detection and Parsing for Transcribed Speech. Proc. of 
the 2nd Meeting of the North American Chapter of the 
Association for Computational Linguistics, pp 118-126. 
 
Collins, M. 2000. Discriminative reranking for natural 
language parsing. Proc.  ICML 2000. 
 
Engel, Donald, Eugene Charniak, and Mark Johnson. 
2002. Parsing and Disfluency Placement. Proc.  
EMNLP, pp 49-54, 2002.  
 
Godfrey, J.J., Holliman, E.C. and McDaniel, J. 
SWITCHBOARD: Telephone speech corpus for 
research and development, Proc. ICASSP, pp 517-520, 
1992. 
 
Heeman, Peter, and James Allen. 1994.  Detecting and 
Correcting Speech Repairs. Proc. of the annual meeting 
of the Association for Computational Linguistics. Las 
Cruces, New Mexico,  pp 295-302, 1994.  
 
Johnson, Mark, and Eugene Charniak. 2004. A TAG-
based noisy-channel model of speech repairs. Proc. of 
the 42nd Annual Meeting of the Association for 
Computational Linguistics. 
 
Kahn, Jeremy G., Mari Ostendorf, and Ciprian Chelba. 
2004. Parsing Conversational Speech Using Enhanced 
Segmentation. Proc. of HLT-NAACL, pp 125-138, 2004. 
 
Kahn, Jeremy G., Matthew Lease, Eugene Charniak, 
Mark Johnson and Mari Ostendorf 2005. Effective Use 
of Prosody in Parsing Conversational Speech. Proc. 
EMNLP, 2005. 
 
Liu, Yang, Elizabeth Shriberg, Andreas Stolcke, 
Barbara Peskin, Jeremy Ang, Dustin Hillard, Mari 
Ostendorf, Marcus Tomalin, Phil Woodland, Mary 
Harper. 2005. Structural Metadata Research in the 
EARS Program. Proc. ICASSP, 2005. 
 
Liu, Yang, Elizabeth Shriberg, Andreas Stolcke. 2003. 
Automatic disfluency identification in conversational 
speech using multiple knowledge sources Proc. 
Eurospeech, 2003 
 
Ostendorf, Mari, Jeremy G. Kahn, Darby Wong, Dustin 
Hillard, and William McNeill. Leveraging Structural 
MDE in Language Processing. EARS RT04 Workshop, 
2004. 
 
Robert E. Schapire and Yoram Singer, 1999. Improved 
Boosting Algorithms Using Confidence-rated 
Predictions. Machine Learning 37(3): 297-336, 1999. 
 
Shriberg, Elizabeth. 1994. Preliminaries to a Theory of 
Speech Disfluencies. Ph.D. Thesis. UC Berkeley,1994.  
 
Shriberg, Elizabeth. 1996. Disfluencies in Switchboard. 
Proc. of ICSLP. 1996. 
Young, S. R. and Matessa, M. (1991). Using pragmatic 
and semantic knowledge to correct parsing of spoken 
language utterances. Proc. Eurospeech 91, Genova, 
Italy. 
185
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 24?25,
Vancouver, October 2005.
A Flexible Conversational Dialog System for MP3 Player
Fuliang Weng
1
 Lawrence Cavedon
2
 Badri Raghunathan
1
 Danilo Mirkovic
2 
Ben Bei
1
Heather Pon-Barry
1
 Harry Bratt
3
 Hua Cheng
2
 Hauke Schmidt
1
 Rohit Mishra
4
 Brian Lathrop
4
Qi Zhang
1
   Tobias Scheideck
1
   Kui Xu
1
    Tess Hand-Bender
1
   Sandra Upson
1
     Stanley Peters
2
Liz Shriberg
3
 Carsten Bergmann
4
Research and Technology Center, Robert Bosch Corp., Palo Alto, California
1
Center for Study of Language and Information, Stanford University, Stanford, California
2
Speech Technology and Research Lab, SRI International, Menlo Park, California
3
Electronics Research Lab, Volkswagen of America, Palo Alto, California
4
{Fuliang.weng,badri.raghunathan,hauke.Schmidt}@rtc.bosch.com
{lcavedon,huac,peters}@csli.Stanford.edu
{harry,ees}@speech.sri.com
{rohit.mishra,carsten.bergmann}@vw.com
1 Abstract
In recent years, an increasing number of new de-
vices have found their way into the cars we drive.
Speech-operated devices in particular provide a
great service to drivers by minimizing distraction,
so that they can keep their hands on the wheel and
their eyes on the road. This presentation will dem-
onstrate our latest development of an in-car dialog
system for an MP3 player designed under a joint
research effort from Bosch RTC, VW ERL, Stan-
ford CSLI, and SRI STAR Lab funded by NIST
ATP [Weng et al2004] with this goal in mind.
This project has developed a number of new tech-
nologies, some of which are already incorporated
in the system.  These include: end-pointing with
prosodic cues, error identification and recovering
strategies, flexible multi-threaded, multi-device
dialog management, and content optimization and
organization strategies. A number of important
language phenomena are also covered in the sys-
tem?s functionality. For instance, one may use
words relying on context, such as ?this,? ?that,? ?it,?
and ?them,? to reference items mentioned in par-
ticular use contexts. Different types of verbal revi-
sion are also permitted by the system, providing a
great convenience to its users. The system supports
multi-threaded dialogs so that users can diverge to
a different topic before the current one is finished
and still come back to the first after the second
topic is done. To lower the cognitive load on the
drivers, the content optimization component orga-
nizes any information given to users based on on-
tological structures, and may also refine users?
queries via various strategies. Domain knowledge
is represented using OWL, a web ontology lan-
guage recommended by W3C, which should
greatly facilitate its portability to new domains.
The spoken dialog system consists of a number of
components (see Fig. 1 for details). Instead of the
hub architecture employed by Communicator pro-
jects [Senef et al 1998], it is developed in Java and
uses a flexible event-based, message-oriented mid-
dleware. This allows for dynamic registration of
new components. Among the component modules
in Figure 1, we use the Nuance speech recognition
engine with class-based ngrams and dynamic
grammars, and the Nuance Vocalizer as the TTS
engine. The Speech Enhancer removes noises and
echo. The Prosody module will provide additional
features to the Natural Language Understanding
(NLU) and Dialogue Manager (DM) modules to
improve their performance.
The NLU module takes a sequence of recognized
words and tags, performs a deep linguistic analysis
with probabilistic models, and produces an XML-
based semantic feature structure representation.
Parallel to the deep analysis, a topic classifier as-
signs top n topics to the utterance, which are used
in the cases where the dialog manager cannot make
24
any sense of the parsed structure. The NLU mod-
ule also supports dynamic updates of the knowl-
edge base.
The CSLI DM module mediates and manages in-
teraction. It uses the dialogue-move approach to
maintain dialogue context, which is then used to
interpret incoming utterances (including fragments
and revisions), resolve NPs, construct salient re-
sponses, track issues, etc. Dialogue states can also
be used to bias SR expectation and improve SR
performance, as has been performed in previous
applications of the DM. Detailed descriptions of
the DM can be found in [Lemon et al2002; Mirk-
ovic & Cavedon 2005].
The Knowledge Manager (KM) controls access to
knowledge base sources (such as domain knowl-
edge and device information) and their updates.
Domain knowledge is structured according to do-
main-dependent ontologies. The current KM
makes use of OWL, a W3C standard, to represent
the ontological relationships between domain enti-
ties. Prot?g? (http://protege.stanford.edu), a do-
main-independent ontology tool, is used to
maintain the ontology offline. In a typical interac-
tion, the DM converts a user?s query into a seman-
tic frame (i.e. a set of semantic constraints) and
sends this to the KM via the content optimizer.
The Content Optimization module acts as an in-
termediary between the dialogue management
module and the knowledge management module
during the query process. It receives semantic
frames from the DM, resolves possible ambigui-
ties, and queries the KM. Depending on the items
in the query result as well as the configurable
properties, the module selects and performs an ap-
propriate optimization strategy.
Early evaluation shows that the system has a
task completion rate of 80% on 11 tasks of MP3
player domain, ranging from playing requests to
music database queries. Porting to a restaurant se-
lection domain is currently under way.
References
Seneff, Stephanie, Ed Hurley, Raymond Lau, Christine Pao,
Philipp Schmid, and Victor Zue, GALAXY-II: A Reference
Architecture for Conversational System Development, In-
ternational Conference on Spoken Language Processing
(ICSLP), Sydney, Australia, December 1998.
Lemon, Oliver, Alex Gruenstein, and Stanley Peters, Collabo-
rative activities and multi-tasking in dialogue systems,
Traitement Automatique des Langues (TAL), 43(2), 2002.
Mirkovic, Danilo, and Lawrence Cavedon, Practical Multi-
Domain, Multi-Device Dialogue Management, Submitted
for publication, April 2005.
Weng, Fuliang, Lawrence Cavedon, Badri Raghunathan, Hua
Cheng, Hauke Schmidt, Danilo Mirkovic, et al, Develop-
ing a conversational dialogue system for cognitively over-
loaded users, International Conference on Spoken
Language Processing (ICSLP), Jeju, Korea, October 2004.
25
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 904?912,
Beijing, August 2010
2D Trie for Fast Parsing
Xian Qian, Qi Zhang, Xuanjing Huang, Lide Wu
Institute of Media Computing
School of Computer Science, Fudan University
{xianqian, qz, xjhuang, ldwu}@fudan.edu.cn
Abstract
In practical applications, decoding speed
is very important. Modern structured
learning technique adopts template based
method to extract millions of features.
Complicated templates bring about abun-
dant features which lead to higher accu-
racy but more feature extraction time. We
propose Two Dimensional Trie (2D Trie),
a novel efficient feature indexing structure
which takes advantage of relationship be-
tween templates: feature strings generated
by a template are prefixes of the features
from its extended templates. We apply
our technique to Maximum Spanning Tree
dependency parsing. Experimental results
on Chinese Tree Bank corpus show that
our 2D Trie is about 5 times faster than
traditional Trie structure, making parsing
speed 4.3 times faster.
1 Introduction
In practical applications, decoding speed is very
important. Modern structured learning technique
adopts template based method to generate mil-
lions of features. Such as shallow parsing (Sha
and Pereira, 2003), named entity recognition
(Kazama and Torisawa, ), dependency parsing
(McDonald et al, 2005), etc.
The problem arises when the number of tem-
plates increases, more features generated, mak-
ing the extraction step time consuming. Espe-
cially for maximum spanning tree (MST) depen-
dency parsing, since feature extraction requires
quadratic time even using a first order model. Ac-
cording to Bohnet?s report (Bohnet, 2009), a fast
FeatureGenerationTemplate:p .word+p .pos0 0 Feature:lucky/ADJ
Index:3228~3233
FeatureRetrievalParse Tree
Build lattice, inference etc.
Figure 1: Flow chart of dependency parsing.
p0.word, p0.pos denotes the word and POS tag
of parent node respectively. Indexes correspond
to the features conjoined with dependency types,
e.g., lucky/ADJ/OBJ, lucky/ADJ/NMOD, etc.
feature extraction beside of a fast parsing algo-
rithm is important for the parsing and training
speed. He takes 3 measures for a 40X speedup,
despite the same inference algorithm. One impor-
tant measure is to store the feature vectors in file
to skip feature extraction, otherwise it will be the
bottleneck.
Now we quickly review the feature extraction
stage of structured learning. Typically, it consists
of 2 steps. First, features represented by strings
are generated using templates. Then a feature in-
dexing structure searches feature indexes to get
corresponding feature weights. Figure 1 shows
the flow chart of MST parsing, where p0.word,
p0.pos denote the word and POS tag of parent
node respectively.
We conduct a simple experiment to investi-
gate decoding time of MSTParser, a state-of-the-
art java implementation of dependency parsing 1.
Chinese Tree Bank 6 (CTB6) corpus (Palmer and
1http://sourceforge.net/projects/mstparser
904
Step Feature Index Other Total
Generation Retrieval
Time 300.27 61.66 59.48 421.41
Table 1: Time spent of each step (seconds) of
MSTParser on CTB6 standard test data (2660 sen-
tences). Details of the hardware and corpus are
described in section 5
Xue, 2009) with standard train/development/test
split is used for evaluation. Experimental results
are shown in Table 1. The observation is that time
spent of inference is trivial compared with feature
extraction. Thus, speeding up feature extraction is
critical especially when large template set is used
for high accuracy.
General indexing structure such as Hash and
Trie does not consider the relationships between
templates, therefore they could not speed up fea-
ture generation, and are not completely efficient
for searching feature indexes. For example, fea-
ture string s1 generated by template ?p0.word?
is prefix of feature s2 from template ?p0.word +
c0.word? (word pair of parent and child), hence
index of s1 could be used for searching s2. Fur-
ther more, if s1 is not in the feature set, then s2
must be absent, its generation can be skipped.
We propose Two Dimensional Trie (2D Trie),
a novel efficient feature indexing structure which
takes advantage of relationship between tem-
plates. We apply our technique to Maximum
Spanning Tree dependency parsing. Experimental
results on CTB6 corpus show that our 2D Trie is
about 5 times faster than traditional Trie structure,
making parsing speed 4.3 times faster.
The paper is structured as follows: in section 2,
we describe template tree which represents rela-
tionship between templates; in section 3, we de-
scribe our new 2D Trie structure; in section 4, we
analyze the complexity of the proposed method
and general string indexing structures for parsing;
experimental results are shown in section 5; we
conclude the work in section 6.
2 Template tree
2.1 Formulation of template
A template is a set of template units which are
manually designed: T = {t1, . . . , tm}. For con-
Unit Meaning
p?i/pi the ith node left/right to parent node
c?i/ci the ith node left/right to child node
r?i/ri the ith node left/right to root node
n.word word of node n
n.pos POS tag of node n
n.length word length of node n
|l conjoin current feature with linear distance
between child node and parent node
|d conjoin current feature with direction of de-
pendency (left/right)
Table 2: Template units appearing in this paper
venience, we use another formulation: T = t1 +
. . .+tm. All template units appearing in this paper
are described in Table 2, most of them are widely
used. For example, ?T = p0.word + c0.word|l ?
denotes the word pair of parent and child nodes,
conjoined with their distance.
2.2 Template tree
In the rest of the paper, for simplicity, let si be a
feature string generated by template Ti.
We define the relationship between templates:
T1 is the ancestor of T2 if and only T1 ? T2, and
T2 is called the descendant of T1. Recall that,
feature string s1 is prefix of feature s2. Suppose
T3 ? T1 ? T2, obviously, the most efficient way
to look up indexes of s1, s2, s3 is to search s3 first,
then use its index id3 to search s1, and finally use
id1 to search s2. Hence the relationship between
T2 and T3 can be neglected.
Therefore we define direct ancestor of T1: T2
is a direct ancestor of T1 if T2 ? T1, and there is
no template T ? such that T2 ? T ? ? T1. Corre-
spondingly, T1 is called the direct descendant of
T2.
Template graph G = (V,E) is a directed graph
that represents the relationship between templates,
where V = {T1, . . . , Tn} is the template set, E =
{e1, . . . , eN} is the edge set. Edge from Ti to Tj
exists, if and only if Ti is the direct ancestor of
Tj . For templates having no ancestor, we add an
empty template as their common direct ancestor,
which is also the root of the graph.
The left part of Figure 2 shows a template
graph for templates T1 =p0.word, T2 =p0.pos ,
T3 =p0.word + p0.pos. In this example, T3 has 2
direct ancestors, but in fact s3 has only one prefix
905
p .word0
p .word +p pos0 0.
root
p .word0
root
p .pos0
p .pos0 p .pos0
Figure 2: Left graph shows template graph for
T1 =p0.word, T2 =p0.pos , T3 =p0.word +
p0.pos. Right graph shows the corresponding tem-
plate tree, where each vertex saves the subset of
template units that do not belong to its father
which depends on the order of template units in
generation step. If s3 = s1 + s2, then its prefix is
s1, otherwise its prefix is s2. In this paper, we sim-
ply use the breadth-first tree of the graph for dis-
ambiguation, which is called template tree. The
only direct ancestor T1 of T2 in the tree is called
father of T2, and T2 is a child of T1. The right
part of Figure 2 shows the corresponding template
tree, where each vertex saves the subset of tem-
plate units that do not belong to its father.
2.3 Virtual vertex
Consider the template tree in the left part of Figure
3, red vertex and blue vertex are partially over-
lapped, their intersection is p0.word, if string s
from template T =p0.word is absent in feature set,
then both nodes can be neglected. For efficiently
pruning candidate templates, each vertex in tem-
plate tree is restricted to have exactly one template
unit (except root). Another important reason for
such restriction will be given in the next section.
To this end, virtual vertexes are created for
multi-unit vertexes. For efficient pruning, the new
virtual vertex should extract the most common
template unit. A natural goal is to minimize the
creation number. Here we use a simple greedy
strategy, for the vertexes sharing a common fa-
ther, the most frequent common unit is extracted
as new vertex. Virtual vertexes are iteratively cre-
ated in this way until all vertexes have one unit.
The final template tree is shown in the right part of
Figure 3, newly created virtual vertexes are shown
in dashed circle.
root
p .word+p .word+p .word-1 01
p .word+p pos0 0. c .word+c pos0 0.
root
p .word0
p .pos0 p .word-1
p .word1
c .word0
c .pos0
Figure 3: Templates that are partially overlapped:
Tred ? Tblue =p0.word, virtual vertexes shown in
dashed circle are created to extract the common
unit
root
p .word0
p .pos0
parse tag
VV NN... ... ... ...
.........
Level 0
Level 1
Level 2 VV ...
Figure 4: 2D Trie for single template, alphabets at
level 1 and level 2 are the word set, POS tag set
respectively
3 2D Trie
3.1 Single template case
Trie stores strings over a fixed alphabet, in our
case, feature strings are stored over several alpha-
bets, such as word list, POS tag list, etc. which are
extracted from training corpus.
To illustrate 2D Trie clearly, we first consider a
simple case, where only one template used. The
template tree degenerates to a sequence, we could
use a Trie like structure for feature indexing, the
only difference from traditional Trie is that nodes
at different levels could have different alphabets.
One example is shown in Figure 4. There are 3
feature strings from template ?p0.word + p0.pos?:
{parse/VV, tag/VV, tag/VV}. Alphabets at level
1 and level 2 are the word set, POS tag set re-
spectively, which are determined by correspond-
ing template vertexes.
As mentioned before, each vertex in template
tree has exactly one template unit, therefore, at
each level, we look up an index of a word or POS
906
HehadbeenasalesandmarketingexecutivewithChryslerfor20years
PRPVBDVBNDTNNSCCNNNNINNNPINCDNNS
2648273111210411506406313374192360231560220300566778
21272804130112120613060214
Figure 5: Look up indexes of words and POS tags
beforehand.
tag in sentence, not their combinations. Hence the
number of alphabets is limited, and all the indexes
could be searched beforehand for reuse, as shown
in Figure 5, the token table is converted to a in-
dex table. For example, when generating features
at position i of a sentence, template ?r0.word +
r1.word? requires index of i+1th word in the sen-
tence, which could be reused for generation at po-
sition i+ 1.
3.2 General case
Generally, for vertex in template tree with K chil-
dren, children of corresponding Trie node are ar-
ranged in a matrix of K rows and L columns, L
is the size of corresponding alphabet. If the vertex
is not virtual, i.e., it generates features, one more
row is added at the bottom to store feature indexes.
Figure 6 shows the 2D Trie for a general template
tree.
3.3 Feature extraction
When extracting features for a pair of nodes in a
sentence, template tree and 2D Trie are visited in
breath first traversal order. Each time, an alpha-
bet and a token index j from index table are se-
lected according to current vertex. For example,
POS tag set and the index of the POS tag of par-
ent node are selected as alphabet and token index
respectively for vertex ?p0.pos?. Then children in
the jth column of the Trie node are visited, valid
children and corresponding template vertexes are
saved for further retrieval or generate feature in-
dexes if the child is at the bottom and current Trie
node is not virtual. Two queues are maintained to
been......
... ......
...
VBNp .word+p .pos?been/VBN0 0...
... ......
p .word?been0... ...
root
root
p .word0p .pos0 c .word0
had ......
...
p .word?had0 ...
VBDp .word+p .pos?had/VBD0 0...
... ......
Hep .word+w .wordhad/He0 0?...
...
nmod vmodobj sub
Featureindex array-1 -13327 2510
nmod vmodobj sub-1 7821-1 -1............ ......
...... beenp .word+w .word?had/been0 0 ...
...
invalid
Figure 6: 2D trie for a general template tree.
Dashed boxes are keys of columns, which are not
stored in the structure
save the valid children and Trie nodes. Details of
feature extraction algorithm are described in Al-
gorithm 1.
3.4 Implementation
When feature set is very large, space complexity
of 2D Trie is expensive. Therefore, we use Double
Array Trie structure (Aoe, 1989) for implementa-
tion. Since children of 2D Trie node are arranged
in a matrix, not an array, so each element of the
base array has a list of bases, not one base in stan-
dard structure. For children that store features,
corresponding bases are feature indexes. One ex-
ample is shown in Figure 7. The root node has
3 bases that point to three rows of the child ma-
trix of vertex ?p0.word? respectively. Number of
bases in each element need not to be stored, since
it can be obtained from template vertex in extrac-
tion procedure.
Building algorithm is similarly to Double Array
Trie, when inserting a Trie node, each row of the
child matrix is independently insert into base and
check arrays using brute force strategy. The inser-
907
been
... been
...
...
...
had
...
had had...
... ...
...
...
been ... had had... ... ... ...... ... been hadroot base1 base2
base3
root base 2base
1
base3
...
VBD... ...
VBN...
...
...VBD
VBN
...
base1
base1 base1
base1
-1 -1... ... 3327 2510 ... ......... ......... -1 7821-1 -1... ...
-1 -1... ... 3327 2510 ... ......... ......... -1 7821-1 -1... ...
Basearray
Feature index array
Feature index array
Figure 7: Build base array for 2D Trie in Figure 6. String in the box represents the key of the child.
Blank boxes are the invalid children. The root node has 3 bases that point to three rows of the child
matrix of vertex ?p0.word? respectively
Algorithm 1 Feature extraction using 2D Trie
Input: 2D Trie that stores features, template
tree, template graph, a table storing token in-
dexes, parent and child positions
Output: Feature index set S of dependency
from parent to child.
Create template vertex queue Q1 and Trie
node queue Q2. Push roots of template tree
and Trie into Q1, Q2 respectively. S = ?
while Q1 is not empty, do
Pop a template vertex T from Q1 and a Trie
node N from Q2. Get token index j from
index table according to T .
for i = 1 to child number of T
if child of N at row i column j is valid,
push it into Q2 and push the ith child
of T into Q1.
else
remove decedents of ith child of T
from template tree
end if
end for
if T is not virtual and the last child of N in
column j is valid
Enumerate dependency types, add
valid feature indexes to S
end if
end while
Return S.
tion repeats recursively until all features stored.
4 Complexity analysis
Let
? |T | = number of templates
? |t| = number of template units
? |V | = number of vertexes in template tree,
i.e, |t|+ number of virtual vertexes
? |F | = number of features
? l = length of sentence
? |f | = average length of feature strings
The procedure of 2D Trie for feature extraction
consists of 2 steps: tokens in string table are
mapped to their indexes, then Algorithm 1 is car-
ried out for all node pairs of sentence. In the first
step, we use double array Trie for efficient map-
ping. In fact, time spent is trivial compared with
step 2 even by binary search. The main time spent
of Algorithm 1 is the traversal of the whole tem-
plate tree, in the worst case, no vertexes removed,
so the time complexity of a sentence is l2|V |,
which is proportional to |V |. In other words, mini-
mizing the number of virtual vertexes is important
for efficiency.
For other indexing structures, feature genera-
tion is a primary step of retrieval. For each node
908
Structure Generation Retrieval
2D Trie l2|V |
Hash / Trie l2|t| l2|f ||T |
Binary Search l2|t| l2|T | log |F |
Table 3: Time complexity of different indexing
structures.
pair of sentence, |t| template units are processed,
including concatenations of tokens and split sym-
bols (split tokens in feature strings), boundary
check ( e.g, p?1.word is out of boundary for be-
ginning node of sentence). Thus the generation
requires l2|t| processes. Notice that, time spent of
each process varies on the length of tokens.
For feature string s with length |s|, if perfect
hashing technique is adopted for index retrieval, it
takes |s| calculations to get hash value and a string
comparison to check the string at the calculated
position. So the time complexity is proportional to
|s|, which is the same as Trie. Hence the total time
for a sentence is l2|f ||T |. If binary search is used
instead, log |F | string comparisons are required,
complexity for a sentence is l2|T | log |F |.
Time complexity of these structures is summa-
rized in Table 3.
5 Experiments
5.1 Experimental settings
We use Chinese Tree Bank 6.0 corpus for evalua-
tion. The constituency structures are converted to
dependency trees by Penn2Malt 2 toolkit and the
standard training/development/test split is used.
257 sentences that failed in the conversion were
removed, yielding 23316 sentences for training,
2060 sentences for development and 2660 sen-
tences for testing respectively.
Since all the dependency trees are projective,
a first order projective MST parser is naturally
adopted. Online Passive Aggressive algorithm
(Crammer et al, 2006) is used for fast training, 2
parameters, i.e, iteration number and C, are tuned
on development data. The quality of the parser is
measured by the labeled attachment score (LAS),
i.e., the percentage of tokens with correct head and
dependency type.
2http://w3.msi.vxu.se/ nivre/research/Penn2Malt.html
Group IDs #Temp. #Vert. #Feat. LAS
1 1-2 72 91 3.23M 79.55%
2 1-3 128 155 10.4M 81.38%
3 1-4 240 275 25.0M 81.97%
4 1-5 332 367 34.8M 82.44%
Table 5: Parsing accuracy and number of tem-
plates, vertexes in template tree, features in decod-
ing stage (zero weighted features are excluded) of
each group.
We compare the proposed structure with Trie
and binary search. We do not compare with per-
fect hashing, because it has the same complex-
ity as Trie, and is often used for large data base
retrieval, since it requires only one IO opera-
tion. For easy comparison, all feature indexing
structures and the parser are implemented with
C++. All experiments are carried out on a 64bit
linux platform (CPU: Intel(R) Xeon(R) E5405,
2.00GHz, Memory: 16G Bytes). For each tem-
plate set, we run the parser five times on test data
and the averaged parsing time is reported.
5.2 Parsing speed comparison
To investigate the scalability of our method, rich
templates are designed to generate large feature
sets, as shown in Table 4. All templates are orga-
nized into 4 groups. Each row of Table 5 shows
the details of a group, including parsing accu-
racy and number of templates, vertexes in tem-
plate tree, and features in decoding stage (zero
weighted features are excluded).
There is a rough trend that parsing accuracy
increases as more templates used. Though such
trend is not completely correct, the clear conclu-
sion is that, abundant templates are necessary for
accurate parsing.
Though algorithm described in section 2.3 for
minimizing the number of virtual vertexes is
heuristic, empirical results are satisfactory, num-
ber of newly created vertexes is only 10% as orig-
inal templates. The reason is that complex tem-
plates are often extended from simple ones, their
differences are often one or two template units.
Results of parsing time comparison are shown
in Table 6. We can see that though time com-
plexity of dynamic programming is cubic, pars-
ing time of all systems is consistently dominated
909
ID Templates
1 pi.word pi.pos pi.word+pi.pos
ci.word ci.pos ci.word+ci.pos (|i| ? 2)
pi.length pi.length+pi.pos
ci.length ci.length+ci.pos (|i| ? 1)
p0.length+c0.length|ld p0.length+c0.length+c0.pos|ld p0.length+p0.pos+c0.length|ld
p0.length+p0.pos+c0.pos|ld p0.pos+c0.length+c0.pos|ld p0.length+p0.pos+c0.length+c0.pos|ld
pi.length+pj .length+ck.length+cm.length|ld (|i|+ |j|+ |k|+ |m| ? 2)r0.word r?1.word+r0.word r0.word+r1.word
r0.pos r?1.pos+r0.pos r0.pos+r1.pos
2 pi.pos+cj .pos|d pi.word+cj .word|d pi.pos+cj .word+cj .pos|d
pi.word+pi.pos+cj .pos|d pi.word+pi.pos+cj .word|d pi.word+cj .word+cj .pos|d
pi.word+pi.pos+cj .word+cj .pos|d (|i|+ |j| = 0)
Conjoin templates in the row above with |l
3 Similar with 2 |i|+ |j| = 1
4 Similar with 2 |i|+ |j| = 2
5 pi.word + pj .word + ck.word|d pi.word + cj .word + ck.word|d
pi.pos + pj .pos + ck.pos|d pi.pos + cj .pos + ck.pos|d (|i|+ |j|+ |k| ? 2)
Conjoin templates in the row above with |l
pi.word + pj .word + pk.word + cm.word|d pi.word + pj .word + ck.word + cm.word|d
pi.word + cj .word + ck.word + cm.word|d
pi.pos + pj .pos + pk.pos + cm.pos|d pi.pos + pj .pos + ck.pos + cm.pos|d
pi.pos + cj .pos + ck.pos + cm.pos|d (|i|+ |j|+ |k|+ |m| ? 2)
Conjoin templates in the row above with |l
Table 4: Templates used in Chinese dependency parsing.
by feature extraction. When efficient indexing
structure adopted, i.e, Trie or Hash, time index re-
trieval is greatly reduced, about 4-5 times faster
than binary search. However, general structures
search features independently, their results could
not guide feature generation. Hence, feature gen-
eration is still time consuming. The reason is that
processing each template unit includes a series of
steps, much slower than one integer comparison
in Trie search.
On the other hand, 2D Trie greatly reduces the
number of feature generations by pruning the tem-
plate graph. In fact, no string concatenation oc-
curs when using 2D Trie, since all tokens are con-
verted to indexes beforehand. The improvement
is significant, 2D Trie is about 5 times faster than
Trie on the largest feature set, yielding 13.4 sen-
tences per second parsing speed, about 4.3 times
faster.
Space requirement of 2D Trie is about 2.1 times
as binary search, and 1.7 times as Trie. One possi-
ble reason is that column number of 2D Trie (e.g.
size of words) is much larger than standard double
array Trie, which has only 256 children, i.e, range
of a byte. Therefore, inserting a 2D Trie node is
more strict, yielding sparser double arrays.
5.3 Comparison against state-of-the-art
Recent works on dependency parsing speedup
mainly focus on inference, such as expected
linear time non-projective dependency parsing
(Nivre, 2009), integer linear programming (ILP)
for higher order non-projective parsing (Martins
et al, 2009). They achieve 0.632 seconds per sen-
tence over several languages. On the other hand,
Goldberg and Elhadad proposed splitSVM (Gold-
berg and Elhadad, 2008) for fast low-degree poly-
nomial kernel classifiers, and applied it to transi-
tion based parsing (Nivre, 2003). They achieve
53 sentences per second parsing speed on En-
glish corpus, which is faster than our results, since
transition based parsing is linear time, while for
graph based method, complexity of feature ex-
traction is quadratic. Xavier Llu??s et al (Llu??s
et al, 2009) achieve 8.07 seconds per sentence
speed on CoNLL09 (Hajic? et al, 2009) Chinese
Tree Bank test data with a second order graphic
model. Bernd Bohnet (Bohnet, 2009) also uses
second order model, and achieves 610 minutes on
CoNLL09 English data (2399 sentences, 15.3 sec-
ond per sentence). Although direct comparison
of parsing time is difficult due to the differences
in data, models, hardware and implementations,
910
Group Structure Total Generation Retrieval Other Memory sent/sec
Trie 87.39 63.67 10.33 13.39 402M 30.44
1 Binary Search 127.84 62.68 51.52 13.64 340M 20.81
2D Trie 39.74 26.29 13.45 700M 66.94
Trie 264.21 205.19 39.74 19.28 1.3G 10.07
2 Binary Search 430.23 212.50 198.72 19.01 1.2G 6.18
2D Trie 72.81 53.95 18.86 2.5G 36.53
Trie 620.29 486.40 105.96 27.93 3.2G 4.29
3 Binary Search 982.41 484.62 469.44 28.35 2.9G 2.71
2D Trie 146.83 119.56 27.27 5.9G 18.12
Trie 854.04 677.32 139.70 37.02 4.9G 3.11
4 Binary Search 1328.49 680.36 609.70 38.43 4.1G 2.00
2D Trie 198.31 160.38 37.93 8.6G 13.41
Table 6: Parsing time of 2660 sentences (seconds) on a 64bit linux platform (CPU: Intel(R) Xeon(R)
E5405, 2.00GHz, Memory: 16G Bytes). Title ?Generation? and ?Retrieval? are short for feature gen-
eration and feature index retrieval steps respectively.
System sec/sent
(Martins et al, 2009) 0.63
(Goldberg and Elhadad, 2008) 0.019
(Llu??s et al, 2009) 8.07
(Bohnet, 2009) 15.3
(Galley and Manning, 2009) 15.6
ours group1 0.015
ours group2 0.027
ours group3 0.055
ours group4 0.075
Table 7: Comparison against state of the art, di-
rect comparison of parsing time is difficult due to
the differences in data, models, hardware and im-
plementations.
these results demonstrate that our structure can
actually result in a very fast implementation of a
parser. Moreover, our work is orthogonal to oth-
ers, and could be used for other learning tasks.
6 Conclusion
We proposed 2D Trie, a novel feature indexing
structure for fast template based feature extrac-
tion. The key insight is that feature strings gener-
ated by a template are prefixes of the features from
its extended templates, hence indexes of searched
features can be reused for further extraction. We
applied 2D Trie to dependency parsing task, ex-
perimental results on CTB corpus demonstrate the
advantages of our technique, about 5 times faster
than traditional Trie structure, yielding parsing
speed 4.3 times faster, while using only 1.7 times
as much memory.
7 Acknowledgments
The author wishes to thank the anonymous
reviewers for their helpful comments. This
work was partially funded by 973 Program
(2010CB327906), The National High Technol-
ogy Research and Development Program of China
(2009AA01A346), Shanghai Leading Academic
Discipline Project (B114), Doctoral Fund of Min-
istry of Education of China (200802460066), and
Shanghai Science and Technology Development
Funds (08511500302).
References
Aoe, Jun?ichi. 1989. An efficient digital
search algorithm by using a double-array struc-
ture. IEEE Transactions on software andengineer-
ing, 15(9):1066?1077.
Bohnet, Bernd. 2009. Efficient parsing of syntactic
and semantic dependency structures. In Proceed-
ings of the Thirteenth Conference on Computational
Natural Language Learning (CoNLL 2009): Shared
Task, pages 67?72, Boulder, Colorado, June. Asso-
ciation for Computational Linguistics.
Crammer, Koby, Joseph Keshet, Shai Shalev-Shwartz,
and Yoram Singer. 2006. Online passive-aggressive
algorithms. In JMLR 2006.
911
Galley, Michel and Christopher D. Manning. 2009.
Quadratic-time dependency parsing for machine
translation. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 773?781,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Goldberg, Yoav and Michael Elhadad. 2008. splitsvm:
Fast, space-efficient, non-heuristic, polynomial ker-
nel computation for nlp applications. In Proceed-
ings of ACL-08: HLT, Short Papers, pages 237?240,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Hajic?, Jan, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Kazama, Jun?ichi and Kentaro Torisawa. A new per-
ceptron algorithm for sequence labeling with non-
local features. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 315?324.
Llu??s, Xavier, Stefan Bott, and Llu??s Ma`rquez. 2009.
A second-order joint eisner model for syntactic and
semantic dependency parsing. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 79?84, Boulder, Colorado, June. Association
for Computational Linguistics.
Martins, Andre, Noah Smith, and Eric Xing. 2009.
Concise integer linear programming formulations
for dependency parsing. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP, pages 342?
350, Suntec, Singapore, August. Association for
Computational Linguistics.
McDonald, Ryan, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics, pages 91?97. Association for Computa-
tional Linguistics.
Nivre, Joakim. 2003. An efficient algorithm for
projective dependency parsing. In Proceedings of
the 11th International Conference on Parsing Tech-
niques, pages 149?160.
Nivre, Joakim. 2009. Non-projective dependency
parsing in expected linear time. In Proceedings of
the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP,
pages 351?359, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
Palmer, Martha and Nianwen Xue. 2009. Adding se-
mantic roles to the Chinese Treebank. Natural Lan-
guage Engineering, 15(1):143?172.
Sha, Fei and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 134?141,
May.
912
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 203?212, Dublin, Ireland, August 23-29 2014.
Time-aware Personalized Hashtag Recommendation on Social Media
Qi Zhang, Yeyun Gong, Xuyang Sun, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, P.R.China
{qz, 12110240006, 13210240106, xjhuang}@fudan.edu.cn
Abstract
The task of recommending hashtags for microblogs has been received considerable attention in
recent years, and many applications can reap enormous benefits from it. Various approaches have
been proposed to study the problem from different aspects. However, the impacts of temporal and
personal factors have rarely been considered in the existing methods. In this paper, we propose a
novel method that extends the translation based model and incorporates the temporal and personal
factors. To overcome the limitation of only being able to recommend hashtags that exist in the
training data of the existing methods, the proposed method also incorporates extraction strategies
into it. The results of experiments on the data collected from real world microblogging services
by crawling demonstrate that the proposed method outperforms state-of-the-art methods that do
not consider these aspects. The relative improvement of the proposed method over the method
without considering these aspects is around 47.8% in F1-score.
1 Introduction
Over the past few years, social media services have become one of the most important communication
channels for people. According to the statistic reported by the Pew Research Center?s Internet &
American Life Project in Aug 5, 2013, about 72% of adult internet users are also members of at least
one social networking site. Hence, microblogs have also been widely used as data sources for public
opinion analyses (Bermingham and Smeaton, 2010; Jiang et al., 2011), prediction (Asur and Huberman,
2010; Bollen et al., 2011), reputation management (Pang and Lee, 2008; Otsuka et al., 2012), and many
other applications (Sakaki et al., 2010; Becker et al., 2010; Guy et al., 2010; Guy et al., 2013). In
addition to the limited number of characters in the content, microblogs also contain a form of metadata
tag (hashtag), which is a string of characters preceded by the symbol (#). Hashtags are used to mark the
keywords or topics of a microblog. They can occur anywhere in a microblog, at the beginning, middle, or
end. Hashtags have been proven to be useful for many applications, including microblog retrieval (Efron,
2010), query expansion (A.Bandyopadhyay et al., 2011), sentiment analysis (Davidov et al., 2010; Wang
et al., 2011). However, only a few microblogs contain hashtags provided by their authors. Hence, the
task of recommending hashtags for microblogs has become an important research topic and has received
considerable attention in recent years.
Existing works have studied discriminative models (Ohkura et al., 2006; Heymann et al., 2008) and
generative models (Krestel et al., 2009; Blei and Jordan, 2003; Ding et al., 2013) based on textual
information from a single microblog. However, from a dataset containing 282.2 million microblogs
crawled from Sina Weibo
1
, we observe that different users may have different perspectives when picking
hashtags, and the perspectives of users are impacted by their own interests or the global topic trend.
Meanwhile,the global topic distribution is likely to change over time. To better understand how the
topics vary over time, we aggregate the microblog posts published in a month as a document. Then, we
use a Latent Dirichlet Allocation (LDA) to estimate their topics. Figure 1 illustrates an example, where
ten active topics are selected. We can observe that the topics distribution varies greatly over time.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
http://www.weibo.com. It is one of the most popular microblog services in China.
203
2012-04 2012-06 2012-08 2012-10 2012-12 2013-02 2013-04
0
200
400
600
800
1000
1200
pay 
official 
staff 
support 
ministry 
statistics 
tomorrow
reproduce 
research
financial 
network 
Japanese 
culture 
together 
yourself life
incentive street
Pisces AriesLeo Horoscope
Pluto
iphoneAppledesign
food 
water
Taurus
Venus
charges
 life
universities 
education
husband
  own
  women
  home
  
successson like
saint 
sunday
employees
film 
loyalty husbandchildren
parent
happyness
like
egg  pumpkin
Figure 1: An example of the topics of retweets in each month. Each colored stripe represents a topic,
whose height is the number of words assigned to the topic. For each topic, the top words of this topic in
each month are placed on the stripe.
Motivated by the methods proposed to handle the vocabulary gap problem for keyphrase extrac-
tion (Liu et al., 2012) and hashtag suggestion (Ding et al., 2013), in this work, we also assume that
the hashtags and textual content in a microblog are parallel descriptions of the same thing in different
languages. To model the document themes, in this paper, we adopt the topical translation model to
facilitate the translation process. Topic-specific word triggers are used to bridge the gap between the
words and hashtags. Since existing topical translation models can only recommend hashtags learned
from the training data, we also incorporate an extraction process into the model.
This work makes three main contributions. First, we incorporate temporal and personal factors into
considerations. Most of the existing works on hashtag recommendation tasks have focused on textual
information. Second, we adopt a topical translation model to combine extraction and translation methods.
This makes it possible to suggest hashtags that are not included in the training data. Third, to evaluate
the task, we construct a large collection of microblogs from a real microblogging service. All of the
microblogs in the collection contain textual content and hashtags labeled by their authors. This can
benefit other researchers investigating the same task or other topics using author-centered data.
The remaining part of this paper is structured as follows: We briefly review existing methods in
related domains in Section 2. Section 3 gives an overview of the proposed generation model. Section
4 introduces the dataset construction, experimental results and analyses. In Section 5, we will conclude
the paper.
2 Related Works
Due to the usefulness of tag recommendation, many methods have been proposed from different
perspectives (Heymann et al., 2008; Krestel et al., 2009; Rendle et al., 2009; Liu et al., 2012; Ding et al.,
2013). Heymann et al. (Heymann et al., 2008) investigated the tag recommendation problem using the
data collected from social bookmarking system. They introduced an entropy-based metric to capture the
generality of a particular tag. In (Song et al., 2008), a Poisson Mixture Model based method is introduced
to achieve the tag recommendation task. Krestel et al. (Krestel et al., 2009) introduced a Latent Dirichlet
Allocation to elicit a shared topical structure from the collaborative tagging effort of multiple users for
recommending tags. Based on the the observation that similar webpages tend to have the same tags, Lu et
al. proposed a method taking both tag information and page content into account to achieve the task (Lu
et al., 2009). Ding et al. proposed to use translation process to model this task (Ding et al., 2013). They
extended the translation based method and introduced a topic-specific translation model to process the
various meanings of words in different topics. In (Tariq et al., 2013), discriminative-term-weights were
used to establish topic-term relationships, of which users? perception were learned to suggest suitable
hashtags for users. To handle the vocabulary problem in keyphrase extraction task, Liu et al. proposed a
204
topical word trigger model, which treated the keyphrase extraction problem as a translation process with
latent topics (Liu et al., 2012).
Most of the works mentioned above are based on textual information. Besides these methods,
personalized methods for different recommendation tasks have also been paid lots of attentions (Liang
et al., 2007; Shepitsen et al., 2008; Garg and Weber, 2008; Li et al., 2010; Liang et al., 2010; Rendle and
Schmidt-Thieme, 2010). Shepitsen et al. (2008) proposed to use hierarchical agglomerative clustering
to take into account personalized navigation context in cluster selection. In (Garg and Weber, 2008),
the problem of personalized, interactive tag recommendation was also studied based on the statics of the
tags co-occurrence. Liang et al. (2010) proposed to the multiple relationships among users, items and
tags to find the semantic meaning of each tag for each user individually and used this information for
personalized item recommendation.
From the brief descriptions given above, we can observe that most of the previous works on hashtag
suggestion focused on textual information. In this work, we propose to incorporate temporal and personal
information into the generative methods. Further more, to over the limitation that translation based
method can only recommend hashtags learned from the training data, we also propose to incorporate an
extraction process into the model.
3 The Proposed Methods
In this section, we firstly introduce the notation and generation process of the proposed method. Then,
we describe the method used for learning parameters. Finally, we present the methods of how do we
apply the learned model to achieve the hashtag recommendation task.
3.1 The Generation Process
We use D to represent the number of microblogs in the given corpus, and the microblogs have been
divided into T epoches. Let t = 1, 2, ..., T be the index of an epoches, ?
t
is the topic distribution of the
epoch t. Each microblog is generated by a user u
i
, where u
i
is an index between 1 and U , and U is the
total number of users. A microblog is a sequence of N
d
words denoted by w
d
= {w
d1
, w
d2
, ..., w
dN
d
}.
Each microblog contains a set of hashtags denoted by h
d
= {h
d1
, h
d2
, ..., h
dM
d
}. A word is defined as
an item from a vocabulary with W distinct words indexed by w = {w
1
, w
2
, ..., w
W
}. Each hashtag is
from the vocabulary with V distinct hashtags indexed by h = {h
1
, h
2
, ..., h
V
}. The notations in this
paper are summarized in Table 1.
The original LDA assumes that a document is contains a mixture of topics, which is represented by a
topic distribution, and each word has a hidden topic label. Although, it is sensible for long document,
due to the limitations of the length of characters in a single microblog, it tends to be about a single topic.
Hence, we associate a single hidden variable with each microblog to indicate its topic. Similar idea of
assigning a single topic to a short sequence of words has also been used for modeling Twitters (Zhao et
al., 2011)
The hashtag recommendation task is to discover a list of hashtags for each unlabeled microblog, In
our method, we first learn a topical translation model, and then we estimate the latent variables for each
microblog, finaly recommending hashtags accord to the learned model.
Fig. 2 shows the graphical representation of the generation process. The generative story for each
microblog is as follows:
3.2 Learning
To learn the parameters of our model, we use collapsed Gibbs sampling (Griffiths and Steyvers, 2004) to
sample the topics assignment z, latent variables assignment x and y.
Given the current state of all but the variable x
d
and z
d
for the dth microblog, we can jointly sample
205
1. Draw pi ? Beta(?), ? ? Beta(?)
2. Draw background word distribution ?
B
? Dirichlet(?
w
)
3. Draw global trendy topic distribution ?
t
? Dirichlet(?) for each time epoch t = 1, 2, ..., T
4. Draw personal topic distribution ?
u
? Dirichlet(?) for each user u = 1, 2, ..., U
5. Draw word distribution ?
z
? Dirichlet(?
w
) for each topic z = 1, 2, ...,K
6. Draw hashtag distribution ?
z,w
? Dirichilet(?
h
) for each topic z = 1, 2, ...,K and each word
w = 1, 2, ...,W
7. For each microblog d = 1, 2, ..., D
a. Draw x
d
? Bernoulli(?)
b. If x
d
= 0 then
Draw a topic z
d
?Multinomial(?
u
)
End if
If x
d
= 1 then
Draw a topic z
d
?Multinomial(?
t
)
End if
c. For each word n = 1, ..., N
d
i. Draw y
dn
? Bernoulli(pi)
ii. If y
dn
= 0 then
Draw a word w
dn
?Multinomial(?
B
)
End if
If y
dn
= 1 then
Draw a word w
dn
?Multinomial(?
z
d
)
End if
d. For each hashtag m = 1, ...,M
d
i. Draw h
dm
? P (h
dm
|w
d
, z
d
, ?
z
d
,w
d
)
w
dn
z
d
?
t
?
u
t
d
u
d
x
d
?
?
? ?
h
dm
y
dn
pi
?
?
z
?
B
?
w
?
w
?
z,w
?
h
T
M
d
N
d
D
K
U
W
K
Figure 2: The graphical representation of the proposed model. Shaded circles are observations or
constants. Unshaded ones are hidden variables.
206
Table 1: The notations used in this work.
D The number of training data set
W The number of unique word in the corpus
V The number of unique hashtag in the corpus
K The number of topics
T The total number of time epoches
U The total number of users
N
d
The number of words in the dth microblog
M
d
The number of hashtags in the dth microblog
z
d
The topic of the dth microblog
x
d
The latent variable decided the distribution category of z
d
y
dn
The latent variable decided the distribution category of w
dn
pi The distribution of latent variable y
dn
? The distribution of latent variable x
d
?
z
The distribution of topic words
?
B
The distribution of background words
?
t
The distribution of topics for time epoch t
?
u
The distribution of topics for user u
t
d
The time epoch for microblog d
u
d
The user of the microblog d
? The topic-specific word alignment table between word and hashtag or itself
x
d
and z
d
, the conditional probability of x
d
= p,z
d
= k is calculated as follows:
Pr(x
d
= p, z
d
= k|z
?d
,x
?d
,y,w,h)
?
N
?
p
+ ?
N
?
(.)
+ 2?
?
N
l
k
+ ?
N
l
(.)
+K?
?
N
d
?
n=1
N
k
w
dn
+ ?
w
N
k
(.)
+W?
w
?
M
d
?
m=1
N
d
?
n=1
M
w
dn
,h
dm
?d,k
+ ?
h
M
w
dn
,(.)
?d,k
+ V ?
h
,
(1)
where l = u
d
when p = 0 and l = t
d
when p = 1. N
?
0
is the number of microblog generated by personal
interests, while N
?
1
is the number of microblog coming from global topical trends, N
?
(.)
= N
?
0
+ N
?
1
.
N
u
d
k
is the number of microblogs generated by user u
d
and under topic k. N
u
d
(.)
is the total number of
microblogs generated by user u
d
. N
t
d
k
=
?
t
d
t
?
=1
e
?t
?
?
N
?
t?t
?
k
,N
?
t?t
?
k
is the number of microblogs assigned
to topic k at time epoch t ? t
?
, e
?t
?
?
is decay factory, and N
t
d
(.)
=
?
K
k=1
N
t
d
k
. N
k
w
dn
is the times of word
w
dn
assigned to topic k, N
k
(.)
is the times of all the word assigned to topic k, M
w
dn
,h
dm
?d,k
is the number of
occurrences that word w
dn
is translated to hashtag h
dm
given topic k. All the counters mentioned above
are calculated with the dth microblog excluded.
We sample y
dn
for each word w
dn
in the dth microblog using the following equation:
Pr(y
dn
= q|z,x,y
?dn
,w,h) ?
N
pi
q
+ ?
N
pi
(.)
+ 2?
?
N
l
w
dn
+ ?
w
N
l
(.)
+W?
w
,
(2)
where l = B when q = 0 and l = z
d
when q = 1. N
pi
0
is the number of words assigned to background
words and N
pi
1
is the number of words under any topic respectively. N
pi
(.)
= N
pi
0
+N
pi
1
, N
B
w
dn
is a count
of word w
dn
occurs as a background word. N
z
d
w
dn
is the number of word w
dn
is assigned to topic z
d
, and
N
z
d
(.)
is the total number of words assigned to topic z
d
. All counters are calculated with taking no account
of the current word w
dn
.
In many cases, hashtag dose not appear in the training data, to solve this problem, we assume that each
word in the microblog can translate to a hashtag in the training data or itself. We assume that each word
207
have aligned ? (we set ? = 1 in this paper after trying some number) times with itself under the specific
topic. After all the hidden variables become stable, we can estimate the alignment probability as follows:
?
h,w,z
=
?
?
?
N
h
z,w
+?
h
N
(.)
z,w
+?+(V+1)?
h
if h is a hashtag in the training data
?+?
h
N
(.)
z,w
+?+(V+1)?
h
if h is the word itself
(3)
where N
h
z,w
is the number of the hashtag h co-occurs with the word w under topic z in the microblogs.
For the probability alignment ? between hashtag and word, the potential size is W ? V ? K. The
data sparsity poses a more serious problem in estimating ? than the topic-free word alignment case.
To remedy the problem, we use interpolation smoothing technique for ?. In this paper, we emplogy
smoothing as follows:
?
?
h,w,z
= ??
h,w,z
+ (1? ?)P (h|w),
(4)
where ?
?
h,w,z
is the smoothed topical alignment probabilities, ?
h,w,z
is the original topical alignment
probabilities. P (h|w) is topic-free word alignment probability. Here we obtain P (h|w) by exploring
IBM model-1 (Brown et al., 1993). ? is trade-off of two probabilities ranging from 0.0 to 1.0. When
? = 0.0, ?
?
h,w,z
will be reduce to topic-free word alignment probability; and when ? = 1.0, there will be
no smoothing in ?
?
h,w,z
. For the word itself there are no smoothing, because it is a pseudo-count.
3.3 Hashtag Extraction
We perform hashtag extraction as follows. Suppose given an unlabeled dataset, we perform Gibbs
Sampling to iteratively estimate the topic and determine topic/background words for each microblog.
The process is the same as described in Section 3.2. After the hidden variables of topic/background
words and the topic of each microblog become stable, we can estimate the distribution of topics for the
dth microblog in unlabeled data by:?
?
dk
=
p(k)p(w
d1
|k)...p(w
dN
d
|k)
Z
where p(w
dn
|k) =
N
pi
1
+?
N
pi
(.)
+2?
?
N
k
w
dn
+?
w
N
k
(.)
+W?
w
and N
k
w
dn
is the number of words w
dn
that are assigned to topic k in the corpus, and p(k) =
N
?
0
+?
N
?
(.)
+2?
?
N
u
k
+?
N
u
(.)
+K?
+
N
?
1
+?
N
?
(.)
+2?
?
N
t
k
+?
N
t
(.)
+K?
is regarded as a prior for topic distribution, Z is the normalized
factor. With topic distribution ?
?
and topical alignment table ?
?
, we can rank hashtags for the dth
microblog in unlabeled data by computing the scores:
P (h
dm
|w
d
, ?
?
d
, ?
?
) ?
K
?
z
d
=1
N
d
?
n=1
P (h
dm
|z
d
, w
dn
, ?
?
) ? P (z
d
|?
?
d
) ? P (w
dn
|w
d
),
(5)
where h
dm
can be a hashtag in the training data or a word in the dth microblog, p(w
dn
|w
d
) is the weight
of the word w
dn
in the microblog, which can be estimated by the IDF score of the word. According to
the ranking scores, we can suggest the top-ranked hashtags for each microblog to users.
4 Experiments
In this section, we introduce the experimental results and the data collection we constructed for training
and evaluation. Firstly, we describe how do we construct the collection and statics of it. Then we
introduce the experiment configurations and baseline methods. Finally, the evaluation results and
analysis are given.
4.1 Data Collection
We use a dataset collected from Sina Weibo to evaluate the proposed approach and alternative methods.
We random select 166,864 microblogs from Aug. 2012 to June 2013. The unique number of hashtags
in the corpus is 17,516. We use the microblogs posted from Aug. 2012 to May 2013 as the training
data. The other microblogs are used for evaluation. The hashtags marked in the original microblogs are
considered as the golden standards.
208
Figure 3: Precision-recall curves of different
methods on this task.
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Prec
ision
Recall
TWTMTTMT-TTMU-TTMTU-TTMK-TTMTUK-TTM
Table 2: Evaluation results of different methods
on the evaluation collection.
Methods Precision Recall F
1
TWTM 0.231 0.202 0.215
SVM 0.418 0.366 0.390
TTM 0.319 0.279 0.297
T-TTM 0.338 0.301 0.319
U-TTM 0.341 0.307 0.323
K-TTM 0.386 0.337 0.360
TU-TTM 0.355 0.310 0.331
TUK-TTM 0.452 0.415 0.433
4.2 Experiment Configurations
We use precision (P ), recall (R), and F1-score (F
1
) to evaluate the performance. Precision is calculated
based on the percentage of ?hashtags truly assigned? among ?hashtags assigned by system?. Recall
is calculated based on the ?hashtags truly assigned? among ?hashtags manually assigned?. F1-score
is the harmonic mean of precision and recall. We do 500 iterations of Gibbs sampling to train the
model. For optimize the hyperparmeters of the proposed method and alternative methods, we use 5-fold
cross-validation in the training data to do it. The number of topics is set to 70. The other settings of
hyperparameters are as follows: ? = 50/K, ?
w
= 0.1, ?
h
= 0.1, ? = 0.01, and ? = 0.01. The
smoothing factor ? in Eq.(3) is set to 0.6. For estimating the translation probability without topical
information, we use GIZA++ 1.07 to do it (Och and Ney, 2003).
For baselines, we compare the proposed model with the following alternative models.
? TWTM: Topical word trigger model (TWTM) was proposed by Liu et al. for keyphrase extraction
using only textual information (Liu et al., 2012). We implemented the model and used it to achieve
the task.
? TTM: Ding et al. (2013) proposed the topical translation model (TTM) for hash tag extraction. We
implemented and extended their method for evaluating it on the corpus constructed in this work.
4.3 Experimental Results
Table 2 shows the comparisons of the proposed method with the state-of-the-art methods on the
constructed evaluation dataset. ?TUK-TTM? denotes the method proposed in this paper. ?T-TTM?
and ?U-TTM? represent the methods incorporating temporal and personal information respectively. ?K-
TTM? represents the method incorporating the extraction factor. From the results, we can observe that
the proposed method is significantly better than other methods at 5% significance level (two-sided).
Comparing to results of the TTM, we can observe that the temporal information, personal information
and extraction strategy can all benefit the task. Among the three additional factors, the extraction strategy
achieves the best result. The limitation of only being able to recommend hashtags that exist in the training
data can be overcome in some degree by the proposed method. The relative improvement of proposed
TUK-TTM over TTM is around 47.8% in F1-score.
Table 3 shows the comparisons of the proposed method with the method ?K-TTM? in two corpus NE-
Corpus and E-Corpus. NE-Corpus include microblogs whose hashtags are not contained in the training
data. E-Corpus include the microblogs whose hashtags appear in the training data. We can observe that
the proposed method significantly better than ?K-TTM? in the E-Corpus. Another observation is that
the method incorporating the extraction factor achieves better performances on the NE-Corpus than E-
Corpus. We think that the reason is that the fewer times hashtag appear, the greater weight it has. Hence,
we can extract this kind of hashtags more easier.
Figure 3 shows the precision-recall curves of TWTW, TTM, T-TTM, U-TTM, TU-TTM, K-TTM,
and TUK-TTM on the evaluation dataset. Each point of a precision-recall curve represents extracting
209
Table 3: Evaluation results of two different corpus.
Corpus Methods P R F
NE-Corpus
K-TTM 0.631 0.553 0.589
TUK-TTM 0.641 0.561 0.598
E-Corpus
K-TTM 0.172 0.162 0.167
TUK-TTM 0.288 0.271 0.279
Table 4: The influence of the number of topics
K of TUK-TTM.
K Precision Recall F
1
10 0.410 0.382 0.396
30 0.435 0.380 0.406
50 0.448 0.413 0.430
70 0.452 0.415 0.433
100 0.439 0.404 0.421
Table 5: The influence of the smoothing
parameter ? of TUK-TTM.
? Precision Recall F
1
0.0 0.379 0.354 0.366
0.2 0.405 0.372 0.388
0.4 0.433 0.398 0.415
0.6 0.452 0.415 0.433
0.8 0.426 0.386 0.405
1.0 0.423 0.381 0.401
different number of hashtags ranging from 1 to 5 respectively. In the figure, curves which are close
to the upper right-hand corner of the graph indicate the better performance. From the results, we can
observe that the performance of TUK-TTM is in the upper right-hand corner. It also demonstrates that
the proposed method achieves better performances than other methods.
From the description of the proposed model, we can know that there are several hyperparameters in
the proposed TUK-TTM. To evaluate the impacts of them, we evaluate two crucial ones, the number of
topics K and the smoothing factor ?. Table 4 shows the influence of the number of topics. From the
table, we can observe that the proposed model obtains the best performance when K is set to 70. And
performance decreases with more number of topics. We think that data sparsity may be one of the main
reasons. With much more topic number, the data sparsity problem will be more serious when estimating
topic-specific translation probability. Table 5 shows the influence of the translation probability smoothing
parameter ?. When ? is set to 0.0, it means that the topical information is omitted. Comparing the results
of ? = 0.0 and other values, we can observe that the topical information can benefit this task. When ? is
set to 1.0, it represents the method without smoothing. The results indicate that it is necessary to address
the sparsity problem through smoothing.
5 Conclusions
In this paper, we propose a novel method which incorporates temporal and personal factors into the
topical translation model for hashtag recommendation task. Since existing translation model based
methods for this task can only recommend hashtags that exist in the training data of the topical translation
model, we also incorporate extraction strategies into the model. To evaluate the proposed method, we
also construct a dataset from real world microblogging services. The results of experiments on the dataset
demonstrate that the proposed method outperforms state-of-the-art methods that do not consider these
aspects.
6 Acknowledgement
The authors wish to thank the anonymous reviewers for their helpful comments. This work was
partially funded by 973 Program (2010CB327900), National Natural Science Foundation of China
(61003092,61073069), Shanghai Leading Academic Discipline Project (B114) and ?Chen Guang?
project supported by Shanghai Municipal Education Commission and Shanghai Education Development
Foundation(11CG05).
210
References
A.Bandyopadhyay, M. Mitra, and P. Majumder. 2011. Query expansion for microblog retrieval. In Proceedings
of The Twentieth Text REtrieval Conference, TREC 2011.
S. Asur and B.A. Huberman. 2010. Predicting the future with social media. In WI-IAT?10, volume 1, pages
492?499.
Hila Becker, Mor Naaman, and Luis Gravano. 2010. Learning similarity metrics for event identification in social
media. In Proceedings of WSDM ?10.
Adam Bermingham and Alan F. Smeaton. 2010. Classifying sentiment in microblogs: is brevity an advantage? In
Proceedings of CIKM?10.
D.M. Blei and M.I. Jordan. 2003. Modeling annotated data. In Proceedings of SIGIR, pages 127?134.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1 ? 8.
Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263?311.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Enhanced sentiment learning using twitter hashtags and
smileys. In Proceedings of COLING ?10.
Zhuoye Ding, Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013. Learning topical translation model for
microblog hashtag suggestion. In Proceedings of IJCAI 2013.
Miles Efron. 2010. Hashtag retrieval in a microblogging environment. In Proceedings of SIGIR ?10.
Nikhil Garg and Ingmar Weber. 2008. Personalized, interactive tag recommendation for flickr. In Proceedings of
RecSys ?08.
T. L. Griffiths and M. Steyvers. 2004. Finding scientific topics. Proceedings of the National Academy of Sciences.
Ido Guy, Naama Zwerdling, Inbal Ronen, David Carmel, and Erel Uziel. 2010. Social media recommendation
based on people and tags. In Proceedings of SIGIR ?10.
Ido Guy, Uri Avraham, David Carmel, Sigalit Ur, Michal Jacovi, and Inbal Ronen. 2013. Mining expertise and
interests from social media. In Proceedings of WWW ?13.
Paul Heymann, Daniel Ramage, and Hector Garcia-Molina. 2008. Social tag prediction. In Proceedings of SIGIR
?08.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment
classification. In Proceedings of ACL 2011, Portland, Oregon, USA.
Ralf Krestel, Peter Fankhauser, and Wolfgang Nejdl. 2009. Latent dirichlet allocation for tag recommendation. In
Proceedings of RecSys ?09.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. 2010. A contextual-bandit approach to personalized
news article recommendation. In Proceedings of the 19th international conference on World wide web, pages
661?670. ACM.
Ting-Peng Liang, Hung-Jen Lai, and Yi-Cheng Ku. 2007. Personalized content recommendation and user
satisfaction: Theoretical synthesis and empirical findings. Journal of Management Information Systems,
23(3):45?70.
Huizhi Liang, Yue Xu, Yuefeng Li, Richi Nayak, and Xiaohui Tao. 2010. Connecting users and items with
weighted tags for personalized item recommendations. In Proceedings of the 21st ACM conference on Hypertext
and hypermedia, pages 51?60. ACM.
Zhiyuan Liu, Chen Liang, and Maosong Sun. 2012. Topical word trigger model for keyphrase extraction. In
Proceedings of COLING.
Yu-Ta Lu, Shoou-I Yu, Tsung-Chieh Chang, and Jane Yung-jen Hsu. 2009. A content-based method to enhance
tag recommendation. In Proceedings of IJCAI?09.
211
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Tsutomu Ohkura, Yoji Kiyota, and Hiroshi Nakagawa. 2006. Browsing system for weblog articles based on
automated folksonomy. Workshop on the Weblogging Ecosystem Aggregation Analysis and Dynamics at WWW.
Takanobu Otsuka, Takuya Yoshimura, and Takayuki Ito. 2012. Evaluation of the reputation network using realistic
distance between facebook data. In Proceedings of WI-IAT ?12.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
Steffen Rendle and Lars Schmidt-Thieme. 2010. Pairwise interaction tensor factorization for personalized tag
recommendation. In Proceedings of the third ACM international conference on Web search and data mining,
pages 81?90. ACM.
Steffen Rendle, Leandro Balby Marinho, Alexandros Nanopoulos, and Lars Schmidt-Thieme. 2009. Learning
optimal ranking with tensor factorization for tag recommendation. In Proceedings of KDD ?09.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes twitter users: real-time event
detection by social sensors. In Proceedings of WWW ?10.
Andriy Shepitsen, Jonathan Gemmell, Bamshad Mobasher, and Robin Burke. 2008. Personalized
recommendation in social tagging systems using hierarchical clustering. In Proceedings of the 2008 ACM
Conference on Recommender Systems, RecSys ?08, pages 259?266, New York, NY, USA. ACM.
Yang Song, Ziming Zhuang, Huajing Li, Qiankun Zhao, Jia Li, Wang-Chien Lee, and C. Lee Giles. 2008. Real-
time automatic tag recommendation. In Proceedings of SIGIR ?08.
Amara Tariq, Asim Karim, Fernando Gomez, and Hassan Foroosh. 2013. Exploiting topical perceptions over
multi-lingual text for hashtag suggestion on twitter. In FLAIRS Conference.
Xiaolong Wang, Furu Wei, Xiaohua Liu, Ming Zhou, and Ming Zhang. 2011. Topic sentiment analysis in twitter:
a graph-based hashtag sentiment classification approach. In Proceedings of CIKM ?11.
Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song, Palakorn Achananuparp, Ee-Peng Lim, and Xiaoming Li.
2011. Topical keyphrase extraction from twitter. In Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Technologies-Volume 1, pages 379?388. Association for
Computational Linguistics.
212
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 688?697, Dublin, Ireland, August 23-29 2014.
A Generative Model for Identifying Target Companies of Microblogs
Yeyun Gong, Yaqian Zhou, Ya Guo, Qi Zhang, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, P.R.China
{12110240006, zhouyaqian, 13210240002, qz, xjhuang}@fudan.edu.cn
Abstract
Microblogging services have attracted hundreds of millions of users to publish their status, ideas
and thoughts, everyday. These microblog posts have also become one of the most attractive and
valuable resources for applications in different areas. The task of identifying the main targets of
microblogs is an important and essential step for these applications. In this paper, to achieve this
task, we propose a novel method which converts the target company identification problem to
the translation process from content to targets. We introduce a topic-specific generative method
to model the translation process. Topic specific trigger words are used to bridge the vocabulary
gap between the words in microblogs and targets. We examine the effectiveness of our approach
via datasets gathered from real world microblogs. Experimental results demonstrate a 20.2%
improvement in terms of F1-score over the state-of-the-art discriminative method.
1 Introduction
With the rapid growth of social media, about 72% of adult internet users are also members of
a social networking site
1
. Over the past few years, microblogging has become one of the most
popular services. Meanwhile, microblogs have also been widely used as sources for analyzing public
opinions (Bermingham and Smeaton, 2010; Jiang et al., 2011), prediction (Asur and Huberman, 2010;
Bollen et al., 2011), reputation management (Pang and Lee, 2008; Otsuka et al., 2012), and many other
applications (Bian et al., 2008; Sakaki et al., 2010; Becker et al., 2010; Guy et al., 2010; Lee and Croft,
2013; Guy et al., 2013). For most of these applications, identifying the microblogs that are relevant to
the targets of interest is one of the basic steps (Lin and He, 2009; Amig?o et al., 2010; Qiu et al., 2011;
Liu et al., 2013). Let us firstly consider the following example:
Example 1: 11? MacBook Air can run for up to five hours on a single charge.
?MacBook Air? can be considered to be the target being discussed on the microblog, and we can also
infer from the microblog that it is related to Apple Inc. The ability to discriminate which company is
being referred to in a microblog is required by many applications.
Previous studies on fine-grained sentiment analysis and aspect-based opinion mining proposed
supervised (Popescu and Etzioni, 2005; Liu et al., 2012a; Liu et al., 2013) and unsupervised methods (Hu
and Liu, 2004; Wu et al., 2009; Zhang et al., 2010) to extract targets of opinion expressions. Based on
the associations between opinion targets and opinion words, some methods were also introduced to
simultaneously solve the opinion expression and target extraction problems (Qiu et al., 2011; Liu et al.,
2012a). However, most of the existing methods in this area only focus on extracting items about which
opinions are expressed in a given domain. The implicated information of targets is rarely considered.
Moreover, domain adaptation is another big challenge for these fine-grained methods in processing
different domains.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
It is reported by the Pew Research Center?s Internet & American Life Project in Aug 5, 2013.
688
The WePS-3
2
(Amig?o et al., 2010) and RepLab 2013
3
(Amig?o et al., 2013) evaluation campaigns also
addressed the problem from the perspective of the disambiguation of company names in microblogs.
Microblogs that contain company names at a lexical level are classified based on whether it refers
to the company or not. Various approaches have been proposed to address the task with different
methods (Pedersen et al., 2006; Yerva et al., 2010; Zhang et al., 2012; Spina et al., 2012; Spina et
al., 2013). However, the microblogs that do not contain company names cannot be correctly processed
using these methods. From analyzing the data, we observe that a variety of microblog posts belong to
this type. They only contain products names, slang terms, and other related company content.
To achieve this task, in this paper, we propose the use of a translation based model to identify the targets
of microblogs. We assume that the microblog posts and targets describe the same topic using different
languages. Hence, the target identification problem can be regarded as a translation process from the
content of the microblogs to the targets. We integrate latent topical information into the translation
model to facilitate the translation process. Because product names, series, and other related information
are important indicators for this task, we also incorporate this background knowledge into the model. To
evaluate the proposed method, we collect a large number of microblogs and manually annotate a subset
of these as golden standards. We compare the proposed method with state-of-the-art methods using the
constructed dataset. Experimental results demonstrate that the proposed approach can achieve better
performance than the other approaches.
2 The Proposed Method
2.1 The Generation Process
Given a corpus D = {d
i
, 1 ? i ? |D|}, which contains a list of microblogs {d
i
}. A microblog is a
sequence of N
d
words denoted by w
d
= {w
d1
, w
d2
, ..., w
dN
d
}. Each microblog contains a set of targets
denoted by c
d
= {c
d1
, c
d2
, ..., c
dM
d
}. A word is defined as an item from a vocabulary with V distinct
words indexed by w = {w
1
, w
2
, ..., w
V
}. The nth word in the dth microblog is associated with not only
one topic z
dn
, but also an indicator variable l
dn
which indicates whether w
dn
belongs to the ontology
(l
dn
= 1), which contains company names, product names, series, and other related information, or is a
common word (l
dn
= 0). Each target is from the vocabulary with C distinct company names indexed by
c = {c
1
, c
2
, ..., c
C
}. The mth target in the dth microblog is associated with a topic z
dm
. The notations
used in this paper are summarized in Table 1. Fig. 1 shows the graphical representation of the generation
process. The generative story for each microblog is as follows:
1. Sample word distribution ?
t,l
fromDir(?
l
) for each topic t = 1, 2, ..., T and each label l = 1, ..., L.
2. For each microblog d=1,2,...,|D|
a. Sample topic distribution ?
d
from Dir(?)
b. For each word n = 1, 2, ..., N
d
i. Sample a topic z
dn
= t from Multinomial(?
d
)
ii. Sample a label l
dn
= l from the distribution over labels, v
d,n
iii. Sample a wordw according to multinomial distribution P (w
dn
= w|z
dn
= t, l
dn
= l, ?
t,l
)
c. For each target m = 1, 2, ...,M
d
i. Sample a topic z
dm
= t from Multinomial(?
d
)
ii. Sample a target c
dm
= c according to probability P (c
dm
= c|w
d
, l
d
, z
dm
= t, B)
As described above, we use l
dn
to incorporate the ontology information into the model. In this work,
we construct an ontology which contains 4,926 company names, 7,632 abbreviations, and 26,732 product
names. These companies names are collected based on the top search queries in different categories
4
.
We propose to use the distribution v
d,n
to indicate the probability of variable l
dn
. We set v
d,n
by applying
2
http://nlp.uned.es/weps/weps-3
3
http://www.limosine-project.eu/events/replab2013
4
http://top.baidu.com/boards
689
wdn
z
dn
?
d
?
c
dm
z
dm
B
?
t,l
?
l
l
dn
v
d,n
f
dn
?
M
d
N
d
|D|
V
T
L
Figure 1: The graphical representation of the proposed model. Shaded circles are observations or
constants. Unshaded ones are hidden variables.
various sources of ontology (presented by ?) and the context features of the word w
dn
(presented by f
dn
).
In this work, we only consider the word itself as its context feature. This information is encoded into
the hyperparameters {?
w
|w ? {w
1
, w
2
, ..., w
V
}}, where ?
w
is hyperparameter for the word w, and
?
w
0
+ ?
w
1
= 1. For each word w in the ontology, we set ?
w
1
to a value 0.9, ?
w
0
to a value 0.1. For each
word w not contained by ontology, we set ?
w
1
to a value 0 and ?
w
0
to a value 1. Based on the ontology,
v
d,n
could be set as follows:
P (l
dn
= l|w
dn
= w) = v
d,n
l
=
?
w
l
?
w
1
+ ?
w
0
, l ? {0, 1}
(1)
2.2 Model Inference
We use collapsed Gibbs sampling (Griffiths and Steyvers, 2004) to obtain samples of hidden variable
assignment and to estimate the model parameters from these samples.
On the microblog content side, the conditional probability of a latent topic and label for the nth word
in the dth microblog is:
Pr(z
dn
= t, l
dn
= l|w
dn
= w,w
?n
, z
?n
, l
?n
) ?
?
w
l
?
w
1
+ ?
w
0
?
N
w,?n
t,l
+ ?
l
N
?n
t,l
+ V ?
l
?
N
t,?n
d
+ ?
N
?n
d
+ T?
,
(2)
where N
w,?n
t,l
is the number of the word w that are assigned to topic t under the label l; N
?n
t,l
is the
number of all the words that are assigned to topic t under the label l; N
t,?n
d
is the number of topic t in
the microblog d; N
?n
d
is the number of all the topics in the document d; ?n indicates taking no account
of the current position n.
Given the conditional probability of z
dn
= t, l
dn
= l, we formalize the marginal probability of z
dn
= t
as follows:
Pr(z
dn
= t|w
dn
= w,w
?n
, z
?n
, l
?n
) ?
L?1
?
l=0
?
w
l
?
w
1
+ ?
w
0
?
N
w,?n
t,l
+ ?
l
N
?n
t,l
+ V ?
l
?
N
t,?n
d
+ ?
N
?n
d
+ T?
(3)
690
Table 1: The notation used in the proposed model.
|D| The number of microblogs in the data set
V The number of unique words in the vocabulary
C The number of companies
T The number of topics
L The number of labels
N
d
The number of words in the dth microblog
M
d
The number of companies in the dth microblog
w
d
All the words in the dth microblog
c
d
All the targets in the dth microblog
z
d
The topic of the words in the dth microblog
l
d
The label of the words in the dth microblog
B The topic-specific word alignment table between a word and a target
?
t,l
Distribution of words for each topic t and each label l
?
d
Distribution of topics in microblog d
v
d,n
Distribution of labels for word w
dn
N
w,?n
t,l
The number of the word w that is assigned to topic t under the label l except the position n
N
?n
t,l
The number of all the words that are assigned to topic t under the label l. except the position n
N
t,?n
d
The number of topic t in the microblog d except the position n
N
?n
d
The number of all the topics in the microblog d except the position n
N
c,w
t,l
The number of the target c that co-occurs with the word w labeled as l under topic t
After re-assigning the topic z
dn
= t for the current word, the conditional probability of ontology label
for the nth word in the dth microblog is:
Pr(l
dn
= l|w
dn
= w, z
dn
= t,w
?n
, z
?n
, l
?n
) ?
?
w
l
?
w
1
+ ?
w
0
?
N
w,?n
t,l
+ ?
l
N
?n
t,l
+ V ?
l
(4)
On the target side, we perform topic assignments for each target as follows:
Pr(z
dm
= t|c
dm
= c, c
?m
,w, l, z
?m
) ?
N
d
?
n=1
?
l
dn
N
c,w
dn
,?m
t,l
dn
N
w
dn
t,l
dn
+ ?C
?
N
t,?m
d
+ ?
N
?m
d
+ T?
,
(5)
where ?
l
dn
is the weight for the label (?
1
> 1, ?
0
= 1); N
c,w
dn
,?m
t,l
dn
is the number of the company c that
co-occurs with the word w
dn
labeled as l
dn
under topic t; ?C is a smoothing part; N
w
dn
t,l
dn
is the number of
the word w
dn
labeled as l
dn
under topic t; N
t,?m
d
is the number of occurrences of topic t in the document
d; N
?m
d
is the number of occurrences of all the topics in the document d; ?m indicates taking no account
of the current position m.
Based on the above equations, after enough sampling iterations, we can estimate word alignment table
B, B
c,w,t,l
= ?
l
N
c,w
t,l
N
w
t,l
+?C
. Some companies just occur few times, and most of the words co-occur with
them also alignment with other companies, for this case, we use ?C to smooth, where C represent the
number of company c. And also we can estimate topic distribution ? for each document, and word
distribution ? for each topic and each label, as follows:
?
t
d
=
N
t
d
+ ?
N
d
+ T?
, ?
t,l
w
=
N
w
t,l
+ ?
l
N
t,l
+ V ?
l
The possibility table B
c,w,t,l
has a potential size of V ?C ?T ?L. The data sparsity may pose a problem
in estimating B
c,w,t,l
. To reduce the data sparsity problem, we introduce the remedy in our model. We
691
employ a linear interpolation with topic-free word alignment probability to avoid data sparsity problem:
B
?
c,w,t,l
= ?B
c,w,t,l
+ (1? ?)P (c|w),
(6)
where P (c|w) is topic-free word alignment probability between the word w and the company c. ? is
trade-off of two probabilities ranging from 0.0 to 1.0.
2.3 Target Company Extraction
Just like standard LDA, the proposed method itself finds a set of topics but does not directly extract
targets. Suppose we have a dataset which contains microblogs without targets, we can use the collapsed
Gibbs sampling to estimate the topic and label for the words in each microblog. The process is the same
as described in Section 3.2.
After the hidden topics and label of the words in each microblog become stable, we can estimate the
distribution of topics for the dth microblog by: P (t|w
d
) = ?
t
d
=
N
t
d
+?
N
d
+T?
. With the word alignment table
B
?
, we can rank companies for the dth microblog in unlabeled data by computing the scores:
Pr(c
dm
|w
d
) ?
T
?
t=1
N
d
?
n=1
P (c
dm
|t, w
dn
, l
dn
, B
?
) ? P (t|w
d
)P (w
dn
|w
d
),
(7)
where P (w
dn
|w
d
) is the weight of the word w
dn
in the microblog content w
d
. In this paper, we use
inverse document frequency (IDF) score to estimate it. Based on the ranking scores calculated by Eq.(7),
we can extract the top-ranked targets for each microblog to users.
3 Experiments
In this section, we will introduce the experimental results and datasets we constructed for training and
evaluation. We will firstly describe the how we construct the datasets and their statistics. Then we
will introduce the experiment configurations and baseline methods. Finally, the evaluation results and
analysis will be given.
3.1 Datasets
We started by using Sina Weibo?s API
5
to collect public microblogs from randomly selected users. The
dataset contains 282.2M microblogs published by 1.1M users. We use RAW-Weibo to represent it in the
following sections. Based on the collected raw microblogs, we constructed three datasets for evaluation
and training.
3.1.1 Training data
Since social media users post thoughts, ideas, or status on various topics in social medias, there are a
huge number of related companies. Manually constructing training data is a time consuming and cost
process. In this work, we propose a weakly manual method based on ontology and hashtag. A hashtag is
a string of characters preceded by the symbol #. In most cases, hashtags can be viewed as an indication
to the context of the tweet or as the core idea expressed in the tweet. Hence, we can use hashtag as the
targets.
We extract the microblogs whose hashtags contain ontology items as training data and the
corresponding ontology items as targets. Obviously, the training data constructed based on this method
is not perfect. However, since this method can effectively generate a great quantity of data, we think
that general characteristics can be modeled with the generated training data. To evaluate the corpus,
we randomly selected 100 microblogs from the training data and manually labeled their targets. The
accuracy of the sampled dataset is 91%. It indicates that the proposed training data generation method
is effective. From the RAW-Weibo dataset, we extracted a total of 1.79M microblogs whose hashtags
contain more than one target. Training instances for 2,574 target companies are included in the training
data.
5
http://open.weibo.com/
692
3.1.2 Test data
For evaluation, we manually constructed a dataset RAN-Weibo, which contains 2,000 microblogs selected
from RAW-Weibo. Three annotators were asked to label the target companies for each microblog. To
evaluate the quality of annotated dataset, we validate the agreements of human annotations using Cohen?s
kappa coefficient. The average ? among all annotators is 0.626. It indicates that the annotations are
reliable.
Since some targets are ambiguous, inspired by the evaluation campaigns WePS-3 and RepLab 2013,
we also constructed a dataset AMB-Weibo, where microblogs include 10 popular company names which
may cause ambiguity. For each target, we randomly selected and annotated 200 microblogs as golden
standards. Three annotators were also asked to label whether the microblog is related the given target or
not. The agreements of human annotations were also validated through Cohen?s kappa coefficient. The
average ? among all annotators is 0.692.
3.2 Experiment Configurations
We use precision (P ), recall (R), and F1-score (F
1
) to evaluate the performance. We ran our model
with 500 iterations of Gibbs sampling. We use 5-fold cross-validation in the training data to optimize
hyperparameters. The number of topics is set to 30. The other settings of hyperparameters are as follows:
? = 50/T , ? = 0.1, ? = 20, ? = 0.5. The smoothing parameter ? is set to 0.8.
For baselines, we compare the proposed model with the following baseline methods.
? Naive Bayes (NB): The target identification task can be easily formalized as a classification task,
where each target is considered as a classification label. Hence, we applied Naive Bayes to model
the posterior probability of each target given a microblog.
? Support Vector Machine (SVM): The content of microblogs are represented as vectors and SVM
is used to model the classification problem.
? IBM1: Translation model (IBM model-1) is applied to obtain the alignment probability between
words and targets.
? TTM: Topical translation model (TTM) was proposed by Ding et al. (2013) to achieve microblog
hashtag suggestion task. We adopted it to estimate the alignment probability between words and
targets.
3.3 Experimental Results
We evaluate the proposed method from the following perspectives: 1) comparing the proposed method
with the state-of-the-art methods on the two evaluation datasets; 2) identifying the impacts of parameters.
Table 2 shows the comparisons of the proposed method with the state-of-the-arts discriminative
and generative methods on the evaluation dataset RAN-Weibo. ?Our? denotes the method proposed
in previous sections. ?Our w/o BG? represents the proposed method without background knowledge.
From the results, we can observe that the proposed method is better than other methods. Discriminative
methods achieve worse results than generative methods. We think that the large number of targets is
one of the main reasons of the low performances. The results of the proposed models with and without
ontology information also show that background knowledge can benefit both the precision and recall.
TTM achieves better performance than IBM1. It indicates that topical information is useful for this
task. The performances of our method are significantly better than TTM. It illustrates that our smoothing
method and incorporation of background knowledge are effective.
From the description of the proposed model, we can know that there are several hyperparameters in
the proposed model. To evaluate the impacts of them, we evaluate two crucial ones among all of them,
the number of topics T and the smoothing factor ?. Table 3 shows the influence of the number of topics.
From the table, we can observe that the proposed model obtains the best performance when T is set to
30. And performance decreases with more number of topics. We think that data sparsity may be one of
the main reasons. With much more topic number, the data sparsity problem will be more serious when
693
Table 2: Evaluation results of NB, SVM, IBM1, TTM, and our method on the evaluation dataset RAN-
Weibo.
Methods Precision Recall F
1
NB 0.168 0.154 0.161
SVM 0.312 0,286 0.298
IBM1 0.236 0.214 0.220
TTM 0.356 0.327 0.341
Our w/o BG 0.488 0.448 0.467
Our 0.522 0.479 0.500
Table 3: The influence of the number of topics T of the proposed method.
T Precision Recall F
1
10 0.516 0.473 0.493
30 0.522 0.479 0.500
50 0.508 0.466 0.486
70 0.489 0.449 0.468
100 0.488 0.448 0.467
estimating topic-specific translation probability. Table 4 shows the influence of the translation probability
smoothing parameter ?. When ? is set to 0.0, it means that the topical information is omitted. Comparing
the results of ? = 0.0 and other values, we can observe that the topical information can benefit this task.
When ? is set to 1.0, it represents the method without smoothing. The results indicate that it is necessary
to address the sparsity problem through smoothing.
Figure 2 shows the results of different methods on the dataset AMB-Weibo. All the models are trained
with same dataset as the above experiments. From the results, we can observe that the F1-scores vary
from less than 0.40 up to almost 0.60. The performances? variations of other methods are also huge. We
think that training data size and difficulty level are two main reasons. The size of training data of different
targets vary greatly in the dataset. However, comparing with other method, the proposed method is the
most stable one. Comparing with other methods, the proposed method achieves better performance than
other methods for all targets.
4 Related Work
Organization name disambiguation task is fundamental problems in many NLP applications. The task
aims to distinguish the real world relevant of a given name with the same surface in context. WePS-
3
6
(Amig?o et al., 2010) and RepLab 2013
7
(Amig?o et al., 2013) evaluation campaigns have also addressed
the problem from the perspective of disambiguation organization names in microblogs. Pedersen et
al. (2006) proposed an unsupervised method for name discrimination. Yerva et al. (2010) used support
vector machines (SVM) classifier with various external resources, such as WordNet, metadata profile,
category profile, Google set, and so on. Kozareva and Ravi (2011) proposed to use latent dirichlet
allocation to incorporate topical information. Zhang et al. (2012) proposed to use adaptive method for
this task. However, most of these methods focused on the text with predefined surface words. The
documents which do not contain organization names or person names can not be well processed by these
methods.
To bridge the vocabulary gap between content and hashtags, Liu et al. (2012b) proposed to use
translation model to handle it. They modeled the tag suggestion task as a translation process from
6
http://nlp.uned.es/weps/weps-3
7
http://www.limosine-project.eu/events/replab2013
694
Table 4: The influence of the smoothing parameter ? of the propose method.
? Precision Recall F
1
0.0 0.471 0.432 0.451
0.2 0.490 0.449 0.469
0.4 0.495 0.454 0.474
0.6 0.511 0.468 0.489
0.8 0.522 0.479 0.500
1.0 0.519 0.476 0.496
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
1 2 3 4 5 6 7 8 9 10
F1
-S
co
re
NB SVM IBM1 TTM Our w/o BG Our
Figure 2: Evaluation results of NB, SVM, IBM1, TTM, and our method on the different companies in
the test dataset AMB-Weibo.
document content to tags. Ding et al. (2013) extended the translation based method and introduced a
topic-specific translation model to process the multiple meanings of words in different topics. Motivated
by these methods, we also propose to use topic-specific translation model to handle vocabulary problem.
Based on the model, in this work, we incorporate the background knowledge information into the model.
5 Conclusions
To identify target companies of microblogs, in this paper, we propose a novel topical translation
model to achieve the task. The main assumption is that the microblog posts and targets describe
the same thing with different languages. We convert the target identification problem to a translation
process from content of microblogs to targets. We integrate latent topical information into translation
model to hand the themes of microblogs in facilitating the translation process. We also incorporate
background knowledge (such as product names, series, et al.) into the generation model. Experimental
results on a large corpus constructed from a real microblog service and a number of manually labeled
golden standards of easily ambiguous entities demonstrate that the proposed method can achieve better
performance than other approaches.
6 Acknowledgement
The authors wish to thank the anonymous reviewers for their helpful comments. This work was
partially funded by 973 Program (2010CB327900), National Natural Science Foundation of China
(61003092,61073069), Shanghai Leading Academic Discipline Project (B114) and ?Chen Guang?
project supported by Shanghai Municipal Education Commission and Shanghai Education Development
Foundation(11CG05).
695
References
Enrique Amig?o, Javier Artiles, Julio Gonzalo, Damiano Spina, Bing Liu, and Adolfo Corujo. 2010.
Weps3 evaluation campaign: Overview of the on-line reputation management task. In CLEF (Notebook
Papers/LABs/Workshops).
Enrique Amig?o, Jorge Carrillo de Albornoz, Irina Chugur, Adolfo Corujo, Julio Gonzalo, Tamara Mart??n, Edgar
Meij, Maarten Rijke, and Damiano Spina. 2013. Overview of replab 2013: Evaluating online reputation
monitoring systems. In Information Access Evaluation. Multilinguality, Multimodality, and Visualization,
volume 8138 of Lecture Notes in Computer Science, pages 333?352. Springer Berlin Heidelberg.
S. Asur and B.A. Huberman. 2010. Predicting the future with social media. In Proceedings of WI-IAT 2010.
Hila Becker, Mor Naaman, and Luis Gravano. 2010. Learning similarity metrics for event identification in social
media. In Proceedings of WSDM ?10.
Adam Bermingham and Alan F. Smeaton. 2010. Classifying sentiment in microblogs: is brevity an advantage? In
Proceedings of CIKM ?10.
Jiang Bian, Yandong Liu, Eugene Agichtein, and Hongyuan Zha. 2008. Finding the right facts in the crowd:
factoid question answering over social media. In Proceedings of WWW ?08.
Johan Bollen, Huina Mao, and Xiaojun Zeng. 2011. Twitter mood predicts the stock market. Journal of
Computational Science, 2(1):1 ? 8.
Zhuoye Ding, Xipeng Qiu, Qi Zhang, and Xuanjing Huang. 2013. Learning topical translation model for
microblog hashtag suggestion. In Proceedings of IJCAI 2013.
Thomas L. Griffiths and Mark Steyvers. 2004. Finding scientific topics. In Proceedings of the National Academy
of Sciences, volume 101, pages 5228?5235.
Ido Guy, Naama Zwerdling, Inbal Ronen, David Carmel, and Erel Uziel. 2010. Social media recommendation
based on people and tags. In Proceedings of SIGIR ?10.
Ido Guy, Uri Avraham, David Carmel, Sigalit Ur, Michal Jacovi, and Inbal Ronen. 2013. Mining expertise and
interests from social media. In Proceedings of WWW ?13.
Minqing Hu and Bing Liu. 2004. Mining opinion features in customer reviews. In Proceedings of AAAI?04.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment
classification. In Proceedings of ACL-HLT 2011, Portland, Oregon, USA.
Zornitsa Kozareva and Sujith Ravi. 2011. Unsupervised name ambiguity resolution using a generative model.
In Proceedings of the First Workshop on Unsupervised Learning in NLP, EMNLP ?11, pages 105?112,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Chia-Jung Lee and W. Bruce Croft. 2013. Building a web test collection using social media. In Proceedings of
SIGIR ?13, SIGIR ?13.
Chenghua Lin and Yulan He. 2009. Joint sentiment/topic model for sentiment analysis. In Proceedings of CIKM
?09.
Kang Liu, Liheng Xu, and Jun Zhao. 2012a. Opinion target extraction using word-based translation model. In
Proceedings of EMNLP-CoNLL ?12.
Zhiyuan Liu, Chen Liang, and Maosong Sun. 2012b. Topical word trigger model for keyphrase extraction. In
Proceedings of COLING.
Kang Liu, Liheng Xu, and Jun Zhao. 2013. Syntactic patterns versus word alignment: Extracting opinion targets
from online reviews. In Proceedings of ACL 2013, Sofia, Bulgaria.
Takanobu Otsuka, Takuya Yoshimura, and Takayuki Ito. 2012. Evaluation of the reputation network using realistic
distance between facebook data. In Proceedings of WI-IAT ?12, Washington, DC, USA.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1?135,
January.
696
Ted Pedersen, Anagha Kulkarni, Roxana Angheluta, Zornitsa Kozareva, and Thamar Solorio. 2006. An
unsupervised language independent method of name discrimination using second order co-occurrence features.
In Computational Linguistics and Intelligent Text Processing, pages 208?222.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In
Proceedings of HL-EMNLP 2005, Vancouver, British Columbia, Canada.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2011. Opinion word expansion and target extraction through
double propagation. Comput. Linguist., 37(1):9?27, March.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes twitter users: real-time event
detection by social sensors. In Proceedings of the 19th international conference on World wide web, WWW
?10, pages 851?860, New York, NY, USA. ACM.
Damiano Spina, Edgar Meij, Maarten de Rijke, Andrei Oghina, Minh Thuong Bui, and Mathias Breuss. 2012.
Identifying entity aspects in microblog posts. In Proceedings of SIGIR ?12.
Damiano Spina, Julio Gonzalo, and Enrique Amig?o. 2013. Discovering filter keywords for company name
disambiguation in twitter. Expert Systems with Applications, 40(12):4986 ? 5003.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu. 2009. Phrase dependency parsing for opinion mining.
In Proceedings of EMNLP 2009, Singapore.
Surender Reddy Yerva, Zoltn Mikls, and Karl Aberer. 2010. It was easy, when apples and blackberries
were only fruits. In Martin Braschler, Donna Harman, and Emanuele Pianta, editors, CLEF (Notebook
Papers/LABs/Workshops).
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn O?Brien-Strain. 2010. Extracting and ranking product features
in opinion documents. In Proceedings of COLING ?10.
Shu Zhang, Jianwei Wu, Dequan Zheng, Yao Meng, and Hao Yu. 2012. An adaptive method for organization name
disambiguation with feature reinforcing. In Proceedings of the 26th Pacific Asia Conference on Language,
Information, and Computation, pages 237?245, Bali,Indonesia, November. Faculty of Computer Science,
Universitas Indonesia.
697
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 187?195,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Joint Training and Decoding Using Virtual Nodes for Cascaded
Segmentation and Tagging Tasks
Xian Qian, Qi Zhang, Yaqian Zhou, Xuanjing Huang, Lide Wu
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, P.R.China
{qianxian, qz, zhouyaqian, xjhuang, ldwu}@fudan.edu.cn
Abstract
Many sequence labeling tasks in NLP require
solving a cascade of segmentation and tag-
ging subtasks, such as Chinese POS tagging,
named entity recognition, and so on. Tradi-
tional pipeline approaches usually suffer from
error propagation. Joint training/decoding in
the cross-product state space could cause too
many parameters and high inference complex-
ity. In this paper, we present a novel method
which integrates graph structures of two sub-
tasks into one using virtual nodes, and per-
forms joint training and decoding in the fac-
torized state space. Experimental evaluations
on CoNLL 2000 shallow parsing data set and
Fourth SIGHAN Bakeoff CTB POS tagging
data set demonstrate the superiority of our
method over cross-product, pipeline and can-
didate reranking approaches.
1 Introduction
There is a typical class of sequence labeling tasks
in many natural language processing (NLP) applica-
tions, which require solving a cascade of segmenta-
tion and tagging subtasks. For example, many Asian
languages such as Japanese and Chinese which
do not contain explicitly marked word boundaries,
word segmentation is the preliminary step for solv-
ing part-of-speech (POS) tagging problem. Sen-
tences are firstly segmented into words, then each
word is assigned with a part-of-speech tag. Both
syntactic parsing and dependency parsing usually
start with a textual input that is tokenized, and POS
tagged.
The most commonly approach solves cascaded
subtasks in a pipeline, which is very simple to im-
plement and allows for a modular approach. While,
the key disadvantage of such method is that er-
rors propagate between stages, significantly affect-
ing the quality of the final results. To cope with this
problem, Shi and Wang (2007) proposed a rerank-
ing framework in which N-best segment candidates
generated in the first stage are passed to the tag-
ging model, and the final output is the one with the
highest overall segmentation and tagging probabil-
ity score. The main drawback of this method is that
the interaction between tagging and segmentation is
restricted by the number of candidate segmentation
outputs. Razvan C. Bunescu (2008) presented an
improved pipeline model in which upstream subtask
outputs are regarded as hidden variables, together
with their probabilities are used as probabilistic fea-
tures in the downstream subtasks. One shortcom-
ing of this method is that calculation of marginal
probabilities of features may be inefficient and some
approximations are required for fast computation.
Another disadvantage of these two methods is that
they employ separate training and the segmentation
model could not take advantages of tagging infor-
mation in the training procedure.
On the other hand, joint learning and decoding
using cross-product of segmentation states and tag-
ging states does not suffer from error propagation
problem and achieves higher accuracy on both sub-
tasks (Ng and Low, 2004). However, two problems
arises due to the large state space, one is that the
amount of parameters increases rapidly, which is apt
to overfit on the training corpus, the other is that
the inference by dynamic programming could be in-
efficient. Sutton (2004) proposed Dynamic Con-
ditional Random Fields (DCRFs) to perform joint
training/decoding of subtasks using much fewer pa-
rameters than the cross-product approach. How-
187
ever, DCRFs do not guarantee non-violation of hard-
constraints that nodes within the same segment get
a single consistent tagging label. Another draw-
back of DCRFs is that exact inference is generally
time consuming, some approximations are required
to make it tractable.
Recently, perceptron based learning framework
has been well studied for incorporating node level
and segment level features together (Kazama and
Torisawa, 2007; Zhang and Clark, 2008). The main
shortcoming is that exact inference is intractable
for those dynamically generated segment level fea-
tures, so candidate based searching algorithm is
used for approximation. On the other hand, Jiang
(2008) proposed a cascaded linear model which has
a two layer structure, the inside-layer model uses
node level features to generate candidates with their
weights as inputs of the outside layer model which
captures non-local features. As pipeline models, er-
ror propagation problem exists for such method.
In this paper, we present a novel graph structure
that exploits joint training and decoding in the fac-
torized state space. Our method does not suffer
from error propagation, and guards against viola-
tions of those hard-constraints imposed by segmen-
tation subtask. The motivation is to integrate two
Markov chains for segmentation and tagging sub-
tasks into a single chain, which contains two types of
nodes, then standard dynamic programming based
exact inference is employed on the hybrid struc-
ture. Experiments are conducted on two different
tasks, CoNLL 2000 shallow parsing and SIGHAN
2008 Chinese word segmentation and POS tagging.
Evaluation results of shallow parsing task show
the superiority of our proposed method over tradi-
tional joint training/decoding approach using cross-
product state space, and achieves the best reported
results when no additional resources at hand. For
Chinese word segmentation and POS tagging task, a
strong baseline pipeline model is built, experimental
results show that the proposed method yields a more
substantial improvement over the baseline than can-
didate reranking approach.
The rest of this paper is organized as follows: In
Section 2, we describe our novel graph structure. In
Section 3, we analyze complexity of our proposed
method. Experimental results are shown in Section
4. We conclude the work in Section 5.
2 Multi-chain integration using Virtual
Nodes
2.1 Conditional Random Fields
We begin with a brief review of the Conditional Ran-
dom Fields(CRFs). Let x = x1x2 . . . xl denote the
observed sequence, where xi is the ith node in the
sequence, l is sequence length, y = y1y2 . . . yl is a
label sequence over x that we wish to predict. CRFs
(Lafferty et al, 2001) are undirected graphic mod-
els that use Markov network distribution to learn the
conditional probability. For sequence labeling task,
linear chain CRFs are very popular, in which a first
order Markov assumption is made on the labels:
p(y|x) = 1Z(x)
?
i
?(x,y, i)
,where
?(x,y, i) = exp
(
wT f(x, yi?1, yi, i)
)
Z(x) =
?
y
?
i
?(x,y, i)
f(x, yi?1, yi, i) =
[f1(x, yi?1, yi, i), . . .,fm(x, yi?1, yi, i)]T , each ele-
ment fj(x, yi?1, yi, i) is a real valued feature func-
tion, here we simplify the notation of state feature
by writing fj(x, yi, i) = fj(x, yi?1, yi, i), m is the
cardinality of feature set {fj}. w = [w1, . . . , wm]T
is a weight vector to be learned from the training
set. Z(x) is the normalization factor over all label
sequences for x.
In the traditional joint training/decoding approach
for cascaded segmentation and tagging task, each
label yi has the form si-ti, which consists of seg-
mentation label si and tagging label ti. Let s =
s1s2 . . . sl be the segmentation label sequence over
x. There are several commonly used label sets such
as BI, BIO, IOE, BIES, etc. To facilitate our dis-
cussion, in later sections we will use BIES label set,
where B,I,E represents Beginning, Inside and End of
a multi-node segment respectively, S denotes a sin-
gle node segment. Let t = t1t2 . . . tl be the tagging
label sequence over x. For example, in named entity
recognition task, ti ? {PER, LOC, ORG, MISC,
O} represents an entity type (person name, loca-
tion name, organization name, miscellaneous entity
188
x2
s?t
2
2
x1
s?t
1
1
S-P S-O
x3
s?t
3
3
S-O
x4
s?t
4
4
B-P
x5
s?t
5
5
E-P
Hendrix ?s girlfriend Kathy Etchingham
Figure 1: Graphical representation of linear chain CRFs
for traditional joint learning/decoding
name and other). Graphical representation of lin-
ear chain CRFs is shown in Figure 1, where tagging
label ?P? is the simplification of ?PER?. For nodes
that are labeled as other, we define si =S, ti =O.
2.2 Hybrid structure for cascaded labeling
tasks
Different from traditional joint approach, our
method integrates two linear markov chains for seg-
mentation and tagging subtasks into one that con-
tains two types of nodes. Specifically, we first
regard segmentation and tagging as two indepen-
dent sequence labeling tasks, corresponding chain
structures are built, as shown in the top and mid-
dle sub-figures of Figure 2. Then a chain of twice
length of the observed sequence is built, where
nodes x1, . . . , xl on the even positions are original
observed nodes, while nodes v1, . . . , vl on the odd
positions are virtual nodes that have no content in-
formation. For original nodes xi, the state space is
the tagging label set, while for virtual nodes, their
states are segmentation labels. The label sequence
of the hybrid chain is y = y1 . . . y2l = s1t1 . . . sltl,
where combination of consecutive labels siti repre-
sents the full label for node xi.
Then we let si be connected with si?1 and si+1
, so that first order Markov assumption is made
on segmentation states. Similarly, ti is connected
with ti?1 and ti+1. Then neighboring tagging and
segmentation states are connected as shown in the
bottom sub-figure of Figure 2. Non-violation of
hard-constraints that nodes within the same seg-
ment get a single consistent tagging label is guar-
anteed by introducing second order transition fea-
tures f(ti?1, si, ti, i) that are true if ti?1 6= ti and
si ? {I,E}. For example, fj(ti?1, si, ti, i) is de-
fined as true if ti?1 =PER, si =I and ti =LOC.
In other words, it is true, if a segment is partially
tagging as PER, and partially tagged as LOC. Since
such features are always false in the training corpus,
their corresponding weights will be very low so that
inconsistent label assignments impossibly appear in
decoding procedure. The hybrid graph structure can
be regarded as a special case of second order Markov
chain.
Hendrix ?s girlfriend Kathy Etchingham
x1 x2 x3 x4 x5
s1 s2 s3 s4 s5
S S S B E
x1 x2 x3 x4 x5
t1 t2 t3 t4 t5
P O O P P
x1 x2 x3 x4 x5
t1 t2 t3 t4 t5
P O O P P
s2s1 s3 s4 s5
S S S B E
v1 v2 v3 v4 v5
Integrate
Figure 2: Multi-chain integration using Virtual Nodes
2.3 Factorized features
Compared with traditional joint model that exploits
cross-product state space, our hybrid structure uses
factorized states, hence could handle more flexible
features. Any state feature g(x, yi, i) defined in
the cross-product state space can be replaced by a
first order transition feature in the factorized space:
f(x, si, ti, i). As for the transition features, we
use f(si?1, ti?1, si, i) and f(ti?1, si, ti, i) instead
of g(yi?1, yi, i) in the conventional joint model.
Features in cross-product state space require that
segmentation label and tagging label take on partic-
ular values simultaneously, however, sometimes we
189
want to specify requirement on only segmentation or
tagging label. For example, ?Smith? may be an end
of a person name, ?Speaker: John Smith?; or a sin-
gle word person name ?Professor Smith will . . . ?. In
such case, our observation is that ?Smith? is likely a
(part of) person name, we do not care about its seg-
mentation label. So we could define state feature
f(x, ti, i) = true, if xi is ?Smith? with tagging la-
bel ti=PER.
Further more, we could define features like
f(x, ti?1, ti, i), f(x, si?1, si, i), f(x, ti?1, si, i),
etc. The hybrid structure facilitates us to use
varieties of features. In the remainder of the
paper, we use notations f(x, ti?1, si, ti, i) and
f(x, si?1, ti?1, si, i) for simplicity.
2.4 Hybrid CRFs
A hybrid CRFs is a conditional distribution that fac-
torizes according to the hybrid graphical model, and
is defined as:
p(s, t|x) = 1Z(x)
?
i
?(x, s, t, i)
?
i
?(x, s, t, i)
Where
?(x, s, t, i) = exp
(
wT1 f(x, si?1, ti?1, si)
)
?(x, s, t, i) = exp
(
wT2 f(x, ti?1, si, ti)
)
Z(x) =
?
s,t
(?
i
?(x, s, t, i)
?
i
?(x, s, t, i)
)
Where w1, w2 are weight vectors.
Luckily, unlike DCRFs, in which graph structure
can be very complex, and the cross-product state
space can be very large, in our cascaded labeling
task, the segmentation label set is often small, so
far as we known, the most complicated segmenta-
tion label set has only 6 labels (Huang and Zhao,
2007). So exact dynamic programming based algo-
rithms can be efficiently performed.
In the training stage, we use second order forward
backward algorithm to compute the marginal proba-
bilities p(x, si?1, ti?1, si) and p(x, ti?1, si, ti), and
the normalization factor Z(x). In decoding stage,
we use second order Viterbi algorithm to find the
best label sequence. The Viterbi decoding can be
Table 1: Time Complexity
Method Training Decoding
Pipeline (|S|2cs + |T |2ct)L (|S|2 + |T |2)U
Cross-Product (|S||T |)2cL (|S||T |)2U
Reranking (|S|2cs + |T |2ct)L (|S|2 + |T |2)NU
Hybrid (|S| + |T |)|S||T |cL (|S| + |T |)|S||T |U
used to label a new sequence, and marginal compu-
tation is used for parameter estimation.
3 Complexity Analysis
The time complexity of the hybrid CRFs train-
ing and decoding procedures is higher than that of
pipeline methods, but lower than traditional cross-
product methods. Let
? |S| = size of the segmentation label set.
? |T | = size of the tagging label set.
? L = total number of nodes in the training data
set.
? U = total number of nodes in the testing data
set.
? c = number of joint training iterations.
? cs = number of segmentation training itera-
tions.
? ct = number of tagging training iterations.
? N = number of candidates in candidate rerank-
ing approach.
Time requirements for pipeline, cross-product, can-
didate reranking and hybrid CRFs are summarized
in Table 1. For Hybrid CRFs, original node xi has
features {fj(ti?1, si, ti)}, accessing all label subse-
quences ti?1siti takes |S||T |2 time, while virtual
node vi has features {fj(si?1, ti?1, si)}, accessing
all label subsequences si?1ti?1si takes |S|2|T | time,
so the final complexity is (|S|+ |T |)|S||T |cL.
In real applications, |S| is small, |T | could be
very large, we assume that |T | >> |S|, so for
each iteration, hybrid CRFs is about |S| times slower
than pipeline and |S| times faster than cross-product
190
Table 2: Feature templates for shallow parsing task
Cross Product CRFs Hybrid CRFs
wi?2yi, wi?1yi, wiyi wi?1si, wisi, wi+1si
wi+1yi, wi+2yi wi?2ti, wi?1ti, witi, wi+1ti, wi+2ti
wi?1wiyi, wiwi+1yi wi?1wisi, wiwi+1si
wi?1witi, wiwi+1ti
pi?2yi, pi?1yi, piyi pi?1si, pisi, pi+1si
pi+1yi, pi+2yi pi?2ti, pi?1ti, pi+1ti, pi+2ti
pi?2pi?1yi, pi?1piyi, pipi+1yi,
pi+1pi+2yi
pi?2pi?1si, pi?1pisi, pipi+1si, pi+1pi+2si
pi?3pi?2ti, pi?2pi?1ti, pi?1piti, pipi+1ti,
pi+1pi+2ti, pi+2pi+3ti, pi?1pi+1ti
pi?2pi?1piyi, pi?1pipi+1yi,
pipi+1pi+2yi
pi?2pi?1pisi, pi?1pipi+1si, pipi+1pi+2si
wipiti
wisi?1si
wi?1ti?1ti, witi?1ti, pi?1ti?1ti, piti?1ti
yi?1yi si?1ti?1si, ti?1siti
method. When decoding, candidate reranking ap-
proach requires more time if candidate number N >
|S|.
Though the space complexity could not be com-
pared directly among some of these methods, hybrid
CRFs require less parameters than cross-product
CRFs due to the factorized state space. This is sim-
ilar with factorized CRFs (FCRFs) (Sutton et al,
2004).
4 Experiments
4.1 Shallow Parsing
Our first experiment is the shallow parsing task. We
use corpus from CoNLL 2000 shared task, which
contains 8936 sentences for training and 2012 sen-
tences for testing. There are 11 tagging labels: noun
phrase(NP), verb phrase(VP) , . . . and other (O), the
segmentation state space we used is BIES label set,
since we find that it yields a little improvement over
BIO set.
We use the standard evaluation metrics, which are
precision P (percentage of output phrases that ex-
actly match the reference phrases), recall R (percent-
age of reference phrases returned by our system),
and their harmonic mean, the F1 score F1 = 2PRP+R
(which we call F score in what follows).
We compare our approach with traditional cross-
product method. To find good feature templates,
development data are required. Since CoNLL2000
does not provide development data set, we divide
the training data into 10 folds, of which 9 folds for
training and 1 fold for developing. After selecting
feature templates by cross validation, we extract fea-
tures and learn their weights on the whole training
data set. Feature templates are summarized in Table
2, where wi denotes the ith word, pi denotes the ith
POS tag.
Notice that in the second row, feature templates
of the hybrid CRFs does not contain wi?2si, wi+2si,
since we find that these two templates degrade per-
formance in cross validation. However, wi?2ti,
wi+2ti are useful, which implies that the proper con-
text window size for segmentation is smaller than
tagging. Similarly, for hybrid CRFs, the window
size of POS bigram features for segmentation is 5
(from pi?2 to pi+2, see the eighth row in the sec-
ond column); while for tagging, the size is 7 (from
pi?3 to pi+3, see the ninth row in the second col-
umn). However for cross-product method, their win-
dow sizes must be consistent.
For traditional cross-product CRFs and our hybrid
CRFs, we use fixed gaussian prior ? = 1.0 for both
methods, we find that this parameter does not signifi-
191
Table 3: Results for shallow parsing task, Hybrid CRFs
significantly outperform Cross-Product CRFs (McNe-
mar?s test; p < 0.01)
Method Cross-Product
CRFs
Hybrid
CRFs
Training Time 11.6 hours 6.3 hours
Feature Num-
ber
13 million 10 mil-
lion
Iterations 118 141
F1 93.88 94.31
cantly affect the results when it varies between 1 and
10. LBFGS(Nocedal and Wright, 1999) method is
employed for numerical optimization. Experimen-
tal results are shown in Table 3. Our proposed CRFs
achieve a performance gain of 0.43 points in F-score
over cross-product CRFs that use state space while
require less training time.
For comparison, we also listed the results of pre-
vious top systems, as shown in Table 4. Our pro-
posed method outperforms other systems when no
additional resources at hand. Though recently semi-
supervised learning that incorporates large mounts
of unlabeled data has been shown great improve-
ment over traditional supervised methods, such as
the last row in Table 4, supervised learning is funda-
mental. We believe that combination of our method
and semi-supervised learning will achieve further
improvement.
4.2 Chinese word segmentation and POS
tagging
Our second experiment is the Chinese word seg-
mentation and POS tagging task. To facilitate com-
parison, we focus only on the closed test, which
means that the system is trained only with a des-
ignated training corpus, any extra knowledge is not
allowed, including Chinese and Arabic numbers, let-
ters and so on. We use the Chinese Treebank (CTB)
POS corpus from the Fourth International SIGHAN
Bakeoff data sets (Jin and Chen, 2008). The train-
ing data consist of 23444 sentences, 642246 Chinese
words, 1.05M Chinese characters and testing data
consist of 2079 sentences, 59955 Chinese words,
0.1M Chinese characters.
We compare our hybrid CRFs with pipeline and
candidate reranking methods (Shi and Wang, 2007)
Table 4: Comparison with other systems on shallow pars-
ing task
Method F1 Additional Re-
sources
Cross-Product CRFs 93.88
Hybrid CRFs 94.31
SVM combination 93.91
(Kudo and Mat-
sumoto, 2001)
Voted Perceptrons 93.74 none
(Carreras and Mar-
quez, 2003)
ETL (Milidiu et al,
2008)
92.79
(Wu et al, 2006) 94.21 Extended features
such as token fea-
tures, affixes
HySOL 94.36 17M words unla-
beled
(Suzuki et al, 2007) data
ASO-semi 94.39 15M words unla-
beled
(Ando and Zhang,
2005)
data
(Zhang et al, 2002) 94.17 full parser output
(Suzuki and Isozaki,
2008)
95.15 1G words unla-
beled data
using the same evaluation metrics as shallow pars-
ing. We do not compare with cross-product CRFs
due to large amounts of parameters.
For pipeline method, we built our word segmenter
based on the work of Huang and Zhao (2007),
which uses 6 label representation, 7 feature tem-
plates (listed in Table 5, where ci denotes the ith
Chinese character in the sentence) and CRFs for pa-
rameter learning. We compare our segmentor with
other top systems using SIGHAN CTB corpus and
evaluation metrics. Comparison results are shown
in Table 6, our segmenter achieved 95.12 F-score,
which is ranked 4th of 26 official runs. Except for
the first system which uses extra unlabeled data, dif-
ferences between rest systems are not significant.
Our POS tagging system is based on linear chain
CRFs. Since SIGHAN dose not provide develop-
ment data, we use the 10 fold cross validation de-
scribed in the previous experiment to turning feature
templates and Gaussian prior. Feature templates are
listed in Table 5, where wi denotes the ith word in
192
Table 5: Feature templates for Chinese word segmentation and POS tagging task
Segmentation feature templates
(1.1) ci?2si, ci?1si, cisi, ci+1si, ci+2si
(1.2) ci?1cisi, cici+1si, ci?1ci+1si
(1.3) si?1si
POS tagging feature templates
(2.1) wi?2ti, wi?1ti, witi, wi+1ti, wi+2ti
(2.2) wi?2wi?1ti, wi?1witi, wiwi+1ti, wi+1wi+2ti, wi?1wi+1ti
(2.3) c1(wi)ti, c2(wi)ti, c3(wi)ti, c?2(wi)ti, c?1(wi)ti
(2.4) c1(wi)c2(wi)ti, c?2(wi)c?1(wi)ti
(2.5) l(wi)ti
(2.6) ti?1ti
Joint segmentation and POS tagging feature templates
(3.1) ci?2si, ci?1si, cisi, ci+1si, ci+2si
(3.2) ci?1cisi, cici+1si, ci?1ci+1si
(3.3) ci?3ti, ci?2ti, ci?1ti, citi, ci+1ti, ci+2ti, ci+3ti
(3.4) ci?3ci?2ti, ci?2ci?1ti, ci?1citi, cici+1ti ci+1ci+2ti, ci+2ci+3ti, ci?2citi, cici+2ti
(3.5) cisiti
(3.6) citi?1ti
(3.7) si?1ti?1si, ti?1siti
Table 6: Word segmentation results on Fourth SIGHAN
Bakeoff CTB corpus
Rank F1 Description
1/26 95.89? official best, using extra un-
labeled data (Zhao and Kit,
2008)
2/26 95.33 official second
3/26 95.17 official third
4/26 95.12 segmentor in pipeline sys-
tem
Table 7: POS results on Fourth SIGHAN Bakeoff CTB
corpus
Rank Accuracy Description
1/7 94.29 POS tagger in pipeline sys-
tem
2/7 94.28 official best
3/7 94.01 official second
4/7 93.24 official third
the sentence, cj(wi), j > 0 denotes the jth Chinese
character of word wi, cj(wi), j < 0 denotes the jth
last Chinese character, l(wi) denotes the word length
of wi. We compare our POS tagger with other top
systems on Bakeoff CTB POS corpus where sen-
tences are perfectly segmented into words, our POS
tagger achieved 94.29 accuracy, which is the best of
7 official runs. Comparison results are shown in Ta-
ble 7.
For reranking method, we varied candidate num-
bers n among n ? {10, 20, 50, 100}. For hybrid
CRFs, we use the same segmentation label set as
the segmentor in pipeline. Feature templates are
listed in Table 5. Experimental results are shown
in Figure 3. The gain of hybrid CRFs over the
baseline pipeline model is 0.48 points in F-score,
about 3 times higher than 100-best reranking ap-
proach which achieves 0.13 points improvement.
Though larger candidate number can achieve higher
performance, such improvement becomes trivial for
n > 20.
Table 8 shows the comparison between our work
and other relevant work. Notice that, such com-
parison is indirect due to different data sets and re-
193
0 20 40 60 80 10090.3
90.4
90.5
90.6
90.7
90.8
90.9
candidate number
F s
cor
e
 
 
candidate reranking
Hybrid CRFs
Figure 3: Results for Chinese word segmentation and
POS tagging task, Hybrid CRFs significantly outperform
100-Best Reranking (McNemar?s test; p < 0.01)
Table 8: Comparison of word segmentation and POS tag-
ging, such comparison is indirect due to different data
sets and resources.
Model F1
Pipeline (ours) 90.40
100-Best Reranking (ours) 90.53
Hybrid CRFs (ours) 90.88
Pipeline (Shi and Wang, 2007) 91.67
20-Best Reranking (Shi and Wang,
2007)
91.86
Pipeline (Zhang and Clark, 2008) 90.33
Joint Perceptron (Zhang and Clark,
2008)
91.34
Perceptron Only (Jiang et al, 2008) 92.5
Cascaded Linear (Jiang et al, 2008) 93.4
sources. One common conclusion is that joint mod-
els generally outperform pipeline models.
5 Conclusion
We introduced a framework to integrate graph struc-
tures for segmentation and tagging subtasks into one
using virtual nodes, and performs joint training and
decoding in the factorized state space. Our approach
does not suffer from error propagation, and guards
against violations of those hard-constraints imposed
by segmentation subtask. Experiments on shal-
low parsing and Chinese word segmentation tasks
demonstrate our technique.
6 Acknowledgements
The author wishes to thank the anonymous review-
ers for their helpful comments. This work was
partially funded by 973 Program (2010CB327906),
The National High Technology Research and De-
velopment Program of China (2009AA01A346),
Shanghai Leading Academic Discipline Project
(B114), Doctoral Fund of Ministry of Education of
China (200802460066), National Natural Science
Funds for Distinguished Young Scholar of China
(61003092), and Shanghai Science and Technology
Development Funds (08511500302).
References
R. Ando and T. Zhang. 2005. A high-performance semi-
supervised learning method for text chunking. In Pro-
ceedings of ACL, pages 1?9.
Razvan C. Bunescu. 2008. Learning with probabilistic
features for improved pipeline models. In Proceedings
of EMNLP, Waikiki, Honolulu, Hawaii.
X Carreras and L Marquez. 2003. Phrase recognition by
filtering and ranking with perceptrons. In Proceedings
of RANLP.
Changning Huang and Hai Zhao. 2007. Chinese word
segmentation: A decade review. Journal of Chinese
Information Processing, 21:8?19.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Lu.
2008. A cascaded linear model for joint chinese word
segmentation and part-of-speech tagging. In Proceed-
ings of ACL, Columbus, Ohio, USA.
Guangjin Jin and Xiao Chen. 2008. The fourth interna-
tional chinese language processing bakeoff: Chinese
word segmentation, named entity recognition and chi-
nese pos tagging. In Proceedings of Sixth SIGHAN
Workshop on Chinese Language Processing, India.
Junichi Kazama and Kentaro Torisawa. 2007. A new
perceptron algorithm for sequence labeling with non-
local features. In Proceedings of EMNLP, pages 315?
324, Prague, June.
Taku Kudo and Yuji Matsumoto. 2001. Chunking with
support vector machines. In Proceedings of NAACL.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML.
Ruy L. Milidiu, Cicero Nogueira dos Santos, and Julio C.
Duarte. 2008. Phrase chunking using entropy guided
transformation learning. In Proceedings of ACL, pages
647?655.
194
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese part-
ofspeech tagging: One-at-a-time or all-at-once? word-
based or character-based? In Proceedings of EMNLP.
J. Nocedal and S. J. Wright. 1999. Numerical Optimiza-
tion. Springer.
Yanxin Shi and Mengqiu Wang. 2007. A dual-layer crfs
based joint decoding method for cascaded segmenta-
tion and labeling tasks. In Proceedings of IJCAI, pages
1707?1712, Hyderabad, India.
C. Sutton, K. Rohanimanesh, and A. McCallum. 2004.
Dynamic conditional random fields: Factorized prob-
abilistic models for labeling and segmenting sequence
data. In Proceedings of ICML.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of ACL, pages
665?673.
Jun Suzuki, Akinori Fujino, and Hideki Isozaki. 2007.
Semi-supervised structured output learning based on
a hybrid generative and discriminative approach. In
Proceedings of EMNLP, Prague.
Yu-Chieh Wu, Chia-Hui Chang, and Yue-Shi Lee. 2006.
A general and multi-lingual phrase chunking model
based on masking method. In Proceedings of Intel-
ligent Text Processing and Computational Linguistics,
pages 144?155.
Yue Zhang and Stephen Clark. 2008. Joint word seg-
mentation and pos tagging using a single perceptron.
In Proceedings of ACL, Columbus, Ohio, USA.
T. Zhang, F. Damerau, and D. Johnson. 2002. Text
chunking based on a generalization of winnow. ma-
chine learning research. Machine Learning Research,
2:615?637.
Hai Zhao and Chunyu Kit. 2008. Unsupervised segmen-
tation helps supervised learning of character tagging
forword segmentation and named entity recognition.
In Proceedings of Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106?111.
195
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1332?1341,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Structural Opinion Mining for Graph-based Sentiment Representation
Yuanbin Wu, Qi Zhang, Xuanjing Huang, Lide Wu
Fudan University
School of Computer Science
{ybwu,qz,xjhuang,ldwu}@fudan.edu.cn
Abstract
Based on analysis of on-line review corpus
we observe that most sentences have compli-
cated opinion structures and they cannot be
well represented by existing methods, such as
frame-based and feature-based ones. In this
work, we propose a novel graph-based rep-
resentation for sentence level sentiment. An
integer linear programming-based structural
learning method is then introduced to produce
the graph representations of input sentences.
Experimental evaluations on a manually la-
beled Chinese corpus demonstrate the effec-
tiveness of the proposed approach.
1 Introduction
Sentiment analysis has received much attention in
recent years. A number of automatic methods have
been proposed to identify and extract opinions, emo-
tions, and sentiments from text. Previous researches
on sentiment analysis tackled the problem on vari-
ous levels of granularity including document, sen-
tence, phrase and word (Pang et al, 2002; Riloff et
al., 2003; Dave et al, 2003; Takamura et al, 2005;
Kim and Hovy, 2006; Somasundaran et al, 2008;
Dasgupta and Ng, 2009; Hassan and Radev, 2010).
They mainly focused on two directions: sentiment
classification which detects the overall polarity of a
text; sentiment related information extraction which
tries to answer the questions like ?who expresses
what opinion on which target?.
Most of the current studies on the second direc-
tion assume that an opinion can be structured as a
frame which is composed of a fixed number of slots.
Typical slots include opinion holder, opinion expres-
sion, and evaluation target. Under this representa-
tion, they defined the task as a slots filling prob-
lem for each of the opinions. Named entity recog-
nition and relation extraction techniques are usually
applied in this task (Hu and Liu, 2004; Kobayashi
et al, 2007; Wu et al, 2009).
However, through data analysis, we observe that
60.5% of sentences in our corpus do not follow the
assumption used by them. A lot of important infor-
mation about an opinion may be lost using those rep-
resentation methods. Consider the following exam-
ples, which are extracted from real online reviews:
Example 1: The interior is a bit noisy on the free-
way1.
Example 2: Takes good pictures during the day-
time. Very poor picture quality at night2.
Based on the definition of opinion unit proposed
by Hu and Liu (2004), from the first example, the
information we can get is the author?s negative opin-
ion about ?interior? using an opinion expression
?noisy?. However, the important restriction ?on the
freeway?, which narrows the scope of the opinion,
is ignored. In fact, the tuple (?noisy?,?on the free-
way?) cannot correctly express the original opinion:
it is negative but under certain condition. The sec-
ond example is similar. If the conditions ?during the
daytime? and ?at night? are dropped, the extracted
elements cannot correctly represent user?s opinions.
Example 3: The camera is actually quite good for
outdoors because of the software.
Besides that, an opinion expression may induce
other opinions which are not expressed directly. In
example 3, the opinion expression is ?good? whose
1http://reviews.carreview.com/blog/2010-ford-focus-
review-the-compact-car-that-can/
2http://www.dooyoo.co.uk/digital-camera/sony-cyber-shot-
dsc-s500/1151680/
1332
target is ?camera?. But the ?software? which trig-
gers the opinion expression ?good? is also endowed
with a positive opinion. In practice, this induced
opinion on ?software? is actually more informative
than its direct counterpart. Mining those opinions
may help to form a complete sentiment analysis re-
sult.
Example 4: The image quality is in the middle of
its class, but it can still be a reasonable choice for
students.
Furthermore, the relations among individual opin-
ions also provide additional information which is
lost when they are considered separately. Example
4 is such a case that the whole positive comment of
camera is expressed by a transition from a negative
opinion to a positive one.
In order to address those issues, this paper de-
scribes a novel sentiment representation and analysis
method. Our main contributions are as follows:
1. We investigate the use of graphs for repre-
senting sentence level sentiment. The ver-
tices are evaluation target, opinion expression,
modifiers of opinion. The Edges represent
relations among them. The semantic rela-
tions among individual opinions are also in-
cluded. Through the graph, various informa-
tion on opinion expressions which is ignored
by current representation methods can be well
handled. And the proposed representation is
language-independent.
2. We propose a supervised structural learning
method which takes a sentence as input and the
proposed sentiment representation for it as out-
put. The inference algorithm is based on in-
teger linear programming which helps to con-
cisely and uniformly handle various properties
of our sentiment representation. By setting ap-
propriate prior substructure constraints of the
graph, the whole algorithm achieves reasonable
performances.
The remaining part of this paper is organized as
follows: In Section 2 we discuss the proposed rep-
resentation method. Section 3 describes the com-
putational model used to construct it. Experimental
results in test collections and analysis are shown in
Section 4. In Section 5, we present the related work
and Section 6 concludes the paper.
2 Graph-based Sentiment Representation
In this work, we propose using directed graph to
represent sentiments. In the graph, vertices are
text spans in the sentences which are opinion ex-
pressions, evaluation targets, conditional clauses etc.
Two types of edges are included in the graph: (1)
relations among opinion expressions and their mod-
ifiers; (2) relations among opinion expressions. The
edges of the first type exist within individual opin-
ions. The second type of the edges captures the re-
lations among individual opinions. The following
sections detail the definition.
2.1 Individual Opinion Representation
Let r be an opinion expression in a sentence, the rep-
resentation unit for r is a set of relations {(r, dk)}.
For each relation (r, dk), dk is a modifier which is a
span of text specifying the change of r?s meaning.
The relations between modifier and opinion ex-
pression can be the type of any kind. In this work,
we mainly consider two basic types:
? opinion restriction. (r, dk) is called an opin-
ion restriction if dk narrows r?s scope, adds a
condition, or places limitations on r?s original
meaning.
? opinion expansion. (r, dk) is an opinion expan-
sion if r?s scope expands to dk, r induces an-
other opinion on dk, or the opinion on dk is im-
plicitly expressed by r.
Mining the opinion restrictions can help to get ac-
curate meaning of an opinion, and the opinion ex-
pansions are useful to cover more indirect opinions.
As with previous sentiment representations, we ac-
tually consider the third type of modifier which dk is
the evaluation target of r.
Figure 1 shows a concrete example. In this ex-
ample, there are three opinion expressions: ?good?,
?sharp?, ?slightly soft?. The modifiers of ?good?
are ?indoors? and ?Focus accuracy?, where relation
(?good?,?indoors?) is an opinion restriction because
?indoors? is the condition under which ?Focus ac-
curacy? is good. On the other hand, the relation
1333
(?sharp?, ?little 3x optical zooms?) is an opinion ex-
pansion because the ?sharp? opinion on ?shot? im-
plies a positive opinion on ?little 3x optical zooms?.
It is worth to remark that: 1) a modifier dk can re-
late to more than one opinion expression. For exam-
ple, multiple opinion expressions may share a same
condition; 2) dk itself can employ a set of relations,
although the case appears occasionally. The follow-
ing is an example:
Example 5: The camera wisely get rid of many
redundant buttons.
In the example, ?redundant buttons? is the eval-
uation target of opinion expression ?wisely get rid
of?, but itself is a relation between ?redundant?
and ?buttons?. Such nested semantic structure is
described by a path: ?wisely get rid of? target?????
[?redundant? target??????buttons?]nested target.
2.2 Relations between Individual Opinion
Representation
Assume ?ri? are opinion expressions ordered by
their positions in sentence, and each of them has
been represented by relations {(ri, dik)} individu-
ally (the nested relations for dik have also been de-
termined). Then we define two relations on adja-
cent pair ri, ri+1: coordination when the polarities
of ri and ri+1 are consistent, and transition when
they are opposite. Those relations among ri form a
set B called opinion thread. In Figure 1, the opin-
ion thread is: {(?good?, ?sharp?), (?sharp?, ?slightly
soft?)}.
The whole sentiment representation for a sentence
can be organized by a direct graphG = (V,E). Ver-
tex set V includes all opinion expressions and mod-
ifiers. Edge set E collects both relations of each
individual opinion and relations in opinion thread.
The edges are labeled with relation types in label set
L={?restriction?, ?expansion?, ?target?, ?coordina-
tion?, ?transition?} 3.
Compared with previous works, the advantages of
using G as sentiment representation are: 1) for in-
dividual opinions, the modifiers will collect more
information than using opinion expression alone.
3We don?t define any ?label? on vertices: if two span of text
satisfy a relation in L, they are chosen to be vertices and an
edge with proper label will appear inE. In other words, vertices
are identified by checking whether there exist relations among
them.
Focus accuracy was good indoors, and although the 
little 3x optical zooms produced sharp shots, the 
edges were slightly soft on the Canon. 
Focus 
accuracy 
edges 
slightly soft shots 
sharp 
little 3x optical 
zooms 
indoors 
good 
Expansion 
Target 
Coordinate 
Transition 
Target 
Target 
Restriction 
r
1 
r
2 
r
3 
d
11 
d
12 
d
21 
d
22 
d
31 
Figure 1: Sentiment representation for an example sen-
tence
Thus G is a relatively complete and accurate rep-
resentation; 2) the opinion thread can help to catch
global sentiment information, for example the gen-
eral polarity of a sentence, which is dropped when
the opinions are separately represented.
3 System Description
To produce the representation graph G for a sen-
tence, we need to extract candidate vertices and
build the relations among them to get a graph struc-
ture. For the first task, the experimental results in
Section 4 demonstrate that the standard sequential
labeling method with simple features can achieve
reasonable performance. In this section, we focus
on the second task, and assume the vertices in the
graph have already been correctly collected in the
following formulation of algorithm.
3.1 Preliminaries
In order to construct graph G, we use a structural
learning method. The framework is from the first or-
der discriminative dependency parsing model (Mc-
donald and Pereira, 2005). A sentence is denoted by
s; x are text spans which will be vertices of graph;
xi is the ith vertex in x ordered by their positions in
s. For a set of vertices x, y is the graph of its sen-
timent representation, and e = (xi, xj) ? y is the
direct edge from xi to xj in y. In addition, x0 is a
1334
virtual root node without inedge. G = {(xn,yn)}Nn
is training set.
Following the edge based factorization, the score
of a graph is the sum of its edges? scores,
score(x,y) =
?
(xi,xj)?y
score(xi, xj)
=
?
(xi,xj)?y
?T f(xi, xj), (1)
f(xi, xj) is a high dimensional feature vector of the
edge (xi, xj). The components of f are either 0 or 1.
For example the k-th component could be
fk(xi, xj) =
?
?
?
1 if xi.POS = JJ and xj .POS = NN
and label of (xi, xj)is restriction
0 otherwise
.
Then the score of an edge is the linear combination
of f ?s components, and the coefficients are in vector
?.
Algorithm 1 shows the parameter learning pro-
cess. It aims to get parameter ? which will assign
the correct graph y with the highest score among all
possible graphs of x (denoted by Y).
Algorithm 1 Online structural learning
Training Set:G = {(xn, yn)}Nn
1: ?0 = 0, r = 0, T =maximum iteration
2: for t = 0 to T do
3: for n = 0 to N do
4: y? = argmaxy?Y score(xn, y) B Inference
5: if y? 6= yn then
6: update ?t to ?t+1 B PA
7: r = r + ?t+1
8: end if
9: end for
10: end for
11: return ? = r/(N ? T )
3.2 Inference
Like other structural learning tasks, the ?argmax?
operation in the algorithm (also called inference)
y? = argmax
y?Y
score(x,y)
= argmax
y?Y
?
(xi,xj)?y
?T f(xi, xj) (2)
is hard because all possible values of y form a huge
search space. In our case, Y is all possible directed
acyclic graphs of the given vertex set, which num-
ber is exponential. Directly solving the problem of
finding maximum weighted acyclic graph is equiva-
lent to finding maximum feedback arc set, which is a
NP-hard problem (Karp, 1972). We will use integer
linear programming (ILP) as the framework for this
inference problem.
3.2.1 Graph Properties
We first show some properties of graph G either
from the definition of relations or corpus statistics.
Property 1. The graph is connected and without
directed cycle. From individual opinion represen-
tation, each subgraph of G which takes an opinion
expression as root is connected and acyclic. Thus
the connectedness is guaranteed for opinion expres-
sions are connected in opinion thread; the acyclic is
guaranteed by the fact that if a modifier is shared by
different opinion expressions, the inedges from them
always keep (directed) acyclic.
Property 2. Each vertex can have one outedge
labeled with coordination or transition at most. The
opinion thread B is a directed path in graph.
Property 3. The graph is sparse. The average
in-degree of a vertex is 1.03 in our corpus, thus the
graph is almost a rooted tree. In other words, the
cases that a modifier connects to more than one opin-
ion expression rarely occur comparing with those
vertices which have a single parent. An explaination
for this sparseness is that opinions in online reviews
always concentrate in local context and have local
semantic connections.
3.2.2 ILP Formulation
Based on the property 3, we divide the inference
algorithm into two steps: i) constructing G?s span-
ning tree (arborescence) with property 1 and 2; ii)
finding additional non-tree edges as a post process-
ing task. The first step is close to the works on ILP
formulations of dependency parsing (Riedel and
Clarke, 2006; Martins et al, 2009). In the second
step, we use a heuristic method which greedily adds
non-tree edges. A similar approximation method
is also used in (Mcdonald and Pereira, 2006) for
acyclic dependency graphs.
Step 1. Find MST. Following the multicommodity
1335
flow formulation of maximum spanning tree (MST)
problem in (Magnanti and Wolsey, 1994), the ILP
for MST is:
max.
?
i,j
yij ? score(xi, xj) (3)
s.t.
?
i,j
yij = |V | ? 1 (4)
?
i
fuij ?
?
k
fujk = ?uj ,1 ? u, j ? |V | (5)
?
k
fu0k = 1, 1 ? u ? |V | (6)
fuij ? yij , 1 ? u, j ? |V |,
0 ? i ? |V | (7)
fuij ? 0, 1 ? u, j ? |V |,
0 ? i ? |V | (8)
yij ? { 0, 1}, 0 ? i, j ? |V |. (9)
In this formulation, yij is an edge indicator vari-
able that (xi, xj) is a spanning tree edge when yij =
1, (xi, xj) is a non-tree edge when yij = 0. Then
output y is represented by the set {yij , 0 ? i, j ?
|V |} 4. Eq(4) ensures that there will be exactly
|V | ? 1 edges are chosen. Thus if the edges cor-
responding to those non zero yij is a connected sub-
graph, y is a well-formed spanning tree. Objective
function just says the optimal solution of yij have
the maximum weight.
The connectedness is guaranteed if for every ver-
tex, there is exactly one path from root to it. It is for-
mulated by using |V | ? 1 flows {fu, 1 ? u ? |V |}.
fu starts from virtual root x0 towards vertex xu.
Each flow fu = {fuij , 0 ? i, j ? |V |}. fuij indi-
cates whether flow fu is through edge (xi, xj). so
it should be 0 if edge (xi, xj) does not exist (by
(7)). The Kronecker?s delta ?uj in (5) guarantees fu
is only assumed by vertex xu, so fu is a well-formed
path from root to xu. (6) ensures there is only one
flow (path) from root to xu. Thus the subgraph is
connected. The following are our constraints:
c1: Constraint on edges in opinion thread (10)-
(11).
From the definition of opinion thread, we impose
a constraint on every vertex?s outedges in opinion
thread, which are labeled with ?coordination? or
4For simplicity, we overload symbol y from the graph of the
sentiment represetation to the MST of it.
?transition?. Let Iob be a characteristic function on
edges: Iob((j, k)) = 1 when edge (xj , xk) is labeled
with ?coordination? or ?transition?, otherwise 0. We
denote q variables for vertices:
qj =
?
k
yjk ? Iob((j, k)), 0 ? j ? |V |. (10)
Then following linear inequalities bound the number
of outedges in opinion thread (? 1) on each vertex:
qj ? 1, 0 ? j ? |V |. (11)
c2: Constraint on target edge (12).
We also bound the number of evaluation targets
for a vertex in a similar way. Let It be characteris-
tic function on edges identifing whether it is labeled
with ?target?,
?
k
yjk ? It((j, k)) ? Ct, 0 ? j ? |V |. (12)
The parameter Ct can be adjusted according to the
style of document. In online reviews, authors tend
to use simple and short comments on individual tar-
gets, so Ct could be set small.
c3: Constraint on opinion thread (13)-(18).
From graph property 2, the opinion thread should
be a directed path. It implies the number of con-
nected components whose edges are ?coordination?
or ?transition? should be less than 1. Two set of ad-
ditional variables are needed: {cj , 0 ? j ? |V |} and
{hj , 0 ? j ? |V |}, where
cj =
{
1 if an opinion thread starts at xj
0 otherwise ,
and
hj =
?
i
yij ? Iob((i, j)). (13)
Then cj = ?hj ? qj , which can be linearized by
cj? qj ? hj , (14)
cj? 1 ? hj , (15)
cj? qj , (16)
cj? 0. (17)
If the sum of cj is no more than 1, the opinion thread
of graph is a directed path.
?
j
cj ? 1. (18)
1336
1 
2 
3 
4 
5 
6 
7 
(a) 
(b) 
1 
2 
3 
4 
5 
6 7 
(c) 
1 
2 3 
4 
5 
6 
7 
Figure 2: The effects of c1 and c3. Assume solid lines
are edges labeled with ?coordination? and ?transition?,
dot lines are edges labeled with other types. (a) is an
arbitrary tree. (b) is a tree with c1 constraints. (c) is a
tree with c1 and c3. It shows c1 are not sufficient for
graph property 2: the edges in opinion thread may not be
connected.
Figure 2 illustrates the effects of c1 and c3.
Equations (10)-(18), together with basic multi-
commodity flow model build up the inference algo-
rithm. The entire ILP formulation involves O(|V |3)
variables and O(|V |2) constraints. Generally, ILP
falls into NPC, but as an important result, in the mul-
ticommodity flow formulation of maximum span-
ning tree problem, the integer constraints (9) on yij
can be dropped. So the problem reduces to a linear
programming which is polynomial solvable (Mag-
nanti and Wolsey, 1994). Unfortunately, with our
additional constraints the LP relaxation is not valid.
Step 2. Adding non-tree edges. We examine the
case that a modifier attaches to different opinion ex-
pressions. That often occurs as the result of the
sharing of modifiers among adjacent opinion expres-
sions. We add those edges in the following heuristic
way: If a vertex ri in opinion thread does not have
any modifier, we search the modifiers of its adjacent
vertices ri+1, ri?1 in the opinion thread, and add
edge (ri, d?) where
d? = argmax
d?S
score(ri, d),
and S are the modifiers of ri?1 and ri+1.
3.3 Training
We use online passive aggressive algorithm (PA)
with Hamming cost of two graphs in training (Cram-
mer et al, 2006).
Unigram Feature Template
xi.text w0.text w1.text
w0.POS w1.POS
wk?1.text wk.text
Inside wk?1.POS wk.POS
Features xi.hasDigital
xi.isSingleWord
xi.hasSentimentWord
xi.hasParallelPhrase
w?1.text w?2.text
w?1.POS w?2.POS
wk+1.text wk+2.text
Outside wk+1.POS wk+2.POS
Features c?1.text c?2.text
c?1.POS c?2.POS
cl+1.text cl+2.text
cl+1.POS cl+2.POS
Other Features
distance between parent and child
dependency parsing relations
Table 1: Feature set
3.4 Feature Construction
For each vertex xi in graph, we use 2 sets of fea-
tures: inside features which are extracted inside the
text span of xi; outside features which are outside
the text span of xi. A vertex xi is described both in
word sequence (w0, w1, ? ? ? , wk) and character se-
quence (c0, c1, ? ? ? , cl), for the sentences are in Chi-
nese.
? ? ? , w?1, w0, w1, w2, ? ? ? , wk?1, wk? ?? ?
xi
, wk+1 ? ? ?
? ? ? , c?1, c0, c1, c2, ? ? ? , cl?1, cl? ?? ?
xi
, cl+1 ? ? ?
For an edge (xi, xj), the high dimensional feature
vector f(xi, xj) is generated by using unigram fea-
tures in Table 1 on xi and xj respectively. The dis-
tance between parent and child in sentence is also
attached in features. In order to involve syntactic
information, whether there is certain type of depen-
dency relation between xi and xj is also used as a
feature.
1337
4 Experiments
4.1 Corpus
We constructed a Chinese online review corpus from
Pcpop.com, Zol.com.cn, and It168.com, which have
a large number of reviews about digital camera. The
corpus contains 138 documents and 1735 sentences.
Since some sentences do not contain any opinion,
1390 subjective sentences were finally chosen and
manually labeled.
Two annotators labeled the corpus independently.
The annotators started from locating opinion expres-
sions, and for each of them, they annotated other
modifiers related to it. In order to keep the relia-
bility of annotations, another annotator was asked
to check the corpus and determine the conflicts. Fi-
nally, we extracted 6103 elements, which are con-
nected by 6284 relations.
Relation Number
Target 2479
Coordinate 1173
Transition 154
Restriction 693
Expansion 386
Table 2: Statistics of relation types
Table 2 shows the number of various relation
types appearing in the labeled corpus. We observe
60.5% of sentences and 32.1% of opinion expres-
sions contain other modifiers besides ?target?. Thus
only mining the relations between opinion expres-
sions and evaluation target is actually at risk of inac-
curate and incomplete results.
4.2 Experiments Configurations
In all the experiments below, we take 90% of the cor-
pus as training set, 10% as test set and run 10 folder
cross validation. In feature construction, we use
an external Chinese sentiment lexicon which con-
tains 4566 positive opinion words and 4370 nega-
tive opinion words. For Chinese word segment, we
use ctbparser 5. Stanford parser (Klein and Man-
ning, 2003) is used for dependency parsing. In the
settings of PA, the maximum iteration number is
5http://code.google.com/p/ctbparser/
set to 2, which is chosen by maximizing the test-
ing performances, aggressiveness parameter C is set
to 0.00001. For parameters in inference algorithm,
Ct = 2, the solver of ILP is lpsolve6.
We evaluate the system from the following as-
pects: 1) whether the structural information helps
to mining opinion relations. 2) How the proposed
inference algorithm performs with different con-
straints. 3) How the various features affect the sys-
tem. Except for the last one, the feature set used for
different experiments are the same (?In+Out+Dep?
in Table 5). The criteria for evaluation are simi-
lar to the unlabeled attachment score in parser eval-
uations, but due to the equation |E| = |V | ? 1
is not valid if G is not a tree, we evaluate pre-
cision P = #true edges in result graph#edges in result graph , recall
R = #true edges in result graph#edges in true graph , and F-score
F = 2P ?RP+R .
4.3 Results
1. The effects of structural information. An alter-
native method to extract relations is directly using
a classifier to judge whether there is a relation be-
tween any two elements. Those kinds of methods
were used in previous opinion mining works (Wu
et al, 2009; Kobayashi et al, 2007). To show the
entire structural information is important for min-
ing relations, we use SVM for binary classification
on candidate pairs. The data point representing a
pair (xi, xj) is the same as the high dimensional fea-
ture vectors f(xi, xj). The setting of our algorithm
?MST+c1+c2+c3? is the basic MSTwith all the con-
straints. The results are shown in the Table 3.
P R F
SVM 64.9 24.0 35.0
MST+c1+c2+c3-m 61.5 74.0 67.2
MST+c1+c2+c3 73.1 71.0 72.1
Table 3: Binary classifier and structural learning
From the results, the performance of SVM (espe-
cially recall) is relatively poor. A possible reason
is that the huge imbalance of positive and negative
training samples (only ?(n) positive pairs among
all n2 pairs). And the absence of global structural
6http://sourceforge.net/projects/lpsolve/
1338
knowledge makes binary classifier unable to use
the information provided by classification results of
other pairs.
In order to examine whether the complicated sen-
timent representation would disturb the classifier in
finding relations between opinion expressions and
its target, we evaluate the system by discarding the
modifiers of opinion restriction and expansion from
the corpus. The result is shown in the second row of
Table 3. We observe that ?MST+c1+c2+c3? is still
better which means at least on overall performance
the additional modifiers do not harm.
2. The effect of constraints on inference algo-
rithm. In the inference algorithm, we utilized the
properties of graph G and adapted the basic multi-
commodity flow ILP to our specific task. To evaluate
how the constraints affect the system, we decompose
the algorithm and combine them in different ways.
P R F
MST 69.3 67.3 68.3
MST+c1 70.0 68.0 69.0
MST+c2 69.8 67.8 68.8
MST+c1+c2 70.6 68.6 69.6
MST+c1+c3 72.4 70.4 71.4
MST+c1+c2+c3 73.1 71.0 72.1
MST+c1+c2+c3+g 72.5 72.3 72.4
Table 4: Results on inference methods. ?MST? is the ba-
sic multicommodity flow formulation of maximum span-
ning tree; c1, c2, c3 are groups of constraint from Section
3.2.2; ?g? is our heuristic method for additional non span-
ning tree edges.
From Table 4, we observe that with any additional
constraints the inference algorithm outperforms the
basic maximum spanning tree method. It implies al-
though we did not use high order model (e.g. involv-
ing grandparent and sibling features), prior struc-
tural constraints can also help to get a better out-
put graph. By comparing with different constraint
combinations, the constraints on opinion thread (c1,
c3) are more effective than constraints on evaluation
targets (c2). It is because opinion expressions are
more important in the entire sentiment representa-
tion. The main structure of a graph is clear once the
relations between opinion expressions are correctly
determined.
3. The effects of various features. We evaluate the
performances of different feature configurations in
Table 5. From the results, the outside feature set is
more effective than inside feature set, even if it does
not use any external resource. A possible reason is
that the content of a vertex can be very complicated
(a vertex even can be a clause), but the features sur-
rounding the vertex are relatively simple and easy
to identify (for example, a single preposition can
identify a complex condition). The dependency fea-
ture has limited effect, due to that lots of online re-
view sentences are ungrammatical and parsing re-
sults are unreliable. And the complexity of vertices
also messes the dependency feature.
P R F
In-s 66.3 66.3 66.3
In 66.7 66.4 66.6
Out 67.8 67.4 67.6
In+Out 72.0 70.5 71.0
In+Out+Dep 72.5 72.3 72.4
Table 5: Results with different features. ?In? repre-
sents the result of inside feature set; ?In-s? is ?In? with-
out the external opinion lexicon feature; ?Out? uses the
outside feature set; ?In+Out? uses both ?In? and ?Out?,
?In+Out+Dep? adds the dependency feature. The infer-
ence algorithm is ?MST+c1+c2+c3+g? in Table 4.
We analyze the errors in test results. A main
source of errors is the confusion of classifier be-
tween ?target? relations and ?coordination?, ?tran-
sition? relations. The reason may be that for a mod-
ification on opinion expression (r, dk), we allow
dk recursively has its own modifiers (Example 5).
Thus an opinion expression can be a modifier which
brings difficulties to classifier.
4. Extraction of vertices. Finally we conduct an
experiment on vertex extraction using standard se-
quential labeling method. The tag set is simply {B,
I, O} which are signs of begin, inside, outside of a
vertex. The underlying model is conditional random
field 7. Feature templates involved are in Table 6.
We only use basic features in the experiment. 10
folder cross validation results are in table 7. We sus-
pect that the performances (especially recall) could
be improved if some external resources(i.e. ontol-
ogy, domain related lexicon, etc.) are involved.
7We use CRF++ toolkit, http://crfpp.sourceforge.net/
1339
Unigram Template
ci.char character
ci.isDigit digit
ci.isAlpha english letter
ci.isPunc punctuation
ci.inDict in a sentiment word
ci.BWord start of a word
ci.EWord end of a word
Table 6: Features for vertex extraction. The sequential
labeling is conducted on character level (ci). The senti-
ment lexicon used in ci.inDict is the same as Table1. We
also use bigram feature templates on ci.char, ci.isAlpha,
ci.inDict with respect to ci?1 and ci+1.
P R F
E+Unigram 56.8 45.1 50.3
E+Unigram+Bigram 57.3 47.9 52.1
O+Unigram 71.9 57.2 63.7
O+Unigram+Bigram 72.3 60.2 65.6
Table 7: Results on vertices extraction with 10 folder
cross validation. We use two criterion: 1) the vertex is
correct if it is exactly same as ground truth(?E?), 2) the
vertex is correct if it overlaps with ground truth(?O?).
5 Related Work
Opinion mining has recently received considerable
attentions. Large amount of work has been done on
sentimental classification in different levels and sen-
timent related information extraction. Researches on
different types of sentences such as comparative sen-
tences (Jindal and Liu, 2006) and conditional sen-
tences (Narayanan et al, 2009) have also been pro-
posed.
Kobayashi et al (2007) presented their work on
extracting opinion units including: opinion holder,
subject, aspect and evaluation. They used slots
to represent evaluations, converted the task to two
kinds of relation extraction tasks and proposed a ma-
chine learning-based method which used both con-
textual and statistical clues.
Jindal and Liu (2006) studied the problem of iden-
tifying comparative sentences. They analyzed dif-
ferent types of comparative sentences and proposed
learning approaches to identify them.
Sentiment analysis of conditional sentences were
studied by Narayanan et al (2009). They aimed
to determine whether opinions expressed on dif-
ferent topics in a conditional sentence are posi-
tive, negative or neutral. They analyzed the con-
ditional sentences in both linguistic and computi-
tional perspectives and used learning method to do
it. They followed the feature-based sentiment anal-
ysis model (Hu and Liu, 2004), which also use flat
frames to represent evaluations.
Integer linear programming was used in many
NLP tasks (Denis and Baldridge, 2007), for its
power in both expressing and approximating various
inference problems, especially in parsing (Riedel
and Clarke, 2006; Martins et al, 2009). Martins
etc. (2009) also applied ILP with flow formulation
for maximum spanning tree, besides, they also han-
dled dependency parse trees involving high order
features(sibling, grandparent), and with projective
constraint.
6 Conclusions
This paper introduces a representation method for
opinions in online reviews. Inspections on corpus
show that the information ignored in previous sen-
timent representation can cause incorrect or incom-
plete mining results. We consider opinion restric-
tion, opinion expansions, relations between opin-
ion expressions, and represent them with a directed
graph. Structural learning method is used to produce
the graph for a sentence. An inference algorithm is
proposed based on the properties of the graph. Ex-
perimental evaluations with a manually labeled cor-
pus are given to show the importance of structural
information and effectiveness of proposed inference
algorithm.
7 Acknowledgement
The author wishes to thank the anonymous review-
ers for their helpful comments. This work was
partially funded by 973 Program (2010CB327906),
National Natural Science Foundation of China
(61003092, 61073069),863 Program of China
(2009AA01A346), Shanghai Science and Tech-
nology Development Funds(10dz1500104), Doc-
toral Fund of Ministry of Education of China
(200802460066), Shanghai Leading Academic Dis-
cipline Project (B114), and Key Projects in
the National Science & Technology Pillar Pro-
1340
gram(2009BAH40B04).
References
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Sajib Dasgupta and Vincent Ng. 2009. Mine the easy,
classify the hard: A semi-supervised approach to auto-
matic sentiment classification. In Proceedings of ACL-
IJCNLP.
Kushal Dave, Steve Lawrence, and David M. Pennock.
2003. Mining the peanut gallery: opinion extraction
and semantic classification of product reviews. In Pro-
ceedings of WWW.
Pascal Denis and Jason Baldridge. 2007. Joint determi-
nation of anaphoricity and coreference resolution us-
ing integer programming. In Proceedings of NAACL-
HLT.
Ahmed Hassan and Dragomir R. Radev. 2010. Identify-
ing text polarity using random walks. In Proceedings
of ACL, pages 395?403, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Proceedings of SIGKDD.
Nitin Jindal and Bing Liu. 2006. Identifying comparative
sentences in text documents. In Proceedings of SIGIR.
R. Karp. 1972. Reducibility among combinatorial prob-
lems. In R. Miller and J. Thatcher, editors, Complex-
ity of Computer Computations, pages 85?103. Plenum
Press.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of the COLING-ACL.
Dan Klein and Christopher D.Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In In Advances in Neural Information Pro-
cessing Systems 15 (NIPS, pages 3?10. MIT Press.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of rela-
tions in opinion mining. In Proceedings of EMNLP-
CoNLL.
Thomas L. Magnanti and Laurence A. Wolsey. 1994.
Optimal trees.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of ACL-IJCNLP.
R. Mcdonald and F. Pereira. 2005. Identifying gene
and protein mentions in text using conditional random
fields. BMC Bioinformatics.
Ryan Mcdonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing al-
gorithms. In Proc. of EACL, pages 81?88.
Ramanathan Narayanan, Bing Liu, and Alok Choudhary.
2009. Sentiment analysis of conditional sentences. In
Proceedings of EMNLP.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using ma-
chine learning techniques. In Proc. of EMNLP 2002.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of EMNLP.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the seventh confer-
ence on Natural language learning at HLT-NAACL.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of COLING.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of ACL.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion mining.
In Proceedings of EMNLP.
1341
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 946?957,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Discourse Level Explanatory Relation Extraction from Product Reviews
Using First-order Logic
Qi Zhang, Jin Qian, Huan Chen, Jihua Kang, Xuanjing Huang
School of Computer Science
Fudan University
Shanghai, P.R. China
{qz, 12110240030, 12210240054, 12210240059, xjhuang}@fudan.edu.cn
Abstract
Explanatory sentences are employed to clarify
reasons, details, facts, and so on. High quality
online product reviews usually include not
only positive or negative opinions, but also a
variety of explanations of why these opinions
were given. These explanations can help
readers get easily comprehensible informa-
tion of the discussed products and aspect-
s. Moreover, explanatory relations can also
benefit sentiment analysis applications. In
this work, we focus on the task of identi-
fying subjective text segments and extracting
their corresponding explanations from prod-
uct reviews in discourse level. We propose
a novel joint extraction method using first-
order logic to model rich linguistic features
and long distance constraints. Experimental
results demonstrate the effectiveness of the
proposed method.
1 Introduction
Through analyzing product reviews with high help-
fulness ratings assigned by readers, we find that a
large number of explanatory sentences are used to
clarify the causes, details, or consequences of opin-
ions. According to the statistic based on the dataset
we crawled from a popular product review website,
more than 56.1% opinion expressions are further
explained by other sentences. Since most consumers
are not experts, these explanations would bring lots
of helpful and easy comprehension information for
them. Suggestions about writing a product review
also advise authors to include not only whether they
like or dislike a product, but also why.1
1http://www.reviewpips.com/
http://www.amazon.com/gp/community-help/customer-
For example, let us consider the following snip-
pets extracted from online reviews:
Example 1: TVs with lower refresh rates may
suffer from motion blur. If you?re watching
a fast-paced football game, for example, you
may notice a bit of blurring as the players run
around the field.
Example 2: The LED screen is highly reflec-
tive. The reflection of my own face makes it very
hard to see the subject I am trying to shoot.
The first sentence of example 1 expresses negative
opinion about refresh rate, which is one of the most
important attributes of TV. The second sentence
describes the consequence of it through an example.
In example 2, detail descriptions are used to explain
the reflection problem of the camera screen.
Although, explanations provide valuable infor-
mation, to the best of our knowledge, there is no
existing work that deals with explanation extraction
for opinions in discourse level. We think that if
explanatory relations can be automatically identified
from reviews, sentiment analysis applications may
benefit from it. Existing opinion mining approaches
mainly focus on subjective text. They try to de-
termine the subjectivity and polarity of fragments
of documents (e.g. a paragraph, a sentence, a
phrase and a word) (Pang et al, 2002; Riloff et
al., 2003; Takamura et al, 2005; Mihalcea et al,
2007; Dasgupta and Ng, ; Hassan and Radev, 2010;
Meng et al, 2012; Dragut et al, 2012). Fine-grained
methods were also introduced to extract opinion
holder, opinion expression, opinion target, and other
opinion elements (Kobayashi et al, 2007; Wu et al,
reviews-guidelines
946
2011; Xu et al, 2013; Yang and Cardie, 2013). Ma-
jor research directions and challenges of sentiment
analysis can also be found in surveys (Pang and Lee,
2008; Liu, 2012).
In this work, we aim to identify subjective tex-
t segments and extract their corresponding expla-
nations from product reviews in discourse level.
We propose to use Markov Logic Networks (ML-
N) (Richardson and Domingos, 2006) to learn the
joint model for subjective classification and explana-
tory relation extraction. MLN has been applied in
several natural language processing tasks (Singla
and Domingos, 2006; Poon and Domingos, 2008;
Yoshikawa et al, 2009; Andrzejewski et al, 2011;
Song et al, 2012) and demonstrated its advantages.
It can easily incorporate rich linguistic features and
global constraints by designing various logic for-
mulas, which can also be viewed as templates or
rules. Logic formulas are combined in a proba-
bilistic framework to model soft constraints. Hence,
the proposed approach can benefit a lot from this
framework.
To evaluate the proposed method, we crawled a
large number of product reviews and constructed a
labeled corpus through Amazon?s Mechanical Turk.
Two tasks were deployed for labeling the corpus.
We compared the proposed method with state-of-
the-art methods on the dataset. Experimental results
demonstrate that the proposed approach can achieve
better performance than state-of-the-art methods.
The remaining part of this paper is organized as
follows: In Section 2, we define the problem and
give some examples to show the challenges of this
task. Section 3 describes the proposed MLN based
method. Dataset construction, experimental results
and analyses are given in Section 4. In Section 5, we
present the related work and Section 6 concludes the
paper.
2 Problem Statement
Motivated by the argument structure of discourse
relations used in Penn Discourse Treebank (Rash-
mi Prasad and Webber, 2008), in this work, we
adopt the clause unit-based definition. It means that
clauses are treated as the basic units of opinion ex-
pressions and explanations. Let d = {c1, c2, ...cn}
be the clauses of document d. Directed graph
G = (V,E) is used to represent the subjectivity
of clauses and explanatory relationships between
them. In the graph, vertices represent clauses,
whose categories are specified by the vertex at-
tributes. Directed edges describe the explanatory
relationships between them, of which the heads are
explanatory clauses. If clause ca describes a set of
facts which clarify the causes, context, situation, or
consequences of another clause cb, ca ?? cb is used
to indicate that clause ca explains cb.
Adopting clause unit-based definition is based
on the following reasons: 1) clause is normally
considered as the smallest grammatical unit which
can express a complete proposition (Kroeger, 2005);
2) from analyzing online reviews, we observe that
a clause can express a complete opinion about one
aspect in most of cases; 3) in Penn Discourse
Treebank, the basic unit of discourse relations (with
a few exceptions) is also taken to be a clause (Rash-
mi Prasad and Webber, 2008).
Figure 1(a) illustrates a sample document. Figure
1(b) is the corresponding output of the given docu-
ment. In the graph, vertices whose color are black
stand for subjective clauses. The other clauses are
represented by white vertices. Edges describe the
explanatory relationships between them, of which
the heads are explanatory clauses.
Although the explanatory relation extraction task
has been studied from the view of linguistic and
discourse representation by existing works (Carston,
1993; Lascarides and Asher, 1993), the automatic
extraction task is still an open question. Consider the
following examples extracting from online reviews:
Example 3: It takes great pictures. Color ren-
ditions, skin tones, exposure levels are all first rate.
From the example, we can observe that the second
sentence explains the first one. However, the second
sentence itself also expresses opinion on various
opinion targets. In other words, both subjective and
objective sentences can be used as explanations.
Example 4: When we called their service center
they made us wait for them the whole day and no
one turned up. This level of service is simply not
acceptable. The first sentence in example 4 explains
the second one. Hence, the feature of relative
location between two sentences does not always
work well in all cases.
Example 5: This backpack is great! its very big
947
(c1) I have both the Panasonic LX3 and the Canon
S90. (c2) Both cameras are quite different but truly
excellent. (c3) The S90 is a true pocket camera.
(c4) It is very compact. (c5) The build quality is
also top notch. (c6) It feels solid and it is easy to
grip. (c7) It is so small and convenient, (c8) you
will find that you will always carry it with you.
C
5
C
3
C
2
C
4
C
6
C
7
C
1
(a) Example Review (b) Directed Graph Representation
C
8
Figure 1: Directed graph representation of a sample document.
and fits more than enough stuff. Many sentences,
which express explanatory relation, do not contain
any connectives (e.g. ?because?, ?the reason is?,
and so on). Lin et al(2009) generalized four chal-
lenges (include ambiguity, inference, context, and
world knowledge) to automated implicit discourse
relation recognition. In this task, we also need to
address those challenges.
From the these examples, we can observe that ex-
tracting explanatory relations from product reviews
is a challenging task. Both linguistic and global
constraints should be carefully studied.
3 The Proposed Approach
In this section, we present our method for jointly
classifying the subjectivity of text segments and
extracting explanatory relations. Firstly, we briefly
describe the framework of Markov Logic Networks.
Then, we introduce the clause extraction method
based on the definition described in the Section 2.
Finally, we present the first-order logic formulas
including local formulas and global formulas used
for joint modeling in this work.
3.1 Markov Logic Networks
A MLN consists of a set of logic formulas that
describe first-order knowledge base. Each formula
consists of a set of first-order predicates, logical
connectors and variables. Different with first-order
logic, these hard logic formulas are softened and
can be violated with some penalty (the weight of
formula) in MLN.
We use M to represent a MLN and {(?i, wi)}
to represent formula ?i and its weight wi. These
weighted formulas define a probability distribution
over sets of possible worlds. Let y denote a possible
world, the p(y) is defined as follows (Richardson
and Domingos, 2006):
p(y) = 1
Z
exp
?
?
?
(?i,wi)?M
wi
?
c?Cn?i
f?ic (y)
?
? ,
where each c is a binding of free variable in ?i to
constraints; f?ic (y) is a binary feature function that
returns 1 if the true value is obtained in the ground
formula we get by replacing the free variables in
?i with the constants in c under the given possible
world y, and 0 otherwise; Cn?i is all possible
bindings of variables to constants, and Z is a nor-
malization constant.
Many methods have been proposed to learn the
weights of MLN using both generative and dis-
criminative approaches (Richardson and Domingos,
2006; Singla and Domingos, 2006). There are
also several MLN learning packages available online
such as thebeast2, Tuffy3, PyMLNs4, Alchemy5, and
so on.
2http://code.google.com/p/thebeast
3http://hazy.cs.wisc.edu/hazy/tuffy/
4http://www9-old.in.tum.de/people/jain/mlns/
5http://alchemy.cs.washington.edu/
948
Describing the attributes of words
subjLexicon(w) The word w belongs to the subjective lexicon (Baccianella et al,
2010).
relationLexicon(w) The word w belongs to the lexicon of explanation relation
connectives (Pitler and Nenkova, 2009).
Describing the attributes of the clause ci
word(i, w) The clause ci has word w.
firstWord(i, w) The first word of clause ci is word w.
pos(i, w, t) The POS tag of word w is t in clause ci.
dep(i, h,m) Word m and h are governor and dependent of a dependency
relation in clause ci.
Describing the attributes of relations between clause ci and clause cj
clauseDistance(i, j,m) Distance between clause ci and clause cj in clauses is m.
sentenceDistance(i, j, n) Distance between clause ci and clause cj in sentences is n.
Table 1: Descriptions of observed predicates.
3.2 Clause Identification
We model the clause boundary identification prob-
lem through sequence labeling and use Conditional
Random Fields (CRFs) to identify clause bound-
aries. Words and part-of-speech (POS) tags are used
as feature sets. Since we do not allow embedded
segments, the performance of our method is promis-
ing, which achieves the F1 score of 92.8%. The
result is comparable with the best results obtained
during the CoNLL-2001 campaign (Tjong et al,
2001).
3.3 Formulas
In this work, we propose to use predicate subj(i)
to indicate that the ith clause is subjective and
explain(i, j) to indicate that the jth clause explains
the ith clause. Both subj and explain are hidden
predicates and jointly modeled by MLN. We use
local and global formulas to model rich linguistic
features and long distance constraints.
3.3.1 Local Formulas
The local formulas relate one or more observed
predicates to exactly one hidden predicate. In this
work, we define a list of observed predicates to
describe the properties of individual clauses and
attributes of relations between two clauses. The
observed predicates and descriptions are shown in
Table 1. The observed predicates can be categorized
into 3 groups: words, clauses, and relations between
clauses. We use two lexicons to capture background
knowledge of words. Lexical, part-of-speech tag,
and dependency relation are used to describe a single
clause. We also propose two predicates to model
distance between clauses.
Table 2 lists the local formulas used in this work.
The ?+? notation in the formulas indicates that each
constant of the logic variable should be weighted
separately. For subjective classification and relation
extraction, we construct a number of formulas re-
spectively.
For subjective classification, the first two formu-
las model the influence of lexical and POS tag. It
is similar as the bag-of-words model, which is a
simplifying representation and has been successfully
used for various natural language processing tasks.
Since words which provide positive or negative
opinions may provide important information for
subjectivity classification, we combine predicates of
words and lexicon of opinion words. Bigrams are
also proved to be useful for textual classification in
several NLP tasks. Hence, we also combine predi-
cates about individual word and POS tag to capture
this kind of information. Word-level relations are
explicitly presented at the dependency trees, we
949
Formulas for subjective classification
word(i,w+)? subj(i)
pos(i,w+,t+)? subj(i)
word(i,w+) ? subjLexicon(w)? subj(i)
pos(i,w+,t+) ? subjLexicon(w)? subj(i)
word(i,w1+) ? word(i,w2+)? subj(i)
pos(i,w1+,t+) ? pos(i,w2+,t+)? subj(i)
word(i,w1+) ? word(i,w2+) ? subjLexicon(w1)? subj(i)
word(i,w1+) ? word(i,w2+) ? subjLexicon(w2)? subj(i)
dep(i,w1+,w2+)? subj(i)
dep(i,w1+,w2+) ? subjLexicon(w1)? subj(i)
dep(i,w1+,w2+) ? subjLexicon(w2)? subj(i)
Formulas for explanatory relation extraction
word(i,w1+) ? word(j,w2+) ? j ?=i? explain(i,j)
pos(i,w1+,t+) ? pos(j,w2+,t+) ? j?=i? explain(i,j)
dep(i,h1+,m1+) ? dep(j,h2+,m2+)? j?=i? explain(i,j)
word(i,w1+) ? word(j,w2+) ? clauseDistance(i,j,m+) ? j?=i? explain(i,j)
pos(i,w1+,t+) ? pos(j,w2+,t+) ? clauseDistance(i,j,m+) ? j?=i? explain(i,j)
dep(i,h1+,m1+) ? dep(j,h2+,m2+) ? clauseDistance(i,j,m+) ? j?=i? explain(i,j)
word(i,w1+) ? word(j,w2+) ? sentenceDistance(i,j,n+) ? j ?=i? explain(i,j)
pos(i,w1+,t+) ? pos(j,w2+,t+) ? sentenceDistance(i,j,n+) ? j ?=i? explain(i,j)
dep(i,h1+,m1+) ? dep(j,h2+,m2+) ? sentenceDistance(i,j,n+) ? j?=i? explain(i,j)
word(i,w1+) ? word(j,w2+) ? firstWord(j,w+) ? j?=i? explain(i,j)
pos(i,w1+,t+) ? pos(j,w2+,t+) ? firstWord(j,w+) ? j?=i? explain(i,j)
dep(i,h1+,m1+) ? dep(j,h2+,m2+) ? firstWord(j,w+) ? j?=i? explain(i,j)
word(i,w1+) ? word(j,w2+) ? subjLexicon(w1) ? j?=i? explain(i,j)
pos(i,w1+,t+) ? pos(j,w2+,t+) ? subjLexicon(w1) ? j?=i? explain(i,j)
dep(i,h1+,m1+) ? dep(j,h2+,m2+) ? subjLexicon(m1) ? j?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? clauseDistance(i,j,m+) ? j?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? sentenceDistance(i,j,n+) ? j ?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? pos(j,w,t+) ? clauseDistance(i,j,m+) ? j ?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? pos(j,w,t+) ? sentenceDistance(i,j,n+) ? j?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? word(i,w1+) ? clauseDistance(i,j,m+) ? j ?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? word(i,w1+) ? sentenceDistance(i,j,n+) ? j?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? word(j,w1+) ? clauseDistance(i,j,m+) ? j ?=i? explain(i,j)
firstWord(j,w+) ? relationLexicon(w) ? word(j,w1+) ? sentenceDistance(i,j,n+) ? j?=i? explain(i,j)
Table 2: Descriptions of local formulas.
950
also construct local formulas based on predicates
extracted from dependency trees of clauses.
For explanatory relation extraction, we firstly use
formulas to capture lexical and syntactic information
from both of the clauses. Since distances between
clauses are helpful in determining the relation, we
incorporate two kinds of distance features with lex-
ical and syntactic predicates. Connective words
such as for example, since, explicitly signal the
presence of the explanation relation. Although some
connective words are ambiguous in terms of relation
they mark (Pitler and Nenkova, 2009), they may still
be useful for explanation relation extraction. Hence,
we construct local formulas with relation lexicon
and other predicates.
3.3.2 Global Formulas
Local formulas are designed to deal with sub-
jective classification of a single clause or relation
determination of a single pair of clauses. Global
formulas are designed to handle global constraints
of multiple clauses. From the definition of explana-
tory relation and corpus statistics, we observe the
following properties:
Property 1: One clause can only serve as the
explanation of one subjective clause.
Property 2: Explanatory clauses occur immedi-
ately before or after their corresponding subjective
clauses.
Property 3: The positions of explanatory clauses
are consecutive. In other words, if clause ck and
ck+2 explain clause cj , the clause ck+1 would also
be explanatory clause of cj .
For property 1, we use the following global for-
mula to make sure that one clause only explains at
most one another clause.
explain(i, j) ? ?explain(k, j) ?k ?= i, j (1)
Based on the property 2 and 3, explanatory claus-
es are consecutive and immediately before or after
their corresponding subjective clauses. We use the
following formulas to guarantee the property:
explain(i, i+ k) ? explain(i, i+m),
1 ? m ? k ? 1 (2)
explain(i, i? k) ? explain(i, i?m),
1 ? m ? k ? 1 (3)
Since our aim is to extract explanatory for subjec-
tive clauses, we also use the following formulas to
make sure that the clauses which are explained are
subjective ones.
explain(i, j) ? subj(i) (4)
4 Experiments
4.1 Data Set
We crawled a number of reviews about digital cam-
eras from Buzzillions6, which is a product review
site and contains more than 16 million reviews.
We randomly select 100 reviews whose usefulness
ratings are 5 on a 5-point scale. They contain 1137
sentences, which are composed by 1665 clauses.
Amazon?s Mechanical Turk is used to deploy two
tasks for labeling the corpus. 694 clauses are labeled
subjective and 478 clauses explain other ones. More
than 56.1% opinion expressions are explained by
their corresponding explanatory sentences.
The two projects we deployed on Amazon?s Me-
chanical Turk are: 1) Determine whether a clause
contains opinion expressions or not; 2) Determine
whether a clause clarifies causes, reasons, or conse-
quences of another given clause. In order to control
the labeling quality, we configured parameters of
the project to make sure that all the tasks should
be judged by at least 20 annotators. Most of the
annotators can complete a task within 25 seconds.
Figure 2 shows the screenshots of the two projects.
Over all, 127 workers participated in the project.
About 72% of them submitted more than 5 tasks.
Although we listed several examples on the project
descriptions, different people may have their own
understanding and criteria for those tasks. In order
to measure the quality of the labeling task, we use
perplexity to evaluate each task. If the perplexity
of a task is below 0.51, which means that more than
80% of the workers submitted the same decision, the
result of the task will be used as training or testing
data. From the statistic of the corpus, we observe
that only 6.2% of the clauses? subjectiveness and
15.6% of explanation relations can not be certainly
decided. For the first project, we treated those claus-
es as objective one. And, those clause pairs in the
second project were not considered as explanation
relations.
6www.buzzillions.com
951
Task2: Help us check whether a sentence is an 
explanation of the opinion sentence. 
The opinion sentence (red one) is extracted from product reviews and 
express opinion towards some attributes/parts of a product. Please 
help us check whether the following blue sentences describe a set of 
facts which clarifies the causes, reason, and consequences of the 
opinion given in the opinion sentence. 
 
click "yes" if there is an explanation relation between them, "no" 
otherwise. 
 
     The battery life is something I come to expect from this line of camera. 
     I can leave the camera on for better than 8 hours shooting  
     ()YES 
     ()NO 
 
     The battery life is something I come to expect from this line of camera. 
     and I have the camera set to shut off the sensor after about 30 seconds   
     ()YES 
     ()NO 
Task1: Help us determine whether a sentence is 
subjective or objective. 
The following sentences are extracted from product reviews. Please 
help us check whether the following sentences  expressing opinion 
towards some attributes/parts of a product. 
 
     The battery life is something I come to expect from this line of camera. 
     ()Subjective 
     ()Objective 
 
     I have the camera set to shut off the sensor after about 30 seconds   
     ()Subjective 
     ()Objective 
Figure 2: Screenshots of the two tasks on Amazon
Mechanical Turk.
4.2 Experiments Configurations
Stanford parser (Klein and Manning, 2003) is used
for extracting features from dependency parse trees.
For resolving Markov logic network, we use the
toolkit thebeast 7. The detailed setting of thebeast
engine is as follows: The inference algorithm is
the MAP inference with a cutting plane approach.
For parameter learning, the weights for formulas are
updated by an online learning algorithm with MIRA
update rule. All the initial weights are set to zeros.
The number of iterations is set to 10 epochs.
Evaluation metrics used for subjectivity classifi-
cation and relation extraction throughout the experi-
ments include: Precision, Recall, and F1-score. We
randomly select 80% reviews as training set and the
others as testing set.
Since the dataset is newly created for this task, to
compare the performance of the proposed method to
other models, we also reimplemented several state-
7http://code.google.com/p/thebeast
of-the-art methods for comparison.
? CRF-Subj: We follow the method proposed by
Zhao et al (2008), which regard the subjec-
tivity of all clauses throughout a paragraph as
a sequential flow of sentiments and use CRFs
to model it. The feature sets are similar as
the local formulas for MLN including words,
POS tags, dependency relations, and opinion
lexicon.
? RAE-Subj: Socher et al (2011) proposed to
use recursive autoencoders for sentence-level
predication of sentiment label distributions. To
compare with it, we also reimplement their
method without any hand designed lexicon.
? PDTB-Rel: For discourse relation extraction,
we use ?PDTB-Styled End-to-End Discourse
Parser? (Lin et al, 2010) to extract discourse
level relations as baseline. Since it is a gener-
al discourse relations identification algorithms,
?Cause?, ?Pragmatic Cause?, ?Instantiation?,
and ?Restatement? relation types are treated as
explanatory relation in this work.
? SVM-Rel: We also use LibSVM (Chang and
Lin, 2011) to classify the relations between
clauses. Following the configurations reported
by Feng and Hirst (2012), we use linear kernel
and probability estimation to model it.
4.3 Results
Table 3 shows the comparisons of the proposed
method with the state-of-the-art systems on subjec-
tivity classification and explanatory relation extrac-
tion. From the results, we can observe that recur-
sive autoencoders based subjectivity classification
method achieves slightly better performance than
our method and conditional random fields based
method. The performances of the proposed method
are similar as CRFs?. We think that the main reason
is that only lexical features are used in MLN models
for subjective classification. However, conditional
random fields consider not only lexical information
but also inference of the contexts of sentences.
RAE method learns vector space representations for
multi-word phrases and uses compositional seman-
tics to understand sentiment.
952
Methods
Subjective Classification
P R F1
CRF-Subj 83.5% 76.9% 80.1%
RAE-Subj 85.3% 79.1% 82.1%
MLN 79.2% 80.6% 79.9%
Methods
Relation Extraction
P R F1
RAE-Subj + PDTB-Rel 28.5% 38.6% 32.8%
RAE-Subj + SVM-Rel 32.4% 89.7% 47.6%
MLN 56.2% 72.9% 63.5%
Table 3: Performance comparisons between the proposed
method and state-of-the-art methods. ?MLN? represents
the method proposed in this work.
For evaluating the performance of relation extrac-
tion, we combine the results of RAE with PDTB-
Rel and SVM-Rel. For all the subjective clauses
identified by RAE, PDTB-Rel and SVM-Rel are
used to extract corresponding explanatory clauses.
The results are shown in the last three rows in
the Table 3. From the results, we can observe
that the proposed joint model achieves best F1
score and precision among all methods. Although
the proposed method achieve slightly worse result
in processing subjectivity classification. We think
that the error propagation is the main reason for
worse results of cascaded methods. The relative
improvement of MLN over SVM-Rel is more than
33.4%.
To show the effectiveness of different observed
predicates, we evaluate the performances of the
proposed method with different predicate sets. We
subtract one observed predicate and its correspond-
ing local formulas from the original sets at a time.
The results of both subjectivity classification and
relation extraction are shown in Table 4. The first
row shows the result of the MLN based method with
all observed predicates and local formulas. From the
results we can observe that the observed predicates
which are not used in the local formulas for sub-
jectivity classification also impact the performance
of subjectivity classification. We think that the per-
formance is effected by the global formulas, which
combine the procedure of subjectivity classification
and relation extraction. Among all predicates, we
observe that words and dependency relations play
the most important roles. Without word predicate,
the F1 score of subjectivity classification and re-
lation extraction significantly drop to 51.2% and
42.9% respectively. For subjectivity classification,
subjective lexicon contributes a lot for recall. For
relation extraction, the impacts of clause distance
and sentence distance are not as significant as the
other features.
5 Related Work
Our work relates to three research areas: sentiment
analysis/opinion mining, discourse-level relation ex-
traction, andMarkov logic networks. Along with the
increasing requirement, subjectivity classification
has recently received considerable attention from
both the industry and researchers. A variety of
approaches and methods have been proposed for
this task from different aspects. Among them, a
number of approaches focus on classifying senti-
ments of text in different levels (e.g. words (Kim
and Hovy, 2004), phrases (Wilson et al, 2005),
sentences (Zhao et al, 2008), documents (Pang et
al., 2002) and so on.), and detecting the overall
polarity of them.
Another research direction tries to convert the
sentiment analysis task into entity identification and
relation extraction. Hu and Liu (2004) proposed
to use a set of methods to produce feature-based
summary of a large number of customer reviews.
Kobayashi et al (2007) assumed that evaluative
opinions could be structured as a frame which is
composed by opinion holder, subject, aspect, and
evaluation. They converted the task to two kinds
of relation extraction tasks and proposed a machine
learning-based method which used both contextual
and statistical clues.
Analysis of some special types of sentences were
also introduced in recent years. Jindal and Li-
u (2006) studied the problem of identifying com-
parative sentences. They analyzed different types
of comparative sentences and proposed learning
approaches to identify them. Conditional sentences
were studied by Narayanan et al(2009). They
analyzed the conditional sentences in both linguistic
and computitional perspectives and used learning
953
Subjective Classification Relation Extraction
P R F1 P R F1
MLN 79.2% 80.6% 79.9% 56.2% 72.9% 63.5%
?subjLexicon(w) 76.6% 70.4% 73.4 % 52.3% 68.6% 59.4%
?relationLexicon(w) 78.2% 79.4% 78.8% 53.6% 70.8% 61.0%
?word(i, w) 52.8% 49.6% 51.2 % 36.4% 52.1% 42.9%
?firstWord(i, w) 76.3% 80.1% 78.2% 56.9% 69.8% 62.7%
?pos(i, w, t) 72.6% 76.8% 74.6 % 52.4% 60.2% 56.0%
?dep(i, h,m) 57.6% 70.6% 63.4% 41.2% 56.8% 47.8%
?clauseDistance(i, j,m) 78.9% 80.2% 79.5% 52.6% 70.6% 60.3%
?sentenceDistance(i, j, n) 78.6% 80.3% 79.4% 52.4% 70.8% 60.2%
Table 4: Performance comparisons of different observed predicates
method to do it. They followed the feature-based
sentiment analysis model (Hu and Liu, 2004), which
also use flat frames to represent evaluations.
Since the cross sentences relations are considered
in this work, the discourse-level relation extrac-
tion methods are also related to ours. Marcu and
Echihabi (2002) proposed to use an unsupervised
approach to recognizing discourse relations. Lin et
al.(2009) analyzed the impacts of features extracted
from contextual information, constituent parse trees,
dependency parse trees, and word pairs. Asher
et al(2009) studied discourse segments containing
opinion expressions from the perspective of linguis-
tics. Chen et al (2010) introduced a multi-label
model to detect emotion causes. They developed
two sets of linguistic features for this task base on
linguistic cues. Zirn et al (2011) proposed to use
MLN framework to capture the context information
in analysing (sub-)sentences.
The most similar work to ours was proposed by
Somasundaran et al(2009). They proposed to use it-
erative classification algorithm to capture discourse-
level associations. However different to us, they
focused on pairwise relationships between opinion
expressions. In this paper, we used MLN framework
to capture another different discourse-level relation,
which exists between subject clauses or subject
clause and objective clause.
Richardson and Domingos (2006) proposed
Markov Logic Networks, which combines first-
order logic and probabilistic graphical models. In
recent years, MLN has been adopted for several
natural language processing tasks and achieved
a certain level of success (Singla and Domingos,
2006; Riedel and Meza-Ruiz, 2008; Yoshikawa
et al, 2009; Andrzejewski et al, 2011; Jiang
et al, 2012; Huang et al, 2012). Singla and
Domingos (2006) modeled the entity resolution
problem with MLN. They demonstrated the
capability of MLN to seamlessly combine a number
of previous approaches. Poon and Domingos (2008)
proposed to use MLN for joint unsupervised
coreference resolution. Yoshikawa et al (2009)
proposed to use Markov logic to incorporate both
local features and global constraints that hold
between temporal relations. Andrzejewski et
al. (2011) introduced a framework for incorporating
general domain knowledge, which is represented by
First-Order Logic (FOL) rules, into LDA inference
to produce topics shaped by both the data and the
rules.
6 Conclusions
In this paper, we propose to use Markov logic
networks to identify subjective text segments and ex-
tract their corresponding explanations in discourse
level. We use MLN to jointly model subjectivity
classification and explanatory relation extraction.
Rich linguistic features and global constraints are
incorporated by various logic formulas and global
formulas. To evaluate the proposed method, we
collected a large number of product reviews and
954
constructed a labeled corpus through Amazon?s Me-
chanical Turk. Experimental results demonstrate
that the proposed approach achieve better perfor-
mance than state-of-the-art methods.
7 Acknowledgement
The authors wish to thank the anonymous reviewers
for their helpful comments and Kang Han for
preparing the corpus. This work was partially
funded by National Natural Science Foundation
of China (61003092, 61073069), Key Projects
in the National Science & Technology Pillar
Program(2012BAH18B01), National Major
Science and Technology Special Project of China
(2014ZX03006005), Shanghai Municipal Science
and Technology Commission (12511504502) and
?Chen Guang? project supported by Shanghai
Municipal Education Commission and Shanghai
Education Development Foundation(11CG05).
References
David Andrzejewski, Xiaojin Zhu, Mark Craven, and
Benjamin Recht. 2011. A framework for incorpo-
rating general domain knowledge into latent dirichlet
allocation using first-order logic. In Proceedings of
the Twenty-Second international joint conference on
Artificial Intelligence - Volume Volume Two, IJCAI?11,
pages 1171?1177. AAAI Press.
Nicholas Asher, Farah Benamara, and Yannick Mathieu.
2009. Appraisal of opinion expressions in discourse.
Lingvistic? Investigationes.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Bente Maegaard, Joseph Mariani, Jan Odijk,
Stelios Piperidis, Mike Rosner, and Daniel Tapias,
editors, Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
R. Carston. 1993. Conjunction, explanation and
relevance. Lingua 90, pages 27?48.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
A library for support vector machines. ACM Trans.
Intell. Syst. Technol., 2(3):27:1?27:27, May.
Ying Chen, Sophia Yat Mei Lee, Shoushan Li, and
Chu-Ren Huang. 2010. Emotion cause detection
with linguistic constructions. In Proceedings ofColing
2010.
Sajib Dasgupta and Vincent Ng. Mine the easy, classify
the hard: A semi-supervised approach to automatic
sentiment classification. In Proceedings of ACL-
IJCNLP 2009.
Eduard Dragut, Hong Wang, Clement Yu, Prasad Sistla,
and Weiyi Meng. 2012. Polarity consistency
checking for sentiment dictionaries. In Proceedings
of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 997?1005, Jeju Island, Korea, July. Association
for Computational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-
level discourse parsing with rich linguistic features.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 60?68, Jeju Island, Korea, July.
Association for Computational Linguistics.
Ahmed Hassan and Dragomir R. Radev. 2010.
Identifying text polarity using random walks. In
Proceedings of ACL 2010, Uppsala, Sweden, July.
Minqing Hu and Bing Liu. 2004. Mining and
summarizing customer reviews. In Proceedings of
SIGKDD 2004.
Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.
2012. Using first-order logic to compress sentences.
In Proceedings of the Twenty-Sixth AAAI Conference
on Artificial Intelligence.
Shangpu Jiang, D. Lowd, and Dejing Dou. 2012. Learn-
ing to refine an automatically extracted knowledge
base using markov logic. In Data Mining (ICDM),
2012 IEEE 12th International Conference on, pages
912?917.
Nitin Jindal and Bing Liu. 2006. Identifying comparative
sentences in text documents. In Proceedings of SIGIR
2006.
Soo-Min Kim and Eduard Hovy. 2004. Determining
the sentiment of opinions. In Proceedings of COLING
2004.
Dan Klein and Christopher D.Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In Proceedings of NIPS 2003.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of
relations in opinion mining. In Proceedings of
EMNLP-CoNLL 2007.
Paul Kroeger. 2005. Analyzing Grammar: An
Introduction. Cambridge.
Alex Lascarides and Nicholas Asher. 1993. Temporal
interpretation, discourse relations, and common sense
entailment. Linguistics and Philosophy, 16(5):437?
493.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of EMNLP 2009.
955
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
A pdtb-styled end-to-end discourse parser. CoRR,
abs/1011.0835.
Bing Liu. 2012. Sentiment Analysis and Opinion
Mining. Morgan & Claypool Publishers.
Daniel Marcu and Abdessamad Echihabi. 2002.
An unsupervised approach to recognizing discourse
relations. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
?02, pages 368?375.
Xinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,
Ge Xu, and Houfeng Wang. 2012. Cross-
lingual mixture model for sentiment classification.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 572?581, Jeju Island, Korea, July.
Association for Computational Linguistics.
RadaMihalcea, Carmen Banea, and JanyceWiebe. 2007.
Learning multilingual subjective language via cross-
lingual projections. In Proceedings of ACL 2007.
Ramanathan Narayanan, Bing Liu, and Alok Choudhary.
2009. Sentiment analysis of conditional sentences. In
Proceedings of EMNLP 2009.
Bo Pang and Lillian Lee. 2008. Opinion mining
and sentiment analysis. Foundations and Trends in
Information Retrieval, 2:1?135, January.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of
EMNLP 2002.
Emily Pitler and Ani Nenkova. 2009. Using syntax to
disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP 2009.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with markov logic. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?08, pages
650?659, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Alan Lee Eleni Miltsakaki Livio Robaldo Aravind Joshi
Rashmi Prasad, Nikhil Dinesh and Bonnie Webber.
2008. The penn discourse treebank 2.0. In
Bente Maegaard Joseph Mariani Jan Odjik Stelios
Piperidis Daniel Tapias Nicoletta Calzolari (Confer-
ence Chair), Khalid Choukri, editor, Proceedings of L-
REC?08, Marrakech, Morocco, may. http://www.lrec-
conf.org/proceedings/lrec2008/.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62(1-
2):107?136.
Sebastian Riedel and Ivan Meza-Ruiz. 2008. Collective
semantic role labelling with markov logic. In
Proceedings of the Twelfth Conference on Compu-
tational Natural Language Learning, CoNLL ?08,
pages 193?197, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of HLT-NAACL 2003.
P. Singla and P. Domingos. 2006. Entity resolution with
markov logic. In Data Mining, 2006. ICDM ?06. Sixth
International Conference on, pages 572?582.
Richard Socher, Jeffrey Pennington, Eric H. Huang,
Andrew Y. Ng, and Christopher D. Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of
the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 151?161,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Swapna Somasundaran, Galileo Namata, Lise Getoor,
and Janyce Wiebe. 2009. Opinion graphs for
polarity and discourse classification. In Proceedings
of TextGraphs-4.
Yang Song, Jing Jiang, Wayne Xin Zhao, Sujian Li, and
Houfeng Wang. 2012. Joint learning for coreference
resolution with markov logic. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1245?1254, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientations of words using
spin model. In Proceedings of ACL 2005.
Erik F. Tjong, Kim Sang, and Herve? De?jean. 2001.
Introduction to the conll-2001 shared task: clause
identification. In Proceedings of the 2001 workshop
on Computational Natural Language Learning -
Volume 7, ConLL ?01, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of HLT-EMNLP
2005.
Yuanbin Wu, Qi Zhang, Xuangjing Huang, and Lide
Wu. 2011. Structural opinion mining for graph-based
sentiment representation. In Proceedings of EMNLP
2011.
Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and
Jun Zhao. 2013. Walk and learn: a two-stage
approach for opinion words and opinion targets co-
extraction. In Proceedings of the 22nd international
conference on World Wide Web companion, WWW
?13 Companion, pages 95?96, Republic and Canton of
Geneva, Switzerland. International World Wide Web
Conferences Steering Committee.
956
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proceedings of
ACL 2013.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki
Asahara, and Yuji Matsumoto. 2009. Jointly
identifying temporal relations with markov logic. In
Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP: Volume 1 - Volume 1, ACL ?09,
pages 405?413, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jun Zhao, Kang Liu, and Gen Wang. 2008. Adding
redundant features for crfs-based sentence sentiment
classification. In Proceedings of EMNLP 2008.
Ca?cilia Zirn, Mathias Niepert, Heiner Stuckenschmidt,
and Michael Strube. 2011. Fine-grained sentiment
analysis with structural features. In Proceedings of 5th
International Joint Conference on Natural Language
Processing, pages 336?344, Chiang Mai, Thailand,
November. Asian Federation of Natural Language
Processing.
957
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 49?54,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
FudanNLP: A Toolkit for Chinese Natural Language Processing
Xipeng Qiu, Qi Zhang, Xuanjing Huang
Fudan University, 825 Zhangheng Road, Shanghai, China
xpqiu@fudan.edu.cn, qz@fudan.edu.cn, xjhuang@fudan.edu.cn
Abstract
The growing need for Chinese natural
language processing (NLP) is largely
in a range of research and commer-
cial applications. However, most of
the currently Chinese NLP tools or
components still have a wide range
of issues need to be further improved
and developed. FudanNLP is an open
source toolkit for Chinese natural lan-
guage processing (NLP), which uses
statistics-based and rule-based meth-
ods to deal with Chinese NLP tasks,
such as word segmentation, part-of-
speech tagging, named entity recogni-
tion, dependency parsing, time phrase
recognition, anaphora resolution and so
on.
1 Introduction
Chinese is one of the most widely used lan-
guages in this world, and the proportion that
Chinese language holds on the Internet is also
quite high. Under the current circumstances,
there are greater and greater demands for in-
telligent processing and analyzing of the Chi-
nese texts.
Similar to English, the main tasks in Chi-
nese NLP include word segmentation (CWS),
part-of-speech (POS) tagging, named en-
tity recognition (NER), syntactic parsing,
anaphora resolution (AR), and so on. Al-
though the general ways are essentially the
same for English and Chinese, the implemen-
tation details are different. It is also non-
trivial to optimize these methods for Chinese
NLP tasks.
There are also some toolkits to be used
for NLP, such as Stanford CoreNLP1, Apache
OpenNLP2, Curator3 and NLTK4. But these
toolkits are developed mainly for English and
not optimized for Chinese.
In order to customize an optimized system
for Chinese language process, we implement
an open source toolkit, FudanNLP5, which is
written in Java. Since most of the state-of-the-
art methods for NLP are based on statistical
learning, the whole framework of our toolkit
is established around statistics-based meth-
ods, supplemented by some rule-based meth-
ods. Therefore, the quality of training data
is crucial for our toolkit. However, we find
that there are some drawbacks in currently
most commonly used corpora, such as CTB
(Xia, 2000) and CoNLL (Haji? et al, 2009)
corpora. For example, in CTB corpus, the set
of POS tags is relative small and some cate-
gories are derived from the perspective of En-
glish grammar. And in CoNLL corpus, the
head words are often interrogative particles
and punctuations, which are unidiomatic in
Chinese. These drawbacks bring more chal-
lenges to further analyses, such as informa-
tion extraction and semantic understanding.
Therefore, we first construct a corpus with
a modified guideline, which is more in ac-
cordance with the common understanding for
Chinese grammar.
In addition to the basic Chinese NLP tasks
1http://nlp.stanford.edu/software/corenlp.
shtml
2http://incubator.apache.org/opennlp/
3http://cogcomp.cs.illinois.edu/page/
software_view/Curator
4http://www.nltk.org/
5http://fudannlp.googlecode.com
49
Figure 1: System Structure of FudanNLP
mentioned above, the toolkit also provides
many minor functions, such as text classifi-
cation, dependency tree kernel, tree pattern-
based information extraction, keywords ex-
traction, translation between simplified and
traditional Chinese, and so on.
Currently, our toolkit has been used by
many universities and companies for various
applications, such as the dialogue system, so-
cial computing, recommendation system and
vertical search.
The rest of the demonstration is organized
as follows. We first briefly describe our system
and its main components in section 2. Then we
show system performances in section 3. Sec-
tion 4 introduces three ways to use our toolkit.
In section 5, we summarize the paper and give
some directions for our future efforts.
2 System Overview
The components of our system have three
layers of structure: data preprocessing, ma-
chine learning and natural language process-
ing, which is shown in Figure 1. We will in-
troduce these components in detail in the fol-
lowing subsections.
2.1 Data Preprocessing Component
In the natural language processing system,
the original input is always text. However,
the statistical machine learning methods often
deal with data with vector-based representa-
tion. So we firstly need to preprocess the input
texts and transform them to the required for-
mat. Due to the fact that text data is usually
discrete and sparse, the sparse vector struc-
ture is largely used. Similar to Mallet (Mc-
Callum, 2002), we use the pipeline structure
for a flexible transformation of various data.
The pipeline consists of several serial or par-
allel modules. Each module, called ?pipe?, is
aimed at a single and simple function.
For example, when we transform a sentence
into a vector with ?bag-of-words?, the trans-
formation process would involve the following
serial pipes:
1. String2Token Pipe: to transform a string
into word tokens.
2. Token2Index Pipe: to look up the word
alphabet to get the indices of the words.
3. WeightByFrequency Pipe: to calculate
the vector weight for each word accord-
ing to its frequency of occurrence.
With the pipeline structure, the data pre-
processing component has good flexibility, ex-
tensibility and reusability.
2.2 Machine Learning Component
The outputs of NLP are often structured,
so the structured learning is our core module.
Structured learning is the task of assigning a
structured label y to an input x. The label y
can be a discrete variable, a sequence, a tree
or a more complex structure.
To illustrate by a sample x, we define the
feature as ?(x,y). Thus, we can label x with
a score function,
y? = arg max
y
F (w,?(x,y)), (1)
where w is the parameter of function F (?).
The feature vector ?(x,y) consists of lots of
overlapping features, which is the chief benefit
of a discriminative model.
For example, in sequence labeling, both x =
x1, . . . , xL and y = y1, . . . , yL are sequences.
For first-order Markov sequence labeling, the
feature can be denoted as ?k(yi?1, yi,x, i),
where i is the position in the sequence. Then
the score function can be rewritten as
y? = arg max
y
F (
L?
i=1
?
k
wk?k(yi?1, yi,x, i)), (2)
where L is the length of x.
Different algorithms vary in the definition of
F (?) and the corresponding objective function.
50
F (?) is usually defined as a linear or exponen-
tial family function. For example, in condi-
tional random fields (CRFs) (Lafferty et al,
2001), F (?) is defined as:
Pw(y|x) =
1
Zw
exp(wT?(x,y)), (3)
where Zw is the normalization constant such
that it makes the sum of all the terms one.
In FudanNLP, the linear function is univer-
sally used as the objective function. Eq. (1) is
written as:
y? = arg max
y
< w,?(x,y) > . (4)
2.2.1 Training
In the training stage, we use the passive-
aggressive algorithm to learn the model pa-
rameters. Passive-aggressive (PA) algorithm
(Crammer et al, 2006) was proposed for nor-
mal multi-class classification and can be easily
extended to structure learning (Crammer et
al., 2005). Like Perceptron, PA is an online
learning algorithm.
2.2.2 Inference
For consistency with statistical machine
learning, we call the process to calculate the
Eq.(1) as ?inference?. In structured learning,
the number of possible solutions is very huge,
so dynamic programming or approximate ap-
proaches are often used for efficiency. For NLP
tasks, the most popular structure is sequence.
To label the sequence, we use Viterbi dynamic
programming to solve the inference problem in
Eq. (4).
Our system can support any order of Viterbi
decoding. In addition, we also implement a
constrained Viterbi algorithm to reduce the
number of possible solutions by pre-defined
rules. For example, when we know the prob-
able labels, we delete the unreachable states
from state transition matrix. It is very useful
for CWS and POS tagging with sequence la-
beling. When we have a word dictionary or
know the POS for some words, we can get
more accurate results.
2.2.3 Other Algorithms
Apart from the core modules of structured
learning, our system also includes several tra-
ditional machine learning algorithms, such as
Perceptron, Adaboost, kNN, k-means, and so
on.
2.3 Natural Language Processing
Components
Our toolkit provides the basic NLP func-
tions, such as word segmentation, part-of-
speech tagging, named entity recognition, syn-
tactic parsing, temporal phrase recognition,
anaphora resolution, and so on. These func-
tions are trained on our developed corpus. We
also develop a visualization module to display-
ing the output. Table 1 shows the output rep-
resentation of our toolkit.
2.3.1 Chinese Word Segmentation
Different from English, Chinese sentences
are written in a continuous sequence of char-
acters without explicit delimiters such as the
blank space. Since the meanings of most Chi-
nese characters are not complete, words are
the basic syntactic and semantic units. There-
fore, it is indispensable step to segment the
sentence into words in Chinese language pro-
cessing.
We use character-based sequence labeling
(Peng et al, 2004) to find the boundaries of
words. Besides the carefully chosen features,
we also use the meaning of character drawn
from HowNet(Dong and Dong, 2006), which
improves the performance greatly. Since un-
known words detection is still one of main chal-
lenges of Chinese word segmentation. We im-
plement a constrained Viterbi algorithm to al-
low users to add their own word dictionary.
2.3.2 POS tagging
Chinese POS tagging is very different from
that in English. There are no morphological
changes for a word among its different POS
tags. Therefore, most of Chinese words may
have multiple POS tags. For example, there
are different morphologies in English for the
word ??? (destroy)?, such as ?destroyed?,
?destroying? and ?destruction?. But in Chi-
nese, there is just one same form(Xia, 2000).
There are two popular guidelines to tag the
word?s POS: CTB (Xia, 2000) and PKU (Yu
et al, 2001). We take into account both
the weaknesses and the strengths of these two
guidelines, and propose our guideline for bet-
ter subsequent analyses, such as parser and
named entity recognition. For example, the
proper name is labeled as ?NR? in CTB, while
we label it with one of four categories: person,
51
Input:
??????????? 1980 ??
John is from Washington, and he was born in 1980.
Output:
.
.?? .?? .??? .? .? .?? .1980 ? .?
.John .is from .Washington ., .he .was born in .1980 ..
.PER .VV .LOC .PU .PRN .NN .PU
.1 .2 .3 .4 .5 .6 .7 .8
Root
SUB
CS:COO1
OBJ
PUN
SUB OBJ
PUN
NER:
1 ? PER
3 ? LOC
AR:
5 ? 1
TIME:
7 ? 1980
1 CS:COO means the coordinate complex sentence.
Table 1: Example of the output representation of our toolkit
location, organization and other proper name.
Conversely, we merge the ?VC? and ?VE? into
?VV? since there is no link verb in Chinese.
Finally, we use a tag set with 39 categories in
total.
Since a POS tag is assigned to each word,
not to each character, Chinese POS tag-
ging has two ways: pipeline method or joint
method. Currently, the joint method is more
popular and effective because it uses more flex-
ible features and can reduce the error propa-
gation (Ng and Low, 2004). In our system,
we implement both methods for POS tagging.
Besides, we also use some knowledge to im-
prove the performance, such as Chinese sur-
name and the common suffixes of the names
of locations and organizations.
2.3.3 Named Entity Recognition
In Chinese named entity recognition (NER),
there are usually three kinds of named enti-
ties (NEs) to be dealt with: names of per-
sons (PER) , locations (LOC) and organiza-
tions (ORG). Unlike English, there is no obvi-
ous identification for NEs, such as initial capi-
tals. The internal structures are also different
for different kinds of NEs, so it is difficult to
build a unified model for named entity recog-
nition.
Our NER is based on the results of POS
tagging and uses some customize features to
detect NEs. First, the number of NEs is very
large and the new NEs are endlessly emerg-
ing, so it is impossible to store them in dic-
tionary. Since the internal structures are rela-
tively more important, we use language mod-
els to capture the internal structures. Second,
we merge the continuous NEs with some rule-
based strategies. For example, we combine the
continuous words ???/NN???/NN? into
? ?????/LOC?.
2.3.4 Dependency parsing
Our syntactic parser is currently a depen-
dency parser, which is implemented with the
shift-reduce deterministic algorithm based on
the work in (Yamada and Matsumoto, 2003).
The syntactic structure of Chinese is more
complex than that of English, and semantic
meaning is more dominant than syntax in Chi-
nese sentences. So we select the dependency
parser to avoid the minutiae in syntactic con-
stituents and wish to pay more attention to
the subsequent semantic analysis. Since the
structure of the Chinese language is quite dif-
ferent from that of English, we use more effec-
tive features according to the characteristics of
Chinese sentences.
The common used corpus for Chinese de-
pendency parsing is CoNLL corpus (Haji? et
al., 2009). However, there are some illogical
cases in CoNLL corpus. For example, the
head words are often interrogative particles
and punctuations. Our guideline is based on
common understanding for Chinese grammar.
The Chinese syntactic components usually in-
clude subject, predicate, object, attribute, ad-
verbial modifier and complement. Figure 2
and 3 show the differences between the trees of
CoNLL and our Corpus. Table 2 shows some
52
primary dependency relations in our guideline.
..? .? .??? .? .? .? .?.want to .go to .Hehuanshan .to see .the snow . .?
.VV .VV .NR .VV .NN .SP .PU
Root
COMP
ADV
COMP
COMP
COMP UNK
Figure 2: Dependency Tree in CoNLL Corpus
..? .? .??? .? .? .? .?.want to .go to .Hehuanshan .to see .the snow . .?
.MD .VV .LOC .VV .NN .SP .PU
Root
ADV
OBJ
OBJ
OBJ
VOC
PUN
Figure 3: Dependency Tree in Our Corpus
Relations Chinese Definitions
SUB ?? Subject
PRED ?? Predicate
OBJ ?? Object
ATT ?? Attribute
ADV ?? Adverbial Modifier
COMP ?? Complement
SVP ?? Serial Verb Phrases
SUB-OBJ ?? Pivotal Construction
VOC ?? Voice
TEN ?? Tense
PUN ?? Punctuation
Table 2: Some primary dependency relations
2.3.5 Temporal Phrase Recognition
and Normalization
Chinese temporal phrases is more flexible
than English. Firstly, there are two calendars:
Gregorian and lunar calendars. Both of them
are frequently used. Secondly, the forms of
same temporal phrase are various, which often
consists of Chinese characters, Arabic numer-
als and English letters, such as ??? 10 ??
and ?10:00 PM?.
Different from the general process based
on machine learning, we implement the time
phrase recognizer with a rule-based method.
These rules include 376 regular expressions
and nearly a hundred logical judgments.
After recognizing the temporal phrases, we
normalize them with a standard time format.
For a phrase indicating a relative time , such
as ????? and ? ?????, we first find the
base time in the context. If no base time is
found, or there is also no temporal phrase to
indicate the base time (such as ????), we
set the base time to the current system time.
Table 3 gives examples for our temporal phrase
recognition module.
Input:
08 ?????????8 ? 8 ??????????
????????????
The Beijing Olympic Games took place from Au-
gust 8, 2008. Four years later, the London Olympic
Games took place from July 21.
???????? 9 ?????????????
I?m busy today, and have to come off duty after 9:00
PM. And I also have to work this Sunday.
Output:
08 ? (2008) 2008
8 ? 8 ? (August 8) 2008-8-8
?????? (July 21) 2012-7-27
?? (today) 2012-2-221
?? 9 ? (9:00 PM) 2012-2-22 21:00
?? (this Sunday) 2012-2-26
1 The base time is 2012-02-22 10:00AM.
Table 3: Examples for Temporal Phrase
Recognition
2.3.6 Anaphora Resolution
Anaphora resolution is to detect the pro-
nouns and find what they are referring to.
We first find all pronouns and entity names,
then use a classifier to predict whether there
is a relation between each pair of pronoun and
entity name. Table 4 gives examples for our
anaphora resolution module.
Input:
??????? 1167 ?????????????
?????????????
Oxford University is founded in 1167. It is located
in Oxford, UK. The university has nurtured a lot
of good students.
Output:
? (It) ????
???? (The
university)
???? (Oxford University)
Table 4: Examples for Anaphora Resolution
3 System Performances
In this section, we investigate the per-
formances for the six tasks: Chinese word
segmentation (CWS), POS tagging (POS),
53
named entity recognition (NER) and de-
pendency parser(DePar), Temporal Phrase
Recognition (TPR) and Anaphora Resolution
(AR). We use 5-fold cross validation on our
developed corpus. The corpus includes 65, 745
sentences and 959, 846 words. The perfor-
mances are shown in Table 5.
Task Accuracy Speed1 Memory
CWS 97.5% 98.9K 66M
POS 93.4% 44.5K 110M
NER 98.40% 38K 30M
DePar 85.3% 21.1 80M
TPR 95.16% 22.9k 237K
AR 70.3% 35.7K 52K
1 characters per second. Test environment:
CPU 2.67GHz, JRE 7.
Table 5: System Performances
4 Usages
We provide three ways to use our toolkit.
Firstly, our toolkit can be used as library.
Users can call application programming inter-
faces (API) in their own applications.
Secondly, users can also invoke the main
NLP modules to process the inputs (strings
or files) from the command line directly.
Thirdly, the web services are provided
for platform-independent and language-
independent use. We use a REST (Represen-
tational State Transfer) architecture, in which
the web services are viewed as resources and
can be identified by their URLs.
5 Conclusions
In this demonstration, we have described
the system, FudanNLP, which is a Java-based
open source toolkit for Chinese natural lan-
guage processing. In the future, we will add
more functions, such as semantic parsing. Be-
sides, we will also optimize the algorithms and
codes to improve the system performances.
Acknowledgments
We would like to thank all the people6
involved with our FudanNLP project. This
work was funded by NSFC (No.61003091
6https://code.google.com/p/fudannlp/wiki/
People
and No.61073069) and 973 Program
(No.2010CB327900).
References
K. Crammer, R. McDonald, and F. Pereira. 2005.
Scalable large-margin online learning for struc-
tured classification. In NIPS Workshop on
Learning With Structured Outputs. Citeseer.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. On-
line passive-aggressive algorithms. Journal of
Machine Learning Research, 7:551?585.
Z. Dong and Q. Dong. 2006. Hownet And the
Computation of Meaning. World Scientific Pub-
lishing Co., Inc. River Edge, NJ, USA.
J. Haji?, M. Ciaramita, R. Johansson, D. Kawa-
hara, M.A. Mart?, L. M?rquez, A. Meyers,
J. Nivre, S. Pad?, J. ?t?p?nek, et al 2009. The
CoNLL-2009 shared task: Syntactic and seman-
tic dependencies in multiple languages. In Pro-
ceedings of the Thirteenth Conference on Com-
putational Natural Language Learning: Shared
Task, pages 1?18. Association for Computa-
tional Linguistics.
John D. Lafferty, Andrew McCallum, and Fer-
nando C. N. Pereira. 2001. Conditional ran-
dom fields: Probabilistic models for segmenting
and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Ma-
chine Learning.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
H.T. Ng and J.K. Low. 2004. Chinese part-
of-speech tagging: one-at-a-time or all-at-once?
word-based or character-based. In Proceedings
of EMNLP, volume 4.
F. Peng, F. Feng, and A. McCallum. 2004. Chi-
nese segmentation and new word detection us-
ing conditional random fields. Proceedings of the
20th international conference on Computational
Linguistics.
F. Xia, 2000. The part-of-speech tagging guidelines
for the penn chinese treebank (3.0).
H. Yamada and Y. Matsumoto. 2003. Statis-
tical dependency analysis with support vector
machines. In Proceedings of the International
Workshop on Parsing Technologies (IWPT),
volume 3.
S. Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun,
H. Wang, Q. Zhao, and W. Zhan. 2001. Process-
ing norms of modern chinese corpus. Technical
report, Technical report.
54
