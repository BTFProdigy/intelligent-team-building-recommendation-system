Fine-Grained Hidden Markov Modeling for Broadcast-
News Story Segmentation
Warren Greiff, Alex Morgan, Randall Fish, Marc Richards, Amlan Kundu,
MITRE Corporation
202 Burlington Road
Bedford, MA 01730-1420
(greiff, amorgan, fishr, marc, akundu)@mitre.org
ABSTRACT
We present the design and development of a Hidden Markov
Model for the division of news broadcasts into story segments.
Model topology, and the textual features used, are discussed,
together with the non-parametric estimation techniques that were
employed for obtaining estimates for both transition and
observation probabilities.  Visualization methods developed for
the analysis of system performance are also presented.
1. INTRODUCTION
Current technology makes the automated capture, storage,
indexing, and categorization of broadcast news feasible allowing
for the development of computational systems that provide for the
intelligent browsing and retrieval of news stories [Maybury,
Merlino & Morey ?97; Kubula, et al, ?00].  To be effective, such
systems must be able to partition the undifferentiated input signal
into the appropriate sequence of news-story segments.
In this paper we discuss an approach to segmentation based on the
use of a fine-grained Hidden Markov Model [Rabiner, `89] to
model the generation of the words produced during a news
program.  We present the model topology, and the textual features
used.  Critical to this approach is the application of non-parametric
estimation techniques, employed to obtain robust estimates for
both transition and observation probabilities. Visualization
methods developed for the analysis of system performance are
also presented.
Typically, approaches to news-story segmentation have been
based on extracting features of the input stream that are likely to
be different at boundaries between stories from what is observed
within the span of individual stories. In [Beeferman, Berger, &
Lafferty ?99], boundary decisions are based on how well
predictions made by a long-range exponential language model
compare to those made by a short range trigram model. [Ponte and
Croft, ?97] utilize Local Context Analysis [Xu, J. and Croft, ?96]
to enrich each sentence with related words, and then use dynamic
programming to find an optimal boundary sequence based on a
measure of word-occurrence similarity between pairs of enriched
sentences. In [Greiff, Hurwitz & Merlino, `99], a na?ve Bayes
classifier is used to make a boundary decision at each word of the
transcript.  In [Yamron, et al, ?98], a fully connected Hidden
Markov Model is based on automatically induced topic clusters,
with one node for each topic.  Observation probabilities for each
node are estimated using smoothed unigram statistics.
The approach reported in this paper goes further along the lines of
find-grained modeling in two respects: 1) differences in feature
patterns likely to be observed at different points in the
development of a news story are exploited, in contrast to
approaches that focus on boudary/no-boundary differences; and 2)
a more detailed modeling of the story-length distribution profile,
unique to each news source (for example, see the histogram of
story lengths for ABC World News Tonight shown in the top
graph of Figure 3, below).
2. GENERATIVE MODEL
We model the generation of news stories as a 251 state Hidden
Markov Model, with the topology shown in Figure 1. States
labeled, 1 to 250, correspond to each of the first 250 words of a
story.  One extra state, labeled 251, is included to model the
production of all words at the end of stories exceeding 250 words
in length.
Several other models were considered, but this model is
particularly suited to the features used, as it allows one to model
features that vary with depth into the story (Section 3.1), while
simultaneously, by delaying certain features.  It also allows one to
model features that occur in specific regions the boundaries
(Section 3.3).  This is possible because all states can feed into the
initial state, i.e. all stories end by going into the first word of a
new story.
1 2 3 250 251
Figure 1:  Current HMM Topology
For example, the original model involved a series of beginning
and then end states, with a single middle state that could be cycled
through (Figure 2).  This proved to be a problem because the ends
of long stories were being mixed with the ends of short stories
which led to problems with our spaced coherence feature (Section
3.1).  Another possibility involved splitting the model into two
main paths, one to model the shorter stories, and one to model the
longer as there is something of a bimodal distribution in story
lengths (Figure 4).  However, the fine-grained nature of our model
would suffer from splitting the data in this manner, and a choice
about at which length to fork the model would be somewhat
artificial.
3. FEATURES
Associated with the model is a set of features.  For each state, the
model assigns a probability distribution over all possible
combinations of values the features may take on.  The probability
assigned to value combinations is assumed to be independent of
the state/observation history, conditioned on the state. We further
assume that the value of any one feature is independent of all
others, once the current state is known. Features have been
explicitly designed with this assumption in mind.  Three
categories of features have been used, which we refer to as
coherence features, x-duration feature, and the trigger features.
3
W
sh
w
do
ap
of
st
tr
ra
fe
COHER-4  (Figures 3b, c & d) correspond to similar features; for
these, however, the buffer is separated by 50, 100, and 150 words,
respectively, from the current word.  Interestingly, the COHER-4
feature actually caused a reduction in performance, and was not
used in the final evaluation.
3.2. X-duration
This feature is based on indications given by the speech recognizer
that it was unable to transcribe a portion of the audio signal. The
existence of an untranscribable section prior to the word gives a
non-zero X-DURATION value based on the extent of the section.
Empirically this is an excellent predictor of boundaries in that an
untranscribable event has uniform likelihood of occurring
anywhere in a news story, except prior to the first word of a story,
where it is extremely likely to occur.
3.3. Triggers
Trigger features correspond to small regions at the beginning and
end of stories, and exploit the fact that some words are far more
likely to occur in these positions than in other parts of a news
segment.  One region, for example, is restricted to the first word of
the story.  In ABC?s World News Tonight, for example, the word
?finally? is far more likely to occur in the first word of a story than
would be expected by its general rate of occurrence in the training
data.  For a word, w, appearing in the input stream, the value of
the feature is an estimate of how likely it is for w to appear in the
region of interest.  The estimate used is given by:
( )Rw
Rw
fn
nRwp /1
1)(?
+
+
=? ?
where Rwn ? is the number of times w appeared in R in the training
data; wn  is the total number of occurrences of w; and Rf  is the
fraction of all tokens of w that occurred in the region.  This
estimate can be viewed as Bayesian estimate with a beta prior.
The beta prior is equivalent to a uniform prior and the observation
of one occurrence of the word in the region out of ( )Rf/1  total
occurrences.  This estimate was chosen so that: 1) the prior
probability would not be greatly affected for words observed only
a few times in the training data; 2) it would be pushed strongly
towards the empirical probability of the word appearing in the
region for words that were encountered in R; 3) it has a prior
probability, Rf , equal to the expectation for a randomly selected
word.  The regions used for the submission were restricted to the
one-word regions for: first word, second word, last word, and
1 2 500 501
Figure 3: Original Topology.1. Coherence
a
b
ce have used four coherence features.  The COHER-1 feature,
own schematically in Figure 2a, is based on a buffer of 50
ords immediately prior to the current word.  If the current word
es not appear in the buffer, the value of COHER-1 is 0.  If it does
pear in the buffer, the value is -log(sw/s), where sw is the number
 stories in which the word appears, and s is the total number of
ories, in the training data. Words that did not appear in the
aining data, are treated as having appeared once.  In this way,
re words get high feature values, and common words get low
ature values.  Three other features: COHER-2, COHER-3, and
next-to-last word.  Limited experimentation with multi-state
regions, was not fruitful.  For example, including the regions,
{3,4,?,10} and {-10,-9,?,-3}, where ?i is interpreted as i words
prior to the end of the story, did not improve segmentation
performance.
Since, as described, the current HMM topology does not model
end-of-story words (earlier versions of the topology did model
these states directly), trigger features for end-of-story regions are
delayed. That means that a trigger related to the last word in a
story would be delayed by a one word buffer.  In this way, it is
linked to the first word in the next story.  For example, the word
?Jennings? (the name of the main anchorperson) is strongly
d
Figure 2: Coherence Features
correlated with the last word in news stories in the ABC World
News Tonight corpus.  The estimated probability of it being the
last word of the story in which it appears is .235 (obtained by the
aforementioned method). The trained model associates a high
likelihood of seeing the value .235 at state = 1; the intuitive
interpretation being, "a word highly likely to appear at the last
word of a story, occurred 1-word ago".
4. PARAMETER ESTIMATION
The Hidden Markov Model requires the estimation of transition
and conditional observation probabilities.  There are 251 transition
probabilities to be estimated.  Much more of a problem are the
observation probabilities, there being 9 features in the model, for
each of which a probability distribution over as many as 100
values must be estimated, for each of 251 states.  With the goal of
developing methods for robust estimation in the context of story
segmentation, we have applied non-parametric kernel estimation
techniques, using the LOCFIT library [Loader, ?99] of the R open-
source statistical analysis package, which is based on the S-plus
system [Venables & Ripley,
`99; Chambers & Hastie, `92, Becker, Chambers & Wilks, `88].
For the transition probabilities, it is assumed that the underlying
probability distribution over story length is smooth, allowing the
empirical histogram, shown at the top of Figure 4, to be
transformed to the probability density estimate shown at the
bottom. From this probability distribution over story lengths, the
conditional transition probabilities can be estimated directly.
Conditional observation probabilities are also deduced from an
estimate of the joint probability distribution.  First, observation
values were binned.  Binning limits were set in an attempt to 1) be
large enough to obtain sufficient counts for the production of
robust probability estimates, and yet, 2) be constrained enough so
that important distinctions in the probabilities for different feature
values will be reflected in the model.  For each bin, the
observation counts are smoothed by performing a non-parametric
regression of the observation counts as a function of state.  The
smoothed observations counts corresponding to the regression are
then normalized so as to sum to the total observation count for the
bin.  The result is a conditional probability distribution over states
for a given binned feature value,  p(State=s|Feature=fv).  Once
this is done for all bin values, each conditional probability is
multiplied by the marginal probability, p(State=s), of being in a
given state, resulting in a joint distribution, p(fv,s), over the entire
space of (Feature,State) values.  From this joint distribution, the
necessary conditional probabilities, p(Feature=fv|State=s), can be
deduced directly.
Figure 5 shows the conditional probability estimates, p(fv | s), for
the feature value COHER-3=20, across all states, confirming the
intuition that, while the probability of seeing a value of 20 is small
for all states, the likelihood of seeing it is much higher in latter
parts of a story than it is in early-story states.
5. SEGMENTATION
Once parameters for the HMM have been determined,
segmentation is straightforward.  The Viterbi algorithm [Rabiner,
`89], is employed to determine the sequence of states most likely
to have produced the observation sequence associated with the
broadcast.  A boundary is then associated with each word
produced from State 1 for the maximum likelihood state sequence.
The version of the Viterbi algorithm we have implemented
provides for the specification of ?state-penalty? parameters, which
we have used for the ?boundary state?, state 1. In effect, the
probability for each path in consideration is multiplied by the
value of this parameter (which can be less than, equal to, or
greater than, 1) for each time the path passes through the boundary
state.  Variation of the parameter effectively controls the
?aggressiveness? of segmentation, allowing for tuning system
behavior in the context of the evaluation metric.
6. RESULTS
Preliminary test results of this approach are encouraging.  After
training on all but 15 of the ABC World News Tonight programs
from the TDT-2 corpus [Nist, ?00], a test on the remaining 15
produced a false-alarm (boundary predicted incorrectly)
probability of .11, with a corresponding miss (true boundary not
predicted) probability of .14, equal to the best performance
reported to date, for this news source.
A more intuitive appreciation for the quality of performance can
be garnered from the graphs in Figure 6, which contrast the
segmentation produced by the system (middle) with ground truth
(the top graph), for a typical member of the ABC test set. The x-
axis corresponds to time (in units of word tokens); i.e., the index
of the word produced by the speech recognizer, and the y-axis
Figure 4: Histograms of story lengths (up to 250 words)
-- raw and smoothed --
Figure 5: Likelihood of COHER-3=2 over all states
corresponds to the state of the HMM model. A path passing
through the point (301, 65), for example, corresponds to a path
through the network that produced the 65th word from state 301.
Returns to state=1 correspond to boundaries between stories. The
bottom graph shows the superposition of the two to help illustrate
the agreement between the path chosen by the system and the path
corresponding to perfect segmentation..
7. VISUALIZATION
The evolution of the segmentation algorithm was driven by
analysis of the behavior of the system, which was supported by
visualization routines developed using the graphing capability of
the R package.  Figure 7 gives an example of the kind of graphical
displays that were used for analysis of the segmentation of a
specific broadcast news program; in this case, analysis of the role
of the X-DURATION feature.  This graphical display allows for
the comparison of the maximum likelihood path produced by the
HMM to the path through the HMM that would be produced by a
perfect system ? one privy to ground-truth.
T
sh
gr
co
po
ha
fr
th
hi
lo
T
li
ge
sy
D
tr
V
st
X
t gative
p n that
r e true
p deling.
T system
p of the
e
Figure 6: Perfohe true state than from the predicted state.  Strongly ne
oints are a major component of the probability calculatio
esulted in the system preferring the path it chose over th
ath.  These points suggest potential deficiencies in the mo
heir identification directs the focus of analysis so that 
erformance can be improved by correcting weaknesses 
xisting model.
rmancehe
ies
og
ful
ut
ed
 is
he
ith
tal
ue
ce
odhe top graph corresponds to the bottom graph of Figure 6,
owing the states traversed by the two systems.  The second
aph shows the value of the X-DURATION feature
rresponding to each word of the broadcast. So, the plotting of a
int at (301, 3) corresponds to an X-DURATION value of 3
ving been observed at time, 301. One thing that can be seen
om this graph is that being at a story boundary (low-points on
e thicker-darker line of the top graph) is more frequent when
gher values of the X-DURATION cue are observed, than when
wer values are observed, as could be expected.
he third graph shows, on a log scale, how many times more
kely it is that the observed X-DURATION value would be
nerated from the true state than from the state predicted by the
stem.  Most points are close to 0, indicating that the X-
URATION value observed was as likely to have come from the
ue state as it is to have come from the state predicted by the
iterbi algorithm.  Of course, this is the case wherever the true
ate has been correctly predicted. Negative points indicate that the
-DURATION value observed is less likely to be produced from
The final graph shows the cumulative sum of the values from t
graph above it. (Note that the sum of the logs of the probabilit
is equivalent to the cumulative product of probabilities on a l
scale.)  The graphing of the cumulative sum can be very use
when the system is performing poorly due to a small b
consistent preference for the observations having been produc
by the state sequence chosen by the system.  This phenomenon
made evident by a steady downward trend in the graph of t
cumulative sum.  This is in contrast to an overall level trend w
occasional downward dips.  Note, that a similar graph for the to
probability (equal to the product of all the individual feature val
probabilities) will always have an overall downward trend, sin
the maximum likelihood path will always have a likeliho
Figure 7: Visualization for x-duration feature
greater than the likelihood of any other path.
Aside from supporting the detailed analysis of specific features,
the productions of these graphs for each of the features, together
with the corresponding graph for the total observation probability,
allowed us to quickly asses which of the features was most
problematic at any given stage of model development.
8. FURTHER WORK
It should be kept in mind that experimentation with this approach
has been based on relatively primitive features ? our focus, to this
point, having been on the development of the core segmentation
mechanism.  Features based on more sophisticated extraction
techniques, which have been reported in the literature ? for
example, the use of exponential models for determining trigger
cues used in [Beeferman, Berger, & Lafferty ?99] ? can easily be
incorporated into this general framework.  Integration of such
techniques can be expected to result in significant further
improvement in segmentation quality.
To date, the binning method described has given much better
results than two dimensional kernel density estimation techniques
which we also attempted to employ.  One of the main difficulties
with using traditional kernel density estimation techniques is that
they tend to inaccurately estimate the density at areas of
discontinuity, such as state=1 in our model and our trigger
features.  Preliminary work with boundary kernels [Scott, ?92] is
very promising.  It is certainly an area worthy of more in-depth
investigation.
Work done by another group [Liu, ?00] to segment documentaries
based on video cues alone has been moderately successful in the
past.  We engineered a neural network in an attempt to identify
video frames containing an anchorperson, a logo, and blank
frames, with a belief that these are all features that would contain
information about story boundaries.  Preliminary work was also
done to extract features directly from the audio signal, such as
trying to identify speaker change. Initial work with the audio and
video has been unable to aid in segmentation, but we feel this is
also an area worth continuing to pursue.
9. REFERENCES
1. [Becker, Chambers & Wilks, `88] Becker, Richard A.,
Chambers, John M., and Wilks, Allan R.  The New S
Language.  Wadsworth & Brooks/Cole, Pacific Grove, Cal.
2. [Beeferman, Berger, & Lafferty ?99] D. Beeferman, D., A.
Berger, A. and Lafferty, J.  Statistical models for text
segmentation.  Machine Learning, vol. 34, pp. 1-34, 1999.
3. [Chambers & Hastie, `88] Chambers, John M. and Hastie,
Trevor, J.  Statistical Models in S.  Wadsworth &
Brooks/Cole, Pacific Grove, Cal., 1988.
4. [Greiff, Hurwitz & Merlino, `99] Greiff, Warren, Hurwitz,
Laurie, and Merlino, Andrew.  MITRE TDT-3 segmentation
system.  TDT-3 Topic Detection and Tracking Conference,
Gathersburg, Md, February, 2000.
5. [Kubula, et al, ?00] Kubula, F., Colbath, S.,  Liu, D.,
Srivastava, A. and Makhoul, J.  Integrated technologies for
indexing spoken language, Communication of the ACM, vol.
43, no. 2, Feb., 2000.
6. [Liu, ?00] Liu, Tiecheng and Kender, John R.  A hidden
Markov model approach to the structure of documentaries.
Proceedings of the IEEE Workshop on Content-based
Access of Image and Video Libraries, 2000.
7. [Loader, `99] Loader, C.  Local Regression and Likelihood.
Springer, Murray Hill, N.J., 1999.
8. [Maybury, Merlino & Morey ?97] Maybury, M., Merlino, A.
Morey, D.  Broadcast news navigation using story
segments. Proceedings of the ACM International
Multimedia Conference, Seattle, WA, Nov., 1997.
9. [Nist, ?00] Topic Detection and Tracking (TDT-3) Evaluation
Project.  http://www.nist.gov/speech/tests/tdt/tdt99/.
10. [Ponte and Croft, ?97] Ponte, J.M. and Croft, W.B.  Text
segmentation by topic, Proceedings of the First European
Conference on Research and Advanced Technology for
Digital Libraries, pp. 120--129, 1997.
11. [Rabiner, `89] L. R. Rabiner, A tutorial on hidden Markov
models and selected applications in speech recognition.
Proceedings of the IEEE, vol. 37, no. 2, pp. 257-86,
February, 1989.
12. [Scott, ?92] David W. Scorr, Boundary kernels, Multivariate
Density Estimation: Theory and Practice, pp 146-149, 1992.
13. [Venables & Ripley, `99]  Venables, W. N. and Ripley, B. D.
Modern Applied Statistics with S-PLUS.  Springer, Murray
Hill, N.J., 1999.
14. [Xu, J. and Croft, ?96] Xu, J. and Croft, W.B., Query
expansion using local and global document analysis,
Proceedings of the Nineteenth Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pp. 4--11, 1996
15. [Yamron, et al, ?98] Yamron, J. P., Carp, I., Gillick, L., Lowe,
S. and van Mulbregt, P.  A Hidden Markov Model approach
to text segmentation and event tracking.  Proceedings
ICASSP-98, Seattle, WA. May, 1998.
Audio Hot Spotting and Retrieval Using Multiple Features  
 
 
Qian Hu 
MITRE Corporation 
qian@mitre.org 
Fred Goodman, Stanley Boykin, 
Randy Fish, Warren Greiff 
MITRE Corporation 
fgoodman@mitre.org, 
sboykin@mitre.org, 
fishr@mitre.org 
greiff@mitre.org 
 
Abstract 
 
This paper reports our on-going efforts to 
exploit multiple features derived from an 
audio stream using source material such as 
broadcast news, teleconferences, and 
meetings. These features are derived from 
algorithms including automatic speech 
recognition, automatic speech indexing, 
speaker identification, prosodic and audio 
feature extraction. We describe our research 
prototype ? the Audio Hot Spotting System ? 
that allows users to query and retrieve data 
from multimedia sources utilizing these 
multiple features.  The system aims to 
accurately find segments of user interest, i.e., 
audio hot spots within seconds of the actual 
event.  In addition to spoken keywords, the 
system also retrieves audio hot spots by 
speaker identity, word spoken by a specific 
speaker, a change of speech rate, and other 
non-lexical features, including applause and 
laughter.  Finally, we discuss our approach to 
semantic, morphological, phonetic query 
expansion to improve audio retrieval 
performance and to access cross-lingual data. 
 
1. Introduction 
 
Audio contains more information than is 
conveyed by the text transcript produced by 
an automatic speech recognizer [Johnson et 
al., 2000; Hakkani-Tur et al, 1999]. 
Information such as: a) who is speaking, b) 
the vocal effort used by each speaker, and c) 
prosodic features and certain non-speech 
background sounds, are lost in a simple 
speech transcript. In addition, due to the 
variability of acoustic channels and noise 
conditions, speaker variance, language 
models the recognizer is based on, and the 
limitations of automatic speech recognition 
(ASR), speech transcripts can be full of 
errors.  Deletion errors can prevent the users 
from finding what they are looking for from 
audio or video data, while insertion and 
substitution errors can be misleading and 
confusing.  Our approach is to automatically 
detect, index, and retrieve multiple features 
from the audio stream to compensate for the 
weakness of using speech transcribed text 
alone.  The multiple time-stamped features 
from the audio include an automatically 
generated index derived from ASR speech 
transcripts, automatic speaker identification, 
and automatically identified prosodic and 
audio cues. In this paper, we describe our 
indexing algorithm that automatically 
identifies potential search keywords that are 
information rich and provide a quick clue to 
the document content. We also describe how 
our Audio Hot Spotting prototype system 
uses multiple features to automatically locate 
regions of interest in an audio or video file 
that meet a user?s specified query criteria.  In 
the query, users may search for keywords or 
phrases, speakers, keywords and speakers 
together, non-verbal speech characteristics, 
or non-speech signals of interest. The system 
also uses multiple features to refine query 
results. Finally, we discuss our query 
expansion mechanism by using natural 
language processing techniques to improve 
retrieval performance and to access cross-
lingual data. 
 
 
2. Data 
 
We use a variety of multimedia data for the 
experiments in order to test Audio Hot Spotting 
algorithms and performance under different 
acoustic and noise environments.  These include 
broadcast news (e.g. HUB4), teleconferences, 
meetings, and MITRE corporate multimedia 
events.  In some cases, synthetic noise was added 
to clean source material to test algorithm 
robustness. 
 
3. Automatic Spoken Keyword 
Indexing 
 
As automatic speech recognition is imperfect, 
automatic speech transcripts contain errors.  Our 
indexing algorithm focuses on finding words that 
are information rich (i.e. content words) and 
machine recognizable.  Our approach is based on 
the principle that short duration and weakly 
stressed words are much more likely to be mis-
recognized, and are less likely to be important.  
To eliminate words that are information poor and 
prone to mis-recognition, our algorithm examines 
the speech recognizer output and creates an index 
list of content words. The index-generation 
algorithm takes the following factors into 
consideration: a) absolute word length by its 
utterance duration, b) the number of syllables, c) 
the recognizer?s own confidence score, and d) the 
part of speech (i.e. verb, noun) using a POS 
tagger with some heuristic rules. Experiments we 
have conducted using broadcast news data, with 
Gaussian white noise added to achieve a desired 
Signal-to-Noise Ratio (SNR), indicate that the 
index list produced typically covers about 10% of 
the total words in the ASR output, while more 
than 90% of the indexed words are actually 
spoken and correctly recognized given a Word 
Error Rate (WER [Fiscus, et al]) of 30%. The 
following table illustrates the performance of the 
automatic indexer as a function of Signal-to-
Noise Ratio during a short pilot study. 
 
 
 
 
 
 
 
SNR 
(dB) 
ASR 
WER 
(%) 
Index 
Coverage 
(%) 
 
IWER 
(%) 
Orig. 26.8 13.6 4.3 
24 32.0 12.3 3.3 
18 39.4 10.8 5.9 
12 54.7 8.0 12.2 
6 75.9 3.4 20.6 
3 87.9 1.4 41.7 
 
Table 1 Indexer SNR Performance 
where Index Coverage is the fraction of the words 
in the transcript chosen as index words and IWER 
is the index word error rate. 
 
As expected, increases in WER result in fewer 
words meeting the criteria for the index list.  
However, the indexer algorithm manages to find 
reliable words even in the presence of very noisy 
data.  At 12dB SNR, while the recognizer WER 
has jumped up to 54.7%, the Index Word Error 
Rate (IWER) has risen to 12.2%. Note that an 
index-word error indicates that an index word 
chosen from the ASR output transcript did not in 
fact occur in the original reference transcription. 
 
Whether this index list is valuable will 
depend on the application. If a user wants to get a 
feel for a 1-hour conversation in just a few 
seconds, automatically generated topic terms such 
as those described in [Kubala  et al, 2000] or an 
index list such as this could be quite valuable. 
 
 
4. Detecting and Using Multiple 
Features from the Audio 
 
Automatic speech recognition has been 
used extensively in spoken document retrieval 
[Garofolo et al, 2000; Rendals et al, 2000].  
However, high speech WER in the speech 
transcript, especially in less-trained domains such 
as spontaneous and non-broadcast quality data, 
greatly reduces the effectiveness of navigation 
and retrieval using the speech transcripts alone.  
Furthermore, the retrieval of a whole document or 
a story still requires the user to read the whole 
document or listen to the entire audio file in order 
to locate the segments where relevant information 
resides.  In our approach, we recognize that there 
is more information in the audio file than just the 
words and that other attributes such as speaker 
identification, prosodic features, and the type of 
background noise may also be helpful for the 
retrieval of information. In addition, we aim to 
retrieve the exact segments of interest rather than 
the whole audio or document so that the user can 
zero in on these specific segments rapidly. One of 
the challenges facing researchers is the need to 
identify "which" non-lexical features have 
information value.  Since these features have not 
been available to users in the past, they don't 
know enough to ask for them.  We have chosen to 
implement a variety of non-lexical cues with the 
intent of stimulating feedback from our user 
community. 
  
As an example of this, by extending a 
research speaker identification algorithm 
[Reynolds, 1995], we integrated speaker 
identification into the Audio Hot Spotting 
prototype to allow a user to retrieve three kinds of 
information.  First, if the user cannot find what 
he/she is looking for using keyword search but 
knows who spoke, the user can retrieve content 
defined by the beginning and ending timestamps 
associated with the specified speaker; assuming 
enough speech exists to build a model for that 
speaker.  Secondly, the system automatically 
generates speaker participation statistics 
indicating how many turns each speaker spoke 
and the total duration of each speaker?s audio.  
Finally, the system uses speaker identification to 
refine the query result by allowing the user to 
query keywords and speaker together.  For 
example, using the Audio Hot Spotting prototype, 
the user can find the audio segment in which 
President Bush spoke the word ?anthrax".   
 
In addition to speaker identification, we 
wanted to illustrate the information value of other 
non-lexical sounds in the audio track.  As a proof-
of-concept, we created detectors for crowd 
applause and laughter. The algorithms used both 
spectral information as well as the estimated 
probability density function (pdf) of the raw audio 
samples to determine when one of these situations 
was present. Laughter has a spectral envelope 
which is similar to a vowel, but since many 
people are voicing at the same time, the audio has 
no coherence. Applause, on the other hand, is 
spectrally speaking, much like noisy speech 
phones such as ?sh? or ?th.? However, we 
determined that the pdf of applause differed from 
those individual sounds in the number of high 
amplitude outlier samples present.  Applying this 
algorithm to the 2003 State of the Union address, 
we identified all instances of applause with only a 
2.6% false alarm rate (results were compared with 
hand-labeled data).  One can imagine a situation 
where a user would choose this non-lexical cue to 
identify statements that generated a positive 
response. 
 
Last year, we began to look at speech 
rate as a separate feature.  Speech rate estimation 
is important, both as an indicator of emotion and 
stress, as well as an aid to the speech recognizer 
itself (see for example [Mirghafori et al, 1996; 
Morgan, 1998; Zheng et al, 2000]).  Currently, 
recognizer word error rates are highly correlated 
to speech rate.  For the user, marking that a 
returned passage is from an abnormal speech rate 
segment and therefore more likely to contain 
errors allows him/her to save time by ignoring 
these passages or reading them with discretion if 
desired.  However, if passages of high stress are 
of interest, these are just the passages to be 
reviewed.  For the recognizer, awareness of 
speech rate allows modification of HMM state 
probabilities, and even permits different 
sequences of phones.  
 
One approach to determine the speech rate 
accurately is to examine the phone-level output of 
the speech recognizer. Even though the phone-
level error rate is quite high, the timing 
information is still valuable for rate estimation. 
By comparing the phone lengths of the recognizer 
output to phone lengths tabulated over many 
speakers, we have found that a rough estimate of 
speech rate is possible [Mirgafori et al 1996]. 
Initial experiments using MITRE Corporate event 
data have shown a rough correspondence between 
human perception of speed and the algorithm 
output. One outstanding issue is how to treat 
audio that includes both fast rate speech and 
significant silences between utterances. Is this 
truly fast speech?  
 
We are currently conducting research to 
detect other prosodic features by estimating vocal 
effort.  These features may indicate when a 
speaker is shouting suggesting elevated emotions 
or near a whisper. Queries based on such features 
can lead to the identification of very interesting 
audio hot spots for the end user.  Initial 
experiments are examining the spectral properties 
of detected glottal pulses obtained during voiced 
speech.  
 
5. Query Expansion and Retrieval 
Tradeoffs 
5.1  Effect of Passage Length 
TREC SDR found both a linear correlation 
between speech word error rate and retrieval rate 
[Garofolo et al, 2000] and that retrieval was 
fairly robust to WER.  However, the robustness 
was attributed to the fact that misrecognized 
words are likely to also be properly recognized in 
the same document if the document is long 
enough.  Since we limit our returned passages to 
roughly 10 seconds, we do not benefit from this 
full-document phenomenon.  The relationship 
between passage retrieval rate and passage length 
was studied by searching 500 hours of broadcast 
news from the TREC SDR corpus.  Using 679 
keywords, each with an error rate across the 
corpus of at least 30%, we found that passage 
retrieval rate was 71.7% when the passage was 
limited to only the query keyword.  It increased to 
76.2% when the passage length was increased to 
10sec and rose to 83.8% if the returned passage 
was allowed to be as long as 120sec. 
 
In our Audio Hot Spotting prototype, we 
experimented with semantic, morphological, and 
phonetic query expansion to achieve two 
purposes, 1) to improve the retrieval rate of 
related passages when exact word match fails, and 
2) to allow cross lingual query and retrieval. 
 
5.2  Keyword Query Expansion 
 
The Audio Hot Spotting prototype 
integrated the Oracle 9i Text engine to expand the 
query semantically, morphologically and 
phonetically.  For morphological expansion, we 
activated the stemming function.  For semantic 
expansion, we utilized expansion to include 
hyponyms, hypernyms, synonyms, and 
semantically related terms.  For example, when 
the user queried for "oppose", the exact match 
yielded no returns, but when semantic and 
morphological expansion options are selected, the 
query was expanded to include anti, anti-
government, against, opposed, opposition, and 
returned several passages containing these 
expanded terms.  
 
To address the noisy nature of speech 
transcripts, we used the phonetic expansion, i.e. 
"sound alike" feature from the Oracle database 
system.  This is helpful especially for proper 
names.  For example, if the proper name Nesbit is 
not in the speech recognizer vocabulary, the word 
will not be correctly transcribed.  In fact, it was 
transcribed as Nesbitt (with two 't's).  By phonetic 
expansion, Nesbit is retrieved.  We are aware of 
the limitations of Oracle?s phonetic expansion 
algorithms, which are simply based on spelling.  
This doesn?t work well when text is a mis-
transcription of the actual speech.  Hypothetically, 
a phoneme-based recognition engine may be a 
better candidate for phonetic query expansion.  
We are currently evaluating a phoneme-based 
audio retrieval system and comparing its 
performance with a word-based speech 
recognition system.  The comparison will help us 
to determine the strengths and weaknesses of each 
system so that we can leverage the strength of 
each system to improve audio retrieval 
performance. 
 
Obviously more is not always better.  
Some of the expanded queries are not exactly 
what the users are looking for, and the number of 
passages returned increases.  In our Audio Hot 
Spotting implementation we made query 
expansion an option allowing the user to choose 
to expand semantically and/or, morphologically, 
or phonetically.   
 
5.3   Cross-lingual Query Expansion 
 
In some applications it is helpful for a 
user to be able to query in a single language and 
retrieve passages of interest from documents in 
several languages. We treated translingual search 
as another form of query expansion.  We created a 
bilingual thesaurus by augmenting Oracle's 
default English thesaurus with Spanish dictionary 
terms.  With this type of query expansion 
enabled, the system retrieves passages that 
contain the keyword in either English or Spanish.  
A straightforward extension of this approach will 
allow other languages to be supported.  
 
6.   Future Directions 
 
As our research and prototype evolve, we 
plan to develop algorithms to detect more 
meaningful prosodic and audio features to allow 
the users to search for and retrieve them. We are 
also developing algorithms that can generate 
speaker identify in the absence of speaker training 
data.  For example, given an audio script, we 
expect the algorithms to automatically identify 
the number of different speakers present and the 
time speaker X changes to Y.  For semantic query 
expansion, we are considering using more 
comprehensive thesauri and local context analysis 
to locate relevant segments to compensate for 
high ASR word error rate.  We are also 
considering combining a word-based speech 
recognition system with a phoneme-based system 
to improve the retrieval performance especially 
for out of vocabulary words and multi-word 
queries. 
7.   Conclusion 
In this paper, we have shown that by 
automatically detecting multiple audio features 
and making use of these features in a relational 
database, our Audio Hot Spotting prototype 
allows a user to begin to apply the range of cues 
available in audio to the task of multi-media 
information retrieval.  Areas of interest can be 
specified using keywords, phrases, speaker 
identity, prosodic features, and information-
bearing background sounds, such as applause and 
laughter.  When matches are found, the system 
displays the recognized text and allows the user to 
play the audio or video in the vicinity of the 
identified "hot spot".  With the advance of 
component technologies such as automatic speech 
recognition, speaker identification, and prosodic 
and audio feature extraction, there will be a wider 
array of audio features for the multimedia 
information systems to query and retrieve, 
allowing the user to access the exact information 
desired rapidly. 
 
 
 
References 
 
1. John Garofolo, et al Nov., 2000. The TREC 
Spoken Document Retrieval Track: A Successful 
Story.  TREC 9. 
 
2. Julia Hirschberg, Steve Whittaker, Don Hindle, 
Fernando Pereira and Amit Singhal. April 1999. 
Finding Information In Audio: A New Paradigm 
For Audio Browsing/Retrieval, ESCA ETRW 
workshop Accessing information in spoken audio, 
Cambridge. 
 
3. Sue Johnson, Pierre Jourlin, Karen Sparck 
Jones, and Philip Woodland. Nov., 2000.  Spoken 
Document Retireval for TREC-9 at Cambridge 
University. TREC-9. 
 
4. John Fiscus, et al Speech Recognition Scoring 
Toolkit   (http://www.nist.gov/speech/tools/) 
 
5. D. Hakkani-Tur, G. Tur, A.Stolcke, E. Shriberg. 
Combining Words and Prosody for Information 
Extraction from Speech. Proc. EUROSPEECH'99, 
 
6.. N. Mirghafori, E. Fosler, and N. H. Morgan. 
Towards Robustness to Fast Speech in ASR, 
Proc. ICASSP, Atlanta, GA, May 1996.  
   
7. N. Morgan and E. Fosler-Lussier. Combining 
Multiple Estimators of peaking Rate,  Proc. 
ICASSP-98, pp. 729-732, Seattle, 1998  
 
8. M. D. Plumpe, T. F. Quatieri, and D. A. 
Reynolds. Modeling of the Glottal Flow 
Derivative Waveform with Application to 
Speaker Identification, IEEE Trans. On Speech 
and Audio Processing, September 1999. 
 
9. S. Rendals, and D. Abberley. The THISL SDR 
System at TREC-9.  TREC-9, Nov., 2000. 
 
10. K. N. Stevens and H. .M. Hanson. 
Classification of Glottal Vibration from Acoustic 
Measurements. In Vocal Fold Physiology: Voice 
Quality Control, Fujimura O., Hirano H. (eds.), 
Singular Publishing Group, San Diego, 1995. 
 
11. J. Zheng, H. Franco, F. Weng, A. Sankar and 
H. Bratt.. Word-level Rate-of-Speech Modeling 
Using Rate-Specific Phones and Pronunciations, 
Proc. ICASSP, vol 3, pp 1775-1778, 2000 
 
12. F. Kubala, S. Colbath, D. Liu, A. Srivastava, J. 
Makhoul. Integrated Technologies For Indexing 
Spoken Language, Communications of the ACM, 
February 2000. 
 
13. D. Reynolds. Speaker Identification And 
Verification Using Gaussian Mixture Speaker 
Models, Speech Communications, vol.17, pp.91, 
1995 
 
