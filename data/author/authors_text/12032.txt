Fine-Grained Hidden Markov Modeling for Broadcast-
News Story Segmentation
Warren Greiff, Alex Morgan, Randall Fish, Marc Richards, Amlan Kundu,
MITRE Corporation
202 Burlington Road
Bedford, MA 01730-1420
(greiff, amorgan, fishr, marc, akundu)@mitre.org
ABSTRACT
We present the design and development of a Hidden Markov
Model for the division of news broadcasts into story segments.
Model topology, and the textual features used, are discussed,
together with the non-parametric estimation techniques that were
employed for obtaining estimates for both transition and
observation probabilities.  Visualization methods developed for
the analysis of system performance are also presented.
1. INTRODUCTION
Current technology makes the automated capture, storage,
indexing, and categorization of broadcast news feasible allowing
for the development of computational systems that provide for the
intelligent browsing and retrieval of news stories [Maybury,
Merlino & Morey ?97; Kubula, et al, ?00].  To be effective, such
systems must be able to partition the undifferentiated input signal
into the appropriate sequence of news-story segments.
In this paper we discuss an approach to segmentation based on the
use of a fine-grained Hidden Markov Model [Rabiner, `89] to
model the generation of the words produced during a news
program.  We present the model topology, and the textual features
used.  Critical to this approach is the application of non-parametric
estimation techniques, employed to obtain robust estimates for
both transition and observation probabilities. Visualization
methods developed for the analysis of system performance are
also presented.
Typically, approaches to news-story segmentation have been
based on extracting features of the input stream that are likely to
be different at boundaries between stories from what is observed
within the span of individual stories. In [Beeferman, Berger, &
Lafferty ?99], boundary decisions are based on how well
predictions made by a long-range exponential language model
compare to those made by a short range trigram model. [Ponte and
Croft, ?97] utilize Local Context Analysis [Xu, J. and Croft, ?96]
to enrich each sentence with related words, and then use dynamic
programming to find an optimal boundary sequence based on a
measure of word-occurrence similarity between pairs of enriched
sentences. In [Greiff, Hurwitz & Merlino, `99], a na?ve Bayes
classifier is used to make a boundary decision at each word of the
transcript.  In [Yamron, et al, ?98], a fully connected Hidden
Markov Model is based on automatically induced topic clusters,
with one node for each topic.  Observation probabilities for each
node are estimated using smoothed unigram statistics.
The approach reported in this paper goes further along the lines of
find-grained modeling in two respects: 1) differences in feature
patterns likely to be observed at different points in the
development of a news story are exploited, in contrast to
approaches that focus on boudary/no-boundary differences; and 2)
a more detailed modeling of the story-length distribution profile,
unique to each news source (for example, see the histogram of
story lengths for ABC World News Tonight shown in the top
graph of Figure 3, below).
2. GENERATIVE MODEL
We model the generation of news stories as a 251 state Hidden
Markov Model, with the topology shown in Figure 1. States
labeled, 1 to 250, correspond to each of the first 250 words of a
story.  One extra state, labeled 251, is included to model the
production of all words at the end of stories exceeding 250 words
in length.
Several other models were considered, but this model is
particularly suited to the features used, as it allows one to model
features that vary with depth into the story (Section 3.1), while
simultaneously, by delaying certain features.  It also allows one to
model features that occur in specific regions the boundaries
(Section 3.3).  This is possible because all states can feed into the
initial state, i.e. all stories end by going into the first word of a
new story.
1 2 3 250 251
Figure 1:  Current HMM Topology
For example, the original model involved a series of beginning
and then end states, with a single middle state that could be cycled
through (Figure 2).  This proved to be a problem because the ends
of long stories were being mixed with the ends of short stories
which led to problems with our spaced coherence feature (Section
3.1).  Another possibility involved splitting the model into two
main paths, one to model the shorter stories, and one to model the
longer as there is something of a bimodal distribution in story
lengths (Figure 4).  However, the fine-grained nature of our model
would suffer from splitting the data in this manner, and a choice
about at which length to fork the model would be somewhat
artificial.
3. FEATURES
Associated with the model is a set of features.  For each state, the
model assigns a probability distribution over all possible
combinations of values the features may take on.  The probability
assigned to value combinations is assumed to be independent of
the state/observation history, conditioned on the state. We further
assume that the value of any one feature is independent of all
others, once the current state is known. Features have been
explicitly designed with this assumption in mind.  Three
categories of features have been used, which we refer to as
coherence features, x-duration feature, and the trigger features.
3
W
sh
w
do
ap
of
st
tr
ra
fe
COHER-4  (Figures 3b, c & d) correspond to similar features; for
these, however, the buffer is separated by 50, 100, and 150 words,
respectively, from the current word.  Interestingly, the COHER-4
feature actually caused a reduction in performance, and was not
used in the final evaluation.
3.2. X-duration
This feature is based on indications given by the speech recognizer
that it was unable to transcribe a portion of the audio signal. The
existence of an untranscribable section prior to the word gives a
non-zero X-DURATION value based on the extent of the section.
Empirically this is an excellent predictor of boundaries in that an
untranscribable event has uniform likelihood of occurring
anywhere in a news story, except prior to the first word of a story,
where it is extremely likely to occur.
3.3. Triggers
Trigger features correspond to small regions at the beginning and
end of stories, and exploit the fact that some words are far more
likely to occur in these positions than in other parts of a news
segment.  One region, for example, is restricted to the first word of
the story.  In ABC?s World News Tonight, for example, the word
?finally? is far more likely to occur in the first word of a story than
would be expected by its general rate of occurrence in the training
data.  For a word, w, appearing in the input stream, the value of
the feature is an estimate of how likely it is for w to appear in the
region of interest.  The estimate used is given by:
( )Rw
Rw
fn
nRwp /1
1)(?
+
+
=? ?
where Rwn ? is the number of times w appeared in R in the training
data; wn  is the total number of occurrences of w; and Rf  is the
fraction of all tokens of w that occurred in the region.  This
estimate can be viewed as Bayesian estimate with a beta prior.
The beta prior is equivalent to a uniform prior and the observation
of one occurrence of the word in the region out of ( )Rf/1  total
occurrences.  This estimate was chosen so that: 1) the prior
probability would not be greatly affected for words observed only
a few times in the training data; 2) it would be pushed strongly
towards the empirical probability of the word appearing in the
region for words that were encountered in R; 3) it has a prior
probability, Rf , equal to the expectation for a randomly selected
word.  The regions used for the submission were restricted to the
one-word regions for: first word, second word, last word, and
1 2 500 501
Figure 3: Original Topology.1. Coherence
a
b
ce have used four coherence features.  The COHER-1 feature,
own schematically in Figure 2a, is based on a buffer of 50
ords immediately prior to the current word.  If the current word
es not appear in the buffer, the value of COHER-1 is 0.  If it does
pear in the buffer, the value is -log(sw/s), where sw is the number
 stories in which the word appears, and s is the total number of
ories, in the training data. Words that did not appear in the
aining data, are treated as having appeared once.  In this way,
re words get high feature values, and common words get low
ature values.  Three other features: COHER-2, COHER-3, and
next-to-last word.  Limited experimentation with multi-state
regions, was not fruitful.  For example, including the regions,
{3,4,?,10} and {-10,-9,?,-3}, where ?i is interpreted as i words
prior to the end of the story, did not improve segmentation
performance.
Since, as described, the current HMM topology does not model
end-of-story words (earlier versions of the topology did model
these states directly), trigger features for end-of-story regions are
delayed. That means that a trigger related to the last word in a
story would be delayed by a one word buffer.  In this way, it is
linked to the first word in the next story.  For example, the word
?Jennings? (the name of the main anchorperson) is strongly
d
Figure 2: Coherence Features
correlated with the last word in news stories in the ABC World
News Tonight corpus.  The estimated probability of it being the
last word of the story in which it appears is .235 (obtained by the
aforementioned method). The trained model associates a high
likelihood of seeing the value .235 at state = 1; the intuitive
interpretation being, "a word highly likely to appear at the last
word of a story, occurred 1-word ago".
4. PARAMETER ESTIMATION
The Hidden Markov Model requires the estimation of transition
and conditional observation probabilities.  There are 251 transition
probabilities to be estimated.  Much more of a problem are the
observation probabilities, there being 9 features in the model, for
each of which a probability distribution over as many as 100
values must be estimated, for each of 251 states.  With the goal of
developing methods for robust estimation in the context of story
segmentation, we have applied non-parametric kernel estimation
techniques, using the LOCFIT library [Loader, ?99] of the R open-
source statistical analysis package, which is based on the S-plus
system [Venables & Ripley,
`99; Chambers & Hastie, `92, Becker, Chambers & Wilks, `88].
For the transition probabilities, it is assumed that the underlying
probability distribution over story length is smooth, allowing the
empirical histogram, shown at the top of Figure 4, to be
transformed to the probability density estimate shown at the
bottom. From this probability distribution over story lengths, the
conditional transition probabilities can be estimated directly.
Conditional observation probabilities are also deduced from an
estimate of the joint probability distribution.  First, observation
values were binned.  Binning limits were set in an attempt to 1) be
large enough to obtain sufficient counts for the production of
robust probability estimates, and yet, 2) be constrained enough so
that important distinctions in the probabilities for different feature
values will be reflected in the model.  For each bin, the
observation counts are smoothed by performing a non-parametric
regression of the observation counts as a function of state.  The
smoothed observations counts corresponding to the regression are
then normalized so as to sum to the total observation count for the
bin.  The result is a conditional probability distribution over states
for a given binned feature value,  p(State=s|Feature=fv).  Once
this is done for all bin values, each conditional probability is
multiplied by the marginal probability, p(State=s), of being in a
given state, resulting in a joint distribution, p(fv,s), over the entire
space of (Feature,State) values.  From this joint distribution, the
necessary conditional probabilities, p(Feature=fv|State=s), can be
deduced directly.
Figure 5 shows the conditional probability estimates, p(fv | s), for
the feature value COHER-3=20, across all states, confirming the
intuition that, while the probability of seeing a value of 20 is small
for all states, the likelihood of seeing it is much higher in latter
parts of a story than it is in early-story states.
5. SEGMENTATION
Once parameters for the HMM have been determined,
segmentation is straightforward.  The Viterbi algorithm [Rabiner,
`89], is employed to determine the sequence of states most likely
to have produced the observation sequence associated with the
broadcast.  A boundary is then associated with each word
produced from State 1 for the maximum likelihood state sequence.
The version of the Viterbi algorithm we have implemented
provides for the specification of ?state-penalty? parameters, which
we have used for the ?boundary state?, state 1. In effect, the
probability for each path in consideration is multiplied by the
value of this parameter (which can be less than, equal to, or
greater than, 1) for each time the path passes through the boundary
state.  Variation of the parameter effectively controls the
?aggressiveness? of segmentation, allowing for tuning system
behavior in the context of the evaluation metric.
6. RESULTS
Preliminary test results of this approach are encouraging.  After
training on all but 15 of the ABC World News Tonight programs
from the TDT-2 corpus [Nist, ?00], a test on the remaining 15
produced a false-alarm (boundary predicted incorrectly)
probability of .11, with a corresponding miss (true boundary not
predicted) probability of .14, equal to the best performance
reported to date, for this news source.
A more intuitive appreciation for the quality of performance can
be garnered from the graphs in Figure 6, which contrast the
segmentation produced by the system (middle) with ground truth
(the top graph), for a typical member of the ABC test set. The x-
axis corresponds to time (in units of word tokens); i.e., the index
of the word produced by the speech recognizer, and the y-axis
Figure 4: Histograms of story lengths (up to 250 words)
-- raw and smoothed --
Figure 5: Likelihood of COHER-3=2 over all states
corresponds to the state of the HMM model. A path passing
through the point (301, 65), for example, corresponds to a path
through the network that produced the 65th word from state 301.
Returns to state=1 correspond to boundaries between stories. The
bottom graph shows the superposition of the two to help illustrate
the agreement between the path chosen by the system and the path
corresponding to perfect segmentation..
7. VISUALIZATION
The evolution of the segmentation algorithm was driven by
analysis of the behavior of the system, which was supported by
visualization routines developed using the graphing capability of
the R package.  Figure 7 gives an example of the kind of graphical
displays that were used for analysis of the segmentation of a
specific broadcast news program; in this case, analysis of the role
of the X-DURATION feature.  This graphical display allows for
the comparison of the maximum likelihood path produced by the
HMM to the path through the HMM that would be produced by a
perfect system ? one privy to ground-truth.
T
sh
gr
co
po
ha
fr
th
hi
lo
T
li
ge
sy
D
tr
V
st
X
t gative
p n that
r e true
p deling.
T system
p of the
e
Figure 6: Perfohe true state than from the predicted state.  Strongly ne
oints are a major component of the probability calculatio
esulted in the system preferring the path it chose over th
ath.  These points suggest potential deficiencies in the mo
heir identification directs the focus of analysis so that 
erformance can be improved by correcting weaknesses 
xisting model.
rmancehe
ies
og
ful
ut
ed
 is
he
ith
tal
ue
ce
odhe top graph corresponds to the bottom graph of Figure 6,
owing the states traversed by the two systems.  The second
aph shows the value of the X-DURATION feature
rresponding to each word of the broadcast. So, the plotting of a
int at (301, 3) corresponds to an X-DURATION value of 3
ving been observed at time, 301. One thing that can be seen
om this graph is that being at a story boundary (low-points on
e thicker-darker line of the top graph) is more frequent when
gher values of the X-DURATION cue are observed, than when
wer values are observed, as could be expected.
he third graph shows, on a log scale, how many times more
kely it is that the observed X-DURATION value would be
nerated from the true state than from the state predicted by the
stem.  Most points are close to 0, indicating that the X-
URATION value observed was as likely to have come from the
ue state as it is to have come from the state predicted by the
iterbi algorithm.  Of course, this is the case wherever the true
ate has been correctly predicted. Negative points indicate that the
-DURATION value observed is less likely to be produced from
The final graph shows the cumulative sum of the values from t
graph above it. (Note that the sum of the logs of the probabilit
is equivalent to the cumulative product of probabilities on a l
scale.)  The graphing of the cumulative sum can be very use
when the system is performing poorly due to a small b
consistent preference for the observations having been produc
by the state sequence chosen by the system.  This phenomenon
made evident by a steady downward trend in the graph of t
cumulative sum.  This is in contrast to an overall level trend w
occasional downward dips.  Note, that a similar graph for the to
probability (equal to the product of all the individual feature val
probabilities) will always have an overall downward trend, sin
the maximum likelihood path will always have a likeliho
Figure 7: Visualization for x-duration feature
greater than the likelihood of any other path.
Aside from supporting the detailed analysis of specific features,
the productions of these graphs for each of the features, together
with the corresponding graph for the total observation probability,
allowed us to quickly asses which of the features was most
problematic at any given stage of model development.
8. FURTHER WORK
It should be kept in mind that experimentation with this approach
has been based on relatively primitive features ? our focus, to this
point, having been on the development of the core segmentation
mechanism.  Features based on more sophisticated extraction
techniques, which have been reported in the literature ? for
example, the use of exponential models for determining trigger
cues used in [Beeferman, Berger, & Lafferty ?99] ? can easily be
incorporated into this general framework.  Integration of such
techniques can be expected to result in significant further
improvement in segmentation quality.
To date, the binning method described has given much better
results than two dimensional kernel density estimation techniques
which we also attempted to employ.  One of the main difficulties
with using traditional kernel density estimation techniques is that
they tend to inaccurately estimate the density at areas of
discontinuity, such as state=1 in our model and our trigger
features.  Preliminary work with boundary kernels [Scott, ?92] is
very promising.  It is certainly an area worthy of more in-depth
investigation.
Work done by another group [Liu, ?00] to segment documentaries
based on video cues alone has been moderately successful in the
past.  We engineered a neural network in an attempt to identify
video frames containing an anchorperson, a logo, and blank
frames, with a belief that these are all features that would contain
information about story boundaries.  Preliminary work was also
done to extract features directly from the audio signal, such as
trying to identify speaker change. Initial work with the audio and
video has been unable to aid in segmentation, but we feel this is
also an area worth continuing to pursue.
9. REFERENCES
1. [Becker, Chambers & Wilks, `88] Becker, Richard A.,
Chambers, John M., and Wilks, Allan R.  The New S
Language.  Wadsworth & Brooks/Cole, Pacific Grove, Cal.
2. [Beeferman, Berger, & Lafferty ?99] D. Beeferman, D., A.
Berger, A. and Lafferty, J.  Statistical models for text
segmentation.  Machine Learning, vol. 34, pp. 1-34, 1999.
3. [Chambers & Hastie, `88] Chambers, John M. and Hastie,
Trevor, J.  Statistical Models in S.  Wadsworth &
Brooks/Cole, Pacific Grove, Cal., 1988.
4. [Greiff, Hurwitz & Merlino, `99] Greiff, Warren, Hurwitz,
Laurie, and Merlino, Andrew.  MITRE TDT-3 segmentation
system.  TDT-3 Topic Detection and Tracking Conference,
Gathersburg, Md, February, 2000.
5. [Kubula, et al, ?00] Kubula, F., Colbath, S.,  Liu, D.,
Srivastava, A. and Makhoul, J.  Integrated technologies for
indexing spoken language, Communication of the ACM, vol.
43, no. 2, Feb., 2000.
6. [Liu, ?00] Liu, Tiecheng and Kender, John R.  A hidden
Markov model approach to the structure of documentaries.
Proceedings of the IEEE Workshop on Content-based
Access of Image and Video Libraries, 2000.
7. [Loader, `99] Loader, C.  Local Regression and Likelihood.
Springer, Murray Hill, N.J., 1999.
8. [Maybury, Merlino & Morey ?97] Maybury, M., Merlino, A.
Morey, D.  Broadcast news navigation using story
segments. Proceedings of the ACM International
Multimedia Conference, Seattle, WA, Nov., 1997.
9. [Nist, ?00] Topic Detection and Tracking (TDT-3) Evaluation
Project.  http://www.nist.gov/speech/tests/tdt/tdt99/.
10. [Ponte and Croft, ?97] Ponte, J.M. and Croft, W.B.  Text
segmentation by topic, Proceedings of the First European
Conference on Research and Advanced Technology for
Digital Libraries, pp. 120--129, 1997.
11. [Rabiner, `89] L. R. Rabiner, A tutorial on hidden Markov
models and selected applications in speech recognition.
Proceedings of the IEEE, vol. 37, no. 2, pp. 257-86,
February, 1989.
12. [Scott, ?92] David W. Scorr, Boundary kernels, Multivariate
Density Estimation: Theory and Practice, pp 146-149, 1992.
13. [Venables & Ripley, `99]  Venables, W. N. and Ripley, B. D.
Modern Applied Statistics with S-PLUS.  Springer, Murray
Hill, N.J., 1999.
14. [Xu, J. and Croft, ?96] Xu, J. and Croft, W.B., Query
expansion using local and global document analysis,
Proceedings of the Nineteenth Annual International ACM
SIGIR Conference on Research and Development in
Information Retrieval, pp. 4--11, 1996
15. [Yamron, et al, ?98] Yamron, J. P., Carp, I., Gillick, L., Lowe,
S. and van Mulbregt, P.  A Hidden Markov Model approach
to text segmentation and event tracking.  Proceedings
ICASSP-98, Seattle, WA. May, 1998.
Gene Name Extraction Using FlyBase Resources 
Alex Morgan 
amorgan@mitre.org 
Lynette Hirschman 
lynette@mitre.org 
The MITRE Corporation 
202 Burlington Road 
Bedford, MA 01730-1420 
Alexander Yeh 
asy@mitre.org 
Marc Colosimo 
mcolosim@brandeis.edu  
 
 
Abstract 
Machine-learning based entity extraction re-
quires a large corpus of annotated training to 
achieve acceptable results.  However, the cost 
of expert annotation of relevant data, coupled 
with issues of inter-annotator variability, 
makes it expensive and time-consuming to 
create the necessary corpora. We report here 
on a simple method for the automatic creation 
of large quantities of imperfect training data 
for a biological entity (gene or protein) extrac-
tion system. We used resources available in 
the FlyBase model organism database; these 
resources include a curated lists of genes and 
the articles from which the entries were 
drawn, together a synonym lexicon.  We ap-
plied simple pattern matching to identify gene 
names in the associated abstracts and filtered 
these entities using the list of curated entries 
for the article.  This process created a data set 
that could be used to train a simple Hidden 
Markov Model (HMM) entity tagger. The re-
sults from the HMM tagger were comparable 
to those reported by other groups (F-measure 
of 0.75). This method has the advantage of be-
ing rapidly transferable to new domains that 
have similar existing resources. 
1 
                                                          
Introduction: Biological Databases 
 
There is currently an information explosion in 
biomedical research.  The growth of literature is 
roughly exponential, as can be seen in Figure 1 
which shows the number of literature references in 
FlyBase1 organized by date of publication over a 
hundred year span.2  This growth of literature 
makes it daunting for researchers to keep track of 
the information, even in very small subfields of 
biology. 
1 FlyBase is a database that focuses on research in the genetics 
and molecular biology of the fruit fly (Drosophila melangas-Figure 1: FlyBase References, 1900-2000 
 
Increasingly, biological databases serve to collect 
and organize published experimental results.  A 
wide range of biological databases exist, including 
model organism databases (e.g., for mouse3 and 
yeast4) as well as various protein databases (e.g., 
Protein Information Resource5 (PIR) or SWISS-                                                                                          
tor), a model organism for genetics research: 
http://www.flybase.org. 
PROT6 and   interaction databases such as the 
Biomolecular Interaction Network Database7 
(BIND). These databases are created by a process 
of curation, which is done by Ph.D. biologists who 
read the published literature to cull experimental 
findings and relations. These facts are organized 
into a set of structured fields of a database and 
 
2 Of course most of these early references in FlyBase are not 
in electronic form. The FlyBase database has been in existence 
since 1993. 
3 http://www.informatics.jax.org/ 
4 http://genome-www.stanford.edu/Saccharomyces/ 
5 http://pir.georgetown.edu/pirwww/pirhome3.shtml 
6 http://us.expasy.org/sprot/ 
7 http://www.bind.ca/ 
linked to the source of information (the journal 
article).  As a result, curation is a time-consuming 
and expensive process; database curators are in-
creasingly eager to adopt text mining and natural 
language processing techniques to make curation 
faster and more consistent. As a result, there has 
been growing interest in the application of entity 
extraction and text classification techniques to the 
problem of biological database curation [Hirsch-
man02]. 
2 
                                                          
Entity Extraction Methods 
There are two approaches to entity extraction.  The 
first requires manual or heuristic creation of rules 
to identify the names mentioned in text; the second 
uses machine learning to create the rules that drive 
the entity tagging. Heuristic systems require expert 
developers to create the rules, and these rules must 
be manually changed to handle new domains. Ma-
chine-learning based systems are dependent on 
large quantities of tagged data, consisting of both 
positive and negative examples.8  Figure 2 shows 
results from the IdentiFinder system [Bikel99] il-
lustrating that performance increases roughly with 
the log of quantity of training data. Given the ex-
pense of manual annotation of large quantities of 
data, the challenge for the machine learning ap-
proach is to find ways of creating sufficient quanti-
ties of training data cheaply. 
     Overall, hand-crafted systems seem to outper-
form learning-based systems for biology. How-
ever, it is clear that the quantities of training have 
been small, relative to the results reported for en-
tity extraction in e.g., newswire [Hirschman03]. 
There are several published sets of performance 
results for automatic named biological entity ex-
traction systems.  The system of Collier et al [Col-
lier00] uses a hidden Markov model to achieve an 
F-measure9 of 0.73 when trained on a corpus of 
29,940 words of text from 100 MEDLINE ab-
stracts.   Contrast this with Figure 2, which reports 
results using over 600,000 words of training data, 
and an F-measure of 0.95 for English newswire 
entity extraction (and 0.91 for Spanish).   
                                                          
8 For negative examples, the "closed world" assumption gen-
erally is taken to apply: if an entity is not tagged, it is assumed 
to be a negative example. 
 
Krauthammer et al [Krauthammer00] have taken a 
somewhat different approach which encodes char-
acters as 4-tuples of  DNA bases; they then use 
BLAST together with a lexicon of gene names to 
search for 'gene name homologies'. They report an 
F-measure of 0.75 without the use of a large set of 
rules or annotated training data. 
 
The PASTA system [Gaizauskas03] uses a combi-
nation of heuristic and machine-learned rules to 
achieve a higher F-measure over a larger number 
of classes: F-measure of 0.83 for the task of identi-
fying 12 classes of entities involved in the descrip-
tion of roles of residues in protein molecules. 
Because they used heuristic rules, they were able 
to get these results with a relatively small training 
corpus of 52 MEDLINE abstracts (roughly 12,000 
words). 
Figure 2: Performance of BBN's IdentiFinder named entity 
recognition system relative to the amount of training data, from 
[Bikel99] 
 
These results suggest that machine learning meth-
ods will not be able to compete with heuristic rules 
until there is a way to generate large quantities of 
annotated training data. Biology has the advantage 
that there are rich resources available, such as lexi-
cons, ontologies and hand-curated databases.  
What is missing is a way to convert these into 
training corpora for text mining and natural lan-
guage processing.  Craven and Kumlien [Cra-
ven99] developed an innovative approach that used 
fields in a biological database to locate abstracts 
which mention physiological localization of pro-
teins. Then via a simple pattern matching algo-
9 
Recall) (Precision
Recall)Precision2(
+
??=F   
Manning D, Schutze H. Foundations of Statistical Natural 
Language Processing, 2002: p 269. 
rithm, they identified those sentences where the 
relation was mentioned and matched these with 
entries in the Yeast Protein Database (YPD).  In 
this way, they were able to automatically create an 
annotated gold standard, consisting of sentences 
paired with the curated relations derived from 
those sentences. They then used these for training 
and testing a machine-learning based system.  This 
approach inspired our interest in using existing 
resources to create an annotated corpus automati-
cally.   
3 
r 
3.1 
                                                          
FlyBase: Organization and Resources 
We focused on FlyBase because we had access to 
FlyBase resources from our work in the creation of 
the KDD 2002 Cup Challenge Task 1 [Yeh03].   
Through this work, we had become familiar with 
the multi-stage process of curation.  An early task 
in the curation pipeline is to determine, for a given 
article, whether there are experimental results that 
need to be added to the database. This was the task 
used as the basis for the KDD text data mining 
"challenge evaluation". A later task in the pipeline 
creates a list of the Drosophila genes discussed in 
each curated article. This is the task we focus on in 
this paper.   
 
An example of a FlyBase entry can be seen in Fig-
ure 3 which shows part of the record for the gene 
Toll. Under Molecular Function and Biological 
Process we see that the gene is responsible for en-
coding a transmembrane receptor protein involved 
in antimicrobial humoral response (part of the 
innate immune system of the fly).  We see furthe
that  ?Tl? and ?CG5490? are synonymous for Toll 
(top of the entry next to Symbol), and the link 
Synonyms leads to a long synonym list which in-
cludes: ?Fs(1)Tl?, ?dToll?, ?CT17414?, ?Toll-1?, 
?Fs(3)Tl?, ?mat(3)9?, ?mel(3)10?, and ?mel(3)9?.  
Many of these facts about Toll are linked to a par-
ticular literature reference in the database.  For ex-
ample, following the link for Transcripts will lead 
to a page with links to the abstract of a paper by 
Tauszig et al [Tauszig00] which reports on ex-
periments which measured the lengths of RNA 
transcribed from the Toll gene. 
 
For FlyBase, Drosophila genes are the key bio-
logical entities; each entity (e.g., gene) is associ-
ated with a unique identifier for the underlying 
physical entity. If there were a one-to-one relation-
ship between gene name and unique identifier, the 
gene identification task would be straightforward.  
However, both polysemy and synonymy occur fre-
quently in the naming of biological entities, and 
the gene names of Drosophila are considered to be 
particularly problematic because of creative nam-
ing conventions10.  For example, ?18 wheeler?, 
?batman?, and ?rutabaga? are all Drosophila gene 
names. A single entity (as represented by a unique 
identifier) may have a number of names like Toll 
or even ATP?, which has 38 synonyms listed in 
FlyBase.     
 
Figure 3: FlyBase entry for Toll 
 
Resources 
We obtained a copy of part the FlyBase database,11 
including the lists of genes discussed in each paper 
examined by the curators.  Using the BioPython12 
modules, we were able to obtain MEDLINE ab-
stracts for 15,144 for these papers.  We decided to 
10 At the other end of the spectrum is the yeast nomenclature 
which is strictly controlled ? see <http://genome- 
www.stanford.edu/Saccharomyces/gene_guidelines.shtml> for 
nomenclature conventions. 
11 Special thanks to William Gelbart, David Emmert, Beverly 
Matthews, Leyla Bayraktaroglu, and Don Gilbert. 
12 http://www.biopython.org/ 
set aside the same articles used in the KDD Cup 
Challenge [Yeh03] for evaluation purposes.  This 
left a training set of 14,033 abstracts, consisting of 
a total of 2,664,324 lexemes identified by our 
tokenizer. 
4 
4.1 
                                                          
 
It was only with some reluctance that we decided 
to focus on journal abstracts. From our earlier 
work, we recognized that the majority of the in-
formation entered into FlyBase is missing from the 
abstracts and can be found only in the full text of 
the article [Hirschman03]. However, due to copy-
right restrictions, there is a paucity of freely avail-
able full text for journal articles.  What articles are 
available in electronic form vary in their format-
ting, which can cause considerable difficulty in 
automatic processing. MEDLINE abstracts have a 
uniform format and are readily available. Many 
other experiments have been performed on 
MEDLINE abstracts for similar reasons. 
 
We also created a synonym lexicon from FlyBase.  
We found 35,971 genes with associated ?gene 
symbols? (e.g. Tl is the gene symbol for Toll) and 
48,434 synonyms; therefore, each gene has an av-
erage of 2.3 alternate naming forms, including the 
gene symbol.  The lexicon also allowed us to asso-
ciate each gene with one a unique FlyBase gene 
identifier, providing "term normalization." 
Experiments 
For purposes of evaluation, our task was the identi-
fication of mentions of Drosophila genes in the 
text of abstracts.  We also included mentions of 
protein or transcript where the associated gene 
shared the same name. This occurs when, for ex-
ample, the gene name appears as a pre-nominal 
modifier, as in "the zygotic Toll protein".  We did 
not include mentions of protein complexes because 
these are created out of multiple polypeptide 
chains with multiple genes (e.g., hemoglobin). We 
also did not include families of proteins or genes 
(e.g. lectin), particular alleles of a gene, genes 
which are not part of the natural Drosophila ge-
nome such as reporter genes (e.g. LacZ), and the 
names of genes from other organisms (e.g. sonic 
hedgehog, the mammalian gene homologous to the 
Drosophila hedgehog gene).13 
Background 
Our initial experiment [Hirschman03] had looked 
at creating a gene name finder by simple pattern 
matching, using the extensive FlyBase list of genes 
and their synonyms and identifying each mention 
which occurred in the lexicon with the appropriate 
unique identifier. This yielded spectacularly poor 
results: recall14 on the full papers was quite high 
(84%), but precision was 2%!  For abstracts, the 
recall was predictably lower (31%) and precision 
remained low at 7%.  Our analysis showed that 
polysemy (described in Section 5) and the large 
intersection of gene names with common English 
words caused most of the performance problems. 
In the initial run, where a name was ambiguous, 
we recorded all gene identifiers; this raised recall 
but lowered precision.  After removing all the 
names which were ambiguous for a gene, precision 
climbed to 5% for full papers and 17% in abstracts, 
with a corresponding drop in recall (77% for full 
papers, 28% for abstracts).  We also tried a few 
simple filters, such as ignoring all terms three 
characters or less in length, but the best precision 
we could achieve was 29% in abstracts, certainly 
unacceptable. 
 
We were, however, encouraged by the relatively 
high recall in full papers. Analysis showed that 
many of the missing names were contained only in 
figures or tables that had not been downloaded.  
While these were counted as recall errors when 
compared to the FlyBase curation, there were, in 
fact, no mentions of these genes in the text that had 
been downloaded for this experiment.  Similarly, 
for abstracts, while the recall appeared low com-
pared to the complete set of genes discussed in the 
full paper, these genes were simply not mentioned 
in the abstract.  So from an information extraction 
13 There are no curated lists of complexes or families in Fly-
Base, so we did not train a tagger for these tasks. In our man-
ual curation, we did create separate tags for complexes and 
families, since we believe that these will be important for fu-
ture tasks.  
14 Note that these measures of recall and precision are based 
on the list of unique Drosophila genes curated in a paper. This 
is quite different from recall and precision measuring the men-
tions of gene names in a paper. We used the measure of 
unique genes in a paper because this allowed us to take advan-
tage of the existing FlyBase expert curated resources. 
point of view, the simple pattern matching 
achieved a very high recall for genes mentioned in 
the text being processed. 
4.2 
4.3 
Generating Noisy Training Data 
The initial experiment demonstrated that exact 
match using rich lexical resources was not useful 
on its own. However, we realized that we could 
use the lists of curated genes from FlyBase to con-
strain the possible matches within an abstract ? that 
is, to "license" the tagging of only those genes 
known to occur in the curated full article.  Our 
hope was that this filtered data would provide large 
quantities of cheap but imperfect or noisy training 
data.  
 
Our next experiment focused on generating this 
large but noisy training corpus.  We used our inter-
nal tokenizer, punctoker, originally designed for 
use with newswire data.  There were some errors in 
tokenization, since biological terms have a very 
different morphology from newswire? see 
[Cohen02] for an interesting discussion of tokeni-
zation issues. Among the problems in tokenization 
were uses of "-" instead of white space, or "/" to 
separate recombinant genes.  However, an informal 
examination of errors did not show tokenization 
errors to be a significant contributor to the overall 
performance of the entity extraction system. 
 
To perform the pattern matching, we created a suf-
fix tree of all the synonyms known to FlyBase for 
those genes. This was important, since many bio-
logical entity names are multi-word terms.   We 
then used longest-extent pattern matching to find 
candidate mentions in the abstract of the paper.  
The system tagged only terms licensed by the as-
sociated list of genes for the abstract, assigning the 
appropriate unique gene identifier. Even with the 
FlyBase filtering, this method resulted in some 
errors.  For example, an examination of an abstract 
describing the gene to revealed the unsurprising 
result that all the uses of the word "to" did not refer 
to the gene.  However, the aim was to create data 
of sufficient quantity to lessen the effects of this 
noise. 
Evaluation 
In order to measure performance, we created a 
small doubly annotated test corpus.  We selected a 
sample of 86 abstracts and had two annotators 
mark these abstracts for gene name mentions as 
previously described.  Mentions of families and 
foreign genes were also identified with different 
tags during this process, but not evaluated.   One 
curator was a professional researcher in biology 
with experience as a model organism genome da-
tabase curator (Colosimo).  This set of annotations 
was taken as the "gold-standard". The second an-
notator was the system developer with no particu-
lar annotation experience (Morgan). With two 
annotators, we were able to measure inter-
annotator agreement (F-measure of 0.87). We also 
measured the quality of the automatically created 4.4
     
training data by using the lexical pattern matching 
procedure with filtering to generate annotations for 
86 abstracts in the test set.  The F-measure was 
0.83, when compared against the gold standard, 
shown in Table 1 below. 
F-measure Precision Recall
Training Data
Quality
0.83 0.78 0.88
Inter-
annotator
Agreement
0.87 0.83 0.91
 
 
Ta
We  
tha s 
me
the
[Pa
trai
and
wa
Fig
15 P
http 
ble 1: Training data quality and inter-annotator agreement  
HMM Tagging With Noisy Training Data 
 now had a large quantity of noisy training data
t we could use to train a statistical tagger.   Thi                                                     
thodology is illustrated in Figure 4.  We chose 
 HMM-based trainable entity tagger phrag15 
lmer99] to extract the names in text.  We 
ned phrag on different amounts of training data 
 measured performance.  Our evaluation metric 
s the standard metric used in named entity 
Abstracts
from
PubMed
Lexicon
FlyBase
Large Quantity
of Noisy
Training Data
Plain Text
Genes Tagged
Gene1 Gene2
Other1 Other2
Start End
Text automatically tagged using
FlyBase references and a lexicon is
used to train up a tagger capable of
tagging gene names in new text,
including gene names never observed
before.
Trainable
Tagger
ure 4: Schematic of  Methodology 
hrag is available for download at 
://www.openchannelfoundation.org/projects/Qanda 
Training Data F-measure Precision Recall
531522 0.62 0.73 0.54
529760 0.64 0.75 0.56
1342039 0.72 0.80 0.65
2664324 0.73 0.79 0.67
No Orthographic Correction
  
Table 2: Performance as a function of training data 
 
Training Data F-measure Precision Recall
531522 0.65 0.76 0.56
529760 0.66 0.74 0.59
522825 0.67 0.76 0.59
1322285 0.72 0.77 0.67
1342039 0.75 0.80 0.70
2664324 0.75 0.78 0.71
Orthographic Correction
 
Table 3: Improved performance with orthographical correction 
for Greek letters and case folding for term matching in training 
data  
 
-
f 
-
  
-
", 
p-
m 
n 
.6 
 
entity identification F-measure of 73%.  We then 
made a simple modification of the algorithm to 
correct for variations in orthography due to capi-
talization and representation of Greek letters:  we 
simply expanded the search for letters such as "?" 
to include "Delta" and "delta".  By expanding the 
matching of terms using the orthographical and 
case variants, performance of phrag improved 
slightly, shown in Table 3, improving our best 
performance to an F-measure of 75%.   
5 
 
Figure 5 shows these results in a graphical form.  
Two things are apparent from this graph.  Based on 
the results shown in Figure 2, we might expect the 
performance to be linear with the logarithm of the 
amount of training data, and in this case there is a 
rough fit with a correlation coefficient of .88.  The 
other result which stands out is that there is con-
siderable variation in the performance when train-
ed on different training sets of the same size.  We 
believe that this is due to the very limited amount 
of testing data. 
Error Analysis 
We have identified three types of polysemy in 
Drosophila gene names in FlyBase.  In some cases, 
one name (e.g., ?Clock?) can refer to two distinct 
genes: period or Clock.  The term with the most 
polysemy is ?P450? which is a family of genes and 
is listed as a synonym for 20 different genes in 
FlyBase.  In addition, the same term is often used 
interchangeably to refer to the gene, RNA tran-
script, or the protein. [Hazivassloglou01] presents 
interesting results that demonstrate that experts 
only agree 78% of the time on whether a particular 
mention refers to a gene or a protein.16  The most 
problematic type of polysemy occurs because 
many Drosophila gene names are also regular Eng-
0.62
0.64
0.66
0.68
0.7
0.72
0.74
0.76
0.78
100000 1000000 10000000
Training Data (# of Lexemes)
F
-m
ea
su
re
Figure 5: Performance as a function of the amount of train-
ing data.  The line is a least-squares logarithmic fit with an 
R2 value of .8814. 
                                                           
lish words such as "white", ?cycle?, and "bizarre". 
There are some particularly troublesome examples 
that occur because of frequent use of short forms 
(abbreviations) of gene names, e.g., "we", "a", 
"not?, and even ?and? each occur as gene names.  
These short forms are often abbreviations for the 
full gene name.  For example, the gene symbol of 
the gene takeout is "to", and the symbol for the 
16 The entity tagging task for FlyBase was defined to extract 
gene-or-protein names; however, in cases where the article 
talks only about the protein and not about the gene, the protein
name may not appear on the list of curated genes for the arti-
cle, leading to apparent false positives in tagging. evaluation, requiring the matching of a name's ex
tent and tag (except that for our experiment, we 
were only concerned with one tag, Drosophila 
gene).   Extent matching meant exact matching o
gene name boundaries at the level of tokens:   Ex
actly matching boundaries were considered a hit.
Inexact answers are considered a miss.  For exam
ple, a multiword gene name such as "fas receptor
which has been tagged for "fas" but not for "rece
tor" would constitute a miss (recall error) and a 
false alarm (precision error).  
 
Table 2 shows the performance of the basic syste
as a function of the amount of training data.  As 
with Figure 2, we see there is a diminishing retur
as the amount of training data is increased.   At 2
million words or training data, phrag achieved an
gene wee is "we".  It may be that more sophisti-
cated handling of abbreviations can address some 
of these issues. 
An error analysis looking at the results of our sta-
tistical tagger demonstrated some unusual behav-
ior.  Because our gene name tagger phrag uses a 
first order Markov model, it relies on local context 
and occasionally makes errors such as not tagging 
all of the occurrences of the term "rutabaga" in an 
abstract about rutabaga as gene names.  This cer-
tainly opens up the opportunity for some sort of 
post processing step to resolve these problems. 
 
The fact that phrag uses this local context can 
sometimes be a strength, enabling it to identify 
gene names it has never seen.  We estimated the 
ability of the system to identify new terms as gene 
names by substituting strings unknown to phrag in 
place of all the occurrences of gene names in the 
evaluation data.  The performance of the system at 
correctly identifying terms it had never observed 
gave a precision of 68%, a recall of 22% and an F-
measure of 33%.  This result is relatively encour-
aging, compared with the 3.3% precision and 4.4% 
recall for novel gene names reported by Krau-
thammer.  Recognizing novel names is important 
because the nomenclature of biological entities is 
constantly changing and entity tagging systems 
should to be able to rapidly adapt and recognize 
new terms.   
6 Conclusion and Future Directions 
We have demonstrated that we can automatically 
produce large quantities of relatively high quality 
training data; these data were good enough to train 
an HMM-based tagger to identify gene mentions 
with an F-measure of 75% (precision of 78% and 
recall of 71%), evaluated on our small develop-
ment test set of 86 abstracts.    This compares fa-
vorably with other reported results as described in 
Section 2, and as discussed below, we believe that 
we can improve upon these results in various ways.  
These results are still considerably below the re-
sults from [Gaizauskas03] and may be too low to 
be useful as a building block for further automated 
processing, such as relation extraction.  However, 
in the absence of any shared benchmark evaluation 
sets, cross-system performance cannot be evalu-
ated since the task definition and evaluation cor-
pora differ from system to system.   
 
We plan to take this work in several directions.  
First, we believe that we can improve the quality of 
the underlying automatically generated data, and 
with this, the quality of the entity tagging. There 
are several things that could be improved.  
 
A morphological analyzer trained for biological 
text would eliminate some of the tokenization er-
rors and perhaps capture some of the underlying 
regularities, such as addition of Greek letters or 
numbers (with or without preceding hyphen) to 
specify sub-types within a gene family. There can 
also be considerable semantic content in gene 
names and their formatting.  For example, many 
Drosophila genes are differentiated from the genes 
of other organisms by prepending a "d" or "D", 
such as "dToll".  Gene names can also be explicit 
descriptions of their chromosomal location or even 
function (e.g. Dopamine receptor). 
 
The problem of matching abbreviations has been 
tackled by a number of researchers [e.g. Puste-
jovsky02 and Liu03].  As was mentioned above, it 
seems that ambiguity for "short forms" of gene 
names could be partially resolved by detecting lo-
cal definitions for abbreviations.  It should also be 
possible to apply part of speech tagging and corpus 
statistics to avoid mis-tagging of common words, 
such as ?to? or ?and?.  
 
In the longer term, this methodology provides an 
opportunity to go beyond gene name tagging for 
Drosophila. It can be extended to other domains 
that have comparable resources (e.g. other model 
organism genome databases, other biological enti-
ties), and entity tagging itself provides the founda-
tion for more complex tasks, such as relation 
extraction (e.g. using the BIND database) or attrib-
ute extraction (e.g. using FlyBase to identify at-
tributes such as RNA transcript length, associated 
with protein coding genes). 
 
Second, the existence of a synonym lexicon with 
unique identifiers provides data for term normali-
zation, a task of potentially greater utility to biolo-
gists than the tagging of every mention in an 
article.  There are currently few corpora with anno-
tated term normalization; using the methodology 
outlined here makes it possible to produce large 
quantities of normalized data.  The identification 
and characterization of abbreviations and other 
transformations would be particularly important in 
normalization.   
By exploiting the rich set of biological resources 
that already exist, it should be possible to generate 
many kinds of corpora useful for training high-
quality information extraction and text mining 
components. 
References 
 
Bikel D, Schwartz R, Weischedel R. An Algorithm that 
Learns What's in a Name. Machine Learning, Special 
Issue on Natural Language Learning 34 (1999):211-31. 
 
Cohen KB, Dolbey A, Hunter L. ?Contrast and variabil-
ity in gene names.? Proceedings of the workshop on 
natural language processing in the biomedical domain, 
Association for Computational Linguistics, 2002 
 
Collier N, Nobata C, Tsujii J. ?Extracting the Names of 
Genes and Gene Products with a Hidden Markov 
Model.? Proceedings of COLING '2000 (2000): 201-07. 
 
Craven M, Kumlien J. ?Constructing Biological Knowl-
edge Bases by Extracting Information from Text 
Sources.? Proceedings of the Seventh International 
Conference on Intelligent Systems for Molecular Biol-
ogy 1999: 77-86. 
 
Gaizauskas R, Demetriou G, Artymiuk PJ, Willett P. 
?Protein Structures and Information Extraction from 
Biological Texts: The PASTA System.? Bioinformatics. 
19  (2003): 135-43. 
 
Hatzivassiloglou V, Duboue P, Rzhetsky A. ?Disam-
biguating Proteins, Genes, and RNA in Text: A Ma-
chine Learning Approach.? Bioinformatics 2001: 97-
106. 
 
Hirschman L, Park J, Tsujii J, Wong L, Wu C. "Accom-
plishments and Challenges in Literature Data Mining 
for Biology," Bioinformatics 17 (2002):1553-61. 
 
Hirschman L, Morgan A, Yeh A.  ?Rutabaga by Any 
Other Name: Extracting Biological Names." Accepted, 
Journal of Biomedical Informatics, Spring 2003.  
 
Krauthammer M, Rzhetsky A, Morosov P, Friedman C. 
?Using BLAST for Identifying Gene and Protein Names 
in Journal Articles.? Gene 259 (2000): 245-52. 
 
Liu H, Friedman C.  ?Mining Terminological Knowl-
edge in Large Biomedical Corpora.?  Proceedings of the 
Pacific Symposium on Biocomputing.  2003. 
 
Palmer D, Burger J, and Ostendorf M. "Information 
Extraction from Broadcast News Speech Data." Pro-
ceedings of the DARPA Broadcast News and Under-
standing Workshop, 1999. 
 
Pustejovsky J, Casta?o J, Saur? R, Rumshisky A, Zhang 
J, Luo W. ?Medstract: Creating Large-scale Information 
Servers for Biomedical Libraries.? Proceedings of the 
ACL 2002 Workshop on Natural Language Processing 
in the Biomedical Domain. 2002. 
 
Tauszig et al ?Toll-related receptors and the control of 
antimicrobial peptide expression in Drosophila.? Pro-
ceedings of the  National Academy of  Sciences 97 
(2000): 10520-5. 
 
Yeh A., Hirschman L,  Morgan A.  "Evaluation of Text 
Data Mining for Database Curation: Lessons Learned 
from the KDD Challenge Cup." Accepted, Intelligent 
Systems in Molecular Biology, Brisbane, June 2003.  
 
 
Proceedings of the Workshop on BioNLP, pages 63?70,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Investigation of Unsupervised Pattern Learning Techniques for Bootstrap
Construction of a Medical Treatment Lexicon
Rong Xu, Alex Morgan, Amar K Das
Biomedical Informatics Program
Stanford University
Stanford, CA 94305, USA
xurong@stanford.edu
Alan Garber
Primary Care and Outcomes Research
Stanford University
Stanford, CA94305, USA
Abstract
Dictionaries of biomedical concepts (e.g. dis-
eases, medical treatments) are critical source
of background knowledge for systems doing
biomedical information retrieval, extraction,
and automated discovery. However, the rapid
pace of biomedical research and the lack of
constraints on usage ensure that such dictio-
naries are incomplete. Focusing on medical
treatment concepts (e.g. drugs, medical pro-
cedures and medical devices), we have devel-
oped an unsupervised, iterative pattern learn-
ing approach for constructing a comprehen-
sive dictionary of medical treatment terms
from randomized clinical trial (RCT) ab-
stracts. We have investigated different meth-
ods of seeding, either with a seed pattern or
seed instances (terms), and have compared
different ranking methods for ranking ex-
tracted context patterns and instances. When
used to identify treatment concepts from 100
randomly chosen, manually annotated RCT
abstracts, our medical treatment dictionary
shows better performance (precision:0.40, re-
call: 0.92 and F-measure: 0.54) over the
most widely used manually created medical
treatment terminology (precision: 0.41, recall:
0.52 and F-measure: 0.42).
1 Introduction
Dictionary based natural language processing sys-
tems have been widely used in recognizing medical
concepts from free text. For example, the MetaMap
program is used to map medical text to concepts
from the most widely used biomedical terminol-
ogy, the Unified Medical Language System (UMLS)
Metathesaurus (Aronson, 2000). It identifies various
forms of UMLS concepts in text and returns them
as a ranked list using a five-step process: identify-
ing simple noun phrases (NP?s), generating variants
of each phrase, finding matched phrases, assign-
ing scores to matched phrases and composing map-
pings. However, its performance largely depends on
the quality of the underlying UMLS Metathesaurus
and its manually created rules and variants. One
study has shown that, of the medical concepts iden-
tified by human subjects, more than 40% were not
in UMLS (Pratt, 2003). Other examples of map-
ping text to controlled biomedical terminologies in-
clude (Cohen, 2005) and (Fang, 2006). Many other
systems make heavy use of biomedical terminolo-
gies directly such as the work of Blaschke, et al
(Blaschke, 2002) and Friedman et al (Friedman,
2001).
Biomedical terminology is highly dynamic, both
because biomedical research is itself highly dy-
namic, but also because there are essentially no con-
straints on the use of new terminological variants,
making the terms used in free text quite different
from the canonical forms listed in controlled ter-
minologies. To contrast UMLS with actual text
mentions, there are 150 different chemotherapy con-
cepts in UMLS. The majority of these terms de-
rive from the diseases they are used to treat. For
example cancer chemotherapy, AIDS chemother-
apy, brain disorder chemotherapy, and alcoholism
chemotherapy. On the other hand, we have identi-
fied more than 1,000 different chemotherapy types
mentioned in RCT (Randomized Clinical Trial) re-
port abstracts, with most of the names derived
63
from the chemicals contained in the chemother-
apy regimen, such as platinum-based chemother-
apy or fluorouracil-based chemotherapy. There is
little overlap between the chemotherapy terms in
UMLS and the ones used in RCT abstracts. Even
for simple drug names as 5-fluorouracil and tamox-
ifen, there are many clinically distinct and important
variants of these drugs which are absent in UMLS
as distinct terms/concepts, such as intralesional 5-
fluorouracil, topical 5-fluorouracil, intrahepatic ar-
terial 5-Fluorouracil, adjuvant sequential tamox-
ifen, and neoadjuvant tamoxifen.
There has been considerable work on expand-
ing the coverage of biomedical dictionaries through
morphological variants, but these approaches re-
quire an initial term dictionary with reasonable
extensive coverage. Examples include the ap-
proaches developed by Krauthammer and Nenadic
(Krauthammer, 2004), Tsuruoka and Tsujii (Tsu-
ruoka, 2004) & (Tsuruoka, 2003), Bodenreider, et
al. (Bodenreider, 2002), and Mukherjea and col-
leagues (Mukherjea, 2004). An important short-
coming with static, human derived terminologies
that cannot easily be addressed by looking for vari-
ants of existing terms is the fact that continual devel-
opments in medical therapies constantly gives rise
to new terms. Examples include, Apomab, Bap-
ineuzumab, Bavituximab, Etaracizumab, and Figi-
tumumab. These all represent a new generation of
targeted biological agents currently in clinical trials
none of which appear in UMLS. Clearly we need to
develop techniques to deal with this dynamic termi-
nology landscape.
MEDLINE is the most extensive and authoritative
source of biomedical information. Large quantities
of biomedical text are available in MEDLINE?s col-
lection of RCT reports with over 500,000 abstracts
available. RCT reports are a critical resource for in-
formation about diseases, their treatments, and treat-
ment efficacy. These reports have the advantage of
being highly redundant (a disease or treatment name
is often reported in multiple RCT abstracts), medi-
cally related, coherent in writing style, trustworthy
and freely available.
In our recent study (Xu, 2008), we have devel-
oped and evaluated an automated, unsupervised, it-
erative pattern learning approach for constructing
a comprehensive disease dictionary from RCT ab-
stracts. When used to identify disease concepts from
100 manually annotated clinical abstracts, the dis-
ease dictionary shows significant performance im-
provement (F1 increased by 35-88%) over UMLS
and other disease terminologies. It remained to
be demonstrated that these bootstrapping techniques
are indeed rapidly retargetable and can be extended
to other situations, and so we have extended our
scope to investigate medical treatment names in ad-
dition to disease terms in this work.
Our approach is inspired by the framework
adopted in several bootstrapping systems for learn-
ing term dictionaries, including (Brin, 1998), (?),
and (Agichtein, 2000). These approaches are based
on a set of surface patterns (Hearst , 1992), which
are matched to the text collection and used to find
instance-concept relations. Similar systems include
that of Snow and colleagues (Snow, 2005), which
integrates syntactic dependency structure into pat-
tern representation and has been applied to the task
of learning instance-of relations, and the approach
developed of Caprosaso, et al (Caprosaso, 2007)
which focussed on learning text context patterns to
identify mentions of point mutations.
All iterative learning systems suffer from the in-
evitable problem of spurious patterns and instances
introduced in the iterative process. To analyze dif-
ferent approaches to addressing this issue, we have
compared three different approaches to ranking ex-
tracted patterns and three different approaches to
ranking extracted instances. Because such systems
also depend on an initial seeding with either a seed
pattern or term instance, an important question is
whether these different starting points lead to dif-
ferent results. We investigated this issue by starting
from each point separately and compared the final
results.
2 Data and Methods
2.1 Data
509,308 RCT abstracts published in MEDLINE
from 1965 to 2008 were parsed into 8,252,797 sen-
tences. Each sentence was lexically parsed to gen-
erate a parse tree using the Stanford Parser. The
Stanford Parser (Klein, 2003) is an unlexicalized
natural language parser, trained on a non-medical
document collection (Wall Street Journal). We used
64
the publicly available information retrieval library,
Lucene, to create an index on sentences and their
corresponding parse trees. For evaluation and com-
parison, 241,793 treatment terms with treatment re-
lated semantics types from UMLS were used.
2.2 Unsupervised Instance Extraction and
Pattern Discovery
Figure 1 describes the bootstrapping algorithm used
in learning instances of treatment and their associ-
ated text patterns. The algorithm can operate in two
modes, either starting with a seed pattern p0, which
represents a typical way of writing about treatments,
or a set of seed instances, (di). For example, the
seed pattern we used was ?treated with NP? (NP:
noun phrase). The program loops over a procedure
consisting of two steps: instance extraction and pat-
tern discovery. In the instance extraction step, pat-
terns are used as search queries to the local search
engine. The parse trees with given patterns are re-
trieved and noun phrases (instances of treatments)
following the pattern are matched from the parse
trees. In the pattern discovery step, instances ex-
tracted from the previous iteration are used as search
queries to the local search engine. Corresponding
sentences containing instance mentions are retrieved
and the bigrams (two words) in front of instances are
extracted as patterns. When seeding with an initial
pattern, only two iterations are typically needed, as
experience shows that most of reliable patterns and
instances have been discovered at this stage. The al-
gorithm stops after a single iteration when seeding
with a list of instances.
2.3 Selecting Seed Instances
Of the 241,793 treatment related terms in the
UMLS, only about 22,000 (9%) of these have ap-
peared in MEDLINE RCT reports. We randomly
selected 500 drug terms and 500 medical procedure
terms from the 22,000 terms as seed instances and
used them in the pattern discovery system described
above.
2.4 Pattern Ranking
A newly discovered pattern is scored on how simi-
lar its output (instances associated with the pattern)
is to the output of the initial seed pattern. Intu-
itively, a reliable pattern is one that is both highly
Instance
Extraction
Pattern 
Discovery
Instance 
& pattern 
ranking
Seed pattern
RCT
DB
Seed Instance
Figure 1: General scheme of the iterative method.
precise (high precision) and general (high recall).
Using the output instances from the seed pattern p0
as a comparison, we developed Precision Based, Re-
call Based, and F1 Based algorithms to rank pat-
terns. We define instances(p) to be the set of
instances matched by pattern p, and the intersec-
tion instances(p)? instances(p0) as the set of in-
stances matched by both pattern p and p0.
1. Precision Based rank:
score1(p) = instances(p)
? instances(p0)
instances(p)
(1)
The precision based ranking method favors
specific patterns.
2. Recall Based rank:
score2(p) = instances(p)
? instances(p0)
instances(p0) (2)
The recall based ranking method favors gen-
eral patterns.
3. F1 based rank:
score3(p) = 2? score1(p)? score2(p)score1(p) + score2(p)
(3)
A combination of the Precision Based and the
Recall Based evaluation methods is the F1
65
Based ranking method, which takes into ac-
count both pattern specificity and pattern gener-
ality. This method favors general patterns while
penalizing overly specific patterns.
2.5 Instance Ranking
A reliable instance is one that is associated with a
reliable pattern many times. We experimented with
three ranking algorithms:
1. Abundance Based rank: A treatment
instance(d) that is obtained multiple times
is more likely to be a real treatment concept
when compared with one that has only a
single mention in the whole corpus. We define
scoreA(d) as the number of times where d
appears in the corpus.
2. Pattern Based rank: A treatment instance ob-
tained from multiple patterns is more likely
to be a real treatment concept when compared
with the one that was obtained by a single pat-
tern (p). Pattern Based rank takes into account
the number of patterns that generated the in-
stance, score of those patterns, and the num-
ber of times that the instance is associated with
each pattern (count(p, d)).
scoreB(d) =
n?
i=0
log score3(pi)?count(pi, d)
(4)
3. Best Pattern Based rank: A treatment instance
obtained from a highly ranked pattern is more
likely to be a real treatment concept when com-
pared with the one that was obtained from a
poorly ranked pattern. First the instances are
ranked by the best pattern (pb) that generated
the instances and then further ties are broken
by the number of times the instance is associ-
ated with that pattern (count(p, d)) to provide
scoreC(d).
2.6 Comparison of Patterns Derived from
Different Seed Types
The patterns extracted when starting with either seed
instances or a seed pattern are ranked by the recall
based method and F1-based method, then the over-
laps at different cutoffs are measured to assess the
similarity of the patterns discovered by starting with
the different starting seed types.
2.7 Evaluation of Stanford Parser in
Identifying Treatment Noun Phrase
An important question is how accurate the Stan-
ford Parser is at identifying the relevant term bound-
aries. We used manually curated treatment names
from UMLS to measure the accuracy of the Stan-
ford Parser in identifying treatment noun phrases.
With NPcount(treatment) defined as number of
times that the Stanford Parser identifies a treatment
as noun phrase or part of a noun phrase in the data
and count(treatment) as number of times the treat-
ment appears in the data.
accuracy = 1n
n?
i=0
(NPcount(di)
count(di)
)
(5)
2.8 Evaluation of the extracted treatment
lexicon
We assessed the quality (precision and recall) of our
lexicon by using it to identify treatment concepts in
100 randomly selected RCT abstracts where treat-
ment names were manually identified. In addition,
we also compared the performance of our lexicon
with that of UMLS.
3 Results
3.1 Evaluation of Stanford Parser in
Identifying Treatment Noun Phrases
Even though the Stanford Parser is trained on non-
medical data, it is highly accurate in identifying
treatments as noun phrases or parts of a noun phrase
with accuracy of 0.95. The reason may be that medi-
cal treatments are indeed often noun phrases or parts
of a noun phrase in RCT reports, and there are strong
syntactical signals for their phrasal roles in the sen-
tences. For example, treatments are often either the
object of a preposition (e.g. efficacy of fluorouracil
and treated with fluorouracil) or the subject of a sen-
tence (e.g. fluorouracil is effective in treating colon
cancer).
3.2 Comparison between Seed Types
There is considerable overlap in discovered patterns
between starting with a single seed pattern and start-
66
ing with the 1,000 seed instances and little differ-
ence in overall performance. 12,241 patterns are
found to be associated with the 1,000 seed treatment
instances. However, only the most highly ranked
patterns are relevant (see Evaluation of The Ex-
tracted Treatment Lexicon, below). Table 1 shows
the intersection of the top ranked patterns between
both seeding methods at different rank cut-offs. We
find a very high level of intersection between the top
ranked patterns from both initial seed types, for ex-
ample eighteen of the top twenty patterns are iden-
tical. These results indicate that starting from either
seed type leads to very similar results.
Rank Recall Based F1 Based
10 0.90 0.80
20 0.90 0.90
30 0.87 0.80
40 0.83 0.85
50 0.84 0.82
60 0.82 0.85
70 0.82 0.79
80 0.83 0.84
90 0.84 0.83
100 0.82 0.83
Table 1: : The ratio of overlap in the top ranking patterns
discovered by different seed types
3.3 Pattern Ranking
Similar to the results observed in our previous study
(Xu, 2008), the Precision Based metric assigns high
scores to very specific but not generalizable patterns
such as ?lornoxicam versus? (Table 2), which ap-
pears only once in the data collection, while the
top 10 patterns based on the Recall Based and F1
Based rankings are typical treatment related pat-
terns. When a different seed pattern ?efficacy of ?
was used, the top 10 patterns were the same with a
different rank ordering.
3.4 Instance ranking
Table 3 shows the top 10 suggested treatment names
when using ?treated with? as the initial seed pattern.
The rank of a proposed treatment instance is deter-
mined by the different ranking methods: Abundance
Based, Pattern Based, or Best Pattern Based ranking
# Precision based Recall based F1 based
1 beta-blockers nor treated with treated with
2 lornoxicam versus treatment
with
treatment with
3 piroxantrone and effects of efficacy of
4 heparin called efficacy of effects of
5 anesthetics con-
taining
dose of dose of
6 antioestrogens and doses of doses of
7 markedly adsorb suggest that suggest that
8 recover following study of safety of
9 Phisoderm and response to response to
10 MitoExtra and effect of effect of
Table 2: Top 10 patterns with ?treated with? as seed pat-
tern
algorithms. None of the top 10 extracted phrases on
the basis of Abundance Based or Pattern Based are
actual treatment names. These two ranking methods
assign high ranks to common, non-specific phrases.
The Best Pattern Based ranking method correctly
identifies specific treatment mentions, mainly be-
cause it reduces the likelihood of selecting irrelevant
patterns.
# Abundance Pattern Best pattern
based based based
1 patients patients placebo
2 treatments the treatment chemotherapy
3 the treatments treatments radiotherapy
4 children the use tamoxifen
5 the effect children antibiotics
6 no significant
differences
surgery insulin
7 placebo the patients interferon
8 surgery changes surgery
9 the effects women corticosteroids
10 the study use cisplatin
Table 3: Top 10 treatments when using ?treated with? as
the seed pattern
3.5 Evaluation of the Extracted Treatment
Lexicon
Our dictionary derived from using ?treated with?
as the seed pattern with two bootstrapping itera-
67
Count Cutoff Precision Recall F1
17,683 1.0% 0.404 0.921 0.540
88,415 5% 0.127 1.0 0.22
132,623 7.5% 0.105 1.0 0.187
176,832 10% 0.088 1.0 0.160
Table 4: Precision, recall and F1 at 4 cutoff values
tions consists of 1,768,320 candidate instances and
78,037 patterns, each with an accompanying confi-
dence score. The top 20 patterns are associated with
more than 90% of the instances. We evaluated the
quality of the dictionary by using it to identify treat-
ment concepts in 100 randomly selected abstracts
where treatment names were manually annotated.
There were an average of three treatment names per
test abstract. Table 4 shows the precision, recall and
F1 values when instances are ranked by the best pat-
tern based ranking method (ScoreC). The precision,
recall and F1 values at each cut-off (percentage of all
instances) were averaged across the 100 abstracts.
The precision, recall and F1 of the UMLS
Metathesaurus in identifying treatment names from
the test dataset are 0.41, 0.52 and 0.42 respectively.
The performance using UMLS on this task is con-
sistent with a previous study (Pratt, 2003). The low
precision may due to the fact that UMLS often tags
irrelevant names as treatment related names. For ex-
ample, common, non-specific terms such as drug,
agent, treatment and procedure appear in the dictio-
nary derived from UMLS. However, we chose not to
edit the lexicon derived from UMLS as it is unclear
how to do so in a systematic matter without essen-
tially creating a new version of UMLS, and we are
interested in studying methods that do not rely on
any human involvement (our Discussion describes
the possible inclusion of human judgments). Also,
the low recall of UMLS is not surprising given the
fact that the names specified in UMLS are often not
the terms authors use in writing. The performance
of our dictionary (precision: 0.40, recall: 0.92, F1:
0.54) is a dramatic improvement over using UMLS.
Our recall is high since all the terms are learned from
the literature directly and exemplify the manner in
which authors write RCT reports. However, the pre-
cision of our dictionary is still low (see Discussion).
4 Discussion
We have demonstrated an automated, unsupervised,
iterative pattern learning approach for bootstrapping
construction of a comprehensive treatment lexicon.
We also compared different pattern and instance
ranking methods and different initial seed types (in-
stances or patterns). On the task of term identifica-
tion, use of our boostrapped lexicon increased per-
formance over using the most widely used manually
curated terminology (UMLS).We have extended our
previous work to the identification of new termi-
nology types, demonstrating the versatility of this
approach. Our approach may also be used with
other data sources such as general health related web
pages. However, there is still significant space in
which to seek improvement in increasing the cover-
age of our lexicon and the quality of our patterns.
Although useful in demonstrating the proof of
concept and allowing us to examine different rank-
ing methods, focusing on bigrams that precede
noun-phrases limited the space of patterns that we
could potentially examine. More complex patterns
might be involved. For example, in the sentence
?Pravastatin is an effective and safe drug? (PMID
08339527), there is a distinctive treatment related
pattern ?NP is an effective and safe drug? that our
technique does not capture. However, most key
terms are mentioned in multiple contexts. For ex-
ample, Pravastatin appears with the seed pattern
treatment with more than 200 times. As our corpus
of literature increases, redundancy will increase the
likelihood of a treatment term being matched by the
type of patterns we recognize. The rapid growth of
biomedical knowledge and literature, which makes
our automatically generated medical treatment vo-
cabulary necessary, can also act to increase its cov-
erage over time.
In order to keep our algorithm simple, we did not
perform deep grammatical analysis. For example, in
the sentence ?Treatment of the subjects with atorvas-
tatin decreased the abundance of IL-12p35 mRNA in
mononuclear cells? (PMID 12492458), atorvastatin
is associated with treatment of, not subjects with.
Since our algorithms simply extracts the two words
in front of treatment names, subjects with will be ex-
tracted as treatment related pattern. In fact, subjects
with is a disease related pattern in RCT reports, for
68
example ?34 subjects with asthma?. But our pattern
ranking algorithm will assign a low score to subjects
with since the terms associated with this pattern are
more disease related and have little overlap with the
output of the seed pattern treatment with.
Our instance ranking assigns high confidence
scores to common and non specific terms like this
drug, the treatment or this procedure since they are
often associated with highly ranked patterns many
times. These anaphoric terms often refer to treat-
ment names previously specified. There are at least
two ways to address this problem. The first is to as-
sign low scores to terms starting with a determiner
such as the or this. Another way to improve the in-
stance ranking algorithm is to take into account of
the overall context of the term. For example, these
anaphora often appear in specific sections of RCT
reports such as the result section, and refer to terms
from previous sections. Specific examples include
?Treatment with this drug should be attempted in
intractable cases? (PMID 09038009) and ?The effi-
cacy of the treatment was 88 and 95% in group 1 and
2, respectively? (PMID 14520944). The terms from
title, background or conclusion sections could be as-
signed higher scores than the ones from result sec-
tion. Beyond these simple heuristics, more sophisti-
cated approaches might take advantage of the work
in anaphora resolution, such as (Baldwin, 2001).
The lexicon consists of terms with mixed hi-
erarchies, including general terms as chemother-
apy, surgery, corticosteroids, antibiotics, and spe-
cific terms as fluorouracil, oral or intravenous 5-
Fluorouracil, cisplatin, nephrectomy. In order to
make this dictionary more useful, additional work
is needed to organize the terms and build ontologies
based on the lexicon.
Previous work has shown that learning multiple
semantic types simultaneously can improve preci-
sion (Thelen, 2002) & (Curran, 2007), and it re-
mains to be seen if that approach can be combined
with the prioritization of pattern and extracted in-
stance rankings here to give better overall perfor-
mance. Other possible extensions and improve-
ments include various approaches to slow the learn-
ing process and discover new patterns and instances
more conservatively, at the expense of more itera-
tions. Further improvements can be expected from
integrating active learning approaches to include
the involvement of a human judge in the process,
analogous to the tag-a-little, learn-a-little method
proposed as part of the Alembic Workbench (Day,
1997). Because our approach ranks both extracted
patterns and instances, it is amenable to such tech-
niques. Indeed, active learning has been found
to provide considerable gains in corpus annotation
(Tomanek, 2007) & (Buyko, 2007), and can be a
model for semi-automated terminology compilation.
All the data and code are available on request
from the author.
Acknowledgments
RX is supported by NLM training grant LM007033
and Stanford Medical School.
References
E. Agichtein, L Gravano. 2000. Snowball: extracting
relations from large plaintext collections, In Proc of
the 5th ACM conference on Digital libraries .
A.R. Aronson 2001. Effective mapping of biomedical
text to the UMLS Metathesaurus: the MetaMap pro-
gram. Proc AMIA Symp:17-21.
B. Baldwin 2001. Text and knowledge mining for coref-
erence resolution. Second meeting of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Language technologies:1-8.
C. Blaschke, A. Valencia. 2002. The frame-based mod-
ule of the SUISEKI information extraction system, In-
telligent Systems, IEEE, 17; 2:14 - 20.
O. Bodenreider, T.C. Rindflesch, A. Burgun, 2002.
Unsupervised, corpus-based method for extending a
biomedical terminology. Proc of the ACL-02 work-
shop on Natural language processing in the biomedi-
cal domain: 53?60.
S. Brin 1998. Extracting patterns and relations from
the world wide web. WebDB Workshop at 6th Interna-
tional Conference on Extending Database Technology
E. Buyko, S. Piao, Y. Tsuruoka, K. Tomanek , J.D. Kim,
J. McNaught, U. Hahn, J. Su, and S. Ananiadou. 2007,
Bootstrep annotation scheme: Encoding information
for text mining, Proc of the 4th Corpus Linguistics
Conference, Birmingham, July 27-30.
J. G. Caprosaso, W.A. Baumgartner, D.A. Randolph,
K.B. Cohen, L. Hunter 2007. Rapid pattern develop-
ment for concept recognition systems: application to
point mutations., Journal of Bioinformatics and Com-
putational Biology, Vol. 5, No. 6, 12331259.
69
A. Cohen 2005. Unsupervised gene/protein named en-
tity normalization using automatically extracted dic-
tionaries. Proc of the ACL-ISMB Workshop on Link-
ing Biological Literature, Ontologies and Databases:
17-24.
M. Collins, Y. Singer 1999. Unsupervised Models for
Named Entity Classification. EMNLP
J.R. Curran, T Murphy, B Scholz 2007. Minimizing
Semantic Drift With Mutual Exclusion Bootstrapping,
Proc of the 10th Conference of PACL:172-180.
D. Day, J. Aberdeen, L. Hirschman, R. Kozierok, P.
Robinson, M. Vilain 1997, Mixed-initiative develop-
ment of language processing systems. Proc of the 5th
ACL Conference on Applied Natural Language Pro-
cessing
H. Fang, K. Murphy, Y. Jin, J.S. Kim, P.S. White 2006.
Human Gene Name Normalization using Text Match-
ing with Automatically Extracted Synonym Dictionar-
ies. Proc of the BioNLP Workshop on Linking Natural
Language Processing and Biology at HLT-NAACL 06:
4148.
C. Friedman, P. Kra, H. Yu, M. Krauthammer, A. Rzhet-
sky. 2001. GENIES: a natural-language processing
system for the extraction of molecular pathways from
journal articles. Bioinformatics, ;17 Suppl 1:S74-82.
M.A. Hearst 1992. Automatic acquisition of hyponyms
from large text corpora, Proc of the 14th conference
on computational linguistics.
D. Klein D, CD. Manning 2003. Accurate Unlexicalized
Parsing, Proc of the 41st Meeting of the Association
for Computational Linguistics, 2003; 423-30.
M. Krauthammer G. Nenadic 2004. Term identifica-
tion in the biomedical literature., J Biomed Inform,
Dec;37(6):512-26.
S. Mukherjea, L.V. Subramaniam, G. Chanda, S.
Sankararaman, R. Kothari, V.S. Batra, D.N. Bhardwaj,
B.Srivastava 2004. Enhancing a biomedical infor-
mation extraction system with dictionary mining and
context disambiguation, IBM Journal of Research and
Development, 48(5-6): 693-702
W. Pratt, M. Yetisgen-Yildiz 2003 A Study of Biomedi-
cal Concept Identification: MetaMap vs. People, Proc
AMIA Symp, 529-533.
R. Snow, D. Jurafsky, A. Ng 2005. Learning syntactic
patterns for automatic hypernym discovery, Proc of
the 17th Conference on Advances in Neural Informa-
tion Processing Systems MIT Press.
M. Thelen, E. Riloff 2002. A Bootstrapping Method for
Learning Semantic Lexicons Using Extraction Pattern
Contexts, Proc of EMNLP.
K. Tomanek, J Wermter, U Hahn. 2007. An Approach
to Text Corpus Construction which Cuts Annotation
Costs and Maintains Reusability of Annotated Data,
Proc of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning:486-495.
Y. Tsuruoka, J. Tsujii 2003, Boosting Precision and
Recall of Dictionary-Based Protein Name Recogni-
tion, Proc of the ACL 2003 Workshop on NLP in
Biomedicine:41-8.
Y. Tsuruoka, J. Tsujii 2004, Improving the performance
of dictionary-based approaches in protein name recog-
nition, J of Biomed Inf 37, 6; December: 461-470.
R. Xu, K. Supekar, A. Morgan, A.Das, A. Garber 2008.
Unsupervised Method for Automatic Construction of a
Disease Dictionary from a Large Free Text Collection,
Proc AMIA Symp.
70
