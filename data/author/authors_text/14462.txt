Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 939?946, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Integrating linguistic knowledge in passage retrieval for question answering
Jo?rg Tiedemann
Alfa Informatica, University of Groningen
Oude Kijk in ?t Jatstraat 26
9712 EK Groningen, The Netherlands
j.tiedemann@rug.nl
Abstract
In this paper we investigate the use of lin-
guistic knowledge in passage retrieval as
part of an open-domain question answer-
ing system. We use annotation produced
by a deep syntactic dependency parser for
Dutch, Alpino, to extract various kinds of
linguistic features and syntactic units to
be included in a multi-layer index. Sim-
ilar annotation is produced for natural lan-
guage questions to be answered by the
system. From this we extract query terms
to be sent to the enriched retrieval index.
We use a genetic algorithm to optimize the
selection of features and syntactic units
to be included in a query. This algo-
rithm is also used to optimize further pa-
rameters such as keyword weights. The
system is trained on questions from the
competition on Dutch question answering
within the Cross-Language Evaluation Fo-
rum (CLEF). We could show an improve-
ment of about 15% in mean total recip-
rocal rank compared to traditional infor-
mation retrieval using plain text keywords
(including stemming and stop word re-
moval).
1 Introduction
Improving information retrieval (IR) through natu-
ral language processing (NLP) has been the goal
for many researchers. NLP techniques such as
lemmatization and compound splitting have been
used in several studies (Krovetz, 1993; Hollink et al,
2003). Linguistically motivated syntactic units such
as noun phrases (Zhai, 1997), head-modifier pairs
(Fagan, 1987; Strzalkowski et al, 1996) and subject-
verb-object triples (Katz and Lin, 2003) have also
been integrated in information retrieval. However,
most of these studies resulted in only little success
or even decreasing performance. It has been argued
that NLP and especially deep syntactic analysis is
still too brittle and ineffective (Katz and Lin, 2003).
Integrating NLP in information retrieval seems
to be very hard because the task here is to match
plain text keywords to natural language documents.
In question answering (QA), however, the task is
to match natural language questions to relevant an-
swers within document collections. For this, we
have to analyze the question in order to determine
what kind of answer the user is expecting. Tradi-
tional information retrieval is used in QA systems to
filter out relevant passages from the document col-
lection which are then processed to extract possible
answers. Hence, the performance of this passage re-
trieval component (especially in terms of recall) is
crucial for the success of the entire system. NLP
tools and linguistic resources are frequently used in
QA systems, e.g. (Bernardi et al, 2003; Moldovan
et al, 2002), although not very often for passage
retrieval (some exceptions are (Strzalkowski et al,
1996; Katz and Lin, 2003; Neumann and Sacaleanu,
2004)).
Our goal is to utilize information that can be ex-
tracted from the analyzed question in order to match
linguistic features and syntactic units in analyzed
939
documents. The main research question is to find
appropriate units and features that actually help to
improve the retrieval component. Furthermore, we
have to find an appropriate way of combining query
terms to optimize IR performance. For this, we ap-
ply an iterative learning approach based on example
questions annotated with their answers.
In the next section we will give a brief description
of our question answering system with focus on the
passage retrieval component. Thereafter we will dis-
cuss the query optimization algorithm followed by a
section on experimental results. The final section
contains our conclusions.
2 Question answering with dependency
relations
Our Dutch question answering system, Joost
(Bouma et al, 2005), consists of two streams: a table
look-up strategy using off-line information extrac-
tion and an on-line strategy using passage retrieval
and on-the-fly answer extraction. In both strate-
gies we use syntactic information produced by a
wide-coverage dependency parser for Dutch, Alpino
(Bouma et al, 2001). In the off-line strategy we use
syntactic patterns to extract information from unre-
stricted text to be stored in fact tables (Jijkoun et
al., 2004). For the on-line strategy, we assume that
there is a certain overlap between syntactic relations
in the question and in passages containing the an-
swers. Furthermore, we also use strategies for rea-
soning over dependency rules to capture semantic
relationships that are expressed by different syntac-
tic patterns (Bouma et al, 2005).
Our focus is set on open-domain question an-
swering using data from the CLEF competition on
Dutch QA. We have parsed the entire corpus pro-
vided by CLEF with about 4,000,000 sentences in
about 190,000 documents. The dependency trees are
stored in XML and are directly accessible from the
QA system. Syntactic patterns for off-line informa-
tion extraction are run on the entire corpus. For the
on-line QA strategy we use traditional information
retrieval to select relevant passages from the corpus
to be processed by the answer extraction modules.
This step is necessary to reduce the search space for
the QA system to make it feasible to run on-line QA.
As segmentation level we used paragraphs marked
in the corpus (about 1.1 million).
Questions are parsed within the QA system using
the same parser. Using their analysis, the system de-
termines the question type and, hence, the expected
answer type. According to the type, we try to find
the answer first in the fact database (if an appropri-
ate table exists) and then (as fallback) in the corpus
using the on-line QA strategy.
2.1 Passage retrieval in Joost
Information retrieval is one of the bottle-necks in the
on-line strategy of our QA system. The system re-
lies on the passages retrieved by this component and
fails if IR does not provide relevant documents. Tra-
ditional IR uses a bag-of-words approach using plain
text keywords to be matched with word-vectors de-
scribing documents. The result is usually a ranked
list of documents. Simple techniques such as stem-
ming and stop word removal are used to improve the
performance of such a system. This is also the base-
line approach for passage retrieval in our QA sys-
tem.
The passage retrieval component in Joost includes
an interface to seven off-the shelf IR systems. One
of the systems supported is Lucene from the Apache
Jakarta project (Jakarta, 2004). Lucene is a widely-
used open-source Java library with several exten-
sions and useful features. This was the IR engine of
our choice in the experiments described here. For
the base-line we use standard settings and a pub-
lic Dutch text analyzer for stemming and stop word
removal. Now, the goal is to extend the base-line
by incorporating linguistic information produced by
the syntactic analyzer. Figure 1 shows a dependency
tree produced for one of the sentences in the CLEF
corpus. We like to include as much information from
the parsed data as possible to find better matches be-
tween an analyzed question and passages that con-
tain answers. From the parse trees, we extract vari-
ous kinds of linguistic features and syntactic units to
be stored in the index. Besides the dependency rela-
tions the parser also produces part-of-speech (POS)
tags, named entity labels and linguistic root forms. It
also recognizes compositional compounds and par-
ticle verbs. All this information might be useful for
our passage retrieval component.
Lucene supports multiple index fields that can be
filled with different types of data. This is a useful
940
top
smain
su
1
np
det
det
het0
hd
noun
embargo1
mod
pp
hd
prep
tegen2
obj1
name
Irak3
vc
ppart
obj1
1
hd
verb
stel in5
mod
pp
hd
prep
na6
obj1
np
det
det
de7
hd
noun
inval8
mod
pp
hd
prep
in9
obj1
name
Koeweit10
mod
pp
hd
prep
in11
obj1
noun
199012
hd
verb
word4
Figure 1: A dependency tree produced by Alpino:
Het embargo tegen Irak werd ingesteld na de inval
in Koeweit in 1990. (The embargo against Iraq has
been declared after the invasion of Kuwait in 1990.)
feature since it allows one to store various kinds of
information in different fields in the index. Hence-
forth, we will call these data fields index layers and,
thus, the index will be called a multi-layer index. We
distinguish between token layers, type layers and an-
notation layers. Token layers include one item per
token in the corpus. Table 1 lists token layers de-
fined in our index.
Table 1: Token layers
text plain text tokens
root root forms
RootPOS root form + POS tag
RootHead root form + head
RootRel root form + relation name
RootRelHead root form + relation + head
We included various combinations of features de-
rived from the dependency trees to make it possi-
ble to test their impact on IR. Features are simply
concatenated (using special delimiting symbols be-
tween the various parts) to create individual items in
the layer. For example, the RootHead layer contains
concatenated dependent-head bigrams taken from
the dependency relations in the tree. Tokens in the
text layer and in the root layer have been split at hy-
phens and underscores to split compositional com-
pounds and particle verbs (Alpino adds underscores
top
whq
whd
1
adv
wanneer0
body
sv1
mod
1
hd
verb
stel in1
su
np
det
det
de2
hd
name
Verenigde Naties3
obj1
np
det
det
een5
hd
noun
embargo6
svp
part
in7
mod
pp
hd
prep
tegen8
obj1
name
Irak9
Figure 2: A dependency tree for a question: Wan-
neer stelde de Verenigde Naties een embargo in
tegen Irak? (When did the United Nations declare
the embargo against Iraq?)
between the compositional parts). Type layers in-
clude only specific types of tokens in the corpus, e.g.
named entities or compounds (see table 2).
Table 2: Type layers
compound compounds
ne named entities
neLOC location names
nePER person names
neORG organization names
Annotation layers include only the labels of (certain)
token types. So far, we defined only one annotation
layer for named entity labels. This layer may contain
the items ?ORG?, ?PER? or ?LOC? if such a named
entity occurs in the text passage.
3 Query formulation
Questions are analyzed in the same way as sentences
in documents. Hence, we can extract appropriate
units from analyzed questions to be matched with
the various layers in the index. For example, we
can extract root-head word pairs to be matched with
the RootHead layer. In this way, each layer can be
queried using keywords of the same type. Further-
more, we can also use linguistic labels to restrict our
query terms in several ways. For example, we can
use part-of-speech labels to exclude keywords of a
certain word class. We can also use the syntactic re-
lation name to define query constraints. Each token
layer can be restricted in this way (even if the feature
used for the restriction is not part of the layer). For
941
example, we can limit our set of root keywords to
nouns even though part-of-speech labels are not part
of the root layer. We can also combine constraints,
for example, RootPOS keywords can be restricted to
nouns that are in an object relation within the ques-
tion.
Another feature of Lucene is the support of key-
word weights. Keywords can be ?boosted? using so-
called ?boost factors?. Furthermore, keywords can
also be marked as ?required?. These two features
can be applied to all kinds of keywords (token layer,
type layer, annotation layer keywords, and restricted
keywords).
The following list summarizes possible keyword
types in our passage retrieval component:
basic: a keyword in one of the index layers
restricted: token-layer keywords can be restricted to a certain
word class and/or a certain relation type. We use only the
following word class restrictions: noun, name, adjective,
verb; and the following relation type restrictions: direct
object, modifier, apposition and subject
weighted: keywords can be weighted using a boost factor
required: keywords can be marked as required
Query keywords from all types can be combined into
a single query. We connect them in a disjunctive way
which is the default operation in Lucene. The query
engine provides ranked query results and, therefore,
each disjunction may contribute to the ranking of the
retrieved documents but does not harm the query if
it does not produce any matching results. We may,
for example, form a query with the following ele-
ments: (1) all plain text tokens; (2) named entities
(ne) boosted with factor 2; (3) RootHead bigrams
where the root is in an object relation; (4) RootRel
keywords for all nouns. Applying these parame-
ters to the question in figure 2 we get the following
query:1
text:(Irak embargo Verenigde Naties stelde)
ne:(Irak?2 Verenigde_Naties?2)
RootHead:(Irak/tegen embargo/stel_in)
RootRel:(embargo/obj1)
Now, query terms from various keyword types may
refer to the same index layer. For example, we may
use weighted plain text keywords restricted to nouns
together with unrestricted plain text keywords. To
1Note that stop words have been removed.
combine them we use a preference mechanism to
keep queries simple and to avoid disjunctions with
conflicting keyword parameters: (a) Restricted key-
word types are more specific than basic keywords;
(b) Keywords restricted in relation type and POS are
more specific than keywords with only one restric-
tion; (c) Relation type restrictions are more specific
than POS restrictions. Using these rules we define
that weights of more specific keywords overwrite
weights of less specific ones. Furthermore, we de-
fine that the ?required-marker? (?+?) overwrites key-
word weights. Using these definitions we would get
the following query if we add two elements to the
query from above: (5) plain text keywords in an ob-
ject relation with boost factor 3 and (6) plain text
keywords labeled as names marked as required.
text:(Irak?3 embargo?3 +Verenigde +Naties
stelde)
ne:(Irak?2 Verenigde_Naties?2)
RootHead:(Irak/tegen embargo/stel_in)
RootRel:(embargo/obj1)
Finally, we can also use the question type deter-
mined by question analysis in the retrieval compo-
nent. The question type corresponds to the expected
answer type, i.e. we expect an entity of that type in
the relevant text passages. In some cases, the ques-
tion type can be mapped to one of the named entity
labels assigned by the parser, e.g. a name question is
looking for names of persons (ne = PER), a question
for a capital is looking for a location (ne = LOC) and
a question for organizations is looking for the name
of an organization (ne = ORG). Hence, we can add
another keyword type, the expected answer type to
be matched with named entity labels in the ne layer,
cf. (Prager et al, 2000).
There are many possible combinations of restric-
tions even with the small set of POS labels and rela-
tion types listed above. However, many of them are
useless because they cannot be instantiated. For ex-
ample, an adjective cannot appear in subject relation
to its head. For simplicity we limit ourselves to the
following eight combined restrictions (POS + rela-
tion type): names + {direct object, modifier, apposi-
tion, subject} and nouns + {direct object, modifier,
apposition, subject}. These can be applied to all to-
ken layers in the same way as the other restrictions
using single constraints.
Altogether we have 109 different keyword types
942
using the layers and the restrictions defined above.
Now the question is to select appropriate keyword
types among them with the optimal parameters
(weights) to maximize retrieval performance. The
following section describes the optimization proce-
dure used to adjust query parameters.
4 Optimization of query parameters
In the previous sections we have seen the internal
structure of the multi-layer index and the queries we
use in our passage retrieval component. Now we
have to address the question of how to select layers
and restrict keywords to optimize the performance
of the system according to the QA task. For this
we employ an automatic optimization procedure that
learns appropriate parameter settings from example
data. We use annotated training material that is de-
scribed in the next section. Thereafter, the optimiza-
tion procedure is introduced.
4.1 CLEF questions and evaluation
We used results from the CLEF competition on
Dutch QA from the years 2003 and 2004 for train-
ing and evaluation. They contain natural language
questions annotated with their answers found in the
CLEF corpus (answer strings and IDs of documents
in which the answer was found). Most of the ques-
tions are factoid questions such as ?Hoeveel inwon-
ers heeft Zweden?? (How many inhabitants does
Sweden have?). Altogether there are 631 questions
with 851 answers.2
Standard measures for evaluating information re-
trieval results are precision and recall. However,
for QA several other specialized measures have
been proposed, e.g. mean reciprocal rank (MRR)
(Vorhees, 1999), coverage and redundancy (Roberts
and Gaizauskas, 2004). MRR accounts only for the
first passage retrieved containing an answer and dis-
regards the following passages. Coverage and re-
dundancy on the other hand disregard the ranking
completely and focus on the sets of passages re-
trieved. However, in our QA system, the IR score
2Each question may have multiple possible answers. We
also added some obvious answers which were not in the original
test set when encountering them in the corpus. For example,
names and numbers can be spelled differently (Kim Jong Il vs.
Kim Jong-Il, Saoedi-Arabie? vs. Saudi-Arabie?, bijna vijftig jaar
vs. bijna 50 jaar)
(on which the retrieval ranking is based) is one of
the clues used by the answer identification modules.
Therefore, we use the mean of the total reciprocal
ranks (MTRR), cf. (Radev et al, 2002), to combine
features of all three measures:
MTRR = 1x
x
?
i=1
?
d?Ai
1
rankRi(d)
Ai is the set of retrieved passages containing an
answer to question number i (subset of Ri) and
rankRi(d) is the rank of document d in the list of
retrieved passages Ri.
In our experiments we used the provided answer
string rather than the document ID to judge if a re-
trieved passage was relevant or not. In this way,
the IR engine may provide passages with correct an-
swers from other documents than the ones marked in
the test set. We do simple string matching between
answer strings and words in the retrieved passages.
Obviously, this introduces errors where the match-
ing string does not correspond to a valid answer in
the context. However, we believe that this does not
influence the global evaluation figure significantly
and therefore we use this approach as a reasonable
compromise when doing automatic evaluation.
4.2 Learning query parameters
As discussed earlier, there is a large variety of possi-
ble keyword types that can be combined to query the
multi-layer index. Furthermore, we have a number
of parameters to be set when formulating a query,
e.g. the keyword weights. Selecting the appropri-
ate keywords and parameters is not straightforward.
We like to carry out a systematic search for optimiz-
ing parameters rather than using our intuition. Here,
we use the information retrieval engine as a black
box with certain input parameters. We do not know
how the ranking is done internally or how the output
is influenced by parameter changes. However, we
can inspect and evaluate the output of the system.
Hence, we need an iterative approach for testing sev-
eral settings to optimize query parameters. The out-
put for each setting has to be evaluated according to
a certain objective function. For this, we need an au-
tomatic procedure because we want to check many
different settings in a batch run. The performance of
the system can be measured in several ways, e.g. us-
943
ing the MTRR scores described in the previous sec-
tion. We have chosen to use this measure and the
annotated CLEF questions to evaluate the retrieval
performance automatically.
We decided to use a simplified genetic algorithm
to optimize query parameters. This algorithm is
implemented as an iterative ?trial-and-error beam
search? through possible parameter settings. The
optimization loop works as follows (using a sub-set
of the CLEF questions):
1. Run initial queries (one keyword type per IR run) with
default weights.
2. Produce a number of new settings by combining two pre-
vious ones (= crossover). For this, select two settings
from an N-best list from the previous IR runs. Apply mu-
tation operations (see next step) until the new settings are
unique (among all settings we have tried so far).
3. Change some of the new settings at random (= mutation)
using pre-defined mutation operations.
4. Run the queries using the new settings and evaluate the
retrieval output (determine fitness).
5. Continue with 2 until some stop condition is satisfied.
This optimization algorithm is very simple but re-
quires some additional parameters. First of all, we
have to set the size of the population, i.e. the num-
ber of IR runs (individuals) to be kept for the next
iteration. We decided to keep the population small
with only 25 individuals. Then we have to decide
how to evaluate fitness to rank retrieval results. This
is done using the MTRR measure. Natural selection
using these rankings is simplified to a top-N search
without giving individuals with lower fitness values
a chance to survive. This also means that we can
update the population directly when a new IR run is
finished. We also have to set a maximum number of
new settings to be created. In our experiments we
limit the process to a maximum of 50 settings that
may be tried simultaneously. A new setting is cre-
ated as soon as there is a spot available.
An important part of the algorithm is the com-
bination of parameters. We simply merge the set-
tings of two previous runs (parents) to produce a
new setting (a child). That means that all keyword
types (with their restrictions) from both parents are
included in the child?s setting. Parents are selected at
random without any preference mechanism. We also
use a very simple strategy in cases where both par-
ents contain the same keyword type. In these cases
we compute the arithmetic mean of the weight as-
signed to this type in the parents? settings (default
weight is one). If the keyword type is marked as re-
quired in one of the parents, it will also be marked as
required in the child?s setting (which will overwrite
the keyword weight if it is set in the other parent).
Another important principle in genetic optimiza-
tion is mutation. It refers to a randomized modifi-
cation of settings when new individuals are created.
First, we apply mutation operations where new set-
tings are not unique.3 Secondly, mutation operations
are applied with fixed probabilities to new settings.
In most genetic algorithms, settings are converted
to genes consisting of bit strings. A mutation op-
eration is then defined as flipping the value of one
randomly chosen bit. In our approach, we do not
use bit strings but define several mutation operations
to modify parameters directly. The following opera-
tions have been defined:
? a new keyword type is added to new settings
with a chance of 0.2
? a keyword type is removed from the settings
with a chance of 0.1
? a keyword weight (boost factor) is modified by
a random value between -5 and 5 with a chance
of 0.2 (but only if the weight remains a positive
value)
? a keyword type is marked as required with a
chance of 0.01
All these parameters are intuitively chosen. We as-
signed rather high probabilities to the mutation op-
erations to reduce the risk of local maximum traps.
Note that there is no obvious condition for termi-
nation. In randomized approaches like this one the
development of the fitness score is most likely not
monotonic and therefore, it is hard to predict when
we should stop the optimization process. However,
we expect the scores to converge at some point and
we may stop if a certain number of new settings does
not improve the scores anymore.
3We require unique settings in our implementation because
we want to avoid re-computation of fitness values for settings
that have been tried already. ?Good? settings survive anyway
using our top-N selection approach.
944
5 Experiments
We selected a random set of 420 questions from the
CLEF data for training and used the remaining 150
questions for evaluation. We used the optimization
algorithm with the settings as described above. IR
was run in parallel on 3-7 Linux workstations on a
local network. We retrieved a maximum of 20 pas-
sages per question. For each setting we computed
the fitness scores for the training set and the eval-
uation set using MTRR. The top scores have been
printed after each 10 runs and compared to the eval-
uation scores. Figure 3 shows a plot of the fitness
score development throughout the optimization pro-
cess in comparison with the evaluation scores.
 0.85
 0.9
 0.95
 1
 1.05
400 800 1200 1600 2000 2400 2800 3200
a
n
sw
e
r 
st
rin
g 
M
TR
R
number of settings
evaluation base-line: 0.8799
training
evaluation
Figure 3: Parameter optimization
The base-line of 0.8799 refers to the retrieval re-
sult on evaluation data when using traditional IR
with plain text keywords only (i.e. using the text
layer, Dutch stemming and stop word removal). The
base-line performance on training data is slightly
worse with 0.8224 MTRR. After 1130 settings the
MTRR scores increased to 0.9446 for training data
and 1.0247 for evaluation data. Thereafter we can
observe a surprising drop in evaluation scores to
around 0.97 in MTRR. This might be due to over-
fitting although the drop seems to be rather radi-
cal. After that the curve of the evaluation scores
goes back to about the same level as achieved be-
fore and the training curve seems to level out. The
MTRR score after 3200 settings is at 1.0169 on eval-
uation data which is a statistically significant im-
provement of the baseline score (tested using the
Wilcoxon matched-pairs signed-ranks test at p <
0.01). MTRR measured on document IDs and eval-
uation data did also increase from 0.5422 to 0.6215
which is statistically significant at p?0.02. Coverage
went up from 78.68% to 81.62% on evaluation data
and the redundancy was improved from 3.824 to
4.272 (significance tests have not been carried out).
Finally, the QA performance using Joost with only
the IR based strategy was increased from 0.289 (us-
ing CLEF scores) to 0.331. This, however, is not sta-
tistically significant according to the Wilcoxon test
and may be due to chance.
Table 3: Optimized parameters (3200 settings)
weighted keywords required keywords
layer restriction weight layer restriction
text 7.43 root name
text name 11.94
text adj 9.14 RootPOS
text mod 5.83 RootPOS obj1
text verb 4.33 RootPOS noun-mod
text noun-app 3.70
root 4.45 RootRel
root noun-su 2.65 RootRel app
root name-mod 9.71 RootRel noun-app
root noun-obj1 0.09 RootRel noun-mod
root mod 0.81 RootRel noun-obj1
root verb 0.01
RootHead noun-app 7.65 RootRelHead su
RootHead noun-mod 5.24 RootRelHead adj
RootHead name-su 1 RootRelHead name-app
RootRel mod 4.45
RootRel name-app 2.17 Q-type
RootRel noun 2.49
RootRelHead obj1 1.60
RootRelHead name-su 1
nePER 0.91
Table 3 shows the features and weights selected in
the training process. The largest weights are given
to names in the text layer, to root forms of names in
modifier relations and to plain text adjectives. Many
keyword types use ?name? or ?noun? as POS restric-
tion. A surprisingly large number of keyword types
are marked as required. Some of them overlap with
each other and are therefore redundant. For exam-
ple, all RootPOS keywords are marked as required
and therefore, the restrictions of RootPOS keywords
are useless because they do not alter the query. How-
ever, in other cases overlapping keyword type defini-
tions do influence the query. For example, RootRel
keywords in general are marked as required. How-
ever, other type definitions replace some of them
with weighted keywords, e.g., RootRel noun key-
945
words. Finally, some of them may be changed back
to required keywords, e.g., RootRel keywords of
nouns in a modifier relation.
6 Conclusions
In this paper we describe an approach for integrat-
ing linguistic information derived from dependency
analyses in passage retrieval for question answer-
ing. Our retrieval component uses a multi-layer in-
dex containing various combinations of linguistic
features and syntactic units extracted from a fully
analyzed corpus of unrestricted Dutch text. Natu-
ral language questions are parsed in the same way.
Their analyses are used to build complex queries to
our extended index. We demonstrated a genetic al-
gorithm for optimizing query parameters to improve
the retrieval performance. The system was trained
on questions from the CLEF competition on open-
domain question answering for Dutch which are an-
notated with corresponding answers in the corpus.
We could show a significant improvement of about
15% in mean total reciprocal rank using extended
queries with optimized parameters compared with
the base-line of traditional information retrieval us-
ing plain text keywords.
References
Raffaella Bernardi, Valentin Jijkoun, Gilad Mishne, and
Maarten de Rijke. 2003. Selectively using linguistic
resources throughout the question answering pipeline.
In Proceedings of the 2nd CoLogNET-ElsNET Sympo-
sium.
Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001. Alpino: Wide coverage computational analysis
of Dutch. In Computational Linguistics in the Nether-
lands CLIN, 2000. Rodopi.
Gosse Bouma, Jori Mur, and Gertjan van Noord. 2005.
Reasoning over dependency relations for QA. In
Knowledge and Reasoning for Answering Questions
(KRAQ?05), IJCAI Workshop, Edinburgh, Scotland.
Joel L. Fagan. 1987. Automatic phrase indexing for
document retrieval. In SIGIR ?87: Proceedings of
the 10th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 91?101, New York, NY, USA. ACM Press.
Vera Hollink, Jaap Kamps, Christof Monz, and Maarten
de Rijke. 2003. Monolingual document retrieval for
European languages. Information Retrieval, (6).
Apache Jakarta. 2004. Apache Lucene - a high-
performance, full-featured text search engine library.
http://lucene.apache.org/java/docs/index.html.
Valentin Jijkoun, Jori Mur, and Maarten de Rijke. 2004.
Information extraction for question answering: Im-
proving recall through syntactic patterns. In Proceed-
ings of COLING-2004.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering. In
Proceedings of the EACL-2003 Workshop on Natural
Language Processing for Question Answering.
Robert Krovetz. 1993. Viewing morphology as an infer-
ence process,. In Proceedings of the Sixteenth Annual
International ACM SIGIR Conference on Research
and Development in Information Retrieval, pages 191?
203.
Dan Moldovan, Sanda Harabagiu, Roxana Girju, Paul
Morarescu, Finley Lacatusu, Adrian Novischi, Adri-
ana Badulescu, and Orest Bolohan. 2002. LCC tools
for question answering. In Proceedings of TREC-11.
Gu?nter Neumann and Bogdan Sacaleanu. 2004. Experi-
ments on robust NL question interpretation and multi-
layered document annotation for a cross-language
question/answering system. In Proceedings of the
CLEF 2004 working notes of the QA@CLEF, Bath.
John Prager, Eric Brown, Anni Cohen, Dragomir Radev,
and Valerie Samn. 2000. Question-answering by
predictive annotation. In In Proceedings of the 23rd
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval,
Athens, Greece, July.
Dragomir R. Radev, Hong Qi, Harris Wu, and Weiguo
Fan. 2002. Evaluating web-based question answering
systems. In Proceedings of LREC, Las Palmas, Spain.
Ian Roberts and Robert Gaizauskas. 2004. Evaluating
passage retrieval approaches for question answering.
In Proceedings of the 26th European Conference on
Information Retrieval (ECIR), pages 72?84.
Tomek Strzalkowski, Louise Guthrie, Jussi Karlgren, Jim
Leistensnider, Fang Lin, Jose? Pe?rez-Carballo, Troy
Straszheim, Jin Wang, and Jon Wilding. 1996. Nat-
ural language information retrieval: TREC-5 report.
Ellen M. Vorhees. 1999. The TREC-8 question answer-
ing track report. In Proceedings of TREC-8, pages 77?
82.
Chengxiang Zhai. 1997. Fast statistical parsing of noun
phrases for document indexing. In Proceedings of the
fifth conference on Applied natural language process-
ing, pages 312?319, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
946
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 866?873,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Finding Synonyms Using Automatic Word Alignment and Measures of
Distributional Similarity
Lonneke van der Plas & Jo?rg Tiedemann
Alfa-Informatica
University of Groningen
P.O. Box 716
9700 AS Groningen
The Netherlands
{vdplas,tiedeman}@let.rug.nl
Abstract
There have been many proposals to ex-
tract semantically related words using
measures of distributional similarity, but
these typically are not able to distin-
guish between synonyms and other types
of semantically related words such as
antonyms, (co)hyponyms and hypernyms.
We present a method based on automatic
word alignment of parallel corpora con-
sisting of documents translated into mul-
tiple languages and compare our method
with a monolingual syntax-based method.
The approach that uses aligned multilin-
gual data to extract synonyms shows much
higher precision and recall scores for the
task of synonym extraction than the mono-
lingual syntax-based approach.
1 Introduction
People use multiple ways to express the same idea.
These alternative ways of conveying the same in-
formation in different ways are referred to by the
term paraphrase and in the case of single words
sharing the same meaning we speak of synonyms.
Identification of synonyms is critical for many
NLP tasks. In information retrieval the informa-
tion that people ask for with a set of words may be
found in in a text snippet that comprises a com-
pletely different set of words. In this paper we
report on our findings trying to automatically ac-
quire synonyms for Dutch using two different re-
sources, a large monolingual corpus and a multi-
lingual parallel corpus including 11 languages.
A common approach to the automatic extrac-
tion of semantically related words is to use dis-
tributional similarity. The basic idea behind this is
that similar words share similar contexts. Systems
based on distributional similarity provide ranked
lists of semantically related words according to
the similarity of their contexts. Synonyms are ex-
pected to be among the highest ranks followed by
(co)hyponyms and hypernyms, since the highest
degree of semantic relatedness next to identity is
synonymy.
However, this is not always the case. Sev-
eral researchers (Curran and Moens (2002), Lin
(1998), van der Plas and Bouma (2005)) have used
large monolingual corpora to extract distribution-
ally similar words. They use grammatical rela-
tions1 to determine the context of a target word.
We will refer to such systems as monolingual
syntax-based systems. These systems have proven
to be quite successful at finding semantically re-
lated words. However, they do not make a clear
distinction between synonyms on the one hand and
related words such as antonyms, (co)hyponyms,
hypernyms etc. on the other hand.
In this paper we have defined context in a mul-
tilingual setting. In particular, translations of a
word into other languages found in parallel cor-
pora are seen as the (translational) context of that
word. We assume that words that share transla-
tional contexts are semantically related. Hence,
relatedness of words is measured using distribu-
tional similarity in the same way as in the mono-
lingual case but with a different type of context.
Finding translations in parallel data can be approx-
1One can define the context of a word in a non-syntactic
monolingual way, that is as the document in which it occurs
or the n words surrounding it. From experiments we have
done and also building on the observations made by other
researchers (Kilgarriff and Yallop, 2000) we can state that
this approach generates a type of semantic similarity that is
of a looser kind, an associative kind,for example doctor and
disease. These words are typically not good candidates for
synonymy.
866
imated by automatic word alignment. We will
refer to this approach as multilingual alignment-
based approaches. We expect that these transla-
tions will give us synonyms and less semantically
related words, because translations typically do
not expand to hypernyms, nor (co)hyponyms, nor
antonyms. The word apple is typically not trans-
lated with a word for fruit nor pear, and neither is
good translated with a word for bad.
In this paper we use both monolingual syntax-
based approaches and multilingual alignment-
based approaches and compare their performance
when using the same similarity measures and eval-
uation set.
2 Related Work
Monolingual syntax-based distributional similar-
ity is used in many proposals to find semanti-
cally related words (Curran and Moens (2002),
Lin (1998), van der Plas and Bouma (2005)).
Several authors have used a monolingual par-
allel corpus to find paraphrases (Ibrahim et al
(2003), Barzilay and McKeown (2001)). How-
ever, bilingual parallel corpora have mostly been
used for tasks related to word sense disambigua-
tion such as target word selection (Dagan et al,
1991) and separation of senses (Dyvik, 1998). The
latter work derives relations such as synonymy and
hyponymy from the separated senses by applying
the method of semantic mirrors.
Turney (2001) reports on an PMI and IR driven
approach that acquires data by querying a Web
search engine. He evaluates on the TOEFL test in
which the system has to select the synonym among
4 candidates.
Lin et al (2003) try to tackle the problem of
identifying synonyms among distributionally re-
lated words in two ways: Firstly, by looking at
the overlap in translations of semantically similar
words in multiple bilingual dictionaries. Secondly,
by looking at patterns specifically designed to fil-
ter out antonyms. They evaluate on a set of 80
synonyms and 80 antonyms from a thesaurus.
Wu and Zhou?s (2003) paper is most closely re-
lated to our study. They report an experiment on
synonym extraction using bilingual resources (an
English-Chinese dictionary and corpus) as well
as monolingual resources (an English dictionary
and corpus). Their monolingual corpus-based ap-
proach is very similar to our monolingual corpus-
based approach. The bilingual approach is dif-
ferent from ours in several aspects. Firstly, they
do not take the corpus as the starting point to re-
trieve word alignments, they use the bilingual dic-
tionary to retrieve multiple translations for each
target word. The corpus is only employed to as-
sign probabilities to the translations found in the
dictionary. Secondly, the authors use a parallel
corpus that is bilingual whereas we use a multi-
lingual corpus containing 11 languages in total.
The authors show that the bilingual method out-
performs the monolingual methods. However a
combination of different methods leads to the best
performance.
3 Methodology
3.1 Measuring Distributional Similarity
An increasingly popular method for acquiring se-
mantically similar words is to extract distribution-
ally similar words from large corpora. The under-
lying assumption of this approach is that seman-
tically similar words are used in similar contexts.
The contexts a given word is found in, be it a syn-
tactic context or an alignment context, are used as
the features in the vector for the given word, the
so-called context vector. The vector contains fre-
quency counts for each feature, i.e., the multiple
contexts the word is found in.
Context vectors are compared with each other
in order to calculate the distributional similarity
between words. Several measures have been pro-
posed. Curran and Moens (2002) report on a large-
scale evaluation experiment, where they evaluated
the performance of various commonly used meth-
ods. Van der Plas and Bouma (2005) present a
similar experiment for Dutch, in which they tested
most of the best performing measures according
to Curran and Moens (2002). Pointwise Mutual
Information (I) and Dice? performed best in their
experiments. Dice is a well-known combinatorial
measure that computes the ratio between the size
of the intersection of two feature sets and the sum
of the sizes of the individual feature sets. Dice?
is a measure that incorporates weighted frequency
counts.
Dice? =
2
?
f min(I(W1, f), I(W2, f))
?
f I(W1, f) + I(W2, f)
,where f is the feature
W1 and W2 are the two words that are being compared,
and I is a weight assigned to the frequency counts.
867
3.2 Weighting
We will now explain why we use weighted fre-
quencies and which formula we use for weighting.
The information value of a cell in a word vec-
tor (which lists how often a word occurred in a
specific context) is not equal for all cells. We
will explain this using an example from mono-
lingual syntax-based distributional similarity. A
large number of nouns can occur as the subject of
the verb have, for instance, whereas only a few
nouns may occur as the object of squeeze. Intu-
itively, the fact that two nouns both occur as sub-
ject of have tells us less about their semantic sim-
ilarity than the fact that two nouns both occur as
object of squeeze. To account for this intuition,
the frequency of occurrence in a vector can be re-
placed by a weighted score. The weighted score
is an indication of the amount of information car-
ried by that particular combination of a noun and
its feature.
We believe that this type of weighting is benefi-
cial for calculating similarity between word align-
ment vectors as well. Word alignments that are
shared by many different words are most probably
mismatches.For this experiment we used Pointwise MutualInformation (I) (Church and Hanks, 1989).
I(W,f) = log P (W,f)P (W )P (f)
,where W is the target word
P(W) is the probability of seeing the word
P(f) is the probability of seeing the feature
P(W,f) is the probability of seeing the word and the feature
together.
3.3 Word Alignment
The multilingual approach we are proposing relies
on automatic word alignment of parallel corpora
from Dutch to one or more target languages. This
alignment is the basic input for the extraction of
the alignment context as described in section 5.2.2.
The alignment context is then used for measuring
distributional similarity as introduced above.
For the word alignment, we apply standard tech-
niques derived from statistical machine transla-
tion using the well-known IBM alignment mod-
els (Brown et al, 1993) implemented in the open-
source tool GIZA++ (Och, 2003). These mod-
els can be used to find links between words in a
source language and a target language given sen-
tence aligned parallel corpora. We applied stan-
dard settings of the GIZA++ system without any
optimisation for our particular input. We also used
plain text only, i.e. we did not apply further pre-
processing except tokenisation and sentence split-
ting. Additional linguistic processing such as lem-
matisation and multi-word unit detection might
help to improve the alignment but this is not part
of the present study.
The alignment models produced are asymmet-
ric and several heuristics exist to combine direc-
tional word alignments to improve alignment ac-
curacy. We believe, that precision is more cru-
cial than recall in our approach and, therefore, we
apply a very strict heuristics namely we compute
the intersection of word-to-word links retrieved by
GIZA++. As a result we obtain partially word-
aligned parallel corpora from which translational
context vectors are built (see section 5.2.2). Note,
that the intersection heuristics allows one-to-one
word links only. This is reasonable for the Dutch
part as we are only interested in single words and
their synonyms. However, the distributional con-
text of these words defined by their alignments is
strongly influenced by this heuristics. Problems
caused by this procedure will be discussed in de-
tail in section 7 of our experiments.
4 Evaluation Framework
In the following, we describe the data used and
measures applied.
The evaluation method that is most suitable
for testing with multiple settings is one that uses
an available resource for synonyms as a gold
standard. In our experiments we apply auto-
matic evaluation using an existing hand-crafted
synonym database, Dutch EuroWordnet (EWN,
Vossen (1998)).
In EWN, one synset consists of several syn-
onyms which represent a single sense. Polyse-
mous words occur in several synsets. We have
combined for each target word the EWN synsets
in which it occurs. Hence, our gold standard con-
sists of a list of all nouns found in EWN and their
corresponding synonyms extracted by taking the
union of all synsets for each word. Precision is
then calculated as the percentage of candidate syn-
onyms that are truly synonyms according to our
gold standard. Recall is the percentage of the syn-
onyms according to EWN that are indeed found
by the system. We have extracted randomly from
all synsets in EWN 1000 words with a frequency
868
above 4 for which the systems under comparison
produce output.
The drawback of using such a resource is that
coverage is often a problem. Not all words that
our system proposes as synonyms can be found in
Dutch EWN. Words that are not found in EWN
are discarded.2 . Moreover, EWN?s synsets are not
exhaustive. After looking at the output of our best
performing system we were under the impression
that many correct synonyms selected by our sys-
tem were classified as incorrect by EWN. For this
reason we decided to run a human evaluation over
a sample of 100 candidate synonyms classified as
incorrect by EWN.
5 Experimental Setup
In this section we will describe results from the
two synonym extraction approaches based on dis-
tributional similarity: one using syntactic context
and one using translational context based on word
alignment and the combination of both. For both
approaches, we used a cutoff n for each row in our
word-by-context matrix. A word is discarded if
the row marginal is less than n. This means that
each word should be found in any context at least
n times else it will be discarded. We refer to this
by the term minimum row frequency. The cutoff is
used to make the feature space manageable and to
reduce noise in the data. 3
5.1 Distributional Similarity Based on
Syntactic Relations
This section contains the description of the syn-
onym extraction approach based on distributional
similarity and syntactic relations. Feature vectors
for this approach are constructed from syntacti-
cally parsed monolingual corpora. Below we de-
scribe the data and resources used, the nature of
the context applied and the results of the synonym
extraction task.
5.1.1 Data and Resources
As our data we used the Dutch CLEF QA cor-
pus, which consists of 78 million words of Dutch
2Note that we use the part of EWN that contains only
nouns
3We have determined the optimum in F-score for the
alignment-based method, the syntax-based method and the
combination independently by using a development set of
1000 words that has no overlap with the test set used in eval-
uation. The minimum row frequency was set to 2 for all
alignment-based methods. It was set to 46 for the syntax-
based method and the combination of the two methods.
subject-verb cat eat
verb-object feed cat
adjective-noun black cat
coordination cat dog
apposition cat Garfield
prep. complement go+to work
Table 1: Types of dependency relations extracted
grammatical relation # pairs
subject 507K
object 240K
adjective 289K
coordination 400 K
apposition 109K
prep. complement 84K
total 1629K
Table 2: Number of word-syntactic-relation pairs
(types) per dependency relation with frequency >
1.
newspaper text (Algemeen Dagblad and NRC
Handelsblad 1994/1995). The corpus was parsed
automatically using the Alpino parser (van der
Beek et al, 2002; Malouf and van Noord, 2004).
The result of parsing a sentence is a dependency
graph according to the guidelines of the Corpus of
Spoken Dutch (Moortgat et al, 2000).
5.1.2 Syntactic Context
We have used several grammatical relations:
subect, object, adjective, coordination, apposi-
tion and prepositional complement. Examples are
given in table 1. Details on the extraction can be
found in van der Plas and Bouma (2005). The
number of pairs (types) consisting of a word and
a syntactic relation found are given in table 2. We
have discarded pairs that occur less than 2 times.
5.2 Distributional Similarity Based on Word
Alignment
The alignment approach to synonym extraction is
based on automatic word alignment. Context vec-
tors are built from the alignments found in a paral-
lel corpus. Each aligned word type is a feature in
the vector of the target word under consideration.
The alignment frequencies are used for weighting
the features and for applying the frequency cutoff.
In the following section we describe the data and
resources used in our experiments and finally the
results of this approach.
869
5.2.1 Data and Resources
Measures of distributional similarity usually re-
quire large amounts of data. For the alignment
method we need a parallel corpus of reasonable
size with Dutch either as source or as target lan-
guage. Furthermore, we would like to experiment
with various languages aligned to Dutch. The
freely available Europarl corpus (Koehn, 2003)
includes 11 languages in parallel, it is sentence
aligned, and it is of reasonable size. Thus, for
acquiring Dutch synonyms we have 10 language
pairs with Dutch as the source language. The
Dutch part includes about 29 million tokens in
about 1.2 million sentences. The entire corpus is
sentence aligned (Tiedemann and Nygaard, 2004)
which is a requirement for the automatic word
alignment described below.
5.2.2 Alignment Context
Context vectors are populated with the links to
words in other languages extracted from automatic
word alignment. We applied GIZA++ and the in-
tersection heuristics as explained in section . From
the word aligned corpora we extracted word type
links, pairs of source and target words with their
alignment frequency attached. Each aligned target
word type is a feature in the (translational) context
of the source word under consideration.
Note that we rely entirely on automatic process-
ing of our data. Thus, results from the automatic
word alignments include errors and their precision
and recall is very different for the various language
pairs. However, we did not assess the quality of
the alignment itself which would be beyond the
scope of this paper.
As mentioned earlier, we did not include any
linguistic pre-processing prior to the word align-
ment. However, we post-processed the alignment
results in various ways. We applied a simple lem-
matizer to the list of bilingual word type links
in order to 1) reduce data sparseness, and 2) to
facilitate our evaluation based on comparing our
results to existing synonym databases. For this
we used two resources: CELEX ? a linguistically
annotated dictionary of English, Dutch and Ger-
man (Baayen et al, 1993), and the Dutch snow-
ball stemmer implementing a suffix stripping al-
gorithm based on the Porter stemmer. Note that
lemmatization is only done for Dutch. Further-
more, we removed word type links that include
non-alphabetic characters to focus our investiga-
tions on ?real words?. In order to reduce alignment
noise, we also applied a frequency threshold to re-
move alignments that occur only once. Finally, we
restricted our study to Dutch nouns. Hence, we
extracted word type links for all words tagged as
noun in CELEX. We also included words which
are not found at all in CELEX assuming that most
of them will be productive noun constructions.
From the remaining word type links we popu-
lated the context vectors as described earlier. Ta-
ble 3 shows the number of context elements ex-
tracted in this manner for each language pair con-
sidered from the Europarl corpus4
#word-transl. pairs #word-transl. pairs
DA 104K FR 90K
DE 133K IT 96K
EL 60K PT 86K
EN 119K SV 97K
ES 119K ALL 994K
FI 89K
Table 3: Number of word-translation pairs for dif-
ferent languages with alignment frequency > 1
6 Results and Discussion
Table 4 shows the precision recall en F-score for
the different methods. The first 10 rows refer
to the results for all language pairs individually.
The 11th row corresponds to the setting in which
all alignments for all languages are combined.
The penultimate row shows results for the syntax-
based method and the last row the combination of
the syntax-based and alignment-based method.
Judging from the precision, recall and F-score
in table 4 Swedish is the best performing lan-
guage for Dutch synonym extraction from parallel
corpora. It seems that languages that are similar
to the target language, for example in word or-
der, are good candidates for finding synonyms at
high precision rates. Also the fact that Dutch and
Swedish both have one-word compounds avoids
mistakes that are often found with the other lan-
guages. However, judging from recall (and F-
score) French is not a bad candidate either. It is
possible that languages that are lexically different
from the target language provide more synonyms.
The fact that Finnish and Greek do not gain high
scores might be due to the fact that there are only
a limited amount of translational contexts (with a
frequency > 1) available for these language (as
is shown in table 3). The reasons are twofold.
4abbreviations taken from the ISO-639 2-letter codes
870
# candidate synonyms
1 2 3
Prec Rec F-sc Prec Rec F-sc Prec Rec F-sc
DA 19.8 5.1 8.1 15.5 7.6 10.2 13.3 9.4 11.0
DE 21.2 5.4 8.6 16.1 7.9 10.6 13.1 9.3 10.9
EL 18.2 4.5 7.2 14.0 6.5 8.9 11.8 7.9 9.4
EN 19.5 5.3 8.3 14.7 7.8 10.2 12.4 9.7 10.9
ES 18.4 5.0 7.9 14.7 7.8 10.2 12.1 9.4 10.6
FI 18.0 3.9 6.5 14.3 5.6 8.1 12.1 6.5 8.5
FR 20.3 5.5 8.7 15.8 8.3 10.9 13.0 10.1 11.4
IT 18.7 4.9 7.8 14.7 7.5 9.9 12.3 9.2 10.5
PT 17.7 4.8 7.6 14.0 7.4 9.7 11.6 8.9 10.1
SV 22.3 5.6 9.0 16.4 7.9 10.7 13.3 9.3 10.9
ALL 22.5 6.4 10.0 16.6 9.4 12.0 13.7 11.5 12.5
SYN 8.8 2.5 3.9 6.9 4.0 5.09 5.9 5.1 5.5
COMBI 19.9 5.8 8.9 14.5 8.4 10.6 11.7 10.1 10.9
Table 4: Precision, recall and F-score (%) at increasing number of candidate synonyms
Firstly, for Greek and Finnish the Europarl corpus
contains less data. Secondly, the fact that Finnish
is a language that has a lot of cases for nouns,
might lead to data sparseness and worse accuracy
in word alignment.
The results in table 4 also show the difference in
performance between the multilingual alignment-
method and the syntax-based method. The mono-
lingual alignment-based method outperforms the
syntax-based method by far. The syntax-based
method that does not rely on scarce multilingual
resources is more portable and also in this exper-
iment it makes use of more data. However, the
low precision scores of this method are not con-
vincing. Combining both methods does not result
in better performance for finding synonyms. This
is in contrast with the results reported by Wu and
Zhou (2003). This might well be due to the more
sophisticated method they use for combining dif-
ferent methods, which is a weighted combination.
The precision scores are in line with the scores
reported by Wu and Zhou (2003) in a similar ex-
periment discussed under related work. The re-
call we attain however is more than three times
higher. These differences can be due to differences
between their approach such as starting from a
bilingual dictionary for acquiring the translational
context versus using automatic word alignments
from a large multilingual corpus directly. Further-
more, the different evaluation methods used make
comparison between the two approaches difficult.
They use a combination of the English Word-
Net (Fellbaum, 1998) and Roget thesaurus (Ro-
get, 1911) as a gold standard in their evaluations.
It is obvious that a combination of these resources
leads to larger sets of synonyms. This could ex-
plain the relatively low recall scores. It does how-
ever not explain the similar precision scores.
We conducted a human evaluation on a sample
of 100 candidate synonyms proposed by our best
performing system that were classified as incor-
rect by EWN. Ten evaluators (authors excluded)
were asked to classify the pairs of words as syn-
onyms or non-synonyms using a web form of the
format yes/no/don?t know. For 10 out of the 100
pairs all ten evaluators agreed that these were syn-
onyms. For 37 of the 100 pairs more than half of
the evaluators agreed that these were synonyms.
We can conclude from this that the scores provided
in our evaluations based on EWN (table 4) are too
pessimistic. We believe that the actual precision
scores lie 10 to 37 % higher than the 22.5 % re-
ported in table 4. Over and above, this indicates
that we are able to extract automatically synonyms
that are not yet covered by available resources.
7 Error Analysis
In table 5 some example output is given for the
method combining word alignments of all 10 for-
eign languages as opposed to the monolingual
syntax-based method. These examples illustrate
the general patterns that we discovered by looking
into the results for the different methods.
The first two examples show that the syntax-
871
ALIGN(ALL) SYNTAX
consensus eensgezindheid evenwicht
consensus consensus equilibrium
herfst najaar winter
autumn autumn winter
eind einde begin
end end beginning
armoede armoedebestrijding werkloosheid
poverty poverty reduction unemployment
alcohol alcoholgebruik drank
alcohol alcohol consumption liquor
bes charme perzik
berry charm peach
definitie definie criterium
definition define+incor.stemm. criterion
verlamming lam verstoring
paralysis paralysed disturbance
Table 5: Example candidate synonyms at 1st rank
and their translations in italics
based method often finds semantically related
words whereas the alignment-based method finds
synonyms. The reasons for this are quite obvious.
Synonyms are likely to receive identical transla-
tions, words that are only semantically related are
not. A translator would not often translate auto
(car) with vrachtwagen (truck). However, the two
words are likely to show up in identical syntactic
relations, such as being the object of drive or ap-
pearing in coordination with motorcycle.
Another observation that we made is that the
syntax-based method often finds antonyms such as
begin (beginning) for the word einde (end). Expla-
nations for this are in line with what we said about
the semantically related words: Synonyms are
likely to receive identical translations, antonyms
are not but they do appear in similar syntactic con-
texts.
Compounds pose a problem for the alignment-
method. We have chosen intersection as align-
ment method. It is well-known that this method
cannot cope very well with the alignment of com-
pounds because it only allows one-to-one word
links. Dutch uses many one-word compounds that
should be linked to multi-word counterparts in
other languages. However, using intersection we
obtain only partially correct alignments and this
causes many mistakes in the distributional simi-
larity algorithm. We have given some examples in
rows 4 and 5 of table 5.
We have used the distributional similarity score
only for ranking the candidate synonyms. In some
cases it seems that we should have used it to set a
threshold such as in the case of berry and charm.
These two words share one translational context :
the article el in Spanish. The distributional sim-
ilarity score in such cases is often very low. We
could have filtered some of these mistakes by set-
ting a threshold.
One last observation is that the alignment-based
method suffers from incorrect stemming and the
lack of sufficient part-of-speech information. We
have removed all context vectors that were built
for a word that was registered in CELEX with a
PoS-tag different from ?noun?. But some words
are not found in CELEX and although they are
not of the word type ?noun? their context vec-
tors remain in our data. They are stemmed using
the snowball stemmer. The candidate synonym
denie is a corrupted verbform that is not found
in CELEX. Lam is ambiguous between the noun
reading that can be translated in English with lamb
and the adjective lam which can be translated with
paralysed. This adjective is related to the word
verlamming (paralysis), but would have been re-
moved if the word was correctly PoS-tagged.
8 Conclusions
Parallel corpora are mostly used for tasks related
to WSD. This paper shows that multilingual word
alignments can be applied to acquire synonyms
automatically without the need for resources such
as bilingual dictionaries. A comparison with a
monolingual syntax-based method shows that the
alignment-based method is able to extract syn-
onyms with much greater precision and recall. A
human evaluation shows that the synonyms the
alignment-based method finds are often missing in
EWN. This leads us to believe that the precision
scores attained by using EWN as a gold standard
are too pessimistic. Furthermore it is good news
that we seem to be able to find synonyms that are
not yet covered by existing resources.
The precision scores are still not satisfactory
and we see plenty of future directions. We would
like to use linguistic processing such as PoS-
tagging for word alignment to increase the accu-
racy of the alignment itself, to deal with com-
pounds more effectively and to be able to filter
out proposed synonyms that are of a different word
class than the target word. Furthermore we would
like to make use of the distributional similarity
score to set a threshold that will remove a lot of
errors. The last thing that remains for future work
is to find a more adequate way to combine the
872
syntax-based and the alignment-based methods.
Acknowledgements
This research was carried out in the project
Question Answering using Dependency Relations,
which is part of the research program for Inter-
act ive Multimedia Information Extraction, IMIX,
financed by NWO, the Dutch Organisation for Sci-
entific Research.
References
R.H. Baayen, R. Piepenbrock, and H. van Rijn. 1993.
The CELEX lexical database (CD-ROM). Lin-
guistic Data Consortium, University of Pennsylva-
nia,Philadelphia.
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Meet-
ing of the Association for Computational Linguis-
tics, pages 50?57.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263?296.
K.W. Church and P. Hanks. 1989. Word association
norms, mutual information and lexicography. Pro-
ceedings of the 27th annual conference of the Asso-
ciation of Computational Linguistics, pages 76?82.
J.R. Curran and M. Moens. 2002. Improvements in
automatic thesaurus extraction. In Proceedings of
the Workshop on Unsupervised Lexical Acquisition,
pages 59?67.
Ido Dagan, Alon Itai, and Ulrike Schwall. 1991. Two
languages are more informative than one. In Meet-
ing of the Association for Computational Linguis-
tics, pages 130?137.
Helge Dyvik. 1998. Translations as semantic mirrors.
In Proceedings of Workshop Multilinguality in the
Lexicon II, ECAI 98, Brighton, UK, pages 24?44.
C. Fellbaum. 1998. Wordnet, an electronic lexical
database. MIT Press.
A. Ibrahim, B. Katz, and J. Lin. 2003. Extract-
ing structural paraphrases from aligned monolingual
corpora.
A. Kilgarriff and C. Yallop. 2000. What?s in a the-
saurus? In Proceedings of the Second Conference
on Language Resource an Evaluation, pages 1371?
1379.
Philipp Koehn. 2003. Europarl: A multilin-
gual corpus for evaluation of machine trans-
lation. unpublished draft, available from
http://people.csail.mit.edu/koehn/publications/europarl/.
Dekang Lin, Shaojun Zhao, Lijuan Qin, and Ming
Zhou. 2003. Identifying synonyms among distribu-
tionally similar words. In IJCAI, pages 1492?1493.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768?774.
Robert Malouf and Gertjan van Noord. 2004. Wide
coverage parsing with stochastic attribute value
grammars. In IJCNLP-04 Workshop Beyond Shal-
low Analyses - Formalisms and stati stical modeling
for deep analyses, Hainan.
Michael Moortgat, Ineke Schuurman, and Ton van der
Wouden. 2000. CGN syntactische annotatie. In-
ternal Project Report Corpus Gesproken Nederlands,
see http://lands. let.kun.nl/cgn.
Franz Josef Och. 2003. GIZA++: Training of
statistical translation models. Available from
http://www.isi.edu/?och/GIZA++.html.
P. Roget. 1911. Thesaurus of English words and
phrases.
Jo?rg Tiedemann and Lars Nygaard. 2004. The OPUS
corpus - parallel & free. In Proceedings of the
Fourth International Conference on Language Re-
sources and Evaluation (LREC?04), Lisbon, Portu-
gal.
Peter D. Turney. 2001. Mining the Web for synonyms:
PMI?IR versus LSA on TOEFL. Lecture Notes in
Computer Science, 2167:491?502.
Leonoor van der Beek, Gosse Bouma, and Gertjan van
Noord. 2002. Een brede computationele grammat-
ica voor het Nederlands. Nederlandse Taalkunde,
7(4):353?374.
Lonneke van der Plas and Gosse Bouma. 2005.
Syntactic contexts for finding semantically similar
words. Proceedings of the Meeting of Computa-
tional Linguistics in the Netherlands (CLIN).
P. Vossen. 1998. Eurowordnet a multilingual database
with lexical semantic networks.
Hua Wu and Ming Zhou. 2003. Optimizing syn-
onym extraction using monolingual and bilingual re-
sources. In Proceedings of the Second International
Workshop on Paraphrasing: Paraphrase Acquisition
and Applications (IWP2003), Sapporo, Japan.
873
Identifying idiomatic expressions using automatic word-alignment
Begon?a Villada Moiro?n and Jo?rg Tiedemann
Alfa Informatica, University of Groningen
Oude Kijk in ?t Jatstraat 26
9712 EK Groningen, The Netherlands
{M.B.Villada.Moiron,J.Tiedemann}@rug.nl
Abstract
For NLP applications that require some
sort of semantic interpretation it would be
helpful to know what expressions exhibit
an idiomatic meaning and what expres-
sions exhibit a literal meaning. We invest-
igate whether automatic word-alignment
in existing parallel corpora facilitates
the classification of candidate expressions
along a continuum ranging from literal and
transparent expressions to idiomatic and
opaque expressions. Our method relies on
two criteria: (i) meaning predictability that
is measured as semantic entropy and (ii),
the overlap between the meaning of an ex-
pression and the meaning of its compon-
ent words. We approximate the mentioned
overlap as the proportion of default align-
ments. We obtain a significant improve-
ment over the baseline with both meas-
ures.
1 Introduction
Knowing whether an expression receives a lit-
eral meaning or an idiomatic meaning is import-
ant for natural language processing applications
that require some sort of semantic interpretation.
Some applications that would benefit from know-
ing this distinction are machine translation (Im-
amura et al, 2003), finding paraphrases (Bannard
and Callison-Burch, 2005), (multilingual) inform-
ation retrieval (Melamed, 1997a), etc.
The purpose of this paper is to explore to what
extent word-alignment in parallel corpora can be
used to distinguish idiomatic multiword expres-
sions from more transparent multiword expres-
sions and fully productive expressions.
In the remainder of this section, we present our
characterization of idiomatic expressions, the mo-
tivation to use parallel corpora and related work.
Section 2 describes the materials required to ap-
ply our method. Section 3 portraits the routine to
extract a list of candidate expressions from auto-
matically annotated data. Experiments with differ-
ent word alignment types and metrics are shown
in section 4. Our results are discussed in section 5.
Finally, we draw some conclusions in section 6.
1.1 What are idiomatic expressions?
Idiomatic expressions constitute a subset of mul-
tiword expressions (Sag et al, 2001). We assume
that literal expressions can be distinguished from
idiomatic expressions provided we know how their
meaning is derived.1 The meaning of linguistic
expressions can be described within a scale that
ranges from fully transparent to opaque (in figur-
ative expressions).
(1) Wat
what
moeten
must
lidstaten
member states
ondernemen
do
om
to
aan
at
haar
her
eisen
demands
te
to
voldoen?
meet?
?What must EU member states do to meet her
demands??
(2) Deze
this
situatie
situation
brengt
brings
de
the
bestaande
existing
politieke
political
barrie`res
barriers
zeer
very
duidelijk
clearly
aan
in
het
the
licht.
light
?This situation brings the existing political
limitations to light very clearly.?
1Here, we ignore morpho-syntactic and pragmatic factors
that could help model the distinction.
33
(3) Wij
we
mogen
may
ons
us
hier
here
niet
not
bij
by
neerleggen,
agree,
maar
but
moeten
must
de
the
situatie
situation
publiekelijk
publicly
aan
op
de
the
kaak
cheek
stellen.
state
?We cannot agree but must denounce the situ-
ation openly.?
Literal and transparent meaning is associated
with high meaning predictability. The meaning of
an expression is fully predictable if it results from
combining the meaning of its individual words
when they occur in isolation (see (1)). When
the expression undergoes a process of metaphor-
ical interpretation its meaning is less predictable.
Moon (1998) considers a continuum of transpar-
ent, semi-transparent and opaque metaphors. The
more transparent metaphors have a rather predict-
able meaning (2); the more opaque have an un-
predictable meaning (3). In general, an unpredict-
able meaning results from the fact that the mean-
ing of the expression has been fossilized and con-
ventionalized. In an uninformative context, idio-
matic expressions have an unpredictable meaning
(3). Put differently, the meaning of an idiomatic
expression cannot be derived from the cumulative
meaning of its constituent parts when they appear
in isolation.
1.2 Why checking translations?
This paper addresses the task of distinguishing lit-
eral (transparent) expressions from idiomatic ex-
pressions. Deciding what sort of meaning an ex-
pression shows can be done in two ways:
? measuring how predictable the meaning of
the expression is and
? assessing the link between (a) the meaning of
the expression as a whole and (b) the cumu-
lative literal meanings of the components.
Fernando and Flavell (1981) observe that no
connection between (a) and (b) suggests the ex-
istence of opaque idioms and, a clear link between
(a) and (b) is observed in clearly perceived meta-
phors and literal expressions.
We believe we can approximate the meaning
of an expression by looking up the expressions?
translation in a foreign language. Thus, we are
interested in exploring to what extent parallel cor-
pora can help us to find out the type of meaning an
expression has.
For our approach we make the following as-
sumptions:
? regular words are translated (more or less)
consistently, i.e. there will be one or only
a few highly frequent translations whereas
translation alternatives will be infrequent;
? an expression has a (almost) literal meaning
if its translation(s) into a foreign language is
the result of combining each word?s transla-
tion(s) when they occur in isolation into a for-
eign language;
? an expression has a non-compositional mean-
ing if its translation(s) into a foreign language
does not result from a combination of the reg-
ular translations of its component words.
We also assume that an automatic word aligner
will get into trouble when trying to align non-
decomposable idiomatic expressions word by
word. We expect the aligner to produce a large
variety of links for each component word in such
expressions and that these links are different from
the default alignments found in the corpus other-
wise.
Bearing these assumptions in mind, our ap-
proach attempts to locate the translation of a MWE
in a target language. On the basis of all recon-
structed translations of a (potential) MWE, it is de-
cided whether the original expression (in source
language) is idiomatic or a more transparent one.
1.3 Related work
Melamed (1997b) measures the semantic entropy
of words using bitexts. Melamed computes the
translational distribution T of a word s in a source
language and uses it to measure the translational
entropy of the word H(T|s); this entropy approx-
imates the semantic entropy of the word that can
be interpreted either as (a) the semantic ambigu-
ity or (b) the inverse of reliability. Thus, a word
with high semantic entropy is potentially very am-
biguous and therefore, its translations are less re-
liable (or highly context-dependent). We also
use entropy to approximate meaning predictabil-
ity. Melamed (1997a) investigates various tech-
niques to identify non-compositional compounds
in parallel data. Non-compositional compounds
34
are those sequences of 2 or more words (adja-
cent or separate) that show a conventionalized
meaning. From English-French parallel corpora,
Melamed?s method induces and compares pairs of
translation models. Models that take into account
non-compositional compounds are highly accurate
in the identification task.
2 Data and resources
We base our investigations on the Europarl corpus
consisting of several years of proceedings from the
European Parliament (Koehn, 2003). We focus on
Dutch expressions and their translations into Eng-
lish, Spanish and German.2 Thus, we used the en-
tire sections of Europarl in these three languages.
The corpus has been tokenized and aligned at the
sentence level (Tiedemann and Nygaard, 2004).
The Dutch part contains about 29 million tokens
in about 1.2 million sentences. The English, Span-
ish and German counterparts are of similar size
between 28 and 30 million words in roughly the
same number of sentences.
Automatic word alignment has been done us-
ing GIZA++ (Och, 2003). We used standard set-
tings of the system to produce Viterbi alignments
of IBM model 4. Alignments have been produced
for both translation directions (source to target and
target to source) on tokenized plain text.3 We also
used a well-known heuristics for combining the
two directional alignments, the so-called refined
alignment (Och et al, 1999). Word-to-word align-
ments have been merged such that words are con-
nected with each other if they are linked to the
same target. In this way we obtained three differ-
ent word alignment files: source to target (src2trg)
with possible multi-word units in the source lan-
guage, target to source (trg2src) with possible
multi-word units in the target language, and re-
fined with possible multi-word units in both lan-
guages. We also created bilingual word type links
from the different word-aligned corpora. These
lists include alignment frequencies that we will
use later on for extracting default alignments for
individual words. Henceforth, we will call them
link lexica.
2This is only a restriction for our investigation but not for
the approach itself.
3Manual corrections and evaluations of the tokenization,
sentence and word alignment have not been done. We rely
entirely on the results of automatic processes.
3 Extracting candidates from corpora
The Dutch section from the Europarl corpus was
automatically parsed with Alpino, a Dutch wide-
coverage parser.4 1.25% of the sentences could
not be parsed by Alpino, given the fact that many
sentences are rather lengthy. We selected those
sentences in the Dutch Europarl section that con-
tain at least one of a group of verbs that can
function as main or support verbs. Support verbs
are prone to lexicalization or idiomatization along
with their complementation (Butt, 2003). The se-
lected verbs are: doen, gaan, geven, hebben, ko-
men, maken, nemen, brengen, houden, krijgen,
stellen and zitten.5
A fully parsed sentence is represented by the list
of its dependency triples. From the dependency
triples, each main verb is tallied with every de-
pendent prepositional phrase (PP). In this way, we
collected all the VERB PP tuples found in the selec-
ted documents. To avoid data sparseness, the NP
inside the PP is reduced to the head noun?s lemma
and verbs are lemmatized, too. Other potential
arguments under a verb phrase node are ignored.
A sample of more than 191,000 candidates types
(413,000 tokens) was collected. To ensure statist-
ical significance, the types that occur less than 50
times were ignored.
For each candidate triple, the log-likelihood
(Dunning, 1993) and salience (Kilgarriff and Tug-
well, 2001) scores were calculated. These scores
have been shown to perform reasonably well in
identifying collocations and other lexicalized ex-
pressions (Villada Moiro?n, 2005). In addition, the
head dependence between each PP in the candid-
ates dataset and its selecting verbs was measured.
Merlo and Leybold (2001) used the head depend-
ence as a diagnostic to determine the argument
(or adjunct) status of a PP. The head dependence
is measured as the amount of entropy observed
among the co-occurring verbs for a given PP as
suggested in (Merlo and Leybold, 2001; Bald-
win, 2005). Using the two association measures
and the head dependence heuristic, three different
rankings of the candidate triples were produced.
The three different ranks assigned to each triple
were uniformly combined to form the final rank-
ing. From this list, we selected the top 200 triples
4Available at http://www.let.rug.nl/
?vannoord/alp/Alpino.
5Butt (2003) maintains that the first 7 verbs are examples
of support verbs crosslinguistically. The other 5 have been
suggested for Dutch by (Hollebrandse, 1993).
35
which we considered a manageable size to test our
method.
4 Methodology
We examine how expressions in the source lan-
guage (Dutch) are conceptualized in a target lan-
guage. The translations in the target language en-
code the meaning of the expression in the source
language. Using the translation links in paral-
lel corpora, we attempt to establish what type of
meaning the expression in the source language
has. To accomplish this we make use of the three
word-aligned parallel corpora from Europarl as
described in section 2.
Once the translation links of each expression in
the source language have been collected, the en-
tropy observed among the translation links is com-
puted per expression. We also take into account
how often the translation of an expression is made
out of the default alignment for each triple com-
ponent. The default ?translation? is extracted from
the corresponding bilingual link lexicon.
4.1 Collecting alignments
For each triple in the source language (Dutch)
we collect its corresponding (hypothetical) trans-
lations in a target language. Thus, we have a list
of 200 VERB PP triples representing 200 potential
MWEs in Dutch. We selected all occurrences of
each triple in the source language and all aligned
sentences containing their corresponding transla-
tions into English, German and Spanish. We re-
stricted ourselves to instances found in 1:1 sen-
tence alignments. Other units contain many er-
rors in word and sentence alignment and, there-
fore, we discarded them. Relying on automated
word-alignment, we collect all translation links for
each verb, preposition and noun occurrence within
the triple context in the three target languages.
To capture the meaning of a source expression
(triple) S, we collect all the translation links of its
component words s in each target language. Thus,
for each triple, we gather three lists of transla-
tion links Ts. Let us see the example AAN LICHT
BRENG representing the MWE iets aan het licht
brengen ?reveal?. Table 1 shows some of the links
found for the triple AAN LICHT BRENG. If a word
in the source language has no link in the target lan-
guage (which is usually due to alignments to the
empty word), NO LINK is assigned.
Note that Dutch word order is more flexible than
Triple Links in English
aan NO LINK, to, of, in, for, from, on, into, at
licht NO LINK, light, revealed, exposed, highlight,
shown, shed light, clarify
breng NO LINK, brought, bring, highlighted,
has, is, makes
Table 1: Excerpt of the English links found for the
triple AAN LICHT BRENG ?bring to light?.
English word order and that, the PP argument in a
candidate expression may be separate from its se-
lecting verb by any number of constituents. This
introduces much noise during retrieving transla-
tion links. In addition, it is known that concepts
may be lexicalized very differently in different
languages. Because of this, words in the source
language may translate to nothing in a target lan-
guage. This introduces many mappings of a word
to NO LINK.
4.2 Measuring translational entropy
According to our intuition it is harder to align
words in idiomatic expressions than other words.
Thus, we expect a larger variety of links (includ-
ing erroneous alignments) for words in such ex-
pressions than for words taken from expressions
with a more literal meaning. For the latter, we
expect fewer alignment candidates, possibly with
only one dominant default translation. Entropy
is a good measure for the unpredictability of an
event. We like to use this measure for comparing
the alignment of our candidates and expect a high
average entropy for idiomatic expressions. In this
way we approximate a measure for meaning pre-
dictability.
For each word in a triple, we compute the en-
tropy of the aligned target words as shown in equa-
tion (1).
H(Ts|s) = ?
?
t?Ts
P (t|s)logP (t|s) (1)
This measure is equivalent to translational en-
tropy (Melamed, 1997b). P (t|s) is estimated as
the proportion of alignment t among all align-
ments of word s found in the corpus in the con-
text of the given triple.6 Finally, the translational
entropy of a triple is the average translational en-
tropy of its components. It is unclear how to
6Note that we also consider cases where s is part of an
aligned multi-word unit.
36
treat NO LINKS. Thus, we experiment with three
variants of entropy: (1) leaving out NO LINKS,
(2) counting NO LINKS as multiple types and (3)
counting all NO LINKS as one unique type.
4.3 Proportion of default alignments (pda)
If an expression has a literal meaning, we expect
the default alignments to be accurate literal trans-
lations. If an expression has idiomatic meaning,
the default alignments will be very different from
the links observed in the translations.
For each triple S, we count how often each of
its components s is linked to one of the default
alignments Ds. For the latter, we used the four
most frequent alignment types extracted from the
corresponding link lexicon as described in section
2. A large proportion of default alignments7 sug-
gests that the expression is very likely to have lit-
eral meaning; a low percentage is suggestive of
non-transparent meaning. Formally, pda is calcu-
lated in the following way:
pda(S) =
?
s?S
?
d?Ds align freq(s, d)
?
s?S
?
t?Ts align freq(s, t)
(2)
where align freq(s, t) is the alignment fre-
quency of word s to word t in the context of the
triple S.
5 Discussion of experiments and results
We experimented with the three word-alignment
types (src2trg, trg2src and refined) and the two
scoring methods (entropy and pda). The 200 can-
didate MWEs have been assessed and classified
into idiomatic or literal expressions by a human
expert. For assessing performance, standard pre-
cision and recall are not applicable in our case be-
cause we do not want to define an artificial cut-
off for our ranked list but evaluate the ranking it-
self. Instead, we measured the performance of
each alignment type and scoring method by ob-
taining another evaluation metric employed in in-
formation retrieval, uninterpolated average preci-
sion (uap), that aggregates precision points into
one evaluation figure. At each point c where a true
positive Sc in the retrieved list is found, the pre-
cision P (S1..Sc) is computed and, all precision
points are then averaged (Manning and Schu?tze,
1999).
7Note that we take NO LINKS into account when comput-
ing the proportions.
uap =
?
Sc P (S1..Sc)
|Sc|
(3)
We used the initial ranking of our candidates
as baseline. Our list of potential MWEs shows an
overall precision of 0.64 and an uap of 0.755.
5.1 Comparing word alignment types
Table 2 summarizes the results of using the en-
tropy measure (leaving out NO LINKS) with the
three alignment types for the NL-EN language
pair.8
Alignment uap
src2trg 0.864
trg2src 0.785
refined 0.765
baseline 0.755
Table 2: uap values of various alignments.
Using word alignments improves the ranking
of candidates in all three cases. Among them,
src2trg shows the best performance. This is
surprising because the quality of word-alignment
from English-to-Dutch (trg2src) in general is
higher due to differences in compounding in the
two languages. However, this is mainly an issue
for noun phrases which make up only one com-
ponent in the triples.
We assume that src2trg works better in our case
because in this alignment model we explicitly link
each word in the source language to exactly one
target word (or the empty word) whereas in the
trg2src model we often get multiple words (in the
target language) aligned to individual words in the
triple. Many errors are introduced in such align-
ment units. Table 3 illustrates this with an example
with links for the Dutch triple op prijs stel corres-
ponding to the expression iets op prijs stellen ?to
appreciate sth.?
src2trg trg2src
source target target source
gesteld appreciate NO LINK stellen
prijs appreciate much appreciate indeed prijs
op appreciate NO LINK op
gesteld be keenly appreciate stellen
prijs delighted fact prijs
op NO LINK NO LINK op
Table 3: Example src2trg and trg2src alignments
for the triple OP PRIJS STEL.
8The performance of the three alignment types remains
uniform across all chosen language pairs.
37
src2trg alignment proposes appreciate as a link
to all three triple components. This type of align-
ment is not possible in trg2src. Instead, trg2src in-
cludes two NO LINKS in the first example in table
3. Furthermore, we get several multiword-units in
the target language linked to the triple compon-
ents also because of alignment errors. This way,
we end up with many NO LINKS and many align-
ment alternatives in trg2src that influence our en-
tropy scores. This can be observed for idiomatic
expressions as well as for literal expressions which
makes translational entropy less reliable in trg2src
alignments for contrasting these two types of ex-
pressions.
The refined alignment model starts with the in-
tersection of the two directional models and adds
iteratively links if they meet some adjacency con-
straints. This results in many NO LINKS and also
alignments with multiple words on both sides.
This seems to have the same negative effect as in
the trg2src model.
5.2 Comparing scoring metrics
Table 4 offers a comparison of applying transla-
tional entropy and the pda across the three lan-
guage pairs. To produce these results, src2trg
alignment was used given that it reaches the best
performance (refer to Table 2).
Score NL-EN NL-ES NL-DE
entropy
- without NO LINKS 0.864 0.892 0.907
- NO LINKS=many 0.858 0.890 0.883
- NO LINKS=one 0.859 0.890 0.911
pda 0.891 0.894 0.894
baseline 0.755 0.755 0.755
Table 4: Translational entropy and the pda across
three language pairs. Alignment is src2trg.
All scores produce better rankings than the
baseline. In general, pda achieves a slightly better
accuracy than entropy except for the NL-DE lan-
guage pair. Nevertheless, the difference between
the metrics is hardly significant.
5.3 Further improvements
One problem in our data is that we deal with word-
form alignments and not with lemmatized ver-
sions. For Dutch, we know the lemma of each
word instance from our candidate set. However,
for the target languages, we only have access to
surface forms from the corpus. Naturally, inflec-
tional variations influence entropy scores (because
of the larger variety of alignment types) and also
the pda scores (where the exact wordforms have to
be matched with the default alignments instead of
lemmas). In order to test the effect of lemmatiz-
ation on different language pairs, we used CELEX
(Baayen et al, 1993) for English and German to
reduce wordforms in the alignments and in the link
lexicon to corresponding lemmas. We assigned the
most frequent lemma to ambiguous wordforms.
Table 5 shows the scores obtained from applying
lemmatization for the src2trg alignment using
entropy (without NO LINKS) and pda.
Setting NL-EN NL-ES NL-DE
using entropy scores
with prepositions
wordforms 0.864 0.892 0.907
lemmas 0.873 ? 0.906
without prepositions
wordforms 0.906 0.923 0.932
lemmas 0.910 ? 0.931
using pda scores
with prepositions
wordforms 0.891 0.894 0.894
lemmas 0.888 ? 0.903
without prepositions
wordforms 0.897 0.917 0.905
lemmas 0.900 ? 0.910
baseline 0.755 0.755 0.755
Table 5: Translational entropy and pda from
src2trg alignments across languages pairs with
different settings.
Surprisingly, lemmatization adds little or even
decreases the accuracy of the pda and entropy
scores. It is also surprising that lemmatization
does not affect the scores for morphologically
richer languages such as German (compared to
English). One possible reason for this is that
lemmatization discards morphological informa-
tion that is crucial to identify idiomatic expres-
sions. In fact, nouns in idiomatic expressions are
more fixed than nouns in literal expressions. By
contrast, verbs in idiomatic expressions often al-
low tense inflection. By clustering wordforms into
lemmas we lose this information. In future work,
we might lemmatize only the verb.
Another issue is the reliability of the word align-
ment that we base our investigation upon. We
want to make use of the fact that automatic word
alignment has problems with the alignment of in-
dividual words that belong to larger lexical units.
However, we believe that the alignment program
in general has problems with highly ambiguous
words such as prepositions. Therefore, preposi-
38
tions might blur the contrast between idiomatic ex-
pressions and literal translations when measured
on the alignment of individual words. Table 5
includes scores for ranking our candidate expres-
sions with and without prepositions. We observe
that there is a large improvement when leaving out
the alignments of prepositions. This is consistent
for all language pairs and the scores we used for
ranking.
rank pda entropy MWE triple
1 9.80 8.3585 ok breng tot stand ?create?
2 9.24 8.0923 ok breng naar voren ?bring up?
3 16.40 7.8741 ok kom in aanmerking ?qualify?
4 15.33 7.8426 ok kom tot stand ?come about?
5 8.70 7.4973 ok stel aan orde ?bring under discussion?
6 5.65 7.4661 ok ga te werk ?act unfairly?
7 17.46 7.4057 ok kom aan bod ?get a chance?
8 9.38 7.1762 ok ga van start ?proceed?
9 14.15 7.1009 ok stel aan kaak ?expose?
10 18.75 7.0321 ok breng op gang ?get going?
11 13.00 6.9304 ok kom ten goede ?benefit?
12 1.78 6.8715 ok neem voor rekening ?pay costs?
13 20.99 6.7411 ok kom tot uiting ?manifest?
14 1.41 6.7360 ok houd in stand ?preserve?
15 0.81 6.6426 ok breng in kaart ?chart?
16 16.71 6.5194 ok breng onder aandacht ?bring to attention?
17 10.25 6.4893 ok neem onder loep ?scrutinize?
18 7.83 6.4666 ok breng aan licht ?reveal?
19 5.99 6.4049 ok roep in leven ?set up?
20 15.89 6.3729 ok neem in aanmerking ?consider?
...
100 1.72 4.6940 ok leg aan band ?control?
101 14.91 4.6884 ok houd voor gek ?pull s.o.?s leg?
102 23.56 4.6865 ok kom te weten ?find out?
103 15.38 4.6713 ok neem in ontvangst ?receive?
104 31.57 4.6556 * ga om waar ?go about where?
105 35.95 4.6380 * houd met daar ?keep with there?
106 34.86 4.6215 * ga om zaak ?go about issue?
107 28.33 4.5846 ok kom tot overeenstemming ?come to terms?
108 6.06 4.5715 ok breng in handel ?launch?
109 35.62 4.5370 * ga om bedrag ?go about amount?
110 22.58 4.5089 * blijk uit feit ?seems from fact?
111 51.12 4.4063 ok ben van belang ?matter?
112 49.69 4.3921 * ga om kwestie ?go about issue?
113 23.61 4.3902 * voorzie in behoefte ?fill gap?
114 16.18 4.3568 ok geef aan oproep ?make appeal?
115 50.00 4.3254 * houd met aspect ?keep with aspect?
116 40.91 4.3006 * houd aan regel ?adhere to rule?
117 20.12 4.3002 * stel vast met voldoening ?settle with satisfaction?
118 36.90 4.2931 ok kom tot akkoord ?reach agreement?
119 36.49 4.2906 ok breng in stemming ?get in mood?
120 14.06 4.2873 ok sta op schroeven ?be unsettled?
...
180 70.53 2.7395 * voldoe aan criterium ?satisfy criterion?
181 52.33 2.7351 * beschik over informatie ?decide over information?
182 74.71 2.6896 * stem voor amendement ?vote for amending?
183 76.56 2.5883 * neem deel aan stemming ?participate in voting?
184 30.26 2.4484 ok kan op aan ?be able to trust?
185 68.89 2.3199 * zeg tegen heer ?tell a gentleman?
186 45.00 2.1113 * verwijs terug naar commissie ?refer to comission?
187 80.39 2.0992 * stem tegen amendement ?vote againsta amending?
188 78.04 2.0924 * onthoud van stemming ?withhold one?s vote?
189 77.63 1.9997 * feliciteer met werk ?congratulate with work?
190 82.21 1.9020 * stem voor verslag ?vote for report?
191 77.78 1.9016 * schep van werkgelegenheid ?set up of employment?
192 86.36 1.8775 * stem voor resolutie ?vote for resolution ?
193 73.33 1.8687 * bedank voor feit ?thank for fact?
194 39.13 1.8497 * was wit van geld ?wash money?
195 82.20 1.7944 * stem tegen verslag ?vote against report?
196 80.49 1.6443 * schep van baan ?set up of job?
197 86.17 1.4260 * stem tegen resolutie ?vote against resolution?
198 85.56 1.1779 * dank voor antwoord ?thank for reply?
199 90.55 1.0398 * ontvang overeenkomstig artikel ?receive similar article?
200 87.88 1.0258 * recht van vrouw ?right of woman?
Table 6: Rank (using entropy), entropy score, and
pda score of 60 candidate MWEs.
Table 6 provides an excerpt from the ranked
list of candidate triples. The ranking has been
done using src2trg alignments from Dutch to Ger-
man with the best setting (see table 5). The score
assigned by the pda metric is also shown. The
column labeled MWE states whether the expres-
sion is idiomatic (?ok?) or literal (?*?). One issue
that emerges is whether we can find a threshold
value that splits candidate expressions into idio-
matic and transparent ones. One should choose
such a threshold empirically however, it will de-
pend on what level of precision is desirable and
also on the final application of the list.
6 Conclusion and future work
In this paper we have shown that assessing auto-
matic word alignment can help to identify idio-
matic multi-word expressions. We ranked candid-
ates according to their link variability using trans-
lational entropy and their link consistency with
regards to default alignments. For our experi-
ments we used a set of 200 Dutch MWE candid-
ates and word-aligned parallel corpora from Dutch
to English, Spanish and German. The MWE can-
didates have been extracted using standard associ-
ation measures and a head dependence heuristic.
The word alignment has been done using standard
models derived from statistical machine transla-
tion. Two measures were tested to re-rank the can-
didates. Translational entropy measures the pre-
dictability of the translation of an expression by
looking at the links of its components to a target
language. Ranking our 200 MWE candidates us-
ing entropy on Dutch to German word alignments
improved the baseline of 75.5% to 93.2% uninter-
polated average precision (uap). The proportion of
default alignments among the links found for MWE
components is another score we explored for rank-
ing our MWE candidates. Here, the accuracy is
rather similar giving us 91.7% while using the res-
ults of a directional alignment model from Dutch
to Spanish. In general, we obtain slightly better
results when using word alignment from Dutch to
German and Spanish, compared to alignment from
Dutch to English.
There emerge several extensions of this work
that we wish to address in the future. Alignment
types and scoring metrics need to be tested in lar-
ger lists of randomly selected MWE candidates to
see if the results remain unaltered. We also want to
apply some weighting scheme by using the num-
39
ber of NO LINKS per expression. Our assump-
tion is that an expression with many NO LINKS is
harder to translate compositionally, and probably
an idiomatic or ambiguous expression. Altern-
atively, an expression with no NO LINKS is very
predictable, thus a literal expression. Finally, an-
other possible improvement is combining several
language pairs. There might be cases where idio-
matic expressions are conceptualized in a similar
way in two languages. For example, a Dutch idio-
matic expression with a cognate expression in Ger-
man might be conceptualized in a different way in
Spanish. By combining the entropy or pda scores
for NL-EN, NL-DE and NL-ES the accuracy might
improve.
Acknowledgments
This research was carried out as part of the re-
search programs for IMIX, financed by NWO and
the IRME STEVIN project. We would also like
to thank the three anonymous reviewers for their
comments on an earlier version of this paper.
References
R.H. Baayen, R. Piepenbrock, and H. van Rijn.
1993. The CELEX lexical database (CD-
ROM). Linguistic Data Consortium, University of
Pennsylvania,Philadelphia.
Timothy Baldwin. 2005. Looking for prepositional
verbs in corpus data. In Proc. of the 2nd ACL-
SIGSEM Workshop on the Linguistic Dimensions of
Prepositions and their use in computational linguist-
ics formalisms and applications, Colchester, UK.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43th Annual Meeting of the ACL,
pages 597?604, Ann Arbor. University of Michigan.
Miriam Butt. 2003. The light verb jungle.
http://ling.uni-konstanz.de/pages/
home/butt/harvard-work.pdf.
Ted Dunning. 1993. Accurate methods for the stat-
istics of surprise and coincidence. Computational
linguistics, 19(1):61?74.
Chitra Fernando and Roger Flavell. 1981. On idiom.
Critical views and perspectives, volume 5 of Exeter
Linguistic Studies. University of Exeter.
Bart Hollebrandse. 1993. Dutch light verb construc-
tions. Master?s thesis, Tilburg University, the Neth-
erlands.
K Imamura, E. Sumita, and Y. Matsumoto. 2003.
Automatic construction of machine translation
knowledge using translation literalness. In Proceed-
ings of the 10th EACL, pages 155?162, Budapest,
Hungary.
Adam Kilgarriff and David Tugwell. 2001. Word
sketch: Extraction & display of significant colloc-
ations for lexicography. In Proceedings of the 39th
ACL & 10th EACL -workshop ?Collocation: Com-
putational Extraction, Analysis and Explotation?,
pages 32?38, Toulouse.
Philipp Koehn. 2003. Europarl: A multilin-
gual corpus for evaluation of machine trans-
lation. unpublished draft, available from
http://people.csail.mit.edu/koehn/publications/europarl/.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Pro-
cessing. The MIT Press, Cambridge, Massachu-
setts.
I. Dan Melamed. 1997a. Automatic discovery of non-
compositional compounds in parallel data. In 2nd
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP?97), Providence, RI.
I. Dan Melamed. 1997b. Measuring semantic entropy.
In ACL-SIGLEX Workshop Tagging Text with Lex-
ical Semantics: Why, What and How, pages 41?46,
Washington.
Paola Merlo and Matthias Leybold. 2001. Automatic
distinction of arguments and modifiers: the case of
prepositional phrases. In Procs of the Fifth Com-
putational Natural Language Learning Workshop
(CoNLL?2001), pages 121?128, Toulouse. France.
Rosamund Moon. 1998. Fixed expressions and Idioms
in English. A corpus-based approach. Clarendom
Press, Oxford.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for statist-
ical machine translation. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Nat-
ural Language Processing and Very Large Corpora
(EMNLP/VLC), pages 20?28, University of Mary-
land, MD, USA.
Franz Josef Och. 2003. GIZA++: Training of
statistical translation models. Available from
http://www.isi.edu/?och/GIZA++.html.
Ivan Sag, T. Baldwin, F. Bond, A. Copestake, and
D. Flickinger. 2001. Multiword expressions: a pain
in the neck for NLP. LinGO Working Paper No.
2001-03.
Jo?rg Tiedemann and Lars Nygaard. 2004. The OPUS
corpus - parallel & free. In Proceedings of the
Fourth International Conference on Language Re-
sources and Evaluation (LREC?04), Lisbon, Por-
tugal.
Begon?a Villada Moiro?n. 2005. Data-driven Identi-
fication of fixed expressions and their modifiability.
Ph.D. thesis, University of Groningen.
40
Word to word alignment strategies
Jo?rg Tiedemann
Department of Linguistics and Philology
Uppsala University
Uppsala/Sweden
joerg@stp.ling.uu.se
Abstract
Word alignment is a challenging task aim-
ing at the identification of translational re-
lations between words and multi-word units
in parallel corpora. Many alignment strate-
gies are based on links between single words.
Different strategies can be used to find the
optimal word alignment using such one-to-
one word links including relations between
multi-word units. In this paper seven algo-
rithms are compared using a word alignment
approach based on association clues and
an English-Swedish bitext together with a
handcrafted reference alignment used for
evaluation.
1 Introduction
Word alignment is the task of identifying trans-
lational relations between words in parallel cor-
pora with the aim of re-using them in natu-
ral language processing. Typical applications
that make use of word alignment techniques
are machine translation and multi-lingual lex-
icography. Several approaches have been pro-
posed for the automatic alignment of words and
phrases using statistical techniques and align-
ment heuristics, e.g. (Brown et al, 1993; Vogel
et al, 1996; Garc??a-Varea et al, 2002; Ahren-
berg et al, 1998; Tiedemann, 1999; Tufis and
Barbu, 2002; Melamed, 2000). Word align-
ment usually includes links between so-called
multi-word units (MWUs) in cases where lex-
ical items cannot be split into separated words
with appropriate translations in another lan-
guage. See for example the alignment between
an English sentence and a Swedish sentence il-
lustrated in figure 1. There are MWUs in both
languages aligned to corresponding translations
in the other language. The Swedish compound
?mittplatsen? corresponds to three words in En-
glish (?the middle seat?) and the English verb
?dislike? is translated into a Swedish particle
verb ?tycker om? (English: like) that has been
negated using ?inte?. Most approaches model
Jag tar mittplatsen, vilket jag inte tycker om, men det g?r mig inte s? mycket.
I take the middle seat, which I dislike, but  I am not really put out.
Figure 1: A word alignment example from Saul
Bellow ?To Jerusalem and back: a personal ac-
count? (Bellow, 1976) and its Swedish transla-
tion (Bellow, 1977) (the Bellow corpus).
word alignment as links between words in the
source language and words in the target lan-
guage as indicated by the arrows in figure 1.
However, in cases like the English expression
?I am not really put out? which corresponds to
the Swedish expression ?det go?r mig inte s?a my-
cket? there is no proper way of connecting single
words with each other in order to express this
relation. In some approaches such relations are
constructed in form of an exhaustive set of links
between all word pairs included in both expres-
sions (Melamed, 1998; Mihalcea and Pedersen,
2003). In other approaches complex expressions
are identified in a pre-processing step in order
to handle them as complex units in the same
manner as single words in alignment (Smadja
et al, 1996; Ahrenberg et al, 1998; Tiedemann,
1999).
The one-to-one word linking approach seems
to be very limited. However, single word links
can be combined in order to describe links be-
tween multi-word units as illustrated in figure
1. In this paper we investigate different align-
ment strategies using this approach1. For this
we apply clue alignment introduced in the next
section.
2 Word alignment with clues
The clue alignment approach has been pre-
sented in (Tiedemann, 2003). Alignment clues
represent probabilistic indications of associa-
1A similar study on statistical alignment models is
included in (Och and Ney, 2003).
tions between lexical items collected from dif-
ferent sources. Declarative clues can be taken
from linguistic resources such as bilingual dic-
tionaries. They may also include pre-defined
relations between lexical items based on cer-
tain features such as parts of speech. Estimated
clues are derived from the parallel data using,
for example, measures of co-occurrence (e.g. the
Dice coefficient (Smadja et al, 1996)), statisti-
cal alignment models (e.g. IBM models from
statistical machine translation (Brown et al,
1993)), or string similarity measures (e.g. the
longest common sub-sequence ratio (Melamed,
1995)). They can also be learned from pre-
viously aligned training data using linguistic
and contextual features associated with aligned
items. Relations between certain word classes
with respect to the translational association of
words belonging to these classes is one example
of such clues that can be learned from aligned
training data. In our experiments, for example,
we will use clues that indicate relations between
lexical items based on their part-of-speech tags
and their positions in the sentence relative to
each other. They are learned from automati-
cally word-aligned training data.
The clue alignment approach implements a
way of combining association indicators on a
word-to-word level. The combination of clues
results in a two-dimensional clue matrix. The
values in this matrix express the collected evi-
dence of an association between word pairs in
bitext segments taken from a parallel corpus.
Word alignment is then the task of identifying
the best links according to the associations indi-
cated in the clue matrix. Several strategies for
such an alignment are discussed in the following
section.
3 Alignment strategies
A clue matrix summarizes information from var-
ious sources that can be used for the identifica-
tion of translation relations. However, there is
no obvious way to utilize this information for
word alignment as we explicitly include multi-
word units (MWUs) in our approach. The clue
matrix in figure 2 has been obtained for a bi-
text segment from our English-Swedish test cor-
pus (the Bellow corpus) using a set of weighted
declarative and estimated clues.
There are many ways of ?clustering? words
together and there is no obvious maximization
procedure for finding the alignment optimum
when MWUs are involved. The alignment pro-
ingen visar sa?rskilt mycket t?alamod
no 29 0 0 1 9
one 16 2 1 1 13
is 1 13 1 2 0
very 0 2 18 17 1
patient 2 1 4 12 6
Figure 2: A clue matrix (all values in %).
cedure depends very much on the definition of
an optimal alignment. The best alignment for
our example would probably be the set of the
following links:
links =
{
no one ingen
is patient visar t?alamod
very sa?rskilt mycket
}
A typical procedure for automatic word align-
ment is to start with one-to-one word links.
Links that have common source or target lan-
guage words are called overlapping links. Sets
of overlapping links, which do not overlap with
any other link outside the set, are called link
clusters (LC). Aligning words one by one of-
ten produces overlaps and in this way implic-
itly creates aligned multi-word-units as part of
link clusters. A general word-to-word align-
ment L for a given bitext segment with N
source language words (s1s2...sN ) and M tar-
get language words (t1t2...tM ) can be formally
described as a set of links L = {L1, L2, ..., Lx}
with Lx = [sx1 , tx2 ] , x1 ? {1..N}, x2 ? {1..M}.
This general definition allows varying num-
bers of links (0 ? x ? N ? M) within possible
alignments L. It is not straightforward how to
find the optimal alignment as L may include
different numbers of links.
3.1 Directional alignment models
One word-to-word alignment approach is to as-
sume a directional word alignment model simi-
lar to the models in statistical machine trans-
lation. The directional alignment model as-
sumes that there is at most one link for each
source language word. Using alignment clues,
this can be expressed as the following optimiza-
tion problem: L?D = argmaxLD
?N
n=1 C(L
D
n )
where LD = {LD1 , L
D
2 , .., L
D
N} is a set of links
LDn =
[
sn, taDn
]
with aDn ? {1..M} and C(L
D
n )
is the combined clue value for the linked items
sn and taDn . In other words, word alignment
is the search for the best link for each source
language word. Directional models do not al-
low multiple links from one item to several tar-
get items. However, target items can be linked
to multiple source language words as they can
be aligned to the same target language word.
The direction of alignment can easily be re-
versed, which leads to the inverse directional
alignment: L?I = argmaxLI
?M
m=1 C(L
I
m) with
links LIm =
[
saIm , tm
]
and aIm ? {1..N}. In the
inverse directional alignment, source language
words can be linked to multiple words but not
the other way around. The following figure il-
lustrates directional alignment models applied
to the example in figure 2:
L?D =
?
???
???
no ingen
one ingen
is visar
very sa?rskilt
patient mycket
?
???
???
LCD =
?
??
??
no one ingen
is visar
very sa?rskilt
patient mycket
?
??
??
Using the inverse directional alignment strategy
we would obtain the following links:
L?I =
?
???
???
no ingen
is visar
very sa?rskilt
very mycket
one t?alamod
?
???
???
LCI =
?
??
??
no ingen
is visar
very sa?rskilt mycket
one t?alamod
?
??
??
3.2 Combined directional alignment
Directional link sets can be combined in several
ways. The union of link sets (L?? = L?D ? L?I)
usually causes many overlaps and, hence, very
large link clusters. On the other hand, an inter-
section of link sets (L?? = L?D ? L?I) removes all
overlaps and leaves only highly confident one-to-
one word links behind. Using the same example
from above we obtain the following alignments:
L?? =
?
???????
???????
no ingen
one ingen
one t?alamod
is visar
very sa?rskilt
very mycket
patient mycket
?
???????
???????
LC? =
{
no one ingen t?alamod
is visar
very patient sa?rskilt mycket
}
The intersection of links produces the following
sets:
L?? =
{
no ingen
is visar
very sa?rskilt
}
LC? = L??
The union and the intersection of links do not
produce satisfactory results as seen in the ex-
ample. Another alignment strategy is a refined
combination of link sets (L?R = {L?D ? L?I} ?
{LR1 , ..., L
R
r }) as suggested by (Och and Ney,
2000b). In this approach, the intersection of
links is iteratively extended by additional links
LRr which pass one of the following two con-
straints:
? A new link is accepted if both items in the
link are not yet algned.
? Mapped on a two-dimensional bitext space,
the new link is either vertically or horizon-
tally adjacent to an existing link and the
new link does not cause any link to be adja-
cent to other links in both dimensions (hor-
izontally and vertically).
Applying this approach to the example, we get:
L?R =
?
?????
?????
no ingen
is visar
very sa?rskilt
very mycket
one ingen
patient t?alamod
?
?????
?????
LCR =
?
??
??
no one ingen
is visar
very sa?rskilt mycket
patient t?alamod
?
??
??
3.3 Competitive linking
Another alignment approach is the compet-
itive linking approach proposed by Melamed
(Melamed, 1996). In this approach, one as-
sumes that there are only one-to-one word
links. The alignment is done in a greedy ?best-
first? search manner where links with the high-
est association scores are aligned first, and the
aligned items are then immediately removed
from the search space. This process is re-
peated until no more links can be found. In
this way, the optimal alignment (L?C) for non-
overlapping one-to-one links is found. The num-
ber of possible links in an alignment is reduced
to min(N,M). Using competitive linking with
our example we yield:
L?C =
?
???
???
no ingen
very sa?rskilt
is visar
one t?alamod
patient mycket
?
???
???
LCC = L?C
3.4 Constrained best-first alignment
Another iterative alignment approach has been
proposed in (Tiedemann, 2003). In this ap-
proach, the link LBx = [sx1 , tx2 ] with the high-
est score in the clue matrix C?(sx1 , tx2) =
maxsi,tj (C(si, tj)) is added to the set of link
clusters if it fulfills certain constraints. The top
score is removed from the matrix (i.e. set to
zero) and the link search is repeated until no
more links can be found. This is basically a
constrained best-first search. Several constraints
are possible. In (Tiedemann, 2003) an adja-
cency check is suggested, i.e. overlapping links
are accepted only if they are adjacent to other
links in one and only one existing link cluster.
Non-overlapping links are always accepted (i.e.
a non-overlapping link creates a new link clus-
ter). Other possible constraints are clue value
thresholds, thresholds for clue score differences
between adjacent links, or syntactic constraints
(e.g. that link clusters may not cross phrase
boundaries). Using a best-first search strategy
with the adjacency constraint we obtain the fol-
lowing alignment:
L?B =
?
???????
???????
no ingen
very sa?rskilt
very mycket
one ingen
is visar
patient mycket
patient t?alamod
?
???????
???????
LCB =
{
no one ingen
is visar
very patient sa?rskilt mycket t?alamod
}
3.5 Summary
None of the alignment approaches described
above produces the preferred reference align-
ment in our example using the given clue ma-
trix. However, simple iterative procedures come
very close to the reference and produce ac-
ceptable alignments even for multi-word units,
which is promising for an automatic clue align-
ment system. Directional alignment models de-
pend very much on the relation between the
source and the target language. One direc-
tion usually works better than the other, e.g.
an alignment from English to Swedish is bet-
ter than Swedish to English because in En-
glish terms and concepts are often split into
several words whereas Swedish tends to con-
tain many compositional compounds. Symmet-
ric approaches to word alignment are certainly
more appropriate for general alignment systems
than directional ones.
4 Evaluation methodology
Word alignment quality is usually measured in
terms of precision and recall. Often, previously
created gold standards are used as reference data
in order to simplify automatic tests of alignment
attempts. Gold standards can be re-used for ad-
ditional test runs which is important when ex-
amining different parameter settings. However,
recall and precision derived from information re-
trieval have to be adjusted for the task of word
alignment. The main difficulty with these mea-
sures in connection with word alignment arises
with links between MWUs that cause partially
correct alignments. It is not straightforward
how to judge such links in order to compute
precision and recall. In order to account for
partiality we use a slightly modified version of
the partiality score Q proposed in (Ahrenberg
et al, 2000)2:
Qprecisionx =
|algxsrc ? corr
x
src|+ |alg
x
trg ? corr
x
trg|
|algxsrc|+ |alg
x
trg|
Qrecallx =
|algxsrc ? corr
x
src|+ |alg
x
trg ? corr
x
trg|
|corrxsrc|+ |corr
x
trg|
The set of algxsrc includes all source language
words of all proposed links if at least one of
them is partially correct with respect to the ref-
erence link x from the gold standard. Similarly,
algxtrg refers to all the proposed target language
words. corrxsrc and corr
x
trg refer to the sets of
source and target language words in link x of
the gold standard. Using the partiality value
Q, we can define the recall and precision met-
rics as follows:
Rmwu =
?X
x=1 Q
recall
x
|correct|
, Pmwu =
?X
x=1 Q
precision
x
|aligned|
A balanced F-score can be used to combine
both, precision and recall:
Fmwu = (2 ? Pmwu ?Rmwu)/(Pmwu +Rmwu).
2Qx ? 0 for incorrect links for both, precision and
recall.
 65
 70
 75
 80
 85
 90
 60  65  70  75  80  85  90
re
ca
ll
precision
different search strategies
dice+pp
giza
giza+pp
dice+pp
giza
giza+pp
F=80%
F=70%
competitive
union
intersection
refined
best-first
directional
inverse
Figure 3: Different alignment search strategies. Clue alignment settings: dice+pp, giza, and
giza+pp. Alignment strategies: directional (LD), inverse directional (LI), union (L?), intersec-
tion (L?), refined (LR), competitive linking (LC), and constrained best-first (LB).
Alternative measures for the evaluation of
one-to-one word links have been proposed in
(Och and Ney, 2000a; Och and Ney, 2003).
However, these measures require completely
aligned bitext segments as reference data. Our
gold standards include random samples from
the corpus instead (Ahrenberg et al, 2000).
Furthermore, we do not split MWU links as pro-
posed by (Och and Ney, 2000a). Therefore, the
measures proposed above are a natural choice
for our evaluations.
5 Experiments
Several alignment search strategies have been
discussed in the previous sections. Our clue
aligner implements these strategies in order to
test their impact on the alignment performance.
In the experiments we used one of our English-
Swedish bitext from the PLUG corpus (S?agvall
Hein, 2002), the novel ?To Jerusalem and back:
A personal account? by Saul Bellow. This cor-
pus is fairly small (about 170,000 words) and
therefore well suited for extensive studies of
alignment parameters. For evaluation, a gold
standard of 468 manually aligned links is used
(Merkel et al, 2002). It includes 122 links with
MWUs either on the source or on the target
side (= 26% of the gold standard). 109 links
contain source language MWUs, 59 links target
language MWUs, and 46 links MWUs in both
languages. 10 links are null links, i.e. a link
of one word to an empty string. Three differ-
ent clue types are used for the alignment: the
Dice coefficient (dice), lexical translation proba-
bilities derived from statistical translation mod-
els (giza) using the GIZA++ toolbox (Och and
Ney, 2003), and, finally, POS/relative-word-
position-clues learned from previous alignments
(pp). Alignment strategies are compared on the
basis of three different settings: dice+pp, giza,
and giza+pp. In figure 3, the alignment results
are shown for the three clue settings using dif-
ferent search strategies as discussed earlier.
5.1 Discussion
Figure 3 illustrates the relation between pre-
cision and recall when applying different algo-
rithms. As expected, the intersection of direc-
tional alignment strategies yields the highest
precision at the expense of recall, which is gen-
erally lower than for the other approaches. Con-
trary to the intersection, the union of directional
links produces alignments with the highest re-
call values but lower precision than all other
search algorithms. Too many (partially) incor-
rect MWUs are included in the union of direc-
tional links. The intersection on the other hand
includes only one-to-one word links that tend
to be correct. However, many links are missed
in this strategy evident in the low recall val-
giza+pp non-MWU MWU-links (122 in total) English MWU Swedish MWU both
P R correct partial P R P R P R P R
directional 82.66 88.73 19 77 59.23 67.10 58.04 69.84 68.93 70.16 68.85 77.52
inverse 81.93 87.28 5 50 64.39 59.74 64.68 57.87 67.62 72.57 69.21 71.78
union 80.13 91.62 21 88 55.61 73.24 55.57 73.29 63.40 81.74 65.50 84.25
intersection 91.67 85.84 0 98 74.35 52.44 73.56 53.44 83.04 60.31 83.33 64.89
competitive 88.44 88.44 0 105 64.66 58.59 66.11 59.71 72.13 67.94 77.66 73.23
refined 84.61 88.15 28 78 65.91 72.61 65.53 72.70 74.37 80.56 75.86 83.03
best-first 85.07 89.02 28 79 66.40 73.40 66.23 73.77 75.38 81.35 77.52 84.48
Table 1: Evaluations of different link types for the setting giza+pp.
ues. Directional alignment strategies generally
yield lower F-values than other refined symmet-
ric alignment strategies. Their implementation
is straightforward but the results are highly de-
pendent on the language pair under consider-
ation. The differences between the two align-
ment directions in our example are surprisingly
inconsistent. Using the giza clues both align-
ment results are very close in terms of preci-
sion and recall whereas a larger difference can
be observed using the other two clue settings
when applying different directional alignment
strategies. Competitive linking is somewhat
in between the intersection approach and the
two symmetric approaches, ?best-first? and ?re-
fined?. This could also be expected as competi-
tive linking only allows non-overlapping one-to-
one word links. The refined bi-directional align-
ment approach and the constrained best-first
approach are almost identical in our examples
with a more or less balanced relation between
precision and recall. One advantage of the best-
first approach is the possibility of incorporating
different constraints that suit the current task.
The adjacency check is just one of the possi-
ble constraints. For example, syntactic criteria
could be applied in order to force linked items
to be complete according to existing syntactic
markup. Non-contiguous elements could also be
identified using the same approach simply by
removing the adjacency constraint. However,
this seems to increase the noise significantly ac-
cording to experiments not shown in this paper.
Further investigations on optimizing alignment
constraints for certain tasks have to be done in
the future. Focusing on MWUs, the numbers in
table 1 show a clear picture about the difficul-
ties of all approaches to find correct MWU links.
Symmetric alignment strategies like refined and
best-first produce in general the best results for
MWU links. However, the main portion of such
links is only partially correct even for these ap-
proaches. Using our partiality measure, the
intersection of directional alignments still pro-
duces the highest precision values when consid-
ering MWU links only even though no MWUs
are included in these alignments at all. The best
results among MWU links are achieved for the
ones including MWUs in both languages. How-
ever, these results are still significantly lower
than for single-word links (non-MWU).
6 Conclusions
According to our results different alignment
strategies can be chosen to suit particular
needs. Concluding from the experiments, re-
strictive methods like the intersection of direc-
tional alignments or competitive linking should
be chosen if results with high precision are re-
quired (which are mostly found among one-
to-one word links). This is, for example, the
case in automatic extraction of bilingual lexi-
cons where noise should be avoided as much as
possible. A strong disadvantage of these ap-
proaches is that they do not include MWUs at
all. Other strategies should be chosen for appli-
cations, which require a comprehensive cover-
age as, for example, machine translation. Sym-
metric approaches such as the refined combi-
nation of directional alignments and the con-
strained best-first alignment strategy yield the
highest overall performance. They produce the
best balance between precision and recall and
the highest scores in terms of F-values.
References
Lars Ahrenberg, Magnus Merkel, and Mikael
Andersson. 1998. A simple hybrid aligner for
generating lexical correspondences in parallel
texts. In Christian Boitet and Pete White-
lock, editors, Proceedings of the 36th Annual
Meeting of the Association for Computational
Linguistics and the 17th International Con-
ference on Computational Linguistics, pages
29?35, Montreal, Canada.
Lars Ahrenberg, Magnus Merkel, Anna S?agvall
Hein, and Jo?rg Tiedemann. 2000. Evaluation
of word alignment systems. In Proceedings
of the 2nd International Conference on Lan-
guage Resources and Evaluation, volume III,
pages 1255?1261, Athens, Greece.
Saul Bellow. 1976. From Jerusalem and back:
a personal account. The Viking Press, New
York, USA.
Saul Bellow. 1977. Jerusalem tur och retur.
Bonniers, Stockholm. Translation of Caj
Lundgren.
Peter F. Brown, Stephen A. Della Pietra, Vin-
cent J. Della Pietra, and Robert L. Mercer.
1993. The mathematics of statistcal machine
translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
Ismael Garc??a-Varea, Franz Josef Och, Her-
mann Ney, and Francisco Casacuberta. 2002.
Improving alignment quality in statistical
machine translation using context-dependent
maximum entropy models. In Proceedings of
the 19th International Conference on Compu-
tational Linguistics, pages 1051?1054, Taipei,
Taiwan, August.
I. Dan Melamed. 1995. Automatic evaluation
and uniform filter cascades for inducing n-
best translation lexicons. In David Yarovsky
and Kenneth Church, editors, Proceedings of
the 3rd Workshop on Very Large Corpora,
pages 184?198, Boston, MA. Association for
Computational Linguistics.
I. Dan Melamed. 1996. Automatic construction
of clean broad-coverage lexicons. In Proceed-
ings of the 2nd Conference the Association for
Machine Translation in the Americas, pages
125?134, Montreal, Canada.
I. Dan Melamed. 1998. Annotation style guide
for the Blinker project, version 1.0. IRCS
Technical Report 98-06, University of Penn-
sylvania, Philadelphia, PA.
I. Dan Melamed. 2000. Models of transla-
tional equivalence among words. Computa-
tional Linguistics, 26(2):221?249, June.
Magnus Merkel, Mikael Andersson, and Lars
Ahrenberg. 2002. The PLUG link annotator
- interactive construction of data from par-
allel corpora. In Lars Borin, editor, Parallel
Corpora, Parallel Worlds. Rodopi, Amster-
dam, New York. Proceedings of the Sympo-
sium on Parallel Corpora, Department of Lin-
guistics, Uppsala University, Sweden,1999.
Rada Mihalcea and Ted Pedersen. 2003. An
evaluation exercise for word alignment. In
Workshop on Building and Using Parallel
Texts: Data Driven Machine Translation and
Beyond, pages 1?10, Edmonton, Canada,
May.
Franz-Josef Och and Hermann Ney. 2000a.
A comparison of alignment models for sta-
tistical machine translation. In Proceed-
ings of the 18th International Conference on
Computational Linguistics, pages 1086?1090,
Saarbru?cken, Germany, July.
Franz Josef Och and Hermann Ney. 2000b. Im-
proved statistical alignment models. In Proc.
of the 38th Annual Meeting of the Associ-
ation for Computational Linguistics, pages
440?447.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguis-
tics, 29(1):19?51.
Anna S?agvall Hein. 2002. The PLUG project:
Parallel corpora in Linko?ping, Uppsala, and
Go?teborg: Aims and achievements. In Lars
Borin, editor, Parallel Corpora, Parallel
Worlds. Rodopi, Amsterdam, New York.
Proceedings of the Symposium on Parallel
Corpora, Department of Linguistics, Uppsala
University, Sweden,1999.
Frank A. Smadja, Kathleen R. McKeown, and
Vasileios Hatzivassiloglou. 1996. Translating
collocations for bilingual lexicons: A statis-
tical approach. Computational Linguistics,
22(1), pages 1?38.
Jo?rg Tiedemann. 1999. Word alignment -
step by step. In Proceedings of the 12th
Nordic Conference on Computational Lin-
guistics, pages 216?227, University of Trond-
heim, Norway.
Jo?rg Tiedemann. 2003. Combining clues for
word alignment. In Proceedings of the 10th
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL), pages 339?346, Budapest, Hungary,
April.
Dan Tufis and Ana-Maria Barbu. 2002. Lexical
token alignment: Experiments, results and
applications. In Proceedings from The 3rd
International Conference on Language Re-
sources and Evaluation, pages 458?465, Las
Palmas, Spain.
Stephan Vogel, Hermann Ney, and Christoph
Tillmann. 1996. HMM-based word alignment
in statistical translation. In Proceedings of
the 16th International Confernece on Compu-
tational Linguistics, pages 836?841, Copen-
hagen, Denmark.
339
340
341
342
343
344
345
346
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 1179?1190, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Document-Wide Decoding for
Phrase-Based Statistical Machine Translation
Christian Hardmeier Joakim Nivre Jo?rg Tiedemann
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
Independence between sentences is an as-
sumption deeply entrenched in the models and
algorithms used for statistical machine trans-
lation (SMT), particularly in the popular dy-
namic programming beam search decoding al-
gorithm. This restriction is an obstacle to re-
search on more sophisticated discourse-level
models for SMT. We propose a stochastic lo-
cal search decoding method for phrase-based
SMT, which permits free document-wide de-
pendencies in the models. We explore the sta-
bility and the search parameters of this method
and demonstrate that it can be successfully
used to optimise a document-level semantic
language model.
1 Motivation
In the field of translation studies, it is undisputed that
discourse-wide context must be considered care-
fully for good translation results (Hatim and Mason,
1990). By contrast, the state of the art in statistical
machine translation (SMT), despite significant ad-
vances in the last twenty years, still assumes that
texts can be translated sentence by sentence under
strict independence assumptions, even though it is
well known that certain linguistic phenomena such
as pronominal anaphora cannot be translated cor-
rectly without referring to extra-sentential context.
This is true both for the phrase-based and the syntax-
based approach to SMT. In the rest of this paper, we
shall concentrate on phrase-based SMT.
One reason why it is difficult to experiment
with document-wide models for phrase-based SMT
is that the dynamic programming (DP) algorithm
which has been used almost exclusively for decod-
ing SMT models in the recent literature has very
strong assumptions of locality built into it. DP
beam search for phrase-based SMT was described
by Koehn et al(2003), extending earlier work on
word-based SMT (Tillmann et al 1997; Och et al
2001; Tillmann and Ney, 2003). This algorithm con-
structs output sentences by starting with an empty
hypothesis and adding output words at the end until
translations for all source words have been gener-
ated. The core models of phrase-based SMT, in par-
ticular the n-gram language model (LM), only de-
pend on a constant number of output words to the
left of the word being generated. This fact is ex-
ploited by the search algorithm with a DP technique
called hypothesis recombination (Och et al 2001),
which permits the elimination of hypotheses from
the search space if they coincide in a certain number
of final words with a better hypothesis and no future
expansion can possibly invert the relative ranking of
the two hypotheses under the given models. Hypoth-
esis recombination achieves a substantial reduction
of the search space without affecting search optimal-
ity and makes it possible to use aggressive pruning
techniques for fast search while still obtaining good
results.
The downside of this otherwise excellent ap-
proach is that it only works well with models that
have a local dependency structure similar to that
of an n-gram language model, so they only de-
pend on a small context window for each target
word. Sentence-local models with longer dependen-
cies can be added, but doing so greatly increases
the risk for search errors by inhibiting hypothesis
recombination. Cross-sentence dependencies can-
not be directly integrated into DP SMT decoding in
1179
any obvious way, especially if joint optimisation of
a number of interdependent decisions over an entire
document is required. Research into models with
a more varied, non-local dependency structure is to
some extent stifled by the difficulty of decoding such
models effectively, as can be seen by the problems
some researchers encountered when they attempted
to solve discourse-level problems. Consider, for in-
stance, the work on cache-based language models
by Tiedemann (2010) and Gong et al(2011), where
error propagation was a serious issue, or the works
on pronominal anaphora by Le Nagard and Koehn
(2010), who implemented cross-sentence dependen-
cies with an ad-hoc two-pass decoding strategy, and
Hardmeier and Federico (2010) with the use of an
external decoder driver to manage backward-only
dependencies between sentences.
In this paper, we present a method for decoding
complete documents in phrase-based SMT. Our de-
coder uses a local search approach whose state con-
sists of a complete translation of an entire document
at any time. The initial state is improved by the ap-
plication of a series of operations using a hill climb-
ing strategy to find a (local) maximum of the score
function. This setup gives us complete freedom to
define scoring functions over the entire document.
Moreover, by optionally initialising the state with
the output of a traditional DP decoder, we can en-
sure that the final hypothesis is no worse than what
would have been found by DP search alone. We start
by describing the decoding algorithm and the state
operations used by our decoder, then we present em-
pirical results demonstrating the effectiveness of our
approach and its usability with a document-level se-
mantic language model, and finally we discuss some
related work.
2 SMT Decoding by Hill Climbing
In this section, we formally describe the phrase-
based SMT model implemented by our decoder as
well as the decoding algorithm we use.
2.1 SMT Model
Our decoder is based on local search, so its state at
any time is a representation of a complete translation
of the entire document. Even though the decoder op-
erates at the document level, it is important to keep
track of sentence boundaries, and the individual op-
erations that are applied to the state are still confined
to sentence scope, so it is useful to decompose the
state of a document into the state of its sentences,
and we define the overall state S as a sequence of
sentence states:
S = S1S2 . . .SN , (1)
where N is the number of sentences. This implies
that we constrain the decoder to emit exactly one
output sentence per input sentence.
Let i be the number of a sentence and mi the num-
ber of input tokens of this sentence, p and q (with
1 ? p ? q ? mi) be positions in the input sentence
and [p;q] denote the set of positions from p up to and
including q. We say that [p;q] precedes [p?;q?], or
[p;q]? [p?;q?], if q < p?. Let ?i([p;q]) be the set of
translations for the source phrase covering positions
[p;q] in the input sentence i as given by the phrase
table. We call A = ?[p;q],?? an anchored phrase
pair with coverage C(A) = [p;q] if ? ? ?i([p;q]) is
a target phrase translating the source words at posi-
tions [p;q]. Then a sequence of ni anchored phrase
pairs
Si = A1A2 . . .Ani (2)
is a valid sentence state for sentence i if the follow-
ing two conditions hold:
1. The coverage sets C(A j) for j in 1, . . . ,ni are
mutually disjoint, and
2. the anchored phrase pairs jointly cover the
complete input sentence, or
ni?
j=1
C(A j) = [1;mi]. (3)
Let f (S) be a scoring function mapping a state S
to a real number. As usual in SMT, it is assumed that
the scoring function can be decomposed into a linear
combination of K feature functions hk(S), each with
a constant weight ?k, so
f (S) =
K
?
k=1
?khk(S). (4)
The problem addressed by the decoder is the search
for the state S? with maximal score, such that
S? = argmax
S
f (S). (5)
1180
The feature functions implemented in our baseline
system are identical to the ones found in the popular
Moses SMT system (Koehn et al 2007). In particu-
lar, our decoder has the following feature functions:
1. phrase translation scores provided by the
phrase table including forward and backward
conditional probabilities, lexical weights and a
phrase penalty (Koehn et al 2003),
2. n-gram language model scores implemented
with the KenLM toolkit (Heafield, 2011),
3. a word penalty score,
4. a distortion model with geometric decay
(Koehn et al 2003), and
5. a feature indicating the number of times a given
distortion limit is exceeded in the current state.
In our experiments, the last feature is used with a
fixed weight of negative infinity in order to limit the
gaps between the coverage sets of adjacent anchored
phrase pairs to a maximum value. In DP search, the
distortion limit is usually enforced directly by the
search algorithm and is not added as a feature. In
our decoder, however, this restriction is not required
to limit complexity, so we decided to add it among
the scoring models.
2.2 Decoding Algorithm
The decoding algorithm we use (algorithm 1) is
very simple. It starts with a given initial document
state. In the main loop, which extends from line 3
to line 12, it generates a successor state S? for the
current state S by calling the function Neighbour,
which non-deterministically applies one of the oper-
ations described in section 3 of this paper to S. The
score of the new state is compared to that of the pre-
vious one. If it meets a given acceptance criterion,
S? becomes the current state, else search continues
from the previous state S. For the experiments in
this paper, we use the hill climbing acceptance cri-
terion, which simply accepts a new state if its score
is higher than that of the current state. Other accep-
tance criteria are possible and could be used to en-
dow the search algorithm with stochastic behaviour.
The main loop is repeated until a maximum num-
ber of steps (step limit) is reached or until a maxi-
mum number of moves are rejected in a row (rejec-
tion limit).
Algorithm 1 Decoding algorithm
Input: an initial document state S;
search parameters maxsteps and maxrejected
Output: a modified document state
1: nsteps? 0
2: nrejected? 0
3: while nsteps < maxsteps and
nrejected < maxrejected do
4: S?? Neighbour(S)
5: if Accept( f (S?), f (S)) then
6: S? S?
7: nrejected? 0
8: else
9: nrejected? nrejected+1
10: end if
11: nsteps? nsteps+1
12: end while
13: return S
A notable difference between this algorithm and
other hill climbing algorithms that have been used
for SMT decoding (Germann et al 2004; Langlais
et al 2007) is its non-determinism. Previous work
for sentence-level decoding employed a steepest as-
cent strategy which amounts to enumerating the
complete neighbourhood of the current state as de-
fined by the state operations and selecting the next
state to be the best state found in the neighbourhood
of the current one. Enumerating all neighbours of
a given state, costly as it is, has the advantage that
it makes it easy to prove local optimality of a state
by recognising that all possible successor states have
lower scores. It can be rather inefficient, since at
every step only one modification will be adopted;
many of the modifications that are discarded will
very likely be generated anew in the next iteration.
As we extend the decoder to the document level,
the size of the neighbourhood that would have to be
explored in this way increases considerably. More-
over, the inefficiency of the steepest ascent approach
potentially increases as well. Very likely, a promis-
ing move in one sentence will remain promising af-
ter a modification has been applied to another sen-
1181
tence, even though this is not guaranteed to be true
in the presence of cross-sentence models. We there-
fore adopt a first-choice hill climbing strategy that
non-deterministically generates successor states and
accepts the first one that meets the acceptance cri-
terion. This frees us from the necessity of gener-
ating the full set of successors for each state. On
the downside, if the full successor set is not known,
it is no longer possible to prove local optimality of a
state, so we are forced to use a different condition for
halting the search. We use a combination of two lim-
its: The step limit is a hard limit on the resources the
user is willing to expend on the search problem. The
value of the rejection limit determines how much of
the neighbourhood is searched for better successors
before a state is accepted as a solution; it is related
to the probability that a state returned as a solution
is in fact locally optimal.
To simplify notations in the description of the in-
dividual state operations, we write
Si ?? S
?
i (6)
to signify that a state operation, when presented with
a document state as in equation 1 and acting on sen-
tence i, returns a new document state of
S? = S1 . . .Si?1 S
?
i Si+1 . . .SN . (7)
Similarly,
Si : A j . . .A j+h?1 ?? A
?
1 . . .A
?
h? (8)
is equivalent to
Si ?? A1 . . .A j?1 A
?
1 . . .A
?
h? A j+h . . .Ani (9)
and indicates that the operation returns a state in
which a sequence of h consecutive anchored phrase
pairs has been replaced by another sequence of h?
anchored phrase pairs.
2.3 Efficiency Considerations
When implementing the feature functions for the de-
coder, we have to exercise some care to avoid re-
computing scores for the whole document at every
iteration. To achieve this, the scores are computed
completely only once, at the beginning of the de-
coding run. In subsequent iterations, scoring func-
tions are presented with the scores of the previous
iteration and a list of modifications produced by the
state operation, a set of tuples ?i,r,s,A?1 . . .A
?
h??, each
indicating that the document should be modified as
described by
Si : Ar . . .As ?? A
?
1 . . .A
?
h? . (10)
If a feature function is decomposable in some way,
as all the standard features developed under the con-
straints of DP search are, it can then update the state
simply by subtracting and adding score components
pertaining to the modified parts of the document.
Feature functions have the possibility to store their
own state information along with the document state
to make sure the required information is available.
Thus, the framework makes it possible to exploit de-
composability for efficient scoring without impos-
ing any particular decomposition on the features as
beam search does.
To make scoring even more efficient, scores are
computed in two passes: First, every feature func-
tion is asked to provide an upper bound on the score
that will be obtained for the new state. In some
cases, it is possible to calculate reasonable upper
bounds much more efficiently than computing the
exact feature value. If the upper bound fails to meet
the acceptance criterion, the new state is discarded
right away; if not, the full score is computed and the
acceptance criterion is tested again.
Among the basic SMT models, this two-pass
strategy is only used for the n-gram LM, which re-
quires fairly expensive parameter lookups for scor-
ing. The scores of all the other baseline models are
fully computed during the first scoring pass. The
n-gram model is more complex. In its state informa-
tion, it keeps track of the LM score and LM library
state for each word. The first scoring pass then iden-
tifies the words whose LM scores are affected by the
current search step. This includes the words changed
by the search operation as well as the words whose
LM history is modified. The range of the history de-
pendencies can be determined precisely by consider-
ing the ?valid state length? information provided by
the KenLM library. In the first pass, the LM scores
of the affected words are subtracted from the total
score. The model only looks up the new LM scores
for the affected words and updates the total score
if the new search state passes the first acceptance
check. This two-pass scoring approach allows us
1182
to avoid LM lookups altogether for states that will
be rejected anyhow because of low scores from the
other models, e. g. because the distortion limit is vi-
olated.
Model score updates become more complex and
slower as the number of dependencies of a model in-
creases. While our decoding algorithm does not im-
pose any formal restrictions on the number or type
of dependencies that can be handled, there will be
practical limits beyond which decoding becomes un-
acceptably slow or the scoring code becomes very
difficult to maintain. These limits are however fairly
independent of the types of dependencies handled
by a model, which permits the exploration of more
varied model types than those handled by DP search.
2.4 State Initialisation
Before the hill climbing decoding algorithm can be
run, an initial state must be generated. The closer the
initial state is to an optimum, the less work remains
to be done for the algorithm. If the algorithm is to be
self-contained, initialisation must be relatively unin-
formed and can only rely on some general prior as-
sumptions about what might be a good initial guess.
On the other hand, if optimal results are sought after,
it pays off to invest some effort into a good starting
point. One way to do this is to run DP search first.
For uninformed initialisation, we chose to imple-
ment a very simple procedure based only on the ob-
servation that, at least for language pairs involving
the major European languages, it is usually a good
guess to keep the word order of the output very sim-
ilar to that of the input. We therefore create the ini-
tial state by selecting, for each sentence in the docu-
ment, a sequence of anchored phrase pairs covering
the input sentence in monotonic order, that is, such
that for all pairs of adjacent anchored phrase pairs
A j and A j+1, we have that C(A j)?C(A j+1).
For initialisation with DP search, we first run the
Moses decoder (Koehn et al 2007) with default
search parameters and the same models as those
used by our decoder. Then we extract the best output
hypothesis from the search graph of the decoder and
map it into a sequence of anchored phrase pairs in
the obvious way. When the document-level decoder
is used with models that are incompatible with beam
search, Moses can be run with a subset of the mod-
els in order to find an approximation of the solution
which is then refined with the complete feature set.
3 State Operations
Given a document state S, the decoder uses a neigh-
bourhood function Neighbour to simulate a move
in the state space. The neighbourhood function non-
deterministically selects a type of state operation and
a location in the document to apply it to and returns
the resulting new state. We use a set of three opera-
tions that has the property that every possible docu-
ment state can be reached from every other state in
a sequence of moves.
Designing operations for state transitions in lo-
cal search for phrase-based SMT is a problem that
has been addressed in the literature (Langlais et
al., 2007; Arun et al 2010). Our decoder?s first-
choice hill climbing strategy never enumerates the
full neighbourhood of a state. We therefore place
less emphasis than previous work on defining a com-
pact neighbourhood, but allow the decoder to make
quite extensive changes to a state in a single step
with a certain probability. Otherwise our operations
are similar to those used by Arun et al(2010).
All of the operations described in this paper make
changes to a single sentence only. Each time it is
called, the Neighbour function selects a sentence
in the document with a probability proportional to
the number of input tokens in each sentence to en-
sure a fair distribution of the decoder?s attention over
the words in the document regardless of varying sen-
tence lengths.
3.1 Changing Phrase Translations
The change-phrase-translation operation re-
places the translation of a single phrase with a ran-
dom translation with the same coverage taken from
the phrase table. Formally, the operation selects an
anchored phrase pair A j by drawing uniformly from
the elements of Si and then draws a new translation
? ? uniformly from the set ?i(C(A j)). The new state
is given by
Si : A j ?? ?C(A j),? ??. (11)
3.2 Changing Word Order
The swap-phrases operation affects the output
word order without changing the phrase translations.
1183
It exchanges two anchored phrase pairs A j and A j+h,
resulting in an output state of
Si : A j . . .A j+h ?? A j+h A j+1 . . .A j+h?1 A j. (12)
The start location j is drawn uniformly from the el-
igible sentence positions; the swap range h comes
from a geometric distribution with configurable de-
cay. Other word-order changes such as a one-way
move operation that does not require another move-
ment in exchange or more advanced permutations
can easily be defined.
3.3 Resegmentation
The most complex operation is resegment, which
allows the decoder to modify the segmentation of the
source phrase. It takes a number of anchored phrase
pairs that form a contiguous block both in the input
and in the output and replaces them with a new set
of phrase pairs covering the same span of the input
sentence. Formally,
Si : A j . . .A j+h?1 ?? A
?
1 . . .A
?
h? (13)
such that
j+h?1?
j?= j
C(A j?) =
h??
j?=1
C(A?j?) = [p;q] (14)
for some p and q, where, for j? = 1, . . . ,h?, we
have that A?j? = ?[p j? ;q j? ],? j??, all [p j? ;q j? ] are mu-
tually disjoint and each ? j? is randomly drawn from
?i([p j? ;q j? ]).
Regardless of the ordering of A j . . .A j+h?1, the
resegment operation always generates a sequence
of anchored phrase pairs in linear order, such that
C(A?j?)?C(A
?
j?+1) for j
? = 1, . . . ,h??1.
As for the other operations, j is generated uni-
formly and h is drawn from a geometric distribution
with a decay parameter. The new segmentation is
generated by extending the sequence of anchored
phrase pairs with random elements starting at the
next free position, proceeding from left to right until
the whole range [p;q] is covered.
4 Experimental Results
In this section, we present the results of a series
of experiments with our document decoder. The
goal of our experiments is to demonstrate the be-
haviour of the decoder and characterise its response
to changes in the fundamental search parameters.
The SMT models for our experiments were cre-
ated with a subset of the training data for the
English-French shared task at the WMT 2011 work-
shop (Callison-Burch et al 2011). The phrase ta-
ble was trained on Europarl, news-commentary and
UN data. To reduce the training data to a manage-
able size, singleton phrase pairs were removed be-
fore the phrase scoring step. Significance-based fil-
tering (Johnson et al 2007) was applied to the re-
sulting phrase table. The language model was a 5-
gram model with Kneser-Ney smoothing trained on
the monolingual News corpus with IRSTLM (Fed-
erico et al 2008). Feature weights were trained with
Minimum Error-Rate Training (MERT) (Och, 2003)
on the news-test2008 development set using the DP
beam search decoder and the MERT implementation
of the Moses toolkit (Koehn et al 2007). Experi-
mental results are reported for the newstest2009 test
set, a corpus of 111 newswire documents totalling
2,525 sentences or 65,595 English input tokens.
4.1 Stability
An important difference between our decoder and
the classical DP decoder as well as previous work in
SMT decoding with local search is that our decoder
is inherently non-deterministic. This implies that re-
peated runs of the decoder with the same search pa-
rameters, input and models will not, in general, find
the same local maximum of the score space. The
first empirical question we ask is therefore how dif-
ferent the results are under repeated runs. The re-
sults in this and the next section were obtained with
random state initialisation, i. e. without running the
DP beam search decoder.
Figure 1 shows the results of 7 decoder runs with
the models described above, translating the news-
test2009 test set, with a step limit of 227 and a rejec-
tion limit of 100,000. The x-axis of both plots shows
the number of decoding steps on a logarithmic scale,
so the number of steps is doubled between two adja-
cent points on the same curve. In the left plot, the
y-axis indicates the model score optimised by the
decoder summed over all 2525 sentences of the doc-
ument. In the right plot, the case-sensitive BLEU
score (Papineni et al 2002) of the current decoder
1184
Figure 1: Score stability in repeated decoder runs
state against a reference translation is displayed.
We note, as expected, that the decoder achieves
a considerable improvement of the initial state with
diminishing returns as decoding continues. Be-
tween 28 and 214 steps, the score increases at a
roughly logarithmic pace, then the curve flattens out,
which is partly due to the fact that decoding for
some documents effectively stopped when the max-
imum number of rejections was reached. The BLEU
score curve shows a similar increase, from an initial
score below 5 % to a maximum of around 21.5 %.
This is below the score of 22.45 % achieved by the
beam search decoder with the same models, which
is not surprising considering that our decoder ap-
proximates a more difficult search problem, from
which a number of strong independence assump-
tions have been lifted, without, at the moment, hav-
ing any stronger models at its disposal to exploit this
additional freedom for better translation.
In terms of stability, there are no dramatic differ-
ences between the decoder runs. Indeed, the small
differences that exist are hardly discernible in the
plots. The model scores at the end of the decod-
ing run range between ?158767.9 and ?158716.9,
a relative difference of only about 0.03 %. Final
BLEU scores range from 21.41 % to 21.63 %, an in-
terval that is not negligible, but comparable to the
variance observed when, e. g., feature weights from
repeated MERT runs are used with one and the same
SMT system. Note that these results were obtained
with random state initialisation. With DP initialisa-
tion, score differences between repeated runs rarely
exceed 0.02 absolute BLEU percentage points.
Overall, we conclude that the decoding results of
our algorithm are reasonably stable despite the non-
determinism inherent in the procedure. In our sub-
sequent experiments, the evaluation scores reported
are calculated as the mean of three runs for each ex-
periment.
4.2 Search Algorithm Parameters
The hill climbing algorithm we use has two param-
eters which govern the trade-off between decoding
time and the accuracy with which a local maximum
is identified: The step limit stops the search pro-
cess after a certain number of steps regardless of the
search progress made or lack thereof. The rejection
limit stops the search after a certain number of un-
successful attempts to make a step, when continued
search does not seem to be promising. In most of our
experiments, we used a step limit of 227 ? 1.3 ? 108
and a rejection limit of 105. In practice, decoding
terminates by reaching the rejection limit for the vast
majority of documents. We therefore examined the
effect of different rejection limits on the learning
curves. The results are shown in figure 2.
The results show that continued search does pay
off to a certain extent. Indeed, the curve for re-
jection limit 107 seems to indicate that the model
score increases roughly logarithmically, albeit to a
higher base, even after the curve has started to flat-
ten out at 214 steps. At a certain point, however, the
probability of finding a good successor state drops
rather sharply by about two orders of magnitude, as
1185
Figure 2: Search performance at different rejection limits
evidenced by the fact that a rejection limit of 106
does not give a large improvement over one of 105,
while one of 107 does. The continued model score
improvement also results in an increase in BLEU
scores, and with a BLEU score of 22.1 % the system
with rejection limit 107 is fairly close to the score of
22.45 % obtained by DP beam search.
Obviously, more exact search comes at a cost, and
in this case, it comes at a considerable cost, which is
an explosion of the time required to decode the test
set from 4 minutes at rejection limit 103 to 224 min-
utes at rejection limit 105 and 38 hours 45 minutes
at limit 107. The DP decoder takes 31 minutes for
the same task. We conclude that the rejection limit
of 105 selected for our experiments, while techni-
cally suboptimal, realises a good trade-off between
decoding time and accuracy.
4.3 A Semantic Document Language Model
In this section, we present the results of the applica-
tion of our decoder to an actual SMT model with
cross-sentence features. Our model addresses the
problem of lexical cohesion. In particular, it rewards
the use of semantically related words in the trans-
lation output by the decoder, where semantic dis-
tance is measured with a word space model based
on Latent Semantic Analysis (LSA). LSA has been
applied to semantic language modelling in previous
research with some success (Coccaro and Jurafsky,
1998; Bellegarda, 2000; Wandmacher and Antoine,
2007). In SMT, it has mostly been used for domain
adaptation (Kim and Khudanpur, 2004; Tam et al
2007), or to measure sentence similarities (Banchs
and Costa-jussa`, 2011).
The model we use is inspired by Bellegarda
(2000). It is a Markov model, similar to a stan-
dard n-gram model, and assigns to each content
word a score given a history of n preceding content
words, where n = 30 below. Scoring relies on a 30-
dimensional LSA word vector space trained with the
S-Space software (Jurgens and Stevens, 2010). The
score is defined based on the cosine similarity be-
tween the word vector of the predicted word and the
mean word vector of the words in the history, which
is converted to a probability by histogram lookup
as suggested by Bellegarda (2000). The model is
structurally different from a regular n-gram model
in that word vector n-grams are defined over content
words occurring in the word vector model only and
can cross sentence boundaries. Stop words, identi-
fied by an extensive stop word list and amounting to
around 60 % of the tokens, are scored by a different
mechanism based on their relative frequency (undis-
counted unigram probability) in the training corpus.
In sum, the score produced by the semantic docu-
ment LM has the following form:
h(w|h)=
?
??
??
punigr(w) if w is a stop word, else
? pcos(w|h) if w is known, else
? if w is unknown,
(15)
where ? is the proportion of content words in the
training corpus and ? is a small fixed probability.
It is integrated into the decoder as an extra feature
function. Since we lack an automatic method for
1186
training the feature weights of document-wide fea-
tures, its weight was selected by grid search over
a number of values, comparing translation perfor-
mance for the newstest2009 test set.
In these experiments, we used DP beam search
to initialise the state of our local search decoder.
Three results are presented (table 1): The first table
row shows the baseline performance using DP beam
search with standard sentence-local features only.
The scores in the second row were obtained by run-
ning the hill climbing decoder with DP initialisation,
but without adding any models. A marginal increase
in scores for all three test sets demonstrates that the
hill climbing decoder manages to fix some of the
search errors made by the DP search. The last row
contains the scores obtained by adding in the seman-
tic language model. Scores are presented for three
publicly available test sets from recent WMT Ma-
chine Translation shared tasks, of which one (news-
test2009) was used to monitor progress during de-
velopment and select the final model.
Adding the semantic language model results in a
small increase in NIST scores (Doddington, 2002)
for all three test sets as well as a small BLEU score
gain (Papineni et al 2002) for two out of three cor-
pora. We note that the NIST score turned out to re-
act more sensitively to improvements due to the se-
mantic LM in all our experiments, which is reason-
able because the model specifically targets content
words, which benefit from the information weight-
ing done by the NIST score. While the results
we present do not constitute compelling evidence
in favour of our semantic LM in its current form,
they do suggest that this model could be improved
to realise higher gains from cross-sentence seman-
tic information. They support our claim that cross-
sentence models should be examined more closely
and that existing methods should be adapted to deal
with them, a problem addressed by our main contri-
bution, the local search document decoder.
5 Related Work
Even though DP beam search (Koehn et al 2003)
has been the dominant approach to SMT decoding
in recent years, methods based on local search have
been explored at various times. For word-based
SMT, greedy hill-climbing techniques were advo-
cated as a faster replacement for beam search (Ger-
mann et al 2001; Germann, 2003; Germann et al
2004), and a problem formulation specifically tar-
geting word reordering with an efficient word re-
ordering algorithm has been proposed (Eisner and
Tromble, 2006).
A local search decoder has been advanced as a
faster alternative to beam search also for phrase-
based SMT (Langlais et al 2007; Langlais et al
2008). That work anticipates many of the features
found in our decoder, including the use of local
search to refine an initial hypothesis produced by
DP beam search. The possibility of using models
that do not fit well into the beam search paradigm is
mentioned and illustrated with the example of a re-
versed n-gram language model, which the authors
claim would be difficult to implement in a beam
search decoder. Similarly to the work by Germann
et al(2001), their decoder is deterministic and ex-
plores the entire neighbourhood of a state in order
to identify the most promising step. Our main con-
tribution with respect to the work by Langlais et al
(2007) is the introduction of the possibility of han-
dling document-level models by lifting the assump-
tion of sentence independence. As a consequence,
enumerating the entire neighbourhood becomes too
expensive, which is why we resort to a ?first-choice?
strategy that non-deterministically generates states
and accepts the first one encountered that meets the
acceptance criterion.
More recently, Gibbs sampling was proposed as
a way to generate samples from the posterior distri-
bution of a phrase-based SMT decoder (Arun et al
2009; Arun et al 2010), a process that resembles
local search in its use of a set of state-modifying
operators to generate a sequence of decoder states.
Where local search seeks for the best state attainable
from a given initial state, Gibbs sampling produces
a representative sample from the posterior. Like all
work on SMT decoding that we know of, the Gibbs
sampler presented by Arun et al(2010) assumes in-
dependence of sentences and considers the complete
neighbourhood of each state before taking a sample.
6 Conclusion
In the last twenty years of SMT research, there has
been a strong assumption that sentences in a text
1187
newstest2009 newstest2010 newstest2011
BLEU NIST BLEU NIST BLEU NIST
DP search only 22.56 6.513 27.27 7.034 24.94 7.170
DP + hill climbing 22.60 6.518 27.33 7.046 24.97 7.169
with semantic LM 22.71 6.549 27.53 7.087 24.90 7.199
Table 1: Experimental results with a cross-sentence semantic language model
are independent of one another, and discourse con-
text has been largely neglected. Several factors have
contributed to this. Developing good discourse-level
models is difficult, and considering the modest trans-
lation quality that has long been achieved by SMT,
there have been more pressing problems to solve and
lower hanging fruit to pick. However, we argue that
the popular DP beam search algorithm, which deliv-
ers excellent decoding performance, but imposes a
particular kind of local dependency structure on the
feature models, has also had its share in driving re-
searchers away from discourse-level problems.
In this paper, we have presented a decoding pro-
cedure for phrase-based SMT that makes it possi-
ble to define feature models with cross-sentence de-
pendencies. Our algorithm can be combined with
DP beam search to leverage the quality of the tradi-
tional approach with increased flexibility for models
at the discourse level. We have presented prelimi-
nary results on a cross-sentence semantic language
model addressing the problem of lexical cohesion to
demonstrate that this kind of models is worth explor-
ing further. Besides lexical cohesion, cross-sentence
models are relevant for other linguistic phenomena
such as pronominal anaphora or verb tense selection.
We believe that SMT research has reached a point of
maturity where discourse phenomena should not be
ignored any longer, and we consider our decoder to
be a step towards this goal.
References
Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blun-
som, Adam Lopez, and Philipp Koehn. 2009. Monte
carlo inference and maximization for phrase-based
translation. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 102?110, Boulder, Colorado,
June. Association for Computational Linguistics.
Abhishek Arun, Barry Haddow, Philipp Koehn, Adam
Lopez, Chris Dyer, and Phil Blunsom. 2010. Monte
Carlo techniques for phrase-based translation. Ma-
chine translation, 24(2):103?121.
Rafael E. Banchs and Marta R. Costa-jussa`. 2011. A se-
mantic feature for Statistical Machine Translation. In
Proceedings of Fifth Workshop on Syntax, Semantics
and Structure in Statistical Translation, pages 126?
134, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Jerome R. Bellegarda. 2000. Exploiting latent semantic
information in statistical language modeling. Proceed-
ings of the IEEE, 88(8):1279?1296.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 Work-
shop on Statistical Machine Translation. In Proceed-
ings of the Sixth Workshop on Statistical Machine
Translation, pages 22?64, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Noah Coccaro and Daniel Jurafsky. 1998. Towards bet-
ter integration of semantic predictors in statistical lan-
guage modeling. In Proceedings of the 5th Interna-
tional Conference on Spoken Language Processing,
Sydney.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the second Interna-
tional conference on Human Language Technology
Research, pages 138?145, San Diego.
Jason Eisner and Roy W. Tromble. 2006. Local search
with very large-scale neighborhoods for optimal per-
mutations in machine translation. In Proceedings of
the HLT-NAACL Workshop on Computationally Hard
Problems and Joint Inference in Speech and Language
Processing, pages 57?75.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Interspeech 2008,
pages 1618?1621. ISCA.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proceed-
ings of 39th Annual Meeting of the Association for
Computational Linguistics, pages 228?235, Toulouse,
France, July. Association for Computational Linguis-
tics.
1188
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2004. Fast and optimal de-
coding for machine translation. Artificial Intelligence,
154(1?2):127?143.
Ulrich Germann. 2003. Greedy decoding for Statis-
tical Machine Translation in almost linear time. In
Proceedings of the 2003 Human Language Technol-
ogy Conference of the North American Chapter of the
Association for Computational Linguistics.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level Statistical Ma-
chine Translation. In Proceedings of the 2011 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 909?919, Edinburgh, Scotland, UK.,
July. Association for Computational Linguistics.
Christian Hardmeier and Marcello Federico. 2010. Mod-
elling Pronominal Anaphora in Statistical Machine
Translation. In Proceedings of the seventh Inter-
national Workshop on Spoken Language Translation
(IWSLT), pages 283?289.
Basil Hatim and Ian Mason. 1990. Discourse and the
Translator. Language in Social Life Series. Longman,
London.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Howard Johnson, Joel Martin, George Foster, and Roland
Kuhn. 2007. Improving translation quality by dis-
carding most of the phrasetable. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 967?
975, Prague, Czech Republic, June. Association for
Computational Linguistics.
David Jurgens and Keith Stevens. 2010. The S-Space
package: An open source package for word space
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30?35, Uppsala, Sweden, July.
Association for Computational Linguistics.
Woosung Kim and Sanjeev Khudanpur. 2004. Cross-
lingual latent semantic analysis for language model-
ing. In IEEE international conference on acoustics,
speech, and signal processing (ICASSP), volume 1,
pages 257?260, Montre?al.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 conference of the North Ameri-
can chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 48?
54, Edmonton.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for Statistical Ma-
chine Translation. In Annual meeting of the Associ-
ation for Computational Linguistics: Demonstration
session, pages 177?180, Prague.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2007. A greedy decoder for phrase-based statistical
machine translation. In TMI-2007: Proceedings of
the 11th International Conference on Theoretical and
Methodological Issues in Machine Translation, pages
104?113, Sko?vde.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2008. Recherche locale pour la traduction statistique
par segments. In TALN 2008, pages 119?128, Avi-
gnon, France, June. ATALA.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding pro-
noun translation with co-reference resolution. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 252?261,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for Statisti-
cal Machine Translation. In Proceedings of the Data-
Driven Machine Translation Workshop, 39th Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 55?62, Toulouse.
Franz Josef Och. 2003. Minimum error rate training in
Statistical Machine Translation. In Proceedings of the
41st annual meeting of the Association for Computa-
tional Linguistics, pages 160?167, Sapporo (Japan).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of Machine Translation. In Proceedings of the
40th annual meeting of the Association for Computa-
tional Linguistics, pages 311?318, Philadelphia. ACL.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2007.
Bilingual LSA-based adaptation for Statistical Ma-
chine Translation. Machine Translation, 21(4):187?
207.
Jo?rg Tiedemann. 2010. To cache or not to cache? Ex-
periments with adaptive models in Statistical Machine
Translation. In Proceedings of the ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 189?194, Uppsala, Sweden. As-
sociation for Computational Linguistics.
Christoph Tillmann and Hermann Ney. 2003. Word re-
ordering and a Dynamic Programming beam search al-
gorithm for Statistical Machine Translation. Compu-
tational linguistics, 29(1):97?133.
Christoph Tillmann, Stephan Vogel, Hermann Ney, and
Alex Zubiaga. 1997. A DP-based search using mono-
tone alignments in Statistical Translation. In Proceed-
ings of the 35th Annual Meeting of the Association for
1189
Computational Linguistics, pages 289?296, Madrid,
Spain, July. Association for Computational Linguis-
tics.
Tonio Wandmacher and Jean-Yves Antoine. 2007.
Methods to integrate a language model with seman-
tic information for a word prediction component. In
Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 506?513, Prague, Czech Republic,
June. Association for Computational Linguistics.
1190
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 380?391,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Latent Anaphora Resolution for Cross-Lingual Pronoun Prediction
Christian Hardmeier J?rg Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
This paper addresses the task of predicting the
correct French translations of third-person sub-
ject pronouns in English discourse, a problem
that is relevant as a prerequisite for machine
translation and that requires anaphora resolu-
tion. We present an approach based on neu-
ral networks that models anaphoric links as
latent variables and show that its performance
is competitive with that of a system with sep-
arate anaphora resolution while not requiring
any coreference-annotated training data. This
demonstrates that the information contained in
parallel bitexts can successfully be used to ac-
quire knowledge about pronominal anaphora
in an unsupervised way.
1 Motivation
When texts are translated from one language into
another, the translation reconstructs the meaning or
function of the source text with the means of the
target language. Generally, this has the effect that
the entities occurring in the translation and their mu-
tual relations will display similar patterns as the enti-
ties in the source text. In particular, coreference pat-
terns tend to be very similar in translations of a text,
and this fact has been exploited with good results to
project coreference annotations from one language
into another by using word alignments (Postolache
et al, 2006; Rahman and Ng, 2012).
On the other hand, what is true in general need
not be true for all types of linguistic elements. For
instance, a substantial percentage of the English third-
person subject pronouns he, she, it and they does
not get realised as pronouns in French translations
(Hardmeier, 2012). Moreover, it has been recognised
by various authors in the statistical machine transla-
tion (SMT) community (Le Nagard and Koehn, 2010;
Hardmeier and Federico, 2010; Guillou, 2012) that
pronoun translation is a difficult problem because,
even when a pronoun does get translated as a pro-
noun, it may require choosing the correct word form
based on agreement features that are not easily pre-
dictable from the source text.
The work presented in this paper investigates
the problem of cross-lingual pronoun prediction for
English-French. Given an English pronoun and its
discourse context as well as a French translation of
the same discourse and word alignments between
the two languages, we attempt to predict the French
word aligned to the English pronoun. As far as we
know, this task has not been addressed in the litera-
ture before. In our opinion, it is interesting for several
reasons. By studying pronoun prediction as a task in
its own right, we hope to contribute towards a better
understanding of pronoun translation with a long-
term view to improving the performance of SMT
systems. Moreover, we believe that this task can lead
to interesting insights about anaphora resolution in a
multi-lingual context. In particular, we show in this
paper that the pronoun prediction task makes it possi-
ble to model the resolution of pronominal anaphora
as a latent variable and opens up a way to solve a
task relying on anaphora resolution without using
any data annotated for anaphora. This is what we
consider the main contribution of our present work.
We start by modelling cross-lingual pronoun pre-
diction as an independent machine learning task after
doing anaphora resolution in the source language
(English) using the BART software (Broscheit et
al., 2010). We show that it is difficult to achieve
satisfactory performance with standard maximum-
380
The latest version released in March is equipped with ... It is sold at ...
La derni?re version lanc?e en mars est dot?e de ... ? est vendue ...
Figure 1: Task setup
entropy classifiers especially for low-frequency pro-
nouns such as the French feminine plural pronoun
elles. We propose a neural network classifier that
achieves better precision and recall and manages to
make reasonable predictions for all pronoun cate-
gories in many cases.
We then go on to extend our neural network archi-
tecture to include anaphoric links as latent variables.
We demonstrate that our classifier, now with its own
source language anaphora resolver, can be trained
successfully with backpropagation. In this setup, we
no longer use the machine learning component in-
cluded in the external coreference resolution system
(BART) to predict anaphoric links. Anaphora reso-
lution is done by our neural network classifier and
requires only some quantity of word-aligned parallel
data for training, completely obviating the need for a
coreference-annotated training set.
2 Task Setup
The overall setup of the classification task we address
in this paper is shown in Figure 1. We are given an
English discourse containing a pronoun along with
its French translation and word alignments between
the two languages, which in our case were computed
automatically using a standard SMT pipeline with
GIZA++ (Och and Ney, 2003). We focus on the four
English third-person subject pronouns he, she, it and
they. The output of the classifier is a multinomial
distribution over six classes: the four French subject
pronouns il, elle, ils and elles, corresponding to mas-
culine and feminine singular and plural, respectively;
the impersonal pronoun ce/c?, which occurs in some
very frequent constructions such as c?est (it is); and
a sixth class OTHER, which indicates that none of
these pronouns was used. In general, a pronoun may
be aligned to multiple words; in this case, a training
example is counted as a positive example for a class
if the target word occurs among the words aligned
to the pronoun, irrespective of the presence of other
0 0 0 1 0version
0 1 0 0 0la
0 0 1 0 0elle
0 .5 0 .5 0
0 0 1 0 0
0 .05 .9 .05 0
p1 = .9
p2 =
.1
word candidate training ex.
Figure 2: Antecedent feature aggregation
aligned tokens.
This task setup resembles the problem that an
SMT system would have to solve to make informed
choices when translating pronouns, an aspect of trans-
lation neglected by most existing SMT systems. An
important difference between the SMT setup and our
own classifiers is that we use context from human-
made translations for prediction. This potentially
makes the task both easier and more difficult; easier,
because the context can be relied on to be correctly
translated, and more difficult, because human transla-
tors frequently create less literal translations than an
SMT system would. Integrating pronoun prediction
into the translation process would require significant
changes to the standard SMT decoding setup in order
to take long-range dependencies in the target lan-
guage into account, which is why we do not address
this issue in our current work.
In all the experiments presented in this paper, we
used features from two different sources:
? Anaphora context features describe the source
language pronoun and its immediate context
consisting of three words to its left and three
words to its right. They are encoded as vec-
tors whose dimensionality is equal to the source
vocabulary size with a single non-zero compo-
nent indicating the word referred to (one-hot
vectors).
? Antecedent features describe an antecedent can-
didate. Antecedent candidates are represented
by the target language words aligned to the syn-
tactic head of the source language markable
381
TED News
ce 16.3 % 6.4 %
elle 7.1 % 10.1 %
elles 3.0 % 3.9 %
il 17.1 % 26.5 %
ils 15.6 % 15.1 %
OTHER 40.9 % 38.0 %
Table 1: Distribution of classes in the training data
noun phrase as identified by the Collins head
finder (Collins, 1999).
The different handling of anaphora context features
and antecedent features is due to the fact that we al-
ways consider a constant number of context words
on the source side, whereas the number of word
vectors to be considered depends on the number of
antecedent candidates and on the number of target
words aligned to each antecedent.
The encoding of the antecedent features is illus-
trated in Figure 2 for a training example with two
antecedent candidates translated to elle and la ver-
sion, respectively. The target words are represented
as one-hot vectors with the dimensionality of the tar-
get language vocabulary. These vectors are then av-
eraged to yield a single vector per antecedent candi-
date. Finally, the vectors of all candidates for a given
training example are weighted by the probabilities
assigned to them by the anaphora resolver (p1 and
p2) and summed to yield a single vector per training
example.
3 Data Sets and External Tools
We run experiments with two different test sets. The
TED data set consists of around 2.6 million tokens of
lecture subtitles released in the WIT3 corpus (Cet-
tolo et al, 2012). The WIT3 training data yields
71,052 examples, which were randomly partitioned
into a training set of 63,228 examples and a test set
of 7,824 examples. The official WIT3 development
and test sets were not used in our experiments. The
news-commentary data set is version 6 of the parallel
news-commentary corpus released as a part of the
WMT 2011 training data1. It contains around 2.8 mil-
lion tokens of news text and yields 31,017 data points,
1http://www.statmt.org/wmt11/translation-task.
html (3 July 2013).
which were randomly split into 27,900 training exam-
ples and 3,117 test instances. The distribution of the
classes in the two training sets is shown in Table 1.
One thing to note is the dominance of the OTHER
class, which pools together such different phenom-
ena as translations with other pronouns not in our list
(e. g., celui-ci) and translations with full noun phrases
instead of pronouns. Splitting this group into more
meaningful subcategories is not straightforward and
must be left to future work.
The feature setup of all our classifiers requires
the detection of potential antecedents and the extrac-
tion of features pairing anaphoric pronouns with an-
tecedent candidates. Some of our experiments also
rely on an external anaphora resolution component.
We use the open-source anaphora resolver BART to
generate this information. BART (Broscheit et al,
2010) is an anaphora resolution toolkit consisting of
a markable detection and feature extraction pipeline
based on a variety of standard natural language pro-
cessing (NLP) tools and a machine learning com-
ponent to predict coreference links including both
pronominal anaphora and noun-noun coreference. In
our experiments, we always use BART?s markable
detection and feature extraction machinery. Mark-
able detection is based on the identification of noun
phrases in constituency parses generated with the
Stanford parser (Klein and Manning, 2003). The set
of features extracted by BART is an extension of the
widely used mention-pair anaphora resolution feature
set by Soon et al (2001) (see below, Section 6).
In the experiments of the next two sections, we
also use BART to predict anaphoric links for pro-
nouns. The model used with BART is a maximum
entropy ranker trained on the ACE02-npaper corpus
(LDC2003T11). In order to obtain a probability dis-
tribution over antecedent candidates rather than one-
best predictions or coreference sets, we modified the
ranking component with which BART resolves pro-
nouns to normalise and output the scores assigned
by the ranker to all candidates instead of picking the
highest-scoring candidate.
4 Baseline Classifiers
In order to create a simple, but reasonable baseline
for our task, we trained a maximum entropy (ME)
382
TED
(Accuracy: 0.685)
P R F
ce 0.593 0.728 0.654
elle 0.798 0.523 0.632
elles 0.812 0.164 0.273
il 0.764 0.550 0.639
ils 0.632 0.949 0.759
OTHER 0.724 0.692 0.708
News commentary
(Accuracy: 0.576)
P R F
ce 0.508 0.294 0.373
elle 0.530 0.312 0.393
elles 0.538 0.062 0.111
il 0.600 0.666 0.631
ils 0.593 0.769 0.670
OTHER 0.564 0.609 0.586
Table 2: Maximum entropy classifier results
TED
(Accuracy: 0.700)
P R F
ce 0.634 0.747 0.686
elle 0.756 0.617 0.679
elles 0.679 0.319 0.434
il 0.719 0.591 0.649
ils 0.663 0.940 0.778
OTHER 0.743 0.678 0.709
News commentary
(Accuracy: 0.576)
P R F
ce 0.477 0.344 0.400
elle 0.498 0.401 0.444
elles 0.565 0.116 0.193
il 0.655 0.626 0.640
ils 0.570 0.834 0.677
OTHER 0.567 0.573 0.570
Table 3: Neural network classifier with anaphoras resolved by BART
classifier with the MegaM software package2 using
the features described in the previous section and the
anaphora links found by BART. Results are shown
in Table 2. The baseline results show an overall
higher accuracy for the TED data than for the news-
commentary data. While the precision is above 50 %
in all categories and considerably higher in some,
recall varies widely.
The pronoun elles is particularly interesting. This
is the feminine plural of the personal pronoun, and
it usually corresponds to the English pronoun they,
which is not marked for gender. In French, elles is a
marked choice which is only used if the antecedent
exclusively refers to females or feminine-gendered
objects. The presence of a single item with mascu-
line grammatical gender in the antecedent will trigger
the use of the masculine plural pronoun ils instead.
This distinction cannot be predicted from the English
source pronoun or its context; making correct pre-
dictions requires knowledge about the antecedent of
the pronoun. Moreover, elles is a low-frequency pro-
noun. There are only 1,909 occurrences of this pro-
2http://www.umiacs.umd.edu/~hal/megam/ (20 June
2013).
noun in the TED training data, and 1,077 in the news-
commentary training set. Because of these special
properties of the feminine plural class, we argue that
the performance of a classifier on elles is a good indi-
cator of how well it can represent relevant knowledge
about pronominal anaphora as opposed to overfitting
to source contexts or acting on prior assumptions
about class frequencies.
In accordance with the general linguistic prefer-
ence for ils, the classifier tends to predict ils much
more often than elles when encountering an English
plural pronoun. This is reflected in the fact that elles
has much lower recall than ils. Clearly, the classifier
achieves a good part of its accuracy by making ma-
jority choices without exploiting deeper knowledge
about the antecedents of pronouns.
An additional experiment with a subset of 27,900
training examples from the TED data confirms that
the difference between TED and news commentaries
is not just an effect of training data size, but that TED
data is genuinely easier to predict than news com-
mentaries. In the reduced data TED condition, the
classifier achieves an accuracy of 0.673. Precision
and recall of all classifiers are much closer to the
383
E
P
R1
L1
R2
L2
R3
L3
p3p2p1
321
H
S
A
Figure 3: Neural network for pronoun prediction
large-data TED condition than to the news commen-
tary experiments, except for elles, where we obtain
an F-score of 0.072 (P 0.818, R 0.038), indicating
that small training data size is a serious problem for
this low-frequency class.
5 Neural Network Classifier
In the previous section, we saw that a simple multi-
class maximum entropy classifier, while making cor-
rect predictions for much of the data set, has a signifi-
cant bias towards making majority class decisions, re-
lying more on prior assumptions about the frequency
distribution of the classes than on antecedent features
when handling examples of less frequent classes. In
order to create a system that can be trained to rely
more explicitly on antecedent information, we cre-
ated a neural network classifier for our task. The intro-
duction of a hidden layer should enable the classifier
to learn abstract concepts such as gender and number
that are useful across multiple output categories, so
that the performance of sparsely represented classes
can benefit from the training examples of the more
frequent classes.
The overall structure of the network is shown in
Figure 3. As inputs, the network takes the same fea-
tures that were available to the baseline ME classifier,
based on the source pronoun (P) with three words
of context to its left (L1 to L3) and three words to
its right (R1 to R3) as well as the words aligned to
the syntactic head words of all possible antecedent
candidates as found by BART (A). All words are
encoded as one-hot vectors whose dimensionality is
equal to the vocabulary size. If multiple words are
aligned to the syntactic head of an antecedent candi-
date, their word vectors are averaged with uniform
weights. The resulting vectors for each antecedent
are then averaged with weights defined by the pos-
terior distribution of the anaphora resolver in BART
(p1 to p3).
The network has two hidden layers. The first layer
(E) maps the input word vectors to a low-dimensional
representation. In this layer, the embedding weights
for all the source language vectors (the pronoun
and its 6 context words) are tied, so if two words
are the same, they are mapped to the same lower-
dimensional embedding irrespective of their position
relative to the pronoun. The embedding of the an-
tecedent word vectors is independent, as these word
vectors represent target language words. The entire
embedding layer is then mapped to another hidden
layer (H), which is in turn connected to a softmax out-
put layer (S) with 6 outputs representing the classes
ce, elle, elles, il, ils and OTHER. The non-linearity of
both hidden layers is the logistic sigmoid function,
f (x) = 1/(1+ e?x).
In all experiments reported in this paper, the dimen-
sionality of the source and target language word em-
beddings is 20, resulting in a total embedding layer
size of 160, and the size of the last hidden layer is
equal to 50. These sizes are fairly small. In experi-
ments with larger layer sizes, we were able to obtain
similar, but no better results.
384
The neural network is trained with mini-batch
stochastic gradient descent with backpropagated gra-
dients using the RMSPROP algorithm with cross-
entropy as the objective function.3 In contrast to
standard gradient descent, RMSPROP normalises the
magnitude of the gradient components by dividing
them by a root-mean-square moving average. We
found this led to faster convergence. Other features
of our training algorithm include the use of momen-
tum to even out gradient oscillations, adaptive learn-
ing rates for each weight as well as adaptation of
the global learning rate as a function of current train-
ing progress. The network is regularised with an `2
weight penalty. Good settings of the initial learning
rate and the weight cost parameter (both around 0.001
in most experiments) were found by manual experi-
mentation. Generally, we train our networks for 300
epochs, compute the validation error on a held-out
set of some 10 % of the training data after each epoch
and use the model that achieved the lowest validation
error for testing.
Since the source context features are very infor-
mative and it is comparatively more difficult to learn
from the antecedents, the network sometimes had a
tendency to overfit to the source features and disre-
gard antecedent information. We found that this prob-
lem can be solved effectively by presenting a part of
the training without any source features, forcing the
network to learn from the information contained in
the antecedents. In all experiments in this paper, we
zero out all source features (input layers P, L1 to L3
and R1 to R3) with a probability of 50 % in each
training example. At test time, no information is ze-
roed out.
Classification results with this network are shown
in Table 3. We note that the accuracy has increased
slightly for the TED test set and remains exactly the
same for the news commentary corpus. However, a
closer look on the results for individual classes re-
veals that the neural network makes better predictions
for almost all classes. In terms of F-score, the only
class that becomes slightly worse is the OTHER class
for the news commentary corpus because of lower
recall, indicating that the neural network classifier is
less biased towards using the uninformative OTHER
3Our training procedure is greatly inspired by a series of on-
line lectures held by Geoffrey Hinton in 2012 (https://www.
coursera.org/course/neuralnets, 10 September 2013).
category. Recall for elle and elles increases consider-
ably, but especially for elles it is still quite low. The
increase in recall comes with some loss in precision,
but the net effect on F-score is clearly positive.
6 Latent Anaphora Resolution
Considering Figure 1 again, we note that the bilin-
gual setting of our classification task adds some in-
formation not available to the monolingual anaphora
resolver that can be helpful when determining the
correct antecedent for a given pronoun. Knowing the
gender of the translation of a pronoun limits the set
of possible antecedents to those whose translation is
morphologically compatible with the target language
pronoun. We can exploit this fact to learn how to
resolve anaphoric pronouns without requiring data
with manually annotated anaphoric links.
To achieve this, we extend our neural network with
a component to predict the probability of each an-
tecedent candidate to be the correct antecedent (Fig-
ure 4). The extended network is identical to the previ-
ous version except for the upper left part dealing with
anaphoric link features. The only difference between
the two networks is the fact that anaphora resolution
is now performed by a part of our neural network
itself instead of being done by an external module
and provided to the classifier as an input.
In this setup, we still use some parts of the BART
toolkit to extract markables and compute features.
However, we do not make use of the machine learn-
ing component in BART that makes the actual pre-
dictions. Since this is the only component trained on
coreference-annotated data in a typical BART con-
figuration, no coreference annotations are used any-
where in our system even though we continue to rely
on the external anaphora resolver for preprocessing
to avoid implementing our own markable and feature
extractors and to make comparison easier.
For each candidate markable identified by BART?s
preprocessing pipeline, the anaphora resolution
model receives as input a link feature vector (T) de-
scribing relevant aspects of the antecedent candidate-
anaphora pair. This feature vector is generated by the
feature extraction machinery in BART and includes
a standard feature set for coreference resolution par-
tially based on work by Soon et al (2001). We use
the following feature extractors in BART, each of
385
E
H
S
1
2
3
L3
R3
L2
R2
L1
R1
P
T
U
V
1 2 3
A
Figure 4: Neural network with latent anaphora resolution
which can generate multiple features:
? Anaphora mention type
? Gender match
? Number match
? String match
? Alias feature (Soon et al, 2001)
? Appositive position feature (Soon et al, 2001)
? Semantic class (Soon et al, 2001)
? Semantic class match
? Binary distance feature
? Antecedent is first mention in sentence
Our baseline set of features was borrowed whole-
sale from a working coreference system and includes
some features that are not relevant to the task at hand,
e. g., features indicating that the anaphora is a pro-
noun, is not a named entity, etc. After removing all
features that assume constant values in the training
set when resolving antecedents for the set of pro-
nouns we consider, we are left with a basic set of 37
anaphoric link features that are fed as inputs to our
network. These features are exactly the same as those
available to the anaphora resolution classifier in the
BART system used in the previous section.
Each training example for our network can have
an arbitrary number of antecedent candidates, each of
which is described by an antecedent word vector (A)
and by an anaphoric link vector (T). The anaphoric
link features are first mapped to a regular hidden layer
with logistic sigmoid units (U). The activations of the
hidden units are then mapped to a single value, which
functions as an element in a softmax layer over all an-
tecedent candidates (V). This softmax layer assigns
a probability to each antecedent candidate, which we
then use to compute a weighted average over the an-
tecedent word vector, replacing the probabilities pi
in Figures 2 and 3.
At training time, the network?s anaphora resolu-
tion component is trained in exactly the same way as
the rest of the network. The error signal from the em-
bedding layer is backpropagated both to the weight
matrix defining the antecedent word embedding and
to the anaphora resolution subnetwork. Note that the
number of weights in the network is the same for
all training examples even though the number of an-
tecedent candidates varies because all weights related
to antecedent word features and anaphoric link fea-
tures are shared between all antecedent candidates.
One slightly uncommon feature of our neural net-
work is that it contains an internal softmax layer to
generate normalised probabilities over all possible
antecedent candidates. Moreover, weights are shared
between all antecedent candidates, so the inputs of
our internal softmax layer share dependencies on
the same weight variables. When computing deriva-
tives with backpropagation, these shared dependen-
cies must be taken into account. In particular, the
outputs yi of the antecedent resolution layer are the re-
sult of a softmax applied to functions of some shared
variables q:
yi =
exp fi(q)
?k exp fk(q)
(1)
386
The derivatives of any yi with respect to q, which
can be any of the weights in the anaphora resolution
subnetwork, have dependencies on the derivatives of
the other softmax inputs with respect to q:
?yi
?q = yi
(
? fi(q)
?q ??k
yk
? fk(q)
?q
)
(2)
This makes the implementation of backpropagation
for this part of the network somewhat more compli-
cated, but in the case of our networks, it has no major
impact on training time.
Experimental results for this network are shown
in Table 4. Compared with Table 3, we note that the
overall accuracy is only very slightly lower for TED,
and for the news commentaries it is actually better.
When it comes to F-scores, the performance for elles
improves by a small amount, while the effect on the
other classes is a bit more mixed. Even where it gets
worse, the differences are not dramatic considering
that we eliminated a very knowledge-rich resource
from the training process. This demonstrates that it
is possible, in our classification task, to obtain good
results without using any data manually annotated for
anaphora and to rely entirely on unsupervised latent
anaphora resolution.
7 Further Improvements
The results presented in the preceding section repre-
sent a clear improvement over the ME classifiers in
Table 2, even though the overall accuracy increased
only slightly. Not only does our neural network clas-
sifier achieve better results on the classification task
at hand without requiring an anaphora resolution clas-
sifier trained on manually annotated data, but it per-
forms clearly better for the feminine categories that
reflect minority choices requiring knowledge about
the antecedents. Nevertheless, the performance is still
not entirely satisfactory.
By subjecting the output of our classifier on a de-
velopment set to a manual error analysis, we found
that a fairly large number of errors belong to two error
types: On the one hand, the preprocessing pipeline
used to identify antecedent candidates does not al-
ways include the correct antecedent in the set pre-
sented to the neural network. Whenever this occurs,
it is obvious that the classifier cannot possibly find
the correct antecedent. Out of 76 examples of the cat-
egory elles that had been mistakenly predicted as ils,
we found that 43 suffered from this problem. In other
classes, the problem seems to be somewhat less com-
mon, but it still exists. On the other hand, in many
cases (23 out of 76 for the category mentioned be-
fore) the anaphora resolution subnetwork does iden-
tify an antecedent manually recognised to belong to
the right gender/number group, but still predicts an in-
correct pronoun. This may indicate that the network
has difficulties learning a correct gender/number rep-
resentation for all words in the vocabulary.
7.1 Relaxing Markable Extraction
The pipeline we use to extract potential antecedent
candidates is borrowed from the BART anaphora
resolution toolkit. BART uses a syntactic parser to
identify noun phrases as markables. When extract-
ing antecedent candidates for coreference prediction,
it starts by considering a window consisting of the
sentence in which the anaphoric pronoun is located
and the two immediately preceding sentences. Mark-
ables in this window are checked for morphological
compatibility in terms of gender and number with the
anaphoric pronoun, and only compatible markables
are extracted as antecedent candidates. If no compat-
ible markables are found in the initial window, the
window is successively enlarged one sentence at a
time until at least one suitable markable is found.
Our error analysis shows that this procedure
misses some relevant markables both because the ini-
tial two-sentence extraction window is too small and
because the morphological compatibility check incor-
rectly filters away some markables that should have
been considered as candidates. By contrast, the ex-
traction procedure does extract quite a number of first
and second person noun phrases (I, we, you and their
oblique forms) in the TED talks which are extremely
unlikely to be the antecedent of a later occurrence of
he, she, it or they. As a first step, we therefore adjust
the extraction criteria to our task by increasing the
initial extraction window to five sentences, exclud-
ing first and second person markables and removing
the morphological compatibility requirement. The
compatibility check is still used to control expansion
of the extraction window, but it is no longer applied
to filter the extracted markables. This increases the
accuracy to 0.701 for TED and 0.602 for the news
387
TED
(Accuracy: 0.696)
P R F
ce 0.618 0.722 0.666
elle 0.754 0.548 0.635
elles 0.737 0.340 0.465
il 0.718 0.629 0.670
ils 0.652 0.916 0.761
OTHER 0.741 0.682 0.711
News commentary
(Accuracy: 0.597)
P R F
ce 0.419 0.368 0.392
elle 0.547 0.460 0.500
elles 0.539 0.135 0.215
il 0.623 0.719 0.667
ils 0.596 0.783 0.677
OTHER 0.614 0.544 0.577
Table 4: Neural network classifier with latent anaphora resolution
TED
(Accuracy: 0.713)
P R F
ce 0.611 0.723 0.662
elle 0.749 0.596 0.664
elles 0.602 0.616 0.609
il 0.733 0.638 0.682
ils 0.710 0.884 0.788
OTHER 0.760 0.704 0.731
News commentary
(Accuracy: 0.626)
P R F
ce 0.492 0.324 0.391
elle 0.526 0.439 0.478
elles 0.547 0.558 0.552
il 0.599 0.757 0.669
ils 0.671 0.878 0.761
OTHER 0.681 0.526 0.594
Table 5: Final classifier results
commentaries, while the performance for elles im-
proves to F-scores of 0.531 (TED; P 0.690, R 0.432)
and 0.304 (News commentaries; P 0.444, R 0.231),
respectively. Note that these and all the following re-
sults are not directly comparable to the ME baseline
results in Table 2, since they include modifications
and improvements to the training data extraction pro-
cedure that might possibly lead to benefits in the ME
setting as well.
7.2 Adding Lexicon Knowledge
In order to make it easier for the classifier to iden-
tify the gender and number properties of infrequent
words, we extend the word vectors with features indi-
cating possible morphological features for each word.
In early experiments with ME classifiers, we found
that our attempts to do proper gender and number
tagging in French text did not improve classification
performance noticeably, presumably because the an-
notation was too noisy. In more recent experiments,
we just add features indicating all possible morpho-
logical interpretations of each word, rather than try-
ing to disambiguate them. To do this, we look up
the morphological annotations of the French words
in the Lefff dictionary (Sagot et al, 2006) and intro-
duce a set of new binary features to indicate whether
a particular reading of a word occurs in that dictio-
nary. These features are then added to the one-hot
representation of the antecedent words. Doing so im-
proves the classifier accuracy to 0.711 (TED) and
0.604 (News commentaries), while the F-scores for
elles reach 0.589 (TED; P 0.649, R 0.539) and 0.500
(News commentaries; P 0.545, R 0.462), respectively.
7.3 More Anaphoric Link Features
Even though the modified antecedent candidate ex-
traction with its larger context window and without
the morphological filter results in better performance
on both test sets, additional error analysis reveals
that the classifiers has greater problems identifying
the correct markable in this setting. One reason for
this may be that the baseline anaphoric link feature
set described above (Section 6) only includes two
very rough binary distance features which indicate
whether or not the anaphora and the antecedent can-
didate occur in the same or in immediately adjacent
sentences. With the larger context window, this may
be too unspecific. In our final experiment, we there-
fore enable some additional features which are avail-
able in BART, but disabled in the baseline system:
388
? Distance in number of markables
? Distance in number of sentences
? Sentence distance, log-transformed
? Distance in number of words
? Part of speech of head word
Most of these encode the distance between the
anaphora and the antecedent candidate in more pre-
cise ways. Complete results for this final system are
presented in Table 5.
Including these additional features leads to another
slight increase in accuracy for both corpora, with sim-
ilar or increased classifier F-scores for most classes
except elle in the news commentary condition. In par-
ticular, we should like to point out the performance
of our benchmark classifier for elles, which suffered
from extremely low recall in the first classifiers and
approaches the performance of the other classes, with
nearly balanced precision and recall, in this final sys-
tem. Since elles is a low-frequency class and cannot
be reliably predicted using source context alone, we
interpret this as evidence that our final neural network
classifier has incorporated some relevant knowledge
about pronominal anaphora that the baseline ME clas-
sifier and earlier versions of our network have no ac-
cess to. This is particularly remarkable because no
data manually annotated for coreference was used for
training.
8 Related work
Even though it was recognised years ago that the
information contained in parallel corpora may pro-
vide valuable information for the improvement of
anaphora resolution systems, there have not been
many attempts to cash in on this insight. Mitkov and
Barbu (2003) exploit parallel data in English and
French to improve pronominal anaphora resolution
by combining anaphora resolvers for the individual
languages with handwritten rules to resolve conflicts
between the output of the language-specific resolvers.
Veselovsk? et al (2012) apply a similar strategy to
English-Czech data to resolve different uses of the
pronoun it. Other work has used word alignments to
project coreference annotations from one language
to another with a view to training anaphora resolvers
in the target language (Postolache et al, 2006; de
Souza and Ora?san, 2011). Rahman and Ng (2012)
instead use machine translation to translate their test
data into a language for which they have an anaphora
resolver and then project the annotations back to the
original language. Completely unsupervised mono-
lingual anaphora resolution has been approached us-
ing, e. g., Markov logic (Poon and Domingos, 2008)
and the Expectation-Maximisation algorithm (Cherry
and Bergsma, 2005; Charniak and Elsner, 2009). To
the best of our knowledge, the direct application of
machine learning techniques to parallel data in a task
related to anaphora resolution is novel in our work.
Neural networks and deep learning techniques
have recently gained some popularity in natural lan-
guage processing. They have been applied to tasks
such as language modelling (Bengio et al, 2003;
Schwenk, 2007), translation modelling in statistical
machine translation (Le et al, 2012), but also part-of-
speech tagging, chunking, named entity recognition
and semantic role labelling (Collobert et al, 2011).
In tasks related to anaphora resolution, standard feed-
forward neural networks have been tested as a clas-
sifier in an anaphora resolution system (Stuckardt,
2007), but the network design presented in our work
is novel.
9 Conclusion
In this paper, we have introduced cross-lingual pro-
noun prediction as an independent natural language
processing task. Even though it is not an end-to-end
task, pronoun prediction is interesting for several rea-
sons. It is related to the problem of pronoun transla-
tion in SMT, a currently unsolved problem that has
been addressed in a number of recent research publi-
cations (Le Nagard and Koehn, 2010; Hardmeier and
Federico, 2010; Guillou, 2012) without reaching a
major breakthrough. In this work, we have shown that
pronoun prediction can be effectively modelled in a
neural network architecture with relatively simple
features. More importantly, we have demonstrated
that the task can be exploited to train a classifier with
a latent representation of anaphoric links. With paral-
lel text as its only supervision this classifier achieves
a level of performance that is similar to, if not bet-
ter than, that of a classifier using a regular anaphora
resolution system trained with manually annotated
data.
389
References
Yoshua Bengio, R?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Research,
3:1137?1155.
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodriguez, Lorenza Romano,
Olga Uryupina, Yannick Versley, and Roberto Zanoli.
2010. BART: A multilingual anaphora resolution sys-
tem. In Proceedings of the 5th International Work-
shop on Semantic Evaluations (SemEval-2010), Upp-
sala, Sweden, 15?16 July 2010.
Mauro Cettolo, Christian Girardi, and Marcello Federico.
2012. WIT3: Web inventory of transcribed and trans-
lated talks. In Proceedings of the 16th Conference
of the European Association for Machine Translation
(EAMT), pages 261?268, Trento, Italy.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of the
12th Conference of the European Chapter of the ACL
(EACL 2009), pages 148?156, Athens, Greece.
Colin Cherry and Shane Bergsma. 2005. An Expecta-
tion Maximization approach to pronoun resolution. In
Proceedings of the Ninth Conference on Computational
Natural Language Learning (CoNLL-2005), pages 88?
95, Ann Arbor, Michigan.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, University
of Pennsylvania.
Ronan Collobert, Jason Weston, L?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2461?2505.
Jos? de Souza and Constantin Ora?san. 2011. Can pro-
jected chains in parallel corpora help coreference reso-
lution? In Iris Hendrickx, Sobha Lalitha Devi, Ant?nio
Branco, and Ruslan Mitkov, editors, Anaphora Process-
ing and Applications, volume 7099 of Lecture Notes in
Computer Science, pages 59?69. Springer, Berlin.
Liane Guillou. 2012. Improving pronoun translation for
statistical machine translation. In Proceedings of the
Student Research Workshop at the 13th Conference of
the European Chapter of the Association for Computa-
tional Linguistics, pages 1?10, Avignon, France.
Christian Hardmeier and Marcello Federico. 2010. Mod-
elling pronominal anaphora in statistical machine trans-
lation. In Proceedings of the seventh International
Workshop on Spoken Language Translation (IWSLT),
pages 283?289, Paris, France.
Christian Hardmeier. 2012. Discourse in statistical ma-
chine translation: A survey and a case study. Discours,
11.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan.
Hai-Son Le, Alexandre Allauzen, and Fran?ois Yvon.
2012. Continuous space translation models with neural
networks. In Proceedings of the 2012 Conference of the
North American Chapter of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 39?48, Montr?al, Canada.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding pro-
noun translation with co-reference resolution. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 252?261,
Uppsala, Sweden.
Ruslan Mitkov and Catalina Barbu. 2003. Using bilingual
corpora to improve pronoun resolution. Languages in
Contrast, 4(2):201?211.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment models.
Computational linguistics, 29:19?51.
Hoifung Poon and Pedro Domingos. 2008. Joint un-
supervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, pages 650?
659, Honolulu, Hawaii.
Oana Postolache, Dan Cristea, and Constantin Ora?san.
2006. Transferring coreference chains through word
alignment. In Proceedings of the 5th Conference
on International Language Resources and Evaluation
(LREC-2006), pages 889?892, Genoa.
Altaf Rahman and Vincent Ng. 2012. Translation-based
projection for multilingual coreference resolution. In
Proceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics: Human Language Technologies, pages 720?
730, Montr?al, Canada.
Beno?t Sagot, Lionel Cl?ment, ?ric Villemonte de La
Clergerie, and Pierre Boullier. 2006. The Lefff 2
syntactic lexicon for French: architecture, acquisition,
use. In Proceedings of the 5th Conference on Inter-
national Language Resources and Evaluation (LREC-
2006), pages 1348?1351, Genoa.
Holger Schwenk. 2007. Continuous space language mod-
els. Computer Speech and Language, 21(3):492?518.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational lin-
guistics, 27(4):521?544.
Roland Stuckardt. 2007. Applying backpropagation net-
works to anaphor resolution. In Ant?nio Branco, editor,
Anaphora: Analysis, Algorithms and Applications. 6th
390
Discourse Anaphora and Anaphor Resolution Collo-
quium, DAARC 2007, number 4410 in Lecture Notes
in Artificial Intelligence, pages 107?124, Berlin.
Kater?ina Veselovsk?, Ngu.y Giang Linh, and Michal
Nov?k. 2012. Using Czech-English parallel corpora in
automatic identification of it. In Proceedings of the 5th
Workshop on Building and Using Comparable Corpora,
pages 112?120, Istanbul, Turkey.
391
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 141?151,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Character-Based Pivot Translation for Under-Resourced Languages and
Domains
Jo?rg Tiedemann
Department of Linguistics and Philology
Uppsala University, Uppsala/Sweden
jorg.tiedemann@lingfil.uu.se
Abstract
In this paper we investigate the use of
character-level translation models to sup-
port the translation from and to under-
resourced languages and textual domains
via closely related pivot languages. Our ex-
periments show that these low-level models
can be successful even with tiny amounts
of training data. We test the approach on
movie subtitles for three language pairs and
legal texts for another language pair in a do-
main adaptation task. Our pivot translations
outperform the baselines by a large margin.
1 Introduction
Data-driven approaches have been extremely suc-
cessful in most areas of natural language pro-
cessing (NLP) and can be considered the main
paradigm in application-oriented research and de-
velopment. Research in machine translation is a
typical example with the dominance of statisti-
cal models over the last decade. This is even en-
forced due to the availability of toolboxes such as
Moses (Koehn et al 2007) which make it pos-
sible to build translation engines within days or
even hours for any language pair provided that ap-
propriate training data is available. However, this
reliance on training data is also the most severe
limitation of statistical approaches. Resources in
large quantities are only available for a few lan-
guages and domains. In the case of SMT, the
dilemma is even more apparent as parallel cor-
pora are rare and usually quite sparse. Some lan-
guages can be considered lucky, for example, be-
cause of political situations that lead to the pro-
duction of freely available translated material on
a large scale. A lot of research and development
would not have been possible without the Euro-
pean Union and its language policies to give an
example.
One of the main challenges of current NLP re-
search is to port data-driven techniques to under-
resourced languages, which refers to the major-
ity of the world?s languages. One obvious ap-
proach is to create appropriate data resources even
for those languages in order to enable the use of
similar techniques designed for high-density lan-
guages. However, this is usually too expensive
and often impossible with the quantities needed.
Another idea is to develop new models that can
work with (much) less data but still make use
of resources and techniques developed for other
well-resourced languages.
In this paper, we explore pivot translation tech-
niques for the translation from and to resource-
poor languages with the help of intermediate
resource-rich languages. We explore the fact
that many poorly resourced languages are closely
related to well equipped languages, which en-
ables low-level techniques such as character-
based translation. We can show that these tech-
niques can boost the performance enormously,
tested for several language pairs. Furthermore, we
show that pivoting can also be used to overcome
data sparseness in specific domains. Even high
density languages are under-resourced in most
textual domains and pivoting via in-domain data
of another language can help to adapt statistical
models. In our experiments, we observe that re-
lated languages have the largest impact in such a
setup.
The remaining parts of the paper are organized
as follows: First we describe the pivot translation
approach used in this study. Thereafter, we dis-
141
cuss character-based translation models followed
by a detailed presentation of our experimental
results. Finally, we briefly summarize related
work and conclude the paper with discussions and
prospects for future work.
2 Pivot Models
Information from pivot languages can be incorpo-
rated in SMT models in various ways. The main
principle refers to the combination of source-
to-pivot and pivot-to-target translation models.
In our setup, one of these models includes a
resource-poor language (source or target) and the
other one refers to a standard model with ap-
propriate data resources. A condition is that we
have at least some training data for the translation
between pivot and the resource-poor language.
However, for the original task (source-to-target
translation) we do not require any data resources
except for purposes of comparison.
We will explore various models for the transla-
tion between the resource-poor language and the
pivot language and most of them are not compat-
ible with standard phrase-based translation mod-
els. Hence, triangulation methods (Cohn and La-
pata, 2007) for combining phrase tables are not
applicable in our case. Instead, we explore a
cascaded approach (also called ?transfer method?
(Wu and Wang, 2009)) in which we translate the
input text in two steps using a linear interpo-
lation for rescoring N-best lists. Following the
method described in (Utiyama and Isahara, 2007)
and (Wu and Wang, 2009), we use the best n hy-
potheses from the translation of source sentences
s to pivot sentences p and combine them with the
top m hypotheses for translating these pivot sen-
tences to target sentences t:
t? ? argmax
t
L?
k=1
??sp
k
hsp
k
(s, p) + (1? ?)?pt
k
hpt
k
(p, t)
where hxyk are feature functions for model xy
with appropriate weights ?xyk .
1 Basically, this
means that we simply add the scores and, sim-
ilar to related work, we assume that the feature
weights can be set independently for each model
using minimum error rate training (MERT) (Och,
1Note, that we do not require the same feature functions
in both models even though the formula above implies this
for simplicity of representation.
2003). In our setup we added the parameter ?
that can be used to weight the importance of one
model over the other. This can be useful as we
do not consider the entire hypothesis space but
only a small subset of N-best lists. In the sim-
plest case, this weight is set to 0.5 making both
models equally important. An alternative to fit-
ting the interpolation weight would be to per-
form a global optimization procedure. However,
a straightforward implementation of pivot-based
MERT would be prohibitively slow due to the
expensive two-step translation procedure over n-
best lists.
A general condition for the pivot approach is to
assume independent training sets for both transla-
tion models as already pointed out by (Bertoldi
et al 2008). In contrast to research presented
in related work (see, for example, (Koehn et al
2009)) this condition is met in our setup in which
all data sets represent different samples over the
languages considered (see section 4).2
3 Character-Based SMT
The basic idea behind character-based translation
models is to take advantage of the strong lexi-
cal and syntactic similarities between closely re-
lated languages. Consider, for example, Figure
1. Related languages like Catalan and Spanish or
Danish and Norwegian have common roots and,
therefore, use similar concepts and express them
in similar grammatical structures. Spelling con-
ventions can still be quite different but those dif-
ferences are often very consistent. The Bosnian-
Macedonian example also shows that we do not
have to require any alphabetic overlap in order to
obtain character-level similarities.
Regularities between such closely related lan-
guages can be captured below the word level. We
can also assume a more or less monotonic rela-
tion between the two languages which motivates
the idea of translation models over character N-
grams treating translation as a transliteration task
(Vilar et al 2007). Conceptually it is straightfor-
ward to think of phrase-based models on the char-
acter level. Sequences of characters can be used
instead of word N-grams for both, translation and
language models. Training can proceed with the
same tools and approaches. The basic task is to
2Note that different samples may still include common
sentences.
142
Figure 1: Some examples of movie subtitle transla-
tions between closely related languages (either sharing
parts of the same alphabet or not).
prepare the data to comply with the training pro-
cedures (see Figure 2).
Figure 2: Data pre-processing for training models on
the character level. Spaces are represented by ? ? and
each sentence is treated as one sequence of characters.
3.1 Character Alignment
One crucial difference is the alignment of charac-
ters, which is required instead of an alignment of
words. Clearly, the traditional IBM word align-
ment models are not designed for this task es-
pecially with respect to distortion. However, the
same generative story can still be applied in gen-
eral. Vilar et al(2007) explore a two-step proce-
dure where words are aligned first (with the tradi-
tional IBM models) to divide sentence pairs into
aligned segments of reasonable size and the char-
acters are then aligned with the same algorithm.
An alternative is to use models designed for
transliteration or related character-level transfor-
mation tasks. Many approaches are based on
transducer models that resemble string edit oper-
ations such as insertions, deletions and substitu-
tions (Ristad and Yianilos, 1998). Weighted fi-
nite state transducers (WFST?s) can be trained on
unaligned pairs of character sequences and have
been shown to be very effective for transliteration
tasks or letter-to-phoneme conversions (Jiampoja-
marn et al 2007). The training procedure usually
employs an expectation maximization (EM) pro-
cedure and the resulting transducer can be used to
find the Viterbi alignment between characters ac-
cording to the best sequence of edit operations ap-
plied to transform one string into the other. Exten-
sions to this model are possible, for example the
use of many-to-many alignments which have been
shown to be very effective in letter-to-phoneme
alignment tasks (Jiampojamarn et al 2007).
One advantage of the edit-distance-based trans-
ducer models is that the alignments they pre-
dict are strictly monotonic and cannot easily be
confused by spurious relations between charac-
ters over longer distances. Long distance align-
ments are only possible in connection with a se-
ries of insertions and deletions that usually in-
crease the alignment costs in such a way that they
are avoided if possible. On the other hand, IBM
word alignment models also prefer monotonic
alignments over non-monotonic ones if there is no
good reason to do otherwise (i.e., there is frequent
evidence of distorted alignments). However, the
size of the vocabulary in a character-level model
is very small (several orders of magnitude smaller
than on the word level) and this may cause serious
confusion of the word alignment model that very
much relies on context-independent lexical trans-
lation probabilities. Hence, for character align-
ment, the lexical evidence is much less reliable
without their context.
It is certainly possible to find a compromise be-
tween word-level and character-level models in
order to generalize below word boundaries but
avoiding alignment problems as discussed above.
Morpheme-based translation models have been
explored in several studies with similar motiva-
tions as in our approach, a better generalization
from sparse training data (Fishel and Kirik, 2010;
Luong et al 2010). However, these approaches
have the drawback that they require proper mor-
phological analyses. Data-driven techniques ex-
ist even for morphology, but their use in SMT
still needs to be shown (Fishel, 2009). The sit-
uation is comparable to the problems of integrat-
ing linguistically motivated phrases into phrase-
based SMT (Koehn et al 2003). Instead we opt
for a more general approach to extend context to
facilitate, especially, the alignment step. Figure 3
shows how we can transform texts into sequences
of bigrams that can be aligned with standard ap-
proaches without making any assumptions about
linguistically motivated segmentations.
143
cu ur rs so o c co on nf fi ir rm ma ad do o . .
? q qu ue? e? e es s e es so o ? ?
Figure 3: Two Spanish sentences as sequences of char-
acter bigrams with a final ? ? marking the end of a sen-
tence.
In this way we can construct a parallel corpus with
slightly richer contextual information as input to
the alignment program. The vocabulary remains
small (for example, 1267 bigrams in the case of
Spanish compared to 84 individual characters in
our experiments) but lexical translation probabili-
ties become now much more differentiated.
With this, it is now possible to use the align-
ment between bigrams to train a character-level
translation system as we have the same number of
bigrams as we have characters (and the first char-
acter in each bigram corresponds to the charac-
ter at that position). Certainly, it is also possible
to train a bigram translation model (and language
model). This has the (one and only) advantage
that one character of context across phrase bound-
aries (i.e. character N-grams) is used in the se-
lection of translation alternatives from the phrase
table.3
3.2 Tuning Character-Level Models
A final remark on training character-based SMT
models is concerned with feature weight tun-
ing. It certainly makes not much sense to com-
pute character-level BLEU scores for tuning fea-
ture weights especially with the standard settings
of matching relatively short N-grams. Instead
we would still like to measure performance in
terms of word-level BLEU scores (or any other
MT evaluation metric used in minimum error
rate training). Therefore, it is important to post-
process character-translated development sets be-
fore adjusting weights. This is simply done
by merging characters accordingly and replacing
the place-holders with spaces again. Thereafter,
MERT can run as usual.
3.3 Evaluation
Character-level translations can be evaluated in
the same way as other translation hypotheses,
for example using automatic measures such as
3Using larger units (trigrams, for example) led to lower
scores in our experiments (probably due to data sparseness)
and, therefore, are not reported here.
BLEU, NIST, METEOR etc. The same simple
post-processing as mentioned in the previous sec-
tion can be applied to turn the character transla-
tions into ?normal? text. However, it can be use-
ful to look at some other measures as well that
consider near matches on the character level in-
stead of matching words and word N-grams only.
Character-level models have the ability to produce
strings that may be close to the reference and still
do not match any of the words contained. They
may generate non-words that include mistakes
which look like spelling-errors or minor gram-
matical mistakes. Those words are usually close
enough to the correct target words to be recog-
nized by the user, which is often more acceptable
than leaving foreign words untranslated. This is
especially true as many unknown words represent
important content words that bear a lot of infor-
mation. The problem of unknown words is even
more severe for morphologically rich language as
many word forms are simply not part of (sparse)
training data sets. Untranslated words are espe-
cially annoying when translating languages that
use different writing systems. Consider, for ex-
ample, the following subtitles in Macedonian (us-
ing Cyrillic letters) that have been translated from
Bosnian (written in Latin characters):
reference: ? ???? ????, ???? ? ???????.
word-based: ? c?as?u vina, ???? ???????.
char-based: ? ???? ????, ???? ???????.
reference: ?? ??????? ??????????.
word-based: ?? starom svetili?stu.
char-based: ?? ???? ????????????.
The underlined parts mark examples of character-
level differences with respect to the reference
translation. For the pivot translation approach, it
is important that the translations generated in the
first step can be handled by the second one. This
means, that words generated by a character-based
model should at least be valid input words for the
second step, even though they might refer to er-
roneous inflections in that context. Therefore, we
add another measure to our experimental results
presented below ? the number of unknown words
with respect to the input language of the second
step. This applies only to models that are used
as the first step in pivot-based translations. For
other models, we include a string similarity mea-
sure based on the longest common subsequence
ratio (LCSR) (Stephen, 1992) in order to give an
impression about the ?closeness? of the system
144
output to the reference translations.
4 Experiments
We conducted a series of experiments to test
the ideas of (character-level) pivot translation for
resource-poor languages. We chose to use data
from a collection of translated subtitles com-
piled in the freely available OPUS corpus (Tiede-
mann, 2009b). This collection includes a large
variety of languages and contains mainly short
sentences and sentence fragments, which suits
character-level alignment very well. The selected
settings represent translation tasks between lan-
guages (and domains) for which only very limited
training data is available or none at all.
Below we present results from two general
tasks:4 (i) Translating between English and a
resource-poor language (in both directions) via
a pivot language that is close related to the
resource-poor language. (ii) Translating between
two languages in a domain for which no in-
domain training data is available via a pivot lan-
guage with in-domain data. We will start with
the presentation of the first task and the character-
based translation between closely related lan-
guages.
4.1 Task 1: Pivoting via Related Languages
We decided to look at resource-poor languages
from two language families: Macedonian repre-
senting a Slavic language from the Balkan re-
gion, Catalan and Galician representing two Ro-
mance languages spoken mainly in Spain. There
is only little or no data available for translating
from or to English for these languages. However,
there are related languages with medium or large
amounts of training data. For Macedonian, we
use Bulgarian (which also uses a Cyrillic alpha-
bet) and Bosnian (another related language that
mainly uses Latin characters) as the pivot lan-
guage. For Catalan and Galician, the obvious
choice was Spanish (however, Portuguese would,
for example, have been another reasonable op-
tion for Galician). Table 1 lists the data avail-
able for training the various models. Furthermore,
we reserved 2000 sentences for tuning parameters
4In all experiments we use standard tools like Moses,
Giza++, SRILM, mteval etc. Details about basic settings are
omitted here due to space constraints but can be found in
the supplementary material. The data sets are available from
here: http://stp.lingfil.uu.se/?joerg/index.php?resources
and another 2000 sentences for testing. For Gali-
cian, we only used 1000 sentences for each set
due to the lack of additional data. We were espe-
cially careful when preparing the data to exclude
all sentences from tuning and test sets that could
be found in any pivot or direct translation model.
Hence, all test sentences are unseen strings for all
models presented in this paper (but they are not
comparable with each other as they are sampled
individually from independent data sets).
language pair #sent?s #words
Galician ? English ? ?
Galician ? Spanish 2k 15k
Catalan ? English 50k 400k
Catalan ? Spanish 64k 500k
Spanish ? English 30M 180M
Macedonian ? English 220k 1.2M
Macedonian ? Bosnian 12k 60k
Macedonian ? Bulgarian 155k 800k
Bosnian ? English 2.1M 11M
Bulgarian ? English 14M 80M
Table 1: Training data for the translation task between
closely related languages in the domain of movie sub-
titles. Number of sentences (#sent?s) and number of
words (#words) in thousands (k) and millions (M) (av-
erages of source and target language).
The data sets represent several interesting test
cases: Galician is the least supported language
with extremely little training data for building our
pivot model. There is no data for the direct model
and, therefore, no explicit baseline for this task.
There is 30 times more data available for Catalan-
English, but still too little for a decent standard
SMT model. Interesting here is that we have more
or less the same amount of data available for the
baseline and for the pivot translation between the
related languages. The data set for Macedonian
? English is by far the largest among the baseline
models and also bigger than the sets available for
the related pivot languages. Especially Macedo-
nian ? Bosnian is not well supported. The inter-
esting questions is whether tiny amounts of pivot
data can still be competitive. In all three cases,
there is much more data available for the trans-
lation models between English and the pivot lan-
guage.
In the following section we will look at the
translation between related languages with vari-
ous models and training setups before we con-
sider the actual translation task via the bridge lan-
guages.
145
bs-mk bg-mk es-gl es-ca
Model BLEU % ?LCSR BLEU % ?LCSR BLEU % ?LCSR BLEU % ?LCSR
word-based 15.43 0.5067 14.66 0.6225 41.11 0.7966 62.73 0.8526
char ? WFST1:1 21.37++ 0.6903 13.33?? 0.6159 36.94 0.7832 73.17++ 0.8728
char ? WFST2:2 19.17++ 0.6737 12.67?? 0.6190 43.39++ 0.8083 70.64++ 0.8684
char ? IBMchar 23.17++ 0.6968 14.57 0.6347 45.21++ 0.8171 73.12++ 0.8767
char ? IBMbigram 24.84++ 0.7046 15.01++ 0.6374 44.06++ 0.8144 74.21++ 0.8803
Table 2: Translating from a related pivot language to the target language. Bosnian (bs) / Bulgarian (bg) ?
Macedonian (mk); Galician (gl) / Catalan (ca) ? Spanish (es). Word-based refers to standard phrase-based SMT
models. All other models use phrases over character sequences. The WFST
x:y models use weighted finite state
transducers for character alignment with units that are at most x and y characters long, respectively. Other
models use Viterbi alignments created by IBM model 4 using GIZA++ (Och and Ney, 2003) between characters
(IBM
char
) or bigrams (IBM
bigram
). LCSR refers to the averaged longest common subsequence ratio between
system translations and references. Results are significantly better (p < 0.01++, p < 0.05+) or worse (p <
0.01??, p < 0.05?) than the word-based baseline.
mk-bs mk-bg gl-es ca-es
Model BLEU % ?UNK BLEU % ?UNK BLEU % ?UNK BLEU % ?UNK
word-based 14.22 17.83% 14.77 5.29% 43.22 10.18% 59.34 3.80%
char ? WFST1:1 21.74++ 1.50% 16.04++ 0.77% 50.24++ 1.17% 62.87++ 0.45%
char ? WFST2:2 19.19++ 2.05% 15.32 0.96% 50.59++ 1.28% 59.84 0.47%
char ? IBMchar 24.15++ 1.30% 17.12++ 0.80% 51.18++ 1.38% 64.35 ++ 0.59%
char ? IBMbigram 24.82++ 1.00% 17.28++ 0.77% 50.70++ 1.36% 65.14++ 0.48%
Table 3: Translating from the source language to a related pivot language. UNK gives the proportion of unknown
words with respect to the translation model from the pivot language to English.
4.1.1 Translating Related Languages
The main challenge for the translation mod-
els between related languages is the restriction to
very limited parallel training data. Character-level
models make it possible to generalize to very ba-
sic translation units leading to robust models in
the sense of models without unknown events. The
basic question is whether they provide reasonable
translations with respect to given accepted refer-
ences. Tables 2 and 3 give a comprehensive sum-
mary of various models for the languages selected
in our experiments.
We can see that at least one character-based
translation model outperforms the standard word-
based model in all cases. This is true (and not very
surprising) for the language pairs with very little
training data but it is also the case for language
pairs with slightly more reasonable data sets like
Bulgarian-Macedonian. The automatic measures
indicate decent translation performances at this
stage which encourages their use in pivot trans-
lation that we will discuss in the next section.
Furthermore, we can also see the influence of
different character alignment algorithms. Some-
what surprisingly, the best results are achieved
with IBM alignment models that are not designed
for this purpose. Transducer-based alignments
produce consistently worse translation models (at
least in terms of BLEU scores). The reason for
this might be that the IBM models can handle
noise in the training data more robustly. How-
ever, in terms of unknown words, WFST-based
alignment is very competitive and often the best
choice (but not much different from the best IBM
based models). The use of character bigrams
leads to further BLEU improvements for all data
sets except Galician-Spanish. However, this data
set is extremely small, which may cause unpre-
dictable results. In any case, the differences
between character-based alignments and bigram-
based ones are rather small and our experiments
do not lead to conclusive results.
4.1.2 Pivot Translation
In this section we now look at cascaded transla-
tions via the related pivot language. Tables 4 and
5 summarize the results for various settings.
As we can see, the pivot translations for Cata-
lan and Galician outperform the baselines by a
large margin. Here, the baselines are, of course,
very weak due to the minimal amount of train-
ing data. Furthermore, the Catalan-English test
set appears to be very easy considering the rela-
tively high BLEU scores achieved even with tiny
146
Model (BLEU in %) 1x1 10x10
English ? Catalan (baseline) 26.70
English ? (Spanish = Catalan) 8.38
English ? Spanish -word- Catalan 38.91++ 39.59++
English ? Spanish -char- Catalan 44.46++ 46.82++
Catalan ? English (baseline) 27.86
(Catalan = Spanish) ? English 9.52
Catalan -word- Spanish ? English 38.41++ 38.65++
Catalan -char- Spanish ? English 40.43++ 40.73++
English ? Galician (baseline) ?
English ? (Spanish = Galician) 7.46
English ? Spanish -word- Galician 20.55 20.76
English ? Spanish -char- Galician 21.12 21.09
Galician ? English (baseline) ?
(Galician = Spanish) ? English 5.76
Galician -word- Spanish ? English 13.16 13.20
Galician -char- Spanish ? English 16.04 16.02
Table 4: Translating between Galician/Catalan and En-
glish via Spanish using a standard phrase-based SMT
baseline, Spanish?English SMT models to translate
from/to Catalan/Galician and pivot-based approaches
using word-level models or character-level models
(based on IBM
bigram
alignments) with either one-best
(1x1) or N-best lists (10x10 with ? = 0.85).
amounts of training data for the baseline. Still, no
test sentence appears in any training or develop-
ment set for either direct translation or pivot mod-
els. From the results, we can also see that Catalan
and Galician are quite different from Spanish and
require language-specific treatment. Using a large
Spanish ? English model (with over 30% BLEU
in both directions) to translate from or to Cata-
lan or Galician is not an option. The experiments
show that character-based pivot models lead to
better translations than word-based pivot models
(in terms of BLEU scores). This reflects the per-
formance gains presented in Table 2. Rescoring
of N-best lists, on the other hand, does not have
a big impact on our results. However, we did not
spend time optimizing the parameters of N-best
size and interpolation weight.
The results from the Macedonian task are not as
clear. This is especially due to the different setup
in which the baseline uses more training data than
any of the related language pivot models. How-
ever, we can still see that the pivot translation via
Bulgarian clearly outperforms the baseline. For
the case of translating to Macedonian via Bulgar-
ian, the word-based model seems to be more ro-
bust than the character-level model. This may be
due to a larger number of non-words generated
by the character-based pivot model. In general,
Model (BLEU in %) 1x1 10x10
English ? Maced. (baseline) 11.04
English ? Bosn. -word- Maced. 7.33?? 7.64
English ? Bosn. -char- Maced. 9.99 10.34
English ? Bulg. -word- Maced. 12.49++ 12.62++
English ? Bulg. -char- Maced. 11.57++ 11.59+
Maced. ? English (baseline) 20.24
Maced. -word- Bosn. ? English 12.36?? 12.48??
Maced. -char- Bosn. ? English 18.73? 18.64??
Maced. -word- Bulg. ? English 19.62 19.74
Maced. -char- Bulg. ? English 21.05 21.10
Table 5: Translating between Macedonian (Maced)
and English via Bosnian (Bosn) / Bulgarian (Bulg).
the BLEU scores are much lower for all models
involved (even for the high-density languages),
which indicates larger problems with the gener-
ation of correct output and intermediate transla-
tions.
Interesting is the fact that we can achieve al-
most the same performance as the baseline when
translating via Bosnian even though we had much
less training data at our disposal for the translation
between Macedonian and Bosnian. In this setup,
we can see that a character-based model was nec-
essary in order to obtain the desired abstraction
from the tiny amount of training data.
4.2 Task 2: Pivoting for Domain Adaptation
Sparse resources are not only a problem for spe-
cific languages but also for specific domains.
SMT models are very sensitive to domain shifts
and domain-specific data is often rare. In the fol-
lowing, we investigate a test case of translating
between two languages (English and Norwegian)
with reasonable amounts of data resources but in
the wrong domain (movie subtitles instead of le-
gal texts). Here again, we facilitate the transla-
tion process by a pivot language, this time with
domain-specific data.
The task is to translate legal texts from Norwe-
gian (Bokma?l) to English and vice versa. The test
set is taken from the English?Norwegian Parallel
Corpus (ENPC) (Johansson et al 1996) and con-
tains 1493 parallel sentences (a selection of Eu-
ropean treaties, directives and agreements). Oth-
erwise, there is no training data available in this
domain for English and Norwegian. Table 6 lists
the other data resources we used in our study.
As we can see, there is decent amount of train-
ing data for English ? Norwegian, but the domain
is strikingly different. On the other hand, there
147
Language pair Domain #sent?s #words
English?Norwegian subtitles 2.4M 18M
Norwegian?Danish subtitles 1.5M 10M
Danish?English DGT-TM 430k 9M
Table 6: Training data available for the domain adapta-
tion task. DGT-TM refers to the translation memories
provided by the JRC (Steinberger et al 2006)
is in-domain data for other languages like Danish
that may act as an intermediate pivot. Further-
more, we have out-of-domain data for the transla-
tion between pivot and Norwegian. The sizes of
the training data sets for the pivot models are com-
parable (in terms of words). The in-domain pivot
data is controlled and very consistent and, there-
fore, high quality translations can be expected.
The subtitle data is noisy and includes various
movie genres. It is important to mention that the
pivot data still does not contain any sentence in-
cluded in the English?Norwegian test set.
Table 7 summarizes the results of our experi-
ments when using Danish and in-domain data as
a pivot in translations from and to Norwegian.
Model (task: English ? Norwegian) BLEU
(step 1) English ?dgt? Danish 52.76
(step 2) Danish ?subswo? Norwegian 29.87
(step 2) Danish ?subsch? Norwegian 29.65
(step 2) Danish ?subsbi? Norwegian 25.65
English ?subs? Norwegian (baseline) 7.20
English ?dgt? (Danish = Norwegian) 9.44++
English ?dgt? Danish -subswo- Norwegian 17.49++
English ?dgt? Danish -subsch- Norwegian 17.61++
English ?dgt? Danish -subsbi- Norwegian 14.07++
Model (task: Norwegian ? English) BLEU
(step 1) Norwegian ?subswo? Danish 30.15
(step 1) Norwegian ?subsch? Danish 27.81
(step 1) Norwegian ?subsbi? Danish 28.52
(step 2) Danish ?dgt? English 57.23
Norwegian ?subs? English (baseline) 11.41
(Norwegian = Danish) ?dgt? English 13.21++
Norwegian ?subs+dgtLM? English 13.33++
Norwegian ?subswo? Danish ?dgt? English 25.75++
(Norwegian ?subsch? Danish ?dgt? English 23.77++
Norwegian ?subsbi? Danish ?dgt? English 26.29++
Table 7: Translating out-of-domain data via Dan-
ish. Models using in-domain data are marked with
dgt and out-of-domain models are marked with subs.
subs+dgtLM refers to a model with an out-of-domain
translation model and an added in-domain language
model. The subscripts wo, ch and bi refer to word,
character and bigram models, respectively.
The influence of in-domain data in the transla-
tion process is enormous. As expected, the out-
of-domain baseline does not perform well even
though it uses the largest amount of training data
in our setup. It is even outperformed by the in-
domain pivot model when pretending that Norwe-
gian is in fact Danish. For the translation into En-
glish, the in-domain language model helps a lit-
tle bit (similar resources are not available for the
other direction). However, having the strong in-
domain model for translating to (and from) the
pivot language improves the scores dramatically.
The out-of-domain model in the other part of the
cascaded translation does not destroy this advan-
tage completely and the overall score is much
higher than any other baseline.
In our setup, we used again a closely related
language as a pivot. However, this time we
had more data available for training the pivot
translation model. Naturally, the advantages of
the character-level approach diminishes and the
word-level model becomes a better alternative.
However, there can still be a good reason for the
use of a character-based model as we can see in
the success of the bigram model (?subsbi?) in the
translation from Norwegian to English (via Dan-
ish). A character-based model may generalize be-
yond domain-specific terminology which leads to
a reduction of unknown words when applied to
a new domain. Note that using a character-based
model in step two could possibly cause more harm
than using it in step one of the pivot-based pro-
cedure. Using n-best lists for a subsequent word-
based translation in step two may fix errors caused
by character-based translation simply by ignoring
hypotheses containing them, which makes such a
model more robust to noisy input.
Finally, as an alternative, we can also look at
other pivot languages. The domain adaptation
task is not at all restricted to closely related pivot
languages especially considering the success of
word-based models in the experiments above. Ta-
ble 8 lists results for three other pivot languages.
Surprisingly, the results are much worse than
for the Danish test case. Apparently, these mod-
els are strongly influenced by the out-of-domain
translation between Norwegian and the pivot lan-
guage. The only success can be seen with an-
other closely related language, Swedish. Lexical
and syntactic similarity seems to be important to
create models that are robust enough for domain
shifts in the cascaded translation setup.
148
Pivot=xx en?xx xx?no en?xx?no
German 53.09 23.60 3.15??
French 66.47 17.84 5.03??
Swedish 52.62 24.79 10.07++
Pivot=xx no?xx xx?en no?xx?en
German 15.02 53.02 5.52??
French 17.69 65.85 8.78??
Swedish 19.72 59.55 16.35++
Table 8: Alternative word-based pivot translations be-
tween Norwegian (no) and English (en).
5 Related Work
There is a wide range of pivot language ap-
proaches to machine translation and a number
of strategies have been proposed. One of them
is often called triangulation and usually refers
to the combination of phrase tables (Cohn and
Lapata, 2007). Phrase translation probabilities
are merged and lexical weights are estimated by
bridging word alignment models (Wu and Wang,
2007; Bertoldi et al 2008). Cascaded translation
via pivot languages are discussed by (Utiyama
and Isahara, 2007) and are frequently used by var-
ious researchers (de Gispert and Marin?o, 2006;
Koehn et al 2009; Wu and Wang, 2009) and
commercial systems such as Google Translate.
A third strategy is to generate or augment data
sets with the help of pivot models. This is, for
example, explored by (de Gispert and Marin?o,
2006) and (Wu and Wang, 2009) (who call it the
synthetic method). Pivoting has also been used
for paraphrasing and lexical adaptation (Bannard
and Callison-Burch, 2005; Crego et al 2010).
(Nakov and Ng, 2009) investigate pivot languages
for resource-poor languages (but only when trans-
lating from the resource-poor language). They
also use transliteration for adapting models to a
new (related) language. Character-level SMT has
been used for transliteration (Matthews, 2007;
Tiedemann and Nabende, 2009) and also for the
translation between closely related languages (Vi-
lar et al 2007; Tiedemann, 2009a).
6 Conclusions and Discussion
In this paper, we have discussed possibilities to
translate via pivot languages on the character
level. These models are useful to support under-
resourced languages and explore strong lexical
and syntactic similarities between closely related
languages. Such an approach makes it possible
to train reasonable translation models even with
extremely sparse data sets. Moreover, charac-
ter level models introduce an abstraction that re-
duce the number of unknown words dramatically.
In most cases, these unknown words represent
information-rich units that bear large portions of
the meaning to be translated. The following illus-
trates this effect on example translations with and
without pivot model:
Example: Catalan  English (via Spanish)
Referene: I have to grade these papers.
Baseline: Tinque qualiar these ex amens.
Pivot
word
: Tinque qualiar these tests.
Pivot
char
: I have to grade these papers.
Example: Maedonian  English (via Bulgarian)
Referene: It's a simple matter of self-preservation.
Baseline: It's simply a question of ??????????????.
Pivot
word
: That's a matter of ??????????????.
Pivot
char
: It's just a question of yourself.
Leaving unseen words untranslated is not only an-
noying (especially if the input language uses a
different writing system) but often makes transla-
tions completely incomprehensible. Pivot trans-
lations will still not be perfect (see example
two above), but can at least be more intelli-
gible. Character-based models can even take
care of tokenization errors as the one shown
above (?Tincque? should be two words ?Tinc
que?). Fortunately, the generation of non-word
sequences (observed as unknown words) does not
seem to be a big problem and no special treatment
is required to avoid such output. We would still
like to address this issue in future work by adding
a word level LM in character-based SMT. How-
ever, (Vilar et al 2007) already showed that this
did not have any positive effect in their character-
based system. In a second study, we also showed
that pivot models can be useful for adapting to
a new domain. The use of in-domain pivot data
leads to systems that outperform out-of-domain
translation models by a large margin. Our find-
ings point to many prospects for future work.
For example, we would like to investigate combi-
nations of character-based and word-based mod-
els. Character-based models may also be used for
treating unknown words only. Multiple source ap-
proaches via several pivots is another possibility
to be explored. Finally, we also need to further
investigate the robustness of the approach with re-
spect to other language pairs, data sets and learn-
ing parameters.
149
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL?05), pages
597?604, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
Nicola Bertoldi, Madalina Barbaiani, Marcello Fed-
erico, and Roldano Cattoni. 2008. Phrase-Based
Statistical Machine Translation with Pivot Lan-
guages. In Proceedings of the International Work-
shop on Spoken Language Translation, pages 143?
149, Hawaii, USA.
Trevor Cohn and Mirella Lapata. 2007. Machine
translation by triangulation: Making effective use
of multi-parallel corpora. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 728?735, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Josep Maria Crego, Aure?lien Max, and Franc?ois Yvon.
2010. Local lexical adaptation in machine transla-
tion through triangulation: SMT helping SMT. In
Proceedings of the 23rd International Conference
on Computational Linguistics (Coling 2010), pages
232?240, Beijing, China, August. Coling 2010 Or-
ganizing Committee.
A. de Gispert and J.B. Marin?o. 2006. Catalan-english
statistical machine translation without parallel cor-
pus: Bridging through spanish. In Proceedings of
the 5th Workshop on Strategies for developing Ma-
chine Translation for Minority Languages (SALT-
MIL?06) at LREC, pages 65?68, Genova, Italy.
Mark Fishel and Harri Kirik. 2010. Linguistically
motivated unsupervised segmentation for machine
translation. In Proceedings of the International
Conference on Language Resources and Evaluation
(LREC), pages 1741?1745, Valletta, Malta.
Mark Fishel. 2009. Deeper than words: Morph-based
alignment for statistical machine translation. In
Proceedings of the Conference of the Pacific Associ-
ation for Computational Linguistics PacLing 2009,
Sapporo, Japan.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme
conversion. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pages
372?379, Rochester, New York, April. Association
for Computational Linguistics.
Stig Johansson, Jarle Ebeling, and Knut Hofland.
1996. Coding and aligning the English-Norwegian
Parallel Corpus. In K. Aijmer, B. Altenberg,
and M. Johansson, editors, Languages in Contrast,
pages 87?112. Lund University Press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch,
Chris Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Chris Dyer, Ondrej Bojar,
Alexandra Constantin, and Evan Herbst. 2007.
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Philipp Koehn, Alexandra Birch, and Ralf Steinberger.
2009. 462 machine translation systems for europe.
In Proceedings of MT Summit XII, pages 65?72, Ot-
tawa, Canada.
Minh-Thang Luong, Preslav Nakov, and Min-Yen
Kan. 2010. A hybrid morpheme-word represen-
tation for machine translation of morphologically
rich languages. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 148?157, Cambridge, MA, Octo-
ber. Association for Computational Linguistics.
David Matthews. 2007. Machine transliteration of
proper names. Master?s thesis, School of Informat-
ics, University of Edinburgh.
Preslav Nakov and Hwee Tou Ng. 2009. Im-
proved statistical machine translation for resource-
poor languages using related resource-rich lan-
guages. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1358?1367, Singapore, August. Associ-
ation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160?167, Sap-
poro, Japan, July. Association for Computational
Linguistics.
Eric Sven Ristad and Peter N. Yianilos. 1998.
Learning string edit distance. IEEE Transactions
on Pattern Recognition and Machine Intelligence,
20(5):522?532, May.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz? Erjavec, and Dan Tufis?.
2006. The JRC-Acquis: A multilingual aligned par-
allel corpus with 20+ languages. In Proceedings of
150
the 5th International Conference on Language Re-
sources and Evaluation (LREC), pages 2142?2147.
Graham A. Stephen. 1992. String Search. Technical
report, School of Electronic Engineering Science,
University College of North Wales, Gwynedd.
Jo?rg Tiedemann and Peter Nabende. 2009. Translat-
ing transliterations. International Journal of Com-
puting and ICT Research, 3(1):33?41.
Jo?rg Tiedemann. 2009a. Character-based PSMT for
closely related languages. In Proceedings of 13th
Annual Conference of the European Association for
Machine Translation (EAMT?09), pages 12 ? 19,
Barcelona, Spain.
Jo?rg Tiedemann. 2009b. News from OPUS - A col-
lection of multilingual parallel corpora with tools
and interfaces. In Recent Advances in Natural Lan-
guage Processing, volume V, pages 237?248. John
Benjamins, Amsterdam/Philadelphia.
Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statisti-
cal machine translation. In Human Language Tech-
nologies 2007: The Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics; Proceedings of the Main Conference,
pages 484?491, Rochester, New York, April. Asso-
ciation for Computational Linguistics.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can we translate letters? In Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, pages 33?39, Prague, Czech Republic, June.
Association for Computational Linguistics.
Hua Wu and Haifeng Wang. 2007. Pivot language ap-
proach for phrase-based statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
856?863, Prague, Czech Republic, June. Associa-
tion for Computational Linguistics.
Hua Wu and Haifeng Wang. 2009. Revisiting pivot
language approach for machine translation. In Pro-
ceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing
of the AFNLP, pages 154?162, Suntec, Singapore,
August. Association for Computational Linguistics.
151
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 301?305,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Combining Word-Level and Character-Level Models
for Machine Translation Between Closely-Related Languages
Preslav Nakov
Qatar Computing Research Institute
Qatar Foundation, P.O. box 5825
Doha, Qatar
pnakov@qf.org.qa
Jo?rg Tiedemann
Department of Linguistics and Philology
Uppsala University
Uppsala, Sweden
jorg.tiedemann@lingfil.uu.se
Abstract
We propose several techniques for improv-
ing statistical machine translation between
closely-related languages with scarce re-
sources. We use character-level translation
trained on n-gram-character-aligned bitexts
and tuned using word-level BLEU, which we
further augment with character-based translit-
eration at the word level and combine with
a word-level translation model. The evalua-
tion on Macedonian-Bulgarian movie subtitles
shows an improvement of 2.84 BLEU points
over a phrase-based word-level baseline.
1 Introduction
Statistical machine translation (SMT) systems, re-
quire parallel corpora of sentences and their transla-
tions, called bitexts, which are often not sufficiently
large. However, for many closely-related languages,
SMT can be carried out even with small bitexts by
exploring relations below the word level.
Closely-related languages such as Macedonian
and Bulgarian exhibit a large overlap in their vo-
cabulary and strong syntactic and lexical similari-
ties. Spelling conventions in such related languages
can still be different, and they may diverge more
substantially at the level of morphology. However,
the differences often constitute consistent regulari-
ties that can be generalized when translating.
The language similarities and the regularities in
morphological variation and spelling motivate the
use of character-level translation models, which
were applied to translation (Vilar et al, 2007; Tiede-
mann, 2009a) and transliteration (Matthews, 2007).
Macedonian Bulgarian
a v m e a h m e
a v m e d a a h m e d a
v e r u v a m v  r v a m
d e k a t o j , q e t o $i
Table 1: Examples from a character-level phrase table
(without scores): mappings can cover words and phrases.
Certainly, translation cannot be adequately mod-
eled as simple transliteration, even for closely-
related languages. However, the strength of phrase-
based SMT (Koehn et al, 2003) is that it can support
rather large sequences (phrases) that capture transla-
tions of entire chunks. This makes it possible to in-
clude mappings that go far beyond the edit-distance-
based string operations usually modeled in translit-
eration. Table 1 shows how character-level phrase
tables can cover mappings spanning over multi-word
units. Thus, character-level phrase-based SMT mod-
els combine the generality of character-by-character
transliteration and lexical mappings of larger units
that could possibly refer to morphemes, words or
phrases, as well as to various combinations thereof.
2 Training Character-level SMT Models
We treat sentences as sequences of characters in-
stead of words, as shown in Figure 1. Due to the
reduced vocabulary, we can use higher-order mod-
els, which is necessary in order to avoid the genera-
tion of non-word sequences. In our case, we opted
for a 10-character language model and a maximum
phrase length of 10 (based on initial experiments).
However, word alignment models are not fit for
character-level SMT, where the vocabulary shrinks.
301
original:
MK: navistina ?
BG: naistina ?
characters:
MK: n a v i s t i n a ?
BG: n a i s t i n a ?
character bigrams:
MK: na av vi is st ti in na a ? ?
BG: na ai is st ti in na a ? ?
Figure 1: Preparing the training corpus for alignment.
Statistical word alignment models heavily rely on
context-independent lexical translation parameters
and, therefore, are unable to properly distinguish
character mapping differences in various contexts.
The alignment models used in the transliteration lit-
erature have the same problem as they are usually
based on edit distance operations and finite-state au-
tomata without contextual history (Jiampojamarn et
al., 2007; Damper et al, 2005; Ristad and Yiani-
los, 1998). We, thus, transformed the input to se-
quences of character n-grams as suggested by Tiede-
mann (2012); examples are shown in Figure 1. This
artificially increases the vocabulary as shown in Ta-
ble 2, making standard alignment models and their
lexical translation parameters more expressive.
Macedonian Bulgarian
single characters 99 101
character bigrams 1,851 1,893
character trigrams 13,794 14,305
words 41,816 30,927
Table 2: Vocabulary size of character-level alignment
models and the corresponding word-level model.
It turns out that bigrams constitute a good com-
promise between generality and contextual speci-
ficity, which yields useful character alignments with
good performance in terms of phrase-based transla-
tion. In our experiments, we used GIZA++ (Och
and Ney, 2003) with standard settings and the grow-
diagonal-final-and heuristics to symmetrize the fi-
nal IBM-model-4-based Viterbi alignments (Brown
et al, 1993). The phrases were extracted and scored
using the Moses training tools (Koehn et al, 2007).1
We tuned the parameters of the log-linear SMT
model using minimum error rate training (Och,
2003), optimizing BLEU (Papineni et al, 2002).
1Note that the extracted phrase table does not include se-
quences of character n-grams. We map character n-gram align-
ments to links between single characters before extraction.
Since BLEU over matching character sequences
does not make much sense, especially if the k-gram
size is limited to small values of k (usually, 4 or
less), we post-processed n-best lists in each tuning
step to calculate the usual word-based BLEU score.
3 Transliteration
We also built a character-level SMT system for
word-level transliteration, which we trained on a list
of automatically extracted pairs of likely cognates.
3.1 Cognate Extraction
Classic NLP approaches to cognate extraction look
for words with similar spelling that co-occur in par-
allel sentences (Kondrak et al, 2003). Since our
Macedonian-Bulgarian bitext (MK?BG) was small,
we further used a MK?EN and an EN?BG bitext.
First, we induced IBM-model-4 word alignments
for MK?EN and EN?BG, from which we extracted
four conditional lexical translation probabilities:
Pr(m|e) and Pr(e|m) for MK?EN, and Pr(b|e) and
Pr(e|b) for EN?BG, where m, e, and b stand for a
Macedonian, an English, and a Bulgarian word.
Then, following (Callison-Burch et al, 2006; Wu
and Wang, 2007; Utiyama and Isahara, 2007), we
induced conditional lexical translation probabilities
as Pr(m|b) =
?
e Pr(m|e) Pr(e|b), where Pr(m|e)
and Pr(e|b) are estimated using maximum likeli-
hood from MK?EN and EN?BG word alignments.
Then, we induced translation probability estima-
tions for the reverse direction Pr(b|m) and we cal-
culated the quantity Piv(m, b) = Pr(m|b) Pr(b|m).
We calculated a similar quantity Dir(m, b), where
the probabilities Pr(m|b) and Pr(b|m) are estimated
using maximum likelihood from the MK?BG bitext
directly. Finally, we calculated the similarity score
S(m, b) = Piv(m, b)+Dir(m, b)+2?LCSR(m, b),
where LCSR is the longest common subsequence of
two strings, divided by the length of the longer one.
The score S(m, b) is high for words that are likely
to be cognates, i.e., that (i) have high probability of
being mutual translations, which is expressed by the
first two terms in the summation, and (ii) have sim-
ilar spelling, as expressed by the last term. Here we
give equal weight to Dir(m, b) and Piv(m, b); we
also give equal weights to the translational similar-
ity (the sum of the first two terms) and to the spelling
similarity (twice LCSR).
302
We excluded all words of length less than three, as
well as all Macedonian-Bulgarian word pairs (m, b)
for which Piv(m, b) + Dir(m, b) < 0.01, and those
for which LCSR(m, b) was below 0.58, a value
found by Kondrak et al (2003) to work well for a
number of European language pairs.
Finally, using S(m, b), we induced a weighted bi-
partite graph, and we performed a greedy approxi-
mation to the maximum weighted bipartite matching
in that graph using competitive linking (Melamed,
2000), to produce the final list of cognate pairs.
Note that the above-described cognate extraction
algorithm has three important components: (1) or-
thographic, based on LCSR, (2) semantic, based
on word alignments and pivoting over English, and
(3) competitive linking. The orthographic compo-
nent is essential when looking for cognates since
they must have similar spelling by definition, while
the semantic component prevents the extraction of
false friends like vreden, which means ?valuable?
in Macedonian but ?harmful? in Bulgarian. Finally,
competitive linking helps prevent issues related to
word inflection that cannot be handled using the se-
mantic component alone.
3.2 Transliteration Training
For each pair in the list of cognate pairs, we added
spaces between any two adjacent letters for both
words, and we further appended special start and
end characters. We split the resulting list into
training, development and testing parts and we
trained and tuned a character-level Macedonian-
Bulgarian phrase-based monotone SMT system sim-
ilar to that in (Finch and Sumita, 2008; Tiedemann
and Nabende, 2009; Nakov and Ng, 2009; Nakov
and Ng, 2012). The system used a character-level
Bulgarian language model trained on words. We set
the maximum phrase length and the language model
order to 10, and we tuned the system using MERT.
3.3 Transliteration Lattice Generation
Given a Macedonian sentence, we generated a lat-
tice where each input Macedonian word of length
three or longer was augmented with Bulgarian al-
ternatives: n-best transliterations generated by the
above character-level Macedonian-Bulgarian SMT
system (after the characters were concatenated to
form a word and the special symbols were removed).
In the lattice, we assigned the original Macedo-
nian word the weight of 1; for the alternatives, we
assigned scores between 0 and 1 that were the sum
of the translation model probabilities of generating
each alternative (the sum was needed since some op-
tions appeared multiple times in the n-best list).
4 Experiments and Evaluation
For our experiments, we used translated movie sub-
titles from the OPUS corpus (Tiedemann, 2009b).
For Macedonian-Bulgarian there were only about
102,000 aligned sentences containing approximately
1.3 million tokens altogether. There was substan-
tially more monolingual data available for Bulgar-
ian: about 16 million sentences containing ca. 136
million tokens.
However, this data was noisy. Thus, we realigned
the corpus using hunalign and we removed some
Bulgarian files that were misclassified as Macedo-
nian and vice versa, using a BLEU-filter. Fur-
thermore, we also removed sentence pairs contain-
ing language-specific characters on the wrong side.
From the remaining data we selected 10,000 sen-
tence pairs (roughly 128,000 words) for develop-
ment and another 10,000 (ca. 125,000 words) for
testing; we used the rest for training.
The evaluation results are summarized in Table 3.
MK?BG BLEU % NIST TER METEOR
Transliteration
no translit. 10.74 3.33 67.92 60.30
t1 letter-based 12.07 3.61 66.42 61.87
t2 cogn.+lattice 22.74 5.51 55.99 66.42
Word-level SMT
w0 Apertium 21.28 5.27 56.92 66.35
w1 SMT baseline 31.10 6.56 50.72 70.53
w2 w1 + t1-lattice 32.19(+1.19) 6.76 49.68 71.18
Character-level SMT
c1 char-aligned 32.28(+1.18) 6.70 49.70 71.35
c2 bigram-aligned 32.71(+1.61) 6.77 49.23 71.65
trigram-aligned 32.07(+0.97) 6.68 49.82 71.21
System combination
w2 + c2 32.92(+1.82) 6.90 48.73 71.71
w1 + c2 33.31(+2.21) 6.91 48.60 71.81
Merged phrase tables
m1 w1 + c2 33.33(+2.13) 6.86 48.86 71.73
m2 w2 + c2 33.94(+2.84) 6.89 48.99 71.76
Table 3: Macedonian-Bulgarian translation and
transliteration. Superscripts show the absolute improve-
ment in BLEU compared to the word-level baseline (w1).
303
Transliteration. The top rows of Table 3 show
the results for Macedonian-Bulgarian transliteration.
First, we can see that the BLEU score for the original
Macedonian testset evaluated against the Bulgarian
reference is 10.74, which is quite high and reflects
the similarity between the two languages. The next
line (t1) shows that many differences between Mace-
donian and Bulgarian stem from mere differences in
orthography: we mapped the six letters in the Mace-
donian alphabet that do not exist in the Bulgarian al-
phabet to corresponding Bulgarian letters and letter
sequences, gaining over 1.3 BLEU points. The fol-
lowing line (t2) shows the results using the sophis-
ticated transliteration described in Section 3, which
takes two kinds of context into account: (1) word-
internal letter context, and (2) sentence-level word
context. We generated a lattice for each Macedonian
test sentence, which included the original Mace-
donian words and the 1-best2 Bulgarian transliter-
ation option from the character-level transliteration
model. We then decoded the lattice using a Bulgar-
ian language model; this increased BLEU to 22.74.
Word-level translation. Naturally, lattice-based
transliteration cannot really compete against stan-
dard word-level translation (w1), which is better
by 8 BLEU points. Still, as line (w2) shows,
using the 1-best transliteration lattice as an input
to (w1) yields3 consistent improvement over (w1)
for four evaluation metrics: BLEU (Papineni et
al., 2002), NIST v. 13, TER (Snover et al, 2006)
v. 0.7.25, and METEOR (Lavie and Denkowski,
2009) v. 1.3. The baseline system is also signifi-
cantly better than the on-line version of Apertium
(http://www.apertium.org/), a shallow transfer-rule-
based MT system that is optimized for closely-
related languages (accessed on 2012/05/02). Here,
Apertium suffers badly from a large number of un-
known words in our testset (ca. 15%).
Character-level translation. Moving down to
the next group of experiments in Table 3, we can
see that standard character-level SMT (c1), i.e.,
simply treating characters as separate words, per-
forms significantly better than word-level SMT. Us-
ing bigram-based character alignments yields fur-
ther improvement of +0.43 BLEU.
2Using 3/5/10/100-best made very little difference.
3The decoder can choose between (a) translating a Macedo-
nian word and (b) using its 1-best Bulgarian transliteration.
System combination. Since word-level and
character-level models have different strengths and
weaknesses, we further tried to combine them.
We used MEMT, a state-of-the-art Multi-Engine
Machine Translation system (Heafield and Lavie,
2010), to combine the outputs of (c3) with the out-
put of (w1) and of (w2). Both combinations im-
proved over the individual systems, but (w1)+(c2)
performed better, by +0.6 BLEU points over (c2).
Combining word-level and phrase-level SMT.
Finally, we also combined (w1) with (c3) in a more
direct way: by merging their phrase tables. First,
we split the phrases in the word-level phrase tables
of (w1) to characters as in character-level models.
Then, we generated four versions of each phrase
pair: with/without ? ? at the beginning/end of the
phrase. Finally, we merged these phrase pairs with
those in the phrase table of (c3), adding two ex-
tra features indicating each phrase pair?s origin: the
first/second feature is 1 if the pair came from the
first/second table, and 0.5 otherwise. This combina-
tion outperformed MEMT, probably because it ex-
pands the search space of the SMT system more di-
rectly. We further tried scoring with two language
models in the process of translation, character-based
and word-based, but we did not get consistent im-
provements. Finally, we experimented with a 1-best
character-level lattice input that encodes the same
options and weights as for (w2). This yielded our
best overall BLEU score of 33.94, which is +2.84
BLEU points of absolute improvement over the (w1)
baseline, and +1.23 BLEU points over (c2).4
5 Conclusion and Future Work
We have explored several combinations of character-
and word-level translation models for translating
between closely-related languages with scarce re-
sources. In future work, we want to use such a model
for pivot-based translations from the resource-poor
language (Macedonian) to other languages (such as
English) via the related language (Bulgarian).
Acknowledgments
The research is partially supported by the EU ICT
PSP project LetsMT!, grant number 250456.
4All improvements over (w1) in Table 3 that are greater or
equal to 0.97 BLEU points are statistically significant according
to Collins? sign test (Collins et al, 2005).
304
References
Peter Brown, Vincent Della Pietra, Stephen Della Pietra,
and Robert Mercer. 1993. The mathematics of statis-
tical machine translation: parameter estimation. Com-
putational Linguistics, 19(2):263?311.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine translation
using paraphrases. In Proceedings of HLT-NAACL
?06, pages 17?24, New York, NY.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of ACL ?05, pages 531?
540, Ann Arbor, MI.
Robert Damper, Yannick Marchand, John-David
Marsters, and Alex Bazin. 2005. Aligning text and
phonemes for speech technology applications using an
EM-like algorithm. International Journal of Speech
Technology, 8(2):149?162.
Andrew Finch and Eiichiro Sumita. 2008. Phrase-based
machine transliteration. In Proceedings of the Work-
shop on Technologies and Corpora for Asia-Pacific
Speech Translation, pages 13?18, Hyderabad, India.
Kenneth Heafield and Alon Lavie. 2010. Combin-
ing machine translation output with open source:
The Carnegie Mellon multi-engine machine transla-
tion scheme. The Prague Bulletin of Mathematical
Linguistics, 93(1):27?36.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden Markov models to letter-to-phoneme con-
version. In Proceedings of NAACL-HLT ?07, pages
372?379, Rochester, New York.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL ?03, pages 48?54, Edmonton, Canada.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of ACL ?07, pages 177?180, Prague, Czech Re-
public.
Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003. Cognates can improve statistical translation
models. In Proceedings of NAACL ?03, pages 46?48,
Edmonton, Canada.
Alon Lavie and Michael Denkowski. 2009. The Meteor
metric for automatic evaluation of machine translation.
Machine Translation, 23:105?115.
David Matthews. 2007. Machine transliteration of
proper names. Master?s thesis, School of Informatics,
University of Edinburgh, Edinburgh, UK.
Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Preslav Nakov and Hwee Tou Ng. 2009. Improved statis-
tical machine translation for resource-poor languages
using related resource-rich languages. In Proceedings
of EMNLP ?09, pages 1358?1367, Singapore.
Preslav Nakov and Hwee Tou Ng. 2012. Improving
statistical machine translation for a resource-poor lan-
guage using related resource-rich languages. Journal
of cial Intelligence Research, 44.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL
?03, pages 160?167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
?02, pages 311?318, Philadelphia, PA.
Eric Ristad and Peter Yianilos. 1998. Learning string
edit distance. IEEE Transactions on Pattern Recogni-
tion and Machine Intelligence, 20(5):522?532.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of AMTA ?06, pages 223?231.
Jo?rg Tiedemann and Peter Nabende. 2009. Translating
transliterations. International Journal of Computing
and ICT Research, 3(1):33?41.
Jo?rg Tiedemann. 2009a. Character-based PSMT for
closely related languages. In Proceedings of EAMT
?09, pages 12?19, Barcelona, Spain.
Jo?rg Tiedemann. 2009b. News from OPUS - A collection
of multilingual parallel corpora with tools and inter-
faces. In Recent Advances in Natural Language Pro-
cessing, volume V, pages 237?248. John Benjamins.
Jo?rg Tiedemann. 2012. Character-based pivot transla-
tion for under-resourced languages and domains. In
Proceedings of EACL ?12, pages 141?151, Avignon,
France.
Masao Utiyama and Hitoshi Isahara. 2007. A compar-
ison of pivot methods for phrase-based statistical ma-
chine translation. In Proceedings of NAACL-HLT ?07,
pages 484?491, Rochester, NY.
David Vilar, Jan-Thorsten Peter, and Hermann Ney.
2007. Can we translate letters? In Proceedings of
WMT ?07, pages 33?39, Prague, Czech Republic.
Hua Wu and Haifeng Wang. 2007. Pivot language
approach for phrase-based statistical machine transla-
tion. Machine Translation, 21(3):165?181.
305
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 43?48,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
LetsMT!: A Cloud-Based Platform for Do-It-Yourself  
Machine Translation 
 
  
Andrejs Vasi?jevs Raivis Skadi?? J?rg Tiedemann 
TILDE TILDE Uppsala University 
Vienbas gatve 75a, Riga Vienbas gatve 75a, Riga Box 635, Uppsala 
LV-1004, LATVIA LV-1004, LATVIA SE-75126, SWEDEN 
andrejs@tilde.com raivis.skadins@ 
tilde.lv 
jorg.tiedemann@ 
lingfil.uu.se 
 
 
 
Abstract 
To facilitate the creation and usage of custom 
SMT systems we have created a cloud-based 
platform for do-it-yourself MT. The platform is 
developed in the EU collaboration project 
LetsMT!. This system demonstration paper 
presents the motivation in developing the 
LetsMT! platform, its main features, 
architecture, and an evaluation in a practical use 
case. 
1 Introduction 
Current mass-market and online MT systems are of 
a general nature and perform poorly for smaller 
languages and domain specific texts. The 
European Union ICT-PSP Programme project 
LetsMT! develops a user-driven MT ?factory in 
the cloud? enabling web users to get customised 
MT that better fits their needs. Harnessing the huge 
potential of the web together with open statistical 
machine translation (SMT) technologies, LetsMT! 
has created an online collaborative platform for 
data sharing and MT building.  
The goal of the LetsMT! project is to facilitate 
the use of open source SMT tools and to involve 
users in the collection of training data. The 
LetsMT! project extends the use of existing state-
of-the-art SMT methods by providing them as 
cloud-based services. An easy-to-use web interface 
empowers users to participate in data collection 
and MT customisation to increase the quality, 
domain coverage, and usage of MT.  
The LetsMT! project partners are companies 
TILDE (coordinator), Moravia, and SemLab, and 
the Universities of Edinburgh, Zagreb, 
Copenhagen, and Uppsala. 
2 LetsMT! Key Features 
The LetsMT! platform 1  (Vasi?jevs et al, 2011) 
gathers public and user-provided MT training data 
and enables generation of multiple MT systems by 
combining and prioritising this data. Users can 
upload their parallel corpora to an online 
repository and generate user-tailored SMT systems 
based on data selected by the user.  
Authenticated users with appropriate 
permissions can also store private corpora that can 
be seen and used only by this user (or a designated 
user group). All data uploaded into the LetsMT! 
repository is kept in internal format, and only its 
metadata is provided to the user. Data cannot be 
downloaded or accessed for reading by any means. 
The uploaded data can only be used for SMT 
training. In such a way, we encourage institutions 
and individuals to contribute their data to be 
publicly used for SMT training, even if they are 
not willing to share the content of the data. 
A user creates SMT system definition by 
specifying a few basic parameters like system 
name, source/target languages, domain, and 
choosing corpora (parallel for translation models or 
monolingual for language models) to use for the 
particular system. Tuning and evaluation data can 
be automatically extracted from the training 
corpora or specified by the user. The access level 
of the system can also be specified - whether it will 
be public or accessible only to the particular user  
or user group. 
                                                          
1 http://letsmt.com 
43
When the system is specified, the user can begin 
training it. Progress of the training can be 
monitored on the dynamic training chart (Figure 1). 
It provides a detailed visualisation of the training 
process showing (i) steps queued for execution of a 
particular training task, (ii) current execution status 
of active training steps, and (iii) steps where any 
errors have occurred. The training chart remains 
available after the training to facilitate analysis of 
the performed trainings. The last step of the 
training task is automatic evaluation using BLEU, 
NIST, TER, and METEOR scores. 
A successfully trained SMT system can be 
started and used for translation in several ways: 
? on the translation webpage of LetsMT! for 
testing and short translations; 
? using LetsMT! plug-ins in computer-
assisted translation (CAT) tools for 
professional translation;  
? integrating the LetsMT! widget for web-
site translation;  
? using LetsMT! plug-ins for IE and FireFox 
to integrate translation into the browsers; 
? using LetsMT! API for MT integration into 
different applications.  
LetsMT! allows for several system instances to 
run simultaneously to speed up translation and 
balance the workload from numerous translation 
requests. 
LetsMT! user authentication and authorisation 
mechanisms control access rights to private 
training data, trained models 
and SMT systems, per-
missions to initiate and 
manage training tasks, run 
trained systems, and access 
LetsMT! services through 
external APIs. 
The LetsMT! platform is 
populated with initial SMT 
training data collected and 
prepared by the project 
partners. It currently contains 
more than 730 million 
parallel sentences in almost 
50 languages. In the first 4 
months since launching the 
invitation only beta version 
of the platform, 82 SMT 
systems have been 
successfully trained. 
3 SMT Training and Decoding Facilities 
The SMT training and decoding facilities of 
LetsMT! are based on the open source toolkit 
Moses. One of the important achievements of the 
project is the adaptation of the Moses toolkit to fit 
into the rapid training, updating, and interactive 
access environment of the LetsMT! platform. 
The Moses SMT toolkit (Koehn et al, 2007) 
provides a complete statistical translation system 
distributed under the LGPL license. Moses 
includes all of the components needed to pre-
process data and to train language and translation 
models. Moses is widely used in the research 
community and has also reached the commercial 
sector. While the use of the software is not closely 
monitored, Moses is known to be in commercial 
use by companies such as Systran (Dugast et al, 
2009), Asia Online, Autodesk (Plitt and Masselot, 
2010), Matrixware2, Adobe, Pangeanic, Logrus3, 
and Applied Language Solutions (Way et al, 
2011). 
The SMT training pipeline implemented in 
Moses involves a number of steps that each require 
a separate program to run. In the framework of 
                                                          
2 Machine Translation at Matrixware: http://ir-facility.net/ 
downloads/mxw_factsheet_smt_200910.pdf 
3 TDA Members doing business with Moses: 
http://www.tausdata.org/blog/2010/10/doing-business-with-
moses-open-source-translation/ 
Figure 1. Training chart providing dynamic representation of training steps. 
44
LetsMT!, this process is streamlined and made 
automatically configurable given a set of user-
specified variables (training corpora, language 
model data, tuning sets). SMT training is 
automated using the Moses experiment mana-
gement system (Koehn, 2010). Other impro-
vements of Moses, implemented by the University 
of Edinburgh as part of LetsMT! project, are: 
? the incremental training of SMT models 
(Levenberg et al, 2010); 
? randomised language models (Levenberg 
et al, 2009); 
? a server mode version of the Moses 
decoder and multithreaded decoding; 
? multiple translation models; 
? distributed language models (Brants et al, 
2007).  
Many improvements in the Moses experiment 
management system were implemented to speed up 
SMT system training and to use the full potential 
of the HPC cluster. We revised and improved 
Moses training routines (i) by finding tasks that are 
executed sequentially but can be executed in 
parallel and (ii) by splitting big training tasks into 
smaller ones and executing them in parallel. 
4 Multitier Architecture 
The LetsMT! system has a multitier architecture 
(Figure 2). It has (i) an interface layer implemen-
ting the user interface and APIs with external 
systems, (ii) an application logic layer for the 
system logic, (iii) a data storage layer consisting of 
file and database storage, and (iv) a high 
performance computing (HPC) cluster. The 
LetsMT! system performs various time and 
resource consuming tasks; these tasks are defined 
by the application logic and data storage and are 
sent to the HPC cluster for execution. 
The Interface layer provides interfaces between 
the LetsMT! system and external users. The system 
has both human and machine users. Human users 
can access the system through web browsers by 
using the LetsMT! web page interface. External 
systems such as Computer Aided Translation 
(CAT) tools and web browser plug-ins can access 
the LetsMT! system through a public API. The 
public API is available through both REST/JSON 
and SOAP protocol web services. An HTTPS 
protocol is used to ensure secure user 
authentication and secure data transfer. 
The application logic layer contains a set of 
modules responsible for the main functionality and 
logic of the system. It receives queries and 
commands from the interface layer and prepares 
answers or performs tasks using data storage and 
the HPC cluster. This layer contains several 
modules such as the Resource Repository Adapter, 
the User Manager, the SMT Training Manager, etc. 
The interface layer accesses the application logic 
layer through the REST/JSON and SOAP protocol 
web services. The same protocols are used for 
communication between modules in the 
application logic layer.  
Figure 2. The LetsMT! system architecture 
The data is stored in one central Resource 
Repository (RR). As training data may change (for 
example, grow), the RR is based on a version-
controlled file system (currently we use SVN as 
the backend system). A key-value store is used to 
keep metadata and statistics about training data and 
trained SMT systems. Modules from the 
application logic layer and HPC cluster access RR 
through a REST-based web service interface.  
A High Performance Computing Cluster is used 
to execute many different computationally heavy 
data processing tasks ? SMT training and running, 
corpora processing and converting, etc. Modules 
from the application logic and data storage layers 
45
create jobs and send them to the HPC cluster for 
execution. The HPC cluster is responsible for 
accepting, scheduling, dispatching, and managing 
remote and distributed execution of large numbers 
of standalone, parallel, or interactive jobs. It also 
manages and schedules the allocation of distributed 
resources such as processors, memory, and disk 
space. The LetsMT! HPC cluster is based on the 
Oracle Grid Engine (SGE). 
The hardware infrastructure of the LetsMT! 
platform is heterogeneous. The majority of 
services run on Linux platforms (Moses, RR, data 
processing tools, etc.). The Web server and 
application logic services run on a Microsoft 
Windows platform.  
The system hardware architecture is designed to 
be highly scalable. The LetsMT! platform contains 
several machines with both continuous and on-
demand availability: 
? Continuous availability machines are used 
to run the core frontend and backend 
services and the HPC grid master to 
guarantee stable system functioning; 
? On-demand availability machines are used 
(i) to scale up the system by adding more 
computing power to training, translation, 
and data import services (HPC cluster 
nodes) and (ii) to increase  performance of 
frontend and backend server instances. 
To ensure scalability of the system, the whole 
LetsMT! system including the HPC cluster is 
hosted by Amazon Web Services infrastructure, 
which provides easy access to on-demand 
computing and storage resources. 
5 Data Storage and Processing Facilities 
As a data sharing and MT platform, the LetsMT! 
system has to store and process large amounts of 
SMT training data (parallel and monolingual 
corpora) as well as trained models of SMT 
systems. The Resource Repository (RR) software 
is fully integrated into the LetsMT! Platform and 
provides the following major components: 
? Scalable data storage based on version-
controlled file systems; 
? A flexible key-value store for metadata; 
? Access-control mechanisms defining three 
levels of permission (private data, public 
data, shared data); 
? Data import modules that include tools for 
data validation, conversion and automatic 
sentence alignment for a variety of popular 
document formats. 
The general architecture of the Resource 
Repository is illustrated in Figure 3. It is 
implemented in terms of a modular package that 
can easily be installed in a distributed environment. 
RR services are provided via Web API?s and 
secure HTTP requests. Data storage can be 
distributed over several servers as is illustrated in 
Figure 3. Storage servers communicate with the 
central database server that manages all metadata 
records attached to resources in the RR. Data 
resources are organised in slots that correspond to 
file systems with user-specific branches. Currently, 
the RR package implements two storage backends: 
a plain file system and a version-controlled file 
system based on subversion (SVN). The latter is 
the default mode, which has several advantages 
over non-revisioned data storage. Revision control 
systems are designed to handle dynamically 
growing collections of mainly textual data in a 
multi-user environment. Furthermore, they keep 
track of modifications and file histories to make it 
possible to backtrack to prior revisions. This can 
be a strong advantage, especially in cases of shared 
data access. Another interesting feature is the 
possibility to create cheap copies of entire 
branches that can be used to enable data 
modifications by other users without 
compromising data integrity for others. Finally, 
SVN also naturally stores data in a compressed 
format, which is useful for large-scale document 
collections. In general, the RR implementation is 
modular, other storage backends may be added 
later, and each individual slot can use its own 
backend type. 
Another important feature of the RR is the 
support of a flexible database for metadata. We 
decided to integrate a modern key-value store into 
the platform in order to allow a maximum of 
flexibility. In contrast to traditional relational 
databases, key-value stores allow the storage of 
arbitrary data sets based on pairs of keys and 
values without being restricted to a pre-defined 
schema or a fixed data model. Our implementation 
relies on TokyoCabinet4, a modern implementation 
of schema-less databases that supports all of our 
                                                          
4 https://fallabs/tokyocabinet 
46
requirements in terms of flexibility and efficiency. 
In particular, we use the table mode of 
TokyoCabinet that supports storage of arbitrary 
data records connected to a single key in the 
database. We use resource URL?s in our repository 
to define unique keys in the database, and data 
records attached to these keys may include any 
number of key-value pairs. In this way, we can add 
any kind of information to each addressable 
resource in the RR. The software also supports 
keys with unordered lists of values, which is useful 
for metadata such as languages (in a data 
collection) and for many other purposes. 
Moreover, TokyoCabinet provides powerful query 
language and software bindings for the most 
common programming languages. It can be run in 
client-server mode, which ensures robustness in a 
multi-user environment and natively supports data 
replication. Using TokyoCabinet as our backend, 
we implemented a key-value store for metadata in 
the RR that can easily be extended and queried 
from the frontend of the LetsMT! Platform via 
dedicated web-service calls. 
Yet another important feature of the RR is the 
collection of import modules that take care of 
validation and conversion of user-provided SMT 
training material. Our main goal was to make the 
creation of appropriate data resources as painless 
as possible. Therefore, we included support for the 
most common data formats to be imported into 
LetsMT!. Pre-aligned parallel data can be uploaded 
in TMX, XLIFF, and Moses formats. Monolingual 
data can be provided in plain text, PDF, and MS 
Word formats. We also support the upload of 
compressed archives in zip and tar format. In the 
future, other formats 
can easily be 
integrated in our 
modular implemen-
tation. 
Validation of such 
a variety of formats is 
important. Therefore 
among others, we 
included XML/DTD 
validation, text en-
coding detection soft-
ware, and language 
identification tools 
with pre-trained mo-
dels for over 60 lan-
guages. 
Furthermore, our system also includes tools for 
automatic sentence alignment. Import processes 
automatically align translated documents with each 
other using standard length-based sentence 
alignment methods (Gale and Church, 1993; Varga 
et al, 2005). 
Finally, we also integrated a general batch-
queuing system (SGE) to run off-line processes 
such as import jobs. In this way, we further 
increase the scalability of the system by taking the 
load off repository servers. Data uploads 
automatically trigger appropriate import jobs that 
will be queued on the grid engine using a dedicated 
job web-service API. 
6 Evaluation for Usage in Localisation 
One of the usage scenarios particularly targeted by 
the project is application in the localisation and 
translation industry. Localisation companies 
usually have collected significant amounts of 
parallel data in the form of translation memories. 
They are interested in using this data to create 
customised MT engines that can increase 
productivity of translators. Productivity is usually 
measured as an average number of words 
translated per hour. For this use case, LetsMT! has 
developed plug-ins for integration into CAT tools. 
In addition to translation candidates from 
translation memories, translators receive 
translation suggestions provided by the selected 
MT engine running on LetsMT!. 
As part of the system evaluation, project partner 
Moravia used the LetsMT! platform to train and 
 Figure 3. Resource repository overview 
47
evaluate SMT systems for Polish and Czech. An 
English-Czech engine was trained on 0.9M parallel 
sentences coming from Moravia translation 
memories in the IT and tech domain part of the 
Czech National Corpus. The resulting system 
increased translator productivity by 25.1%. An 
English-Polish system was trained on 1.5M 
parallel sentences from Moravia production data in 
the IT domain. Using this system, translator 
productivity increased by 28.5%. 
For evaluation of English-Latvian translation, 
TILDE created a MT system using a significantly 
larger corpus of 5.37M parallel sentence pairs, 
including 1.29M pairs in the IT domain. 
Additional tweaking was made by manually 
adding a factored model over disambiguated 
morphological tags. The resulting system 
increased translator productivity by 32.9% 
(Skadi?? et al, 2011). 
7 Conclusions 
The results described in this paper show that the 
LetsMT! project is on track to fulfill its goal to 
democratise the creation and usage of custom SMT 
systems. LetsMT! demonstrates that the open 
source SMT toolkit Moses is reaching maturity to 
serve as a base for large scale and heavy use 
production purposes. The architecture of the 
platform and Resource Repository enables 
scalability of the system and very large amounts of 
data to be handled in a variety of formats. 
Evaluation shows a strong increase in translation 
productivity by using LetsMT! systems in IT 
localisation. 
Acknowledgments 
The research within the LetsMT! project has 
received funding from the ICT Policy Support 
Programme (ICT PSP), Theme 5 ? Multilingual 
web, grant agreement 250456. 
References  
L. Dugast, J. Senellart, P. Koehn. 2009. Selective 
addition of corpus-extracted phrasal lexical rules to a 
rule-based machine translation system. Proceedings 
of MT Summit XII. 
T. Brants, A.C.  Popat, P.  Xu, F.J  Och, J. Dean. 2007. 
Large Language Models in Machine Translation. 
Proceedings of the 2007 Joint Conference on 
Empirical Methods in Natural Language Processing 
and Computational Natural Language Learning 
(EMNLP-CoNLL), 858-867. Prague, Czech Republic 
W. A. Gale, K. W. Church. 1993. A Program for 
Aligning Sentences in Bilingual Corpora. 
Computational Linguistics 19 (1): 75?102 
P. Koehn, M. Federico, B. Cowan, R. Zens, C. Duer, O. 
Bojar, A. Constantin, E. Herbst. 2007. Moses: Open 
Source Toolkit for Statistical Machine Translation. 
Proceedings of the ACL 2007 Demo and Poster 
Sessions, 177-180. Prague. 
P. Koehn. 2010. An experimental management system. 
The Prague Bulletin of Mathematical Linguistics, 94.  
A. Levenberg, M. Osborne. 2009. Stream-based Ran-
domised Language Models for SMT. Proceedings of 
the 2009 Conference on Empirical Methods in 
Natural Language Processing. 
A. Levenberg, C. Callison-Burch, M. Osborne. 2010. 
Stream-based Translation Models for Statistical 
Machine Translation. Human Language 
Technologies: The 2010 Annual Conference of the 
North American Chapter of the Association for 
Computational Linguistics (HLT '10) 
M. Plitt, F. Masselot. 2010. A Productivity Test of 
Statistical Machine Translation Post-Editing in a 
Typical Localisation Context. The Prague Bulletin of 
Mathematical Linguistics, 93(January 2010): ?16 
R. Skadi??, M. Puri??, I. Skadi?a, A. Vasi?jevs. 2011. 
Evaluation of SMT in localization to under-resourced 
inflected language. Proceedings of the 15th 
International Conference of the European 
Association for Machine Translation EAMT 2011, 
35-40. Leuven, Belgium 
A. Vasi?jevs, R. Skadi??, I. Skadi?a. 2011. Towards 
Application of User-Tailored Machine Translation in 
Localization. Proceedings of the Third Joint 
EM+/CNGL Workshop ?Bringing MT to the User: 
Research Meets Translators? JEC 2011, 23-31. 
Luxembourg 
D. Varga, L. N?meth, P. Hal?csy, A. Kornai, V. Tr?n, 
V. Nagy. 2005. Parallel corpora for medium density 
languages. Recent Advances in Natural Language 
Processing IV Selected papers from RANLP05, 590-
596 
A. Way, K. Holden, L. Ball, G. Wheeldon. 2011. 
SmartMATE: online self-serve access to state-of-the-
art SMT.  Proceedings of the Third Joint EM+/CNGL 
Workshop ?Bringing MT to the User: Research 
Meets Translators? (JEC ?11), 43-52. Luxembourg 
48
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 193?198,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Docent: A Document-Level Decoder for
Phrase-Based Statistical Machine Translation
Christian Hardmeier Sara Stymne J?rg Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
We describe Docent, an open-source de-
coder for statistical machine translation
that breaks with the usual sentence-by-
sentence paradigm and translates complete
documents as units. By taking transla-
tion to the document level, our decoder
can handle feature models with arbitrary
discourse-wide dependencies and consti-
tutes an essential infrastructure compon-
ent in the quest for discourse-aware SMT
models.
1 Motivation
Most of the research on statistical machine trans-
lation (SMT) that was conducted during the last
20 years treated every text as a ?bag of sentences?
and disregarded all relations between elements in
different sentences. Systematic research into ex-
plicitly discourse-related problems has only begun
very recently in the SMT community (Hardmeier,
2012) with work on topics such as pronominal
anaphora (Le Nagard and Koehn, 2010; Hard-
meier and Federico, 2010; Guillou, 2012), verb
tense (Gong et al, 2012) and discourse connect-
ives (Meyer et al, 2012).
One of the problems that hamper the develop-
ment of cross-sentence models for SMT is the fact
that the assumption of sentence independence is
at the heart of the dynamic programming (DP)
beam search algorithm most commonly used for
decoding in phrase-based SMT systems (Koehn et
al., 2003). For integrating cross-sentence features
into the decoding process, researchers had to adopt
strategies like two-pass decoding (Le Nagard and
Koehn, 2010). We have previously proposed an
algorithm for document-level phrase-based SMT
decoding (Hardmeier et al, 2012). Our decoding
algorithm is based on local search instead of dy-
namic programming and permits the integration of
document-level models with unrestricted depend-
encies, so that a model score can be conditioned on
arbitrary elements occurring anywhere in the input
document or in the translation that is being gen-
erated. In this paper, we present an open-source
implementation of this search algorithm. The de-
coder is written in C++ and follows an object-
oriented design that makes it easy to extend it with
new feature models, new search operations or dif-
ferent types of local search algorithms. The code
is released under the GNU General Public License
and published on Github1 to make it easy for other
researchers to use it in their own experiments.
2 Document-Level Decoding with Local
Search
Our decoder is based on the phrase-based SMT
model described by Koehn et al (2003) and im-
plemented, for example, in the popular Moses
decoder (Koehn et al, 2007). Translation is
performed by splitting the input sentence into
a number of contiguous word sequences, called
phrases, which are translated into the target lan-
guage through a phrase dictionary lookup and op-
tionally reordered. The choice between different
translations of an ambiguous source phrase and the
ordering of the target phrases are guided by a scor-
ing function that combines a set of scores taken
from the phrase table with scores from other mod-
els such as an n-gram language model. The actual
translation process is realised as a search for the
highest-scoring translation in the space of all the
possible translations that could be generated given
the models.
The decoding approach that is implemented in
Docent was first proposed by Hardmeier et al
(2012) and is based on local search. This means
that it has a state corresponding to a complete, if
possibly bad, translation of a document at every
1https://github.com/chardmeier/docent/wiki
193
stage of the search progress. Search proceeds by
making small changes to the current search state in
order to transform it gradually into a better trans-
lation. This differs from the DP algorithm used in
other decoders, which starts with an empty trans-
lation and expands it bit by bit. It is similar to
previous work on phrase-based SMT decoding by
Langlais et al (2007), but enables the creation of
document-level models, which was not addressed
by earlier approaches.
Docent currently implements two search al-
gorithms that are different generalisations of the
hill climbing local search algorithm by Hardmeier
et al (2012). The original hill climbing algorithm
starts with an initial state and generates possible
successor states by randomly applying simple ele-
mentary operations to the state. After each op-
eration, the new state is scored and accepted if
its score is better than that of the previous state,
else rejected. Search terminates when the decoder
cannot find an acceptable successor state after a
certain number of attempts, or when a maximum
number of steps is reached.
Simulated annealing is a stochastic variant of
hill climbing that always accepts moves towards
better states, but can also accept moves towards
lower-scoring states with a certain probability that
depends on a temperature parameter in order to
escape local maxima. Local beam search gener-
alises hill climbing in a different way by keeping
a beam of a fixed number of multiple states at any
time and randomly picking a state from the beam
to modify at each move. The original hill climb-
ing procedure can be recovered as a special case
of either one of these search algorithms, by call-
ing simulated annealing with a fixed temperature
of 0 or local beam search with a beam size of 1.
Initial states for the search process can be gen-
erated either by selecting a random segmentation
with random translations from the phrase table in
monotonic order, or by running DP beam search
with sentence-local models as a first pass. For
the second option, which generally yields better
search results, Docent is linked with the Moses
decoder and makes direct calls to the DP beam
search algorithm implemented by Moses. In addi-
tion to these state initialisation procedures, Docent
can save a search state to a disk file which can be
loaded again in a subsequent decoding pass. This
saves time especially when running repeated ex-
periments from the same starting point obtained
by DP search.
In order to explore the complete search space
of phrase-based SMT, the search operations in a
local search decoder must be able to change the
phrase translations, the order of the output phrases
and the segmentation of the source sentence into
phrases. The three operations used by Hardmeier
et al (2012), change-phrase-translation, reseg-
ment and swap-phrases, jointly meet this require-
ment and are all implemented in Docent. Addi-
tionally, Docent features three extra operations, all
of which affect the target word order: The move-
phrases operation moves a phrase to another loca-
tion in the sentence. Unlike swap-phrases, it does
not require that another phrase be moved in the
opposite direction at the same time. A pair of
operations called permute-phrases and linearise-
phrases can reorder a sequence of phrases into ran-
dom order and back into the order corresponding
to the source language.
Since the search algorithm in Docent is
stochastic, repeated runs of the decoder will gen-
erally produce different output. However, the vari-
ance of the output is usually small, especially
when initialising with a DP search pass, and it
tends to be lower than the variance introduced
by feature weight tuning (Hardmeier et al, 2012;
Stymne et al, 2013a).
3 Available Feature Models
In its current version, Docent implements a selec-
tion of sentence-local feature models that makes
it possible to build a baseline system with a con-
figuration comparable to that of a typical Moses
baseline system. The published source code
also includes prototype implementations of a few
document-level models. These models should be
considered work in progress and serve as a demon-
stration of the cross-sentence modelling capabilit-
ies of the decoder. They have not yet reached a
state of maturity that would make them suitable
for production use.
The sentence-level models provided by Docent
include the phrase table, n-gram language models
implemented with the KenLM toolkit (Heafield,
2011), an unlexicalised distortion cost model with
geometric decay (Koehn et al, 2003) and a word
penalty cost. All of these features are designed
to be compatible with the corresponding features
in Moses. From among the typical set of baseline
features in Moses, we have not implemented the
194
lexicalised distortion model, but this model could
easily be added if required. Docent uses the same
binary file format for phrase tables as Moses, so
the same training apparatus can be used.
DP-based SMT decoders have a parameter
called distortion limit that limits the difference in
word order between the input and the MT out-
put. In DP search, this is formally considered to
be a parameter of the search algorithm because it
affects the algorithmic complexity of the search
by controlling how many translation options must
be considered at each hypothesis expansion. The
stochastic search algorithm in Docent does not re-
quire this limitation, but it can still be useful be-
cause the standard models of SMT do not model
long-distance reordering well. Docent therefore
includes a separate indicator feature to indicate
a violated distortion limit. In conjunction with a
very large weight, this feature can effectively en-
sure that the distortion limit is enforced. In con-
trast with the distortion limit parameter of a DP de-
coder, the weight of our distortion limit feature can
potentially be tuned to permit occasional distor-
tion limit violations when they contribute to better
translations.
The document-level models included in Docent
include a length parity model, a semantic lan-
guage model as well as a collection of document-
level readability models. The length parity model
is a proof-of-concept model that ensures that all
sentences in a document have either consistently
odd or consistently even length. It serves mostly as
a template to demonstrate how a simple document-
level model can be implemented in the decoder.
The semantic language model was originally pro-
posed by Hardmeier et al (2012) to improve lex-
ical cohesion in a document. It is a cross-sentence
model over sequences of content words that are
scored based on their similarity in a word vector
space. The readability models serve to improve
the readability of the translation by encouraging
the selection of easier and more consistent target
words. They are described and demonstrated in
more detail in section 5.
Docent can read input files both in the NIST-
XML format commonly used to encode docu-
ments in MT shared tasks such as NIST or WMT
and in the more elaborate MMAX format (M?ller
and Strube, 2003). The MMAX format makes
it possible to include a wide range of discourse-
level corpus annotations such as coreference links.
These annotations can then be accessed by the
feature models. To allow for additional target-
language information such as morphological fea-
tures of target words, Docent can handle simple
word-level annotations that are encoded in the
phrase table in the same way as target language
factors in Moses.
In order to optimise feature weights we have
adapted the Moses tuning infrastructure to Do-
cent. In this way we can take advantage of all its
features, for instance using different optimisation
algorithms such as MERT (Och, 2003) or PRO
(Hopkins and May, 2011), and selective tuning of
a subset of features. Since document features only
give meaningful scores on the document level and
not on the sentence level, we naturally perform
optimisation on document level, which typically
means that we need more data than for the op-
timisation of sentence-based decoding. The res-
ults we obtain are relatively stable and competit-
ive with sentence-level optimisation of the same
models (Stymne et al, 2013a).
4 Implementing Feature Models
Efficiently
While translating a document, the local search de-
coder attempts to make a great number of moves.
For each move, a score must be computed and
tested against the acceptance criterion. An over-
whelming majority of the proposed moves will be
rejected. In order to achieve reasonably fast de-
coding times, efficient scoring is paramount. Re-
computing the scores of the whole document at
every step would be far too slow for the decoder
to be useful. Fortunately, score computation can
be sped up in two ways. Knowledge about how
the state to be scored was generated from its pre-
decessor helps to limit recomputations to a min-
imum, and by adopting a two-step scoring proced-
ure that just computes the scores that can be calcu-
lated with little effort at first, we need to compute
the complete score only if the new state has some
chance of being accepted.
The scores of SMT feature models can usu-
ally be decomposed in some way over parts of
the document. The traditional models borrowed
from sentence-based decoding are necessarily de-
composable at the sentence level, and in practice,
all common models are designed to meet the con-
straints of DP beam search, which ensures that
they can in fact be decomposed over even smal-
195
ler sequences of just a few words. For genuine
document-level features, this is not the case, but
even these models can often be decomposed in
some way, for instance over paragraphs, anaphoric
links or lexical chains. To take advantage of this
fact, feature models in Docent always have access
to the previous state and its score and to a list of
the state modifications that transform the previous
state into the next. The scores of the new state are
calculated by identifying the parts of a document
that are affected by the modifications, subtract-
ing the old scores of this part from the previous
score and adding the new scores. This approach
to scoring makes feature model implementation
a bit more complicated than in DP search, but it
gives the feature models full control over how they
decompose a document while still permitting effi-
cient decoding.
A feature model class in Docent implements
three methods. The initDocument method is called
once per document when decoding starts. It
straightforwardly computes the model score for
the entire document from scratch. When a state
is modified, the decoder first invokes the estim-
ateScoreUpdate method. Rather than calculating
the new score exactly, this method is only required
to return an upper bound that reflects the max-
imum score that could possibly be achieved by this
state. The search algorithm then checks this upper
bound against the acceptance criterion. Only if the
upper bound meets the criterion does it call the
updateScore method to calculate the exact score,
which is then checked against the acceptance cri-
terion again.
The motivation for this two-step procedure is
that some models can compute an upper bound ap-
proximation much more efficiently than an exact
score. For any model whose score is a log probab-
ility, a value of 0 is a loose upper bound that can
be returned instantly, but in many cases, we can do
much better. In the case of the n-gram language
model, for instance, a more accurate upper bound
can be computed cheaply by subtracting from the
old score all log-probabilities of n-grams that are
affected by the state modifications without adding
the scores of the n-grams replacing them in the
new state. This approximation can be calculated
without doing any language model lookups at all.
On the other hand, some models like the distor-
tion cost or the word penalty are very cheap to
compute, so that the estimateScoreUpdate method
can simply return the precise score as a tight up-
per bound. If a state gets rejected because of a
low score on one of the cheap models, this means
we will never have to compute the more expensive
feature scores at all.
5 Readability: A Case Study
As a case study we report initial results on how
document-wide features can be used in Docent in
order to improve the readability of texts by encour-
aging simple and consistent terminology (Stymne
et al, 2013b). This work is a first step towards
achieving joint SMT and text simplification, with
the final goal of adapting MT to user groups such
as people with reading disabilities.
Lexical consistency modelling for SMT has
been attempted before. The suggested approaches
have been limited by the use of sentence-level
decoders, however, and had to resort to proced-
ures like post processing (Carpuat, 2009), multiple
decoding runs with frozen counts from previous
runs (Ture et al, 2012), or cache-based models
(Tiedemann, 2010). In Docent, however, we al-
ways have access to a full document translation,
which makes it straightforward to include features
directly into the decoder.
We implemented four features on the document
level. The first two features are type token ra-
tio (TTR) and a reformulation of it, OVIX, which
is less sensitive to text length. These ratios have
been related to the ?idea density? of a text (M?h-
lenbock and Kokkinakis, 2009). We also wanted
to encourage consistent translations of words, for
which we used the Q-value (Del?ger et al, 2006),
which has been proposed to measure term qual-
ity. We applied it on word level (QW) and phrase
level (QP). These features need access to the full
target document, which we have in Docent. In ad-
dition, we included two sentence-level count fea-
tures for long words that have been used to meas-
ure the readability of Swedish texts (M?hlenbock
and Kokkinakis, 2009).
We tested our features on English?Swedish
translation using the Europarl corpus. For train-
ing we used 1,488,322 sentences. As test data, we
extracted 20 documents with a total of 690 sen-
tences. We used the standard set of baseline fea-
tures: 5-gram language model, translation model
with 5 weights, a word penalty and a distortion
penalty.
196
Baseline Readability features Comment
de ?rade ledam?terna (the honourable
Members)
ledam?terna (the members) / ni
(you)
+ Removal of non-essential words
p? ett s?dant s?tt att (in such a way
that)
s? att (so that) + Simplified expression
gemenskapslagstiftningen (the
community legislation)
gemenskapens lagstiftning (the
community?s legislation)
+ Shorter words by changing long
compound to genitive construction
V?rldshandelsorganisationen (World
Trade Organisation)
WTO (WTO) ? Changing long compound to
English-based abbreviation
handlingsplanen (the action plan) planen (the plan) ? Removal of important word
?gnat s?rskild uppm?rksamhet ?t (paid
particular attention to)
s?rskilt uppm?rksam p?
(particular attentive on)
? Bad grammar because of changed
part of speech and missing verb
Table 2: Example translation snippets with comments
Feature BLEU OVIX LIX
Baseline 0.243 56.88 51.17
TTR 0.243 55.25 51.04
OVIX 0.243 54.65 51.00
QW 0.242 57.16 51.16
QP 0.243 57.07 51.06
All 0.235 47.80 49.29
Table 1: Results for adding single lexical consist-
ency features to Docent
To evaluate our system we used the BLEU score
(Papineni et al, 2002) together with a set of read-
ability metrics, since readability is what we hoped
to improve by adding consistency features. Here
we used OVIX to confirm a direct impact on con-
sistency, and LIX (Bj?rnsson, 1968), which is a
common readability measure for Swedish. Unfor-
tunately we do not have access to simplified trans-
lated text, so we calculate the MT metrics against a
standard reference, which means that simple texts
will likely have worse scores than complicated
texts closer to the reference translation.
We tuned the standard features using Moses and
MERT, and then added each lexical consistency
feature with a small weight, using a grid search ap-
proach to find values with a small impact. The res-
ults are shown in Table 1. As can be seen, for in-
dividual features the translation quality was main-
tained, with small improvements in LIX, and in
OVIX for the TTR and OVIX features. For the
combination we lost a little bit on translation qual-
ity, but there was a larger effect on the readability
metrics. When we used larger weights, there was
a bigger impact on the readability metrics, with a
further decrease on MT quality.
We also investigated what types of changes the
readability features could lead to. Table 2 shows a
sample of translations where the baseline is com-
pared to systems with readability features. There
are both cases where the readability features help
and cases where they are problematic. Overall,
these examples show that our simple features can
help achieve some interesting simplifications.
There is still much work to do on how to take
best advantage of the possibilities in Docent in or-
der to achieve readable texts. This attempt shows
the feasibility of the approach. We plan to ex-
tend this work for instance by better feature op-
timisation, by integrating part-of-speech tags into
our features in order to focus on terms rather than
common words, and by using simplified texts for
evaluation and tuning.
6 Conclusions
In this paper, we have presented Docent, an open-
source document-level decoder for phrase-based
SMT released under the GNU General Public Li-
cense. Docent is the first decoder that permits the
inclusion of feature models with unrestricted de-
pendencies between arbitrary parts of the output,
even crossing sentence boundaries. A number of
research groups have recently started to investig-
ate the interplay between SMT and discourse-level
phenomena such as pronominal anaphora, verb
tense selection and the generation of discourse
connectives. We expect that the availability of a
document-level decoder will make it substantially
easier to leverage discourse information in SMT
and make SMT models explore new ground bey-
ond the next sentence boundary.
References
Carl-Hugo Bj?rnsson. 1968. L?sbarhet. Liber, Stock-
holm.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 19?27, Boulder, Colorado.
197
Louise Del?ger, Magnus Merkel, and Pierre Zweigen-
baum. 2006. Enriching medical terminologies: an
approach based on aligned corpora. In International
Congress of the European Federation for Medical
Informatics, pages 747?752, Maastricht, The Neth-
erlands.
Zhengxian Gong, Min Zhang, Chew Lim Tan, and
Guodong Zhou. 2012. N-gram-based tense models
for statistical machine translation. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 276?285,
Jeju Island, Korea.
Liane Guillou. 2012. Improving pronoun translation
for statistical machine translation. In Proceedings of
the Student Research Workshop at the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 1?10, Avignon,
France.
Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In Proceedings of the seventh In-
ternational Workshop on Spoken Language Transla-
tion (IWSLT), pages 283?289, Paris, France.
Christian Hardmeier, Joakim Nivre, and J?rg
Tiedemann. 2012. Document-wide decoding
for phrase-based statistical machine translation.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Processing
and Computational Natural Language Learning,
pages 1179?1190, Jeju Island, Korea.
Christian Hardmeier. 2012. Discourse in statistical
machine translation: A survey and a case study. Dis-
cours, 11.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1352?1362, Edinburgh, Scotland.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 conference of the North Amer-
ican chapter of the Association for Computational
Linguistics on Human Language Technology, pages
48?54, Edmonton.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for Statistical Ma-
chine Translation. In Annual meeting of the Associ-
ation for Computational Linguistics: Demonstration
session, pages 177?180, Prague, Czech Republic.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2007. A greedy decoder for phrase-based statist-
ical machine translation. In TMI-2007: Proceedings
of the 11th International Conference on Theoretical
and Methodological Issues in Machine Translation,
pages 104?113, Sk?vde, Sweden.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statist-
ical Machine Translation and MetricsMATR, pages
252?261, Uppsala, Sweden.
Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,
and Andrea Gesmundo. 2012. Machine translation
of labeled discourse connectives. In Proceedings of
the Tenth Biennial Conference of the Association for
Machine Translation in the Americas (AMTA), San
Diego, California, USA.
Katarina M?hlenbock and Sofie Johansson Kokkinakis.
2009. LIX 68 revisited ? an extended readability. In
Proceedings of the Corpus Linguistics Conference,
Liverpool, UK.
Christoph M?ller and Michael Strube. 2003. Multi-
level annotation in MMAX. In Proceedings of the
Fourth SIGdial Workshop on Discourse and Dia-
logue, pages 198?207, Sapporo, Japan.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167, Sapporo, Ja-
pan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, USA.
Sara Stymne, Christian Hardmeier, J?rg Tiedemann,
and Joakim Nivre. 2013a. Feature weight optim-
ization for discourse-level SMT. In Proceedings of
the Workshop on Discourse in Machine Translation
(DiscoMT), Sofia, Bulgaria.
Sara Stymne, J?rg Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013b. Statistical machine trans-
lation with readability constraints. In Proceedings of
the 19th Nordic Conference of Computational Lin-
guistics (NODALIDA 2013), pages 375?386, Oslo,
Norway.
J?rg Tiedemann. 2010. Context adaptation in stat-
istical machine translation using models with ex-
ponentially decaying cache. In Proceedings of the
ACL 2010 Workshop on Domain Adaptation for Nat-
ural Language Processing (DANLP), pages 8?15,
Uppsala, Sweden.
Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 417?426, Montr?al, Canada.
198
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 17?25
Manchester, UK. August 2008
Simple is Best: Experiments with Different Document Segmentation
Strategies for Passage Retrieval
Jo?rg Tiedemann
Information Science
University of Groningen
j.tiedemann@rug.nl
Jori Mur
Information Science
University of Groningen
j.mur@rug.nl
Abstract
Passage retrieval is used in QA to fil-
ter large document collections in order
to find text units relevant for answering
given questions. In our QA system we ap-
ply standard IR techniques and index-time
passaging in the retrieval component. In
this paper we investigate several ways of
dividing documents into passages. In par-
ticular we look at semantically motivated
approaches (using coreference chains and
discourse clues) compared with simple
window-based techniques. We evaluate
retrieval performance and the overall QA
performance in order to study the impact
of the different segmentation approaches.
From our experiments we can conclude
that the simple techniques using fixed-
sized windows clearly outperform the se-
mantically motivated approaches, which
indicates that uniformity in size seems to
be more important than semantic coher-
ence in our setup.
1 Introduction
Passage retrieval in question answering is differ-
ent from information retrieval in general. Extract-
ing relevant passages from large document col-
lections is only one step in answering a natural
language question. There are two main differ-
ences: i) Passage retrieval queries are generated
from complete sentences (questions) compared to
bag-of-keyword queries usually used in IR. ii) Re-
trieved passages have to be processed further in or-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
der to extract concrete answers to the given ques-
tion. Hence, the size of the passages retrieved is
important and smaller units are preferred. Here,
the division of documents into passages is crucial.
The textual units have to be big enough to en-
sure IR works properly and they have to be small
enough to enable efficient and accurate QA. In this
study we investigate whether semantically moti-
vated passages in the retrieval component lead to
better QA performance compared to the use of
document retrieval and window-based segmenta-
tion approaches.
1.1 Index-time versus Search-time Passaging
In this paper, we experiment with various possi-
bilities of dividing documents into passages before
indexing them. This is also called index-time pas-
saging and refers to a one-step process of retriev-
ing appropriate textual units for subsequent an-
swer extraction modules (Roberts and Gaizauskas,
2004; Greenwood, 2004). This is in contrast to
other strategies using a two-step procedure consist-
ing of document retrieval and search-time passag-
ing thereafter. Here, we can distinguish between
approaches that only return one passage per rel-
evant document (see, for example, (Robertson et
al., 1992)) and the ones that allow multiple pas-
sages per document (see, for example (Moldovan
et al, 2000)). In general, allowing multiple pas-
sages per document is preferable for QA as possi-
ble answers can be contained at various positions
in a document (Roberts and Gaizauskas, 2004).
For this, an index-time approach has the advan-
tage that the retrieval of multiple passages per doc-
uments is straightforward because all of them com-
pete which each other in the same index using the
same metric for ranking.
A comparison between index-time and search-
17
time passaging has been carried out in (Roberts
and Gaizauskas, 2004). In their experiments,
index-time passaging performs similarly to search-
time passaging in terms of coverage and redun-
dancy (measures which have been introduced in
the same paper; see section 4.2 for more informa-
tion). Significant differences between the various
approaches can only be observed in redundancy on
higher ranks (above 50). However, as we will see
later in our experiments (section 4.2), redundancy
is not as important as coverage for our QA system
. Furthermore, retrieving more than about 40 pas-
sages does not produce significant improvements
of the QA system anymore but slows down the pro-
cessing time substantially.
Another argument for our focus on a one-step
retrieval procedure can be taken from (Tellex et al,
2003). In this paper, the authors do not actually use
any index-time passaging approach but compare
various search-time passage retrieval algorithms.
However, they obtain a huge performance differ-
ence when applying an oracle document retriever
(only returning relevant documents in the first re-
trieval step) instead of a standard IR engine. Com-
pared to this, the differences between the various
passage retrieval approaches tested is very small.
From this we can conclude that much improve-
ment can be gained by improving the initial re-
trieval step, which seems to be the bottleneck in the
entire process. Unfortunately, the authors do not
compare their results with index-time approaches.
However, looking at the potential gain in document
retrieval and keeping in mind that the performance
of index-time and search-time approaches is rather
similar (as we have discussed earlier) we believe
that the index-time approach is preferable.
1.2 Passages in IR
Certainly, IR performance is effected by chang-
ing the size of the units to be indexed. The task
in document segmentation for our index-time pas-
saging approach is to find the proper division of
documents into text passages which optimize the
retrieval in terms of overall QA performance.
The general advantages of passage retrieval over
full-text document retrieval has been investigated
in various studies, e.g., (Kaszkiel and Zobel, 2001;
Callan, 1994; Hearst and Plaunt, 1993; Kaszkiel
and Zobel, 1997). Besides the argument of de-
creasing the search space for subsequent answer
extraction modules in QA, passage retrieval also
improves standard IR techniques by ?normaliz-
ing? textual units in terms of size which is espe-
cially important in cases where documents come
from very diverse sources. IR is based on similar-
ity measures between documents and queries and
standard approaches have shortcomings when ap-
plying them to documents of various sizes and text
types. Often there is a bias for certain types raising
problems of discrimination between documents of
different lengths and content densities. Passages
on the other hand provide convenient units to be
returned to the user avoiding such ranking difficul-
ties (Kaszkiel and Zobel, 2001). For IR, passage-
level evidence may be incorporated into document
retrieval (Callan, 1994; Hearst and Plaunt, 1993)
or passages may be used directly as retrieval unit
(Kaszkiel and Zobel, 2001; Kaszkiel and Zobel,
1997). For QA only the latter is interesting and
will be applied in our experiments.
Passages can be defined in various ways. The
most obvious way is to use existing markup (ex-
plicit discourse information) to divide documents
into smaller units. Unfortunately, such markup is
not always available or ambiguous with other types
of separators. For example, headers, list elements
or table cells might be separated in the same way
(for example using an empty line) as discourse
related paragraphs. Also, the division into para-
graphs may differ a lot depending on the source
of the document. For example, Wikipedia entries
are divided on various levels into rather small units
whereas newspaper articles often include very long
paragraphs.
There are several ways of automatically divid-
ing documents into passages without relying on
existing markup. One way is to search for linguis-
tic clues that indicate a separation of consecutive
text blocks. These clues may include lexical pat-
terns and relations. We refer to such approaches
as semantically motivated document segmentation.
Another approach is to cut documents into arbi-
trary pieces ignoring any other type of informa-
tion. For example, we can use fixed-sized win-
dows to divide documents into passages of simi-
lar size. Such windows can be defined in terms of
words and characters (Kaszkiel and Zobel, 2001;
Monz, 2003) or sentences and paragraphs (Zobel
et al, 1995; Llopis et al, 2002). It is also possi-
ble to allow varying window sizes and overlapping
sections to be indexed (Kaszkiel and Zobel, 2001;
Monz, 2003). In this case it is up to the IR engine
18
to decide which of the competing window types is
preferred and it may even return overlapping sec-
tions multiple times.
In the following sections we will discuss
two techniques of semantically motivated docu-
ment segmentation and compare them to simple
window-based techniques in terms of passage re-
trieval and QA performance.
2 Passage Retrieval in our QA system
Our QA system is an open-domain question an-
swering system for Dutch. It includes two
strategies: (1) A table-lookup strategy using fact
databases that have been created off-line, and, (2)
an ?on-line? answer extraction strategy with pas-
sage retrieval and subsequent answer identification
and ranking modules. We will only look at the
second strategy as we are interested in the passage
retrieval component and its impact on QA perfor-
mance.
The passage retrieval component is imple-
mented as an interface to several open-source IR
engines. The query is generated from the given
natural language question after question analysis.
Keywords are sent to the IR engine(s) and results
(in form of sentence IDs) are returned to the QA
system.
In the experiments described here, we apply
Zettair (Lester et al, 2006), an open-source IR en-
gine developed by the search engine group at the
RMIT University in Melbourne, Australia. It im-
plements a very efficient standard IR engine with
high retrieval performance according to our exper-
iments with various alternative systems. Zettair is
optimized for speed and is very efficient in both
indexing and retrieval. The outstanding speed in
indexing is very fortunate for our experiments in
which we had to create various indexes with dif-
ferent document segmentation strategies.
3 Document Segmentation
We now discuss the different methods for docu-
ment segmentation, starting with the semantically
motivated ones and then looking at the window-
based techniques.
3.1 Using Coreference Chains
Coreference is the relation which holds between
two NPs both of which are interpreted as refer-
ring to the same unique referent in the context
in which they occur ((Van Deemter and Kibble,
2000)). Since the coreference relation is an equiv-
alence relation and consequently a transitive rela-
tion chains of coreferring entities can be detected
in arbitrary documents. We can use these coref-
erence chains to demarcate passages in the text.
The assumption in this approach is that corefer-
ence chains mark semantically coherent passages,
which are good candidates for splitting up docu-
ments.
Figure 1 illustrates chains detected by a resolu-
tion system in five successive sentences.
1. [Jim McClements en Susan Sandvig-Shobe]
i
hebben
een onrechtmatig argument gebruikt.
2. [De Nederlandse scheidsrechter]
j
[Jacques de Koning]
j
bevestigt dit.
3. [Kuipers]
k
versloeg zondag in een rechtstreeks duel
[Shani Davis]
m
.
4. Toch werd [hij]
k
in de rangschikking achter [de
Amerikaan]
m
geklasseerd.
5. [De twee hoofdarbiters]
i
verklaarden dat [Kuipers?]
k
voorste schaats niet op de grond stond.
Cluster i (1,5): [Jim McClements en Susan Sandvig-Shobe]
[De twee hoofdarbiters]
Cluster j (2): [De Nederlandse scheidsrechter]
[Jacques de Koning]
Cluster k (3-5): [Kuipers] [hij] [Kuipers?]
Cluster m (3,4): [Shani Davis] [de Amerikaan]
Figure 1: Example of coreference chains used for
document segmentation
The coreferential units can then be used to form
passages consisting of all sentences the corefer-
ence chain spans over, i.e. the boundaries of pas-
sages are sentences containing the first occurrence
of the referent and the last occurrence of a refer-
ent. Thus, in the example in figure 1 we obtain
four passages: 1) sentence one to sentence five, 2)
sentence two, 3) sentence three to five, and, 4) sen-
tence three and four. Note that such passages can
be included in others and may overlap with yet oth-
ers. Furthermore, there might be sentences which
are not included in any chain which have to be han-
dled by some other techniques.
For our purposes we used our own coreference
resolution system which is based on information
derived from Alpino, a wide-coverage dependency
parser for Dutch (van Noord, 2006). We ap-
proached the task of coreference resolution as a
19
clustering-based ranking task. Some NP pairs are
more likely to be coreferent than others. The sys-
tem ranks possible antecedents for each anaphor
considering syntactic features, semantic features
and surface structure features from the anaphor
and the candidate itself, as well as features from
the cluster to which the candidate belongs. It picks
the most likely candidate as the coreferring an-
tecedent.
References relations are detected between pro-
nouns, common nouns and named entities. The
resolution system yields a precision of 67.9% and
a recall of 45.6% (F-score = 54.5%) using MUC
scores (Vilain et al, 1993) on the annotated test
corpus developed by (Hoste, 2005) which consist
of articles taken from KNACK, a Flemish weekly
news magazine.
3.2 TextTiling
TextTiling is a well-known algorithm for segment-
ing texts into subtopic passages (Hearst, 1997).
It is based on the assumption that a significant
portion of a set of lexical items in use during
the course of a given subtopic discussion changes
when that subtopic in the text changes.
Topic shifts are found by searching for lexi-
cal co-occurrence patterns and comparing adja-
cent blocks. First the text is subdivided into
pseudo-sentences of a predefined size rather than
using syntactically-determined sentences. These
pseudo-sentences are called token-sequences by
Hearst.
The algorithm identifies discourse boundaries
by calculating a score for each token-sequence
gap. This score is based on two methods,
block comparison and vocabulary introduction.
The block comparison method compares adjacent
blocks of text to see how similar they are accord-
ing to how many words the adjacent blocks have
in common. The vocabulary introduction method
is based on how many new words were seen in
the interval in which the token-sequence gap is the
midpoint.
The boundaries are assumed to occur at the
largest valleys in the graph that results from plot-
ting the token-sequences against their scores. In
this way the algorithm produces a flat subtopic
structure from a given document.
3.3 Window-based
The simplest way of dividing documents into pas-
sages is to use a fixed-sized window. Here we
do not take any discourse information nor seman-
tic clue into account but split documents at arbi-
trary positions. Windows can be defined in various
ways, in terms of characters, words or sentences.
In our case it is important to keep sentences to-
gether because of the answer extraction compo-
nent in our QA system that works on that level
and expects complete sentences. Window-based
segmentation techniques may be applied with var-
ious amounts of overlaps. The simplest method is
to split documents into passages in a greedy way,
starting a new passage immediately after the pre-
vious one (and starting the entire process at the be-
ginning of each document)1. Another method is to
allow some overlap between consecutive passages,
i.e. starting a new passage at some position within
the previous one. If we use the maximum possible
overlap such an approach is usually called a ?slid-
ing window? in which the difference between two
consecutive passages is only two basic units (sen-
tences) - the first and the last one.
4 Experiments
4.1 Setup
For our experiments we applied the Dutch news-
paper corpus used at the QA track at CLEF, the
cross-language evaluation forum. It contains about
190,000 documents consisting of about 4,000,000
sentences (roughly 80 million words). As men-
tioned earlier, we applied the open-source IR en-
gine, Zettair, in our experiments and used a lan-
guage modeling metric with Dirichlet smoothing,
which is implemented in the system.
The evaluation is based on 778 Dutch CLEF
questions from the QA tracks in the years 2003 ?
2005 which are annotated with their answers. We
use simple matching of possible answer strings to
determine if a passage is relevant for finding an
accepted answer or not. Similarly, answer string
matching is applied to evaluate the output of the
entire QA system; i.e. an answer by the system
is counted as correct if it is identical to one of the
accepted answer strings without looking at the sup-
porting sentence/passage. For evaluation we used
the standard measure of MRR which is defined as
follows:
1Note that in our approach we still keep the document
boundaries intact, i.e. the segmentation ends at the end of
each document and starts from scratch at the beginning of the
next document. In this way, the last passage in a document
may be smaller than the pre-defined fixed size.
20
MRR
QA
=
1
N
N
?
1
1
rank(first correct answer)
Using the string matching strategy for evalu-
ation this corresponds to the lenient MRR mea-
sures frequently used in the literature. Strict MRR
scores (requiring a match with supporting docu-
ments) is less appropriate for our data coming from
the CLEF QA tracks. In CLEF there are usually
only a few participants and, therefore, only a small
fraction of relevant documents are known for the
given questions.
4.2 Evaluation of Passage Retrieval
There are various metrics that can be employed for
evaluating passage retrieval. Commonly it is ar-
gued that passage retrieval for QA is merely a fil-
tering task and ranking (precision) is less impor-
tant than recall. Therefore, the measure of redun-
dancy has been introduced which is defined as the
average number of relevant passages retrieved per
question (independent of any ranking). Passage re-
trieval is, of course, a bottleneck in QA systems
that make use of such a component. The system
has no chance to find an answer if the retrieval en-
gine fails to return relevant passages. Therefore,
another measure, coverage is often used in combi-
nation with redundancy. It is defined as the pro-
portion of questions for which at least one relevant
passage is found. In order to validate the use of
these measures in our setup we experimented with
retrieving various amounts of paragraphs. Figure 2
illustrates the relation of coverage and redundancy
scores compared to the overall QA performance
measured in terms of MRR scores.
From the figure we can conclude that cover-
age is more important than redundancy in our sys-
tem. In other words, our QA system is quite good
in finding appropriate answers if there is at least
one relevant passage in the set of retrieved ones.
Redundancy on the other hand does not seem to
provide valuable insides for the end-to-end perfor-
mance of our QA system.
However, our system also uses the passage re-
trieval score (and, hence, the ranking) as a clue
for answer extraction. Therefore, other standard
IR measures might be interesting for our investi-
gations as well. The following three metrics are
common in the IR literature.
 0
 20
 40
 60
 80
 100
 0  20  40  60  80  100
co
ve
ra
ge
/M
RR
 (in
 %
)
number of paragraphs retrieved
IR coverage
re
du
nd
an
cy
2.5
5
7.5
10
IR redundancy       QA MRR
Figure 2: The correlation between coverage and
redundancy and MRR
QA
with varying numbers
of paragraphs retrieved. Note that redundancy and
coverage use different scales on the y-axis which
makes them not directly comparable. The inten-
tion of this plot is to illustrate the tendency of both
measures in comparison with QA performance.
Mean average precision (MAP): Average of
precision scores for top k documents; MAP
is the mean of these averages over all the N
queries.
MAP =
1
N
N
?
n=1
1
K
K
?
k=1
P
n
(1..k)
(P
n
(1..k) is the precision of the top k docu-
ments retrieved for query q
n
)
Uninterpolated average precision (UAP):
Average of precision scores at each relevant
document retrieved; UAP is the mean of
these averages over the N queries.
UAP =
1
N
N
?
n=1
1
|D
n
r
|
?
k:d
k
?D
n
r
P
n
(1..k)
(Dn
r
is the set of relevant documents among
the ones retrieved for question n)
Mean reciprocal ranks: The mean of the recip-
rocal rank of the first relevant passage re-
trieved.
MRR
IR
=
1
N
N
?
1
1
rank(first relevant passage)
In figure 3 the correlation of these measures with
the overall QA performance is illustrated.
21
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 0  20  40  60  80  100
M
R
R
/U
AP
/M
AP
 (in
 %
)
number of paragraphs retrieved
  IR MRR
  IR UAP
  IR MAP QA MRR
Figure 3: The correlation between IR evaluation
measures (MAP , UAP and MRR
IR
) and QA
evaluation scores (MRR
QA
) with varying num-
bers of paragraphs retrieved.
From the picture we can clearly see that the
MRR
IR
scores correlate the most with the QA
evaluation scores when retrieving different num-
bers of paragraphs. This, again, confirms the im-
portance of coverage as the MRR
IR
score only
takes the first relevant passage into account and ig-
nores the fact that there might be more answers
to be found in lower ranked passages. Hence,
MRR
IR
seems to be a good measure that com-
bines coverage with an evaluation of the rank-
ing and, therefore, we will use it as our main IR
evaluation metric instead of coverage, redundancy,
MAP & UAP.
4.3 Baselines
The CLEF newspaper corpus comes with para-
graph markup which can easily be used as the seg-
mentation granularity for passage retrieval. Table
1 shows the scores obtained by different baseline
retrieval approaches using either sentences, para-
graphs or documents as base units.
We can see from the results that document re-
trieval (used for QA) is clearly outperformed by
both sentence and paragraph retrieval. Surpris-
ingly, sentence retrieval works even better than
paragraph retrieval when looking at the QA per-
formance even though all IR evaluation measures
(cov, red, MRR
IR
) suggest a lower score. Note
that MRR
IR
is almost as good as MRR
QA
for
sentence retrieval whereas the difference between
them is quite large for the other settings. This indi-
cates the importance of narrowing down the search
space for the answer extraction modules. The
MRR
#sent cov red IR QA CLEF
sent 16,737 0.784 2.95 0.490 0.487 0.430
par 80,046 0.842 4.17 0.565 0.483 0.416
doc 618,865 0.877 6.13 0.666 0.457 0.387
Table 1: Baselines with sentence (sent), paragraph
(par) and document (doc) retrieval (20 units).
MRR
QA
is measured on the top 5 answers re-
trieved. CLEF is the accuracy of the QA system
measured on the top answer provided by the sys-
tem. cov refers to coverage and red refers to redun-
dancy. #sent gives the total number of sentences
included in the retrieved text units to give an im-
pression about the amount of text to be processed
by subsequent answer extraction modules.
amount of data to be processed is much smaller
for sentence retrieval than for the other two while
coverage is still reasonably high. The CLEF scores
(accuracy measured on the top answer provided by
the system) follow the same pattern. Here, the dif-
ference between sentence retrieval and document
retrieval is even more apparent.
Certainly, the success of the retrieval compo-
nent depends on the metric used for ranking doc-
uments as implemented in the IR engine. In or-
der to verify the importance of document seg-
mentation in a QA setting we also ran experi-
ments with another standard metric implemented
in Zettair, the Okapi BM-25 metric (Robertson et
al., 1992). Similar to the previous setting using
the LM metric, QA with paragraph retrieval (now
yielding MRR
QA
= 0.460) outperforms QA with
document retrieval (MRR
QA
= 0.449). How-
ever, sentence retrieval does not perform as well
(MRR
QA
= 0.420) which suggests that the Okapi
metric is not suited for very small retrieval units.
Still, the success of paragraph retrieval supports
the advantage of passage retrieval compared to
document retrieval and suggests potential QA per-
formance gains with improved document segmen-
tation strategies. In the remaining we only report
results using the LM metric for retrieval due to its
superior performance.
4.4 Semantically Motivated Passages
As described earlier, coreference chains can be
used to extract semantically coherent passages
from textual documents. In our experiments we
used several settings for the integration of such
passages in the retrieval engine. First of all, coref-
22
erence chains have been used as the only way
of forming passages. Sentences which are not
included in any passage are included as single-
sentence passages. This settings is referred to as
sent/coref.
In the second setting we restrict the passages in
length. Coreference chains can be arbitrary long
and, as we can see in the results in table 2, the
IR engine tends to prefer long passages which is
not desirable in the QA setting. Hence, we define
the constraint that passages have to be longer than
200 characters and shorter than 1000. This setup is
referred to as sent/coref (200-1000).
In the third setting we combine paragraphs (us-
ing existing markup) and coreference chain pas-
sages including the length restriction. This is
mainly to get rid of the single-sentence passages
included in the previous settings. Note that all
paragraphs are used even if all sentences within
them are included in coreferential passages. Note
also that in all settings passages may refer to
overlapping text units as coreference chains may
stretch over various overlapping passages of a doc-
ument.
We did not perform an exhaustive optimization
of the length restriction. However, we experi-
mented with various settings and 200-1000 was the
best performing one in our experiments. For illus-
tration we include one additional experiment using
a slightly different length constraint (200-400) in
table 2.
For the document segmentation strategy us-
ing TextTiling we used a freely available im-
plementation of that algorithm (the Perl Module
Lingua::EN::Segmenter::TextTiling available
at CPAN). Note that we do not include other pas-
sages in this approach (paragraphs using existing
markup nor single-sentence passages).
Table 2 summarizes the scores obtained by the
various settings when applied for passage retrieval
and when embedded into the QA system.
It is worth noting that including coreferential
chains without length restriction forced the re-
trieval engine to return a lot of very long passages
which resulted in a degraded QA performance
(also in terms of processing time which is not
shown here). The combination of paragraphs and
coreferential passages with length restrictions pro-
duced MRR
QA
scores above the baseline. How-
ever, these improvements are not statistically sig-
nificant according to the Wilcoxon matched-pair
MRR
#sent IR QA CLEF
sent/coref 490,968 0.604 0.469 0.405
sent/coref (200-1000) 76,865 0.535 0.462 0.395
par+coref (200-1000) 82,378 0.560 0.493 0.426
par+coref (200-400) 67,580 0.555 0.489 0.422
TextTiling 107,879 0.586 ? 0.503 0.434
Table 2: Passage retrieval with document segmen-
tation using coreference chains and TextTiling (re-
trieving a maximum of 20 passages; ? means sig-
nificant with p < 0.05 and Wilcoxon Matched-pair
Signed-Ranks Test compared to paragraph base-
line ? only tested for MRR
QA
)
signed-ranks test and looking at the corresponding
CLEF scores we can even see a slight drop in per-
formance. Applying TextTiling yielded improved
scores in both passage retrieval and QA perfor-
mance (MRR
QA
and CLEF). The MRR
QA
im-
provement is statistically significant according to
the same test.
4.5 Window-based Passages
In comparison to the semantically motivated pas-
sages discussed above we also looked at simple
window-based passages as described earlier. Here
we do not consider any linguistic clues for divid-
ing the documents besides the sentence and docu-
ment boundaries. Table 3 summarizes the results
obtained for various fixed-sized windows used for
document segmentation.
MRR
#sent IR QA CLEF
2 sentences 33468 0.545 ? 0.506 0.443
3 sentences 50190 0.554 0.504 0.436
4 sentences 66800 0.581 ? 0.512 0.447
5 sentences 83575 0.588 0.493 0.422
6 sentences 100110 0.583 0.489 0.423
7 sentences 116872 0.572 0.491 0.422
8 sentences 133504 0.577 0.481 0.409
9 sentences 150156 0.578 0.475 0.405
10 sentences 166810 0.596 0.470 0.396
Table 3: Passage retrieval with window-based doc-
ument segmentation (? means significant with
p < 0.05 and Wilcoxon Matched-pair Signed-
Ranks Test)
Surprisingly, we can see that window-based seg-
mentation approaches with small sizes between 2
and 7 sentences yield improved scores compared
to the baseline. Two of the improvements (using
2-sentence passages and 4-sentence passages) are
statistically significant. Three settings also out-
23
perform the best semantically motivated segmen-
tation approach. This result was unexpected espe-
cially considering the naive way of splitting docu-
ments into parts disregarding any discourse struc-
ture (besides document boundaries) and other se-
mantic clues.
We did another experiment using window-based
segmentation and a sliding window approach.
Here, fixed-sized passages are included starting at
every point in the document and, hence, various
overlapping passages are included in the index. In
this way we split documents at various points and
leave it to the IR engine to select the most ap-
propriate ones for a given query. The results are
shown in table 4.
MRR
#sent IR QA CLEF
2 sent (sliding) 29095 0.548 ? 0.516 0.456
3 sent (sliding) 36415 0.549 0.484 0.411
4 sent (sliding) 41565 0.546 0.476 0.409
5 sent (sliding) 45737 0.534 0.465 0.403
6 sent (sliding) 49091 0.528 0.454 0.390
7 sent (sliding) 51823 0.529 0.439 0.372
8 sent (sliding) 54600 0.535 0.428 0.360
9 sent (sliding) 57071 0.531 0.420 0.351
10 sent (sliding) 59352 0.542 0.420 0.354
Table 4: Passage retrieval with window-based doc-
ument segmentation and a sliding window
Again, we see a significant improvement with
2-sentence passages (the overall best score so far)
but the performance degrades when increasing the
window size. Note that the number of sentences re-
trieved is growing very slowly for larger windows.
This is because more and more of the overlapping
regions are retrieved and, hence, the total number
of unique sentences does not grow with the size
of the window as we have seen in the non-sliding
approach.
5 Discussion & Conclusions
Our experiments show that passage retrieval is in-
deed different to general document retrieval. Im-
proved retrieval scores do not necessarily lead to
better QA performance. Important for QA is to
reduce the search space for subsequent answer ex-
traction modules and, hence, passage retrieval has
to balance retrieval accuracy and retrieval size. In
our setup it seems to be preferable to return very
small units with a reasonable coverage. For this,
index-time passaging is very effective.
In this study we were especially interested in se-
mantically motivated approaches to document seg-
mentation. In particular, two techniques have been
investigated, one using the well-known TextTil-
ing algorithm and one using coreference chains for
passage boundary detection. We compared them
to simple window-based techniques using various
sizes. From our experiments we can conclude that
simple document segmentation techniques using
small fixed-sized windows work best among the
ones tested here. Semantically motivated passages
in the retrieval component helped to slightly im-
prove QA performance but do not justify the effort
spent in producing them. One of the main reasons
for the failure of using coreference chains for seg-
mentation might be the fact that this approach pro-
duces many overlapping passages which does not
seem to be favorable for passage retrieval. This can
also be seen in the sliding window approach which
did not perform as well as the one without over-
lapping units (except for two-sentence passages).
In conclusion, uniformity in terms of length and
uniqueness (in terms of non-overlapping contents)
seem to be more important than semantic coher-
ence for one-step passage retrieval in QA. A fu-
ture direction could be to test an approach that bal-
ances both a uniform document segmentation and
semantic coherence.
References
Callan, James P. 1994. Passage-level evidence in doc-
ument retrieval. In SIGIR ?94: Proceedings of the
17th annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 302?310, New York, NY, USA. Springer-
Verlag New York, Inc.
Greenwood, Mark A. 2004. Using pertainyms to im-
prove passage retrieval for questions requesting in-
formation about a location. In Proceedings of the
Workshop on Information Retrieval for Question An-
swering (SIGIR 2004), Sheffield, UK.
Hearst, Marti A. and Christian Plaunt. 1993. Subtopic
structuring for full-length document access. In Re-
search and Development in Information Retrieval,
pages 59?68.
Hearst, Marti A. 1997. Texttiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Hoste, V. 2005. Optimization Issues in Machine Learn-
ing of Coreference Resolution. Ph.D. thesis, Univer-
sity of Antwerp.
Kaszkiel, Marcin and Justin Zobel. 1997. Passage re-
trieval revisited. In SIGIR ?97: Proceedings of the
20th annual international ACM SIGIR conference on
24
Research and development in information retrieval,
pages 178?185, New York, NY, USA. ACM Press.
Kaszkiel, Marcin and Justin Zobel. 2001. Ef-
fective ranking with arbitrary passages. Journal
of the American Society of Information Science,
52(4):344?364.
Lester, Nicholas, Hugh Williams, Justin Zobel, Falk
Scholer, Dirk Bahle, John Yiannis, Bodo von
Billerbeck, Steven Garcia, and William Web-
ber. 2006. The Zettair search engine.
http://www.seg.rmit.edu.au/zettair/.
Llopis, F., J. Vicedo, and A. Ferra?ndez. 2002. Pas-
sage selection to improve question answering. In
Proceedings of the COLING 2002 Workshop on Mul-
tilingual Summarization and Question Answering.
Moldovan, D., S. Harabagiu, M. Pasca, R. Mihalcea,
R. Girju, R. Goodrum, and V. Rus. 2000. The struc-
ture and performance of an open-domain question
answering system.
Monz, Christof. 2003. From Document Retrieval to
Question Answering. Ph.D. thesis, University of
Amsterdam.
Roberts, Ian and Robert Gaizauskas. 2004. Evaluating
passage retrieval approaches for question answering.
In Proceedings of 26th European Conference on In-
formation Retrieval.
Robertson, Stephen E., Steve Walker, Micheline
Hancock-Beaulieu, Aarron Gull, and Marianna Lau.
1992. Okapi at TREC-3. In Text REtrieval Confer-
ence, pages 21?30.
Tellex, S., B. Katz, J. Lin, A. Fernandes, and G. Marton.
2003. Quantitative evaluation of passage retrieval al-
gorithms for question answering. In Proceedings of
the SIGIR conference on Research and development
in informaion retrieval, pages 41?47. ACM Press.
Van Deemter, K. and R. Kibble. 2000. On coreferring:
Coreference in muc and related annotation schemes.
Computational Linguistics, 26(4):629?637.
van Noord, Gertjan. 2006. At Last Parsing Is Now
Operational. In TALN 2006 Verbum Ex Machina,
Actes De La 13e Conference sur Le Traitement
Automatique des Langues naturelles, pages 20?42,
Leuven.
Vilain, M., J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1993. A model-theoretic coreference
scoring scheme. In Proceedings of the 6th confer-
ence on Message understanding (MUC 6), pages 45?
52.
Zobel, Justin, Alistair Moffat, Ross Wilkinson, and Ron
Sacks-Davis. 1995. Efficient retrieval of partial doc-
uments. Information Processing and Management,
31(3):361?377.
25
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 50?57
Manchester, UK. August 2008
Using lexico-semantic information for query expansion in passage
retrieval for question answering
Lonneke van der Plas
LATL
University of Geneva
Switzerland
lonneke.vanderplas@lettres.unige.ch
Jo?rg Tiedemann
Alfa-Informatica
University of Groningen
The Netherlands
j.tiedemann@rug.nl
Abstract
In this paper we investigate the use of sev-
eral types of lexico-semantic information
for query expansion in the passage retrieval
component of our QA system. We have
used four corpus-based methods to acquire
semantically related words, and we have
used one hand-built resource. We eval-
uate our techniques on the Dutch CLEF
QA track.1 In our experiments expansions
that try to bridge the terminological gap
between question and document collection
do not result in any improvements. How-
ever, expansions bridging the knowledge
gap show modest improvements.
1 Introduction
Information retrieval (IR) is used in most QA sys-
tems to filter out relevant passages from large doc-
ument collections to narrow down the search for
answer extraction modules in a QA system. Accu-
rate IR is crucial for the success of this approach.
Answers in paragraphs that have been missed by
IR are lost for the entire QA system. Hence, high
performance of IR especially in terms of recall is
essential. Furthermore, high precision is desirable
as IR scores are used for answer extraction heuris-
tics and also to reduce the chance of subsequent
extraction errors.
Because the user?s formulation of the question
is only one of the many possible ways to state the
information need that the user might have, there is
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1The Cross-Language Evaluation Forum (http://clef-
qa.itc.it/)
often a discrepancy between the terminology used
by the user and the terminology used in the doc-
ument collection to describe the same concept. A
document might hold the answer to the user?s ques-
tion, but it will not be found due to the TERMI-
NOLOGICAL GAP. Moldovan et al (2002) show
that their system fails to answer many questions
(25.7%), because of the terminological gap, i.e.
keyword expansion would be desirable but is miss-
ing. Query expansion techniques have been devel-
oped to bridge this gap.
However, we believe that there is more than just
a terminological gap. There is also a KNOWLEDGE
GAP. Documents are missed or do not end up high
in the ranks, because additional world knowledge
is missing. We are not speaking of synonyms here,
but words belonging to the same subject field. For
example, when a user is looking for information
about the explosion of the first atomic bomb, in
his/her head a subject field is active that could in-
clude: war, disaster, World War II.
We have used three corpus-based methods
to acquire semantically related words: the
SYNTAX-BASED METHOD, the ALIGNMENT-
BASED METHOD, and the PROXIMITY-BASED
METHOD. The nature of the relations between
words found by the three methods is very differ-
ent. Ranging from free associations to synonyms.
Apart from these resources we have used cate-
gorised named entities, such as Van Gogh IS-A
painter and synsets from EWN as candidate ex-
pansion terms.
In this paper we have applied several types of
lexico-semantic information to the task of query
expansion for QA. We hope that the synonyms
retrieved automatically, and in particular the syn-
onyms retrieved by the alignment-based method,
as these are most precise, will help to overcome the
50
terminological gap. With respect to the knowledge
gap, we expect that the proximity-based method
would be most helpful as well as the list of cate-
gorised named entities. For example, knowing that
Monica Seles is a tennis player helps to find rele-
vant passages regarding this tennis star.
2 Related work
There are many ways to expand queries and ex-
pansions can be acquired from several sources.
For example, one can make use of collection-
independent resources, such as EWN. In contrast,
collection-dependent knowledge structures are of-
ten constructed automatically based on data from
the collection.
The results from using collection-independent,
hand-built sources are varied. Moldovan et al
(2003) show that using a lexico-semantic feed-
back loop that feeds lexico-semantic alternations
from WordNet as keyword expansions to the re-
trieval component of their QA system increments
the scores by 15%. Also, Pasc?a and Harabagiu
(2001) show substantial improvements when us-
ing lexico-semantic information from WordNet for
keyword alternation on the morphological, lexical
and semantic level. They evaluated their system on
question sets of TREC-8 and TREC-9. For TREC-
8 they reach a precision score of 55.3% with-
out including any alternations for question key-
words, 67.6% if lexical alternations are allowed
and 73.7% if both lexical and semantic alternations
are allowed.
However, Yang and Chua (2003) report that
adding additional terms from WordNet?s synsets
and glosses adds more noise than information to
the query. Also, Voorhees (1993) concludes that
expanding by automatically generated synonym
sets from EWN can degrade results.
In Yang et al (2003) the authors use external
knowledge extracted from WordNet and the Web
to expand queries for QA. Minor improvements
are attained when the Web is used to retrieve a
list of nearby (one sentence or snippet) non-trivial
terms. When WordNet is used to rank the retrieved
terms, the improvement is reduced. The best re-
sults are reached when structure analysis is added
to knowledge from the Web and WordNet. Struc-
ture analysis determines the relations that hold be-
tween the candidate expansion terms to identify
semantic groups. Semantic groups are then con-
nected by conjunction in the Boolean query.
Monz (2003) ran experiments using pseudo rel-
evance feedback for IR in a QA system. The author
reports dramatic decreases in performance. He ar-
gues that this might be due to the fact that there
are usually only a small number of relevant doc-
uments. Another reason he gives is the fact that
he used the full document to fetch expansion terms
and the information that allows one to answer the
question is expressed very locally.
A global technique that is most similar to ours
uses syntactic context to find suitable terms for
query expansion (Grefenstette, 1992; Grefenstette,
1994). The author reports that the gain is mod-
est: 2% when expanded with nearest neighbours
found by his system and 5 to 6%, when apply-
ing stemming and a second loop of expansions
of words that are in the family of the augmented
query terms.2 Although the gain is greater than
when using document co-occurrence as context,
the results are mixed, with expansions improving
some query results and degrading others.
Also, the approach by Qiu and Frei (1993) is
a global technique. They automatically construct
a similarity thesaurus, based on what documents
terms appear in. They use word-by-document ma-
trices, where the features are document IDs, to de-
termine the similarity between words. Expansions
are selected based on the similarity to the query
concept, i.e. all words in the query together, and
not based on the single words in the query inde-
pendently. The results they get are promising.
Pantel and Ravichandran (2004) have used a
method that is not related to query expansion,
but yet very related to our work. They have se-
mantically indexed the TREC-2002 IR collection
with the ISA-relations found by their system for
179 questions that had an explicit semantic answer
type, such as What band was Jerry Garcia with?
They show small gains in performance of the IR
output using the semantically indexed collection.
Recent work (Shen and Lapata, 2007; Kaisser
and Webber, 2007) that falls outside the scope of
this paper, but that is worth mentioning success-
fully applies semantic roles to question answering.
3 Lexico-semantic information
We have used several types of lexico-semantic
information as sources for candidate expansion
terms. The first three are automatically acquired
2i.e. words that appear in the same documents and that
share the first three, four or five letters.
51
from corpora by means of distributional methods.
? Nearest neighbours from proximity-based
distributional similarity
? Nearest neighbours from syntax-based distri-
butional similarity
? Nearest neighbours from alignment-based
distributional similarity
The idea behind distributional methods is rooted
in the DISTRIBUTIONAL HYPOTHESIS (Harris,
1968). Similar words appear in similar context.
The way words are distributed over contexts tells
us something about their meaning. Context can
be defined in several ways. The way the context
is defined determines the type of lexico-semantic
knowledge we will retrieve.
For example, one can define the context of a
word as the n words surrounding it. In that case
proximity to the head word is the determining
factor. We refer to these methods that use un-
structured context as PROXIMITY-BASED METH-
ODS. The nearest neighbours resulting from such
methods are rather unstructured as well. They are
merely associations between words, such as baby
and cry. We have used the 80 million-word corpus
of Dutch newspaper text (the CLEF corpus) that is
also part of the document collection in the QA task
to retrieve co-occurrences within sentences.
Another approach is one in which the context
of a word is determined by syntactic relations. In
this case, the head word is in a syntactic relation
to a second word and this second word accom-
panied by the syntactic relation form the context
of the head word. We refer to these methods as
SYNTAX-BASED METHODS. We have used several
syntactic relations to acquire syntax-based context
for our headwords. This method results in nearest
neighbours that at least belong to the same seman-
tic and syntactic class, for example baby and son.
We have used 500 million words of newspaper text
(the TwNC corpus parsed by Alpino (van Noord,
2006)) of which the CLEF corpus is a subset.
A third method we have used is the
ALIGNMENT-BASED METHOD. Here, trans-
lations of word, retrieved from the automatic
word alignment of parallel corpora are used to
determine the similarity between words. This
method results in even more tightly related data,
as it mainly finds synonyms, such as infant and
baby. We have used the Europarl corpus (Koehn,
2003) to extract word alignments from.3
By calculating the similarity between the con-
texts words are found in, we can retrieve a
ranked list of nearest neighbours for any head-
word. We gathered nearest neighbours for a
frequency-controlled list of words, that was still
manageable to retrieve. We included all words
(nouns, verbs, adjectives and proper names) with
a frequency of 150 and higher in the CLEF cor-
pus. This resulted in a ranked list of nearest neigh-
bours for the 2,387 most frequent adjectives, the
5,437 most frequent nouns, the 1,898 most fre-
quent verbs, and the 1,399 most frequent proper
names. For all words we retrieved a ranked list
of its 100 nearest neighbours with accompanying
similarity score.
In addition to the lexico-semantic information
resulting from the three distributional methods we
used:
? Dutch EuroWordNet (Vossen, 1998)
? Categorised named entities
With respect to the first resource we can be
short. We selected the synsets of this hand-built
lexico-semantic resource for nouns, verbs, adjec-
tives and proper names.
The categorised named entities are a by-product
of the syntax-based distributional method. From
the example in (1) we extract the apposition rela-
tion between Van Gogh and schilder ?painter? to
determine that the named entity Van Gogh belongs
to the category of painters.
(1) Van Gogh, de beroemde schilder huurde
een atelier, Het Gele huis, in Arles.
?Van Gogh, the famous painter, rented a
studio, The Yellow House, in Arles.?
We used the data of the TwNC corpus (500M
words) and Dutch Wikipedia (50M words) to ex-
tract apposition relations. The data is skewed. The
Netherlands appears with 1,251 different labels.
To filter out incorrect and highly unlikely labels
(often the result of parsing errors) we determined
the relative frequency of the combination of the
named entity and a category with regard to the fre-
quency of the named entity overall. All categorised
named entities with relative frequencies under 0.05
3In van der Plas and Tiedemann (2006) there is more in-
formation on the syntax-based and alignment-based distribu-
tional methods.
52
Lex. info Nouns Adj Verbs Proper
Proximity 5.3K 2.4K 1.9K 1.2K
Syntax 5.4K 2.3K 1.9K 1.4K
Align 4.0K 1.2K 1.6K
Cat. NEs 218K
EWN 44.9K 1.5K 9.0K 1.4K
Table 1: Number of words for which lexico-
semantic information is available
were discarded. This cutoff made the number of
unwanted labels considerably lower.
In Table 1 we see the amount of information
that is contained in individual lexico-semantic re-
sources. It is clear from the numbers that the
alignment-based method does not provide near-
est neighbours for all head words selected. Only
4.0K nouns from the 5.4K retrieve nearest neigh-
bours. The data is sparse. Also, the alignment-
based method does not have any nearest neigh-
bours for proper names, due to decisions we made
earlier regarding preprocessing: All words were
transformed to lowercase.
The proximity-based method also misses a num-
ber of words, but the number is far less impor-
tant. The amount of information the lists of cate-
gorised named entities provide is much larger than
the amount of information comprised in the list
provided by distributional methods. EWN also
provides more information than the distributional
methods, except for adjectives.4
4 Methodology
In order to test the performance of the var-
ious lexico-semantic resources we ran several
tests. The baseline is running a standard full-text
retrieval engine using Apache Lucene (Jakarta,
2004). Documents have been lemmatised and stop
words have been removed.
We applied the nearest neighbours resulting
from the three distributional methods as described
in section 3. For all methods we selected the top-
5 nearest neighbours that had a similarity score of
more than 0.2 as expansions.
For EWN all words in the same synset (for all
senses) were added as expansions. Since all syn-
onyms are equally similar, we do not have similar-
ity scores for them to be used in a threshold.
The categorised named entities were not only
used to expand named entities with the corre-
4Note that the number of nouns from EWN is the result of
subtracting the proper names.
sponding label, but also to expand nouns with
named entities. In the first case all labels were
selected. The maximum is not more than 18 la-
bels. In the second case some nouns get many
expansions. For example, a noun, such as vrouw
?woman?, gets 1,751 named entities as expansions.
We discarded nouns with more than 50 expansions,
as these were deemed too general and hence not
very useful.
The last two settings are the same for the expan-
sions resulting from distributional methods and the
last two types of lexico-semantic information.
? Expansions were added as root forms
? Expansions were given a weight such that all
expansions for one original keyword add up
to 0.5.
5 Evaluation
For evaluation we used data collected from the
CLEF Dutch QA tracks. The CLEF text collec-
tion contains 4 years of newspaper text, approxi-
mately 80 million words and Dutch Wikipedia, ap-
proximately 50 million words. We used the ques-
tion sets from the competitions of the Dutch QA
track in 2003, 2004, and 2005 (774 in total). Ques-
tions in these sets are annotated with valid answers
found by the participating teams including IDs of
supporting documents in the given text collection.
We expanded these list of valid answers where nec-
essary.
We calculated for each run the Mean Reciprocal
Rank (MRR).5 The MRR measures the percentage
of passages for which a correct answer was found
in the top-k passages returned by the system. The
MRR score is the average of 1/R where R is the
rank of the first relevant passage computed over
the 20 highest ranked passages. Passages retrieved
were considered relevant when one of the possible
answer strings was found in that passage.
6 Results
In Table 2 the MRR (Mean Reciprocal Rank) is
given for the various expansion techniques. Scores
are given for expanding the several syntactic cat-
egories, where possible. The baseline does not
5We used MRR instead of other common evaluation mea-
sures because of its stronger correlation with the overall per-
formance of our QA system than, for example, coverage and
redundancy (see Tiedemann and Mur (2008)).
53
MRR
SynCat EWN Syntax Align Proxi Cat.NEs
Nouns 51.52 51.15 51.21 51.38 51.75
Adj 52.33 52.27 52.38 51.71
Verbs 52.40 52.33 52.21 52.62
Proper 52.59 50.16 53.94 55.68
All 51.65 51.21 51.02 53.36 55.29
Table 2: MRR scores for the IR component with
query expansion from several sources
#questions (+/-)
SynCat EWN Syntax Align Proxi Cat.NEs
Nouns 27/50 28/61 17/58 64/87 17/37
Adj 3/6 1/2 1/2 31/47
Verbs 31/51 5/10 8/32 51/56
Proper 3/2 30/80 76/48 157/106
All 56/94 56/131 25/89 161/147 168/130
Table 3: Number of questions that receive a higher
(+) or lower (-) RR when using expansions from
several sources
make use of any expansion for any syntactic cat-
egory and amounts to 52.36.
In Table 3 the number of questions that get a
higher and lower reciprocal rank (RR) after ap-
plying the individual lexico-semantic resources are
given. Apart from expansions on adjectives and
proper names from EWN, the impact of the expan-
sion is substantial. The fact that adjectives have
so little impact is due to the fact that there are not
many adjectives among the query terms.6
The negligible impact of the proper names from
EWN is surprising since EWN provides more en-
tries for proper names than the proximity-based
method (1.2K vs 1.4K, as can be seen in 1). The
proximity-based method clearly provides informa-
tion about proper names that are more relevant for
the corpus used for QA, as it is built from a subset
of that same corpus. This shows the advantage of
using corpus-based methods. The impact of the ex-
pansions resulting from the syntax-based method
lies in between the two previously mentioned ex-
pansions. It uses a corpus of which the corpus used
for QA is a subset.
The type of expansions that result from the
proximity-based method have a larger effect on
the performance of the system than those result-
ing from the syntax-based method. In Chapter 5 of
van der Plas (2008) we explain in greater detail that
the proximity-based method uses frequency cut-
6Moreover, the adjectives related to countries, such as
German and French and their expansion Germany, France are
handled by a separate list.
offs to keep the co-occurrence matrix manageable.
The larger impact of the proximity-based nearest
neighbours is probably partly due to this decision.
The cutoffs for the alignment-based and syntax-
based method have been determined after evalu-
ations on EuroWordNet (Vossen, 1998) (see also
van der Plas (2008)).
The largest impact results from expanding
proper names with categorised named entities. We
know from Table 1 in section 3, that this resource
has 70 times more data than the proximity-based
resource.
For most of the resources the number of ques-
tions that show a rise in RR is smaller than the
number of questions that receive a lower RR, ex-
cept for the expansion of proper names by the cat-
egorised named entities and the proximity-based
method.
The expansions resulting from the syntax-based
method do not result in any improvements. As
expected, the expansion of proper names from
the syntax-based method hurts the performance
most. Remember that the nearest neighbours of the
syntax-based method often include co-hyponyms.
For example, Germany would get The Netherlands
and France as nearest neighbours. It does not seem
to be a good idea to expand the word Germany
with other country names when a user, for exam-
ple, asks the name of the Minister of Foreign Af-
fairs of Germany. However, also the synonyms
from EWN and the alignment-based method do not
result in improvements.
The categorised named entities provide the most
successful lexico-semantic information, when
used to expand named entities with their category
label. The MRR is augmented by almost 3,5%. It
is clear that using the same information in the other
direction, i.e. to expand nouns with named enti-
ties of the corresponding category hurts the scores.
The proximity-based nearest neighbours of proper
names raises the MRR scores with 1,5%.
Remember from the introduction that we made
a distinction between the terminological gap and
the knowledge gap. The lexico-semantic re-
sources that are suited to bridge the terminolog-
ical gap, such as synonyms from the alignment-
based method and EWN, do not result in improve-
ments in the experiments under discussion. How-
ever, the lexico-semantic resources that may be
used to bridge the knowledge gap, i.e. associations
from the proximity-based method and categorised
54
CLEF score
EWN Syntax Align Proxi Cat.NEs Baseline
46.3 47.0 46.6 47.6 47.9 46.8
Table 4: CLEF scores of the QA system with query
expansion from several sources
named entities, do result in improvements of the
IR component.
To determine the effect of query expansion on
the QA system as a whole we determined the av-
erage CLEF score when using the various types
of lexico-semantic information for the IR com-
ponent. The CLEF score gives the precision of
the first (highest ranked) answer only. For EWN,
the syntax-based, and the alignment-based nearest
neighbours we have used all expansions for all syn-
tactic categories together. For the proximity-based
nearest neighbours and the categorised named en-
tities we have limited the expansions to the proper
names as these performed rather well.
The positive effect of using categorised named
entities and proximity-based nearest neighbours
for query expansion is visible in the CLEF scores
as well, although less apparent than in the MRR
scores from the IR component in Table 2.
6.1 Error analysis
Let us first take a look at the disappointing re-
sults regarding the terminological gap, before we
move to the more promising results related to the
knowledge gap. We expected that the expansions
of verbs would be particularly helpful to overcome
the terminological gap that is large for verbs, since
there is much variation. We will give some exam-
ples of expansion from EWN and the alignment-
based method.
(2) Wanneer werd het Verdrag van Rome getekend?
?When was the Treaty of Rome signed??
Align: teken ?sign?? typeer ?typify?, onderteken ?sign?
EWN: teken ?sign? ? typeer ?typify?, kentekenen ?charac-
terise?, kenmerk ?characterise?, schilder ?paint?, kenschets
?characterise?, signeer ?sign?, onderteken ?sign?, schets
?sketch?, karakteriseer ?characterise?.
For the example in (2) both the alignment-based
expansions and the expansion from EWN result in
a decrease in RR of 0.5. The verb teken ?sign? is
ambiguous. We see three senses of the verb repre-
sented in the EWN list, i.e. drawing, characteris-
ing, and signing as in signing an official document.
One out of the two expansions for the alignment-
based method and 2 out of 9 for EWN are in princi-
ple synonyms of teken ?sign? in the right sense for
this question. However, the documents that hold
the answer to this question do not use synonyms
for the word teken. The expansions only introduce
noise.
We found a positive example in (3). The RR
score is improved by 0.3 for both the alignment-
based expansions and the expansions from EWN,
when expanding explodeer ?explode? with ontplof
?blow up?.
(3) Waar explodeerde de eerste atoombom?
?Where did the first atomic bomb explode??
Align: explodeer ?explode? ? ontplof ?blow up?.
EWN: explodeer ?explode?? barst los ?burst?, ontplof ?blow
up?, barst uit ?crack?, plof ?boom?.
To get an idea of the amount of terminologi-
cal variation between the questions and the doc-
uments, we determined the optimal expansion
words for each query, by looking at the words
that appear in the relevant documents. When in-
specting these, we learned that there is in fact lit-
tle to be gained by terminological variation. In
the 25 questions we inspected we found 1 near-
synonym only that improved the scores: gekke-
koeienziekte ?mad cow disease? ? Creutzfeldt-
Jacob-ziekte ?Creutzfeldt-Jacob disease?.
The fact that we find only few synonyms might
be related to a point noted by Mur (2006): Some
of the questions in the CLEF track that we use for
evaluation look like back formulations.
After inspecting the optimal expansions, we
were under the impression that most of the expan-
sions that improved the scores were related to the
knowledge gap, rather than the terminological gap.
We will now give some examples of good and bad
expansions related to the knowledge gap.
The categorised named entities result in the best
expansions, followed by the proximity-based ex-
pansions. In (4) an example is given for which cat-
egorised named entities proved very useful:
(4) Wie is Keith Richard?
?Who is Keith Richard??
Cat. NEs: Keith Richard ? gitarist ?guitar player?, lid
?member?, collega ?colleague?, Rolling Stones-gitarist
?Rolling Stones guitar player?, Stones-gitarist ?Stones guitar
player?.
It is clear that this type of information helps a lot
in answering the question in (4). It contains the
answer to the question. The RR for this question
goes from 0 to 1. We see the same effect for the
55
question Wat is NASA? ?What is NASA??.
It is a known fact that named entities are an im-
portant category for QA. Many questions ask for
named entities or facts related to named entities.
From these results we can see that adding the ap-
propriate categories to the named entities is useful
for IR in QA.
The categorised named entities were not always
successful. In (5) we show that the proximity-
based expansion proved more helpful in some
cases.
(5) Welke bevolkingsgroepen voerden oorlog in
Rwanda?
?What populations waged war in Rwanda??
Proximity: Rwanda? Za??re, Hutu, Tutsi, Ruanda, Rwandees
?Rwandese?.
Cat. NEs: Rwanda ? bondgenoot ?ally?, land ?country?,
staat ?state?, buurland ?neighbouring country?.
In this case the expansions from the proximity-
based method are very useful (except for Zaire),
since they include the answer to the question. That
is not always the case, as can be seen in (6). How-
ever, the expansions from the categorised named
entities are not very helpful in this case either.
(6) Wanneer werd het Verdrag van Rome getekend?
?When was the treaty of Rome signed??
Proximity: Rome ? paus ?pope?, Italie?, bisschop ?bishop?,
Italiaans ?Italian?, Milaan ?Milan?.
Cat. NEs: Rome ? provincie ?province?, stad ?city?,
hoofdstad ?capital?, gemeente ?municipality?.
IR does identify Verdrag van Rome ?Treaty of
Rome? as a multi-word term, however it adds the
individual parts of multi-word terms as keywords
as a form of compound analysis. It might be bet-
ter to expand the multi-word term only and not
its individual parts to decrease ambiguity. Ver-
drag van Rome ?Treaty of Rome? is not found in
the proximity-based nearest neighbours, because it
does not include multi-word terms.
Still, it is not very helpful to expand the word
Rome with pope for this question that has nothing
to do with religious affairs. We can see this as a
problem of word sense disambiguation. The as-
sociation pope belongs to Rome in the religious
sense, the place where the Catholic Church is
seated. Rome is often referred to as the Catholic
Church itself, as in Henry VIII broke from Rome.
Gonzalo et al (1998) showed in an experiment,
where words were manually disambiguated, that
a substantial increase in performance is obtained
when query words are disambiguated, before they
are expanded.
We tried to take care of these ambiguities by
using an overlap method. The overlap method
selects expansions that were found in the near-
est neighbours of more than two query words.
Unfortunately, as Navigli and Velardi (2003),
who implement a similar technique, using lexico-
semantic information from WordNet, note, the
COMMON NODES EXPANSION TECHNIQUE works
very badly. Also, Voorhees (1993) who uses a
similar method to select expansions concludes that
the method has the tendency to select very general
terms that have more than one sense themselves.
In future work we would like to implement the
method by Qiu and Frei (1993), as discussed in
section 2, that uses a more sophisticated technique
to combine the expansions of several words in the
query.
7 Conclusion
We can conclude from these experiments on query
expansion for passage retrieval that query expan-
sion with synonyms to overcome the terminolog-
ical gap is not very fruitful. We believe that the
noise introduced by ambiguity of the query terms
is stronger than the positive effect of adding lexi-
cal variants. This is in line with findings by Yang
and Chua (2003). On the contrary, Pasc?a and
Harabagiu (2001) were able to improve their QA
system by using lexical and semantic alternations
from WordNet using feedback loops.
The disappointing results might also be due to
the small amount of terminological variation be-
tween questions and document collection.
However, adding extra information with regard
to the subject field of the query, query expansions
that bridge the knowledge gap, proved slightly
beneficial. The proximity-based expansions aug-
ment the MRR scores with 1.5%. Most successful
are the categorised named entities. These expan-
sions were able to augment the MRR scores with
nearly 3.5%.
The positive effect of using categorised named
entities and proximity-based nearest neighbours
for query expansion is visible in the CLEF scores
for the QA system overall as well. However, the
improvements are less apparent than in the MRR
scores from the IR component.
56
Acknowledgements
This research was carried out in the project
Question Answering using Dependency Relations,
which is part of the research program for Interac-
tive Multimedia Information eXtraction, IMIX, fi-
nanced by NWO, the Dutch Organisation for Scien-
tific Research and partly by the European Commu-
nity?s Seventh Framework Programme (FP7/2007-
2013) under grant agreement n 216594 (CLASSIC
project: www.classic-project.org).
References
Gonzalo, J., F. Verdejo, I. Chugur, and J. Cigarran.
1998. Indexing with WordNet synsets can improve
text retrieval. In Proceedings of the COLING/ACL
Workshop on Usage of WordNet for NLP.
Grefenstette, G. 1992. Use of syntactic context to pro-
duce term association lists for text retrieval. In Pro-
ceedings of the Annual International Conference on
Research and Development in Information Retrieval
(SIGIR).
Grefenstette, G. 1994. Explorations in automatic the-
saurus discovery. Kluwer Academic Publishers.
Harris, Z. S. 1968. Mathematical structures of lan-
guage. Wiley.
Jakarta, Apache. 2004. Apache Lucene - a high-
performance, full-featured text search engine library.
http://lucene.apache.org/java/docs/index.html.
Kaisser, M. and B. Webber. 2007. Question answering
based on semantic roles. In Proceedings of de ACL
workshop on deep linguistic processing.
Koehn, P. 2003. Europarl: A multilingual corpus for
evaluation of machine translation.
Moldovan, D., M. Passc?a, S. Harabagiu, and M. Sur-
deanu. 2002. Performance issues and error analysis
in an open-domain question answering system. In
Proceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL).
Moldovan, D., M. Pasc?a, S. Harabagiu, and M. Sur-
deanu. 2003. Performance issues and error analysis
in an open-domain question answering system. ACM
Transactions on Information Systems., 21(2):133?
154.
Monz, C. 2003. From Document Retrieval to Question
Answering. Ph.D. thesis, University of Amsterdam.
Mur, J. 2006. Increasing the coverage of answer ex-
traction by applying anaphora resolution. In Fifth
Slovenian and First International Language Tech-
nologies Conference (IS-LTC).
Navigli, R. and P. Velardi. 2003. An analysis of
ontology-based query expansion strategies. In Pro-
ceedings of the Workshop on Adaptive Text Extrac-
tion and Mining (ATEM), in the 14th European Con-
ference on Machine Learning (ECML 2003).
Pantel, P. and D. Ravichandran. 2004. Automati-
cally labeling semantic classes. In Proceedings of
the Conference on Human Language Technology and
Empirical Methods in Natural Language Processing
(HLT/EMNLP).
Pasc?a, M. and S Harabagiu. 2001. The informative role
of wordnet in open-domain question answering. In
Proceedings of the NAACL 2001 Workshop on Word-
Net and Other Lexical Resources.
Qiu, Y. and H.P. Frei. 1993. Concept-based query ex-
pansion. In Proceedings of the Annual International
Conference on Research and Development in Infor-
mation Retrieval (SIGIR), pages 160?169.
Shen, D. and M. Lapata. 2007. Using semantic roles
to improve question answering. In Proceedings of
EMNLP.
Tiedemann, J. and J. Mur. 2008. Simple is best: Exper-
iments with different document segmentation strate-
gies for passage retrieval. In Proceedings of the
Coling workshop Information Retrieval for Question
Answering. To appear.
van der Plas, L. and J. Tiedemann. 2006. Finding
synonyms using automatic word alignment and mea-
sures of distributional similarity. In Proceedings of
COLING/ACL.
van der Plas, Lonneke. 2008. Automatic lexico-
semantic acquisition for question answering. Ph.D.
thesis, University of Groningen. To appear.
van Noord, G. 2006. At last parsing is now operational.
In Actes de la 13eme Conference sur le Traitement
Automatique des Langues Naturelles.
Voorhees, E.M. 1993. Query expansion using lexical-
semantic relations. In Proceedings of the Annual
International Conference on Research and Develop-
ment in Information Retrieval (SIGIR).
Vossen, P. 1998. EuroWordNet a multilingual database
with lexical semantic networks.
Yang, H. and T-S. Chua. 2003. Qualifier: question an-
swering by lexical fabric and external resources. In
Proceedings of the Conference on European Chap-
ter of the Association for Computational Linguistics
(EACL).
Yang, H., T-S. Chua, Sh. Wang, and Ch-K. Koh. 2003.
Structured use of external knowledge for event-based
open domain question answering. In Proceedings
of the Annual International Conference on Research
and Development in Information Retrieval (SIGIR).
57
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 189?194,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
To Cache or not to Cache?
Experiments with Adaptive Models in Statistical Machine Translation
Jo?rg Tiedemann
Department of Linguistics and Philology
Uppsala University, Uppsala/Sweden
jorg.tiedemann@lingfil.uu.se
Abstract
We report results of our submissions to
the WMT 2010 shared translation task in
which we applied a system that includes
adaptive language and translation mod-
els. Adaptation is implemented using ex-
ponentially decaying caches storing pre-
vious translations as the history for new
predictions. Evidence from the cache is
then mixed with the global background
model. The main problem in this setup is
error propagation and our submissions es-
sentially failed to improve over the com-
petitive baseline. There are slight im-
provements in lexical choice but the global
performance decreases in terms of BLEU
scores.
1 Motivation
The main motivation of our submission was to
test the use of adaptive language and translation
models in a standard phrase-based SMT setting
for the adaptation to wider context beyond sen-
tence boundaries. Adaptive language models have
a long tradition in the speech recognition commu-
nity and various approaches have been proposed
to reduce model perplexity in this way. The gen-
eral task is to adjust statistical models to essen-
tial properties of natural language which are usu-
ally not captured by standard n-gram models or
other local dependency models. First of all, it is
known that repetition is very common especially
among content words (see, for example, words
like ?honey?, ?milk?, ?land? and ?flowing? in fig-
ure 1). In most cases a repeated occurrence of a
content word is much more likely than its first ap-
pearance, which is not predicted in this way by a
static language model. Secondly, the use of ex-
pressions is related to the topic in the current dis-
course and the chance of using the same topic-
related expressions again in running text is higher
than a mixed-topic model would predict.
In translation another phenomenon can be ob-
served, namely the consistency of translations.
Polysemous terms are usually not ambiguous in
their context and, hence, their translations become
consistent according to the contextual sense. Even
the choice between synonymous translations is
rather consistent in translated texts as we can see
in the example of subtitle translations in figure 1
(taken from the OPUS corpus (Tiedemann, 2009)).
The 10 commandments Kerd ma lui
To some land flowing with milk
and honey!
Till ett land fullt av mjo?lk och
honung.
I?ve never tasted honey.
Jag har aldrig smakat honung.
...
But will sympathy lead us to
this land flowing with milk and
honey?
Men kan sympati leda oss till detta
mjo?lkens och honungens land?
Mari honey ...
Mari, gumman
Sweetheart,
where are you
going?
A?lskling, var
ska du?
...
Who was that,
honey?
Vem var det,
gumman?
Figure 1: Repetition and translation consistency
Ambiguous terms like ?honey? are consistently
translated into the Swedish counterpart ?honung?
(in the sense of the actual substance) or ?gumman?
(in the metaphoric sense). Observe that this is true
even in the latter case where synonymous transla-
tions such as ?a?lskling? would be possible as well.
In other words, deciding to stick to consistent lexi-
cal translations should be preferred in MT because
the chance of alternative translations in repeated
cases is low. Here again, common static transla-
tion models do not capture this property at all.
In the following we explain our attempt to inte-
grate contextual dependencies using cache-based
adaptive models in a standard SMT setup. We
have already successfully applied this technique
to a domain-adaptation task (Tiedemann, 2010).
189
Now we would like to investigate the robustness
of this model in a more general case where some
in-domain training data is available and input data
is less repetitive.
2 Cache-based Adaptive Models
The basic idea behind cache-based models is to
mix a large static background model with a small
local model that is dynamically estimated from re-
cent items from the input stream. Dynamic cache
language models have been introduced by (Kuhn
and Mori, 1990) and are often implemented in the
form of linear mixtures:
P (wn|history) = (1 ? ?)Pbackground(wn|history) +
?Pcache(wn|history)
The background model is usually a standard n-
gram model taking limited amount of local context
from the history into account and the cache model
is often implemented as a simple (unsmoothed)
unigram model using the elements stored in a
fixed-size cache (100-5000 words) to estimate
its parameters. Another improvement can be
achieved by making the importance of cached el-
ements a function of recency. This can be done
by introducing a decaying factor in the estima-
tion of cache probabilities (Clarkson and Robin-
son, 1997):
Pcache(wn|wn?k..wn?1) ?
1
Z
n?1?
i=n?k
I(wn = wi)e
??(n?i)
This is basically the model that we applied in our
experiments as it showed the largest perplexity re-
duction in our previous experiments on domain
adaptation.
Similarly, translation models can be adapted as
well. This is especially useful to account for trans-
lation consistency forcing the decoder to prefer
identical translations for repeated terms. In our
approach we try to model recency again using a
decay factor to compute translation model scores
from the cache in the following way (only for
source language phrases fn for which a transla-
tion option exist in the cache; we use a score of
zero otherwise):
?cache(en|fn) =
?K
i=1 I(?en, fn? = ?ei, fi?) ? e
??i
?K
i=1 I(fn = fi)
The importance of a cached translation option ex-
ponentially decays and we normalize the sum of
cached occurrences by the number of translation
options with the same foreign language item that
we condition on.
Plugging this in into a standard phrase-based SMT
engine is rather straightforward. The use of cache-
based language models in SMT have been in-
vestigated before (Raab, 2007). In our case we
used Moses as the base decoder (Koehn et al,
2007). The cache-based language model can be
integrated in the decoder by simply adjusting the
call to the language modeling toolkit appropri-
ately. We implemented the exponentially decaying
cache model within the standard SRILM toolkit
(Stolcke, 2002) and added command line argu-
ments to Moses to switch to that model and to set
cache parameters such as interpolation, cache size
and decay. Adding the translation model cache is
a bit more tricky. For this we added a new feature
function to the global log-linear model and im-
plemented the decaying cache as explained above
within the decoder. Again, simple command-line
arguments can be used to switch caching on or off
and to adjust cache parameters.
One important issue is to decide when and what
to cache. As we explore a lot of different options
in decoding it is not feasible to adapt the cache
continuously. This would mean a lot of cache op-
erations trying to add and remove hypotheses from
the cache memory. Therefore, we opted for a con-
text model that considers history only from previ-
ous sentences. Once decoding is finished transla-
tion options from the best hypothesis found in de-
coding are put into language and translation model
cache. This is arguably a strong approximation of
the adaptive approach. However, considering our
special concern about wider context across sen-
tence boundaries this seems to be a reasonable
compromise between completeness and efficiency.
Another issue is related to the selection of items
to be cached. As discussed earlier repetition is
most likely to be found among content words.
Similarly, translation consistency is less likely to
be true for function words. In the best case one
would know the likelihood of specific terms to
be repeated. This could be trained on some de-
velopment data possibly in connection with word
classes instead of fully lexicalized parameters in
order to overcome data sparseness and to improve
generality. Even though this idea is very tempt-
190
ing it would require a substantial extension of our
model and would introduce language and domain-
specific parameters. Therefore, we just added a
simplistic approach filtering tokens by their length
in characters instead. Assuming that longer items
are more likely to be content words we simply set
a threshold to decide whether to add a term to the
cache or not. This threshold can be adjusted using
command-line arguments.
Finally, we also need to be careful about noise
in the cache. This is essential as the caching ap-
proach is prone to error propagation. However,
detecting noise is difficult. If there would be a no-
tion of noise in translation hypotheses, the decoder
would avoid it. In related work (Nepveu et al,
2004) have studied cache-based translation mod-
els in connection with interactive machine trans-
lation. In that case, one can assume correct input
after post-editing the translation suggestions. One
way to approach noise reduction in non-interactive
MT is to make use of transition costs in the transla-
tion lattice. Assuming that this cost (which is esti-
mated internally within the decoder during the ex-
pansion of translation hypotheses) refers to some
kind of confidence we can discard translation op-
tions above a certain threshold, which is what we
did in the implementation of our translation model
cache.
3 Experiments
We followed the setup proposed in the shared
translation task. Primarily we concentrated our
efforts on German-English (de-en) and English-
German (en-de) using the constrained track, i.e.
using the provided training and development data
from Europarl and the News domain. Later we
also added experiments for Spanish (es) and En-
glish using a similar setup.
Our baseline system incorporates the following
components: We trained two separate 5-gram lan-
guage models for each language with the standard
smoothing strategies (interpolation and Kneser-
Ney discounting), one for Europarl and one for the
News data. All of them were estimated using the
SRILM toolkit except the English News LM for
which we applied RandLM (Talbot and Osborne,
2007) to cope with the large amount of training
data. We also included two separate translation
models, one for the combined Europarl and News
data and one for the News data only. They were
estimated using the standard tools GIZA++ (Och
and Ney, 2003) and Moses (Koehn et al, 2007)
applying default settings and lowercased training
data. Lexicalized reordering was trained on the
combined data set. All baseline models were then
tuned on the News test data from 2008 using mini-
mum error rate training (MERT) (Och, 2003). The
results in terms of lower-case BLEU scores are
listed in table 1.
n-gram scores
BLEU 1 2 3 4
de-en baseline 21.3 57.4 27.8 15.1 8.6
de-en cache 21.5 58.1 28.1 15.2 8.7
en-de baseline 15.6 52.5 21.7 10.6 5.5
en-de cache 14.4 52.6 21.0 9.9 4.9
es-en baseline 26.7 61.7 32.7 19.9 12.6
es-en cache 26.1 62.6 32.7 19.8 12.5
en-es baseline 26.9 61.5 33.3 20.5 12.9
en-es cache 23.0 60.6 30.4 17.6 10.4
Table 1: Results on the WMT10 test set.
In the adaptation experiments we applied exactly
the same models using the feature weights from
the baseline with the addition of the caching com-
ponents in both, language models and translation
models. Cache parameters are not particularly
tuned for the task in our initial experiments which
could be one reason for the disappointing results
we obtained. Some of them can be integrated in
the MERT procedure, for example, the interpola-
tion weight of the translation cache. However, tun-
ing these parameters with the standard procedures
appears to be difficult as we will see in later ex-
periments presented in section 3.2. Initially we
used settings that appeared to be useful in previ-
ous experiments. In particular, we used a language
model cache of 10,000 words with a decay of
? = 0.0005 and an interpolation weight of 0.001.
A cache was used in all language models except
the English News model for which caching was
not available (because we did not implement this
feature for RandLM). The translation cache size
was set to 5,000 with a decay factor of 0.001. The
weight for the translation cache was set to 0.001.
Furthermore, we filtered items for the translation
cache using a length constraint of 4 characters or
more and a transition cost threshold (log score) of
-4.
The final results of the adaptive runs are shown
in table 1. In all but one case the cache-based re-
sult is below the baseline which is, of course, quite
disappointing. For German-English a small im-
provement can be observed. However, this may
be rather accidental. In general, it seems that
191
the adaptive approach cannot cope with the noise
added to the cache.
3.1 Discussion
There are two important observations that should
be mentioned here. First of all, the adaptive ap-
proach assumes coherent text input. However, the
WMT test-set is composed of many short news
headlines with various topics involved. We, there-
fore, also ran the adaptive approach on individual
news segments. The results are illustrated in figure
2.
Basically, the results do not change compared
to the previous run. Still, cache-based models per-
form worse on average except for the German-
English test-set for which we obtained a slight
but insignificant improvement. Figure 2 plots the
BLEU score differences between standard models
and cached models for the individual news items.
We can see a very blurred picture of these indi-
vidual scores and the general conclusion is that
caching failed. One problem is that the individ-
ual news items are very short (around 20 sentences
each) which is probably too little for caching to
show any positive effect. Surprising, however, is
the negative influence of caching even on these
small documents which is quite similar to the runs
on the entire sets. The drop in performance for
English-Spanish is especially striking. We have
no explanation at this point for this exceptional be-
havior.
A second observation is the variation in individ-
ual n-gram precision scores (see table 1). In all but
one case the unigram precision goes up which in-
dicates that the cache models often improve lexical
choice at least in terms of individual words. The
first example in figure 2 could be seen as a slight
improvement due to a consistent lexical choice of
?missile? (instead of ?rocket?).
The main problem, however, in the adaptive ap-
proach seems to appear in local contexts which
might be due to the simplistic language modeling
cache. It would be interesting to study possibilities
of integrating local dependencies into the cache
models. However, there are serious problems with
data sparseness. Initial experiments with a bigram
LM cache did not produce any improvements so
far.
Another crucial problem with the cache-based
model is of course error propagation. An exam-
ple which is probably due to this issue can be seen
baseline until the end of the journey , are , in turn , tech-
nical damage to the rocket .
cache until the end of the journey , in turn , technical
damage to the missile .
reference but near the end of the flight there was technical
damage to the missile .
baseline iran has earlier criticism of its human rights
record .
cache iran rejected previous criticism of its human
rights record .
reference iran has dismissed previous criticism of its hu-
man rights record .
baseline facing conservationists is accused of extortion
cache facing conservationists is accused of extortion
reference Nature protection officers accused of blackmail
baseline the leitmeritz-polizei accused the chairman of
the bu?rgervereinigung ? naturschutzgemein-
schaft leitmeritz ? because of blackmail .
cache the leitmeritz-polizei accused the chairman of
the bu?rgervereinigung ? naturschutzgemein-
schaft leitmeritz ? because of extortion .
reference The Litomerice police have accused the chair-
man of the Litomerice Nature Protection Soci-
ety civil association of blackmail.
Table 2: German to English example translations.
in table 2 in the last two translations (propagation
of the translation option ?extortion?). This prob-
lem is difficult to get around especially in case
of bad baseline translations. One possible idea
would be to implement a two-pass procedure to
run over the entire input first only to fill the cache
and to identify reliable evidence for certain trans-
lation options (possibly focusing on simple trans-
lation tasks such as short sentences). Then, in the
second pass the adaptive model can be applied to
prefer repetition and consistency according to the
parameters learned in the first pass.
3.2 Parameter Optimization
Another question is if the cache parameters re-
quire careful optimization in order to make this
approach effective. An attempt to investigate the
influence of the cache components by simply vary-
ing the interpolation weights gave us the following
results for English-German (see table 3).
fixed cache TM parameters fixed cache LM parameters
?LM BLEU ?TM BLEU
0.1 14.12 0.1 12.75
0.01 14.39 0.01 13.04
0.005 14.40 0.005 13.57
0.001 14.44 0.001 14.42
0.0005 14.43 0.0005 14.57
Table 3: Results for English to German with vary-
ing mixture weights.
Looking at these results the tendency of the scores
192
-10
-8
-6
-4
-2
 0
 2
 4
en-de
-10
-8
-6
-4
-2
 0
 2
 4
de-en
-10
-8
-6
-4
-2
 0
 2
 4 en-es
-10
-8
-6
-4
-2
 0
 2
 4 es-en
Figure 2: BLEU score differences between a standard model and a cached model for individual news
segments from the WMT test-set.
seems to suggest that switching off caching is the
right thing to do (as one might have expected al-
ready from the initial experimental results). We
did not perform the same type of investigation for
the other language pairs but we expect a similar
behavior.
Even though these results did not encourage us
very much to investigate the possibilities of cache
parameter optimization any further we still tried to
look at the integration of the interpolation weights
into the MERT procedure. The weight of the TM
cache is especially suited for MERT as this com-
ponent is implemented in terms of a separate fea-
ture function within the global log-linear model
used in decoding. The LM mixture model, on
the other hand, is implemented internally within
SRILM and therefore not so straightforward to in-
tegrate into standard MERT. We, therefore, dou-
bled the number of LM?s included in the SMT
model using two standard LM?s and two LM?s
with cache (one for Europarl and one for News
in both cases). The latter are actually mixtures as
well using a fixed interpolation weight of ?LM =
0.5 between the cached component and the back-
ground model. In this way the cached LM?s bene-
fit from the smoothing with the static background
model. Individual weights for all four LM?s are
then learned in the global MERT procedure. Un-
fortunately, other cache parameters cannot be op-
timized in this way as they do not produce any par-
ticular values for individual translation hypotheses
in decoding.
We applied this tuning setup to the English-
German translation task and ran MERT on the
same development data as before. Actually,
caching slows down translation quite substantially
which makes MERT very slow. Due to the se-
quential caching procedure it is also not possible
to parallelize tuning. Furthermore, the extra pa-
rameters seem to cause problems in convergence
and we had to stop the optimization after 30 iter-
ations when BLEU scores seemed to start stabi-
lizing around 14.9 (in the standard setup only 12
iterations were required to complete tuning). Un-
fortunately, the result is again quite disappointing
(see table 4).
Actually, the final BLEU score after tuning is even
lower than in our initial runs with fixed cache
parameters taken from previous unrelated exper-
iments. This is very surprising and it looks like
that MERT just failed to find settings close to the
global optimum because of some strong local sub-
optimal points in the search space. One would ex-
pect that it should be possible to obtain at least the
193
BLEU on dev-set (no caching) 15.2
BLEU on dev-set (with caching) 14.9
Europarl LM 0.000417
News LM 0.057042
Europarl LM (with cache) 0.002429
News LM (with cache) -0.000604
?TM 0.000749
BLEU on test-set (no caching) 15.6
BLEU on test-set (with caching) 12.7
Table 4: Tuning cache parameters.
same score on the development set which was not
the case in our experiment. However, as already
mentioned, we had to interrupt tuning and there
is still some chance that MERT would have im-
proved in later iterations. At least intuitively, there
seems to be some logic behind the tuned weights
(shown in table 4). The out-of-domain LM (Eu-
roparl) obtains a higher weight with caching than
without and the in-domain LM (News) is better
without it and, therefore, the cached version ob-
tains a negative weight. Furthermore, the TM
cache weight is quite similar to the one we used in
the initial experiments. However, applying these
settings to the test-set did not work at all.
4 Conclusions
In our WMT10 experiments cache-based adaptive
models failed to improve translation quality. Pre-
vious experiments have shown that they can be
useful in adapting SMT models to new domains.
However, they seem to have their limitations in the
general case with mixed topics involved. A gen-
eral problem is error propagation and the corrup-
tion of local dependencies due to over-simplified
cache models. Parameter optimization seems to
be difficult as well. These issues should be inves-
tigated further in future research.
References
P.R. Clarkson and A. J. Robinson. 1997. Language
model adaptation using mixtures and an exponen-
tially decaying cache. In International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), pages 799?802, Munich, Germany.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
ACL ?07: Proceedings of the 45th Annual Meeting
of the ACL, pages 177?180, Morristown, NJ, USA.
Roland Kuhn and Renato De Mori. 1990. A cache-
based natural language model for speech recogni-
tion. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 12(6):570?583.
Laurent Nepveu, Lapalme, Guy, Langlais, Philippe,
and George Foster. 2004. Adaptive Language and
Translation Models for Interactive Machine Trans-
lation. In Proceedings of the 9th Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 190?197, Barcelona, Spain.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Associa-
tion for Computational Linguistics, pages 160?167,
Morristown, NJ, USA.
Martin Raab. 2007. Language Modeling for Machine
Translation. VDM Verlag, Saarbru?cken, Germany.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the 7th
international conference on spoken language pro-
cessing (ICSLP 2002), pages 901?904, Denver, CO,
USA.
David Talbot and Miles Osborne. 2007. Randomised
language modelling for statistical machine transla-
tion. In ACL ?07: Proceedings of the 45th Annual
Meeting of the ACL, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
Jo?rg Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In Recent Advances in Natural Language
Processing, volume V, pages 237?248. John Ben-
jamins, Amsterdam/Philadelphia.
Jo?rg Tiedemann. 2010. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In ACL 2010 Workshop on
Domain Adaptation for Natural Language Process-
ing (DANLP).
194
Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing, ACL 2010, pages 8?15,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Context Adaptation in Statistical Machine Translation Using Models with
Exponentially Decaying Cache
Jo?rg Tiedemann
Department of Linguistics and Philology
Uppsala University, Uppsala/Sweden
jorg.tiedemann@lingfil.uu.se
Abstract
We report results from a domain adapta-
tion task for statistical machine translation
(SMT) using cache-based adaptive lan-
guage and translation models. We apply
an exponential decay factor and integrate
the cache models in a standard phrase-
based SMT decoder. Without the need for
any domain-specific resources we obtain a
2.6% relative improvement on average in
BLEU scores using our dynamic adapta-
tion procedure.
1 Introduction
Most data-driven approaches to natural lan-
guage processing (NLP) are subject to the well-
known problem of lack of portability to new do-
mains/genres. Usually there is a substantial drop
in performance when testing on data from a do-
main different to the training data. Statistical ma-
chine translation is no exception. Despite its pop-
ularity, standard SMT approaches fail to provide a
framework for general application across domains
unless appropriate training data is available and
used in parameter estimation and tuning.
The main problem is the general assumption
of independent and identically distributed (i.i.d.)
variables in machine learning approaches applied
in the estimation of static global models. Recently,
there has been quite some attention to the prob-
lem of domain switching in SMT (Zhao et al,
2004; Ueffing et al, 2007; Civera and Juan, 2007;
Bertoldi and Federico, 2009) but ground breaking
success is still missing. In this paper we report
our findings in dynamic model adaptation using
cache-based techniques when applying a standard
model to the task of translating documents from a
very different domain.
The remaining part of the paper is organized as
follows: First, we will motivate the chosen ap-
proach by reviewing the general phenomenon of
repetition and consistency in natural language text.
Thereafter, we will briefly discuss the dynamic ex-
tensions to language and translation models ap-
plied in the experiments presented in the second
last section followed by some final conclusions.
2 Motivation
Domain adaptation can be tackled in various ways.
An obvious choice for empirical systems is to ap-
ply supervised techniques in case domain-specific
training data is available. It has been shown that
small(er) amounts of in-domain data are suffi-
cient for such an approach (Koehn and Schroeder,
2007). However, this is not really a useful alter-
native for truly open-domain systems, which will
be confronted with changing domains all the time
including many new, previously unknown ones
among them.
There are also some interesting approaches to
dynamic domain adaptation mainly using flexible
mixture models or techniques for the automatic se-
lection of appropriate resources (Hildebrand et al,
2005; Foster and Kuhn, 2007; Finch and Sumita,
2008). Ideally, a system would adjust itself to the
current context (and thus to the current domain)
without the need of explicit topic mixtures. There-
fore, we like to investigate techniques for general
context adaptation and their use in out-of-domain
translation.
There are two types of properties in natural lan-
guage and translation that we like to explore. First
of all, repetition is very common ? much more
than standard stochastic language models would
predict. This is especially true for content words.
See, for instance, the sample of a medical docu-
ment shown in figure 1. Many content words are
repeated in close context. Hence, appropriate lan-
guage models should incorporate changing occur-
rence likelihoods to account for these very com-
mon repetitions. This is exactly what adaptive lan-
guage models try to do (Bellegarda, 2004).
8
?They may also have episodes of depression . Abilify is
used to treat moderate to severe manic episodes and to
prevent manic episodes in patients who have responded to
the medicine in the past . The solution for injection is used
for the rapid control of agitation or disturbed behaviour
when taking the medicine by mouth is not appropriate .
The medicine can only be obtained with a prescription .?
Figure 1: A short example from a document from
the European Medicines Agency (EMEA)
Another known fact about natural language is con-
sistency which is also often ignored in statistical
models. A main problem in most NLP applica-
tions is ambiguity. However, ambiguity is largely
removed within specific domains and contexts in
which ambiguous items have a well-defined and
consistent meaning. This effect of ?meaning con-
sistency? also known as the principle of ?one sense
per discourse? has been applied in word sense
disambiguation with quite some success (Gale et
al., 1992). For machine translation this means
that adapting to the local domain and sticking to
consistent translation choices within a discourse
seems to be better than using a global static model
and context independent translations of sentences
in isolation. For an illustration, look at the exam-
ples in figure 2 taken from translated movie subti-
tles. Interesting is not only the consistent meaning
of ?honey? within each discourse but also the con-
sistent choice among equivalent translations (syn-
onyms ?a?lskling? och ?gumman?). Here, the dis-
tinction between ?honey? and ?sweetheart? has
been transferred to Swedish using consistent trans-
lations.
The 10 commandments Kerd ma lui
To some land flowing with
milk and honey!
Till ett land fullt av mjo?lk
och honung.
I?ve never tasted honey.
Jag har aldrig smakat ho-
nung.
...
Mari honey ...
Mari, gumman ...
Sweetheart, where are
you going?
A?lskling, var ska du?
...
Who was that, honey?
Vem var det, gumman?
Figure 2: Consistency in subtitle translations
In summary: Repetition and consistency are very
important when modeling natural language and
translation. A proper translation engine should
move away from translating sentences in isolation
but should consider wider context to include these
discourse phenomena. In the next section we dis-
cuss the cache-based models that we implemented
to address this challenge.
3 Cache-based Models
The main idea behind cache-based language mod-
els (Kuhn and Mori, 1990) is to mix a large global
(static) language model with a small local (dy-
namic) model estimated from recent items in the
history of the input stream. It is common to use
simple linear interpolations and fixed cache sizes k
(100-5000 words) to achieve this: P (wn|history) =
(1? ?)Pn?gram(wn|history) + ?Pcache(wn|history)
Due to data sparseness one is usually restricted
to simple cache models. However, unigram mod-
els are often sufficient and smoothing is not nec-
essary due to the interpolation with the smoothed
background model. From the language model-
ing literature we know that caching is an effi-
cient way to reduce perplexity (usually leading to
modest improvements on in-domain data and large
improvements on out-of-domain data). Table 1
shows this effect yielding 53% reduction of per-
plexity on our out-of-domain data.
different settings for ?
cache 0.05 0.1 0.2 0.3
0 376.1 376.1 376.1 376.1
50 270.7 259.2 256.4 264.9
100 261.1 246.6 239.2 243.3
500 252.2 233.1 219.1 217.0
1000 240.6 218.0 199.2 192.9
2000 234.6 209.6 187.9 179.1
5000 235.3 209.1 185.8 175.8
10000 237.6 210.7 186.6 176.1
20000 239.9 212.5 187.7 176.7
Table 1: Perplexity of medical texts (EMEA) us-
ing a language model estimated on Europarl and a
unigram cache component
Even though a simple unigram cache is quite ef-
fective it now requires a careful optimization of its
size. In order to avoid the dependence on cache
size and to account for recency a decaying factor
can be introduced (Clarkson and Robinson, 1997):
Pcache(wn|wn?k..wn?1) ?
1
Z
n?1?
i=n?k
I(wn = wi)e
??(n?i)
Here, I(A) = 1 if A is true and 0 otherwise. Z
is a normalizing constant. Figure 3 illustrates the
effect of cache decay on our data yielding another
significant reduction in perplexity (even though
9
 168
 170
 172
 174
 176
 178
 180
 182
 184
 0  0.0005  0.001  0.0015  0.002  0.0025  0.003
pe
rp
lex
ity
decay ratio
cache size = 2000
cache size = 5000
cache size = 10000
Figure 3: Out-of-domain perplexity using lan-
guage models with decaying cache.
the improvement is much less impressive than the
one obtained by introducing the cache).
The motivation of using these successful tech-
niques in SMT is obvious. Language models play
a crucial role in fluency ranking and a better fit
to real data (supporting the tendency of repetition)
should be preferred. This, of course, assumes cor-
rect translation decisions in the history in our SMT
setting which will almost never be the case. Fur-
thermore, simple cache models like the unigram
model may wrongly push forward certain expres-
sions without considering local context when us-
ing language models to discriminate between var-
ious translation candidates. Therefore, success-
fully applying these adaptive language models in
SMT is surprisingly difficult (Raab, 2007) espe-
cially due to the risk of adding noise (leading to
error propagation) and corrupting local dependen-
cies.
In SMT another type of adaptation can be ap-
plied: cache-based adaptation of the translation
model. Here, not only the repetition of content
words is supported but also the consistency of
translations as discussed earlier. This technique
has already been tried in the context of interactive
machine translation (Nepveu et al, 2004) in which
cache features are introduced to adapt both the lan-
guage model and the translation model. However,
in their model they require an automatic align-
ment of words in the user edited translation and the
source language input. In our experiments we in-
vestigate a close integration of the caching proce-
dure into the decoding process of fully automatic
translation. For this, we fill our cache with trans-
lation options used in the best (final) translation
hypothesis of previous sentences. In our imple-
mentation of the translation model cache we use
again a decaying factor in order to account for re-
cency. For known source language items (fn for
which translation options exist in the cache) the
following formula is used to compute the cache
translation score:
?cache(en|fn) =
?K
i=1 I(?en, fn? = ?ei, fi?) ? e
??i
?K
i=1 I(fn = fi)
Unknown items receive a score of zero. This score
is then used as an additional feature in the standard
log-linear model of phrase-based SMT1.
4 Experiments
Our experiments are focused on the unsupervised
dynamic adaptation of language and translation
models to a new domain using the cache-based
mixture models as described above. We apply
these techniques to a standard task of translat-
ing French to English using a model trained on
the publicly available Europarl corpus (Koehn,
2005) using standard settings and tools such as the
Moses toolkit (Koehn et al, 2007), GIZA++ (Och
and Ney, 2003) and SRILM (Stolcke, 2002). The
log-linear model is then tuned as usual with mini-
mum error rate training (Och, 2003) on a separate
development set coming from the same domain
(Europarl). We modified SRILM to include a de-
caying cache model and implemented the phrase
translation cache within the Moses decoder. Fur-
thermore, we added the caching procedures and
other features for testing the adaptive approach.
Now we can simply switch the cache models on
or off using additional command-line arguments
when running Moses as usual.
4.1 Experimental Setup
For testing we chose to use documents from the
medical domain coming from the EMEA corpus
that is part of the freely available collection of
parallel corpora OPUS2 (Tiedemann, 2009). The
reason for selecting this domain is that these doc-
uments include very consistent instructions and
repetitive texts which ought to favor our caching
techniques. Furthermore, they are very different
1Logarithmic values are used in the actual implementation
which are floored to a low constant in case of zero ? scores.
2The OPUS corpus is available at this URL:
http://www.let.rug.nl/tiedeman/OPUS/.
10
from the training data and, thus, domain adapta-
tion is very important for proper translations. We
randomly selected 102 pairs of documents with al-
together 5,478 sentences. Sentences have an aver-
age length of about 19 tokens with a lot of varia-
tion among them. Documents are compiled from
the European Public Assessment Reports (EPAR)
which reflect scientific conclusions at the end of a
centralized evaluation procedure for medical prod-
ucts. They include a lot of domain-specific ter-
minology, short facts, lists and tables but also de-
tailed textual descriptions of medicines and their
use. The overall lowercased type/token ratio in the
English part of our test collection is about 0.045
which indicates quite substantial repetitions in the
text. This ratio is, however, much higher for indi-
vidual documents.
In the experiment each document is processed
individually in order to apply appropriate dis-
course breaks. The baseline score for applying a
standard phrase-based SMT model yields an aver-
age score of 28.67 BLEU per document (28.60 per
sentence) which is quite reasonable for an out-of-
domain test. Intuitively, the baseline performance
should be crucial for the adaptation. As discussed
earlier the cache-based approach assumes correct
history and better baseline performance should in-
crease the chance of adding appropriate items to
the cache.
4.2 Applying the LM Cache
In our first experiment we applied a decaying uni-
gram cache in the language model. We performed
a simple linear search on a separate development
set for optimizing the interpolation weight which
gave as a value of ? = 0.001. The size of the cache
was set to 10,000 and the decay factor was set to
? = 0.0005 (according to our findings in figure
3). The results on our test data compared to the
standard model are illustrated (with white boxes)
in figure 4.
There is quite some variation in the effect of the
cache LM on our test documents. The translations
of most EMEA documents could be improved ac-
cording to BLEU scores, some of them substan-
tially, whereas others degraded slightly. Note that
the documents differ in size and some of them are
very short which makes it a bit difficult to interpret
and directly compare these scores. On average the
BLEU score is improved by 0.43 points per doc-
ument and 0.39 points per sentence. This might
-2
-1
 0
 1
 2
 3
 4
 5
BL
EU
 sc
or
e 
dif
fe
re
nc
e
cache LM vs. standard LM
cache TM vs. standard TM
Figure 4: The differences in BLEU between a
standard model and models with cache for 102
EMEA documents (sorted by overall BLEU score
gain ? see figure 5)
be not as impressive as we were hoping for af-
ter the tremendous perplexity reduction presented
earlier. However, considering the simplicity of the
approach that does not require any additional re-
sources nor training it is still a valuable achieve-
ment.
4.3 Applying the TM Cache
In the next experiment we tested the effect of the
TM cache on translation quality. Using our hy-
pothesis of translation consistency we expected
another gain on our test set. In order to reduce
problems of noise we added two additional con-
straints: We only cache phrases that contain at
least one word longer than 4 characters (a simplis-
tic attempt to focus on content words rather than
function words) and we only cache translation op-
tions for which the transition costs (of adding this
option to the current hypothesis) in the global de-
coding model is larger than a given threshold (an
attempt to use some notion of confidence for the
current phrase pair; in our experiments we used a
log score of -4). Using this setup and applying the
phrase cache in decoding we obtained the results
illustrated with filled boxes in the figure 4 above.
Again, we can observe a varied outcome but
mostly improvements. The impact of the phrase
translation cache (with a size of 5,000 items) is not
as strong as for the language model cache which
might be due to the rather conservative settings
(? = 0.001, ? = 0.001) and the fact that matching
phrase pairs are less likely to appear than matching
target words. On average the gain is about 0.275
11
BLEU points per document (0.26 per sentence).
4.4 Combining the Cache Models
Finally, we applied both types of cache in one
common system using the same settings from the
individual runs. The differences to the baseline
model are shown in figure 5.
-2
-1
 0
 1
 2
 3
 4
 5
BL
EU
 sc
or
e 
dif
fe
re
nc
e
cache models vs. standard models
Figure 5: The BLEU score differences between a
standard model and a model with cache for both
TM and LM (sorted by BLEU score gain).
In most cases, applying the two types of cache
together has a positive effect on the final BLEU
score. Now, we see only a few documents with
a drop in translation performance. On average
the gain has increased to about 0.78 BLEU points
per document (0.74 per sentence) which is about
2.7% relative improvement compared to the base-
line (2.6% per sentence).
5 Discussion
Our experiments seem to suggest that caching
could be a way to improve translation quality on
a new domain. However, the differences are small
and the assumption that previous translation hy-
potheses are good enough to be cached is risky.
One obvious question is if the approach is ro-
bust enough to be helpful in general. If that is
the the case we should also see positive effects
on in-domain data where a cache model could ad-
just to topical shifts within that domain. In order
to test this ability we ran an experiment with the
2006 test data from the workshop on statistical ma-
chine translation (Koehn and Monz, 2006) using
the same models and settings as above. This re-
sulted in the following scores (lowercased BLEU):
BLEUbaseline = 32.46 (65.0/38.3/25.4/17.6, BP=0.999)
BLEUcache = 31.91 (65.1/38.1/25.1/17.3, BP=0.991)
Clearly, the cache models failed on this test even
though the difference between the two runs is not
large. There is a slight improvement in unigram
matches (first value in brackets) but a drop on
larger n-gram scores and also a stronger brevity
penalty (BP). This could be an effect of the sim-
plicity of the LM cache (a simple unigram model)
which may improve the choice of individual lexi-
cal items but without respecting contextual depen-
dencies.
One difference is that the in-domain data was
translated in one step without clearing the cache at
topical shifts. EMEA documents were translated
one by one with empty caches at the beginning. It
is now the question if proper initialization is es-
sential and if there is a correlation between docu-
ment length and the effect of caching. How much
data is actually needed to take advantage of cached
items and is there a point where a positive effect
degrades because of topical shifts within the docu-
ment? Let us, therefore, have a look at the relation
between document length and BLEU score gain in
our test collection (figure 6).
-2
-1
 0
 1
 2
 3
 4
 5
 0  200  400  600  800  1000  1200  1400  1600  1800  2000
BL
EU
 sc
or
e 
dif
fe
re
nc
e
document length
Figure 6: Correlation between document lengths
(in number of tokens) and BLEU score gains with
caching.
Concluding from this figure there does not seem
to be any correlation. The length of the document
does not seem to influence the outcome. What
else could be the reason for the different behaviour
among our test documents? One possibility is the
quality of baseline translations assuming that bet-
ter performance increases the chance of caching
correct translation hypotheses. Figure 7 plots the
BLEU score gains in comparison with the baseline
scores.
Again, no immediate correlation can be seen.
12
-2
-1
 0
 1
 2
 3
 4
 5
 10  15  20  25  30  35  40  45
BL
EU
 sc
or
e 
dif
fe
re
nc
e
baseline BLEU
Figure 7: Correlation between baseline BLEU
scores and BLEU score gains with caching
The baseline performance does not seem to give
any clues for a possible success of caching. This
comes as a surprise as our intuitions suggested that
good baseline performance should be essential for
the adaptive approach.
Another reason for their success should be the
amount of repetition (especially among content
words) in the documents to be translated. An in-
dication for this can be given by type/token ratios
assuming that documents with lower ratios contain
a larger amount of repetitive text. Figure 8 plots
the type/token ratios of all test documents in com-
parison with the BLEU score gains obtained with
caching.
-2
-1
 0
 1
 2
 3
 4
 5
 0.25  0.3  0.35  0.4  0.45  0.5  0.55
BL
EU
 sc
or
e 
dif
fe
re
nc
e
type/token ratio
Figure 8: Correlation between type/token ratios
and BLEU score gains with caching
Once again there does not seem to be any obvi-
ous correlation. So far we could not identify any
particular property of documents that might help
to reliably predict the success of caching. The an-
swer is probably a combination of various factors.
Further experiments are needed to see the effect on
different data sets and document types.
Note that some results may also be an artifact
of the automatic evaluation metrics applied. Qual-
itative evaluations using manual inspection could
probably reveal important aspects of the caching
approach. However, tracing changes caused by
caching is rather difficult due to the interaction
with other factors in the global decoding process.
Some typical cases may still be identified. Fig-
ure 9 shows an example of a translation that has
been improved in the cached model by making the
translation more consistent (this is from a docu-
ment that actually got a lower BLEU score in the
end with caching).
baseline: report ( evaluation of european public epar )
vivanza
in the short epar public
this document is a summary of the european public to
evaluation report ( epar ) .
cache: report european public assessment ( epar )
vivanza
epar to sum up the public
this document is a summary of the european public as-
sessment report ( epar ) .
reference: european public assessment report ( epar )
vivanza
epar summary for the public
this document is a summary of the european public as-
sessment report ( epar ) .
Figure 9: A translation improved by caching.
Other improvements may not be recognized by au-
tomatic evaluation metrics and certain acceptable
differences may be penalized. Look, for instance,
at the examples in figure 10.
This is, of course, not a general claim that
cache-based translations are more effected by this
problem than, for example, the baseline system.
However, this could be a direction for further in-
vestigations to quantify these issues.
6 Conclusions
In this paper we presented adaptive language and
translation models that use an exponentially de-
caying cache. We applied these models to a do-
main adaptation task translating medical docu-
ments with a standard model trained on Europarl.
On average the dynamic adaptation approach led
to a gain of about 2.6% relative BLEU points per
sentence. The main advantage of this approach is
that it does not require any domain-specific train-
13
baseline: the medication is issued on orders .
cache: the medication is issued on prescription-only .
reference: the medicine can only be obtained with a prescription .
baseline: benefix is a powder keg , and a solvent to dissolve the injection for .
cache: benefix consists of a powder and a solvent to dissolve the injection for .
reference: benefix is a powder and solvent that are mixed together for injection .
baseline: the principle of active benefix is the nonacog alfa ( ix coagulation factor of recombinant ) which favours
the coagulation blood .
cache: the principle of benefix is the nonacog alfa ( ix coagulation factor of recombinant ) which favours the
coagulation blood .
reference: benefix contains the active ingredient nonacog alfa ( recombinant coagulation factor ix , which helps
blood to clot ) .
baseline: in any case , it is benefix used ?
cache: in which case it is benefix used ?
reference: what is benefix used for ?
baseline: benefix is used for the treatment and prevention of saignements among patients with haemophilia b ( a
disorder he?morragique hereditary due to a deficiency in factor ix ) .
cache: benefix is used for the treatment and prevention of saignements among patients suffering haemophilia
b ( a disorder he?morragique hereditary due to a lack factor in ix ) .
reference: benefix is used for the treatment and prevention of bleeding in patients with haemophilia b ( an inher-
ited bleeding disorder caused by lack of factor ix ) .
baseline: benefix can be used for adults and children over 6 years .
cache: benefix can be used for adults and children of more than 6 years
reference: benfix can be used in adults and children over the age of 6.
Figure 10: Examples translations with and without caching.
ing, tuning (assuming that interpolation weights
and other cache parameters can be fixed after some
initial experiments) nor the incorporation of any
other in-domain resources. Cache based adapta-
tion can directly be applied to any new domain
and similar gains should be possible. However, a
general conclusion cannot be drawn from our ini-
tial results presented in this paper. Further exper-
iments are required to verify these findings and to
explore the potentials of cache-based techniques.
The main obstacle is the invalid assumption that
initial translations are correct. The success of the
entire method crucially depends on this assump-
tion. Error propagation and the reinforcement of
wrong decisions is the largest risk. Therefore,
strategies to reduce noise in the cache are impor-
tant and can still be improved using better selec-
tion criteria. A possible strategy could be to iden-
tify simple cases in a first run that can be used
to reliably fill the cache and to use the full cache
model on the entire text in a second run. Another
idea for improvement is to attach weights to cache
entries according to the translation costs assigned
by the model. These weights could easily be incor-
porated into the cache scores returned for match-
ing items. In future, we would like to explore these
ideas and also possibilities to combine cache mod-
els with other types of adaptation techniques.
References
Jerome R. Bellegarda. 2004. Statistical language
model adaptation: review and perspectives. Speech
Communication, 42:93?108.
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation
with monolingual resources. In StatMT ?09: Pro-
ceedings of the Fourth Workshop on Statistical Ma-
chine Translation, pages 182?189, Morristown, NJ,
USA. Association for Computational Linguistics.
Jorge Civera and Alfons Juan. 2007. Domain adap-
tation in statistical machine translation with mixture
modelling. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 177?180,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
P.R. Clarkson and A. J. Robinson. 1997. Language
model adaptation using mixtures and an exponen-
tially decaying cache. In International Confer-
ence on Acoustics, Speech, and Signal Processing
(ICASSP), pages 799?802, Munich, Germany.
Andrew Finch and Eiichiro Sumita. 2008. Dynamic
model interpolation for statistical machine transla-
tion. In StatMT ?08: Proceedings of the Third Work-
shop on Statistical Machine Translation, pages 208?
215, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for SMT. In Proceedings of the
Second Workshop on Statistical Machine Transla-
tion, pages 128?135, Prague, Czech Republic. As-
sociation for Computational Linguistics.
14
William A. Gale, Kenneth W. Church, and David
Yarowsky. 1992. One sense per discourse. In HLT
?91: Proceedings of the workshop on Speech and
Natural Language, pages 233?237, Morristown, NJ,
USA. Association for Computational Linguistics.
Almut Silja Hildebrand, Matthias Eck, Stephan Vo-
gel, and Alex Waibel. 2005. Adaptation of the
translation model for statistical machine translation
based on information retrieval. In Proceedings of
the 10th Conference of the European Association for
Machine Translation (EAMT), pages 133?142, Bu-
dapest.
Philipp Koehn and Christof Monz, editors. 2006. Pro-
ceedings on the Workshop on Statistical Machine
Translation. Association for Computational Lin-
guistics, New York City, June.
Philipp Koehn and Josh Schroeder. 2007. Experi-
ments in domain adaptation for statistical machine
translation. In Proceedings of the Second Workshop
on Statistical Machine Translation, pages 224?227,
Prague, Czech Republic. Association for Computa-
tional Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
ACL ?07: Proceedings of the 45th Annual Meet-
ing of the ACL on Interactive Poster and Demon-
stration Sessions, pages 177?180, Morristown, NJ,
USA. Association for Computational Linguistics.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of the
10th Machine Translation Summit (MT Summit X).
Roland Kuhn and Renato De Mori. 1990. A cache-
based natural language model for speech recogni-
tion. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 12(6):570?583.
Laurent Nepveu, Lapalme, Guy, Langlais, Philippe,
and George Foster. 2004. Adaptive Language and
Translation Models for Interactive Machine Trans-
lation. In Proceedings of the 9th Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 190?197, Barcelona, Spain.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In ACL ?03:
Proceedings of the 41st Annual Meeting on Asso-
ciation for Computational Linguistics, pages 160?
167, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Martin Raab. 2007. Language Modeling for Machine
Translation. VDM Verlag, Saarbru?cken, Germany.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the 7th
international conference on spoken language pro-
cessing (ICSLP 2002), pages 901?904, Denver, CO,
USA.
Jo?rg Tiedemann. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In Recent Advances in Natural Language
Processing, volume V, pages 237?248. John Ben-
jamins, Amsterdam/Philadelphia.
Nicola Ueffing, Gholamreza Haffari, and Anoop
Sarkar. 2007. Semi-supervised model adaptation
for statistical machine translation. Machine Trans-
lation, 21(2):77?94.
Bing Zhao, Matthias Eck, and Stephan Vogel. 2004.
Language model adaptation for statistical machine
translation with structured query models. In COL-
ING ?04: Proceedings of the 20th international con-
ference on Computational Linguistics, page 411,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
15
Proceedings of the 6th Workshop on Ontologies and Lexical Resources (Ontolex 2010), pages 28?37,
Beijing, August 2010
Finding Medical Term Variations using Parallel Corpora and
Distributional Similarity
Lonneke van der Plas
Department of Linguistics
University of Geneva
lonneke.vanderplas@unige.ch
Jo?rg Tiedemann
Department of Linguistics and Philology
Uppsala University
jorg.tiedemann@lingfil.uu.se
Abstract
We describe a method for the identifica-
tion of medical term variations using par-
allel corpora and measures of distribu-
tional similarity. Our approach is based
on automatic word alignment and stan-
dard phrase extraction techniques com-
monly used in statistical machine transla-
tion. Combined with pattern-based filters
we obtain encouraging results compared
to related approaches using similar data-
driven techniques.
1 Introduction
Ontologies provide a way to formally represent
knowledge, for example for a specific domain.
Ontology building has received a lot of atten-
tion in the medical domain. This interest is re-
flected in the existence of numerous medical on-
tologies, such as the Unified Medical Language
System (UMLS) (McCray and Hole, 1990) with
its metathesaurus, semantic network, and special-
ist lexicon. Although the UMLS includes infor-
mation for languages other than English, the cov-
erage for other languages is generally smaller.
In this paper we describe an approach to acquire
lexical information for the Dutch medical domain
automatically. In the medical domain variations in
terminology often include multi-word terms such
as aangeboren afwijking ?birth defect? for con-
genitale aandoening ?congenital disorder?. These
multiple ways to refer to the same concept using
distinct (multi-word) terms are examples of syn-
onymy1 but are often referred to as term varia-
1Spelling variants are a type of term variations that are
not included in the definition of synonymy.
tions. These term variations could be used to en-
hance existing medical ontologies for the Dutch
language.
Our technique builds on the distributional hy-
pothesis, the idea that semantically related words
are distributed similarly over contexts (Harris,
1968). This is in line with the Firthian saying that,
?You shall know a word by the company it keeps.?
(Firth, 1957). In other words, you can grasp the
meaning of a word by looking at its contexts.
Context can be defined in many ways. Previous
work has been mainly concerned with the syntac-
tic contexts a word is found in (Lin, 1998; Cur-
ran, 2003). For example, the verbs that are in
a subject relation with a particular noun form a
part of its context. In accordance with the Firthian
tradition these contexts can be used to determine
the semantic relatedness of words. For instance,
words that occur in a object relation with the verb
to drink have something in common: they are liq-
uid. Other work has been concerned with the bag-
of-word context, where the context of a word are
the words that are found in its proximity (Wilks et
al., 1993; Schu?tze, 1992).
Yet another context, that is much less studied, is
the translational context. The translational context
of a word is the set of translations it gets in other
languages. For example, the translational context
of cat is kat in Dutch and chat in French. This
requires a rather broad understanding of the term
context. The idea is that words that share a large
number of translations are similar. For example
both autumn and fall get the translation herfst in
Dutch, Herbst in German, and automne in French.
This indicates that autumn and fall are synonyms.
28
A straightforward place to start looking for
translational context is in bilingual dictionaries.
However, these are not always publicly available
for all languages. More importantly, dictionar-
ies are static and therefore often incomplete re-
sources. We have chosen to automatically acquire
word translations in multiple languages from text.
Text in this case should be understood as multi-
lingual parallel text. Automatic alignment gives
us the translations of a word in multiple lan-
guages. The so-called alignment-based distribu-
tional methods described in Van der Plas (2008)
apply the translational context for the discovery
of single word synonyms for the general domain.
Any multilingual parallel corpus can be used for
this purpose. It is thus possible to focus on
a special domain, such as the medical domain
we are considering in this paper. The automatic
alignment provides us also with domain-specific
frequency information for every translation pair,
which is helpful in case words are ambiguous.
Aligned parallel corpora have often been used
in the field of word sense discovery, the task of
discriminating the different senses words have.
The idea behind it is that a word that receives dif-
ferent translations might be polysemous. For ex-
ample, a word such as wood receives the transla-
tion woud and hout in Dutch, the former referring
to an area with many trees and the latter referring
to the solid material derived from trees. Whereas
this type of work is all built upon the divergence of
translational context, i.e. one word in the source
language is translated by many different words in
the target language, we are interested in the con-
vergence of translations, i.e. two words in the
source language receiving the same translation in
the target language. Of course these two phenom-
ena are not independent. The alleged conversion
of the target language might well be a hidden di-
version of the source language. Since the English
word might be polysemous, the fact that woud and
hout in Dutch are both translated in English by
wood does not mean that woud and hout in Dutch
are synonyms. However, the use of multiple lan-
guages overshadows the noise resulting from pol-
ysemy (van der Plas, 2008).
Van der Plas (2008) shows that the way the
context is defined influences the type of lexico-
semantic knowledge that is discovered. After
gold standard evaluations and manual inspection
the author concludes that when using translational
contexts more tight semantic relations such as
synonymy are found whereas the conventional
syntax-based approaches retrieve hypernyms, co-
hyponyms, and antonyms of the target word. The
performance on synonym acquisition when using
translational contexts is almost twice as good as
when using syntactic contexts, while the amount
of data used is much smaller. Van der Plas (2008)
ascribed the fact that the syntax-based method be-
haves in this way to the fact that loosely related
words, such as wine and beer, are often found in
the same syntactic contexts. The alignment-based
method suffers less from this indiscriminant ac-
ceptance because words are typically translated by
words with the same meaning. The word wine is
typically not translated with a word for beverage
nor with a word for beer, and neither is good trans-
lated with the equivalence of bad.
In this paper we are concerned with medical
term variations that are in fact (multi-word) syn-
onyms. We will use the translational context to
compute similarity between terms. The transla-
tional context is not only very suitable to find
tight relations between words, the transition from
single-word synonyms to multi-word term varia-
tions is also straightforward due to advances in
phrase-based machine translation. We will use
word alignment techniques in combination with
phrase extraction techniques from statistical ma-
chine translation to extract phrases and their trans-
lations from a medical parallel corpus. We com-
bine this approach with Part-of-Speech (PoS) pat-
terns from the term extraction literature to extract
candidate terms from the phrase tables. Using
similarity measures used in distributional methods
we finally compute ranked lists of term variations.
We already noted that these term variations
could be used to enhance existing ontologies for
the Dutch language. On top of that we believe that
the multi-lingual method that uses translations of
multi-word terms in several languages could be
used to expand resources built for English with
translations in other languages (semi-) automati-
cally. This last point falls outside the scope of this
paper.
29
In the following section we will describe the
alignment-based approaches to distributional sim-
ilarity. In section 3 we will describe the method-
ology we followed in this paper in detail. We de-
scribe our evaluation in section 4 and discuss the
results in section 5. Section 6 concludes this pa-
per.
2 Alignment-based methods
In this section we explain the alignment-based ap-
proaches to distributional similarity. We will give
some examples of translational context and we
will explain how measures serve to determine the
similarity of these contexts. We end this section
with a discussion of related work.
2.1 Translational context
The translational context of a word or a multi-
word term is the set of translations it gets in other
languages. For the acquisition of translations for
the Dutch medical terms we rely on automatic
word alignment in parallel corpora.
Figure 1: Example of bidirectional word align-
ments of two parallel sentences
Figure 1 illustrates the automatic word alignment
between a Dutch and an English phrase as a re-
sult of using the IBM alignment models (Brown
et al, 1993) implemented in the open-source tool
GIZA++ (Och, 2003). The alignment of two texts
is bi-directional. The Dutch text is aligned to
the English text and vice versa (dotted lines ver-
sus continuous lines). The alignment models pro-
duced are asymmetric. Several heuristics exist
to combine directional word alignments which is
usually called ?symmetrization?. In order to cover
multi-word terms standard phrase extraction tech-
niques can be used to move from word alignment
to linked phrases (see section 3.2 for more de-
tails).
2.2 Measures for computing similarity
Translational co-occurrence vectors are used to
find distributionally similar words. For ease of
reading, we give an example of a single-word
term kat in Table 1. In our current setting the
terms can be both single- or multi-word terms
such as werkzame stof ?active ingredient?. Ev-
ery cell in the vector refers to a particular transla-
tional co-occurrence type. For example, kat ?cat?
gets the translation Katze in German. The value
of these cells indicate the number of times the co-
occurrence type under consideration is found in
the corpus.
Each co-occurrence type has a cell frequency.
Likewise each head term has a row frequency.
The row frequency of a certain head term is the
sum of all its cell frequencies. In our example the
row frequency for the term kat ?cat? is 65. Cut-
offs for cell and row frequency can be applied to
discard certain infrequent co-occurrence types or
head terms respectively.
DE FR IT EN total
Katze chat gatto cat
kat 17 26 8 13 64
Table 1: Translational co-occurrence vector for
kat (?cat?) based on four languages
The more similar the vectors are, the more dis-
tributionally similar the head terms are. We need a
way to compare the vectors for any two head terms
to be able to express the similarity between them
by means of a score. Various methods can be used
to compute the distributional similarity between
terms. We will explain in section 3 what measures
we have chosen in the current experiments.
2.3 Related work
Multilingual parallel corpora have mostly been
used for tasks related to word sense disambigua-
tion such as separation of senses (Resnik and
Yarowsky, 1997; Dyvik, 1998; Ide et al, 2002).
However, taking sense separation as a basis,
Dyvik (2002) derives relations such as synonymy
and hyponymy by applying the method of se-
mantic mirrors. The paper illustrates how the
method works. First, different senses are iden-
tified on the basis of manual word translations
in sentence-aligned Norwegian-English data (2,6
million words in total). Second, senses are
grouped in semantic fields. Third, features are
30
assigned on the basis of inheritance. Lastly, se-
mantic relations such synonymy and hyponymy
are detected based on intersection and inclusion
among feature sets .
Improving the syntax-based approach for syn-
onym identification using bilingual dictionaries
has been discussed in Lin et al (2003) and Wu and
Zhou (2003). In the latter parallel corpora are also
applied as a reference to assign translation likeli-
hoods to candidates derived from the dictionary.
Both of them are limited to single-word terms.
Some researchers employ multilingual corpora
for the automatic acquisition of paraphrases (Shi-
mota and Sumita, 2002; Bannard and Callison-
Burch, 2005; Callison-Burch, 2008). The last two
are based on automatic word alignment as is our
approach.
Bannard and Callison-Burch (2005) use a
method that is also rooted in phrase-based statis-
tical machine translation. Translation probabili-
ties provide a ranking of candidate paraphrases.
These are refined by taking contextual informa-
tion into account in the form of a language model.
The Europarl corpus (Koehn, 2005) is used. It has
about 30 million words per language. 46 English
phrases are selected as a test set for manual evalu-
ation by two judges. When using automatic align-
ment, the precision reached without using contex-
tual refinement is 48.9%. A precision of 55.3%
is reached when using context information. Man-
ual alignment improves the performance by 26%.
A precision score of 55% is attained when using
multilingual data.
In a more recent publication Callison-Burch
(2008) improved this method by using syntac-
tic constraints and multiple languages in parallel.
We have implemented a combination of Bannard
and Callison-Burch (2005) and Callison-Burch
(2008), in which we use PoS filters instead of
syntactic constraints to compare our results with.
More details can be found in the Section 5.
Apart from methods that use parallel corpora
mono-lingual pattern-based methods have been
used to find term variations. Fahmi (2009) ac-
quired term variation for the medical domain us-
ing a two-step model. As a first step an initial list
of synonyms are extracted using a method adapted
from DIPRE (Brin, 99). During this step syntactic
patterns guide the extraction of candidate terms in
the same way as they will guide the extraction in
this paper. This first step results in a list of candi-
date synonyms that are further filtered following a
method described in Lin et al (2003), which uses
Web pages as an external source to measure the
synonym compatibility hits of each pair. The pre-
cision and recall scores presented in Fahmi (2009)
are high. We will give results for this method
on our test set in Section 5 and refer to it as the
pattern- and web-based approach.
3 Materials and methods
In the following subsections we describe the setup
for our experiments.
3.1 Data collection
Measures of distributional similarity usually re-
quire large amounts of data. For the alignment
method we need a parallel corpus of reasonable
size with Dutch either as source or as target lan-
guage coming from the domain we are interested
in. Furthermore, we would like to experiment
with various languages aligned to Dutch.
The freely available EMEA corpus (Tiede-
mann, 2009) includes 22 languages in parallel
with a reasonable size of about 12-14 million to-
kens per language. The entire corpus is aligned
at the sentence level for all possible combinations
of languages. Thus, for acquiring Dutch syn-
onyms we have 21 language pairs with Dutch as
the source language. Each language pair includes
about 1.1 million sentence pairs. Note that there
is a lot of repetition in EMEA and the number
of unique sentences (sentence fragments) is much
smaller: around 350,000 sentence pairs per lan-
guage pair with about 6-7 million tokens per lan-
guage.
3.2 Word alignment and phrase extraction
For sentence alignment we applied hunalign
(Varga et al, 2005) with the ?realign? function that
induces lexical features from the bitext to be com-
bined with length based features. Word alignment
has been performed using GIZA++ (Och, 2003).
We used standard settings defined in the Moses
toolkit (Koehn et al, 2007) to generate Viterbi
word alignments of IBM model 4 for sentences
31
not longer than 80 tokens. In order to improve
the statistical alignment we used lowercased to-
kens and lemmas in case we had them available
(produced by the Tree-Tagger (Schmid, 1994) and
the Alpino parser (van Noord, 2006)).
We used the grow heuristics to combine the
asymmetric word alignments which starts with
the intersection of the two Viterbi alignments and
adds block-neighboring points to it in a second
step. In this way we obtain high precision links
with some many-to-many alignments. Finally we
used the phrase extraction tool from Moses to ex-
tract phrase correspondences. Phrases in statisti-
cal machine translation are defined as sequences
of consecutive words and phrase extraction refers
to the exhaustive extraction of all possible phrase
pairs that are consistent with the underlying word
alignment. Consistency in this case means that
words in a legal phrase are only aligned to words
in the corresponding phrase and not to any other
word outside of that phrase. The extraction mech-
anism can be restricted by setting a maximum
phrase length which is seven in the default set-
tings of Moses. However, we set the maximum
phrase length to four, because we do not expect
many terms in the medical domain to be longer
than 4 words.
As explained above, word alignment is carried
out on lowercased and possibly lemmatised ver-
sions of the corpus. However, for phrase extrac-
tion, we used surface wordforms and extracted
them along with the part-of-speech (PoS) tags for
Dutch taken from the corresponding Alpino parse
trees. This allows us to lowercase all words except
the words that have been tagged as name. Further-
more, the inclusion of PoS tags enabled us to fil-
ter the resulting phrase table according to typical
patterns of multi-word terms. We also removed
phrases that consist of only non-alphabetical char-
acters. Note that we rely entirely on automatic
processing of our data. Thus, the results from
automatic tagging, lemmatisation and word align-
ment include errors. Bannard and Callison-Burch
(2005) show that when using manual alignment
the percentage of correct paraphrases significantly
rises from 48.9% to 74.9%.
3.3 Selecting candidate terms
As we explained above we can select those
phrases that are more likely to be good terms
by using a regular expression over PoS tags.
We apply a pattern using adjectives (A), nouns
(NN), names (NM) and prepositions (P) as its
components based on Justeson and Katz. (1995)
which was adapted to Dutch by Fahmi (2009):
((A|NN|NM)+|(((A|NN|NM)*
(NN|NM P)?)(A|NN|NM)*))NN+
To explain this regular expression in words, a
candidate term is either a sequence of adjectives
and/or nouns and/or names, ending in a noun or
name or it consists of two such strings, separated
by a single preposition.
After applying the filters and removing all ha-
paxes we are left with 9.76 M co-occurrences of a
Dutch (multi-word) term and a foreign translation.
3.4 Comparing vectors
To compare the vectors of the terms we need a
similarity measures. We have chosen to describe
the functions used in this paper using an extension
of the notation used by Lin (1998), adapted by
Curran (2003). Co-occurrence data is described
as tuples: ?word, language, word??, for example,
?kat, EN, cat?.
Asterisks indicate a set of values ranging over
all existing values of that component of the rela-
tion tuple. For example, (w, ?, ?) denotes for a
given word w all translational contexts it has been
found in in any language. For the example of
kat in, this would denote all values for all transla-
tional contexts the word is found in: Katze DE:17,
chat FR:26 etc. Everything is defined in terms
of co-occurrence data with non-zero frequencies.
The set of attributes or features for a given corpus
is defined as:
(w, ?, ?) ? {(r, w?)|?(w, r, w?)}
Each pair yields a frequency value, and the se-
quence of values is a vector indexed by r:w? val-
ues, rather than natural numbers. A subscripted
asterisk indicates that the variables are bound to-
gether:
?
(wm, ?r, ?w?) ? (wn, ?r, ?w?)
32
The above refers to a dot product of the vectors
for term wm and term wn summing over all the
r:w? pairs that these two terms have in common.
For example we could compare the vectors for kat
and some other term by applying the dot product
to all bound variables.
We have limited our experiments to using Co-
sine2. We chose this measure, since it performed
best in experiments reported in Van der Plas
(2008). Cosine is a geometrical measure. It re-
turns the cosine of the angle between the vectors
of the words and is calculated as the dot product
of the vectors:
Cosine =
? (W1, ?r, ?w?) ? (W2, ?r, ?w?)?? (W1, ?, ?)2 ?? (W2, ?, ?)2
If the two words have the same distribution the
angle between the vectors is zero.
3.5 Post-processing
A well-known problem of phrase-based meth-
ods to paraphrase or term variation acquisition
is the fact that a large proportion of the term
variations or paraphrases proposed by the sys-
tem are super- or sub-strings of the original term
(Callison-Burch, 2008). To remedy this prob-
lem we removed all term variations that are ei-
ther super- or sub-strings of the original term from
the lists of candidate term variations output by the
system.
4 Evaluation
There are several evaluation methods available to
assess lexico-semantic data. Curran (2003) distin-
guishes two types of evaluation: direct evaluation
and indirect evaluation. Direct evaluation meth-
ods compare the semantic relations given by the
2Feature weights have been used in previous work for
syntax-based methods to account for the fact that co-
occurrences have different information values. Selectionally
weak (Resnik, 1993) or light verbs such as hebben ?to have?
have a lower information value than a verb such as uitpersen
?squeeze? that occurs less frequently. Although weights that
promote features with a higher information value work very
well for syntax-based methods, Van der Plas (2008) showed
that weighting only helps to get better synonyms for very in-
frequent nouns when applied in alignment-based approaches.
In the current setting we do not consider very infrequent
terms so we did not use any weighting.
system against human performance or expertise.
Indirect approaches evaluate the system by mea-
suring its performance on a specific task.
Since we are not aware of a task in which we
could test the term variations for the Dutch medi-
cal domain and ad-hoc human judgments are time
consuming and expensive, we decided to com-
pare against a gold standard. Thereby denying
the common knowledge that the drawback of us-
ing gold standard evaluations is the fact that gold
standards often prove to be incomplete. In previ-
ous work on synonym acquisition for the general
domain, Van der Plas and Tiedemann (2006) used
the synsets in Dutch EuroWordnet (Vossen, 1998)
for the evaluation of the proposed synonyms. In
an evaluation with human judgments, Van der Plas
and Tiedemann (2006) showed that in 37% of the
cases the majority of the subjects judged the syn-
onyms proposed by the system to be correct even
though they were not found to be synonyms in
Dutch EuroWordnet. For evaluating medical term
variations in Dutch there are not many gold stan-
dards available. Moreover, the gold standards that
are available are even less complete than for the
general domain.
4.1 Gold standard
We have chosen to evaluate the nearest neighbours
of the alignment-based method on the term vari-
ations from the Elseviers medical encyclopedia
which is intended for the general audience con-
taining 379K words. The encyclopedia was made
available to us by Spectrum B.V.3.
The test set is comprised of 848 medical terms
from aambeeld ?incus? to zwezerik ?thymus? and
their term variations. About 258 of these entries
contain multiword terms. For most of the terms
the list from Elseviers medical encyclopedia gives
only one term variation, 146 terms have two term
variations and only one term has three variations.
For each of these medical terms in the test set the
system generates a ranked list of term variations
that will be evaluated against the term variations
in the gold standard.
3http://www.kiesbeter.nl/medischeinformatie/
33
5 Results and Discussion
Before we present our results and give a detailed
error analysis we would like to remind the reader
of the two methods we compare our results with
and give some more detail on the implementation
of the second method.
5.1 Two methods for comparison
The first method is the pattern- and web-based ap-
proach described in Fahmi (2009). Note that we
did not re-implement the method, so we were not
able to run the method on the same corpus we
are using in our experiments. The corpus used
in Fahmi (2009) is a medical corpus developed
in Tilburg University (http://ilk.uvt.nl/rolaquad).
It consists of texts from a medical encyclopedia
and a medical handbook and contains 57,004 sen-
tences. The system outputs a ranked list of term
variation pairs. We selected the top-100 pairs
that are output by the system and evaluated these
on the test set described in Subsection 4.1. The
method is composed of two main steps. In the
first step candidate terms are extracted from the
corpus using a PoS filter, that is similar to the
PoS filter we applied. In the second step pairs of
candidate term variations are re-ranked on the ba-
sis of information from the Web. Phrasal patterns
such as XorY are used to get synonym compat-
ibility hits as opposed to XandY that points to
non-synonymous terms.
The second method we compare with is the
phrase-based translation method first introduced
by Bannard and Callison-Burch (2005). Statisti-
cal word alignment can be used to measure the re-
lation between source language items. Here, one
makes use of the estimated translation likelihoods
of phrases (p(f |e) and p(e|f)) that are used to
build translation models in standard phrase-based
statistical machine translation systems (Koehn et
al., 2007). Bannard and Callison-Burch (2005)
define the problem of paraphrasing as the follow-
ing search problem:
e?2 = argmaxe2:e2 6=e1p(e2|e1) where
p(e2|e1) ?
?
f
p(f |e1)p(e2|f)
Certainly, for paraphrasing we are not only inter-
ested in e?2 but for the top-ranked paraphrase can-
didates but this essentially does not change the al-
gorithm. In their paper, Bannard and Callison-
Burch (2005) also show that systematic errors
(usually originating from bad word alignments)
can be reduced by summing over several language
pairs.
e?2 ? argmaxe2:e2 6=e1
?
C
?
fC
p(fC |e1)p(e2|fC)
This is the approach that we also adapted for our
comparison. The only difference in our imple-
mentation is that we applied a PoS-filter to extract
candidate terms as explained in section 3.3. In
some sense this is a sort of syntactic constraint in-
troduced in Callison-Burch (2008). Furthermore,
we set the maximum phrase length to 4 and ap-
plied the same post-processing as described in
Subsection 3.5 to obtain comparable results.
5.2 Results
Table 2 shows the results for our method com-
pared with the method adapted from Bannard and
Callison-Burch (2005) and the method by Fahmi
(2009). Precision and recall are given at several
values of k. At k=1, only the top-1 term varia-
tions the system proposes are taken into account.
At k=3 the top-3 candidate term variations are in-
cluded in the calculations.
The last column shows the coverage of the sys-
tem. A coverage of 40% means that for 40% of the
850 terms in the test set one or more term varia-
tions are found. Recall is measured for the terms
covered by the system.
From Table 2 we can read that the method we
propose is able to get about 30% of the term vari-
ations right, when only the top-1 candidates are
considered. It is able to retrieve roughly a quarter
of the term variations provided in the gold stan-
dard4. If we increase k precision goes down and
recall goes up. This is expected, because the sys-
tem proposes a ranked list of candidate term vari-
ations so at higher values of k the quality is lower,
but more terms from the gold standard are found.
4Note that a recall of 100% is not possible, because some
terms have several term variations.
34
Method k=1 k=2 k=3 Coverage
P R P R P R
Phrase-based Distr. Sim 28.9 22.8 21.8 32.7 17.3 37.2 40.0
Bannard&Callison-Burch (2005) 18.4 15.3 16.9 27.3 13.7 32.3 48.1
Fahmi (2009) 38.2 35.1 37.1 35.1 37.1 35.1 4.0
Phrase-based Distr. Sim (hapaxes) 25.4 20.9 20.4 32.1 16.1 36.8 47.8
Table 2: Percent precision and recall at several values of k and percent coverage for the method pro-
posed in this paper (plus a version including hapaxes), the method adapted from Bannard and Callison-
Burch (2005) and the output of the system proposed by Fahmi (2009)
In comparison, the scores resulting from our
adapted implementation of Bannard and Callison-
Burch (2005) are lower. They do however, man-
age to find more terms from the test set covering
around 48% of the words in the gold standard.
This is due to the cut-off that we use when cre-
ating the co-occurrence vector to remove unreli-
able data points. In our approach we discarded
hapaxes, whereas for the Bannard and Callison-
Burch approach the entire phrase table is used.
We therefore ran our system once again without
this cut-off. As expected, the coverage went up
in that setting ? actually to 48% as well.5 How-
ever, we can see that the precision and recall re-
mained higher, than the scores we got with the
implementation following Bannard and Callison-
Burch (2005). Hence, our vector-based approach
seems to outperform the direct use of probabilities
from phrase-based MT.
Finally, we also compare our results with the
data set extracted using the pattern- and web-
based approach from Fahmi (2009). The precision
and recall figures of that data set are the highest in
our comparison. However, since the coverage of
this method is very low (which is not surprising
since a smaller corpus is used to get these results)
the precision and recall are calculated on the ba-
sis of a very small number of examples (35 to be
precise). The results are therefore not very reli-
able. The precision and recall figures presented
in Fahmi (2009), however, point in the same di-
rection. To get an idea of the actual coverage of
this method we would need to apply this extrac-
tion technique to the EMEA corpus. This is espe-
cially difficult due to the heavy use of web queries
5The small difference in coverage is due to some mistakes
in tokenisation for our method.
which makes it problematic to apply this method
to large data sets.
5.3 Error analysis
The most important finding we did, when closely
inspecting the output of the system is that many of
the term variations proposed by the system are not
found in the gold standard, but are in fact correct.
Here, we give some examples below:
arts, dokter (?doctor?)
ademnood, ademhalingsnood (?respiratory distress?)
aangezichtsverlamming, gelaatsparalyse (?facial paralysis?)
alvleesklierkanker, pancreaskanker (?cancer of the pan-
creas?)
The scores given in Table 2 are therefore pes-
simistic and a manual evaluation with domain spe-
cialist would certainly give us more realistic and
probably much higher scores. We also found some
spelling variants which are usually not covered by
the gold standard. Look, for instance, at the fol-
lowing examples:
astma, asthma (?asthma?)
atherosclerose, Artherosclerosis (?atherosclerosis?)
autonoom zenuwstelsel, autonome zenuwstelsel (?autonomic
nervous system?)
Some mistakes could have been avoided using
stemming or proper lemmatisation (plurals that
are counted as wrong):
abortus, zwangerschapsafbrekingen (?abortion?)
adenoom, adenomen (?adenoma?)
indigestie, spijsverteringsstoornissen (?indigestion?)
After removing the previous cases from the data,
some of the remaining mistakes are related to the
problem we mentioned in section 3.5: Phrase-
35
based methods to paraphrase or term variation ac-
quisition have the tendency to propose term vari-
ations that are super- or sub-strings of the origi-
nal term. We were able to filter out these super-
or sub-strings, but not in cases where a candidate
term is a term variation of a super- or sub-string of
the original term. Consider, for example the term
bloeddrukverlaging ?blood pressure decrease? and
the candidate afname ?decrease?, where afname is
a synonym for verlaging.
6 Conclusions
In this article we have shown that translational
context together with measures of distributional
similarity can be used to extract medical term vari-
ations from aligned parallel corpora. Automatic
word alignment and phrase extraction techniques
from statistical machine translation can be applied
to collect translational variations across various
languages which are then used to identify seman-
tically related words and phrases. In this study, we
additionally apply pattern-based filters using part-
of-speech labels to focus on particular patterns of
single and multi-word terms. Our method out-
performs another alignment-based approach mea-
sured on a gold standard taken from a medical en-
cyclopedia when applied to the same data set and
using the same PoS filter. Precision and recall are
still quite poor according to the automatic evalu-
ation. However, manual inspection suggests that
many candidates are simply misjudged because of
the low coverage of the gold standard data. We
are currently setting up a manual evaluation. Alto-
gether our approach provides a promising strategy
for the extraction of term variations using straight-
forward and fully automatic techniques. We be-
lieve that our results could be useful for a range of
applications and resources and that the approach
in general is robust and flexible enough to be ap-
plied to various languages and domains.
Acknowledgements
The research leading to these results has received
funding from the EU FP7 programme (FP7/2007-
2013) under grant agreement nr 216594 (CLAS-
SIC project: www.classic-project.org).
References
Bannard, C. and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. In Proceedings
of the annual Meeting of the Association for Com-
putational Linguistics (ACL).
Brin, S. 99. Extracting patterns and relations from the
World Wide Web. In WebDB ?98: Selected papers
from the International Workshop on The World Wide
Web and Databases.
Brown, P.F., S.A. Della Pietra, V.J. Della Pietra, and
R.L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational Linguistics, 19(2):263?296.
Callison-Burch, C. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of EMNLP.
Curran, J.R. 2003. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
Dyvik, H. 1998. Translations as semantic mirrors.
In Proceedings of Workshop Multilinguality in the
Lexicon II (ECAI).
Dyvik, H. 2002. Translations as semantic mirrors:
from parallel corpus to wordnet. Language and
Computers, Advances in Corpus Linguistics. Pa-
pers from the 23rd International Conference on En-
glish Language Research on Computerized Corpora
(ICAME 23), 16:311?326.
Fahmi, I. 2009. Automatic Term and Relation Extrac-
tion for Medical Question Answering System. Ph.D.
thesis, University of Groningen.
Firth, J.R. 1957. A synopsis of linguistic theory 1930-
1955. Studies in Linguistic Analysis (special vol-
ume of the Philological Society), pages 1?32.
Harris, Z.S. 1968. Mathematical structures of lan-
guage. Wiley.
Ide, N., T. Erjavec, and D. Tufis. 2002. Sense discrim-
ination with parallel corpora. In Proceedings of the
ACL Workshop on Sense Disambiguation: Recent
Successes and Future Directions.
Justeson, J. and S. Katz. 1995. Technical terminol-
ogy: some linguistic properties and an algorithm for
identification in text. Natural Language Engineer-
ing, 1:9?27.
Koehn, P., H. Hoang, A. Birch, C. Callison-Burch,
M.Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A.Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings
of the Annual Meeting of the Association for Com-
putational Linguistics.
36
Koehn, P. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In Proceedings of the MT
Summit, pages 79?86, Phuket, Thailand.
Lin, D., S. Zhao, L. Qin, and M. Zhou. 2003. Identify-
ing synonyms among distributionally similar words.
In Proceedings of the International Joint Confer-
ence on Artificial Intelligence (IJCAI).
Lin, D. 1998. Automatic retrieval and clustering of
similar words. In Proceedings of COLING/ACL.
McCray, A. and W. Hole. 1990. The scope and struc-
ture of the first version of the umls semantic net-
work. In Symposium on Computer Applications in
Primary Care (SCAMC-90), IEEE Computer Soci-
ety, pages 126?130, , Washington DC, IEEE Com-
puter Society. 126-130.
Och, F.J. 2003. GIZA++: Training of sta-
tistical translation models. Available from
http://www.isi.edu/?och/GIZA++.html.
Resnik, P. and D. Yarowsky. 1997. A perspective on
word sense disambiguation methods and their eval-
uation. In Proceedings of ACL SIGLEX Workshop
on Tagging Text with Lexical Semantics: Why, what,
and how?
Resnik, P. 1993. Selection and information. Unpub-
lished doctoral thesis, University of Pennsylvania.
Schmid, Helmut. 1994. Probabilistic part-of-
speech tagging using decision trees. In Pro-
ceedings of International Conference on New
Methods in Language Processing, pages 44?49,
Manchester, UK, September. http://www.ims.uni-
stuttgart.de/?schmid/.
Schu?tze, H. 1992. Dimensions of meaning. In Pro-
ceedings of the ACM/IEEE conference on Super-
computing.
Shimota, M. and E. Sumita. 2002. Automatic para-
phrasing based on parallel corpus for normalization.
In Proceedings of the International Conference on
Language Resources and Evaluation (LREC).
Tiedemann, Jo?rg. 2009. News from OPUS - A collec-
tion of multilingual parallel corpora with tools and
interfaces. In Nicolov, N., K. Bontcheva, G. An-
gelova, and R. Mitkov, editors, Recent Advances
in Natural Language Processing, volume V, pages
237?248, Borovets, Bulgaria. John Benjamins, Am-
sterdam/Philadelphia.
van der Plas, L. and J. Tiedemann. 2006. Finding syn-
onyms using automatic word alignment and mea-
sures of distributional similarity. In Proceedings of
COLING/ACL.
van der Plas. 2008. Automatic lexico-semantic acqui-
sition for question answering. Groningen disserta-
tions in linguistics.
van Noord, G. 2006. At last parsing is now oper-
ational. In Actes de la 13eme Conference sur le
Traitement Automatique des Langues Naturelles.
Varga, D., L. Nmeth, P. Halcsy, A. Kornai, V. Trn, and
V. Nagy. 2005. Parallel corpora for medium density
languages. In Proceedings of RANLP 2005, pages
590?596.
Vossen, P. 1998. EuroWordNet a multilingual
database with lexical semantic networks.
Wilks, Y., D. Fass, Ch. M. Guo, J. E. McDonald,
and B. M. Slator T. Plate. 1993. Providing ma-
chine tractable dictionary tools. Machine Transla-
tion, 5(2):99?154.
Wu, H. and M. Zhou. 2003. Optimizing synonym ex-
traction using monolingual and bilingual resources.
In Proceedings of the International Workshop on
Paraphrasing: Paraphrase Acquisition and Appli-
cations (IWP).
37
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 372?378,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
The Uppsala-FBK systems at WMT 2011
Christian Hardmeier
Jo?rg Tiedemann
Uppsala universitet
Inst. fo?r lingvistik och filologi
Uppsala, Sweden
first.last@lingfil.uu.se
Markus Saers
Human Language
Technology Center
Hong Kong Univ. of
Science & Technology
masaers@cs.ust.hk
Marcello Federico
Mathur Prashant
Fondazione Bruno Kessler
Human Language Technologies
Trento, Italy
lastname@fbk.eu
Abstract
This paper presents our submissions to the
shared translation task at WMT 2011. We
created two largely independent systems
for English-to-French and Haitian Creole-to-
English translation to evaluate different fea-
tures and components from our ongoing re-
search on these language pairs. Key features
of our systems include anaphora resolution,
hierarchical lexical reordering, data selection
for language modelling, linear transduction
grammars for word alignment and syntax-
based decoding with monolingual dependency
information.
1 English to French
Our submission to the English-French task was a
phrase-based Statistical Machine Translation based
on the Moses decoder (Koehn et al, 2007). Phrase
tables were separately trained on Europarl, news
commentary and UN data and then linearly inter-
polated with uniform weights. For language mod-
elling, we used 5-gram models trained with the
IRSTLM toolkit (Federico et al, 2008) on the mono-
lingual News corpus and parts of the English-French
109 corpus. More unusual features of our system
included a special component to handle pronomi-
nal anaphora and the hierarchical lexical reordering
model by Galley and Manning (2008). Selected fea-
tures of our system will be discussed in depth in the
following sections.
1.1 Handling pronominal anaphora
Pronominal anaphora is the use of pronominal ex-
pressions to refer to ?something previously men-
tioned in the discourse? (Strube, 2006). It is a very
common phenomenon found in almost all kinds of
texts. Anaphora can be local to a sentence, or it can
cross sentence boundaries. Standard SMT methods
do not handle this phenomenon in a satisfactory way
at present: For sentence-internal anaphora, they de-
pend on the n-gram language model with its lim-
ited history, while cross-sentence anaphora is left
to chance. We therefore added a word-dependency
model (Hardmeier and Federico, 2010) to our sys-
tem to handle anaphora explicitly.
Our processing of anaphoric pronouns follows
the procedure outlined by Hardmeier and Federico
(2010). We use the open-source coreference resolu-
tion system BART (Broscheit et al, 2010) to link
pronouns to their antecedents in the text. Coref-
erence links are handled differently depending on
whether or not they cross sentence boundaries. If
a coreference link points to a previous sentence, we
process the sentence containing the antecedent with
the SMT system and look up the translation of the
antecedent in the translated output. If the corefer-
ence link is sentence-internal, the translation lookup
is done dynamically by the decoder during search.
In either case, the word-dependency model adds a
feature function to the decoder score representing
the probability of a particular pronoun choice given
the translation of the antecedent.
In our English-French system, this model was
only applied to the inanimate pronouns it and they,
which seemed to be the most promising candidates
for improvement since their French equivalents re-
quire gender marking. It was trained on data au-
tomatically annotated for anaphora taken from the
news-commentary corpus, and the vocabulary of the
predicted pronouns was limited to words recognised
as pronouns by the POS tagger.
372
1.2 Hierarchical lexical reordering
The basic word order model of SMT penalises any
divergence between the order of the words in the in-
put sentence and the order of their translation equiv-
alents in the MT output. All reordering must thus be
driven by the language model when no other reorder-
ing model is present. Lexical reordering models
making certain word order choices in the MT out-
put conditional on the identity of the words involved
have been a standard component in SMT for some
years. The lexical reordering model usually em-
ployed in the Moses decoder was implemented by
Koehn et al (2005). Adopting the perspective of the
SMT decoder, which produces the target sentence
from left to right while covering source phrases in
free order, the model distinguishes between three or-
dering classes, monotone, swap and discontinuous,
depending on whether the source phrases giving rise
to the two last target phrases emitted were adjacent
in the same order, adjacent in swapped order or sep-
arated by other source words. Probabilities for each
ordering class given source and target phrase are
estimated from a word-aligned training corpus and
integrated into MT decoding as extra feature func-
tions.
In our submission, we used the hierarchical lexi-
cal reordering model proposed by Galley and Man-
ning (2008) and recently implemented in the Moses
decoder.1 This model uses the same approach of
classifying movements as monotone, swap or dis-
continuous, but unlike the phrase-based model, it
does not require the source language phrases to be
strictly adjacent in order to be counted as monotone
or swap. Instead, a phrase can be recognised as ad-
jacent to, or swapped with, a contiguous block of
source words that has been segmented into multi-
ple phrases. Contiguous phrase blocks are recog-
nised by the decoder with a shift-reduce parsing al-
gorithm. As a result, fewer jumps are labelled with
the uninformative discontinuous class.
1.3 Data selection from the WMT Giga corpus
One of the supplied language resources for this eval-
uation is the French-English WMT Giga corpus,
1The hierarchical lexical reordering model was imple-
mented in Moses during MT Marathon 2010 by Christian Hard-
meier, Gabriele Musillo, Nadi Tomeh, Ankit Srivastava, Sara
Stymne and Marcello Federico.
 60 80 100 120 140 160 180 200 220 240 260 280
 100  150  200  250  300  350  400 60 80 100 120 140 160 180 200 220 240 260 280LM Perplexity LM size (million 5-grams)Data Selection ThresholdThreshold vs PerplexityThreshold vs LM Size
Figure 1: Perplexity and size of language models trained
on data of the WMT Giga corpus that were selected using
different perplexity thresholds.
aka 109 corpus, a large collection of parallel sen-
tences crawled from Canadian and European Union
sources. While this corpus was too large to be used
for model training with the means at our disposal,
we exploited it as a source of parallel data for trans-
lation model training as well as monolingual French
data for the language model by filtering it down to a
manageable size. In order to extract sentences close
to the news translation task, we applied a simple
data selection procedure based on perplexity. Sen-
tence pairs were selected from the WMT Giga cor-
pus if the perplexity of their French part with respect
to a language model (LM) trained on French news
data was below a given threshold. The rationale is
that text sentences which are better predictable by
the LM should be closer to the news domain. The
threshold was set in a way to capture enough novel
n-grams, from one side, but also to avoid adding too
many irrelevant n-grams. It was tuned by training
a 5-gram LM on the selected data and checking its
size and its perplexity on a development set. In fig-
ure 1 we plot perplexity and size of the WMT Giga
LM for different values of the data-selection thresh-
old. Perplexities are computed on the newstest2009
set. As a good perplexity-size trade-off, the thresh-
old 250 was chosen to estimate an additional 5-gram
LM (WMT Giga 250) that was interpolated with
the original News LM. The resulting improvement
in perplexity is reported in table 1. For translation
model data, a perplexity threshold of 159 was ap-
plied.
373
LM Perplexity OOV rate
News 146.84 0.82
News + WMT Giga 250 130.23 0.71
Table 1: Perplexity reduction after interpolating the News
LM with data selected from the 109 corpus.
newstest
2009 2010 2011
Primary submission 0.246 0.286 0.284
w/o Anaphora handling 0.246 0.286 0.284
WMT Giga data
w/o LM 0.244 0.289 0.280
w/o TM 0.247 0.286 0.282
w/o LM and TM 0.247 0.289 0.278
Lexical reordering
phrase-based reo 0.239 0.281 0.275
no lexical reo 0.239 0.281 0.275
with LDC data 0.254 0.293 0.291
Table 2: Ablation test results (case-sensitive BLEU)
1.4 Results and Ablation tests
Owing to time constraints, we were not able to run
thorough tests on our system before submitting it to
the evaluation campaign. We therefore evaluated the
various components included in a post hoc fashion
by running ablation tests. In each test, we left out
one of the system components to identify its effect
on the overall performance. The results of these tests
are reported in table 2.
Performance-wise, the most important particular-
ity of our SMT system was the hierarchical lexical
reordering model, which led to a sizeable improve-
ment of 0.7, 0.5 and 0.9 BLEU points for the 2009,
2010 and 2011 test sets, respectively. We had previ-
ously seen negative results when trying to apply the
same model to English-German SMT, so its perfor-
mance seems to be strongly dependent on the lan-
guage pair it is used with.
Compared to the scores obtained using the full
system, the anaphora handling system did not have
any effect on the BLEU scores. This result is
similar to our result for English-German transla-
tion (Hardmeier and Federico, 2010). Unfortu-
nately, for English-French, the negative results ex-
tends to the pronoun translation scores (not reported
here), where slightly higher recall with the word-
dependency model was overcompensated by de-
graded precision, so the outcome of the experiments
clearly suggests that the anaphora handling proce-
dure is in need of improvement.
The effect of the WMT Giga language model dif-
fers among the test sets. For the 2009 and 2011
test sets, it results in an improvement of 0.2 and 0.4
BLEU points, respectively, while the 2010 test set
fares better without this additional language model.
However, it should be noted that there may be a
problem with the 2010 test set and the News lan-
guage model, which was used as a component in all
our systems. In particular, upgrading the News LM
data from last year?s to this year?s release led to an
improvement of 4 BLEU points on the 2010 test set
and an unrealistically low perplexity of 73 as com-
pared to 130 for the 2009 test set, which makes us
suspect that the latest News LM data may be tainted
with data from the 2010 test corpus. If this is the
case, the 2010 test set should be considered unreli-
able for LM evaluation. The benefit of adding WMT
Giga data to the translation model is less clear. For
the 2009 and 2010 test sets, this leads to a slight
degradation, but for the 2011 corpus, we obtained
a small improvement.
Our shared task submission did not use the French
Gigaword corpus from the Linguistic Data Consor-
tium (LDC2009T28), which is not freely available
to sites without LDC membership. After the sub-
mission, we ran a contrastive experiment including
a 5-gram model trained on this corpus, which led
to a sizeable improvement of 0.7?0.8 BLEU points
across all test sets.
2 Haitian Creole to English
Our experiments with the Haitian Creole-English
data are independent of the system presented for the
English to French task above. We experimented with
both phrase-based SMT and syntax-based SMT. The
main questions we investigated were i) whether we
can improve word alignment and phrase extraction
for phrase-based SMT and ii) whether we can in-
tegrate dependency parsing into a syntax-based ap-
proach. All our experiments were conducted on the
clean data set using Moses for training and decod-
ing. In the following we will first describe the exper-
iments with phrase-based models and linear trans-
374
duction grammars for word alignment and, there-
after, our findings from integrating English depen-
dency parses into a syntax-based approach.
2.1 Phrase-based SMT
The phrase-based system that we used in this series
of experiments uses a rather traditional setup. For
the translations into English we used the news data
provided for the other translations tasks in WMT
2011 to build a large scale-background language
model. The English data from the Haitian Creole
task were used as a separate domain-specific lan-
guage model. For the other translation direction we
only used the in-domain data provided. We used
standard 5-gram models with Witten-Bell discount-
ing and backoff interpolation for all language mod-
els. For the translation model we applied standard
techniques and settings for phrase extraction and
score estimations. However, we applied two differ-
ent systems for word alignment: One is the standard
GIZA++ toolbox implementing the IBM alignment
models (Och and Ney, 2003) and extensions and the
other is based on transduction grammars which will
briefly be introduced in the next section.
2.1.1 Alignment with PLITGs
By making the assumption that the parallel cor-
pus constitutes a linear transduction (Saers, 2011)2
we can induce a grammar that is the most likely to
have generated the observed corpus. The grammar
induced will generate a parse forest for each sen-
tence pair in the corpus, and each parse tree in that
forest will correspond to an alignment between the
two sentences. Following Saers et al (2010), the
alignment corresponding to the best parse can be ex-
tracted and used instead of other word alignment ap-
proaches such as GIZA++. There are several gram-
mar types that generate linear transductions, and in
this work, stochastic bracketing preterminalized lin-
ear inversion transduction grammars (PLITG) were
used (Saers and Wu, 2011). Since we were mainly
interested in the word alignments, we did not induce
phrasal grammars.
Although alignments from PLITGs may not reach
the same level of translation quality as GIZA++,
they make different mistakes, so both complement
2A transduction is a set of pairs of strings, and thus repre-
sents a relation between two languages.
each other. By duplicating the training corpus and
aligning each copy of the corpus with a different
alignment tool, the phrase extractor seems to be able
to pick the best of both worlds, producing a phrase
table that is superior to one produced with either of
the alignments tools used in isolation.
2.1.2 Results
In the following we present our results on the pro-
vided test set3 for translating into both languages
with phrase-based systems trained on different word
alignments. Table 3 summarises the BLEU scores
obtained.
English-Haitian BLEU phrase-table
GIZA++ 0.2567 3,060,486
PLITG 0.2407 5,007,254
GIZA++ & PLITG 0.2572 7,521,754
Haitian-English BLEU phrase-table
GIZA++ 0.3045 3,060,486
PLITG 0.2922 5,049,280
GIZA++ & PLITG 0.3105 7,561,043
Table 3: Phrase-based SMT (pbsmt) on the Haitian
Creole-English test set with different word alignments.
From the table we can see that phrase-based sys-
tems trained on PLITG alignments performs slightly
worse than the ones trained on GIZA++. However
combining both alignments with the simple data du-
plication technique mentioned earlier produces the
overall best scores in both translation directions.
The fact that both alignments lead to complemen-
tary information can be seen in the size of the phrase
tables extracted (see table 3).
2.2 Syntax-based SMT
We used Moses and its syntax-mode for our exper-
iments with hierarchical phrase-based and syntax-
augmented models. Our main interest was to in-
vestigate the influence of monolingual parsing on
the translation performance. In particular, we tried
to integrate English dependency parses created by
MaltParser (Nivre et al, 2007) trained on the Wall
Street Journal section of the Penn Treebank (Mar-
cus et al, 1993) extended with about 4000 questions
3We actually swapped the development set and the test set
by mistake. But, of course, we never mixed development and
test data in any result reported.
375
from the Question Bank (Judge et al, 2006). The
conversion to dependency trees was done using the
Stanford Parser (de Marneffe et al, 2006). Again,
we ran both translation directions to test our settings
in more than just one task. Interesting here is also
the question whether there are significant differences
when integrating monolingual parses on the source
or on the target side.
The motivation for applying dependency parsing
in our experiments is to use the specific information
carried by dependency relations. Dependency struc-
tures encode functional relations between words that
can be seen as an interface to the semantics of a
sentence. This information is usually not avail-
able in phrase-structure representations. We believe
that this type of information can be beneficial for
machine translation. For example, knowing that a
noun acts as the subject of a sentence is more in-
formative than just marking it as part of a noun
phrase. Whether or not this information can be ex-
plored by current syntax-based machine translation
approaches that are optimised for phrase-structure
representations is a question that we liked to inves-
tigate. For comparison we also trained hierarchical
phrase-based models without any additional annota-
tion.
2.2.1 Converting projective dependency trees
First we needed to convert dependency parses to
a tree representation in order to use our data in
the standard models of syntax-based models imple-
mented in Moses. In our experiments, we used
a parser model that creates projective dependency
graphs that can be converted into tree structures of
nested segments. We used the yield of each word
(referring to that word and its transitive dependents)
to define spans of phrases and their dependency rela-
tions are used as span labels. Furthermore, we also
defined pre-terminal nodes that encode the part-of-
speech information of each word. These tags were
obtained using the HunPos tagger (Hala?csy et al,
2007) trained on the Wall Street Journal section of
the Penn Treebank. Figure 2 illustrates the conver-
sion process. Tagging and parsing is done for all En-
glish data without any manual corrections or optimi-
sation of parameters. After the conversion, we were
able to use the standard training procedures imple-
mented in Moses.
-ROOT- andCC howWRB oldJJ isVBZ yourPRP$ nephewNN ?.
advmoddep possnsubjcc
punctnull
<tree label="null">
<tree label="cc">
<tree label="CC">and</tree>
</tree>
<tree label="dep">
<tree label="advmod">
<tree label="WRB">how</tree>
</tree>
<tree label="JJ">old</tree>
</tree>
<tree label="VBZ">is</tree>
<tree label="nsubj">
<tree label="poss">
<tree label="PRP$">your</tree>
</tree>
<tree label="NN">nephew</tree>
</tree>
<tree label="punct">
<tree label=".">?</tree>
</tree>
</tree>
Figure 2: A dependency graph from the training corpus
and its conversion to a nested tree structure. The yield of
each word in the sentence defines a span with the label
taken from the relation of that word to its head. Part-of-
speech tags are used as additional pre-terminal nodes.
2.2.2 Experimental Results
We ran several experiments with slightly differ-
ent settings. We used the same basic setup for
all of them including the same language models
and GIZA++ word alignments that we have used
for the phrase-based models already. Further, we
used Moses for extracting rules of the syntax-based
translation model. We use standard settings for
the baseline system (=hiero) that does not employ
any linguistic markup. For the models that include
dependency-based trees we changed the maximum
span threshold to a high value of 999 (default: 15)
in order to extract as many rules as possible. This
large degree of freedom is possible due to the oth-
erwise strong constraints on rule flexibility imposed
by the monolingual syntactic markup. Rule tables
are dramatically smaller than for the unrestricted hi-
erarchical models (see table 4).
However, rule restriction by linguistic constraints
usually hurts performance due to the decreased cov-
erage of the rule set. One common way of improving
376
reference Are you going to let us die on Ile a` Vaches which is located close the city of Les Cayes. I am ...
pbsmt Do you are letting us die in Ilavach island?s on in Les Cayes. I am ...
hiero do you will let us die in the island Ilavach on the in Les Cayes . I am ...
samt2 Are you going to let us die in the island Ilavach the which is on the Les. My name is ...
reference I?m begging you please help me my situation is very critical.
pbsmt Please help me please. Because my critical situation very much.
hiero please , please help me because my critical situation very much .
samt2 Please help me because my situation very critical.
reference I don?t have money to go and give blood in Port au Prince from La Gonave.
pbsmt I don?t have money, so that I go to give blood Port-au-Prince since lagonave.
hiero I don ?t have any money , for me to go to give blood Port-au-Prince since lagonave .
samt2 I don?t have any money, to be able to go to give blood Port-au-Prince since Gona?ve Island.
Figure 3: Example translations for various models.
English-Haitian BLEU number of rules
hiero 0.2549 34,118,622
malt (source) 0.2180 1,628,496
- binarised 0.2327 9,063,933
- samt1 0.2311 11,691,279
- samt2 0.2366 29,783,694
Haitian-English BLEU number of rules
hiero 0.3034 33,231,535
malt (target) 0.2739 1,922,688
- binarised 0.2857 8,922,343
- samt1 0.2952 11,073,764
- samt2 0.2954 24,554,317
Table 4: Syntax-based SMT on the Haitian Creole-
English test set with (=malt) or without (=hiero) English
parse trees and various parse relaxation strategies. The
final system submitted to WMT11 is malt(target)-samt2.
rule extraction is based on tree manipulation and re-
laxed extraction algorithms. Moses implements sev-
eral algorithms that have been proposed in the lit-
erature. Tree binarisation is one of them. This can
be done in a left-branching and in a right-branching
mode. We used a combination of both in the set-
tings denoted as binarised. The other relaxation al-
gorithms are based on methods proposed for syntax-
augmented machine translation (Zollmann et al,
2008). We used two of them: samt1 combines pairs
of neighbouring children nodes into combined com-
plex nodes and creates additional complex nodes of
all children nodes except the first child and similar
complex nodes for all but the last child. samt2 com-
bines any pair of neighbouring nodes even if they are
not children of the same parent. All of these relax-
ation algorithms lead to increased rule sets (table 4).
In terms of translation performance there seems to
be a strong correlation between rule table size and
translation quality as measured by BLEU. None of
the dependency-based models beats the unrestricted
hierarchical model. Both translation directions be-
have similar with slightly worse performances of
the dependency-based models (relative to the base-
line) when syntax is used on the source language
side. Note also that all syntax-based models (includ-
ing hiero) are below the corresponding phrase-based
SMT systems. Of course, automatic evaluation has
its limits and interesting qualitative differences may
be more visible in manual assessments. The use of
linguistic information certainly has an impact on the
translation hypotheses produced as we can see in the
examples in figure 3. In the future, we plan to inves-
tigate the effect of dependency information on gram-
maticality of translated sentences in more detail.
3 Conclusions
In our English-French and Haitian Creole-English
shared task submissions, we investigated the use
of anaphora resolution, hierarchical lexical reorder-
ing and data selection for language modelling
(English-French) as well as LTG word alignment
and syntax-based decoding with dependency infor-
mation (Haitian Creole-English). While the re-
sults for the systems with anaphora handling were
somewhat disappointing and the effect of data fil-
tering was inconsistent, hierarchical lexical reorder-
ing brought substantial improvements. We also ob-
tained consistent gains by combining information
from different word aligners, and we presented a
simple way of including dependency parses in stan-
dard tree-based decoding.
377
Acknowledgements
Most of the features used in our English-French sys-
tem were originally developed while Christian Hard-
meier was at FBK. Activities at FBK were supported
by the EuroMatrixPlus project (IST-231720) and the
T4ME network of excellence (IST-249119), both
funded by the DG INFSO of the European Commis-
sion through the Seventh Framework Programme.
References
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodriguez, Lorenza Romano,
Olga Uryupina, Yannick Versley, and Roberto Zanoli.
2010. BART: A multilingual anaphora resolution sys-
tem. In Proceedings of the 5th International Workshop
on Semantic Evaluations (SemEval-2010), Uppsala.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an open source toolkit for handling
large scale language models. In Interspeech 2008,
pages 1618?1621. ISCA.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing,
pages 847?855, Honolulu, Hawaii, October. Associa-
tion for Computational Linguistics.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz. 2007.
Hunpos: an open source trigram tagger. In Proceed-
ings of the 45th Annual Meeting of the ACL on Inter-
active Poster and Demonstration Sessions, pages 209?
212.
Christian Hardmeier and Marcello Federico. 2010. Mod-
elling Pronominal Anaphora in Statistical Machine
Translation. In Marcello Federico, Ian Lane, Michael
Paul, and Franc?ois Yvon, editors, Proceedings of the
seventh International Workshop on Spoken Language
Translation (IWSLT), pages 283?289.
John Judge, Aoife Cahill, and Josef van Genabith. 2006.
Questionbank: creating a corpus of parse-annotated
questions. In Proceedings of the 21st International
Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 497?504.
Philipp Koehn, Amittai Axelrod, Alexandra
Birch Mayne, et al 2005. Edinburgh system
description for the 2005 iwslt speech translation
evaluation. In International workshop on spoken
language translation, Pittsburgh.
Philipp Koehn, Hieu Hoang, Alexandra Birch, et al
2007. Moses: open source toolkit for Statistical Ma-
chine Translation. In Annual meeting of the Associ-
ation for Computational Linguistics: Demonstration
session, pages 177?180, Prague.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: The Penn Treebank. Computational
Linguistics, 19:313?330, June.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational linguistics, 29:19?51.
Markus Saers and Dekai Wu. 2011. Principled induction
of phrasal bilexica. In Proceedings of the 15th Annual
Conference of the European Association for Machine
Translation, Leuven, Belgium, May.
Markus Saers, Joakim Nivre, and Dekai Wu. 2010. Word
alignment with stochastic bracketing linear inversion
transduction grammar. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 341?344, Los Angeles, California,
June.
Markus Saers. 2011. Translation as Linear Transduc-
tion: Models and Algorithms for Efficient Learning in
Statistical Machine Translation. Ph.D. thesis, Uppsala
University, Department of Linguistics and Philology.
M. Strube. 2006. Anaphora and coreference resolution,
Statistical. In Encyclopedia of language and linguis-
tics, pages 216?222. Elsevier.
Andreas Zollmann, Ashish Venugopal, Franz Och, and
Jay Ponte. 2008. A systematic comparison of phrase-
based, hierarchical and syntax-augmented statistical
mt. In Proceedings of the 22nd International Confer-
ence on Computational Linguistics - Volume 1, pages
1145?1152.
378
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 109?113,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Tree Kernels for Machine Translation Quality Estimation
Christian Hardmeier and Joakim Nivre and Jo?rg Tiedemann
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
This paper describes Uppsala University?s
submissions to the Quality Estimation (QE)
shared task at WMT 2012. We present a QE
system based on Support Vector Machine re-
gression, using a number of explicitly defined
features extracted from the Machine Transla-
tion input, output and models in combination
with tree kernels over constituency and de-
pendency parse trees for the input and output
sentences. We confirm earlier results suggest-
ing that tree kernels can be a useful tool for
QE system construction especially in the early
stages of system design.
1 Introduction
The goal of the WMT 2012 Quality Estimation
(QE) shared task (Callison-Burch et al, 2012) was
to create automatic systems to judge the quality
of the translations produced by a Statistical Ma-
chine Translation (SMT) system given the input
text, the proposed translations and information about
the models used by the SMT system. The shared
task organisers provided a training set of 1832 sen-
tences drawn from earlier WMT Machine Transla-
tion test sets, translated from English to Spanish
with a phrase-based SMT system, along with the
models used and diagnostic output produced by the
SMT system as well as manual translation quality
annotations on a 1?5 scale for each sentence. Ad-
ditionally, a set of 17 baseline features was made
available to the participants. Systems were evalu-
ated on a test set of 422 sentences annotated in the
same way.
Uppsala University submitted two systems to this
shared task. Our systems were fairly successful and
achieved results that were outperformed by only one
competing group. They improve over the baseline
performance in two ways, building on and extend-
ing earlier work by Hardmeier (2011), on which
the system description in the following sections is
partly based: On the one hand, we enhance the set
of 17 baseline features provided by the organisers
with another 82 explicitly defined features. On the
other hand, we use syntactic tree kernels to extract
implicit features from constituency and dependency
parse trees over the input sentences and the Machine
Translation (MT) output. The experimental results
confirm the findings of our earlier work, showing
tree kernels to be a valuable tool for rapid prototyp-
ing of QE systems.
2 Features
Our QE systems used two types of features: On
the one hand, we used a set of explicit features that
were extracted from the data before running the Ma-
chine Learning (ML) component. On the other hand,
syntactic parse trees of the MT input and output
sentences provided implicit features that were com-
puted directly by the ML component using tree ker-
nels.
2.1 Explicit features
Both of the QE systems we submitted to the shared
task used the complete set of 17 baseline features
provided by the workshop organisers. Additionally,
the UU best system also contained all the features
presented by Hardmeier (2011) with the exception
109
of a few features specific to the film subtitle genre
and inapplicable to the text type of the shared task,
as well as a small number of features not included
in that work. Many of these features were modelled
on QE features described by Specia et al (2009). In
particular, the following features were included in
addition to the baseline feature set:
? number of words, length ratio (4 features)
? source and target type-token ratios (2 features)
? number of tokens matching particular patterns
(3 features each):
? numbers
? opening and closing parentheses
? strong punctuation signs
? weak punctuation signs
? ellipsis signs
? hyphens
? single and double quotes
? apostrophe-s tokens
? short alphabetic tokens (? 3 letters)
? long alphabetic tokens (? 4 letters)
? source and target language model (LM) and
log-LM scores (4 features)
? LM and log-LM scores normalised by sentence
length (4 features)
? number and percentage of out-of-vocabulary
words (2 features)
? percentage of source 1-, 2-, 3- and 4-grams oc-
curring in the source part of the training corpus
(4 features)
? percentage of source 1-, 2-, 3- and 4-grams in
each frequency quartile of the training corpus
(16 features)
? a binary feature indicating that the output con-
tains more than three times as many alphabetic
tokens as the input (1 feature)
? percentage of unaligned words and words with
1 : 1, 1 : n, n : 1 and m : n alignments (10 fea-
tures)
? average number of translations per word, un-
weighted and weighted by word frequency and
reciprocal word frequency (3 features)
? translation model entropy for the input words,
cumulatively per sentence and averaged per
word, computed based on the SMT lexical
weight model (2 features).
Whenever applicable, features were computed for
both the source and the target language, and addi-
tional features were added to represent the squared
difference of the source and target language feature
values. All feature values were scaled so that their
values ranged between 0 and 1 over the training set.
The total number of features of the UU best sys-
tem amounted to 99. It should be noted, however,
that there is considerable redundancy in the feature
set and that the 82 features of Hardmeier (2011)
overlap with the 17 baseline features to some extent.
We did not make any attempt to reduce feature over-
lap and relied on the learning algorithm for feature
selection.
2.2 Parse trees
Both the English input text and the Spanish Machine
Translations were annotated with syntactic parse
trees from which to derive implicit features. In En-
glish, we were able to produce both constituency and
dependency parses. In Spanish, we were limited to
dependency parses because of the better availability
of parsing models. English constituency parses were
produced with the Stanford parser (Klein and Man-
ning, 2003) using the model bundled with the parser.
For dependency parsing, we used MaltParser (Nivre
et al, 2006). POS tagging was done with HunPOS
(Hala?csy et al, 2007) for English and SVMTool
(Gime?nez and Ma?rquez, 2004) for Spanish, with the
models provided by the OPUS project (Tiedemann,
2009). As in previous work (Hardmeier, 2011), we
treated the parser as a black box and made no at-
tempt to handle the fact that parsing accuracy may
be decreased over malformed SMT output.
To be used with tree kernels, the output of the de-
pendency parser had to be transformed into a sin-
gle tree structure with a unique label per node and
unlabelled edges, similar to a constituency parse
tree. We followed Johansson and Moschitti (2010)
in using a tree representation which encodes part-
of-speech tags, dependency relations and words as
sequences of child nodes (see fig. 1).
110
Figure 1: Representation of the dependency tree fragment
for the words Nicole ?s dad
A tree and some of its Subset Tree Fragments
S 
N 
NP 
D N 
VP 
V Mary 
brought 
a    cat 
NP 
D N 
a    cat 
N 
   cat 
D 
a 
V 
brought 
N 
Mary 
NP 
D N 
VP 
V 
brought 
a    cat 
Fig. 1. A syntactic parse tree with its sub-
trees (STs).
NP 
D N 
a   cat 
NP 
D N 
NP 
D N 
a 
NP 
D N 
NP 
D N 
VP 
V 
brought 
a    cat 
  cat 
NP 
D N 
VP 
V 
a    cat 
NP 
D N 
VP 
V 
N 
   cat 
D 
a 
V 
brought 
N 
Mary 
? 
Fig. 2. A tree with some of its subset trees
(SSTs).
NP 
D N 
VP 
V 
brought 
a    cat 
NP 
D N 
VP 
V 
a    cat 
NP 
D N 
VP 
a    cat 
NP 
D N 
VP 
a 
NP 
D 
VP 
a 
NP 
D 
VP 
NP 
N 
VP 
NP 
N 
NP NP 
D N D 
NP 
? 
VP 
Fig. 3. A tree with some of its partial trees
(PTs).
 
 
 
 
 
is 
What offer 
an plan 
direct      stock   purchase 
Fig. 4. A dependency tree of a question.
constraint over the SSTs, we obtain a more general form of substructures that we
call partial trees (PTs). These can be generated by the application of partial
production rules of the grammar, consequently [VP [V]] and [VP [NP]] are
valid PTs. Figure 3 shows that the number of PTs derived from the same tree as
before is still higher (i.e. 30 PTs). These different substructure numbers provide
an intuitive quantification of the different information levels among the tree-
based representations.
3 Fast Tree Kernel Functions
The main idea of tree kernels is to compute the number of common substructures
between two trees T1 and T2 without explicitly considering the whole fragment
space. We have designed a general function to compute the ST, SST and PT
kernels. Our fast evaluation of the PT kernel is inspired by the efficient evaluation
of non-continuous subsequences (described in [13]). To increase the computation
speed of the above tree kernels, we also apply the pre-selection of node pairs
which have non-null kernel.
3.1 The Partial Tree Kernel
The evaluation of the common PTs rooted in nodes n1 and n2 requires the
selection of the shared child subsets of the two nodes, e.g. [S [DT JJ N]] and
[S [DT N N]] have [S [N]] (2 times) and [S [DT N]] in common. As the order
of the children is important, we can use subsequence kernels for their generation.
More in detail, let F = {f1, f2, .., f|F|} be a tree fragment space of type PTs and
let the indicator function Ii(n) be equal to 1 if the target fi is rooted at node n
and 0 otherwise, we define the PT kernel as:
A tree and some of its Partial Tree Fragments
Figure 2: Tree fragments extracted by the Subset Tree
Kernel and by the Partial Tree Kernel. Illustrations by
Moschitti (2006a).
3 M chine Learning compon nt
3.1 Overview
The QE shared task asked both for an estimate of
a 1?5 quality score for each segment in the test set
and for a ranking of t e sentences according to qual-
ity. We decided to treat score estimation as primary
and address the task as a regression problem. For
the ranking task, we simply submitted the ranking
induced by the regression output, breaking ties ran-
domly.
Our system was based on SVM regression as
implemented by the SVMlight software (Joachims,
1999) with tree kernel extensions (Moschitti,
2006b). Predicted scores less than 1 were set to 1
and predicted scores greater than 5 were set to 5
as this was known to be the range of valid scores.
Our learning algorithm had some free hyperparam-
eters. Three of them were optimised by joint grid
search with 5-fold cross-validation over the training
set: the SVM training error/margin trade-off (C pa-
rameter), one free parameter of the explicit feature
kernel and the ratio between explicit feature and tree
kernels (see below). All other parameters were left
at their default values. Before running it over the
test set, the system was retrained on the complete
training set using the parameters found with cross-
validation.
3.2 Kernels for explicit features
To select a good kernel for our explicit features,
we initially followed the advice given by Hsu et al
(2010), using a Gaussian RBF kernel and optimis-
ing the SVM C parameter and the ? parameter of the
RBF with grid search. While this gave reasonable
results, it turned out that slightly better prediction
could be achieved by using a polynomial kernel, so
we chose to use this kernel for our final submission
and used grid search to tune th degree of the poly-
nomial instead. The improvement over the Gaussian
kernel was, however, marginal.
3.3 Tree kernels
To exploit parse tree information in our Machine
Learning (ML) component, we used tree kernel
functions. Tree kernels (Collins and Duffy, 2001)
are kernel functions defined over pairs of tree struc-
tures. They measure the similarity between two trees
by counting the number of common substructures.
Implicitly, they define an infinite-dimensional fea-
ture space whose dimensions correspond to all pos-
sible tree fragments. Features are thus available to
cover different kinds of abstract node configurations
that can occ r in a tree. The important feature i-
mensions are effectively selected by the SVM train-
ing algorithm through the selection and weighting
of the support vectors. The intuition behind our
use of tree kernels is that they may help us iden-
tify constructions that are difficult to translate in the
source language, and doubtful syntactic structures in
the output language. Note that we do not currently
compare parse trees across languages; tree kernels
111
Cross-validation Test set
Features T C d ? ? MAE RMS ? ? MAE RMS
UU best 99 explicit + TK 0.05 4 2 0.506 0.566 0.550 0.692 0.56 0.62 0.64 0.79
(a) 99 explicit + TK 0.03 8 3 0.502 0.564 0.552 0.700 0.56 0.61 0.63 0.78
(b) 17 explicit + TK 0.05 4 2 0.462 0.530 0.568 0.714 0.57 0.61 0.65 0.79
UU bltk 17 explicit + TK 0.03 8 3 0.466 0.534 0.566 0.712 0.58 0.61 0.64 0.79
(c) 99 explicit 0 8 2 0.492 0.560 0.554 0.700 0.56 0.59 0.65 0.80
(d) 17 explicit 0 8 2 0.422 0.466 0.598 0.748 0.52 0.55 0.70 0.83
(e) TK only ? 4 ? 0.364 0.392 0.632 0.782 0.51 0.51 0.70 0.85
T : Tree kernel weight C: Training error/margin trade-off d: Degree of polynomial kernel
?: DeltaAvg score ?: Spearman rank correlation MAE: Mean Average Error
RMS: Root Mean Square Error TK: Tree kernels
Table 1: Experimental results
are applied to trees of the same type in the same lan-
guage only.
We used two different types of tree kernels for the
different types of parse trees (see fig. 2). The Sub-
set Tree Kernel (Collins and Duffy, 2001) consid-
ers tree fragments consisting of more than one node
with the restriction that if one child of a node is in-
cluded, then all its siblings must be included as well
so that the underlying production rule is completely
represented. This kind of kernel is well suited for
constituency parse trees and was used for the source
language constituency parses. For the dependency
trees, we used the Partial Tree Kernel (Moschitti,
2006a) instead. It extends the Subset Tree Kernel by
permitting also the extraction of tree fragments com-
prising only part of the children of any given node.
Lifting this restriction makes sense for dependency
trees since a node and its children do not correspond
to a grammatical production in a dependency tree in
the same way as they do in a constituency tree (Mos-
chitti, 2006a). It was used for the dependency trees
in the source and in the target language.
The explicit feature kernel and the three tree ker-
nels were combined additively, with a single weight
parameter to balance the sum of the tree kernels
against the explicit feature kernel. This coefficient
was optimised together with the other two hyperpa-
rameters mentioned above. It turned out that best re-
sults could be obtained with a fairly low weight for
the tree kernels, but in the cross-validation experi-
ments adding tree kernels did give an improvement
over not having them at all.
4 Experimental Results
Results for some of our experiments are shown in
table 1. The two systems we submitted to the shared
task are marked with their system identifiers. A few
other systems are included for comparison and are
numbered (a) to (e) for easier reference.
Our system using only the baseline features (d)
performs a bit worse than the reference system of
the shared task organisers. We use the same learn-
ing algorithm, so this seems to indicate that the ker-
nel and the hyperparameters they selected worked
slightly better than our choices. Using only tree
kernels with no explicit features at all (e) creates a
system that works considerably worse under cross-
validation, however we note that its performance on
the test set is very close to that of system (d).
Adding the 82 additional features of Hardmeier
(2011) to the system without tree kernels slightly im-
proves the performance both under cross-validation
and on the test set (c). Adding tree kernels has a
similar effect, which is a bit less pronounced for
the cross-validation setting, but quite comparable on
the test set (UU bltk, b). Finally, combining the
full feature set with tree kernels results in an addi-
tional gain under cross-validation, but unfortunately
the improvement does not carry over to the test set
(UU best, a).
5 Conclusions
In sum, the results confirm the findings made in our
earlier work (Hardmeier, 2011). They show that tree
kernels can be a valuable tool to boost the initial
112
performance of a Quality Estimation system without
spending much effort on feature engineering. Unfor-
tunately, it seems that the gains achieved by tree ker-
nels over simple parse trees and by the additional ex-
plicit features used in our systems do not necessarily
add up. Nevertheless, comparison with other partici-
pating systems shows that either of them is sufficient
for state-of-the-art performance.
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Machine
Translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of NIPS
2001, pages 625?632.
Jesu?s Gime?nez and Llu??s Ma?rquez. 2004. SVMTool: A
general POS tagger generator based on Support Vec-
tor Machines. In Proceedings of the 4th Conference
on International Language Resources and Evaluation
(LREC-2004), Lisbon.
Pe?ter Hala?csy, Andra?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics. Companion Volume: Pro-
ceedings of the Demo and Poster Sessions, pages 209?
212, Prague, Czech Republic, June. Association for
Computational Linguistics.
Christian Hardmeier. 2011. Improving machine transla-
tion quality prediction with syntactic tree kernels. In
Mikel L. Forcada, Heidi Depraetere, and Vincent Van-
deghinste, editors, Proceedings of the 15th conference
of the European Association for Machine Translation
(EAMT 2011), pages 233?240, Leuven, Belgium.
Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.
2010. A practical guide to support vector classifica-
tion. Technical report, Department of Computer Sci-
ence, National Taiwan University.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods ? Sup-
port Vector Learning. MIT Press.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67?76, Uppsala, Sweden, July. Association
for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, July. As-
sociation for Computational Linguistics.
Alessandro Moschitti. 2006a. Efficient convolution ker-
nels for dependency and constituent syntactic trees. In
Proceedings of the 17th European Conference on Ma-
chine Learning, Berlin.
Alessandro Moschitti. 2006b. Making tree kernels prac-
tical for natural language learning. In Proceedings of
the Eleventh International Conference of the European
Association for Computational Linguistics, Trento.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
MaltParser: A language-independent system for data-
driven dependency parsing. In Proceedings of the 5th
Conference on International Language Resources and
Evaluation (LREC-2006), pages 2216?2219, Genoa.
Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran
Wang, and John Shawe-Taylor. 2009. Improving the
confidence of Machine Translation quality estimates.
In Proceedings of MT Summit XII, Ottawa.
Jo?rg Tiedemann. 2009. News from OPUS ? a collection
of multilingual parallel corpora with tools and inter-
face. In N. Nicolov, K. Bontcheva, G. Angelova, and
R. Mitkov, editors, Recent Advances in Natural Lan-
guage Processing, pages 237?248. John Benjamins,
Amsterdam.
113
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 225?231,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Tunable Distortion Limits and Corpus Cleaning for SMT
Sara Stymne Christian Hardmeier Jo?rg Tiedemann Joakim Nivre
Uppsala University
Department of Linguistics and Philology
firstname.lastname@lingfil.uu.se
Abstract
We describe the Uppsala University sys-
tem for WMT13, for English-to-German
translation. We use the Docent decoder,
a local search decoder that translates at
the document level. We add tunable dis-
tortion limits, that is, soft constraints on
the maximum distortion allowed, to Do-
cent. We also investigate cleaning of the
noisy Common Crawl corpus. We show
that we can use alignment-based filtering
for cleaning with good results. Finally we
investigate effects of corpus selection for
recasing.
1 Introduction
In this paper we present the Uppsala University
submission to WMT 2013. We have submitted one
system, for translation from English to German.
In our submission we use the document-level de-
coder Docent (Hardmeier et al, 2012; Hardmeier
et al, 2013). In the current setup, we take advan-
tage of Docent in that we introduce tunable dis-
tortion limits, that is, modeling distortion limits as
soft constraints instead of as hard constraints. In
addition we perform experiments on corpus clean-
ing. We investigate how the noisy Common Crawl
corpus can be cleaned, and suggest an alignment-
based cleaning method, which works well. We
also investigate corpus selection for recasing.
In Section 2 we introduce our decoder, Docent,
followed by a general system description in Sec-
tion 3. In Section 4 we describe our experiments
with corpus cleaning, and in Section 5 we describe
experiments with tunable distortion limits. In Sec-
tion 6 we investigate corpus selection for recasing.
In Section 7 we compare our results with Docent
to results using Moses (Koehn et al, 2007). We
conclude in Section 8.
2 The Docent Decoder
Docent (Hardmeier et al, 2013) is a decoder for
phrase-based SMT (Koehn et al, 2003). It differs
from other publicly available decoders by its use
of a different search algorithm that imposes fewer
restrictions on the feature models that can be im-
plemented.
The most popular decoding algorithm for
phrase-based SMT is the one described by Koehn
et al (2003), which has become known as stack
decoding. It constructs output sentences bit by
bit by appending phrase translations to an initially
empty hypothesis. Complexity is kept in check,
on the one hand, by a beam search approach that
only expands the most promising hypotheses. On
the other hand, a dynamic programming technique
called hypothesis recombination exploits the lo-
cality of the standard feature models, in particu-
lar the n-gram language model, to achieve a loss-
free reduction of the search space. While this de-
coding approach delivers excellent search perfor-
mance at a very reasonable speed, it limits the
information available to the feature models to an
n-gram window similar to a language model his-
tory. In stack decoding, it is difficult to implement
models with sentence-internal long-range depen-
dencies and cross-sentence dependencies, where
the model score of a given sentence depends on
the translations generated for another sentence.
In contrast to this very popular stack decod-
ing approach, our decoder Docent implements a
search procedure based on local search (Hard-
meier et al, 2012). At any stage of the search pro-
cess, its search state consists of a complete docu-
ment translation, making it easy for feature mod-
els to access the complete document with its cur-
rent translation at any point in time. The search
algorithm is a stochastic variant of standard hill
climbing. At each step, it generates a successor
of the current search state by randomly applying
225
one of a set of state changing operations to a ran-
dom location in the document. If the new state
has a better score than the previous one, it is ac-
cepted, else search continues from the previous
state. The operations are designed in such a way
that every state in the search space can be reached
from every other state through a sequence of state
operations. In the standard setup we use three op-
erations: change-phrase-translation replaces the
translation of a single phrase with another option
from the phrase table, resegment alters the phrase
segmentation of a sequence of phrases, and swap-
phrases alters the output word order by exchang-
ing two phrases.
In contrast to stack decoding, the search algo-
rithm in Docent leaves model developers much
greater freedom in the design of their feature func-
tions because it gives them access to the transla-
tion of the complete document. On the downside,
there is an increased risk of search errors because
the document-level hill-climbing decoder cannot
make as strong assumptions about the problem
structure as the stack decoder does. In prac-
tice, this drawback can be mitigated by initializing
the hill-climber with the output of a stack decod-
ing pass using the baseline set of models without
document-level features (Hardmeier et al, 2012).
Since its inception, Docent has been used to ex-
periment with document-level semantic language
models (Hardmeier et al, 2012) and models to
enhance text readability (Stymne et al, 2013b).
Work on other discourse phenomena is ongoing.
In the present paper, we focus on sentence-internal
reordering by exploiting the fact that Docent im-
plements distortion limits as soft constraints rather
than strictly enforced limitations. We do not in-
clude any of our document-level feature functions.
3 System Setup
In this section we will describe our basic system
setup. We used all corpora made available for
English?German by the WMT13 workshop. We
always concatenated the two bilingual corpora Eu-
roparl and News Commentary, which we will call
EP-NC. We pre-processed all corpora by using
the tools provided for tokenization and we also
lower-cased all corpora. For the bilingual corpora
we also filtered sentence pairs with a length ra-
tio larger than three, or where either sentence was
longer than 60 tokens. Recasing was performed as
a post-processing step, trained using the resources
in the Moses toolkit (Koehn et al, 2007).
For the language model we trained two sepa-
rate models, one on the German side of EP-NC,
and one on the monolingual News corpus. In
both cases we trained 5-gram models. For the
large News corpus we used entropy-based prun-
ing, with 10?8 as a threshold (Stolcke, 1998). The
language models were trained using the SRILM
toolkit (Stolcke, 2002) and during decoding we
used the KenLM toolkit (Heafield, 2011).
For the translation model we also trained two
models, one with EP-NC, and one with Common
Crawl. These two models were interpolated and
used as a single model at decoding time, based on
perplexity minimization interpolation (Sennrich,
2012), see details in Section 4. The transla-
tion models were trained using the Moses toolkit
(Koehn et al, 2007), with standard settings with
5 features, phrase probabilities and lexical weight-
ing in both directions and a phrase penalty. We ap-
plied significance-based filtering (Johnson et al,
2007) to the resulting phrase tables. For decod-
ing we used the Docent decoder with random ini-
tialization and standard parameter settings (Hard-
meier et al, 2012; Hardmeier et al, 2013), which
beside translation and language model features in-
clude a word penalty and a distortion penalty.
Parameter optimization was performed using
MERT (Och, 2003) at the document-level (Stymne
et al, 2013a). In this setup we calculate both
model and metric scores on the document-level
instead of on the sentence-level. We produce k-
best lists by sampling from the decoder. In each
optimization run we run 40,000 hill-climbing it-
erations of the decoder, and sample translations
with interval 100, from iteration 10,000. This
procedure has been shown to give competitive re-
sults to standard tuning with Moses (Koehn et
al., 2007) with relatively stable results (Stymne
et al, 2013a). For tuning data we concate-
nated the tuning sets news-test 2008?2010 and
newssyscomb2009, to get a higher number of doc-
uments. In this set there are 319 documents and
7434 sentences.
To evaluate our system we use newstest2012,
which has 99 documents and 3003 sentences. In
this article we give lower-case Bleu scores (Pap-
ineni et al, 2002), except in Section 6 where we
investigate the effect of different recasing models.
226
Cleaning Sentences Reduction
None 2,399,123
Basic 2,271,912 5.3%
Langid 2,072,294 8.8%
Alignment-based 1,512,401 27.0%
Table 1: Size of Common Crawl after the different
cleaning steps and reduction in size compared to
the previous step
4 Cleaning of Common Crawl
The Common Crawl (CC) corpus was collected
from web sources, and was made available for the
WMT13 workshop. It is noisy, with many sen-
tences with the wrong language and also many
non-corresponding sentence pairs. To make better
use of this resource we investigated two methods
for cleaning it, by making use of language identi-
fication and alignment-based filtering. Before any
other cleaning we performed basic filtering where
we only kept pairs where both sentences had at
most 60 words, and with a length ratio of maxi-
mum 3. This led to a 5.3% reduction of sentences,
as shown in Table 1.
Language Identification For language identifi-
cation we used the off-the-shelf tool langid.py (Lui
and Baldwin, 2012). It is a python library, cover-
ing 97 languages, including English and German,
trained on data drawn from five different domains.
It uses a naive Bayes classifier with a multino-
mial event model, over a mixture of byte n-grams.
As for many language identification packages it
works best for longer texts, but Lui and Bald-
win (2012) also showed that it has good perfor-
mance for short microblog texts, with an accuracy
of 0.89?0.94.
We applied langid.py for each sentence in the
CC corpus, and kept only those sentence pairs
where the correct language was identified for both
sentences with a confidence of at least 0.999. The
total number of sentences was reduced by a further
8.8% based on the langid filtering.
We performed an analysis on a set of 1000 sen-
tence pairs. Among the 907 sentences that were
kept in this set we did not find any cases with
the wrong language. Table 2 shows an analysis
of the 93 sentences that were removed from this
test set. The overall accuracy of langid.py is much
higher than indicated in the table, however, since
it does not include the correctly identified English
and German sentences. We grouped the removed
sentences into four categories, cases where both
languages were correctly identified, but under the
confidence threshold of 0.999, cases where both
languages were incorrectly identified, and cases
where one language was incorrectly identified.
Overall the language identification was accurate
on 54 of the 93 removed sentences. In 18 of the
cases where it was wrong, the sentences were not
translation correspondents, which means that we
only wrongly removed 21 out of 1000 sentences.
It was also often the case when the language was
wrongly identified, that large parts of the sentence
consisted of place names, such as ?Forums about
Conil de la Frontera - Ca?diz.? ? ?Foren u?ber Conil
de la Frontera - Ca?diz.?, which were identified as
es/ht instead of en/de. Even though such sentence
pairs do correspond, they do not contain much use-
ful translation material.
Alignment-Based Cleaning For the alignment-
based cleaning, we aligned the data from the pre-
vious step using GIZA++ (Och and Ney, 2003)
in both directions, and used the intersection of
the alignments. The intersection of alignments is
more sparse than the standard SMT symmetriza-
tion heuristics, like grow-diag-final-and (Koehn et
al., 2005). Our hypothesis was that sentence pairs
with very few alignment points in the intersection
would likely not be corresponding sentences.
We used two types of filtering thresholds based
on alignment points. The first threshold is for the
ratio of the number of alignment points and the
maximum sentence length. The second threshold
is the absolute number of alignment points in a
sentence pair. In addition we used a third thresh-
old based on the length ratio of the sentences.
To find good values for the filtering thresholds,
we created a small gold standard where we man-
ually annotated 100 sentence pairs as being cor-
responding or not. In this set the sentence pairs
did not match in 33 cases. Table 3 show results for
some different values for the threshold parameters.
Overall we are able to get a very high precision
on the task of removing non-corresponding sen-
tences, which means that most sentences that are
removed based on this cleaning are actually non-
corresponding sentences. The recall is a bit lower,
indicating that there are still non-corresponding
sentences left in our data. In our translation sys-
tem we used the bold values in Table 3, since it
gave high precision with reasonable recall for the
removal of non-corresponding sentences, meaning
227
Identification Total Wrong lang. Non-corr Corr Languages identified
English and German < 0.999 15 0 7 8
Both English and German wrong 6 2 2 2 2:na/es, 2:et/et, 1: es/an, 1:es/ht
English wrong 13 1 6 6 5: es 4: fr 1: br, it, de, eo
German wrong 59 51 3 5 51: en 3: es 2:nl 1: af, la, lb
Total 93 54 18 21
Table 2: Reasons and correctness for removing sentences based on language ID for 93 sentences out of
a 1000 sentence subset, divided into wrong lang(uage), non-corr(esponding) pairs, and corr(esponding)
pairs.
Ratio align Min align Ratio length Prec. Recall F Kept
0.1 4 2 0.70 0.77 0.73 70%
0.28 4 2 0.94 0.72 0.82 57%
0.42 4 2 1.00 0.56 0.72 41%
0.28 2 2 0.91 0.73 0.81 59%
0.28 6 2 0.94 0.63 0.76 51%
0.28 4 1.5 0.94 0.65 0.77 52%
0.28 4 3 0.91 0.75 0.82 60%
Table 3: Results of alignment-based cleaning for different values of the filtering parameters, with pre-
cision, recall and F-score for the identification of erroneous sentence pairs and the percentage of kept
sentence pairs
that we kept most correctly aligned sentence pairs.
This cleaning method is more aggressive than
the other cleaning methods we described. For the
gold standard only 57% of sentences were kept,
but in the full training set it was a bit higher, 73%,
as shown in Table 1.
Phrase Table Interpolation To use the CC cor-
pus in our system we first trained a separate phrase
table which we then interpolated with the phrase
table trained on EP-NC. In this way we could al-
ways run the system with a single phrase table. For
interpolation, we used the perplexity minimization
for weighted counts method by Sennrich (2012).
Each of the four weights in the phrase table, back-
ward and forward phrase translation probabilities
and lexical weights, are optimized separately. This
method minimizes the cross-entropy based on a
held-out corpus, for which we used the concate-
nation of all available News development sets.
The cross-entropy and the contribution of CC
relative to EP-NC, are shown for phrase transla-
tion probabilities in both directions in Table 4. The
numbers for lexical weights show similar trends.
For each cleaning step the cross-entropy is re-
duced and the contribution of CC is increased. The
difference between the basic cleaning and langid is
very small, however. The alignment-based clean-
ing shows a much larger effect. After that cleaning
step the CC corpus has a similar contribution to
EP-NC. This is an indicator that the final cleaned
CC corpus fits the development set well.
p(S|T ) p(T |S)
Cleaning CE IP CE IP
Basic 3.18 0.12 3.31 0.06
Langid 3.17 0.13 3.29 0.07
Alignment-based 3.02 0.47 3.17 0.61
Table 4: Cross-entropy (CE) and relative interpo-
lation weights (IP) compared to EP-NC for the
Common Crawl corpus, with different cleaning
Results In Table 5 we show the translation re-
sults with the different types of cleaning of CC,
and without it. We show results of different corpus
combinations both during tuning and testing. We
see that we get the overall best result by both tun-
ing and testing with the alignment-based cleaning
of CC, but it is not as useful to do the extra clean-
ing if we do not tune with it as well. Overall we
get the best results when tuning is performed in-
cluding a cleaned version of CC. This setup gives
a large improvement compared to not using CC at
all, or to use it with only basic cleaning. There is
little difference in Bleu scores when testing with
either basic cleaning, or cleaning based on lan-
guage ID, with a given tuning, which is not sur-
prising given their small and similar interpolation
weights. Tuning was, however, not successful
when using CC with basic cleaning.
Overall we think that alignment-based corpus
cleaning worked well. It reduced the size of the
corpus by over 25%, improved the cross-entropy
for interpolation with the EP-NC phrase-table, and
228
Testing
Tuning not used basic langid alignment
not used 14.0 13.9 13.9 14.0
basic 14.2 14.5 14.3 14.3
langid 15.2 15.3 15.3 15.3
alignment 12.7 15.3 15.3 15.7
Table 5: Bleu scores with different types of clean-
ing and without Common Crawl
gave an improvement on the translation task. We
still think that there is potential for further improv-
ing this filtering and to annotate larger test sets to
investigate the effects in more detail.
5 Tunable Distortion Limits
The Docent decoder uses a hill-climbing search
and can perform operations anywhere in the sen-
tence. Thus, it does not need to enforce a strict
distortion limit. In the Docent implementation, the
distortion limit is actually implemented as a fea-
ture, which is normally given a very large weight,
which effectively means that it works as a hard
constraint. This could easily be relaxed, however,
and in this work we investigate the effects of using
soft distortion limits, which can be optimized dur-
ing tuning, like other features. In this way long-
distance movements can be allowed when they are
useful, instead of prohibiting them completely. A
drawback of using no or soft distortion limits is
that it increases the search space.
In this work we mostly experiment with variants
of one or two standard distortion limits, but with a
tunable weight. We also tried to use separate soft
distortion limits for left- and right-movement. Ta-
ble 6 show the results with different types of dis-
tortion limits. The system with a standard fixed
distortion limits of 6 has a somewhat lower score
than most of the systems with no or soft distortion
limits. In most cases the scores are similar, and
we see no clear affects of allowing tunable lim-
its over allowing unlimited distortion. The system
that uses two mono-directional limits of 6 and 10
has slightly higher scores than the other systems,
and is used in our final submission.
One possible reason for the lack of effect of al-
lowing more distortion could be that it rarely hap-
pens that an operator is chosen that performs such
distortion, when we use the standard Docent set-
tings. To investigate this, we varied the settings of
the parameters that guide the swap-phrases opera-
tor, and used the move-phrases operator instead of
swap-phrases. None of these changes led to any
DL type Limit Bleu
No DL ? 15.5
Hard DL 6 15.0
One soft DL 6 15.5
8 14.2
10 15.5
Two soft DLs 4,8 15.5
6,10 15.7
Bidirectional soft DLs 6,10 15.5
Table 6: Bleu scores for different distortion limit
(DL) settings
improvements, however.
While we saw no clear effects when using tun-
able distortion limits, we plan to extend this work
in the future to model movement differently based
on parts of speech. For the English?German lan-
guage pair, for instance, it would be reasonable to
allow long distance moves of verb groups with no
or little cost, but use a hard limit or a high cost for
other parts of speech.
6 Corpus Selection for Recasing
In this section we investigate the effect of using
different corpus combinations for recasing. We
lower-cased our training corpus, which means that
we need a full recasing step as post-processing.
This is performed by training a SMT system on
lower-cased and true-cased target language. We
used the Moses toolkit to train the recasing system
and to decode during recasing. We investigate the
effect of using different combinations of the avail-
able training corpora to train the recasing model.
Table 7 show case sensitive Bleu scores, which
can be compared to the previous case-insensitive
scores of 15.7. We see that there is a larger effect
of including more data in the language model than
in the translation model. There is a performance
jump both when adding CC data and when adding
News data to the language model. The results
are best when we include the News data, which
is not included in the English?German translation
model, but which is much larger than the other cor-
pora. There is no further gain by using News in
combination with other corpora compared to using
only News. When adding more data to the trans-
lation model there is only a minor effect, with the
difference between only using EP-NC and using
all available corpora is at most 0.2 Bleu points.
In our submitted system we use the monolingual
News corpus both in the LM and the TM.
There are other options for how to treat recas-
229
Language model
TM EP-NC EP-NC-CC News EP-NC-News EP-NC-CC-News
EP-NC 13.8 14.4 14.8 14.8 14.8
EP-NC-CC 13.9 14.5 14.9 14.8 14.8
News 13.9 14.5 14.9 14.9 14.9
EP-NC-News 13.9 14.5 14.9 14.9 14.9
EP-NC-CC-News 13.9 14.5 14.9 14.9 15.0
Table 7: Case-sensitive Bleu scores with different corpus combinations for the language model and
translation model (TM) for recasing
ing. It is common to train the system on true-
cased data instead of lower-cased data, which has
been shown to lead to small gains for the English?
German language pair (Koehn et al, 2008). In this
framework there is still a need to find the correct
case for the first word of each sentence, for which
a similar corpus study might be useful.
7 Comparison to Moses
So far we have only shown results using the Do-
cent decoder on its own, with a random initializa-
tion, since we wanted to submit a Docent-only sys-
tem for the shared task. In this section we also
show contrastive results with Moses, and for Do-
cent initialized with stack decoding, using Moses,
and for different type of tuning.
Previous research have shown mixed results for
the effect of initializing Docent with and with-
out stack decoding, when using the same feature
sets. In Hardmeier et al (2012) there was a drop
of about 1 Bleu point for English?French trans-
lation based on WMT11 data when random ini-
tialization was used. In Stymne et al (2013a),
on the other hand, Docent gave very similar re-
sults with both types of initialization for German?
English WMT13 data. The latter setup is similar
to ours, except that no Common Crawl data was
used.
The results with our setup are shown in Ta-
ble 8. In this case we lose around a Bleu point
when using Docent on its own, without Moses ini-
tialization. We also see that the results are lower
when using Moses with the Docent tuning method,
or when combining Moses and Docent with Do-
cent tuning. This indicates that the document-
level tuning has not given satisfactory results in
this scenario, contrary to the results in Stymne et
al. (2013a), which we plan to explore further in
future work. Overall we think it is important to
develop stronger context-sensitive models for Do-
cent, which can take advantage of the document
context.
Test system Tuning system Bleu
Docent (random) Docent 15.7
Docent (stack) Docent 15.9
Moses Docent 15.9
Docent (random) Moses 15.9
Docent (stack) Moses 16.8
Moses Moses 16.8
Table 8: Bleu scores for Docent initialized ran-
domly or with stack decoding compared to Moses.
Tuning is performed with either Moses or Docent.
For the top line we used tunable distortion lim-
its 6,10 with Docent, in the other cases a standard
hard distortion limit of 6, since Moses does not al-
low soft distortion limits.
8 Conclusion
We have presented the Uppsala University system
for WMT 2013. Our submitted system uses Do-
cent with random initialization and two tunable
distortion limits of 6 and 10. It is trained with the
Common Crawl corpus, cleaned using language
identification and alignment-based filtering. For
recasing we used the monolingual News corpora.
For corpus-cleaning, we present a novel method
for cleaning noisy corpora based on the number
and ratio of word alignment links for sentence
pairs, which leads to a large reduction of corpus
size, and to small improvements on the transla-
tion task. We also experiment with tunable dis-
tortion limits, which do not lead to any consistent
improvements at this stage.
In the current setup the search algorithm of
Docent is not strong enough to compete with
the effective search in standard decoders like
Moses. We are, however, working on developing
discourse-aware models that can take advantage of
the document-level context, which is available in
Docent. We also need to further investigate tuning
methods for Docent.
230
References
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179?1190, Jeju Island, Korea.
Christian Hardmeier, Sara Stymne, Jo?rg Tiedemann,
and Joakim Nivre. 2013. Docent: A document-level
decoder for phrase-based statistical machine transla-
tion. In Proceedings of the 51st Annual Meeting of
the ACL, Demonstration session, Sofia, Bulgaria.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland.
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 967?
975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the NAACL, pages 48?54, Edmonton,
Alberta, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh system descrip-
tion for the 2005 IWSLT speech translation evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation, Pittsburgh, Penn-
sylvania, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL,
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008. Towards better machine translation quality for
the German-English language pairs. In Proceedings
of the Third Workshop on Statistical Machine Trans-
lation, pages 139?142, Columbus, Ohio, USA.
Marco Lui and Timothy Baldwin. 2012. langid.py: An
off-the-shelf language identification tool. In Pro-
ceedings of the 50th Annual Meeting of the ACL,
System Demonstrations, pages 25?30, Jeju Island,
Korea.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 42nd Annual Meeting of the ACL, pages 160?
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311?
318, Philadelphia, Pennsylvania, USA.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 16th
Annual Conference of the European Association
for Machine Translation, pages 539?549, Avignon,
France.
Andreas Stolcke. 1998. Entropy-based pruning of
backoff language models. In Proceedings of the
DARPA Broadcast News Transcription and Under-
standing Workshop, pages 270?274, Landsdowne,
Virginia, USA.
Andreas Stolcke. 2002. SRILM ? an extensible
language modeling toolkit. In Proceedings of the
Seventh International Conference on Spoken Lan-
guage Processing, pages 901?904, Denver, Col-
orado, USA.
Sara Stymne, Christian Hardmeier, Jo?rg Tiedemann,
and Joakim Nivre. 2013a. Feature weight opti-
mization for discourse-level SMT. In Proceedings
of the ACL 2013 Workshop on Discourse in Machine
Translation (DiscoMT 2013), Sofia, Bulgaria.
Sara Stymne, Jo?rg Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013b. Statistical machine trans-
lation with readability constraints. In Proceedings
of the 19th Nordic Conference on Computational
Linguistics (NODALIDA?13), pages 375?386, Oslo,
Norway.
231
Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60?69,
Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational Linguistics
Feature Weight Optimization for Discourse-Level SMT
Sara Stymne, Christian Hardmeier, Jo?rg Tiedemann and Joakim Nivre
Uppsala University
Department of Linguistics and Philology
Box 635, 751 26 Uppsala, Sweden
firstname.lastname@lingfil.uu.se
Abstract
We present an approach to feature weight
optimization for document-level decoding.
This is an essential task for enabling future
development of discourse-level statistical
machine translation, as it allows easy inte-
gration of discourse features in the decod-
ing process. We extend the framework of
sentence-level feature weight optimization
to the document-level. We show experi-
mentally that we can get competitive and
relatively stable results when using a stan-
dard set of features, and that this frame-
work also allows us to optimize document-
level features, which can be used to model
discourse phenomena.
1 Introduction
Discourse has largely been ignored in traditional
machine translation (MT). Typically each sentence
has been translated in isolation, essentially yield-
ing translations that are bags of sentences. It is
well known from translation studies, however, that
discourse is important in order to achieve good
translations of documents (Hatim and Mason,
1990). Most attempts to address discourse-level
issues for statistical machine translation (SMT)
have had to resort to solutions such as post-
processing to address lexical cohesion (Carpuat,
2009) or two-step translation to address pronoun
anaphora (Le Nagard and Koehn, 2010). Recently,
however, we presented Docent (Hardmeier et al,
2012; Hardmeier et al, 2013), a decoder based
on local search that translates full documents. So
far this decoder has not included a feature weight
optimization framework. However, feature weight
optimization, or tuning, is important for any mod-
ern SMT decoder to achieve a good translation
performance.
In previous research with Docent, we used grid
search to find weights for document-level features
while base features were optimized using stan-
dard sentence-level techniques. This approach is
impractical since many values for the extra fea-
tures have to be tried, and, more importantly, it
might not give the same level of performance as
jointly optimizing all parameters. Principled fea-
ture weight optimization is thus essential for re-
searchers that want to use document-level features
to model discourse phenomena such as anaphora,
discourse connectives, and lexical consistency. In
this paper, we therefore propose an approach that
supports discourse-wide features in document-
level decoding by adapting existing frameworks
for sentence-level optimization. Furthermore, we
include a thorough empirical investigation of this
approach.
2 Discourse-Level SMT
Traditional SMT systems translate texts sentence
by sentence, assuming independence between sen-
tences. This assumption allows efficient algo-
rithms based on dynamic programming for explor-
ing a large search space (Och et al, 2001). Be-
cause of the dynamic programming assumptions it
is hard to directly include discourse-level features
into a traditional SMT decoder. Nevertheless,
there have been several attempts to integrate inter-
sentential and long distance models for discourse-
level phenomena into standard decoders, usually
as ad-hoc additions to standard models, address-
ing a single phenomenon.
Several studies have tried to improve pro-
noun anaphora by adding information about the
antecedent, either by using two-step decoding
(Le Nagard and Koehn, 2010; Guillou, 2012) or
by extracting information from previously trans-
lated sentences (Hardmeier and Federico, 2010),
unfortunately without any convincing results. To
address the translation of discourse connectives,
source-side pre-processing has been used to anno-
tate surface forms either in the corpus or in the
60
phrase-table (Meyer and Popescu-Belis, 2012) or
by using factored decoding (Meyer et al, 2012)
to disambiguate connectives, with small improve-
ments. Lexical consistency has been addressed
by the use of post-processing (Carpuat, 2009),
multi-pass decoding (Xiao et al, 2011; Ture et al,
2012), and cache models (Tiedemann, 2010; Gong
et al, 2011). Gong et al (2012) addressed the
issue of tense selection for translation from Chi-
nese, by the use of inter-sentential tense n-grams,
exploiting information from previously translated
sentences. Another way to use a larger context
is by integrating word sense disambiguation and
SMT. This has been done by re-initializing phrase
probabilities for each sentence (Carpuat and Wu,
2007), by introducing extra features in the phrase-
table (Chan et al, 2007), or as a k-best re-ranking
task (Specia et al, 2008). Another type of ap-
proach is to integrate topic modeling into phrase
tables (Zhao and Xing, 2010; Su et al, 2012). For
a more thorough overview of discourse in SMT,
see Hardmeier (2012).
Here we instead choose to work with the re-
cent document-level SMT decoder Docent (Hard-
meier et al, 2012). Unlike in traditional decod-
ing were documents are generated sentence by
sentence, feature models in Docent always have
access to the complete discourse context, even
before decoding is finished. It implements the
phrase-based SMT approach (Koehn et al, 2003)
and is based on local search, where a state con-
sists of a full translation of a document, which is
improved by applying a series of operations to im-
prove the translation. A hill-climbing strategy is
used to find a (local) maximum. The operations
allow changing the translation of a phrase, chang-
ing the word order by swapping the positions of
two phrases, and resegmenting phrases. The initial
state can either be initialized randomly in mono-
tonic order, or be based on an initial run from a
standard sentence-based decoder. The number of
iterations in the decoder is controlled by two pa-
rameters, the maximum number of iterations and
a rejection limit, which stops the decoder if no
change was made in a certain number of iterations.
This setup is not limited by dynamic programming
constraints, and enables the use of the translated
target document to extract features. It is thus easy
to directly integrate discourse-level features into
Docent. While we use this specific decoder in our
experiments, the method proposed for document-
level feature weight optimization is not limited to
it. It can be used with any decoder that outputs
feature values at the document level.
3 Sentence-Level Tuning
Traditionally, feature weight optimization, or tun-
ing, for SMT is performed by an iterative process
where a development set is translated to produce a
k-best list. The parameters are then optimized us-
ing some procedure, generally to favor translations
in the k-best list that have a high score on some
MT metric. The translation step is then repeated
using the new weights for decoding, and optimiza-
tion is continued on a new k-best list, or on a com-
bination of all k-best lists. This is repeated until
some end condition is satisfied, for instance for a
set number of iterations, until there is only very
small changes in parameter weights, or until there
are no new translations in the k-best lists.
SMT tuning is a hard problem in general, partly
because the correct output is unreachable and
also because the translation process includes la-
tent variables, which means that many efficient
standard optimization procedures cannot be used
(Gimpel and Smith, 2012). Nevertheless, there
are a number of techniques including MERT (Och,
2003), MIRA (Chiang et al, 2008; Cherry and
Foster, 2012), PRO (Hopkins and May, 2011),
and Rampion (Gimpel and Smith, 2012). All of
these optimization methods can be plugged into
the standard optimization loop. All of the meth-
ods work relatively well in practice, even though
there are limitations, for instance that many meth-
ods are non-deterministic meaning that their re-
sults are somewhat unstable. However, there are
some important differences. MERT is based on
scores for the full test set, whereas the other meth-
ods are based on sentence-level scores. MERT
also has the drawback that it only works well for
small sets of features. In this paper we are not
concerned with the actual optimization algorithm
and its properties, though, but instead we focus
on the integration of document-level decoding into
the existing optimization frameworks.
In order to adapt sentence-level frameworks to
our needs we need to address the granularity of
scoring and the process of extracting k-best lists.
For document-level features we do not have mean-
ingful scores on the sentence level which are re-
quired in standard optimization frameworks. Fur-
thermore, the extraction of k-best lists is not as
61
Input: inputDocs, refDocs, init weights ?0, max decoder iters max, sample start ss, sample interval si,
Output: learned weights ?
1: ? ? ?0
2: Initialize empty klist
3: run? 1
4: repeat
5: Initialize empty klistrun
6: for doc? 1, inputDocs.size do Initialize decoder state randomly for inputDocs[doc]
7: for iter? 1,max do
8: Perform one hill-climbing step for inputDocs[doc]
9: if iter >= ss & iter mod si == 0 then
10: Add translation for inputDocs[doc] to klistrun
11: end if
12: end for
13: end for
14: Merge klistrun with klist
15: modelScoresdoc ? ComputeModelScores(klist)
16: metricStatsdoc ? ComputeMetricStats(klist, refDocs)
17: ?run ? ?
18: ? ? Optimize(?run,modelScoresdoc,metricStatsdoc)
19: run? run + 1
20: until Done(run, ?, ?run)
Figure 1: Document-level feature weight optimization algorithm
straightforward in our hill-climbing decoder as in
standard sentence-level decoders such as Moses
(Koehn et al, 2007) where such a list can be ap-
proximated easily from the internal beam search
strategy. Working on output lattices is another op-
tion in standard approaches (Cherry and Foster,
2012) which is also not applicable in our case.
In the following section we describe how we
can address these issues in order to adapt sentence-
level frameworks for our purposes.
4 Document-Level Tuning
To allow document-level feature weight optimiza-
tion, we make some small changes to the sentence-
level framework. Figure 1 shows the algorithm we
use. It assumes access to an optimization algo-
rithm, Optimize, and an end criterion, Done.
The changes from standard sentence-level opti-
mization is that we compute scores on the docu-
ment level, and that we sample translations instead
of using standard k-best lists.
The main challenge is that we need meaning-
ful scores which we do not have at the sentence
level in document decoding. We handle this by
simply computing all scores (model scores and
metric scores) exclusively at the document level.
Remember that all standard MT metrics based on
sentence-level comparisons with reference trans-
lations can be aggregated for a complete test set.
Here we do the same for all sentences in a given
document. This can actually be an advantage com-
pared to optimization methods that use sentence-
level scores, which are known to be unreliable
(Callison-Burch et al, 2012). Document-level
scores should thus be more stable, since they are
based on more data. A potential drawback is that
we get fewer data points with a test set of the same
size, which might mean that we need more data to
achieve as good results as with sentence-level op-
timization. We will see the ability of our approach
to optimize weights with reasonable data sets in
our experiments further down.
The second problem, the extraction of k-best
lists can be addressed in several ways. It is pos-
sible to get a k-best list from Docent by extract-
ing the results from the last k iterations. However,
since Docent operates on the document-level and
does not accept updates in each iteration, there will
be many identical and/or very similar hypotheses
with such an approach. Another option would be
to extract the translations from the k last differ-
ent iterations, which would require some small
changes to the decoder. Instead, we opt to use k-
lists, lists of translations sampled with some inter-
val, which contains k translations, but not neces-
sarily all the k best translations that could be found
by the decoder. A k-best list is of course a k-list,
which we get with a sample interval of 1.
We also choose to restart Docent randomly in
each optimization iteration, since it allows us to
explore a larger part of the search space. We
empirically found that this strategy worked better
than restarting the decoder from the previous best
state.
62
German?English English?Swedish
Type Sentences Documents Type Sentences Documents
Training
Europarl 1.9M ? Europarl 1.5M ?
News Commentary 178K ? ? ? ?
Tuning
News2009 2525 111 Europarl (Moses) 2000 ?
News2008-2010 7567 345 Europarl (Docent) 1338 100
Test News2012 3003 99 Europarl 690 20
Table 1: Domain and number of sentences and documents for the corpora
As seen in Figure 1, there are some additional
parameters in our procedure: the sample start iter-
ation and the sample interval. We also need to set
the number of decoder iterations to run. In Sec-
tion 5 we empirically investigate the effect of these
parameters.
Compared to sentence-level optimization, we
also have a smaller number of units to get scores
from, since we use documents as units, and not
sentences. The importance of this depends on the
optimization algorithm. MERT calculates metric
scores over the full tuning set, not for individual
sentences, and should not be affected too much
by the change in granularity. Many other opti-
mization algorithms, like PRO, work on the sen-
tence level, and will likely be more affected by
the reduction of units. In this work we focus on
MERT, which is the most commonly used opti-
mization procedure in the SMT community, and
which tends to work quite well with relatively few
features. However, we also show contrastive re-
sults for PRO (Hopkins and May, 2011). A fur-
ther issue is that Docent is non-deterministic, i.e.,
it can give different results with the same param-
eter weights. Since the optimization process is al-
ready somewhat unstable this is a potential issue
that needs to be explored further, which we do in
Section 5.
Implementation-wise we adapted Docent to out-
put k-lists and adapted the infrastructure available
for tuning in the Moses decoder (Koehn et al,
2007) to work with document-level scores. This
setup allows us to use the variety of optimization
procedures implemented there.
5 Experiments
In this section we report experimental results
where we investigate several issues in connec-
tion with document-level feature weight optimiza-
tion for SMT. We first describe the experimental
setup, followed by baseline results using sentence-
level optimization. We then present validation ex-
periments with standard sentence-level features,
which can be compared to standard optimization.
Finally, we report results with a set of document-
level features that have been proposed for joint
translation and text simplification (Stymne et al,
2013).
5.1 Experimental Setup
Most of our experiments are for German-to-
English news translation using data from the
WMT13 workshop.1 We also show results with
document-level features for English-to-Swedish
Europarl (Koehn, 2005). The size of the training,
tuning, and test sets are shown in Table 1. First of
all, we need to extract documents for tuning and
testing with Docent. Fortunately, the news data al-
ready contain document markup, corresponding to
individual news articles. For Europarl we define a
document as a consecutive sequence of utterances
from a single speaker. To investigate the effect of
the size of the tuning set, we used different subsets
of the available tuning data.
All our document-level experiments are car-
ried out with Docent but we also contrast with
the Moses decoder (Koehn et al, 2007). For the
purpose of comparison, we use a standard set of
sentence-level features used in Moses in most of
our experiments: five translation model features,
one language model feature, a distance-based re-
ordering penalty, and a word count feature. For
feature weight optimization we also apply the
standard settings in the Moses toolkit. We opti-
mize towards the Bleu metric, and optimization
ends either when no weights are changed by more
than 0.00001, or after 25 iterations. MERT is used
unless otherwise noted.
Except for one of our baselines, we always run
Docent with random initialization. For test we run
the document decoder for a maximum of 227 iter-
ations with a rejection limit of 100,000. In our
experiments, the decoder always stopped when
reaching the rejection limit, usually between 1?5
1http://www.statmt.org/wmt13/
translation-task.html
63
million iterations.
We show results on the Bleu (Papineni et al,
2002) and NIST (Doddington, 2002) metrics. For
German?English we show the average result and
standard deviation of three optimization runs, to
control for optimizer instability as proposed by
Clark et al (2011). For English?Swedish we re-
port results on single optimization runs, due to
time constraints.
5.2 Baselines
Most importantly, we would like to show the effec-
tiveness of the document-level tuning procedure
described above. In order to do this, we created
a baseline using sentence-level optimization with
a tuning set of 2525 sentences and the News2009
corpus for evaluation. Increasing the tuning set is
known to give only modest improvements (Turchi
et al, 2012; Koehn and Haddow, 2012).
The feature weights optimized with the stan-
dard Moses decoder can then directly be used in
our document-level decoder as we only include
sentence-level features in our baseline model. As
expected, these optimized weights also lead to
a better performance in document-level decoding
compared to an untuned model as shown in Ta-
ble 2. Note, that Docent can be initialized in
two ways, by Moses and randomly. Not surpris-
ingly, the result for the runs initialized with Moses
are identical with the pure sentence-level decoder.
Initializing randomly gives a slightly lower Bleu
score but with a larger variation than with Moses
initialization, which is also expected. Docent is
non-deterministic, and can give somewhat varying
results with the same weights. However, this vari-
ation has been shown experimentally to be very
small (Hardmeier et al, 2012).
Our goal now is to show that document-level
tuning can perform equally well in order to verify
our approach. For this, we set up a series of ex-
periments looking at varying tuning sets and dif-
ferent parameters of the decoding and optimiza-
tion procedure. With this we like to demonstrate
the stability of the document-level feature weight
optimization approach presented above. Note that
the most important baselines for comparison with
the results in the next sections are the ones with
Docent and random initialization.
5.3 Sentence-Level Features
In this section we present validation results where
we investigate different aspects of document-
System Tuning Bleu NIST
Moses None 17.7 6.25
Docent-M None 17.7 6.25
Docent-R None 15.2 (0.05) 5.88 (0.00)
Moses Moses 18.3 (0.04) 6.22 (0.01)
Docent-M Moses 18.3 (0.04) 6.22 (0.01)
Docent-R Moses 18.1 (0.13) 6.23 (0.01)
Table 2: Baseline results, where Docent-M is ini-
tialized with Moses and Docent-R randomly
Docs Sent. Min Max Bleu NIST
111 2525 3 127 18.0 (0.11) 6.19 (0.04)
345 7567 3 127 18.1 (0.14) 6.25 (0.02)
100 1921 8 40 18.0 (0.05) 6.25 (0.10)
200 3990 8 40 17.9 (0.25) 6.20 (0.09)
100 2394 8 100 18.0 (0.12) 6.27 (0.07)
200 4600 8 100 18.1 (0.29) 6.26 (0.10)
300 6852 8 100 18.2 (0.13) 6.27 (0.03)
Table 3: Results for German?English with varying
sizes of tuning set, where the number of sentences
and documents are varied, as well as the minimum
and maximum number of sentences per document
level feature weight optimization with standard
sentence-level features. In this way we can com-
pare the results directly to standard sentence-level
optimization, and to the results of Moses.
Corpus size We investigate how tuning is af-
fected by corpus size. The corpus size was var-
ied in two ways, by changing the number of docu-
ments in the tuning set, and by changing the length
of documents in the tuning sets. In this exper-
iment we run 20000 decoder iterations per opti-
mization iteration, and use a k-list of size 101,
with sample interval 100. Table 3 shows the re-
sults with varying tuning set sizes for German?
English. There is very little variation between the
scores, and no clear tendencies. All results are of
similar quality to the baseline with random initial-
ization and sentence-level tuning, and better than
not using any tuning. The top line in Table 3 is
News2009, the same tuning set as for the base-
lines. The scores are somewhat more unstable than
the baseline scores, but stability is not related to
corpus size. In the following sections we will use
the tuning set with 200 documents, size 8-40.
Number of decoder iterations and k-list sam-
pling Two issues that are relevant for feature
weight optimization with the document-level de-
coder is the number of decoder hill-climbing iter-
ations in each optimization iteration, and the set-
tings for k-list sampling. These choices affect the
64
Iterations K-list UTK Bleu NIST
20000 101 55.6 17.9 (0.25) 6.20 (0.09)
30000 201 67.2 17.9 (0.06) 6.21 (0.01)
40000 301 79.9 18.2 (0.11) 6.28 (0.09)
50000 401 86.9 18.1 (0.20) 6.22 (0.05)
75000 651 99.2 17.8 (0.15) 6.13 (0.03)
100000 901 106.8 17.9 (0.17) 6.16 (0.03)
30000 101 21.6 18.0 (0.15) 6.21 (0.02)
40000 101 12.6 17.7 (0.53) 6.12 (0.15)
50000 101 8.2 17.9 (0.24) 6.18 (0.06)
Table 4: Results for German?English with a vary-
ing number of iterations and k-list size (UTK is
the average number of unique translations per doc-
ument in the k-lists)
quality of the translations in each optimization it-
eration, and the spread in the k-list. We will report
the average number of unique translations per doc-
ument in the k-lists, UTK, during feature weight
optimization, in this section.
The top half of Table 4 shows results with a
different number of iterations, when we sample
k-lists from iteration 10000 with interval 100 for
German?English, which means that the size of the
k-lists also changes. The differences on MT met-
rics are very small. The number of new unique
translations in the k-lists decrease with the number
of decoder iterations. With 20K iterations, 55%
of the k-lists entries are unique, which could be
compared to only 12% with 100K iterations. The
majority of the unique translations are thus found
in the beginning of the decoding, which is not sur-
prising.
The bottom half of Table 4 shows results with
a different number of decoder iterations, but a set
k-list size. In this setting the number of unique
hypotheses in the k-lists obviously decreases with
the number of decoder iterations. Despite this,
there are mostly small result differences, except
for 40K iterations, which has more unstable results
than the other settings. It does not seem useful to
increase the number of decoder iterations without
also increasing the size of the k-list. An even bet-
ter strategy might be to only include unique entries
in the k-lists. We will explore this in future work.
We also ran experiments where we did not
restart the decoder with a random state in each iter-
ation, but instead saved the previous state and con-
tinued decoding with the new weights from there.
This, however, was largely unsuccessful, and gave
very low scores. We believe that the reason for this
is mainly that a much smaller part of the search
space is explored when the decoder is not restarted
Interval Start UTK Bleu NIST
1 19900 1.4 18.2 (0.07) 6.25 (0.04)
10 19000 5.2 18.1 (0.08) 6.22 (0.03)
100 10000 55.6 17.9 (0.25) 6.20 (0.09)
200 0 82.2 17.9 (0.19) 6.15 (0.05)
Table 5: Results with different k-list-sample inter-
vals for k-lists size 101 (UTK is the average num-
ber of unique translations per document in the k-
lists)
with a new seed repeatedly. The fact that a higher
overall quality can be achieved with a higher num-
ber of iterations (see Figure 2) can apparently not
compensate for this drawback.
Finally, we investigate the effect of the sam-
ple interval for the k-lists. To get k-lists of equal
size, 101, we start the sampling at different itera-
tions. Table 5 shows the results, and we can see
that with a small sample interval, the number of
unique translations decreases drastically. Despite
this, there are no large result differences. There
is actually a slight trend that a smaller sample in-
terval is better. This does not confirm our intuition
that it is important with many different translations
in the k-list. Especially for interval 1 it is surpris-
ing, since there is often only 1 unique translation
for a single document. We believe that the fact that
k-lists from different iterations are joined, can be
part of the explanation for these results. We think
more work is needed in the future, to further ex-
plore these settings, and the interaction with the
total number of decoder iteration, and the k-list
sampling.
To further shed some light to these results, we
show learning curves from the optimization. Fig-
ure 2 shows Bleu scores for the system optimized
with 100K decoder iterations after different num-
bers of iterations, for the last three iterations in
each of the three optimization runs. As shown in
Hardmeier et al (2012), the translation quality in-
creases fast at first, but start to level out at around
40K iterations. Despite this, the optimization re-
sults are good even with 20K iterations, which is
somewhat surprising. Figures 3 and 4 show the
Bleu scores after each tuning iteration for the sys-
tems in Tables 4 and 5. As is normal for SMT tun-
ing, the convergence is slow, and there are some
oscillations even late in the optimization. Over-
all systems with many iterations seem somewhat
more stable.
Overall, the results are better than the untuned
65
 
14
 
14.5 15
 
15.5 16
 
16.5 17
 
17.5
 
10
 
20
 
30
 
40
 
50
 
60
 
70
 
80
 
90 
100
Bleu
Dec
ode
r ite
ratio
ns (*
1000
)
1-23 1-24 1-25 2-23 2-24 2-25 3-23 3-24 3-25
Figure 2: Bleu scores during 100000 Docent iter-
ations during feature weight optimization
 
4
 
6
 
8
 
10
 
12
 
14
 
16
 
18  
0
 
5
 
10
 
15
 
20
 
25
Bleu
Tun
ing 
itera
tion
s20K 30K 40K 50K 75K 100
K
Figure 3: Bleu scores during feature weight opti-
mization for systems with different number of de-
coder iterations and k-list sizes.
baseline and on par with the sentence-level tuning
baselines in all settings, with a relatively modest
variation, even across settings. In fact, if we cal-
culate the total scores of all 36 systems in Tables 4
and 5, we get a Bleu score of 18.0 (0.23) and a
NIST score of 6.19 (0.07), with a variation that is
not higher than for many of the different settings.
Optimization method In this section we com-
pare the performance of the MERT optimiza-
tion algorithm with that of PRO, and a combi-
nation that starts MERT with weights initialized
with PRO (MERT+PRO), suggested by Koehn and
Haddow (2012). Here we run 30000 decoder it-
erations. Table 6 shows the results. Initializing
MERT with PRO did not affect the scores much.
The scores with only PRO, however, are slightly
lower than for MERT, and have a much larger
score variation. This could be because PRO is
 
4
 
6
 
8
 
10
 
12
 
14
 
16
 
18  
0
 
5
 
10
 
15
 
20
 
25
Bleu
Tun
ing 
itera
tion
s1 (20
K)
10 (2
0K)
100
 (20K
)
100
 (30K
)
100
 (40K
)
100
 (50K
)
200
 (20K
)
Figure 4: Bleu scores during feature weight opti-
mization for systems with different k-list sample
interval and number of decoder iterations.
Bleu NIST
MERT 17.9 (0.06) 6.21 (0.01)
PRO 17.5 (0.41) 6.15 (0.20)
MERT+PRO 18.0 (0.12) 6.18 (0.06)
Table 6: Results with different optimization algo-
rithms for German?English
likely to need more data, since it calculates met-
ric scores on individual units, sentences or docu-
ments, not across the full tuning set, like MERT.
This likely means that 200 documents are too few
for stable results with optimization methods that
depend on unit-level metric scores.
5.4 Document-Level Features
In this section we investigate the effect of opti-
mization with a number of document-level fea-
tures. We use a set of features proposed in Stymne
et al (2013), in order to promote the readability
of texts. In this scenario, however, we use these
features in a standard SMT setting, where they
can potentially improve the lexical consistency of
translations. The features are:
? Type token ratio (TTR) ? the ratio of types,
unique words, to tokens, total number of
words
? OVIX ? a reformulation of TTR that has tra-
ditionally been used for Swedish and that is
less sensible to text length than TTR, see
Eq. 1
? Q-value, phrase level (QP) - The Q-value was
developed as a measure for bilingual term
quality (Dele?ger et al, 2006), to promote
common and consistently translated terms.
See Eq. 2, where f(st) is the frequency of
66
German?English English?Swedish
System Optimization Bleu NIST Bleu NIST
Moses Sentence 18.3 (0.04) 6.22 (0.01) 24.3 6.12
Docent Sentence 18.1 (0.13) 6.23 (0.01) 24.1 6.06
Docent Document 17.9 (0.25) 6.20 (0.09) 23.4 6.01
TTR Document 18.3 (0.16) 6.33 (0.04) 23.6 6.15
OVIX Document 18.3 (0.13) 6.30 (0.03) 23.4 5.99
QW Document 18.1 (0.14) 6.22 (0.03) 24.2 6.11
QP Document 18.0 (0.10) 6.23 (0.05) 21.2 5.70
Table 7: Results when using document-level features
the phrase pair, n(s) is the number of unique
target phrases which the source phrase is
aligned to in the document, and n(t) is the
same for the target phrase. Here the Q-value
is applied on the phrase level.
? Q-value, word level (QW) - Same as above,
but here we apply the Q-value for source
words and their alignments on the target side.
OVIX =
log(count(tokens))
log
(
2?
log(count(types))
log(count(tokens))
) (1)
Q-value =
f(st)
n(s) + n(t)
(2)
We added these features one at a time to the
standard feature set. Optimization was performed
with 20000 decoder iterations, and a k-list of size
101. As shown in the previous sections, there
are slightly better settings, which could have been
used to boost the results somewhat.
The results are shown in Table 7. For German?
English, the results are generally on par with the
baselines for Bleu and slightly higher on NIST for
OVIX and TTR. For English?Swedish, we used a
smaller tuning set on the document level than on
the sentence level, see Table 1, due to time con-
straints. This is reflected in the scores, which are
generally lower than for sentence-level decoding.
Using the QW feature, however, we receive com-
petitive scores to the sentence-based baselines,
which indicates that it can be meaningful to use
document-level features with the suggested tuning
approach.
While the results do not improve much over
the baselines, these experiments still show that
we can optimize discourse-level features with
our approach. We need to identify more useful
document-level features in future work, however.
6 Conclusion
We have shown how the standard feature weight
optimization workflow for SMT can be adapted to
document-level decoding, which allows easy inte-
gration of discourse-level features into SMT. We
modified the standard framework by calculating
scores on the document-level instead of the sen-
tence level, and by using k-lists rather than k-best
lists.
Experimental results show that we can achieve
relatively stable results, on par with the results for
sentence-level optimization and better than with-
out tuning, with standard features. This is de-
spite the fact that we use the hill-climbing de-
coder without initialization by a standard decoder,
which means that it is somewhat unstable, and
is not guaranteed to find any global maximum,
even according to the model. We also show that
we can optimize document-level features success-
fully. We investigated the effect of a number of
parameters relating to tuning set size, the number
of decoder iterations, and k-list sampling. There
were generally small differences relating to these
parameters, however, indicating that the suggested
approach is robust. The interaction between pa-
rameters does need to be better explored in future
work, and we also want to explore better sampling,
without duplicate translations.
This is the first attempt of describing and exper-
imentally investigating feature weight optimiza-
tion for direct document-level decoding. While we
show the feasibility of extending sentence-level
optimization to the document level, there is still
much more work to be done. We would, for in-
stance, like to investigate other optimization pro-
cedures, especially for systems with a high num-
ber of features. Most importantly, there is a large
need for the development of useful discourse-level
features for SMT, which can now be optimized.
Acknowledgments
This work was supported by the Swedish strategic
research programme eSSENCE.
67
References
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada.
Marine Carpuat and Dekai Wu. 2007. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 61?72, Prague, Czech Republic.
Marine Carpuat. 2009. One translation per discourse.
In Proceedings of the Workshop on Semantic Evalu-
ations: Recent Achievements and Future Directions
(SEW-2009), pages 19?27, Boulder, Colorado.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves statisti-
cal machine translation. In Proceedings of the 45th
Annual Meeting of the ACL, pages 33?40, Prague,
Czech Republic.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
427?436, Montre?al, Canada.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of Human
Language Technologies: The 2008 Annual Con-
ference of the NAACL, pages 224?233, Honolulu,
Hawaii.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for op-
timizer instability. In Proceedings of the 49th An-
nual Meeting of the ACL: Human Language Tech-
nologies, pages 176?181, Portland, Oregon, USA.
Louise Dele?ger, Magnus Merkel, and Pierre Zweigen-
baum. 2006. Enriching medical terminologies:
an approach based on aligned corpora. In Inter-
national Congress of the European Federation for
Medical Informatics, pages 747?752, Maastricht,
The Netherlands.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology, pages 228?231, San Diego, California,
USA.
Kevin Gimpel and Noah A. Smith. 2012. Struc-
tured ramp loss minimization for machine transla-
tion. In Proceedings of the 2012 Conference of
the NAACL: Human Language Technologies, pages
221?231, Montre?al, Canada.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 909?919, Edinburgh, Scotland,
UK.
Zhengxian Gong, Min Zhang, Chew Lim Tan, and
Guodong Zhou. 2012. N-gram-based tense models
for statistical machine translation. In Proceedings
of the 2012 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 276?285,
Jeju Island, Korea.
Liane Guillou. 2012. Improving pronoun translation
for statistical machine translation. In Proceedings of
the EACL 2012 Student Research Workshop, pages
1?10, Avignon, France.
Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 283?289, Paris, France.
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1179?1190, Jeju Island, Korea.
Christian Hardmeier, Sara Stymne, Jo?rg Tiedemann,
and Joakim Nivre. 2013. Docent: A document-level
decoder for phrase-based statistical machine transla-
tion. In Proceedings of the 51st Annual Meeting of
the ACL, Demonstration session, Sofia, Bulgaria.
Christian Hardmeier. 2012. Discourse in statistical
machine translation: A survey and a case study. Dis-
cours, 11.
Basil Hatim and Ian Mason. 1990. Discourse and the
Translator. Longman, London, UK.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1352?1362, Edinburgh, Scotland.
Philipp Koehn and Barry Haddow. 2012. Towards
effective use of training data in statistical machine
translation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 317?321,
Montre?al, Canada.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the NAACL, pages 48?54, Edmonton,
Alberta, Canada.
68
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL,
Demo and Poster Sessions, pages 177?180, Prague,
Czech Republic.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of MT Summit X, pages 79?86, Phuket, Thailand.
Ronan Le Nagard and Philipp Koehn. 2010. Aiding
pronoun translation with co-reference resolution. In
Proceedings of the Joint Fifth Workshop on Statis-
tical Machine Translation and MetricsMATR, pages
252?261, Uppsala, Sweden.
Thomas Meyer and Andrei Popescu-Belis. 2012. Us-
ing sense-labeled discourse connectives for statisti-
cal machine translation. In Proceedings of the Joint
Workshop on Exploiting Synergies between Informa-
tion Retrieval and Machine Translation (ESIRMT)
and Hybrid Approaches to Machine Translation
(HyTra), pages 129?138, Avignon, France.
Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,
and Andrea Gesmundo. 2012. Machine translation
of labeled discourse connectives. In Proceedings of
the 10th Biennial Conference of the Association for
Machine Translation in the Americas, San Diego,
California, USA.
Franz Josef Och, Nicola Ueffing, and Hermann Ney.
2001. An efficient A* search algorithm for Statisti-
cal Machine Translation. In Proceedings of the ACL
2001 Workshop on Data-Driven Machine Transla-
tion, pages 55?62, Toulouse, France.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 42nd Annual Meeting of the ACL, pages 160?
167, Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the ACL, pages 311?
318, Philadelphia, Pennsylvania, USA.
Lucia Specia, Baskaran Sankaran, and Maria das
Grac?as Volpe Nunes. 2008. N-best reranking for
the efficient integration of word sense disambigua-
tion and statistical machine translation. In Proceed-
ings of the 9th International Conference on Intelli-
gent Text Processing and Computational Linguistics
(CICLING), pages 399?410, Haifa, Israel.
Sara Stymne, Jo?rg Tiedemann, Christian Hardmeier,
and Joakim Nivre. 2013. Statistical machine trans-
lation with readability constraints. In Proceedings
of the 19th Nordic Conference on Computational
Linguistics (NODALIDA?13), pages 375?386, Oslo,
Norway.
Jinsong Su, Hua Wu, Haifeng Wang, Yidong Chen,
Xiaodong Shi, Huailin Dong, and Qun Liu. 2012.
Translation model adaptation for statistical machine
translation with monolingual topic information. In
Proceedings of the 50th Annual Meeting of the ACL,
pages 459?468, Jeju Island, Korea.
Jo?rg Tiedemann. 2010. Context adaptation in statisti-
cal machine translation using models with exponen-
tially decaying cache. In Proceedings of the ACL
2010 Workshop on Domain Adaptation for Natural
Language Processing (DANLP), pages 8?15, Upp-
sala, Sweden.
Marco Turchi, Tijl De Bie, Cyril Goutte, and Nello
Cristianini. 2012. Learning to translate: A statis-
tical and computational analysis. Advances in Arti-
ficial Intelligence, 2012. Article ID 484580.
Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of the 2012 Conference of the
NAACL: Human Language Technologies, pages
417?426, Montre?al, Canada.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in
machine translation. In Proceedings of MT Summit
XIII, pages 131?138, Xiamen, China.
Bing Zhao and Eric P. Xing. 2010. HM-BiTAM: Bilin-
gual topic exploration, word alignment,and transla-
tion. In Advances in Neural Information Processing
Systems 20 (NIPS), pages 1689?1696, Cambridge,
Massachusetts, USA.
69
