c? 2004 Association for Computational Linguistics
Fast Approximate Search in Large
Dictionaries
Stoyan Mihov? Klaus U. Schulz?
Bulgarian Academy of Sciences University of Munich
The need to correct garbled strings arises in many areas of natural language processing. If a
dictionary is available that covers all possible input tokens, a natural set of candidates for correcting
an erroneous input P is the set of all words in the dictionary for which the Levenshtein distance to P
does not exceed a given (small) bound k. In this article we describe methods for efficiently selecting
such candidate sets. After introducing as a starting point a basic correction method based on the
concept of a ?universal Levenshtein automaton,? we show how two filtering methods known from
the field of approximate text search can be used to improve the basic procedure in a significant
way. The first method, which uses standard dictionaries plus dictionaries with reversed words,
leads to very short correction times for most classes of input strings. Our evaluation results
demonstrate that correction times for fixed-distance bounds depend on the expected number of
correction candidates, which decreases for longer input words. Similarly the choice of an optimal
filtering method depends on the length of the input words.
1. Introduction
In this article, we face a situation in which we receive some input in the form of strings
that may be garbled. A dictionary that is assumed to contain all possible correct input
strings is at our disposal. The dictionary is used to check whether a given input is
correct. If it is not, we would like to select the most plausible correction candidates
from the dictionary. We are primarily interested in applications in the area of natural
language processing in which the background dictionary is very large and fast selection
of an appropriate set of correction candidates is important. By a ?dictionary,? we
mean any regular (finite or infinite) set of strings. Some possible concrete application
scenarios are the following:
? The dictionary describes the set of words of a highly inflectional or
agglutinating language (e.g., Russian, German, Turkish, Finnish,
Hungarian) or a language with compound nouns (German). The
dictionary is used by an automated or interactive spelling checker.
? The dictionary is multilingual and describes the set of all words of a
family of languages. It is used in a system for postcorrection of results of
OCR in which scanned texts have a multilingual vocabulary.
? Linguistic Modelling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences,
25A, Akad. G. Bonchev Str., 1113 Sofia, Bulgaria. E-mail: stoyan@lml.bas.bg
? Centrum fu?r Informations-und Sprachverarbeitung, Ludwig-Maximilians-Universita?t-Mu?nchen,
Oettingenstr. 67, 80538 Munchen, Germany. E-mail: schulz@cis.uni-muenchen.de
Submission received: 12 July 2003; Revised submission received: 28 February 2004; Accepted for
publication: 25 March 2004
452
Computational Linguistics Volume 30, Number 4
? The dictionary describes the set of all indexed words and phrases of an
Internet search engine. It is used to determine the plausibility that a new
query is correct and to suggest ?repaired? queries when the answer set
returned is empty.
? The input is a query to some bibliographic search engine. The dictionary
contains titles of articles, books, etc.
The selection of an appropriate set of correction candidates for a garbled input P is
often based on two steps. First, all entries W of the dictionary are selected for which the
distance between P and W does not exceed a given bound k. Popular distance measures
are the Levenshtein distance (Levenshtein 1966; Wagner and Fischer 1974; Owolabi
and McGregor 1988; Weigel, Baumann, and Rohrschneider 1995; Seni, Kripasundar,
and Srihari 1996; Oommen and Loke 1997) or n-gram distances (Angell, Freund, and
Willett 1983; Owolabi and McGregor 1988; Ukkonen 1992; Kim and Shawe-Taylor 1992,
1994) Second, statistical data, such as frequency information, may be used to compute
a ranking of the correction candidates. In this article, we ignore the ranking problem
and concentrate on the first step. For selection of correction candidates we use the
standard Levenshtein distance (Levenshtein 1966). In most of the above-mentioned
applications, the number of correction candidates becomes huge for large values of k.
Hence small bounds are more realistic.
In light of this background, the algorithmic problem discussed in the article can
be described as follows:
Given a pattern P, a dictionary D, and a small bound k, efficiently
compute the set of all entries W in D such that the Levenshtein distance
between P and W does not exceed k.
We describe a basic method and two refinements for solving this problem. The basic
method depends on the new concept of a universal deterministic Levenshtein au-
tomaton of fixed degree k. The automaton of degree k may be used to decide, for
arbitrary words U and V, whether the Levenshtein distance between U and V does
not exceed k. The automaton is ?universal? in the sense that it does not depend on
U and V. The input of the automaton is a sequence of bitvectors computed from U
and V. Though universal Levenshtein automata have not been discussed previously
in the literature, determining Levenshtein neighborhood using universal Levenshtein
automata is closely related to a more complex table-based method described by the
authors Schulz and Mihov (2002). Hence the main advantage of the new notion is its
conceptual simplicity. In order to use the automaton for solving the above problem,
we assume that the dictionary is given as a determininistic finite-state automaton. The
basic method may then be described as a parallel backtracking traversal of the uni-
versal Levenshtein automaton and the dictionary automaton. Backtracking procedures
of this form are well-known and have been used previously: for example, by Oflazer
(1996) and the authors Schulz and Mihov (2002).
For the first refinement of the basic method, a filtering method used in the field
of approximate text search is adapted to the problem of approximate search in a dic-
tionary. In this approach, an additional ?backwards? dictionary D?R (representing the
set of all reverses of the words of a given dictionary D) is used to reduce approximate
search in D with a given bound k ? 1 to related search problems for smaller bounds
k? < k in D and D?R. As for the basic method, universal Levenshtein automata are used
to control the search. Ignoring very short input words and correction bound k = 1,
453
Mihov and Schulz Fast Approximate Search in Large Dictionaries
this approach leads to a drastic increase in speed. Hence the ?backwards dictionary
method? can be considered the central contribution of this article.
The second refinement, which is only interesting for bound k = 1 and short input
words, also uses a filtering method from the field of approximate text search (Muth
and Manber 1996; Mor and Fraenkel 1981). In this approach, ?dictionaries with single
deletions? are used to reduce approximate search in a dictionary D with bound k = 1
to a conventional lookup technique for finite-state transducers. Dictionaries with single
deletions are constructed by deleting the symbol at a fixed position n in all words of
a given dictionary.
For the basic method and the two refinements, detailed evaluation results are given
for three dictionaries that differ in terms of the number and average length of entries:
a dictionary of the Bulgarian language with 965,339 entries (average length 10.23 sym-
bols), a dictionary of German with 3,871,605 entries (dominated by compound nouns,
average length 18.74 symbols), and a dictionary representing a collection of 1,200,073
book titles (average length 47.64 symbols). Tests were restricted to distance bounds
k = 1, 2, 3. For the approach based on backwards dictionaries, the average correction
time for a given input word?including the displaying of all correction suggestions?
is between a few microseconds and a few milliseconds, depending on the dictionary,
the length of the input word, and the bound k. Correction times over one millisecond
occur only in a few cases for bound k = 3 and short input words. For bound k = 1,
which is important for practical applications, average correction times did not exceed
40 microseconds.
As a matter of fact, correction times are a joint result of hardware improvements
and algorithmic solutions. In order to judge the quality of the correction procedure in
absolute terms, we introduce an ?idealized? correction algorithm in which any kind
of blind search and superfluous backtracking is eliminated. Based on an analysis of
this algorithm, we believe that using purely algorithmic improvements, our correction
times can be improved only by a factor of 50?250, depending on the kind of dictionary
used. This factor represents a theoretical limit in the sense that the idealized algorithm
probably cannot be realized in practice.
This article is structured as follows. In Section 2, we collect some formal prelimi-
naries. In Section 3, we briefly summarize some known techniques from approximate
string search in a text. In Section 4, we introduce universal deterministic Levenshtein
automata of degree k and describe how the problem of deciding whether the Lev-
enshtein distance between two strings P and W does not exceed k can be efficiently
solved using this automaton. Since the method is closely related to a table-based
approach introduced by the authors (Schulz and Mihov 2002), most of the formal de-
tails have been omitted. Sections 5, 6, and 7 describe, respectively, the basic method,
the refined approach based on backwards dictionaries, and the approach based on
dictionaries with single deletions. Evaluation results are given for the three dictio-
naries mentioned above. In Section 8 we briefly comment on the difficulties that
we encountered when trying to combine dictionary automata and similarity keys
(Davidson 1962; Angell, Freund, and Willett 1983; Owolabi and McGregor 1988; Sinha
1990; Kukich 1992; Anigbogu and Belaid 1995; Zobel and Dart 1995; de Bertrand de
Beuvron and Trigano 1995). Theoretical bounds for correction times are discussed in
Section 9.
The problem considered in this article is well-studied. Since the number of contri-
butions is enormous, a complete review of related work cannot be given here. Relevant
references with an emphasis on spell-checking and OCR correction are Blair (1960),
Riseman and Ehrich (1971), Ullman (1977), Angell, Freund, and Willett (1983), Srihari,
Hull, and Choudhari (1983), Srihari (1985), Takahashi et al (1990), Kukich (1992), Zobel
454
Computational Linguistics Volume 30, Number 4
and Dart (1995), and Dengel et al (1997). Exact or approximate search in a dictionary
is discussed, for example, in Wells et al (1990), Sinha (1990), Bunke (1993), Oflazer
(1996), and Baeza-Yates and Navarro (1998). Some relevant work from approximate
search in texts is described in Section 3.
2. Formal Preliminaries
We assume that the reader is familiar with the basic notions of formal language theory
as described, for example, by Hopcroft and Ullman (1979) or Kozen (1997). As usual,
finite-state automata (FSA) are treated as tuples of the form A = ??, Q, q0, F,??, where
? is the input alphabet, Q is the set of states, q0 ? Q is the initial state, F is the set
of final states, and ? ? Q ? ?? ? Q is the transition relation. Here ? denotes the
empty string and ?? := ??{?}. The generalized transition relation ?? is defined as the
smallest subset of Q ? ?? ? Q with the following closure properties:
? For all q ? Q we have (q, ?, q) ? ??.
? For all q1, q2, q3 ? Q and W1, W2 ? ??, if (q1, W1, q2) ? ?? and
(q2, W2, q3) ? ?, then also (q1, W1W2, q3) ? ??.
We write L(A) for the language accepted by A. We have L(A) = {W ? ?? | ?q ?
F : (q0, W, q) ? ??}. Given A as above, the set of active states for input W ? ?? is
{q ? Q | (q0, W, q) ? ??}.
A finite-state automaton A is deterministic if the transition relation is a function
? : Q ? ? ? Q. Let A = ??, Q, q0, F, ?? be a deterministic FSA, and let ?? : Q ? ?? ? Q
denote the generalized transition function, which is defined in the usual way. For
q ? Q, we write LA(q) := {U ? ?? | ??(q, U) ? F} for the language of all words that
lead from q to a final state.
The length of a word W is denoted by |W|. Regular languages over ? are defined
in the usual way. With L1 ? L2 we denote the concatenation of the languages L1 and
L2. It is well-known that for any regular language L, there exists a deterministic FSA
AL such that L(A) = L and AL is minimal (with respect to number of states) among
all deterministic FSA accepting L. AL is unique up to renaming of states.
A p-subsequential transducer is a tuple T = ??,?, Q, q0, F, ?,?,??, where
? ??, Q, q0, F, ?? is a deterministic finite-state automaton;
? ? is a finite output alphabet;
? ? : Q ? ? ? ?? is a function called the transition output function;
? the final function ? : F ? 2?? assigns to each f ? F a set of strings over
?, where |?(f )| ? p.
The function ? is extended to the domain Q ? ?? by the following definition of ??:
?q ? Q (??(q, ?) = ?)
?q ? Q ?U ? ?? ?a ? ? (??(q, Ua) = ??(q, U)?(??(q, U), a))
The input language of the transducer is L(T) := {U ? ?? | ??(q0, U) ? F}. The
subsequential transducer maps each word from the input language to a set of at most
455
Mihov and Schulz Fast Approximate Search in Large Dictionaries
p output words. The output function OT : L(T) ? 2?
?
of the transducer is defined as
follows:
?U ? L(T) (OT(U) = ??(q0, U) ??(??(q0, U)))
By a dictionary, we mean a regular (finite or infinite) set of strings over a given
alphabet ?. Using the algorithm described by Daciuk et al (2000), the minimal deter-
ministic FSA AD accepting a finite dictionary D can be effectively computed.
By a dictionary with output sets, we mean a regular (finite or infinite) set of
input strings over a given alphabet together with a function that maps each of the
input strings to a finite set of output strings. Given a finite dictionary with output
sets, we can effectively compute, using the algorithm described by Mihov and Maurel
(2001), the minimal subsequential transducer that maps each input string to its set of
output strings.
3. Background
In this section, we describe some established work that is of help in understanding
the remainder of the article from a nontechnical, conceptual point of view. After in-
troducing the Levenshtein distance, we describe methods for computing the distance,
for checking whether the distance between two words exceeds a given bound, and for
approximate search for a pattern in a text. The similarities and differences described
below between approximate search in a text, on the one hand, and approximate search
in a dictionary, on the other hand, should help the reader understand the contents of
the following sections from a broader perspective.
3.1 Computation of Levenshtein Distance
The most prominent metric for comparing strings is the Levenshtein distance, which
is based on the notion of a primitive edit operation. In this article, we consider the
standard Levenshtein distance. Here the primitive operations are the substitution of
one symbol for another symbol, the deletion of a symbol, and the insertion of a
symbol. Obviously, given two words W and V in the alphabet ?, it is always possible
to rewrite W into V using primitive edit operations.
Definition 1
Let P, W be words in the alphabet ?. The (standard) Levenshtein distance between
P and W, denoted dL(P, W), is the minimal number of primitive edit operations (sub-
stitutions, deletions, insertions) that are needed to transform P into W.
The Levenshtein distance between two words P and W can be computed using the
following simple dynamic programming scheme, described, for example, by Wagner
and Fischer (1974):
dL(?, W) = |W|
dL(P, ?) = |P|
dL(Pa, Wb) =
{
dL(P, W) if a = b
1 + min(dL(P, W), dL(Pa, W), dL(P, Wb)) if a = b
for P, W ? ?? and a, b ? ?. Given P = p1 . . . pm and W = w1 . . .wn (m, n ? 0), a standard
way to apply the scheme is as follows: Proceeding top-down and from left to right,
the cells of an (m + 1)? (n + 1) table TL(P, W) are filled, where entry (i, j) of T(P, W)
is dL(p1 . . . pi, w1 . . .wj) (0 ? i ? m, 0 ? j ? n) (Wagner and Fischer 1974). The first two
456
Computational Linguistics Volume 30, Number 4
0 1 2 3 4 5 6
1 1 1 2 3 4 5
2 1 2 1 2 3 4
3 2 2 2 1 2 3
4 3 3 3 2 1 2
5 4 4 4 3 2 1
h c h o l d
c
h
o
l
d
Figure 1
Computation of the Levenshtein distance using dynamic programming and filling table
TL(chold, hchold). Shaded regions represent diagonals in Ukkonen?s approach (cf. Section 3.2).
clauses above are used for initialization and yield, respectively, the first column and
the first row. The third clause is used to compute the remaining entries. The table for
the strings chold and hchold is shown in Figure 1.
3.2 Testing Levenshtein Neighborhood
The algorithm of Wagner and Fischer, which has time complexity O(m ? n), has been
improved and generalized in many aspects. (See, for example, Stephen [1994] for a
survey). We briefly sketch a more efficient variant that can be used for the restricted
problem of deciding whether the Levenshtein distance between two words P and W
exceeds a fixed bound, k. Ukkonen (1985a) shows that in this case only the values of
2k + 1 ?diagonals? of TL(P, W) are essential for a test to make such a determination.
Figure 1 illustrates the situation in which k = 2. Ukkonen obtained an algorithm
with time complexity O(k ? min(m, n)). He used the test for determining whether the
Levenshtein distance between two words exceeds a given bound to derive an algorithm
for computing the edit distance with complexity O(min(m, n) ? dL(P, W)).
3.3 Approximate Search for a Pattern in a Text
A problem closely related to approximate search in a dictionary is approximate search
for a pattern in a text (AST): Given two strings P and T (called, respectively, the pattern
and the text), find all occurrences T? of substrings of T that are within a given distance
of P. Each occurrence T? is called a hit. In the following discussion, we consider the
case in which a fixed bound k for the Levenshtein distance between P and potential
hits is specified.
3.3.1 Adapting the Dynamic Programming Scheme. A simple adaptation of the
Wagner-Fischer algorithm may be used for approximate search for a pattern P =
p1 ? ? ? pm in a text T = t1 ? ? ? tn. As before, we compute an (m+1)?(n+1) table TAST(P, T).
Entry (i, j) of TAST(P, T) has value h if h is the minimal Levenshtein distance between
p1 ? ? ? pi and a substring of T with last symbol (position) tj (j). From the definition, we
see that all cells in line 0 have to be initialized with 0. The remaining computation
proceeds as above. For a given bound k, the output consists of all positions j such that
entry (m, j) does not exceed k. Note that the positions j in the output are the end points
in T of approximate matches of P. Figure 2 illustrates a search employing dynamic
programming.
3.3.2 Automaton Approach. Several more-efficient methods for approximate search
of a pattern P in a text T take as their starting point a simple nondeterministic finite-
state automaton, AAST(P, k), which accepts the language of all words with Levenshtein
457
Mihov and Schulz Fast Approximate Search in Large Dictionaries
0 0 0 0 0 0 0
1 1 1 1 1 1 0
2 2 1 2 2 2 1
3 3 2 2 3 3 2
4 4 3 3 3 4 3
5 5 4 4 4 4 4
t h i s _ c
c
h
o
l
d
0
1
0
1
2
3
h
0
1
1
1
2
3
i
0
1
2
2
1
2
l
0
1
2
3
2
1
d
Figure 2
Approximate search of pattern chold in a text using dynamic programming.
distance ? k to some word in ?? ?P (Ukkonen 1985b; Wu and Manber 1992; Baeza-Yates
and Navarro 1999). The automaton for pattern chold and distance bound k = 2 is shown
in Figure 3. States are numbered in the form be. The ?base number? b determines the
position of the state in the pattern. The ?exponent? e indicates the error level, that
is, the number of edit errors that have been observed. Horizontal transitions encode
?normal? transitions in which the text symbol matches the expected next symbol of
the pattern. Vertical transitions represent insertions, nonempty (respectively, empty)
diagonal transitions represent substitutions (respectively, deletions). In the example
shown in Figure 3, final states are 50, 51, and 52. It is obvious that when using a given
text T as input, we reach a final state of AAST(P, 2) exactly at those positions where
a substring T? of T ends such that dL(P, T?) ? 2. For other bounds k, we just have to
vary the number of levels. Note that a string can be accepted in AAST(P, k) at several
final states. In order to determine the optimal distance between P and a substring T?
of T ending at a certain position, it is necessary to determine the final state that can
be reached that has the smallest exponent.
In the remainder of the article, the set of all states with base number i is called
the ith column of AAST(P, k).
Remark 1
There is a direct relationship between the entries in column j of the dynamic program-
ming table TAST(P, T) and the set of active states of AAST(P, k) that are reached with
input t1 ? ? ? tj. Entry (i, j) of TAST(P, T) has the value h ? k iff h is the exponent of the
bottom-most active state in the ith column of AAST(P, k). For example, in Figure 3, the
set of active states of AAST(chold, k) reached after reading the two symbols t and h
is highlighted. The bottom-most elements 00, 11, 21, and 32 correspond to the entries
0, 1, 1, and 2 shaded in the upper part of the third column of Figure 2.
?
524232221202
514131211101
00 10 20 30 40 50c h o l d
?
?
?
?
?
?
?
?
?
?
? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?
?
?
c h o l d
c h o l d
0
1
2
?
?
Figure 3
Nondeterministic automaton AAST(chold, 2) for approximate search with pattern chold and
distance bound k = 2. Active states after symbols t and h have been read are highlighted.
458
Computational Linguistics Volume 30, Number 4
?
524232221202
514131211101
00 10 20 30 40 50c h o l d
?
?
?
?
?
?
?
?
?
?
? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?
? ?
?
c h o l d
c h o l d
0
1
2
?
?
Figure 4
Nondeterministic automaton A(chold, 2) for testing Levenshtein distance with bound k = 2 for
pattern chold. Triangular areas are highlighted. Dark states are active after symbols h and c
have been read.
The direct use of the nondeterministic automaton AAST(P, k) for conducting ap-
proximate searches is inefficient. Furthermore, depending on the length m of the pat-
tern and the error bound k, the explicit construction and storage of a deterministic
version of AAST(P, k) might be difficult or impossible. In practice, simulation of deter-
minism via bit-parallel computation of sets of active states gives rise to efficient and
flexible algorithms. See Navarro (2001) and Navarro and Raffinot (2002) for surveys
of algorithms along this line.
4. Testing Levenshtein Neighborhood with Universal Deterministic
Levenshtein Automata
In our approach, approximate search of a pattern P in a dictionary D is traced back
to the problem of deciding whether the Levenshtein distance between P and an entry
W of D exceeds a given bound k. A well-known method for solving this problem is
based on a nondeterministic automaton A(P, k) similar to AAST(P, k). A string W is
accepted by A(P, k) iff dL(P, W) ? k. The automaton A(P, k) does not have the initial ?
loop that is needed in AAST(P, k) to traverse the text. The automaton for pattern chold
and distance bound k = 2 is shown in Figure 4. Columns of A(P, k) with numbers
0, . . . , m = |P| are defined as for AAST(P, k). In A(P, k), we use as final states all states
q from which we can reach one of the states in column m using a (possibly empty)
sequence of -transitions. The reason for this modification?which obviously does not
change the set of accepted words?will become apparent later.
We now show that for fixed small error bounds k, the explicit computation of
A(P, k), in a deterministic or nondeterministic variant, can be completely avoided. In
our approach, pattern P and entry W = w1 ? ? ?wn are compared to produce a sequence
of n bitvectors ?1, . . . , ?n. This sequence is used as input for a fixed automaton A?(k).
The automaton A?(k) is deterministic and ?universal? in the sense that it does not
depend on a given pattern P. For each bound k, there is just one fixed automaton
A?(k), which is precomputed once and used for arbitrary patterns P and words W.
A?(k) accepts input ?1, . . . , ?n iff dL(P, W) ? k. The efficiency of this method relies
on the fact that given A?(k), we need only one operation for each transition of the
recognition phase after the initial computation of the bitvectors ?1, . . . , ?n.
It is worth mentioning that the possibility of using a fixed universal automaton
A?(k) instead of a specific automaton A(P, k) for each pattern P is based on special
features of the automata A(P, k) (cf. Remark 2); a similar technique for AAST(P, k)
459
Mihov and Schulz Fast Approximate Search in Large Dictionaries
appears to be impossible. For defining states, input vectors and transitions of A?(k),
the following two definitions are essential:
Definition 2
The characteristic vector ?(w, V) of a symbol w ? ? in a word V = v1 ? ? ? vn ? ?? is
the bitvector of length n where the ith bit is set to 1 iff w = vi.
Definition 3
Let P denote a pattern of length m. The triangular area of a state p of A(P, k) consists of
all states q of A(P, k) that can be reached from p using a (potentially empty) sequence
of u upward transitions and, in addition, h ? u horizontal or reverse (i.e., leftward)
horizontal transitions. Let 0 ? i ? m. By triangular area i, we mean the triangular area
of state i0. For j = 1, . . . , k, by triangular area m+j, we mean the triangular area of the
state mj.
For example, in Figure 4, triangular areas 0, . . . , 7 of A(chold, 2) are shown.
In Remark 1, we pointed to the relationship between the entries in column i of table
TAST(P, T) and the set of active states of AAST(P, k) that are reached with input w1 ? ? ?wi.
A similar relationship holds between the entries in column i of table TL(P, T) and the
set of active states of the automaton A(P, k) that are reached with input w1 ? ? ?wi.
Triangular area i corresponds to the ith column of the subregion of TL(P, T) given by
the 2k+ 1 diagonals used in Ukkonen?s (1985a) approach. The left-to-right orientation
in A(P, k) corresponds to a top-down orientation in TL(P, T). As an illustration, the
active states of A(chold, 2) after symbols h and c have been consumed are marked in
Figure 4. The exponents 2, 1, 2, and 2 of the bottom-most active states in columns 1,
2, 3, and 4 are found in the shaded region of the third column of Figure 1.
Remark 2
It is simple to see that for any input string W = w1 . . .wn, the set of active states of
A(P, k) reached after reading the ith symbol wi of W is a subset of triangular area i
(0 ? i ? min{n, m + k}). For i > m + k, the set is empty. Furthermore, the set of active
states that is reached after reading symbol wi depends only
1. on the previous set of active states (the set reached after reading
w1 . . .wi?1, a subset of the triangular area i ? 1);
2. on the characteristic vector ?(wi, pl ? ? ? pi ? ? ? pr) where l = max{1, i ? k}
and r = min{m, i + k}.
The following description of A?(k) proceeds in three steps that introduce, in order,
input vectors, states, and the transition function. States and transition function are
described informally.
1. Input vectors. Basically we want to use the vectors ?(wi, pl ? ? ? pi ? ? ? pr), which
are of length ? 2k + 1, as input for A?(k). For technical reasons, we introduce two
modifications. First, in order to standardize the length of the characteristic vectors that
are obtained for the initial symbols w1, w2, . . ., we define p0 = p?1 = . . . = p?k+1 := $.
In other words, we attach to P a new prefix with k symbols $. Here $ is a new symbol
that does not occur in W. Second imagine that we get to triangular area i after reading
the ith letter wi (cf. Remark 2). As long as i ? m ? k ? 1, we know that we cannot
reach a triangular area containing final states after reading wi. In order to encode
460
Computational Linguistics Volume 30, Number 4
this information in the input vectors, we enlarge the relevant subword of P for input
wi and consider one additional position i + k + 1 on the right-hand side (whenever
i + k + 1 ? m). This means that we use the vectors ?i := ?(wi, pi?k ? ? ? pi ? ? ? pr), where
r = min{m, i+k+1}, as input for A?(k), for 1 ? i ? min{n, m+k}. Consequently, for 0 ?
i ? m?k?1, the length of ?i is 2k+2; for i = m?k (respectively, m?k+1, . . . , m, . . . , m+k),
the length of ?i is 2k + 1 (respectively, 2k, . . . , k + 1, . . . , 1).
Example 1
Consider Figure 4 where P is chold and k = 2. Input hchold is translated into the vectors
?1 = ?(h, $$chol) = 000100
?2 = ?(c, $chold) = 010000
?3 = ?(h, chold) = 01000
?4 = ?(o, hold) = 0100
?5 = ?(l, old) = 010
?6 = ?(d, ld) = 01
The computation of the vectors ?i for input W = w1 . . .wn is based on a prelimi-
nary step in which we compute for each ? ? ? the vector ?(?) := ?(?, $ . . . $p1 . . . pm)
(using k copies of $). The latter vectors are initialized in the form ?(w) := 0k+m. We
then compute for i = 1, . . . , n the value ?(wi) := ?(wi) | 0k+i?110m?i. Here the sym-
bol | denotes bitwise OR. Once we have obtained the values ?(wi), which are repre-
sented as arrays, the vectors ?i := ?(wi, pi?k . . . pi . . . pr) can be accessed in constant
time.
2. States. Henceforth, states of automata A(P, k) will be called positions. Recall
that a position is given by a base number and an exponent e, 0 ? e ? k represent-
ing the error count. By a symbolic triangular area, we mean a triangular area in
which ?explicit? base numbers (like 1, 2, . . .) in positions are replaced by ?symbolic?
base numbers of a form described below. Two kinds of symbolic triangular areas
are used. A unique ?I-area? represents all triangular areas of automata A(P, k) that
do not contain final positions. The ?integer variable? I is used to abstract from pos-
sible base numbers i, 0 ? i ? m ? k ? 1. Furthermore, k + 1 ?M-areas? are used
to represent triangular areas of automata A(P, k) that contain final positions. Vari-
able M is meant to abstract from concrete values of m, which differ for distinct P.
Symbolic base numbers are expressions of the form I, I + 1, I ? 1, I + 2, I ? 2 . . . (I-
areas) or M, M ? 1, M ? 2, . . . (M-areas). The elements of the symbolic areas, which
are called symbolic positions, are symbolic base numbers together with exponents
indicating an error count. Details should become clear in Example 2. The use of ex-
pressions such as (I + 2)2 simply enables a convenient labeling of states of A?(k)
(cf. Figure 6). Using this kind of labeling, it is easy to formulate a correspondence be-
tween derivations in automata A(P, k) and in A?(k) (cf. properties C1 and C2 discussed
below).
Example 2
The symbolic triangular areas for bound k = 1 are
(I-area) {I0, (I ? 1)1, I1, (I + 1)1}
(First M-area) {M0, (M ? 1)1, M1}
(Second M-area) {(M ? 1)0, (M ? 2)1, (M ? 1)1, M1}
461
Mihov and Schulz Fast Approximate Search in Large Dictionaries
M2
M0
M1
(M-1)0
(M-1)1
(M-1)2(M-2)2
(M-2)1
(M-2)0
(M-3)1
(M-3)2(M-4)2I2
I0
I1(I-1)1
(I-1)2(I-2)2
(I+1)1
(I+1)2 (I+2)2
Figure 5
Symbolic triangular areas and symbolic final positions for bound k = 2 (cf. Example 2).
Symbolic final positions for k = 1 are M1, M0, and (M ? 1)0. The symbolic triangular
areas for k = 2 are indicated in Figure 5, in which the ellipses around symbolic final
positions are rendered in boldface.
States of A?(k) are merely subsets of symbolic triangular areas. Subsets containing
symbolic final positions are final states of A?(k), and {I0} is the start state. A special
technique is used to reduce the number of states. Returning to automata of the form
A(P, k), it is simple to see that triangular areas often contain positions p = ge and
q = hf where p ?subsumes? q in the following sense: If, for some fixed input rest U, it
is possible to reach a final position of A(P, k) starting from q and consuming U, then
we may also reach a final position starting from p using U. A corresponding notion of
subsumption can be defined for symbolic positions. States of A?(k) are then defined
as subsets of symbolic triangular areas that are free of subsumption in the sense that a
symbolic position of a state is never subsumed by another position of the same state.
Example 3
The states of automaton A?(1) are shown in Figure 6. As a result of the above reduction
technique, the only state containing the symbolic position I0 is {I0}, the start state. Each
of the symbolic positions (I ? 1)1, I1, (I + 1)1 is subsumed by I0.
3. Transition function. It remains to define the transition function ?? of A?(k). We
describe only the basic idea. Imagine an automaton A(P, k), where the pattern P has
length m. Let W = w1 . . .wn denote an input word. Let SPi denote the set of active
positions of A(P, k) that are reached after reading the ith symbol wi (1 ? i ? n). For
simplicity, we assume that in each set, all subsumed positions are erased. In A?(k) we
have a parallel acceptance procedure in which we reach, say, state S?i after reading
?i := ?(wi, pi?k ? ? ? pi ? ? ? pr), where r = min{m, i + k + 1}, as above, for 1 ? i ? n.
Transitions are defined in such a way that C1 and C2 hold:
C1. For all parallel sets SPi and S
?
i of the two sequences
SP0 S
P
1 . . . S
P
i . . . S
P
n
S?0 S
?
1 . . . S
?
i . . . S
?
n
the set SPi is obtained from S
?
i by instantiating the letter I by i whenever
S?i uses variable I and instantiating M by m in the other cases.
C2. Whenever SPi contains a final position, then S
?
i is final.
Given properties C1 and C2, it follows immediately that A(P, k) accepts w1 . . .wn iff
A?(k) accepts ?1, . . . , ?n.
462
Computational Linguistics Volume 30, Number 4
I1,I+11
I-11,I+11
I0
I1
I-11,I1
M-21,M-11,M1
M-21,M1
M1
M-11,M1
M0
_1
I-11
I+11
I-11,I1,I+11
M-10
_1_
_1
111_
_11_
_00(_)
1_(_)(_)
11_(_)
_1_(_)
_ _1_
_01
1_1_
_1_ _
11
111
011
101
1_
_11
11
1_1
001
011_
_01_
110(_)
100(_)
010(_)
101_
_01
1
10(_)(_)
01
01_(_)
1_0(_)
_1
_10_
001_
0_1_
_01_
_ _1
0_1
_10
10
01
_1
_0
_0
_1
_
Figure 6
The universal deterministic Levenshtein automaton A?(1). See Example 4 for notation.
Example 4
The universal deterministic automaton A?(1) is shown in Figure 6. (Some redundant
transitions departing from nonfinal states S = {I0} and using vectors of length ? 3
have been omitted.) The symbol stands for either 1 or 0. Moreover, ?( ) is shorthand
for ? or ?. In order to illustrate the use of A?(1), consider the pattern P of the form
chold. Input child is translated into the sequence
?1 = ?(c, $cho) = 0100
?2 = ?(h, chol) = 0100
?3 = ?(i, hold) = 0000
?4 = ?(l, old) = 010
?5 = ?(d, ld) = 01
Starting from state {I0}, we successively reach {I0}, {I0}, {(I?1)1, I1}, {I1}, {M1}. Hence
child is accepted. In a similar way the input word cold is translated into the sequence
0100?0010?0010?001. Starting from {I0}, we successively reach {I0}, {(I?1)1, I1, (I+1)1},
{(I + 1)1}, {M1}. Hence cold is also accepted. Third, input hchold is translated into
the sequence 0010?1000?1000?100?10?1. We reach successively {(I ? 1)1, I1, (I + 1)1},
{(I ? 1)1}, {(I ? 1)1}, {(I ? 1)1}, {(I ? 1)1}, {M1}. Hence hchold is accepted as well.
For larger values of k, the number of states of A?(k) grows rapidly. A?(2) has 50
nonfinal states and 40 final states. The automaton A?(3) has 563 states. When we tried
463
Mihov and Schulz Fast Approximate Search in Large Dictionaries
to minimize the automata A?(1), A?(2), and A?(3), we found that these three automata
are already minimal. However, we do not have a general proof that our construction
always leads to minimal automata.
5. Approximate Search in Dictionaries Using Universal Levenshtein Automata
We now describe how to use the universal deterministic Levenshtein automaton A?(k)
for approximate search for a pattern in a dictionary.
5.1 Basic Correction Algorithm
Let D denote the background dictionary, and let P = p1 . . . pm denote a given pattern.
Recall that we want to compute for some fixed bound k the set of all entries W ? D
such that dL(P, W) ? k. We assume that D is implemented in the form of a determin-
istic finite-state automaton AD = ??, QD, qD0 , FD, ?D?, the dictionary automaton. Hence
L(AD) represents the set of all correct words.
Let A?(k) = ??, Q?, q?0 , F?, ??? denote the universal deterministic Levenshtein au-
tomaton for bound k. We assume that we can access, for each symbol ? ? ? and
each index 1 ? i ? m + k, the characteristic vector ?(?, pi?k ? ? ? pi ? ? ? pr), where r =
min{m, i+ k+ 1}, in constant time (cf. Section 4). We traverse the two automata A?(k)
and AD in parallel, using a standard backtracking procedure. At each step, a symbol
? read in AD representing the ith symbol of the current dictionary path is translated
into the bitvector ?(?, pi?k ? ? ? pi ? ? ? pr), r = min{m, i+ k+1}, which is used as input for
A?(k).
push (<0,?, qD0 , q
?
0>);
while not empty(stack) do begin
pop (<i, W, qD, q?>);
for ? in ? do begin
? := ?(?, pi?k ? ? ? pi ? ? ? pr);
qD1 := ?
D(qD,?);
q?1 := ??(q
?, ?);
if (qD1 <> NIL) and (q
?
1 <> NIL) then begin
W1 := concat(W,?);
push(<i + 1, W1, qD1 , q
?
1>);
if (qD1 ? FD) and (q?1 ? F?) then output(W1);
end;
end;
end;
Starting with the pair of initial states ?qD0 , q?0 ?, position i = 0, and the empty word
?, each step of the traversal adds a new symbol ? ? ? to the actual word W and
leads from a pair of states ?qD, q?? ? QD ? Q? to ??D(qD,?), ??(q?, ?)?. We proceed as
long as both components are distinct from the empty failure state,1 NIL. Whenever
a final state is reached in both automata, the actual word W is added to the output.
It is trivial to show that the list of all output words represents exactly the set of all
dictionary entries W such that dL(W, P) ? k.
The computational cost of the above algorithm is bounded by the size of the
dictionary automaton AD and depends on the bound k used. If k reaches the length of
1 A failure state is a state q whose language LA(q) is empty.
464
Computational Linguistics Volume 30, Number 4
the longest word in the dictionary, then in general (e.g., for the empty input word), the
algorithm will result in a complete traversal of AD. In practice, small bounds are used,
and only a small portion of AD will be visited. For bound 0, the algorithm validates
in time O(|P|) if the input pattern P is in the dictionary.
5.2 Evaluation Results for Basic Correction Algorithm
Experimental results were obtained using a Bulgarian lexicon (BL) with 965, 339 word
entries (average length 10.23 symbols), a German dictionary (GL) with 3, 871, 605
entries (dominated by compound nouns, average length 18.74 symbols), and a ?lex-
icon? (TL) containing 1, 200, 073 bibliographic titles from the Bavarian National
Library (average length 47.64 symbols). The German dictionary and the title
dictionary are nonpublic. They were provided to us by Franz Guenthner and
the Bavarian National Library, respectively, for the tests we conducted. The
following table summarizes the dictionary automaton statistics for the three
dictionaries:
BL GL TL
Number of words 956,339 3,871,605 1,200,073
Automaton states 39,339 4,068,189 29,103,779
Automaton transitions 102,585 6,954,377 30,252,173
Size (bytes) 1,191,548 90,206,665 475,615,320
The basic correction algorithm was implemented in C and tested on a 1.6 GHz
Pentium IV machine under Linux.
5.2.1 A Baseline. Before we present our evaluation results, we give a simplified base-
line. Let a garbled word W be given. In order to find all words from the dictionary
within Levenshtein distance k, we can use two simple methods:
1. For each dictionary word V, check whether dL(V, W) ? k.
2. For each string V such that dL(V, W) ? k, check whether V is in the
dictionary.
We consider input words W of length 10. Visiting a state in an automaton takes
about 0.1 ?s. Using Method 1, the time needed to check whether dL(V, W) ? k for a
dictionary word V using the universal Levenshtein automaton can be estimated as 1
?s (a crude approximation). When using Method 2, we need about 1 ?s for the dic-
tionary lookup of a word with 10 symbols. Assume that the alphabet has 30 symbols.
Given the input W, we have 639 strings within Levenshtein distance 1, about 400,000
strings within distance 2, and about 260,000,000 strings within distance 3. Assum-
ing that the dictionary has 1,000,000 words, we get the following table of correction
times:
Distance 1 Distance 2 Distance 3
Method 1 1,000 ms 1,000 ms 1,000 ms
Method 2 0.639 ms 400 ms 260,000 ms
5.2.2 Correction with BL. To test the basic correction algorithm with the Bulgarian
lexicon, we used a Bulgarian word list containing randomly introduced errors. In each
word, we introduced between zero and four randomly selected symbol substitutions,
465
Mihov and Schulz Fast Approximate Search in Large Dictionaries
insertions, or deletions. The number of test words created for each length is shown in
the following table:
Length 3 4 5 6 7 8
 words 3,563 11,066 27,196 53,763 90,202 128,620
Length 9 10 11 12 13 14
 words 155,888 163,318 148,879 117,783 81,481 50,291
Length 15 16 17 18 19 20
 words 28,481 15,079 8,048 4,350 2,526 1,422
Table 1 lists the results of the basic correction algorithm using BL and standard
Levenshtein distance with bounds k = 1, 2, 3. Column 1 shows the length of the input
words. Column 2 (CT1) describes the average time needed for the parallel traversal of
the dictionary automaton and the universal Levenshtein automaton using Levenshtein
distance 1. The time needed to output the correction candidates is always included;
hence the column represents the total correction time. Column 3 (NC1) shows the
average number of correction candidates (dictionary words within the given distance
bound) per input word. (For k = 1, there are cases in which this number is below 1.
This shows that for some of the test words, no candidates were returned: These words
were too seriously corrupted for correction suggestions to be found within the given
distance bound.) Similarly Columns 4 (CT2) and 6 (CT3) yield, respectively, the total
correction times per word (averages) for distance bounds 2 and 3, and Columns 5
(NC2) and 7 (NC3) yield, respectively, the average number of correction candidates
per word for distance bounds 2 and 3. Again, the time needed to output all corrections
is included.
5.2.3 Correction with GL. To test the correction times when using the German lexicon,
we again created a word list with randomly introduced errors. The number of test
words of each particular length is shown in the following table:
Length 1?14 15?24 25?34 35?44 45?54 55?64
 words 100,000 100,000 100,000 9,776 995 514
The average correction times and number of correction candidates for GL are sum-
marized in Table 2, which has the same arrangement of columns (with corresponding
interpretations) as Table 1.
5.2.4 Correction with TL. To test the correction times when using the title ?lexicon,?
we again created a word list with randomly introduced errors. The number of test
words of each length is presented in the following table:
Length 1?14 15?24 25?34 35?44 45?54 55?64
 words 91,767 244,449 215,094 163,425 121,665 80,765
Table 3 lists the results for correction with TL and standard Levenshtein distance
with bounds k = 1, 2, 3. The arrangement of columns is the same as for Table 1, with
corresponding interpretations.
5.2.5 Summary. For each of the three dictionaries, evaluation times strongly depend on
the tolerated number of edit operations. When fixing a distance bound, the length of
the input word does not have a significant influence. In many cases, correction works
faster for long input words, because the number of correction candidates decreases.
The large number of entries in GL leads to increased correction times.
466
Computational Linguistics Volume 30, Number 4
Table 1
Evaluation results for the basic correction algorithm, Bulgarian dictionary, standard
Levenshtein distance, and distance bounds k = 1, 2, 3. Times in milliseconds.
Length (CT1) (NC1) (CT2) (NC2) (CT3) (NC3)
3 0.107 12.03 0.974 285.4 4.589 2983.2
4 0.098 8.326 1.048 192.1 5.087 2426.6
5 0.085 5.187 1.086 105.0 5.424 1466.5
6 0.079 4.087 0.964 63.29 5.454 822.77
7 0.079 3.408 0.853 40.95 5.426 466.86
8 0.081 3.099 0.809 30.35 5.101 294.84
9 0.083 2.707 0.824 22.36 4.631 187.12
10 0.088 2.330 0.794 16.83 4.410 121.73
11 0.088 1.981 0.821 12.74 4.311 81.090
12 0.088 1.633 0.831 9.252 4.277 51.591
13 0.089 1.337 0.824 6.593 4.262 31.405
14 0.089 1.129 0.844 4.824 4.251 19.187
15 0.089 0.970 0.816 3.748 4.205 12.337
16 0.087 0.848 0.829 3.094 4.191 9.1752
17 0.086 0.880 0.805 2.970 4.138 8.1250
18 0.087 0.809 0.786 2.717 4.117 7.1701
19 0.087 0.810 0.792 2.646 4.078 6.6544
20 0.091 0.765 0.795 2.364 4.107 5.7686
Table 2
Evaluation results for the basic correction algorithm, German dictionary, standard Levenshtein
distance, and distance bounds k = 1, 2, 3. Times in milliseconds.
Length (CT1) (NC1) (CT2) (NC2) (CT3) (NC3)
1?14 0.225 0.201 4.140 0.686 23.59 2.345
15?24 0.170 0.605 3.210 1.407 19.66 3.824
25?34 0.249 0.492 4.334 0.938 24.58 1.558
35?44 0.264 0.449 4.316 0.781 24.06 1.187
45?54 0.241 0.518 3.577 0.969 20.18 1.563
55?64 0.233 0.444 3.463 0.644 19.03 0.737
Table 3
Evaluation results for the basic correction algorithm, title ?lexicon,? standard Levenshtein
distance, and distance bounds k = 1, 2, 3. Times in milliseconds.
Length (CT1) (NC1) (CT2) (NC2) (CT3) (NC3)
1?14 0.294 0.537 3.885 2.731 19.31 24.67
15?24 0.308 0.451 4.024 0.872 19.50 1.703
25?34 0.321 0.416 4.160 0.644 19.98 0.884
35?44 0.330 0.412 4.225 0.628 20.20 0.844
45?54 0.338 0.414 4.300 0.636 20.44 0.857
55?64 0.344 0.347 4.340 0.433 20.61 0.449
467
Mihov and Schulz Fast Approximate Search in Large Dictionaries
6. Using Backwards Dictionaries for Filtering
In the related area of pattern matching in strings, various filtering methods have been
introduced that help to find portions of a given text in which an approximate match of
a given pattern P is not possible. (See Navarro [2001] and Navarro and Raffinot [2002]
for surveys). In this section, we show how one general method of this form (Wu
and Manber 1992; Myers 1994; Baeza-Yates and Navarro 1999; Navarro and Baeza-
Yates 1999) can be adapted to approximate search in a dictionary, improving the basic
correction algorithm.
For approximate text search, the crucial observation is the following: If the Leven-
shtein distance between a pattern P and a portion of text T? does not exceed a given
bound k, and if we cut P into k + 1 disjoint pieces P1, . . . , Pk+1, then T? must contain
at least one piece. Hence the search in text T can be started with an exact multipat-
tern search for {P1, . . . , Pk+1}, which is much faster than approximate search for P.
When finding one of the pieces Pi in the text, the full pattern P is searched for (return-
ing now to approximate search) within a small neighborhood around the occurrence.
Generalizations of this idea rely on the following lemma (Myers 1994; Baeza-Yates and
Navarro 1999; Navarro and Raffinot 2002):
Lemma 1
Let T? match P with ? k errors. Let P be represented as the concatenation of j words
P1, . . . , Pj. Let a1, . . . , aj denote arbitrary integers, and define A =
?j
i=1 ai. Then, for
some i ? {1, . . . , j}, Pi matches a substring of T? with ? aik/A errors.2
In our experiments, which were limited to distance bounds k = 1, 2, 3, we used the
following three instances of the general idea. Let P denote an input pattern, and let W
denote an entry of the dictionary D. Assume we cut P into two pieces, representing it
in the form P = P1P2:
1. If dL(P, W) ? 3, then W can be represented in the form W = W1W2,
where we have the following mutually exclusive cases:
(a) dL(P1, W1) = 0 and dL(P2, W2) ? 3
(b) 1 ? dL(P1, W1) ? 3 and dL(P2, W2) = 0
(c) dL(P1, W1) = 1 and 1 ? dL(P2, W2) ? 2
(d) dL(P1, W1) = 2 and dL(P2, W2) = 1
2. If dL(P, W) ? 2, then W can be represented in the form W = W1W2,
where we have the following mutually exclusive cases:
(a) dL(P1, W1) = 0 and dL(P2, W2) ? 2
(b) dL(P2, W2) = 0 and 1 ? dL(P1, W1) ? 2
(c) dL(P1, W1) = 1 = dL(P2, W2)
3. If dL(P, W) ? 1, then W can be represented in the form W = W1W2,
where we have the following mutually exclusive cases:
(a) dL(P1, W1) = 0 and dL(P2, W2) ? 1
(b) dL(P1, W1) = 1 and dL(P2, W2) = 0
2 As usual, r denotes the largest integer ? r.
468
Computational Linguistics Volume 30, Number 4
In order to make use of these observations, we compute, given dictionary D, the back-
wards dictionary D?R := {W?R | W ? D}.3 Dictionary D and backwards dictionary
D?R are compiled into deterministic finite-state automata AD and AD?R , respectively. If
the dictionary is infinite and directly given as a finite-state automaton AD, the automa-
ton AD?R may be computed using standard techniques from formal language theory.
Further steps depend on the bound k. We will describe only approximate search with
bound k = 3; the methods for bounds k = 1, 2 are similar.
Let P denote the given pattern. P is cut into two pieces P1, P2 of approximately the
same length. We compute P?R2 and P
?R
1 . We then start four subsearches, corresponding
to Cases (1a)?(1d) specified above.
For subsearch (1a), we first traverse AD using input P1. Let q denote the state that is
reached. Starting from q and the initial state {I0} of A?(3), we continue with a parallel
traversal of AD and A?(3). Transition symbols in AD are translated into input bitvectors
for A?(3) by matching them against appropriate subwords of $$$P2, as described in
Section 5. The sequence of all transition labels of the actual paths in AD is stored as
usual. Whenever we reach a pair of final states, the current sequence?which includes
the prefix P1?is passed to the output. Clearly, each output sequence has the form
P1P?2, where dL(P2, P
?
2) ? 3. Conversely, any dictionary word of this form is found
using subsearch (1a).
For subsearch (1b), we first traverse AD?R using P
?R
2 . Let q denote the state that
is reached. Starting from q and the initial state {I0} of A?(3), we continue with a
parallel traversal of AD?R and A?(3). Transition symbols in AD?R are translated into
input bitvectors for A?(3) by matching them against appropriate subwords of $$$P?R1 .
Whenever we reach a pair of final states, the inversed sequence is passed to the output.
Clearly, each output sequence has the form P?1P2, where dL(P1, P
?
1) ? 3. Conversely, any
dictionary word of this form is found using a search of this form. For a given output,
a closer look at the final state S that is reached in A?(3) may be used to exclude cases
in which P1 = P?1. (Simple details are omitted).
For subsearch (1c), we start with a parallel traversal of AD and A?(1). Transition
symbols in AD are translated into input bitvectors for A?(1) by matching them against
appropriate subwords of $P1. For each pair of states (q, S) that are reached, where S
represents a final state of A?(1), we start a parallel traversal of AD and A?(2), departing
from q and the initial state {I0} of A?(2). Transition symbols in AD are translated
into input bitvectors for A?(2) by matching them against appropriate subwords of
$$P2. Whenever we reach a pair of final states, the current sequence is passed to the
output. Clearly, each output sequence has the form P?1P
?
2, where dL(P1, P
?
1) ? 1 and
dL(P2, P?2) ? 2. Conversely, any dictionary word of this form is found using a search
of this form. A closer look at the final states that are respectively reached in A?(1)
and A?(2) may be used to exclude cases in which P1 = P?1 or P2 = P
?
2. (Again, simple
details are omitted).
For subsearch (1d), we start with a parallel traversal of AD?R and A?(1). Tran-
sition symbols in AD?R are translated into input bitvectors for A?(1) by matching
them against appropriate subwords of $P?R2 . For each pair of states (q, S) that are
reached, where S represents a final state of A?(1), we start a parallel traversal of AD?R
and A?(2), departing from q and the initial state {I0} of A?(2). Transition symbols
in AD?R are translated into input bitvectors for A?(2) by matching them against ap-
propriate subwords of $$P?R1 . Whenever we reach a pair of final states, the inversed
sequence is passed to the output. Clearly, each output sequence has the form P?1P
?
2,
3 W?R denotes the reverse of W.
469
Mihov and Schulz Fast Approximate Search in Large Dictionaries
where dL(P1, P?1) ? 2 and dL(P2, P?2) ? 1. Conversely, any word in the dictionary of
this form is found using a search of this form. A closer look at the final states that
are reached in A?(1) and A?(2) may be used to exclude cases where P2 = P?2 or
dL(P1, P?1) ? 1, respectively. (Again, simple details are omitted).
It should be noted that the output sets obtained from the four subsearches (1a)?
(1d) are not necessarily disjoint, because a dictionary entry W may have more than
one partition W = W1W2 of the form described in cases (1a)?(1d).
6.1 Evaluation Results
The following table summarizes the statistics of the automata for the three backwards
dictionaries:
BL GL TL
Number of words 956,339 3,871,605 1,200,073
Automaton states 54,125 4,006,357 29,121,084
Automaton transitions 183,956 7,351,973 30,287,053
Size (bytes) 2,073,739 92,922,493 475,831,001
Note that the size of the backwards-dictionary automata is approximately the same
as the size of the dictionary automata.
Tables 4, 5, and 6 present the evaluation results for the backwards dictionary
filtering method using dictionaries BL, GL, and TL, respectively. We have constructed
additional automata for the backwards dictionaries.
For the tests, we used the same lists of input words as in Section 5.2 in order
to allow a direct comparison to the basic correction method. Dashes indicate that the
correction times were too small to be measured with sufficient confidence in their level
of precision. In columns 3, 5, and 7, we quantify the speedup factor, that is, the ratio
of the time taken by the basic algorithm to that taken by the backwards-dictionary
filtering method.
6.2 Backwards-Dictionary Method for Levenshtein Distance with Transpositions
Universal Levenshtein automata can also be constructed for the modified Levenshtein
distance, in which character transpositions count as a primitive edit operation, along
with insertions, deletions, and substitutions. This kind of distance is preferable when
correcting typing errors. A generalization of the techniques presented by the authors
(Schulz and Mihov 2002) for modified Levenshtein distances?using either transpo-
sitions or merges and splits as additional edit operations?has been described in
Schulz and Mihov (2001). It is assumed that all edit operations are applied in par-
allel, which implies, for example, that insertions between transposed letters are not
possible.
If we want to apply the filtering method using backwards dictionaries for the
modified Levenshtein distance d?L(P, W) with transpositions, we are faced with the
following problem: Assume that the pattern P = a1a2 . . . amam+1 . . . an is split into P1 =
a1a2 . . . am and P2 = am+1am+2 . . . an. When we apply the above procedure, the case in
which am and am+1 are transposed is not covered. In order to overcome this problem,
we can draw on the following observation:
If d?L(P, W) ? 3, then W can be represented in the form W = W1W2, where there
are seven alternatives, inlcuding the following four:
1. d?L(P1, W1) = 0 and d
?
L(P2, W2) ? 3
2. 1 ? d?L(P1, W1) ? 3 and d?L(P2, W2) = 0
470
Computational Linguistics Volume 30, Number 4
Table 4
Evaluation results using the backwards-dictionary filtering method, Bulgarian dictionary, and
distance bounds k = 1, 2, 3. Times in milliseconds and speedup factors (ratio of times) with
respect to basic algorithm.
Length (CT1) Speedup 1 (CT2) Speedup 2 (CT3) Speedup 3
3 0.031 3.45 0.876 1.11 6.466 0.71
4 0.027 3.63 0.477 2.20 4.398 1.16
5 0.018 4.72 0.450 2.41 2.629 2.06
6 0.016 4.94 0.269 3.58 2.058 2.65
7 0.011 7.18 0.251 3.40 1.327 4.09
8 0.012 6.75 0.196 4.13 1.239 4.12
9 0.009 9.22 0.177 4.66 0.828 5.59
10 0.010 8.80 0.159 4.99 0.827 5.33
11 0.008 11.0 0.147 5.59 0.603 7.15
12 0.008 11.0 0.142 5.85 0.658 6.50
13 0.006 14.8 0.128 6.44 0.457 9.33
14 0.006 14.8 0.123 6.86 0.458 9.28
15 0.005 17.8 0.112 7.29 0.321 13.1
16 0.005 17.4 0.111 7.47 0.320 13.1
17 0.005 17.2 0.108 7.45 0.283 14.6
18 0.005 17.4 0.108 7.28 0.280 14.7
19 0.004 21.8 0.103 7.69 0.269 15.2
20 ? ? 0.105 7.57 0.274 15.0
Table 5
Evaluation results using the backwards-dictionary filtering method, German dictionary, and
distance bounds k = 1, 2, 3. Times in milliseconds and speedup factors (ratio of times) with
respect to basic algorithm.
Length (CT1) Speedup 1 (CT2) Speedup 2 (CT3) Speedup 3
1?14 0.007 32.1 0.220 18.8 0.665 35.5
15?24 0.010 17.0 0.175 18.3 0.601 32.7
25?34 0.009 27.7 0.221 19.6 0.657 37.4
35?44 0.007 37.7 0.220 19.6 0.590 40.8
45?54 ? ? 0.201 17.8 0.452 44.6
55?64 ? ? 0.195 17.8 0.390 48.8
3. d?L(P1, W1) = 1 and 1 ? d?L(P2, W2) ? 2
4. d?L(P1, W1) = 2 and d
?
L(P2, W2) = 1
In the remaining three alternatives, W1 = W?1am+1 ends with the symbol am+1, and
W2 = amW?2 starts with am. For P
?
1 := a1a2 . . . am?1 and P
?
2 := am+2am+3 . . . an, we have
the following three alternatives:
5. d?L(P
?
1, W
?
1) = 0 and dL(P
?
2, W
?
2) ? 2
6. d?L(P
?
1, W
?
1) = 1 and d
?
L(P
?
2, W
?
2) ? 1
7. d?L(P
?
1, W
?
1) = 2 and d
?
L(P
?
2, W
?
2) = 0
The cases for distance bounds k = 1, 2 are solved using similar extensions of the
original subcase analysis. In each case, it is straightforward to realize a search proce-
dure with subsearches corresponding to the new subcase analysis, using an ordinary
dictionary and a backwards dictionary, generalizing the above ideas.
471
Mihov and Schulz Fast Approximate Search in Large Dictionaries
Table 6
Evaluation results using the backwards-dictionary filtering method, title ?lexicon,? and
distance bounds k = 1, 2, 3. Times in milliseconds and speedup factors (ratio of times) with
respect to basic algorithm.
Length (CT1) Speedup 1 (CT2) Speedup 2 (CT3) Speedup 3
1?14 0.032 9.19 0.391 9.94 1.543 12.5
15?24 0.019 16.2 0.247 16.3 0.636 30.7
25?34 0.028 11.5 0.260 16.0 0.660 30.3
35?44 0.029 11.4 0.295 14.3 0.704 28.7
45?54 0.037 9.14 0.332 13.0 0.759 26.9
55?64 0.038 9.05 0.343 12.7 0.814 25.3
Table 7
Evaluation results using the backwards-dictionary filtering method for the modified
Levenshtein distance d?L with transpositions, for German dictionary and title ?lexicon,?
distance bound k = 3. Times in milliseconds and speedup factors (ratio of times) with respect
to basic algorithm.
Length (CT3 GL) Speedup GL (CT3 TL) Speedup TL
1?14 1.154 23.0 2.822 7.7
15?24 1.021 21.3 1.235 17.5
25?34 1.148 23.7 1.261 17.7
35?44 1.096 24.3 1.283 17.6
45?54 0.874 25.5 1.326 17.3
55?64 0.817 25.7 1.332 17.4
We have tested the new search procedure for the modified Levenshtein distance
d?L. In Table 7 we present the experimental results with the German dictionary and the
title ?lexicon? for distance bound k = 3.
6.2.1 Summary. The filtering method using backwards dictionaries drastically im-
proves correction times. The increase in speed depends both on the length of the
input word and on the error bound. The method works particularly well for long
input words. For GL, a drastic improvement can be observed for all subclasses. In
contrast, for very short words of BL, only a modest improvement is obtained. When
using BL and the modified Levenshtein distance d?L with transpositions, the backwards-
dictionary method improved the basic search method only for words of length ? 9.
For short words, a large number of repetitions of the same correction candidates was
observed. The analysis of this problem is a point of future work.
Variants of the backwards-dictionary method also can be used for the Levenshtein
distance d??L , in which insertions, deletions, substitutions, merges, and splits are treated
as primitive edit operations. Here, the idea is to split the pattern at two neighboring
positions, which doubles the number of subsearches. We did not evaluate this variant.
7. Using Dictionaries with Single Deletions for Filtering
The final technique that we describe here is again an adaptation of a filtering method
from pattern matching in strings (Muth and Manber 1996; Navarro and Raffinot 2002).
When restricted to the error bound k = 1, this method is very efficient. It can be used
only for finite dictionaries. Assume that the pattern P = p1 . . . pm matches a portion of
text, T?, with one error. Then m ? 1 letters of P are found in T? in the correct order.
472
Computational Linguistics Volume 30, Number 4
This fact can be used to compute m+1 derivatives of P that are compared with similar
derivatives of a window T? of length m that is slid over the text. A derivative of a word
V can be V or a word that is obtained by deleting exactly one letter of V. Coincidence
between derivatives of P and T? can be used to detect approximate matches of P of
the above form. For details we refer to Navarro and Raffinot (2002). In what follows
we describe an adaptation of the method to approximate search of a pattern P in a
dictionary D.
Let i be an integer. With V[i] we denote the word that is obtained from a word
V by deleting the ith symbol of V. For |V| < i, we define V[i] = V. By a dictionary
with output sets, we mean a list of strings W, each of which is associated with a set
of output strings O(W). Each string W is called a key. Starting from the conventional
dictionary D, we compute the following dictionaries with output sets Dall, D1, D2, . . .,
Dn0 , where n0 is the maximal length of an entry in D:
? The set of keys of Dall is D ? {V | ?i ? 1, W ? D such that V = W[i]}. The
output set for key V is
Oall(V) := {W ? D | W = V ? V = W[i] for some i ? 1}.
? The set of keys for Di is D ? {V | ?W ? D such that V = W[i]}. The
output set for a key V is Oi(V) := {W ? D | W = V ? V = W[i]}.
Lemma 2
Let P denote a pattern, and let W ? D. Then dL(P, W) ? 1 iff either W ? Oall(P) or
there exists i, 1 ? i ? |P|, such that W ? Oi(P[i]).
The proof of the lemma is simple and has therefore been omitted.
In our approach, the dictionaries with output sets Dall, D1, D2, . . ., Dn0 are com-
piled, respectively, into minimal subsequential transducers Aall, A1, A2, . . . , An0 . Given a
pattern P, we compute the union of the output sets Oall(P),O1(P[1]),. . .,O|P|(P[|P|]) us-
ing these transducers. It follows from Lemma 2 that we obtain as result the set of all
entries W of D such that dL(P, W) ? 1. It should be noted that the output sets are not
necessarily disjoint. For example, if P itself is a dictionary entry, then P ? Oi(P[i]) for
all 1 ? i ? |P|.
After we implemented the above procedure for approximate search, we found that
a similar approach based on hashing had been described as early as 1981 in a technical
report by Mor and Fraenkel (1981).
7.1 Evaluation Results
Table 8 presents the evaluation results for edit distance 1 using dictionaries with single
deletions obtained from BL. The total size of the constructed single-deletion dictionary
automata is 34.691 megabytes. The word lists used for tests are those described in
Section 5.2. GL and TL are not considered here, since the complete system of subdic-
tionaries needed turned out to be too large. For a small range of input words of length
3?6, filtering using dictionaries with single deletions behaves better than filtering using
the backwards-dictionary method.
8. Similarity Keys
A well-known technique for improving lexical search not mentioned so far is the use
of similarity keys. A similarity key is a mapping ? that assigns to each word W a sim-
plified representation ?(W). Similarity keys are used to group dictionaries into classes
473
Mihov and Schulz Fast Approximate Search in Large Dictionaries
of ?similar? entries. Many concrete notions of ?similarity? have been considered, de-
pending on the application domain. Examples are phonetic similarity (e.g., SOUNDEX
system; cf. Odell and Russell [1918, 1922] and Davidson [1962]), similarity in terms of
word shape and geometric form (e.g., ?envelope representation? [Sinha 1990; Anig-
bogu and Belaid 1995] ) or similarity under n-gram analysis (Angell, Freund, and
Willett 1983; Owolabi and McGregor 1988). In order to search for a pattern P in the
dictionary, the ?code? ?(P) is computed. The dictionary is organized in such a way
that we may efficiently retrieve all regions containing entries with code (similar to)
?(P). As a result, only small parts of the dictionary must be visited, which speeds up
search. Many variants of this basic idea have been discussed in the literature (Kukich
1992; Zobel and Dart 1995; de Bertrand de Beuvron and Trigano 1995).
In our own experiments we first considered the following simple idea. Given a
similarity key ?, each entry W of dictionary D is equipped with an additional pre-
fix of the form ?(W)&. Here & is a special symbol that marks the border between
codes and original words. The enhanced dictionary D? with all entries of the form
?(W)&W is compiled into a deterministic finite-state automaton AD?. Approximate
search for pattern P in D is then reorganized in the following way. The enhanced
pattern ?(P)&P is used for search in AD?. We distinguish two phases in the backtrack-
ing process. In Phase 1, which ends when the special symbol & is read, we compute
an initial path of AD? in which the corresponding sequence of transition labels rep-
resents a code ? such that dL(?(P),?) ? k. All paths of this form are visited. Each
label sequence ?& of the above form defines a unique state q of the automaton AD?
such that LAD?(q) = {W ? D | ?(W) = ?}. In Phase 2, starting from q, we compute all
entries W with code ?(W) = ? such that dL(W, P) ? k. In both phases the automa-
ton A?(k) is used to control the search, and transition labels of AD? are translated into
characteristic vectors. In order to guarantee completeness of the method, the distance
between codes of a pair of words should not exceed the distance between the words
themselves.
It is simple to see that in this method, the backtracking search is automatically
restricted to the subset of all dictionary entries V such that dL(?(V),?(P)) ? k. Unfor-
tunately, despite this, the approach does not lead to reduced search times. A closer
look at the structure of (conventional) dictionary automata AD for large dictionaries D
shows that there exists an enormous number of distinct initial paths of AD of length
3?5. During the controlled traversal of AD, most of the search time is spent visiting
paths of this initial ?wall.? Clearly, most of these paths do not lead to any correction
candidate. Unfortunately, however, these ?blind? paths are recognized too late. Using
the basic method described in Section 5, we have to overcome one single wall in AD for
the whole dictionary. In contrast, when integrating similarity keys in the above form,
we have to traverse a similar wall for the subdictionary D?(W) := {V ? D | ?(V) =
?(W)} for each code ?(W) found in Phase 1. Even if the sets D?(W) are usually much
smaller than D, the larger number of walls that are visited leads to increased traversal
times.
As an alternative, we tested a method in which we attached to each entry W of D all
prefixes of the form ?&, where ? represents a possible code such that dL(?(W),?) ? k.
Using a procedure similar to the one described above, we have to traverse only one
wall in Phase 2. With this method, we obtained a reduction in search time. However,
with this approach, enhanced dictionaries D? are typically much larger than original
dictionaries D. Hence the method can be used only if both dictionary D and bound
k are not too large and if the key is not too fine. Since the method is not more effi-
cient than filtering using backwards dictionaries, evaluation results are not presented
here.
474
Computational Linguistics Volume 30, Number 4
Table 8
Results for BL, using dictionaries with single deletions for filtering, distance bound k = 1.
Times in milliseconds and speedup factors (ratio of times) with respect to basic algorithm.
Length (CT1) Speedup 1 Length (CT1) Speedup 1
3 0.011 9.73 12 0.026 3.38
4 0.011 8.91 13 0.028 3.18
5 0.010 8.50 14 0.033 2.70
6 0.011 7.18 15 0.035 2.54
7 0.013 6.08 16 0.039 2.23
8 0.015 5.40 17 0.044 1.95
9 0.017 4.88 18 0.052 1.67
10 0.020 4.40 19 0.055 1.58
11 0.022 4.00 20 0.063 1.44
9. Concluding Remarks
In this article, we have shown how filtering methods can be used to improve finite-
state techniques for approximate search in large dictionaries. As a central contribution
we introduced a new correction method, filtering based on backwards dictionaries
and partitioned input patterns. Though this method generally leads to very short
correction times, we believe that correction times could possibly be improved further
using refinements and variants of the method or introducing other filtering methods.
There are, however, reasons to assume that we are not too far from a situation in
which further algorithmic improvements become impossible for fundamental reasons.
The following considerations show how an ?optimal? correction time can be estimated
that cannot be improved upon without altering the hardware or using faster access
methods for automata.
We used a simple backtracking procedure to realize a complete traversal of the
dictionary automaton AD. During a first traversal, we counted the total number of
visits to any state. Since AD is not a tree, states may be passed through several times
during the complete traversal. Each such event counts as one visit to a state. The ratio
of the number of visits to the total number of symbols in the list of words D gives the
average number of visits per symbol, denoted v0. In practice, the value of v0 depends
on the compression rate that is achieved when compiling D into the automaton AD. It is
smaller than 1 because of numerous prefixes of dictionary words that are shared in AD.
We then used a second traversal of AD?not counting visits to states?to compute the
total traversal time. The ratio of the total traversal time to the number of visits yields
the time t0 that is needed for a single visit. For the three dictionaries, the following
values were obtained:
BL GL TL
Average number v0 of visits per symbol 0.1433 0.3618 0.7335
Average time t0 for one visit (in ?s) 0.0918 0.1078 0.0865
Given an input V, we may consider the total number nV of symbols in the list of
correction candidates. Then nV ?v0?t0 can be used to estimate the optimal correction time
for V. In fact, in order to achieve this correction time, we need an oracle that knows
how to avoid any kind of useless backtracking. Each situation in which we proceeded
on a dictionary path that does not lead to a correction candidate for V would require
some extra time that is not included in the above calculation. From another point of
view, the above idealized algorithm essentially just copies the correction candidates
into a resulting destination. The time that is consumed is proportional to the sum of
the length of the correction candidates.
475
Mihov and Schulz Fast Approximate Search in Large Dictionaries
For each of the three dictionaries, we estimated the optimal correction time for
one class of input words. For BL we looked at input words of length 10. The average
number of correction candidates for Levenshtein distance 3 is 121.73 (cf. Table 1).
Assuming that the average length of correction candidates is 10, we obtain a total of
1, 217.3 symbols in the complete set of all correction candidates. Hence the optimal
correction time is approximately
1217.3 ? 0.0000918 ms ? 0.1433 = 0.016 ms
The actual correction time using filtering with the backwards-dictionary method is
0.827 ms, which is 52 times slower.
For GL, we considered input words of length 15?24 and distance bound 3. We have
on average 3.824 correction candidates of length 20, that is, 76.48 symbols. Hence the
optimal correction time is approximately
76.48 ? 0.0001078 ms ? 0.3618 = 0.003 ms
The actual correction time using filtering with the backwards-dictionary method is
0.601 ms, which is 200 times slower.
For TL, we used input sequences of length 45?54 and again distance bound 3. We
have on average 0.857 correction candidates of length 50, that is, 42.85 symbols. Hence
the optimal correction time is approximately
42.85 ? 0.0000865 ms ? 0.7335 = 0.003 ms
The actual correction time using filtering with the backwards-dictionary method is
0.759 ms, which is 253 times slower.
These numbers coincide with our basic intuition that further algorithmic improve-
ments are simpler for dictionaries with long entries. For example, variants of the
backwards-dictionary method could be considered in which a finer subcase analysis
is used to improve filtering.
Acknowledgments
This work was funded by a grant from
VolkswagenStiftung. The authors thank the
anonymous referees for many suggestions
that helped to improve the presentation.
References
Angell, Richard C., George E. Freund, and
Peter Willett. 1983. Automatic spelling
correction using a trigram similarity
measure. Information Processing and
Management, 19:255?261.
Anigbogu, Julain C. and Abdel Belaid. 1995.
Hidden Markov models in text
recognition. International Journal of Pattern
Recognition and Artificial Intelligence,
9(6):925?958.
Baeza-Yates, Ricardo A. and Gonzalo
Navarro. 1998. Fast approximative string
matching in a dictionary. In R. Werner,
editor, Proceedings SPIRE?98, pages 14?22.
IEEE Computer Science.
Baeza-Yates, Ricardo A. and Gonzalo
Navarro. 1999. Faster approximate string
matching. Algorithmica, 23(2):127?158.
Blair, Charles R. 1960. A program for
correcting spelling errors. Information and
Control, 3:60?67.
Bunke, Horst. 1993. A fast algorithm for
finding the nearest neighbor of a word in
a dictionary. In Proceedings of the Second
International Conference on Document
Analysis and Recognition (ICDAR?93), pages
632?637, Tsukuba, Japan.
Daciuk, Jan, Stoyan Mihov, Bruce W.
Watson, and Richard E. Watson. 2000.
Incremental construction of minimal
acyclic finite state automata.
Computational Linguistics, 26(1):3?16.
Davidson, Leon. 1962. Retrieval of
misspelled names in an airline passenger
record system. Communications of the ACM,
5(3):169?171.
476
Computational Linguistics Volume 30, Number 4
de Bertrand de Beuvron, Francois and
Philippe Trigano. 1995. Hierarchically
coded lexicon with variants. International
Journal of Pattern Recognition and Artificial
Intelligence, 9(1):145?165.
Dengel, Andreas, Rainer Hoch, Frank
Ho?nes, Thorsten Ja?ger, Michael Malburg,
and Achim Weigel. 1997. Techniques for
improving OCR results. In Horst Bunke
and Patrick S. P. Wang, editors, Handbook
of Character Recognition and Document Image
Analysis, 227?258. World Scientific.
Hopcroft, John E. and Jeffrey D. Ullman.
1979. Introduction to Automata Theory,
Languages, and Computation.
Addison-Wesley, Reading, MA.
Kim, Jong Yong and John Shawe-Taylor.
1992. An approximate string-matching
algorithm. Theoretical Computer Science,
92:107?117.
Kim, Jong Yong and John Shawe-Taylor.
1994. Fast string matching using an
n-gram algorithm. Software?Practice and
Experience, 94(1):79?88.
Kozen, Dexter C. 1997. Automata and
Computability. Springer.
Kukich, Karen. 1992. Techniques for
automatically correcting words in texts.
ACM Computing Surveys, 24:377?439.
Levenshtein, Vladimir I. 1966. Binary codes
capable of correcting deletions, insertions,
and reversals. Soviet Physics-Doklady,
10:707?710.
Mihov, Stoyan and Denis Maurel. 2001.
Direct construction of minimal acyclic
subsequential transducers. In S. Yu and
A. Pun, editors, Implementation and
Application of Automata: Fifth International
Conference (CIAA?2000) (Lecture Notes in
Computer Science no. 2088), pages
217?229. Springer.
Mor, Moshe and Aviezri S. Fraenkel. 1981.
A hash code method for detecting and
correcting spelling errors. Technical
Report CS81-03, Department of Applied
Mathematics, Weizmann Institute of
Science, Rehovot, Israel.
Muth, Robert and Udi Manber. 1996.
Approximate multiple string search. In
Proceedings of the Seventh Annual
Symposium on Combinatorical Pattern
Matching (Lecture Notes in Computer
Science, no. 1075) pages 75?86. Springer.
Myers, Eugene W. 1994. A sublinear
algorithm for approximate keyword
searching. Algorithmica, 12(4/5):345?374.
Navarro, Gonzalo. 2001. A guided tour to
approximate string matching. ACM
Computing Surveys, 33(1):31?88.
Navarro, Gonzalo and Ricardo A.
Baeza-Yates. 1999. Very fast and simple
approximate string matching. Information
Processing Letters, 72:65?70.
Navarro, Gonzalo and Mathieu Raffinot.
2002. Flexible Pattern Matching in Strings.
Cambridge University Press,
Cambridge.
Odell, Margaret K. and Robert C. Russell.
1918. U.S. Patent Number 1, 261, 167. U.S.
Patent Office, Washington, DC.
Odell, Margaret K. and Robert C. Russell.
1992. U.S. Patent Number 1,435,663. U.S.
Patent Office, Washington, DC.
Oflazer, Kemal. 1996. Error-tolerant
finite-state recognition with applications
to morphological analysis and spelling
correction. Computational Linguistics,
22(1):73?89.
Oommen, B. John and Richard K. S. Loke.
1997. Pattern recognition of strings with
substitutions, insertions, deletions, and
generalized transpositions. Pattern
Recognition, 30(5):789?800.
Owolabi, Olumide and D. R. McGregor.
1988. Fast approximate string matching.
Software?Practice and Experience,
18(4):387?393.
Riseman, Edward M. and Roger W. Ehrich.
1971. Contextual word recognition using
binary digrams. IEEE Transactions on
Computers, C-20(4):397?403.
Schulz, Klaus U. and Stoyan Mihov. 2001.
Fast string correction with
Levenshtein-automata. Technical Report
01-127, Centrum fu?r Informations= und
sprachverarbeitung, University of
Munich.
Schulz, Klaus U. and Stoyan Mihov. 2002.
Fast string correction with
Levenshtein-automata. International
Journal of Document Analysis and
Recognition, 5(1):67?85.
Seni, Giovanni, V. Kripasundar, and
Rohini K. Srihari. 1996. Generalizing edit
distance to incorporate domain
information: Handwritten text recognition
as a case study. Pattern Recognition,
29(3):405?414.
Sinha, R. M. K. 1990. On partitioning a
dictionary for visual text recognition.
Pattern Recognition, 23(5):497?500.
Srihari, Sargur N. 1985. Computer Text
Recognition and Error Correction. Tutorial.
IEEE Computer Society Press, Silver
Spring, MD.
Srihari, Sargur N., Jonathan J. Hull, and
Ramesh Choudhari. 1983. Integrating
diverse knowledge sources in text
recognition. ACM Transactions on Office
Information Systems, 1(1):68?87.
Stephen, Graham A. 1994. String Searching
Algorithms. World Scientific, Singapore.
477
Mihov and Schulz Fast Approximate Search in Large Dictionaries
Takahashi, Hiroyasu, Nobuyasu Itoh, Tomio
Amano, and Akio Yamashita. 1990. A
spelling correction method and its
application to an OCR system. Pattern
Recognition, 23(3/4):363?377.
Ukkonen, Esko. 1985a. Algorithms for
approximate string matching. Information
and Control, 64:100?118.
Ukkonen, Esko. 1985b. Finding approximate
patterns in strings. Journal of Algorithms,
6(1?3):132?137.
Ukkonen, Esko. 1992. Approximate
string-matching with q-grams and
maximal matches. Theoretical Computer
Science, 92:191?211.
Ullman, Jeffrey R. 1977. A binary n-gram
technique for automatic correction of
substitution, deletion, insertion and
reversal errors. Computer Journal,
20(2):141?147.
Wagner, Robert A. and Michael J. Fischer.
1974. The string-to-string correction
problem. Journal of the ACM, 21(1):168?173.
Weigel, Achim, Stephan Baumann, and
J. Rohrschneider. 1995. Lexical
postprocessing by heuristic search and
automatic determination of the edit costs.
In Proceedings of the Third International
Conference on Document Analysis and
Recognition (ICDAR 95), pages 857?860.
Wells, C. J., L. J. Evett, Paul E. Whitby, and
R.-J. Withrow. 1990. Fast dictionary
look-up for contextual word recognition.
Pattern Recognition, 23(5):501?508.
Wu, Sun and Udi Manber. 1992. Fast text
searching allowing errors. Communications
of the ACM, 35(10):83?91.
Zobel, Justin and Philip Dart. 1995. Finding
approximate matches in large lexicons.
Software?Practice and Experience,
25(3):331?345.
Orthographic Errors in Web Pages:
Toward Cleaner Web Corpora
Christoph Ringlstetter?
Klaus U. Schulz?
CIS, University of Munich
Stoyan Mihov?
Bulgarian Academy of Science, Sofia
Since the Web by far represents the largest public repository of natural language texts, recent
experiments, methods, and tools in the area of corpus linguistics often use the Web as a corpus.
For applications where high accuracy is crucial, the problem has to be faced that a non-negligible
number of orthographic and grammatical errors occur in Web documents. In this article we in-
vestigate the distribution of orthographic errors of various types in Web pages. As a by-product,
methods are developed for efficiently detecting erroneous pages and for marking orthographic
errors in acceptable Web documents, reducing thus the number of errors in corpora and linguistic
knowledge bases automatically retrieved from the Web.
1. Introduction
The automated analysis of large corpora has many useful applications (Church and
Mercer 1993). Suitable language repositories can be used for deriving models of a
given natural language, as needed for speech recognition (Ostendorf, Digalakis,
and Kimball 1996; Jelinek 1997; Chelba and Jelinek 2002), language generation (Oh
and Rudickny 2000), and text correction (Kukich 1992; Amengual and Vidal 1998;
Strohmaier et al 2003b). Other corpus-based methods determine associations between
words (Grefenstette 1992; Dunning 1993; Lin et al 1998), which yields a basis for com-
puting thesauri, or dictionaries of terminological expressions and multiword lexemes
(Gaizauskas, Demetriou, and Humphreys 2000; Grefenstette 2001).
From multilingual texts, translation lexica can be generated (Gale and Church
1991; Kupiec 1993; Kumano and Hirakawa 1994; Boutsis, Piperidis, and Demiros 1999;
Grefenstette 1999). The analysis of technical texts is used to automatically build dictio-
naries of acronyms for a given field (Taghva and Gilbreth 1999; Yeates, Bainbridge, and
Witten 2000), and related methods help to compute dictionaries that cover the special
vocabulary of a given thematic area (Strohmaier et al 2003a). In computer-assisted lan-
guage learning (CALL), mining techniques for corpora are used to create individualized
and user-centric exercises for grammar and text understanding (Schwartz, Aikawa, and
Pahud 2004; Brown and Eskenazi 2004; Fletcher 2004a).
By Zipf?s law, most words, phrases, and specific grammatical constructions have
a very low frequency. Furthermore, the number of text genres and special thematic
? Funded by German Research Foundation (DFG)
? Funded by VolkswagenStiftung
Submission received: 21 January 2005; revised submission received: 3 August 2005; accepted for
publication: 10 December 2005.
? 2006 Association for Computational Linguistics
Computational Linguistics Volume 32, Number 3
areas that come with their own picture of language is large. This explains that most
of the aforementioned applications can only work when built on top of huge heteroge-
neous corpora. Since the Web represents by far the largest public repository for natural
language texts, and since Web search engines such as Google offer simple access to
pages where language material of a given orthographic, grammatical, or thematic kind
is found, many recent experiments and technologies use the Web as a corpus (Kehoe and
Renouf 2002; Morley, Renouf, and Kehoe 2003; Kilgarriff and Grefenstette 2003; Resnik
and Smith 2003; Way and Gough 2003; Fletcher 2004b).
One potential problem for Web-based corpus linguistics is caused by the fact that
words and phrases occurring in Web pages are sometimes erroneous. Typing errors
represent one widespread phenomenon. Many Web pages, say, in English, are written
by non-native speakers, or by persons with very modest language competence. As a
consequence, spelling errors and grammatical bugs result. The character sets that are
used for writing Web pages are often not fully adequate for the alphabet of a given
language, which represents another systematic source for inaccuracies. Furthermore, a
small number of texts found in the Web is obtained via optical character recognition
(OCR), which may again lead to garbled words. As a consequence of these and other
error sources, the Web contains a considerable number of ?bad? pages with language
material that is inappropriate for corpus construction.
In one way or the other, all the aforementioned applications are affected by these
inadequacies. While the problem is probably not too serious for approaches that merely
collect statistical information about given language items, the construction of dictio-
naries and related linguistic knowledge bases?which are, after all, meant to be used
in different scenarios of automated language processing?becomes problematic if too
many erroneous entries are retrieved from Web pages. Obviously, in computer-assisted
language learning it is a principal concern that words and phrases from the Web that
are presented to the user are error free.
In discussions we found that problems resulting from erroneous language material
in Web pages for distinct applications are broadly acknowledged (see also Section 4.4 of
Kilgarriff and Grefenstette [2003]). Still, to the best of our knowledge, a serious analysis
of the frequency and distribution of orthographic errors in the Web is missing, and no
general methods have been developed that help to detect and exclude pages with too
many erroneous words. In this article we first report on a series of experiments that try
to answer the following questions:
1. What are important types of orthographic errors found in Web pages?
2. How frequent are errors of a given kind? For a given error level
(percentage of erroneous tokens) ?, which percentage of Web pages
exceeds error level ??
3. How do these figures depend on the language, on the thematic area,
and on the genre of the Web pages that are considered? How do
these figures depend on the document format of the Web pages that
are considered?
We then look at the problem indicated above.
4. Which methods help to automatically detect Web pages with many
orthographic errors?
Which methods help to mark orthographic errors found in Web pages?
296
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
To answer questions 1?3, we retrieved and analyzed a collection of large English
and German corpora from the Web, using suitable queries to Web search engines. In our
error statistics we wanted to distinguish between (1) ?general? Web pages collected
without any specific thematic focus on the one hand and Web pages from specific
thematic areas on the other hand, and (2) between Web pages written in HTML and
Web documents written in PDF. To cover the first difference, for both languages we
retrieved two general corpora as well as a number of corpora for specific thematic
areas. All these corpora only contain HTML pages. A parallel series of general cor-
pora was collected that are composed of PDF documents. Details are provided in
Section 2.
Special Vocabulary. Web pages often contain tokens that do not belong to the standard
vocabulary of the respective language. Typical categories are, for example, special
names, slang, archaic language, expressions from foreign languages, and special ex-
pressions from computer science/programming. Classification and detection of special
vocabulary is outside the scope of the present article. Since sometimes a clear separation
between special vocabulary and errors is difficult, we briefly come back to this problem
in Section 5.4.
Proper Errors. Focusing on garbled standard vocabulary, tokens may be seriously dam-
aged in an ?unexplainable? way. Most of the remaining errors can be assigned to one of
the four classes mentioned above:
 typing errors (i.e., errors caused by a confusion of keys when typing a
document),
 spelling errors (?cognitive? errors resulting from insufficient language
competence),
 errors resulting from inadequate character encoding, and
 OCR errors.
In order to estimate the number of errors of a given kind in the corpora, special
error dictionaries were built. These dictionaries, which only list garbled words of
a given language that do not accidentally represent correct words, try to cover a
high number of the conventional errors of each type that are typically found in Web
pages and other documents. Section 3 motivates the use of error dictionaries for er-
ror detection. Details of the construction of the error dictionaries are discussed in
Section 4.
In Section 5, we estimate the number of orthographic errors in the corpora that
remain undetected because they do not occur in the error dictionaries. We also estimate
the percentage of correct tokens of the corpora that are erroneously treated as errors
since they appear in the error dictionaries. Our results show that the number of tokens
of a text that appear in the error dictionaries can be considered as a lower approximation
of the number of real orthographic errors.
In Section 6, we describe the distribution of orthographic errors of the types dis-
tinguished above in the general test corpora, counting occurrences of entries of the
error dictionaries. Section 7 summarizes the most important differences that arise when
using PDF corpora, or corpora for special thematic areas. Section 8 presents various
297
Computational Linguistics Volume 32, Number 3
results that illuminate the relationship between the error rate of a document and
its genre.
In our experiments we observed in all corpora a rich spectrum of error rates, ranging
from perfect documents to a small number of clearly unacceptable pages. This moti-
vates the design of filters that efficiently recognize and reject pages with an error rate
beyond a user-specified threshold. The construction of appropriate filters is described
in Section 9, where we also demonstrate the effect of using these filters, comparing
the figures obtained in Section 6 with the corresponding figures for filtered corpora.
Filters work surprisingly well due to a Zipf-like distribution of error frequencies in
Web pages.
In Section 10, we present two experiments that exemplify how the methods de-
veloped in the article may in fact help to improve corpus-based methods. The general
question of how deeply distinct methods from computational linguistics based on Web
corpora are affected by orthographic errors in Web pages and to what extent the meth-
ods developed in the article help to remedy these deficiencies are too complex to be
discussed here.
The main insights and contributions are summarized in the Conclusion (Section 11)
where we also comment on future work and on some practical difficulties one has to
face when collecting and analyzing large corpora from the Web.
2. Corpora
The basis for the evaluations described below is a collection of corpora, each composed
of Web pages retrieved with Web search engines (Google/AllTheWeb). In order to
study how specific features of a language might influence the distribution of ortho-
graphic errors, all corpora were built up in two variants. The English and German
variant, respectively, contain Web pages that were classified as English and German Web
pages by the search engine. As described above, for both languages we collected general
corpora with Web pages without any thematic focus and, in addition, corpora that cover
five specific thematic areas to be described below. Statements on the ?representative-
ness? of corpora derived from the Web are notoriously difficult. The composition of
corpora retrieved with Web search engines depends on the kind of queries that are used,
on the ranking mechanisms of the engine, and on the details of the collection strategy.
We mainly concentrated on simple queries and straightforward collection strategies.
Still, the large number of subcorpora and pages that were evaluated should guarantee
that accidental results are avoided.
2.1 General Web Corpora
In a first attempt, we tried to obtain a general German HTML corpus using the mean-
ingless query der die das, i.e., the three German definite articles. However, queries
of this and a similar form did not lead to satisfactory results: As a consequence of
Google?s ranking mechanism, which prefers ?authorities? (Brin and Page 1998), mainly
portals of big organizations, companies, and others were retrieved. These pages are
often dominated by graphical elements. Portions of text are usually small and carefully
edited, which means that orthographic errors are less frequent than in other ?less
official? pages.
To achieve a more realistic scenario we randomly generated quintuples, each col-
lecting five terms of the 10,000 top frequent German words. We used Google to retrieve
298
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
10 pages per query (quintuple) until we obtained 1,000 pages. A considerable number
of the URLs were found to be inactive. After conversion to ASCII and a preliminary
analysis of error rates with methods described below, some of the remaining pages were
found to contain very large lists of general keywords, including many orthographic
errors. Apparently these lists and errors were only added to improve the ranking of
the page in search engines, even for ill-formed queries. We excluded these pages. The
remaining documents represent the ?primary? general German HTML corpus. Since we
wanted to know how results depend on the peculiarities of the selected set of pages, a
second series of queries of the same type was sent to Google to retrieve a ?secondary?
general German HTML corpus with a completely disjoint set of pages.
Similar procedures were used to obtain a primary and a secondary general English
HTML corpus, a general German PDF corpus, and a general English PDF corpus. The
translation from PDF to ASCII was found to be error prone, in particular for German
documents (cf. Gartner 2003). Due to this process, some converted PDF documents
were seriously damaged. Since we focus on errors in original Web pages (as opposed
to converted versions of such pages), these files were excluded as well. We found
these pages when computing error rates based on error dictionaries as described in
Sections 6 and 7.
The number of Web pages and the number of normal tokens (i.e., tokens composed
of standard letters only) in the resulting six corpora are shown in Table 1. Numbers (1)
and (2) stand for the primary and secondary corpora, respectively.
2.2 Web Corpora for Specific Thematic Areas
We looked at the thematic areas ?Middle Ages,? ?Holocaust,? ?Fish,? ?Mushrooms,?
and ?Neurology.? The given selection of topics tries to cover scientific areas as well as
history and hobbies.
Simple Crawl. A first series of corpora was collected by sending a query with 25
?terminological? keywords mechanically found in a small corpus of the given area to
the AllTheWeb search engine and collecting the answer set. For example, the queries
mushrooms mushroom pine edible harvesting morels harvested harvesters dried
chanterelle matsutake poisonous flavor chanterelles caps fungi drying stuffing
humidity varieties boletes recipes spores conifers pickers
Table 1
Number of Web pages, number of normal tokens (tokens composed of standard letters only),
and sizes in megabytes of the ?general? corpora. Numbers (1) and (2) refer to primary and
secondary corpora, respectively.
General corpora Web pages Normal tokens Size (MB)
English HTML (1) 829 7,900,337 157
English HTML (2) 929 7,152,783 188
German HTML (1) 618 9,525,484 189
German HTML (2) 857 11,539,035 284
English PDF 570 2,193,598 393
German PDF 603 1,528,914 240
299
Computational Linguistics Volume 32, Number 3
disorder disorders anxiety self hallucinations delusions anatomy cortex delusion
neuroscience disturbance conscious psychotic stimulus hallucination unconsciously
receptors cognitive psychoanalytic unconscious consciously stimuli ego schizophrenia
impairment
were respectively used for collecting the corpora Mushrooms E and Neurology E. The
ranking mechanism of AllTheWeb prefers pages containing hits for several keywords
of a disjunctive query. Since this form of corpus construction is straightforward, not all
pages in the resulting corpora belong to the given thematic area.
Refined Crawl. We wanted to see how results are affected when using less naive crawl-
ing methods. For the three areas ?Fish,? ?Mushrooms,? and ?Neurology,? the sec-
ondary corpora were retrieved using the following refined procedure: Starting from
a small tagged seed corpus for the given domain, we mechanically extracted termino-
logical open compounds for English (Sornlertlamvanich and Tanaka 1996; Smadja and
McKeown 1990) and compound nouns for German. Examples are amino group, action
potential, defense mechanism (English, neurology), truffle species, morel areas, harvesting tips
(English, mushrooms), Koffeinstoffwechsel, and Eisenkonzentration (German, neurology).
Each of these expressions was sent as a query to Google. From each answer set we
collected a maximum of 30 top-ranked hits (many answer sets were smaller). For each
document in the resulting corpus, the similarity with the seed corpus was controlled,
using a cosine measure (in practice, almost all documents passed the similarity filter).
Our method can be considered as a variant of Baroni and Bernardini?s (2004) and leads
to corpora with a strong thematic focus.
The statistics for all thematic corpora are summarized in Table 2. Numbers (1) and
(2) stand for corpora crawled with the simple and the refined crawling strategy, respec-
tively. The numbers indicate one interesting effect: Documents in the thematic corpora
obtained with the refined crawling strategy turned out to be typically rather short. Since
we only used the 30 top-ranked documents for each single query, this probably points
to a special feature of Google?s ranking mechanism. A manual inspection of hundreds
of documents for both the simple and the refined crawl did not lead to additional
insights.
3. Error Detection
For detecting orthographic errors of a particular type in texts, two naive base methods
may be distinguished.
1. A representative list of errors of the respective type is created and
manually checked. Each token of the text appearing in the list represents
an error (lower approximation).
2. A spell checker or a large-scale dictionary is used to detect ?suspicious?
words (error candidates). For each such token W we manually check
if W really represents an error and we determine its type (upper
approximation).
For large corpora, both methods have serious deficiencies. With Method 1, only a small
percentage of all errors is detected. On this basis, it is difficult to estimate the real
number of errors. When using Method 2, the number of tokens that have to be manually
300
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
Table 2
Selected topics and statistics of English (E) and German (G) corpora for specific thematic areas.
Numbers (1) and (2) refer to corpora crawled with the simple and the refined strategy,
respectively.
Topic/Language Web pages Normal tokens Size (MB)
Middle Ages E 710 5,069,796 172
Fish E (1) 510 10,090,682 266
Fish E (2) 940 547,407 22
Holocaust E 699 8,797,882 199
Mushrooms E (1) 676 7,876,067 197
Mushrooms E (2) 933 734,337 22
Neurology E (1) 624 8,765,899 197
Neurology E (2) 923 779,699 24
Middle Ages G 614 6,774,794 195
Fish G (1) 655 7,621,579 199
Fish G (2) 804 688,882 32
Holocaust G 616 5,659,924 160
Mushrooms G (1) 527 5,951,305 147
Mushrooms G (2) 614 538,575 28
Neurology G (1) 486 4,322,952 115
Neurology G (2) 323 345,070 12
checked becomes too large. In practice, a large number of error candidates represent
correct tokens. This is mainly due to special names and other types of nonstandard
vocabulary found in Web pages, as mentioned in the introduction.
We decided to use a third strategy, which can be considered as a synthesis and
compromise between the above two approaches. As a starting point, we took stan-
dard dictionaries of English, D(English); German, D(German); French, D(French);
and Spanish, D(Spanish); and a dictionary of geographic entities, D(Geos); a dictio-
nary of proper names, D(Names); and a dictionary of abbreviations and acronyms,
D(Abbreviations).1 The number of entries in the dictionaries is described in Table 3.
The German dictionary contains compound nouns, which explains the large number
of entries.
From these standard dictionaries, we derived special error dictionaries that were
used in the experiments described later. First, for each of the four error types mentioned
above we manually collected a number of general patterns that ?explain? possible
mutations from correct words to erroneous entries. In a second step, these patterns were
used to garble the words of the given background dictionaries. Third, garbled words
that were found to correspond to correct words (entries of the above dictionaries) were
excluded (filtering step). Collecting the remaining erroneous strings, we obtained large
error dictionaries for each type of orthographic error.
Experiments described in Section 5 show that our error dictionaries cover the major
part of all orthographic errors occurring in the English and German Web pages. At
1 These dictionaries are nonpublic. They have been built up at the Centre for Information and Language
Processing (CIS) during the last two decades (Maier-Meyer 1995; Guenthner 1996). Each entry comes
with a frequency value that describes the number of occurrences in a 1.5-terabyte subcorpus of the Web
from 1999. Dictionaries for French and Spanish were included to improve the filtering step. Suitable
dictionaries for other languages were not available.
301
Computational Linguistics Volume 32, Number 3
Table 3
Size of background dictionaries.
Dictionary Number of entries
D(English) 315, 300
D(German) 2, 235, 136
D(French) 85, 895
D(Spanish) 69, 634
D(Geos) 195, 700
D(Names) 372, 628
D(Abbreviations) 2, 375
the same time, the number of tokens that are erroneously treated as errors due to the
unavoidable incompleteness of the filtering step remains acceptable. On this basis, an
estimate of the number of conventional orthographic errors occurring in Web pages is
possible, counting the number of occurrences of entries of the error dictionaries.2 Before
we comment on these points, we describe the construction of the error dictionaries in
more detail. In the remainder of the article, by Dconv we denote the union of all the
conventional dictionaries listed above.
4. Construction of Error Dictionaries
For the construction of error dictionaries, the most important error patterns for each
type of error were determined. For typing errors and errors caused by character en-
coding problems, error patterns were obtained analytically. For spelling errors and
Optical Character Recognition (OCR) errors, important mutation patterns were col-
lected empirically. As a general rule, all error dictionaries were restricted to entries of
length >4. Many tokens of length ?4 occurring in texts represent acronyms, special
names, and abbreviations, and it is difficult to mechanically distinguish between this
special kind of vocabulary and errors.
4.1 Error Dictionaries for Typing Errors
Typing errors can be partitioned into transpositions, deletions, substitutions, and inser-
tions. Transpositions of two letters occur if two keys are hit in the wrong order. Deletions
result if a key is not properly pushed down. Substitutions occur if a neighbor key is
pressed down instead of the intended one. Horizontal and vertical shifts of fingers may
be distinguished. If a finger hits the middle between two keys, a neighbor key may be
pressed in addition to the intended one. The wrong letter may occur before or after the
correct letter.
Transpositions, deletions, substitutions, and insertions cover most of the typing
errors discussed in the literature (Kukich 1992). We ignored homologous errors, that
is, substitutions that are traced back to a confusion of the left and right hand. Since
2 Note that we do not capture false friends, that is, garbled strings that accidentally represent correct
words of the dictionary. Detection of false friends is known to be notoriously difficult and outside the
scope of this article.
302
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
there are many possible positions for both hands, this kind of error leads to large
confusion sets.
Since we did not find other patterns in the texts, only mutation variants that are
exclusively composed of standard letters (as opposed to digits and other special sym-
bols) were taken into account. Furthermore, since typing errors in general do not affect
the first letter of a word,3 we left this letter unmodified. We analyzed the number of
mutated variants of a given word. Both for the American and for the German keyboard
we have approximately 16l variants for a word of length l. This shows that the above
patterns for typing errors are very productive. It is not possible to garble all the words
of our background dictionary for constructing the error dictionaries. For the generation
of the dictionary of English typing errors, Derr(English,typing), we took the 100,000
entries of the English background dictionary with the highest frequency. Applying the
above mutation patterns we generated 10,785,675 strings. After removal of duplicates
and deletion of words in Dconv (filtering step), we obtained 9, 427, 051 entries for the
dictionary Derr(English,typing).
The same procedure was used for creating the dictionary of German typing errors,
Derr(German,typing). Since the average length of German words is large, we obtained
13, 656, 866 entries.
4.2 Error Dictionaries for Spelling Errors
English. In order to find the most characteristic patterns for English spelling errors, a
bootstrapping procedure was used to compute an initial list of errors. We started with
the misspelled English words verry, noticable, arguement, and inteligence. For each term
we retrieved 20 Web documents. After conversion to ASCII we computed the list of all
normal tokens occurring in these documents. The resulting list was sorted by frequency,
and words in Dconv were filtered out. After a manual selection of new errors with high
Google counts, the procedure was iterated until we did not find new erroneous words
with high frequency. During the bootstrapping procedure, we also found Web pages
that listed some ?common misspelled words? of English. The most frequent errors
mentioned in these lists were also added. Table 4 presents some strings that were found
with a large number of Google hits.4
Most of the errors that we found can be traced back to a rule set partially described
in Table 5. The full rule set contains 95 rules. We applied each rule to D(English),
introducing one error at the first possible position, for each entry of the appropriate
form. As a result we obtained a list with 1,223,128 garbled strings. After applying the
standard filtering procedure, we obtained the dictionary Derr(English,spell) of English
spelling errors with 1,202,997 entries.
German. Similarly as for English, we built an initial error list. Bootstrapping was started
with the misspelled German terms na?hmlich, addresse, resourcen, and vorraus. Table 6
shows some of the resulting German words, the misspelled variant, and the number
of Google hits of the garbled version. From the initial error list, we obtained a set of 65
rules partially described in Table 7. We applied these rules to D(German), introducing
one error for each entry of the appropriate form. Each rule was applied to each entry
using the first possible position for mutation. For example, for the lexical entry Adresse of
3 A phenomenon often discussed in the literature; see, for example, Kukich (1992), page 388f.
4 It is well-known that the number of Google hits for a phrase can vary from one day to the next.
303
Computational Linguistics Volume 32, Number 3
Table 4
Some frequently misspelled English words and the number of Google hits of the correct
and misspelled forms.
Word Google hits Transformation Misspelled variant Google hits
accommodate 5,800,000 mm ? m accomodate 559,000
category 109,000,000 teg ? tag catagory 525,000
definitely 10,800,000 itely ? ately definately 1,270,000
independent 25,700,000 dent ? dant independant 523,000
millennium 10,500,000 nn ? n millenium 2,540,000
occurrence 4,640,000 rr ? r occurence 279,000
receive 57,000,000 ie ? ei recieve 1,260,000
recommend 31,400,000 mm ? m recomend 707,000
separate 26,300,000 ara ? era seperate 1,340,000
the German standard dictionary we obtained the following error terms: adrese, ahdresse,
adrehsse, addresse, adrresse. As a result we obtained a list with 19, 265, 271 strings. The
large size is mainly caused by the rules for reduplication of consonants, which are
not restricted by word context. The filtering procedure led to an error dictionary with
18, 970, 716 entries.
Table 5
Rule set (incomplete) for the generation of English spelling errors with examples for each
transformation class.
Deletion of doubled consonants
cc ? c occasionally ? ocasionally
nn ? n drunkenness ? drunkeness
Deletion of consecutive consonants
mn ? m column ? colum
rh ? r rhythm ? rythm
Deletion of doubled vowels
ee ? e exceed ? exced
uu ? u vacuum ? vacum
Deletion in vowel pair
aison ? ason liaison ? liason
ou ? o mischievous ? mischievos
ievous ? evious mischievous ? mischevious
Deletion of silent vowels
?ed ? ?d maintained ? maintaind
Substitution of consonants
sede ? cede supersede ? supercede
dent ? dant independent ? independant
Substitution of vowels
itely ? ately definitely ? definately
teg ? tag category ? catagory
Insertion/reduplication of consonants
? ? {c,d,f,l,n,m,p,r,s,t} ? ?? always ? allways
Transposition of consonants
ght ? gth right ? rigth
Transposition of vowels
ie ? ei believe ? beleive
304
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
Table 6
Some frequently misspelled German words and the number of Google hits of the
misspelled version.
Word Google hits Transformation Misspelled version Google hits
Weihnachten 5,450,000 ih ? i Weinachten 99,600
Adresse 8,040,000 d ? dd Addresse 676,000
Videothek 581,000 th ? t Videotek 18,300
Kamera 10,900,000 mm ? m Kammera 14,200
deshalb 8,330,000 s ? ss desshalb 33,900
ziemlich 2,970,000 i ? ih ziehmlich 48,900
ekelig 20,600 lig ? lich ekelich 17,200
na?mlich 1,620,000 a? ? a?h na?hmlich 53,800
Maschine 1,840,000 i ? ie Maschiene 28,300
direkt 18,200,000 ek ? eck direckt 20,600
danach 5,100,000 n ? nn dannach 46,200
voraus 1,960,000 r ? rr vorraus 214,000
4.3 Error Dictionaries for OCR Errors
As a starting point we used a list of typical OCR errors that we found in a corpus with
200 pages of OCR output (Ringlstetter 2003). Error types are shown in Table 8.
Table 7
Rule set (incomplete) for the generation of German spelling errors. The symbol ?t means that t is
not the preceding letter.
Deletion of doubled consonants
dd ? d Kuddelmuddel ? Kudelmuddel
mm ? m Kommando ? Komando
Special rules for deletion of consonants
mn ? m Kolumne ? Kolume
a?h ? a? a?hnlich ? a?nlich
Deletion of vowels
ie ? i ziemlich ? zimlich
aa ? a Aal ? Al
Substitution of consonants
nt ? nd eigentlich ? eigendlich
rd ? rt Standard ? Standart
Substitution of vowels
a? ? e Empfa?nger ? Empfenger
era ? ara Temperatur ? Temparatur
Insertion/reduplication of consonants
[aeioua?o?u?] ? [aeioua?o?u?]h viel ? viehl
[aeioua?o?u?]k ? [aeioua?o?u?]ck direkt ? direckt
? ? {d,f,l,n,m,p,r,t} ? ?? Gro?britannien ? Gro?brittannien
?tz ? tz Schweiz ? Schweitz
Insertion of vowels
i ? ie Maschine ? Maschiene
Shifting
a?u ? au? a?u?erst ? au??erst
llel ? lell parallel ? paralell
305
Computational Linguistics Volume 32, Number 3
Table 8
List of typical OCR errors.
Character substitutions Character merges Character splits
l ? i rn ? m m ? rn
i ? l ri ? n n ? ri
g ? q cl ? d u? ? ii
o ? p
l ? t
v ? y
y ? v
o ? c
e ? c
l ? 1
English. The error dictionary Derr(English,ocr) was generated by applying to the en-
tries of D(English) the transformation rules listed in Table 8. The transformation of
D(English) with its 315, 300 entries led to a list of 1,697,189 entries. The filtering pro-
cedure where we erase words from Dconv led to the error dictionary Derr(English, ocr)
with 1, 532, 741 entries. Table 9 shows some of the most frequent English words, the
transformation result, and the number of Google hits of the garbled variant.
German. When scanning German texts, vowels a?, o?, and u? are often replaced by
their counterparts a, o, u. However, even more frequently, this kind of replacement
occurs as the result of a character encoding problem (see below). Since we wanted
to avoid having our statistics for OCR errors being heavily overloaded with errors
caused by character encoding problems, we did not add these patterns to the list of
typical OCR errors for German texts. This means that we applied to D(German) the
transformation rules mentioned in Table 8. The transformation of D(German) with its
2, 235, 136 entries led to a list of 11, 623, 989 strings. After filtering, we obtained the error
dictionary Derr(German,ocr) with 10, 608, 635 entries. Table 10 shows some frequent
German words, the transformation result, and the number of Google hits of the garbled
variant.
Table 9
Some members of the top 1,000 most frequent English words transformed by typical OCR error
transformations and the number of Google hits of a garbled version.
Word Transformation Garbled result Google hits
company m?rn cornpany 1.220
from m ? rn frorn 5,310
government rn ? m governrnent 705
many m ? rn rnany 541
market m ? rn rnarket 282
more m ? rn rnore 707
most m ? rn rnost 1,540
only y ? v onlv 4,080
said d ? cl saicl 172
system m ? rn systern 2,060
time m ? rn tirne 2,090
will ll ? 11 wi11 3,570
306
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
Table 10
Some members of the top 1,000 most frequent German words transformed by typical OCR error
transformations and the number of Google hits of a garbled version.
Word Transformation Garbled result Google hits
Dipl-Ing l ? i Dipi-Ing 213
u?ber u? ? ii iiber 2,360
vorne rn ? m vome 1,110
davon o ? p davpn 96
lager g ? q laqer 164
ferner rn ? m femer 841
4.4 Error Dictionaries with Erroneous Character Encoding of German Words
In character sets used for the encoding of Web pages, often the German letters A?, O?, U?,
a?, o?, u?, and ? (?sharp s?) are not available. In many of these cases, vocals are replaced,
following the substitution scheme (e-transformation):
A? ? Ae, O? ? Oe, U? ? Ue, a? ? ae, o? ? oe, u? ? ue.
In other Web pages, the aforementioned vocals are replaced using the following scheme:
A? ? A, O? ? O, U? ? U, a? ? a, o? ? o, u? ? u.
This transformation, which is typically found in Web pages written by non-native
speakers of German, will be called -transformation.
Table 11 shows some transformed terms of the top 1,000 German words and gives
the number of Google hits for correct and incorrect spellings. The right-hand side of
the table gives the corresponding numbers for PDF documents. The numbers show that
misspellings caused by e-transformation are a widespread phenomenon. Note that the
quality of PDF corpora is much better in this respect.
When applying the e- or -transformation, letter ? is typically replaced by ss
(s-transformation). For two reasons, the distinction between ? and ss is a delicate matter.
Since the Swiss spelling is ss, a string representing an erroneous German word may be a
correct Swiss word. To make things even more complicated, the correct spelling of many
German words has been changed during the so-called ?Rechtschreibereform? some
years ago, which affected the selection between ? and ss (e.g., Mi?versta?ndnis became
Missversta?ndnis). Still (and unofficially), the old spelling variant is broadly used. In what
follows, a token written with ss that is officially written with ? is treated as an error.
We built two error dictionaries respectively representing errors introduced via
e-transformation and -transformation. All vowels of the form a?, o?, u? (or upper-case
variants) in the German dictionary were replaced by their images under the respective
transformation. Letter ? occurring in the entries was categorically replaced by ss. For
the e-transformation we obtained a list of 436, 198 strings. The filtering procedure led to
an error dictionary Derr(German, enc-e) with 432, 987 entries.
Applying the -transformation and the usual filtering step, we generated the error
dictionary Derr(German,enc-) with 407, 013 entries. A considerable number of well-
formed words was generated and filtered out. The rules of German morphology yield a
307
Computational Linguistics Volume 32, Number 3
partial explanation: For so-called strong verbs some paradigmatic forms only differ by
a mutation of vowels (mo?chte-mochte).
An extra error dictionary Derr(German,enc-s) was built by replacing ? by ss in
German dictionary entries without occurrences of vocals A?, O?, U?, a?, o?, u?. The dictionary
has 42, 340 entries.
4.5 Summary and Maximal Error Dictionaries
Using the union of all error dictionaries for both languages, we constructed the maximal
error dictionaries Derr(English,all) and Derr(German,all). Table 12 summarizes the sizes
of all error dictionaries.
5. Error Overproduction and Underproduction
Before we analyze the number of tokens in the corpora that represent entries of the
error dictionaries, we comment on the limitations of this kind of analysis. Obviously,
not all orthographic errors of a given type occur in the respective error dictionary
(underproduction). On the other hand, some tokens classified as errors by the error
dictionary might in fact be correct words (overproduction) due to the incompleteness of
Table 11
Most frequent German words with vowels a?, o?, u?; frequencies of correct spelling and frequency
after applying e-transformation. Frequencies are counted in arbitrary Web pages (left part of the
table) and in PDF documents in the Web.
Word Norm. Transf. Percentage PDF norm. PDF transf. Percentage
fu?r 19,000,000 5,140,000 27.05 4,050,000 30,900 0.76
u?ber 17,800,000 2,330,000 13.08 3,610,000 16,000 0.44
ko?nnen 14,500,000 290,000 2.00 1,790,000 3,960 0.22
mu?ssen 7,420,000 177,000 2.38 1,090,000 2,060 0.18
wa?re 3,500,000 173,000 4.94 590,000 631 0.11
fu?nf 2,470,000 291,000 11.78 541,000 570 0.10
ko?nnte 2,900,000 165,000 5.69 570,000 618 0.11
ha?tten 815,000 43,100 5.28 234,000 315 0.13
dafu?r 3,580,800 124,000 3.46 814,000 865 0.11
wu?rde 3,770,000 162,000 4.30 601,000 693 0.11
Table 12
Size of error dictionaries.
Error dictionary Entries Error dictionary Entries
Derr(English,typing) 9, 427, 051 Derr(German,typing) 13, 656, 866
Derr(English,spell) 1, 202, 997 Derr(German,spell) 18, 970, 716
Derr(English,ocr) 1, 532, 741 Derr(German,ocr) 10, 608, 635
Derr(German,enc-e) 432, 987
Derr(German,enc-) 407, 013
Derr(German,enc-s) 42, 340
Derr(English,all) 11, 884, 284 Derr(German,all) 43, 688, 771
308
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
the final filtering step in the construction of the error dictionaries. From the construction
of the error dictionaries we may expect that incompleteness/underproduction is mainly
caused by
 missing patterns for spelling errors and OCR errors, and
 the fact that we do not seriously damage words when constructing the
error dictionaries.
For both English and German, to estimate under/overproduction of the error dictio-
naries, the primary general HTML corpus was split into four subclasses. The class
Best contains all documents where the number of hits (tokens representing entries
of the maximal error dictionary) per 1,000 tokens is ?1. For class Good (Bad, Worst,
respectively), the number of hits per 1,000 tokens is 1?5 (5?10, >10, respectively). The
number of documents in each class is found in Tables 13 and 14.
5.1 Estimating Underproduction
To estimate underproduction of the English error dictionaries, the English general
HTML corpus was split into subfiles, each containing 300 tokens. We then randomly
selected such subfiles and analyzed the proper errors found in these portions. Since
we wanted to avoid an unbalanced selection where most errors are from the doc-
ument class Worst, a maximum of three errors from each subfile was used for the
analysis. Error candidates were found with the help of a spell checker and using
our standard dictionaries as a second control. Slang and special vocabulary were
not used for the statistics. We also excluded errors where two words were merged.
We found that most of the latter errors were caused by the conversion process from
HTML to ASCII. Each candidate was manually controlled; in difficult cases we con-
sulted Merriam-Webster Online. We continued the search until 1,000 proper errors were
isolated. From these, 624 (62.4%) turned out to be entries of the maximal English error
dictionary.
Table 13 refines these statistics and shows the number of errors and the percentage
of errors found in the error dictionary for the four quality classes of documents. As a
tendency, recall of the error dictionary is better in ?bad? documents.
The same procedure was used for German and confirmed this tendency. From 1,000
errors in the German general HTML corpus, 638 (63.80%) were found in the maximal
German error dictionary. The statistics for the four quality classes of documents is
presented in Table 14.
5.2 Estimating Overproduction
In our first experiment with English texts we found that a considerable number of hits
corresponded to special names introduced in the documents. Many of these names are
artificial (e.g., Hitty). In order to avoid all difficulties with special names we decided
to restrict the error analysis in English texts to words starting with a lowercase letter.
In each of the four classes, 1,000 hits of this form were randomly selected. We then
manually checked which of these tokens represent correct words, reading contexts and
consulting Merriam-Webster Online in difficult cases.
The results are presented in Table 15 and show a clear tendency. The percentage of
proper errors is larger in documents with a large number of hits. In the class Worst, 95%
309
Computational Linguistics Volume 32, Number 3
Table 13
Underproduction of the maximal error dictionary in the primary English general HTML corpus.
Document class Documents Errors found Entries of error dict. %
Worst 24 248 166 66.93
Bad 39 194 131 67.53
Good 226 389 242 62.21
Best 540 169 85 50.29
Table 14
Underproduction of the maximal error dictionary in the primary German general HTML corpus.
Document class Documents Errors found Entries of error dict. %
Worst 50 389 307 78.92
Bad 42 166 101 60.84
Good 297 385 201 52.21
Best 229 60 29 48.33
of all hits are proper errors; in the class Best, only 60% of the hits represent orthographic
errors. Most of the remaining hits could be assigned to one of the following categories:
correct standard expressions (missing entries of the standard dictionaries), names and
geographic expressions, foreign language expressions, archaic and literary word forms,
and abbreviations. The number of hits in each category is found in Table 15. The large
number of standard words among the hits in the class Best is caused by an incomplete-
ness of our English dictionary, which does not always contain both the British and the
American spelling variants.
In the German general HTML corpus, where we could not restrict the experiment
to tokens starting with a lowercase letter, a more shallow picture is obtained (Table 16).
For the classes Best (61% proper errors), Good (62% proper errors), and Worst (88%
proper errors), results are similar to the English case and confirm the above-mentioned
general tendency. Due to the large number of names, foreign language expressions, and
archaic/literary word forms that are found in class Bad, we here have only 56% proper
errors. The results show that overproduction could be considerably reduced when filter-
ing error dictionaries with better standard dictionaries for geographic entities, personal
names, foreign language expressions, and archaic and literary word forms.
Table 15
Overproduction of the maximal error dictionary in the English general HTML corpus.
Document class Best Good Bad Worst
Hits 1,000 1,000 1,000 1,000
Percentage proper errors 72 86 89 95
Proper errors 722 856 894 952
Standard words 206 31 21 5
Personal names and geographic entities 23 35 24 27
Foreign language expressions 32 42 36 12
Archaic and literary word forms 9 28 1 1
Abbreviations 8 6 24 2
310
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
5.3 Summary So Far
From the above percentages we obtain a naive estimate for the ratio between the real
number of errors and the number of hits of the error dictionaries, which is presented in
Table 17. The results show that the number of hits can be seen as a lower approximation
of the real number of errors. The ratio between both numbers is larger for English. It
does not differ dramatically between the distinct quality classes. However, since both
over- and underproduction are larger for ?good? documents, error estimates for these
classes come with a larger degree of uncertainty.
5.4 Difficulties
The above analysis turned out to be much more time-consuming and difficult than
it might appear. One problem is caused by the fact that nonstandard vocabulary and
errors do not represent disjoint categories. Orthographic errors are sometimes ?abused?
as slang expressions. A separation between archaic/foreign language expressions and
orthographic errors is often only possible when taking the sentence context into
account. These and other examples explain that demarcation issues are sometimes
difficult to solve. The construction of special dictionaries for slang, foreign language
expressions, special names, and archaic word forms represents an important step
for future work. Using these dictionaries in the filtering step of the construction of
the error dictionaries, overproduction may probably be reduced in a significant way.
Furthermore, these dictionaries should help to detect Web pages with nonstandard
vocabulary of a particular type.
Table 16
Overproduction of the maximal error dictionary in the German general HTML corpus.
Document class Best Good Bad Worst
Hits 1,000 1,000 1,000 1,000
Percentage proper errors 61 62 56 88
Proper errors 615 624 564 884
Standard words 126 123 47 3
Names and geos 201 147 193 49
Foreign language expressions 31 46 103 37
Archaic and literary word forms 18 44 82 24
Abbreviations 9 16 11 3
Table 17
Naive estimates of the ratio between the real number of errors and the number of hits of the
error dictionaries for distinct quality classes.
English German
Best 0.72/0.5029 = 1.43 Best 0.61/0.4833 = 1.26
Good 0.86/0.6221 = 1.38 Good 0.62/0.5221 = 1.19
Bad 0.89/0.6753 = 1.32 Bad 0.56/0.6084 = 0.92
Worst 0.95/0.6693 = 1.42 Worst 0.88/0.7892 = 1.12
311
Computational Linguistics Volume 32, Number 3
6. Distribution of Orthographic Errors in the General HTML Corpora
We define the error rate of a text with respect to an error dictionary Derr as the average
number of entries of Derr that are found among 1,000 tokens of the text. In this section
we describe the distribution of error rates for all types of errors in the general HTML
corpora. Experiments for other corpora are summarized in the following section. The
results of the previous section indicate that the error rate represents a reasonable lower
approximation for the real number of errors per 1,000 tokens in the document. Incom-
pleteness of the rule sets for generating spelling errors and OCR errors should be kept
in mind. Recall that in English documents, only words starting with a lowercase letter
are taken into account.
6.1 Distribution of Error Rates for Orthographic Errors
In the first test, we consider orthographic errors, that is, errors of arbitrary type. Ac-
cordingly, error rates for documents are computed with respect to the maximal error
dictionaries. For a coarse survey, as in the previous section, we distinguish four quality
classes Best, Good, Bad, Worst, respectively, covering pages with error rates in the
intervals [0, 1), [1, 5), [5, 10), and [10,?).
English. The histograms in Figure 1 show the percentage of documents in each class in
the primary (left-hand side) and secondary (right-hand side) English corpora. Remark-
ably, the differences between the two corpora are almost negligible. In both cases, most
documents belong to class Best; only a small percentage of documents belongs to classes
Bad and Worst.
Table 18 presents the average error rate for various document classes. As to the
length of documents in the corpora, drastic differences exist. We did not find a cor-
relation between document length and error rates, with the following eye-catching
exception: small (larger) documents of an excellent quality tend to have an error rate
0 (close to 0, but >0).5 In order to avoid a dominating influence of long documents,
we simply computed the arithmetic mean of all error rates obtained for the single
documents. The class Best 80% collects 80% of all documents with lowest error rate,
and similarly for the class Best 90%.
Note that a significant difference exists between the average rate for all documents
(2.47, 2.24, respectively) and the means for the Best 80% classes (0.67, 0.68, respectively).
These numbers point to an effect that will be found again in other figures and exper-
iments: The large majority of all documents in the corpora have a very good quality.
Yet, at the ?bad end? of the spectrum we find a considerable number of unacceptable
documents with a very large number of errors. The phenomenon becomes even more
apparent in Figure 2 (left diagram) where we depict the error rates of all documents.
In what follows we often describe mean error rates for all documents and for the class
Best 80%. When comparing distinct corpora, the two values help to see if deviations
concern the class of all documents or if they are rather caused by a small number of
?bad? documents.
Note also that all corresponding average error rates obtained for the primary and
secondary corpora are almost identical. This gives at least some evidence to the conjec-
5 This explains the special effect seen in Figures 14 and 15 where the refined crawl produces many short
documents.
312
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
Figure 1
Percentage of documents in the four quality classes for the primary (left-hand side) and
secondary (right-hand side) English corpora. The four quality classes cover distinct error
rates for orthographic errors.
ture that for corpora crawled with similar queries and collection strategies, error rates
will not differ too much. As we see next, the situation for the German corpora is more
complex.
German. The histogram in Figure 3 shows the percentage of documents in each class
of the primary (left-hand side) and secondary (right-hand side) German corpora. A
large number of documents belongs to class Good. We now find a larger difference
between the primary and secondary corpora. Several phenomena might be responsible.
As mentioned above, for the German corpora we did not restrict the analysis to tokens
starting with a lowercase letter. Hence, documents with many names can cause special
effects and lead to differences between corpora. Second, errors caused by encoding
of special characters represent an important extra source for errors in German docu-
ments where numbers may vary from one corpus to another. This is seen in Table 20
where we analyze all error types occurring in the primary and secondary German
corpus. The means for e-transformation are 0.62 for the primary corpus and 1.40 for the
secondary corpus.
The average error rates obtained for distinct documents classes of the German
corpus, which are presented in Table 19, show that
 for all classes we have more errors than in the English documents, and
 for different corpora, sometimes nontrivial deviations must be expected.
Table 18
Mean error rate for arbitrary orthographic errors in various document classes; results for the
general English HTML corpus.
Document class Best Good Bad Worst Best 80% Best 90% All
E (1) 0.30 2.31 8.83 23.23 0.67 1.06 2.47
E (2) 0.27 2.19 6.77 21.61 0.68 1.03 2.24
313
Computational Linguistics Volume 32, Number 3
Figure 2
Distribution of error rates for arbitrary orthographic errors in the primary English (left diagram,
829 documents, mean error rate 2.47) and the German (right diagram, 618 documents, mean
error rate 3.86) general HTML corpora. On the x-axis, documents are ordered by error rates;
documents with high rates are found on the right-hand side. In the left diagram, 7 documents
with error rates ranging from 42.99 to 64.22 have been omitted to simplify scaling. In the right
diagram, one document with error rate 40.07 is omitted.
A more detailed picture of the error rates in the primary German corpus is given
in Figure 2 (right diagram). The two curves of the figure show that despite the afore-
mentioned differences between English and German, basic features of the error rate
distribution are very similar.
6.2 Error Rates for Particular Error Classes
Typographic Errors. The most widespread subclass of errors found in the corpora
are typographic errors. For the primary English corpus, as many as 2.31 of 2.47 hits
(93.5%) can be classified as typing errors.6 The percentage is lower in the German corpus
(2.15/3.86, 55.7%) where e-transformation, -transformation, and s-transformation rep-
resent additional important sources for errors (see below). In absolute numbers, error
rates for typographic errors observed in the two corpora are similar.
The histograms in Figure 4 show the percentage of documents with error rates for
typographic errors in the four intervals [0, 1), [1, 5), [5, 10), and [10,?) for the primary
and secondary English corpora (upper diagrams of Figure 4) and the corresponding
German corpora (lower diagrams of Figure 4). Note again the close similarity between
the two English corpora. The detailed distribution curves, which are similar to the
curves obtained for orthographic errors in Figure 2, are omitted.
Spelling Errors. The two diagrams in Figure 5 show that the error rates found in
the primary English corpus (mean 0.39) are similar to the ones found in the primary
German corpus (mean 0.45). The results presented in Section 5.1 indicate that our error
dictionaries for spelling errors are incomplete. Hence the real number of spelling errors
is probably larger. We also computed error rates for spelling errors in the secondary
6 Recall that the error type of a garbled token may be ambiguous.
314
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
Figure 3
Percentage of documents in the four quality classes for the primary (left-hand side) and
secondary (right-hand side) German corpora. The four quality classes cover distinct error
rates for orthographic errors.
corpora; results are presented in Table 20. The tendency observed earlier for ortho-
graphic errors was confirmed: the difference between the two English corpora (mean
0.39 versus mean 0.38) is negligible; for the two German corpora, the difference is larger
(mean 0.45 versus mean 0.58).
OCR Errors. The diagrams in Figure 6 show that most documents do not contain any
OCR errors. Of course this result is not surprising. Probably not all errors that contribute
to the two diagrams are really caused by wrong character recognition. Although some
of the documents with the highest errors were explicitly marked to contain scanned
Table 19
Mean error rate for arbitrary orthographic errors in various document classes; results for the
general German HTML corpus.
Document class Best Good Bad Worst Best 80% Best 90% All
G (1) 0.41 2.61 7.30 15.15 1.89 2.58 3.86
G (2) 0.48 2.57 7.21 24.38 2.40 3.09 5.40
Table 20
Mean of error rates for all error types in primary and secondary general HTML corpora.
Error type Mean error rate Mean error rate Mean error rate Mean error rate
English corpus English corpus German corpus German corpus
HTML (1) HTML (2) HTML (1) HTML (2)
arbitrary 2.47 2.24 3.86 5.40
typographic 2.31 2.03 2.15 2.79
spelling 0.39 0.38 0.45 0.58
OCR 0.06 0.07 0.13 0.18
e-transformation 0.003 0.004 0.62 1.40
-transformation 0.02 0.01 0.19 0.24
s-transformation 0.00003 0.00 0.76 0.96
315
Computational Linguistics Volume 32, Number 3
Figure 4
Typographic errors: the percentage of documents in the four quality classes in the general
English (upper part) and German (lower part) HTML corpora. Quality classes refer to error rates
for typographic errors.
text, it is natural to assume that the total number of such documents in the corpus is
very small. Ambiguous error types might explain some of the errors found in Figure 6;
see the discussion below. As a matter of fact, the number of OCR errors will grow when
analyzing corpora with many OCRed pages.
Figure 5
Distribution of error rates for spelling errors in the primary English (left diagram, mean error rate
0.39) and German (right diagram, mean error rate 0.45) general HTML corpora. In the left (right)
diagram, one document with error rate 14.95 (11.31) is omitted.
316
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
Figure 6
Distribution of error rates for OCR errors in the primary English (left diagram, mean error rate
0.06) and the German (right diagram, mean error rate 0.13) general HTML corpora.
e-transformation and -transformation. Figures 7 and 8 show some interesting differ-
ences between the use of both transformations in German Web pages. In the primary
German corpus, e-transformation errors are concentrated in a small class of documents
(documents with rank >480) where we have a nontrivial number of occurrences, lead-
ing to a mean error rate of 0.62. The mean error rate for -transformation is much
smaller (0.19). Still, there are more documents containing an -transformation error.
This indicates that e-transformation is applied more systematically. The small plateau
in Figure 7 is generated by some portion of text that was found in several documents.
The error rates that arise when applying e-transformation in a completely systematic
way are typically larger. In the corpus we found some documents of this kind; since the
rates are too high, these documents are not depicted in the figure.
We also looked for e- and -transformation errors in the documents of the En-
glish general HTML corpus. These errors, which mutate German words, only occur
Figure 7
Distribution of error rates for e-transformation in the primary German general HTML corpus.
Mean: 0.62. Here 7 documents with error rates ranging from 13.16 to 34.10 are omitted.
317
Computational Linguistics Volume 32, Number 3
Figure 8
Distribution of error rates for -transformation in the primary German general HTML corpus.
Mean 0.19.
in a small number of English documents. Whereas German writers strongly prefer the
e-transformation in situations where the correct characters are not available, we find a
clear preference for the -transformation in the English documents.
s-Transformation. Figure 9 shows the distribution of error rates for s-transformation
in the primary German general HTML corpus. Since the corpus contains some Swiss
documents, where ??? is categorically written ?ss? (cf. Section 4.4), the high mean (0.76)
has to be relativized.
Overview. Table 20 summarizes the error rates of all types of errors in the general HTML
corpora. The numbers show that not all errors can be traced back to a unique error
type.
Figure 9
Distribution of error rates for s-transformation in the primary German general HTML corpus.
Mean 0.76. One document with error rate 11.46 is omitted.
318
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
6.3 Summary So Far
For both languages, the large majority of all documents has a small number of ortho-
graphic errors. On the other hand, at the ?bad end? of the spectrum, a considerable
number of unacceptable documents with high error rates is found. Mean values for
error rates are strongly influenced by the latter documents; the average error rate for
the Best 80% class is usually much lower. The latter rate should also be considered when
comparing results obtained for two corpora.
Phenomena observed in English corpora seem to be more stable than those for
German: Results obtained for the primary and the secondary English general HTML
corpus are almost identical. Differences between the two German corpora may partially
be explained by names occurring in texts and by special character encoding problems.
Table 20 illustrates this effect, showing the mean error rates for all error types in the
primary and secondary HTML corpora.
The most important error class are typographic errors. In the German documents,
e-transformation and s-transformation represent another typical error source. Whereas
the number of spelling errors is significant, OCR errors do not play an essential role.
Interestingly, the basic form of the distribution curves in Figure 2 is found again in
all corresponding curves for other error types and other corpora (see also Figures 14
and 15); although the absolute numbers for error rates and details are of course distinct.
The close similarity of all distribution curves is striking and gives some evidence to the
assumption that relevant features of the error rate distribution are stable, regardless of
the corpora that are investigated.
7. Differences for Special Corpora
We summarize the error rates that we found in PDF corpora and in corpora for special
thematic fields. In Figures 14 and 15, we present a small selection of distribution curves
for error rates. Similarities of the distribution curves mentioned in the previous section
should also be noted.
7.1 Distribution of Orthographic Errors in the General PDF Corpora
Figure 10 presents the mean error rates for distinct error types found in the general
PDF and (primary) HTML corpus for English. The results show that PDF documents in
general have a better quality than HTML documents. Whereas we have a mean error
rate of 2.47 for orthographic errors in the HTML documents, the corresponding mean
is only 1.38 for PDF. For the Best 80% documents the means are 0.67 (HTML) and 0.38
(PDF).
In principle, the same tendency was observed in the documents of the parallel
German corpora. However, special effects polluted the picture. As we mentioned in
Section 2.1, the conversion of the German PDF documents to ASCII is very error
prone. Although we excluded all converted documents that were obviously garbled
by the conversion, we also found in the remaining documents examples of errors that
were caused by the conversion process. In this sense, the error rates in the original
PDF documents are probably smaller. Mean error rates are 2.15 (HTML) versus 2.04
(PDF) for typographic errors, 0.45 versus 0.41 for spelling errors, 0.13 versus 0.09
for OCR errors, 0.62 versus 0.07 for e-transformation errors, and 0.19 versus 0.16 for
-transformation errors. Since the conversion tool categorically replaces letter ??? by
319
Computational Linguistics Volume 32, Number 3
Figure 10
PDF versus HTML: mean error rates for distinct error types in the general corpora (English).
Black rectangles describe mean error rates for the Best 80% subclass.
?ss?, a very high number of s-transformation errors led to the effect that the overall
mean error rate for the German PDF (3.95) is even larger than the rate for the German
HTML (3.86).
7.2 Distribution of Orthographic Errors in Distinct Thematic Corpora
Figure 11 describes the average error rates for orthographic errors and spelling errors
in the English corpora. In almost all thematic areas, mean error rates are larger than
the corresponding means in the general corpora; the differences are significant and
remarkable. With a mean error rate of 2.05 (0.30) for orthographic (spelling) errors,
the English Neurology corpus is very clean and represents an exception. For the Fish
corpus, even the mean error rate for the Best 80% subclass is 2.72. We conjecture that
corpora that are collected without a special thematic focus often contain a large number
of ?professional? and carefully edited Web pages. Web pages for special thematic areas
Figure 11
Thematic corpora versus general corpora: mean error rates for orthographic errors and spelling
errors in distinct English corpora. All results refer to the primary thematic corpora crawled with
the simple strategy (cf. Section 2.2). Black rectangles represent mean error rates for the Best 80%
subclass.
320
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
are perhaps less ?publicity oriented.? Furthermore, as a rule of thumb, documents in
thematic fields related to hobbies (e.g. Fish) contain more orthographic errors than
documents in scientific fields (Holocaust, Neurology). Corpora with a focus on history
seem to occupy a middle position.
In the German corpora we have the means for orthographic/spelling error rates
presented in Table 21; numbers in brackets refer to the Best 80% subclass. The second
column shows that, by and large, the ranking order for thematic areas induced by mean
error rates observed in the English corpora is found again in the German part. The
German corpus Neurology, with its high error rate, does not follow this line. The high
means for the Best 80% subclasses in the German corpora are remarkable and show that
the low quality is not caused by a small number of bad documents.
7.3 Differences between the Two Crawling Strategies
Table 22 summarizes the differences for the English corpora retrieved with the simple
strategy on the one hand and the corpora retrieved with the refined strategy on the other
hand. Numbers represent average error rates for the corpora. Numbers in brackets refer
to the Best 80% subclass.
Surprisingly, all corpora crawled with the refined strategy always have a better
(smaller) average error rate than those retrieved with the simple strategy, pointing to
a significant difference between the two types of collection strategies. An analysis of the
document genres found in the two types of corpora presented in Section 8 offers a good
explanation; see Table 26.
Table 21
Mean error rates for orthographic errors and spelling errors in thematic German corpora.
German Orthographic errors Spelling errors
General PDF 3.95 (2.31) 0.41 (0.06)
Neurology G (1) (HTML) 6.94 (4.48) 0.51 (0.26)
General HTML (1) 3.86 (1.81) 0.45 (0.16)
Holocaust G (HTML) 4.97 (3.03) 0.50 (0.27)
Mushrooms G (1) (HTML) 7.91 (3.69) 0.78 (0.32)
Middle Ages G (HTML) 7.84 (4.30) 0.96 (0.38)
Fish G (1) (HTML) 9.34 (4.47) 1.35 (0.52)
Table 22
Dependency of mean error rates on the crawling strategy for distinct English thematic corpora.
Orthographic errors Spelling errors
(1) (2) (1) (2)
English Simple crawl Refined crawl Simple crawl Refined crawl
Fish E 7.08 (2.72) 3.39 (0.35) 0.98 (0.27) 0.47 (0)
Mushrooms E 4.10 (1.49) 2.58 (0.32) 0.52 (0.13) 0.50 (0)
Neurology E 2.05 (0.79) 1.77 (0.25) 0.30 (0.05) 0.26 (0)
321
Computational Linguistics Volume 32, Number 3
Table 23
Dependency of mean error rates on the crawling strategy for distinct German thematic corpora.
Orthographic errors Spelling errors
(1) (2) (1) (2)
German Simple crawl Refined crawl Simple crawl Refined crawl
Fish G 9.34 (4.67) 7.71 (3.31) 1.35 (0.52) 1.00 (0.17)
Mushrooms G 7.91 (3.69) 8.51 (3.50) 0.78 (0.32) 0.76 (0.08)
Neurology G 6.94 (4.48) 7.08 (2.86) 0.51 (0.26) 0.47 (0.00)
Figures 14 and 15 show that the corpora crawled with the refined strategy have a
large number of documents with error rate 0. This special effect is caused by the large
number of short documents that are obtained. For example, the mean length of all the
documents with error rate 0 in the corpus Fish E (2) is 322 (number of lowercase normal
tokens), whereas the average length of the documents in the corpus Fish E (1) is 14,196
(cf. Table 2).
The relative order between the three thematic fields was not affected by the crawling
strategy. For both crawls, the Neurology corpus achieves the best results, followed by
Mushrooms and Fish. The excellent quality of the Best 80% classes obtained with the
refined crawl are remarkable.
For the German variant of the corpora, as Table 23 shows, a more shallow picture
is obtained. For two thematic areas, the simple crawl even leads to lower error rates,
although the difference is small. The ranking order between the three thematic areas
obtained from the two crawls is not the same.
Figure 14 presents the error rates for orthographic errors in the English HTML
corpora Fish, Mushrooms, and Neurology, comparing the simple strategy (left-hand
side diagrams) with the refined strategy (right-hand side diagrams). Figure 15 gives
the error rates for spelling errors in the German HTML corpora Fish, Mushrooms, and
Neurology, again comparing the simple and the refined strategies.
7.4 Summary So Far
PDF corpora were found to have lower error rates. Corpora covering pages from non-
scientific thematic areas often have higher error rates than corpora crawled without a
fixed thematic focus. Error rates in the corpora are influenced by the crawling strategy.
For English texts, refined crawling strategies that collect pages with a strong thematic
focus seem to produce better corpora.
8. Error Rates and Document Genre
Classifying Web documents by genre (Kessler, Nunberg, and Schu?tze 1997; Finn and
Kushmerick 2003; Dimitrova et al 2003) represents one possible way to improve Web
search techniques. Web-based corpus linguistics may benefit from these techniques
since they enable a better control of the kind of language material that is added to
a collection. In this section we want to see which kind of correlation exists between
the error rates observed in a document and its genre. After manual inspection of
322
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
Figure 12
Zipf curves with logarithmic frequencies for English (upper diagram, 1,175,894 entries) and
German (lower diagram, 454,709 entries) ranked error lists. The diagrams respectively
illustrate the frequency of particular orthographic errors in English and German Web pages
from a 1.4-terabyte subcorpus of the Web.
hundreds of Web pages, we decided to use the following set of document genres for
our investigations:
 The class Prof contains all Web pages with professional texts from
organizations, enterprises, and administrations. Also, scientific texts,
professional literature, and fiction are added to this class.
 The class Priv contains private homepages and texts written from a
personal point of view. A clue term is the personal pronoun I. Texts of this
form may dominate in a Web page run by an organization. In this case, the
page was classified as Priv.
 The class Chat contains chat and related collections of private statements
and contributions such as guest books, mailing lists, and so forth.
323
Computational Linguistics Volume 32, Number 3
Figure 13
Distribution of error rates in documents (passed/rejected) by the filter F3 for threshold ? = 5
(English test corpus). The left (right) diagram describes the distribution of documents passed
(rejected) by the filter. The average error rate for accepted (rejected) documents is 1.08 (16.81).
 The class Junk contains documents where the language is ?polluted,? for
example, by large lists of erroneous keywords, lists of foreign language
expressions, dominating subparts only consisting of program code, archaic
language, and so forth.
 The class Other contains all other documents. In practice we tried to assign
to each document one of the above four classes, and most documents in
the class Other are (almost) empty files.
Even with this small number of classes, separation issues were sometimes difficult to
address. We did not introduce finer subclasses since we expected that the number of
ambiguous and problematic cases would be multiplied.
Our experiments on document genre were restricted to English corpora. We looked
at the primary general English HTML corpus and on the English corpora for the
domains Fish, Mushrooms, and Neurology. For each of the latter three domains, both
the corpus obtained with the simple crawling strategy and the corpus retrieved with the
refined crawl were taken into account. Hence, a total of 7 corpora were investigated.
8.1 Genre Distribution of the Four Quality Classes
For each corpus, all documents in the classes Worst and Bad were manually classi-
fied, assigning one of the classes Prof, Priv, Chat, Junk, or Other to the document.
From the classes Good and Best, 100 documents were randomly selected and clas-
sified in the same way. Table 24 presents the classification results for the primary
English general HTML corpus. Not surprisingly, classes Chat and Junk dominate at
the bad end of the quality spectrum, whereas class Prof dominates for good doc-
uments. The same tendency was found for all corpora, although the percentage of
Prof documents in distinct quality classes was often larger. To add one further typical
example, Table 25 presents the result for the corpus Fish E (1) retrieved with the simple
crawling strategy. Note that even for the Bad class, 50.62% of the documents are of
type Prof.
324
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
Figure 14
Distribution of error rates for arbitrary orthographic errors in the 6 English HTML corpora:
Fish E (1) and Fish E (2) (upper diagrams), Mushrooms E (1) and Mushrooms E (2) (middle), and
Neurology E (1) and Neurology E (2) (bottom diagrams). Letters (1) (diagrams on the left-hand
side) refer to corpora retrieved with the simple crawling strategy. Letters (2) (diagrams on the
right-hand side) stand for the refined crawling strategy. From the refined crawl (right-hand
sides) a large number of documents without any error hit is obtained. Corpora crawled with the
refined strategy typically contain a large number of short documents (cf. Sections 2.2 and 7.3),
and short documents of good quality often have an error rate 0. A comparison along the vertical
dimension illuminates differences between the three thematic areas: corpora Fish E contain more
errors than corpora Mushrooms E, which contain more errors than the corpora Neurology E.
Mean error rates are 7.08/3.39 [Fish E (1)/Fish E (2)]; 4.10/2.58 [Mushrooms E (1)/Mushrooms E
(2)]; and 2.05/1.77 [Neurology E (1)/Neurology E (2)]. In the diagrams, some documents with
high error rates are omitted to simplify scaling.
325
Computational Linguistics Volume 32, Number 3
Figure 15
Distribution of error rates for spelling errors in the 6 German HTML corpora: Fish G (1) and Fish
G (2) (upper diagrams), Mushrooms G (1) and Mushrooms G (2) (middle), and Neurology G (1)
and Neurology G (2) (bottom diagrams). Letters (1) (diagrams on the left-hand side) refer to
corpora retrieved with the simple crawling strategy. Letters (2) (diagrams on the right-hand
side) stand for the refined crawling strategy. The latter strategy leads to a large number of
short documents without any hits in the error dictionaries. See the discussion in Section 7.3.
Similarly as for English HTML, corpora Fish G contain more errors than corpora Mushrooms G,
which contain more errors than the corpora Neurology G. Mean error rates are 1.35/1.00
[Fish G (1)/Fish G (2)]; 0.78/0.76 [Mushrooms G (1)/Mushrooms G (2)]; and 0.51/0.47
[Neurology G (1)/Neurology G (2)]. In the diagrams, some documents with high error rates
are omitted to simplify scaling.
326
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
Table 24
Genre distribution of the four quality classes for the primary general English HTML corpus.
English HTML (1) Worst (%) Bad (%) Good (%) Best (%)
Chat 42.31 56.41 24.00 1.00
Junk 38.46 5.13 1.00 0.00
Priv 3.85 10.26 14.00 9.00
Prof 15.38 28.20 61.00 90.00
Other 0.00 0.00 0.00 0.00
Table 25
Genre distribution of the four quality classes for the corpus Fish E (1).
Fish E (1) Worst (%) Bad (%) Good (%) Best (%)
Chat 37.39 20.99 4.00 3.00
Junk 26.09 6.17 0.00 6.00
Priv 8.70 22.22 9.00 3.00
Prof 27.82 50.62 84.00 84.00
Other 0.00 0.00 3.00 4.00
8.2 Genre Distribution: Simple Crawl versus Refined Crawl
The analysis of genres presented in Table 26 illuminates an important difference be-
tween the thematic corpora retrieved with the simple and the refined crawling strategy:
In the latter corpora, the percentage of documents of type Chat and Junk is lower;
differences are significant. At the same time, corpora retrieved with the refined strategy
contain more documents of type Prof. We conjecture that the open compounds that
were used in the queries for the refined crawl (cf. Section 2.2) represent a kind of ?high-
level language expressions? that are typically used in a professional or scientific context.
With the above background, it is not surprising that the refined crawling strategy leads
to better error rates.
8.3 Error Rates for Genres
Table 27 presents estimates for the mean error rates obtained for the distinct document
genres in the seven corpora. These numbers represent estimates since not all documents
Table 26
Composition of corpora retrieved with the simple (1) and the refined (2) crawling strategies. The
refined strategy (2) helps to avoid documents of type Chat and Junk, attracting documents of
type Prof at the same time.
Crawls Fish E Fish E Mushr. E Mushr. E Neur. E Neur. E
(1) (%) (2) (%) (1) (%) (2) (%) (1) (%) (2) (%)
Chat 13.86 2.69 8.63 3.52 3.87 2.87
Junk 9.10 0.88 5.40 3.15 2.97 0.11
Priv 8.79 16.13 12.70 11.96 7.49 2.44
Prof 66.03 80.30 73.27 80.01 82.83 94.58
Other 2.22 0.00 0.00 1.36 2.84 0.00
327
Computational Linguistics Volume 32, Number 3
Table 27
Mean error rates (estimates) for distinct document genres in seven corpora.
Crawls English Fish E Fish E Mushr. E Mushr. E Neur. E Neur. E
HTML (1) (1) (2) (1) (2) (1) (2)
Chat 6.90 13.05 14.29 10.71 6.27 4.94 11.22
Junk 27.31 23.61 59.05 12.37 16.00 4.59 3.15
Priv 2.82 7.85 3.16 3.34 3.37 3.79 5.89
Prof 1.26 3.68 2.04 2.94 1.20 1.67 1.31
of the classes Good and Best were classified. In all corpora, the mean error rate for class
Prof is better than the rate for class Priv, which is better than the rate for class Chat.
The results indicate that the error rate of a document might be an interesting feature
for genre classification: High error rates typically point to documents of the genres Junk
and Chat; excellent error rates typically point to documents of type Prof. Results for
the Neurology corpora indicate that ?scientific Chat/Junk? may come with low error
rates.
8.4 Summary So Far
An obvious correlation exists between the genre of a document and its error rate. Error
rates might be used as one feature for genre classification. The analysis of genres helps
one to understand the differences between corpora retrieved with distinct crawling
strategies and the error rates observed in the corpora.
9. Filtering Methods
The figures seen in the previous sections show that corpora collected from the Web
typically contain a non-negligible number of documents with an unacceptable number
of orthographic errors. We now turn to the question of how to use error dictionaries for
recognizing and filtering Web pages with a high percentage of errors, thus excluding
them from the corpus construction process. The question of what should be considered
as a ?high percentage? has to be answered for each application. Generally speaking we
would like to exclude at least those documents that are found at the right end of the
diagrams presented in the previous sections.
Definition
By a filter, we mean a pair F = ?D,?? consisting of an error dictionary, D, and a
filter threshold, ?. The filter rejects a text document (Web page) T iff the average
number of entries of D that are found among 1,000 tokens of T exceeds ?.
As a matter of fact, we may use the maximal error dictionaries for filtering. For
some applications, small error dictionaries, which occupy less space and are easier
to handle, may be advantageous. The results presented below show that when one
uses a more rigid filter threshold ?, the filtering effect achieved with ?small? error
dictionaries is very similar to the effect when using the maximal error dictionaries.
With an obvious interpolation, this observation supports the assumption that the
328
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
incompleteness of our maximal error dictionaries does not seriously reduce their fil-
tering capacities.
9.1 Distribution of Error Frequencies
Since error dictionaries are necessarily incomplete in the sense that not all possible
errors can be covered, it is natural to ask if filters of the above-mentioned form
can work. We shall see below that even filters with small error dictionaries are use-
ful. The reason is that the frequency of orthographic errors in the Web follows a
Zipf-like7 distribution. Since a relatively small number of erroneous tokens already
covers a substantial number of all error occurrences, it should not be surprising that
even small error dictionaries help to identify pages with many errors. In Figure 12, we
show the logarithmic frequencies of errors in a 1.4-terabyte subcorpus retrieved from the
Web in 1999 (?Web-in-a-box?). The upper diagram shows the distribution of all errors
from the maximal English error dictionary, Derr(English,all), in English Web pages. Only
errors with at least two occurrences are covered. Similarly the lower diagram shows the
distribution of errors from Derr(German,all) in German Web pages.
9.2 Basic Filter Scenario
Suppose we are given a collection of Web pages, C. We may fix a user-defined threshold
? in terms of the average number of errors per 1,000 tokens that we are willing to accept
in a document to be added to our corpus. A document where the average number of
errors per 1,000 tokens does not exceed ? is called acceptable, other documents are
called unacceptable. In practice, since we cannot count real errors, a token is considered
erroneous if and only if it occurs in one of our error dictionaries. In Section 5, we have
seen that the number of entries of the error dictionary found in a text yields a lower
approximation for the real number of errors.
In terms of information retrieval, acceptable documents can be considered as rele-
vant documents that we would like to retrieve for ?query? ?. To extend this analogy,
we define the answer set of a filter F w.r.t. C as the set of all documents in C that are
passed by F . With these notions we may now define the parameters? precision and
recall.
Definition
Let ?, C, and F as above. The precision of F with respect to ? and C is the
percentage of acceptable documents in the answer set of F . The recall of F with
respect to ? and C is the number of acceptable documents in the answer set of F
divided by the number of all acceptable documents in C.
In the remainder of the section, we define and evaluate filters for the English and
German general HTML corpora, which are denoted CE and CG, respectively. We consider
three user-defined thresholds: ? = 10, ? = 5, and ? = 1. The first bound is meant to
exclude a small number of documents with an extraordinary number of orthographic
errors. The second bound is more ambitious. The third bound might be used in
7 Zipf?s law describes the frequency of words in large corpora. It states that the i-th most frequent word
appears as many times as the most frequent one divided by i?, for some constant ? ? 1.
329
Computational Linguistics Volume 32, Number 3
situations where high accuracy is needed and we want to retrieve only documents with
a negligible number of orthographic errors.
9.3 Automated Filter Construction
We define a hierarchy of filters
F1 = ?D1,?1?,F2 = ?D2,?2?,F3 = ?D3,?3?, . . .
Filters Fk with higher index k generally lead to better results. On the negative side, they
are more complex in terms of the number of entries of Dk. In the following description
we generally assume that a user-defined threshold ? has been fixed. For simplicity, we
refer to the construction of filters for the English corpus, CE. The same construction was
used, mutatis mutandis, for CG. All filters are computed automatically on the basis of
training data. For training, two inputs were used.
1. Ranked error list. We computed a list of all entries of the maximal English
error dictionary, Derr(English,all), that occur at least twice in the corpus
Web-in-a-box (cf. Section 9.1). The list was ordered by descending
frequency of occurrence, as in Figure 12. The resulting ranked error list
contains 1, 175, 894 entries.
2. 2. Training corpus. The corpus CE was randomly split into a training
subcorpus (427 documents) and a test subcorpus (407 documents).8
From the training corpus, all documents were excluded that did not
contain at least five distinct errors from the ranked error list, leaving
384 documents.
Definition of Filters. The error dictionary Dk for filter Fk was defined as the minimal
initial segment S of the ranked error list such that each unacceptable document in the
training corpus contains at least k distinct entries of the segment S. The threshold ?k
was defined as the minimal average number of occurrences of entries of Dk per 1,000
tokens in an unacceptable document of the training corpus. These entries need not
be distinct.
Clearly, with the given threshold we achieve a precision of 100% on the training
corpus. The philosophy behind this selection of a threshold is simple: We do not want
to add any unacceptable document to the corpus to be built. Precision is much more
important than recall, as long as a substantial number of documents is retrieved. As
a matter of fact, we cannot expect a 100% precision on the test corpus. However, our
results show that the loss of precision is not significant.
9.4 Filtering Results for English General HTML Corpus
In what follows we consider the three user-defined thresholds ? = 10, ? = 5, and ? = 1.
For each of the filters F1 = (D1,?1), . . . ,F5 = (D5,?5), as defined earlier, Table 28 shows
8 The distinct sizes of both corpora seem to indicate that the random selection was not perfectly balanced.
We ignored this problem, which does not influence the construction.
330
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
Table 28
Evaluation of filters Fk, 1 ? k ? 5, for English general HTML corpus, user-defined threshold
? = 10 (top), ? = 5 (middle), and ? = 1 (bottom).
|Dk| ?k PTrain (%) RTrain (%) PTest (%) RTest (%)
? = 10
k = 1 12,217 0.91 100.00 85.42 99.67 80.00
k = 2 21,037 1.83 100.00 89.79 99.69 84.73
k = 3 46,111 2.19 100.00 91.83 99.40 87.63
k = 4 110,201 4.63 100.00 93.87 99.71 91.31
k = 5 291,309 5.62 100.00 93.00 99.70 89.21
? = 5
k = 1 34,322 1.23 100.00 87.42 99.34 86.00
k = 2 47,747 2.19 100.00 95.70 98.50 94.00
k = 3 90,160 3.53 100.00 98.77 97.47 97.42
k = 4 110,201 3.71 100.00 98.77 97.70 97.42
k = 5 291,309 4.83 100.00 100.00 96.15 100.00
? = 1
k = 1 37,994 0.13 100.00 51.15 93.43 55.89
k = 2 169,507 0.49 100.00 78.35 96.75 78.16
k = 3 279,543 0.63 100.00 86.14 97.02 85.58
k = 4 299,397 0.67 100.00 90.90 97.10 87.77
k = 5 580,330 0.89 100.00 97.40 96.06 96.91
the size of the filter dictionary Dk (second column), the filter threshold ?k (third column),
and the precision and recall values achieved with the filter on the training and test
corpora (columns 4, 5, 6, 7).
Baselines. When treating the complete test corpus as a ?naive? answer set (recall 100%),
we obtain
1. for ? = 10, a precision of 94.76%, corresponding to 380 acceptable and 21
unacceptable documents,
2. for ? = 5, a precision of 87.28%, corresponding to 350 acceptable and 51
unacceptable documents.
3. for ? = 1, a precision of 57.10%, corresponding to 229 acceptable and 172
unacceptable documents.
For ? = 10, with a precision (recall) of 99.40% (87.63%) on the test corpus, the filter
F3 represents a good compromise between size and quality. Precision is almost optimal.
The answer set for the filter contains only one unacceptable document with an error rate
of 13.24, which is very close to the threshold.
For ? = 5, using the filter F3 we obtain a precision (recall) of 97.47% (97.42%). An
inspection of the nine unacceptable documents in the answer set of the filter shows that
they come very close to the bound ? = 5. Note that error dictionaries D1, D2, and D3
are larger than the corresponding dictionaries for the threshold k = 10 due to the larger
number of unacceptable documents in the training corpus.
For ? = 1, using the filter F3 we obtain a precision (recall) of 97.02% (85.58%). There
are six unacceptable documents in the answer set, all with an error rate below 2. The
numbers in Table 28 show how a more rigid (smaller) filter threshold compensates for
331
Computational Linguistics Volume 32, Number 3
the reduced size of error dictionaries essentially without sacrificing precision and with
a modest loss of recall. To illustrate the effect of filtering, yet from another perspective,
Figure 13 presents the distribution of error rates (number of entries from the maximal
English error dictionary Derr(English,all) per 1,000 tokens) in the answer set and in the
set of documents rejected by the filter F3 constructed for the user-defined threshold ? =
5. The filter was evaluated on the test subcorpus. The figure shows that almost all docu-
ments passed (rejected) by the filter have an error frequency below (beyond) 5 errors per
1,000 tokens.
9.5 Filtering Results for the German General HTML Corpus
For computing the ranked error list, a list with the frequencies of 18, 624, 436 tokens in
German Web pages was used. Via intersection with the list of all entries of the maximal
German error dictionary, Derr(German,all), we obtained a ranked error list with 454, 709
entries. The training and test corpora contain 314 and 308 documents, respectively, from
the German general HTML corpus. Since the results are similar to the English case,
we only point to some differences. Frequencies decrease more rapidly in the German
ranked error list, as may be seen in Figure 12. In the German list, the top-ranked part is
dominated by e/-transformation errors and errors where the letter ? is replaced by ss.
The 10 top-ranked entries and their frequencies are shown in Table 29. This special class
of frequent errors leads to small filter dictionaries. For example, the filter dictionary for
? = 10, k = 5 has 16,277 entries, and the dictionary for ? = 5, k = 5 has 127,023 entries.
On the other hand, the recall values achieved with the dictionaries in general are lower
than in the English case.
10. Example Applications
Obviously, the methods described above are very useful for all corpus tools that visually
present linguistic data from Web pages (words, n-grams, concordances, phrases, sen-
tences, aligned bilingual material, etc.) to the user. Filters help to exclude inappropriate
pages. In the remaining data, tokens that represent entries of the error dictionaries can
be marked. Depending on the application, the system may then decide to suppress this
material or to add a warning when presenting it. In the remainder of this section, two
case studies are presented that demonstrate the usefulness of filtering techniques and
error dictionaries in distinct applications.
10.1 Text Correction with Crawled Dictionaries
It has often been observed that fixed handcrafted dictionaries only have a modest
coverage when applied to new texts and corpora.9 Still, for various text processing tasks,
dictionaries with high coverage are needed. The generation of crawled dictionaries that
collect the vocabulary of appropriate Web pages is one way to obtain a better coverage.
As a matter of fact, the quality of these dictionaries suffers from orthographic errors in
the analyzed pages. Using the above filters helps to reduce the number of errors that are
9 Kukich (1992) describes an experiment by Walker and Amsler (1986): ?Nearly two thirds (61%) of the
words in the Merriam-Webster Seventh Collegiate Dictionary did not appear in an eight million word corpus
of New York Times news wire text, and, conversely, almost two-thirds (64%) of the words in the text were
not in the dictionary.?
332
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
Table 29
Top-ranked errors in German ranked error list and their frequencies.
Entry of error list Correct word Error frequency
Universitaet Universita?t 131,494
grossen gro?en 107,904
koennen ko?nnen 107,730
knnen ko?nnen (kennen?) 87,167
heisst hei?t 76,667
andern a?ndern (anderen?) 73,972
Gruss Gru? 51,721
ausser au?er 42,410
waere wa?re 37,071
muessen mu?ssen 35,864
imported. In order to further improve a crawled dictionary, we may either eliminate all
words that represent entries of the error dictionaries, or we may mark these words for
a manual inspection. In what follows we report on an experiment in the area of lexical
text correction where these techniques improved:
1. the quality of crawled dictionaries by avoiding erroneous entries,
2. the accuracy of lexical text correction achieved with these dictionaries,
using a high-level text correction system (Strohmaier et al 2003a, 2003b).
Correction Strategy. Ignoring details, we used the following correction strategy10: For
each token11 of the input text, the most similar words are retrieved from the dictionary
as a set of correction candidates. In many cases the token will be found in the dictionary
and represents a correction candidate with optimal similarity. Based on (1) the similarity
between text token and correction candidate and (2) the frequency of the correction
candidate in a corpus, each candidate receives a score. If the score of the best candidate
exceeds a given threshold ?, the token is replaced by this candidate. In the other case, the
token is left unmodified. A good balance between similarity and frequency information
in the score is obtained via training. The threshold, which is also optimized via training,
guarantees that the input token is only replaced if additional confidence is available
that the best correction candidate in fact represents the corrected version of the token.
In the experiment described below, the system was trained on a corpus for the domain
Mushrooms. The evaluation corpus is from the domain Fish. Hence, the two corpora
are disjoint and cover distinct thematic areas. More details on the correction system can
be found in Strohmaier et al (2003b).
Garbled Input Text for Correction. We collected 10 texts from the domain Fish, all
containing a nontrivial number of errors. Texts were retrieved from the Web, using
queries to Google with spelling errors, such as fish anglers infomation realy. We checked
that the texts do not contain paragraphs that are also found in the documents of the
corpora Fish E introduced in Section 2.2. The concatenation of the 10 texts was used as
10 To simplify evaluation, a fully automated variant of text correction was considered.
11 In what follows, by a token, we always mean a token composed of standard letters only.
333
Computational Linguistics Volume 32, Number 3
input to the text correction system. For the evaluation, a corrected version of the full text
was manually created. The full text contains 17,697 tokens of which 418 (2.36%) were
found to be erroneous.
Background Dictionaries for Correction. As a baseline, a first crawled dictionary
Dcrawl with 505,652 entries was built, collecting all words from the documents in the
corpus Fish E (1). A second dictionary D+Fcrawl used only those pages that were not
rejected by the filter for threshold ? = 2, based on the maximal English error dic-
tionary Derr(English,all).12 In this case, 324 documents passed the filter, whereas 186
were rejected. In this case we obtained 291,065 entries. Deleting in D+Fcrawl all words that
represent entries of Derr(English,all), a third dictionary D
+F+ED
crawl with 269,079 entries was
computed.
Note that we did not extend D+Fcrawl and D
+F+ED
crawl by analyzing an additional set of
filtered Web pages. Hence, D+Fcrawl is in fact a subdictionary of Dcrawl, and similarly for
D+F+EDcrawl and D
+F
crawl. This explains why the coverage of D
+F
crawl (D
+F+ED
crawl ) is smaller than
the coverage of Dcrawl (D
+F
crawl); see below. With an extended filtered crawl, even better
coverage and accuracy results would probably be possible.
Evaluation Results. We then compared the lexical coverage (percentage of tokens of
the correct version of the input text found in the dictionary) and correction accuracy
(percentage of correct tokens after automated correction) for each of the three dictio-
naries. The results are presented in Table 30. The accuracy of the input text is 97.64%.
The fifth column gives the improvement in accuracy, taking the input text as a baseline.
The last column mentions the number of erroneous tokens in the text that are found in
the respective error dictionary.
Note that the use of the filtered corpus leads to a measurable improvement in
correction accuracy. The second step in which we eliminate all entries of the error
dictionaries in the correction dictionary leads to an additional gain.
Overproduction and Underproduction of the Error Dictionary. As mentioned above,
418 tokens of the input text represented proper errors. From these, 254 (60.77%) turned
out to be entries of the maximal English error dictionary Derr(English,all). Note that
this value for underproduction is very compatible with our estimates in Section 5.
Remarkably, only seven of the correct tokens of the input text occurred in the error
dictionary.
Analyzing the Effect of Using Filters and Error Dictionaries. The most important error
source in the correction process are erroneous tokens of the text that?by accident?
represent entries of the crawled dictionaries. Using the above strategy, these false
friends are only replaced by another word w of the correction dictionary if overwhelm-
ing frequency information is available that leads to a preference of w after computing
the balanced score for similarity and frequency. The dictionary Dcrawl contains 262 of the
418 erroneous tokens of the text. The dictionary D+Fcrawl, which collects the vocabulary
of filtered pages, contains only 92 erroneous tokens. After eliminating all entries of the
maximal error dictionary, the new dictionary D+F+EDcrawl contains only 49 false friends.
Note that the latter tokens represent errors not contained in the error dictionary. A
very interesting additional number is the following: when eliminating in Dcrawl all
12 Other filter thresholds for ? = 1, 0.5, and 0 were also tested and led to very similar accuracy values.
334
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
Table 30
Measuring the quality of distinct dictionaries for text correction. Dcrawl is produced by an
unfiltered crawl, D+Fcrawl by a filtered crawl. For D
+F+ED
crawl , a filtered crawl is used and remaining
entries of error dictionaries are eliminated.
Dictionary Entries Coverage (%) Accuracy (%) ? (%) False friends
Dcrawl 505,652 99.08 98.45 0.81 262
D+Fcrawl 291,065 98.77 98.61 0.97 92
D+F+EDcrawl 269,079 98.75 98.74 1.10 49
the entries that are found in Derr(English,all), the resulting dictionary contains 105
erroneous tokens of the text. This shows that the filtering step eliminates 56 (= 105 ? 49)
erroneous tokens of the text that are not found in the error dictionary and proves that
a two-step procedure?first using filters for crawling pages, then eliminating entries of
error dictionaries afterwards?leads to optimal results.
10.2 Generating Translation Data from Parallel Corpora
Parallel texts represent an important resource for automatic acquisition of bilingual
dictionaries. Since only a small number of large parallel corpora are available, which
are moreover specialized both with respect to form and contents, the Web represents
an important archive for mining parallel texts (Resnik and Smith 2003). When building
up bilingual dictionaries for machine translation, or when presenting parallel phrases
to users, correctness is an important issue. Hence, it is interesting to see how error
dictionaries help to reduce errors in parallel corpora. Our methods can be applied to
any kind of parallel corpus. For our experiments we used the freely available Europarl
corpus.13 The corpus covers the proceedings of the European Parliament 1996?2001 in
11 official languages of the European Union. We only analyzed the English and German
versions of the parallel texts. The 488 documents in the corpus are of an excellent quality.
Our goal was to find English and German texts with a nontrivial number of errors (if
any) and to detect these errors. Since the overproduction of error dictionaries in very
accurate texts is high, the problem is challenging. The maximal error dictionaries for
the two languages were used to determine the error rate of each document. Table 31
shows the twenty documents with the highest error rates for both the English and the
German subcollection of the corpora. Columns 4 and 5 describe the number of tokens
that represent entries of the respective error dictionary and the number of real errors
among these hits. The results show that when analyzing very accurate texts, the error
rate is not always a safe indicator for a corresponding number of real errors. Still, the
experiment isolates 246 real errors, only looking at 40 documents. When collecting
translation correspondences, we may simply discard all phrases/sentences with a hit
in an error dictionary, together with their aligned counterparts. Many translation pairs
with errors will be avoided. Given the length of the documents, the number of hits of
the error dictionaries is small, hence the loss of recall is not essential. In this way our
13 The corpus, which was also used by Koehn, Och, and Marcu (2003), is available at
http://www.isi.edu/k?oehn/europarl/.
335
Computational Linguistics Volume 32, Number 3
Table 31
English (E) and German (G) documents of the Europarl corpora, sizes, error rates w.r.t. maximal
English and German error dictionaries, numbers of hits of the error dictionaries, and numbers of
real errors among hits.
Documents Tokens Error rate Hits Real errors Percentage
ep-96-09-20.txt (E) 9,945 1.31 13 2 15.38
ep-97-04-24.txt (E) 8,074 0.99 8 8 100.00
ep-97-09-19.txt (E) 3,230 0.93 3 0 0.00
ep-97-02-21.txt (E) 5,830 0.86 5 5 100.00
ep-99-01-28.txt (E) 5,347 0.75 4 0 0.00
ep-97-06-25.txt (E) 20,012 0.70 14 11 78.57
ep-96-07-19.txt (E) 4,383 0.68 3 3 100.00
ep-97-04-23.txt (E) 21,930 0.64 14 14 100.00
ep-97-12-04.txt (E) 9,463 0.63 6 6 100.00
ep-99-02-12.txt (E) 5,426 0.55 3 3 100.00
ep-00-03-29.txt (E) 22,252 0.54 12 12 100.00
ep-96-07-17.txt (E) 34,381 0.52 18 14 77.77
ep-99-03-10.txt (E) 31,509 0.51 16 0 0.00
ep-00-11-15.txt (E) 35,167 0.48 17 1 5.88
ep-97-04-10.txt (E) 16,653 0.48 8 6 75.00
ep-97-05-15.txt (E) 20,942 0.48 10 2 20.00
ep-97-10-20.txt (E) 8,601 0.46 4 4 100.00
ep-97-04-11.txt (E) 6,857 0.44 3 1 33.33
ep-99-01-15.txt (E) 9,193 0.43 4 0 0.00
ep-96-06-18.txt (E) 32,768 0.43 14 6 42.86
ep-03-01-13.txt (G) 15,926 2.57 41 2 4.89
ep-97-05-16.txt (G) 12,344 1.94 24 15 62.50
ep-02-09-02.txt (G) 14,845 1.62 24 1 4.16
ep-98-11-05.txt (G) 15,035 1.46 22 3 13.64
ep-99-01-28.txt (G) 6,798 1.32 9 0 0.00
ep-02-04-25.txt (G) 10,842 1.29 14 4 28.57
ep-97-10-02.txt (G) 13,650 1.25 17 9 52.94
ep-99-07-20.txt (G) 2,431 1.23 3 0 0.00
ep-00-03-15.txt (G) 34,904 1.20 42 31 73.81
ep-96-06-21.txt (G) 8,474 1.18 10 9 90.00
ep-96-06-17.txt (G) 9,408 1.17 11 2 18.18
ep-99-04-16.txt (G) 8,667 1.15 10 9 90.00
ep-96-04-19.txt (G) 8,694 1.15 10 2 20.00
ep-00-12-15.txt (G) 6,964 1.15 8 3 37.50
ep-00-09-08.txt (G) 4,374 1.14 5 0 0.00
ep-96-07-04.txt (G) 10,975 1.09 12 11 91.66
ep-01-04-05.txt (G) 26,941 1.08 29 20 68.96
ep-97-06-09.txt (G) 11,152 1.08 12 12 100.00
ep-97-07-14.txt (G) 11,180 1.07 12 5 41.66
ep-97-07-18.txt (G) 10,392 1.06 11 10 90.90
methods may help to improve the generation of translation data even from collections
of very accurate parallel texts.
11. Conclusion
In this article we investigated the distribution of orthographic errors of distinct types in
the English and German Web. Experiments based on a variety of very large error dic-
tionaries showed that Web corpora typically contain a non-negligible number of pages
336
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
with an unacceptable number of orthographic errors. Typing errors represent the most
important subclass. In German Web pages, errors resulting from character encoding
problems represent another important category. In our experiments, PDF documents
were found to contain less orthographic errors than HTML documents, and corpora
covering specific thematic areas were found to contain more errors than collections of
?general? pages without such a focus. Some differences were remarkable; in particular,
our corpora for special thematic areas related to hobbies contain many pages with a
high number of orthographic errors. We also found that mean error rates are influenced
by the collection strategy. Specific crawling strategies help to avoid chat and junk while
attracting professional documents. Since document genre and error rates are correlated,
refined crawling strategies may help to reduce mean error rates.
Error dictionaries, even subdictionaries of modest size, can be used as filters that
help to detect and eliminate pages with many orthographic errors. Filters with user-
defined thresholds work well for both languages. Obviously, the possibility of deleting
pages with many orthographic errors and of marking all entries of error dictionaries
in the remaining documents opens a wide range of interesting applications in distinct
areas of corpus linguistics. To exemplify possible applications we showed how to im-
prove the quality of Web-crawled dictionaries for text correction. With these filtered dic-
tionaries, higher values for correction accuracy were obtained than with those directly
obtained from Web crawls. In a second experiment, we showed how error dictionar-
ies may be used to improve the automated collection of translation correspondences,
avoiding translation pairs with orthographic errors.
Going beyond corpus linguistics, it might be interesting to design (special modes of)
Web search engines where the error rate of a given document is used as one parameter
in the ranking of answers. In many search scenarios, answer documents with a large
number of orthographic errors appear to be less reliable, and the user might wish to
concentrate on ?professional? or carefully edited Web pages.
In our practical work we found that the collection and analysis of very large Web
corpora is difficult for many reasons. For example, it is not clear how to treat pages
with artificial vocabulary that is only introduced to obtain a better ranking. We learned
that often these junk lists are intensionally enriched with many orthographic errors to
obtain a better ranking, in particular for erroneous queries. In our experiments, some
of these pages were found immediately, looking at error rates, and excluded. Later,
when inspecting documents for genre classification, other less eye-catching examples
were found. Some portions of text occurred in several documents. The conversion of
Web pages into ASCII represents a potential source for new errors. In particular the
conversion of German PDF documents to ASCII turned out to be very error prone.
Nonstandard vocabulary (special names, foreign language expressions, archaic lan-
guage, programming code, slang, etc.) is another source that makes various pages
inappropriate for corpus construction.
One step for future work is the development of special dictionaries for frequent
foreign language expressions, archaic language, programming code, and slang. Special
dictionaries for these expressions would not only help to detect and exclude pages with
a high amount of nonstandard vocabulary, but they could also be used as additional
filters in the construction of error dictionaries. The results in Section 5.2 indicate that
the overproduction of our error dictionaries could be reduced in a significant way by
eliminating entries that represent expressions of the earlier-mentioned type. As a matter
of fact, new types of spelling errors were found during the experiments described
earlier. It might be interesting to enlarge the error dictionaries for spelling errors, taking
the new patterns into account.
337
Computational Linguistics Volume 32, Number 3
We also found that enlarged error dictionaries that store with each garbled entry
the correct word from which it was derived are very useful for error correction. In
contrast to our first intuitions, the number of ambiguities arising from this correction
strategy is small, and the predictive power of enlarged error dictionaries is high.
More details on text correction with error dictionaries will be given in a forthcoming
paper.
Acknowledgments
The authors thank the anonymous referees of
Computational Linguistics. Their remarks and
suggestions helped to improve the contents
and presentation of the article. Special thanks
to Annette Gotscharek and Uli Reffle for all
their help.
References
Amengual, Juan Carlos and Enrique Vidal.
1998. Efficient error-correcting viterbi
parsing. IEEE Transactions on PAMI,
20(10):1?109.
Baroni, Marco and Silvia Bernardini. 2004.
BootCaT: Bootstrapping corpora and terms
from the web. In Proceedings of LREC 2004,
pages 1313?1316, Lisbon.
Boutsis, Sotiris, Stelious Piperidis, and Iason
Demiros. 1999. Generating translation
lexica from multilingual texts. Applied
Artificial Intelligence, 13(6):583?606.
Brin, Sergey and Lawrence Page. 1998. The
anatomy of a large-scale hypertextual Web
search engine. Computer Networks and
ISDN Systems, 30:107?117.
Brown, Jonathan and Maxine Eskenazi. 2004.
Retrieval of authentic documents for
reader-specific lexical practice. In
Proceedings of the InSTIL/ICALL2004
Symposium on Computer Aided Language
Learning, pages 25?28, Venice.
Chelba, Ciprian and Frederick Jelinek.
2002. Recognition performance of
a structured language model. In
Proceedings of Sixth European Conference
on Speech Communication and Technology
(EUROSPEECH?99), pages 1567?1570,
Budapest.
Church, Kenneth W. and Robert L. Mercer.
1993. Introduction to the special issue on
computational linguistics using large
corpora. Computational Linguistics,
19(1):1?24.
Dimitrova, Maya, Nicholas Kushmerick,
Petia Radeva, and Joan Jose Villanueva.
2003. User assessment of a visual Web
genre classifier. In Third International
Conference on Visualization, Imaging, and
Image Processing, Malaga.
Dunning, Ted. 1993. Accurate models for the
statistics of surprise and coincidence.
Computational Linguistics, 19(1):61?74.
Finn, Aidan and Nicholas Kushmerick. 2003.
Learning to classify documents according
to genre. In IJCAI-03 Workshop on
Computational Approaches to Text Style and
Synthesis, Acapulco. Journal of the American
Society for Information Science and
Technology (in press).
Fletcher, William H. 2004a. Facilitating the
compilation and dissemination of ad-hoc
web corpora. In Guy Aston, Silvia
Bernardini, and Dominic Stewart, editors,
Corpora and Language Learners, number 17
in Studies in Corpus Linguistics. John
Benjamins Publishing Company,
Amsterdam.
Fletcher, William H. 2004b. Making the web
more useful as a source for linguistic
corpora. In U. Connor and T. Upton,
editors, Corpus Linguistics in North America
2002. Rodopi, Amsterdam.
Gaizauskas, Robert, George Demetriou, and
Kevin Humphreys. 2000. Term recognition
in biological science journal articles. In
Proceedings of the Workshop on
Computational Terminology for Medical and
Biological Applications, 2nd International
Conference on Natural Language Processing
(NLP-2000), pages 37?44, Patras.
Gale, William A. and Kenneth W. Church.
1991. Identifying word correspondences
in parallel texts. In Proceedings of
Fourth DARPA Workshop on Speech
and Natural Language, pages 152?157,
Pacific Grove, CA.
Gartner, Hans-Ju?rgen. 2003. Extraktion
von semantischer Information aus
Layout-orientierten Daten. Master?s
thesis, Technical University of Graz.
Grefenstette, Gregory. 1992. Use of syntactic
context to produce term association lists
for text retrieval. In Proceedings of the 15th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, pages 89?97, Copenhagen.
Grefenstette, Gregory. 1999. The WWW as a
resource for example-based MT tasks.
Paper presented at ASLIB ?Translating
and the Computer? conference, London.
338
Ringlstetter, Schulz, and Mihov Orthographic Errors in Web Pages
Grefenstette, Gregory. 2001. Very large
lexicons. In Computational Linguistics in the
Netherlands 2000: Selected Papers from the
Eleventh CLIN Meeting, Language and
Computers, Amsterdam.
Guenthner, Franz. 1996. Electronic lexica and
corpora research at CIS. International
Journal of Corpus Linguistics, 1(2):287?301.
Jelinek, Frederick. 1997. Statistical Methods
for Speech Recognition. MIT Press,
Cambridge, MA.
Kehoe, Andrew and Antoinette Renouf. 2002.
WebCorp: Applying the web to linguistics
and linguistics to the Web. In Poster
Proceedings of the 11th International World
Wide Web Conference, WWW02, Honolulu.
Kessler, Brett, Geoffrey Nunberg, and
Hinrich Schu?tze. 1997. Automated
detection of text genre. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics and the 8th
Meeting of the European Chapter of the
Association for Computational Linguistics,
pages 32?38, Madrid.
Kilgarriff, Adam and Gregory Grefenstette.
2003. Introduction. Computational
Linguistics?Special Issue on the Web as
Corpus, 29(3):333?348.
Koehn, Philipp, Franz Josef Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of the Human
Language Technology and North American
Association for Computational Linguistics
Conference (HLT/NAACL), Edmonton.
Kukich, Karen. 1992. Techniques for
automatically correcting words in
texts. ACM Computing Surveys,
24(4):377?439.
Kumano, Akira and Hideki Hirakawa. 1994.
Building a MT dictionary from parallel
texts based on linguistic and statistical
information. In Proceedings of the 15th
International Conference on Computational
Linguistics (COLING?94), pages 76?81,
Kyoto.
Kupiec, Julian. 1993. An algorithm for
finding noun phrase correspondences in
bilingual corpora. In Proceedings of the 31st
Annual Meeting of the Association for
Computational Linguistics (ACL?93),
pages 17?22, Columbus, OH.
Lin, Shian-Hua, Chi-Sheng Shih,
Meng Chang Chen, Jan-Ming Ho,
Ming-Tat Ko, and Yueh-Ming Huang.
1998. Extracting classification knowledge
of internet documents with mining term
associations: A semantic approach. In
Proceedings of the 21st International ACM
SIGIR Conference on Research and
Development in Information Retrieval, pages
241?249, Melbourne, Australia.
Maier-Meyer, Petra. 1995. Lexikon und
automatische Lemmatisierung. Ph.D. thesis,
CIS, University of Munich.
Morley, Barry, Antoinette Renouf, and
Andrew Kehoe. 2003. Linguistic research
with the XML/RDF aware WebCorp tool.
In Poster Proceedings of the 12th International
World Wide Web Conference, WWW03,
Budapest.
Oh, Alice H. and Alexander I. Rudickny.
2000. Stochastic language generation
for spoken dialogue systems. In
ANLP/NAACL 2000 Workshop on
Conversational Systems, pages 27?32,
Seattle.
Ostendorf, Mari, Vassilios V. Digalakis, and
Owen A. Kimball. 1996. From HMMs to
segment models: A unified view of
stochastic modeling for speech
recognition. IEEE Transactions Speech and
Audio Processing, 4(5):360?378.
Resnik, Philip and Noah A. Smith. 2003. The
web as a parallel corpus. Computational
Linguistics - Special Issue on the Web as
Corpus, 29(3):349?380.
Ringlstetter, Christoph. 2003. OCR-
Korrektur und Bestimmung von
Levenshtein-Gewichten. Master?s
thesis, LMU, University of Munich.
Schwartz, Lee, Takako Aikawa, and
Michel Pahud. 2004. Dynamic language
learning tools. In Proceedings of the
InSTIL/ICALL2004 Symposium on
Computer Aided Language Learning,
pages 107?110, Venice.
Smadja, Frank A. and Kathleen R. McKeown.
1990. Automatically extracting and
representing collocations for language
generation. In Proceedings of the 28th Annual
Meeting of the Association for Computational
Linguistics, pages 252?259, Pittsburgh, PA.
Sornlertlamvanich, Virach and Hozumi
Tanaka. 1996. The automatic extraction
of open compounds from text corpora.
In Proceedings of the 16th Conference on
Computational Linguistics, pages 1143?1146,
Copenhagen.
Strohmaier, Christian, Christoph Ringlstetter,
Klaus U. Schulz, and Stoyan Mihov. 2003a.
Lexical postcorrection of OCR-results: The
web as a dynamic secondary dictionary?
In Proceedings of the Seventh International
Conference on Document Analysis and
Recognition (ICDAR 03), pages 1133?1137,
Edinburgh.
Strohmaier, Christian, Christoph Ringlstetter,
Klaus U. Schulz, and Stoyan Mihov.
339
Computational Linguistics Volume 32, Number 3
2003b. A visual and interactive tool for
optimizing lexical postcorrection of
OCR results. In Proceedings of the IEEE
Workshop on Document Image Analysis
and Recognition, DIAR?03, Madison, WI.
Taghva, Kazem and Jeff Gilbreth. 1999.
Recognizing acronyms and their
definitions. International Journal on
Document Analysis and Recognition,
1(4):191?198.
Walker, Donald E. and Robert A. Amsler.
1986. The use of machine-readable
dictionaries in sublanguage analysis. In
Analyzing Language in Restricted Domains:
Sublanguage Description and Processing.
Lawrence Erlbaum, Hillsdale, NJ,
pages 69?83.
Way, Andy and Nano Gough. 2003.
wEBMT: Developing and validating
an example-based machine translation
system using the world wide web.
Computational Linguistics?Special Issue
on the Web as Corpus, 29(3):421?458.
Yeates, Stuart, David Bainbridge, and
Ian H. Witten. 2000. Using compression
to identify acronyms in text. In Proceedings
of the Conference on Data Compression,
page 582, Snowbird, UT.
340
