Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 917?927, Dublin, Ireland, August 23-29 2014.
Triple based Background Knowledge Ranking for Document Enrichment
Muyu Zhang, Bing Qin
?
, Ting Liu, Mao Zheng
Research Center for Social Computing and Information Retrieval
Harbin Institute of Technology, China
{myzhang,qinb,tliu,mzheng}@ir.hit.edu.cn
Abstract
Document enrichment is the task of retrieving additional knowledge from external resource over
what is available through source document. This task is essential because of the phenomenon
that text is generally replete with gaps and ellipses since authors assume a certain amount of
background knowledge. The recovery of these gaps is intuitively useful for better understanding
of document. Conventional document enrichment techniques usually rely on Wikipedia which
has great coverage but less accuracy, or Ontology which has great accuracy but less cover-
age. In this study, we propose a document enrichment framework which automatically extracts
?argument
1
, predicate, argument
2
? triple from any text corpus as background knowledge, so
that to ensure the compatibility with any resource (e.g. news text, ontology, and on-line ency-
clopedia) and improve the enriching accuracy. We first incorporate source document and back-
ground knowledge together into a triple based document-level graph and then propose a global
iterative ranking model to propagate relevance score and select the most relevant knowledge
triple. We evaluate our model as a ranking problem and compute the MAP and P&N score to
validate the ranking result. Our final result, a MAP score of 0.676 and P&20 score of 0.417
outperform a strong baseline based on search engine by 0.182 in MAP and 0.04 in P&20.
1 Introduction
Document enrichment is the task to acquire background knowledge from external resources and recover
the omitted information automatically for certain document. This task is essential because authors usu-
ally omit basic but well-known information to make the document more concise. For example, author
omits ?Baghdad is the captain of Iraqi? in the text of Figure 1 (a), which is well-known to readers. Dur-
ing reading process, these gaps will be automatically plugged effortlessly by the background knowledge
in human brain. However, the situation is different for machine because it lacks the ability to acquire
and select the proper background knowledge, which limits the performances of certain NLP applica-
tions. Document enrichment has been proved helpful in these tasks such as web search (Pantel and
Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity
disambiguation (Bunescu and Pasca, 2006; Sen, 2012).
In the past, there are mainly two kinds of document enrichment researches according to the resource
they relying on. The first line of works make use of WikiPedia, the largest available on-line encyclopedia
as resource and link the entity (e.g. Baghdad) of document to its corresponding Wiki page (e.g. Baghdad
1
in WikiPedia), so that to enrich the document with the context of Wiki page (Bunescu and Pasca, 2006;
Cucerzan, 2007; Han et al., 2011; Kataria et al., 2011; Sen, 2012; He et al., 2013). Despite the great
success of these methods, there remain a great challenge that not all information in the linked Wiki page
is helpful to the understanding of corresponding document. For example, the Wiki page of Baghdad
contains lots of information about city history and culture, which are not quite relevant to the semantic of
context in Figure 1 (a). So treating the whole Wiki page as the enrichment to document may cause noise
?
Corresponding author.
This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer
are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/
1
http://en.wikipedia.org/wiki/Baghdad
917
S1: Coalition may never know if   Iraqi   president   Saddam 
Hussein survived a U.S. air strike yesterday.
S2: A B-1 bomber dropped four 2,000-pound bombs on a building 
in a residential area of   Baghdad  . 
S3: They had got an intelligence reports senior officials were 
meeting there, possibly including Saddam Hussein and  his sons .
BaghdadIraqi hasCapital
Saddam Hussein diedIn
Qusay HusseinSaddam Hussein hasChild
Kadhimiya
k1:
k2:
k3:
Global 
Ranking
(a) Source document (b) Top-3 background knowledge
S1: Coalition may never know if   Iraqi   president   Saddam Hussein 
survived a U.S. air strike yesterday.
S2: A B-1 bomber dropped four 2,000-pound bombs on a building 
in a residential area of   Baghdad  . 
S3: They had got an intelligence reports senior officials were 
meeting there, possibly including  Saddam Hussein  and   his sons .
(a) Source document (b) Two relevant background knowledge
Iraq
Baghdad
Saddam Hussein
Captain
Died In Kadhimiya
Global 
Ranking
k1:
k2:
Figure 1: An example of document enrichment with background knowledge: (a) source document talking
about a U.S. air strike aiming at Saddam in Baghdad (b) two important relevant information, which is
omitted in source document but acquired by our model and enriched as background knowledge .
problem. Another line of works rely on the Ontologies constructed with supervision or even manually
which have great accuracy but less coverage (Motta et al., 2000; Passant, 2007; Fodeh et al., 2011; Kumar
and Salim, 2012). Besides, these methods usually rely on special ontology which is rather difficult to
construct and in turn limits the coverage and application of these methods.
Ideally, we would wish to integrate both coverage and accuracy, where an triple based background
knowledge ranking model may help. Our framework extracts knowledge from any corpus resource in-
cluding WikiPedia to ensure coverage and present knowledge as ?argument
1
, predicate, argument
2
?
triple to reduce noise. This model ranks background knowledge triples according to their relevance to
the source document. The key idea behind the model is that document is constructed by several units of
information, which can be extracted automatically. For every background knowledge b extracted auto-
matically from a relevant corpus, the more units are relevant to b and the more important they are, the
more relevant b becomes to the source document. Thus, we extract both source document information
and background knowledge automatically and present them together in a document-level graph. Then
we propagate the relevance score from the source document information to the background knowledge
during an iterative process. After convergence, we obtain the Top n relevant background knowledge,
rather than retrieving all of them without filtering.
To evaluate our model, we use ACE
2
corpus as source documents and output the ranked list of back-
ground knowledge. Then we train three annotators to check the ranking result and annotating whether
certain knowledge is relevant to corresponding source document separately. We totally annotated more
than 7000 background knowledge by three annotators. We evaluate their annotation consistence by com-
puting the Fleiss
?
Kappa (Fleiss, 1971), a famous criterion in multi-annotator consistence evaluation.
We achieve a Fleiss
?
Kappa of value 0.8066 in best situation and 0.7076 in average, which indicates
the great consistence between three annotators. The ranking result is evaluated with MAP score and
P&N score (Voorhees et al., 2005). We finally achieve aMAP score of 0.676 and P&20 score of 0.417
in Top 20 background knowledge, which are higher by 0.182 and 0.04 than a strong baseline based
on search engine. We also evaluate the effect of the automatically extraction to source document and
background knowledge, which is key to the performance of our method in real application.
2 Triple Graph based Document Representation
We believe that different parts of document are related to each other, rather than isolated. Hence, we
propose a triple graph based document representation to incorporate source document information and
background knowledge. In this presentation, ?argument
1
, predicate, argument
2
? triple serves as node
and the edge between nodes indicates their semantic relevance. In this part, we introduce triple graph
and the way to extract source document information and background knowledge automatically.
2.1 Motivation for triple presentation
Compared to Wiki Page, triple based enrichment helps to reduce noise illustrated in Section 1. Compared
to bag of words, triple based presentation help to reduce ambiguity of single word which is shown in
2
http://catalog.ldc.upenn.edu/LDC2006T06
918
B ar ac k O bam a
H ar v ar d  U niv e r s it y W h it e  H ou s e
(a) Ambiguity (b) Disambiguation
B ar ac k O bam a,  e a r n ,  l a w  d e g r e e
H ar v ar d  U niv e r s it y W h it e  H ou s e
Figure 2: The motivation for the form of triple (a) relevance ambiguity of single word Obama, which
is related to Harvard and White House (b) disambiguation with the help of other triple elements, where
?earn, law degree? help to limit Obama to the graduate of Harvard.
Figure 2. Figre 2 (a) shows that the single word of Obama is related to multiple semantic information
such as Harvard University as a law graduate and White House as the president. After introducing the
information from other elements of the triple, ?earn, law degree? help to disambiguate and limit Obama
to the law graduate of Harvard University only in Figure 2 (b). The form of triple has been used as the
presentation of knowledge in some researches such as knowledge base (Hoffart et al., 2013).
2.2 Nodes in the Graph
There are two kinds of nodes in the triple graph: source document nodes (sd-nodes) and background
knowledge nodes (bk-nodes). Both of them are extracted automatically with Open Information Ex-
traction (Open IE) technology which focuses on extracting assertions from massive corpora without a
pre-specied vocabulary (Banko et al., 2007). Open IE systems are unlexicalized-formed only in terms of
syntactic tokens and closed-word classes, instead of specific nouns and verbs at all costs.
There are existing Open IE systems such as TextRunner (Banko et al., 2007), WOE (Wu and Weld,
2010), and StatSnowball (Zhu et al., 2009). The output of these systems has been used to support many
NLP tasks such as learning selectional preference (Ritter et al., 2010), acquiring sense knowledge (Lin
et al., 2010), and recognizing entailment (Schoenmackers et al., 2010). In this work, we use the famous
Open IE system Reverb (Etzioni et al., 2011), which is generated from TextRunner (Etzioni et al., 2008),
to extract source document information and background knowledge automatically. We use the newest
version of ReVerb (version 1.3) without modification, which is free download on-line
3
.
Source document node (sd-node) Sd-nodes consists of the information extracted from source docu-
ment automatically by open information extraction technology (Banko et al., 2007), especially Reverb,
the famous Open IE system developed by University of Washington (Etzioni et al., 2011). The output
of ReVerb is formed as ?argument
1
, predicate, argument
2
?, which is naturally presented as triple. In
this study, we use ACE corpus as source documents and all sd-nodes are extracted by ReVerb. The setup
of automatic extraction makes our method usable in many real applications. To evaluate the effect of
automatic extraction, we also use the golden annotation within ACE (Doddington et al., 2004) corpus as
source document information and compare the performance that with automatic extraction.
Background knowledge node (bk-node) Bk-nodes consist of the background knowledge extracted
from external corpus resources automatically by Reverb too. We do not rely on certain existed knowl-
edge base and extract background knowledge from external corpus resources for corresponding source
document. This setup makes our methods usable in many real applications. Although we do not rely on
special knowledge base, we do adapt our method for the existed knowledge base such as YAGO (Hoffart
et al., 2013) and compare the performance to evaluate the effect of different knowledge sources.
2.3 Edges in the Graph
The edges between two nodes indicate their semantic relevance, which is evaluated in Section 3.1. There
are two kinds of edges: (1) sd-node to sd-node (2) sd-node to bk-node, both of them are undirected.
Considering all the relevance score originating from sd-nodes, we connect no edge between bk-nodes.
3
http://reverb.cs.washington.edu/
919
Edges between sd-nodes All sd-nodes are extracted from the same document, so they should be related
to each other. We connect each pair of sd-nodes with an edge and set the weight of edge as their semantic
relevance computed in Section 3.1. With this setup, we combine the source document as a whole where
different parts affect each other through the edge.
Edges between sd-node and bk-node The basic idea of our model is to propagate relevance score
from the sd-nodes to bk-nodes. Hence, we connect each pair of sd-node and bk-node with an edge and
set the weight of the edge as their relevance computed in Section 3.1. These edges are all undirected,
which indicates that bk-nodes also affect the relevance score of the sd-nodes during the ranking process.
3 Global Ranking Model
In this study, source document D is presented as the graph of sd-nodes. For every background knowledge
b, the task of evaluating the relevance between b and D is naturally converted into evaluating the relevance
between b and the graph of sd-nodes. So the relevance between b and document D can be computed by
propagating the relevance score from every sd-node of D to b iteratively. After the convergence, the
relevance between b and D can be evaluated by the relevance score of b. Intuitively, three factors affect
their relevance:
? How many sd-nodes is b relevant to ?
? How relevant is b to these sd-nodes?
? How important are these sd-nodes ?
For the first factor, b should be more relevant to source document D if more sd-nodes are relevant
to b. We capture this information by allowing b to receive relevance score from all the sd-nodes. For
the second factor, b should be more relevant to D if more relevant b is to sd-nodes. We consider this
information by evaluating the relevance between b and every sd-node (Section 3.1). For the last factor,
important sd-nodes should have higher impact. We consider this information by evaluating the impor-
tance of sd-nodes and assigning higher initial value to importance ones (Section 3.3). We combine all
factors in the global ranking process to select the top-n relevant background knowledge (Section3.2).
3.1 Relevance Evaluation between Nodes
In this section, we evaluate the semantic relevance between different nodes which is the weight of the
edge between them. We introduce Search Engine as a resource, which has been proven effective in
relevance evaluation (Gligorov et al., 2007). This method is motivated by the phenomenon that the
number of results returned by search engine for query p ? qindicates the relevance between p and q.
However, considering the different popularization of queries, this number alone can not accurately
express their semantic relevance. For example, query car ? automobile gets 294, 300, 000 results,
whereas query car?apple gets 683, 000, 000, which is 2 times higher than the previous one. Obviously,
automobile is more relevant to car rather than Apple. The reason of this phenomenon is that apple
is far more popular than automobile, which increase its possibility of co-occurrence with car. So we
consider the number of results for p?q together with p and q withWebJaccard Coefficient (Bollegala
et al., 2007) to evaluate the relevance between p and q according to Formula 1, where H(p), H(q), and
H(p ? q) indicate the number of results for query p, p, and p ? q.
WebJaccard(p, q) =
{
0 if H(p ? q) ? C
H(p?q)
H(p)+H(q)?H(p?q)
otherwise.
(1)
To convert one ?argument
1
, predicate, argument
2
? triple into query, we use argument
1
?
argument
2
as the query for one triple. We have tried argument
1
? predicate ? argument
2
which
920
is usually very sparse. Besides, the combination of two arguments usually maintain better semantic com-
pleteness of triple compared to other combinations according to our analysis. So this setup aims to bal-
ance completeness and sparseness. Accordingly, two triples are combined as argument
1
?argument
2
?
argument
?
1
? argument
?
2
. Considering the scale and noise in the Web data, it is possible for two words
to appear together accidentally. To reduce the adverse effects attributed to random co-occurrences, we
set 0 to the WebJaccard Coefficient of query p? q, if the number of result is less than a threshold C.
3.2 Iterative Relevance Propagation
Here we propose the relevance propagation based iterative process to evaluate the relevance between cer-
tain background knowledge and source document. Note that standard label propagation mainly focuses
on classification task (Wang and Zhang, 2008). However, we focus on a ranking problem where the best
ranking result is computed during an iterative process in this study. So we make two modifications to
suit the ranking problem better: not reseting the relevance score and introducing the propagation between
source document information during iteration.
Propagation possibility The edge between node
i
and node
j
is weighted by r(i, j) to measure their
relevance. However, r(i, j) cannot completely present the propagation possibility because one node can
be equally relevant to all of its neighbors. Thus, we define p(i, j) based on r(i, j) in formula 2 to indicate
the propagation possibility between node
i
and node
j
.
p(i, j) =
r(i, j)? ?(i, j)
?
k?N
r(k, j)? ?(k, j)
(2)
N is the set of all nodes, ?(i, j) denotes whether an edge exists between node
i
and node
l
in the triple-
graph or not, which indicates whether they may propagate to each other or not. E is the set of edges.
?(i, j) =
{
1 if (i, j) ? E
0 otherwise
(3)
Iterative propagation There are n ? n pairs of nodes, the p(i, j) of them is stored in a matrix P .
we use
~
W = (w
1
, w
2
, ? ? ? , w
n
) to denote the relevance score of all nodes, in which w
i
indicates the
relevance between node
i
and source document D. Here the node
i
can indicate both sd-nodes and bk-
nodes because they are processed during one fellow step. So that we keep updating both sd-nodes and
bk-nodes and do not distinguish them explicitly. The only difference between them is that we initialize
the w
i
of sd-nodes as its importance to D (Section 3.1) while bk-nodes as 0 at the beginning. We use
matrix P together with ?(i, j) to compute the
~
W during a iterative process, where
~
W is updated to
~
W
?
during the end of every iteration. The matrix
~
W
?
is updated according to the following Formula 4:
~
W
?
=
~
W ? P
=
~
W ?
?
?
?
?
p(1, 1) p(1, 2) ? ? ? p(1, n)
p(2, 1) p(2, 2) ? ? ? p(2, n)
? ? ? ? ? ? ? ? ? ? ? ?
p(n, 1) p(n, 2) ? ? ? p(n, n)
?
?
?
?
(4)
each w
i
in
~
W is updated to w
?
i
according to the formula 5, where w
i
is propagated from all the other
w
j
(j 6= i) according to their propagation possibility p(j, i). We also introduce the propagation from
bk-nodes to sd-nodes, where bk-nodes serve as intermediate to help mining latent semantics.
w
?
i
= w
1
? p(1, i) + w
2
? p(2, i) + ? ? ?+ w
n
? p(n, i)
=
?
k?N
w
k
? p(k, i)
=
?
k?N
w
k
?
(
r(i, j)? ?(i, j)
?
k?N
r(k, j)? ?(k, j)
)
(5)
921
3.3 Importance Evaluation for sd-nodes
The main idea of our model is to propagate relevance score from sd-nodes to bk-nodes (Section 3.2).
So the initialization of sd-node is important, which indicates the importance of different source docu-
ment information. This section solves this problem by evaluating the importance of sd-nodes to source
document. We use v
j
to denote the initialization of sd-nodes, which indicates the importance of node
j
(node
j
? set of sd-nodes) to source document. In this section, we propose a modified relevance propa-
gation method to evaluate v
j
for sd-notes. We first construct a triple-graph consisting of sd-nodes only.
Then we initialize the relevance score of sd-nodes according to a simple approach based on text fre-
quency (Kohlsch?utter et al., 2010). We use similar relevance propagation process without resetting the
relevance score at the beginning of every iteration, until a global stable state is achieved. Finally, we
normalize all the relevance scores to get
~
V , which indicates the importance of sd-nodes to the source
document. We return
~
V to the global ranking model (Section 3.2) as part of the input. The initial impor-
tance of bk-nodes is set as 0 at the beginning, which denotes that all bk-nodes are ir-relevant to source
document before the starting of global ranking process.
4 Experiment
We treat our task as a ranking problem, which takes a document as input and output the ranked list of
background knowledge. We evaluate our method as a ranking problem similarly to information retrieval
task and focus on the performances of models with different setups.
4.1 Data Preparation
The experiment data consists of two parts: source document information and corresponding background
knowledge. To select source documents, we use the ACE corpus (Doddington et al., 2004) for 2005 eval-
uation
4
which consists of 599 articles from multiple sources. We use ReVerb to extract these documents
into multi-triples. For background knowledge, we first retrieve relevant web pages with simply term
matching method and then extract these pages with ReVerb into a set of triples serving as background
knowledge. To ensure the quality, we filter them according to the confidence given by ReVerb.
Besides automatic extraction, we also adapt our system to the golden annotation of ACE as source
document information and standard YAGO knowledge base
5
as background knowledge (Hoffart et al.,
2013). We compare its performance with that in fully automatic system and evaluate the effect of auto-
matic extraction. For better comparison with YAGO, we retrieve relevant pages from WikiPedia although
our automatic extraction method is applicable to any corpus resources.
For every outputted list, three trained annotators check the result and decide which background knowl-
edge is relevant to source document. They work separately and check the same list, so that we can e-
valuate their annotation consistence. They totally annotated more than 7000 background knowledge and
achieved a Fleiss
?
Kappa value of 0.8066 in best situation and 0.7076 in average between three anno-
tators, which is a good consistence between multi-annotator (Fleiss, 1971). When collision happened,
we choose the label selected by more annotators.
4.2 Baseline system
Although we treat our task as a ranking problem, it is difficult to apply corresponding methods in tra-
ditional ranking tasks such as information retrieval (IR) (Manning et al., 2008) and entity linking (EL)
(Han et al., 2011; Kataria et al., 2011; Sen, 2012) directly in our task. First, both IR and EL make use of
the link structure between web or Wiki pages. However, our task takes single document as input and no
link exists between documents which makes it difficult to apply IR and EL methods such as page rank
(Page et al., 1999) and collective method (Han et al., 2011; Sen, 2012) in this task directly. Second, EL
usually evaluate the text similarity between certain document and target page in WikiPedia. However,
our task focuses on the ranking of ?argument
1
, predicate, argument
2
? triple, which contains little text
information. Lack of text information also limits the application of corresponding methods in our task.
4
http://catalog.ldc.upenn.edu/LDC2006T06
5
http://www.mpi-inf.mpg.de/yago-naga/yago
922
Setup MAP P&20
Baseline 0.494 0.377
AutoSD + AutoBK + NoInitial 0.504 0.378
AutoSD + AutoBK + WithInitial 0.531 0.406
GoldSD + AutoBK + NoInitial 0.564 0.417
GoldSD + AutoBK + WithInitial 0.553 0.406
GoldSD + YAGO + NoInitial 0.676 0.328
GoldSD + YAGO + WithInitial 0.676 0.328
Table 1: The result of our model in different setups: GoldSD indicates using annotation of ACE corpus as
source document information; YAGO indicates using YAGO knowledge base as background knowledge;
AutoSD and AutoBK means aotomatic extraction to source document and background knowledge; NoIni-
tial and WithInitial means whether using different initial importance to source document information.
For better comparison, we introduce search engine as resource which is proved effective in relevance
evaluation (Gligorov et al., 2007) and propose a search engine based strong baseline. As illustrated
before, the relevance R
i
between background knowledge b
i
and source document D has been converted
into the relevance between b
i
and the triples of D. Hence, we compute R
i
by accumulating all r
ij
, the
relevance scores between b
i
and every sd-node s
j
with the same method in Section 3.1 (R
i
=
?
s
j
?S
r
ij
,
S is the set of sd-nodes). Then we rank all background knowledge according to the value ofR
i
and output
the ranked list as final result. We extract source document and background knowledge automatically in
the baseline system, which makes it applicable in different setups.
4.3 Experiment setup
We evaluate our model in different setups. First, we extract both source document information and
background knowledge automatically. Second, we use golden annotation of ACE as source document
information but extract background knowledge automatically. Third, we use golden annotation of ACE
and introduce standard YAGO as background knowledge. For all of them three, we evaluate the different
performances with and without initial importance of sd-nodes(Section 3.3). We evaluate the performance
with two famous criteria in ranking problem: MAP (Voorhees et al., 2005) requires more accuracy
and focuses on the knowledge in higher position; P&N which require more coverage and pays more
attention to the number of relevant ones in Top N knowledge. Note that we do not evaluate the Recall
performance because there can be millions of background knowledge to be ranked for every document.
It is impossible to check all of them. So we focus on the Top N candidates and evaluate the performance
with MAP and P&N . In this study, we evaluate the Top 20 background knowledge triples which are
most easily to be viewed by users.
4.4 Experiment Result
The performance of our model is shown in Table 1. Our search engine based baseline system achieve
a rather good performance: a MAP value of 0.494 and 0.377 in P&20. At the same time, our model
outperforms the baseline system in almost every setup and evaluation criterion. The best performance of
MAP is achieved by GoldSD+YAGO (0.676), while the best performance of P&20 is achieved by GoldS-
D+AutoBK (0.417). To analyze the result further, we find that the initial importance, automatic extraction
to source document, and to background knowledge have different effect on the final performance.
4.4.1 Effect of automatic extraction to source document
We use ACE corpus as source documents, which contain golden annotation to document information.
So we can evaluate the effect of automatic extraction to source document by comparing the performance
with and without golden annotation. The performance without golden annotation is shown in AutoS-
D+AutoBK of Table 1, while the other one shown in GoldSD+AutoBK. We can find that the performance
of GoldSD+AutoBK is better than that of AutoSD+AutoBK in both MAP and P&20, which indicates that
golden annotation do help to improve the ranking result.
923
We further analyze the result and find an interesting phenomenon: these two systems performs greatly
different with the setup of NoInitial, but equally with the setup of WithInitial, which indicates that the
performance of AutoSD+AutoBK has been improved by evaluating the importance of source document
information (Section 3.3). So we can naturally infer that, with a better importance evaluating method
in AutoSD+AutoBK, we may achieve similar performance compared to that in golden annotation. Note
that, AutoSD+AutoBK is compatible with any corpus which is more useful in real applications.
4.4.2 Effect of automatic extraction to background knowledge
We evaluate the effect of automatic extraction to background knowledge by comparing the performances
between GoldSD+AutoBK and GoldSD+YAGO. In GoldSD+AutoBK, the background knowledge is ex-
tracted automatically with ReVerb, which has greater coverage but less accuracy. In contrast, the GoldS-
D+YAGO make use of YAGO as background knowledge, which is less coverage but better accuracy. This
difference are reflected on the system performance, where GoldSD+YAGO achieves much better result in
MAP, but much worse in P&20. This is partly because that MAP focus on the background knowledge in
higher position which requires more accuracy, while P&20 pays more attention to the number of relevant
background knowledge which require more coverage.
In general, automatic extraction system has better coverage but less accuracy compared to YAGO
based system. However, automatic extraction to background knowledge may help in real applications by
improving coverage greatly. Besides, the loss of accuracy is partly due to the technology of information
extraction which may be improved in the future. In addition, we can also combine these two ways to
acquire background knowledge to balance coverage and accuracy in the future.
4.4.3 Effect of initial importance to source document information
Initial importance to source document information (Section 3.3) is important to the performance of
our models as shown in Table 1. The model AutoSD+AutoBK+WithInitial outperforms the AutoS-
D+AutoBK+NoInitial compared to other setups, which indicates the help of initial importance to the
ranking result. Especially, initial importance to source document information helps most in the set-
up of AutoSD+AutoBK, which is most useful in real applications. So we can naturally infer that, by
proposing better importance evaluating method, we may further improve the performance of AutoS-
D+AutoBK+WithInitial, which will great helpful in the future application of this method.
5 Related Work
Document enrichment focuses on introducing external knowledge into source document. There are main-
ly two kinds of works in this topic according to the resource they relying on. The first line of works make
use of WikiPedia and enrich source document by linking the entity to its corresponding Wiki page (Bunes-
cu and Pasca, 2006; Cucerzan, 2007). In early stage, most researches rely on the similarity between the
context of the mention and the definition of candidate entities by proposing different measuring crite-
ria such as dot product, cosine similarity, KL divergence, Jaccard distance and more complicated ones
(Bunescu and Pasca, 2006; Cucerzan, 2007; Zheng et al., 2010; Hoffart et al., 2011; Zhang et al., 2011).
However, these methods mainly rely on text similarity but neglect the internal structure between mention-
s. So another kind of works explore the structure information with collective disambiguation (Kulkarni
et al., 2009; Kataria et al., 2011; Sen, 2012; He et al., 2013). These methods make use of structure infor-
mation within context and resolve different mentions based on the coherence among decisions. Despite
the success, the entity linking methods rely on WikiPedia which has great coverage but less accuracy.
Another line of works try to improve the accuracy of enrichment by introducing ontologies (Motta
et al., 2000; Passant, 2007; Fodeh et al., 2011; Kumar and Salim, 2012) and structured knowledge
such as WordNet (Nastase et al., 2010) and Mesh (Wang and Lim, 2008). In these studies, resources
usually provides word or phrase semantic information such as synonym (Sun et al., 2011) and antonym
(Sansonnet and Bouchet, 2010). However, these methods rely on special ontologies constructed with
supervision or even manually, which is difficult to expand and in turn limits the application of them.
924
6 Conclusion and Future Work
This study presents a triple based background knowledge ranking model to acquire most relevant back-
ground knowledge to certain source document. We first develop a triple graph based document presen-
tation to combine source document together with the background knowledge. Then we propose a global
iterative ranking model to acquire Top n relevant knowledge, which provide additional information be-
yond the source document. Note that, both source document information and background knowledge
are extracted automatically which is useful in real application. The experiments show that our model
achieves better results over a strong baseline, which indicates the effectiveness of our framework.
Another interesting phenomenon is that YAGO based enrichment model achieved better ranking ac-
curacy, but less coverage compared to automatic extraction model. To combine these two sources of
background knowledge may help to overcome both coverage and accuracy problem. So exploiting prop-
er way to incorporate knowledge base and automatic extraction is an important topic in our future work.
Finally, we believe that this background knowledge based document enriching technology may help in
those semantic based NLP applications such as coherence evaluation, coreference resolution and question
answering. In our future work, we will explore how to make use of these background knowledge in real
applications, hopefully to improve the performance significantly in the future.
Acknowledgements
We thank Muyun Yang and Jianhui Ji for their great help. This work was supported by National Natural
Science Foundation of China(NSFC) via grant 61133012, the National 863 Leading Technology Re-
search Project via grant 2012AA011102 and the National Natural Science Foundation of China Surface
Project via grant 61273321.
References
Michele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni. 2007. Open
information extraction from the web. In IJCAI, volume 7, pages 2670?2676.
Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka. 2007. Measuring semantic similarity between words
using web search engines. www, 7:757?766.
Volha Bryl, Claudio Giuliano, Luciano Serafini, and Kateryna Tymoshenko. 2010. Using background knowledge
to support coreference resolution. In ECAI, volume 10, pages 759?764.
Razvan C Bunescu and Marius Pasca. 2006. Using encyclopedic knowledge for named entity disambiguation. In
EACL, volume 6, pages 9?16.
Silviu Cucerzan. 2007. Large-scale named entity disambiguation based on wikipedia data. In EMNLP-CoNLL,
volume 7, pages 708?716.
George R Doddington, Alexis Mitchell, Mark A Przybocki, Lance A Ramshaw, Stephanie Strassel, and Ralph M
Weischedel. 2004. The automatic content extraction (ace) program-tasks, data, and evaluation. In LREC.
Citeseer.
Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld. 2008. Open information extraction from
the web. Communications of the ACM, 51(12):68?74.
Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam Mausam. 2011. Open infor-
mation extraction: The second generation. In Proceedings of the Twenty-Second international joint conference
on Artificial Intelligence-Volume Volume One, pages 3?10. AAAI Press.
Joseph L Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological bulletin, 76(5):378.
Samah Fodeh, Bill Punch, and Pang-Ning Tan. 2011. On ontology-driven document clustering using core semantic
features. Knowledge and information systems, 28(2):395?421.
Risto Gligorov, Warner ten Kate, Zharko Aleksovski, and Frank van Harmelen. 2007. Using google distance to
weight approximate ontology matches. In Proceedings of the 16th international conference on World Wide Web,
pages 767?776. ACM.
925
Xianpei Han, Le Sun, and Jun Zhao. 2011. Collective entity linking in web text: a graph-based method. In
Proceedings of the 34th international ACM SIGIR conference on Research and development in Information
Retrieval, pages 765?774. ACM.
Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai Zhang, and Houfeng Wang. 2013. Learning entity repre-
sentation for entity disambiguation. Proc. ACL2013.
Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F?urstenau, Manfred Pinkal, Marc Spaniol, Bilyana
Taneva, Stefan Thater, and Gerhard Weikum. 2011. Robust disambiguation of named entities in text. In Pro-
ceedings of the Conference on Empirical Methods in Natural Language Processing, pages 782?792. Association
for Computational Linguistics.
Johannes Hoffart, Fabian M Suchanek, Klaus Berberich, and Gerhard Weikum. 2013. Yago2: a spatially and
temporally enhanced knowledge base from wikipedia. Artificial Intelligence, 194:28?61.
Xiaohua Hu, Xiaodan Zhang, Caimei Lu, Eun K Park, and Xiaohua Zhou. 2009. Exploiting wikipedia as external
knowledge for document clustering. In Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 389?396. ACM.
Saurabh S Kataria, Krishnan S Kumar, Rajeev R Rastogi, Prithviraj Sen, and Srinivasan H Sengamedu. 2011.
Entity disambiguation with hierarchical topic models. In Proceedings of the 17th ACM SIGKDD international
conference on Knowledge discovery and data mining, pages 1037?1045. ACM.
Christian Kohlsch?utter, Peter Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate detection using shallow text
features. In Proceedings of the third ACM international conference on Web search and data mining, pages
441?450. ACM.
Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti. 2009. Collective annotation of
wikipedia entities in web text. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 457?466. ACM.
Yogan Jaya Kumar and Naomie Salim. 2012. Automatic multi document summarization approaches. Journal of
Computer Science, 8(1).
Thomas Lin, Oren Etzioni, et al. 2010. Identifying functional relations in web text. In Proceedings of the
2010 Conference on Empirical Methods in Natural Language Processing, pages 1266?1276. Association for
Computational Linguistics.
Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch?utze. 2008. Introduction to information retrieval,
volume 1. Cambridge University Press Cambridge.
Enrico Motta, Simon Buckingham Shum, and John Domingue. 2000. Ontology-driven document enrichment:
principles, tools and applications. International Journal of Human-Computer Studies, 52(6):1071?1109.
Vivi Nastase, Michael Strube, Benjamin B?orschinger, C?acilia Zirn, and Anas Elghafari. 2010. Wikinet: A very
large scale multi-lingual concept network. In LREC.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. 1999. The pagerank citation ranking: Bring-
ing order to the web.
Patrick Pantel and Ariel Fuxman. 2011. Jigs and lures: Associating web queries with structured entities. In
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language
Technologies-Volume 1, pages 83?92. Association for Computational Linguistics.
Alexandre Passant. 2007. Using ontologies to strengthen folksonomies and enrich information retrieval in we-
blogs. In Proceedings of International Conference on Weblogs and Social Media.
Alan Ritter, Oren Etzioni, et al. 2010. A latent dirichlet allocation method for selectional preferences. In Proceed-
ings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424?434. Association
for Computational Linguistics.
Jean-Paul Sansonnet and Franc?ois Bouchet. 2010. Extraction of agent psychological behaviors from glosses of
wordnet personality adjectives. In Proc. of the 8th European Workshop on Multi-Agent Systems (EUMAS10).
Stefan Schoenmackers, Oren Etzioni, Daniel S Weld, and Jesse Davis. 2010. Learning first-order horn clauses
from web text. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,
pages 1088?1098. Association for Computational Linguistics.
926
Prithviraj Sen. 2012. Collective context-aware topic models for entity disambiguation. In Proceedings of the 21st
international conference on World Wide Web, pages 729?738. ACM.
Koun-Tem Sun, Yueh-Min Huang, and Ming-Chi Liu. 2011. A wordnet-based near-synonyms and similar-looking
word learning system. Educational Technology & Society, 14(1):121?134.
Ellen M Voorhees, Donna K Harman, et al. 2005. TREC: Experiment and evaluation in information retrieval,
volume 63. MIT press Cambridge.
Xudong Wang and Azman O Lim. 2008. Ieee 802.11 s wireless mesh networks: Framework and challenges. Ad
Hoc Networks, 6(6):970?984.
Fei Wang and Changshui Zhang. 2008. Label propagation through linear neighborhoods. Knowledge and Data
Engineering, IEEE Transactions on, 20(1):55?67.
Fei Wu and Daniel S Weld. 2010. Open information extraction using wikipedia. In Proceedings of the 48th An-
nual Meeting of the Association for Computational Linguistics, pages 118?127. Association for Computational
Linguistics.
Wei Zhang, Yan Chuan Sim, Jian Su, and Chew Lim Tan. 2011. Entity linking with effective acronym expansion,
instance selection and topic modeling. In Proceedings of the Twenty-Second international joint conference on
Artificial Intelligence-Volume Volume Three, pages 1909?1914. AAAI Press.
Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010. Learning to link entities with knowledge
base. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, pages 483?491. Association for Computational Linguistics.
Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen. 2009. Statsnowball: a statistical approach to
extracting entity relationships. In Proceedings of the 18th international conference on World wide web, pages
101?110. ACM.
927
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 127?130,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
1 
 
 Coreference Resolution System using Maximum Entropy Classifier 
 
 
Weipeng Chen,Muyu Zhang,Bing Qin  
                                                               Center for Information Retrieval 
Harbin Institute of Technology 
                                     {wpchen,myzhang,bing.qin}@ir.hit.edu.cn 
  
  
 
 
Abstract 
In this paper, we present our supervised 
learning approach to coreference resolution 
in ConLL corpus. The system relies on a 
maximum entropy-based classifier for pairs 
of mentions, and adopts a rich linguisitical- 
ly motivated feature set, which mostly has 
been introduced by Soon et al(2001), and 
experiment with alternaive resolution proc- 
ess, preprocessing tools,and classifiers. We 
optimize the system?s performance for M- 
UC (Vilain et al 1995), BCUB (Bagga and 
Baldwin, 1998) and CEAF (Luo, 2005) .  
1. Introduction 
The coreference resolution is the task in which all  
expressions refer to the same entity in a discourse 
will be identified. As the core of natural language 
processing, coreference resolution is significant to 
message understanding, information extraction, 
text summarization, information retrieval, informa-
tion filtration, and machine translation. 
A considerable engineering efforts is needed for 
the full coreference resolution task, and a signifi-
cant part of this effort concerns feature engineering.  
The backbone of our system can be split into two 
subproblems: mention detection and creation of 
entitly. We train a mention detector on the training 
texts. Once the mentions are identified, coreference 
resolution involves partitioning them into subsets 
corresponding to the same entity. This problem is 
cast into the binary classification problem of decid-
ing whether two given mentions are coreferent. 
Our system relies on maximum entropy-based 
classifier for pairs of mentions. Our system relies 
on a rich linguistically motivated feature set. Our 
system architecture makes it possible to define 
other kinds of features: atmoic word and markable 
features. This approach to feature engineering is 
suitable not only for knowledge-rich but also for 
knowledge-poor datasets. Finally, we use the best-
first clustering to create the coreference chains. 
 
2. System Description  
This section briefly describes our system. First the 
mention detection is presented. Next, the features 
which we import are described. Finally, we de-
scribled the learning and encoding methods. 
2.1 Mention Detector  
The first stage of the coreference resolution 
process try to identify the occurrence of mentions 
in document. To detect system mention from a test 
text, we train a mention detector on the training 
data. We formulate the mention problem as a clas-
sification, by assigning to each token in the text a 
label, indicating whether it is a mention or not. 
Hence, to learn the detector, we create one training 
text and derive its class value (one of b, i, o) from 
the annotated data. Each instance represents the  , 
the token under consideration, and consists of 19 
linguistic features, many of which are modeled af-
ter the systems of Bikel et al (1999) and Florian et 
al. (2004) , as describled below. 
(1) Lexical: Tokens in the windows of  three 
words before and after the target word: 
{     ,?,    }. 
(2) Capitalization: Determine whether    is 
IsAllCaP (all the characters of word are ca-
pitalized, such as ?BBN?), IsInitCap (the 
word starts with a capitalized character, 
127
2 
 
such as ?Sally? ), IsCapPeriod (more than 
one characters of word are capitalized but 
not all, and the first character is not capita-
lized too, such ?M.? ), and IsAllLower (all 
the character of word aren?t capitalized, 
such as ?can? ) (see Bikel et al  (1999)). 
(3) Grammatical: The single POS tags of the 
tokens in the window of three words before 
and after the target word{    ,?,    }. 
(4) Semantic: The  named entity (NE) tag and  
the Noun Phrase tag of  .  
We employ maximum entropy-based classifier, for 
training the mention detector. These detected 
mentions are to be used as system mentions in our 
coreference experiment. 
2.2 Features 
To determine which mentions belong to same en-
titly, we need to devise a set of features that is use-
ful in determining whether two mentions corefer or 
not. All the feature value are computed automati-
cally, without any manual intervention. 
     
(1) Distance Feature: A non-negative integer 
feature capture the distance between anap- 
hor and antecedent. If anaphor and antece-
dent are in the same sentence, the value is 
0; If their sentence distance is 1, the value 
is 1, and so on. 
(2) Antecedent-pronoun Feature: A Boolean 
feature capture whether the antecedent is p- 
ronoun or not. True if the antecedent is a p- 
ronoun. Pronouns include reflexive prono-
uns, personal pronouns, and possessive pr- 
onouns.  
(3) Anaphor-pronoun Feature: A Boolean f- 
eature capture whether  the anaphor is pro-
noun or not. True if the anaphor is a pron- 
oun. 
(4) String Match Feature: A non-negative in-
teger feature. If one candidate is a substrin-
g of another, its value is 0, else the value is 
0 plus the edit distance. 
(5) Anaphor Definite Noun Phrase Feature: 
A Boolean feature capture whether the ana- 
phor is a definite noun phrase or not. True 
if the anaphor is a pronoun. In our definiti- 
on, a definite noun phrase is someone that 
start with the word ?the?. 
(6) Anaphor Demonstrative Noun Phrase F-
eature:  A Boolean feature capture wheth- 
er the anaphor is a demonstractive  noun or 
not. True if the anaphor is a demonstractive  
noun. In our definition, a demonstractive  n 
oun is someone that start with the word, su- 
ch as this, that, those, these. 
(7) ProperName Feature: A Boolean feature. 
True if  anphor and antecedent both are pr- 
oper name. 
(8) Gender Feature: Its value are true, false   
or  unknow. If gender of pair of  instance   
matches, its value is true,else if  the value  
is umatches, the value is false; If one of the 
pair instance?s gender is unknown, the val-
ue is uknown.  
(9) Number Feature: A Boolean feature. True 
if the  number of pair of instance is match-
es; 
(10) Alias Feature: A Boolean feature. True if 
two markables refer to the same entity usi- 
ng different notation(acronyms, shorthands, 
etc), its value is true. 
(11) Semantic Feature: Its value are true, fals- 
e, or unknown. If semantic class relateness 
of a pair instance is the same, or one is the 
parent of other, its value is true; Else if the- 
y are unmatch,the value is false; If one of t- 
he the pair instance?s semantic class is unk- 
nown, the value is unknown. 
 
2.3 Learning   
We did not make any effort to optimize the nu- 
mber of training instances for the pair-wise learne- 
r: a positive instance for each adjacent coreferent 
markable pair and negative training instances for a 
markable m and all markables disreferent with m 
that occur before m (Soon et al,2001). For decod-
ing it generates all the possible links inside a win-
dow of 100 markables. 
Our system integrate many machine learning m 
ethods, such as maximum entropy (Tsuruoka,  200- 
6) , Descision Tree,Support Vector Machine  (Joa- 
chims, 2002) . We compare the result using differ- 
ent method in our system, and decide to rely on m-
aximum entropy-based classifier, and it led to the 
best results. 
2.4 Decoding 
In the decoding step, the coreference chains are 
created by the best-first clustering. Each mention is 
128
3 
 
compared with all of its previous mentions with 
probability greater than a fixed threshold, and is 
clustered with the one hightest probability. If none 
has probability greater than the threshold, the men-
tion becomes a new cluster. 
      
3. Setting and data 
3.1 Setting 
Our system has participated in the closed settings 
for English. Which means all the knowledge re-
quired by the mention detector and feature detector   
is obtained from the annotation of the corpus(see 
Pradhan et al  (2007)), with the exception of Wor- 
dNet.  
 
3.2 Data  
We selecte all ConLL training data and develop-
ment data, contain ?gold? files and ?auto? file, to 
train our final system. The "gold" indicates that 
the annotation is that file is hand-annotated and 
adjudicated quality, whereas the second means it 
was produced using a combination of automatic 
tools. The training data distribution is shown in 
Table 1. 
 
Category bc bn mz nw wb 
Quantity 40 1708 142 1666 190 
  Table 1: Final system?s training data distribution 
 
 
In this paper, we report the results from our dev- 
elopment system, which were trained on the traini- 
ng data and tested on the development set. The de- 
tail is shown in Table 2,3. 
 
Category bc bn mz nw wb 
Quantity 32 1526 128 1490 166 
  Table 2: Experiment system?s training data distribution 
  
 
Category bc bn mz nw wb 
Quantity 8 182 14 176 24 
   Table 3: Experiment system?s test set distribution 
 
 
4. Evaluation  
First, we have evaluated our mention detector mo- 
dule, which is train by the ConLL training data. It 
regards all the token as the candidate, and cast it i- 
nto the mention detector, and the detector decides 
it is  mention or not. The mention detector?s result 
is shown in Table4. 
 
 
Metric R P F 
Value 63.6 55.26 59.14 
Table 4: Performance of  mention detector on the de-
velopment set 
 
Second, we have evaluated our system with the 
system mention, and we use the previous mention 
detector to determine the mention boundary. As fo- 
llow, we list the system perfomance  of using MUC, 
B-CUB,CEAF (E) , CEAF (M) , BLANC (Recasens a- 
nd Hovy, in prep)  in Table 5 . 
 
Metric R P F 
MUC 45.53 47.00 46.25 
BCUB 61.29 68.07 64.50 
CEAF(M) 47.47 47.47 47.47 
CEAF(E) 39.23 37.91 38.55 
BLANC 64.00 68.31 65.81 
Table 5 :Result using  system mentions 
 
 
Finally, we  have evaluated our system with the 
gold mentions, which mention?s boundary is corect. 
The system performance is shown in Table 6: 
 
Metric R P F 
MUC 50.15 80.49 61.78 
BCUB 48.87 85.75 62.62 
CEAF(M) 54.50 54.50 54.50 
CEAF(E) 67.38 32.72 44.05 
BLANC 66.03 78.41 70.02 
Table6:Result using  gold mentions 
 
 
Result of system shows a big difference  betwee- 
n using gold mentions and using system mentions. 
In comparison to the system using system mention- 
s, we see that the F-score rises significantly by 
4.21- 15.53 for the system using gold mentions. It  
is worth noting that the F-scorer when using the B- 
CUB metric, the system using system mention rise- 
129
4 
 
s 2.12 for system using gold mention. Although t- 
his is surprising, in my opinion this correlation is 
because the mention detection recall more candid- 
ate mention, and the BCUB metric is benefit for t- 
he mention which is merge into the erroneous 
chain.  
5. Conclusion 
In this paper, we have presented a new modular 
system for coreference in English. We train a men-
tion detector to find the mention?s boundary based 
on maximum entropy classifier to decide pairs of 
mention refer to or not.  
     Due to the flexible architecture, it allows us ex-
tend the system to multi-language. And if it is ne-
cessary, we can obtain other modules to support 
the system. The results obtained confirm the feasi-
bility of our system. 
 
 
References  
Wee Meng Soon,Hwee You Ng,and Daniel Chung 
Yong Lim.2001.A machine learing approach to core-
ference resolution of noun phrases.Computational 
Linguistic(special Issue on Computational Anaphora 
Resolution),27(4):521-544 
Marc Vilain,John Burger,John Aberdeen,Dennis Con-
nolly,and Lynette Hirschman.1995.A modeltheoretic 
coreference scoring scheme.In Proceedings of the 6th 
Message Understanding Conference,pages 45-52. 
Amit Bagga and Breck baldwin.1998.Algorithms for 
scoring coreference chains.In Proceedings of the lin-
guistic Coreference Workshoop at the International 
Conference on Language Resources and Evalua-
tion(LREC-1998),pages 563-566. 
Xiaoqiang Luo.2005.On coreference resoluton perfor-
mance metrics.In Proceeddings of the Annual Meet-
ing of the North American Chapter of the Association 
for Computational Linguistics-Human Language 
Technology Conference(NAACL/HLY-2005),pages 
25-32 
Josef Steinberger,Massimo Poesio,Mijail A.kabadjov- 
b,and Karel jezek.2007.Two uses of anaphora resolu-
tion in summarization.In Information Processing and 
management,Special issue on Summarization,pages 
1663-1680 
Bikel,R.Schwartz,and R.Weischedel.1999.An algorithm 
that learns what's in a name.Machine Learning,34(1-
3):pages211-231 
Florian,H.Hassan,A.Ittycheriah,H.Jing,N.Kambhatla, X. 
Luo,N.Nicolov,and I.Zitouni.2004.A statistical model 
for multilingual entity detection and tracking.In 
Proc.of HLA/NAACL. 
Sameer Pradhan and Lance Ramshaw and Ralph Wei-
schedel and Jessica MacBride and Linnea Micciulla. 
2007.Unrestricted Coreference: Identifying Entities 
and Events in OntoNotes. In Proceedings of the IEEE 
International Conference on Semantic Computing 
(ICSC), Irvine, CA 
Marta Recasens and Eduard Hovy.in prep.BLAN- 
C:Implementing the rand index for coreference eval-
uation. 
Yoshimasa Tsuruoka.2006.A simple c++ library for 
maxium entropy classifiction.Ysujii laboratory,Dep- 
artment of Computer Science,University of Tokyo. 
Throsten Joachims.1999.Making large-scale SVM 
learning practical.In B.Scholkopf,C.Burges,and A.S- 
mola,editors,Advances in Kernel Methods-Support 
Vector Learning.MIT-Press. 
 
 
 
 
 
 
 
 
 
 
 
 
 
130
