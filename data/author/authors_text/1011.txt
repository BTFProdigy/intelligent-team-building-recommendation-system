Exploring Asymmetric Clustering for Statistical Language Modeling 
Jianfeng Gao  
Microsoft Research, Asia 
Beijing, 100080, P.R.C  
jfgao@microsoft.com 
Joshua T. Goodman  
Microsoft Research, Redmond 
Washington 98052, USA  
joshuago@microsoft.com 
Guihong Cao1  
Department of Computer 
Science and Engineering of 
Tianjin University, China  
Hang Li  
Microsoft Research, Asia 
Beijing, 100080, P.R.C  
hangli@microsoft.com 
                                                     
1 This work was done while Cao was visiting Microsoft Research Asia. 
Abstract 
The n-gram model is a stochastic model, 
which predicts the next word (predicted 
word) given the previous words 
(conditional words) in a word sequence.  
The cluster n-gram model is a variant of 
the n-gram model in which similar words 
are classified in the same cluster. It has 
been demonstrated that using different 
clusters for predicted and conditional 
words leads to cluster models that are 
superior to classical cluster models which 
use the same clusters for both words.  This 
is the basis of the asymmetric cluster 
model (ACM) discussed in our study.  In 
this paper, we first present a formal 
definition of the ACM. We then describe 
in detail the methodology of constructing 
the ACM. The effectiveness of the ACM 
is evaluated on a realistic application, 
namely Japanese Kana-Kanji conversion.  
Experimental results show substantial 
improvements of the ACM in comparison 
with classical cluster models and word 
n-gram models at the same model size.  
Our analysis shows that the 
high-performance of the ACM lies in the 
asymmetry of the model. 
1 Introduction 
The n-gram model has been widely applied in many 
applications such as speech recognition, machine 
translation, and Asian language text input [Jelinek, 
1990; Brown et al, 1990; Gao et al, 2002].  It is a 
stochastic model, which predicts the next word 
(predicted word) given the previous n-1 words 
(conditional words) in a word sequence. 
The cluster n-gram model is a variant of the word 
n-gram model in which similar words are classified 
in the same cluster.  This has been demonstrated as 
an effective way to deal with the data sparseness 
problem and to reduce the memory sizes for realistic 
applications. Recent research [Yamamoto et al, 
2001] shows that using different clusters for 
predicted and conditional words can lead to cluster 
models that are superior to classical cluster models, 
which use the same clusters for both words [Brown 
et al, 1992].  This is the basis of the asymmetric 
cluster model (ACM), which will be formally 
defined and empirically studied in this paper.  
Although similar models have been used in previous 
studies [Goodman and Gao, 2000; Yamamoto et al, 
2001], several issues have not been completely 
investigated. These include: (1) an effective 
methodology for constructing the ACM, (2) a 
thorough comparative study of the ACM with 
classical cluster models and word models when they 
are applied to a realistic application, and (3) an 
analysis of the reason why the ACM is superior. 
The goal of this study is to address the above 
three issues. We first present a formal definition of 
the ACM; then we describe in detail the 
methodology of constructing the ACM including (1) 
an asymmetric clustering algorithm in which 
different metrics are used for clustering the 
predicted and conditional words respectively; and 
(2) a method for model parameter optimization in 
which the optimal cluster numbers are found for 
different clusters.  We evaluate the ACM on a real 
application, Japanese Kana-Kanji conversion, which 
converts phonetic Kana strings into proper Japanese 
orthography. The performance is measured in terms 
of character error rate (CER).  Our results show 
substantial improvements of the ACM in 
comparison with classical cluster models and word 
n-gram models at the same model size.  Our analysis 
shows that the high-performance of the ACM comes 
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 183-190.
                         Proceedings of the 40th Annual Meeting of the Association for
from better structure and better smoothing, both of 
which lie in the asymmetry of the model. 
This paper is organized as follows: Section 1 
introduces our research topic, and then Section 2 
reviews related work. Section 3 defines the ACM 
and describes in detail the method of model 
construction. Section 4 first introduces the Japanese 
Kana-Kanji conversion task; it then presents our 
main experiments and a discussion of our findings.  
Finally, conclusions are presented in Section 5. 
2 Related Work 
A large amount of previous research on clustering 
has been focused on how to find the best clusters 
[Brown et al, 1992; Kneser and Ney, 1993; 
Yamamoto and Sagisaka, 1999; Ueberla, 1996; 
Pereira et al, 1993; Bellegarda et al, 1996; Bai et 
al., 1998]. Only small differences have been 
observed, however, in the performance of the 
different techniques for constructing clusters. In this 
study, we focused our research on novel techniques 
for using clusters ? the ACM, in which different 
clusters are used for predicted and conditional words 
respectively. 
The discussion of the ACM in this paper is an 
extension of several studies below. The first similar 
cluster model was presented by Goodman and Gao 
[2000] in which the clustering techniques were 
combined with Stolcke?s [1998] pruning to reduce 
the language model (LM) size effectively. Goodman 
[2001] and Gao et al [2001] give detailed 
descriptions of the asymmetric clustering algorithm.  
However, the impact of the asymmetric clustering 
on the performance of the resulting cluster model 
was not empirically studied there.  Gao et al, [2001] 
presented a fairly thorough empirical study of 
clustering techniques for Asian language modeling. 
Unfortunately, all of the above work studied the 
ACM without applying it to an application; thus 
only perplexity results were presented.  The first real 
application of the ACM was a simplified bigram 
ACM used in a Chinese text input system [Gao et al  
2002]. However, quite a few techniques (including 
clustering) were integrated to construct a Chinese 
language modeling system, and the contribution of 
using the ACM alone was by no means completely 
investigated. 
Finally, there is one more point worth 
mentioning. Most language modeling improvements 
reported previously required significantly more 
space than word trigram models [Rosenfeld, 2000]. 
Their practical value is questionable since all 
realistic applications have memory constraints. In 
this paper, our goal is to achieve a better tradeoff 
between LM performance (perplexity and CER) and 
model size. Thus, whenever we compare the 
performance of different models (i.e. ACM vs. word 
trigram model), Stolcke?s pruning is employed to 
bring the models compared to similar sizes. 
3 Asymmetric Cluster Model 
3.1 Model  
The LM predicts the next word wi given its history h 
by estimating the conditional probability P(wi|h). 
Using the trigram approximation, we have 
P(wi|h)?P(wi|wi-2wi-1), assuming that the next word 
depends only on the two preceding words.  
In the ACM, we will use different clusters for 
words in different positions.  For the predicted word, 
wi, we will denote the cluster of the word by PWi, 
and we will refer to this as the predictive cluster. .For 
the words wi-2 and wi-1 that we are conditioning on, 
we will denote their clusters by CWi-2 and CWi-1 
which we call conditional clusters.  When we which 
to refer to a cluster of a word w in general we will 
use the notation W.  The ACM estimates the 
probability of wi given the two preceeding words wi-2 
and wi-1 as the product of the following two 
probabilities: 
(1) The probability of the predicted cluster PWi 
given the preceding conditional clusters CWi-2 
and CWi-1, P(PWi|CWi-2CWi-1), and 
(2) The probability of the word given its cluster PWi 
and the preceding conditional clusters CWi-2 and 
CWi-1, P(wi|CWi-2CWi-1PWi). 
Thus, the ACM can be parameterized by 
)|()|()|( 1212 iiiiiiii PWCWCWwPCWCWPWPhwP ???? ?? (1) 
The ACM consists of two sub-models: (1) the 
cluster sub-model P(PWi|CWi-2CWi-1), and (2) the 
word sub-model P(wi|CWi-2CWi-1PWi). To deal with 
the data sparseness problem, we used a backoff 
scheme (Katz, 1987) for the parameter estimation of 
each sub-model. The backoff scheme recursively 
estimates the probability of an unseen n-gram by 
utilizing (n-1)-gram estimates. 
The basic idea underlying the ACM is the use of 
different clusters for predicted and conditional 
words respectively.  Classical cluster models are 
symmetric in that the same clusters are employed for 
both predicted and conditional words.  However, the 
symmetric cluster model is suboptimal in practice. 
For example, consider a pair of words like ?a? and 
?an?.  In general, ?a? and ?an? can follow the same 
words, and thus, as predicted words, belong in the 
same cluster. But, there are very few words that can 
follow both ?a? and ?an?. So as conditional words, 
they belong in different clusters. 
In generating clusters, two factors need to be 
considered: (1) clustering metrics, and (2) cluster 
numbers.  In what follows, we will investigate the 
impact of each of the factors. 
3.2 Asymmetric clustering  
The basic criterion for statistical clustering is to 
maximize the resulting probability (or minimize the 
resulting perplexity) of the training data. Many 
traditional clustering techniques [Brown et al, 
1992] attempt to maximize the average mutual 
information of adjacent clusters 
?=
21 , 2
12
2121 )(
)|(log)(),(
WW WP
WWPWWPWWI , (2) 
where the same clusters are used for both predicted 
and conditional words. We will call these clustering 
techniques symmetric clustering, and the resulting 
clusters both clusters.  In constructing the ACM, we 
used asymmetric clustering, in which different 
clusters are used for predicted and conditional 
words. In particular, for clustering conditional 
words, we try to minimize the perplexity of training 
data for a bigram of the form P(wi|Wi-1), which is 
equivalent to maximizing 
?
=
?
N
i
ii WwP
1
1)|( . (3) 
where N is the total number of words in the training 
data.  We will call the resulting clusters conditional 
clusters denoted by CW. For clustering predicted 
words, we try to minimize the perplexity of training 
data of P(Wi|wi-1)?P(wi|Wi). We will call the 
resulting clusters predicted clusters denoted by PW. 
We have2 
??
= ?
?
=
? ?=?
N
i i
ii
i
iiN
i
iiii WP
wWP
wP
WwPWwPwWP
1 1
1
1
1 )(
)(
)(
)()|()|(  
  ?
=
?
?
?= N
i i
ii
i
ii
WP
WwP
wP
wWP
1
1
1 )(
)(
)(
)(  
  ?
= ??
?= N
i
ii
i
i WwPwP
wP
1
1
1
)|()(
)( . 
Now, 
)(
)(
1?i
i
wP
wP is independent of the clustering used. 
Therefore, for the selection of the best clusters, it is 
sufficient to try to maximize 
?
=
?
N
i
ii WwP
1
1 )|( . (4) 
This is very convenient since it is exactly the op-
posite of what was done for conditional clustering. It 
                                                     
2 Thanks to Lillian Lee for suggesting this justification of 
predictive clusters. 
means that we can use the same clustering tool for 
both, and simply switch the order used by the 
program used to get the raw counts for clustering.  
The clustering technique we used creates a binary 
branching tree with words at the leaves.  The ACM 
in this study is a hard cluster model, meaning that 
each word belongs to only one cluster.  So in the 
clustering tree, each word occurs in a single leaf.   In 
the ACM, we actually use two different clustering 
trees. One is optimized for predicted words, and the 
other for conditional words. 
The basic approach to clustering we used is a 
top-down, splitting clustering algorithm. In each 
iteration, a cluster is split into two clusters in the 
way that the splitting achieves the maximal entropy 
decrease (estimated by Equations (3) or (4)). Finally, 
we can also perform iterations of swapping all words 
between all clusters until convergence i.e. no more 
entropy decrease can be found3. We find that our 
algorithm is much more efficient than agglomerative 
clustering algorithms ? those which merge words 
bottom up.  
3.3 Parameter optimization 
Asymmetric clustering results in two binary 
clustering trees. By cutting the trees at a certain 
level, it is possible to achieve a wide variety of 
different numbers of clusters.  For instance, if the 
tree is cut after the 8th level, there will be roughly 
28=256 clusters.  Since the tree is not balanced, the 
actual number of clusters may be somewhat smaller. 
We use Wl to represent the cluster of a word w using 
a tree cut at level l.  In particular, if we set l to the 
value ?all?, it means that the tree is cut at infinite 
depth, i.e. each cluster contains a single word. The 
ACM model of Equation (1) can be rewritten as 
 P(PWil|CWi-2jCWi-1j)?P(wi|PWi-2kCWi-1kCWil). (5) 
To optimally apply the ACM to realistic applications 
with memory constraints, we are always seeking the 
correct balance between model size and 
performance. We used Stolcke?s pruning method to 
produce many ACMs with different model sizes. In 
our experiments, whenever we compare techniques, 
we do so by comparing the performance (perplexity 
and CER) of the LM techniques at the same model 
sizes. Stolcke?s pruning is an entropy-based cutoff 
                                                     
3 Notice that for experiments reported in this paper, we 
used the basic top-down algorithm without swapping. 
Although the resulting clusters without swapping are not 
even locally optimal, our experiments show that the 
quality of clusters (in terms of the perplexity of the 
resulting ACM) is not inferior to that of clusters with 
swapping. 
method, which can be described as follows: all 
n-grams that change perplexity by less than a 
threshold are removed from the model. For pruning 
the ACM, we have two thresholds: one for the 
cluster sub-model P(PWil|CWi-2jCWi-1j) and one for 
the word sub-model P(wi|CWi-2kCWi-1kPWil) 
respectively, denoted by tc and  tw below. 
In this way, we have 5 different parameters that 
need to be simultaneously optimized: l, j, k, tc, and 
tw, where j, k, and l are the numbers of clusters, and tc 
and tw are the pruning thresholds.  
A brute-force approach to optimizing such a large 
number of parameters is prohibitively expensive. 
Rather than trying a large number of combinations 
of all 5 parameters, we give an alternative technique 
that is significantly more efficient. Simple math 
shows that the perplexity of the overall model 
P(PWil|CWi-2jCWi-1j)? P(wi|CWi-2kCWi-1kPWil) is 
equal to the perplexity of the cluster sub-model 
P(PWil|CWi-2jCWi-1j) times the perplexity of the 
word sub-model P(wi|CWi-2kCWi-1kPWil).  The size of 
the overall model is clearly the sum of the sizes of 
the two sub-models.  Thus, we try a large number of 
values of j, l, and a pruning threshold tc for 
P(PWil|CWi-2jCWi-1j), computing sizes and 
perplexities of each, and a similarly large number of 
values of l,  k, and a separate threshold tw for 
P(wi|CWi-2kCWi-1kPWil).  We can then look at all 
compatible pairs of these models (those with the 
same value of l) and quickly compute the perplexity 
and size of the overall models.  This allows us to 
relatively quickly search through what would 
otherwise be an overwhelmingly large search space. 
4 Experimental Results and Discussion 
4.1 Japanese Kana-Kanji Conversion Task 
Japanese Kana-Kanji conversion is the standard 
method of inputting Japanese text by converting a 
syllabary-based Kana string into the appropriate 
combination of ideographic Kanji and Kana. This is 
a similar problem to speech recognition, except that 
it does not include acoustic ambiguity. The 
performance is generally measured in terms of 
character error rate (CER), which is the number of 
characters wrongly converted from the phonetic 
string divided by the number of characters in the 
correct transcript. The role of the language model is, 
for all possible word strings that match the typed 
phonetic symbol string, to select the word string 
with the highest language model probability. 
Current products make about 5-10% errors in con-
version of real data in a wide variety of domains. 
4.2 Settings 
In the experiments, we used two Japanese 
newspaper corpora: the Nikkei Newspaper corpus, 
and the Yomiuri Newspaper corpus. Both text 
corpora have been word-segmented using a lexicon 
containing 167,107 entries.  
We performed two sets of experiments: (1) pilot 
experiments, in which model performance is 
measured in terms of perplexity and (2) Japanese 
Kana-Kanji conversion experiments, in which the 
performance of which is measured in terms of CER. 
In the pilot experiments, we used a subset of the 
Nikkei newspaper corpus: ten million words of the 
Nikkei corpus for language model training, 10,000 
words for held-out data, and 20,000 words for 
testing data. None of the three data sets overlapped.  
In the Japanese Kana-Kanji conversion experiments, 
we built language models on a subset of the Nikkei 
Newspaper corpus, which contains 36 million 
words. We performed parameter optimization on a 
subset of held-out data from the Yomiuri Newspaper 
corpus, which contains 100,000 words. We 
performed testing on another subset of the Yomiuri 
Newspaper corpus, which contains 100,000 words. 
In both sets of experiments, word clusters were 
derived from bigram counts generated from the 
training corpora. Out-of-vocabulary words were not 
included in perplexity and error rate computations. 
4.3 Impact of asymmetric clustering 
As described in Section 3.2, depending on the 
clustering metrics we chose for generating clusters, 
we obtained three types of clusters: both clusters 
(the metric of Equation (2)), conditional clusters 
(the metric of Equation (3)), and predicted clusters 
(the metric of Equation (4)). We then performed a 
series of experiments to investigate the impact of 
different types of clusters on the ACM. We used 
three variants of the trigram ACM: (1) the predictive 
cluster model P(wi|wi-2wi-1Wi)? P(Wi|wi-2wi-1) where 
only predicted words are clustered, (2) the 
conditional cluster model P(wi|Wi-2Wi-1) where only 
conditional words are clustered, and (3) the IBM 
model P(wi|Wi)? P(Wi|Wi-2Wi-1) which can be treated 
as a special case of the ACM of Equation (5) by 
using the same type of cluster for both predicted and 
conditional words, and setting k = 0, and l = j. For 
each cluster trigram model, we compared their 
perplexities and CER results on Japanese Kana- 
Kanji conversion using different types of clusters. 
For each cluster type, the number of clusters were 
fixed to the same value 2^6 just for comparison.  The 
results are shown in Table 1. It turns out that the 
benefit of using different clusters in different 
positions is obvious.  For each cluster trigram 
model, the best results were achieved by using the 
?matched? clusters, e.g. the predictive cluster model 
P(wi|wi-2wi-1Wi)? P(Wi|wi-2wi-1) has the best 
performance when the cluster Wi is the predictive 
cluster PWi generated by using the metric of 
Equation (4). In particular, the IBM model achieved 
the best results when predicted and conditional 
clusters were used for predicted and conditional 
words respectively. That is, the IBM model is of the 
form P(wi|PWi)? P(PWi|CWi-2CWi-1). 
 Con Pre Both Con + Pre 
Perplexity 287.7 414.5 377.6 --- Con 
model CER (%) 4.58 11.78 12.56 --- 
Perplexity 103.4 102.4 103.3 --- Pre 
model CER (%) 3.92 3.63 3.82 --- 
Perplexity 548.2 514.4 385.2 382.2 IBM 
model CER (%) 6.61 6.49 5.82 5.36 
Table 1: Comparison of different cluster types 
with cluster-based models 
4.4 Impact of parameter optimization 
In this section, we first present our pilot experiments 
of finding the optimal parameter set of the ACM (l, j, 
k, tc, tw) described in Section 2.3. Then, we compare 
the ACM to the IBM model, showing that the 
superiority of the ACM results from its better 
structure. 
In this section, the performance of LMs was 
measured in terms of perplexity, and the size was 
measured as the total number of parameters of the 
LM: one parameter for each bigram and trigram, one 
parameter for each normalization parameter ? that 
was needed, and one parameter for each unigram.  
We first used the conditional cluster model of the 
form P(wi|CWi-2jCWi-1j). Some sample settings of 
parameters (j, tw) are shown in Figure 1. The 
performance was consistently improved by 
increasing the number of clusters j, except at the 
smallest sizes.  The word trigram model was 
consistently the best model, except at the smallest 
sizes, and even then was only marginally worse than 
the conditional cluster models.  This is not surprising 
because the conditional cluster model always 
discards information for predicting words. 
We then used the predictive cluster model of the 
form P(PWil|wi-2wi-1)?P(wi|wi-2wi-1PWil), where only 
predicted words are clustered. Some sample settings 
of the parameters (l, tc, tw) are shown in Figure 2. For 
simplicity, we assumed tc=tw, meaning that the same 
pruning threshold values were used for both 
sub-models. It turns out that predictive cluster 
models achieve the best perplexity results at about 
2^6 or 2^8 clusters. The models consistently 
outperform the baseline word trigram models.  
We finally returned to the ACM of Equation (5), 
where both conditional words and the predicted 
word are clustered (with different numbers of 
clusters), and which is referred to as the combined 
cluster model below.  In addition, we allow different 
values of the threshold for different sub-models. 
Therefore, we need to optimize the model parameter 
set l, j, k, tc, tw.  
Based on the pilot experiment results using 
conditional and predictive cluster models, we tried 
combined cluster models for values l? [4, 10], j, 
k? [8, 16]. We also allow j, k=all. Rather than plot 
all points of all models together, we show only the 
outer envelope of the points.  That is, if for a given 
model type and a given point there is some other 
point of the same type with both lower perplexity 
and smaller size than the first point, then we do not 
plot the first, worse point.  
The results are shown in Figure 3, where the 
cluster number of IBM models is 2^14 which 
achieves the best performance for IBM models in 
our experiments.  It turns out that when l? [6, 8] and 
j, k>12, combined cluster models yield the best 
results. We also found that the predictive cluster 
models give as good performance as the best 
combined ones while combined models 
outperformed very slightly only when model sizes 
are small. This is not difficult to explain. Recall that 
the predictive cluster model is a special case of the 
combined model where words are used in 
conditional positions, i.e. j=k=all. Our experiments 
show that combined models achieved good 
performance when large numbers of clusters are 
used for conditional words, i.e. large j, k>12, which 
are similar to words. 
The most interesting analysis is to look at some 
sample settings of the parameters of the combined 
cluster models in Figure 3. In Table 2, we show the 
best parameter settings at several levels of model 
size. Notice that in larger model sizes, predictive 
cluster models (i.e. j=k=all) perform the best in 
some cases. The ?prune? columns (i.e. columns 6 and 
7) indicate the Stolcke pruning parameter we used.  
First, notice that the two pruning parameters (in 
columns 6 and 7) tend to be very similar.  This is 
desirable since applying the theory of relative 
entropy pruning predicts that the two pruning 
parameters should actually have the same value.   
Next, let us compare the ACM 
P(PWil|CWi-2jCWi-1j)?P(wi|CWi-2kCWi-1kPWil) to 
traditional IBM clustering of the form 
P(Wil|Wi-2lWi-1l)?P(wi|Wil), which is equal to 
P(Wil|Wi-2lWi-1l)?P(wi|Wi-20Wi-10Wil) (assuming the   
105
110
115
120
125
130
135
140
145
150
0.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06
size
pe
rp
lex
ity
2^12 clusters
2^14 clusters
2^16 clusters
word trigram
 
Figure 1. Comparison of conditional models 
applied with different numbers of clusters 
100
105
110
115
120
125
130
135
140
145
150
0.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06size
pe
rp
lex
ity
2^4 clusters
2^6 clusters
2^8 clusters
2^10 clusters
word trigram
 
Figure 2. Comparison of predictive models 
applied with different numbers of clusters 
100
110
120
130
140
150
160
170
0.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06size
pe
rp
lex
ity
ACM
IBM
word trigram
predictive model
 
Figure 3. Comparison of ACMs, predictive 
cluster model, IBM model, and word trigram 
model 
same type of cluster is used for both predictive and 
conditional words). Our results in Figure 3 show that 
the performance of IBM models is roughly an order 
of magnitude worse than that of ACMs. This is 
because in addition to the use of the symmetric 
cluster model, the traditional IBM model makes two 
more assumptions that we consider suboptimal.  
First, it assumes that j=l.  We see that the best results 
come from unequal settings of j and l.  Second, more 
importantly, IBM clustering assumes that k=0.  We 
see that not only is the optimal setting for k not 0, but 
also typically the exact opposite is the optimal: k=all 
in which case P(wi|CWi-2kCWi-1kPWil)= 
P(wi|wi-2wi-1PWil), or k=14, 16, which is very 
similar. That is, we see that words depend on the 
previous words and that an independence 
assumption is a poor one.  Of course, many of these 
word dependencies are pruned away ? but when a 
word does depend on something, the previous words 
are better predictors than the previous clusters. 
Another important finding here is that for most of 
these settings, the unpruned model is actually larger 
than a normal trigram model ? whenever k=all or 14, 
16, the unpruned model P(PWil|CWi-2jCWi-1j) ? 
P(wi|CWi-2kCWi-1kPWil) is actually larger than an 
unpruned model P(wi|wi-2wi-1). 
This analysis of the data is very interesting ? it 
implies that the gains from clustering are not from 
compression, but rather from capturing structure.  
Factoring the model into two models, in which the 
cluster is predicted first, and then the word is 
predicted given the cluster, allows the structure and 
regularities of the model to be found. This larger, 
better structured model can be pruned more 
effectively, and it achieved better performance than 
a word trigram model at the same model size. 
Model size Perplexity l j k tc tw 
2.0E+05 141.1 8 12 14 24 24 
2.5E+05 135.7 8 12 14 12 24 
5.0E+05 118.8 6 14 16 6 12 
7.5E+05 112.8 6 16 16 3 6 
1.0E+06 109.0 6 16 16 3 3 
1.3E+06 107.4 6 16 16 2 3 
1.5E+06 106.0 6 All all 2 2 
1.9E+06 104.9 6 All all 1 2 
Table 2: Sample parameter settings for the ACM 
4.5 CER results 
Before we present CER results of the Japanese 
Kana-Kanji conversion system, we briefly describe 
our method for storing the ACM in practice.  
One of the most common methods for storing 
backoff n-gram models is to store n-gram 
probabilities (and backoff weights) in a tree 
structure, which begins with a hypothetical root 
node that branches out into unigram nodes at the first 
level of the tree, and each of those unigram nodes in 
turn branches out into bigram nodes at the second 
level and so on. To save storage, n-gram 
probabilities such as P(wi|wi-1) and backoff weights 
such as ?(wi-2wi-1) are stored in a single (bigram) 
node array (Clarkson and Rosenfeld, 1997). 
Applying the above tree structure to storing the 
ACM is a bit complicated ? there are some 
representation issues. For example, consider the 
cluster sub-model P(PWil|CWi-2jCWi-1j). N-gram 
probabilities such as P(PWil|CWi-1j) and backoff 
weights such as ?(CWi-2jCWi-1j) cannot be stored in a 
single (bigram) node array, because l ? j and 
PW?CW. Therefore, we used two separate trees to 
store probabilities and backoff weights, 
respectively. As a result, we used four tree structures 
to store ACMs in practice: two for the cluster 
sub-model P(PWil|CWi-2jCWi-1j), and two for the 
word sub-model P(wi|CWi-2kCWi-1kPWil).  We found 
that the effect of the storage structure cannot be 
ignored in a real application. 
In addition, we used several techniques to 
compress model parameters (i.e. word id, n-gram 
probability, and backoff weight, etc.) and reduce the 
storage space of models significantly. For example, 
rather than store 4-byte floating point values for all 
n-gram probabilities and backoff weights, the values 
are quantized to a small number of quantization 
levels. Quantization is performed separately on each 
of the n-gram probability and backoff weight lists, 
and separate quantization level look-up tables are 
generated for each of these sets of parameters.  We 
used 8-bit quantization, which shows no 
performance decline in our experiments. 
Our goal is to achieve the best tradeoff between 
performance and model size. Therefore, we would 
like to compare the ACM with the word trigram 
model at the same model size. Unfortunately, the 
ACM contains four sub-models and this makes it 
difficult to be pruned to a specific size. Thus for 
comparison, we always choose the ACM with 
smaller size than its competing word trigram model 
to guarantee that our evaluation is under-estimated. 
Experiments show that the ACMs achieve 
statistically significant improvements over word 
trigram models at even smaller model sizes (p-value 
=8.0E-9). Some results are shown in Table 3.  
Word trigram model ACM 
Size 
(MB) 
CER Size 
(MB) 
CER  CER 
Reduction 
1.8 4.56% 1.7 4.25% 6.8% 
5.8 4.08% 4.5 3.83% 6.1% 
11.7 4.04% 10.7 3.73% 7.7% 
23.5 4.00% 21.7 3.63% 9.3% 
42.4 3.98% 40.4 3.63% 8.8% 
Table 3:  CER results of ACMs and word 
trigram models at different model sizes 
Now we discuss why the ACM is superior to 
simple word trigrams.  In addition to the better 
structure as shown in Section 3.3, we assume here 
that the benefit of our model also comes from its 
better smoothing. Consider a probability such as 
P(Tuesday| party on). If we put the word ?Tuesday? 
into the cluster WEEKDAY, we decompose the 
probability 
When each word belongs to one class, simple math 
shows that this decomposition is a strict equality. 
However, when smoothing is taken into 
consideration, using the clustered probability will be 
more accurate than using the non-clustered 
probability. For instance, even if we have never seen 
an example of ?party on Tuesday?, perhaps we have 
seen examples of other phrases, such as ?party on 
Wednesday?; thus, the probability P(WEEKDAY | 
party on) will be relatively high. Furthermore, 
although we may never have seen an example of 
?party on WEEKDAY Tuesday?, after we backoff or 
interpolate with a lower order model, we may able to 
accurately estimate P(Tuesday | on WEEKDAY). 
Thus, our smoothed clustered estimate may be a 
good one. 
Our assumption can be tested empirically by 
following experiments.  We first constructed several 
test sets with different backoff rates4. The backoff 
rate of a test set, when presented to a trigram model, 
is defined as the number of words whose trigram 
probabilities are estimated by backoff bigram 
probabilities divided by the number of words in the 
test set.  Then for each test set, we obtained a pair of 
CER results using the ACM and the word trigram 
model respectively.  As shown in Figure 4, in both 
cases, CER increases as the backoff rate increases 
from 28% to 40%. But the curve of the word trigram 
model has a steeper upward trend.  The difference of 
the upward trends of the two curves can be shown 
more clearly by plotting the CER difference between 
them, as shown in Figure 5.  The results indicate that 
because of its better smoothing, when the backoff 
rate increases, the CER using the ACM does not 
increase as fast as that using the word trigram model.  
Therefore, we are reasonably confident that some 
portion of the benefit of the ACM comes from its 
better smoothing. 
2.1
2.3
2.5
2.7
2.9
3.1
3.3
3.5
3.7
3.9
0.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4 0.41backoff rate
er
ro
r r
ate
word trigram model
ACM
 
Figure 4: CER vs. backoff rate. 
                                                     
4  The backoff rates are estimated using the baseline 
trigram model, so the choice could be biased against the 
word trigram model. 
P(Tuesday | party on) = P(WEEKDAY | party on)? 
P(Tuesday | party on WEEKDAY). 
0.25
0.27
0.29
0.31
0.33
0.35
0.37
0.39
0.41
0.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42
backoff rate
er
ro
r r
ate
 di
ffe
re
nc
e
 
Figure 5: CER difference vs. backoff rate. 
5 Conclusion 
There are three main contributions of this paper. 
First, after presenting a formal definition of the 
ACM, we described in detail the methodology of 
constructing the ACM effectively. We showed 
empirically that both the asymmetric clustering and 
the parameter optimization (i.e. optimal cluster 
numbers) have positive impacts on the performance 
of the resulting ACM.  The finding demonstrates 
partially the effectiveness of our research focus: 
techniques for using clusters (i.e. the ACM) rather 
than techniques for finding clusters (i.e. clustering 
algorithms).  Second, we explored the actual 
representation of the ACM and evaluate it on a 
realistic application ? Japanese Kana-Kanji 
conversion.  Results show approximately 6-10% 
CER reduction of the ACMs in comparison with the 
word trigram models, even when the ACMs are 
slightly smaller.  Third, the reasons underlying the 
superiority of the ACM are analyzed. For instance, 
our analysis suggests the benefit of the ACM comes 
partially from its better structure and its better 
smoothing. 
All cluster models discussed in this paper are 
based on hard clustering, meaning that each word 
belongs to only one cluster. One area we have not 
explored is the use of soft clustering, where a word w 
can be assigned to multiple clusters W with a 
probability P(W|w) [Pereira et al, 1993]. Saul and 
Pereira [1997] demonstrated the utility of soft 
clustering and concluded that any method that 
assigns each word to a single cluster would lose 
information. It is an interesting question whether our 
techniques for hard clustering can be extended to 
soft clustering.  On the other hand, soft clustering 
models tend to be larger than hard clustering models 
because a given word can belong to multiple 
clusters, and thus a training instance P(wi|wi-2wi-1) 
can lead to multiple counts instead of just 1.  
References 
Bai, S., Li, H., Lin, Z., and Yuan, B. (1998). Building 
class-based language models with contextual statistics. In 
ICASSP-98, pp. 173-176. 
Bellegarda, J. R., Butzberger, J. W., Chow, Y. L., Coccaro, N. 
B., and Naik, D. (1996). A novel word clustering algorithm 
based on latent semantic analysis. In ICASSP-96.  
Brown, P. F., Cocke, J., DellaPietra, S. A., DellaPietra, V. J., 
Jelinek, F., Lafferty, J. D., Mercer, R. L., and Roossin, P. S. 
(1990). A statistical approach to machine translation. 
Computational Linguistics, 16(2), pp. 79-85. 
Brown, P. F., DellaPietra V. J., deSouza, P. V., Lai, J. C., and 
Mercer, R. L. (1992). Class-based n-gram models of natural 
language. Computational Linguistics, 18(4), pp. 467-479. 
Clarkson, P. R., and Rosenfeld, R. (1997). Statistical language 
modeling using the CMU-Cambridge toolkit. In Eurospeech 
1997, Rhodes, Greece. 
Gao, J. Goodman, J. and Miao, J. (2001). The use of clustering 
techniques for language model ? application to Asian 
language. Computational Linguistics and Chinese Language 
Processing. Vol. 6, No. 1, pp 27-60. 
Gao, J., Goodman, J., Li, M., and Lee, K. F. (2002). Toward a 
unified approach to statistical language modeling for Chinese. 
ACM Transactions on Asian Language Information 
Processing. Vol. 1, No. 1, pp 3-33. 
Goodman, J. (2001). A bit of progress in language modeling.  In 
Computer Speech and Language, October 2001, pp 403-434. 
Goodman, J., and Gao, J. (2000). Language model size 
reduction by predictive clustering. ICSLP-2000, Beijing. 
Jelinek, F. (1990). Self-organized language modeling for speech 
recognition. In Readings in Speech Recognition, A. Waibel 
and K. F. Lee, eds., Morgan-Kaufmann, San Mateo, CA, pp. 
450-506. 
Katz, S. M. (1987). Estimation of probabilities from sparse data 
for the language model component of a speech recognizer. 
IEEE Transactions on Acoustics, Speech and Signal 
Processing, ASSP-35(3):400-401, March. 
Kneser, R. and Ney, H. (1993).  Improved clustering techniques 
for class-based statistical language modeling. In Eurospeech, 
Vol. 2, pp. 973-976, Berlin, Germany. 
Ney, H., Essen, U., and Kneser, R. (1994). On structuring 
probabilistic dependences in stochastic language modeling. 
Computer, Speech, and Language, 8:1-38. 
Pereira, F., Tishby, N., and Lee L. (1993). Distributional 
clustering of English words. In Proceedings of the 31st Annual 
Meeting of the ACL. 
Rosenfeld, R. (2000). Two decades of statistical language 
modeling: where do we go from here. In Proceeding of the 
IEEE, 88:1270-1278, August. 
Saul, L., and Pereira, F.C.N. (1997). Aggregate and mixed-order 
Markov models for statistical language processing. In 
EMNLP-1997. 
Stolcke, A. (1998). Entropy-based Pruning of Backoff 
Language Models. Proc. DARPA News Transcription and 
Understanding Workshop, 1998, pp. 270-274. 
Ueberla, J. P. (1996). An extended clustering algorithm for 
statistical language models. IEEE Transactions on Speech 
and Audio Processing, 4(4): 313-316. 
Yamamoto, H., Isogai, S., and Sagisaka, Y. (2001). Multi-Class 
Composite N-gram Language Model for Spoken Language 
Processing Using Multiple Word Clusters. 39th Annual 
meetings of the Association for Computational Linguistics 
(ACL?01), Toulouse, 6-11 July 2001. 
Yamamoto, H., and Sagisaka, Y. (1999). Multi-class Composite 
N-gram based on Connection Direction, In Proceedings of the 
IEEE International Conference on Acoustics, Speech and 
Signal Processing, May, Phoenix, Arizona. 
  
Combining Linguistic Features with Weighted Bayesian Classifier 
for Temporal Reference Processing 
 
Guihong Cao 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong 
csghcao@comp.polyu.edu.hk 
Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong
cswjli@comp.polyu.edu.hk 
Kam-Fai Wong 
Department of Systems Engineering and Engineering 
Management 
The Chinese University of Hong Kong, Hong Kong 
kfwong@se.cuhk.edu.hk 
Chunfa Yuan 
Department of Computer Science and Technology 
Tsinghua University, Beijing, China. 
cfyuan@tsinghua.edu.cn 
 
Abstract 
Temporal reference is an issue of determining 
how events relate to one another. Determining 
temporal relations relies on the combination of 
the information, which is explicit or implicit in 
a language. This paper reports a computational 
model for determining temporal relations in 
Chinese. The model takes into account the ef-
fects of linguistic features, such as tense/aspect, 
temporal connectives, and discourse structures, 
and makes use of the fact that events are repre-
sented in different temporal structures. A ma-
chine learning approach, Weighted Bayesian 
Classifier, is developed to map their combined 
effects to the corresponding relations. An em-
pirical study is conducted to investigate differ-
ent combination methods, including lexical-
based, grammatical-based, and role-based 
methods. When used in combination, the 
weights of the features may not be equal. Incor-
porating with an optimization algorithm, the 
weights are fine tuned and the improvement is 
remarkable. 
 
1 Introduction 
Temporal information describes changes and time 
of the changes. In a language, the time of an event 
may be specified explicitly, for example ????
1997??????????? (They solved the traf-
fic problem of the city in 1997)?; or it may be related 
to the time of another event, for example ?????
???, ???????????? (They solved the 
traffic problem of the city after the street bridge had 
been built?. Temporal reference describes how 
events relate to one another, which is essential to 
natural language processing (NLP). Its major appli-
cations cover syntactic structural disambiguation 
(Brent, 1990), information extraction and question 
answering (Li, 2002), language generation and ma-
chine translation (Dorr, 2002). 
Many researchers have attempted to characterize 
the nature of temporal reference in a discourse. Iden-
tifying temporal relations1 between two events de-
                                                 
1 The relations under examined include both intra-sentence and inter-
pends on a combination of information resources. 
This information is provided by explicit tense and 
aspect markers, implicit event classes or discourse 
structures. It has been used to explain semantics of 
temporal expressions (Moens, 1988; Webber, 1988), 
to constrain possible temporal interpretations 
(Hitzeman, 1995; Sing, 1997), or to generate appro-
priate temporally conjoined clauses (Dorr, 2002). 
The purpose of our work is to develop a computa-
tional model, which automatically determines tempo-
ral relations in Chinese. While temporal reference 
interpretation in English has been well studied, Chi-
nese has been rarely discussed. In our study, thirteen 
related features are identified from linguistic per-
spective. How to combine these features and how to 
map their combined effects to the corresponding rela-
tions are the critical issues to be addressed in this 
paper. 
Previous work was limited in that they just con-
structed constraint or preference rules for some rep-
resentative examples. These methods are ineffective 
for computing purpose, especially when a large 
number of the features are involved and the interac-
tion among them is unclear. Therefore, a machine 
learning approach is applied and the empirical stud-
ies are carried out in our work. 
The rest of this paper is organized as follows. Sec-
tion 2 introduces temporal relation representations. 
Section 3 provides linguistic background of temporal 
reference and investigates linguistic features for de-
termining temporal relations in Chinese. Section 4 
explains the methods used to combine linguistic fea-
tures with Bayesian Classifier. It is followed by a 
description of the optimization algorithm which is 
used for estimating feature weights in Section 5. Fi-
nally, Section 6 concludes the paper. 
2 Representing Temporal Relations 
With the growing interests to temporal information 
processing in NLP, a variety of temporal systems 
have been introduced to accommodate the character-
istics of temporal information. In order to process 
temporal reference in a discourse, a formal represen-
                                                                            
sentence relations. 
  
tation of temporal relations is required. Among those 
who worked on representing or explaining temporal 
relations, some have taken the work of Reichenbach 
(Reichenbach, 1947) as a starting point, while others 
based their works on Allen?s (Allen, 1983). 
Reichenbach proposed a point-based temporal the-
ory. Reichenbach?s representation associated English 
tenses and aspects with three time points, namely 
event time (E), speech time (S) and reference time 
(R). The reference of E-R and R-S was either before 
(or after in reverse order) or simultaneous. This the-
ory was later enhanced by Bruce who defined seven 
temporal relations (Bruce, 1972). Given two durative 
events, the interval relations between them were 
modeled by the order between the greatest lower 
bounding point and least upper bounding point of the 
two events. In the other camp, instead of adopting 
time points, Allen took intervals as temporal primi-
tives to facilitate temporal reasoning and introduced 
thirteen basic relations. In this interval-based repre-
sentation, points were relegated to a subsidiary status 
as ?meeting places? of intervals. An extension to 
Allen?s theory, which treated both points and inter-
vals as primitives on an equal footing, was later in-
vestigated by Knight and Ma (Knight, 1994). 
In natural languages, events described can be ei-
ther punctual or durative in nature. A punctual event, 
e.g., ?? (explore), occurs instantaneously. It takes 
time but does not last in a sense that it lacks of a 
process of change. It is adequate to represent a punc-
tual event with a simple point structure. Whilst, a 
durative event, e.g., ?? (built a house), is more 
complex and its accomplishment as a whole involves 
a process spreading in time. Representing a durative 
event requires an interval representation. For this 
reason, Knight and Ma?s model is adopted in our 
work (see Figure 1). Taking the sentence ?????
???, ???????????? (They solved the 
traffic problem of the city after the street bridge had 
been built)? as an example, the relation held between 
building the bridge (i.e., an interval) and solving the 
problem (i.e., a point) is BEFORE. 
 
Figure 1 13 relations represented with points and intervals 
3 Linguistic Background of Temporal Refer-
ence in a Discourse 
3.1 Literature Review 
There were a number of theories in the literature 
about how temporal relations between events can be 
determined in English. Most of the researches on 
temporal reference were based on Reichenbach?s 
notion of tense/aspect structure, which was known as 
Basic Tense Structure (BTS). As for relating two 
events adjoined by a temporal/causal connective, 
Hornstein (Hornstein, 1990) proposed a neo-
Reichenbach structure which organized the BTSs 
into a Complex Tense Structure (CTS). It has been 
argued that all sentences containing a matrix and an 
adjunct clause were subject to linguistic constraints 
on tense structure regardless of the lexical words in-
cluded in the sentence. Generally, constraints were 
used to support syntactic disambiguation (Brent, 
1990) or to generate acceptable sentences (Dorr, 
2002). 
In a given CTS, a past perfect clause should pre-
cede the event described by a simple past clause. 
However, the order of two events in CTS does not 
necessarily correspond to the order imposed by the 
interpretation of the connective (Dorr, 2002). Tem-
poral/casual connective, such as ?after?, ?before? or 
?because?, can supply explicit information about the 
temporal ordering of events. Passonneau (Passon-
neau, 1988), Brent (Brent, 1990 and Sing (Sing, 1997) 
determined intra-sentential relations by accounting 
for temporal or causal connectives. Dorr and Gaast-
erland (Dorr, 2002), on the other hand, studied how 
to generate the sentences which reflect event tempo-
ral relations by selecting proper connecting words. 
However, temporal connectives can be ambiguous. 
For instance, a ?when? clause permits many possible 
temporal relations. 
Several researchers have developed the models 
that incorporated aspectual types (such as those dis-
tinct from states, processes and events) to interpret 
temporal relations between clauses connected with 
?when?. Moens and Steedmen (Moens, 1988) devel-
oped a tripartite structure of events2, and emphasized 
it was the notion of causation and consequence that 
played a central role in defining temporal relations of 
events. Webber (Webber, 1988) improved upon the 
above work by specifying rules for how events are 
related to one another in a discourse and Sing and 
Sing defined semantic constraints through which 
events can be related (Sing, 1997). The importance 
of aspectual information in retrieving proper aspects 
and connectives for sentence generation was also 
recognized by Dorr and Gaasterland (Dorr, 2002). 
Some literature claimed that discourse structures 
suggested temporal relations. Lascarides and Asher 
(Lascarides, 1991) investigated various contextual 
effects on rhetorical relations (such as narration, 
elaboration, explanation, background and result). 
They corresponded each of the discourse relations to 
a kind of temporal relation. Later, Hitzeman (Hitze-
man, 1995) described a method for analyzing tempo-
ral structure of a discourse by taking into account the 
effects of tense, aspect, temporal adverbials and rhe-
                                                 
2  The structure comprises a culmination, an associated preparatory 
process and a consequence state. 
A punctual event (i.e. represented in time point) 
A durative event (i.e. represented in time interval) 
BEFORE/AFTER 
MEETS/MET-BY 
OVERLAPS/OVERLAPPED-BY 
STARTS/STARTED-BY 
DURING/CONTAINS 
FINISHES/FINISHED-BY 
SAME-AS 
  
torical relations. A hierarchy of rhetorical and tempo-
ral relations was adopted so that they could mutually 
constrain each other.  
 To summarize, the interpretation of temporal rela-
tions draws on the combination of various informa-
tion resources, including explicit tense/aspect and 
connectives (temporal or otherwise), temporal 
classes implicit in events, or rhetorical relations hid-
den in a discourse. This conclusion, although drawn 
from the studies of English, provides the common 
understanding on what information is required for 
determining temporal relations across languages. 
3.2 Linguistic Features for Determining Tem-
poral Relations in Chinese 
Thirteen related linguistic features are recognized 
for determining Chinese temporal relations in this 
paper (See Table 1). The selected features are scat-
tered in various grammatical categories due to the 
unique nature of language, but they fall into the fol-
lowing three groups. 
(1) Tense/aspect in English is manifested by verb 
inflections. But such morphological variations are 
inapplicable to Chinese verbs. Instead, they are 
conveyed lexically. In other words, tense and as-
pect in Chinese are expressed using a combination 
of, for example, time words, auxiliaries, temporal 
position words, adverbs and prepositions, and 
particular verbs. They are known as Tense/Aspect 
Markers. 
(2) Temporal Connectives in English primarily in-
volve conjunctions, such as ?after? and ?before?, 
which are the key components in discourse struc-
tures. In Chinese, however, conjunctions, conjunc-
tive adverbs, prepositions and position words, or 
their combinations are required to represent 
connectives. A few verbs that express cause/effect 
imply a temporal relation. They are also regarded 
as a feature relating to discourse structure3. The 
words which contribute to the tense/aspect and 
temporal connective expressions are explicit in a 
sentence and generally known as Temporal Indica-
                                                 
3 The casual conjunctions such as ?because? are included in this 
group. 
tors. 
(3) Event Classes are implicit in a sentence. Events 
can be classified according to their inherent tem-
poral characteristics, such as the degree of telicity 
and atomicity. The four widespread accepted tem-
poral classes are state, process, punctual event and 
developing event (Li, 2002). Based on their 
classes, events interact with the tense/aspect of 
verbs to determine the temporal relations between 
two events. 
Temporal indicators and event classes are both re-
ferred to as Linguistic Features. Table 1 shows the 
association between a temporal indicator and its ef-
fects. Note that the association is not one-to-one. For 
example, adverbs affect tense/aspect (e.g. ?, being) 
as well as discourse structure (e.g. ?, at the same 
time). For another example, tense/aspect can be 
jointly affected by auxiliary words (e.g. ? , 
were/was), trend verbs (??, begin to), and so on. 
Obviously, it is not a simple task to map the com-
bined effects of the thirteen linguistic features to the 
corresponding relations. Therefore, a machine learn-
ing approach is proposed, which investigates how 
these features contribute to the task and how they 
should be combined. 
4 Combining Linguistic Features with Machine 
Learning Approach 
Previous efforts in corpus-based NLP have incor-
porated machine learning methods to coordinate mul-
tiple linguistic features, for example, in accent resto-
ration (Yarowsky, 1994) and event classification 
(Siegel, 1998).  
Temporal relation determination can be modeled 
as a relation classification task. We formulate the 
thirteen temporal relations (see Figure 1) as the 
classes to be decided by a classifier. The classifica-
tion process is to assign an event pair to one class 
according to their linguistic features. There existed 
numerous classification algorithms based upon su-
pervised learning principle. One of the most effective 
classifiers is Bayesian Classifier, introduced by Duda 
and Hart (Duda, 1973) and analyzed in more detail 
by Langley and Thompson (Langley, 1992). Its pre-
dictive performance is competitive with state-of-the-
Linguistic Feature Symbol POS Tag Effect Example 
With/Without punctuations PT Not Applicable Not Applicable Not Applicable 
Speech verbs VS TI_vs Tense ??, ??, ? 
Trend verbs TR TI_tr Aspect ??, ?? 
Preposition words P TI_p Discourse Structure/Aspect ?, ?, ? 
Position words PS TI_f Discourse Structure ?, ?, ?? 
Verbs with verb objects VV TI_vv Tense/Aspect ??, ??, ? 
Verbs expressing wish/hope VA TI_va Tense ??, ?, ? 
Verbs related to causality VC TI_vc Discourse Structure ??, ??, ?? 
Conjunctive words C TI_c Discourse Structure ?, ??, ?? 
Auxiliary words U TI_u Aspect ?, ?, ? 
Time words T TI_t Tense ??, ??, ?? 
Adverbs D TI_d Tense/Aspect/Discourse Structure ?, ?, ??, ? 
Event class EC E0/E1/E2/E3 Event Classification State, Punctual Event, De-
veloping Event, Process 
Table 1 Linguistic features: eleven temporal indicators and one event class 
  
art classifiers, such as C4.5 and SVM (Friedman, 
1997). 
4.1 Bayesian Classifier 
Given the class c , Bayesian Classifier learns from 
training data the conditional probability of each at-
tribute. Classification is performed by applying 
Bayes rule to compute the posterior probability of c  
given a particular instance x , and then predicting the 
class with the highest posterior probability ratio. Let 
],...,,,,[ 2121 nttteex = , Eee ?21 ,  are the two event 
classes and Tttt n ?,...,, 21 are the temporal indicators 
(i.e. the words). E is the set of event classes. T is the 
set of temporal indicators. Then x is classified as: 
???
?
???
?=
),...,,,,|(
),...,,,,|(
logmaxarg
2121
2121*
n
n
c ttteecP
ttteecP
c  (E1)
where c denotes the classes different from c . As-
suming event classes are independent of temporal 
indicators given c , we have: 
???
?
???
?=
???
?
???
?
)()|,...,,,,(
)()|,...,,,,(
log
),...,,,,|(
),...,,,,|(
log
2121
2121
2121
2121
cPcttteeP
cPcttteeP
ttteecP
ttteecP
n
n
n
n
                          (E2)
???
?
???
?+???
?
???
?+???
?
???
?=
)|,...,,(
)|,...,,(
log
)|,(
)|,(
log
)(
)(log
21
21
21
21
ctttP
ctttP
ceeP
ceeP
cP
cP
n
n  
Assuming temporal indicators are independent of 
each other, we have 
?
=
= n
i i
i
n
n
ctP
ctP
ctttP
ctttP
121
21
)|(
)|(
)|,...,,(
)|,...,,( ,    ( ni ,...2,1= ) (E3)
A Na?ve Bayesian Classifier assumes strict inde-
pendence among all attributes. However, this as-
sumption is not satisfactory in the context of tempo-
ral relation determination. For example, if the 
relation between 1e  and 2e  is SAME_AS, 1e  and 2e  
have to be identical. We release the independence 
assumption for 1e and 2e , and decompose the second 
part of (E2) as: 
),|()|(
),|()|(
)|,(
)|,(
121
121
21
21
ceePceP
ceePceP
ceeP
ceeP =  (E4)
Estimation of ),|( 12 ceep is motivated by Absolute 
Discounting N-Gram language model (Goodman, 
2001): 
??
??
?
=
>?=
0),,( if     )|(),(
0),,( if    
),(
),,(
),|(
1221
12
1
12
12
ceeCcePce
ceeC
ceC
DceeC
ceP e
?
 
 
(E5) 
here D is the discount factor and is set to 0.5 experi-
mentally. From the fact that 1),|(
2
12 =?
e
ceeP , we get: 
?
?
>
>
?
?
=
0),,(|
2
0),,(|
12
1
122
122
)|(1
),|(1
),(
ceeCe
ceeCe
ceP
ceeP
ce?  
 
(E6)
)|( ctp i and )|( cep i  are estimated by MLE with 
Dirichlet Smoothing method: 
?
?
+
+=
Tt
i
i
i
i
TuctC
uctC
ctP
||),(
),(
)|(      ( ni ,...2,1= )  
(E7)
?
?
+
+=
Ee
i
i
i
i
EuceC
uceC
ceP
||),(
),(
)|(     ( 2,1=i )  
(E8)
where u (=0.5) is the smoothing factor. Then, 
)|( ctp i , )|( cep i and ),|( 12 ceeP  can be estimated 
with (E5) - (E8) by substituting c  with c . 
4.2 Estimating )|,...,( 21 ctttP n  with Lexical-POS 
Information 
The effects of a temporal indicator are constrained 
by its positions in a sentence. For instance, the con-
junctive word ?? (because) may represent the dif-
ferent relations when it occurs before or after the first 
event. Therefore, in estimating )|,...,( 21 ctttp n , we 
consider an indicator located in three positions: (1) 
BEFORE the first event; (2) AFTER the first event 
and BEFORE the second and it modifies the first 
event; (3) the same as (2) but it modifies the second 
event; and (4) AFTER the second event. Note that 
cases (2) and (3) are ambiguous. The positions of the 
temporal indicators are the same. But it is uncertain 
whether these indicators modify the first or the sec-
ond event if there is no punctuation (such as comma, 
period, exclamation or question mark) separating 
their roles. The ambiguity is resolved by using POS 
information. We assume that an indicator modifies 
the first event if it is an auxiliary word, a trend word 
or a position word; otherwise it modifies the second. 
Thus, we rewrite )|,...,( 21 ctttP n as ,,...,( 1111 nttP  
)|,...,,...,,,...,
432 441331221
ctttttt nnn , where jn is the total 
number of the temporal indicators occurring in the 
position j . 4,3,2,1=j  represents the four positions 
and nn
j
j =?=
4
1
. Assuming jit are independent of each 
other, then ?
=
n
i
i ctP
1
)|( in (E3) is revised as 
??
= =
4
1 1
)|(
j
n
i
ji
j
ctP . Accordingly, (E7) is revised as: 
?
?
+
+=
Tt
ji
ji
ji
ji
TuctC
uctC
ctP
||),(
),(
)|(  
( 4,3,2,1=j  and jni ,...2,1= ) 
 
(E7?)
In addition to taking positions into account, we 
further classify the temporal indicators into two 
groups according to their grammatical categories or 
semantic roles. The rationale of grouping will be 
demonstrated in Section 4.3. 
4.3 Experimental Results 
Several experiments have been designed to evalu-
ate the proposed Bayesian Classifier in combining 
linguistic features for temporal relation determination 
and to reveal the impact of linguistic features on 
learning performance. 700 instances are extracted 
from Ta Kong Pao (a local Hong Kong Chinese 
newspaper) financial version. Among them, 500 are 
used as training data, and 200 as test data, which are 
  
partitioned equally into two sets. One is similar as 
training data in class distribution, while the other is 
quite different. 209 lexical words, gathered from lin-
guistic books and corpus, are used as the temporal 
indicators and manually marked with the tags given 
in Table 1. 
4.3.1 Impact of Individual Features 
From linguistic perspective, the thirteen features 
(see Table 1) are useful for temporal relation deter-
mination. To examine the impact of each individual 
feature, we feed a single linguistic feature to the 
Bayesian Classifier learning algorithm one at a time 
and study the accuracy of the resultant classifier. The 
experimental results are given in Table 2. It shows 
that event classes have greatest accuracy, followed 
by conjunctions in the second place, and adverbs in 
the third in the close test. Since punctuation shows 
no contribution, we only use it as a syntactic feature 
to differentiate cases (2) and (3) mentioned in Sec-
tion 4.2. 
4.3.2 Features in Combination 
We now use Bayesian Classifier introduced in Sec-
tions 4.1 and 4.2 to combine all the related temporal 
indicators and event classes, since none of the fea-
tures can achieve a good result alone. The simplest 
way is to combine the features without distinction. 
The conditional probability )|( ctP ji is estimated by 
(E7?). This model is called Ungrouped Model (UG). 
However, as illustrated in table 1, the temporal in-
dicators play different roles in building temporal ref-
erence. It is not reasonable to treat them equally. We 
claim that the temporal indicators have two functions, 
i.e., representing the connections of the clauses, or 
representing the tense/aspect of the events. We iden-
tify them as connective words or tense/aspect mark-
ers and separate them into two groups. This allows 
features to be compared with those in the same group. 
Let ],[ 21 TTT = , where 1T is the set of connective 
words and 2T is the set of tense/aspect markers. We 
have 1112
1
1 ,..,, Tttt m ? and 222221 ,..,, Tttt l ? , m and l are 
the number of the connective words and the 
tense/aspect markers in a sentence respectively. We 
assume that the occurrences of the two groups are 
independent. By taking both grouping and position 
features into account, we replace ?
=
n
i
i ctP
1
)|(  with 
???
= = =
2
1
4
1 1
)|(
k j
n
i
k
ji
k
j
ctP , 2,1=k  represents the two groups 
and j
k
k
j nn =?=
2
1
. To build the grouping-based Bayes-
ian Classifier, (E7?) is modified as: 
?
?
+
+=
kk
ji Tt
kk
ji
k
jik
ji TuctC
uctC
ctP
||),(
),(
)|(  
( 2,1=k , 4,3,2,1=j  and jni ,...2,1= ) 
 
(E7??)
4.3.3 Grouping Features by Grammatical Cate-
gories or Semantic Roles 
We partition temporal indicators into connective 
words and tense/aspect markers in two ways. One is 
simply based on their grammatical categories (i.e. 
POS information). It separates conjunctions (e.g., ?
?, after; ??, because) and verbs relating to causal-
ity (e.g., ??, cause) from others. They are assumed 
to be connective words (i.e. 1T? ), while others are 
tense/aspect markers (i.e. 2T? ). This model is called 
Grammatical Function based Grouping Model (GFG). 
Unfortunately, such a separation is ineffective. In 
comparison with UG, the performance of GFG de-
creases as shown in figure 2. This reveals the com-
plexity of Chinese in connecting expressions. It 
arises from the fact that some other words, such as 
adverbs (e.g., ???, meanwhile), prepositions (e.g., 
?, at) and position words (e.g., ??, before), can 
also serve such a connecting function (see Table 1). 
Actually, the roles of the words falling into these 
grammatical categories are ambiguous. For instance, 
the adverb? can express an event happened in the 
past, e.g., ????????? (He just finished the 
report)?. It can be also used in a connecting expres-
sion (such as ????), e.g., ??????????
??? (He went to the library after he had finished 
the report)?. 
This finding suggests that temporal indicators 
should be divided into two groups according to their 
semantic roles rather than grammatical categories. 
Therefore we propose the third model, namely 
Semantic Role based Grouping Model (SRG), in 
which the indicators are manually re-marked as 
TI_j_pos or TI_at_pos4. 
Figure 2 shows the accuracies of four models (i.e. 
DM. UG, GFG and SRG) based on the three tests. 
Test 1 is the close test carried out on training data 
and tests 2 and 3 are open tests performed on differ-
ent test data. DM (i.e., Default Model) assigns all 
incoming cases with the most likely class and it is 
used as evaluation baseline. In our case, it is 
SAME_AS, which holds 50.2% in training data. 
SRG model outperforms UG and GFG models. 
These results validate our previous assumption em-
pirically. 
                                                 
4 ?j? and ?at? are the tags representing connecting and tense/aspect 
roles respectively. ?pos? is the POS tag of the temporal indicator TI. 
Accuracy Accuracy  
Feature Close 
test 
Open 
test 1 
Open 
test 2 
 
Feature Close 
test 
Open 
test 1
Open 
test 2
VS 53.4% 48% 30% VA 57% 50% 37%
VC 56.6% 56% 49% C 62.6% 52% 45%
TR 50.2% 46% 28% U 51.8% 50% 32%
P 52.4% 49% 30% T 57.2% 48% 32%
PS 59% 53% 38% D 59.6% 55% 47%
VV 51% 49% 29% EC 72.4% 69% 68%
Table 2 Impact of each individual linguistic feature 
  
20%
30%
40%
50%
60%
70%
80%
90%
Close Test Open Test1 Open Test2
A
cc
ur
ac
y
DM UG GFG SRG
 
Figure 2 Comparing DM, UG, GFG and SRG models 
4.3.4 Impact of Semantic Roles in SRG Model 
When the temporal indicators are classified into 
two groups based on their semantic roles in SRG 
model, there are three types of linguistic features 
used in the Bayesian Classifier, i.e., tense/aspect 
markers, connective words and event classes. A set 
of experiments are conducted to investigate the im-
pacts of each individual feature type and the impacts 
when they are used in combination (shown in Table 
3). We find that the performance of methods 1 and 2 
in the open tests drops dramatically compared with 
those in the close test. But the predictive strength of 
event classes in method 3 is surprisingly high. Two 
conclusions are thus drawn. Firstly, the models using 
tense/aspect markers and connective words are more 
likely to encounter over-fitting problem with insuffi-
cient training data. Secondly, different features have 
varied weights. We then incorporate an optimization 
approach to adjust the weights of the three types of 
features, and propose an algorithm to tackle over-
fitting problem in the next section. 
Method Semantic Groups 
Close 
test 
Open 
test 1 
Open 
test 2
1 Tense/aspect markers 71% 58% 40%
2 Connective words 75% 65% 57%
3 Event classes 66.6% 69% 68%
4 1+2 84.8% 70% 56%
5 1+3 76.6% 72% 66%
6 2+3 82.4% 84% 81%
7 1+2+3 89.8% 84% 80%
8 Default 50.2% 46% 28%
Table 3: Impact of Semantic Role based Groups 
5. Weighted Bayesian Classifier  
Let 1? , 2? , 3? be the weights of event classes, con-
nective words and tense/aspect markers respectively. 
Then the Weighted Bayesian Classifier is: 
???
?
???
?
),...,,,,|(
),...,,,,|(
log
2121
2121
n
n
ttteecP
ttteecP  
???
?
???
?+???
?
???
?=
)|,(
)|,(
log
)(
)(log
21
21
1 ceeP
ceeP
cP
cP ?                             (E9)
???
?
???
?+???
?
???
?+
)|,...,,(
)|,...,,(
log
)|,...,,(
)|,...,,(
log 22
2
2
1
22
2
2
1
311
2
1
1
11
2
1
1
2 ctttP
ctttP
ctttP
ctttP
l
l
m
m ??  
In order to estimate the weights, we need a suit-
able optimization approach to search for the opti-
mal value of ],,[ 321 ???  automatically. 
5.1 Estimating Weights with Simulated Anneal-
ing Algorithm 
Quite a lot optimization approaches are available 
to compute the optimal value of ],,[ 321 ??? . Here, 
Simulated Annealing algorithm is employed to per-
form the task, which is a general and powerful opti-
mization approach with excellent global convergence 
(Kirkpatrick, 1983). Figure 3 shows the procedure of 
searching for an optimal weight vector with the algo-
rithm. 
1. 1=k , )( 1?= kk tTt  
2. Generates a random change from the current weight vec-
tor iv . The updated weight vector is denoted by jv . Then 
computes the increasement of the objective function, i.e. 
)()( ij vfvf ?=? . 
3 Accepts jv as an optimal vector and substitutes iv with the 
following accept rate: 
??
??
?
??
?
? <
>
= 0 if  )exp(
0 if             1
)(
k
ji
t
vvP  
4 If kLk < , lets 1+= kk , goes to step 2. 
5 Else if fk Tt < , goes to step 1. 
6 Else stops looping and outputs the current optimal weight 
vector.  
Figure 3 Simulated Annealing algorithm 
In Figure 3, Markov chain length 20=kL ; tem-
perature update function ttT *9.0)( = ; starting point 
],,[ 03
0
2
0
1
0 ???=v =[1,1,1]; initial temperature 200 =t  
and final temperature 810?=ft . Note that the initial 
temperature is critical for a simulated annealing algo-
rithm (Kirkpatrick, 1983). Its value should assure 
that the initial accept rate is greater than 90%. 
5.2 K-fold Cross-Validation 
The accuracy of the classifier is defined as the ob-
jective function of the Simulated Annealing algo-
rithm illustrated in Figure 3. If it is evaluated with 
the accuracy over all training data, the Weighted 
Bayesian Classifier may trap into over-fitting prob-
lem and lower the performance due to insufficient 
data. To avoid this, we employ K-fold Cross-
Validation technique. It partitions the original set of 
data into K parts. One part is selected arbitrarily as 
evaluating data and the other K-1 parts as training 
data. Then K accuracies on evaluating data are ob-
tained after K iterations and their average is used as 
the objective function. 
5.3 Experimental Results 
Table 4 shows the result of the experiment which 
compares WSRG (Weighted SRG) with SRG. We 
use error reduction to evaluate the benefit from in-
corporating weight parameters into Bayesian Classi-
fier. It is defined as: 
SRG
WSRGSRG
rateerror
rateerrorrateerror
_
__
reductionerror 
?=  
  
The experimental results show that the Weighted 
Bayesian Classifier outperforms the Bayesian Classi-
fier significantly in the two open tests and it tackles 
the over-fitting problem well. To test Simulated An-
nealing algorithm?s global convergence, we ran-
domly choose several initial values and they finally 
converge to a small area [7.2?0.09, 5.8?0.02, 
3.0?0.02]. The empirical result demonstrates that the 
output of a Simulated Annealing algorithm is a 
global optimal weighting vector. 
6 Conclusions 
Temporal reference processing has received grow-
ing attentions in last decades. However this topic has 
not been well studied in Chinese. In this paper, we 
proposed a method to determine temporal relations in 
Chinese by employing linguistic knowledge and ma-
chine learning approaches. Thirteen related linguistic 
features were recognized and temporal indicators 
were further grouped with respect to grammatical 
functions or semantic roles. This allows features to 
be compared with those in the same group. To ac-
commodate the fact that the different types of fea-
tures support varied importance, we extended Na?ve 
Bayesian Classifier to Weighted Bayesian Classifier 
and applied Simulated Annealing algorithm to opti-
mize weight parameters. To avoid over-fitting prob-
lem, K-fold Cross-Validation technique was incorpo-
rated to evaluate the objective function of the optimi-
zation algorithm. Establishing the temporal relations 
between two events could be extended to provide a 
determination of the temporal relations among multi-
ple events in a discourse. With such an extension, 
this temporal analysis approach could be incorpo-
rated into various NLP applications, such as question 
answering and machine translation. 
Acknowledgements 
The work presented in this paper is partially sup-
ported by Research Grants Council of Hong Kong 
(RGC reference number PolyU5085/02E) and CUHK 
Strategic Grant (account number 4410001). 
References 
Allen J., 1983. Maintaining Knowledge about Temporal 
Intervals. Communications of the ACM, 26(11):832-
843. 
Brent M., 1990. A Simplified Theory of Tense Repre-
sentations and Constraints on Their Composition, In 
Proceedings of the 28th Annual Conference of the As-
sociation for Computational Linguistics, pages 119-
126. Pittsburgh. 
Bruce B., 1972. A Model for Temporal References and 
its Application in Question-Answering Program. Arti-
ficial Intelligence, 3(1):1-25. 
Dorr B. and Gaasterland T., 2002. Constraints on the 
Generation of Tense, Aspect, and Connecting Words 
from Temporal Expressions. submitted to Journal of 
Artificial Intelligence Research. 
Duda, R. O. and P. E. Hart, 1973. Pattern Classification 
and Scene Analysis. New York. 
Friedman N., Geiger D. and Goldszmidt M., 1997. 
Bayesian Network Classifiers. Machine Learning 
29:131-163, Kluwer Academic Publisher. 
Goodman J., 2001. A Bit of Progress in Language Mod-
eling. Microsoft Research Technical Report MSR-
TR-2001-72. 
Hitzeman J., Moens M. and Grover C., 1995. Algo-
rithms for Analyzing the Temporal Structure of Dis-
course. In Proceedings of the 7th European Meeting 
of the Association for Computational Linguistics, 
pages 253-260. Dublin, Ireland.  
Hornstein N., 1990. As Time Goes By. MIT Press, Cam-
bridge, MA. 
Kirkpatrick, S., Gelatt C.D., and Vecchi M.P., 1983. 
Optimization by Simulated Annealing. Science, 
220(4598): 671-680. 
Knight B. and Ma J., 1997. Temporal Management Us-
ing Relative Time in Knowledge-based Process Con-
trol, Engineering Applications of Artificial Intelli-
gence, 10(3):269-280.  
Langley, P.W. and Thompson K., 1992. An Analysis of 
Bayesian Classifiers. In Proceedings of the 10th Na-
tional Conference on Artificial Intelligence, pages 
223?228. San Jose, CA. 
Lascarides A. and Asher N., 1991. Discourse Relations 
and Defensible Knowledge. In Proceedings of the 29th 
Meeting of the Association for Computational Lin-
guistics, pages 55-62. Berkeley, USA. 
Li W.J. and Wong K.F., 2002. A Word-based Approach 
for Modeling and Discovering Temporal Relations 
Embedded in Chinese Sentences, ACM Transaction 
on Asian Language Processing, 1(3):173-206. 
Moens M. and Steedmen M., 1988. Temporal Ontology 
and Temporal Reference. Computational Linguistics, 
14(2):15-28. 
Passonneau R., 1988. A Computational Model of the 
Semantics of Tense and Aspect. Computational Lin-
guistics, 14(2):44-60. 
Reichenbach H., 1947. The Elements of Symbolic Logic. 
The Free Press, New York. 
Siegel E.V. and McKeown K.R., 2000. Learning Meth-
ods to Combine Linguistic Indicators: Improving As-
pectual Classification and Revealing Linguistic In-
sights. Computational Linguistics, 26(4):595-627. 
Singh M. and Singh M., 1997. On the Temporal Struc-
ture of Events. In Proceedings of AAAI-97 Workshop 
on Spatial and Temporal Reasoning, pages 49-54. 
Providence, Rhode Island. 
Webber B., 1988. Tense as Discourse Anaphor. Compu-
tational Linguistics, 14(2):61-73.  
Yarowsky D., 1994. Decision Lists for Lexical Ambi-
guity Resolution: Application to the Accent Restora-
tion in Spanish and French. In Proceeding of the 32nd 
Annual Meeting of the Association for Computational 
Linguistics, pages 88-95. San Francisco, CA. 
Error Rate 
Model 
Close Test Open Test1 Open Test2
SRG 10.2% 16% 20% 
WSRG 12.4%% 11% 13% 
Error Reduction -21.57% 31.25% 35% 
Table 4 Compare WSRG with SRG on error rates 
 
Applying Machine Learning to Chinese Temporal Relation Resolution 
 
Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong
cswjli@comp.polyu.edu.hk 
Kam-Fai Wong 
Department of Systems Engineering and Engineering 
Management 
The Chinese University of Hong Kong, Hong Kong
kfwong@se.cuhk.edu.hk 
Guihong Cao 
Department of Computing 
The Hong Kong Polytechnic University, Hong Kong
csghcao@comp.polyu.edu.hk 
Chunfa Yuan 
Department of Computer Science and Technology 
Tsinghua University, Beijing, China. 
cfyuan@tsinghua.edu.cn 
 
 
 
Abstract 
Temporal relation resolution involves extraction 
of temporal information explicitly or implicitly 
embedded in a language. This information is of-
ten inferred from a variety of interactive gram-
matical and lexical cues, especially in Chinese. 
For this purpose, inter-clause relations (tempo-
ral or otherwise) in a multiple-clause sentence 
play an important role. In this paper, a computa-
tional model based on machine learning and 
heterogeneous collaborative bootstrapping is 
proposed for analyzing temporal relations in a 
Chinese multiple-clause sentence. The model 
makes use of the fact that events are represented 
in different temporal structures. It takes into ac-
count the effects of linguistic features such as 
tense/aspect, temporal connectives, and dis-
course structures. A set of experiments has been 
conducted to investigate how linguistic features 
could affect temporal relation resolution.  
 
1 Introduction 
In language studies, temporal information de-
scribes changes and time of changes expressed in a 
language. Such information is critical in many typi-
cal natural language processing (NLP) applications, 
e.g. language generation and machine translation, etc. 
Modeling temporal aspects of an event in a written 
text is more complex than capturing time in a physi-
cal time-stamped system. Event time may be speci-
fied explicitly in a sentence, e.g. ???? 1997??
????????? (They solved the traffic prob-
lem of the city in 1997)?; or it may be left implicit, to 
be recovered by readers from context. For example, 
one may know that ???????????????
?????? (after the street bridge had been built, 
they solved the traffic problem of the city)?, yet 
without knowing the exact time when the street 
bridge was built. As reported by Partee (Partee, 
1984), the expression of relative temporal relations 
in which precise times are not stated is common in 
natural language. The objective of relative temporal 
relation resolution is to determine the type of rela-
tive relation embedded in a sentence. 
In English, temporal expressions have been 
widely studied. Lascarides and Asher (Lascarides, 
Asher and Oberlander, 1992) suggested that tempo-
ral relations between two events followed from dis-
course structures. They investigated various 
contextual effects on five discourse relations 
(namely narration, elaboration, explanation, back-
ground and result) and then corresponded each of 
them to a kind of temporal relations. Hitzeman et al 
(Hitzeman, Moens and Grover, 1995) described a 
method for analyzing temporal structure of a dis-
course by taking into account the effects of tense, 
aspect, temporal adverbials and rhetorical relations 
(e.g. causation and elaboration) on temporal order-
ing. They argued that rhetorical relations could be 
further constrained by event temporal classification. 
Later, Dorr and Gaasterland (Dorr and Gaasterland, 
2002) developed a constraint-based approach to 
generate sentences, which reflect temporal relations, 
by making appropriate selections of tense, aspect 
and connecting words (e.g. before, after and when). 
Their works, however, are theoretical in nature and 
have not investigated computational aspects. 
The pioneer work on Chinese temporal relation 
extraction was first reported by Li and Wong (Li and 
Wong, 2002). To discover temporal relations em-
bedded in a sentence, they devised a set of simple 
rules to map the combined effects of temporal indi-
cators, which are gathered from different grammati-
cal categories, to their corresponding relations. 
However, their work did not focus on relative tem-
poral relations. Given a sentence describing two 
temporally related events, Li and Wong only took 
the temporal position words (including before, after 
and when, which serve as temporal connectives) and 
the tense/aspect markers of the second event into 
consideration. The proposed rule-based approach 
was simple; but it suffered from low coverage and 
was particularly ineffective when the interaction be-
tween the linguistic elements was unclear. 
This paper studies how linguistic features in Chi-
nese interact to influence relative relation resolution. 
For this purpose, statistics-based machine learning 
approaches are applied. The remainder of the paper 
is structured as follows: Section 2 summarizes the 
linguistic features, which must be taken into account 
in temporal relation resolution, and introduces how 
these features are expressed in Chinese. In Section 3, 
the proposed machine learning algorithms to identify 
temporal relations are outlined; furthermore, a het-
erogeneous collaborative bootstrapping technique 
for smoothing is presented. Experiments designed 
for studying the impact of different approaches and 
linguistic features are described in Section 4. Finally, 
Section 5 concludes the paper. 
2 Modeling Temporal Relations 
2.1 Temporal Relation Representations 
As the importance of temporal information proc-
essing has become apparent, a variety of temporal 
systems have been introduced, attempting to ac-
commodate the characteristics of relative temporal 
information. Among those who worked on temporal 
relation representations, many took the work of Rei-
chenbach (Reichenbach, 1947) as a starting point, 
while some others based their works on Allen?s (Al-
len, 1981). 
Reichenbach proposed a point-based temporal 
theory. This was later enhanced by Bruce who de-
fined seven relative temporal relations (Bruce. 1972). 
Given two durative events, the interval relations be-
tween them were modeled by the order between the 
greatest lower bounding points and least upper 
bounding points of the two events. In the other camp, 
instead of adopting time points, Allen took intervals 
as temporal primitives and introduced thirteen basic 
binary relations. In this interval-based theory, points 
are relegated to a subsidiary status as ?meeting 
places? of intervals. An extension to Allen?s theory, 
which treated both points and intervals as primitives 
on an equal footing, was later investigated by Ma 
and Knight (Ma and Knight, 1994). 
In natural language, events can either be punctual 
(e.g. ?? (explore)) or durative (e.g. ?? (built a 
house)) in nature. Thus Ma and Knight?s model is 
adopted in our work (see Figure 1). Taking the sen-
tence ????????????????????? 
(after the street bridge had been built, they solved 
the traffic problem of the city)? as an example, the 
relation held between building the bridge (i.e. an 
interval) and solving the problem (i.e. a point) is 
BEFORE. 
Figure 1. Thirteen temporal relations between points and 
intervals 
2.2 Linguistic Features for Determining Relative 
Relations 
Relative relations are generally determined by 
tense/aspect, connecting words (temporal or other-
wise) and event classes.  
Tense/Aspect in English is manifested by verb in-
flections. But such morphological variations are in-
applicable to Chinese verbs; instead, they are 
conveyed lexically (Li and Wong, 2002). In other 
words, tense and aspect in Chinese are expressed 
using a combination of time words, auxiliaries, tem-
poral position words, adverbs and prepositions, and 
particular verbs. 
Temporal Connectives in English primarily in-
volve conjunctions, e.g. after, before and when (Dorr 
and Gaasterland, 2002). They are key components in 
discourse structures. In Chinese, however, conjunc-
tions, conjunctive adverbs, prepositions and position 
words are required to represent connectives. A few 
verbs which express cause and effect also imply a 
forward movement of event time. The words, which 
contribute to the tense/aspect and temporal connec-
tive expressions, are explicit in a sentence and gen-
erally known as Temporal Indicators. 
Event Class is implicit in a sentence. Events can 
be classified according to their inherent temporal 
characteristics, such as the degree of telicity and/or 
atomicity (Li and Wong, 2002). The four widespread 
accepted temporal classes1 are state, process, punc-
tual event and developing event. Based on their 
classes, events interact with the tense/aspect of verbs 
to define the temporal relations between two events. 
Temporal indicators and event classes are together 
referred to as Linguistic Features (see Table 1). For 
example, linguistic features are underlined in the 
sentence ?(??)?????(??)????????
????? after/because the street bridge had been 
built (i.e. a developing event), they solved the traffic 
problem of the city (i.e. a punctual event)?. 
                                                          
1 Temporal classification refers to aspectual classification. 
A punctual event (i.e. represented in time point) 
A durative event (i.e. represented in time interval) 
BEFORE/AFTER 
MEETS/MET-BY 
OVERLAPS/OVERLAPPED-BY
STARTS/STARTED-BY 
DURING/CONTAINS 
FINISHES/FINISHED-BY 
SAME-AS 
Table 1 shows the mapping between a temporal 
indicator and its effects. Notice that the mapping is 
not one-to-one. For example, adverbs affect 
tense/aspect as well as discourse structure. For an-
other example, tense/aspect can be affected by auxil-
iary words, trend verbs, etc. This shows that 
classification of temporal indicators based on part-
of-speech (POS) information alone cannot determine 
relative temporal relations. 
3 Machine Learning Approaches for Relative 
Relation Resolution 
Previous efforts in corpus-based natural language 
processing have incorporated machine learning 
methods to coordinate multiple linguistic features 
for example in accent restoration (Yarowsky, 1994) 
and event classification (Siegel and McKeown, 
1998), etc. 
Relative relation resolution can be modeled as a 
relation classification task. We model the thirteen 
relative temporal relations (see Figure 1) as the 
classes to be decided by a classifier. The resolution 
process is to assign an event pair (i.e. the two events 
under concern)2 to one class according to their lin-
guistic features. For this purpose, we train two clas-
sifiers, a Probabilistic Decision Tree Classifier 
(PDT) and a Na?ve Bayesian Classifier (NBC). We 
then combine the results by the Collaborative Boot-
strapping (CB) technique which is used to mediate 
the sparse data problem arose due to the limited 
number of training cases. 
                                                          
2 It is an object in machine learning algorithms. 
3.1 Probabilistic Decision Tree (PDT) 
Due to two domain-specific characteristics, we 
encounter some difficulties in classification. (a) Un-
known values are common, for many events are 
modified by less than three linguistic features. (b) 
Both training and testing data are noisy. For this rea-
son, it is impossible to obtain a tree which can com-
pletely classify all training examples. To overcome 
this predicament, we aim to obtain more adjusted 
probability distributions of event pairs over their 
possible classes. Therefore, a probabilistic decision 
tree approach is preferred over conventional deci-
sion tree approaches (e.g. C4.5, ID3). We adopt a 
non-incremental supervised learning algorithm in 
TDIDT (Top Down Induction of Decision Trees) 
family. It constructs a tree top-down and the process 
is guided by distributional information learned from 
examples (Quinlan, 1993). 
3.1.1 Parameter Estimation 
Based on probabilities, each object in the PDT ap-
proach can belong to a number of classes. These 
probabilities could be estimated from training cases 
with Maximum Likelihood Estimation (MLE). Let l 
be the decision sequence, z the object and c the class. 
The probability of z belonging to c is: 
?? ?=
ll
zlplcpzclpzcp )|()|()|,()|(  (1)
let nBBBl ...21= , by MLE we have: 
)(
),(
)|()|(
n
n
n Bf
Bcf
Bcplcp =?   (2)
),( nBcf  is the count of the items whose leaf nodes 
are Bn and belonging to class c. And 
Linguistic Feature Symbol POS Tag Effect Example 
With/Without 
punctuations 
PT Not Applica-
ble 
Not Applicable Not Applicable 
Speech verbs VS TI_vs Tense ??, ??, ? 
Trend verbs TR TI_tr Aspect ??, ?? 
Preposition words P TI_p Discourse Structure/Aspect ?, ?, ? 
Position words PS TI_f Discourse Structure ?, ?, ?? 
Verbs with verb 
objects 
VV TI_vv Tense/Aspect ??, ??, ? 
Verbs expressing 
wish/hope 
VA TI_va Tense ??, ?, ? 
Verbs related to 
causality 
VC TI_vc Discourse Structure ??, ??, ?? 
Conjunctive words C TI_c Discourse Structure ?, ??, ?? 
Auxiliary words U TI_u Aspect ?, ?, ? 
Time words T TI_t Tense ??, ??, ?? 
Adverbs D TI_d Tense/Aspect/Discourse Structure ?, ?, ??, ? 
Event class EC E0/E1/E2/E3 Event Classification State, Punctual Event, 
Developing Event, 
Process 
Table 1. Linguistic features: eleven temporal indicators and one event class 
),...|(...
),,|(),|()|()|(
11
213121
zBBBp
zBBBpzBBpzBpzlp
nn ?
=
 
 
(3)
where 
)|...(
)|...(
),...|(
121
121
121 zBBBp
zBBBBp
zBBBBp
mm
mmm
mmm
??
??
?? =
)|...(
)|...(
121
121
zBBBf
zBBBBf
mm
mmm
??
??= , ( nm ,...,3,2= ).  
An object might traverse more than one decision 
path if it has unknown attribute values. 
)|...( 121 zBBBBf mmm ??  is the count of the item z, 
which owns the decision paths from B1 to Bm. 
3.1.2 Classification Attributes 
Objects are classified into classes based on their 
attributes. In the context of temporal relation resolu-
tion, how to categorize linguistic features into classi-
fication attributes is a major design issue. We extract 
all temporal indicators surrounding an event. As-
sume m and n are the anterior and posterior window 
size. They represent the numbers of the indicators 
BEFORE and AFTER respectively. Consider the 
most extreme case where an event consists of at 
most 4 temporal indicators before and 2 after. We 
set m and n to 4 and 2 initially. Experiments show 
that learning performance drops when m>4 and n>2 
and there is only very little difference otherwise (i.e. 
when m?4 and n?2).  
In addition to temporal indicators alone, the posi-
tion of the punctuation mark separating the two 
clauses describing the events and the classes of the 
events are also useful classification attributes.  We 
will outline why this is so in Section 4.1. Altogether, 
the following 15 attributes are used to train the PDT 
and NBC classifiers: 
,,),(,,,, 2
1
1
1
1
1
2
1
3
1
4
1 1
r
e
r
e
l
e
l
e
l
e
l
e TITIeclassTITITITI  
2
2
1
2
1
2
2
2
3
2
4
2
,),(,,,,, / 2
r
e
r
e
l
e
l
e
l
e
l
e TITIeclassTITITITIpuncwowi  
li (i=1,2,3,4) and rj (j=1,2) are the ith indictor before 
and the jth indicator after the event ek (k=1,2). Given 
a sentence, for example, ?/TI_d ?/E0 ?/TI_u ??
/n ?/w ?/TI_d ?/E2 ?/TI_u ??/n ?/w, the at-
tribute vector could be represented as: [0, 0, 0, ?, 
E0, ?, 0, 1, 0, 0, 0, ?, E2, ?, 0]. 
3.1.3 Attribute Selection Function 
Many similar attribute selection functions were 
used to construct a decision tree (Marquez, 2000). 
These included information gain and information 
gain ratio (Quinlan, 1993), 2? Test and Symmetrical 
Tau (Zhou and Dillon, 1991). We adopt the one pro-
posed by Lopez de Mantaraz (Mantaras, 1991) for it 
shows more stable performance than Quinlan?s 
information gain ratio in our experiments. Compared 
with Quinlan?s information gain ratio, Lopez?s dis-
tance-based measurement is unbiased towards the 
attributes with a large number of values and is capa-
ble of generating smaller trees with no loss of accu-
racy (Marquez, Padro and Rodriguez, 2000). This 
characteristic makes it an ideal choice for our work, 
where most attributes have more than 200 values. 
3.2 Na?ve Bayesian Classifier (NBC) 
NBC assumes independence among features. 
Given the class label c, NBC learns from training 
data the conditional probability of each attribute Ai 
(see Section 3.1.2). Classification is then performed 
by applying Bayes rule to compute the probability of 
c given the particular instance of A1,?,An, and then 
predicting the class with the highest posterior prob-
ability ratio. 
),...,,,|(maxarg 321
*
n
c
AAAAcscorec =  (4)
),...,,,|(
),...,,,|(
),...,,,|(
321
321
321
n
n
n AAAAcp
AAAAcp
AAAAcscore =  (5)
Apply Bayesian rule to (5), we have: 
),...,,,|(
),...,,,|(
),...,,,|(
321
321
321
n
n
n AAAAcp
AAAAcp
AAAAcscore =
)()|,...,,,(
)()|,...,,,(
321
321
cpcAAAAp
cpcAAAAp
n
n=
)()|(
)()|(
1
1
cpcAp
cpcAp
n
i
i
n
i
i
?
?
=
=?  
 
 
 
(6)
)|( cAp i and )|( cAp i  are estimated by MLE from 
training data with Dirichlet Smoothing method: 
?
=
?+
+= n
j
j
i
i
nucAc
ucAc
cAp
1
),(
),(
)|(   (7)
?
=
?+
+= n
j
j
i
i
nucAc
ucAc
cAp
1
),(
),(
)|(   (8)
3.3 Collaborative Bootstrapping (CB) 
PDT and NB are both supervised learning ap-
proach. Thus, the training processes require many 
labeled cases. Recent results (Blum and Mitchell, 
1998; Collins, 1999) have suggested that unlabeled 
data could also be used effectively to reduce the 
amount of labeled data by taking advantage of col-
laborative bootstrapping (CB) techniques. In previ-
ous works, CB trained two homogeneous classifiers 
based on different independent feature spaces. How-
ever, this approach is not applicable to our work 
since only a few temporal indicators occur in each 
case. Therefore, we develop an alternative CB algo-
rithm, i.e. to train two different classifiers based on 
the same feature spaces. PDT (a non-linear classifier) 
and NBC (a linear classifier) are under consideration. 
This is inspired by Blum and Mitchell?s theory that 
two collaborative classifiers should be conditionally 
independent so that each classifier can make its own 
contribution (Blum and Mitchell, 1998). The learn-
ing steps are outlined in Figure 2. 
Inputs: A collection of the labeled cases and unla-
beled cases is prepared. The labeled cases 
are separated into three parts, training 
cases, test cases and held-out cases.  
Loop: While the breaking criteria is not satisfied 
1 Build the PDT and NBC classifiers us-
ing training cases 
2 Use PDT and NBC to classify the unla-
beled cases, and exchange with the se-
lected cases which have higher 
Classification Confidence (i.e. the un-
certainty is less than a threshold). 
3 Evaluate the PDT and NBC classifiers 
with the held-out cases. If the error rate 
increases or its reduction is below a 
threshold break the loop; else go to step 
1. 
Output: Use the optimal classifier to label the test 
cases 
Figure 2. Collaborative bootstrapping algorithm 
3.4 Classification Confidence Measurement 
Classification confidence is the metric used to 
measure the correctness of each labeled case auto-
matically (see Step 2 in Figure 2). The desirable 
metric should satisfy two principles:  
? It should be able to measure the uncertainty/ cer-
tainty of the output of the classifiers; and 
? It should be easy to calculate. 
We adopt entropy, i.e. an information theory 
based criterion, for this purpose. Let x be the classi-
fied object, and },...,,,{ 321 nccccC = the set of output. 
x is classified as ci with the probability 
)|( xcp i ni ,..,3,2,1= . The entropy of the output is 
then calculated as: 
?
=
?= n
i
ii xcpxcpxCe
1
)|(log)|()|(  (9)
Once )|( xcp i is known, the entropy can be deter-
mined. These parameters can be easily determined in 
PDT, as each incoming case is classified into each 
class with a probability. However, the incoming 
cases in NBC are grouped into one class which is 
assigned the highest score. We then have to estimate 
)|( xcp i  from those scores. Without loss of general-
ity, the probability is estimated as: 
?
=
= n
j
j
i
i
xcscore
xcscore
xcp
1
)|(
)|(
 )|(   (10)
where )|( xcscore i  is the ranking score of x be-
longing to ci. 
4 Experiment Setup and Evaluation 
Several experiments have been designed to evalu-
ate the proposed learning approaches and to reveal 
the impact of linguistic features on learning per-
formance. 700 sentences are extracted from Ta Kong 
Pao (a local Hong Kong Chinese newspaper) finan-
cial version. 600 cases are labeled manually and 100 
left unlabeled. Among those labeled, 400 are used as 
training data, 100 as test data and the rest as held-out 
data. 
4.1 Use of Linguistic Features As Classification 
Attributes 
The impact of a temporal indicator is determined 
by its position in a sentence. In PDT and NBC, we 
consider an indicator located in four positions: (1) 
BEFORE the first event; (2) AFTER the first event 
and BEFORE the second and it modifies the first 
event; (3) the same as (2) but it modifies the second 
event; and (4) AFTER the second event. Cases (2) 
and (3) are ambiguous. The positions of the temporal 
indicators are the same. But it is uncertain whether 
these indicators modify the first or the second event 
if there is no punctuation separating their roles. We 
introduce two methods, namely NA and SAP to 
check if the ambiguity affects the two learning ap-
proaches. 
N(atural) O(rder): the temporal indicators between 
the two events are extracted and compared accord-
ing to their occurrence in the sentences regardless 
which event they modify.  
S(eparate) A(uxiliary) and P(osition) words: we 
try to resolve the above ambiguity with the gram-
matical features of the indicators. In this method, 
we assume that an indicator modifies the first 
event if it is an auxiliary word (e.g. ?), a trend 
verb (e.g. ??) or a position word (e.g. ?); oth-
erwise it modifies the second event. 
Temporal indicators are either tense/aspect or con-
nectives (see Section 2.2). Intuitively, it seems that 
classification could be better achieved if connective 
features are isolated from tense/ aspect features, 
allowing like to be compared with like. Methods 
SC1 and SC2 are designed based on this assumption. 
Table 2 shows the effect the different classification 
methods. 
SC1 (Separate Connecting words 1): it separates 
conjunctions and verbs relating to causality from 
others. They are assumed to contribute to dis-
course structure (intra- or inter-sentence structure), 
and the others contribute to the tense/aspect ex-
pressions for each individual event. They are built 
into 2 separate attributes, one for each event. 
SC2 (Separate Connecting words 2): it is the same 
as SC1 except that it combines the connecting 
word pairs (i.e. as a single pattern) into one attrib-
ute. 
EC (Event Class): it takes event classes into con-
sideration. 
Accuracy Method PDT NBC 
NO 82.00% 81.00% 
SAP 82.20% 81.50% 
SAP +SC1 80.20% 78.00% 
SAP +SC2 81.70% 79.20% 
SAP +EC 85.70% 82.25% 
Table 2. Effect of encoding linguistic features in the dif-
ferent ways 
4.2 Impact of Individual Features 
From linguistic perspectives, 13 features (see Ta-
ble 1) are useful for relative relation resolution. To 
examine the impact of each individual feature, we 
feed a single linguistic feature to the PDT learning 
algorithm one at a time and study the accuracy of the 
resultant classifier. The experimental results are 
given in Table 3. It shows that event classes have 
greatest accuracy, followed by conjunctions in the 
second place, and adverbs in the third. 
Feature Accuracy Feature Accuracy
PT 50.5% VA 56.5% 
VS 54% C 62% 
VC 54% U 51.5% 
TR 50.5% T 57.2% 
P 52.2 % D 61.7% 
PS 58.7% EC 68.2% 
VS 51.2% None 50.5% 
Table 3. Impact of individual linguistic features 
4.3 Discussions 
Analysis of the results in Tables 2 and 3 reveals 
some linguistic insights: 
1. In a situation where temporal indicators appear 
between two events and there is no punctuation 
mark separating them, POS information help re-
duce the ambiguity. Compared with NO, SAP 
shows a slight improvement from 82% to 82.2%. 
But the improvement seems trivial and is not as 
good as our prediction. This might due to the 
small percent of such cases in the corpus. 
2. Separating conjunctions and verbs relating to 
causality from others is ineffective. This reveals 
the complexity of Chinese in connecting expres-
sions. It is because other words (such as adverbs, 
proposition and position words) also serve such 
a function. Meanwhile, experiments based on 
SC1 and SC2 suggest that the connecting ex-
pressions generally involve more than one word 
or phrase. Although the words in a connecting 
expression are separated in a sentence, the action 
is indeed interactive. It would be more useful to 
regard them as one attribute. 
3. The effect of event classification is striking. 
Taking this feature into account, the accuracies 
of both PDT and NB improved significantly. As 
a matter of fact, different event classes may in-
troduce different relations even if they are con-
strained by the same temporal indicators. 
4.4 Collaborative Bootstrapping 
Table 4 presents the evaluation results of the four 
different classification approaches. DM is the default 
model, which classifies all incoming cases as the 
most likely class. It is used as evaluation baseline. 
Compare with DM, PDT and NBC show improve-
ment in accuracy (i.e. above 60% improvement). 
And CB in turn outperforms PDT and NBC. This 
proves that using unlabeled data to boost the per-
formance of the two classifiers is effective. 
Accuracy Approach Close test Open test 
DM 50.50% 55.00% 
NBC 82.25% 72.00% 
PDT 85.70% 74.00% 
CB 88.70% 78.00% 
Table 4. Evaluation of NBC, PDT and CB approaches 
5 Conclusions 
Relative temporal relation resolution received 
growing attentions in recent years. It is important for 
many natural language processing applications, such 
as information extraction and machine translation. 
This topic, however, has not been well studied, es-
pecially in Chinese. In this paper, we propose a 
model for relative temporal relation resolution in 
Chinese. Our model combines linguistic knowledge 
and machine learning approaches. Two learning ap-
proaches, namely probabilistic decision tree (PDT) 
and naive Bayesian classifier (NBC) and 13 linguis-
tic features are employed. Due to the limited labeled 
cases, we also propose a collaborative bootstrapping 
technique to improve learning performance. The 
experimental results show that our approaches are 
encouraging. To our knowledge, this is the first at-
tempt of collaborative bootstrapping, which involves 
two heterogeneous classifiers, in NLP application. 
This lays down the main contribution of our research. 
In this pilot work, temporal indicators are selected 
based on linguistic knowledge. It is time-consuming 
and could be error-prone. This suggests two direc-
tions for future studies. We will try to automate or at 
least semi-automate feature selection process. An-
other future work worth investigating is temporal 
indicator clustering. There are two methods we 
could investigate, i.e. clustering the recognized indi-
cators which occur in training corpus according to 
co-occurrence information or grouping them into 
two semantic roles, one related to tense/aspect ex-
pressions and the other to connecting expressions 
between two events. 
 
Acknowledgements 
The work presented in this paper is partially sup-
ported by Research Grants Council of Hong Kong 
(RGC reference number PolyU5085/02E) and 
CUHK Strategic Grant (account number 4410001). 
 
References 
Allen J., 1981. An Interval-based Represent Action 
of Temporal Knowledge. In Proceedings of 7th In-
ternational Joint Conference on Artificial Intelli-
gence, pages 221-226. Los Altos, CA. 
Blum, A. and Mitchell T., 1998. Combining Labeled 
and Unlabeled Data with Co-Training. In Proceed-
ings of the Eleventh Annual Conference on Com-
putational Learning Theory, Madison, Wisconsin, 
pages 92-100 
Bruce B., 1972. A Model for Temporal References 
and its Application in Question-Answering Pro-
gram. Artificial Intelligence, 3(1):1-25. 
Collins M. and Singer Y, 1999. Unsupervised Mod-
els for Named Entity Classification. In Proceed-
ings of the Joint SIGDAT Conference on 
Empirical Methods in Natural Language Process-
ing and Very Large Corpora, pages 189-196. Uni-
versity of Maryland. 
Dorr B. and Gaasterland T., 2002. Constraints on the 
Generation of Tense, Aspect, and Connecting 
Words from Temporal Expressions. (submitted to 
JAIR) 
Hitzeman J., Moens M. and Grover C., 1995. Algo-
rithms for Analyzing the Temporal Structure of 
Discourse. In Proceedings of the 7th European 
Meeting of the Association for Computational 
Linguistics, pages 253-260. Dublin, Ireland.  
Lascarides A., Asher N. and Oberlander J., 1992. 
Inferring Discourse Relations in Context. In 
Proceedings of the 30th Meeting of the 
Association for Computational Linguistics, pages 
1-8, Newark, Del. 
Li W.J. and Wong K.F., 2002. A Word-based Ap-
proach for Modeling and Discovering Temporal 
Relations Embedded in Chinese Sentences, ACM 
Transaction on Asian Language Processing, 
1(3):173-206. 
Ma J. and Knight B., 1994. A General Temporal 
Theory. The Computer Journal, 37(2):114- 123. 
M?ntaras L., 1991. A Distance-based Attribute Se-
lection Measure for Decision Tree Induction. Ma-
chine Learning, 6(1): 81?92. 
M?rquez L., Padr? L. and Rodr?guez H., 2000. A 
Machine Learning Approach to POS Tagging. 
Machine Learning, 39(1):59-91. Kluwer Aca-
demic Publishers. 
Partee, B., 1984. Nominal and Temporal Anaphora. 
Linguistics and Philosophy, 7(3):287-324. 
Quinlan J., 1993. C4.5 Programs for Machine 
Learning. Morgan Kauman Press. 
Reichenbach H., 1947. Elements of Symbolic Logic. 
Berkeley CA, University of California Press.  
Siegel E. and McKeown K., 2000. Learning Meth-
ods to Combine Linguistic Indicators: Improving 
Aspectual Classification and Revealing Linguistic 
Insights. Computational Linguistics, 26(4): 595-
627. 
Wiebe, J.M., O'Hara, T.P., Ohrstrom-Sandgren, T. 
and McKeever, K.J, 1998. An Empirical Approach 
to Temporal Reference Resolution. Journal of Ar-
tificial Intelligence Research, 9:247-293. 
Wong F., Li W., Yuan C., etc., 2002. Temporal Rep-
resentation and Classification in Chinese. Interna-
tional Journal of Computer Processing of Oriental 
Languages, 15(2):211-230. 
Yarowsky D., 1994. Decision Lists for Lexical Am-
biguity Resolution: Application to the Accent Res-
toration in Spanish and French. In Proceeding of 
the 32rd Annual Meeting of ACL, San Francisco, 
CA. 
Zhou X., Dillon T., 1991. A Statistical-heuristic Fea-
ture Selection Criterion for Decision Tree Induc-
tion. IEEE Transaction on Pattern Analysis and 
Machine Intelligence, 13(8): 834-841. 
 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 463?470,
New York, June 2006. c?2006 Association for Computational Linguistics
	

	


	
		
	

Proceedings of ACL-08: HLT, pages 148?155,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Selecting Query Term Alterations for Web Search by Exploiting Query 
Contexts 
 
Guihong Cao Stephen Robertson Jian-Yun Nie 
Dept. of Computer Science and 
Operations Research 
Microsoft Research at 
Cambridge 
Dept. of Computer Science and 
Operations Research 
University of Montreal, Canada Cambridge, UK University of Montreal, Canada 
caogui@iro.umontreal.ca ser@microsoft.com nie@iro.umontreal.ca 
 
  
 
 
Abstract 
Query expansion by word alterations (alterna-
tive forms of a word) is often used in Web 
search to replace word stemming. This allows 
users to specify particular word forms in a 
query. However, if many alterations are 
added, query traffic will be greatly increased. 
In this paper, we propose methods to select 
only a few useful word alterations for query 
expansion. The selection is made according to 
the appropriateness of the alteration to the 
query context (using a bigram language 
model), or according to its expected impact 
on the retrieval effectiveness (using a regres-
sion model). Our experiments on two TREC 
collections will show that both methods only 
select a few expansion terms, but the retrieval 
effectiveness can be improved significantly. 
1 Introduction 
Word stemming is a basic NLP technique used in 
most of Information Retrieval (IR) systems. It 
transforms words into their root forms so as to in-
crease the chance to match similar words/terms 
that are morphological variants. For example, with 
stemming, ?controlling? can match ?controlled? 
because both have the same root ?control?. Most 
stemmers, such as the Porter stemmer (Porter, 
1980) and Krovetz stemmer (Krovetz, 1993), deal 
with stemming by stripping word suffixes accord-
ing to a set of morphological rules. Rule-based ap-
proaches are intuitive and easy to implement. 
However, while in general, most words can be 
stemmed correctly; there is often erroneous stem-
ming that unifies unrelated words. For instance, 
?jobs? is stemmed to ?job? in both ?find jobs in 
Apple? and ?Steve Jobs at Apple?. This is particu-
larly problematic in Web search, where users often 
use special or new words in their queries. A stan-
dard stemmer such as Porter?s will wrongly stem 
them.  
To better determine stemming rules, Xu and 
Croft (1998) propose a selective stemming method 
based on corpus analysis. They refine the Porter 
stemmer by means of word clustering: words are 
first clustered according to their co-occurrences in 
the text collection. Only word variants belonging 
to the same cluster will be conflated.  
Despite this improvement, the basic idea of 
word stemming is to transform words in both doc-
uments and queries to a standard form. Once this is 
done, there is no means for users to require a spe-
cific word form in a query ? the word form will be 
automatically transformed, otherwise, it will not 
match documents. This approach does not seem to 
be appropriate for Web search, where users often 
specify particular word forms in their queries. An 
example of this is a quoted query such as ?Steve 
Jobs?, or ?US Policy?. If documents are stemmed, 
many pages about job offerings or US police may 
be returned (?policy? conflates with ?police? in 
Porter stemmer). Another drawback of stemming is 
that it usually enhances recall, but may hurt preci-
sion (Kraaij and Pohlmann, 1996). However, gen-
eral Web search is basically a precision-oriented 
task.  
One alternative approach to word stemming is to 
do query expansion at query time.  The original 
query terms are expanded by their related forms 
having the same root. All expansions can be com-
bined by the Boolean operator ?OR?.  For example, 
148
the query ?controlling acid rain? can be expanded 
to ?(control OR controlling OR controller OR con-
trolled OR controls) (acid OR acidic OR acidify) 
(rain OR raining OR rained OR rains)?. We will 
call each such expansion term an alteration to the 
original query term. Once a set of possible altera-
tions is determined, the simplest approach to per-
form expansion is to add all possible alterations. 
We call this approach Naive Expansion. One can 
easily show that stemming at indexing time is 
equivalent to Naive Expansion at retrieval time. 
This approach has been adopted by most commer-
cial search engines (Peng et al, 2007). However, 
the expansion approaches proposed previously can 
have several serious problems: First, they usually 
do not consider expansion ambiguity ? each query 
term is usually expanded independently. However, 
some expansion terms may not be appropriate. The 
case of ?Steve Jobs? is one such example, for 
which the word ?job? can be proposed as an ex-
pansion term. Second, as each query term may 
have several alterations, the na?ve approach using 
all the alterations will create a very long query. As 
a consequence, query traffic (the time required for 
the evaluation of a query) is greatly increased. 
Query traffic is a critical problem, as each search 
engine serves millions of users at the same time. It 
is important to limit the query traffic as much as 
possible. 
In practice, we can observe that some word al-
terations are irrelevant and undesirable (as in the 
?Steve Jobs? case), and some other alterations have 
little impact on the retrieval effectiveness (for ex-
ample, if we expand a word by a rarely used word 
form). In this study, we will address these two 
problems. Our goal is to select only appropriate 
word alterations to be used in query expansion. 
This is done for two purposes: On the one hand, 
we want to limit query traffic as much as possible 
when query expansion is performed. On the other 
hand, we also want to remove irrelevant expansion 
terms so that fewer irrelevant documents will be 
retrieved, thereby improve the retrieval effective-
ness. 
To deal with the two problems we mentioned 
above, we will propose two methods to select al-
terations. In the first method, we make use of the 
query context to select only the alterations that fit 
the query. The query context is modeled by a bi-
gram language model. To reduce query traffic, we 
select only one alteration for each query term, 
which is the most coherent with the bigram model. 
We call this model Bigram Expansion. Despite the 
fact that this method adds far fewer expansion 
terms than the na?ve expansion, our experiments 
will show that we can achieve comparable or even 
better retrieval effectiveness. 
Both the Naive Expansion and the Bigram Ex-
pansion determine word alterations solely accord-
ing to general knowledge about the language 
(bigram model or morphological rules), and no 
consideration about the possible effect of the ex-
pansion term is made. In practice, some alterations 
will have virtually no impact on retrieval effec-
tiveness. They can be ignored. Therefore, in our 
second method, we will try to predict whether an 
alteration will have some positive impact on re-
trieval effectiveness. Only the alterations with pos-
itive impact will be retained. In this paper, we will 
use a regression model to predict the impact on 
retrieval effectiveness. Compared to the bigram 
expansion method, the regression method results in 
even fewer alterations, but experiments show that 
the retrieval effectiveness is even better.  
Experiments will be conducted on two TREC 
collections, Gov2 data for Web Track and 
TREC6&7&8 for ad-hoc retrieval. The results 
show that the two methods we propose both out-
perform the original queries significantly with less 
than two alterations per query on average. Com-
pared to the Naive Expansion method, the two me-
thods can perform at least equally well, while 
query traffic is dramatically reduced.  
In the following section, we provide a brief re-
view of related work. Section 3 shows how to gen-
erate alteration candidates using a similar approach 
to Xu and Croft?s corpus analysis (1998). In sec-
tion 4 and 5, we describe the Bigram Expansion 
method and Regression method respectively. Sec-
tion 6 presents some experiments on TREC 
benchmarks to evaluate our methods. Section 7 
concludes this paper and suggests some avenues 
for future work.  
2 Related Work 
Many stemmers have been implemented and used 
as standard processing in IR. Among them, the 
Porter stemmer (Porter, 1980) is the most widely 
used. It strips term suffixes step-by-step according 
to a set of morphological rules. However, the Por-
ter stemmer sometimes wrongly transforms a term 
into an unrelated root. For example, it will unify 
149
?news? and ?new?, ?execute? and ?executive?. On 
the other hand, it may miss some conflations, such 
as ?mice? and ?mouse?, ?europe? and ?european?. 
Krovetz (1993) developed another stemmer, which 
uses a machine-readable dictionary, to improve the 
Porter stemmer. It avoids some of the Porter 
stemmer?s wrong stripping, but does not produce 
consistent improvement in IR experiments.  
Both stemmers use generic rules for English to 
strip each word in isolation. In practice, the re-
quired stemming may vary from one text collection 
to another. Therefore, attempts have been made to 
use corpus analysis to improve existing rule-based 
stemmers. Xu and Croft (1998) create equivalence 
clusters of words which are morphologically simi-
lar and occur in similar contexts. 
As we stated earlier, the stemming-based IR ap-
proaches are not well suited to Web search. Query 
expansion has been used as an alternative (Peng et 
al. 2007). To limit the number of expansion terms, 
and thus the query traffic, Peng et al only use al-
terations for some of the query words: They seg-
ment each query into phrases and only the head 
word in each phrase is expanded. The assumptions 
are: 1)Queries issued in Web search often consist 
of noun phrases. 2) Only the head word in the noun 
phrase varies in form and needs to be expanded. 
However, both assumptions may be questionable. 
Their experiments did not show that the two as-
sumptions hold.  
Stemming is related to query expansion or query 
reformulation (Jones et al, 2006; Anick, 2003; Xu 
and Croft, 1996), although the latter is not limited 
to word variants. If the expansion terms used are 
those that are variant forms of a word, then query 
expansion can produce the same effect as word 
stemming. However, if we add all possible word 
alterations, query expansion/reformulation will run 
the risk of adding many unrelated terms to the 
original query, which may result in both heavy 
traffic and topic drift. Therefore, we need a way to 
select the most appropriate expansion terms. In 
(Peng et al 2007), a bigram language model is 
used to determine the alteration of the head word 
that best fits the query. In this paper, one of the 
proposed methods will also use a bigram language 
model of the query to determine the appropriate 
alteration candidates. However, in our approach, 
alterations are not limited to head words. In addi-
tion, we will also propose a supervised learning 
method to predict if an alteration will have a posi-
tive impact on retrieval effectiveness. To our 
knowledge, no previous method uses the same ap-
proach. 
In the following sections, we will describe our 
approach, which consists of two steps: the genera-
tion of alteration candidates, and the selection of 
appropriate alterations for a query. The first step is 
query-independent using corpus analysis, while the 
second step is query-dependent. The selected word 
alterations will be OR-ed with the original query 
words. 
3  Generating Alteration Candidates 
Our method to generate alteration candidates can 
be described as follows. First, we do word cluster-
ing using a Porter stemmer. All words in the vo-
cabulary sharing the same root form are grouped 
together. Then we do corpus analysis to filter out 
the words which are clustered incorrectly, accord-
ing to word distributional similarity, following (Xu 
and Croft, 1998; Lin 1998). The rationale behind 
this is that words sharing the same meaning tend to 
occur in the same contexts.  
The context of each word in the vocabulary is 
represented by a vector containing the frequencies 
of the context words which co-occur with the word 
within a predefined window in a training corpus. 
The window size is set empirically at 3 words and 
the training corpus is about 1/10 of the GOV2 cor-
pus (see section 5 for details about the collection). 
Similarity is measured by the cosine distance be-
tween two vectors. For each word, we select at 
most 5 similar words as alteration candidates.  
In the next sections, we will further consider ways 
to select appropriate alterations according to the 
query. 
4 Bigram Expansion Model for Alteration 
Selection 
In this section, we try to select the most suitable 
alterations according to the query context. The 
query context is modeled by a bigram language 
model as in (Peng et al 2007).  
Given a query described by a sequence of 
words, we consider each of the query word as rep-
resenting a concept ci. In addition to the given 
word form, ci can also be expressed by other alter-
native forms. However, the appropriate alterations 
do not only depend on the original word of ci, but 
also on other query words or their alterations.  
150
  
 
 
 
 
 
 
Figure 1: Considering all Combinations to Calculate the 
Plausibility of Alterations 
 
Accordingly, a confidence weight is determined 
for each alteration candidate. For example, in the 
query ?Steve Jobs at Apple?, the alteration ?job? of 
?jobs? should have a low confidence; while in the 
query ?finding jobs in Apple?, it should have a 
high confidence.  
One way to measure the confidence of an altera-
tion is the plausibility of its appearing in the query. 
Since each concept may be expressed by several 
alterations, we consider all the alterations of con-
text concepts when calculating the plausibility of a 
given word. Suppose we have the query ?control-
ling acid rain?. The second concept has two altera-
tions - ?acidify? and ?acidic?. For each of the 
alterations, our method will consider all the com-
binations with other words, as illustrated in figure 
1, where each combination is shown as a path. 
More precisely, for a query of n words (or their 
corresponding concepts), let ei,j?ci, j=1,2,?,|ci| be 
the alterations of concept ci. Then we have: 
?
? ? ? ?
=
= = =? =+
?
?
+
+
=
||
1, ,,,2,1
||
1,1
||
1,2
||
1,1
||
1,1
),...,,...,,(...            
......)(
21
1
1
2
2
1
1
1
1
n
n ni
i
i
i
i
c
jn jnjijj
c
j
c
j
c
ji
c
jiij
eeeeP
eP
            (1) 
In equation 1, 
ni jnjijj eeee ,,,2,1 ,...,,...,, 21 is a path 
passing through ei,j. For simplicity, we abbreviate it 
as e1e2?ei?en. In this work, we used bigram lan-
guage model to calculate the probability of each 
path. Then we have: 
?
=
?
=
n
k kkni
eePePeeeeP
2 1121
)|()(),...,,...,,(               (2) 
P(ek|ek-1) is estimated with a back-off bigram lan-
guage model (Goodman, 2001). In the experiments 
with TREC6&7&8, we train the model with all 
text collections; while in the experiments with 
Gov2 data, we only used about 1/10 of the GOV2 
data to train the bigram model because the whole 
Gov2 collection is too large.   
Directly calculating P(eij) by summing the prob-
abilities of all paths passing through eij is an NP 
problem (Rabiner, 1989), and is intractable if the 
query is long. Therefore, we use the forward-
backward algorithm (Bishop, 2006) to calculate 
P(eij) in a more efficient way. After calculating 
P(eij) for each ci, we select one alteration which 
has the highest probability. We limit the number of 
additional alterations to 1 in order to limit query 
traffic. Our experiments will show that this is often 
sufficient. 
5 Regression Model for Alteration Selec-
tion 
None of the previous selection methods considers 
how well an alteration would perform in retrieval. 
The Bigram Expansion model assumes that the 
query replaced with better alterations should have 
a higher likelihood. This approach belongs to the 
family of unsupervised learning. In this section, we 
introduce a method belonging to supervised learn-
ing family. This method develops a regression 
model from a set of training data, and it is capable 
of predicting the expected change in performance 
when the original query is augmented by this al-
teration. The performance change is measured by 
the difference in the Mean Average Precision 
(MAP) between the augmented and the original 
query. The training instances are defined by the 
original query string, an original query term under 
consideration and one alteration to the query term. 
A set of features will be used, which will be de-
fined later in this section.  
5.1 Linear Regression Model  
The goal of the regression model is to predict the 
performance change when a query term is aug-
mented with an alteration. There are several re-
gression models, ranging from the simplest linear 
regression model to non-linear alternatives, such as 
a neural network (Duda et al, 2001), a Regression 
SVM (Bishop, 2006). For simplicity, we use linear 
regression model here. We denote an instance in 
the feature space as X, and the weights of features 
are denoted as W. Then the linear regression model 
is defined as: 
f(X)=WTX                                                             (3) 
where WT is the transpose of W. However, we will 
have a technical problem if we set the target value 
to the performance change directly: The range of 
controlling 
control 
controlled 
controller 
acidify 
acidic 
rain 
rains 
raining 
151
values of f(X) is ),( +??? , while the range of per-
formance change is [-1,1]. The two value ranges do 
not match. This inconsistency may result in severe 
problems when the scales of feature values vary 
dramatically (Duda et al, 2001). To solve this 
problem, we do a simple transformation on the per-
formance change. Let the change be ]1,1[??y , then 
the transformed performance change is: 
]1,1[     
1
1log)( ??
+?
++
= y
y
yy
?
??                            (4) 
where ? is a very small positive real number (set to 
be 1e-37 in the experiments), which acts as a 
smoothing factor. The value of )(y? can be an arbi-
trary real number.  )(y?  is a monotonic function 
defined in the range of [-1,1]. Moreover, the fixed 
point of )(y? is 0, i.e., yy =)(? when y=0. This 
property is nice; it means that the expansion brings 
positive improvement if and only if f(X)>0, which 
makes it easy to determine which alteration is bet-
ter.  
We train the regression model by minimizing 
the mean square error. Suppose there are training 
instances X1,X2,?,Xm, and the corresponding per-
formance change is yi, i=1,2,?,m. We calculate 
the mean square error with the following equation: 
?
=
?=
m
i ii
T yXWWerr
1
2))(()( ?                                 (5) 
Then the optimal weight is defined as: 
?
=
?=
=
m
i ii
T
W
W
yXW
WerrW
1
2
*
))((minarg       
)(minarg
?
                  (6) 
Because err(W) is a convex function of W, it has 
a global minimum and obtains its minimum when 
the gradient is zero (Bazaraa et al, 2006). Then we 
have: 
0))(()(
1*
*
=?=
?
? ?
=
m
i
T
iii
T XyXW
W
Werr ?  
So,  ??
==
=
m
i
T
ii
m
i
T
ii
T XyXXW
11
* )(?  
In fact,  ?
=
m
i
T
ii XX1  is a square matrix, we denote 
it as XXT. Then we have: [ ]?
=
?
=
m
i ii
T XyXXW
1
1* )()( ?                                   (7) 
The matrix  XXT is an ll ? square matrix, where l 
is the number of features. In our experiments, we 
only use three features. Therefore the optimal 
weights can be calculated efficiently even we have 
a large number of training instances. 
5.2 Constructing Training Data 
As a supervised learning method, the regression 
model is trained with a set of training data. We 
illustrate here the procedure to generate training 
instances with an example.  
Given a query ?controlling acid rain?, we obtain 
the MAP of the original query at first. Then we 
augment the query with an alteration to the original 
term (one term at a time) at each time. We retain 
the MAP of the augmented query and compare it 
with the original query to obtain the performance 
change. For this query, we expand ?controlling? by 
?control? and get an augmented query ?(control-
ling OR control) acid rain?. We can obtain the dif-
ference between the MAP of the augmented query 
and that of the original query. By doing this, we 
can generate a series of training instances consist-
ing of the original query string, the original query 
term under consideration, its alteration and the per-
formance change, for example: 
<controlling acid rain, controlling, control,  0.05> 
Note that we use MAP to measure performance, 
but we could well use other metrics such as NDCG 
(Peng et al, 2007) or P@N (precision at top-N 
documents).  
5.3 Features Used for Regression Model 
Three features are used. The first feature reflects to 
what degree an alteration is coherent with the other 
terms. For example, for the query ?controlling acid 
rain?, the coherence of the alteration ?acidic? is 
measured by the logarithm of its co-occurrence 
with the other query terms within a predefined 
window (90 words) in the corpus. That is: 
log(count(controlling?acidic?rain|window)+0.5) 
where ??? means there may be some words be-
tween two query terms. Word order is ignored.  
The second feature is an extension to point-wise 
mutual information (Rijsbergen, 1979), defined as 
follows: 
???
?
???
?
)()()(
)|......(log
rainPacidicPgcontrollinP
windowrainacidicgcontrollinP
 
where P(controlling?acidic?rain|window) is the 
co-occurrence probability of the trigram containing 
acidic within a predefined window (50 words). 
P(controlling), p(acidic), P(rain) are probabilities 
of the three words in the collection. The three 
words are defined as: the term under consideration, 
the first term to the left of that term, and the first 
term to the right. If a query contains less than 3 
152
terms or the term under consideration is the begin-
ning/ending term in the query, we will set the 
probability of the missed term/terms to be 1. 
Therefore, it becomes point-wise mutual informa-
tion when the query contains only two terms. In 
fact, this feature is supplemental to the first feature. 
When the query is very long and the first feature 
always obtains a value of log(0.5), so it does not 
have any discriminative ability. On the other hand, 
the second feature helps because it can capture 
some co-occurrence information no matter how 
long the query is.  
The last feature is the bias, whose value is al-
ways set to be 1.0.   
The regression model is trained in a leave-one-
out cross-validation manner on three collections; 
each of them is used in turn as a test collection 
while the two others are used for training.  For 
each incoming query, the regression model pre-
dicts the expected performance change when one 
alteration is used. For each query term, we only 
select the alteration with the largest positive per-
formance change. If none of its alterations produce 
a positive performance change, we do not expand 
the query term. This selection is therefore more 
restrictive than the Bigram Expansion Model. 
Nevertheless, our experiments show that it im-
proves retrieval effectiveness further. 
6 Experiments 
6.1 Experiment Settings 
In this section, our aim is to evaluate the two con-
text-sensitive word alteration selection methods. 
The ideal evaluation corpus should be composed of 
some Web data. Unfortunately, such data are not 
publicly available and the results also could not be 
compared with other published results. Therefore, 
we use two TREC collections. The first one is the 
ad-hoc retrieval test collections used for 
TREC6&7& 8. This collection is relative small and 
homogeneous. The second one is the Gov2 data. It 
is obtained by crawling the entire .gov domain and 
has been used for three TREC Terabyte tracks 
(TREC2004-2006). Table 1 shows some statistics 
of the two collections. For each collection, we use 
150 queries. Since the Regression model needs 
some data for training, we divided the queries into 
three parts, each containing 50 queries. We then 
use leave-one-out cross-validation. The evaluation 
metrics shown below are the average value of the  
 
Name Description Size 
(GB) 
#Doc Query 
TREC6 
&7&8 
TREC disk4&5, 
Newpapers 
1.7 500,447 301-450 
Gov2 2004 crawl of entire 
.gov domain 
427 25,205,179 701-850 
Table1: Overview of Test Collections 
 
three-fold cross-validation. Because the queries in 
Web are usually very short, we use only the title 
field of each query.  
To correspond to Web search practice, both 
documents and queries are not stemmed. We do 
not filter the stop words either.  
Two main metrics are used: the Mean Average 
Precision (MAP) for the top 1000 documents to 
measure retrieval effectiveness, and the number of 
terms in the query to reflect query traffic. In addi-
tion, we also provide precision for the top 30 doc-
uments (P@30) to show the impact on top ranked 
documents. We also conducted t-tests to determine 
whether the improvement is statistically significant. 
The Indri 2.5 search engine (Strohman et al, 
2004) is used as our basic retrieval system. It pro-
vides for a rich query language allowing disjunc-
tive combinations of words in queries.  
6.2 Experimental Results 
The first baseline method we compare with only 
uses the original query, which is named Original. 
In addition to this, we also compare with the fol-
lowing methods: 
Na?ve Exp: The Na?ve expansion model expands 
each query term with all terms in the vocabu-
lary sharing the same root with it. This model is 
equivalent to the traditional stemming method. 
UMASS: This is the result reported in (Metzler et al, 
2006) using Porter stemming for both document 
and query terms. This reflects a state-of-the-art 
result using Porter stemming. 
Similarity: We select the alterations (at most 5) 
with the highest similarity to the original term. 
This is the method described in section 3.  
The two methods we propose in this paper are the 
following ones: 
Bigram Exp: the alteration is chosen by a Bigram 
Expansion model. 
Regression: the alteration is chosen by a Regres-
sion model.  
 
153
Model P@30 #term MAP Imp. 
Original 0.4701 158 0.2440 ---- 
UMASS ------- ------- 0.2666 9.26 
Na?ve Exp 0.4714 1345 0.2653 8.73 
Similarity 0.4900 303 0.2689 10.20* 
Bigram Exp 0.5007 303 0.2751 12.75** 
Regression 0.5054 237 0.2773 13.65** 
Table 2: Results of Query 701-750 Over Gov2 Data 
 
Model P@30 #term MAP Imp. 
Original 0.4907 158 0.2738 ---- 
UMASS ------- ------- 0.3251 18.73 
Naive Exp 0.5213 1167 0.3224 17.75** 
Similarity 0.5140 290 0.3043 11.14** 
Bigram Exp. 0.5153 290 0.3107 13.47** 
Regression 0.5140 256 0.3144 14.82** 
Table 3: Results of Query 751-800 over Gov2 Data 
 
Model P@30 #term MAP Imp. 
Original 0.4710 154 0.2887 ---- 
UMASS ------- ------- 0.2996 3.78 
Na?ve Exp 0.4633 1225 0.2999 3.87 
Similarity 0.4710 288 0.2976 3.08 
Bigram Exp 0.4730 288 0.3137 8.66** 
Regression 0.4748 237 0.3118 8.00* 
Table 4: Results of Query 801-850 over Gov2 Data 
 
Model P@30 #term MAP Imp. 
Original 0.2673 137 0.1669 ---- 
Na?ve Exp 0.3053 783 0.2146 28.57** 
Similarity 0.3007 255 0.2020 21.03** 
Bigram Exp 0.3033 255 0.2091 25.28** 
Regression 0.3113 224 0.2161 29.48** 
Table 5: Results of Query 301-350 over TREC6&7&8 
 
Model P@30 #term MAP Imp. 
Original 0.2820 126 0.1639 ----- 
Naive Exp 0.2787 736 0.1665 1.59 
Similarity 0.2867 244 0.1650 0.67 
Bigram Exp. 0.2800 244 0.1641 0.12 
Regression 0.2867 214 0.1664 1.53 
Table 6: Results of Query 351-400 over TREC6&7&8 
 
Model P@30 #term MAP Imp. 
Original 0.2833 124 0.1759 ----- 
Na?ve Exp 0.3167 685 0.2138 21.55** 
Similarity 0.3080 240 0.2066 17.45** 
Bigram Exp 0.3133 240 0.2080 18.25** 
Regression 0.3220 187 0.2144 21.88** 
Table7: Results of Query 401-450 over TREC6&7&8 
 
Tables 2, 3, 4 show the results of Gov2 data 
while table 5, 6, 7 show the results of the 
TREC6&7&8 collection. In the tables, the * mark 
indicates that the improvement over the original 
model is statistically significant with p-value<0.05, 
and ** means the p-values<0.01.  
From the tables, we see that both word stem-
ming (UMASS) and expansion with word altera-
tions can improve MAP for all six tasks. In most 
cases (except in table 4 and 6), it also improve the 
precision of top ranked documents. This shows the 
usefulness of word stemming or word alteration 
expansion for IR. 
We can make several additional observations: 
1). Stemming Vs Expansion. UMASS uses docu-
ment and query stemming while Naive Exp uses 
expansion by word alteration. We stated that both 
approaches are equivalent. The equivalence is 
confirmed by our experiment results: for all Gov2 
collections, these approaches perform equiva-
lently.  
2). The Similarity model performs very well. Com-
pared with the Na?ve Expansion model, it pro-
duces quite similar retrieval effectiveness, while 
the query traffic is dramatically reduced. This 
approach is similar to the work of Xu and Croft 
(1998), and can be considered as another state-of-
the-art result. 
3). In comparison, the Bigram Expansion model 
performs better than the Similarity model. This 
shows that it is useful to consider query context 
in selecting word alterations. 
4). The Regression model performs the best of all 
the models. Compared with the Original query, it 
adds fewer than 2 alterations for each query on 
average (since each group has 50 queries); never-
theless we obtained improvements on all the six 
collections. Moreover, the improvements on five 
collections are statistically significant. It also per-
forms slightly better than the Similarity and Bi-
gram Expansion methods, but with fewer 
alterations. This shows that the supervised learn-
ing approach, if used in the correct way, is supe-
rior to an unsupervised approach. Another 
advantage over the two other models is that the 
Regression model can reduce the number of al-
terations further. Because the Regression model 
selects alterations according to their expected 
improvement, the improvement of the alterations 
to one query term can be compared with that of 
the alterations to other query terms. Therefore, 
we can select at most one optimal alteration for 
the whole query. However, with the Similarity or 
Bigram Expansion models, the selection value, 
either similarity or query likelihood, cannot be 
154
compared across the query terms. As a conse-
quence, more alterations need to be selected, 
leading to heavier query traffic.  
7 Conclusion  
Traditional IR approaches stem terms in both doc-
uments and queries. This approach is appropriate 
for general purpose IR, but is ill-suited for the spe-
cific retrieval needs in Web search such as quoted 
queries or queries with a specific word form that 
should not be stemmed. The current practice in 
Web search is not to stem words in index, but ra-
ther to perform a form of expansion using word 
alteration. 
However, a na?ve expansion will result in many 
alterations and this will increase the query traffic. 
This paper has proposed two alternative methods 
to select precise alterations by considering the 
query context. We seek to produce similar or better 
improvements in retrieval effectiveness, while lim-
iting the query traffic. 
In the first method proposed ? the Bigram Ex-
pansion model, query context is modeled by a bi-
gram language model. For each query term, the 
selected alteration is the one which maximizes the 
query likelihood. In the second method - Regres-
sion model, we fit a regression model to calculate 
the expected improvement when the original query 
is expanded by an alteration. Only the alteration 
that is expected to yield the largest improvement to 
retrieval effectiveness is added. 
The proposed methods were evaluated on two 
TREC benchmarks: the ad-hoc retrieval test collec-
tion for TREC6&7&8 and the Gov2 data. Our ex-
perimental results show that both proposed 
methods perform significantly better than the orig-
inal queries. Compared with traditional word 
stemming or the na?ve expansion approach, our 
methods can not only  improve retrieval effective-
ness, but also greatly reduce the query traffic. 
This work shows that query expansion with 
word alterations is a reasonable alternative to word 
stemming. It is possible to limit the query traffic by 
a query-dependent selection of word alterations. 
Our work shows that both unsupervised and super-
vised learning can be used to perform alteration 
selection. 
Our methods can be further improved in several 
aspects. For example, we could integrate other fea-
tures in the regression model, and use other non-
linear regression models, such as Bayesian regres-
sion models (e.g. Gaussian Process regression) 
(Rasmussen and Williams, 2006). The additional 
advantage of these models is that we can not only 
obtain the expected improvement in retrieval effec-
tiveness for an alteration, but also the probability 
of obtaining an improvement (i.e. the robustness of 
the alteration).  
Finally, it would be interesting to test the ap-
proaches using real Web data. 
References  
Anick, P. (2003) Using Terminological Feedback for 
Web Search Refinement: a Log-based Study. In 
SIGIR, pp. 88-95. 
Bazaraa, M., Sherali, H., and Shett, C. (2006). Nonlin-
ear Programming, Theory and Algorithms. John 
Wiley & Sons Inc.  
Bishop, C. (2006). Pattern Recognition and Machine 
Learning. Springer.  
Duda, R.,  Hart, P.,  and Stork, D. (2001). Pattern Clas-
sification, John Wiley & Sons, Inc.  
Goodman, J. (2001). A Bit of Progress in Language 
Modeling. Technical report. 
Jones, R., Rey, B., Madani, O., and Greiner, W. (2006). 
Generating Query Substitutions.  In WWW2006, pp. 
387-396 
Kraaij, W. and Pohlmann, R. (1996) Viewing Stemming 
as Recall Enhancement. Proc. SIGIR, pp. 40-48. 
Krovetz, R. (1993). Viewing Morphology as an Infer-
ence Process.  Proc. ACM SIGIR, pp. 191-202.  
Lin, D. (1998). Automatic Retrieval and Clustering of 
Similar Words. In COLING-ACL, pp. 768-774. 
Metzler, D., Strohman, T. and Croft, B. (2006). Indri 
TREC Notebook 2006: Lessons learned from Three 
Terabyte Tracks. In the Proceedings of TREC 2006.  
Peng, F., Ahmed, N., Li, X., and Lu, Y. (2007). Context 
Sensitive Stemming for Web Search. Proc. ACM 
SIGIR, pp. 639-636 .  
Porter, M. (1980) An Algorithm for Suffix Stripping. 
Program, 14(3): 130-137. 
Rabiner, L. (1989). A Tutorial on Hidden Markov Mod-
els and Selected Applications in Speech Recognition. 
In Proceedings of IEEE Vol. 77(2), pp. 257-286.  
Rijsbergen, V. (1979). Information Retrieval. Butter-
worths, second version.  
Strohman, T., Metzler, D. and Turtle, H., and Croft, B. 
(2004). Indri: A Language Model-based Search En-
gine for Complex Queries. In Proceedings of the In-
ternational conference on Intelligence Analysis.  
Xu, J. and Croft, B. (1996). Query Expansion Using 
Local and Global Document Analysis. Proc. ACM 
SIGIR, pp. 4-11.  
Xu, J. and Croft, B. (1998). Corpus-based Stemming 
Using Co-occurrence of Word Variants. ACM 
TOIS, 16(1): 61-81.  
155
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 75?78,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
NUKTI: English-Inuktitut Word Alignment System Description
Philippe Langlais, Fabrizio Gotti, Guihong Cao
RALI
De?partement d?Informatique et de Recherche Ope?rationnelle
Universite? de Montre?al
Succursale Centre-Ville
H3C 3J7 Montre?al, Canada
http://rali.iro.umontreal.ca
Abstract
Machine Translation (MT) as well as
other bilingual applications strongly
rely on word alignment. Efficient align-
ment techniques have been proposed
but are mainly evaluated on pairs of
languages where the notion of word
is mostly clear. We concentrated our
effort on the English-Inuktitut word
alignment shared task and report on
two approaches we implemented and a
combination of both.
1 Introduction
Word alignment is an important step in exploiting
parallel corpora. When efficient techniques have
been proposed (Brown et al, 1993; Och and Ney,
2003), they have been mostly evaluated on ?safe?
pairs of languages where the notion of word is
rather clear.
We devoted two weeks to the intriguing task
of aligning at the word level pairs of sentences
of English and Inuktitut. We experimented with
two different approaches. For the first one, we re-
lied on an in-house sentence alignment program
(JAPA) where English and Inuktitut tokens were
considered as sentences. The second approach
we propose takes advantage of associations com-
puted between any English word and roughly any
subsequence of Inuktitut characters seen in the
training corpus. We also investigated the combi-
nation of both approaches.
2 JAPA: Word Alignment as a Sentence
Alignment Task
To adjust our systems, the organizers made avail-
able to the participants a set of 25 pairs of sen-
tences where words had been manually aligned.
A fast inspection of this material reveals that in
most of the cases, the alignment produced are
monotonic and involve cepts of n adjacent En-
glish words aligned to a single Inuktitut word.
Many sentence alignment techniques strongly
rely on the monotonic nature of the inherent align-
ment. Therefore, we conducted a first experi-
ment using an in-house sentence alignment pro-
gram called JAPA that we developed within the
framework of the Arcade evaluation campaign
(Langlais et al, 1998). The implementation de-
tails of this aligner can be found in (Langlais,
1997), but in a few words, JAPA aligns pairs of
sentences by first grossly aligning their words
(making use of either cognate-like tokens, or a
specified bilingual dictionary). A second pass
aligns the sentences in a way similar1 to the algo-
rithm described by Gale and Church (1993), but
where the search space is constrained to be close
to the one delimited by the word alignment. This
technique happened to be among the most accu-
rate of the ones tested during the Arcade exercise.
To adapt JAPA to our needs, we only did two
things. First, we considered single sentences as
documents, and tokens as sentences (we define
a token as a sequence of characters delimited by
1In our case, the score we seek to globally maximize by
dynamic programming is not only taking into account the
length criteria described in (Gale and Church, 1993) but also
a cognate-based one similar to (Simard et al, 1992).
75
1-1 0.406 4-1 0.092 4-2 0.015
2-1 0.172 5-1 0.038 5-2 0.011
3-1 0.123 7-1 0.027 3-2 0.011
Table 1: The 9 most frequent English-Inuktitut
patterns observed on the development set. A total
of 24 different patterns have been observed.
white space). Second, since in its default setting,
JAPA only considers n-m sentence-alignment pat-
terns with n,m ? [0, 2], we provided it with a new
pattern distribution we computed from the devel-
opment corpus (see Table 1). It is interesting to
note that although English and Inuktitut have very
different word systems, the length ratio (in char-
acters) of the two sides of the TRAIN corpus is
1.05.
Each pair of documents (sentences) were then
aligned separately with JAPA. 1-n and n-1
alignments identified by JAPA where output with-
out further processing. Since the word alignment
format of the shared task do not account directly
for n-m alignments (n,m > 1) we generated the
cartesian product of the two sets of words for all
these n-m alignments produced by JAPA.
The performance of this approach is reported
in Table 2. Clearly, the precision is poor. This
is partly explained by the cartesian product we re-
sorted to when n-m alignments were produced by
JAPA. We provide in section 4 a way of improving
upon this scenario.
Prec. Rec. F-meas. AER
22.34 78.17 34.75 74.59
Table 2: Performance of the JAPA alignment tech-
nique on the DEV corpus.
3 NUKTI: Word and Substring
Alignment
Martin et al (2003) documented a study in build-
ing and using an English-Inuktitut bitext. They
described a sentence alignment technique tuned
for the specificity of the Inuktitut language, and
described as well a technique for acquiring cor-
respondent pairs of English tokens and Inuktitut
substrings. The motivation behind their work was
to populate a glossary with reliable such pairs.
We extended this line of work in order to achieve
word alignment.
3.1 Association Score
As Martin et al (2003) pointed out, the strong ag-
glutinative nature of Inuktitut makes it necessary
to consider subunits of Inuktitut tokens. This is
reflected by the large proportion of token types
and hapax words observed on the Inuktitut side
of the training corpus, compared to the ratios ob-
served on the English side (see table 3).
Inutktitut % English %
tokens 2 153 034 3 992 298
types 417 407 19.4 27 127 0.68
hapax 337 798 80.9 8 792 32.4
Table 3: Ratios of token types and happax words
in the TRAIN corpus.
The main idea presented in (Martin et al, 2003)
is to compute an association score between any
English word seen in the training corpus and all
the Inuktitut substrings of those tokens that were
seen in the same region. In our case, we com-
puted a likelihood ratio score (Dunning, 1993) for
all pairs of English tokens and Inuktitut substrings
of length ranging from 3 to 10 characters. A max-
imum of 25 000 associations were kept for each
English word (the top ranked ones).
To reduce the computation load, we used a suf-
fix tree structure and computed the association
scores only for the English words belonging to the
test corpus we had to align. We also filtered out
Inuktitut substrings we observed less than three
times in the training corpus. Altogether, it takes
about one hour for a good desktop computer to
produce the association scores for one hundred
English words.
We normalize the association scores such that
for each English word e, we have a distribution of
likely Inuktitut substrings s:
?
s pllr(s|e) = 1.
3.2 Word Alignment Strategy
Our approach for aligning an Inuktitut sentence
of K tokens IK1 with an English sentence of N
tokens EN1 (where K ? N )2 consists of finding
2As a matter of fact, the number of Inuktitut words in
the test corpus is always less than or equal to the number of
English tokens for any sentence pair.
76
K ? 1 cutting points ck?[1,K?1] (ck ? [1, N ? 1])
on the English side. A frontier ck delimits adja-
cent English words Eckck?1+1 that are translation of
the single Inuktitut word Ik. With the convention
that c0 = 0, cK = N and ck?1 < ck, we can for-
mulate our alignment problem as seeking the best
word alignment A = A(IK1 |EN1 ) by maximizing:
A = argmax
cK1
K?
k=1
p(Ik|E
ck
ck?1+1)
?1 ? p(dk)
?2
(1)
where dk = ck?ck?1 is the number of English
words associated to Ik; p(dk) is the prior proba-
bility that dk English words are aligned to a single
Inuktitut word, which we computed directly from
Table 1; and ?1 and ?2 are two weighting coeffi-
cients.
We tried the following two approximations to
compute p(Ik|Eckck?1+1). The second one led to
better results.
p(Ik|E
ck
ck?1+1) '
?
??
??
maxckj=ck?1+1 p(Ik|Ej)
or
?ck
j=ck?1+1
p(Ik|Ej)
We considered several ways of computing the
probability that an Inuktitut token I is the transla-
tion of an English one E; the best one we found
being:
p(I|E) '
?
s?I
?pllr(s|E) + (1? ?)pibm2(s|E)
where the summation is carried over all sub-
strings s of I of 3 characters or more. pllr(s|E)
is the normalized log-likelihood ratio score de-
scribed above and pibm2(s|E) is the probability
obtained from an IBM model 2 we trained after
the Inuktitut side of the training corpus was seg-
mented using a recursive procedure optimizing a
frequency-based criterion. ? is a weighting coef-
ficient.
We tried to directly embed a model trained
on whole (unsegmented) Inuktitut tokens, but no-
ticed a degradation in performance (line 2 of Ta-
ble 4).
3.3 A Greedy Search Strategy
Due to its combinatorial nature, the maximiza-
tion of equation 1 was barely tractable. There-
fore we adopted a greedy strategy to reduce the
search space. We first computed a split of the En-
glish sentence into K adjacent regions cK1 by vir-
tually drawing a diagonal line we would observe
if a character in one language was producing a
constant number of characters in the other one.
An initial word alignment was then found by sim-
ply tracking this diagonal at the word granularity
level.
Having this split in hand (line 1 of Table 4), we
move each cutting point around its initial value
starting from the leftmost cutting point and going
rightward. Once a locally optimal cutting point
has been found (that is, maximizing the score of
equation 1), we proceed to the next one directly
to its right.
3.4 Results
We report in Table 4 the performance of different
variants we tried as measured on the development
set. We used these performances to select the best
configuration we eventually submitted.
variant Prec. Rec. F-m. AER
start (diag) 51.7 53.66 52.66 49.54
greedy (word) 61.6 63.94 62.75 35.93
greedy (best) 63.5 65.92 64.69 34.21
Table 4: Performance of several NUKTI align-
ment techniques measured on the DEV corpus.
It is interesting to note that the starting point
of the greedy search (line 1) does better than our
first approach. However, moving from this ini-
tial split clearly improves the performance (line
3). Among the greedy variants we tested, we no-
ticed that putting much of the weight ? on the
IBM model 2 yielded the best results. We also no-
ticed that p(dk) in equation 1 did not help (?2 was
close to zero). A character-based model might
have been more appropriate to the case.
4 Combination of JAPA and NUKTI
One important weakness of our first approach lies
in the cartesian product we generate when JAPA
produces a n-m (n,m > 1) alignment. Thus,
we tried a third approach: we apply NUKTI on
any n-m alignment JAPA produces as if this ini-
tial alignment were in fact two (small) sentences
to align, n- and m-word long respectively. We can
77
therefore avoid the cartesian product and select
word alignments more discerningly. As can be
seen in Table 5, this combination improved over
JAPA alone, while being worse than NUKTI alone.
5 Results
We submitted 3 variants to the organizers. The
performances for each method are gathered in Ta-
ble 5. The order of merit of each approach was
consistent with the performance we measured on
the DEV corpus, the best method being the NUKTI
one. Curiously, we did not try to propose any Sure
alignment but did receive a credit for it for two of
the variants we submitted.
variant T. Prec. Rec. F-m. AER
JAPA P 26.17 74.49 38.73 71.27
JAPA + S 9.62 67.58 16.84
NUKTI P 51.34 53.60 52.44 46.64
NUKTI S 12.24 86.01 21.43
p 63.09 65.87 64.45 30.6
Table 5: Performance of the 3 alignments we sub-
mitted for the TEST corpus. T. stands for the type
of alignment (Sure or Possible).
6 Discussion
We proposed two methods for aligning an
English-Inuktitut bitext at the word level and a
combination of both. The best of these meth-
ods involves computing an association score be-
tween English tokens and Inuktitut substrings. It
relies on a greedy algorithm we specifically de-
vised for the task and which seeks a local opti-
mum of a cumulative function of log-likelihood
ratio scores. This method obtained a precision
and a recall above 63% and 65% respectively.
We believe this method could easily be im-
proved. First, it has some intrinsic limitations,
as for instance, the fact that NUKTI only recog-
nizes 1-n cepts and do not handle at all unaligned
words. Indeed, our method is not even suited to
aligning English sentences with fewer words than
their respective Inuktitut counterpart. Second, the
greedy search we devised is fairly aggressive and
only explores a tiny bit of the full search. Last,
the computation of the association scores is fairly
time-consuming.
Our idea of redefining word alignment as a sen-
tence alignment task did not work well; but at the
same time, we adapted poorly JAPA to this task.
In particular, JAPA does not benefit here from all
the potential of the underlying cognate system be-
cause of the scarcity of these cognates in very
small sequences (words).
If we had to work on this task again, we would
consider the use of a morphological analyzer. Un-
fortunately, it is only after the submission dead-
line that we learned of the existence of such a tool
for Inuktitut3.
Acknowledgement
We are grateful to Alexandre Patry who turned
the JAPA aligner into a nicely written and efficient
C++ program.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statistical
Machine Translation: Parameter Estimation. Com-
putational Linguistics, 19(2):263?311.
T. Dunning. 1993. Accurate Methods for the Statis-
tics of Surprise and Coincidence. Computational
Linguistics, 19(1).
W. A. Gale and K. W. Church. 1993. A Program for
Aligning Sentences in Bilingual Corpora. In Com-
putational Linguistics, volume 19, pages 75?102.
P. Langlais, M. Simard, and J. Ve?ronis. 1998. Meth-
ods and Practical Issues in Evaluating Alignment
Techniques. In 36th annual meeting of the ACL,
Montreal, Canada.
P. Langlais. 1997. A System to Align Complex Bilin-
gual Corpora. QPSR 4, TMH, Stockholm, Sweden.
J. Martin, H. Johnson, B. Farley, and A. Maclach-
lan. 2003. Aligning and Using an English-Inuktitut
Parallel Corpus. In Building and using Parallel
Texts: Data Driven Machine Translation and Be-
yond, pages 115?118, Edmonton, Canada.
F.J. Och and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Compu-
tational Linguistics, 29:19?51.
M. Simard, G.F. Foster, and P. Isabelle. 1992. Using
Cognates to Align Sentences in Bilingual Corpora.
In Conference on Theoretical and Methodological
Issues in Machine Translation, pages 67?81.
3See http://www.inuktitutcomputing.ca/
Uqailaut/
78
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 137?140,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
RALI: SMT shared task system description
Philippe Langlais, Guihong Cao and Fabrizio Gotti
RALI
De?partement d?Informatique et de Recherche Ope?rationnelle
Universite? de Montre?al
Succursale Centre-Ville
H3C3J7 Montre?al, Canada
http://rali.iro.umontreal.ca
Abstract
Thanks to the profusion of freely avail-
able tools, it recently became fairly
easy to built a statistical machine trans-
lation (SMT) engine given a bitext. The
expectations we can have on the quality
of such a system may however greatly
vary from one pair of languages to an-
other. We report on our experiments
in building phrase-based translation en-
gines for the four pairs of languages we
had to consider for the SMT shared-
task.
1 Introduction
Machine translation is nowadays mature enough
that it is possible without too much effort to de-
vise automatically a statistical translation system
from just a parallel corpus. This is possible
thanks to the dissemination of valuable packages.
The performance of such a system may however
greatly vary from one pair of languages to an-
other. Indeed, there is no free lunch for system
developers, and if a black box approach can some-
times be good enough for some applications (we
can surely accomplish translation gisting with the
French-English and Spanish-English systems we
developed during this exercice), making use of
the output of such a system for, let?s say, qual-
ity translation is another kettle of fish (especially
in our case with the Finnish-English system we
ended-up with).
We devoted two weeks to the SMT shared task,
the aim of which was precisely to see how well
systems can do across different language families.
We began with a core system which is described
in the next section and from which we obtained
baseline performances that we tried to improve
upon.
Since the French- and Spanish-English sys-
tems produced output that were comprehensi-
ble enough1, we focussed on the two languages
whose translations were noticeably worse: Ger-
man and Finnish. For German, we tried to move
around words in order to mimic English word or-
der; and we tried to split compound words. This
is described in section 4. For the Finnish/English
pair, we tried to decompose Finnish words into
smaller substrings (see section 5).
In parallel to that, we tried to smooth a phrase-
based model (PBM) making use of WORDNET.
We report on this experiment in section 3. We de-
scribe in section 6 the final setting of the systems
we used for submitting translations and their of-
ficial results as computed by the organizers. Fi-
nally, we conclude our two weeks of efforts in
section 7.
2 The core system
We assembled up a phrase-based statistical engine
by making use of freely available packages. The
translation engine we used is the one suggested
within the shared task: PHARAOH (Koehn, 2004).
The input of this decoder is composed of a phrase-
based model (PBM), a trigram language model
and an optional set of coefficients and thresholds
1What we mean by this is nothing more than we were
mostly able to infer the original meaning of the source sen-
tence by reading its automatic translation.
137
pair WER SER NIST BLEU
fi-en 66.53 99.20 5.3353 18.73
de-en 60.70 98.40 5.8411 21.11
fr-en 53.77 98.20 6.4717 27.69
es-en 53.84 98.60 6.5571 28.08
Table 1: Baseline performances measured on the
500 top sentences of the DEV corpus in terms of
WER (word error rate), SER (sentence error rate),
NIST and BLEU scores.
which control the decoder.
For acquiring a PBM, we followed the ap-
proach described by Koehn et al (2003). In brief,
we relied on a bi-directional word alignment of
the training corpus to acquire the parameters of
the model. We used the word alignment pro-
duced by Giza (Och and Ney, 2000) out of an
IBM model 2. We did try to use the alignment
produced with IBM model 4, but did not notice
significant differences over our experiments; an
observation consistent with the findings of Koehn
et al (2003). Each parameter in a PBM can be
scored in several ways. We considered its rela-
tive frequency as well as its IBM-model 1 score
(where the transfer probabilities were taken from
an IBM model 2 transfer table). The language
model we used was the one provided within the
shared task.
We obtained baseline performances by tuning
the engine on the top 500 sentences of the devel-
opment corpus. Since we only had a few param-
eters to tune, we did it by sampling the parameter
space uniformly. The best performance we ob-
tained, i.e., the one which maximizes the BLEU
metric as measured by the mteval script2 is re-
ported for each pair of languages in Table 1.
3 Smoothing PBMs with WORDNET
Among the things we tried but which did not
work well, we investigated whether smoothing
the transfer table of an IBM model (2 in our case)
with WORDNET would produce better estimates
for rare words. We adapted an approach proposed
by Cao et al (2005) for an Information Retrieval
task, and computed for any parameter (ei, fj) be-
2http://www.nist.gov/speech/tests/mt/
mt2001/resource
longing to the original model the following ap-
proximation:
p?(ei|fj) ?
?
e?E
pwn(ei|e)? pn(e|fj)
where E is the English vocabulary, pn desig-
nates the native distribution and pwn is the proba-
bility that two words in the English side are linked
together. We estimated this distribution by co-
occurrence counts over a large English corpus3.
To avoid taking into account unrelated but co-
occurring words, we used WORDNET to filter in
only the co-occurrences of words that are in re-
lation according to WORDNET. However, since
many words are not listed in this resource, we had
to smooth the bigram distribution, which we did
by applying Katz smoothing (Katz, 1997):
pkatz(ei|e) =
{
c?(ei,e|W,L)P
ej
c(ej ,e|W,L)
if c(ei, e|W,L) > 0
?(e)pkatz(ei) otherwise
where c?(a, b|W,L) is the good-turing dis-
counted count of times two words a and b that are
linked together by a WORDNET relation, co-occur
in a window of 2 sentences.
We used this smoothed model to score the pa-
rameters of our PBM instead of the native trans-
fer table. The results were however disappoint-
ing for both the G-E and S-E translation direc-
tions we tested. One reason for that, may be
that the English corpus we used for computing
the co-occurrence counts is an out-of-domain cor-
pus for the present task. Another possible ex-
planation lies in the fact that we considered both
synonymic and hyperonymic links in WORDNET;
the latter kind of links potentially introducing too
much noise for a translation task.
4 The German-English task
We identified two major problems with our ap-
proach when faced with this pair of languages.
First, the tendency in German to put verbs at the
end of a phrase happens to ruin our phrase acqui-
sition process, which basically collects any box
of aligned source and target adjacent words. This
3For this, we used the English side of the provided train-
ing corpus plus the English side of our in-house Hansard bi-
text; that is, a total of more than 7 million pairs of sentences.
138
can be clearly seen in the alignment matrix of fig-
ure 1 where the verbal construction could clarify
is translated by two very distant German words
ko?nnten and erla?utern. Second, there are many
compound words in German that greatly dilute
the various counts embedded in the PBM table.
. . . . . . . . . . . . . ?
erla?utern . . . . . . . ? . . . . .
punkt . . . . . . . . . ? . . .
einen . . . . . . . . ? . " . .
mir . . . . . . . . . . . ? .
sie . . . . . ? . . . . . . .
oder . . . . ? . . . . . . . .
kommission . . . ? . . . . . . . . .
die . . ? . . . . . . . . . .
ko?nnten . . . . . . ? . . . . . .
vielleicht . ? . . . . . . . . . . .
NULL . . . . . . . . . . . . .
N p t c o y c c a p f m .
U e h o r o o l o o e
L r e m u u a i r
L h m l r n
English perhaps the commission or you could
clarify a point for me .
German vielleicht ko?nnten die kommission oder
sie mir einen punkt erla?utern .
Figure 1: Bidirectional alignment matrix. A cross
in this matrix designates an alignment valid in
both directions, while the " symbol indicates an
uni-directional alignment (for has been aligned
with einen, but not the other way round).
4.1 Moving around German words
For the first problem, we applied a memory-based
approach to move around words in the German
side in order to better synchronize word order
in both languages. This involves, first, to learn-
ing transformation rules from the training corpus,
second, transforming the German side of this cor-
pus; then training a new translation model. The
same set of rules is then applied to the German
text to be translated.
The transformation rules we learned concern a
few (five in our case) verbal constructions that
we expressed with regular expressions built on
POS tags in the English side. Once the locus
e
v
u of a pattern has been identified, a rule is col-
lected whenever the following conditions apply:
for each word e in the locus, there is a target word
f which is aligned to e in both alignment direc-
tions; these target words when moved can lead to
a diagonal going from the target word (l) associ-
ated to eu?1 to the target word r which is aligned
to ev+1.
The rules we memorize are triplets (c, i, o)
where c = (l, r) is the context of the locus and i
and o are the input and output German word order
(that is, the order in which the tokens are found,
and the order in which they should be moved).
For instance, in the example of Figure 1,
the Verb Verb pattern match the locus could
clarify and the following rule is acquired:
(sie einen, ko?nnten erla?utern,
ko?nnten erla?utern), a paraphrase of
which is: ?whenever you find (in this order)
the word ko?nnten and erla?utern in a German
sentence containing also (in this order) sie and
einen, move ko?nnten and erla?utern between sie
and einen.
A set of 124 271 rules have been acquired
this way from the training corpus (for a total of
157 970 occurrences). The most frequent rule ac-
quired is (ich herrn, mo?chte danken,
mo?chte danken), which will transform a sen-
tence like ?ich mo?chte herrn wynn fu?r seinen
bericht danken.? into ?ich mo?chte danken herrn
wynn fu?r seinen bericht.?.
In practice, since this acquisition process does
not involve any generalization step, only a few
rules learnt really fire when applied to the test ma-
terial. Also, we devised a fairly conservative way
of applying the rules, which means that in prac-
tice, only 3.5% of the sentences of the test corpus
where actually modified.
The performance of this procedure as measured
on the development set is reported in Table 2. As
simple as it is, this procedure yields a relative gain
of 7% in BLEU. Given the crudeness of our ap-
proach, we consider this as an encouraging im-
provement.
4.2 Compound splitting
For the second problem, we segmented German
words before training the translation models. Em-
pirical methods for compound splitting applied to
139
system WER SER NIST BLEU
baseline 60.70 98.40 5.8411 21.11
swap 60.73 98.60 5.9643 22.58
split 60.67 98.60 5.7511 21.99
swap+split 60.57 98.40 5.9685 23.10
Table 2: Performances of the swapping and the
compound splitting approaches on the top 500
sentences of the development set.
German have been studied by Koehn and Knight
(2003). They found that a simple splitting strat-
egy based on the frequency of German words was
the most efficient method of the ones they tested,
when embedded in a phrase-based translation en-
gine. Therefore, we applied such a strategy to
split German words in our corpora. The results
of this approach are shown in Table 2.
Note: Both the swapping strategy and the com-
pound splitting yielded improvements in terms of
BLEU score. Only after the deadline did we find
time to train new models with a combination of
both techniques; the results of which are reported
in the last line of Table 2.
5 The Finnish-English task
The worst performances were registered on the
Finnish-English pair. This is due to the aggluti-
native nature of Finnish. We tried to segment the
Finnish material into smaller units (substrings) by
making use of the frequency of all Finnish sub-
strings found in the training corpus. We main-
tained a suffix tree structure for that purpose.
We proceeded by recursively finding the most
promising splitting points in each Finnish token
of C characters FC1 by computing split(FC1 )
where:
split(F ji ) =
?
?
?
|F ji | if j ? i < 2
maxc?[i+2,j?2] |F
c
i |?
split(F jc+1) otherwise
This approach yielded a significant degradation
in performance that we still have to analyze.
6 Submitted translations
At the time of the deadline, the best translations
we had were the baselines ones for all the lan-
guage pairs, except for the German-English one
where the moving of words ranked the best. This
defined the configuration we submitted, whose re-
sults (as provided by the organizers) are reported
in Table 3.
pair BLEU p1/p2/p3/p4
fi-en 18.87 55.2/24.7/13.1/7.1
de-en 22.91 58.9/29.0/16.8/10.3
es-en 28.49 62.4/34.5/21.9/14.4
fr-en 28.89 62.6/34.7/22.0/14.6
Table 3: Results measured by the organizers for
the TEST corpus.
7 Conclusion
We found that, while comprehensible translations
were produced for pairs of languages such as
French-English and Spanish-English; things did
not go as well for the German-English pair and
especially not for the Finnish-English pair. We
had a hard time improving our baseline perfor-
mance in such a tight schedule and only man-
aged to improve our German-English system. We
were less lucky with other attempts we imple-
mented, among them, the smoothing of a trans-
fer table with WORDNET, and the segmentation
of the Finnish corpus into smaller units.
References
G. Cao, J. Nie, and J. Bai. 2005. Integrating Word
relationships into Language Models. In to appear
in Proc. of SIGIR.
S. Katz. 1997. Estimation of Probabilities from
Sparse Data for the Language Model Component of
a Speech Recognizer. IEEE Transactions on Acous-
tics Speech and Signal Processing, 35.
Philipp Koehn and Kevin Knight. 2003. Empirical
Methods for Compound Splitting. In EACL, Bu-
dapest, Hungary.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical
Phrase-Based Translation. In Proceedings of HLT,
pages 127?133.
P. Koehn. 2004. Pharaoh: a Beam Search Decoder
for Phrase-Based SMT. In Proceedings of AMTA,
pages 115?124.
F.J. Och and H. Ney. 2000. Improved Statistical
Alignment Models. In Proceedings of ACL, pages
440?447, Hongkong, China.
140
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 551?559,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 
Context-Dependent Term Relations for Information Retrieval 
 
Jing Bai      Jian-Yun Nie     Guihong Cao
DIRO, University of Montreal 
CP. 6128, succ. Centre-ville, Montreal,  
Quebec H3C 3J7, Canada 
{baijing,nie,caogui}@iro.umontreal.ca 
 
 
 
Abstract 
Co-occurrence analysis has been used to 
determine related words or terms in many 
NLP-related applications such as query 
expansion in Information Retrieval (IR). 
However, related words are usually 
determined with respect to a single word, 
without relevant information for its 
application context. For example, the word 
?programming? may be considered to be 
strongly related to ?Java?, and applied 
inappropriately to expand a query on ?Java 
travel?. To solve this problem, we propose 
to add another context word in the relation 
to specify the appropriate context of the 
relation, leading to term relations of the 
form ?(Java, travel) ? Indonesia?. The 
extracted relations are used for query 
expansion in IR. Our experiments on 
several TREC collections show that this 
new type of context-dependent relations 
performs much better than the traditional 
co-occurrence relations.  
1. Introduction 
A query usually is a poor expression of an 
information need. This is not only due to its short 
length (usually a few words), but also due to the 
inability of users to provide the best terms to 
describe their information need. At best, one can 
expect that some, but not all, relevant terms are 
used in the query. Query expansion thus aims to 
improve query expression by adding related 
terms to the query. However, the effect of query 
expansion is strongly determined by the term 
relations used (Peat and Willett, 1991). For 
example, even if ?programming? is strongly 
related to ?Java?, if this relation is used to 
expand a query on ?Java travel?, the retrieval 
result will likely deteriorate because the 
irrelevant term ?programming? is introduced, 
leading to the retrieval of irrelevant documents 
about ?programming?.  
    A number of attempts have been made to deal 
with the problem of selecting appropriate 
expansion terms. For example, Wordnet has been 
used in (Voorhees, 1994) to determine the 
expansion terms. However, the experiments did 
not show improvement on retrieval effectiveness. 
Many experiments have been carried out using 
associative relations extracted from term co-
occurrences; but they showed variable results 
(Peat and Willett, 1991). In (Qiu and Frei, 1993), 
it is observed that one of the reasons is that one 
tried to determine expansion terms according to 
each original query term separately, which may 
introduce much noise. Therefore, they proposed 
to determine the expansion terms by summing up 
the relations of a candidate expansion term to 
each of the query terms. In so doing, a candidate 
expansion term is preferred if it has a strong 
relationship with many of the query terms. 
However, it is still difficult to prevent the 
expansion process from adding ?programming? 
to a query on ?Java travel? because of its very 
strong relation with ?Java?. 
The approach used in (Qiu and Frei, 1993) 
indeed tries to correct a handicap inherent in the 
relations: as term relations are created between 
two single words such as ?Java ? 
programming?, no information is available to 
help determine the appropriate context to apply 
it. The approach used in (Qiu and Frei, 1993) can 
simply alleviate the problem without solving it 
radically. 
    In this paper, we argue that the solution lies in 
the relations themselves. They have to contain 
more information to help determine the 
appropriate context to apply them. We thus 
propose a way to add some context information 
into the relations: we introduce an additional 
word into the condition part of the relation, such 
as ?(Java, computer) ? programming?, which 
551
 means ?programming? is related to ?(Java, 
computer)? together. In so doing, we would be 
able to prevent from extracting and applying a 
relation such as ?(Java, travel) ? 
programming?.  
    In this paper, we will test the extracted 
relations in query expansion for IR. We choose to 
implement query expansion within the language 
modeling (LM) framework because of its 
flexibility and high performance. The 
experiments on several TREC collections will 
show that our query expansion approach can 
bring large improvements in retrieval 
effectiveness. 
    In the following sections, we will first review 
some of the relevant approaches on query 
expansion and term relation extraction. Then we 
will describe our general IR models and the 
extraction of term relations. The experimental 
results will be reported and finally some 
conclusions will be drawn. 
2. Query Expansion and Term Relations 
It has been found that a key factor that 
determines the effect of query expansion is the 
selection of appropriate expansion terms (Peat 
and Willett, 1991). To determine expansion 
terms, one possible resource is thesauri 
constructed manually, such as Wordnet. Thesauri 
contain manually validated relations between 
terms, which can be used to suggest related 
terms. (Voorhees, 1994) carried out a series of 
experiments on selecting related terms (e.g. 
synonyms, hyonyms, etc.) from Wordnet. 
However, the experiments did not show that this 
can improve retrieval effectiveness. Some of the 
reasons are as follows: Although Wordnet 
contains many relations validated by human 
experts, the coverage is far from complete for the 
purposes of IR: not only linguistically motivated 
relations, but also association relations, are useful 
in IR. Another problem is the lack of information 
about the appropriate context to apply relations. 
For example, Wordnet contains two synsets for 
?computer?, one for the sense of ?machine? and 
another for ?human expert?. It is difficult to 
automatically select the correct synset to expand 
the word ?computer? even if we know that the 
query?s area is computer science. 
Another often used resource is associative 
relations extracted from co-occurrences: two 
terms that co-occur frequently are thought to be 
associated to each other (Jing and Croft, 1994). 
However, co-occurrence relations are noisy: 
Frequently co-occurring terms are not necessarily 
related. On the other hand, they can also miss 
true relations. The most important problem is still 
that of ambiguity: when one term is associated 
with another, it may be related for one sense and 
not for other possible senses. It is then difficult to 
determine when the relation applies. 
In most of the previous studies, relations 
extracted are restricted between one word and 
another. This limitation makes the relations 
ambiguous, and their utilization in query 
expansion often introduces undesired terms. We 
believe that the key to make a relation less 
ambiguous is to add some contextual 
information. 
In an attempt to select better expansion terms, 
(Qiu and Frei, 1993) proposed the following 
approach to select expansion terms: terms are 
selected according to their relation to the whole 
query, which is calculated as the sum of their 
relations to each of the query terms. Therefore, a 
term that is related to several query terms will be 
favored. In a similar vein, (Bai et al 2005) also 
try to determine the relationship of a word to a 
group of words by combining its relationships to 
each of the words in the group. This can indeed 
select better expansion terms. The consideration 
of other query terms produces a weak contextual 
effect. However, this effect is limited due to the 
nature of the relations extracted, in which a term 
depends on only one other term. Much of the 
noise in the sets will remain after selection.  
For a query composed of several words, what 
we would really like to have is a set of terms that 
are related to all the words taken together (and 
not separately). By combining words in the 
condition part such as ?(Java, travel)? or ?(base, 
bat)?, each word will serve as a context to the 
other in order to constrain the related terms. In 
these cases, we would expect that ?hotel?, 
?island? or ?Indonesia? would co-occur much 
more often with ?(Java, travel)? than 
?programming?, and ?ball?, ?catcher? etc. co-
occur much more often with ?(base, bat)? than 
?animal? or ?foundation?. 
One naturally would suggest that compound 
terms can be used for this purpose. However, for 
many queries, it is difficult to form a legitimate 
compound term. Even if we can detect one 
occurrence of a compound, we may miss others 
that use its variants. For example, if ?Java travel? 
is used as a query, we will likely be able to 
consider it as a compound term. The same 
compound (or its variant) would be difficult to 
552
 detect in a document talking about traveling to 
Java: the two words may appear at some distance 
or not in some specific syntactic structure as 
required in (Lin, 1997). This will lead to the 
problem of mismatching between document and 
query. 
In fact, compound terms are not the only way 
to add contextual information to a word. By 
putting two words together (without forming a 
compound term), we usually obtain a more 
precise sense for each word. For example, from 
?Java travel?, we can guess that the intended 
meaning is likely related to ?traveling to Java 
Island?. People will not interpret this 
combination in the sense of ?Java 
programming?. In the same way, people would 
not consider ?animal? to be a related term to 
?base, bat?. These examples show that in a 
combination of words, each word indeed serves 
to specify a context to interpret another word. It 
then suggests the following approach: we can 
adjunct some additional word(s) in the condition 
part of a relation, such as ?(Java, travel) ? 
Indonesia?, which means ?Indonesia? is related 
to ?(Java, travel)? together. It is expected that 
one would not obtain ?(Java, travel) ? 
programming?. 
Owing to the context effect explained above, 
we will call the relations with multiple words in 
the condition part context-dependent relations. In 
order to limit the computation complexity, we 
will only consider adding one additional word 
into relations.  
The proposed approach follows the same 
principle as (Yarowsky, 1995), which tried to 
determine the appropriate word sense according 
to one relevant context word. However, the 
requirement for query expansion is less than 
word sense disambiguation: we do not need to 
know the exact word sense to make expansion. 
We only need to determine the relevant 
expansion terms. Therefore, there is no need to 
determine manually a set of seeds before the 
learning process takes place. 
To some extent, the proposed approach is also 
related to (Sch?tze and Pedersen, 1997), which 
calculate term similarity according to the words 
appearing in the same context, or to second-order 
co-occurrences. However, a key difference is that 
(Sch?tze and Pedersen, 1997) consider only 
separate context words, while we consider 
multiple context words together. 
Once term relations are determined, they will 
be used in query expansion. The basic IR process 
will be implemented in a language modeling 
framework. This framework is chosen for its 
flexibility to integrate term relations. Indeed, the 
LM framework has proven to be capable of 
integrating term relations and query expansion 
(Bai et al, 2005; Berger and Lafferty, 1999; Zhai 
and Lafferty, 2001). However, none of the above 
studies has investigated the extraction of strong 
context-dependent relations from text collections. 
In the next section, we will describe the 
general LM framework and our query expansion 
models. Then the extraction of term relation will 
be explained. 
3. Context-Dependent Query Expansion 
in Language Models 
The basic IR approach based on LM (Ponte and 
Croft, 1998) determines the score of relevance of 
a document D by its probability to generate the 
query Q. By assuming independence between 
query terms, we have: 
??
??
?=
Qw
i
Qw
i
ii
DwPDwPDQP )|(log)|()|(  
where )|( DwP i denotes the probability of a word 
in the language model of the document D. As no 
ambiguity will arise, we will use D to mean both 
the language model of the document and the 
document itself (similarly for a query model and 
a query Q). 
Another score function is based on KL-
divergence or cross entropy between the 
document model and the query model: 
?
?
=
Vw
ii
i
DwPQwPQDscore )|(log)|(),(
 
where V is the vocabulary. Although we have 
both document and query models in the above 
formulation, usually only the document model is 
smoothed, while the query model uses Maximum 
Likelihood Estimation (MLE) )|( QwP iML . Then 
we have: 
?
?
=
Qw
iiML
i
DwPQwPQDscore )|(log)|(),(  
However, it is obvious that a distance (KL-
divergence) measured between a short query of a 
few words and a document cannot be precise. A 
better expression would contain all the related 
terms. The construction of a better query 
expression is the very motivation for query 
expansion in traditional IR systems. It is the same 
in LM for IR: to create a better query expression 
(model) to be able to measure the distance to a 
553
 document in a more precise way. The key to 
creating the new model is the integration of term 
relations. 
3.1 LM for Query Expansion 
Term relations have been used in several recent 
language models in IR. (Berger and Lafferty, 
1999) proposed a translation model that expands 
the document model. The same approach can also 
be used to expand the query model. Following 
(Berger and Lafferty, 1999), we arrive at the first 
expansion model as follows, which has also been 
used in (Bai et al, 2005): 
Model 1: Context-independent query 
expansion model (CIQE) 
??
??
==
Qq
jMLjiR
Vq
jiRiR
jj
QqPqwPQqwPQwP )|()|()|,()|(  
In this model, each original query term qj is 
expanded by related terms wi. The relations 
between them are determined by )|( jiR qwP . We 
will explain how this probability is defined in 
Section 3.2. However, we can already see here 
that wi is determined solely by one of the query 
term qj. So, we call this model ?context-
independent query expansion model? (CIQE). 
The above expanded query model enables us 
to obtain new related expansion terms, to which 
we also have to add the original query. This can 
be obtained through the following smoothing: 
?
?
?
+=
Qq
jMLjiR
iMLi
j
QqPqwP
QwPQwP
)|()|()1(                   
)|()|(
1
1
?
?
      (1) 
where 1? is a smoothing parameter. 
However, if the query model is expanded on 
all the vocabulary (V), the query evaluation will 
be very time consuming because the query and 
the document have to be compared on every word 
(dimension). In practice, we observe that only a 
small number of terms have strong relations with 
a given term, and the terms having weak relations 
usually are not truly related. So we can limit the 
expansion terms only to the strongly related ones. 
By doing this, we can also expect to filter out 
some noise and considerably reduce the retrieval 
time. 
Suppose that we have selected a set E of 
strong expansion terms. Then we have: 
?
?
??
?
?
=
QEw
ii
Vw
ii
i
i
DwPQwP
DwPQwPQDscore
)|(log)|(                    
)|(log)|(),(
 
This query expansion method uses the same 
principle as (Qiu and Frei, 1993), but in a LM 
setting: the selected expansion terms are those 
that are strongly related to all the query terms 
(this is what the summation means). The 
approach used in (Bai et al, 2005) is slightly 
different: A context vector is first built for each 
word; then a context vector for a group of words 
(e.g. a multi-word query) is composed from the 
context vectors of the words of the group; finally 
related terms to the group of words are 
determined according to the similarity of their 
context vectors to that of the group. This last step 
uses second-order co-occurrences similarly to 
(Sch?tze and Pedersen, 1997). In both (Qiu and 
Frei, 1993) and (Bai et al, 2005), the terms 
related to a group of words are determined from 
their relations to each of the words in the group, 
while the latter relations are extracted separately. 
Irrelevant expansion terms can be retained. 
As we showed earlier, in many cases, when 
one additional word is used with another word, 
the sense of each of them can usually be better 
determined. This additional word may be 
sufficient to interpret correctly many multi-word 
user queries. Therefore, our goal is to extract 
stronger context-dependent relations of the form 
(qj qk) ? wi, or to build a probability 
function )|( kjiR qqwP . Once this function is 
determined, it can be integrated into a new 
language model as follows. 
Model 2: Context-dependent query expansion 
model (CDQE) 
?
?
?
?
?
=
Qqq
kjkjiR
Vqq
kjkjiRiR
kj
kj
QqqPqqwP
QqqPqqwPQwP
,
,
)|()|(                 
)|()|()|(
 
As )|( kjiR qqwP  is a relation with two terms as 
condition, we will also call it a biterm relation. 
The name ?biterm? is due to (Srikanth and 
Srihari, 2002), which means two terms co-
occurring within some distance. Similarly, 
)|( jiR qwP  will be called unigram relation. The 
corresponding query models will be called biterm 
relation model and unigram relation model.  
As in general LM, the biterm relation model 
can be smoothed with a unigram model. Then we 
have the following score function: 
?
?
?
+=
Qqq
kjkjiR
iMLiR
kj
QqqPqqwP
QwPQwP
,
2
2
)|()|()1(                    
)|()|(
?
?
  (2) 
554
 where 2?  is another smoothing parameter. 
3.2 Extraction of Term Relations 
The key problem now is to obtain the relations 
we need: )|( jiR wwP  and )|( kjiR wwwP . For the first 
probability, as in many previous studies, we 
exploit term co-occurrences. )|( jiR wwP  could be 
built as a traditional bigram model. However, this 
is not a good approach for IR because two related 
terms do not necessarily co-occur side by side. 
They often appear at some distance. Therefore, 
this model is indeed a biterm model (Srikanth 
and Srihari, 2002), i.e., we allow two terms be 
separated within some distance. We use the 
following formula to determine this probability: 
?=
lw
jl
ji
jiR
wwc
wwc
wwP
),(
),()|(  
where ),( ji wwc  is the frequency of co-occurrence 
of the biterm ),( ji ww , i.e. two terms in the same 
window of fixed size across the collection. In our 
case, we set the window size at 10 (because this 
size turned out to be reasonable in our pilot 
experiments). 
For )|( kji wwwP , we further extend the biterm 
to triterm, and we use the frequency of co-
occurrences of three terms ),,( kji wwwc  within the 
same windows in the document collection: 
?=
lw
kjl
kji
kjiR
wwwc
wwwc
wwwP
),,(
),,()|(  
The number of relations determined in this 
way can be very large. The upper bound for 
)|( ji wwP  and )|( kji wwwP  are respectively 
O(|V|2) and O(|V|3). However, many relations 
have very low probabilities and are often noise. 
As we only consider a subset of strong expansion 
terms, the relations with low probability are 
almost never used. Therefore, we set two filtering 
criteria: 
? The biterm in the condition of a relation should 
be higher than a threshold (10 in our case); 
? The probability of a relation should be higher 
than another threshold (0.0001 in our case). 
? One more filtering criterion is mutual 
information (MI), which reflects the 
relatedness of two terms in their combination 
),( kj ww . To keep a relation )|( kji wwwP , we 
require ),( kj ww  be a meaningful combination. 
We use the following pointwise MI (Church 
and Hanks 1989): 
)()(
),(
log),(
kj
kj
kj
wPwP
wwP
wwMI =  
 We only keep meaningful combinations such 
that 0),( >kj wwMI .  
By these filtering criteria, we are able to 
reduce considerably the number of biterms and 
triterms. For example, on a collection of about 
200MB, with a vocabulary size of about 148K, 
we selected only about 2.7M useful biterms and 
about 137M triterms, which remain tractable. 
3.3 Probability of Biterms 
In LM used in IR, each query term is attributed 
the same weight. This is equivalent to a uniform 
probability distribution, i.e.: 
U
i QQqP ||
1)|( =  
where |Q|U is the number of unigrams in the 
query. In CIQE model, we use the same method.  
In CDQE, we also need to attribute a 
probability )|( QqqP kj , to the biterm ),( kj qq . 
Several options are possible. 
Uniform probability 
This simple approach distributes the probability 
uniformly among all biterms in the query, i.e.: 
B
kj QQqqP ||
1)|( =  
where BQ ||  is the number of biterms in Q.  
According to mutual information 
In a query, if two words are strongly associated, 
this also means that their association is more 
meaningful to the query, thus should be weighted 
higher. Therefore, a natural way to assign a 
probability to a biterm in the query is to use 
mutual information, which denotes the strength 
of association between two words. We use again 
the pointwise mutual information MI(qj, qk). If it 
is negative, we consider that the biterm is not 
meaningful, and is ignored. Therefore, we arrive 
at the following probability function: 
?
?
=
Qqq
ml
kj
kj
ml
qqMI
qqMIQqqP
)(
),(
),()|(  
where Qqq ml ?)(  means all the meaningful 
biterms in the query.  
555
 Statistical parsing 
In (Gao et al, 2002), a statistical parsing 
approach is used to determine the best 
combination of translation words for a query. The 
approach is similar to building a minimal 
spanning tree, which is also used in (Smeaton and 
Van Rijsbergen, 1983), to select the strongest 
term relations that cover the whole query. This 
approach can also be used in our model to 
determine the minimal set of the strongest 
biterms that cover the query.  
In our experiments, we tested all the three 
weighting schemas. It turns out that the best 
weighting is the one with MI. Therefore, in the 
next section, we will only report the results with 
the second option. 
4. Experimental Evaluation 
We evaluate query expansion with different 
relations on four TREC collections, which are 
described in Table 1. All documents have been 
processed in a standard manner: terms are 
stemmed using Porter stemmer and stopwords are 
removed. We only use titles of topics as queries, 
which contain 3.58 words per query on average.  
Table 1. TREC collection statistics 
Coll. Description Size (Mb) Vocab. # Doc. Query 
AP Associated Press (1988-89) 491 196,933 164,597 51-100 
SJM 
San Jose 
Mercury News 
(1991) 
286 146,514 90,257 101-150 
WSJ 
Wall Street 
Journal (1990-
92) 
242 121,946 74,520 51-100 
In our experiments, the document model 
remains the same while the query model changes. 
The document model uses the following Dirichlet 
smoothing: 
?
?
+
+
=
U
iMLi
i D
CwPDwtf
DwP ||
)|(),()|(
 
where ),( Dwtf i is the term frequency of wi in D, 
)|( CwP iML  is the collection model and ?  is the 
Dirichlet prior, which is set at 1000 following 
(Zhai and Lafferty, 2001).  
There are two other smoothing parameters 1? , 
and 2?  to be determined. In our experiments, we 
use a simple method to set them: the parameters 
are tuned empirically using a training collection 
containing AP1989 documents and queries 101-
150. These preliminary tests suggest that the best 
value of 1?  and 2?  (in Equations 1-2) are 
relatively stable (we will show this later). In the 
experiments reported below, we will use 4.01 =? ,  
and 3.02 =? . 
4.1 Experimental Results 
The main experimental results are described in 
Table 2, which reports average precision with 
different methods as well as the number of 
relevant documents retrieved. UM is the basic 
unigram model without query expansion (i.e. we 
use MLE for the query model, while the 
document model is smoothed with Dirichlet 
method). CIQE is the context-independent query 
expansion model using unigram relations (Model 
1). CDQE is the context-dependent query 
expansion model using biterm relations (Model 
2). In the table, we also indicate whether the 
improvement in average precision obtained is 
statistically significant (t-test). 
Table 2. Avg. precision and Recall  
Coll. 
#Rel. UM CIQE CDQE 
0.2767 0.2902 (+5%*) 0.3383  (+22%**) 
             [+17%**] AP 6101 3677 3897 4029 
0.2017 0.2225 (+10%**) 0.2448 (+21%**) 
            [+10%*] SJM 2559 1641 1761 1873 
0.2373 0.2393 (+1%) 0.2710 (+14%**) 
            [+13%*] WSJ 2172 1588 1626 1737 
* and ** indicate that the difference is statistically 
significant according to t-test: * indicates p<0.05, ** 
indicates p<0.01; (.) is compared to UM and [.] is 
compared to CIQE. 
CIQE and CDQE vs. UM 
It is interesting to observe that query expansion, 
either by CIQE or CDQE, consistently 
outperforms the basic unigram model on all the 
collections. In all the cases except CIQE for 
WSJ, the improvements in average precision are 
statistically significant. At the same time, the 
increases in the number of relevant documents 
retrieved are also consistent with those in average 
precision. 
The improvement scales obtained with CIQE 
are relatively small: from 1% to 10%. These 
correspond to the typical figure using this 
method.  
Comparing CIQE and CDQE, we can see that 
context-dependent query expansion (CDQE) 
556
 always produces better effectiveness than 
context-independent expansion (CIQE). The 
improvements range between 10% and 17%. All 
the improvements obtained by CDQE are 
statistically significant. This result strongly 
suggests that in general, the context-dependent 
term relations identify better expansion terms 
than context-independent unigram relations. This 
confirms our earlier hypothesis.  
Indeed, when we look at the expansion 
results, we see that the expansion terms 
suggested by biterm relations are usually better. 
For example, the (stemmed) expansion terms for 
the query ?insider trading? suggested 
respectively by CIQE and CDQE are as follows: 
CIQE:  stock:0.0141 market:0.0113 US:0.0112 
year:0.0102 exchang:0.0101 trade:0.0092 
report:0.0082 price:0.0076 dollar:0.0071 
1:0.0069 govern:0.0066 state:0.0065 
futur:0.0061 million:0.0061 dai:0.0060 
offici:0.0059 peopl:0.0059 york:0.0057 
issu:0.0057 ? 
CDQE:  secur:0.0161 charg:0.0158 stock:0.0137 
scandal:0.0128 boeski:0.0125 inform:0.0119 
street:0.0113 wall:0.0112 case:0.0106 
year:0.0090 million:0.0086 investig:0.0082 
exchang:0.0080 govern:0.0077 sec:0.0077 
drexel:0.0075 fraud:0.0071 law:0.0063 
ivan:0.0060 ? 
We can see that in general, the terms suggested 
by CDQE are much more relevant. In particular, 
it has been able to suggest ?boeski? (Boesky) 
who is involved in an insider trading scandal. 
Several other terms are also highly relevant, such 
as scandal, investing, sec, drexel, fraud, etc. 
The addition of these new terms does not only 
improve recall. Precision of top-ranked 
documents is also improved. This can be seen in 
Figure 1 where we compare the full precision-
recall curve for the AP collection for the three 
models. We can see that at all the recall levels, 
the precision values always follow the following 
order: CDQE > UM. The same observation is 
also made on the other collections. This shows 
that the CDQE method does not increase recall to 
the detriment of precision, but both of them. In 
contrast, CIQE increases precision at all but 0.0 
recall points: the precision at the 0.0 recall point 
is 0.6565 for CIQE and 0.6699 for UM. This 
shows that CIQE can slightly deteriorate the top-
ranked few documents. 
Figure 1. Comparison of three models on AP 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
Pr
e
c
is
io
n CDQE
CIQE
UM
 
CDQE vs. Pseudo-relevance feedback 
Pseudo-relevance feedback is widely considered 
to be an effective query expansion method. In 
many previous experiments, it produced very 
good results. The mixture model (Zhai and 
Lafferty, 2001) is a representative and effective 
method to implement pseudo-relevance feedback: 
It uses a set of feedback documents to smooth the 
original query model. Compared to the mixture 
model, our CDQE method is also more effective: 
By manually tuning the parameters of the mixture 
model to their best, we obtained the average 
precisions of 0.3171, 0.2393 and 0.2565 
respectively for AP, SJM and WSJ collections. 
These values are lower than those obtained with 
CDQE, which has not been heavily tuned.  
For the same query ?insider trading?, the mixture 
model determines the following expansion terms: 
Mixture: stock:0.0259256 secur:0.0229553 
market:0.0157057 sec:0.013992 
inform:0.011658 firm:0.0110419 
exchang:0.0100346 law:0.00827076 
bill:0.007996 case:0.00764544 
profit:0.00672575 investor:0.00662856 
japan:0.00625859 compani:0.00609675 
commiss:0.0059618 foreign:0.00582441 
bank:0.00572947 investig:0.00572276 
We can see that some of these terms overlap with 
those suggested by biterm relations. However, 
interesting words such as boeski, drexel and 
scandal are not suggested. 
The above comparison shows that our method 
outperforms the state-of-the-art methods of query 
expansion developed so far. 
4.2 Effect of the Smoothing Parameter  
In the previous experiments, we have fixed the 
smoothing parameters. In this series of tests, we 
557
 analyze the effect of this smoothing parameter on 
retrieval effectiveness. The following figure 
shows the change of average precision (AvgP) 
using CDQE (Model 2) along with the change of 
the parameter 2? (UM is equivalent to 12 =? ).  
Figure 2. Effectiveness w.r.t. 2?  
0.15
0.2
0.25
0.3
0.35
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Lambda
A
v
g.
P AP
WSJ
SJM
 
We can see that for all the three collections, 
the effectiveness is good when the parameter is 
set in the range of 0.1-0.5. The best value for 
different collections remains stable: 0.2-0.3.  
The effect of 1?  on Model 1 is slightly 
different, but we observe the same trend. 
4.3 Number of Expansion Terms 
In the previous tests, we limit the number of 
expansion terms to 80. When different numbers 
of expansion terms are used, we obtain different 
effectiveness measures. The following figure 
shows the variation of average precision (AvgP) 
with different numbers of expansion terms, using 
CDQE method.  
Figure 3. Effectiveness w.r.t. #expansion terms 
0.15
0.20
0.25
0.30
0.35
10 20 40 80 150 300
No. expansion terms
Av
g.
P AP
WSJ
SJM
 
We can see that when more expansion terms 
are added, the effectiveness does not always 
increase. In general, a number around 80 will 
produce good results. In some cases, even if 
better effectiveness can be obtained with more 
expansion terms, the retrieval time is also longer. 
The number 80 seems to produce a good 
compromise between effectiveness and retrieval 
speed: the retrieval time remains less than 1 sec. 
per query. 
4.4 Suitability of Relations Across 
Collections 
In many real applications (e.g. Web search), we 
do not have a static document collection from 
which relations can be extracted. The question is 
whether it is possible and beneficial to extract 
relations from one text collection and use them to 
retrieve documents in another text collection. Our 
intuition is that this is possible because the 
relations (especially context-dependent relations) 
encode general knowledge, which can be applied 
to a different collection. In order to show this, we 
extracted term relations from each collection, and 
applied them on other collections. The following 
tables show the effectiveness produced using 
respectively unigram and bi-term relations. 
Table 3. Cross-utilization of relations 
 
Unigram relation Biterm relation 
   Rel. 
Coll. AP SJM WSJ AP SJM WSJ 
AP 0.2902  0.2803  0.2793 0.3383 0.3057 0.2987 
SJM 0.2271 0.2225 0.2267 0.2424 0.2448 0.2453 
WSJ 0.2541  0.2445  0.2393 0.2816 0.2636 0.2710 
 
From this table, we can observe that relations 
extracted from any collection are useful to some 
degree: they all outperform UM (see Table 2). In 
particular, the relations extracted from AP are the 
best for almost all the collections. This can be 
explained by the larger size and wider coverage 
of the AP collection. This suggests that we do not 
necessarily need to extract term relations from 
the same text collection on which retrieval is 
performed. It is possible to extract relations from 
a large text collection, and apply them to other 
collections. This opens the door to the possibility 
of constructing a general relation base for various 
document collections. 
5. Related Work 
Co-occurrence analysis is a common method to 
determine term relations. The previous studies 
have been limited to relations between two 
words, which we called unigram relations. This 
expansion approach has been integrated both in 
traditional retrieval models (Jing and Croft, 
1994) and in LM (Berger and Lafferty 1999). As 
we observed, this type of relation will introduce 
much noise into the query, leading to unstable 
effectiveness. 
Several other studies tried to filter out noise 
expansion (or translation) terms by considering 
the relations between them (Gao et al, 2002; 
558
 Jang et al 1999; Qiu and Frei, 1993; Bai et al 
2005). However, this is insufficient to detect all 
the noise. The key issue is the ambiguity of 
relations due to the lack of context information in 
the relations. In this paper, we proposed a method 
to add some context information into relations.  
 (Lin, 1997) also tries to solve word ambiguity 
by adding syntactic dependency as context. 
However, our approach does not require 
determining syntactic dependency. The principle 
of our approach is more similar to (Yarowsky, 
1995). Compared to this latter, our approach is 
less demanding: we do not need to identify 
manually the exact word senses and seed context 
words. The process is fully automatic. This 
simplification is made possible due to the 
requirement for IR: only in-context related words 
are required, but not the exact senses.  
Our work is also related to (Smadja and 
McKeown, 1996), which tries to determine the 
translation of collocations. Term combinations or 
biterms we used can be viewed as collocations. 
Again, there is much less constraint for our 
related terms than translations in (Smadja and 
McKeown, 1996). 
6. Conclusions 
In many NLP applications such as IR, we need to 
determine relations between terms. In most 
previous studies, one tries to determine the 
related terms to one single term (word). This 
makes the resulting relations ambiguous. 
Although several approaches have been proposed 
to remove afterwards some of the inappropriate 
terms, this only affects part of the noise, and 
much still remains. In this paper, we argue that 
the solution to this problem lies in the addition of 
context information in the relations between 
terms. We proposed to add another word in the 
condition of the relations so as to help constrain 
the context of application. Our experiments 
confirm that this addition of limited context 
information can indeed improve the quality of 
term relations and query expansion in IR. 
In this paper, we only compared biterm 
relations and unigram relations, the general 
method can be extended to triterm relations or 
more complex relations, provided that they can 
be extracted efficiently.  
This paper only investigated the utilization of 
context-dependent relations in IR. These relations 
can be applied in many other tasks, such as 
machine translation, word sense disambiguation / 
discrimination, and so on. These are some 
interesting research work in the future. 
References 
Bai, J., Song, D., Bruza, P., Nie, J. Y. and Cao, G. 
2005. Query expansion using term relationships in 
language models for information retrieval, ACM 
CIKM, pp. 688-695. 
Berger, A. and Lafferty, J. 1999. Information retrieval 
as statistical translation. ACM SIGIR, pp. 222-229. 
Church, K. W. and Hanks, P. 1989. Word association 
norms, mutual information, and lexicography. ACL, 
Vol. 16, pp. 22-29. 
Gao, J., Nie, J.Y., He, H, Chen, W., Zhou, M. 2002. 
Resolving query translation ambiguity using a 
decaying co-occurrence model and syntactic 
dependency relations. ACM SIGIR, pp. 11-15. 
Jang, M. G., Myaeng, S. H., and Park, S. Y. 1999. 
Using mutual information to resolve query 
translation ambiguities and query term weighting. 
ACL, pp. 223-229. 
Jing, Y. and Croft, W.B. 1994. An association 
thesaurus for information retrieval. RIAO, pp. 146-
160. 
Lin, D. 1997. Using syntactic dependency as local 
context to resolve word sense ambiguity, ACL, pp. 
64-71. 
Peat, H.J. and Willett, P. 1991. The limitations of term 
co-occurrence data for query expansion in document 
retrieval systems. JASIS, 42(5): 378-383. 
Ponte, J. and Croft, W.B. 1998. A language modeling 
approach to information retrieval. ACM SIGIR, pp. 
275-281. 
Qiu, Y. and Frei, H.P. 1993. Concept based query 
expansion. ACM SIGIR, pp.160-169. 
Sch?tze, H. and Pedersen J.O. 1997. A cooccurrence-
based thesaurus and two applications to information 
retrieval, Information Processing and Management, 
33(3): 307-318. 
Smeaton, A. F. and Van Rijsbergen, C. J. 1983. The 
retrieval effects of query expansion on a feedback 
document retrieval system. Computer Journal, 26(3): 
239-246. 
Smadja, F., McKeown, K.R., 1996. Translating 
collocations for bilingual lexicons: A statistical 
approach, Computational Linguistics, 22(1): 1-38. 
Srikanth, M. and Srihari, R. 2002. Biterm language 
models for document retrieval. ACM SIGIR, pp. 425-
426  
Voorhees, E. 1994. Query expansion using lexical-
semantic relations. ACM SIGIR, pp. 61-69. 
Yarowsky, D. 1995. Unsupervised word sense 
disambiguation rivaling supervised methods. ACL, 
pp. 189-196. 
Zhai, C. and Lafferty, J. 2001. Model-based feedback 
in the language modeling approach to information 
retrieval. ACM SIGIR, pp. 403-410.  
559
