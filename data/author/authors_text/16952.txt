First Joint Conference on Lexical and Computational Semantics (*SEM), pages 502?505,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
BUAP: A First Approximation to Relational Similarity Measuring
Mireya Tovar, J. Alejandro Reyes,
Azucena Montes
CENIDET, Department of
Computer Science
Int. Internado Palmira S/N, Col. Palmira
Cuernavaca, Morelos, Me?xico
{mtovar, alexreyes06c, amr}
@cenidet.edu.mx
Darnes Vilarin?o, David Pinto,
Saul Leo?n
B. Universidad Auto?noma de Puebla,
Faculty of Computer Science
14 Sur y Av. San Claudio, CU
Puebla, Puebla, Me?xico
{darnes, dpinto}@cs.buap.mx
saul.ls@live.com
Abstract
We describe a system proposed for measuring
the degree of relational similarity beetwen a
pair of words at the Task #2 of Semeval 2012.
The approach presented is based on a vec-
torial representation using the following fea-
tures: i) the context surrounding the words
with a windows size = 3, ii) knowledge ex-
tracted from WordNet to discover several se-
mantic relationships, such as meronymy, hy-
ponymy, hypernymy, and part-whole between
pair of words, iii) the description of the pairs
with their POS tag, morphological informa-
tion (gender, person), and iv) the average num-
ber of words separating the two words in text.
1 Introduction
The Task # 2 of Semeval 2012 focuses on measuring
the degree of relational similarity between the ref-
erence words pairs (training) and the test pairs for a
given class (Jurgens et al, 2012).
The training data set consists of 10 classes and
the testing data set consists of the 69 classes. These
datasets as well as the particularities of the task are
better described at overview paper (Jurgens et al,
2012). In this paper we report the approach submit-
ted to the competition, which is based on a vector
space model representation for each pair (Salton et
al., 1975). With respect to the type of features used,
we have observed that Fabio Celli (Celli, 2010) con-
siders that contextual information is useful, as well
the lexical and semantic information are in the ex-
traction of semantic relationships task. Additionally,
in (Chen et al, 2010) and (Negri and Kouylekov,
2010) are proposed WordNet based features with the
same purpose.
In the experiments carried out in this paper, we
use a set of lexical, semantic, WordNet-based and
contextual features which allows to construct the
vectors. Actually, we have tested a subset of the 20
contextual features proposed by Celli (Celli, 2010)
and some of those proposed by Chen (Chen et al,
2010) and Negri (Negri and Kouylekov, 2010).
The cosine similarity measure is used for deter-
mining the degree of relational similarity (Frakes
and Baeza-Yates, 1992) among the vectors.
The rest of this paper is structured as follows.
Section 2 describes the system employed. Section
3 show the obtained results. Finally, in Section 4 the
final conclusions are given.
2 System description
The approach reported in this paper measures the
relational similarity of a set of word pairs that be-
long to the same semantic relationship. Those word
pairs are represented by means of the vector space
model (Salton et al, 1975). Each value of the vec-
tor represents the average value of the correspond-
ing feature. This average is calculated using 100
samples obtained from Internet by employing the
Google search engine. The search process is car-
ried out assuming that those words co-occurring in
the same context contain some kind of semantic re-
lationship.
Let (w1, w2) be a word pair, then the vectorial
representation of this pair (~x) using semantic, con-
textual, lexical, and WordNet-based features may be
expressed as it can be seen in Eq. (1).
502
~x = (avg(f1), avg(f2), ..., avg(fn)) (1)
where avg(fk) is the average value of the feature fk.
The cardinality of the vector is 42, because we
extracted 4 lexical features, 6 semantic features, 7
WordNet-based features and 25 contextual features
(n = 42). Each word pair is then represented by
a unique vector with values associated to each fea-
ture. In Figure 1, we show the vectorial represen-
tation of the word pair (transportation, bus) using
a unique text sample (s). In this example, the num-
ber and type of features described below is followed,
i.e., the first 4 values are lexical, the following 6 are
semantic and so on.
s =?The Toyama Chih Railway is a transporta-
tion company that operates railway, tram, and
bus lines in the eastern part of the prefecture.?
~x = (6, 1, 0, 0, 27, 4, 4, 4, 4, 5, 2, 4, 5, 25, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4,
4, 0, 4, 4, 4, 4)
Figure 1: Example of a feature vector for a word pair and
its corresponding sentence s.
The previous example is only illustrative, since
we have gathered 100 sentence per word pair. In
total, we collected a corpus containing 2,054,687 to-
kens, with a average class terms of 26,684 and with
an average class vocabulary of 4,006.
The features extracted are described as follows:
2.1 Lexical features
The lexical features describe morphologically and
syntactically the word pair (w1, w2). The lexical
features extracted are the following:
? Average number of words separating the two
words (w1, w2) in the text.
? The position of w1 with respect to w2 in the
text. If w1 appears before w2 then the feature
value is 1, otherwise, the value is 2.
? The Part of Speech Tag for each word in the
pair (two features). We use the FreeLing PoS-
tagger (Padro? et al, 2010) for obtaining the
grammatical category. The possible values are
the following: adjective=1; adverb=2; arti-
cle=3; noun=4; verb=5; pronoun=6; conjunc-
tion=7; preposition=8
2.2 Semantic features
The following four semantic features are boolean
values (true or false) indicating:
? If w1 and w2 are named entities (two features)1.
? If w1 and w2 are entities defined (two fea-
tures)2.
The following two semantic features indicate:
? The type of prepositional phrase in case of
existing for w1 and w2. The feature val-
ues are nominal: about=1; after=2; at=3; be-
hind=4; between=5; by=6; except=7; from=8;
into=9; near=10; of=11; over=12; through=13;
until=14; under=15; upon=16; without=17;
above=18; among=19; before=20; below=21;
beside=22; but=23; down=24; for=25; in=26;
on=27; since=28; to=29; with=30.
2.3 WordNet-based features
The semantic features are boolean values (true or
false) indicating whether or not w2 is contained in:
? the synonym set of w1
? the antonym set of w1
? the meronymy set of w1
? the hyponymy set of w1
? the hypernymy set of w1
? the part-whole set of w1
? the gloss set of w1
We used WordNet (Fellbaum, 1998) in order to de-
termine the relationship set for word w1.
1A named entity is defined by a Proper Noun Phrase, which
was detected using the module NER-Named Entity Recognition
of the FreeLing 2.1 tool.
2A defined sentence is one that begins with a definite article.
503
2.4 Contextual features
Contextual features considers values for the words
that occur in the context of w1 and w2 (in a window
size of 3). The description of those features follows.
? Nominal values indicating the Part of Speech
Tag (adjective=1; adverb=2; article=3; noun=4;
verb=5; pronoun=6; conjunction=7; preposi-
tion=8) for the three words at:
? the left context of w1 (three features).
? the right context of w1 (three features).
? the left context of w2 (three features).
? the right context of w2 (three features).
? A Nominal value indicating number of the fol-
lowing grammatical categories between w1 and
w2: verbs, adjectives and nouns (three fea-
tures).
? Nominal values indicating the frequencies of
the verbs: be, do, have, locate, know, make, use,
become, include, take between w1 and w2 (ten
features).
2.5 Feature selection
We carried out a feature selection process with the
aim of discarding irrelevant features. In this step,
we apply the attribute selection filter reported in
(Hall, 1999), that evaluates the worth of a subset
of attributes by considering the individual predic-
tive ability of each feature along with the degree of
redundancy between them and an exhaustive search
method.
The following features were obtained as relevant:
the average number of words between w1 and w2;
Named Entity of w1 and w2; phrase defined of w1
and w2; prepositional phrase type w1 and w2; part
of speech tag w1 and w2; part of speech tag of
right context of w1 with a windows size of 3; oc-
currences of verbs between w1 and w2; frequency of
verbs be, do, make, locate, take; synonym, antonym,
meronymy, hyponymy, hypernymy, part-whole and
gloss relationships between w1 and w2.
After applying the aforementioned feature selec-
tion method, we removed 17 features, and the vec-
torial representation of each word pair will be done
with only 25 values (features).
2.6 Determining the degree of similarity
We have used the features mentioned before for con-
structing a prototype vector representing a given se-
mantic class. In order to do so, we have employed
the training corpus for gathering samples from Inter-
net and, thereafter, we average the feature values in
order to construct such prototype vector.
For each word pair in the test dataset, we ob-
tained a vector using the same process explained
before. We determined the similarity for each test
feature vector with respect to the prototype of the
given class by using the cosine similarity coefficient
(Frakes and Baeza-Yates, 1992), i.e., measuring the
cosine of the angle between the two vectors.
In this way, we obtain a similarity measure of each
test word pair with respect to its corresponding class.
Finally, we may output a ranking of all the word
pairs at the test dataset by sorting these similarity
values obtained.
3 Experimental results
The approach submitted to the Task #2 of SemEval
2012 obtained very poor results. The Spearman cor-
relation coefficient, which measured the correlation
of the approach with respect to the gold standard, it
is quite low (see Table 1).
Team-Algorithm Spearman MaxDiff
UTD-NB 0.23 39.4
UTD-SVM 0.12 34.7
DULUTH-V0 0.05 32.4
DULUTH-V1 0.04 31.5
DULUTH-V2 0.04 31.1
BUAP 0.01 31.7
Random 0.02 31.2
Table 1: Spearman and MaxDiff scores obtained at the
Task #2 of Semeval 2012
Actually, it shows that the run submitted does not
correlate with the gold standard. We consider that
this behavior is derived from the nature of the sup-
port corpus used for obtaining the features set. The
number of sentences (100) used for representing the
word pairs was not enough for constructing a real
prototype of both, the semantic class and the word
pairs. A further analysis will confirm this issue.
504
Despite this limitation we note that the MaxDiff
score was 31.7% slightly above the baseline (31.2%)
and not far from the best score of the task (39.4%).
That is, we achieved an average of 31.7% of ques-
tions answered correctly.
4 Discussion and conclusion
In this paper we report the set of features used in
the approach submitted for measuring the degrees of
relational similarity between a given reference word
pair and a variety of other pairs. The results obtained
are not encouraging with a Spearman correlation co-
efficient close to zero, which mean that there are
not correlation between the run submitted and the
gold standard. A deeper analysis of the approach is
needed in order to determine if the limitation of the
system falls in the features used, the similarity mea-
sure, or the support corpus used for extracting the
features.
Acknowledgments
This project has been partially supported by projects
CONACYT #106625, VIEP #PIAD-ING11-II and
#VIAD-ING11-II.
References
Fabio Celli. 2010. Unitn: Part-of-speech counting in
relation extraction. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, SemEval
?10, pages 198?201, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Yuan Chen, Man Lan, Jian Su, Zhi Min Zhou, and
Yu Xu. 2010. Ecnu: Effective semantic relations
classification without complicated features or multi-
ple external corpora. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, SemEval
?10, pages 226?229, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press.
William B. Frakes and Ricardo A. Baeza-Yates, editors.
1992. Information Retrieval: Data Structures & Algo-
rithms. Prentice-Hall.
Mark A. Hall. 1999. Correlation-based Feature Sub-
set Selection for Machine Learning. Ph.D. thesis, De-
partment of Computer Science, University of Waikato,
Hamilton, New Zealand.
David A. Jurgens, Saif M. Mohammad, Peter D. Turney,
and Keith J. Holyoak. 2012. Semeval-2012 task 2:
Measuring degrees of relational similarity. In Pro-
ceedings of the 6th International Workshop on Seman-
tic Evaluation (SemEval 2012), Montreal, Canada.
Matteo Negri and Milen Kouylekov. 2010. Fbk nk: A
wordnet-based system for multi-way classification of
semantic relations. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, SemEval
?10, pages 202?205, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Llu??s Padro?, Miquel Collado, Samuel Reese, Marina
Lloberes, and Irene Castello?n. 2010. Freeling
2.1: Five years of open-source language processing
tools. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta. European Language Re-
sources Association (ELRA).
G. Salton, A. Wong, and C. S. Yang. 1975. A vector
space model for automatic indexing. Commun. ACM,
18(11):613?620, November.
505
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 631?634,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
BUAP: Three Approaches for Semantic Textual Similarity
Maya Carrillo, Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n, Esteban Castillo
Beneme?rita Universidad Auto?noma de Puebla,
Faculty of Computer Science
14 Sur & Av. San Claudio, CU
Puebla, Puebla, Me?xico
{cmaya, darnes, dpinto, mtovar}@cs.buap.mx
saul.ls@live.com, ecjbuap@gmail.com
Abstract
In this paper we describe the three approaches
we submitted to the Semantic Textual Similar-
ity task of SemEval 2012. The first approach
considers to calculate the semantic similar-
ity by using the Jaccard coefficient with term
expansion using synonyms. The second ap-
proach uses the semantic similarity reported
by Mihalcea in (Mihalcea et al, 2006). The
third approach employs Random Indexing and
Bag of Concepts based on context vectors. We
consider that the first and third approaches ob-
tained a comparable performance, meanwhile
the second approach got a very poor behav-
ior. The best ALL result was obtained with
the third approach, with a Pearson correlation
equal to 0.663.
1 Introduction
Finding the semantic similarity between two sen-
tences is very important in applications of natural
language processing such as information retrieval
and related areas. The problem is complex due to the
small number of terms involved in sentences which
are tipically less than 10 or 15. Additionally, it is re-
quired to ?understand? the meaning of the sentences
in order to determine the ?semantic? similarity of
texts, which is quite different of finding the lexical
similarity.
There exist different works at literature dealing
with semantic similarity, but the problem is far to
be solved because of the aforementioned issues.
In (Mihalcea et al, 2006), for instance, it is pre-
sented a method for measuring the semantic simi-
larity of texts, using corpus-based and knowledge-
based measures of similarity. The approaches pre-
sented in (Shrestha, 2011) are based on the Vector
Space Model, with the aim to capture the contex-
tual behavior, senses and correlation, of terms. The
performance of the method is better than the base-
line method that uses vector based cosine similarity
measure.
In this paper, we present three different ap-
proaches for the Textual Semantic Similarity task of
Semeval 2012 (Agirre et al, 2012). The task is de-
scribed as follows: Given two sentences s1 and s2,
the aim is to compute how similar s1 and s2 are,
returning a similarity score, and an optional confi-
dence score. The approaches should provide values
between 0 and 5 for each pair of sentences. These
values roughly correspond to the following consid-
erations, even when the system should output real
values:
5: The two sentences are completely equivalent,
as they mean the same thing.
4: The two sentences are mostly equivalent, but
some unimportant details differ.
3: The two sentences are roughly equivalent, but
some important information differs/missing.
2: The two sentences are not equivalent, but share
some details.
1: The two sentences are not equivalent, but are
on the same topic.
0: The two sentences are on different topics.
631
The description of the runs submitted to the com-
petition follows.
2 Experimentation setup
The three runs submitted to the competition use
completely different mechanisms to find the degree
of semantic similarity between two sentences. The
approaches are described as follows:
2.1 Approach BUAP-RUN-1: Term expansion
with synonyms
Let s1 = w1,1w1,2...w1,|s1| and s2 =
w2,1w2,2...w2,|s2| be two sentences. The synonyms
of a given word wi,k, expressed as synonyms(wi,k),
are obtained from online dictionaries by extracting
the synonyms of wi,k. A better matching between
the terms contained in the text fragments and the
terms at the dictionary are obtained by stemming all
the terms (using the Porter stemmer).
In order to determine the semantic similarity be-
tween any pair of terms of the two sentences (w1,i
and w2,j) we use Eq. (1).
sim(w1,i, w2,j) =
?
?
?
?
?
?
?
?
?
1 if (w1,i == w2,j) ||
w1,i ? synonyms(w2,j) ||
w2,j ? synonyms(w1,i)
0 otherwise
(1)
The similarity between sentences s1 and s2 is cal-
culated as shown in Eq. (2).
similarity(s1, s2) =
5 ? ?ni=1
?n
j=1 sim(w1i, w2j)
|s1 ? s2|
(2)
2.2 Approach BUAP-RUN-2
In this approach, the similarity of s1 and s2 is calcu-
lated as shown in Eq. (3) (Mihalcea et al, 2006).
similarity(s1, s2) = 12 (
?
w?{s1}
(maxSim(w,s2)?idf(w))
?
w?{s1}
idf(w)
+
?
w?{s2}
(maxSim(w,s1)?idf(w))
?
w?{s2}
idf(w) )
(3)
where idf(w) is the inverse document frequency of
the word w, and maxSim(w, s2) is the maximum
lexical similarity between the word w in sentence s2
and all the words in sentence s2 calculated by means
of the Eq. (4) reported by (Wu and Palmer, 1994).
The sentence terms are assumed to be concepts, LCS
is the depth of the least common subsumer, and the
equation is calculated using the NLTK libraries1.
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(4)
2.3 Approach BUAP-RUN-3: Random
Indexing and Bag of Concepts
The vector space model (VSM) for document rep-
resentation supporting search is probably the most
well-known IR model. The VSM assumes that term
vectors are pair-wise orthogonal. This assumption
is very restrictive because words are not indepen-
dent. There have been various attempts to build
representations for documents that are semantically
richer than only vectors based on the frequency of
terms occurrence. One example is Latent Seman-
tic Indexing (LSI), a method of word co-occurrence
analysis to compute semantic vectors (context vec-
tors) for words. LSI applies singular-value decom-
position (SVD) to the term-document matrix in or-
der to construct context vectors. As a result the di-
mension of the produced vector space will be signif-
icantly smaller; consequently the vectors that repre-
sent terms cannot be orthogonal. However, dimen-
sion reduction techniques such as SVD are expen-
sive in terms of memory and processing time. Per-
forming the SVD takes time O (nmz), where n is
the vocabulary size, m is the number of documents,
and z is the number of nonzero elements per column
in the words-by-documents matrix. As an alterna-
tive, there is a vector space methodology called Ran-
dom Indexing (RI) (Sahlgren, 2005), which presents
an efficient, scalable, and incremental method for
building context vectors. Its computational com-
plexity is O (nr) where n is as previously described
and r is the vector dimension. Particularly, we apply
RI to capture the inherent semantic structure using
Bag of Concepts representation (BoC) as proposed
by Sahlgren and Co?ster (Sahlgren and Co?ster, 2004),
where the meaning of a term is considered as the
sum of contexts in which it occurs.
1http://www.nltk.org/
632
2.3.1 Random Indexing
Random Indexing (RI) is a vector space method-
ology that accumulates context vectors for words
based on co-occurrence data. The technique can be
described as:
? First a unique random representation known as
index vector is assigned to each context (docu-
ment). Index vectors are binary vectors with a
small number of non-zero elements, which are
either +1 or -1, with equal amounts of both.
For example, if the index vectors have twenty
non-zero elements in a 1024-dimensional vec-
tor space, they have ten +1s and ten -1s. Index
vectors serve as indices or labels for documents
? Index vectors are used to produce context vec-
tors by scanning through the text and every
time a target word occurs in a context, the in-
dex vector of the context is added to the con-
text vector of the target word. Thus, at each
encounters of the target word t with a context c
the context vector of t is updated as follows: ct
+ = ic where ct is the context vector of t and ic
is the index vector of c. In this way, the context
vector of a word keeps track of the contexts in
which it occurred.
RI methodology is similar to latent semantic in-
dexing (LSI) (Deerwester et al, 1990). However,
to reduce the co-occurrence matrix no dimension re-
duction technique such as SVD is needed, since the
dimensionality d of the random index vectors is pre-
established as a parameter (implicit dimension re-
duction). Consequently d does not change once it
has been set; as a result, the dimensionality of con-
text vectors will never change with the addition of
new data.
2.3.2 Bag of Concepts
Bag of Concepts (BoC) is a recent representa-
tion scheme proposed by Sahlgren and Co?ster in
(Sahlgren and Co?ster, 2004), which is based on the
perception that the meaning of a document can be
considered as the union of the meanings of its terms.
This is accomplished by generating term context
vectors from each term within the document, and
generating a document vector as the weighted sum
of the term context vectors contained within that
document. Therefore, we use RI to represent the
meaning of a word as the sum of contexts (entire
documents) in which it occurs. Illustrating this tech-
nique, suppose you have two documents: D1: A man
with a hard hat is dancing, and D2: A man wearing
a hard hat is dancing. Let us suppose that they have
index vectors ID1 and ID2, respectively: the context
vector for hat will be the ID1 + ID2, because this
word appears in both documents. Once the context
vectors have been built by RI, they are used to repre-
sent the document as BoC. For instance, supposing
CV1, CV2, CV3, . . . and CV8, are the context vec-
tors of each word in D1, then document D1 will be
represented as the weighted sum of these eight con-
text vectors.
2.3.3 Implementation
The sentences of each file were processed to gen-
erate the BoC representations of them. BoC rep-
resentations were generated by first stemming all
words in the sentences. We then used random index-
ing to produce context vectors for each word in the
files (i.e. STS.input.MSRpar, STS.input.MSRvid,
etc.), each file was considered a different corpus and
documents were the sentences in them. The dimen-
sion of the context vectors was fixed at 2048, de-
termined by experimentation using the training set.
These context vectors were then tf ? idf -weighted,
according to the corpus, and added up for each sen-
tence, to produce BoC representations. Therefore
the similarity values were calculated by the cosine
function. Finally cosine values were multiplied by 5
to produce values between 0 and 5.
3 Experimental results
In Table 1 we show the results obtained by the
three approaches submitted to the competition. The
columns of Table 1 stand for:
? ALL: Pearson correlation with the gold stan-
dard for the five datasets, and corresponding
rank.
? ALLnrm: Pearson correlation after the system
outputs for each dataset are fitted to the gold
standard using least squares, and corresponding
rank.
633
Run ALL Rank ALL
nrm
Rank
Nrm
Mean Rank
Mean
MSR
par
MSR
vid
SMT
eur
On -
WN
SMT-
news
BUAP-
RUN-1
0.4997 63 0.7568 62 0.4892 57 0.4037 0.6532 0.4521 0.605 0.4537
BUAP-
RUN-2
-0.026 89 0.5933 89 0.0669 89 0.1109 0.0057 0.0348 0.1788 0.1964
BUAP-
RUN-3
0.663 25 0.7474 64 0.488 59 0.4018 0.6378 0.4758 0.5691 0.4057
Table 1: Results of approaches of BUAP in Task 6.
? Mean: Weighted mean across the 5 datasets,
where the weight depends on the number of
pairs in the dataset.
Followed by Pearson for individual datasets.
At this moment, we are not aware of the reasons
because the second approach obtained a very poor
performance. The way in which the idf(w) is calcu-
lated could be one of the reasons, because the corpus
used is relatively small and also from a different do-
main. With respect to the other two approaches, we
consider that they (first and third) obtained a com-
parable performance, even when the third approach
obtained the best ALL result with a Pearson correla-
tion equal to 0.663.
4 Discussion and conclusion
We have presented three different approaches for
tackling the problem of Semantic Textual Similarity.
The use of term expansion by synonyms performed
well in general and obtained a comparable behavior
than the third approach which used random index-
ing and bag of concepts. It is interesting to observe
that these two approaches performed similar when
the two term expansion mechanism are totally dif-
ferent. As further, it is important to analyze the poor
behavior of the second approach. We would like also
to introduce semantic relationships other than syn-
onyms in the process of term expansion.
Acknowledgments
This project has been partially supported by
projects CONACYT #106625, #VIAD-ING11-II,
PROMEP/103.5/11/4481 and VIEP #PIAD-ING11-
II.
References
E. Agirre, D. Cer, M. Diab, and B. Dolan. 2012.
SemEval-2012 Task 6: Semantic Textual Similarity.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012).
Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-
dauer, George W. Furnas, and Richard A. Harshman.
1990. Indexing by Latent Semantic Analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In proceedings of AAAI?06,
pages 775?780.
Magnus Sahlgren and Rickard Co?ster. 2004. Using bag-
of-concepts to improve the performance of support
vector machines in text categorization. In Proceedings
of the 20th international conference on Computational
Linguistics, COLING ?04, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
M. Sahlgren. 2005. An Introduction to Random Index-
ing. Methods and Applications of Semantic Indexing
Workshop at the 7th International Conference on Ter-
minology and Knowledge Engineering, TKE 2005.
Prajol Shrestha. 2011. Corpus-based methods for short
text similarity. In TALN 2011, Montpellier, France.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In 32nd. Annual Meeting of the
Association for Computational Linguistics, pages 133
?138, New Mexico State University, Las Cruces, New
Mexico.
634
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 706?709,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
BUAP: Lexical and Semantic Similarity for Cross-lingual Textual
Entailment
Darnes Vilarin?o, David Pinto, Mireya Tovar, Saul Leo?n, Esteban Castillo
Beneme?rita Universidad Auto?noma de Puebla,
Faculty of Computer Science
14 Sur & Av. San Claudio, CU
Puebla, Puebla, Me?xico
{darnes, dpinto, mtovar}@cs.buap.mx
saul.ls@live.com, ecjbuap@gmail.com
Abstract
In this paper we present a report of the two di-
fferent runs submitted to the task 8 of Semeval
2012 for the evaluation of Cross-lingual Tex-
tual Entailment in the framework of Content
Synchronization. Both approaches are based
on textual similarity, and the entailment judg-
ment (bidirectional, forward, backward or no
entailment) is given based on a set of decision
rules. The first approach uses textual simi-
larity on the translated and original versions
of the texts, whereas the second approach ex-
pands the terms by means of synonyms. The
evaluation of both approaches show a similar
behavior which is still close to the average and
median.
1 Introduction
Cross-lingual Textual Entailment (CLTE) has been
recently proposed by (Mehdad et al, 2010; Mehdad
et al, 2011) as an extension of the Textual Entail-
ment task (Dagan and Glickman, 2004). Given a text
(T ) and an hypothesis (H) in different languages,
the CLTE task consists of determining if the mea-
ning of H can be inferred from the meaning of T .
In this paper we present a report of the obtained
results after submitting two different runs for the
Task 8 of Semeval 2012, named ?Cross-lingual Tex-
tual Entailment for Content Synchronization? (Negri
et al, 2012). In this task, the Cross-Lingual Tex-
tual Entailment addresses textual entailment recog-
nition under a new dimension (cross-linguality), and
within a new challenging application scenario (con-
tent synchronization). The task 8 of Semeval 2012
may be formally defined as follows:
Given a pair of topically related text fragments
(T1 and T2) in different languages, the task consists
of automatically annotating it with one of the follo-
wing entailment judgments:
? Bidirectional (T1 ? T2 & T1 ? T2): the two
fragments entail each other (semantic equiva-
lence)
? Forward (T1 ? T2 & T1 ! ? T2): unidirec-
tional entailment from T1 to T2
? Backward (T1 ! ? T2 & T1 ? T2): unidirec-
tional entailment from T2 to T1
? No Entailment (T1 ! ? T2 & T1 ! ? T2): there
is no entailment between T1 and T2
In this task, both T1 and T2 are assumed to be
TRUE statements; hence in the dataset there are no
contradictory pairs. Cross-lingual datasets are avai-
lable for the following language combinations:
? Spanish/English (SPA-ENG)
? German/English (DEU-ENG)
? Italian/English (ITA-ENG)
? French/English (FRA-ENG)
The remaining of this paper is structured as fo-
llows: Section 2 describes the two different approa-
ches presented in the competition. The obtained re-
sults are shown and dicussed in Section 3. Finally,
the findings of this work are given in Section 4.
706
2 Experimental setup
For this experiment we have considered to tackle the
CLTE task by means of textual similarity and textual
length. In particular, the textual similarity is used to
determine whether some kind of entailment exists or
not. We have established the threshold of 0.5 for the
similarity function as evidence of textual entailment.
Since the two sentences to be evaluated are written
in two different languages, we have translated each
sentence to the other language, so that, we have two
sentences in English, and two sentences in the origi-
nal language (Spanish, German, Italian and French).
We have used the Google translate for this purpose
1
.
The corpora used in the experiments comes from
a cross-lingual Textual Entailment dataset presented
in (Negri et al, 2011), and provided by the task orga-
nizers. We have employed the training dataset only
for adjust some parameters of the system, but the
approach is knowledge-based and, therefore, it does
not need a training corpus. Both, the training and
test corpus contain 500 sentences for each language.
The textual length is used to determine the entail-
ment judgment (bidirectional, forward, backward,
no entailment). We have basically, assumed that the
length of a text may give some evidence of the type
of entailment. The decision rules used for determi-
ning the entailment judgment are described in Sec-
tion 2.3.
In this competition we have submitted two diffe-
rent runs which differ with respect to the type of tex-
tual similarity used (lexical vs semantic). The first
one, calculates the similarity using only the trans-
lated version of the original sentences, whereas the
second approach uses text expansion by means of
synonyms and, thereafter, it calculates the similarity
between the pair of sentences.
Let T1 be the sentence in the original language,
T2 the T1 topically related text fragment (written in
English). Let T3 be the English translation of T1,
and T4 the translation of T2 to the original language
(Spanish, German, Italian and French). The formal
description of these two approaches are given as fo-
llows.
1http://translate.google.com.mx/
2.1 Approach 1: Lexical similarity
The evidence of textual entailment between T1 and
T2 is calculated using two formulae of lexical si-
milarity. Firstly, we determine the similarity bet-
ween the two texts written in the source language
(SimS). Additionally, we calculate the lexical simi-
larity between the two sentences written in the target
language (SimT ), in this case English.
Given the limited text length of the text fragments,
we have used the Jaccard coefficient as similarity
measure. Eq. (1) shows the lexical similarity for the
two texts written in the original language, whereas,
Eq. (2) presents the Jaccard coefficient for the texts
written in English.
simS = simJaccard(T1, T4) =
|T1 ? T4|
|T1 ? T4|
(1)
simT = simJaccard(T2, T3) =
|T2 ? T3|
|T2 ? T3|
(2)
2.2 Approach 2: Semantic similarity
In this case we calculate the semantic similarity bet-
ween the two texts written in the original language
(simS), and the semantic similarity between the two
text fragments written in English (simT ). The se-
mantic level of similarity is given by considering
the synonyms of each term for each sentence (in
the original and target language). For this purpose,
we have employed five dictionaries containing syno-
nyms for the five different languages considered in
the competition (English, Spanish, German, Italian,
and French)2. In Table 1 we show the number of
terms, so as the number of synonyms in average by
term considered for each language.
Let T1 = w1,1w1,2...w1,|T1|, T2 =
w2,1w2,2...w2,|T2| be the source and target
sentences, and let T3 = w3,1w3,2...w3,|T3|,
T4 = w4,1w4,2...w4,|T4| be translated version of the
original source and target sentences, respectively.
The synonyms of a given word wi,k, expressed as
synset(wi,k), are obtained from the aforementioned
dictionaries by extracting the synonyms of wi,k. In
order to obtain a better matching between the terms
contained in the text fragments and the terms in the
2http://extensions.services.openoffice.org/en/dictionaries
707
Table 1: Dictionaries of synonyms used for term expan-
sion
Language Terms synonyms per term
(average)
English 2,764 60
Spanish 9,887 45
German 21,958 115
Italian 25,724 56
French 36,207 93
dictionary, we have stemmed all the terms using the
Porter stemmer.
In order to determine the semantic similarity bet-
ween two terms of sentences written in the source
language (w1,i and w4,j) we use Eq. (3). The se-
mantic similariy between two terms of the English
sentences are calculated as shown in Eq. (4).
sim(w1,i, w4,j) =
?
?
?
?
?
?
?
?
?
1 if (w1,i == w4,j) ||
w1,i ? synset(w4,j) ||
w4,j ? synset(w1,i)
0 otherwise
(3)
sim(w2,i, w3,j) =
?
?
?
?
?
?
?
?
?
1 if (w2,i == w3,j) ||
w2,i ? synset(w3,j) ||
w3,j ? synset(w2,i)
0 otherwise
(4)
Both equations consider the existence of semantic
similarity when the two words are identical, or when
the some of the two words appear in the synonym set
of the other word.
The semantic similarity of the complete text frag-
ments T1 and T4 (simS) is calculated as shown in
Eq. (5). Whereas, the semantic similarity of the
complete text fragments T2 and T3 (simT ) is cal-
culated as shown in Eq. (6).
simS(T1, T4) =
?|T1|
i=1
?|T4|
j=1 sim(w1,i,w4,j)
|T1?T4|
(5)
simT (T2, T3) =
?|T2|
i=1
?|T3|
j=1 sim(w2,i,w3,j)
|T2?T3|
(6)
2.3 Decision rules
Both approches used the same decision rules in or-
der to determine the entailment judgment for a given
pair of text fragments (T1 and T2). The following al-
gorithm shows the decision rules used.
Algorithm 1.
If |T2| < |T3| then
If (simT > 0.5 and simS > 0.5)
then forward
ElseIf |T2| > |T3| then
If (simT > 0.5 and simS > 0.5)
then backward
ElseIf (|T1| == |T4| and |T2| == |T3|) then
If (simT > 0.5 and simS > 0.5)
then bidirectional
Else no entailment
As mentioned above, the rules employed the le-
xical or semantic textual similarity, and the textual
length for determining the textual entailment.
3 Results
In Table 2 we show the overall results obtained by
the two approaches submitted to the competition.
We also show the highest, lowest, average and me-
dian overall results obtained in the competition.
SPA-
ENG
ITA-
ENG
FRA-
ENG
DEU-
ENG
Highest 0.632 0.566 0.57 0.558
Average 0.407 0.362 0.366 0.357
Median 0.346 0.336 0.336 0.336
Lowest 0.266 0.278 0.278 0.262
BUAP run1 0.35 0.336 0.334 0.33
BUAP run2 0.366 0.344 0.342 0.268
Table 2: Overall statistics obtained in the Task 8 of Se-
meval 2012
The runs submitted perform similar, but the se-
mantic approach obtained a slightly better perfor-
mance. The two results are above the median but
below the average. We consider that better results
may be obtained if the two features used (textual si-
milarity and textual length) were introduced into a
supervised classifier, so that, the decision rules were
approximated on the basis of a training dataset, ins-
tead of the empirical setting done in this work. Fu-
ture experiments will be carried out in this direction.
708
4 Discussion and conclusion
Two different approaches for the Cross-lingual Tex-
tual Entailment for Content Synchronization task of
Semeval 2012 are reported in this paper. We used
two features for determining the textual entailment
judgment between two texts T1 and T2 (written in
two different languages). The first approach pro-
posed used lexical similarity, meanwhile the second
used semantic similarity by means of term expan-
sion with synonyms.
Even if the performance of both approaches is
above the median and slighly below the average,
we consider that we may easily improve this perfor-
mance by using syntactic features of the text frag-
ments. Additionally, we are planning to integrate
some supervised techniques based on decision rules
which may be trained in a supervised dataset. Future
experiments will be executed in this direction.
Acknowledgments
This project has been partially supported by projects
CONACYT #106625, #VIAD-ING11-II and VIEP
#PIAD-ING11-II.
References
Ido Dagan and Oren Glickman. 2004. Probabilistic Tex-
tual Entailment: Generic Applied Modeling of Lan-
guage Variability. In Learning Methods for Text Un-
derstanding and Mining, January.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2010. Towards Cross-Lingual Textual Entailment. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 321?
324, Los Angeles, California, June. Association for
Computational Linguistics.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
2011. Using bilingual parallel corpora for cross-
lingual textual entailment. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Volume
1, HLT ?11, pages 1336?1345, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Matteo Negri, Luisa Bentivogli, Yashar Mehdad, Danilo
Giampiccolo, and Alessandro Marchetti. 2011. Di-
vide and conquer: crowdsourcing the creation of cross-
lingual textual entailment corpora. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, EMNLP ?11, pages 670?679,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
M. Negri, A. Marchetti, Y. Mehdad, L. Bentivogli, and
D. Giampiccolo. 2012. Semeval-2012 Task 8: Cross-
lingual Textual Entailment for Content Synchroniza-
tion. In Proceedings of the 6th International Workshop
on Semantic Evaluation (SemEval 2012).
709
