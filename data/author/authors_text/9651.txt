A Framework Based on Graphical Models with Logic for
Chinese Named Entity Recognition ?
Xiaofeng YU Wai LAM Shing-Kit CHAN
Information Systems Laboratory
Department of Systems Engineering & Engineering Management
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong
{xfyu,wlam,skchan}@se.cuhk.edu.hk
Abstract
Chinese named entity recognition (NER) has re-
cently been viewed as a classification or sequence
labeling problem, and many approaches have been
proposed. However, they tend to address this
problem without considering linguistic informa-
tion in Chinese NEs. We propose a new framework
based on probabilistic graphical models with first-
order logic for Chinese NER. First, we use Condi-
tional Random Fields (CRFs), a standard and the-
oretically well-founded machine learning method
based on undirected graphical models as a base
system. Second, we introduce various types of
domain knowledge into Markov Logic Networks
(MLNs), an effective combination of first-order
logic and probabilistic graphical models for vali-
dation and error correction of entities. Experimen-
tal results show that our framework of probabilis-
tic graphical models with first-order logic signifi-
cantly outperforms the state-of-the-art models for
solving this task.
1 Introduction
Named entity recognition (NER) is the task of identifying
and classifying phrases that denote certain types of named
entities (NEs), such as person names (PERs), locations
(LOCs) and organizations (ORGs) in text documents. It
is a well-established task in the NLP and data mining com-
munities and is regarded as crucial technology for many
higher-level applications, such as information extraction,
question answering, information retrieval and knowledge
management. The NER problem has generated much in-
terest and great progress has been made, as evidenced by
its inclusion as an understanding task to be evaluated in the
?The work described in this paper is substantially supported by
grants from the Research Grant Council of the Hong Kong Special
Administrative Region, China (Project Nos: CUHK 4179/03E
and CUHK4193/04E) and the Direct Grant of the Faculty of En-
gineering, CUHK (Project Codes: 2050363 and 2050391). This
work is also affiliated with the Microsoft-CUHK Joint Laboratory
for Human-centric Computing and Interface Technologies.
Message Understanding Conference (MUC), the Multilin-
gual Entity Task (MET) evaluations, and the Conference on
Computational Natural Language Learning (CoNLL).
Compared to European-language NER, Chinese NER
seems to be more difficult (Yu et al, 2006). Recent ap-
proaches to Chinese NER are a shift away from manu-
ally constructed rules or finite state patterns towards ma-
chine learning or statistical methods. However, rule-
based NER systems lack robustness and portability. Sta-
tistical methods often suffer from the problem of data
sparsity, and machine learning approaches (e.g., Hidden
Markov Models (HMMs) (Bikel et al, 1999; Zhou and
Su, 2002), Support Vector Machines (SVMs) (Isozaki and
Kazawa, 2002), Maximum Entropy (MaxEnt) (Borthwick,
1999; Chieu and Ng, 2003), Transformation-based Learn-
ing (TBL) (Brill, 1995) or variants of them) might be un-
satisfactory to learn linguistic information in Chinese NEs.
Current state-of-the-art models often view Chinese NER as
a classification or sequence labeling problem without con-
sidering the linguistic and structural information in Chinese
NEs. They assume that entities are independent, however
in most cases this assumption does not hold because of
the existing relationships among the entities. They seek
to locate and identify named entities in text by sequentially
classifying tokens (words or characters) as to whether or
not they participate in an NE, which is sometimes prone to
noise and errors.
In fact, Chinese NEs have distinct linguistic character-
istics in their composition and human beings usually use
prior knowledge to recognize NEs. For example, about 365
of the highest frequently used surnames cover 99% Chi-
nese surnames (Sun et al, 1995). Some LOCs contain
location salient words, while some ORGs contain organi-
zation salient words. For the LOC ??lA?/Hong Kong
Special Region?, ??l/Hong Kong? is the name part and
?A?/Special Region? is the salient word. For the ORG
??lA??/Hong Kong Special Region Government?,
??l/Hong Kong? is the LOC name part, ?A?/Special
Region? is the LOC salient word and ??/Government?
is the ORG salient word. Some ORGs contain one or
more PERs, LOCs and ORGs. A more complex exam-
335
ple is the nested ORG ????D??u??O?
??/School of Computer Science, Tsinghua Univer-
sity, Haidian District, Beijing City? which contains two
ORGs ??u??/Tsinghua University? and ?O???
/School of Computer Science? and two LOCs ??
?/Beijing City? and ??D?/Haidian District?. The two
ORGs contain ORG salient words ???/University? and
??/School?, while the two LOCs contain LOC salient
words ??/City? and ??/District? respectively.
Inspired by the above observation, we propose a new
framework based on probabilistic graphical models with
first-order logic which treats Chinese NER 1 as a statisti-
cal relational learning (SRL) problem and makes use of
domain knowledge. First, we employ Conditional Random
Fields (CRFs), a discriminatively trained undirected graph-
ical model which has theoretical justification and has been
shown to be an effective approach to segmenting and label-
ing sequence data, as our base system. We then exploit a
variety of domain knowledge into Markov Logic Networks
(MLNs), a powerful combination of logic and probability,
to validate and correct errors made in the base system. We
show how a variety of domain knowledge can be formu-
lated as first-order logic and incorporated into MLNs. We
use three Markov chain Monte Carlo (MCMC) algorithms,
including Gibbs sampling, Simulated Tempering, as well as
MC-SAT, andMaximum a posteriori/Most Probable Expla-
nation (MAP/MPE) algorithm for probabilistic inference
in MLNs. Experimental results show that our framework
based on graphical models with logic yields substantially
better NER results, leading to a relative error reduction of
up to 23.75% on the F-measure over state-of-the-art mod-
els. McNemar?s tests confirm that the improvements we
obtained are statistically highly significant.
2 State of the Art
2.1 CRF Model for Chinese NER
Conditional Random Fields (CRFs) (Lafferty et al, 2001)
are undirected graphical models trained to maximize the
conditional probability of the desired outputs given the cor-
responding inputs. CRFs have the great flexibility to en-
code a wide variety of arbitrary, non-independent features
and to straightforwardly combine rich domain knowledge.
Furthermore, they are discriminatively trained, and are of-
ten more accurate than generative models, even with the
same features. CRFs have been successfully applied to a
number of real-world tasks, including NP chunking (Sha
and Pereira, 2003), Chinese word segmentation (Peng et
al., 2004), information extraction (Pinto et al, 2003; Peng
and McCallum, 2004), named entity identification (Mc-
Callum and Li, 2003; Settles, 2004), and many others.
1In this paper we only focus on PERs, LOCs and ORGs. Since
temporal, numerical and monetary phrases can be well identified
with rule-based approaches.
Recently, CRFs have been shown to perform excep-
tionally well on Chinese NER shared task on the third
SIGHAN Chinese language processing bakeoff (SIGHAN-
06) (Zhou et al, 2006; Chen et al, 2006b,a). We follow
the state-of-the-art CRF models using features that have
been shown to be very effective in Chinese NER, namely
the current character and its part-of-speech (POS) tag, sev-
eral characters surrounding (both before and after) the cur-
rent character and their POS tags, current word and several
words surrounding the current word.
We also observe some important issues that significantly
influence the performance as follows:
Window size: The primitive window size we use is 5 ( 2
characters preceding the current character and 2 following
the current character). We extend the window size to 7 but
find that it slightly hurts. The reason is that CRFs can deal
with non-independent features. A larger window size may
introduce noisy and irrelevant features.
Feature representation: For character features, we use
character identities. For word features, BIES representa-
tion (each character is beginning of a word, inside of a
word, end of a word, or a single word) is employed.
Labeling scheme: The labeling scheme can be BIO, BIOE
or BIOES representation. In BIO representation, each char-
acter is tagged as either the beginning of a named entity
(B), a character inside a named entity (I), or a character
outside a named entity (O). In BIOE, the last character in
an entity is labeled as E while in BIOES, single-character
entities are labeled as S. In general, BIOES representation
is more informative and yields better results than both BIO
and BIOE.
2.2 Error Analysis
Even though the CRFmodel is able to accommodate a large
number of well-engineered features which can be easily ob-
tained across languages, some NEs, especially LOCs and
ORGs are difficult to identify due to the lack of linguistic
or structural characteristics. Since predictions are made to-
ken by token, some typical and serious tagging errors are
still made, as shown below:
? ORG is incorrectly tagged as LOC: In Chinese, many
ORGs contain location information. The CRF model only
tags the location information (in the ORGs) as LOCs.
For example, ?/?n??/Tangshan Technical Insti-
tute? and ??H???/Hainan Provincial Committee ? are
ORGs and they contain LOCs ?/?/Tangshan? and ??H
?/Hainan Province?, respectively. ?/?/Tangshan? and
??H?/Hainan Province? are only incorrectly tagged as
LOCs. This affects the tagging performance of both ORGs
and LOCs.
? LOC is incorrectly tagged as ORG: The LOCs ?GZy?
/Sydney Opera? and ??N?,/Beijing Gymnasium?
are mistakenly tagged as ORGs by the CRF model with-
out taking into account the location salient words ?y?
/Opera? and ?N?,/Gymnasium?.
336
? The boundary of entity is tagged incorrectly: This mis-
take occurs for all the entities. For example, the PER
?)0???d/Tom Cruise? may be tagged as a PER ?)
0/Tom?; the LOC ??5r/Bremen? may be tagged as
a LOC ?5r/Laimei?, which is a meaningless word; the
ORG ?u??i/Huawei Corporation? may be tagged as an
ORG ?u?/Huawei?. The reasons for these errors are both
complicated and varied. However, some of them are related
to linguistic knowledge.
? Common nouns are incorrectly tagged as entities: For ex-
ample, the two common nouns ?y???/Modern Mathe-
matics? and ??=????/Galanz Microwave Oven? may
be improperly tagged as a LOC and an ORG. Some tagging
errors could be easily rectified. Take the erroneous ORG
???|??/City Committee Organizes,? for example, in-
tuitively it is not an ORG since an entity cannot span any
punctuation.
3 Our Proposed Framework
3.1 Overview
We propose a framework based on probabilistic graphical
models with first-order logic for Chinese NER. As shown
in Figure 1, the framework is composed of three main com-
ponents. The CRF model is used as a base model. Then we
incorporate domain knowledge that can be well formulated
into first-order logic to extract entity candidates from CRF
results. Finally, the Markov Logic Network (MLN), an
undirected graphical model for statistical relational learn-
ing, is used to validate and correct the errors made in the
base model. We begin by briefly reviewing the necessary
background of MLNs, including weight learning and infer-
ence.
3.2 Markov Logic Networks
A Markov Network (also known as Markov Random Field)
is a model for the joint distribution of a set of variables
(Pearl, 1988). It is composed of an undirected graph G =
(V,E) and a set of real-valued potential functions ?k. A
First-Order Knowledge Base (KB) (Genesereth and Nisls-
son, 1987) is a set of sentences or formulas in first-order
logic.
A Markov Logic Network (MLN) (Richardson and
Domingos, 2006) is a KB with a weight attached to each
formula (or clause). Together with a set of constants
representing objects in the domain, it species a ground
Markov Network containing one feature for each possi-
ble grounding of a first-order formula Fi in the KB, with
the corresponding weight wi. The basic idea in MLNs
is that: when a world violates one formula in the KB
it is less probable, but not impossible. The fewer for-
mulas a world violates, the more probable it is. The
weights associated with the formulas in an MLN jointly
determine the probabilities of those formulas (and vice
versa) via a log-linear model. An MLN is a statisti-
cal relational model that defines a probability distribution
over Herbrand interpretations (possible worlds), and can
Figure 1: Framework Overview
be thought of as a template for constructing Markov Net-
works. Given different sets of constants, it will produce
different networks. These networks will have certain reg-
ularities in structure and parameter given by the MLN
and they are called ground Markov Networks. Suppose
Peter(A), Smith(B) and IBM(X) are 3 constants,
a KB and generated features are listed in Table 1. The
formula Employ(x,y)?Person(x),Company(y)
means x is employed by y and Colleague(x,y)?
Employ(x,z)?Employ(y,z) means x and y are col-
leagues if they are employed by the same company. Fig-
ure 2 shows the graph of the ground Markov network
defined by the formulas in Table 1 and the 3 constants
Peter(A), Smith(B) and IBM(X). The probability
distribution over possible worlds x specified by the ground
Markov Network ML,C is given by
P (X = x) =
1
Z
exp(
?
wini(x )) =
1
Z
?
?i
(
x{i}
)ni(x)
(1)
where ni (x) is the number of true groundings of Fi in
x, x{i} is the true value of the atoms appearing in Fi, and
?i
(
x{i}
)
= ewi .
In the case of Chinese NER, a named entity can be con-
nected to another named entity for instance, because they
share the same location salient word. Thus in an undirected
graph, two node types exist, the LOC nodes and the loca-
tion salient word nodes. The links (edges) indicate the rela-
tion (LOCs contain location salient words) between them.
This representation can be well expressed by MLNs.
However, one problem concerning relational data is, how
to extract useful relations for Chinese NER. There are many
kinds of relations between NEs, some relations are critical
to the NER problemwhile others not. Another problem that
we address is whether these relations can be formulated in
first-order logic and combined in MLNs. In Section 3.3,
we exploit domain knowledge. We will show how these
knowledge can capture essential characteristics of Chinese
NEs and can be well and concisely formulated in first-order
logic in Section 3.4.
337
Table 1: Example of a KB and Generated Features
Fist-Order Logic (KB) Generated Features
? x,y Employ(x,y)?Person(x),Company(y) Employ(Peter,IBM)?Person(Peter),Company(IBM)
Employ(Smith,IBM)?Person(Smith),Company(IBM)
? x,y,z Colleague(x,y)? Employ(x,z)?Employ(y,z) Colleague(Peter,Smith)? Employ(Peter,IBM)
?Employ(Smith,IBM)
3.2.1 Learning Weights
Given a relational database, MLN weights can in princi-
ple be learned generatively by maximizing the likelihood of
this database on the closed world assumption. The gradient
of the log-likelihood with respect to the weights is
?
?wi
logPw(X = x) = ni (x) ?
?
Pw(X = x
?)ni(x
?)
(2)
where the sum is over all possible databases x? , and
Pw(X = x?) is P (X = x?) computed using the cur-
rent weight vector w = (w1, ..., wi, ...). Unfortunately,
computing these expectations can be very expensive. In-
stead, we can maximize the pseudo-log-likelihood of the
data more efficiently. If x is a possible database and xl is
the lth ground atom?s truth value, the pseudo-log-likelihood
of x given weights w is
logP ?w(X = x) =
n?
l=1
logPw(Xl=xl | MBx(Xl )) (3)
where MBx (Xl) is the state of Xl?s Markov blanket 2
in the data. Computing Equation 3 and its gradient does
not require inference over the model, and is therefore much
faster. We can optimize the pseudo-log-likelihood using
the limited-memory BFGS algorithm (Liu and Nocedal,
1989).
3.2.2 Inference
If F1 and F2 are two formulas in first-order logic, C is
a finite set of constants including any constants that appear
in F1 or F2, and L is an MLN, then
P (F1 | F2, L, C) = P (F1 | F2,ML,C)
=
P (F1 ? F2 | ML,C)
P (F2 | ML,C)
=
?
x??F1??F2
P (X = x | ML,C)
?
x??F2
P (X = x | ML,C)
(4)
where ?Fi is the set of worlds where Fi holds, and P (x |
ML,C) is given by Equation 1. The question of whether a
knowledge base entails a formula F in first-order logic is
the question of whether P (F | LKB, CKB,F ) = 1, where
LKB is the MLN obtained by assigning infinite weight to
2 The Markov blanket of a node is the minimal set of nodes
that renders it independent of the remaining network; in a MLN,
this is simply the node?s neighbors in the graph.
Figure 2: A Ground Markov network defined by the formu-
las in Table 1 and the constants Peter(A), Smith(B)
and IBM(X).
all the formulas in KB, andCKB,F is the set of all constants
appearing in KB or F .
A large number of efficient inference techniques are ap-
plicable to MLNs. The most widely used approximate so-
lution to probabilistic inference in MLNs is Markov chain
Monte Carlo (MCMC) (Gilks et al, 1996). In this frame-
work, the Gibbs sampling algorithm is to generate an in-
stance from the distribution of each variable in turn, con-
ditional on the current values of the other variables. The
key to the Gibbs sampler is that one only considers uni-
variate conditional distributions-the distribution when all
of the random variables but one are assigned fixed values.
One way to speed up Gibbs sampling is by Simulated Tem-
pering (Marinari and Parisi, 1992), which performs simu-
lation in a generalized ensemble, and can rapidly achieve
an equilibrium state. Poon and Domingos (2006) pro-
posedMC-SAT, an inference algorithm that combines ideas
from MCMC and satisfiability. MC-SAT works well and is
guaranteed to be sound, even when deterministic or near-
deterministic dependencies are present in real-world rea-
soning.
Besides MCMC framework, maximum a posteriori
(MAP) inference can be carried out using a weighted sat-
isfiability solver like MaxWalkSAT. It is closely related to
maximum likelihood (ML), but employs an augmented op-
timization objective which incorporates a prior distribution
over the quantity one wants to estimate. MAP estimation
can therefore be seen as a regularization of ML estimation.
3.3 Domain Knowledge
We incorporate various kinds of domain knowledge via
MLNs to predict the newly extracted NE candidates from
338
CRF hypotheses. We extract 165 location salient words
and 843 organization salient words from Wikipedia3 and
the LDC Chinese-English bi-directional NE lists compiled
from Xinhua News database, as shown in Table 2. We also
make a punctuation list which contains 18 items and some
stopwords which Chinese NEs cannot contain. The stop-
words are mainly conjunctions, auxiliary and functional
words. We extract new NE candidates from the CRF re-
sults according to the following consideration:
? Definitely, if a chunk (a series of continuous characters) oc-
curs in the training data as a PER or a LOC or an ORG, then
this chunk should be a PER or a LOC or an ORG in the test-
ing data. In general, a unique string is defined as a PER, it
cannot be a LOC somewhere else.
? Obviously, if a tagged entity ends with a location salient
word, it is a LOC. If a tagged entity ends with an organi-
zation salient word, it is an ORG.
? If a tagged entity is close to a subsequent location salient
word, probably they should be combined together as a LOC.
The closer they are, the more likely that they should be com-
bined.
? If a series of consecutive tagged entities are close to a sub-
sequent organization salient word, they should probably be
combined together as an ORG because an ORG may contain
multiple PERs, LOCs and ORGs.
? Similarly, if there exists a series of consecutive tagged enti-
ties and the last one is tagged as an ORG, it is likely that all
of them should be combined as an ORG.
? Entity length restriction: all kinds of tagged entities cannot
exceed 25 Chinese characters.
? Stopword restriction: intuitively, all tagged entities cannot
comprise any stopword.
? Punctuation restriction: in general, all tagged entities cannot
span any punctuation.
? Since all NEs are proper nouns, the tagged entities should
end with noun words.
? The CRF model tags each token (Chinese character) with
a conditional probability. A low probability implies a
low-confidence prediction. For a chunk with low condi-
tional probabilities, all the above assumptions are adopted
(The marginal probabilities are normalized, and probabili-
ties lower than the user-defined threshold are regarded as
low conditional probabilities).
All the above domain knowledge can be formulated as
first-order logic to construct the structure of MLNs. And
all the extracted chunks are accepted as new NE candidates
(or common nouns). We train an MLN to recognize them.
3http://en.wikipedia.org/wiki/.
Table 2: Domain Knowledge for Chinese NER
Location Salient Word Organization Salient Word
g??/Municipality z??i/Department Store
???/Railway Station n??/Technical Institute
U,/Hotel ?1/Travel Agency
?	/Park ??/Press
p/Plateau <??/Personnel Department
?/Province ?1/Bank
	/Town ??/University
?/City ??/City Committee
Stopword Punctuation
E,/still "
?/but ?
?~/very ?
ff/of ;
/and so on ?
@/that ?
3.4 First-Order Logic Representation
We declared 14 predicates (person(candidate), lo
cation(candidate), organization(candidat
e), endwith(candidate, salientword), clos
eto(candidate, salientword), containstop
word(candidate), containpunctuation(cand
idate), etc) and specified 15 first-order formulas (See
Table 3 for some examples) according to the domain
knowledge described in Section 3.3. For example, we
used person(candidate) to specify whether a candi-
date is a PER. Formulas are recursively constructed from
atomic formulas using logical connectives and quantifiers.
They are constructed using four types of symbols: con-
stants, variables, functions, and predicates. Constant sym-
bols represent objects in the domain of interest (e.g., ?
?/Beijing? and ???/Shanghai? are LOCs). Variable
symbols (e.g., r and p) range over the objects in the do-
main. To reduce the size of ground Markov Network,
variables and constants are typed; for example, the vari-
able r may range over candidates, and the constant ?
?/Beijing? may represent a LOC. Function symbols repre-
sent mappings from tuples of objects to objects. Predicate
symbols represent relations among objects (e.g., person)
in the domain or attributes of objects (e.g., endwith). A
ground atom is an atomic formula all of whose arguments
are ground terms (terms containing no variables). For ex-
ample, the ground atom location(??) conveys
that ???/Beijing City? is a LOC.
For example in Table 3, ???/Wu City? is mis-tagged
as an ORG by the CRF model, but it contains the location
salient word ??/City?. So it is extracted as a new entity
candidate, and the corresponding formula endwith(r,
p)?locsalientword(p)?location(r) means if
r ends with a location salient word p, then it is a LOC.
Besides the formulas listed in Table 3, we also speci-
fied logic such as person(p)?!(location(p) v
organization(p)), which means a candidate p can
339
Table 3: Examples of NE Candidates and First-Order Formulas
Mis-tagged NEs New NE Candidates First-Order Logic
F.p[common noun] F.p occurperson(p)?person(p)
?m[PER] ?m occurlocation(p)?location(p)
??8?[common noun] ??8? occurorganization(p)?organization(p)
??[ORG] ?? endwith(r,p)?locsalientword(p)?location(r)
=?[LOC] =? endwith(r,p)?orgsalientword(p)?organization(r)
?[LOC]s	 ?s	 closeto(r,p)?locsalientword(p)?location(r)
a?[LOC]?? a??? closeto(r,p)?orgsalientword(p)?organization(r)
??ff?A[LOC] ??ff?A containstopword(p)?!(person(p) v location(p) v
organization(p))
?z?????%[ORG] ?z?????% containpunctuation(p)?!(person(p) v location(p)
v organization(p))
only belong to one class.
We assume that the relational database contains only bi-
nary relations. Each extracted NE candidate is represented
by one or more strings appearing as arguments of ground
atoms in the database. The goal of NE prediction is to de-
termine whether the candidates are entities and the types of
entities (query predicates), given the evidence predicates
and other relations that can be deterministically derived
from the database. As we will see, despite their simplic-
ity and consistency, these first-order formulas incorporate
the essential features for NE prediction.
4 Experiments
4.1 Dataset
We used People?s Daily corpus (January-Jun, 1998) in
our experiments, which contains approximately 357K sen-
tences, 156K PERs, 219K LOCs and 87K ORGs, respec-
tively. We did some modifications on the original data to
make it cleaner. We enriched some tags so that the abbre-
viation proper nouns are well labeled. We preprocessed
some nested names to make them in better form. We also
processed some person names. We enriched tags for differ-
ent kinds of person names (e.g., Chinese and transliterated
names) and separated consecutive person names.
4.2 The Baseline NER System
We use CRFs to build a character-based Chinese NER sys-
tem, with features described in Section 2.1. To avoid over-
fitting, we penalized the log-likelihood by the commonly
used zero-mean Gaussian prior over the parameters. In
addition, we exploit clue word features which can capture
non-local dependencies. This gives us a competitive base-
line CRF model using both local and non-local information
for Chinese NER.
For clue word features, we employ 412 career titles (e.g.,
o?/President,?/Professor,?	/Police), 59 family ti-
tles (e.g.,ww/Father,~~/Sister), 33 personal pronouns
(e.g., \?/Your, ??/We) and 109 direction words (e.g.,
?/North, H?/South) to represent non-local informa-
tion. Career titles, family titles and personal pronouns may
Figure 3: An Example of Non-local Dependency. The Ca-
reer Title ??? Indicates a PER ??^?
imply a nearby PER and direction words may indicate a
LOC or an ORG. Figure 3 illustrates an example of non-
local dependency.
We do not take the advantage of using the golden-
standard word segmentation and POS tagging provided in
the original corpus, since such information is hardly avail-
able in real text. Instead, we use an off-the-shelf Chi-
nese lexical analysis system, the open source ICTCLAS
(Zhang et al, 2003), to segment and POS tag the corpus.
This module employs a hierarchical Hidden Markov Model
(HHMM) and provides word segmentation, POS tagging
(labels Chinese words using a set of 39 tags) and unknown
word recognition. It performs reasonably well, with seg-
mentation precision recently evaluated at 97.58%. The re-
call of unknown words using role tagging is over 90%.
We use one-month corpus for training and 9-day corpus
for testing. Table 4 shows the experimental results.
4.3 NER System Based on Graphical Models with
Logic
To test the effectiveness of our proposed model, we extract
all the NEs (19,879 PERs, 25,661 LOCs and 11,590 ORGs)
from the training corpus. An MLN training database,
which consists of 14 predicates, 16,620 constants and
97,992 ground atoms was built.
The MLNs were trained using a Gaussian prior with
zero mean and unit variance on each weight to penalize
the pseudo-likelihood, and with the weights initialized at
the mode of the prior (zero). During MLN learning, each
formula is converted to Conjunctive Normal Form (CNF),
and a weight is learned for each of its clauses. The weight
340
Table 4: Chinese NER by CRF Model
Precision Recall F?=1
Character features
PER 92.88% 79.42% 85.62
LOC 90.95% 82.88% 86.73
ORG 88.16% 83.86% 85.96
Overall 90.92% 82.07% 86.27
Character+Word
PER 93.27% 82.99% 87.83
LOC 91.49% 85.16% 88.21
ORG 88.94% 84.79% 86.82
Overall 91.48% 84.46% 87.83
Character+Word+POS
PER 92.17% 90.64% 91.40
LOC 90.56% 89.74% 90.15
ORG 89.15% 85.19% 87.12
Overall 90.76% 89.13% 89.94
All features
PER 92.12% 90.57% 91.34
LOC 90.62% 89.74% 90.18
ORG 89.72% 85.44% 87.53
Overall 90.89% 89.16% 90.02
Table 5: Chinese NER by Graphical Models with Logic
Precision Recall F?=1 RER
CRF Baseline
PER 92.12% 90.57% 91.34
LOC 90.62% 89.74% 90.18
ORG 89.72% 85.44% 87.53
Overall 90.89% 89.16% 90.02
Graphical Models (GS Inference)
PER 93.52% 93.32% 93.42
LOC 93.19% 91.91% 92.55
ORG 90.16% 90.71% 90.43
Overall 92.70% 92.09% 92.39 23.75%
Graphical Models (ST Inference)
PER 93.52% 93.32% 93.42
LOC 93.19% 91.91% 92.55
ORG 90.16% 90.71% 90.43
Overall 92.70% 92.09% 92.39 23.75%
Graphical Models (MC-SAT Inference)
PER 93.52% 93.32% 93.42
LOC 93.19% 91.91% 92.55
ORG 90.16% 90.71% 90.43
Overall 92.70% 92.09% 92.39 23.75%
Graphical Models (MAP/MPE Inference)
PER 92.87% 93.15% 93.01
LOC 93.15% 91.61% 92.37
ORG 90.56% 89.10% 89.82
Overall 92.57% 91.58% 92.07 20.54%
of a clause is used as the mean of a Gaussian prior for the
learned weight. These weights reflect how often the clauses
are actually observed in the training data.
We extract 529 entity candidates to construct the MLN
testing database, which contains 2,543 entries and these en-
tries are used as evidence for inference. Inference is per-
formed by grounding the minimal subset of the network re-
quired for answering the query predicates. We employed 3
MCMC algorithms: Gibbs sampling (GS), Simulated Tem-
pering (ST) as well as MC-SAT, and the MAP/MPE algo-
rithm for inference and the comparative NER results are
shown. The probabilistic graphical models greatly outper-
form the CRF model stand-alone by a large margin. It can
be seen from Table 5, the probabilistic graphical models
integrating first-order logic improve the precision and re-
call for all kinds of entities, thus boosting the overall F-
measure. We achieve a 23.75% relative error reduction
(RER) on F-measure by using 3 MCMC algorithms and
a 20.54% RER by using MAP/MPE algorithm, over an al-
ready competitive CRF baseline. We obtained the same
results using GS, ST and MC-SAT algorithms. MCMC al-
gorithms yields slightly better results than the MAP/MPE
algorithm.
4.4 Significance Test
Ideally, comparisons among NER systems would control
for feature sets, data preparation, training and test proce-
dures, parameter tuning, and estimate the statistical sig-
nificance of performance differences. Unfortunately, re-
ported results sometimes leave out details needed for ac-
curate comparisons.
We give statistical significance estimates using McNe-
mar?s paired tests 4 (Gillick and Cox, 1989) on labeling
disagreements for CRF model and graphical probabilistic
models that we evaluated directly.
Table 6 summarizes the correctness of the labeling de-
cisions between the models with a 95% confidence inter-
val (CI). These tests suggest that the graphical probabilistic
models are significantly more accurate and confirm that the
gains we obtained are statistically highly significant.
Table 6: McNemar?s Tests on Labeling Disagreements
Null Hypothesis 95% CI p-value
Proposed Model (GS) vs. CRFs 5.71-9.52 < 1 ? 10?6
Proposed Model (ST) vs. CRFs 5.71-9.52 < 1 ? 10?6
Proposed Model (MC-SAT) vs. CRFs 5.71-9.52 < 1 ? 10?6
Proposed Model (MAP/MPE) vs. CRFs 4.50-7.37 < 1 ? 10?6
5 Related Work
As a well-established task, Chinese NER has been studied
extensively and a number of techniques for this task have
been reported in the literature. Most recently, the trend
in Chinese NER is to use improved machine learning ap-
proaches, or to integrate various kinds of useful evidences,
features, or resources.
Fu and Luke (2005) presented a lexicalized HMM-
based approach to unifying unknown word identification
4Most researchers refer to statistically significant as p < 0.05
and statistically highly significant as p < 0.001.
341
and NER as a single tagging task on a sequence of known
words. Although lexicalized HMMs was shown to be su-
perior to standard HMMs, this approach has some disad-
vantages: it is a purely statistical model and it suffers from
the problem of data sparseness. And the model fails to tag
some complicated NEs (e.g., nested ORGs) correctly due
to lack of domain adaptive techniques. The F-measures of
LOCs and ORGs are only 87.13 and 83.60, which show
that there is still a room for improving.
A method of incorporating heuristic human knowledge
into a statistical model was proposed in (Wu et al, 2005).
Here Chinese NER was regarded as a probabilistic tagging
problem and the heuristic human knowledge was used to
reduce the searching space. However, this method assumes
that POS tags are golden-standard in the training data and
heuristic human knowledge is often ad hoc. These draw-
backs make the method unstable and highly sensitive to
POS errors; and when golden-standard POS tags are not
available (this is often the case), it may degrade the perfor-
mance.
Cohen and Sarawagi (2004) proposed a semi-Markov
model which combines a Markovian, HMM-like extrac-
tion process and a dictionary component. This process is
based on sequentially classifying segments of several ad-
jacent words. However, this technique requires that entire
segments have the same class label, while our technique
does not. Moreover, compared to a large-scale dictionary,
our domain knowledge is much easier to obtain.
However, all the above models treat NER as classifi-
cation or sequence labeling problem. To the best of our
knowledge, MLNs have not been previously used for NER
problem. To our knowledge, we first view Chinese NER
as a statistical relational learning problem and exploit do-
main knowledge which can be concisely formulated in
MLNs, allowing the training and inference algorithms to
be directly applied to them.
6 Conclusion and Future Work
The contribution of this paper is three-fold. First, we for-
mulate Chinese NER as a statistical relational learning
problem and propose a new framework incorporating prob-
abilistic graphical models and first-order logic for Chinese
NER which achieves state-of-the-art performance. Second,
We incorporate domain knowledge to capture the essen-
tial features of the NER task via MLNs, a unified frame-
work for SRL which produces a set of weighted first-
order clauses to predict new NE candidates. To the best
of our knowledge, this is the first attempt at using MLNs
for the NER problem in the NLP community. Third,
our proposed framework can be extendable to language-
independent NER, due to the simplicity of the domain
knowledge we could access. Directions for future work
include learning the structure of MLNs automatically and
using MLNs for information extraction (e.g., entity relation
extraction).
References
Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel. An algorithm that learns what?s in
a name. Machine Learning, 34(1-3):211?231, February 1999.
Andrew Borthwick. A Maximum Entropy Approach to Named Entity Recognition. PhD thesis,
New York University, September 1999.
Eric Brill. Transformation-based error-driven learning and natural language processing: A case
study in part-of-speech tagging. Computational Linguistics, 21(4):543?565, 1995.
Aitao Chen, Fuchun Peng, Roy Shan, and Gordon Sun. Chinese named entity recognition with
conditional probabilistic models. In 5th SIGHAN Workshop on Chinese Language Processing,
Australia, July 2006.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara. Chinese named entity recognition with condi-
tional random fields. In 5th SIGHAN Workshop on Chinese Language Processing, Australia,
July 2006.
Hai Leong Chieu and Hwee Tou Ng. Named entity recognition with a maximum entropy approach.
In Proceedings of CoNLL-03, 2003.
William W. Cohen and Sunita Sarawagi. Exploiting dictionaries in named entity extraction: Com-
bining semi-Markov extraction processes and data integration methods. In Proceedings of
ACM-SIGKDD 2004, 2004.
Guohong Fu and Kang-Kwong Luke. Chinese named entity recognition using lexicalized HMMs.
ACM SIGKDD Explorations Newsletter, 7:19?25, June 2005.
Michael R. Genesereth and Nils J. Nislsson. Logical foundations of artificial intelligence. Morgan
Kaufmann Publishers Inc., San Mateo, CA, 1987.
W.R. Gilks, S. Richardson, and D.J. Spiegelhalter. Markov chain Monte Carlo in practice. Chap-
man and Hall, London, UK, 1996.
L. Gillick and Stephen Cox. Some statistical issues in the comparison of speech recognition algo-
rithms. In Proceedings of ICASSP-89, pages 532?535, 1989.
Hideki Isozaki and Hideto Kazawa. Efficient support vector classifiers for named entity recogni-
tion. In Proceedings of COLING-02, pages 1?7, Taipei, Taiwan, 2002.
John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Proceedings of ICML-01, pages 282?
289. Morgan Kaufmann, San Francisco, CA, 2001.
Dong C. Liu and Jorge Nocedal. On the limited memory BFGS method for large scale optimiza-
tion. Mathematical Programming, 45:503?528, 1989.
EnzoMarinari and Giorgio Parisi. Simulated Tempering: A newMonte Carlo scheme. Europhysics
Letters, 19:451?458, 1992.
AndrewMcCallum andWei Li. Early results for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons. In Proceedings of CoNLL-03, 2003.
Judea Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. Mor-
gan Kaufmann Publishers Inc., San Francisco, CA, 1988.
Fuchun Peng and Andrew McCallum. Accurate information extraction from research papers using
conditional random fields. In Proceedings of HLT-NAACL 2004, pages 329?336, 2004.
Fuchun Peng, Fangfang Feng, and Andrew McCallum. Chinese segmentation and new word de-
tection using conditional random fields. In Proceedings of COLING-04, pages 562?568, 2004.
David Pinto, AndrewMcCallum, XingWei, andW. Bruce Croft. Table extraction using conditional
random fields. In Proceedings of ACM SIGIR-03, 2003.
Hoifung Poon and Pedro Domingos. Sound and efficient inference with probabilistic and deter-
ministic dependencies. In Proceedings of AAAI-06, Boston, Massachusetts, July 2006. The
AAAI Press.
Matthew Richardson and Pedro Domingos. Markov logic networks. Machine Learning, 62(1-
2):107?136, 2006.
Burr Settles. Biomedical named entity recognition using conditional random fields and rich feature
sets. In Proceedings of the COLING 2004 International Joint Workshop on Natural Language
Processing in Biomedicine and its Applications, Geneva, Switzerland, 2004.
Fei Sha and Fernando Pereira. Shallow parsing with conditional random fields. In Proceedings of
HLT-NAACL 2003, pages 213?220, 2003.
Maosong Sun, Changning Huang, Haiyan Gao, and Jie Fang. Identifying Chinese names in unre-
stricted texts. Journal of Chinese Information Processing, 1995.
Youzheng Wu, Jun Zhao, Bo Xu, and Hao Yu. Chinese named entity recognition based on multiple
features. In Proceedings of HLT-EMNLP 2005, 2005.
Xiaofeng Yu, Marine Carpuat, and Dekai Wu. Boosting for Chinese named entity recognition. In
5th SIGHAN Workshop on Chinese Language Processing, Australia, July 2006.
Hua Ping Zhang, Qun Liu, Xue-Qi Cheng, Hao Zhang, and Hong Kui Yu. Chinese lexical analysis
using Hierarchical Hidden Markov Model. In 2nd SIGHAN Workshop on Chinese Language
Processing, volume 17, pages 63?70, 2003.
Guodong Zhou and Jian Su. Named entity recognition using an HMM-based chunk tagger. In
Proceedings of ACL-02, pages 473?480, Philadelphia, USA, 2002.
Junsheng Zhou, Liang He, Xinyu Dai, and Jiajun Chen. Chinese named entity recognition with
a multi-phase model. In 5th SIGHAN Workshop on Chinese Language Processing, Australia,
July 2006.
342
An Online Cascaded Approach to Biomedical Named Entity Recognition ?
Shing-Kit Chan, Wai Lam, Xiaofeng Yu
Department of Systems Engineering and Engineering Management
The Chinese University of Hong Kong
Shatin, Hong Kong
{skchan, wlam, xfyu}@se.cuhk.edu.hk
Abstract
We present an online cascaded approach to
biomedical named entity recognition. This
approach uses an online training method
to substantially reduce the training time re-
quired and a cascaded framework to relax
the memory requirement. We conduct de-
tailed experiments on the BioNLP dataset
from the JNLPBA shared task and com-
pare the results with other systems and pub-
lished works. Our experimental results show
that our approach achieves comparable per-
formance with great reductions in time and
space requirements.
1 Introduction
In the biomedical domain, the vast amount of data
and the great variety of induced features are two ma-
jor bottlenecks for further natural language process-
ing on the biomedical literature. In this paper, we
investigate the biomedical named entity recognition
(NER) problem. This problem is particularly impor-
tant because it is a necessary pre-processing step in
many applications.
This paper addresses two main issues that arise
from biomedical NER.
?The work described in this paper is substantially supported
by grants from the Research Grant Council of the Hong Kong
Special Administrative Region, China (Project Nos: CUHK
4179/03E and CUHK4193/04E) and the Direct Grant of the
Faculty of Engineering, CUHK (Project Codes: 2050363 and
2050391). This work is also affiliated with the Microsoft-
CUHK Joint Laboratory for Human-centric Computing and In-
terface Technologies.
Long Training Time: Traditional approaches
that depend on the maximum likelihood training
method are slow even with large-scale optimiza-
tion methods such as L-BFGS. This problem wors-
ens with the sheer volume and growth rate of the
biomedical literature. In this paper, we propose the
use of an online training method that greatly reduces
training time.
Large Memory Space: The total number of
features used to extract named entities from docu-
ments is very large. To extract biomedical named
entities, we often need to use extra features in addi-
tion to those used in general-purpose domains, such
as prefix, suffix, punctuation, and more orthographic
features. We need a correspondingly large mem-
ory space for processing, exacerbating the first issue.
We propose to alleviate this problem by employing
a cascaded approach that divides the NER task into
a segmentation task and a classification task.
The overall approach is the online cascaded ap-
proach, which is described in the remaining sections
of this paper: Section 2 describes the general model
that is used to address the above issues. We address
the issue of long training time in Section 3. The is-
sue of large memory space is addressed in Section 4.
Experimental results and analysis are presented in
Section 5. We discuss related work in Section 6 and
conclude with Section 7.
2 Model Descriptions
Our proposed model is similar to a conditional ran-
dom field in a sequence labeling task, but we avoid
directly dealing with the probability distribution. We
use a joint feature representation F(x,y) for each
595
input sequence x and an arbitrary output sequence
y, as follows.
F(x,y) =
|x|
?
i=1
f(x,y, i) (1)
where each f(x,y, i) is a local feature function at
position i. For example, in a segmentation task using
the IOB2 notation, the k-th local feature in f(x,y, i)
can be defined as
fk(x,y, i) =
?
?
?
1 if xi is the word ?boy?,
and yi is the label ?B?
0 otherwise
(2)
With parameter w, the best output sequence y? for
an input sequence x can be found by calculating the
best score:
y? = argmax
y?
w ? F(x,y?) (3)
3 Online Training
We propose to estimate the parameter w in an online
manner. In particular, we use the online passive-
aggressive algorithm (Crammer et al, 2006). Pa-
rameters are estimated by margin-based training,
which chooses the set of parameters that attempts
to make the ?margin? on each training instance
(xt,yt) greater than a predefined value ?,
w ? F(xt,yt) ? w ? F(xt,y?) ? ? ?y? 6= yt
(4)
A hinge loss function ?(w;xt) is defined as
?(w;xt) =
{
0 if ?t ? ?
? ? ?t otherwise
(5)
where ?t is the margin on input xt defined as
?t = w ? F(xt,yt) ? max
y? 6=yt
w ? F(xt,y?) (6)
In online training, the parameter w is updated itera-
tively. Formally speaking, in the t-th iteration with
the parameter wt and the training instance xt, we
try to solve the following optimization problem.
wt+1 = argmin
w
1
2?w ? wt?
2 + C?
(7)
such that ?(w; (xt,yt)) ? ?
where C > 0 is a user-defined aggressiveness pa-
rameter and ? ? 0 is a slack term for the training
data when it is not linearly-separable. C controls
the penalty of the slack term and the aggressiveness
of each update step. A larger C implies a more ag-
gressive update and hence a higher tendency to over-
fit. The solution to Problem (7) is
wt+1 = wt ? ?t[F(xt,yt) ? F(xt, y?t)]
(8)
where ?t = min
{
C, ?(wt; (xt,yt))?F(xt,yt) ? F(xt, y?t)?2
}
(9)
The passiveness of this algorithm comes from the
fact that the parameter wt is not updated when the
hinge loss for xt is zero. It can be proved that the rel-
ative loss bound on the training data (and which also
bounds the number of prediction mistakes on the
training data) cannot be much worse than the best
fixed parameter chosen in hindsight. See (Crammer
et al, 2006) for a detailed proof.
Following most of the work on margin-based
training, in this paper we choose ? to be a function
of the correct output sequence y and the predicted
output sequence y?.
?(y, y?) =
{
0 if y = y?
?|y|
i=1[[yi 6= y?i]] otherwise
(10)
where [[z]] is 1 if z is true, and 0 otherwise.
The major computation difficulty in this online
training comes from Equation (3). Finding the best
output y? is in general an intractable task. We fol-
low the usual first-order independence assumption
made in a linear-chained CRF (Lafferty et al, 2001)
model and calculate the best score using the Viterbi
algorithm.
4 Cascaded Framework
We divide the NER task into a segmentation task
and a classification task. In the segmentation task,
a sentence x is segmented, and possible segments
of biomedical named entities are identified. In the
classification task, the identified segments are clas-
sified into one of the possible named entity types or
rejected.
596
In other words, in the segmentation task, the sen-
tence x are segmented by
y?s = argmax
y?
ws ? Fs(x,y?) (11)
where Fs(?) is the set of segment features, and ws is
the parameter for segmentation.
In the classification task, the segments (which can
be identified by ys) in a sentence x are classified by
y?c = argmax
y?
wc ? Fc(x,ys,y?) (12)
where Fc(?) is the set of classification features, and
wc is the parameter for classification.
In this cascaded framework, the number of possi-
ble labels in the segmentation task is Ns. For exam-
ple, Ns = 3 in the IOB2 notation. In the classifi-
cation task, the number of possible labels is Nc + 1,
which is the number of entity types and one label for
?Other?. Following the first-order independence as-
sumption, the maximum total number of features in
the two tasks is O(max(N2s ,N2c )), which is much
smaller than the single-phase approach in which the
total number of features is O((NsNc)2).
Another potential advantage of dividing the NER
task into two tasks is that it allows greater flexibility
in choosing an appropriate set of features for each
task. In fact, adding more features may not nec-
essarily increase performance. (Settles, 2004) re-
ported that a system using a subset of features out-
performed one using a full set of features.
5 Experiments
We conducted our experiments on the GENIA cor-
pus (Kim et al, 2003) provided in the JNLPBA (Kim
et al, 2004) shared task1. There are 2,000 MED-
LINE abstracts in the GENIA corpus with named
entities tagged in the IOB2 format. There are 18,546
sentences and 492,551 words in the training set, and
3,856 sentences and 101,039 words in the evalua-
tion set. The line indicating the MEDLINE abstract
ID boundary information is not used in our experi-
ments. Each word is tagged with ?B-X?, ?I-X?, or
?O? to indicate that the word is at the ?beginning?
(B) or ?inside? (I) of a named entity of type X, or
1http://research.nii.ac.jp/?collier/
workshops/JNLPBA04st.htm
System F1
(Zhou and Su, 2004) 72.55
Online Cascaded 72.16
(Okanohara et al, 2006) 71.48
(Kim et al, 2005) 71.19
(Finkel et al, 2004) 70.06
(Settles, 2004) 69.80
Table 1: Comparisons with other systems on overall
performance (in percentage).
?outside? (O) of a named entity. The named entity
types are: DNA, RNA, cell line, cell type, and pro-
tein.
5.1 Features
The features used in our experiments mainly fol-
low the work of (Settles, 2004) and (Collins, 2001).
For completeness, we briefly describe the features
here. They include word features, orthographic fea-
tures, parts-of-speech (POS), and two lexicons. The
word features include unigram, bigram, and trigram
(e.g. the previous word, the next word, and the
previous two words), whereas the orthographic fea-
tures include capital letter, dash, punctuation, and
word length. Word class (WC) features are also
added, which replace a capital letter with ?A?, a
lower case letter with ?a?, a digit with ?0?, and all
other characters with ? ?. Similar brief word class
(BWC) features are added by collapsing all of the
consecutive identical characters in the word class
features into one character. For example, for the
word NF-kappa, WC = AA aaaaa, and BWC
= A a. These are listed in Tables 2 and 3. The POS
features are added by the GENIA tagger2.
All of these features except for the prefix/suffix
features are applied to the neighborhood window
[i ? 1, i + 1] for every word. Two lexicons for cell
lines and genes are drawn from two online public
databases: the Cell Line Database3 and the BBID4.
The prefix/suffix and lexicon features are applied to
position i only. All of the above features are com-
2http://www-tsujii.is.s.u-tokyo.ac.jp/
GENIA/tagger/
3http://www.biotech.ist.unige.it/cldb/
cname-tz.html
4http://bbid.grc.nia.nih.gov/bbidgene.
html
597
Unigram (w?2), (w?1), (w0),
(w1), (w2)
Bigram (w?2 w?1), (w?1 w0),
(w0 w1), (w1 w2)
Trigram (w?2 w?1 w0),
(w?1 w0 w1),
(w0 w1 w2)
Table 2: Word features used in the experiment: w0
is the current word, w?1 is the previous word, etc.
Word features as in Table 2
Prefix/suffix Up to a length of 5
Word Class WC
Brief Word Class BWC
Capital Letter ?[A-Z][a-z]
[A-Z]{2,}
[a-z]+[A-Z]+
Digit [0-9]+
?[?0-9]*[0-9][?0-9]*$
?[?0-9]*[0-9][0-9][?0-9]*$
?[0-9]+$
[0-9]+[,.][0-9,.]+
[A-Za-z]+[0-9]+
[0-9]+[A-Za-z]+
Dash [-]+
?[-]+
[-]+$
Punctuation [,;:?!-+?"\/]+
Word length length of the current word xi
Table 3: Features used in the JNLPBA experiment.
The features for Capital Letter, Digit, Dash, and
Punctuation are represented as regular expressions.
bined with the previous label yi?1 and the current
label yi to form the final set of features.
In the segmentation task, only three labels (i.e. B,
I, O) are needed to represent the segmentation re-
sults. In the classification task, the possible labels
are the five entity types and ?Other?. We also add
the segmentation results as features in the classifica-
tion task.
5.2 Results
We tried different methods to extract the named en-
tities from the JNLPBA dataset for comparisons.
These programs were developed based on the same
basic framework. All of the experiments were run
on a Unix machine with a 2.8 GHz CPU and 16 GB
RAM. In particular, the CRF trained by maximum-
likelihood uses the L-BFGS algorithm (Liu and No-
cedal, 1989), which converges quickly and gives
a good performance on maximum entropy mod-
els (Malouf, 2002; Sha and Pereira, 2003). We com-
pare our experimental results in several dimensions.
Training Time: Referring to Table 4, the train-
ing time of the online cascaded approach is substan-
tially shorter than that of all of the other approaches.
In the single-phase approach, training a CRF by
maximum likelihood (ML) using the L-BFGS algo-
rithm is the slowest and requires around 28 hours.
The online method greatly reduces the training time
to around two hours, which is 14 times faster. By
employing a two-phase approach, the training time
is further reduced to half an hour.
Memory Requirement: Table 4 shows the num-
ber of features that are required by the different
methods. For methods that use the single-phase ap-
proach, because the full set of features (See Sec-
tion 4) is too big for practical experiments on our
machine, we need to set a higher cutoff value to re-
duce the number of features. With a cutoff of 20
(i.e. only features that occur more than 20 times are
used), the number of features can still go up to about
8 million. However, in the two-phase approach, even
with a smaller cutoff of 5, the number of features can
still remain at about 8 million.
F1-measure: Table 4 shows the F1-measure in
our experiments, and Table 1 compares our results
with different systems in the JNLPBA shared tasks
and other published works5. Our performance of the
single-phase CRF with maximum likelihood train-
ing is 69.44%, which agrees with (Settles, 2004)
who also uses similar settings. The single-phase on-
line method increases the performance to 71.17%.
By employing a cascaded framework, the perfor-
mance is further increased to 72.16%, which can be
regarded as comparable with the best system in the
JNLPBA shared task.
6 Related Work
The online training approach used in this paper
is based on the concept of ?margin? (Cristianini,
2001). A pioneer work in online training is the
perceptron-like algorithm used in training a hidden
Markov model (HMM) (Collins, 2002). (McDonald
5We are aware of the high F1 in (Vishwanathan et al, 2006).
We contacted the author and found that their published result
may be incomplete.
598
Experiments no. of features training time F1 rel. err.
red. on F1
single-phase CRF + ML 8,004,392 1699 mins 69.44 ?
CRF + Online 8,004,392 116 mins 71.17 5.66%
two-phase Online seg: 2,356,590 14 + 15 72.16 8.90%
+ Cascaded class: 8,278,794 = 29 mins
Table 4: The number of features, training time, and F1 that are used in our experiments. The cutoff thresh-
olds for the single-phase CRFs are set to 20, whereas that of the online cascaded approach is set to 5 in both
segmentation and classification. The last column shows the relative error reductions on F1 (compared to
CRF+ML).
Experiments R P F1
Segmentation 80.13 73.68 76.77
Classification 92.75 92.76 92.76
Table 5: Results of the individual task in the online
cascaded approach. The F1 of the classification task
is 92.76% (which is based on the fully correct seg-
mented testing data).
et al, 2005) also proposed an online margin-based
training method for parsing. This type of training
method is fast and has the advantage that it does
not need to form the dual problem as in SVMs. A
detailed description of the online passive-aggressive
algorithm used in this paper and its variants can
be found in (Crammer et al, 2006). The Margin
Infused Relaxed Algorithm (MIRA), which is the
ancestor of the online passive-aggressive algorithm
and mainly for the linearly-separable case, can be
found in (Crammer and Singer, 2003).
(Kim et al, 2005) uses a similar two-phase
approach but they need to use rule-based post-
processing to correct the final results. Their CRFs
are trained on a different dataset that contains all of
the other named entities such as lipid, multi cell, and
other organic compound. Table 1 shows the com-
parisons of the final results.
In the JNLPBA shared task, eight NER systems
were used to extract five types of biomedical named
entities. The best system (Zhou and Su, 2004) uses
?deep knowledge?, such as name alias resolution,
cascaded entity name resolution, abbreviation res-
olution, and in-domain POS. Our approach is rela-
tively simpler and uses a unified model to accom-
plish the cascaded tasks. It also allows other post-
processing tasks to enhance performance.
7 Conclusion
We have presented an online cascaded approach to
biomedical named entity recognition. This approach
substantially reduces the training time required and
relaxes the memory requirement. The experimen-
tal results show that our approach achieves perfor-
mance comparable to the state-of-the-art system.
References
Michael Collins. 2001. Ranking algorithms for named-
entity extraction: boosting and the voted perceptron.
In ACL ?02: Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
489?496.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: theory and experiments
with perceptron algorithms. In EMNLP ?02: Proceed-
ings of the ACL-02 conference on Empirical methods
in natural language processing, pages 1?8.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551?585.
Nello Cristianini. 2001. Support vector and ker-
nel machines. ICML tutorial. Available at
http://www.support-vector.net/icml-tutorial.pdf.
J. Finkel, S. Dingare, H. Nguyen, M. Nissim, C. Man-
ning, and G. Sinclair. 2004. Exploiting context for
biomedical entity recognition: from syntax to the web.
In Proceedings of the International Joint Workshop on
599
Natural Language Processing in Biomedicine and its
Applications (NLPBA), pages 88?91.
J.d. Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. Ge-
nia corpus - a semantically annotated corpus for bio-
textmining. Bioinformatics (Supplement: Eleventh
International Conference on Intelligent Systems for
Molecular Biology), 19:180?182.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier.
2004. Introduction to the bio-entity recognition task at
JNLPBA. In N. Collier, P. Ruch, and A. Nazarenko,
editors, Proceedings of the International Joint Work-
shop on Natural Language Processing in Biomedicine
and its Applications (JNLPBA), Geneva, Switzerland,
pages 70?75, August 28?29. held in conjunction with
COLING?2004.
Seonho Kim, Juntae Yoon, Kyung-Mi Park, and Hae-
Chang Rim. 2005. Two-phase biomedical named en-
tity recognition using a hybrid method. In Proceedings
of The Second International Joint Conference on Nat-
ural Language Processing (IJCNLP-05), pages 646?
657.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc.
18th International Conf. on Machine Learning, pages
282?289.
D. C. Liu and J. Nocedal. 1989. On the limited mem-
ory bfgs method for large scale optimization. Math.
Program., 45(3):503?528.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of CoNLL-2002, pages 49?55.
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In ACL ?05: Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 91?98.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving the scal-
ability of semi-markov conditional random fields for
named entity recognition. In ACL ?06: Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the ACL,
pages 465?472.
B. Settles. 2004. Biomedical named entity recognition
using conditional random fields and rich feature sets.
In Proceedings of the International Joint Workshop on
Natural Language Processing in Biomedicine and its
Applications (NLPBA), pages 104?107.
Fei Sha and Fernando Pereira. 2003. Shallow parsing
with conditional random fields. In NAACL ?03: Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 134?
141.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark W.
Schmidt, and Kevin P. Murphy. 2006. Accelerated
training of conditional random fields with stochastic
gradient methods. In ICML ?06: Proceedings of the
23rd international conference on Machine learning,
pages 969?976.
GuoDong Zhou and Jian Su. 2004. Exploring deep
knowledge resources in biomedical name recognition.
In COLING 2004 International Joint workshop on
Natural Language Processing in Biomedicine and its
Applications (NLPBA/BioNLP) 2004, pages 99?102.
600
Chinese NER Using CRFs and Logic for the Fourth SIGHAN Bakeoff ?
Xiaofeng YU Wai LAM Shing-Kit CHAN Yiu Kei WU Bo CHEN
Information Systems Laboratory
Department of Systems Engineering & Engineering Management
The Chinese University of Hong Kong
Shatin, N.T., Hong Kong
{xfyu,wlam,skchan,ykwu,bchen}@se.cuhk.edu.hk
Abstract
We report a high-performance Chinese NER
system that incorporates Conditional Random
Fields (CRFs) and first-order logic for the fourth
SIGHAN Chinese language processing bake-
off (SIGHAN-6). Using current state-of-the-
art CRFs along with a set of well-engineered
features for Chinese NER as the base model,
we consider distinct linguistic characteristics in
Chinese named entities by introducing various
types of domain knowledge into Markov Logic
Networks (MLNs), an effective combination of
first-order logic and probabilistic graphical mod-
els for validation and error correction of enti-
ties. Our submitted results achieved consistently
high performance, including the first place on the
CityU open track and fourth place on the MSRA
open track respectively, which show both the at-
tractiveness and effectiveness of our proposed
model.
1 Introduction
We participated in the Chinese named entity recognition
(NER) task for the fourth SIGHAN Chinese language
processing bakeoff (SIGHAN-6). We submitted results
for the open track of the NER task. Our official re-
sults achieved consistently high performance, including
the first place on the CityU open track and fourth place on
the MSRA open track. This paper presents an overview
of our system due to space limit. A more detailed de-
scription of our model is presented in (Yu et al, 2008).
Our Chinese NER system combines the strength of two
graphical discriminative models, Conditional Random
?The work described in this paper is substantially supported
by grants from the Research Grant Council of the Hong Kong
Special Administrative Region, China (Project Nos: CUHK
4179/03E, CUHK4193/04E, and CUHK4128/07) and the Di-
rect Grant of the Faculty of Engineering, CUHK (Project Codes:
2050363 and 2050391). This work is also affiliated with the
Microsoft-CUHK Joint Laboratory for Human-centric Comput-
ing and Interface Technologies.
Fields (CRFs) and Markov Logic Networks (MLNs).
First, we employ CRFs, a discriminatively trained undi-
rected graphical model which has been shown to be an
effective approach to segmenting and labeling sequence
data, as our base system. Second, we model the linguis-
tic and structural information in Chinese named entity
composition. We exploit a variety of domain knowledge
which can capture essential characteristics of Chinese
named entities into Markov Logic Networks (MLNs), a
powerful combination of first-order logic and probability,
to (1) validate and correct errors made in the base sys-
tem and (2) find and extract new entity candidates. These
domain knowledge is easy to obtain and can be well and
concisely formulated in first-order logic and incorporated
into MLNs.
2 Conditional Random Fields as Base
Model
Conditional Random Fields (CRFs) (Lafferty et al, 2001)
are undirected graphical models trained to maximize the
conditional probability of the desired outputs given the
corresponding inputs. CRFs have been shown to perform
well on Chinese NER shared task on SIGHAN-4 (Zhou et
al. (2006), Chen et al (2006a), Chen et al (2006b)). We
employ CRFs as the base model in our framework. In this
base model, we design features similar to the state-of-the-
art CRF models for Chinese NER. We use character fea-
tures, word segmentation features, part-of-speech (POS)
features, and dictionary features, as described below.
Character features: These features are the current char-
acter, 2 characters preceding the current character and 2
following the current character. We extend the window
size to 7 but find that it slightly hurts. The reason is that
CRFs can deal with non-independent features. A larger
window size may introduce noisy and irrelevant features.
Word segmentation and POS features: We train our
own model for conducting Chinese word segmentation
and POS tagging. We employ a unified framework to
integrate cascaded Chinese word segmentation and POS
tagging tasks by joint decoding that guards against vi-
102
Sixth SIGHAN Workshop on Chinese Language Processing
olations of those hard-constraints imposed by segmenta-
tion task based on dual-layer CRFs introduced by Shi and
Wang (2007).
We separately train the Chinese word segmentation
and POS tagging CRF models using 8-month and 2-
month PKU 2000 corpus, respectively. The original PKU
2000 corpus contains more than 100 different POS tags.
To reduce the training time for POS tagging experiment,
we merge some similar tags and obtain only 42 tags fi-
nally. For example, {ia, ib, id, in, iv}?i. We use
the same features as described in (Shi and Wang, 2007),
except that we do not use the HowNet features for word
segmentation. Instead, we use max-matching segmenta-
tion features based on a word dictionary. This dictionary
contains 445456 words which are extracted from People?s
Daily corpus (January-June, 1998), CityU, MSRA, and
PKU word segmentation training corpora in SIGHAN-6.
For decoding, we first perform individual decoding for
each task. We then set 10-best segmentation and POS
tagging results for reranking and joint decoding in order
to find the most probable joint decodings for both tasks.
Dictionary features: We obtain a named entity dictio-
nary extracted from People?s Daily 1998 corpus and PKU
2000 corpus, which contains 68305 PERs, 28408 LOCs
and 55596 ORGs. We use the max-matching algorithm
to search whether a string exists in this dictionary.
In summary, we list the features used for our CRF base
model in Table 1. Besides the unigram feature template,
CRFs also allow bigram feature template. With this tem-
plate, a combination of the current output token and pre-
vious output token (bigram) is automatically generated.
We use CRF++ toolkit (version 0.48) (Kudo, 2005) in
our experiments. We find that setting the cut-off threshold
f for the features not only decreases the training time, but
improves the NER performance. CRFs can use the fea-
tures that occurs no less than f times in the given training
data. We set f = 5 in our system.
We extend the BIO representation for the chunk tag
which was employed in the CoNLL-2002 and CoNLL-
2003 evaluations. We use the BIOES representation in
which each character is tagged as either the beginning of
a named entity (B tag), a character inside a named en-
tity (I tag), the last character in an entity (E tag), single-
character entities (S tag), or a character outside a named
entity (O tag). We find that BIOES representation is
more informative and yields better results than BIO rep-
resentation.
3 Markov Logic Networks as Error
Correction Model
Even though the CRF model is able to accommodate a
large number of well-engineered features which can be
easily obtained across languages, some NEs, especially
Table 1: Feature template for CRF model.
Character features (1.1) Cn, n ? [?2, 2]
(1.2) CnCn+1, n ? [?2, 1]
Word features (1.3) Wn, n ? [?3, 3]
(1.4) WnWn+1, n ? [?3, 2]
POS features (1.5) Pn, n ? [?3, 3]
(1.6) PnPn+1, n ? [?3, 2]
Dictionary features (1.7) Dn, n ? [?2, 2]
(1.8) DnDn+1, n ? [?2, 1]
(1.9) D?1D+1
LOCs and ORGs are difficult to identify due to the lack
of linguistic or structural characteristics.
We incorporate domain knowledge that can be well
formulated into first-order logic to extract entity candi-
dates from CRF results. Then, the Markov Logic Net-
works (MLNs), an undirected graphical model for statis-
tical relational learning, is used to validate and correct
the errors made in the base model.
MLNs conduct relational learning by incorporating
first-order logic into probabilistic graphical models under
a single coherent framework (Richardson and Domingos,
2006). Traditional first-order logic is a set of hard con-
straints in which a world violates even one formula has
zero probability. The advantage of MLNs is to soften
these constraints so that when the fewer formulae a world
violates, the more probable it is. MLNs have been applied
to tackle the problems of gene interaction discovery from
biomedical texts and citation entity resolution from cita-
tion texts with state-of-the-art performance (Riedel and
Klein (2005), Singla and Domingos (2006)).
We use the Alchemy system (Beta version) (Kok et al,
2005) in our experiment, which is a software package
providing a series of algorithms for statistical relational
learning and probabilistic logic inference, based on the
Markov logic representation.
3.1 Domain Knowledge
We extract 165 location salient words and 843 organiza-
tion salient words from Wikipedia and the LDC Chinese-
English bi-directional NE lists compiled from Xinhua
News database. We also make a punctuation list which
contains 18 items and some stopwords which Chinese
NEs cannot contain. We extract new NE candidates from
the CRF results according to the following consideration:
? If a chunk (a series of continuous characters) occurs in the
training data as a PER or a LOC or an ORG, then this
chunk should be a PER or a LOC or an ORG in the testing
data. In general, a unique string is defined as a PER, it
cannot be a LOC somewhere else.
? If a tagged entity ends with a location salient word, it is a
LOC. If a tagged entity ends with an organization salient
word, it is an ORG.
103
Sixth SIGHAN Workshop on Chinese Language Processing
Table 2: Statistics of NER training and testing corpora.
Corpus Training NEs PERs/LOCs/ORGs Testing NEs PERs/LOCs/ORGs
CityU 66255 16552/36213/13490 13014 4940/4847/3227
MSRA 37811 9028/18522/10261 7707 1864/3658/2185
NEs: Number of named entities; PERs: Number of person names;
LOCs: Number of location names; ORGs: Number of organization names.
Table 3: OOV Rate of NER testing corpora.
Corpus Overall (IVs/OOVs/OOV-Rate) PER (IVs/OOVs/OOV-Rate) LOC (IVs/OOVs/OOV-Rate) ORG (IVs/OOVs/OOV-Rate)
CityU 6660/6354/0.4882 1062/3878/0.7850 3947/900/0.1857 1651/1576/0.4884
MSRA 6056/1651/0.2142 1300/564/0.3026 3343/315/0.0861 1413/772/0.3533
IVs: number of IV (named entities in vocabulary); OOVs: number of OOV
(named entities out of vocabulary); OOV-Rate: ratio of named entities out of vocabulary.
? If a tagged entity is close to a subsequent location salient
word, probably they should be combined together as a
LOC. The closer they are, the more likely that they should
be combined.
? If a series of consecutive tagged entities are close to a sub-
sequent organization salient word, they should probably
be combined together as an ORG because an ORG may
contain multiple PERs, LOCs and ORGs.
? Similarly, if there exists a series of consecutive tagged en-
tities and the last one is tagged as an ORG, it is likely that
all of them should be combined as an ORG.
? Entity length restriction: all kinds of tagged entities can-
not exceed 25 Chinese characters.
? Stopword restriction: intuitively, all tagged entities cannot
comprise any stopword.
? Punctuation restriction: in general, all tagged entities can-
not span any punctuation.
? Since all NEs are proper nouns, the tagged entities should
end with noun words.
? For a chunk with low conditional probabilities, all the
above assumptions are adopted.
3.2 First-Order Logic Construction
All the above domain knowledge can also be formulated
as first-order logic to construct the structure of MLNs.
First-order formulae are recursively constructed from
atomic formulae using logical connectives and quanti-
fiers. Atomic formulae are constructed using constants,
variables, functions, and predicates.
For example, we use the predicate organization(
candidate) to specify whether a candidate is an ORG.
If ??I?/China Government? is mis-tagged as a
LOC by the CRF model, but it contains the organization
salient word ??/Government?. The corresponding
formula endwith(r, p)?orgsalientword(p)
?organization(r) means if a tagged entity r ends
with an organization salient word p, then it is extracted as
a new ORG entity. Typically only a small number (e.g.,
10-20) of formulae are needed. We declare 14 predi-
cates and 15 first-order formulae according to the domain
knowledge mentioned in Section 3.1.
3.3 Training and Inference for Named Entity
Correction
Each extracted new NE candidate is represented by one
or more strings appearing as arguments of ground atoms
in the database. The goal of NE prediction is to deter-
mine whether the candidates are entities and the types of
entities (query predicates), given the evidence predicates
and other relations that can be deterministically derived
from the database.
We extract all the NEs from the official training cor-
pora, and then convert them to the first-order logic repre-
sentation according to the domain knowledge. The MLN
training database that consists of predicates, constants,
and ground atoms was built automatically. We also ex-
tract new entity candidates from CRF results and con-
struct MLN testing database in the same way.
During MLN learning, each formula is converted to
Conjunctive Normal Form (CNF), and a weight is learned
for each of its clauses. These weights reflect how often
the clauses are actually observed in the training data. In-
ference is performed by grounding the minimal subset
of the network required for answering the query pred-
icates. Conducting maximum a posteriori (MAP) in-
ference which finds the most likely values of a set of
variables given the values of observed variables can be
performed via approximate solution using Markov chain
Monte Carlo (MCMC) algorithms. Gibbs sampling can
be adopted by sampling each non-evidence variable in
turn given its Markov blanket, and counting the fraction
of samples that each variable is in each state.
4 Experiment Details
4.1 Data and Preprocessing
The training corpora provided by the SIGHAN bakeoff
organizers were in the CoNLL two column format, with
one Chinese character per line and hand-annotated named
entity chunks in the second column. The CityU corpus
was traditional Chinese. We converted this corpus to sim-
plified Chinese and we used UTF-8 encoding in all the
experiments so that all the resources (e.g., word dictio-
nary and named entity dictionary) are compatible in our
104
Sixth SIGHAN Workshop on Chinese Language Processing
Table 4: Official results on CityU andMSRA open tracks.
Precision Recall F?=1
CityU
PER 97.21% 95.26% 96.23
LOC 92.35% 93.42% 92.88
ORG 88.05% 66.44% 75.73
Overall 93.42% 87.43% 90.33
MSRA
PER 98.33% 94.58% 96.42
LOC 93.97% 93.36% 93.66
ORG 92.80% 84.39% 88.40
Overall 94.71% 91.11% 92.88
system.
Table 2 shows the statistics of NER training and testing
corpora and Table 3 shows the OOV (Out of Vocabulary)
rate of NER testing corpora 1. The number of NEs in
CityU corpus is almost twice as many as that in MSRA
corpus. The OOV rate in CityU corpus is much higher
than in MSRA corpus for PERs, LOCs and ORGs. These
numbers indicate that NER on CityU corpus is much
more difficult to handle.
4.2 Model Development
We performed holdout methodology to develop our
model. We randomly selected 5000 sentences fromCityU
training corpus for development testing and the rest for
training. We did the same thing for MSRA training cor-
pus.
To avoid overfitting for CRF model, we penalized
the log-likelihood by the commonly used zero-mean
Gaussian prior over the parameters. Also, the MLNs
were trained using a Gaussian prior with zero mean and
unit variance on each weight to penalize the pseudo-
likelihood, and with the weights initialized at the mode
of the prior (zero).
We found an optimal value for the parameter c 2 for
CRFs. Using held-out data, we tested all c values, c ?
[0.2, 2.2], with an incremental step of 0.4. Finally, we set
c = 1.8 for CityU corpus and c = 1.0 for MSRA corpus.
5 Official Results
Table 4 presents the results obtained on the official CityU
and MSRA test sets. Our results are consistently good:
we obtained the first place on the CityU open track (90.33
overall F-measure) and fourth place on the MSRA open
track (92.88 overall F-measure) respectively. The lower
1The NER on the PKU corpus was cancelled by the orga-
nizer due to the tagging inconsistency of this corpus.
2This parameter trades the balance between overfitting and
underfitting. With larger c value, CRF tends to overfit to the give
training corpus. The results will significantly be influenced by
this parameter
F-measure obtained on CityU corpus can be attributed to
the higher OOV rate of this corpus.
6 Conclusion
We have described a Chinese NER system incorporating
probabilistic graphical models and first-order logic which
achieves state-of-the-art performance on the open track of
SIGHAN-6. We exploited domain knowledge which can
capture the essential features of Chinese NER and can
be concisely formulated in MLNs, allowing the training
and inference algorithms to be directly applied to them.
Our proposed framework can also be extendable to NER
for other languages, due to the simplicity of the domain
knowledge we could access.
References
Aitao Chen, Fuchun Peng, Roy Shan, and Gordon Sun. Chi-
nese named entity recognition with conditional probabilistic
models. In 5th SIGHAN Workshop on Chinese Language
Processing, Australia, July 2006.
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara. Chinese
named entity recognition with conditional random fields. In
5th SIGHAN Workshop on Chinese Language Processing,
Australia, July 2006.
Stanley Kok, Parag Singla, Matthew Richardson, and Pedro
Domingos. The Alchemy system for statistical relational
AI. Technical report, Department of Computer Science and
Engineering, University of Washington, Seattle, WA, 2005.
http://www.cs.washington.edu/ai/alchemy.
Taku Kudo. CRF++: Yet another CRF tool kit.
http://crfpp.sourceforge.net/, 2005.
John Lafferty, Andrew McCallum, and Fernando Pereira. Con-
ditional random fields: Probabilistic models for segment-
ing and labeling sequence data. In Proceedings of ICML-
01, pages 282?289. Morgan Kaufmann, San Francisco, CA,
2001.
Matthew Richardson and Pedro Domingos. Markov logic net-
works. Machine Learning, 62(1-2):107?136, 2006.
Sebastian Riedel and Ewan Klein. Genic interaction extraction
with semantic and syntactic chains. In Proceedings of the
Learning Language in Logic Workshop (LLL-05), pages 69?
74, 2005.
Yanxin Shi and Mengqiu Wang. A dual-layer CRFs based
joint decoding method for cascaded segmentation and label-
ing tasks. In Proceedings of IJCAI-07, pages 1707?1712,
Hyderabad, India, 2007.
Parag Singla and Pedro Domingos. Entity resolution with
Markov logic. In Proceedings of ICDM-06, pages 572?582,
Hong Kong, 2006.
Xiaofeng Yu, Wai Lam, and Shing-Kit Chan. A framework
based on graphical models with logic for Chinese named en-
tity recognition. In Proceedings of IJCNLP-08, Hyderabad,
India, 2008. To appear.
Junsheng Zhou, Liang He, Xinyu Dai, and Jiajun Chen. Chinese
named entity recognition with a multi-phase model. In 5th
SIGHAN Workshop on Chinese Language Processing, Aus-
tralia, July 2006.
105
Sixth SIGHAN Workshop on Chinese Language Processing
