Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1210?1219,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Probabilistic Soft Logic for Semantic Textual Similarity
Islam Beltagy
?
Katrin Erk
?
Raymond Mooney
?
?
Department of Computer Science
?
Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
?
{beltagy,mooney}@cs.utexas.edu
?
katrin.erk@mail.utexas.edu
Abstract
Probabilistic Soft Logic (PSL) is a re-
cently developed framework for proba-
bilistic logic. We use PSL to combine
logical and distributional representations
of natural-language meaning, where distri-
butional information is represented in the
form of weighted inference rules. We ap-
ply this framework to the task of Seman-
tic Textual Similarity (STS) (i.e. judg-
ing the semantic similarity of natural-
language sentences), and show that PSL
gives improved results compared to a pre-
vious approach based on Markov Logic
Networks (MLNs) and a purely distribu-
tional approach.
1 Introduction
When will people say that two sentences are sim-
ilar? This question is at the heart of the Semantic
Textual Similarity task (STS)(Agirre et al, 2012).
Certainly, if two sentences contain many of the
same words, or many similar words, that is a good
indication of sentence similarity. But that can be
misleading. A better characterization would be to
say that if two sentences use the same or similar
words in the same or similar relations, then those
two sentences will be judged similar.
1
Interest-
ingly, this characterization echoes the principle of
compositionality, which states that the meaning of
a phrase is uniquely determined by the meaning of
its parts and the rules that connect those parts.
Beltagy et al (2013) proposed a hybrid ap-
proach to sentence similarity: They use a very
1
Mitchell and Lapata (2008) give an amusing example of
two sentences that consist of all the same words, but are very
different in their meaning: (a) It was not the sales manager
who hit the bottle that day, but the office worker with the
serious drinking problem. (b) That day the office manager,
who was drinking, hit the problem sales worker with a bottle,
but it was not serious.
deep representation of sentence meaning, ex-
pressed in first-order logic, to capture sentence
structure, but combine it with distributional sim-
ilarity ratings at the word and phrase level. Sen-
tence similarity is then modelled as mutual entail-
ment in a probabilistic logic. This approach is in-
teresting in that it uses a very deep and precise
representation of meaning, which can then be re-
laxed in a controlled fashion using distributional
similarity. But the approach faces large hurdles
in practice, stemming from efficiency issues with
the Markov Logic Networks (MLN) (Richardson
and Domingos, 2006) that they use for performing
probabilistic logical inference.
In this paper, we use the same combined logic-
based and distributional framework as Beltagy et
al., (2013) but replace Markov Logic Networks
with Probabilistic Soft Logic (PSL) (Kimmig et
al., 2012; Bach et al, 2013). PSL is a proba-
bilistic logic framework designed to have efficient
inference. Inference in MLNs is theoretically in-
tractable in the general case, and existing approxi-
mate inference algorithms are computationally ex-
pensive and sometimes inaccurate. Consequently,
the MLN approach of Beltagy et al (2013) was
unable to scale to long sentences and was only
tested on the relatively short sentences in the Mi-
crosoft video description corpus used for STS
(Agirre et al, 2012). On the other hand, inference
in PSL reduces to a linear programming problem,
which is theoretically and practically much more
efficient. Empirical results on a range of prob-
lems have confirmed that inference in PSL is much
more efficient than in MLNs, and frequently more
accurate (Kimmig et al, 2012; Bach et al, 2013).
We show how to use PSL for STS, and describe
changes to the PSL framework that make it more
effective for STS. For evaluation, we test on three
STS datasets, and compare our PSL system with
the MLN approach of Beltagy et al, (2013) and
with distributional-only baselines. Experimental
1210
results demonstrate that, overall, PSL models hu-
man similarity judgements more accurately than
these alternative approaches, and is significantly
faster than MLNs.
The rest of the paper is organized as follows:
section 2 presents relevant background material,
section 3 explains how we adapted PSL for the
STS task, section 4 presents the evaluation, and
sections 5 and 6 discuss future work and conclu-
sions, respectively.
2 Background
2.1 Logical Semantics
Logic-based representations of meaning have a
long tradition (Montague, 1970; Kamp and Reyle,
1993). They handle many complex semantic phe-
nomena such as relational propositions, logical
operators, and quantifiers; however, their binary
nature prevents them from capturing the ?graded?
aspects of meaning in language. Also, it is difficult
to construct formal ontologies of properties and re-
lations that have broad coverage, and semantically
parsing sentences into logical expressions utilizing
such an ontology is very difficult. Consequently,
current semantic parsers are mostly restricted to
quite limited domains, such as querying a specific
database (Kwiatkowski et al, 2013; Berant et al,
2013). In contrast, our system is not limited to any
formal ontology and can use a wide-coverage tool
for semantic analysis, as discussed below.
2.2 Distributional Semantics
Distributional models (Turney and Pantel, 2010),
on the other hand, use statistics on contextual
data from large corpora to predict semantic sim-
ilarity of words and phrases (Landauer and Du-
mais, 1997; Mitchell and Lapata, 2010). They are
relatively easier to build than logical representa-
tions, automatically acquire knowledge from ?big
data,? and capture the ?graded? nature of linguis-
tic meaning, but do not adequately capture logical
structure (Grefenstette, 2013).
Distributional models are motivated by the ob-
servation that semantically similar words occur in
similar contexts, so words can be represented as
vectors in high dimensional spaces generated from
the contexts in which they occur (Landauer and
Dumais, 1997; Lund and Burgess, 1996). Such
models have also been extended to compute vec-
tor representations for larger phrases, e.g. by
adding the vectors for the individual words (Lan-
dauer and Dumais, 1997) or by a component-wise
product of word vectors (Mitchell and Lapata,
2008; Mitchell and Lapata, 2010), or more com-
plex methods that compute phrase vectors from
word vectors and tensors (Baroni and Zamparelli,
2010; Grefenstette and Sadrzadeh, 2011). We use
vector addition (Landauer and Dumais, 1997), and
component-wise product (Mitchell and Lapata,
2008) as baselines for STS. Vector addition was
previously found to be the best performing sim-
ple distributional method for STS (Beltagy et al,
2013).
2.3 Markov Logic Networks
Markov Logic Networks (MLN) (Richardson and
Domingos, 2006) are a framework for probabilis-
tic logic that employ weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks). Weighting the rules is a way of soft-
ening them compared to hard logical constraints
and thereby allowing situations in which not all
clauses are satisfied. MLNs define a probability
distribution over possible worlds, where a world?s
probability increases exponentially with the to-
tal weight of the logical clauses that it satisfies.
A variety of inference methods for MLNs have
been developed, however, developing a scalable,
general-purpose, accurate inference method for
complex MLNs is an open problem. Beltagy et
al. (2013) use MLNs to represent the meaning of
natural language sentences and judge textual en-
tailment and semantic similarity, but they were un-
able to scale the approach beyond short sentences
due to the complexity of MLN inference.
2.4 Probabilistic Soft Logic
Probabilistic Soft Logic (PSL) is a recently pro-
posed alternative framework for probabilistic logic
(Kimmig et al, 2012; Bach et al, 2013). It uses
logical representations to compactly define large
graphical models with continuous variables, and
includes methods for performing efficient proba-
bilistic inference for the resulting models. A key
distinguishing feature of PSL is that ground atoms
have soft, continuous truth values in the interval
[0, 1] rather than binary truth values as used in
MLNs and most other probabilistic logics. Given
a set of weighted logical formulas, PSL builds a
graphical model defining a probability distribution
over the continuous space of values of the random
variables in the model.
1211
A PSL model is defined using a set of weighted
if-then rules in first-order logic, as in the following
example:
?x, y, z. friend(x, y) ? votesFor(y, z)?
votesFor(x, z) | 0.3 (1)
?x, y, z. spouse(x, y) ? votesFor(y, z)?
votesFor(x, z) | 0.8 (2)
In our notation, we use lower case letters like
x, y, z to represent variables and upper case let-
ters for constants. The first rule states that a per-
son is likely to vote for the same person as his/her
friend. The second rule encodes the same regular-
ity for a person?s spouse. The weights encode the
knowledge that a spouse?s influence is greater than
a friend?s in this regard.
In addition, PSL includes similarity functions.
Similarity functions take two strings or two sets as
input and return a truth value in the interval [0, 1]
denoting the similarity of the inputs. For example,
in our application, we generate inference rules that
incorporate the similarity of two predicates. This
can be represented in PSL as:
?x. similarity(?predicate1?, ?predicate2?) ?
predicate1(x)? predicate2(x)
As mentioned above, each ground atom, a,
has a soft truth value in the interval [0, 1],
which is denoted by I(a). To compute soft truth
values for logical formulas, Lukasiewicz?s re-
laxation of conjunctions(?), disjunctions(?) and
negations(?) are used:
I(l
1
? l
1
) = max{0, I(l
1
) + I(l
2
)? 1}
I(l
1
? l
1
) = min{I(l
1
) + I(l
2
), 1}
I(?l
1
) = 1? I(l
1
)
Then, a given rule r ? r
body
? r
head
, is said to be
satisfied (i.e. I(r) = 1) iff I(r
body
) ? I(r
head
).
Otherwise, PSL defines a distance to satisfaction
d(r) which captures how far a rule r is from being
satisfied: d(r) = max{0, I(r
body
) ? I(r
head
)}.
For example, assume we have the set of evidence:
I(spouse(B,A)) = 1, I(votesFor(A,P )) =
0.9, I(votesFor(B,P )) = 0.3, and that r
is the resulting ground instance of rule (2).
Then I(spouse(B,A) ? votesFor(A,P )) =
max{0, 1 + 0.9 ? 1} = 0.9, and d(r) =
max{0, 0.9? 0.3} = 0.6.
Using distance to satisfaction, PSL defines a
probability distribution over all possible interpre-
tations I of all ground atoms. The pdf is defined
as follows:
p(I) =
1
Z
exp [?
?
r?R
?
r
(d(r))
p
]; (3)
Z =
?
I
exp [?
?
r?R
?
r
(d(r))
p
]
where Z is the normalization constant, ?
r
is the
weight of rule r, R is the set of all rules, and p ?
{1, 2} provides two different loss functions. For
our application, we always use p = 1
PSL is primarily designed to support MPE in-
ference (Most Probable Explanation). MPE infer-
ence is the task of finding the overall interpretation
with the maximum probability given a set of evi-
dence. Intuitively, the interpretation with the high-
est probability is the interpretation with the lowest
distance to satisfaction. In other words, it is the
interpretation that tries to satisfy all rules as much
as possible. Formally, from equation 3, the most
probable interpretation, is the one that minimizes
?
r?R
?
r
(d(r))
p
. In case of p = 1, and given
that all d(r) are linear equations, then minimizing
the sum requires solving a linear program, which,
compared to inference in other probabilistic logics
such as MLNs, can be done relatively efficiently
using well-established techniques. In case p = 2,
MPE inference can be shown to be a second-order
cone program (SOCP) (Kimmig et al, 2012).
2.5 Semantic Textual Similarity
Semantic Textual Similarity (STS) is the task of
judging the similarity of a pair of sentences on
a scale from 0 to 5, and was recently introduced
as a SemEval task (Agirre et al, 2012). Gold
standard scores are averaged over multiple hu-
man annotations and systems are evaluated using
the Pearson correlation between a system?s out-
put and gold standard scores. The best perform-
ing system in 2012?s competition was by B?ar et
al. (2012), a complex ensemble system that inte-
grates many techniques including string similarity,
n-gram overlap, WordNet similarity, vector space
similarity and MT evaluation metrics. Two of the
datasets we use for evaluation are from the 2012
competition. We did not utilize the new datasets
added in the 2013 competition since they did not
contain naturally-occurring, full sentences, which
is the focus of our work.
1212
2.6 Combining logical and distributional
methods using probabilistic logic
There are a few recent attempts to combine log-
ical and distributional representations in order to
obtain the advantages of both. Lewis and Steed-
man (2013) use distributional information to deter-
mine word senses, but still produce a strictly log-
ical semantic representation that does not address
the ?graded? nature of linguistic meaning that is
important to measuring semantic similarity.
Garrette et al (2011) introduced a framework
for combining logic and distributional models us-
ing probabilistic logic. Distributional similarity
between pairs of words is converted into weighted
inference rules that are added to the logical repre-
sentation, and Markov Logic Networks are used to
perform probabilistic logical inference.
Beltagy et al (2013) extended this framework
by generating distributional inference rules from
phrase similarity and tailoring the system to the
STS task. STS is treated as computing the prob-
ability of two textual entailments T |= H and
H |= T , where T and H are the two sentences
whose similarity is being judged. These two en-
tailment probabilities are averaged to produce a
measure of similarity. The MLN constructed to
determine the probability of a given entailment
includes the logical forms for both T and H as
well as soft inference rules that are constructed
from distributional information. Given a similar-
ity score for all pairs of sentences in the dataset,
a regressor is trained on the training set to map
the system?s output to the gold standard scores.
The trained regressor is applied to the scores in
the test set before calculating Pearson correlation.
The regression algorithm used is Additive Regres-
sion (Friedman, 2002).
To determine an entailment probability, first,
the two sentences are mapped to logical repre-
sentations using Boxer (Bos, 2008), a tool for
wide-coverage semantic analysis that maps a CCG
(Combinatory Categorial Grammar) parse into a
lexically-based logical form. Boxer uses C&C for
CCG parsing (Clark and Curran, 2004).
Distributional semantic knowledge is then en-
coded as weighted inference rules in the MLN.
A rule?s weight (w) is a function of the cosine
similarity (sim) between its antecedent and con-
sequent. Rules are generated on the fly for each
T and H . Let t and h be the lists of all words
and phrases in T and H respectively. For all
pairs (a, b), where a ? t, b ? h, it generates
an inference rule: a ? b | w, where w =
f(sim(
??
a ,
??
b )). Both a and b can be words or
phrases. Phrases are defined in terms of Boxer?s
output. A phrase is more than one unary atom
sharing the same variable like ?a little kid? which
in logic is little(K) ? kid(K). A phrase also can
be two unary atoms connected by a relation like
?a man is driving? which in logic is man(M) ?
agent(D,M) ? drive(D). The similarity func-
tion sim takes two vectors as input. Phrasal vec-
tors are constructed using Vector Addition (Lan-
dauer and Dumais, 1997). The set of generated
inference rules can be regarded as the knowledge
base KB.
Beltagy et al (2013) found that the logical con-
junction in H is very restrictive for the STS task,
so they relaxed the conjunction by using an aver-
age evidence combiner (Natarajan et al, 2010).
The average combiner results in computationally
complex inference and only works for short sen-
tences. In case inference breaks or times-out, they
back off to a simpler combiner that leads to much
faster inference but loses most of the structure of
the sentence and is therefore less accurate.
Given T , KB and H from the previous
steps, MLN inference is then used to compute
p(H|T,KB), which is then used as a measure of
the degree to which T entails H .
3 PSL for STS
For several reasons, we believe PSL is a more ap-
propriate probabilistic logic for STS than MLNs.
First, it is explicitly designed to support efficient
inference, therefore it scales better to longer sen-
tences with more complex logical forms. Sec-
ond, it was also specifically designed for com-
puting similarity between complex structured ob-
jects rather than determining probabilistic logical
entailment. In fact, the initial version of PSL
(Broecheler et al, 2010) was called Probabilis-
tic Similarity Logic, based on its use of similar-
ity functions. This initial version was shown to be
very effective for measuring the similarity of noisy
database records and performing record linkage
(i.e. identifying database entries referring to the
same entity, such as bibliographic citations refer-
ring to the same paper). Therefore, we have devel-
oped an approach that follows that of Beltagy et
al. (2013), but replaces Markov Logic with PSL.
This section explains how we formulate the STS
1213
task as a PSL program. PSL does not work very
well ?out of the box? for STS, mainly because
Lukasiewicz?s equation for the conjunction is very
restrictive. Therefore, we use a different interpre-
tation for conjunction that uses averaging, which
requires corresponding changes to the optimiza-
tion problem and the grounding technique.
3.1 Representation
Given the logical forms for a pair of sentences,
a text T and a hypothesis H , and given a set of
weighted rules derived from the distributional se-
mantics (as explained in section 2.6) composing
the knowledge base KB, we build a PSL model
that supports determining the truth value of H in
the most probable interpretation (i.e. MPE) given
T and KB.
Consider the pair of sentences is ?A man is driv-
ing?, and ?A guy is walking?. Parsing into logical
form gives:
T : ?x, y. man(x) ? agent(y, x) ? drive(y)
H : ?x, y. guy(x) ? agent(y, x) ? walk(y)
The PSL program is constructed as follows:
T : The text is represented in the evidence set. For
the example, after Skolemizing the existential
quantifiers, this contains the ground atoms:
{man(A), agent(B,A), drive(B)}
KB: The knowledge base is a set of lexical and
phrasal rules generated from distributional
semantics, along with a similarity score for
each rule (section 2.6). For the exam-
ple, we generate the rules: ?x. man(x) ?
vs sim(?man?, ?guy?)? guy(x) ,
?x.drive(x)?vs sim(?drive?, ?walk?)?
walk(x)
where vs sim is a similarity function that
calculates the distributional similarity score
between the two lexical predicates. All rules
are assigned the same weight because all
rules are equally important.
H: The hypothesis is represented as H ?
result(), and then PSL is queried for the
truth value of the atom result(). For
our example, the rule is: ?x, y. guy(x) ?
agent(y, x) ? walk(y)? result().
Priors: A low prior is given to all predicates. This
encourages the truth values of ground atoms
to be zero, unless there is evidence to the con-
trary.
For each STS pair of sentences S
1
, S
2
, we run
PSL twice, once where T = S
1
, H = S
2
and
another where T = S
2
, H = S
1
, and output the
two scores. To produce a final similarity score, we
train a regressor to learn the mapping between the
two PSL scores and the overall similarity score.
As in Beltagy et al, (2013) we use Additive Re-
gression (Friedman, 2002).
3.2 Changing Conjunction
As mentioned above, Lukasiewicz?s formula for
conjunction is very restrictive and does not work
well for STS. For example, for T: ?A man is driv-
ing? and H: ?A man is driving a car?, if we use the
standard PSL formula for conjunction, the output
value is zero because there is no evidence for a car
and max(0, X + 0 ? 1) = 0 for any truth value
0 ? X ? 1. However, humans find these sen-
tences to be quite similar.
Therefore, we introduce a new averaging inter-
pretation of conjunction that we use for the hy-
pothesis H . The truth value for a conjunction
is defined as I(p
1
? .... ? p
n
) =
1
n
?
n
i=1
I(p
i
).
This averaging function is linear, and the result is
a valid truth value in the interval [0, 1], therefore
this change is easily incorporated into PSL with-
out changing the complexity of inference which
remains a linear-programming problem.
It would perhaps be even better to use a
weighted average, where weights for different
components are learned from a supervised train-
ing set. This is an important direction for future
work.
3.3 Grounding Process
Grounding is the process of instantiating the vari-
ables in the quantified rules with concrete con-
stants in order to construct the nodes and links in
the final graphical model. In principle, ground-
ing requires instantiating each rule in all possible
ways, substituting every possible constant for each
variable in the rule. However, this is a combinato-
rial process that can easily result in an explosion in
the size of the final network. Therefore, PSL em-
ploys a ?lazy? approach to grounding that avoids
the construction of irrelevant groundings. If there
is no evidence for one of the antecedents in a par-
ticular grounding of a rule, then the normal PSL
formula for conjunction guarantees that the rule is
1214
Algorithm 1 Heuristic Grounding
Input: r
body
= a
1
? ....?a
n
: antecedent of a rule
with average interpretation of conjunction
Input: V : set of variables used in r
body
Input: Ant(v
i
): subset of antecedents a
j
con-
taining variable v
i
Input: Const(v
i
): list of possible constants of
variable v
i
Input: Gnd(a
i
): set of ground atoms of a
i
.
Input: GndConst(a, g, v): takes an atom a,
grounding g for a, and variable v, and returns
the constant that substitutes v in g
Input: gnd limit: limit on the number of
groundings
1: for all v
i
? V do
2: for all C ? Const(v
i
) do
3: score(C) =
?
a?Ant(v
i
)
(max I(g))
for g ? Gnd(a) ?GndConst(a, g, v
i
) = C
4: end for
5: sort Const(v
i
) on scores, descending
6: end for
7: return For all v
i
? V , take the Cartesian-
product of the sortedConst(v
i
) and return the
top gnd limit results
trivially satisfied (I(r) = 1) since the truth value
of the antecedent is zero. Therefore, its distance to
satisfaction is also zero, and it can be omitted from
the ground network without impacting the result of
MPE inference.
However, this technique does not work once
we switch to using averaging to interpret conjunc-
tions. For example, given the rule ?x. p(x) ?
q(x) ? t() and only one piece of evidence p(C)
there are no relevant groundings because there is
no evidence for q(C), and therefore, for normal
PSL, I(p(C) ? q(C)) = 0 which does not affect
I(t()). However, when using averaging with the
same evidence, we need to generate the grounding
p(C)?q(C) because I(p(C)?q(C)) = 0.5 which
does affect I(t()).
One way to solve this problem is to eliminate
lazy grounding and generate all possible ground-
ings. However, this produces an intractably large
network. Therefore, we developed a heuristic ap-
proximate grounding technique that generates a
subset of the most impactful groundings.
Pseudocode for this heuristic approach is shown
in algorithm 1. Its goal is to find constants that
participate in ground propositions with high truth
value and preferentially use them to construct a
limited number of groundings of each rule.
The algorithm takes the antecedents of a rule
employing averaging conjunction as input. It also
takes the grounding limit which is a threshold on
the number of groundings to be returned. The al-
gorithm uses several subroutines, they are:
? Ant(v
i
): given a variable v
i
, it returns the set
of rule antecedent atoms containing v
i
. E.g,
for the rule: a(x) ? b(y) ? c(x), Ant(x) re-
turns the set of atoms {a(x), c(x)}.
? Const(v
i
): given a variable v
i
, it returns the
list of possible constants that can be used to
instantiate the variable v
i
.
? Gnd(a
i
): given an atom a
i
, it returns the set
of all possible ground atoms generated for a
i
.
? GndConst(a, g, v): given an atom a and
grounding g for a, and a variable v, it finds
the constant that substitutes for v in g. E.g,
assume there is an atom a = a
i
(v
1
, v
2
), and
the ground atom g = a
i
(A,B) is one of its
groundings. GndConst(a, g, v
2
) would re-
turn the constant B since it is the substitution
for the variable v
2
in g.
Lines 1-6 loop over all variables in the rule. For
each variable, lines 2-5 construct a list of constants
for that variable and sort it based on a heuristic
score. In line 3, each constant is assigned a score
that indicates the importance of this constant in
terms of its impact on the truth value of the overall
grounding. A constant?s score is the sum, over all
antedents that contain the variable in question, of
the maximum truth value of any grounding of that
antecedent that contains that constant.
Pushing constants with high scores to the top
of each variable?s list will tend to make the over-
all truth value of the top groundings high. Line
7 computes a subset of the Cartesian product of
the sorted lists of constants, selecting constants in
ranked order and limiting the number of results to
the grounding limit.
One point that needs to be clarified about this
approach is how it relies on the truth values of
ground atoms when the goal of inference is to ac-
tually find these values. PSL?s inference is ac-
tually an iterative process where in each itera-
tion a grounding phase is followed by an opti-
mization phase (solving the linear program). This
loop repeats until convergence, i.e. until the truth
1215
values stop changing. The truth values used in
each grounding phase come from the previous op-
timization phase. The first grounding phase as-
sumes only the propositions in the evidence pro-
vided have non-zero truth values.
4 Evaluation
This section evaluates the performance of PSL on
the STS task.
4.1 Datasets
We evaluate our system on three STS datasets.
? msr-vid: Microsoft Video Paraphrase Cor-
pus from STS 2012. The dataset consists
of 1,500 pairs of short video descriptions
collected using crowdsourcing (Chen and
Dolan, 2011) and subsequently annotated for
the STS task (Agirre et al, 2012). Half of
the dataset is for training, and the second half
is for testing.
? msr-par: Microsoft Paraphrase Corpus from
STS 2012 task. The dataset is 5,801
pairs of sentences collected from news
sources (Dolan et al, 2004). Then, for STS
2012, 1,500 pairs were selected and anno-
tated with similarity scores. Half of the
dataset is for training, and the second half is
for testing.
? SICK: Sentences Involving Compositional
Knowledge is a dataset collected for SemEval
2014. Only the training set is available at this
point, which consists of 5,000 pairs of sen-
tences. Pairs are annotated for RTE and STS,
but we only use the STS data. Training and
testing was done using 10-fold cross valida-
tion.
4.2 Systems Compared
We compare our PSL system with several others.
In all cases, we use the distributional word vec-
tors employed by Beltagy et al (2013) based on
context windows from Gigaword.
? vec-add: Vector Addition (Landauer and
Dumais, 1997). We compute a vector rep-
resentation for each sentence by adding the
distributional vectors of all of its words and
measure similarity using cosine. This is a
simple yet powerful baseline that uses only
distributional information.
? vec-mul: Component-wise Vector Multipli-
cation (Mitchell and Lapata, 2008). The
same as vec-add except uses component-
wise multiplication to combine word vectors.
? MLN: The system of Beltagy et al (2013),
which uses Markov logic instead of PSL for
probabilistic inference. MLN inference is
very slow in some cases, so we use a 10
minute timeout. When MLN times out, it
backs off to a simpler sentence representation
as explained in section 2.6.
? PSL: Our proposed PSL system for combin-
ing logical and distributional information.
? PSL-no-DIR: Our PSL system without dis-
tributional inference rules(empty knowledge
base). This system uses PSL to compute sim-
ilarity of logical forms but does not use dis-
tributional information on lexical or phrasal
similarity. It tests the impact of the proba-
bilistic logic only
? PSL+vec-add: PSL ensembled with vec-
add. Ensembling the MLN approach with a
purely distributional approach was found to
improve results (Beltagy et al, 2013), so we
also tried this with PSL. The methods are en-
sembled by using both entailment scores of
both systems as input features to the regres-
sion step that learns to map entailment scores
to STS similarity ratings. This way, the train-
ing data is used to learn how to weight the
contribution of the different components.
? PSL+MLN: PSL ensembled with MLN in
the same manner.
4.3 Experiments
Systems are evaluated on two metrics, Pearson
correlation and average CPU time per pair of sen-
tences.
? Pearson correlation: The Pearson correlation
between the system?s similarity scores and
the human gold-standards.
? CPU time: This metric only applies to MLN
and PSL. The CPU time taken by the infer-
ence step is recorded and averaged over all
pairs in each of the test datasets. In many
cases, MLN inference is very slow, so we
timeout after 10 minutes and report the num-
ber of timed-out pairs on each dataset.
1216
msr-vid msr-par SICK
vec-add 0.78 0.24 0.65
vec-mul 0.76 0.12 0.62
MLN 0.63 0.16 0.47
PSL-no-DIR 0.74 0.46 0.68
PSL 0.79 0.53 0.70
PSL+vec-add 0.83 0.49 0.71
PSL+MLN 0.79 0.51 0.70
Best Score (B?ar
et al, 2012)
0.87 0.68 n/a
Table 1: STS Pearson Correlations
PSL MLN
time time timeouts/total
msr-vid 8s 1m 31s 132/1500
msr-par 30s 11m 49s 1457/1500
SICK 10s 4m 24s 1791/5000
Table 2: Average CPU time per STS pair, and
number of timed-out pairs in MLN with a 10
minute time limit. PSL?s grounding limit is set to
10,000 groundings.
We also evaluated the effect of changing the
grounding limit on both Pearson correlation and
CPU time for the msr-par dataset. Most of the
sentences in msr-par are long, which results is
large number of groundings, and limiting the num-
ber of groundings has a visible effect on the over-
all performance. In the other two datasets, the
sentences are fairly short, and the full number of
groundings is not large; therefore, changing the
grounding limit does not significantly affect the re-
sults.
4.4 Results and Discussion
Table 1 shows the results for Pearson correlation.
PSL out-performs the purely distributional base-
lines (vec-add and vec-mul) because PSL is able
to combine the information available to vec-add
and vec-mul in a better way that takes sentence
structure into account. PSL also outperforms
the unaided probabilistic-logic baseline that does
not use distributional information (PSL-no-DIR).
PSL-no-DIR works fairly well because there is
significant overlap in the exact words and struc-
ture of the paired sentences in the test data, and
PSL combines the evidence from these similari-
ties effectively. In addition, PSL always does sig-
nificantly better than MLN, because of the large
Figure 1: Effect of PSL?s grounding limit on the
correlation score for the msr-par dataset
number of timeouts, and because the conjunction-
averaging in PSL is combining evidence bet-
ter than MLN?s average-combiner, whose perfor-
mance is sensitive to various parameters. These
results further support the claim that using prob-
abilistic logic to integrate logical and distribu-
tional information is a promising approach to
natural-language semantics. More specifically,
they strongly indicate that PSL is a more effective
probabilistic logic for judging semantic similarity
than MLNs.
Like for MLNs (Beltagy et al, 2013), en-
sembling PSL with vector addition improved the
scores a bit, except for msr-par where vec-add?s
performance is particularly low. However, this en-
semble still does not beat the state of the art (B?ar et
al., 2012) which is a large ensemble of many dif-
ferent systems. It would be informative to add our
system to their ensemble to see if it could improve
it even further.
Table 2 shows the CPU time for PSL and MLN.
The results clearly demonstrate that PSL is an or-
der of magnitude faster than MLN.
Figures 1 and 2 show the effect of changing the
grounding limit on Pearson correlation and CPU
time. As expected, as the grounding limit is in-
creased, accuracy improves but CPU time also
increases. However, note that the difference in
scores between the smallest and largest grounding
limit tested is not large, suggesting that the heuris-
tic approach to limiting grounding is quite effec-
tive.
5 Future Work
As mentioned in Section 3.2, it would be good
to use a weighted average to compute the truth
1217
Figure 2: Effect of PSL?s grounding limit on CPU
time for the msr-par dataset
values for conjunctions, weighting some predi-
cates more than others rather than treating them
all equally. Appropriate weights for different com-
ponents could be learned from training data. For
example, such an approach could learn that the
type of an object determined by a noun should be
weighted more than a property specified by an ad-
jective. As a result, ?black dog? would be appro-
priately judged more similar to ?white dog? than
to ?black cat.?
One of the advantages of using a probabilis-
tic logic is that additional sources of knowledge
can easily be incorporated by adding additional
soft inference rules. To complement the soft in-
ference rules capturing distributional lexical and
phrasal similarities, PSL rules could be added that
encode explicit paraphrase rules, such as those
mined from monolingual text (Berant et al, 2011)
or multi-lingual parallel text (Ganitkevitch et al,
2013).
This paper has focused on STS; however, as
shown by Beltagy et al (2013), probabilistic logic
is also an effective approach to recognizing tex-
tual entailment (RTE). By using the appropriate
functions to combine truth values for various log-
ical connectives, PSL could also be adapted for
RTE. Although we have shown that PSL outper-
forms MLNs on STS, we hypothesize that MLNs
may still be a better approach for RTE. However, it
would be good to experimentally confirm this in-
tuition. In any case, the high computational com-
plexity of MLN inference could mean that PSL is
still a more practical choice for RTE.
6 Conclusion
This paper has presented an approach that uses
Probabilistic Soft Logic (PSL) to determine Se-
mantic Textual Similarity (STS). The approach
uses PSL to effectively combine logical seman-
tic representations of sentences with soft infer-
ence rules for lexical and phrasal similarities com-
puted from distributional information. The ap-
proach builds upon a previous method that uses
Markov Logic (MLNs) for STS, but replaces the
probabilistic logic with PSL in order to improve
the efficiency and accuracy of probabilistic infer-
ence. The PSL approach was experimentally eval-
uated on three STS datasets and was shown to out-
perform purely distributional baselines as well as
the MLN approach. The PSL approach was also
shown to be much more scalable and efficient than
using MLNs
Acknowledgments
This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the author and do not necessarily reflect the view
of DARPA, DoD or the US government. Some ex-
periments were run on the Mastodon Cluster sup-
ported by NSF Grant EIA-0303609.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of Semantic Evaluation (SemEval-12).
Stephen H. Bach, Bert Huang, Ben London, and Lise
Getoor. 2013. Hinge-loss Markov random fields:
Convex inference for structured prediction. In Pro-
ceedings of Uncertainty in Artificial Intelligence
(UAI-13).
Daniel B?ar, Chris Biemann, Iryna Gurevych, and
Torsten Zesch. 2012. UKP: Computing seman-
tic textual similarity by combining multiple content
similarity measures. In Proceedings of Semantic
Evaluation (SemEval-12).
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-10).
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
1218
Montague meets Markov: Deep semantics with
probabilistic logical form. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM-13).
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of Association for Computational Lin-
guistics (ACL-11).
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP-13).
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of Semantics in Text
Processing (STEP-08).
Matthias Broecheler, Lilyana Mihalkova, and Lise
Getoor. 2010. Probabilistic Similarity Logic. In
Proceedings of Uncertainty in Artificial Intelligence
(UAI-20).
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of Association for Computational Lin-
guistics (ACL-11).
Stephen Clark and James R. Curran. 2004. Parsing
the WSJ using CCG and log-linear models. In Pro-
ceedings of Association for Computational Linguis-
tics (ACL-04).
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of the International Conference on
Computational Linguistics (COLING-04).
Jerome H Friedman. 2002. Stochastic gradient boost-
ing. Journal of Computational Statistics & Data
Analysis (CSDA-02).
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT-13).
Dan Garrette, Katrin Erk, and Raymond Mooney.
2011. Integrating logical representations with prob-
abilistic information using Markov logic. In Pro-
ceedings of International Conference on Computa-
tional Semantics (IWCS-11).
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing (EMNLP-11).
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of Second Joint Conference
on Lexical and Computational Semantics (*SEM
2013).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to Probabilistic Soft Logic.
In Proceedings of NIPS Workshop on Probabilistic
Programming: Foundations and Applications (NIPS
Workshop-12).
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-13).
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato?s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation
of knowledge. Psychological Review.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. Transactions
of the Association for Computational Linguistics
(TACL-13).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of Association for Computational Linguistics (ACL-
08).
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Journal of
Cognitive Science.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36:373?398.
Sriraam Natarajan, Tushar Khot, Daniel Lowd, Prasad
Tadepalli, Kristian Kersting, and Jude Shavlik.
2010. Exploiting causal independence in Markov
logic networks: Combining undirected and directed
models. In Proceedings of European Conference in
Machine Learning (ECML-10).
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107?136.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research
(JAIR-10).
1219
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 11?21, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Montague Meets Markov: Deep Semantics with Probabilistic Logical Form
Islam Beltagy?, Cuong Chau?, Gemma Boleda?, Dan Garrette?, Katrin Erk?,
Raymond Mooney?
?Department of Computer Science
?Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
?{beltagy,ckcuong,dhg,mooney}@cs.utexas.edu
?gemma.boleda@utcompling.com,katrin.erk@mail.utexas.edu
Abstract
We combine logical and distributional rep-
resentations of natural language meaning by
transforming distributional similarity judg-
ments into weighted inference rules using
Markov Logic Networks (MLNs). We show
that this framework supports both judg-
ing sentence similarity and recognizing tex-
tual entailment by appropriately adapting the
MLN implementation of logical connectives.
We also show that distributional phrase simi-
larity, used as textual inference rules created
on the fly, improves its performance.
1 Introduction
Tasks in natural language semantics are very diverse
and pose different requirements on the underlying
formalism for representing meaning. Some tasks
require a detailed representation of the structure of
complex sentences. Some tasks require the ability to
recognize near-paraphrases or degrees of similarity
between sentences. Some tasks require logical infer-
ence, either exact or approximate. Often it is neces-
sary to handle ambiguity and vagueness in meaning.
Finally, we frequently want to be able to learn rele-
vant knowledge automatically from corpus data.
There is no single representation for natural lan-
guage meaning at this time that fulfills all require-
ments. But there are representations that meet some
of the criteria. Logic-based representations (Mon-
tague, 1970; Kamp and Reyle, 1993) provide an
expressive and flexible formalism to express even
complex propositions, and they come with standard-
ized inference mechanisms. Distributional mod-
hamster(gerbil(
sim( #                ?hamster, #         ?gerbil) = w
8x hamster(x) ! gerbil(x)  | f(w)
Figure 1: Turning distributional similarity into a
weighted inference rule
els (Turney and Pantel, 2010) use contextual sim-
ilarity to predict semantic similarity of words and
phrases (Landauer and Dumais, 1997; Mitchell and
Lapata, 2010), and to model polysemy (Schu?tze,
1998; Erk and Pado?, 2008; Thater et al, 2010).
This suggests that distributional models and logic-
based representations of natural language meaning
are complementary in their strengths (Grefenstette
and Sadrzadeh, 2011; Garrette et al, 2011), which
encourages developing new techniques to combine
them.
Garrette et al (2011; 2013) propose a framework
for combining logic and distributional models in
which logical form is the primary meaning repre-
sentation. Distributional similarity between pairs of
words is converted into weighted inference rules that
are added to the logical form, as illustrated in Fig-
ure 1. Finally, Markov Logic Networks (Richardson
and Domingos, 2006) (MLNs) are used to perform
weighted inference on the resulting knowledge base.
However, they only employed single-word distribu-
tional similarity rules, and only evaluated on a small
11
set of short, hand-crafted test sentences.
In this paper, we extend Garrette et al?s approach
and adapt it to handle two existing semantic tasks:
recognizing textual entailment (RTE) and seman-
tic textual similarity (STS). We show how this sin-
gle semantic framework using probabilistic logical
form in Markov logic can be adapted to support both
of these important tasks. This is possible because
MLNs constitute a flexible programming language
based on probabilistic logic (Domingos and Lowd,
2009) that can be easily adapted to support multiple
types of linguistically useful inference.
At the word and short phrase level, our approach
model entailment through ?distributional? similarity
(Figure 1). If X and Y occur in similar contexts, we
assume that they describe similar entities and thus
there is some degree of entailment between them. At
the sentence level, however, we hold that a stricter,
logic-based view of entailment is beneficial, and we
even model sentence similarity (in STS) as entail-
ment.
There are two main innovations in the formalism
that make it possible for us to work with naturally
occurring corpus data. First, we use more expres-
sive distributional inference rules based on the sim-
ilarity of phrases rather than just individual words.
In comparison to existing methods for creating tex-
tual inference rules (Lin and Pantel, 2001b; Szpek-
tor and Dagan, 2008), these rules are computed on
the fly as needed, rather than pre-compiled. Second,
we use more flexible probabilistic combinations of
evidence in order to compute degrees of sentence
similarity for STS and to help compensate for parser
errors. We replace deterministic conjunction by an
average combiner, which encodes causal indepen-
dence (Natarajan et al, 2010).
We show that our framework is able to han-
dle both sentence similarity (STS) and textual en-
tailment (RTE) by making some simple adapta-
tions to the MLN when switching between tasks.
The framework achieves reasonable results on both
tasks. On STS, we obtain a correlation of r = 0.66
with full logic, r = 0.73 in a system with weak-
ened variable binding, and r = 0.85 in an ensemble
model. On RTE-1 we obtain an accuracy of 0.57.
We show that the distributional inference rules ben-
efit both tasks and that more flexible probabilistic
combinations of evidence are crucial for STS. Al-
though other approaches could be adapted to handle
both RTE and STS, we do not know of any other
methods that have been explicitly tested on both
problems.
2 Related work
Distributional semantics Distributional models
define the semantic relatedness of words as the
similarity of vectors representing the contexts in
which they occur (Landauer and Dumais, 1997;
Lund and Burgess, 1996). Recently, such mod-
els have also been used to represent the meaning
of larger phrases. The simplest models compute
a phrase vector by adding the vectors for the indi-
vidual words (Landauer and Dumais, 1997) or by a
component-wise product of word vectors (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010).
Other approaches, in the emerging area of distribu-
tional compositional semantics, use more complex
methods that compute phrase vectors from word
vectors and tensors (Baroni and Zamparelli, 2010;
Grefenstette and Sadrzadeh, 2011).
Wide-coverage logic-based semantics Boxer
(Bos, 2008) is a software package for wide-coverage
semantic analysis that produces logical forms using
Discourse Representation Structures (Kamp and
Reyle, 1993). It builds on the C&C CCG parser
(Clark and Curran, 2004).
Markov Logic In order to combine logical and
probabilistic information, we draw on existing work
in Statistical Relational AI (Getoor and Taskar,
2007). Specifically, we utilize Markov Logic Net-
works (MLNs) (Domingos and Lowd, 2009), which
employ weighted formulas in first-order logic to
compactly encode complex undirected probabilistic
graphical models. MLNs are well suited for our ap-
proach since they provide an elegant framework for
assigning weights to first-order logical rules, com-
bining a diverse set of inference rules and perform-
ing sound probabilistic inference.
An MLN consists of a set of weighted first-order
clauses. It provides a way of softening first-order
logic by allowing situations in which not all clauses
are satisfied. More specifically, they provide a
well-founded probability distribution across possi-
ble worlds by specifying that the probability of a
12
world increases exponentially with the total weight
of the logical clauses that it satisfies. While methods
exist for learning MLN weights directly from train-
ing data, since the appropriate training data is lack-
ing, our approach uses weights computed using dis-
tributional semantics. We use the open-source soft-
ware package Alchemy (Kok et al, 2005) for MLN
inference, which allows computing the probability
of a query literal given a set of weighted clauses as
background knowledge and evidence.
Tasks: RTE and STS Recognizing Textual En-
tailment (RTE) is the task of determining whether
one natural language text, the premise, implies an-
other, the hypothesis. Consider (1) below.
(1) p: Oracle had fought to keep the forms from
being released
h: Oracle released a confidential document
Here, h is not entailed. RTE directly tests whether
a system can construct semantic representations that
allow it to draw correct inferences. Of existing RTE
approaches, the closest to ours is by Bos and Mark-
ert (2005), who employ a purely logical approach
that uses Boxer to convert both the premise and hy-
pothesis into first-order logic and then checks for
entailment using a theorem prover. By contrast, our
approach uses Markov logic with probabilistic infer-
ence.
Semantic Textual Similarity (STS) is the task of
judging the similarity of two sentences on a scale
from 0 to 5 (Agirre et al, 2012). Gold standard
scores are averaged over multiple human annota-
tions. The best performer in 2012?s competition was
by Ba?r et al (2012), an ensemble system that inte-
grates many techniques including string similarity,
n-gram overlap, WordNet similarity, vector space
similarity and MT evaluation metrics.
Weighted inference, and combined structural-
distributional representations One approach to
weighted inference in NLP is that of Hobbs et al
(1993), who proposed viewing natural language in-
terpretation as abductive inference. In this frame-
work, problems like reference resolution and syntac-
tic ambiguity resolution become inferences to best
explanations that are associated with costs. How-
ever, this leaves open the question of how costs are
assigned. Raina et al (2005) use this framework for
RTE, deriving inference costs from WordNet simi-
larity and properties of the syntactic parse.
Garrette et al (2011; 2013) proposed an approach
to RTE that uses MLNs to combine traditional log-
ical representations with distributional information
in order to support probabilistic textual inference.
This approach can be viewed as a bridge between
Bos and Markert (2005)?s purely logical approach,
which relies purely on hard logical rules and the-
orem proving, and distributional approaches, which
support graded similarity between concepts but have
no notion of logical operators or entailment.
There are also other methods that combine dis-
tributional and structured representations. Stern et
al. (2011) conceptualize textual entailment as tree
rewriting of syntactic graphs, where some rewrit-
ing rules are distributional inference rules. Socher
et al (2011) recognize paraphrases using a ?tree of
vectors,? a phrase structure tree in which each con-
stituent is associated with a vector, and overall sen-
tence similarity is computed by a classifier that inte-
grates all pairwise similarities. (This is in contrast to
approaches like Baroni and Zamparelli (2010) and
Grefenstette and Sadrzadeh (2011), who do not of-
fer a proposal for using vectors at multiple levels in
a syntactic tree simultaneously.)
3 MLN system
Our system extends that of Garrette et al (2011;
2013) to support larger-scale evaluation on standard
benchmarks for both RTE and STS. We conceptual-
ize both tasks as probabilistic entailment in Markov
logic, where STS is judged as the average degree of
mutual entailment, i.e. we compute the probability
of both S1 |= S2 and S2 |= S1 and average the re-
sults. Below are some sentence pairs that we use as
examples in the discussion below:
(2) S1: A man is slicing a cucumber.
S2: A man is slicing a zucchini.
(3) S1: A boy is riding a bicycle.
S2: A little boy is riding a bike.
(4) S1: A man is driving.
S2: A man is driving a car.
13
System overview. To compute the probability of
an entailment S1 |= S2, the system first constructs
logical forms for each sentence using Boxer and
then translates them into MLN clauses. In example
(2) above, the logical form for S1:
?x0, e1, x2
(
man(x0) ? slice(e1) ?Agent(e1, x0)?
cucumber(x2) ? Patient(e1, x2)
)
is used as evidence, and the logical form for S2 is
turned into the following formula (by default, vari-
ables are assumed to be universally quantified):
man(x) ? slice(e) ?Agent(e, x)?
zucchini(y) ? Patient(e, y)? result()
where result() is the query for which we have
Alchemy compute the probability.
However, S2 is not strictly entailed by S1 because
of the mismatch between ?cucumber? and ?zuc-
chini?, so with just the strict logical-form transla-
tions of S1 and S2, the probability of result() will
be zero. This is where we introduce distributional
similarity, in this case the similarity of ?cucumber?
and ?zucchini?, cos( #                  ?cucumber, #               ?zucchini). We cre-
ate inference rules from such similarities as a form
of background knowledge. We then treat similarity
as degree of entailment, a move that has a long tradi-
tion (e.g., (Lin and Pantel, 2001b; Raina et al, 2005;
Szpektor and Dagan, 2008)). In general, given two
words a and b, we transform their cosine similarity
into an inference-rule weight wt(a, b) using:
wt(a, b) = log( cos(
#?a , #?b )
1? cos( #?a , #?b )
)? prior (5)
Where prior is a negative weight used to initialize
all predicates, so that by default facts are assumed
to have very low probability. In our experiments,
we use prior = ?3. In the case of sentence pair
(2), we generate the inference rule:
cucumber(x)? zucchini(x) | wt(cuc., zuc.)
Such inference rules are generated for all pairs of
words (w1, w2) where w1 ? S1 and w2 ? S2.1
1We omit inference rules for words (a, b) where cos(a, b) <
? for a threshold ? set to maximize performance on the training
data. Low-similarity pairs usually indicate dissimilar words.
This removes a sizeable number of rules for STS, while for RTE
the tuned threshold was near zero.
The distributional model we use contains all lem-
mas occurring at least 50 times in the Gigaword cor-
pus (Graff et al, 2007) except a list of stop words.
The dimensions are the 2,000 most frequent of these
words, and cell values are weighted with point-wise
mutual information. 2
Phrase-based inference rules. Garrette et al only
considered distributional inference rules for pairs of
individual words. We extend their approach to dis-
tributional inference rules for pairs of phrases in or-
der to handle cases like (3). To properly estimate
the similarity between S1 and S2 in (3), we not only
need an inference rule linking ?bike? to ?bicycle?,
but also a rule estimating how similar ?boy? is to
?little boy?. To do so, we make use of existing ap-
proaches that compute distributional representations
for phrases. In particular, we compute the vector for
a phrase from the vectors of the words in that phrase,
using either vector addition (Landauer and Dumais,
1997) or component-wise multiplication (Mitchell
and Lapata, 2008; Mitchell and Lapata, 2010). The
inference-rule weight, wt(p1, p2), for two phrases
p1 and p2 is then determined using Eq. (5) in the
same way as for words.
Existing approaches that derive phrasal inference
rules from distributional similarity (Lin and Pantel,
2001a; Szpektor and Dagan, 2008; Berant et al,
2011) precompile large lists of inference rules. By
comparison, distributional phrase similarity can be
seen as a generator of inference rules ?on the fly?,
as it is possible to compute distributional phrase
vectors for arbitrary phrases on demand as they are
needed for particular examples.
Inference rules are generated for all pairs of con-
stituents (c1, c2) where c1 ? S1 and c2 ? S2, a
constituent is a single word or a phrase. The log-
ical form provides a handy way to extract phrases,
as they are generally mapped to one of two logical
constructs. Either we have multiple single-variable
predicates operating on the same variable. For ex-
ample the phrase ?a little boy? has the logical form
boy(x) ? little(x). Or we have two unary predi-
cates connected with a relation. For example, ?pizza
slice? and ?slice of pizza? are both mapped to the
2It is customary to transform raw counts in a way that cap-
tures association between target words and dimensions, for ex-
ample through point-wise mutual information (Lowe, 2001).
14
logical form, slice(x0) ? of(x0, x1) ? pizza(x1).
We consider all binary predicates as relations.
Average Combiner to determine similarity in the
presence of missing phrases. The logical forms
for the sentences in (4): are
S1: ?x0, e1
(
man(x0)?agent(x0, e1)?drive(e1)
)
S2: ?x0, e1, x2
(
man(x0) ? agent(x0, e1) ?
drive(e1) ? patient(e1, x2) ? car(x2)
)
If we try to prove S1 |= S2, the probability of
the result() will be zero: There is no evidence for
a car, and the hypothesis predicates are conjoined
using a deterministic AND. For RTE, this makes
sense: If one of the hypothesis predicates is False,
the probability of entailment should be zero. For the
STS task, this should in principle be the same, at
least if the omitted facts are vital, but it seems that
annotators rated the data points in this task more for
overall similarity than for degrees of entailment. So
in STS, we want the similarity to be a function of
the number of elements in the hypothesis that are
inferable. Therefore, we need to replace the deter-
ministic AND with a different way of combining
evidence. We chose to use the average evidence
combiner for MLNs introduced by Natarajan et al
(2010). To use the average combiner, the full logi-
cal form is divided into smaller clauses (which we
call mini-clauses), then the combiner averages their
probabilities. In case the formula is a list of con-
juncted predicates, a mini-clause is a conjunction
of a single-variable predicate with a relation predi-
cate(as in the example below). In case the logical
form contains a negated sub-formula, the negated
sub-formula is also a mini-clause. The hypothesis
above after dividing clauses for the average com-
biner looks like this:
man(x0) ? agent(x0, e1)? result(x0, e1, x2) | w
drive(e1) ? agent(x0, e1)? result(x0, e1, x2) | w
drive(e1) ? patient(e1, x2)? result(x0, e1, x2) | w
car(x2) ? patient(e1, x2)? result(x0, e1, x2) | w
where result is again the query predicate. Here,
result has all of the variables in the clause as argu-
ments in order to maintain the binding of variables
across all of the mini-clauses. The weights w are the
following function of n, the number of mini-clauses
(4 in the above example):
w = 1n ? (log(
p
1? p)? prior) (6)
where p is a value close to 1 that is set to maximize
performance on the training data, and prior is the
same negative weight as before. Setting w this way
produces a probability of p for the result() in cases
that satisfy the antecedents of all mini-clauses. For
the example above, the antecedents of the first two
mini-clauses are satisfied, while the antecedents of
the last two are not since the premise provides no
evidence for an object of the verb drive. The simi-
larity is then computed to be the maximum probabil-
ity of any grounding of the result predicate, which
in this case is around p2 .
3
An interesting variation of the average combiner
is to omit variable bindings between the mini-
clauses. In this case, the hypothesis clauses look
like this for our example:
man(x) ? agent(x, e)? result() | w
drive(e) ? agent(x, e)? result() | w
drive(e) ? patient(e, x)? result() | w
car(x) ? patient(e, x)? result() | w
This implementation loses a lot of information,
for example it does not differentiate between ?A
man is walking and a woman is driving? and ?A
man is driving and a woman is walking?. In fact,
logical form without variable binding degrades to a
representation similar to a set of independent syn-
tactic dependencies, 4 while the average combiner
with variable binding retains all of the information
in the original logical form. Still, omitting variable
binding turns out to be useful for the STS task.
It is also worth commenting on the efficiency of
the inference algorithm when run on the three dif-
ferent approaches to combining evidence. The aver-
age combiner without variable binding is the fastest
and has the least memory requirements because all
cliques in the ground network are of limited size
(just 3 or 4 nodes). Deterministic AND is much
slower than the average combiner without variable
binding, because the maximum clique size depends
on the sentence. The average combiner with vari-
able binding is the most memory intensive since the
3One could also give mini-clauses different weights depend-
ing on their importance, but we have not experimented with this
so far.
4However, it is not completely the same since we do not
divide up formulas under negation into mini-clauses.
15
number of arguments of the result() predicate can
become large (there is an argument for each individ-
ual and event in the sentence). Consequently, the
inference algorithm needs to consider a combinato-
rial number of possible groundings of the result()
predicate, making inference very slow.
Adaptation of the logical form. As discussed by
Garrette et al (2011), Boxer?s output is mapped to
logical form and augmented with additional infor-
mation to handle a variety of semantic phenomena.
However, we do not use their additional rules for
handling implicatives and factives, as we wanted to
test the system without background knowledge be-
yond that supplied by the vector space.
Unfortunately, current MLN inference algorithms
are not able to efficiently handle complex formu-
las with nested quantifiers. For that reason, we re-
placed universal quantifiers in Boxer?s output with
existentials since they caused serious problems for
Alchemy. Although this is a radical change to the
semantics of the logical form, due to the nature of
the STS and RTE data, it only effects about 5% of
the sentences, and we found that most of the uni-
versal quantifiers in these cases were actually due
to parsing errors. We are currently exploring more
effective ways of dealing with this issue.
4 Task 1: Recognizing Textual Entailment
4.1 Dataset
In order to compare directly to the logic-based sys-
tem of Bos and Markert (2005), we focus on the
RTE-1 dataset (Dagan et al, 2005), which includes
567 Text-Hypothesis (T-H) pairs in the development
set and 800 pairs in the test set. The data covers a
wide range of issues in entailment, including lexical,
syntactic, logical, world knowledge, and combina-
tions of these at different levels of difficulty. In both
development and test sets, 50% of sentence pairs are
true entailments and 50% are not.
4.2 Method
We run our system for different configurations of in-
ference rules and evidence combiners. For distri-
butional inference rules (DIR), three different lev-
els are tested: without inference rules (no DIR),
inference rules for individual words (word DIR),
and inference rules for words and phrases (phrase
DIR). Phrase vectors were built using vector addi-
tion, as point-wise multiplication performed slightly
worse. To combine evidence for the result() query,
three different options are available: without av-
erage combiner which is just using Deterministic
AND (Deterministic AND), average combiner with
variable binding (AvgComb) and average combiner
without variable binding (AvgComb w/o VarBind).
Different combinations of configurations are tested
according to its suitability for the task; RTE and
STS.
We also tested several ?distributional only? sys-
tems. The first such system builds a vector represen-
tation for each sentence by adding its word vectors,
then computes the cosine similarity between the sen-
tence vectors for S1 and S2 (VS-Add). The second
uses point-wise multiplication instead of vector ad-
dition (VS-Mul). The third scales pairwise words
similarities to the sentence level using weighted av-
erage where weights are inverse document frequen-
cies idf as suggested by Mihalcea et al (2006) (VS-
Pairwise).
For the RTE task, systems were evaluated using
both accuracy and confidence-weighted score (cws)
as used by Bos and Markert (2005) and the RTE-
1 challenge (Dagan et al, 2005). In order to map
a probability of entailment to a strict prediction of
True or False, we determined a threshold that op-
timizes performance on the development set. The
cws score rewards a system?s ability to assign higher
confidence scores to correct predictions than incor-
rect ones. For cws, a system?s predictions are sorted
in decreasing order of confidence and the score is
computed as:
cws = 1n
n?
i=1
#correct-up-to-rank-i
i
where n is the number of the items in the test set,
and i ranges over the sorted items. In our systems,
we defined the confidence value for a T-H pair as
the distance between the computed probability for
the result() predicate and the threshold.
4.3 Results
The results are shown in Table 1. They show
that the distributional only baselines perform very
poorly. In particular, they perform worse than strict
16
Method acc cws
Chance 0.50 0.50
Bos & Markert, strict 0.52 0.55
Best system in RTE-1 challenge
(Bayer et al, 2005)
0.59 0.62
VS-Add 0.49 0.53
VS-Mul 0.51 0.52
VS-Pairwise 0.50 0.50
AvgComb w/o VarBind + phrase
DIR
0.52 0.53
Deterministic AND + phrase DIR 0.57 0.57
Table 1: Results on the RTE-1 Test Set.
entailment from Bos and Markert (2005), a system
that uses only logic. This illustrates the important
role of logic-based representations for the entail-
ment task. Due to intractable memory demands of
Alchemy inference, our current system with deter-
ministic AND fails to execute on 118 of the 800 test
pairs, so, by default, the system classifies these cases
as False (non-entailing) with very low confidence.
Comparing the two configurations of our system,
using deterministic AND vs. the average combiner
without variable binding (last two lines in Table 1),
we see that for RTE, it is essential to retain the full
logical form.
Our system with deterministic AND obtains both
an accuracy and cws of 0.57. The best result in
the RTE-1 challenge by Bayer et al (2005) ob-
tained an accuracy of 0.59 and cws of 0.62. 5 In
terms of both accuracy and cws, our system outper-
forms both ?distributional only? systems and strict
logical entailment, showing again that integrating
both logical form and distributional inference rules
using MLNs is beneficial. Interestingly, the strict
entailment system of Bos and Markert incorporated
generic knowledge, lexical knowledge (from Word-
Net) and geographical knowledge that we do not
utilize. This demonstrates the advantage of us-
ing a model that operationalizes entailment between
words and phrases as distributional similarity.
5On other RTE datasets there are higher previous results.
Hickl (2008) achieves 0.89 accuracy and 0.88 cws on the com-
bined RTE-2 and RTE-3 dataset.
5 Task 2: Semantic Textual Similarity
5.1 Dataset
The dataset we use in our experiments is the MSR
Video Paraphrase Corpus (MSR-Vid) subset of the
STS 2012 task, consisting of 1,500 sentence pairs.
The corpus itself was built by asking annotators
from Amazon Mechanical Turk to describe very
short video fragments (Chen and Dolan, 2011). The
organizers of the STS 2012 task (Agirre et al, 2012)
sampled video descriptions and asked Turkers to as-
sign similarity scores (ranging from 0 to 5) to pairs
of sentences, without access to the video. The gold
standard score is the average of the Turkers? annota-
tions. In addition to the MSR Video Paraphrase Cor-
pus subset, the STS 2012 task involved data from
machine translation and sense descriptions. We do
not use these because they do not consist of full
grammatical sentences, which the parser does not
handle well. In addition, the STS 2012 data included
sentences from the MSR Paraphrase Corpus, which
we also do not currently use because some sentences
are long and create intractable MLN inference prob-
lems. This issue is discussed further in section 6.
Following STS standards, our evaluation compares
a system?s similarity judgments to the gold standard
scores using Pearson?s correlation coefficient r.
5.2 Method
Our system can be tested for different configuration
of inference rules and evidence combiners which
are explained in section 4.2. The tested systems on
the STS task are listed in table 2. Out experiments
showed that using average combiner (AvgComb) is
very memory intensive and MLN inference for 28 of
the 1,500 pairs either ran out of memory or did not
finish in reasonable time. In such cases, we back off
to AvgComb w/o VarBind.
We compare to several baselines; our MLN
system without distributional inference rules
(AvgComb + no DIR), and distributional-only
systems (VS-Add, VS-Mul, VS-Pairwise). These
are the natural baselines for our system, since they
use only one of the two types of information that
we combine (i.e. logical form and distributional
representations).
Finally, we built an ensemble that combines the
output of multiple systems using regression trained
17
Method r
AvgComb + no DIR 0.58
AvgComb + word DIR 0.60
AvgComb + phrase DIR 0.66
AvgComb w/o VarBind + no DIR 0.58
AvgComb w/o VarBind + word DIR 0.60
AvgComb w/o VarBind + phrase DIR 0.73
VS-Add 0.78
VS-Mul 0.58
VS-Pairwise 0.77
Ensemble (VS-Add + VS-Mul + VS-
Pairwise)
0.83
Ensemble ([AvgComb + phrase DIR] +
VS-Add + VS-Mul + VS-Pairwise)
0.85
Best MSR-Vid score in STS 2012 (Ba?r
et al, 2012)
0.87
Table 2: Results on the STS video dataset.
on the training data. We then compare the perfor-
mance of an ensemble with and without our sys-
tem. This is the same technique used by Ba?r et al
(2012) except we used additive regression (Fried-
man, 2002) instead of linear regression since it gave
better results.
5.3 Results
Table 2 summarizes the results of our experiments.
They show that adding distributional information
improves results, as expected, and also that adding
phrase rules gives further improvement: Using only
word distributional inference rules improves results
from 0.58 to 0.6, and adding phrase inference rules
further improves them to 0.66. As for variable bind-
ing, note that although it provides more precise in-
formation, the STS scores actually improve when it
is dropped, from 0.66 to 0.73. We offer two expla-
nations for this result: First, this information is very
sensitive to parsing errors, and the C&C parser, on
which Boxer is based, produces many errors on this
dataset, even for simple sentences. When the C&C
CCG parse is wrong, the resulting logical form is
wrong, and the resulting similarity score is greatly
affected. Dropping variable binding makes the sys-
tems more robust to parsing errors. Second, in con-
trast to RTE, the STS dataset does not really test the
important role of syntax and logical form in deter-
mining meaning. This also explains why the ?dis-
tributional only? baselines are actually doing better
than the MLN systems.
Although the MLN system on its own does not
perform better than the distributional compositional
models, it does provide complementary information,
as shown by the fact that ensembling it with the rest
of the models improves performance (0.85 with the
MLN system, compared to 0.83 without it). The per-
formance of this ensemble is close to the current best
result for this dataset (0.87).
6 Future Work
The approach presented in this paper constitutes a
step towards achieving the challenging goal of effec-
tively combining logical representations with dis-
tributional information automatically acquired from
text. In this section, we discuss some of limita-
tions of the current work and directions for future
research.
As noted before, parse errors are currently a sig-
nificant problem. We use Boxer to obtain a logi-
cal representation for a sentence, which in turn re-
lies on the C&C parser. Unfortunately, C&C mis-
parses many sentences, which leads to inaccurate
logical forms. To reduce the impact of misparsing,
we plan to use a version of C&C that can produce
the top-n parses together with parse re-ranking (Ng
and Curran, 2012). As an alternative to re-ranking,
one could obtain logical forms for each of the top-
n parses, and create an MLN that integrates all of
them (together with their certainty) as an underspec-
ified meaning representation that could then be used
to directly support inferences such as STS and RTE.
We also plan to exploit a greater variety of dis-
tributional inference rules. First, we intend to in-
corporate logical form translations of existing dis-
tributional inference rule collections (e.g., (Berant
et al, 2011; Chan et al, 2011)). Another issue
is obtaining improved rule weights based on dis-
tributional phrase vectors. We plan to experiment
with more sophisticated approaches to computing
phrase vectors such as those recently presented by
Baroni and Zamparelli (2010) and Grefenstette and
Sadrzadeh (2011). Furthermore, we are currently
deriving symmetric similarity ratings between word
pairs or phrase pairs, when really what we need is di-
18
rectional similarity. We plan to incorporate directed
similarity measures such as those of Kotlerman et al
(2010) and Clarke (2012).
A primary problem for our approach is the limita-
tions of existing MLN inference algorithms, which
do not effectively scale to large and complex MLNs.
We plan to explore ?coarser? logical representa-
tions such as Minimal Recursion Semantics (MRS)
(Copestake et al, 2005). Another potential approach
to this problem is to trade expressivity for efficiency.
Domingos and Webb (2012) introduced a tractable
subset of Markov Logic (TML) for which a future
software release is planned. Formulating the infer-
ence problem in TML could potentially allow us to
run our system on longer and more complex sen-
tences.
7 Conclusion
In this paper we have used an approach that com-
bines logic-based and distributional representations
for natural language meaning. It uses logic as
the primary representation, transforms distributional
similarity judgments to weighted inference rules,
and uses Markov Logic Networks to perform in-
ferences over the weighted clauses. This approach
views textual entailment and sentence similarity as
degrees of ?logical? entailment, while at the same
time using distributional similarity as an indicator
of entailment at the word and short phrase level. We
have evaluated the framework on two different tasks,
RTE and STS, finding that it is able to handle both
tasks given that we adapt the way evidence is com-
bined in the MLN. Even though other entailment
models could be applied to STS, given that similar-
ity can obviously be operationalized as a degree of
mutual entailment, this has not been done before to
our best knowledge. Our framework achieves rea-
sonable results on both tasks. On RTE-1 we obtain
an accuracy of 0.57. On STS, we obtain a correla-
tion of r = 0.66 with full logic, r = 0.73 in a system
with weakened variable binding, and r = 0.85 in an
ensemble model. We find that distributional word
and phrase similarity, used as textual inference rules
on the fly, leads to sizeable improvements on both
tasks. We also find that using more flexible proba-
bilistic combinations of evidence is crucial for STS.
Acknowledgements
This research was supported in part by the NSF CA-
REER grant IIS 0845925, by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026, by
MURI ARO grant W911NF-08-1-0242 and by an
NDSEG grant. Any opinions, findings, and conclu-
sions or recommendations expressed in this material
are those of the author and do not necessarily reflect
the view of DARPA, AFRL, ARO, DoD or the US
government.
Some of our experiments were run on the
Mastodon Cluster supported by NSF Grant EIA-
0303609.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pi-
lot on semantic textual similarity. In Proceedings of
SemEval.
Daniel Ba?r, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. In SemEval-2012.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193, Cambridge, MA, October. Association for
Computational Linguistics.
Samuel Bayer, John Burger, Lisa Ferro, John Hender-
son, and Alexander Yeh. 2005. MITREs Submissions
to the EU Pascal RTE Challenge. In In Proceedings
of the PASCAL Challenges Workshop on Recognising
Textual Entailment, pages 41?44.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of ACL, Portland, OR.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceedings
of EMNLP 2005, pages 628?635, Vancouver, B.C.,
Canada.
Johan Bos. 2008. Wide-coverage semantic analysis with
boxer. In Johan Bos and Rodolfo Delmonte, editors,
Semantics in Text Processing. STEP 2008 Conference
Proceedings, Research in Computational Semantics,
pages 277?286. College Publications.
Tsz Ping Chan, Chris Callison-Burch, and Benjamin
Van Durme. 2011. Reranking bilingually extracted
19
paraphrases using monolingual distributional similar-
ity. In Proceedings of the GEMS 2011 Workshop on
GEometrical Models of Natural Language Semantics,
pages 33?42, Edinburgh, UK.
David L. Chen and William B. Dolan. 2011. Collect-
ing highly parallel data for paraphrase evaluation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 190?200,
Portland, Oregon, USA, June.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In Proceed-
ings of ACL 2004, pages 104?111, Barcelona, Spain.
Daoud Clarke. 2012. A context-theoretic framework for
compositionality in distributional semantics. Compu-
tational Linguistics, 38(1).
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A
Sag. 2005. Minimal recursion semantics: An intro-
duction. Research on Language and Computation,
3(2-3):281?332.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entailment
Challenge. In In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
pages 1?8.
Pedro Domingos and Daniel Lowd. 2009. Markov
Logic: An Interface Layer for Artificial Intelligence.
Synthesis Lectures on Artificial Intelligence and Ma-
chine Learning. Morgan & Claypool Publishers.
Pedro Domingos and W Austin Webb. 2012. A tractable
first-order probabilistic logic. In Proceedings of the
Twenty-Sixth National Conference on Artificial Intel-
ligence.
Katrin Erk and Sebastian Pado?. 2008. A structured vec-
tor space model for word meaning in context. In Pro-
ceedings of EMNLP 2008, pages 897?906, Honolulu,
HI.
Jerome H Friedman. 2002. Stochastic gradient boosting.
Computational Statistics & Data Analysis, 38(4):367?
378.
Dan Garrette, Katrin Erk, and Raymond Mooney. 2011.
Integrating logical representations with probabilistic
information using markov logic. In Proceedings of
IWCS, Oxford, UK.
Dan Garrette, Katrin Erk, and Raymond Mooney. 2013.
A formal approach to linking logical form and vector-
space lexical semantics. In Harry Bunt, Johan Bos,
and Stephen Pulman, editors, Computing Meaning,
Vol. 4.
Lise Getoor and Ben Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
David Graff, Junbo Kong, Ke Chen, and Kazuaki
Maeda. 2007. English Gigaword Third Edi-
tion. http://www.ldc.upenn.edu/
Catalog/CatalogEntry.jsp?catalogId=
LDC2007T07.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
EMNLP, Edinburgh, Scotland, UK.
Andrew Hickl. 2008. Using Discourse Commitments
to Recognize Textual Entailment. In Proceedings of
COLING 2008, pages 337?344.
Jerry R. Hobbs, Mark Stickel, Douglas Appelt, and Paul
Martin. 1993. Interpretation as abduction. Artificial
Intelligence, 63(1?2):69?142.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic; An Introduction to Modeltheoretic Semantics of
Natural Language, Formal Logic and DRT. Kluwer,
Dordrecht.
Stanley Kok, Parag Singla, Matthew Richardson, and Pe-
dro Domingos. 2005. The Alchemy system for sta-
tistical relational AI. Technical report, Department
of Computer Science and Engineering, University
of Washington. http://www.cs.washington.
edu/ai/alchemy.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distributional
similarity for lexical inference. Natural Language En-
gineering, 16(04):359?389.
Thomas Landauer and Susan Dumais. 1997. A solution
to Platos problem: the latent semantic analysis theory
of acquisition, induction, and representation of knowl-
edge. Psychological Review, 104(2):211?240.
Dekang Lin and Patrick Pantel. 2001a. DIRT - discovery
of inference rules from text. In In Proceedings of the
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323?328.
Dekang Lin and Patrick Pantel. 2001b. Discovery of
inference rules for question answering. Natural Lan-
guage Engineering, 7(4):343?360.
Will Lowe. 2001. Towards a theory of semantic space.
In Proceedings of the Cognitive Science Society, pages
576?581.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instruments,
and Computers, 28:203?208.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
page 775. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236?244.
20
Jeff Mitchell and Mirella Lapata. 2010. Composition in
distributional models of semantics. Cognitive Science,
34(8):1388?1429.
Richard Montague. 1970. Universal grammar. Theoria,
36:373?398. Reprinted in Thomason (1974), pp 7-27.
Sriraam Natarajan, Tushar Khot, Daniel Lowd, Prasad
Tadepalli, Kristian Kersting, and Jude Shavlik. 2010.
Exploiting causal independence in markov logic net-
works: Combining undirected and directed models.
In Proceedings of European Conference in Machine
Learning (ECML), Barcelona, Spain.
Dominick Ng and James R Curran. 2012. Dependency
hashing for n-best ccg parsing. In In Proceedings of
the 50th Annual Meeting of the Association for Com-
putational Linguistics.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning, 62:107?
136.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1).
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew Ng,
and Christopher Manning. 2011. Dynamic pooling
and unfolding recursive autoencoders for paraphrase
detection. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett,
F.C.N. Pereira, and K.Q. Weinberger, editors, Pro-
ceedings of NIPS.
Asher Stern, Amnon Lotan, Shachar Mirkin, Eyal
Shnarch, Lili Kotlerman, Jonathan Berant, and Ido Da-
gan. 2011. Knowledge and tree-edits in learnable en-
tailment proofs. In TAC, Gathersburg, MD.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING.
Stefan Thater, Hagen Fu?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of ACL 2010, pages 948?957, Uppsala, Sweden.
Richmond H. Thomason, editor. 1974. Formal Philoso-
phy. Selected Papers of Richard Montague. Yale Uni-
versity Press, New Haven.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
21
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 796?801,
Dublin, Ireland, August 23-24, 2014.
UTexas: Natural Language Semantics using Distributional Semantics and
Probabilistic Logic
Islam Beltagy
?
, Stephen Roller
?
, Gemma Boleda
?
, Katrin Erk
?
, Raymond J. Mooney
?
?
Department of Computer Science
?
Department of Linguistics
The University of Texas at Austin
{beltagy, roller, mooney}@cs.utexas.edu
gemma.boleda@upf.edu, katrin.erk@mail.utexas.edu
Abstract
We represent natural language semantics
by combining logical and distributional in-
formation in probabilistic logic. We use
Markov Logic Networks (MLN) for the
RTE task, and Probabilistic Soft Logic
(PSL) for the STS task. The system is
evaluated on the SICK dataset. Our best
system achieves 73% accuracy on the RTE
task, and a Pearson?s correlation of 0.71 on
the STS task.
1 Introduction
Textual Entailment systems based on logical infer-
ence excel in correct reasoning, but are often brit-
tle due to their inability to handle soft logical in-
ferences. Systems based on distributional seman-
tics excel in lexical and soft reasoning, but are un-
able to handle phenomena like negation and quan-
tifiers. We present a system which takes the best
of both approaches by combining distributional se-
mantics with probabilistic logical inference.
Our system builds on our prior work (Belt-
agy et al., 2013; Beltagy et al., 2014a; Beltagy
and Mooney, 2014; Beltagy et al., 2014b). We
use Boxer (Bos, 2008), a wide-coverage semantic
analysis tool to map natural sentences to logical
form. Then, distributional information is encoded
in the form of inference rules. We generate lexical
and phrasal rules, and experiment with symmetric
and asymmetric similarity measures. Finally, we
use probabilistic logic frameworks to perform in-
ference, Markov Logic Networks (MLN) for RTE,
and Probabilistic Soft Logic (PSL) for STS.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
2 Background
2.1 Logical Semantics
Logic-based representations of meaning have a
long tradition (Montague, 1970; Kamp and Reyle,
1993). They handle many complex semantic phe-
nomena such as relational propositions, logical
operators, and quantifiers; however, they can not
handle ?graded? aspects of meaning in language
because they are binary by nature.
2.2 Distributional Semantics
Distributional models use statistics of word co-
occurrences to predict semantic similarity of
words and phrases (Turney and Pantel, 2010;
Mitchell and Lapata, 2010), based on the obser-
vation that semantically similar words occur in
similar contexts. Words are represented as vec-
tors in high dimensional spaces generated from
their contexts. Also, it is possible to compute vec-
tor representations for larger phrases composition-
ally from their parts (Mitchell and Lapata, 2008;
Mitchell and Lapata, 2010; Baroni and Zampar-
elli, 2010). Distributional similarity is usually a
mixture of semantic relations, but particular asym-
metric similarity measures can, to a certain ex-
tent, predict hypernymy and lexical entailment
distributionally (Kotlerman et al., 2010; Lenci and
Benotto, 2012; Roller et al., 2014). Distribu-
tional models capture the graded nature of mean-
ing, but do not adequately capture logical struc-
ture (Grefenstette, 2013).
2.3 Markov Logic Network
Markov Logic Networks (MLN) (Richardson and
Domingos, 2006) are a framework for probabilis-
tic logic that employ weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks). Weighting the rules is a way of soft-
ening them compared to hard logical constraints.
796
MLNs define a probability distribution over pos-
sible worlds, where the probability of a world in-
creases exponentially with the total weight of the
logical clauses that it satisfies. A variety of in-
ference methods for MLNs have been developed,
however, computational overhead is still an issue.
2.4 Probabilistic Soft Logic
Probabilistic Soft Logic (PSL) is another recently
proposed framework for probabilistic logic (Kim-
mig et al., 2012). It uses logical representations to
compactly define large graphical models with con-
tinuous variables, and includes methods for per-
forming efficient probabilistic inference for the re-
sulting models. A key distinguishing feature of
PSL is that ground atoms (i.e., atoms without vari-
ables) have soft, continuous truth values on the
interval [0, 1] rather than binary truth values as
used in MLNs and most other probabilistic logics.
Given a set of weighted inference rules, and with
the help of Lukasiewicz?s relaxation of the logical
operators, PSL builds a graphical model defining a
probability distribution over the continuous space
of values of the random variables in the model
(Kimmig et al., 2012). Then, PSL?s MPE infer-
ence (Most Probable Explanation) finds the overall
interpretation with the maximum probability given
a set of evidence. This optimization problem is a
second-order cone program (SOCP) (Kimmig et
al., 2012) and can be solved in polynomial time.
2.5 Recognizing Textual Entailment
Recognizing Textual Entailment (RTE) is the task
of determining whether one natural language text,
the premise, Entails, Contradicts, or is not related
(Neutral) to another, the hypothesis.
2.6 Semantic Textual Similarity
Semantic Textual Similarity (STS) is the task of
judging the similarity of a pair of sentences on
a scale from 1 to 5 (Agirre et al., 2012). Gold
standard scores are averaged over multiple human
annotations and systems are evaluated using the
Pearson correlation between a system?s output and
gold standard scores.
3 Approach
3.1 Logical Representation
The first component in the system is Boxer (Bos,
2008), which maps the input sentences into logical
form, in which the predicates are words in the sen-
tence. For example, the sentence ?A man is driving
a car? in logical form is:
?x, y, z. man(x) ? agent(y, x) ? drive(y) ?
patient(y, z) ? car(z)
3.2 Distributional Representation
Next, distributional information is encoded in
the form of weighted inference rules connecting
words and phrases of the input sentences T and H .
For example, for sentences T : ?A man is driving
a car?, and H: ?A guy is driving a vehicle?, we
would like to generate rules like ?x. man(x) ?
guy(x) |w
1
, ?x.car(x)? vehicle(x) |w
2
, where
w
1
and w
2
are weights indicating the similarity of
the antecedent and consequent of each rule.
Inferences rules are generated as in Beltagy et
al. (2013). Given two input sentences T and H ,
for all pairs (a, b), where a and b are words or
phrases of T and H respectively, generate an infer-
ence rule: a ? b | w, where the rule weight w is
a function of sim(
??
a ,
??
b ), and sim is a similarity
measure of the distributional vectors
??
a ,
??
b . We
experimented with the symmetric similarity mea-
sure cosine, and asym, the supervised, asymmet-
ric similarity measure of Roller et al. (2014).
The asym measure uses the vector difference
(
??
a ?
??
b ) as features in a logistic regression clas-
sifier for distinguishing between four different
word relations: hypernymy, cohyponymy, meron-
omy, and no relation. The model is trained us-
ing the noun-noun subset of the BLESS data set
(Baroni and Lenci, 2011). The final similarity
weight is given by the model?s estimated probabil-
ity that the word relationship is either hypernymy
or meronomy: asym(
??
a ,
??
b ) = P (hyper(a, b))+
P (mero(a, b)).
Distributional representations for words are de-
rived by counting co-occurrences in the ukWaC,
WaCkypedia, BNC and Gigaword corpora. We
use the 2000 most frequent content words as ba-
sis dimensions, and count co-occurrences within
a two word context window. The vector space is
weighted using Positive Pointwise Mutual Infor-
mation.
Phrases are defined in terms of Boxer?s output
to be more than one unary atom sharing the same
variable like ?a little kid? (little(k) ? kid(k)),
or two unary atoms connected by a relation like
?a man is driving? (man(m) ? agent(d,m) ?
drive(d)). We compute vector representations of
797
phrases using vector addition across the compo-
nent predicates. We also tried computing phrase
vectors using component-wise vector multiplica-
tion (Mitchell and Lapata, 2010), but found it per-
formed marginally worse than addition.
3.3 Probabilistic Logical Inference
The last component is probabilistic logical infer-
ence. Given the logical form of the input sen-
tences, and the weighted inference rules, we use
them to build a probabilistic logic program whose
solution is the answer to the target task. A proba-
bilistic logic program consists of the evidence set
E, the set of weighted first order logical expres-
sions (rule base RB), and a query Q. Inference is
the process of calculating Pr(Q|E,RB).
3.4 Task 1: RTE using MLNs
MLNs are the probabilistic logic framework we
use for the RTE task (we do not use PSL here as
it shares the problems of fuzzy logic with proba-
bilistic reasoning). The RTE classification prob-
lem for the relation between T and H can be
split into two inference tasks. The first is test-
ing if T entails H , Pr(H|T,RB). The second
is testing if the negation of the text ?T entails H ,
Pr(H|?T,RB). In case Pr(H|T,RB) is high,
while Pr(H|?T,RB) is low, this indicates En-
tails. In case it is the other way around, this in-
dicates Contradicts. If both values are close, this
means T does not affect the probability of H and
indicative of Neutral. We train an SVM classifier
with LibSVM?s default parameters to map the two
probabilities to the final decision.
The MLN implementation we use is
Alchemy (Kok et al., 2005). Queries in Alchemy
can only be ground atoms. However, in our
case the query is a complex formula (H). We
extended Alchemy to calculate probabilities of
queries (Beltagy and Mooney, 2014). Probability
of a formula Q given an MLN K equals the ratio
between the partition function Z of the ground
network of K with and without Q added as a hard
rule (Gogate and Domingos, 2011)
P (Q | K) =
Z(K ? {(Q,?)})
Z(K)
(1)
We estimate Z of the ground networks using Sam-
pleSearch (Gogate and Dechter, 2011), an ad-
vanced importance sampling algorithm that is suit-
able for ground networks generated by MLNs.
A general problem with MLN inference is
its computational overhead, especially for the
complex logical formulae generated by our ap-
proach. To make inference faster, we reduce the
size of the ground network through an automatic
type-checking technique proposed in Beltagy and
Mooney (2014). For example, consider the ev-
idence ground atom man(M) denoting that the
constant M is of type man. Then, consider an-
other predicate like car(x). In case there are no in-
ference rule connecting man(x) and car(x), then
we know that M which we know is a man cannot
be a car, so we remove the ground atom car(M)
from the ground network. This technique reduces
the size of the ground network dramatically and
makes inference tractable.
Another problem with MLN inference is that
quantifiers sometimes behave in an undesir-
able way, due to the Domain Closure Assump-
tion (Richardson and Domingos, 2006) that MLNs
make. For example, consider the text-hypothesis
pair: ?There is a black bird? and ?All birds are
black?, which in logic are T : bird(B)?black(B)
and H : ?x. bird(x) ? black(x). Because of
the Domain Closure Assumption, MLNs conclude
that T entails H because H is true for all constants
in the domain (in this example, the single constant
B). We solve this problem by introducing extra
constants and evidence in the domain. In the ex-
ample above, we introduce evidence of a new bird
bird(D), which prevents the hypothesis from be-
ing true. The full details of the technique of deal-
ing with the domain closure is beyond the scope of
this paper.
3.5 Task 2: STS using PSL
PSL is the probabilistic logic we use for the STS
task since it has been shown to be an effective
approach for computing similarity between struc-
tured objects. We showed in Beltagy et al. (2014a)
how to perform the STS task using PSL. PSL
does not work ?out of the box? for STS, be-
cause Lukasiewicz?s equation for the conjunction
is very restrictive. We address this by replacing
Lukasiewicz?s equation for conjunction with an
averaging equation, then change the optimization
problem and grounding technique accordingly.
For each STS pair of sentences S
1
, S
2
, we run
PSL twice, once where E = S
1
, Q = S
2
and an-
other where E = S
2
, Q = S
1
, and output the two
scores. The final similarity score is produced from
798
an Additive Regression model with WEKA?s de-
fault parameters trained to map the two PSL scores
to the overall similarity score (Friedman, 1999;
Hall et al., 2009).
3.6 Task 3: RTE and STS using Vector
Spaces and Keyword Counts
As a baseline, we also attempt both the RTE and
STS tasks using only vector representations and
unigram counts. This baseline model uses a super-
vised regressor with features based on vector sim-
ilarity and keyword counts. The same input fea-
tures are used for performing RTE and STS, but a
SVM classifier and Additive Regression model is
trained separately for each task. This baseline is
meant to establish whether the task truly requires
the sophisticated logical inference of MLNs and
PSL, or if merely checking for logical keywords
and textual similarity is sufficient.
The first two features are simply the cosine and
asym similarities between the text and hypothesis,
using vector addition of the unigrams to compute
a single vector for the entire sentence.
We also compute vectors for both the text and
hypothesis using vector addition of the mutually
exclusive unigrams (MEUs). The MEUs are de-
fined as the unigrams of the premise and hypoth-
esis with common unigrams removed. For exam-
ple, if the premise is ?A dog chased a cat? and the
hypothesis is ?A dog watched a mouse?, the MEUs
are ?chased cat? and ?watched mouse.? We com-
pute vector addition of the MEUs, and compute
similarity using both the cosine and asym mea-
sures. These form two features for the regressor.
The last feature of the model is a keyword
count. We count how many times 13 different
keywords appear in either the text or the hypoth-
esis. These keywords include negation (no, not,
nobody, etc.) and quantifiers (a, the, some, etc.)
The counts of each keyword form the last 13 fea-
tures as input to the regressor. In total, there are
17 features used in this baseline system.
4 Evaluation
The dataset used for evaluation is SICK:
Sentences Involving Compositional Knowledge
dataset, a task for SemEval 2014 (Marelli et al.,
2014a; Marelli et al., 2014b). The dataset is
10,000 pairs of sentences, 5000 training and 5000
for testing. Sentences are annotated for both tasks.
SICK-RTE SICK-STS
Baseline 70.0 71.1
MLN/PSL + Cosine 72.8 68.6
MLN/PSL + Asym 73.2 68.9
Ensemble 73.2 71.5
Table 1: Test RTE accuracy and STS Correlation.
4.1 Systems Compared
We compare multiple configurations of our proba-
bilistic logic system.
? Baseline: Vector- and keyword-only baseline
described in Section 3.6;
? MLN/PSL + Cosine: MLN and PSL based
methods described in Sections 3.4 and 3.5,
using cosine as a similarity measure;
? MLN/PSL + Asym: MLN and PSL based
methods described in Sections 3.4 and 3.5,
using asym as a similarity measure;
? Ensemble: An ensemble method which uses
all of the features in the above methods as in-
puts for the RTE and STS classifiers.
4.2 Results and Discussion
Table 1 shows our results on the held-out test set
for SemEval 2014 Task 1.
On the RTE task, we see that both the MLN +
Cosine and MLN + Asym models outperformed
the Baseline, indicating that textual entailment re-
quires real inference to handle negation and quan-
tifiers. The MLN + Asym and Ensemble sys-
tems perform identically on RTE, further suggest-
ing that the logical inference subsumes keyword
detection.
The MLN + Asym system outperforms the
MLN + Cosine system, emphasizing the impor-
tance of asymmetric measures for predicting lex-
ical entailment. Intuitively, this makes perfect
sense: dog entails animal, but not vice versa.
In an error analysis performed on a development
set, we found our RTE system was extremely con-
servative: we rarely confused the Entails and Con-
tradicts classes, indicating we correctly predict the
direction of entailment, but frequently misclassify
examples as Neutral. An examination of these ex-
amples showed the errors were mostly due to miss-
ing or weakly-weighted distributional rules.
On STS, our vector space baseline outperforms
both PSL-based systems, but the ensemble outper-
forms any of its components. This is a testament to
799
the power of distributional models in their ability
to predict word and sentence similarity. Surpris-
ingly, we see that the PSL + Asym system slightly
outperforms the PSL + Cosine system. This may
indicate that even in STS, some notion of asymme-
try plays a role, or that annotators may have been
biased by simultaneously annotating both tasks.
As with RTE, the major bottleneck of our system
appears to be the knowledge base, which is built
solely using distributional inference rules.
Results also show that our system?s perfor-
mance is close to the baseline system. One of
the reasons behind that could be that sentences are
not exploiting the full power of logical represen-
tations. On RTE for example, most of the con-
tradicting pairs are two similar sentences with one
of them being negated. This way, the existence
of any negation cue in one of the two sentences is
a strong signal for contradiction, which what the
baseline system does without deeply representing
the semantics of the negation.
5 Conclusion & Future Work
We showed how to combine logical and distribu-
tional semantics using probabilistic logic, and how
to perform the RTE and STS tasks using it. The
system is tested on the SICK dataset.
The distributional side can be extended in many
directions. We would like to use longer phrases,
more sophisticated compositionality techniques,
and contextualized vectors of word meaning. We
also believe inference rules could be dramatically
improved by integrating from paraphrases collec-
tions like PPDB (Ganitkevitch et al., 2013).
Finally, MLN inference could be made more ef-
ficient by exploiting the similarities between the
two ground networks (the one with Q and the one
without). PLS inference could be enhanced by us-
ing a learned, weighted average of rules, rather
than the simple mean.
Acknowledgements
This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026.
Some experiments were run on the Mastodon
Cluster supported by NSF Grant EIA-0303609.
The authors acknowledge the Texas Advanced
Computing Center (TACC)
1
for providing grid re-
sources that have contributed to these results. We
thank the anonymous reviewers and the UTexas
1
http://www.tacc.utexas.edu
Natural Language and Learning group for their
helpful comments and suggestions.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of Semantic Evaluation (SemEval-12).
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the GEMS 2011 Workshop on GE-
ometrical Models of Natural Language Semantics,
pages 1?10, Edinburgh, UK, July. Association for
Computational Linguistics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-10).
Islam Beltagy and Raymond J. Mooney. 2014. Ef-
ficient Markov logic inference for natural language
semantics. In Proceedings of AAAI 2014 Workshop
on Statistical Relational AI (StarAI-14).
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets Markov: Deep semantics with
probabilistic logical form. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM-13).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014a. Probabilistic soft logic for semantic textual
similarity. In Proceedings of Association for Com-
putational Linguistics (ACL-14).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014b. Semantic parsing using distributional se-
mantics and probabilistic logic. In Proceedings
of ACL 2014 Workshop on Semantic Parsing (SP-
2014).
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of Semantics in Text
Processing (STEP-08).
J.H. Friedman. 1999. Stochastic gradient boosting.
Technical report, Stanford University.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT-13).
Vibhav Gogate and Rina Dechter. 2011. Sample-
search: Importance sampling in presence of deter-
minism. Artificial Intelligence, 175(2):694?729.
Vibhav Gogate and Pedro Domingos. 2011. Proba-
bilistic theorem proving. In 27th Conference on Un-
certainty in Artificial Intelligence (UAI-11).
800
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of Second Joint Conference
on Lexical and Computational Semantics (*SEM
2013).
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to Probabilistic Soft Logic.
In Proceedings of NIPS Workshop on Probabilistic
Programming: Foundations and Applications (NIPS
Workshop-12).
Stanley Kok, Parag Singla, Matthew Richardson, and
Pedro Domingos. 2005. The Alchemy system
for statistical relational AI. Technical report, De-
partment of Computer Science and Engineering,
University of Washington. http://www.cs.
washington.edu/ai/alchemy.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the first Joint Conference on Lexical
and Computational Semantics (*SEM-12).
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zam-
parelli. 2014a. SemEval-2014 task 1: Evaluation of
compositional distributional semantic models on full
sentences through semantic relatedness and textual
entailment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval-2014),
Dublin, Ireland.
Marco Marelli, Stefano Menini, Marco Baroni, Luisa
Bentivogli, Raffaella Bernardi, and Roberto Zam-
parelli. 2014b. A sick cure for the evaluation
of compositional distributional semantic models.
In Nicoletta Calzolari (Conference Chair), Khalid
Choukri, Thierry Declerck, Hrafn Loftsson, Bente
Maegaard, Joseph Mariani, Asuncion Moreno, Jan
Odijk, and Stelios Piperidis, editors, Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC?14), Reykjavik, Ice-
land, may. European Language Resources Associa-
tion (ELRA).
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of Association for Computational Linguistics (ACL-
08).
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(3):1388?1429.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36:373?398.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107?136.
Stephen Roller, Katrin Erk, and Gemma Boleda. 2014.
Inclusive yet selective: Supervised distributional hy-
pernymy detection. In Proceedings of the Twenty
Fifth International Conference on Computational
Linguistics (COLING-14), Dublin, Ireland.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37(1):141?188.
801
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 7?11,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
Semantic Parsing using Distributional Semantics and Probabilistic Logic
Islam Beltagy
?
Katrin Erk
?
Raymond Mooney
?
?
Department of Computer Science
?
Department of Linguistics
The University of Texas at Austin
Austin, Texas 78712
?
{beltagy,mooney}@cs.utexas.edu
?
katrin.erk@mail.utexas.edu
Abstract
We propose a new approach to semantic
parsing that is not constrained by a fixed
formal ontology and purely logical infer-
ence. Instead, we use distributional se-
mantics to generate only the relevant part
of an on-the-fly ontology. Sentences and
the on-the-fly ontology are represented in
probabilistic logic. For inference, we
use probabilistic logic frameworks like
Markov Logic Networks (MLN) and Prob-
abilistic Soft Logic (PSL). This seman-
tic parsing approach is evaluated on two
tasks, Textual Entitlement (RTE) and Tex-
tual Similarity (STS), both accomplished
using inference in probabilistic logic. Ex-
periments show the potential of the ap-
proach.
1 Introduction
Semantic Parsing is probably best defined as the
task of representing the meaning of a natural lan-
guage sentence in some formal knowledge repre-
sentation language that supports automated infer-
ence. A semantic parser is best defined as having
three parts, a formal language, an ontology, and an
inference mechanism. Both the formal language
(e.g. first-order logic) and the ontology define the
formal knowledge representation. The formal lan-
guage uses predicate symbols from the ontology,
and the ontology provides them with meanings by
defining the relations between them.
1
. A formal
expression by itself without an ontology is insuf-
ficient for semantic interpretation; we call it un-
interpreted logical form. An uninterpreted logical
form is not enough as a knowledge representation
1
For conciseness, here we use the term ?ontology? to refer
to a set of predicates as well as a knowledge base (KB) of
axioms that defines a complex set of relationships between
them
because the predicate symbols do not have mean-
ing in themselves, they get this meaning from the
ontology. Inference is what takes a problem repre-
sented in the formal knowledge representation and
the ontology and performs the target task (e.g. tex-
tual entailment, question answering, etc.).
Prior work in standard semantic parsing uses a
pre-defined set of predicates in a fixed ontology.
However, it is difficult to construct formal ontolo-
gies of properties and relations that have broad
coverage, and very difficult to do semantic parsing
based on such an ontology. Consequently, current
semantic parsers are mostly restricted to fairly lim-
ited domains, such as querying a specific database
(Kwiatkowski et al., 2013; Berant et al., 2013).
We propose a semantic parser that is not re-
stricted to a predefined ontology. Instead, we
use distributional semantics to generate the needed
part of an on-the-fly ontology. Distributional se-
mantics is a statistical technique that represents
the meaning of words and phrases as distributions
over context words (Turney and Pantel, 2010; Lan-
dauer and Dumais, 1997). Distributional infor-
mation can be used to predict semantic relations
like synonymy and hyponymy between words and
phrases of interest (Lenci and Benotto, 2012;
Kotlerman et al., 2010). The collection of pre-
dicted semantic relations is the ?on-the-fly ontol-
ogy? our semantic parser uses. A distributional
semantics is relatively easy to build from a large
corpus of raw text, and provides the wide cover-
age that formal ontologies lack.
The formal language we would like to use in the
semantic parser is first-order logic. However, dis-
tributional information is graded in nature, so the
on-the-fly ontology and its predicted semantic re-
lations are also graded. This means, that standard
first-order logic is insufficient because it is binary
by nature. Probabilistic logic solves this problem
because it accepts weighted first order logic for-
mulas. For example, in probabilistic logic, the
7
synonymy relation between ?man? and ?guy? is
represented by: ?x. man(x) ? guy(x) | w
1
and
the hyponymy relation between ?car? and ?vehi-
cle? is: ?x. car(x) ? vehicle(x) | w
2
where w
1
and w
1
are some certainty measure estimated from
the distributional semantics.
For inference, we use probabilistic logic
frameworks like Markov Logic Networks
(MLN) (Richardson and Domingos, 2006) and
Probabilistic Soft Logic (PSL) (Kimmig et al.,
2012). They are Statistical Relational Learning
(SRL) techniques (Getoor and Taskar, 2007) that
combine logical and statistical knowledge in one
uniform framework, and provide a mechanism for
coherent probabilistic inference. We implemented
this semantic parser (Beltagy et al., 2013; Beltagy
et al., 2014) and used it to perform two tasks
that require deep semantic analysis, Recognizing
Textual Entailment (RTE), and Semantic Textual
Similarity (STS).
The rest of the paper is organized as follows:
section 2 presents background material, section
3 explains the three components of the semantic
parser, section 4 shows how this semantic parser
can be used for RTE and STS tasks, section 5
presents the evaluation and 6 concludes.
2 Background
2.1 Logical Semantics
Logic-based representations of meaning have a
long tradition (Montague, 1970; Kamp and Reyle,
1993). They handle many complex semantic phe-
nomena such as relational propositions, logical
operators, and quantifiers; however, they can not
handle ?graded? aspects of meaning in language
because they are binary by nature. Also, the logi-
cal predicates and relations do not have semantics
by themselves without an accompanying ontology,
which we want to replace in our semantic parser
with distributional semantics.
To map a sentence to logical form, we use Boxer
(Bos, 2008), a tool for wide-coverage semantic
analysis that produces uninterpreted logical forms
using Discourse Representation Structures (Kamp
and Reyle, 1993). It builds on the C&C CCG
parser (Clark and Curran, 2004).
2.2 Distributional Semantics
Distributional models use statistics on contextual
data from large corpora to predict semantic sim-
ilarity of words and phrases (Turney and Pantel,
2010; Mitchell and Lapata, 2010), based on the
observation that semantically similar words occur
in similar contexts (Landauer and Dumais, 1997;
Lund and Burgess, 1996). So words can be rep-
resented as vectors in high dimensional spaces
generated from the contexts in which they occur.
Distributional models capture the graded nature
of meaning, but do not adequately capture log-
ical structure (Grefenstette, 2013). It is possi-
ble to compute vector representations for larger
phrases compositionally from their parts (Lan-
dauer and Dumais, 1997; Mitchell and Lapata,
2008; Mitchell and Lapata, 2010; Baroni and
Zamparelli, 2010; Grefenstette and Sadrzadeh,
2011). Distributional similarity is usually a mix-
ture of semantic relations, but particular asymmet-
ric similarity measures can, to a certain extent,
predict hypernymy and lexical entailment distri-
butionally (Lenci and Benotto, 2012; Kotlerman
et al., 2010).
2.3 Markov Logic Network
Markov Logic Network (MLN) (Richardson and
Domingos, 2006) is a framework for probabilis-
tic logic that employ weighted formulas in first-
order logic to compactly encode complex undi-
rected probabilistic graphical models (i.e., Markov
networks). Weighting the rules is a way of soft-
ening them compared to hard logical constraints.
MLNs define a probability distribution over possi-
ble worlds, where a world?s probability increases
exponentially with the total weight of the logical
clauses that it satisfies. A variety of inference
methods for MLNs have been developed, however,
their computational complexity is a fundamental
issue.
2.4 Probabilistic Soft Logic
Probabilistic Soft Logic (PSL) is another recently
proposed framework for probabilistic logic (Kim-
mig et al., 2012). It uses logical representations to
compactly define large graphical models with con-
tinuous variables, and includes methods for per-
forming efficient probabilistic inference for the re-
sulting models. A key distinguishing feature of
PSL is that ground atoms have soft, continuous
truth values in the interval [0, 1] rather than bi-
nary truth values as used in MLNs and most other
probabilistic logics. Given a set of weighted in-
ference rules, and with the help of Lukasiewicz?s
relaxation of the logical operators, PSL builds a
graphical model defining a probability distribution
8
over the continuous space of values of the random
variables in the model. Then, PSL?s MPE infer-
ence (Most Probable Explanation) finds the over-
all interpretation with the maximum probability
given a set of evidence. It turns out that this op-
timization problem is second-order cone program
(SOCP) (Kimmig et al., 2012) and can be solved
efficiently in polynomial time.
2.5 Recognizing Textual Entailment
Recognizing Textual Entailment (RTE) is the task
of determining whether one natural language text,
the premise, Entails, Contradicts, or not related
(Neutral) to another, the hypothesis.
2.6 Semantic Textual Similarity
Semantic Textual Similarity (STS) is the task of
judging the similarity of a pair of sentences on
a scale from 1 to 5 (Agirre et al., 2012). Gold
standard scores are averaged over multiple human
annotations and systems are evaluated using the
Pearson correlation between a system?s output and
gold standard scores.
3 Approach
A semantic parser is three components, a formal
language, an ontology, and an inference mecha-
nism. This section explains the details of these
components in our semantic parser. It also points
out the future work related to each part of the sys-
tem.
3.1 Formal Language: first-order logic
Natural sentences are mapped to logical form us-
ing Boxer (Bos, 2008), which maps the input
sentences into a lexically-based logical form, in
which the predicates are words in the sentence.
For example, the sentence ?A man is driving a car?
in logical form is:
?x, y, z. man(x) ? agent(y, x) ? drive(y) ?
patient(y, z) ? car(z)
We call Boxer?s output alone an uninterpreted
logical form because predicates do not have mean-
ing by themselves. They still need to be connected
with an ontology.
Future work: While Boxer has wide coverage,
additional linguistic phenomena like generalized
quantifiers need to be handled.
3.2 Ontology: on-the-fly ontology
Distributional information is used to generate the
needed part of an on-the-fly ontology for the given
input sentences. It is encoded in the form of
weighted inference rules describing the seman-
tic relations connecting words and phrases in the
input sentences. For example, for sentences ?A
man is driving a car?, and ?A guy is driving a
vehicle?, we would like to generate rules like
?x.man(x)? guy(x) |w
1
indicating that ?man?
and ?guy? are synonyms with some certainty w
1
,
and ?x. car(x)? vehicle(x) | w
2
indicating that
?car? is a hyponym of ?vehicle? with some cer-
tainty w
2
. Other semantic relations can also be
easily encoded as inference rules like antonyms
?x. tall(x)? ?short(x) |w, contextonymy rela-
tion ?x. hospital(x) ? ?y. doctor(y) | w. For
now, we generate inference rules only as syn-
onyms (Beltagy et al., 2013), but we are experi-
menting with more types of semantic relations.
In (Beltagy et al., 2013), we generate infer-
ence rules between all pairs of words and phrases.
Given two input sentences T and H , for all pairs
(a, b), where a and b are words or phrases of T
and H respectively, generate an inference rule:
a ? b | w, where the rule?s weight w =
sim(
??
a ,
??
b ), and sim is the cosine of the angle
between vectors
??
a and
??
b . Note that this simi-
larity measure cannot yet distinguish relations like
synonymy and hypernymy. Phrases are defined in
terms of Boxer?s output to be more than one unary
atom sharing the same variable like ?a little kid?
which in logic is little(k) ? kid(k), or two unary
atoms connected by a relation like ?a man is driv-
ing? which in logic is man(m) ? agent(d,m) ?
drive(d). We used vector addition (Mitchell and
Lapata, 2010) to calculate vectors for phrases.
Future Work: This can be extended in many
directions. We are currently experimenting with
asymmetric similarity functions to distinguish se-
mantic relations. We would also like to use longer
phrases and other compositionality techniques as
in (Baroni and Zamparelli, 2010; Grefenstette and
Sadrzadeh, 2011). Also more inference rules can
be added from paraphrases collections like PPDB
(Ganitkevitch et al., 2013).
3.3 Inference: probabilistic logical inference
The last component is probabilistic logical infer-
ence. Given the logical form of the input sen-
tences, and the weighted inference rules, we use
them to build a probabilistic logic program whose
solution is the answer to the target task. A proba-
bilistic logic program consists of the evidence set
9
E, the set of weighted first order logical expres-
sions (rule base RB), and a query Q. Inference is
the process of calculating Pr(Q|E,RB).
Probabilistic logic frameworks define a proba-
bility distribution over all possible worlds. The
number of constants in a world depends on the
number of the discourse entities in the Boxer out-
put, plus additional constants introduced to han-
dle quantification. Mostly, all constants are com-
bined with all literals, except for rudimentary type
checking.
4 Tasks
This section explains how we perform the RTE
and STS tasks using our semantic parser.
4.1 Task 1: RTE using MLNs
MLNs are the probabilistic logic framework we
use for the RTE task (we do not use PSL here as
it shares the problems of fuzzy logic with proba-
bilistic reasoning). The RTE?s classification prob-
lem for the relation between T and H , and given
the rule base RB generated as in 3.2, can be
split into two inference tasks. The first is find-
ing if T entails H , Pr(H|T,RB). The second
is finding if the negation of the text ?T entails H ,
Pr(H|?T,RB). In case Pr(H|T,RB) is high,
while Pr(H|?T,RB) is low, this indicates En-
tails. In case it is the other way around, this indi-
cates Contradicts. If both values are close to each
other, this means T does not affect probability of
H and that is an indication of Neutral. We train a
classifier to map the two values to the final classi-
fication decision.
Future Work: One general problem with
MLNs is its computational overhead especially
for the type of inference problems we have. The
other problem is that MLNs, as with most other
probabilistic logics, make the Domain Closure
Assumption (Richardson and Domingos, 2006)
which means that quantifiers sometimes behave in
an undesired way.
4.2 Task 2: STS using PSL
PSL is the probabilistic logic we use for the STS
task since it has been shown to be an effective
approach to compute similarity between struc-
tured objects. PSL does not work ?out of the
box? for STS, because Lukasiewicz?s equation for
the conjunction is very restrictive. We addressed
this problem (Beltagy et al., 2014) by replacing
SICK-RTE SICK-STS
dist 0.60 0.65
logic 0.71 0.68
logic+dist 0.73 0.70
Table 1: RTE accuracy and STS Correlation
Lukasiewicz?s equation for the conjunction with
an averaging equation, then change the optimiza-
tion problem and the grounding technique accord-
ingly.
For each STS pair of sentences S
1
, S
2
, we run
PSL twice, once where E = S
1
, Q = S
2
and
another where E = S
2
, Q = S
1
, and output the
two scores. The final similarity score is produced
from a regressor trained to map the two PSL scores
to the overall similarity score.
Future Work: Use a weighted average where
different weights are learned for different parts of
the sentence.
5 Evaluation
The dataset used for evaluation is SICK:
Sentences Involving Compositional Knowledge
dataset, a task for SemEval 2014. The initial data
release for the competition consists of 5,000 pairs
of sentences which are annotated for both RTE and
STS. For this evaluation, we performed 10-fold
cross validation on this initial data.
Table 1 shows results comparing our full
approach (logic+dist) to two baselines, a
distributional-only baseline (dist) that uses vector
addition, and a probabilistic logic-only baseline
(logic) which is our semantic parser without distri-
butional inference rules. The integrated approach
(logic+dist) out-performs both baselines.
6 Conclusion
We presented an approach to semantic parsing that
has a wide-coverage for words and relations, and
does not require a fixed formal ontology. An
on-the-fly ontology of semantic relations between
predicates is derived from distributional informa-
tion and encoded in the form of soft inference rules
in probabilistic logic. We evaluated this approach
on two task, RTE and STS, using two probabilistic
logics, MLNs and PSL respectively. The semantic
parser can be extended in different direction, es-
pecially in predicting more complex semantic re-
lations, and enhancing the inference mechanisms.
10
Acknowledgments
This research was supported by the DARPA DEFT
program under AFRL grant FA8750-13-2-0026.
Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of
the author and do not necessarily reflect the view
of DARPA, DoD or the US government. Some ex-
periments were run on the Mastodon Cluster sup-
ported by NSF Grant EIA-0303609.
References
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A
pilot on semantic textual similarity. In Proceedings
of Semantic Evaluation (SemEval-12).
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of Conference on Empirical Methods in
Natural Language Processing (EMNLP-10).
Islam Beltagy, Cuong Chau, Gemma Boleda, Dan Gar-
rette, Katrin Erk, and Raymond Mooney. 2013.
Montague meets Markov: Deep semantics with
probabilistic logical form. In Proceedings of the
Second Joint Conference on Lexical and Computa-
tional Semantics (*SEM-13).
Islam Beltagy, Katrin Erk, and Raymond Mooney.
2014. Probabilistic soft logic for semantic textual
similarity. In Proceedings of Association for Com-
putational Linguistics (ACL-14).
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP-13).
Johan Bos. 2008. Wide-coverage semantic analysis
with Boxer. In Proceedings of Semantics in Text
Processing (STEP-08).
Stephen Clark and James R. Curran. 2004. Parsing
the WSJ using CCG and log-linear models. In Pro-
ceedings of Association for Computational Linguis-
tics (ACL-04).
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT-13).
L. Getoor and B. Taskar, editors. 2007. Introduction
to Statistical Relational Learning. MIT Press, Cam-
bridge, MA.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of Conference on Empirical Methods in Natural
Language Processing (EMNLP-11).
Edward Grefenstette. 2013. Towards a formal distri-
butional semantics: Simulating logical calculi with
tensors. In Proceedings of Second Joint Conference
on Lexical and Computational Semantics (*SEM
2013).
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
Angelika Kimmig, Stephen H. Bach, Matthias
Broecheler, Bert Huang, and Lise Getoor. 2012.
A short introduction to Probabilistic Soft Logic.
In Proceedings of NIPS Workshop on Probabilistic
Programming: Foundations and Applications (NIPS
Workshop-12).
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-13).
T. K. Landauer and S. T. Dumais. 1997. A solution to
Plato?s problem: The Latent Semantic Analysis the-
ory of the acquisition, induction, and representation
of knowledge. Psychological Review.
Alessandro Lenci and Giulia Benotto. 2012. Identify-
ing hypernyms in distributional semantic spaces. In
Proceedings of the first Joint Conference on Lexical
and Computational Semantics (*SEM-12).
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings
of Association for Computational Linguistics (ACL-
08).
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Journal of
Cognitive Science.
Richard Montague. 1970. Universal grammar. Theo-
ria, 36:373?398.
Matthew Richardson and Pedro Domingos. 2006.
Markov logic networks. Machine Learning,
62:107?136.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research
(JAIR-10).
11
