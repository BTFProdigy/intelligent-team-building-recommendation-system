Proceedings of the ACL Student Research Workshop, pages 172?179,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Deepfix: Statistical Post-editing of Statistical Machine Translation Using
Deep Syntactic Analysis
Rudolf Rosa and David Marec?ek and Ales? Tamchyna
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague
{rosa,marecek,tamchyna}@ufal.mff.cuni.cz
Abstract
Deepfix is a statistical post-editing sys-
tem for improving the quality of statis-
tical machine translation outputs. It at-
tempts to correct errors in verb-noun va-
lency using deep syntactic analysis and a
simple probabilistic model of valency. On
the English-to-Czech translation pair, we
show that statistical post-editing of statis-
tical machine translation leads to an im-
provement of the translation quality when
helped by deep linguistic knowledge.
1 Introduction
Statistical machine translation (SMT) is the cur-
rent state-of-the-art approach to machine transla-
tion ? see e.g. Callison-Burch et al (2011). How-
ever, its outputs are still typically significantly
worse than human translations, containing vari-
ous types of errors (Bojar, 2011b), both in lexical
choices and in grammar.
As shown by many researchers, e.g. Bojar
(2011a), incorporating deep linguistic knowledge
directly into a translation system is often hard to
do, and seldom leads to an improvement of trans-
lation output quality. It has been shown that it is
often easier to correct the machine translation out-
puts in a second-stage post-processing, which is
usually referred to as automatic post-editing.
Several types of errors can be fixed by employ-
ing rule-based post-editing (Rosa et al, 2012b),
which can be seen as being orthogonal to the sta-
tistical methods employed in SMT and thus can
capture different linguistic phenomena easily.
But there are still other errors that cannot be cor-
rected with hand-written rules, as there exist many
linguistic phenomena that can never be fully de-
scribed manually ? they need to be handled statis-
tically by automatically analyzing large-scale text
corpora. However, to the best of our knowledge,
English Czech
go to the doctor j??t k doktorovi dative case
go to the centre j??t do centra genitive case
go to a concert j??t na koncert accusative case
go for a drink j??t na drink accusative case
go up the hill j??t na kopec accusative case
Table 1: Examples of valency of the verb ?to go?
and ?j??t?. For Czech, the morphological cases of
the nouns are also indicated.
Source: The government spends on the middleschools.
Moses: Vla?da utra?c?? str?edn?? s?koly.
Meaning: The government destroys the middleschools.
Reference: Vla?da utra?c?? za str?edn?? s?koly.
Meaning: The government spends on the middleschools.
Table 2: Example of a valency error in output of
Moses SMT system.
there is very little successful research in statistical
post-editing (SPE) of SMT (see Section 2).
In our paper, we describe a statistical approach
to correcting one particular type of English-to-
Czech SMT errors ? errors in the verb-noun va-
lency. The term valency stands for the way in
which verbs and their arguments are used together,
usually together with prepositions and morpholog-
ical cases, and is described in Section 4. Several
examples of the valency of the English verb ?to go?
and the corresponding Czech verb ?j??t? are shown
in Table 1.
We conducted our experiments using a state-of-
the-art SMT system Moses (Koehn et al, 2007).
An example of Moses making a valency error is
translating the sentence ?The government spends
on the middle schools.?, adapted from our devel-
opment data set. As shown in Table 2, Moses
translates the sentence incorrectly, making an er-
ror in the valency of the ?utra?cet ? s?kola? (?spend ?
school?) pair. The missing preposition changes the
meaning dramatically, as the verb ?utra?cet? is pol-
172
ysemous and can mean ?to spend (esp. money)? as
well as ?to kill, to destroy (esp. animals)?.
Our approach is to use deep linguistic analysis
to automatically determine the structure of each
sentence, and to detect and correct valency errors
using a simple statistical valency model. We de-
scribe our approach in detail in Section 5.
We evaluate and discuss our experiments in
Section 6. We then conclude the paper and pro-
pose areas to be researched in future in Section 7.
2 Related Work
The first reported results of automatic post-editing
of machine translation outputs are (Simard et al,
2007) where the authors successfully performed
statistical post-editing (SPE) of rule-based ma-
chine translation outputs. To perform the post-
editing, they used a phrase-based SMT system in a
monolingual setting, trained on the outputs of the
rule-based system as the source and the human-
provided reference translations as the target, to
achieve massive translation quality improvements.
The authors also compared the performance of the
post-edited rule-based system to directly using the
SMT system in a bilingual setting, and reported
that the SMT system alone performed worse than
the post-edited rule-based system. They then tried
to post-edit the bilingual SMT system with another
monolingual instance of the same SMT system,
but concluded that no improvement in quality was
observed.
The first known positive results in SPE of SMT
are reported by Oflazer and El-Kahlout (2007)
on English to Turkish machine translation. The
authors followed a similar approach to Simard
et al (2007), training an SMT system to post-
edit its own output. They use two iterations of
post-editing to get an improvement of 0.47 BLEU
points (Papineni et al, 2002). The authors used
a rather small training set and do not discuss the
scalability of their approach.
To the best of our knowledge, the best results re-
ported so far for SPE of SMT are by Be?chara et al
(2011) on French-to-English translation. The au-
thors start by using a similar approach to Oflazer
and El-Kahlout (2007), getting a statistically sig-
nificant improvement of 0.65 BLEU points. They
then further improve the performance of their
system by adding information from the source
side into the post-editing system by concatenat-
ing some of the translated words with their source
Direction Baseline SPE Context SPE
en?cs 10.85?0.47 10.70?0.44 10.73?0.49
cs?en 17.20?0.53 17.11?0.52 17.18?0.54
Table 3: Results of SPE approach of Be?chara et al
(2011) evaluated on English-Czech SMT.
words, eventually reaching an improvement of
2.29 BLEU points. However, similarly to Oflazer
and El-Kahlout (2007), the training data used are
very small, and it is not clear how their method
scales on larger training data.
In our previous work (Rosa et al, 2012b), we
explored a related but substantially different area
of rule-based post-editing of SMT. The resulting
system, Depfix, manages to significantly improve
the quality of several SMT systems outputs, using
a set of hand-written rules that detect and correct
grammatical errors, such as agreement violations.
Depfix can be easily combined with Deepfix,1 as
it is able to correct different types of errors.
3 Evaluation of Existing SPE
Approaches
First, we evaluated the utility of the approach of
Be?chara et al (2011) for the English-Czech lan-
guage pair. We used 1 million sentence pairs from
CzEng 1.0 (Bojar et al, 2012b), a large English-
Czech parallel corpus. Identically to the paper, we
split the training data into 10 parts, trained 10 sys-
tems (each on nine tenths of the data) and used
them to translate the remaining part. The second
step was then trained on the concatenation of these
translations and the target side of CzEng. We also
implemented the contextual variant of SPE where
words in the intermediate language are annotated
with corresponding source words if the alignment
strength is greater than a given threshold. We lim-
ited ourselves to the threshold value 0.8, for which
the best results are reported in the paper. We tuned
all systems on the dataset of WMT11 (Callison-
Burch et al, 2011) and evaluated on the WMT12
dataset (Callison-Burch et al, 2012).
Table 3 summarizes our results. The reported
confidence intervals were estimated using boot-
strap resampling (Koehn, 2004). SPE did not lead
to any improvements of BLEU in our experiments.
In fact, SPE even slightly decreased the score (but
1Depfix (Rosa et al, 2012b) performs rule-based post-
editing on shallow-syntax dependency trees, while Deepfix
(described in this paper) is a statistical post-editing system
operating on deep-syntax dependency trees.
173
the difference is statistically insignificant in all
cases).
We conclude that this method does not improve
English-Czech translation, possibly because our
training data is too large for this method to bring
any benefit. We therefore proceed with a more
complex approach which relies on deep linguistic
knowledge.
4 Deep Dependency Syntax, Formemes,
and Valency
4.1 Tectogrammatical dependency trees
Tectogrammatical trees are deep syntactic depen-
dency trees based on the Functional Generative
Description (Sgall et al, 1986). Each node in
a tectogrammatical tree corresponds to a content
word, such as a noun, a full verb or an adjec-
tive; the node consists of the lemma of the con-
tent word and several other attributes. Functional
words, such as prepositions or auxiliary verbs, are
not directly present in the tectogrammatical tree,
but are represented by attributes of the respective
content nodes. See Figure 1 for an example of two
tectogrammatical trees (for simplicity, most of the
attributes are not shown).
In our work, we only use one of the
many attributes of tectogrammatical nodes, called
formeme (Dus?ek et al, 2012). A formeme is a
string representation of selected morpho-syntactic
features of the content word and selected auxiliary
words that belong to the content word, devised to
be used as a simple and efficient representation of
the node.
A noun formeme, which we are most interested
in, consists of three parts (examples taken from
Figure 1):
1. The syntactic part-of-speech ? n for nouns.
2. The preposition if the noun has one (empty
otherwise), as in n:on+X or n:za+4.
3. A form specifier.
? In English, it typically marks the subject
or object, as in n:subj. In case of a
noun accompanied by a preposition, the
third part is always X, as in n:on+X.
? In Czech, it denotes the morphologi-
cal case of the noun, represented by
its number (from 1 to 7 as there are
seven cases in Czech), as in n:1 and
n:za+4.
t-treezone=en
government n:subj
spend v:fin
middle adj:attr
school n:on+X
t-treezone=cs
vl?da n:1
utr?cet v:fin
st?edn? adj:attr
?kola n:za+4
Figure 1: Tectogrammatical trees for the sentence
?The government spends on the middle schools.? ?
?Vla?da utra?c?? za str?edn?? s?koly.?; only lemmas and
formemes of the nodes are shown.
Adjectives and nouns can also have the
adj:attr and n:attr formemes, respectively,
meaning that the node is in morphological agree-
ment with its parent. This is especially important
in Czech, where this means that the word bears the
same morphological case as its parent node.
4.2 Valency
The notion of valency (Tesnie`re and Fourquet,
1959) is semantic, but it is closely linked to syn-
tax. In the theory of valency, each verb has one
or more valency frames. Each valency frame de-
scribes a meaning of the verb, together with argu-
ments (usually nouns) that the verb must or can
have, and each of the arguments has one or several
fixed forms in which it must appear. These forms
can typically be specified by prepositions and mor-
phological cases to be used with the noun, and thus
can be easily expressed by formemes.
For example, the verb ?to go?, shown in Ta-
ble 1, has a valency frame that can be expressed
as n:subj go n:to+X, meaning that the sub-
ject goes to some place.
The valency frames of the verbs ?spend?
and ?utra?cet? in Figure 1 can be written as
n:subj spend n:on+X and n:1 utra?cet
n:za+4; the subject (in Czech this is a noun in
nominative case) spends on an object (in Czech,
the preposition ?za? plus a noun in accusative
case).
In our work, we have extended our scope also
to noun-noun valency, i.e. the parent node can be
either a verb or a noun, while the arguments are al-
ways nouns. Practice has proven this extension to
be useful, although the majority of the corrections
174
performed are still of the verb-noun valency type.
Still, we keep the traditional notion of verb-noun
valency throughout the text, especially to be able
to always refer to the parent as ?the verb? and to
the child as ?the noun?.
5 Our Approach
5.1 Valency models
To be able to detect and correct valency errors, we
created statistical models of verb-noun valency.
We model the conditional probability of the noun
argument formeme based on several features of the
verb-noun pair. We decided to use the following
two models:
P (fn|lv, fEN ) (1)
P (fn|lv, ln, fEN ) (2)
where:
? fn is the formeme of the Czech noun argu-
ment, which is being modelled
? lv is the lemma of the Czech parent verb
? ln is the lemma of the Czech noun argument
? fEN is the formeme of the English noun
aligned to the Czech noun argument
The input is first processed by the model (1),
which performs more general fixes, in situations
where the (lv, fEN ) pair rather unambiguously de-
fines the valency frame required.
Then model (2) is applied, correcting some er-
rors of the model (1), in cases where the noun
argument requires a different valency frame than
is usual for the (lv, fEN ) pair, and making some
more fixes in cases where the correct valency
frame required for the (lv, fEN ) pair was too am-
biguous to make a correction according to model
(1), but the decision can be made once information
about ln is added.
We computed the models on the full training set
of CzEng 1.0 (Bojar et al, 2012b) (roughly 15 mil-
lion sentences), and smoothed the estimated prob-
abilities with add-one smoothing.
5.2 Deepfix
We introduce a new statistical post-editing system,
Deepfix, whose input is a pair of an English sen-
tence and its Czech machine translation, and the
output is the Czech sentence with verb-noun va-
lency errors corrected.
The Deepfix pipeline consists of several steps:
1. the sentences are tokenized, tagged and lem-
matized (a lemma and a morphological tag is
assigned to each word)
2. corresponding English and Czech words are
aligned based on their lemmas
3. deep-syntax dependency parse trees of the
sentences are built, the nodes in the trees are
labelled with formemes
4. improbable noun formemes are replaced with
correct formemes according to the valency
model
5. the words are regenerated according to the
new formemes
6. the regenerating continues recursively to chil-
dren of regenerated nodes if they are in
morphological agreement with their parents
(which is typical for adjectives)
To decide whether the formeme of the noun is
incorrect, we query the valency model for all pos-
sible formemes and their probabilities. If an alter-
native formeme probability exceeds a fixed thresh-
old, we assume that the original formeme is incor-
rect, and we use the alternative formeme instead.
For our example sentence, ?The government
spends on the middle schools.? ? ?Vla?da utra?c?? za
str?edn?? s?koly.?, we query the model (2) and get the
following probabilities:
? P(n:4 | utra?cet, s?kola, n:on+X) = 0.07
(the original formeme)
? P(n:za+4 | utra?cet, s?kola, n:on+X) = 0.89
(the most probable formeme)
The threshold for this change type is 0.86, is
exceeded by the n:za+4 formeme and thus the
change is performed: ?s?koly? is replaced by ?za
s?koly?.
5.3 Tuning the Thresholds
We set the thresholds differently for different types
of changes. The values of the thresholds that we
used are listed in Table 4 and were estimated man-
ually. We distinguish changes where only the
morphological case of the noun is changed from
changes to the preposition. There are three possi-
ble types of a change to a preposition: switching
one preposition to another, adding a new preposi-
tion, and removing an existing preposition. The
175
Correction type Thresholds for models(1) (2)
Changing the noun case only 0.55 0.78
Changing the preposition 0.90 0.84
Adding a new preposition ? 0.86
Removing the preposition ? ?
Table 4: Deepfix thresholds
change to the preposition can also involve chang-
ing the morphological case of the noun, as each
preposition typically requires a certain morpho-
logical case.
For some combinations of a change type and a
model, as in case of the preposition removing, we
never perform a fix because we observed that it
nearly never improves the translation. E.g., if a
verb-noun pair can be correct both with and with-
out a preposition, the preposition-less variant is
usually much more frequent than the prepositional
variant (and thus is assigned a much higher prob-
ability by the model). However, the preposition
often bears a meaning that is lost by removing it
? in Czech, which is a relatively free-word-order
language, the semantic roles of verb arguments
are typically distinguished by prepositions, as op-
posed to English, where they can be determined
by their relative position to the verb.
5.4 Implementation
The whole Deepfix pipeline is implemented in
Treex, a modular NLP framework (Popel and
Z?abokrtsky?, 2010) written in Perl, which provides
wrappers for many state-of-the-art NLP tools. For
the analysis of the English sentence, we use the
Morc?e tagger (Spoustova? et al, 2007) and the
MST parser (McDonald et al, 2005). The Czech
sentence is analyzed by the Featurama tagger2 and
the RUR parser (Rosa et al, 2012a) ? a parser
adapted to parsing of SMT outputs. The word
alignment is created by GIZA++ (Och and Ney,
2003); the intersection symmetrization is used.
6 Evaluation
6.1 Automatic Evaluation
We evaluated our method on three datasets:
WMT10 (2489 parallel sentences), WMT11 (3003
parallel sentences), and WMT12 (3003 parallel
sentences) by Callison-Burch et al (2010; 2011;
2012). For evaluation, we used outputs of a
state-of-the-art SMT system, Moses (Koehn et al,
2http://featurama.sourceforge.net/
2007), tuned for English-to-Czech translation (Bo-
jar et al, 2012a). We used the WMT10 dataset
and its Moses translation as our development data
to tune the thresholds. In Table 5, we report the
achieved BLEU scores (Papineni et al, 2002),
NIST scores (Doddington, 2002), and PER (Till-
mann et al, 1997).
The improvements in automatic scores are low
but consistently positive, which suggests that
Deepfix does improve the translation quality.
However, the changes performed by Deepfix are
so small that automatic evaluation is unable to re-
liably assess whether they are positive or negative
? it can only be taken as an indication.
6.2 Manual Evaluation
To reliably assess the performance of Deepfix,
we performed manual evaluation on the WMT12
dataset translated by the Moses system.
The dataset was evenly split into 4 parts and
each of the parts was evaluated by one of two an-
notators (denoted ?A? and ?B?). For each sentence
that was modified by Deepfix, the annotator de-
cided whether the Deepfix correction had a posi-
tive (?improvement?) or negative (?degradation?)
effect on the translation quality, or concluded that
this cannot be decided (?indefinite?) ? either be-
cause both of the sentences are correct variants, or
because both are incorrect.3
The results in Table 6 prove that the overall ef-
fect of Deepfix is positive: it modifies about 20%
of the sentence translations (569 out of 3003 sen-
tences), improving over a half of them while lead-
ing to a degradation in only a quarter of the cases.
We measured the inter-annotator agreement on
100 sentences which were annotated by both an-
notators. For 60 sentence pairs, both of the anno-
tators were able to select which sentence is better,
i.e. none of the annotators used the ?indefinite?
marker. The inter-annotator agreement on these
60 sentence pairs was 97%.4
3The evaluation was done in a blind way, i.e. the annota-
tors did not know which sentence is before Deepfix and which
is after Deepfix. They were also provided with the source En-
glish sentences and the reference human translations.
4If all 100 sentence pairs are taken into account, requiring
that the annotators also agree on the ?indefinite? marker, the
inter-annotator agreement is only 65%. This suggests that
deciding whether the translation quality differs significantly
is much harder than deciding which translation is of a higher
quality.
176
Dataset BLEU score (higher is better) NIST score (higher is better) PER (lower is better)Baseline Deepfix Difference Baseline Deepfix Difference Baseline Deepfix Difference
WMT10* 15.66 15.74 +0.08 5.442 5.470 +0.028 58.44% 58.26% -0.18
WMT11 16.39 16.42 +0.03 5.726 5.737 +0.011 57.17% 57.09% -0.08
WMT12 13.81 13.85 +0.04 5.263 5.283 +0.020 60.04% 59.91% -0.13
Table 5: Automatic evaluation of Deepfix on outputs of the Moses system on WMT10, WMT11 and
WMT12 datasets. *Please note that WMT10 was used as the development dataset.
Part Annotator Changed sentences Improvement Degradation Indefinite
1 A 126 57 (45%) 35 (28%) 34 (27%)
2 B 112 62 (55%) 29 (26%) 21 (19%)
3 A 150 88 (59%) 29 (19%) 33 (22%)
4 B 181 114 (63%) 42 (23%) 25 (14%)
Total 569 321 (56%) 135 (24%) 113 (20%)
Table 6: Manual evaluation of Deepfix on outputs of Moses Translate system on WMT12 dataset.
6.3 Discussion
When a formeme change was performed, it was
usually either positive or at least not harmful (sub-
stituting one correct variant for another correct
variant).
However, we also observed a substantial
amount of cases where the change of the formeme
was incorrect. Manual inspection of a sample of
these cases showed that there can be several rea-
sons for a formeme change to be incorrect:
? incorrect analysis of the Czech sentence
? incorrect analysis of the English sentence
? the original formeme is a correct but very rare
variant
The most frequent issue is the first one. This is
to be expected, as the Czech sentence is often er-
roneous, whereas the NLP tools that we used are
trained on correct sentences; in many cases, it is
not even clear what a correct analysis of an incor-
rect sentence should be.
7 Conclusion and Future Work
On the English-Czech pair, we have shown that
statistical post-editing of statistical machine trans-
lation outputs is possible, even when translating
from a morphologically poor to a morphologi-
cally rich language, if it is grounded by deep lin-
guistic knowledge. With our tool, Deepfix, we
have achieved improvements on outputs of two
state-of-the-art SMT systems by correcting verb-
noun valency errors, using two simple probabilis-
tic valency models computed on large-scale data.
The improvements have been confirmed by man-
ual evaluation.
We encountered many cases where the per-
formance of Deepfix was hindered by errors of
the underlying tools, especially the taggers, the
parsers and the aligner. Because the use of the
RUR parser (Rosa et al, 2012a), which is partially
adapted to SMT outputs parsing, lead to a reduc-
tion of the number of parser errors, we find the ap-
proach of adapting the tools for this specific kind
of data to be promising.
We believe that our method can be adapted
to other language pairs, provided that there is a
pipeline that can analyze at least the target lan-
guage up to deep syntactic trees. Because we only
use a small subset of information that a tectogram-
matical tree provides, it is sufficient to use only
simplified tectogrammatical trees. These could be
created by a small set of rules from shallow-syntax
dependency trees, which can be obtained for many
languages using already existing parsers.
Acknowledgments
This research has been supported by the 7th FP
project of the EC No. 257528 and the project
7E11042 of the Ministry of Education, Youth and
Sports of the Czech Republic.
Data and some tools used as a prerequisite
for the research described herein have been pro-
vided by the LINDAT/CLARIN Large Infrastruc-
tural project, No. LM2010013 of the Ministry of
Education, Youth and Sports of the Czech Repub-
lic.
We would like to thank two anonymous review-
ers for many useful comments on the manuscript
of this paper.
177
References
Hanna Be?chara, Yanjun Ma, and Josef van Genabith.
2011. Statistical post-editing for a statistical MT
system. MT Summit XIII, pages 308?315.
Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran.
2012a. Probes in a taxonomy of factored phrase-
based models. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 253?
260, Montre?al, Canada. Association for Computa-
tional Linguistics.
Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012b. The joy of parallelism with CzEng
1.0. In Proceedings of the 8th International Confer-
ence on Language Resources and Evaluation (LREC
2012), pages 3921?3928, I?stanbul, Turkey. Euro-
pean Language Resources Association.
Ondr?ej Bojar. 2011a. Rich morphology and what
can we expect from hybrid approaches to MT. In-
vited talk at International Workshop on Using Lin-
guistic Information for Hybrid Machine Translation
(LIHMT-2011), November.
Ondr?ej Bojar. 2011b. Analyzing error types in
English-Czech machine translation. Prague Bulletin
of Mathematical Linguistics, 95:63?76, March.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metric-
sMATR, pages 17?53, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22?64, Edinburgh, Scot-
land, July. Association for Computational Linguis-
tics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montre?al, Canada, June. Association for
Computational Linguistics.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, pages 138?145. Morgan Kauf-
mann Publishers Inc.
Ondr?ej Dus?ek, Zdene?k Z?abokrtsky?, Martin Popel, Mar-
tin Majlis?, Michal Nova?k, and David Marec?ek.
2012. Formemes in English-Czech deep syntac-
tic MT. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 267?274,
Montre?al, Canada. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
ACL 2007, Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP,
Barcelona, Spain.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of
dependency parsers. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 91?98. Association for Computa-
tional Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Kemal Oflazer and Ilknur Durgar El-Kahlout. 2007.
Exploring different representational units in
English-to-Turkish statistical machine translation.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, pages 25?32. Association
for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL 2002,
Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 311?
318, Philadelphia, Pennsylvania.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. Tec-
toMT: modular NLP framework. In Proceedings of
the 7th international conference on Advances in nat-
ural language processing, IceTAL?10, pages 293?
304, Berlin, Heidelberg. Springer-Verlag.
Rudolf Rosa, Ondr?ej Dus?ek, David Marec?ek, and Mar-
tin Popel. 2012a. Using parallel features in pars-
ing of machine-translated sentences for correction of
grammatical errors. In Proceedings of Sixth Work-
shop on Syntax, Semantics and Structure in Statis-
tical Translation (SSST-6), ACL, pages 39?48, Jeju,
Korea. ACL.
Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek.
2012b. DEPFIX: A system for automatic correction
of Czech MT outputs. In Proceedings of the Sev-
enth Workshop on Statistical Machine Translation,
178
pages 362?368, Montre?al, Canada. Association for
Computational Linguistics.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The meaning of the sentence in its semantic and
pragmatic aspects. Springer.
Michel Simard, Cyril Goutte, and Pierre Isabelle.
2007. Statistical phrase-based post-editing. In Hu-
man Language Technologies 2007: The Conference
of the North American Chapter of the Association
for Computational Linguistics; Proceedings of the
Main Conference, pages 508?515, Rochester, New
York, April. Association for Computational Linguis-
tics.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel
Krbec, and Pavel Kve?ton?. 2007. The best of
two worlds: Cooperation of statistical and rule-
based taggers for Czech. In Proceedings of the
Workshop on Balto-Slavonic Natural Language Pro-
cessing 2007, pages 67?74, Praha, Czechia. Uni-
verzita Karlova v Praze, Association for Computa-
tional Linguistics.
Lucien Tesnie`re and Jean Fourquet. 1959. Ele?ments de
syntaxe structurale. E?ditions Klincksieck, Paris.
Christoph Tillmann, Stephan Vogel, Hermann Ney,
Alex Zubiaga, and Hassan Sawaf. 1997. Ac-
celerated dp based search for statistical translation.
In European Conf. on Speech Communication and
Technology, pages 2667?2670.
179
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 426?432,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Two-step translation with grammatical post-processing?
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova? and Ondr?ej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague
{marecek,rosa,galuscakova,bojar}@ufal.mff.cuni.cz
Abstract
This paper describes an experiment in which
we try to automatically correct mistakes in
grammatical agreement in English to Czech
MT outputs. We perform several rule-based
corrections on sentences parsed to dependency
trees. We prove that it is possible to improve
the MT quality of majority of the systems par-
ticipating in WMT shared task. We made both
automatic (BLEU) and manual evaluations.
1 Introduction
This paper is a joint report on two English-to-Czech
submissions to the WMT11 shared translation task.
The main contribution is however the proposal and
evaluation of a rule-based post-processing system
DEPFIX aimed at correcting errors in Czech gram-
mar applicable to any MT system. This is somewhat
the converse of other approaches (e.g. Simard et al
(2007)) where a statistical system was applied for
the post-processing of a rule-based one.
2 Our phrase-based systems
This section briefly describes our underlying phrase-
based systems. One of them (CU-BOJAR) was sub-
mitted directly to the WMT11 manual evaluation,
the other one (CU-TWOSTEP) was first corrected by
the proposed method (Section 3 below) and then
submitted under the name CU-MARECEK.
?This research has been supported by the European Union
Seventh Framework Programme (FP7) under grant agreement
n? 247762 (Faust), n? 231720 (EuroMatrix Plus), and by the
grants GAUK 116310 and GA201/09/H057.
2.1 Data for statistical systems
Our training parallel data consists of CzEng 0.9
(Bojar and Z?abokrtsky?, 2009), the News Commen-
tary corpus v.6 as released by the WMT11 orga-
nizers, the EMEA corpus, a corpus collected from
the transcripts of TED talks (http://www.ted.com),
the parallel news and separately some of the par-
allel web pages of the European Commission
(http://ec.europa.eu), and the Official Journal of the
European Union as released by the Apertium con-
sortium (http://apertium.eu/data).
A custom web crawler was used for the European
Commission website. English and Czech websites
were matched according to their URLs. Unfortu-
nately, Czech websites very often contain untrans-
lated parts of English texts. Because of this, we
aimed especially at the news articles, which are very
often translated correctly and also more relevant for
the shared task. Texts were segmented using train-
able tokenizer (Klyueva and Bojar, 2008) and dedu-
plicated. Processed texts were automatically aligned
by Hunalign (Varga and others, 2005).
The data from the Official Journal were first con-
verted from XML to plain text. The documents were
paired according to their filenames. To better han-
dle the nature of these data, we decided to divide
the documents into two classes based on the aver-
age number of words per sentence: ?lists? are docu-
ments with less than 2.8 words per sentence, other
documents are called ?texts?. The corresponding
?lists? were aligned line by line. The corresponding
?texts? were automatically segmented by trainable
tokenizer and aligned automatically by Hunalign.
We use the following two Czech language mod-
426
els, their weights are optimized in MERT:
? 5-gram LM from the Czech side of CzEng (ex-
cluding the Navajo section). The LM was con-
structed by interpolating LMs of the individual do-
mains (news, EU legislation, technical documenta-
tion, etc.) to achieve the lowest perplexity on the
WMT08 news test set.
? 6-gram LM from the monolingual data supplied by
WMT11 organizers (news of the individual years
and News Commentary), the Czech National Cor-
pus and a web collection of Czech texts. Again, the
final LM is constructed by interpolating the smaller
LMs1 for the WMT08 news test set.
2.2 Baseline Moses (CU-BOJAR)
The system denoted CU-BOJAR for English-to-
Czech is simple phrase-based translation, i.e. Moses
without factors. We tokenized, lemmatized and
tagged all texts using the tools wrapped in TectoMT
(Popel and Z?abokrtsky?, 2010). We further tokenize
e.g. dashed words (?23-year?) after all the process-
ing is finished. Phrase-based MT is then able to
handle such expressions both at once, or decompose
them as needed to cover unseen variations. We use
lexicalized reordering (orientation-bidirectional-fe).
The translation runs in ?supervised truecase?, which
means that we use the output of our lemmatizers
to decide whether the word should be lowercased
or should preserve uppercasing. After the transla-
tion, the first letter in the output is simply upper-
cased. The model is optimized using Moses? stan-
dard MERT on the WMT09 test set.
The organizers of WMT11 encouraged partici-
pants to apply simple normalization to their data
(both for training and testing).2 The main purpose
of the normalization is to improve the consistency of
typographical rules. Unfortunately, some of the au-
tomatic changes may accidentally damage the mean-
ing of the expression.3 We therefore opted to submit
1The interpolated LM file (gzipped ARPA format) is 5.1 GB
so we applied LM pruning as implemented in SRI toolkit with
the threshold 10?14 to reduce the file size to 2.3 GB.
2http://www.statmt.org/wmt11/normalize-punctuation.perl
3Fixing the ordering of the full stop and the quote is wrong
because the order (at least in Czech typesetting) depends on
whether it is the full sentence or a final phrase that is captured
in the quotes. Even riskier are rules handling decimal and thou-
sand separators in numbers. While there are language-specific
conventions, they are not always followed and the normaliza-
tion can in such cases confuse the order of magnitude by 3.
the output based on non-normalized test sets as our
primary English-to-Czech submission.
We invested much less effort into the submission
called CU-BOJAR for Czech-to-English. The only
interesting feature there is the use of alternative de-
coding paths to translate either from the Czech form
or from the Czech lemma equipped with meaning-
bearing morphological properties, e.g. the number
of nouns. Bojar and Kos (2010) used the same setup
with simple lemmas in the fallback decoding path.
The enriched lemmas perform marginally better.
2.3 Two-step translation
Our two-step translation is essentially the same
setup as detailed by Bojar and Kos (2010): (1)
the English source is translated to simplified Czech,
and (2) the simplified Czech is monotonically trans-
lated to fully inflected Czech. Both steps are sim-
ple phrase-based models. Instead of word forms, the
simplified Czech uses lemmas enriched by a sub-
set of morphological features selected manually to
encode only properties overt both in English and
Czech such as the tense of verbs or number of nouns.
Czech-specific morphological properties indicating
various agreements (e.g. number and gender of ad-
jectives, gender of verbs) are imposed in the second
step solely on the basis of the language model.
The first step uses the same parallel and mono-
lingual corpora as CU-BOJAR, except the LMs being
trained on the enriched lemmas, not on word forms.
The second step uses exactly the same LM as CU-
BOJAR but the phrase-table is extracted from all our
Czech monolingual data (phrase length limit of 1.)
3 Grammatical post-processing
Phrase-based machine translation systems often
have problems with grammatical agreement, espe-
cially on longer dependencies. Sometimes, there is
a mistake in agreement even between adjacent words
because each one belongs to a different phrase. The
goal of our post-processing is to correct forms of
some words so that they do not violate grammatical
rules (eg. grammatical agreement).
The problem is how to find the correct syntactic
relations in the output of an MT system. Parsers
trained on correct sentences can rely on grammat-
ical agreement, according to which they determine
427
the dependencies between words. Unfortunately, the
agreement in MT outputs is often wrong and the
parser fails to produce a correct parse tree. There-
fore, we would need a parser trained on a manually
annotated treebank consisting of specific outputs of
machine translation systems. Such a treebank does
not exist and we do not even want to create one, be-
cause the MT systems are changing constantly and
also because manual annotation of texts that are of-
ten not even understandable would be almost a su-
perhuman task.
The DEPFIX system was implemented in TectoMT
framework (Popel and Z?abokrtsky?, 2010). MT out-
puts were tagged by Morc?e tagger (Spoustova? et al,
2007) and then parsed with MST parser (McDon-
ald et al, 2005) that was trained on the Prague De-
pendency Treebank (Hajic? and others, 2006), i.e.
on correct Czech sentences. We used an improved
implementation with some additional features es-
pecially tuned for Czech (Nova?k and Z?abokrtsky?,
2007). The parser accuracy is much lower on the
?noisy? MT output sentences, but a lot of dependen-
cies in which we are to correct grammatical agree-
ment are determined correctly. Adapting the parser
for outputs of MT systems will be addressed in the
coming months.
A typical example of a correction is the agreement
between the subject and the predicate: they should
share the morphological number and gender. If they
do not, we simply change the number and gender
of the predicate in agreement with the subject.4 An
example of such a changed predicate is in Figure 1.
Apart from the dependency tree of the target sen-
tence, we can also use the dependency tree of the
source sentence. Source sentences are grammat-
ically correct and the accuracy of the tagger and
the parser is accordingly higher there. Words in
the source and target sentences are aligned using
GIZA++5 (Och and Ney, 2003) but verbose outputs
of the original MT systems would be possibly a bet-
ter option. The rules for fixing grammatical agree-
ment between words can thus consider also the de-
pendency relations and morphological caregories of
their English counterparts in the input sentence.
4In this case, we suppose that the number of the subject has
a much higher chance to be correct.
5GIZA++ was run on lemmatized texts in both directions
and intersection symmetrization was used.
Some
people
came
later
Atr
Sb
Pred
Advplpl
.AuxK
p?i?liPredpl
N?kte??
lid?
p?i?el
pozd?ji
Atr
Sb
Pred
Advsg, mpl
.AuxK
Figure 1: Example of fixing subject-predicate agreement.
The Czech word pr?is?el [he came] has a wrong morpho-
logical number and gender.
3.1 Grammatical rules
We have manually devised a set of the following
rules. Their input is the dependency tree of a Czech
sentence (MT output) and its English source sen-
tence (MT input) with the nodes aligned where pos-
sible. Each of the rules fires if the specified con-
ditions (?IF?) are matched, executes the command
(?DO?) , usually changing one or more morphologi-
cal categories of the word, and generates a new word
form for any word which was changed.
The rules make use of several morphological cat-
egories of the word (node:number, node:gender...),
its syntactic relation to its parent in the dependency
tree (node:afun) and the same information for its
English counterpart (node:en) and other nodes in
the dependency trees.
The order of the rules in this paper follows the
order in which they are applied; this is important, as
often a rule changes a morphological category of a
word which is then used by a subsequent rule.
3.1.1 Noun number (NounNum)
In Czech, a word in singular sometimes has the
same form as in plural. Because the tagger often
fails to tag the word correctly, we try to correct the
tag of a noun tagged as singular if its English coun-
terpart is in plural, so that the subsequent rules can
work correctly.
We trust the form of the word but changing the
number may also require to change the morphologi-
cal case (i.e. the tagger was wrong with both number
and case). In such cases we choose the first (linearly
428
from nominative to instrumentative) case matching
the form. The rule is:
IF: node:pos = noun &
node:number = singular &
node:en:number = plural
DO: node:number := plural;
node:case := find case(node:form, plural);
3.1.2 Subject case (SubjCase)
The subject of a Czech sentence must be in the
nominative case. Since the parser often fails in
marking the correct word as a subject, we use the
English source sentence and presuppose that the
Czech counterpart of the English subject is also a
subject in the Czech sentence.
IF: node:en:afun = subject
DO: node:case := nominative;
3.1.3 Subject-predicate agreement (SubjPred)
Subject and predicate in Czech agree in their mor-
phological number. To identify a Czech Subject, we
trust the subject in the English sentence. Then we
copy the number from the (Czech) Subject to the
Czech Predicate.
IF: node:en:afun = subject &
parent:afun = predicate
DO: parent:number := node:number;
3.1.4 Subject-past participle agreement (SubjPP)
Czech past participles agree with subject in
morphological gender.
IF: node:pos = noun|pronoun &
node:en:afun = subject &
parent:pos = verb past participle
DO: parent:number := node:number;
parent:gender := node:gender;
3.1.5 Preposition without children (PrepNoCh)
In our dependency trees, the preposition is the
parent of the words it belongs to (usually a noun). A
preposition without children is incorrect so we find
nodes aligned to its English counterpart?s children
and rehang them under the preposition.
IF: node:afun = preposition &
!node:has children &
node:en:has children
DO: foreach node:en:child;
node:en:child:cs:parent := node;
3.1.6 Preposition-noun agreement (PrepNoun)
Every prepositions gets a morphological case as-
signed to it by the tagger, with which the dependent
noun should agree.
IF: parent:pos = preposition &
node:pos = noun
DO: node:case := parent:case;
3.1.7 Noun-adjective agreement (NounAdj)
Czech adjectives and nouns agree in morpholog-
ical gender, number and case. We assume that the
noun is correct and change the adjective accordingly.
IF: node:pos = adjective &
parent:pos = noun
DO: node:gender := parent:gender;
node:number := parent:number;
node:case := parent:case;
3.1.8 Reflexive particle deletion (ReflTant)
Czech reflexive verbs are accompanied by reflex-
ive particles (?se? and ?si?). We delete particles not
beloning to any verb (or adjective derived from a
verb).
IF: node:form = ?se?|?si? &
node:pos = pronoun &
parent:pos != verb|verbal adjective
DO: remove node;
4 Experiments and results
We tested our CU-TWOSTEP system with DEPFIX
post-processing on both WMT10 and WMT11 test-
ing data. This combined system was submitted to
shared translation task as CU-MARECEK. We also
ran the DEPFIX post-processing on all other partici-
pating systems.
4.1 Automatic evaluation
The achieved BLEU scores are shown in Tables 1
and 2. They show the scores before and after the
DEPFIX post-processing. It is interesting that the
improvements are quite different between the years
2010 and 2011 in terms of their BLEU score. While
the average improvement on WMT10 test set was
0.21 BLEU points, it was only 0.05 BLEU points on
the WMT11 test set. Even the results of the same
TWOSTEP system differ in a similar way, so it must
have been caused by the different data.
429
system before after improvement
cu-twostep 15.98 16.13 0.15 (0.05 - 0.26)
cmu-heaf. 16.95 17.04 0.09 (-0.01 - 0.20)
cu-bojar 15.85 16.09 0.24 (0.14 - 0.36)
cu-zeman 12.33 12.55 0.22 (0.12 - 0.32)
dcu 13.36 13.59 0.23 (0.13 - 0.37)
dcu-combo 18.79 18.90 0.11 (0.02 - 0.23)
eurotrans 10.10 10.11 0.01 (-0.04 - 0.07)
koc 11.74 11.91 0.17 (0.08 - 0.26)
koc-combo 16.60 16.86 0.26 (0.16 - 0.37)
onlineA 11.81 12.08 0.27 (0.17 - 0.38)
onlineB 16.57 16.79 0.22 (0.11 - 0.33)
potsdam 12.34 12.57 0.23 (0.14 - 0.35)
rwth-combo 17.54 17.79 0.25 (0.15 - 0.35)
sfu 11.43 11.83 0.40 (0.29 - 0.52)
uedin 15.91 16.19 0.28 (0.18 - 0.40)
upv-combo 17.51 17.73 0.22 (0.10 - 0.34)
Table 1: Depfix improvements on the WMT10 systems
in BLEU score. Confidence intervals, which were com-
puted on 1000 bootstrap samples, are in brackets.
system before after improvement
cu-twostep 16.57 16.60 0.03 (-0.07 - 0.13)
cmu-heaf. 20.24 20.32 0.08 (-0.03 - 0.19)
commerc2 09.32 09.32 0.00 (-0.04 - 0.04)
cu-bojar 16.88 16.85 -0.03 (-0.12 - 0.07)
cu-popel 14.12 14.11 -0.01 (-0.06 - 0.03)
cu-tamch. 16.32 16.28 -0.04 (-0.14 - 0.06)
cu-zeman 14.61 14.80 0.19 (0.09 - 0.29)
jhu 17.36 17.42 0.06 (-0.03 - 0.16)
online-B 20.26 20.31 0.05 (-0.06 - 0.16)
udein 17.80 17.88 0.08 (-0.02 - 0.17)
upv-prhlt. 20.68 20.69 0.01 (-0.08 - 0.11)
Table 2: Depfix improvements on the WMT11 systems
in BLEU score. Confidence intervals are in brackets.
4.2 Manual evaluation
Two independent annotators evaluated DEPFIX man-
ually on the outputs of CU-TWOSTEP and ONLINE-
B. We randomly selected 1000 sentences from the
newssyscombtest2011 data set and the appropri-
ate translations made by these two systems. The
annotators got the outputs before and after DEPFIX
post-processing and their task was to decide which
translation6 from these two is better and label it by
the letter ?a?. If it was not possible to determine
6They were also provided with the source English sentence
and the reference translation. The options were shuffled and
indentical candidate sentences were collapsed.
A / B improved worsened indefinite total
improved 273 20 15 308
worsened 12 59 7 78
indefinite 53 35 42 130
total 338 114 64 516
Table 5: Matrix of the inter-annotator agreement
rule fired impr. wors. % impr.
SubjCase 51 46 5 90.2
SubjPP 193 165 28 85.5
NounAdj 434 354 80 81.6
NounNum 156 122 34 78.2
PrepNoun 135 99 36 73.3
SubjPred 68 48 20 70.6
ReflTant 15 10 5 66.7
PrepNoCh 45 29 16 64.4
Table 6: Rules and their utility.
which is better, they labeled both by ?n?.
Table 3 below shows that about 60% of sentences
fixed by DEPFIX were improved and only about 20%
were worsened. DEPFIX worked a little better on the
ONLINE-B, making fewer changes but also fewer
wrong changes. It is probably connected with the
fact that overall better translations by ONLINE-B are
easier to parse.
The matrix of inter-annotator agreement is in Ta-
ble 5. Our two annotators agreed in 374 sentences
(out of 516), that is 72.5%. On the other hand, if
we consider only cases where both annotators chose
different translation as better (no indefinite marks),
we get only 8.8% disagreement (32 out of 364).
Using the manual evaluation, we can also measure
performance of the individual rules. Table 6 shows
the number of all, improved or worsened sentences
where a particular rule was applied. Definitely, the
most useful rule (used often and quite reliable) was
the one correcting noun-adjective agreement, fol-
lowed by the subject-pastparticiple agreement rule.
In each changed sentence, two rules (not neces-
sarily related ones) were applied on average.
4.3 Manual evaluation across data sets
The fact that the improvements in BLEU scores on
WMT10 test set are much higher has led us to one
more experiment: we compare manual annotations
of 330 sentences from each of the WMT10 and
430
system annotator changed improved worsened indefinite
count % count % count %
cu-bojar-twostep A 269 152 56.5 39 14.5 78 29.0
cu-bojar-twostep B 269 173 64.3 50 18.6 46 17.1
online-B A 247 156 63.1 39 15.9 52 21.1
online-B B 247 165 66.8 64 25.9 18 7.3
Table 3: Manual evaluation of the DEPFIX post-processing on 1000 randomly chosen sentences from WMT11 test set.
test set changed improved worsened indefinite BLEU
count % count % count % before after diff
newssyscombtest2010 104 52 50.0 20 19.2 32 30.8 16.99 17.38 0.39
newssyscombtest2011 101 66 65.3 19 18.8 16 15.8 13.99 13.87 -0.12
Table 4: Manual and automatic evaluation of the DEPFIX post-processing on CU-TWOSTEP system across different
datasets. 330 sentences were randomly selected from each of the WMT10 and WMT11 test sets. Both manual scores
and BLEU are computed only on the sentences that were changed by the DEPFIX post-processing.
WMT11 sets as translated by CU-TWOSTEP and cor-
rected by DEPFIX. Table 4 shows that WMT10 and
WMT11 are comparable in manually estimated im-
provement (50?65%). BLEU does not indicate that
and even estimates a drop in quality on this subset
WMT11. (The absolute BLEU scores differ from
BLEUs on the whole test sets but we are interested
only in the change of the scores.) BLEU is thus not
very suitable for the evaluation of DEPFIX.
5 Conclusions and future work
Manual evaluation shows that our DEPFIX approach
to improving MT output quality is sensible. Al-
though it is unable to correct many serious MT er-
rors, such as wrong lexical choices, it can improve
the grammaticality of the output in a way that the
language model often cannot, which leads to out-
put that is considered to be better by humans. We
also suggest that BLEU is not appropriate metric
for measuring changes in grammatical correctness
of sentences, especially with inflective languages.
An advantage of our method is that it is possible
to apply it on output of any MT system (although it
works better for phrase-based MT systems). While
DEPFIX has been developed using the output of CU-
BOJAR, the rules we devised are not specific to any
MT system. They simply describe several grammat-
ical rules of Czech language that can be machine-
checked and if errors are found, the output can be
corrected. Moreover, our method only requires the
source sentence and the translation output for its op-
eration ? i.e. it is not necessary to modify the MT
system itself.
We are now considering modifications of the
parser so that it is able to parse the incorrect sen-
tences produced by MT. Theoretically it would be
possible to train the parser on annotated ungrammat-
ical sentences, but we do not want to invest such an-
notation labour. Instead, when parsing the Czech
sentence we will make the parser utilize the infor-
mation contained in the parse tree of the English
sentence, which is usually correct. We will proba-
bly also have to make the parser put less weight to
the often incorrect tagger output. An alternative is
to avoid parsing of the target and project the source
parse to the target side using word alignments, if
provided by the MT system.
Because some of our rules are able to work using
only the tagger output, we will also try to apply them
before the parsing as they might help the parser by
correcting some of the tags.
We will also try several modifications of the tag-
ger, but the English sentence does not help us so
much here, because it does not contain any infor-
mation regarding the most common errors ? in-
correct assignment of morphological gender and
case. However, it could help with part of speech
and morphological number disambiguation. More-
over, it would be probably helpful for us if the tag-
ger included several most probable hypotheses, as
the single-output-only disambiguation is often erro-
neous on ungrammatical sentences.
431
References
Ondrej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T0 1, Philadelphia.
Natalia Klyueva and Ondr?ej Bojar. 2008. UMC 0.1:
Czech-Russian-English Multilingual Corpus. In Pro-
ceedings of International Conference Corpus Linguis-
tics, pages 188?195, Saint-Petersburg.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In HLT ?05: Proceed-
ings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada.
Va?clav Nova?k and Zdene?k Z?abokrtsky?. 2007. Feature
engineering in maximum spanning tree dependency
parser. In Va?clav Matous?ek and Pavel Mautner, edi-
tors, Lecture Notes in Artificial Intelligence, Proceed-
ings of the 10th I nternational Conference on Text,
Speech and Dialogue, Lecture Notes in Computer Sci-
ence, pages 92?98, Pilsen, Czech Republic. Springer
Science+Business Media Deutschland GmbH.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
modular NLP framework. In Proceedings of the 7th
international conference on Advances in natural lan-
guage processing, IceTAL?10, pages 293?304, Berlin,
Heidelberg. Springer-Verlag.
Michel Simard, Cyril Goutte, and Pierre Isabelle. 2007.
Statistical phrase-based post-editing. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 508?515, Rochester, New York, April.
Association for Computational Linguistics.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Da?niel Varga et al 2005. Parallel corpora for medium
density languages. In Proceedings of the Recent Ad-
vances in Natural Language Processing, pages 590?
596, Borovets, Bulgaria.
432
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 362?368,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
DEPFIX: A System for Automatic Correction of Czech MT Outputs?
Rudolf Rosa, David Marec?ek and Ondr?ej Dus?ek
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague
{rosa,marecek,odusek}@ufal.mff.cuni.cz
Abstract
We present an improved version of DEPFIX
(Marec?ek et al, 2011), a system for auto-
matic rule-based post-processing of English-
to-Czech MT outputs designed to increase
their fluency. We enhanced the rule set used
by the original DEPFIX system and measured
the performance of the individual rules.
We also modified the dependency parser of
McDonald et al (2005) in two ways to adjust
it for the parsing of MT outputs. We show that
our system is able to improve the quality of the
state-of-the-art MT systems.
1 Introduction
The today?s outputs of Machine Translation (MT)
often contain serious grammatical errors. This
is particularly apparent in statistical MT systems
(SMT), which do not employ structural linguistic
rules. These systems have been dominating the area
in the recent years (Callison-Burch et al, 2011).
Such errors make the translated text less fluent and
may even lead to unintelligibility or misleading
statements. The problem is more evident in lan-
guages with rich morphology, such as Czech, where
morphological agreement is of a relatively high im-
portance for the interpretation of syntactic relations.
The DEPFIX system (Marec?ek et al, 2011) at-
tempts to correct some of the frequent SMT sys-
?This research has been supported by the European Union
Seventh Framework Programme (FP7) under grant agree-
ment n? 247762 (Faust), and by the grants GAUK116310,
GA201/09/H057 (Res-Informatica), and LH12093.
tems? errors in English-to-Czech translations.1 It an-
alyzes the target sentence (the SMT output in Czech
language) using a morphological tagger and a de-
pendency parser and attempts to correct it by apply-
ing several rules which enforce consistency with the
Czech grammar. Most of the rules use the source
sentence (the SMT input in English language) as a
source of information about the sentence structure.
The source sentence is also tagged and parsed, and
word-to-word alignment with the target sentence is
determined.
In this paper, we present DEPFIX 2012, an im-
proved version of the original DEPFIX 2011 system.
It makes use of a new parser, described briefly in
Section 3, which is adapted to handle the generally
ungrammatical target sentences better. We have also
enhanced the set of grammar correction rules, for
which we give a detailed description in Section 4.
Section 5 gives an account of the experiments per-
formed to evaluate the DEPFIX 2012 system and
compare it to DEPFIX 2011. Section 6 then con-
cludes the paper.
2 Related Work
Our approach can be regarded as converse to the
more common way of using an SMT system to auto-
matically post-edit the output of a rule-based transla-
tion system, as described e.g. in (Simard et al, 2007)
or (Lagarda et al, 2009).
The DEPFIX system is implemented in the
1Although we apply the DEPFIX system just to SMT systems
in this paper as it mainly targets the errors induced by this type
of MT systems, it can be applied to virtually any MT system
(Marec?ek et al, 2011).
362
TectoMT/Treex NLP framework (Popel and
Z?abokrtsky?, 2010),2 using the Morc?e tagger (Spous-
tova? et al, 2007) and the MST parser (McDonald
et al, 2005) trained on the CoNLL 2007 Shared
Task English data (Nivre et al, 2007) to analyze the
source sentences. The source and target sentences
are aligned using GIZA++ (Och and Ney, 2003).
3 Parsing
The DEPFIX 2011 system used the MST parser (Mc-
Donald et al, 2005) with an improved feature set
for Czech (Nova?k and Z?abokrtsky?, 2007) trained on
the Prague Dependency Treebank (PDT) 2.0 (Hajic?
and others, 2006) to analyze the target sentences.
DEPFIX 2012 uses a reimplementation of the MST
parser capable of utilizing parallel features from the
source side in the parsing of the target sentence.
The source text is usually grammatical and there-
fore is likely to be analyzed more reliably. The
source structure obtained in this way can then pro-
vide hints for the target parser. We use local features
projected through the GIZA++ word alignment ? i.e.
for each target word, we add features computed over
its aligned source word, if there is one.
To address the differences between the gold stan-
dard training data and SMT outputs, we ?worsen?
the treebank used to train the parser, i.e. introduce
errors similar to those found in target sentences:
The trees retain their correct structure, only the word
forms are modified to resemble SMT output.
We have computed a ?part-of-speech tag er-
ror model? on parallel sentences from the Prague
Czech-English Dependency Treebank (PCEDT) 2.0
(Bojar et al, 2012), comparing the gold standard
Czech translations to the output of an SMT system
(Koehn et al, 2007) and estimating the Maximum
Likelihood probabilities of errors for each part-of-
speech tag. We then applied this error model to the
Czech PCEDT 2.0 sentences and used the resulting
?worsened? treebank to train the parser.
4 Rules
DEPFIX 2012 uses 20 hand-written rules, address-
ing various frequent errors in MT output. Each
rule takes an analyzed target sentence as its in-
put, often together with its analyzed source sen-
2http://ufal.mff.cuni.cz/treex
tence, and attempts to correct any errors found ?
usually by changing morphosyntactic categories of
a word (such as number, gender, case, person and
dependency label) and regenerating the correspond-
ing word form if necessary, more rarely by deleting
superfluous particles or auxiliary words or changing
the target dependency tree structure. However, nei-
ther word order problems nor bad lexical choices are
corrected.
Many rules were already present in DEPFIX 2011.
However, most were modified in DEPFIX 2012 to
achieve better performance (denoted as modified),
and new rules were added (new). Rules not modified
since DEPFIX 2011 are denoted as reused.
The order of rule application is important as there
are dependencies among the rules, e.g. FixPrepo-
sitionNounAgreement (enforcing noun-preposition
congruency) depends on FixPrepositionalCase (fix-
ing incorrectly tagged prepositional case). The rules
are applied in the order listed in Table 2.
4.1 Analysis Fixing Rules
Analysis fixing rules try to detect and rectify tagger
and parser errors. They do not change word forms
and are therefore invisible on the output as such;
however, rules of other types benefit from their cor-
rections.
FixPrepositionalCase (new)
This rule corrects part-of-speech-tag errors in
prepositional phrases. It looks for all words that de-
pend on a preposition and do not match its part-of-
speech tag case. It tries to find and assign a com-
mon morphological case fitting for both the word
form and the preposition. Infrequent preposition-
case combinations are not considered.
FixReflexiveTantum (new)
If the word form ?se? or ?si? is classified as reflex-
ive tantum particle by the parser, but does not be-
long to an actual reflexive tantum verb (or a dever-
bative noun or an adjective), its dependency label is
changed to a different value, based on the context.
FixNounNumber (reused)
If a noun is tagged as singular in target but as plu-
ral in source, the tag is likely to be incorrect. This
rule tries to find a tag that would match both the
363
source number and the target word form, changing
the target case if necessary.
FixPrepositionWithoutChildren (reused)
A target preposition with no child nodes is clearly
an analysis error. This rule tries to find children for
childless prepositions by projecting the children of
the aligned source preposition to the target side.
FixAuxVChildren (new)
Since auxiliary verbs must not have child nodes,
we rehang all their children to the governing full
verb.
4.2 Agreement Fixing Rules
These rules relate to morphological agreement re-
quired by Czech grammar, which they try to enforce
in case it is violated. Czech grammar requires agree-
ment in morphological gender, number, case and
person where applicable.
These rules typically use the source sentence only
for confirmation.
FixRelativePronoun (new)
The Czech word relative pronoun ?ktery?? is as-
signed gender and number identical to the closest
preceding noun or pronoun, if the source analysis
confirms that it depends on this noun/pronoun.
FixSubject (modified)
The subject (if the subject dependency label is
confirmed by the source analysis) will have its case
set to nominative; the number is changed if this leads
to the word form staying unchanged.
FixVerbAuxBeAgreement (modified)
If an auxiliary verb is a child of an infinitive, the
auxiliary verb receives the gender and number of the
subject, which is a child of the infinitive (see also
FixAuxVChildren).
FixSubjectPredicateAgreement (modified)
An active verb form receives the number and per-
son from its subject (whose relation to the verb must
be confirmed by the source).
FixSubjectPastParticipleAgreement (modified)
A past participle verb form receives the number
and gender from its subject (confirmed by the source
analysis).
FixPassiveAuxBeAgreement (modified)
An auxiliary verb ?by?t? (?to be?) depending on a
passive verb form receives its gender and number.
FixPrepositionNounAgreement (modified)
A noun or adjective depending on a preposition
receives its case. The dependency must be con-
firmed in the source.
FixNounAdjectiveAgreement (modified)
An adjective (or an adjective-like pronoun or nu-
meral) preceding its governing noun receives its
gender, number and case.
4.3 Translation Fixing Rules
The following rules detect and correct structures of-
ten mistranslated by SMT systems. They usually de-
pend heavily on the source sentence.
FixBy (new)
English preposition ?by? is translated to Czech us-
ing the instrumental case (if modifying a verb, e.g.
?built by David?: ?postaveno Davidem?) or using the
genitive case (if modifying a noun, e.g. ?songs by
David?: ?p??sne? Davida?).
FixPresentContinuous (modified)
If the source sentence is in a continuous tense (e.g.
?Ondr?ej isn?t experimenting.?), the auxiliary verb ?to
be? must not appear on the output, which is often
the case (e.g. *?Ondr?ej nen?? experimentovat.?). This
rule deletes the auxiliary verb in target and transfers
its morphological categories to the main verb (e.g.
?Ondr?ej neexperimentuje.?).
FixVerbByEnSubject (new)
If the subject of the source sentence is a personal
pronoun, its following morphological categeries are
propagated to the target predicate:
? person
? number (except for ?you?, which does not ex-
hibit number)
? gender (only in case of ?he? or ?she?, which ex-
hibit the natural gender)
FixOf (new)
English preposition ?of? modifying a noun is
translated to Czech using the genitive case (e.g. ?pic-
tures of Rudolf?: ?obra?zky Rudolfa?).
364
FixAuxT (reused)
Reflexive tantum particles ?se? or ?si? not belong-
ing to any verb or adjective are deleted. This situa-
tion usually occurs when the meaning of the source
verb/adjective is lost in translation and only the par-
ticle is produced.
4.4 Other Rules
VocalizePrepos (reused)
Prepositions ?k?, ?s?, ?v?, ?z? are vocalized (i.e.
changed to ?ke?, ?se?, ?ve?, ?ze?) where neces-
sary. The vocalization rules in Czech are similar to
?a?/?an? distinction in English.
FixFirstWordCapitalization (new)
If the first word of source is capitalized and the
first word of target is not, this rule capitalizes it.
5 Experiments and Results
For parameter tuning, we used datasets from the
WMT10 translation task and translations by ON-
LINEB and CU-BOJAR systems.
5.1 Manual Evaluation
Manual evaluation of both DEPFIX 2011 and DEP-
FIX 2012 was performed on the WMT113 test set
translated by ONLINEB. 500 sentences were ran-
domly selected and blind-evaluated by two indepen-
dent annotators, who were presented with outputs of
ONLINEB, DEPFIX 2011 and DEPFIX 2012. (For
246 sentences, at least one of the DEPFIX setups
modified the ONLINEB translation.) They provided
us with a pairwise comparison of the three setups,
with the possibility to mark the sentence as ?indef-
inite? if translations were of equal quality. The re-
sults are given in Table 1.
In Table 2, we use the manual evaluation to mea-
sure the performance of the individual rules in DEP-
FIX 2012. For each rule, we ran DEPFIX 2012 with
this rule disabled and compared the output to the
output of the full DEPFIX 2012. The number of
affected sentences on the whole WMT11 test set,
given as ?changed?, represents the impact of the
rule. The number of affected sentences selected for
manual evaluation is listed as ?evaluated?. Finally,
the annotators? ratings of the ?evaluated? sentences
3http://www.statmt.org/wmt11
A / B
Setup 1 Setup 2
Indefinite
better better
Setup 1 better 55% 1% 11%
Setup 2 better 1% 8% 4%
Indefinite 3% 2% 15%
Table 3: Inter-annotator agreement matrix for ONLINEB
+ DEPFIX 2012 as Setup 1 and ONLINEB as Setup 2.
(suggesting whether the rule improved or worsened
the translation, or whether the result was indefinite)
were counted and divided by the number of anno-
tators to get the average performance of each rule.
Please note that the lower the ?evaluated? number,
the lower the confidence of the results.
The inter-annotator agreement matrix for com-
parison of ONLINEB + DEPFIX 2012 (denoted as
Setup 1) with ONLINEB (Setup 2) is given in Ta-
ble 3. The results for the other two setup pairs were
similar, with the average inter-annotator agreement
being 77%.
5.2 Automatic Evaluation
We also performed several experiments with auto-
matic evaluation using the standard BLEU metric
(Papineni et al, 2002). As the effect of DEPFIX in
terms of BLEU is rather small, the results are not as
confident as the results of manual evaluation.4
In Table 4, we compare the DEPFIX 2011 and
DEPFIX 2012 systems and measure the contribution
of parser adaptation (Section 3) and rule improve-
ments (Section 4). It can be seen that the com-
bined effect of applying both system modifications
is greater than when they are applied alone. The im-
provement of DEPFIX 2012 over ONLINEB without
DEPFIX is statistically significant at 95% confidence
level.
The effect of DEPFIX 2012 on the outputs of some
of the best-scoring SMT systems in the WMT12
Translation Task5 is shown in Table 5. Although
DEPFIX 2012 was tuned only on ONLINEB and CU-
BOJAR system outputs, it improves the BLEU score
of all the best-scoring systems, which suggests that
4As already noted by Marec?ek et al (2011), BLEU seems
not to be very suitable for evaluation of DEPFIX. See (Kos and
Bojar, 2009) for a detailed study of BLEU performance when
applied to evaluation of MT systems with Czech as the target
language.
5http://www.statmt.org/wmt12
365
Setup 1 Setup 2
Differing
Annotator
Setup 1 Setup 2
Indefinite
sentences better better
ONLINEB
ONLINEB 169
A 58% 13% 29%
+ DEPFIX 2011 B 47% 11% 42%
ONLINEB
ONLINEB 234
A 65% 14% 21%
+ DEPFIX 2012 B 59% 11% 30%
ONLINEB ONLINEB
148
A 54% 24% 22%
+ DEPFIX 2012 + DEPFIX 2011 B 56% 22% 22%
Table 1: Manual pairwise comparison on 500 sentences from WMT11 test set processed by ONLINEB, ONLINEB +
DEPFIX 2011 and ONLINEB + DEPFIX 2012. Evaluated by two independent annotators.
Sentences
Rule changed evaluated impr. % wors. % indef. %
FixPrepositionalCase 34 5 3 60 2 40 0 0
FixReflexiveTantum 1 0 ? ? ? ? ? ?
FixNounNumber 80 11 5 45 5 45 1 9
FixPrepositionWithoutChildren 16 6 3 50 3 50 0 0
FixBy 75 13 10.5 81 1 8 1.5 12
FixAuxVChildren 26 6 4.5 75 0 0 1.5 25
FixRelativePronoun 56 8 6 75 2 25 0 0
FixSubject 142 18 13.5 75 3 17 1.5 8
FixVerbAuxBeAgreement 8 2 1 50 1 50 0 0
FixPresentContinuous 30 7 5.5 79 1 14 0.5 7
FixSubjectPredicateAgreement 87 10 5.5 55 1 10 3.5 35
FixSubjectPastParticipleAgreement 396 63 46.5 74 9.5 15 7 11
FixVerbByEnSubject 25 6 5 83 0 0 1 17
FixPassiveAuxBeAgreement 43 8 6 75 0.5 6 1.5 19
FixPrepositionNounAgreement 388 62 40 65 13 21 9 15
FixOf 84 13 11.5 88 0 0 1.5 12
FixNounAdjectiveAgreement 575 108 69.5 64 20 19 18.5 17
FixAuxT 38 7 4 57 1 14 2 29
VocalizePrepos 53 12 6 50 2.5 21 3.5 29
FixFirstWordCapitalization 0 0 ? ? ? ? ? ?
Table 2: Impact and accuracy of individual DEPFIX 2012 rules using manual evaluation on 500 sentences from
WMT11 test set translated by ONLINEB. The number of changed sentences is counted on the whole WMT11 test
set, i.e. 3003 sentences. The numbers of improved, worsened and indefinite translations are averaged over the annota-
tors.
366
DEPFIX setup BLEU
without DEPFIX 19.37
DEPFIX 2011 19.41
DEPFIX 2011 + new parser 19.42
DEPFIX 2011 + new rules 19.48
DEPFIX 2012 19.56
Table 4: Performance of ONLINEB and various DEPFIX
setups on the WMT11 test set.
System BLEU
ONLINEB 16.25
ONLINEB + DEPFIX 2012 16.31
UEDIN 15.54
UEDIN + DEPFIX 2012 15.75
CU-BOJAR 15.41
CU-BOJAR + DEPFIX 2012 15.45
CU-TAMCH-BOJ 15.35
CU-TAMCH-BOJ + DEPFIX 2012 15.39
Table 5: Comparison of BLEU of baseline system output
and corrected system output on WMT12 test set.
it is able to improve the quality of various SMT
systems when applied to their outputs. (The im-
provement on UEDIN is statistically significant at
95% confidence level.) We submitted the ONLINEB
+ DEPFIX 2012 system to the WMT12 Translation
Task as CU-DEPFIX.
6 Conclusion
We have presented two improvements to DEPFIX,
a system of rule-based post-editing of English-to-
Czech Machine Translation outputs proven by man-
ual and automatic evaluation to improve the qual-
ity of the translations produced by state-of-the-art
SMT systems. First, improvements in the existing
rules and implementation of new ones, which can be
regarded as an additive, evolutionary change. Sec-
ond, a modified dependency parser, adjusted to pars-
ing of SMT outputs by training it on a parallel tree-
bank with worsened word forms on the Czech side.
We showed that both changes led to a better perfor-
mance of the new DEPFIX 2012, both individually
and combined.
In future, we are planning to incorporate deeper
analysis, devising rules that would operate on the
deep-syntactic, or tectogrammatical, layer. The
Czech and English tectogrammatical trees are more
similar to each other, which should enable us to ex-
ploit more information from the source sentences.
We also hope to be able to perform more complex
corrections, such as changing the part of speech of a
word when necessary.
Following the success of our modified parser, we
would also like to modify the tagger in a similar way,
since incorrect analyses produced by the tagger of-
ten hinder the correct function of our rules, some-
times leading to a rule worsening the translation in-
stead of improving it.
As observed e.g. by Groves and Schmidtke (2009)
for English-to-German and English-to-French trans-
lations, SMT systems for other language pairs also
tend to produce reoccurring grammatical errors. We
believe that these could be easily detected and cor-
rected in a rule-based way, using an approach similar
to ours.
References
Ondr?ej Bojar, Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?,
Petr Sgall, Silvie Cinkova?, Eva Fuc???kova?, Marie
Mikulova?, Petr Pajas, Jan Popelka, Jir??? Semecky?,
Jana S?indlerova?, Jan S?te?pa?nek, Josef Toman, Zden?ka
Ures?ova?, and Zdene?k Z?abokrtsky?. 2012. Announc-
ing Prague Czech-English Dependency Treebank 2.0.
In Proceedings of LREC 2012, Istanbul, Turkey, May.
ELRA, European Language Resources Association.
In print.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Declan Groves and Dag Schmidtke. 2009. Identification
and analysis of post-editing patterns for MT. Proceed-
ings of MT Summit XII, pages 429?436.
Jan Hajic? et al 2006. Prague Dependency Treebank 2.0.
CD-ROM, Linguistic Data Consortium, LDC Catalog
No.: LDC2006T0 1, Philadelphia.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007, Proceedings of the 45th Annual Meeting of the
367
Association for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Kamil Kos and Ondr?ej Bojar. 2009. Evaluation of ma-
chine translation metrics for czech as the target lan-
guage. The Prague Bulletin of Mathematical Linguis-
tics, 92(-1):135?148.
Antonio L. Lagarda, Vicent Alabau, Francisco Casacu-
berta, Roberto Silva, and Enrique Diaz-de Liano.
2009. Statistical post-editing of a rule-based ma-
chine translation system. In Proceedings of Human
Language Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, Companion Vol-
ume: Short Papers, pages 217?220. Association for
Computational Linguistics.
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, and
Ondr?ej Bojar. 2011. Two-step translation with gram-
matical post-processing. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
426?432. Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In HLT ?05: Proceed-
ings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL 2007
Shared Task. Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), June.
Va?clav Nova?k and Zdene?k Z?abokrtsky?. 2007. Feature
engineering in maximum spanning tree dependency
parser. In Va?clav Matous?ek and Pavel Mautner, edi-
tors, Lecture Notes in Artificial Intelligence, Proceed-
ings of the 10th I nternational Conference on Text,
Speech and Dialogue, Lecture Notes in Computer Sci-
ence, pages 92?98, Pilsen, Czech Republic. Springer
Science+Business Media Deutschland GmbH.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In ACL 2002, Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318, Philadel-
phia, Pennsylvania.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
modular NLP framework. In Proceedings of the 7th
international conference on Advances in natural lan-
guage processing, IceTAL?10, pages 293?304, Berlin,
Heidelberg. Springer-Verlag.
Michel Simard, Cyril Goutte, and Pierre Isabelle. 2007.
Statistical phrase-based post-editing. In Human Lan-
guage Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 508?515, Rochester, New York, April.
Association for Computational Linguistics.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
368
Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39?48,
Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational Linguistics
Using Parallel Features in Parsing of Machine-Translated Sentences for
Correction of Grammatical Errors ?
Rudolf Rosa, Ondr?ej Dus?ek, David Marec?ek, and Martin Popel
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
{rosa,odusek,marecek,popel}@ufal.mff.cuni.cz
Abstract
In this paper, we present two dependency
parser training methods appropriate for pars-
ing outputs of statistical machine transla-
tion (SMT), which pose problems to standard
parsers due to their frequent ungrammatical-
ity. We adapt the MST parser by exploiting
additional features from the source language,
and by introducing artificial grammatical er-
rors in the parser training data, so that the
training sentences resemble SMT output.
We evaluate the modified parser on DEP-
FIX, a system that improves English-Czech
SMT outputs using automatic rule-based cor-
rections of grammatical mistakes which re-
quires parsed SMT output sentences as its in-
put. Both parser modifications led to im-
provements in BLEU score; their combina-
tion was evaluated manually, showing a sta-
tistically significant improvement of the trans-
lation quality.
1 Introduction
The machine translation (MT) quality is on a steady
rise, with mostly statistical systems (SMT) dominat-
ing the area (Callison-Burch et al, 2010; Callison-
Burch et al, 2011). Most MT systems do not employ
structural linguistic knowledge and even the state-
of-the-art MT solutions are unable to avoid making
serious grammatical errors in the output, which of-
ten leads to unintelligibility or to a risk of misinter-
pretations of the text by a reader.
?This research has been supported by the EU Seventh
Framework Programme under grant agreement n? 247762
(Faust), and by the grants GAUK116310 and GA201/09/H057.
This problem is particularly apparent in target lan-
guages with rich morphological inflection, such as
Czech. As Czech often conveys the relations be-
tween individual words using morphological agree-
ment instead of word order, together with the word
order itself being relatively free, choosing the cor-
rect inflection becomes crucial.
Since the output of phrase-based SMT shows fre-
quent inflection errors (even in adjacent words) due
to each word belonging to a different phrase, a
possible way to address the grammaticality prob-
lem is a combination of statistical and structural ap-
proach, such as SMT output post-editing (Stymne
and Ahrenberg, 2010; Marec?ek et al, 2011).
In this paper, we focus on improving SMT output
parsing quality, as rule-based post-editing systems
rely heavily on the quality of SMT output analy-
sis. Parsers trained on gold standard parse trees of-
ten fail to produce the expected result when applied
to SMT output with grammatical errors. This is
partly caused by the fact that when parsing highly in-
flected free word-order languages the parsers have to
rely on morphological agreement, which, as stated
above, is often erroneous in SMT output.
Training a parser specifically by creating a man-
ually annotated treebank of MT systems? outputs
would be very expensive, and the application of such
treebank to other MT systems than the ones used
for its generation would be problematic. We address
this issue by two methods of increasing the quality
of SMT output parsing:
? a different application of previous works on
bitext parsing ? exploiting additional features
from the source language (Section 3), and
39
? introducing artificial grammatical errors in the
target language parser training data, so that the
sentences resemble the SMT output in some
ways (Section 4). This technique is, to our
knowledge, novel with regards to its applica-
tion to SMT and the statistical error model.
We test these two techniques on English-Czech
MT outputs using our own reimplementation of the
MST parser (McDonald et al, 2005) named RUR1
parser. and evaluate their contribution to the SMT
post-editing quality of the DEPFIX system (Marec?ek
et al, 2011), which we outline in Section 5. We
describe the experiments carried out and present the
most important results in Section 6. Section 7 then
concludes the paper and indicates more possibilities
of further improvements.
2 Related Work
Our approach to parsing with parallel features is
similar to various works which seek to improve the
parsing accuracy on parallel texts (?bitexts?) by us-
ing information from both languages. Huang et
al. (2009) employ ?bilingual constraints? in shift-
reduce parsing to disambiguate difficult syntac-
tic constructions and resolve shift-reduce conflicts.
Chen et al (2010) use similar subtree constraints to
improve parser accuracy in a dependency scenario.
Chen et al (2011) then improve the method by ob-
taining a training parallel treebank via SMT. In re-
cent work, Haulrich (2012) experiments with a setup
very similar to ours: adding alignment-projected
features to an originally monolingual parser.
However, the main aim of all these works is to im-
prove the parsing accuracy on correct parallel texts,
i.e. human-translated. This paper applies similar
methods, but with a different objective in mind ? in-
creasing the ability of the parser to process ungram-
matical SMT output sentences and, ultimately, im-
prove rule-based SMT post-editing.
Xiong et al (2010) use SMT parsing in translation
quality assessment, providing syntactic features to a
classifier detecting erroneous words in SMT output,
yet they do not concentrate on improving parsing ac-
curacy ? they employ a link grammar parser, which
1The abbreviation ?RUR? parser stands for ?Rudolph?s Uni-
versal Robust? parser.
is robust, but not tuned specifically to process un-
grammatical input.
There is also another related direction of research
in parsing of parallel texts, which is targeted on pars-
ing under-resourced languages, e.g. the works by
Hwa et al (2005), Zeman and Resnik (2008), and
McDonald et al (2011). They address the fact that
parsers for the language of interest are of low qual-
ity or even non-existent, whereas there are high-
quality parsers for the other language. They ex-
ploit common properties of both languages and de-
lexicalization. Zhao et al (2009) uses information
from word-by-word translated treebank to obtain ad-
ditional training data and boost parser accuracy.
This is different from our situation, as there ex-
ist high performance parsers for Czech (Buchholz
and Marsi, 2006; Nivre et al, 2007; Hajic? et al,
2009). Boosting accuracy on correct sentences is
not our primary goal and we do not intend to re-
place the Czech parser by an English parser; instead,
we aim to increase the robustness of an already ex-
isting Czech parser by adding knowledge from the
corresponding English source, parsed by an English
parser.
Other works in bilingual parsing aim to parse the
parallel sentences directly using a grammar formal-
ism fit for this purpose, such as Inversion Trans-
duction Grammars (ITG) (Wu, 1997). Burkett et
al. (2010) further include ITG parsing with word-
alignment in a joint scenario. We concentrate here
on using dependency parsers because of tools and
training data availability for the examined language
pair.
Regarding treebank adaptation for parser robust-
ness, Foster et al (2008) introduce various kinds of
artificial errors into the training data to make the fi-
nal parser less sensitive to grammar errors. How-
ever, their approach concentrates on mistakes made
by humans (such as misspellings, word repetition or
omission etc.) and the error models used are hand-
crafted. Our work focuses on morphology errors of-
ten encountered in SMT output and introduces sta-
tistical error modelling.
3 Parsing with Parallel Features
This section describes our SMT output parsing setup
with features from analyzed source sentences. We
40
explain our motivation for the inclusion of parallel
features in Section 3.1, then provide an account of
the parsers used (including our RUR parser) in Sec-
tion 3.2, and finally list all the monolingual and par-
allel features included in the parser training (in Sec-
tions 3.3 and 3.4, respectively).
3.1 Motivation
An advantage of SMT output parsing over general
dependency parsing is that one can also make use of
source ? English sentences in our case. Moreover,
although SMT output is often in many ways ungram-
matical, source is usually grammatical and therefore
easier to process (in our case especially to tag and
parse). This was already noticed in Marec?ek et al
(2011), who use the analysis of source sentence to
provide additional information for the DEPFIX rules,
claiming it to be more reliable than the analysis of
SMT output sentence.
We have carried this idea further by having de-
vised a simple way of making use of this information
in parsing of the SMT output sentences: We parse
the source sentence first and include features com-
puted over the parsed source sentence in the set of
features used for parsing SMT output. We first align
the source and SMT output sentences on the word
level and then use alignment-wise local features ?
i.e. for each SMT output word, we add features com-
puted over its aligned source word, if applicable (cf.
Section 3.4 for a listing).
3.2 Parsers Used
We have reimplemented the MST parser (McDonald
et al, 2005) in order to provide for a simple insertion
of the parallel features into the models.
We also used the original implementation of the
MST parser by McDonald et al (2006) for com-
parison in our experiments. To distinguish the two
variants used, we denote the original MST parser
as MCD parser,2 and the new reimplementation as
RUR parser.
We trained RUR parser in a first-order non-
projective setting with single-best MIRA. Depen-
dency labels are assigned in a second stage by a
2MCD uses k-best MIRA, does first- and second-order
parsing, both projectively and non-projectively, and can be
obtained from http://sourceforge.net/projects/
mstparser.
MIRA-based labeler, which has been implemented
according to McDonald (2006) and Gimpel and Co-
hen (2007).
We used the Prague Czech-English Dependency
Treebank3 (PCEDT) 2.0 (Bojar et al, 2012) as the
training data for RUR parser ? a parallel treebank
created from the Penn Treebank (Marcus et al,
1993) and its translation into Czech by human trans-
lators. The dependency trees on the English side
were converted from the manually annotated phrase-
structure trees in Penn Treebank, the Czech trees
were created automatically using MCD. Words of
the Czech and English sentences were aligned by
GIZA++ (Och and Ney, 2003).
We apply RUR parser only for SMT output pars-
ing; for source parsing, we use MCD parser trained
on the English CoNLL 2007 data (Nivre et al,
2007), as the performance of this parser is sufficient
for this task.
3.3 Monolingual Features
The set of monolingual features used in RUR parser
follows those described by McDonald et al (2005).
For parsing, we use the features described below.
The individual features are computed for both the
parent node and the child node of an edge and con-
joined in various ways. The coarse morphological
tag and lemma are provided by the Morc?e tagger
(Spoustova? et al, 2007).
? coarse morphological tag ? Czech two-letter
coarse morphological tag, as described in
(Collins et al, 1999),4
? lemma ? morphological lemma,
? context features: preceding coarse morpholog-
ical tag, following coarse morphological tag
? coarse morphological tag of a neighboring
node,
? coarse morphological tags in between ? bag of
coarse morphological tags of nodes positioned
between the parent node and the child node,
3http://ufal.mff.cuni.cz/pcedt
4The first letter is the main POS (12 possible values), the
second letter is either the morphological case field if the main
POS displays case (i.e. for nouns, adjectives, pronouns, numer-
als and prepositions; 7 possible values), or the detailed POS if
it does not (22 possible values).
41
? distance ? signed bucketed distance of the par-
ent and the child node in the sentence (in # of
words), using buckets 1, 2, 3, 4, 5 and 11.
To assign dependency labels, we use the same
set as described above, plus the following features
(called ?non-local? by McDonald (2006)), which
make use of the knowledge of the tree structure.
? is first child, is last child ? a boolean indicating
whether the node appears in the sentence as the
first/last one among all the child nodes of its
parent node,
? child number ? the number of syntactic chil-
dren of the current node.
3.4 Parallel Features
Figure 1: Example sentence for parallel features illustra-
tion (see Table 1).
In RUR parser we use three types of parallel fea-
tures, computed for the parent and child node of an
edge, which make use of the source English nodes
aligned to the parent and child node.
? aligned tag: morphological tag following the
Penn Treebank Tagset (Marcus et al, 1993) of
the English node aligned to the Czech node
Feature Feature value on
parent node child node
word form jel Martin
aligned tag VBD NNP
aligned dep. label Pred Sb
aligned edge existence true
word form jel autem
aligned tag VBD NN
aligned dep. label Pred Adv
aligned edge existence false
word form do zahranic???
aligned tag ? RB
aligned dep. label ? Adv
aligned edge existence ?
word form #root# .
aligned tag #root# .
aligned dep. label AuxS AuxK
aligned edge existence true
Table 1: Parallel features for several edges in Figure 1.
? aligned dependency label: dependency label of
the English node aligned to the Czech node in
question, according to the PCEDT 2.0 label set
(Bojar et al, 2012)
? aligned edge existence: a boolean indicating
whether the English node aligned to the Czech
parent node is also the parent of the English
node aligned to the Czech child node
The parallel features are conjoined with the
monolingual coarse morphological tag and lemma
features in various ways.
If there is no source node aligned to the parent
or child node, the respective feature cannot be com-
puted and is skipped.
An example of a pair of parallel sentences is given
in Figure 1 with the corresponding values of parallel
features for several edges in Table 1.
4 Worsening Treebanks to Simulate Some
of the SMT Frequent Errors
Addressing the issue of great differences between
the gold standard parser training data and the actual
analysis input (SMT output), we introduced artificial
inconsistencies into the training treebanks, in order
to make the parsers more robust in the face of gram-
mar errors made by SMT systems. We have concen-
42
trated solely on modeling incorrect word flection,
i.e. the dependency trees retained their original cor-
rect structures and word lemmas remained fixed, but
the individual inflected word forms have been modi-
fied according to an error model trained on real SMT
output. We simulate thus, with respect to morphol-
ogy, a treebank of parsed MT output sentences.
In Section 4.1 we describe the steps we take to
prepare the worsened parser training data. Sec-
tion 4.2 contains a description of our monolingual
greedy alignment tool which is needed during the
process to map SMT output to reference transla-
tions.
4.1 Creating the Worsened Parser Training
Data
The whole process of treebank worsening consists
of five steps:
1. We translated the English side of PCEDT5 to
Czech using SMT (we chose the Moses sys-
tem (Koehn et al, 2007) for our experiments)
and tagged the resulting translations using the
Morc?e tagger (Spoustova? et al, 2007).
2. We aligned the Czech side of PCEDT, now
serving as a reference translation, to the SMT
output using our Monolingual Greedy Aligner
(see Section 4.2).
3. Collecting the counts of individual errors, we
estimated the Maximum Likelihood probabili-
ties of changing a correct fine-grained morpho-
logical tag (of a word from the reference) into
a possibly incorrect fine-grained morphological
tag of the aligned word (from the SMT output).
4. The tags on the Czech side of PCEDT were
randomly sampled according to the estimated
?fine-grained morphological tag error model?.
In those positions where fine-grained morpho-
logical tags were changed, new word forms
were generated using the Czech morphological
generator by Hajic? (2004).6
5This approach is not conditioned by availability of parallel
treebanks. Alternatively, we might translate any text for which
reference translations are at hand. The model learned in the
third step would then be applied (in the fourth step) to a different
text for which parse trees are available.
6According to the ?fine-grained morphological tag error
We use the resulting ?worsened? treebank to train
our parser described in Section 3.2.
4.2 The Monolingual Greedy Aligner
Our monolingual alignment tool, used in treebank
worsening to tie reference translations to MT out-
put (see Section 4.1), scores all possible alignment
links and then greedily chooses the currently highest
scoring one, creating the respective alignment link
from word A (in the reference) to word B (in the
SMT output) and deleting all scores of links from A
or to B, so that one-to-one alignments are enforced.
The process is terminated when no links with a score
higher than a given threshold are available; some
words may thus remain unaligned.
The score is computed as a linear combination of
the following four features:
? word form (or lemma if available) similar-
ity based on Jaro-Winkler distance (Winkler,
1990),
? fine-grained morphological tag similarity,
? similarity of the relative position in the sen-
tence,
? and an indication whether the word following
(or preceding) A was already aligned to the
word following (or preceding) B.
Unlike bilingual word aligners, this tool needs no
training except for setting weights of the four fea-
tures and the threshold.7
5 The DEPFIX System
The DEPFIX system (Marec?ek et al, 2011) applies
various rule-based corrections to Czech-English
SMT output sentences, especially of morphological
agreement. It also employs the parsed source sen-
tences, which must be provided on the input together
with the SMT output sentences.
The corrections follow the rules of Czech gram-
mar, e.g. requiring that the clause subject be in the
model?, about 20% of fine-grained morphological tags were
changed. In 4% of cases, no word form existed for the new
fine-grained morphological tag and thus it was not changed.
7The threshold and weights were set manually using just ten
sentence pairs. The resulting alignment quality was found suf-
ficient, so no additional weights tuning was performed.
43
nominative case or enforcing subject-predicate and
noun-attribute agreements in morphological gender,
number and case, where applicable. Morphological
properties found violating the rules are corrected and
the corresponding word forms regenerated.
The source sentence parse, word-aligned to the
SMT output using GIZA++ (Och and Ney, 2003),
is used as a source of morpho-syntactic information
for the correction rules. An example of a correction
rule application is given in Figure 2.
Some
people
came
later
Atr
Sb
Pred
Advplpl
.AuxK
p?i?liPredpl
N?kte??
lid?
p?i?el
pozd?ji
Atr
Sb
Pred
Advsg, mpl
.AuxK
Figure 2: Example of fixing subject-predicate agreement.
The Czech word pr?is?el [he came] has a wrong morpho-
logical number and gender. Adapted from Marec?ek et al
(2011).
The system is implemented within the
TectoMT/Treex NLP framework (Popel and
Z?abokrtsky?, 2010). Marec?ek et al (2011) feed the
DEPFIX system with analyses by the MCD parser
trained on gold-standard treebanks for parsing of
English source sentences as well as Czech SMT
output.
6 Experiments and Results
We evaluate RUR parser indirectly by using it in the
DEPFIX system and measuring the performance of
the whole system. This approach has been chosen
instead of direct evaluation of the SMT output parse
trees, as the task of finding a correct parse tree of
a possibly grammatically incorrect sentence is not
well defined and considerably difficult to do.
We used WMT10, WMT11 and WMT12 En-
glish to Czech translation test sets, newssyscomb-
test2010, newssyscombtest2011 and news-
test2012,8 (denoted as WMT10, WMT11 and
8http://www.statmt.org/wmt10,
WMT12) for the automatic evaluation. The data sets
include the source (English) text, its reference trans-
lation and translations produced by several MT sys-
tems. We used the outputs of three SMT systems:
GOOGLE,9 UEDIN (Koehn et al, 2007) and BOJAR
(Bojar and Kos, 2010).
For the manual evaluation, two sets of 1000 ran-
domly selected sentences from WMT11 and from
WMT12 translated by GOOGLE were used.
6.1 Automatic Evaluation
Table 2 shows BLEU scores (Papineni et al, 2002)
for the following setups of DEPFIX:
? SMT output: output of an SMT system without
applying DEPFIX
? MCD: parsing with MCD
? RUR: parsing with RUR (Section 3.2)
? RUR+PARA: parsing with RUR using parallel
features (Section 3.4)
? RUR+WORS: parsing with RUR trained on
worsened treebank (Section 4)
? RUR+WORS+PARA: parsing with RUR
trained on worsened treebank and using
parallel features
It can be seen that both of the proposed ways of
adapting the parser to parsing of SMT output of-
ten lead to higher BLEU scores of translations post-
processed by DEPFIX, which suggests that they both
improve the parsing accuracy.
We have computed 95% confidence intervals
on 1000 bootstrap samples, which showed that
the BLEU score of RUR+WORS+PARA was sig-
nificantly higher than that of MCD and RUR
parser in 4 and 3 cases, respectively (results
where RUR+WORS+PARA achieved a significantly
higher score are marked with ?*?). On the other
hand, the score of neither RUR+WORS+PARA nor
RUR+WORS and RUR+PARA was ever signifi-
cantly lower than the score of MCD or RUR parser.
This leads us to believe that the two proposed meth-
ods are able to produce slightly better SMT output
parsing results.
http://www.statmt.org/wmt11,
http://www.statmt.org/wmt12
9http://translate.google.com
44
Test set WMT10 WMT11 WMT12
SMT system BOJAR GOOGLE UEDIN BOJAR GOOGLE UEDIN BOJAR GOOGLE UEDIN
SMT output *15.85 *16.57 *15.91 *16.88 *20.26 *17.80 14.36 16.25 *15.54
MCD 16.09 16.95 *16.35 *17.02 20.45 *18.12 14.35 16.32 *15.65
RUR 16.08 *16.85 *16.29 17.03 20.42 *18.09 14.37 16.31 15.66
RUR+PARA 16.13 *16.90 *16.35 17.05 20.47 18.19 14.35 16.31 15.72
RUR+WORS 16.12 16.96 *16.45 17.06 20.53 18.21 14.40 16.31 15.71
RUR+WORS+PARA 16.13 17.03 16.54 17.12 20.53 18.25 14.39 16.30 15.74
Table 2: Automatic evaluation using BLEU scores for the unmodified SMT output (output of BOJAR, GOOGLE and
UEDIN systems on WMT10, WMT11 and WMT12 test sets), and for SMT output parsed by various parser setups and
processed by DEPFIX. The score of RUR+WORS+PARA is significantly higher at 95% confidence level than the scores
marked with ?*? on the same data.
6.2 Manual Evaluation
Performance of RUR+WORS+PARA setup was man-
ually evaluated by doing a pairwise comparison with
other setups ? SMT output, MCD and RUR parser.
The evaluation was performed on both the WMT11
(Table 4) and WMT12 (Table 5) test set. 1000 sen-
tences from the output of the GOOGLE system were
randomly selected and processed by DEPFIX, using
the aforementioned SMT output parsers. The anno-
tators then compared the translation quality of the
individual variants in differing sentences, selecting
the better variant from a pair or declaring two vari-
ants ?same quality? (indefinite). They were also pro-
vided with the source sentence and a reference trans-
lation. The evaluation was done as a blind test, with
the sentences randomly shuffled.
The WMT11 test set was evaluated by two inde-
pendent annotators. (The WMT12 test set was eval-
uated by one annotator only.) The inter-annotator
agreement and Cohen?s kappa coefficient (Cohen
and others, 1960), shown in Table 3, were computed
both including all annotations (?with indefs?), and
disregarding sentences where at least one of the an-
notators marked the difference as indefinite (?with-
out indefs?) ? we believe a disagreement in choos-
ing the better translation to be more severe than a
disagreement in deciding whether the difference in
quality of the translations allows to mark one as be-
ing better.
For both of the test sets, RUR+WORS+PARA sig-
nificantly outperforms both MCD and RUR base-
line, confirming that a combination of the proposed
modifications of the parser lead to its better perfor-
mance. Statistical significance of the results was
RUR+WORS+PARA with indefs without indefs
compared to IAA Kappa IAA Kappa
SMT output 77% 0.54 92% 0.74
MCD 79% 0.66 95% 0.90
RUR 75% 0.60 94% 0.85
Table 3: Inter-annotator agreement on WMT11 data set
translated by GOOGLE
confirmed by a one-sided pairwise t-test, with the
following differences ranking: RUR+WORS+PARA
better = 1, baseline better = -1, indefinite = 0.
6.3 Inspection of Parser Modification Benefits
For a better understanding of the benefits of using
our modified parser, we inspected a small number of
parse trees, produced by RUR+WORS+PARA, and
compared them to those produced by RUR.
In many cases, the changes introduced by
RUR+WORS+PARA were clearly positive. We
provide two representative examples below.
Subject Identification
Czech grammar requires the subject to be in nom-
inative case, but this constraint is often violated in
SMT output and a parser typically fails to identify
the subject correctly in such situations. By wors-
ening the training data, we make the parser more ro-
bust in this respect, as the worsening often switches
the case of the subject; by including parallel fea-
tures, especially the aligned dependency label fea-
ture, RUR+WORS+PARA parser can often identify
the subject as the node aligned to the source subject.
45
Out of the differing sentences
Annotator Baseline Differing sentences RUR+WORS+PARA better baseline better indefinite
count percent count percent count percent
SMT output 422 301 71% 79 19% 42 10%
A MCD 211 120 57% 65 31% 26 12%
RUR 217 123 57% 64 29% 30 14%
SMT output 422 284 67% 69 16% 69 16%
B MCD 211 107 51% 56 26% 48 23%
RUR 217 118 54% 53 24% 46 21%
Table 4: Manual comparison of RUR+WORS+PARA with various baselines, on 1000 sentences from WMT11 data set
translated by GOOGLE, evaluated by two independent annotators.
Out of the differing sentences
Annotator Baseline Differing sentences RUR+WORS+PARA better baseline better indefinite
count percent count percent count percent
SMT output 420 270 64% 88 21% 62 15%
A MCD 188 86 45% 64 34% 38 20%
RUR 187 96 51% 57 30% 34 18%
Table 5: Manual comparison of RUR+WORS+PARA with various baselines, on 1000 sentences from WMT12 data set
translated by GOOGLE.
Governing Noun Identification
A parser for Czech typically relies on morpho-
logical agreement between an adjective and its gov-
erning noun (in morphological number, gender and
case), which is often violated in SMT output. Again,
RUR+WORS+PARA is more robust in this respect,
aligned edge existence now being the crucial feature
for the correct identification of this relation.
7 Conclusions and Future Work
We have studied two methods of improving the pars-
ing quality of Machine Translation outputs by pro-
viding additional information to the parser.
In Section 3, we propose a method of integrat-
ing additional information known at runtime, i.e.
the knowledge of the source sentence (source), from
which the sentence being parsed (SMT output) has
been translated. This knowledge is provided by
extending the parser feature set with new features
from the source sentence, projected through word-
alignment.
In Section 4, we introduce a method of utilizing
additional information known in the training phase,
namely the knowledge of the ways in which SMT
output differs from correct sentences. We provide
this knowledge to the parser by adjusting its training
data to model some of the errors frequently encoun-
tered in SMT output, i.e. incorrect inflection forms.
We have evaluated the usefulness of these two
methods by integrating them into the DEPFIX rule-
based MT output post-processing system (Marec?ek
et al, 2011), as MT output parsing is crucial for the
operation of this system. When used with our im-
proved parsing, the DEPFIX system showed better
performance both in automatic and manual evalua-
tion on outputs of several, including state-of-the-art,
MT systems.
We believe that the proposed methods of improv-
ing MT output parsing can be extended beyond their
current state. The parallel features used in our setup
are very few and very simple; it thus remains to
be examined whether more elaborate features could
help utilize the additional information contained in
the source sentence to a greater extent. Modeling
other types of SMT output inconsistencies in parser
training data is another possible step.
We also believe that the methods could be adapted
for use in other applications, e.g. automatic classifi-
cation of translation errors, confidence estimation or
multilingual question answering.
46
References
Ondr?ej Bojar and Kamil Kos. 2010. 2010 Failures in
English-Czech Phrase-Based MT. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 60?66, Uppsala, Swe-
den, July. Association for Computational Linguistics.
Ondr?ej Bojar, Jan Hajic?, Eva Hajic?ova?, Jarmila Panevova?,
Petr Sgall, Silvie Cinkova?, Eva Fuc???kova?, Marie
Mikulova?, Petr Pajas, Jan Popelka, Jir??? Semecky?,
Jana S?indlerova?, Jan S?te?pa?nek, Josef Toman, Zden?ka
Ures?ova?, and Zdene?k Z?abokrtsky?. 2012. Announc-
ing Prague Czech-English Dependency Treebank 2.0.
In Proceedings of LREC 2012, Istanbul, Turkey, May.
ELRA, European Language Resources Association.
In print.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Natural Language Learning, pages 149?164.
Association for Computational Linguistics.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 127?135. Association for Computational Lin-
guistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July. Associ-
ation for Computational Linguistics.
Wenliang Chen, Jun?ichi Kazama, and Kentaro Torisawa.
2010. Bitext dependency parsing with bilingual sub-
tree constraints. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 21?29. Association for Computational Lin-
guistics.
Wenliang Chen, Jun?ichi Kazama, Min Zhang, Yoshi-
masa Tsuruoka, Yujie Zhang, Yiou Wang, Kentaro
Torisawa, and Haizhou Li. 2011. SMT helps bitext
dependency parsing. In EMNLP, pages 73?83. ACL.
Jacob Cohen et al 1960. A coefficient of agreement for
nominal scales. Educational and psychological mea-
surement, 20(1):37?46.
Michael Collins, Lance Ramshaw, Jan Hajic?, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th annual meeting
of the Association for Computational Linguistics on
Computational Linguistics, ACL ?99, pages 505?512,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Jennifer Foster, Joachim Wagner, and Josef Van Gen-
abith. 2008. Adapting a WSJ-trained parser to gram-
matically noisy text. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics on Human Language Technologies: Short
Papers, pages 221?224. Association for Computa-
tional Linguistics.
Kevin Gimpel and Shay Cohen. 2007. Discriminative
online algorithms for sequence labeling- a comparative
study.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, et al 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the Thir-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, pages 1?18. Associa-
tion for Computational Linguistics.
Jan Hajic?. 2004. Disambiguation of rich inflection: com-
putational morphology of Czech. Karolinum.
Martin Haulrich. 2012. Data-Driven Bitext Dependency
Parsing and Alignment. Ph.D. thesis.
Liang Huang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing:
Volume 3-Volume 3, pages 1222?1231. Association for
Computational Linguistics.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11:311?325, September.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In ACL
2007, Proceedings of the 45th Annual Meeting of the
Association for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
47
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Comput. Lin-
guist., 19:313?330, June.
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, and
Ondr?ej Bojar. 2011. Two-step translation with gram-
matical post-processing. In Chris Callison-Burch,
Philipp Koehn, Christof Monz, and Omar Zaidan, edi-
tors, Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 426?432, Edinburgh, UK.
Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-projective dependency parsing
using spanning tree algorithms. In HLT ?05: Proceed-
ings of the conference on Human Language Technol-
ogy and Empirical Methods in Natural Language Pro-
cessing, pages 523?530, Vancouver, British Columbia,
Canada.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of the
Tenth Conference on Computational Natural Lan-
guage Learning, CoNLL-X ?06, pages 216?220,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing, pages
62?72. Association for Computational Linguistics.
Ryan McDonald. 2006. Discriminative learning and
spanning tree algorithms for dependency parsing.
Ph.D. thesis, Philadelphia, PA, USA. AAI3225503.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on de-
pendency parsing. In Proceedings of the CoNLL 2007
Shared Task. Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), June.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19?51.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In ACL 2002, Proceed-
ings of the 40th Annual Meeting of the Association for
Computational Linguistics, pages 311?318, Philadel-
phia, Pennsylvania.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. TectoMT:
modular NLP framework. In Proceedings of the 7th
international conference on Advances in natural lan-
guage processing, IceTAL?10, pages 293?304, Berlin,
Heidelberg. Springer-Verlag.
Drahom??ra Spoustova?, Jan Hajic?, Jan Votrubec, Pavel Kr-
bec, and Pavel Kve?ton?. 2007. The best of two worlds:
Cooperation of statistical and rule-based taggers for
Czech. In Proceedings of the Workshop on Balto-
Slavonic Natural Language Processing, ACL 2007,
pages 67?74, Praha.
Sara Stymne and Lars Ahrenberg. 2010. Using a gram-
mar checker for evaluation and postprocessing of sta-
tistical machine translation. In Proceedings of LREC,
pages 2175?2181.
William E. Winkler. 1990. String comparator met-
rics and enhanced decision rules in the Fellegi-Sunter
model of record linkage. In Proceedings of the Section
on Survey Research Methods (American Statistical As-
sociation), pages 354?359.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377?403.
Deyi Xiong, Min Zhang, and Haizhou Li. 2010. Er-
ror detection for statistical machine translation using
linguistic features. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 604?611. Association for Computational
Linguistics.
Daniel Zeman and Philip Resnik. 2008. Cross-language
parser adaptation between related languages. NLP for
Less Privileged Languages, page 35.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1-Volume 1, pages
55?63. Association for Computational Linguistics.
48
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 92?98,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Chimera ? Three Heads for English-to-Czech Translation
Ondr?ej Bojar and Rudolf Rosa and Ales? Tamchyna
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostranske? na?me?st?? 25, Prague, Czech Republic
surname@ufal.mff.cuni.cz
Abstract
This paper describes our WMT submis-
sions CU-BOJAR and CU-DEPFIX, the lat-
ter dubbed ?CHIMERA? because it com-
bines on three diverse approaches: Tec-
toMT, a system with transfer at the deep
syntactic level of representation, factored
phrase-based translation using Moses, and
finally automatic rule-based correction of
frequent grammatical and meaning errors.
We do not use any off-the-shelf system-
combination method.
1 Introduction
Targeting Czech in statistical machine transla-
tion (SMT) is notoriously difficult due to the
large number of possible word forms and com-
plex agreement rules. Previous attempts to resolve
these issues include specific probabilistic models
(Subotin, 2011) or leaving the morphological gen-
eration to a separate processing step (Fraser et al,
2012; Marec?ek et al, 2011).
TectoMT (CU-TECTOMT, Galus?c?a?kova? et al
(2013)) is a hybrid (rule-based and statistical) MT
system that closely follows the analysis-transfer-
synthesis pipeline. As such, it suffers from many
issues but generating word forms in proper agree-
ments with their neighbourhood as well as the
translation of some diverging syntactic structures
are handled well. Overall, TectoMT sometimes
even ties with a highly tuned Moses configuration
in manual evaluations, see Bojar et al (2011).
Finally, Rosa et al (2012) describes Depfix, a
rule-based system for post-processing (S)MT out-
put that corrects some morphological, syntactic
and even semantic mistakes. Depfix was able to
significantly improve Google output in WMT12,
so now we applied it on an open-source system.
Our WMT13 system is thus a three-headed
creature where, hopefully: (1) TectoMT provides
missing word forms and safely handles some non-
parallel syntactic constructions, (2) Moses ex-
ploits very large parallel and monolingual data,
and boosts better lexical choice, (3) Depfix at-
tempts to fix severe flaws in Moses output.
2 System Description
TectoMT
Moses
cu-tectomt
Depfix
cu-bojar
cu-depfix = Chimera
Input
Figure 1: CHIMERA: three systems combined.
CHIMERA is a sequential combination of three
diverse MT systems as depicted in Figure 1. Each
of the intermediate stages of processing has been
submitted as a separate primary system for the
WMT manual evalution, allowing for a more thor-
ough analysis.
Instead of an off-the-shelf system combination
technique, we use TectoMT output as synthetic
training data for Moses as described in Section 2.1
and finally we process its output using rule-based
corrections of Depfix (Section 2.2). All steps di-
rectly use the source sentence.
2.1 Moses Setup for CU-BOJAR
We ran a couple of probes with reduced training
data around the setup of Moses that proved suc-
cessful in previous years (Bojar et al, 2012a).
2.1.1 Pre-processing
We use a stable pre-processing pipeline that in-
cludes normalization of quotation marks,1 tok-
enization, tagging and lemmatization with tools
1We do not simply convert them to unpaired ASCII quotes
but rather balance them and use other heuristics to convert
most cases to the typographically correct form.
92
Case recaser lc?form utc stc
BLEU 9.05 9.13 9.70 9.81
Table 1: Letter Casing
included in the Treex platform (Popel and
Z?abokrtsky?, 2010).
This year, we evaluated the end-to-end effect of
truecasing. Ideally, English-Czech SMT should be
trained on data where only names are uppercased
(and neither the beginnings of sentences, nor all-
caps headlines or exclamations etc). For these ex-
periments, we trained a simple baseline system on
1 million sentence pairs from CzEng 1.0.
Table 1 summarizes the final (case-sensitive!)
BLEU scores for four setups. The standard ap-
proach is to train SMT lowercase and apply a re-
caser, e.g. the Moses one, on the output. Another
option (denoted ?lc?form?) is to lowercase only
the source side of the parallel data. This more
or less makes the translation model responsible
for identifying names and the language model for
identifying beginnings of sentences.
The final two approaches attempt at ?truecas-
ing? the data, i.e. the ideal lowercasing of ev-
erything except names. Our simple unsupervised
truecaser (?utc?) uses a model trained on monolin-
gual data (1 million sentences in this case, same
as the parallel training data used in this experi-
ment) to identify the most frequent ?casing shape?
of each token type when it appears within a sen-
tence and then converts its occurrences at the be-
ginnings of sentences to this shape. Our super-
vised truecaser (?stc?) casts the case of the lemma
on the form, because our lemmatizers for English
and Czech produce case-sensitive lemmas to indi-
cate names. After the translation, only determinis-
tic uppercasing of sentence beginnings is needed.
We confirm that ?stc? as we have been using it
for a couple of years is indeed the best option, de-
spite its unpleasingly frequent omissions of names
(incl. ?Spojene? sta?ty?, ?the United States?). One
of the rules in Depfix tries to cast the case from
the source to the MT output but due to alignment
errors, it is not perfect in fixing these mistakes.
Surprisingly, the standard recasing worked
worse than ?lc?form?, suggesting that two Moses
runs in a row are worse than one joint search.
We consider using a full-fledged named entity
recognizer in the future.
Tokens [M]
Corpus Sents [M] English Czech
CzEng 1.0 14.83 235.67 205.17
Europarl 0.65 17.61 15.00
Common Crawl 0.16 4.08 3.63
Table 2: Basic Statistics of Parallel Data.
2.1.2 Factored Translation for Morphological
Coherence
We use a quite standard factored configuration of
Moses. We translate from ?stc? to two factors:
?stc? and ?tag? (full Czech positional morpholog-
ical tag). Even though tags on the target side make
the data somewhat sparser (a single Czech word
form typically represents several cases, numbers
or genders), we do not use any back-off or alterna-
tive decoding path. A high-order language model
on tags is used to promote grammatically correct
and coherent output. Our system is thus less prone
to errors in local morphological agreement.
2.1.3 Large Parallel Data
The main source of our parallel data was CzEng
1.0 (Bojar et al, 2012b). We also used Europarl
(Koehn, 2005) as made available by WMT13 orga-
nizers.2 The English-Czech part of the new Com-
mon Crawl corpus was quite small and very noisy,
so we did not include it in our training data. Ta-
ble 2 provides basic statistics of the data.
Processing large parallel data can be challeng-
ing in terms of time and computational resources
required. The main bottlenecks are word align-
ment and phrase extraction.
GIZA++ (Och and Ney, 2000) has been the
standard tool for computing word alignment in
phrase-based MT. A multi-threaded version exists
(Gao and Vogel, 2008), which also supports incre-
mental extensions of parallel data by applying a
saved model on a new sentence pair. We evaluated
these tools and measured their wall-clock time3 as
well as the final BLEU score of a full MT system.
Surprisingly, single-threaded GIZA++ was con-
siderably faster than single-threaded MGIZA. Us-
ing 12 threads, MGIZA outperformed GIZA++
but the difference was smaller than we expected.
Table 3 summarizes the results. We checked the
difference in BLEU using the procedure by Clark
et al (2011) and GIZA++ alignments were indeed
2http://www.statmt.org/wmt13/
translation-task.html
3Time measurements are only indicative, they were af-
fected by the current load in our cluster.
93
Alignment Wallclock Time BLEU
GIZA++ 71 15.5
MGIZA 1 thread 114 15.4
MGIZA 12 threads 51 15.4
Table 3: Rough wallclock time [hours] of word
alignment and the resulting BLEU scores.
Corpus Sents [M] Tokens [M]
CzEng 1.0 14.83 205.17
CWC Articles 36.72 626.86
CNC News 28.08 483.88
CNA 47.00 830.32
Newspapers 64.39 1040.80
News Crawl 24.91 444.84
Total 215.93 3631.87
Table 4: Basic Statistics of Monolingual Data.
little but significantly better than MGIZA in three
MERT runs.
We thus use the standard GIZA++ aligner.
2.1.4 Large Language Models
We were able to collect a very large amount of
monolingual data for Czech: almost 216 million
sentences, 3.6 billion tokens. Table 4 lists the
corpora we used. CWC Articles is a section of
the Czech Web Corpus (Spoustova? and Spousta,
2012). CNC News refers to a subset of the Czech
National Corpus4 from the news domain. CNA
is a corpus of Czech News Agency stories from
1998 to 2012. Newspapers is a collection of ar-
ticles from various Czech newspapers from years
1998 to 2002. Finally, News Crawl is the mono-
lingual corpus made available by the organizers of
WMT13.
We created an in-domain language model from
all the corpora except for CzEng (where we only
used the news section). We were able to train a 4-
gram language model using KenLM (Heafield et
al., 2013). Unfortunately, we did not manage to
use a model of higher order. The model file (even
in the binarized trie format with probability quan-
tization) was so large that we ran out of memory
in decoding.5 We also tried pruning these larger
models but we did not have enough RAM.
To cater for a longer-range coherence, we
trained a 7-gram language model only on the News
Crawl corpus (concatenation of all years). In this
case, we used SRILM (Stolcke, 2002) and pruned
n-grams so that (training set) model perplexity
4http://korpus.cz/
5Due to our cluster configuration, we need to pre-load lan-
guage models.
Token Order Sents Tokens ARPA.gz Trie
[M] [M] [GB] [GB]
stc 4 201.31 3430.92 28.2 11.8
stc 7 24.91 444.84 13.1 8.1
tag 10 14.83 205.17 7.2 3.0
Table 5: LMs used in CU-BOJAR.
does not increase more than 10?14. The data for
this LM exactly match the domain of WMT test
sets.
Finally, we model sequences of morphological
tags on the target side using a 10-gram LM es-
timated from CzEng. Individual sections of the
corpus (news, fiction, subtitles, EU legislation,
web pages, technical documentation and Navajo
project) were interpolated to match WMT test sets
from 2007 to 2011 best. This allows even out-of-
domain data to contribute to modeling of overall
sentence structure. We filtered the model using the
same threshold 10?14.
Table 5 summarizes the resulting LM files as
used in CU-BOJAR and CHIMERA.
2.1.5 Bigger Tuning Sets
Koehn and Haddow (2012) report benefits from
tuning on a larger set of sentences. We experi-
mented with a down-scaled MT system to com-
pare a couple of options for our tuning set: the
default 3003 sentences of newstest2011, the de-
fault and three more Czech references that were
created by translating from German, the default
and two more references that were created by post-
editing a variant of our last year?s Moses system
and also a larger single-reference set consisting
of several newstest years. The preliminary re-
sults were highly inconclusive: negligibly higher
BLEU scores obtained lower manual scores. Un-
able to pick the best configuration, we picked the
largest. We tune our systems on ?bigref?, as spec-
ified in Table 6. The dataset consists of 11583
source sentences, 3003 of which have 4 reference
translations and a subset (1997 sents.) of which
has 2 reference translations constructed by post-
editing. The dataset does not include 2010 data as
a heldout for other foreseen experiments.
2.1.6 Synthetic Parallel Data
Galus?c?a?kova? et al (2013) describe several possi-
bilities of combining TectoMT and phrase-based
approaches. Our CU-BOJAR uses one of the sim-
pler but effective ones: adding TectoMT output on
the test set to our training data. As a contrast to
94
English Czech # Refs # Snts
newstest2011 official + 3 more from German 4 3003
newstest2011 2 post-edits of a system 2 1997
similar to (Bojar et al, 2012a)
newstest2009 official 1 2525
newstest2008 official 1 2051
newstest2007 official 1 2007
Total 4 11583
Table 6: Our big tuning set (bigref).
CU-BOJAR, we also examine PLAIN Moses setup
which is identical but lacks the additional syn-
thetic phrase table by TectoMT.
In order to select the best balance between
phrases suggested by TectoMT and our parallel
data, we provide these data as two separate phrase
tables. Each phrase table brings in its own five-
tuple of scores, one of which, the phrase-penalty
functions as an indicator how many phrases come
from which of the phrase tables. The standard
MERT is then used to optimize the weights.6,7
We use one more trick compared to
Galus?c?a?kova? et al (2013): we deliberately
overlap our training and tuning datasets. When
preparing the synthetic parallel data, we use the
English side of newstests 08 and 10?13. The
Czech side is always produced by TectoMT. We
tune on bigref (see Table 6), so the years 08, 11
and 12 overlap. (We could have overlapped also
years 07, 09 and 10 but we had them originally
reserved for other purposes.) Table 7 summarizes
the situation and highlights that our setup is fair:
we never use the target side of our final evaluation
set newstest2013. Some test sets are denoted
?could have? as including them would still be
correct.
The overlap allows MERT to estimate how use-
ful are TectoMT phrases compared to the standard
phrase table not just in general but on the spe-
cific foreseen test set. This deliberate overfitting
to newstest 08, 11 and 12 then helps in translating
newstest13.
This combination technique in its current state
is rather expensive as a new phrase table is re-
quired for every new input document. However,
if we fix the weights for the TectoMT phrase ta-
6Using K-best batch MIRA (Cherry and Foster, 2012) did
not work any better in our setup.
7We are aware of the fact that Moses alternative decoding
paths (Birch and Osborne, 2007) with similar phrase tables
clutter n-best lists with identical items, making MERT less
stable (Eisele et al, 2008; Bojar and Tamchyna, 2011). The
issue was not severe in our case, CU-BOJAR needed 10 itera-
tions compared to 3 iterations needed for PLAIN.
Used in
Test Set Training Tuning Final Eval
newstest07 could have en+cs ?
newstest08 en+TectoMT en+cs ?
newstest09 could have en+cs ?
newstest10 en+TectoMT could have ?
newstest11 en+TectoMT en+cs ?
newstest12 en+TectoMT en+cs ?
newstest13 en+TectoMT ? en+cs
Table 7: Summary of test sets usage. ?en? and
?cs? denote the official English and Czech sides,
resp. ?TectoMT? denotes the synthetic Czech.
ble, we can avoid re-tuning the system (whether
this would degrade translation quality needs to be
empirically evaluated). Moreover, if we use a dy-
namic phrase table, we could update it with Tec-
toMT outputs on the fly, thus bypassing the need
to retrain the translation model.
2.2 Depfix
Depfix is an automatic post-editing tool for cor-
recting errors in English-to-Czech SMT. It is ap-
plied as a post-processing step to CU-BOJAR, re-
sulting in the CHIMERA system. Depfix 2013 is an
improvement of Depfix 2012 (Rosa et al, 2012).
Depfix focuses on three major types of language
phenomena that can be captured by employing lin-
guistic knowledge but are often hard for SMT sys-
tems to get right:
? morphological agreement, such as:
? an adjective and the noun it modifies have to
share the same morphological gender, num-
ber and case
? the subject and the predicate have to agree in
morphological gender, number and person, if
applicable
? transfer of meaning in cases where the same
meaning is expressed by different grammatical
means in English and in Czech, such as:
? a subject in English is marked by being a left
modifier of the predicate, while in Czech a
subject is marked by the nominative morpho-
logical case
? English marks possessiveness by the preposi-
tion ?of?, while Czech uses the genitive mor-
phological case
? negation can be marked in various ways in
English and Czech
? verb-noun and noun-noun valency?see (Rosa
et al, 2013)
Depfix first performs a complex lingustic anal-
95
System BLEU TER WMT Ranking
Appraise MTurk
CU-TECTOMT 14.7 0.741 0.455 0.491
CU-BOJAR 20.1 0.696 0.637 0.555
CU-DEPFIX 20.0 0.693 0.664 0.542
PLAIN Moses 19.5 0.713 ? ?
GOOGLE TR. ? ? 0.618 0.526
Table 8: Overall results.
ysis of both the source English sentence and its
translation to Czech by CU-BOJAR. The anal-
ysis includes tagging, word-alignment, and de-
pendency parsing both to shallow-syntax (?analyt-
ical?) and deep-syntax (?tectogrammatical?) de-
pendency trees. Detection and correction of errors
is performed by rule-based components (the va-
lency corrections use a simple statistical valency
model). For example, if the adjective-noun agree-
ment is found to be violated, it is corrected by
projecting the morphological categories from the
noun to the adjective, which is realized by chang-
ing their values in the Czech morphological tag
and generating the appropriate word form from the
lemma-tag pair using the rule-based generator of
Hajic? (2004).
Rosa (2013) provides details of the current ver-
sion of Depfix. The main additions since 2012 are
valency corrections and lost negation recovery.
3 Overall Results
Table 8 reports the scores on the WMT13 test
set. BLEU and TER are taken from the evalu-
ation web site8 for the normalized outputs, case
insensitive. The normalization affects typeset-
ting of punctuation only and greatly increases
automatic scores. ?WMT ranking? lists results
from judgments from Appraise and Mechanical
Turk. Except CU-TECTOMT, the manual evalua-
tion used non-normalized MT outputs. The fig-
ure is the WMT12 standard interpretation as sug-
gested by Bojar et al (2011) and says how often
the given system was ranked better than its com-
petitor across all 18.6k non-tying pairwise com-
parisons extracted from the annotations.
We see a giant leap from CU-TECTOMT to CU-
BOJAR, confirming the utility of large data. How-
ever, CU-TECTOMT had something to offer since it
improved over PLAIN, a very competitive baseline,
by 0.6 BLEU absolute. Depfix seems to slightly
worsen BLEU score but slightly improve TER; the
8http://matrix.statmt.org/
System # Tokens % Tokens
All 22920 76.44
Moses 3864 12.89
TectoMT 2323 7.75
Other 877 2.92
Table 9: CHIMERA components that contribute
?confirmed? tokens.
System # Tokens % Tokens
None 21633 79.93
Moses 2093 7.73
TectoMT 2585 9.55
Both 385 1.42
CU-BOJAR 370 1.37
Table 10: Tokens missing in CHIMERA output.
manual evaluation is similarly indecisive.
4 Combination Analysis
We now closely analyze the contributions of
the individual engines to the performance of
CHIMERA. We look at translations of the new-
stest2013 sets produced by the individual systems
(PLAIN, CU-TECTOMT, CU-BOJAR, CHIMERA).
We divide the newstest2013 reference tokens
into two classes: those successfully produced by
CHIMERA (Table 9) and those missed (Table 10).
The analysis can suffer from false positives as well
as false negatives, a ?confirmed? token can violate
some grammatical constraints in MT output and
an ?unconfirmed? token can be a very good trans-
lation. If we had access to more references, the
issue of false negatives would decrease.
Table 9 indicates that more than 3/4 of to-
kens confirmed by the reference were available
in all CHIMERA components: PLAIN Moses, CU-
TECTOMT alone but also in the subsequent combi-
nations CU-BOJAR and the final CU-DEPFIX.
PLAIN Moses produced 13% tokens that Tec-
toMT did not provide and TectoMT output
roughly 8% tokens unknown to Moses. However,
note that it is difficult to distinguish the effect of
different model weights: PLAIN might have pro-
duced some of those tokens as well if its weights
were different. The row ?Other? includes cases
where e.g. Depfix introduced a confirmed token
that none of the previous systems had.
Table 10 analyses the potential of CHIMERA
components. These tokens from the reference
were not produced by CHIMERA. In almost 80%
of cases, the token was not available in any 1-best
output; it may have been available in Moses phrase
96
tables or the input sentence.
TectoMT offered almost 10% of missed tokens,
but these were not selected in the subsequent com-
bination. The potential of Moses is somewhat
lower (about 8%) because our phrase-based com-
bination is likely to select wordings that score well
in a phrase-based model. 385 tokens were sug-
gested by both TectoMT and Moses alone, but the
combination in CU-BOJAR did not select them, and
finally 370 tokens were produced by the combina-
tion while they were not present in 1-best output of
neither TectoMT nor Moses. Remember, all these
tokens eventually did not get to CHIMERA output,
so Depfix must have changed them.
4.1 Depfix analysis
Table 11 analyzes the performance of the individ-
ual components of Depfix. Each evaluated sen-
tence was either modified by a Depfix component,
or not. If it was modified, its quality could have
been evaluated as better (improved), worse (wors-
ened), or the same (equal) as before. Thus, we can
evaluate the performance of the individual compo-
nents by the following measures:9
precision = #improved#improved+#worsened (1)
impact = #modified#evaluated (2)
useless = #equal#modified (3)
Please note that we make an assumption that if
a sentence was modified by multiple Depfix com-
ponents, they all have the same effect on its qual-
ity. While this is clearly incorrect, it is impossible
to accurately determine the effect of each individ-
ual component with the evaluation data at hand.
This probably skews especially the reported per-
formance of ?high-impact? components, which of-
ten operate in combination with other components.
The evaluation is computed on 871 hits in which
CU-BOJAR and CHIMERA were compared.
The results show that the two newest compo-
nents ? Lost negation recovery and Valency model
? both modify a large number of sentences. Va-
lency model seems to have a slightly negative ef-
fect on the translation quality. As this is the only
statistical component of Depfix, we believe that
this is caused by the fact that its parameters were
not tuned on the final CU-BOJAR system, as the
9We use the term precision for our primary measure for
convenience, even though the way we define it does not match
exactly its usual definition.
Depfix component Prc. Imp. Usl.
Aux ?be? agr. ? 1.4% 100%
No prep. without children ? 0.5% 100%
Sentence-initial capitalization 0% 0.1% 0%
Prepositional morph. case 0% 2.1% 83%
Preposition - noun agr. 40% 3.8% 70%
Noun number projection 41% 7.2% 65%
Valency model 48% 10.6% 66%
Subject - nominal pred. agr. 50% 3.8% 76%
Noun - adjective agr. 55% 17.8% 75%
Subject morph. case 56% 8.5% 57%
Tokenization projection 56% 3.0% 38%
Verb tense projection 58% 5.2% 47%
Passive actor with ?by? 60% 1.0% 44%
Possessive nouns 67% 0.9% 25%
Source-aware truecasing 67% 2.8% 50%
Subject - predicate agr. 68% 5.1% 57%
Pro-drop in subject 73% 3.4% 63%
Subject - past participle agr. 75% 6.3% 42%
Passive - aux ?be? agr. 77% 4.8% 69%
Possessive with ?of? 78% 1.5% 31%
Present continuous 78% 1.5% 31%
Missing reflexive verbs 80% 1.6% 64%
Subject categories projection 83% 3.7% 62%
Rehang children of aux verbs 83% 5.5% 62%
Lost negation recovery 90% 7.2% 38%
Table 11: Depfix components performance analy-
sis on 871 sentences from WMT13 test set.
tuning has to be done semi-manually and the fi-
nal system was not available in advance. On the
other hand, Lost negation recovery seems to have
a highly positive effect on translation quality. This
is to be expected, as a lost negation often leads to
the translation bearing an opposite meaning to the
original one, which is probably one of the most
serious errors that an MT system can make.
5 Conclusion
We have reached our chimera to beat Google
Translate. We combined all we have: a deep-
syntactic transfer-based system TectoMT, very
large parallel and monolingual data, factored setup
to ensure morphological coherence, and finally
Depfix, a rule-based automatic post-editing sys-
tem that corrects grammaticality (agreement and
valency) of the output as well as some features vi-
tal for adequacy, namely lost negation.
Acknowledgments
This work was partially supported by the grants
P406/11/1499 of the Grant Agency of the Czech
Republic, FP7-ICT-2011-7-288487 (MosesCore)
and FP7-ICT-2010-6-257528 (Khresmoi) of the
European Union and by SVV project number 267
314.
97
References
Alexandra Birch and Miles Osborne. 2007. CCG Su-
pertags in Factored Statistical Machine Translation.
In In ACL Workshop on Statistical Machine Trans-
lation, pages 9?16.
Ondr?ej Bojar and Ales? Tamchyna. 2011. Improving
Translation Model by Monolingual Data. In Proc.
of WMT, pages 330?336. ACL.
Ondr?ej Bojar, Milos? Ercegovc?evic?, Martin Popel, and
Omar Zaidan. 2011. A Grain of Salt for the WMT
Manual Evaluation. In Proc. of WMT, pages 1?11.
ACL.
Ondr?ej Bojar, Bushra Jawaid, and Amir Kamran.
2012a. Probes in a Taxonomy of Factored Phrase-
Based Models. In Proc. of WMT, pages 253?260.
ACL.
Ondr?ej Bojar, Zdene?k Z?abokrtsky?, Ondr?ej Dus?ek, Pe-
tra Galus?c?a?kova?, Martin Majlis?, David Marec?ek, Jir???
Mars???k, Michal Nova?k, Martin Popel, and Ales? Tam-
chyna. 2012b. The Joy of Parallelism with CzEng
1.0. In Proc. of LREC, pages 3921?3928. ELRA.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proc. of NAACL/HLT, pages 427?436. ACL.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. Better hypothesis testing for
statistical machine translation: Controlling for opti-
mizer instability. In Proc. of ACL/HLT, pages 176?
181. ACL.
Andreas Eisele, Christian Federmann, Herve? Saint-
Amand, Michael Jellinghaus, Teresa Herrmann, and
Yu Chen. 2008. Using Moses to Integrate Multi-
ple Rule-Based Machine Translation Engines into a
Hybrid System. In Proc. of WMT, pages 179?182.
ACL.
Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-
bienne Cap. 2012. Modeling Inflection and Word-
Formation in SMT. In Proc. of EACL 2012. ACL.
Petra Galus?c?a?kova?, Martin Popel, and Ondr?ej Bojar.
2013. PhraseFix: Statistical Post-Editing of Tec-
toMT. In Proc. of WMT13. Under review.
Qin Gao and Stephan Vogel. 2008. Parallel Implemen-
tations of Word Alignment Tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?
57. ACL.
Jan Hajic?. 2004. Disambiguation of rich inflection:
computational morphology of Czech. Karolinum.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Proc.
of ACL.
Philipp Koehn and Barry Haddow. 2012. Towards Ef-
fective Use of Training Data in Statistical Machine
Translation. In Proc. of WMT, pages 317?321. ACL.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Trans-
lation Summit X, pages 79?86.
David Marec?ek, Rudolf Rosa, Petra Galus?c?a?kova?, and
Ondr?ej Bojar. 2011. Two-step translation with
grammatical post-processing. In Proc. of WMT,
pages 426?432. ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In ACL. ACL.
Martin Popel and Zdene?k Z?abokrtsky?. 2010. Tec-
toMT: Modular NLP Framework. In Hrafn Lofts-
son, Eirikur Ro?gnvaldsson, and Sigrun Helgadottir,
editors, IceTAL 2010, volume 6233 of Lecture Notes
in Computer Science, pages 293?304. Iceland Cen-
tre for Language Technology (ICLT), Springer.
Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek.
2012. DEPFIX: A system for automatic correction
of Czech MT outputs. In Proc. of WMT, pages 362?
368. ACL.
Rudolf Rosa, David Marec?ek, and Ales? Tamchyna.
2013. Deepfix: Statistical Post-editing of Statistical
Machine Translation Using Deep Syntactic Analy-
sis. Ba?lgarska akademija na naukite, ACL.
Rudolf Rosa. 2013. Automatic post-editing of phrase-
based machine translation outputs. Master?s thesis,
Charles University in Prague, Faculty of Mathemat-
ics and Physics, Praha, Czechia.
Johanka Spoustova? and Miroslav Spousta. 2012. A
High-Quality Web Corpus of Czech. In Proc. of
LREC. ELRA.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Intl. Conf. on
Spoken Language Processing, volume 2, pages 901?
904.
Michael Subotin. 2011. An exponential translation
model for target language morphology. In Proc. of
ACL/HLT, pages 230?238. ACL.
98
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 195?200,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
CUNI in WMT14: Chimera Still Awaits Bellerophon
Ale
?
s Tamchyna, Martin Popel, Rudolf Rosa, Ond
?
rej Bojar
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk?e n?am?est?? 25, Prague, Czech Republic
surname@ufal.mff.cuni.cz
Abstract
We present our English?Czech and
English?Hindi submissions for this
year?s WMT translation task. For
English?Czech, we build upon last year?s
CHIMERA and evaluate several setups.
English?Hindi is a new language pair for
this year. We experimented with reverse
self-training to acquire more (synthetic)
parallel data and with modeling target-side
morphology.
1 Introduction
In this paper, we describe translation systems sub-
mitted by Charles University (CU or CUNI) to the
Translation task of the Ninth Workshop on Statis-
tical Machine Translation (WMT) 2014.
In ?2, we present our English?Czech systems,
CU-TECTOMT, CU-BOJAR, CU-DEPFIX and CU-
FUNKY. The systems are very similar to our sub-
missions (Bojar et al., 2013) from last year, the
main novelty being our experiments with domain-
specific and document-specific language models.
In ?3, we describe our experiments with
English?Hindi translation, which is a translation
pair new both to us and to WMT. We unsuccess-
fully experimented with reverse self-training and a
morphological-tags-based language model, and so
our final submission, CU-MOSES, is only a basic
instance of Moses.
2 English?Czech
Our submissions for English?Czech build upon
last year?s successful CHIMERA system (Bojar
et al., 2013). We combine several different ap-
proaches:
? factored phrase-based Moses model (?2.1),
? domain-adapted language model (?2.2),
? document-specific language models (?2.3),
? deep-syntactic MT system TectoMT (?2.4),
? automatic post-editing system Depfix (?2.5).
We combined the approaches in several ways
into our four submissions, as made clear by Ta-
ble 1. CU-TECTOMT is the stand-alone TectoMT
translation system, while the other submissions
are Moses-based, using TectoMT indirectly to pro-
vide an additional phrase-table. CU-BOJAR uses
a factored model and a domain-adapted language
model; in CU-DEPFIX, Depfix post-processing is
added; and CU-FUNKY also employs document-
specific language models.
C
U
-
T
E
C
T
O
M
T
C
U
-
B
O
J
A
R
C
U
-
D
E
P
F
I
X
C
U
-
F
U
N
K
Y
TectoMT (?2.4) D D D D
Factored Moses (?2.1) D D D
Adapted LM (?2.2) D D D
Document-specific LMs (?2.3) D
Depfix (?2.5) D D
Table 1: EN?CS systems submitted to WMT.
2.1 Our Baseline Factored Moses System
Our baseline translation system (denoted ?Base-
line? in the following) is similar to last year ? we
trained a factored Moses model on the concatena-
tion of CzEng (Bojar et al., 2012) and Europarl
(Koehn, 2005), see Table 2. We use two fac-
tors: tag, which is the part-of-speech tag, and stc,
which is ?supervised truecasing?, i.e. the surface
form with letter case set according to the lemma;
see (Bojar et al., 2013). Our factored Moses sys-
tem translates from English stc to Czech stc | tag
in one translation step.
Our basic language models are identical to last
year?s submission. We added an adapted language
195
Tokens [M]
Corpus Sents [M] English Czech
CzEng 1.0 14.83 235.67 205.17
Europarl 0.65 17.61 15.00
Table 2: English?Czech parallel data.
Corpus Sents [M] Tokens [M]
CzEng 1.0 14.83 205.17
CWC Articles 36.72 626.86
CNC News 28.08 483.88
CNA 47.00 830.32
Newspapers 64.39 1040.80
News Crawl 24.91 444.84
Total 215.93 3631.87
Table 3: Czech monolingual data.
model which we describe in the following section.
Tables 3 and 4 show basic data about the language
models. Aside from modeling surface forms, our
language models also capture morphological co-
herence to some degree.
2.2 Adapted Language Model
We used the 2013 News Crawl to create a language
model adapted to the domain of the test set (i.e.
news domain) using data selection based on infor-
mation retrieval (Tamchyna et al., 2012). We use
the Baseline system to translate the source sides of
WMT test sets 2012?2014. The translations then
constitute a ?query corpus? for Lucene.
1
For each
sentence in the query corpus, we use Lucene to
retrieve 20 most similar sentences from the 2013
News Crawl. After de-duplication, we obtained a
monolingual corpus of roughly 250 thousand sen-
tences and trained an additional 6-gram language
model on this data.
Domain Factor Order Sents Tokens ARPA.gz Trie
[M] [M] [GB] [GB]
General stc 4 201.31 3430.92 28.2 11.8
General stc 7 24.91 444.84 13.1 8.1
General tag 10 14.83 205.17 7.2 3.0
News stc 6 0.25 4.73 0.2 ?
Table 4: Czech LMs used in CU-BOJAR. The last
small model is described in ?2.2.
1
http://lucene.apache.org
2.3 Document-Specific Language Models
CU-FUNKY further extends the idea described in
?2.2. Taking advantage of document IDs which
are included in WMT development and test data,
we split our dev- (WMT 13) and test-set (WMT
14) into documents. We translate each document
with the Baseline system and use Lucene to re-
trieve 10,000 most similar target-side sentences
from News Crawl 2013 for each document sen-
tence.
Using this procedure, we obtain a corpus for
each document. On average, the corpora con-
tain roughly 208 thousand sentences after de-
duplication. Each corpus then serves as the
training data for the document-specific language
model.
We implemented an alternative to
moses-parallel.perl which splits the
input corpus based on document IDs and runs a
separate Moses instance/job for each document.
Moreover, it allows to modify the Moses config-
uration file according to document ID. We use
this feature to plant the correct document-specific
language model to each job.
In tuning, our technique only adds one weight.
In each split, the weight corresponds to a differ-
ent language model. The optimizer then hope-
fully averages the utility of this document-specific
LM across all documents. The same weight is ap-
plied also in the test set translation, exchanging the
document-specific LM file.
2.4 TectoMT Deep-Syntactic MT System
TectoMT
2
was one of the three key components
in last year?s CHIMERA. It is a linguistically-
motivated tree-to-tree deep-syntactic translation
system with transfer based on Maximum Entropy
context-sensitive translation models (Mare?cek et
al., 2010) and Hidden Tree Markov Models
(
?
Zabokrtsk?y and Popel, 2009). It is trained on
the WMT-provided data: CzEng 1.0 (parallel data)
and News Crawl (2007?2012 Czech monolingual
sets).
We maintain the same approach to combining
TectoMT with Moses as last year ? we translate
WMT test sets from years 2007?2014 and use
them as additional synthetic parallel training data ?
a corpus consisting of the test set source side (En-
glish) and TectoMT output (synthetic Czech). We
then use the standard extraction pipeline to create
2
http://ufal.mff.cuni.cz/tectomt/
196
an additional phrase table from this corpus. The
translated data overlap completely both with our
development and test data for Moses so that tuning
can assign an appropriate weight to the synthetic
phrase table.
2.5 Depfix Automatic Post-Editing
As in the previous years, we used Depfix (Rosa,
2013) to post-process the translations. Depfix is
an automatic post-editing system which is mainly
rule-based and uses various linguistic tools (tag-
gers, parsers, morphological generators, etc.) to
detect and correct errors, especially grammatical
ones. The system was slightly improved since last
year, and a new fixing rule was added for correct-
ing word order in noun clusters translated as geni-
tive constructions.
In English, a noun can behave as an adjective,
as in ?according to the house owners?, while in
Czech, this is not possible, and a genitive construc-
tion has to be used instead, similarly to ?according
to the owners of the house? ? the modifier is in the
genitive morphological case and follows the noun.
However, SMT systems translating into Czech do
not usually focus much on word reordering, which
leads to non-fluent or incomprehensible construc-
tions, such as ?podle domu
gen
vlastn??k?u
gen
? (ac-
cording to-the-house of-the-owners). Fortunately,
such cases are easy to distinguish with the help
of a dependency parser and a morphological tag-
ger ? genitive modifiers usually do not precede the
head but follow it (unless they are parts of named
entities), so we can safely switch the word order
to the correct one: ?podle vlastn??k?u
gen
domu
gen
?
(according to-the-owners of-the-house).
2.6 Results
We report scores of automatic metrics as shown in
the submission system,
3
namely (case-sensitive)
BLEU (Papineni et al., 2002) and TER (Snover
et al., 2006). The results, summarized in Ta-
ble 5, show that CU-FUNKY is the most success-
ful of our systems according to BLEU, while
the simpler CU-DEPFIX wins in TER. The re-
sults of manual evaluation suggest that CU-DEPFIX
(dubbed CHIMERA) remains the best performing
English?Czech system.
In comparison to other English?Czech sys-
tems submitted to WMT 2014, CU-FUNKY ranked
as the second in BLEU, and CU-DEPFIX ranked
3
http://matrix.statmt.org/
as the second in TER; the winning system, ac-
cording to both of these metrics, was UEDIN-
UNCONSTRAINED.
System BLEU TER Manual
CU-DEPFIX 21.1 0.670 0.373
UEDIN-UNCONSTRAINED 21.6 0.667 0.357
CU-BOJAR 20.9 0.674 0.333
CU-FUNKY 21.2 0.675 0.287
GOOGLE TRANSLATE 20.2 0.687 0.168
CU-TECTOMT 15.2 0.716 -0.177
CU-BOJAR +full 2013 news 20.7 0.677 ?
Table 5: Scores of automatic metrics and results of
manual evaluation for our systems. The table also
lists the best system according to automatic met-
rics and Google Translate as the best-performing
commercial system.
Our analysis of CU-FUNKY suggests that it is
not the best performing system on average (de-
spite achieving the highest BLEU scores from our
submissions), but that it is rather the most volatile
system. Some sentences were obviously improved
compared to CU-BOJAR but most got degraded es-
pecially in adequacy. We are well aware of the
many shortcomings our current implementation
has, the most severe of which lie in the sentence
selection by Lucene. For instance, we do not use
any stopwords or keyword detection methods, and
also pretending that each sentence in our monolin-
gual corpus is a ?document? for the information
retrieval system is far from ideal.
We also evaluated a version of CU-BOJAR which
uses not only the adapted LM but also an addi-
tional LM trained on the full 2013 News Crawl
data (see ?CU-BOJAR +full 2013 news? in Table 5)
but found no improvement compared to using just
the adapted model (trained on a subset of the data).
3 English?Hindi
English-Hindi is a new language pair this
year. We submitted an unconstrained system for
English?Hindi translation.
We used HindEnCorp (Bojar et al., 2014) as the
sole source of parallel data (nearly 276 thousand
sentence pairs, around 3.95 million English tokens
and 4.09 million Hindi tokens).
Given that no test set from previous years was
available and that the size of the development set
provided by WMT organizers was only 500 sen-
tence pairs, we held out the first 5000 sentence
pairs of HindEnCorp for this purpose. Our de-
velopment set then consisted of the 500 provided
197
Corpus Sents [M] Tokens [M]
NewsCrawl 1.27 27.27
HindEnCorp 0.28 4.09
HindMonoCorp 43.38 945.43
Total 44.93 976.80
Table 6: Hindi monolingual data.
sentences plus 1500 sentence pairs from HindEn-
Corp. The remaining 3500 sentence pairs taken
from HindEnCorp constituted our test set.
As for monolingual data, we used the News
Crawl corpora provided for the task and the new
monolingual HindMonoCorp, which makes our
submission unconstrained. Table 6 shows statis-
tics of our monolingual data.
We tagged and lemmatized the English data us-
ing Mor?ce (Spoustov?a et al., 2007) and the Hindi
data using Siva Reddy?s POS tagger.
4
3.1 Baseline System
The baseline system was eventually our best-
performing one. Its design is completely straight-
forward ? it uses one phrase table trained on
all parallel data (we translate from ?supervised-
truecased? English into Hindi forms) and one 5-
gram language model trained on all monolingual
data. We used KenLM (Heafield et al., 2013) for
estimating the model as the data was rather large
(see Table 6).
We used GIZA++ (Och and Ney, 2000) as
our word alignment tool. We experimented with
several coarser representations to make the final
alignment more reliable. Table 7 shows the re-
sults. The factor ?stem4? refers to simply taking
the first four characters of each word. For lem-
mas, we used the outputs of the tools mentioned
above. However, lemmas as output by the Hindi
tagger were not much coarser than surface forms
? the ratio between the number of types is merely
1.11 ? so we also tried ?stemming? the lemmas
(lemma4). Of these variants, stem4-stem4 align-
ment worked best and we used it for the rest of our
experiments.
3.2 Reverse Self-Training
Bojar and Tamchyna (2011) showed a simple tech-
nique for improving translation quality in situa-
tions where there is only a small amount of par-
4
http://sivareddy.in/downloads#hindi_
tools
English Hindi BLEU
stem4 stem4 22.96?1.17
lemma lemma4 22.59?1.17
lemma lemma 22.41?1.20
Table 7: Comparison of different factor combina-
tions for word alignment.
allel data available but where there is a sufficient
quantity of target-side monolingual texts. The so-
called ?reverse self-training? uses a factored sys-
tem trained in the opposite direction to translate
the large monolingual data into the source lan-
guage. The translation (in the source language,
i.e. English in our case) and the original target-
side data (Hindi) can be used as additional syn-
thetic parallel data. The authors recommend creat-
ing a separate phrase table from it and combining
the two translation models as alternatives in the
log-linear model (letting tuning weigh their impor-
tance).
The factored setup of the reverse system
(Hindi?English) is essential ? alternative decod-
ing paths with a back-off to a coarser representa-
tion (e.g. stems) on the source side (Hindi) give
the system the ability to generalize beyond surface
forms observed in the training data. The main aim
of this technique is to learn new forms of known
words.
The technique is thus aimed at translating into a
morphologically richer language than the source.
Indeed, the authors showed that if the target lan-
guage has considerably more word types than the
source, the gains achieved by reverse self-training
are higher. In this respect, English?Hindi is not
an ideal candidate given that the ratio we observed
is only 1.2.
The choice of back-off representation is impor-
tant. We measure the vocabulary reduction of
several options and summarize the results in Ta-
ble 8. E.g. for stem4, the vocabulary size is
roughly 30% compared to the number of surface
word forms.
Bojar and Tamchyna (2011) achieved the best
results using ?nosuf3? (?suffix trimming?, i.e. cut-
ting of the last 3 characters of each word); how-
ever, they experimented with European languages
and the highest reduction of vocabulary reported
in the paper is to roughly one half. In our case, the
vocabulary is reduced much more, so we opted for
a more conservative back-off, namely ?nosuf2?.
198
Back-off % of vocab. size
stem4 30.21
lemma4 32.36
nosuf3 36.36
nosuf2 50.76
stem5 53.48
lemma5 57.47
lemma 90.09
Table 8: Options for back-off factors in reverse
self-training and the percentage of their vocabu-
lary size compared to surface forms.
We translated roughly 2 million sentences from
the Hindi monolingual data, focusing on news
to maintain a domain match with the WMT test
set. However, adding the synthetic phrase table
did not bring any improvement and in fact, the
BLEU score dropped to 22.37?1.17 (baseline is
22.96?1.17).
We can attribute the failure of reverse self-
training to the nature of the language pair at hand.
While Hindi has some synthetic properties (e.g.
future tense of verbs or inflection of adjectives are
marked by suffixes), its inflectional morphemes
are realized mainly by post-positions which are
separated from their head-words. Overlooking this
essential property, we attempted to use reverse
self-training but our technique could contribute
only very little.
3.3 Target-Side Morphology
We also experimented with a setup that tradition-
ally works very well for English?Czech trans-
lation: using a high-order language model on
morphological tags to explicitly model target-side
morphological coherence in translation. We used
the same monolingual data as for the baseline lan-
guage model; however, the order of our morpho-
logical language model was set to 10.
This setup also brought no improvement over
the baseline ? in fact, the BLEU score dropped
even further to 22.27?1.14.
4 Conclusion
We presented our contributions to the Translation
task of WMT 2014.
As we have focused on English?Czech trans-
lation for many years, we have developed sev-
eral complex and well-performing systems for it
? an adaptation of the phrase-based Moses sys-
tem, a linguistically-motivated syntax-based Tec-
toMT system, and an automatic post-editing Dep-
fix system. We combine the individual systems
using a very simple yet effective method and the
combined system called CHIMERA confirmed its
state-of-the-art performance.
For English?Hindi translation, which was a
new task for us, we managed to get competitive
results by using a baseline Moses setup, but were
unable to improve upon those by employing ad-
vanced techniques that had proven to be effective
for other translation directions.
Acknowledgments
This research was supported by the grants FP7-
ICT-2013-10-610516 (QTLeap), FP7-ICT-2011-
7-288487 (MosesCore), SVV 260 104. and
GAUK 1572314. This work has been using lan-
guage resources developed, stored and distributed
by the LINDAT/CLARIN project of the Ministry
of Education, Youth and Sports of the Czech Re-
public (project LM2010013).
References
Ond?rej Bojar and Ale?s Tamchyna. 2011. Improving
Translation Model by Monolingual Data. In Proc.
of WMT, pages 330?336. ACL.
Ond?rej Bojar, Zden?ek
?
Zabokrtsk?y, Ond?rej Du?sek, Pe-
tra Galu?s?c?akov?a, Martin Majli?s, David Mare?cek, Ji?r??
Mar?s??k, Michal Nov?ak, Martin Popel, and Ale?s Tam-
chyna. 2012. The Joy of Parallelism with CzEng
1.0. In Proc. of LREC, pages 3921?3928. ELRA.
Ondrej Bojar, Rudolf Rosa, and Ale?s Tamchyna. 2013.
Chimera ? Three Heads for English-to-Czech Trans-
lation. In Proceedings of the Eighth Workshop on
Statistical Machine Translation, pages 90?96.
Ond?rej Bojar, Vojt?ech Diatka, Pavel Rychl?y, Pavel
Stra?n?ak, V??t Suchomel, Ale?s Tamchyna, and Daniel
Zeman. 2014. HindEnCorp ? Hindi-English and
Hindi-only Corpus for Machine Translation. Reyk-
jav??k, Iceland. European Language Resources Asso-
ciation.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable Modified
Kneser-Ney Language Model Estimation. In Proc.
of ACL.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Machine Trans-
lation Summit X, pages 79?86.
David Mare?cek, Martin Popel, and Zden?ek
?
Zabokrtsk?y.
2010. Maximum entropy translation model in
199
dependency-based MT framework. In Proc. of WMT
and MetricsMATR, pages 201?206. ACL.
Franz Josef Och and Hermann Ney. 2000. Improved
Statistical Alignment Models. In Proc. of ACL,
pages 440?447, Hong Kong. ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311?318, Stroudsburg, PA, USA. ACL.
Rudolf Rosa. 2013. Automatic post-editing of phrase-
based machine translation outputs. Master?s thesis,
Charles University in Prague, Faculty of Mathemat-
ics and Physics, Praha, Czechia.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study
of translation edit rate with targeted human annota-
tion. In In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Drahom??ra Spoustov?a, Jan Haji?c, Jan Votrubec, Pavel
Krbec, and Pavel Kv?eto?n. 2007. The best of two
worlds: Cooperation of statistical and rule-based
taggers for Czech. In Proceedings of the Work-
shop on Balto-Slavonic Natural Language Process-
ing, ACL 2007, pages 67?74, Praha.
Ale?s Tamchyna, Petra Galu?s?c?akov?a, Amir Kamran,
Milo?s Stanojevi?c, and Ond?rej Bojar. 2012. Select-
ing Data for English-to-Czech Machine Translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, WMT ?12, pages 374?
381, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Zden?ek
?
Zabokrtsk?y and Martin Popel. 2009. Hidden
Markov Tree Model in Dependency-based Machine
Translation. In Proc. of ACL-IJCNLP Short Papers,
pages 145?148.
200
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 221?228,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Machine Translation of Medical Texts in the Khresmoi Project
Ond
?
rej Du
?
sek, Jan Haji
?
c, Jaroslava Hlav
?
a
?
cov
?
a, Michal Nov
?
ak,
Pavel Pecina, Rudolf Rosa, Ale
?
s Tamchyna, Zde
?
nka Ure
?
sov
?
a, Daniel Zeman
Charles University in Prague, Faculty of Mathematics and Physics
Institute of Formal and Applied Linguistics
Malostransk?e n?am?est?? 25, 11800 Prague, Czech Republic
{odusek,hajic,hlavacova,mnovak,pecina,rosa,tamchyna,uresova,zeman}@ufal.mff.cuni.cz
Abstract
This paper presents the participation of
the Charles University team in the WMT
2014 Medical Translation Task. Our sys-
tems are developed within the Khresmoi
project, a large integrated project aim-
ing to deliver a multi-lingual multi-modal
search and access system for biomedical
information and documents. Being in-
volved in the organization of the Medi-
cal Translation Task, our primary goal is
to set up a baseline for both its subtasks
(summary translation and query transla-
tion) and for all translation directions.
Our systems are based on the phrase-
based Moses system and standard meth-
ods for domain adaptation. The con-
strained/unconstrained systems differ in
the training data only.
1 Introduction
The WMT 2014 Medical Translation Task poses
an interesting challenge for Machine Translation
(MT). In the ?standard? translation task, the end
application is the translation itself. In the Medi-
cal Translation Task, the MT system is considered
a part of a larger system for Cross-Lingual Infor-
mation Retrieval (CLIR) and is used to solve two
different problems: (i) translation of user search
queries, and (ii) translation of summaries of re-
trieved documents.
In query translation, the end user does not even
necessarily see the MT output as their queries are
translated and search is performed on documents
in the target language. In summary translation, the
sentences to be translated come from document
summaries (snippets) displayed to provide infor-
mation on each of the documents retrieved by the
search. Therefore, translation quality may not be
the most important measure in this task ? the per-
formance of the CLIR system as a whole is the
final criterion. Another fundamental difference
from the standard task is the nature of the trans-
lated texts. While we can consider document sum-
maries to be ordinary texts (despite their higher in-
formation density in terms of terminology from a
narrow domain), search queries in the medical do-
main are an extremely specific type of data, and
traditional techniques for system development and
domain adaptation are truly put to a test here.
This work is a part of the of the large integrated
EU-funded Khresmoi project.
1
Among other
goals, such as joint text and image retrieval of ra-
diodiagnostic records, Khresmoi aims to develop
technology for transparent cross-lingual search of
medical sources for both professionals and laypeo-
ple, with the emphasis primarily on publicly avail-
able web sources.
In this paper, we describe the Khresmoi sys-
tems submitted to the WMT 2014 Medical Trans-
lation Task. We participate in both subtasks (sum-
mary translation and query translation) for all
language pairs (Czech?English, German?English,
and French?English) in both directions (to English
and from English). Our systems are based on the
Moses phrase-based translation toolkit and stan-
dard methods for domain adaptation. We submit
one constrained and one unconstrained system for
each subtask and translation direction. The con-
strained and unconstrained systems differ in train-
ing data only: The former use all allowed training
data, the latter take advantage of additional web-
crawled data.
We first summarize previous works in MT do-
main adaptation in Section 2, then describe the
data we used for our systems in Section 3. Sec-
1
http://www.khresmoi.eu/
221
tion 4 contains an account of the submitted sys-
tems and their performance in translation of search
queries and document summaries. Section 5 con-
cludes the paper.
2 Related work
To put our work in the context of other approaches,
we first describe previous work on domain adap-
tation in Statistical Machine Translation (SMT),
then focus specifically on SMT in the medical do-
main.
2.1 Domain adaptation of Statistical machine
translation
Many works on domain adaptation examine the
usage of available in-domain data to directly im-
prove in-domain performance of SMT. Some au-
thors attempt to combine the predictions of two
separate (in-domain and general-domain) transla-
tion models (Langlais, 2002; Sanchis-Trilles and
Casacuberta, 2010; Bisazza et al., 2011; Nakov,
2008) or language models (Koehn and Schroeder,
2007). Wu and Wang (2004) use in-domain data
to improve word alignment in the training phase.
Carpuat et al. (2012) explore the possibility of us-
ing word sense disambiguation to discriminate be-
tween domains.
Other approaches concentrate on the acquisition
of larger in-domain corpora. Some of them ex-
ploit existing general-domain corpora by select-
ing data that resemble the properties of in-domain
data (e.g., using cross-entropy), thus building a
larger pseudo-in-domain training corpus. This
technique is used to adapt language models (Eck
et al., 2004b; Moore and Lewis, 2010) as well as
translation models (Hildebrand et al., 2005; Axel-
rod et al., 2011) or their combination (Mansour et
al., 2011). Similar approaches to domain adapta-
tion are also applied in other tasks, e.g., automatic
speech recognition (Byrne et al., 2004).
2.2 Statistical machine translation in the
medical domain
Eck et al. (2004a) employ an SMT system for the
translation of dialogues between doctors and pa-
tients and show that according to automatic met-
rics, a dictionary extracted from the Unified Medi-
cal Language System (UMLS) Metathesaurus and
its semantic type classification (U.S. National Li-
brary of Medicine, 2009) significantly improves
translation quality from Spanish to English when
applied to generalize the training data.
Wu et al. (2011) analyze the quality of MT on
PubMed
2
titles and whether it is sufficient for pa-
tients. The conclusions are very positive espe-
cially for languages with large training resources
(English, Spanish, German) ? the average fluency
and content scores (based on human evaluation)
are above four on a five-point scale. In automatic
evaluation, their systems substantially outperform
Google Translate. However, the SMT systems are
specifically trained, tuned, and tested on the do-
main of PubMed titles, and it is not evident how
they would perform on other medical texts.
Costa-juss`a et al. (2012) are less optimistic re-
garding SMT quality in the medical domain. They
analyze and evaluate the quality of public web-
based MT systems (such as Google Translate) and
conclude that in both automatic and manual eval-
uation (on 7 language pairs), the performance of
these systems is still not good enough to be used
in daily routines of medical doctors in hospitals.
Jimeno Yepes et al. (2013) propose a method
for obtaining in-domain parallel corpora from ti-
tles and abstracts of publications in the MED-
LINE
3
database. The acquired corpora contain
from 30,000 to 130,000 sentence pairs (depending
on the language pair) and are reported to improve
translation quality when used for SMT training,
compared to a baseline trained on out-of-domain
data. However, the authors use only one source
of in-domain parallel data to adapt the translation
model, and do not use any in-domain monolingual
data to adapt the language model.
In this work, we investigate methods combining
the different kinds of data ? general-domain, in-
domain, and pseudo-in-domain ? to find the opti-
mal approach to this problem.
3 Data description
This section includes an overview of the parallel
and monolingual data sources used to train our
systems. Following the task specification, they
are split into constrained and unconstrained sec-
tions. The constrained section includes medical-
domain data provided for this task (extracted by
the provided scripts), and general-domain texts
provided as constrained data for the standard task
(?general domain? here is used to denote data
2
http://www.ncbi.nlm.nih.gov/pubmed/
3
http://www.nlm.nih.gov/pubs/
factsheets/medline.html
222
Czech?English German?English French?English
dom set pairs source target pairs source target pairs source target
med con 2,498 18,126 19,964 4,998 123,686 130,598 6,139 202,245 171,928
gen con 15,788 226,711 260,505 4,520 112,818 119,404 40,842 1,470,016 1,211,516
gen unc ? ? ? 9,320 525,782 574,373 13,809 961,991 808,222
Table 1: Number of sentence pairs and tokens (source/target) in parallel training data (in thousands).
dom set English Czech German French
med con 172,991 1,848 63,499 63,022
gen con 6,132,107 627,493 1,728,065 1,837,457
med unc 3,275,272 36,348 361,881 908,911
gen unc 618,084 ? 339,595 204,025
Table 2: Number of tokens in monolingual training data (in thousands).
which comes from a mixture of various different
domains, mostly news, parliament proceedings,
web-crawls, etc.). The unconstrained section con-
tains automatically crawled data from medical and
health websites and non-medical data from patent
collections.
3.1 Parallel data
The parallel data summary is presented in Table 1.
The main sources of the medical-domain data
for all the language pairs include the EMEA cor-
pus (Tiedemann, 2009), the UMLS metathesaurus
of health and biomedical vocabularies and stan-
dards (U.S. National Library of Medicine, 2009),
and bilingual titles of Wikipedia articles belonging
to the categories identified to be medical domain.
Additional medical-domain data comes from the
MAREC patent collection: PatTR (W?aschle and
Riezler, 2012) available for DE?EN and FR?EN,
and COPPA (Pouliquen and Mazenc, 2011) for
FR?EN (only patents from the medical categories
A61, C12N, and C12P are allowed in the con-
strained systems).
The constrained general-domain data include
three parallel corpora for all the language pairs:
CommonCrawl (Smith et al., 2013), Europarl ver-
sion 6 (Koehn, 2005), the News Commentary cor-
pus (Callison-Burch et al., 2012). Further, the con-
strained data include CzEng (Bojar et al., 2012)
for CS?EN and the UN corpus for FR?EN.
For our unconstrained experiments, we also em-
ploy parallel data from the non-medical patents
from the PatTR and COPPA collections (other cat-
egories than A61, C12N, and C12P).
3.2 Monolingual data
The monolingual data is summarized in Table 2.
The main sources of the medical-domain mono-
lingual data for all languages involve Wikipedia
pages, UMLS concept descriptions, and non-
parallel texts extracted from the medical patents
of the PatTR collections. For English, the main
source is the AACT collection of texts from Clin-
icalTrials.gov. Smaller resources include: Drug-
Bank (Knox et al., 2011), GENIA (Kim et al.,
2003), FMA (Rosse and Mejino Jr., 2008), GREC
(Thompson et al., 2009), and PIL (Bouayad-Agha
et al., 2000).
In the unconstrained systems, we use additional
monolingual data from web pages crawled within
the Khresmoi project: a collection of about one
million HON-certified
4
webpages in English re-
leased as the test collection for the CLEF 2013
eHealth Task 3 evaluation campaign,
5
additional
web-crawled HON-certified pages (not publicly
available), and other webcrawled medical-domain
related webpages.
The constrained general-domain resources in-
clude: the News corpus for CS, DE, EN, and FR
collected for the purpose of the WMT 2014 Stan-
dard Task, monolingual parts of the Europarl and
News-Commentary corpora, and the Gigaword for
EN and FR.
For the FR?EN and DE?EN unconstrained sys-
tems, the additional general domain monolingual
data is taken from monolingual texts of non-
medical patents in the PatTR collection.
4
https://www.hon.ch/
5
https://sites.google.com/site/
shareclefehealth/
223
medical general
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
u
n
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
Figure 1: Distribution of the domain-specificity
scores in the English?French parallel data sets.
3.3 Data preprocessing
The data consisting of crawled web pages, namely
CLEF, HON, and non-HON, needed to be cleaned
and transformed into a set of sentences. The
Boilerpipe (Kohlsch?utter et al., 2010) and Justext
(Pomik?alek, 2011) tools were used to remove boil-
erplate texts and extract just the main content from
the web pages. The YALI language detection tool
(Majli?s, 2012) trained on both in-domain and gen-
eral domain data then filtered out those cleaned
pages which were not identified as written in one
of the concerned languages.
The rest of the preprocessing procedure was ap-
plied to all the datasets mentioned above, both
parallel and monolingual. The data were tok-
enized and normalized by converting or omit-
ting some (mostly punctuation) characters. A
set of language-dependent heuristics was applied
in an attempt to restore and normalize the open-
ing/closing quotation marks, i.e. convert "quoted"
to ?quoted? (Zeman, 2012). The motivation here
is twofold: First, we hope that paired quota-
tion marks could occasionally work as brackets
and better denote parallel phrases for Moses; sec-
ond, if Moses learns to output directed quotation
marks, the subsequent detokenization will be eas-
ier. For all systems which translate from German,
decompounding is employed to reduce source-side
data sparsity. We used BananaSplit for this task
(M?uller and Gurevych, 2006).
We perform all training and internal evaluation
on lowercased data; we trained recasers to post-
process the final submissions.
medical general
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
u
n
c
o
n
s
t
r
a
i
n
e
d
?15
?10
?5
0
5
10
15
?15
?10
?5
0
5
10
15
Figure 2: Distribution of the domain-specificity
scores in the French monolingual data sets.
4 Submitted systems
We first describe our technique of psedo-in-
domain data selection in Section 4.1, then com-
pare two methods of combining the selected data
in Section 4.2. This, along with using constrained
and unconstrained data sets to train the systems
(see Section 3), amounts to a total of four system
variants submitted for each task. A description of
the system settings used is given in Section 4.3.
4.1 Data selection
We follow an approach originally proposed for
selection of monolingual sentences for language
modeling (Moore and Lewis, 2010) and its modi-
fication applied to selection of parallel sentences
(Axelrod et al., 2011). This technique assumes
two language models for sentence scoring, one
trained on (true) in-domain text and one trained
on (any) general-domain text in the same lan-
guage (e.g., English). For both data domains
(general and medical), we score each sentence
by the difference of its cross-perplexity given the
in-domain language model and cross-perplexity
given the general-domain language model (in this
order). We only keep sentences with a negative
score in our data, assuming that these are the
most ?medical-like?. Visualisation of the domain-
specificity scores (cross-perplexity difference) in
the FR?EN parallel data and FR monolingual data
is illustrated in Figures 1 and 2, respectively.
6
The
scores (Y axis) are presented for each sentence in
increasing order from left to right (X axis).
6
For the medical domain, constrained and unconstrained
parallel data are identical.
224
cs?en de?en en?cs en?de en?fr fr?en
con concat 33.64?1.14 32.84?1.24 18.10?0.94 18.29?0.92 33.39?1.11 36.71?1.17
con interpol 32.94?1.11 32.31?1.20 18.96?0.93 18.41?0.93 34.06?1.11 37.42?1.21
unc concat 34.10?1.11 34.52?1.20 21.12?1.03 19.76?0.92 36.23?1.03 38.15?1.16
unc interpol 34.48?1.16 34.92?1.17 22.15?1.06 20.81?0.95 36.26?1.13 37.91?1.13
Table 3: BLEU scores of summary translations.
cs?en de?en en?cs en?de en?fr fr?en
con concat 30.87?4.70 33.21?5.03 23.25?4.85 17.72?4.75 28.64?3.77 35.56?4.94
con interpol 32.46?5.05 33.74?4.97 21.56?4.80 16.90?4.39 29.34?3.73 35.28?5.26
unc concat 34.88?5.04 31.24?5.59 22.61?4.91 19.13?5.66 33.08?3.80 36.73?4.88
unc interpol 33.82?5.16 34.19?5.27 23.93?5.16 15.87?11.31 31.19?3.73 40.25?5.14
Table 4: BLEU scores of query translations.
The two language models for sentence scoring
are trained with a restricted vocabulary extracted
from the in-domain training data as words occur-
ring at least twice (singletons and other words are
treated as out-of-vocabulary). In our experiments,
we apply this technique to select both monolin-
gual data for language models and parallel data
for translation models. Selection of parallel data
is based on the English side only. The in-domain
models are trained on the monolingual data in the
target language (constrained or unconstrained, de-
pending on the setting). The general-domain mod-
els are trained on the WMT News data.
Compared to the approach of Moore and Lewis
(2010) and Axelrod et al. (2011), we prune the
model vocabulary more aggressively ? we discard
not only the singletons, but also all words with
non-Latin characters, which helps clean the mod-
els from noise introduced by the automatic process
of data acquisition by web crawling.
4.2 Data combination
For both parallel and monolingual data, we obtain
two data sets after applying the data selection:
? ?medical-like? data from the medical domain
? ?medical-like? data from the general domain.
For each language pair and for each system
type (constrained/unconstrained), we submitted
two system variants which differ in how the se-
lected data are combined. The first variant uses
a simple concatenation of the two datasets both
for parallel data and for language model data. In
the second variant, we train separate models for
each section and use linear interpolation to com-
bine them into a single model. For language mod-
els, we use the SRILM linear interpolation feature
(Stolcke, 2002). We interpolate phrase tables us-
ing Tmcombine (Sennrich, 2012). In both cases,
the held-out set for minimizing the perplexity is
the system development set.
4.3 System details
We compute word alignment on lowercase 4-cha-
racter stems using fast align (Dyer et al., 2013).
We create phrase tables using the Moses toolkit
(Koehn et al., 2007) with standard settings. We
train 5-gram language models on the target-side
lowercase forms using SRILM. We use MERT
(Och, 2003) to tune model weights in our systems
on the development data provided for the task.
The only difference between the system variants
for query and summary translation is the tuning
set. In both cases, we use the respective sets pro-
vided offcially for the shared task.
4.4 Results
Tables 3 and 4 show case-insensitive BLEU scores
of our systems.
7
As expected, the unconstrained
systems outperform the constrained ones. Linear
interpolation outperforms data concatenation quite
reliably across language pairs for summary trans-
lation. While the picture for query translation is
similar, there is more variance in the results, so
we cannot state that interpolation definitely works
7
As we use the same recasers for both summary and query
translation, our systems are heavily penalized for wrong let-
ter case in query translation. However, letter case is not taken
into account in most CLIR systems. All BLEU scores re-
ported in this paper will be case-insensitive for this reason.
225
better in this case. This is due to the sizes of the
development and test sets and most importantly
due to sentence lengths ? queries are very short,
making BLEU unreliable, MERT unstable, and
bootstrap resampling intervals wide.
If we compare our score to the other competi-
tors, we are clearly worse than the best systems for
summary translation. From this perspective, our
data filtering seems overly eager (i.e., discarding
all sentence pairs with a positive perplexity differ-
ence). An experiment which we leave for future
work is doing one more round of interpolation to
combine a model trained on the data with negative
perplexity with models trained on the remainder.
5 Conclusions
We described the Charles University MT system
used in the Shared Medical Translation Task of
WMT 2014. Our primary goal was to set up a
baseline for both the subtasks and all translation
directions. The systems are based on the Moses
toolkit, pseudo-in-domain data selection based on
perplexity difference and two different methods of
in-domain and out-of-domain data combination:
simple data concatenation and linear model inter-
polation.
We report results of constrained and uncon-
strained systems which differ in the training data
only. In most experiments, using additional data
improved the results compared to the constrained
systems and using linear model interpolation out-
performed data concatenation. While our systems
are on par with best results for case-insensitive
BLEU score in query translation, our overly ea-
ger data selection techniques caused lower scores
in summary translation. In future work, we plan
to include a special out-of-domain model in our
setup to compensate for this problem.
Acknowledgments
This work was supported by the EU FP7 project
Khresmoi (contract no. 257528), the Czech Sci-
ence Foundation (grant no. P103/12/G084), and
SVV project number 260 104. This work has
been using language resources developed, stored,
and distributed by the LINDAT/CLARIN project
of the Ministry of Education, Youth and Sports of
the Czech Republic (project LM2010013).
References
A. Axelrod, X. He, and J. Gao. 2011. Domain adap-
tation via pseudo in-domain data selection. In Pro-
ceedings of the 2011 Conference on Empirical Meth-
ods in Natural Language Processing, pages 355?
362, Edinburgh, United Kingdom. ACL.
A. Bisazza, N. Ruiz, and M. Federico. 2011. Fill-
up versus interpolation methods for phrase-based
SMT adaptation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 136?143, San Francisco, CA, USA. Interna-
tional Speech Communication Association.
O. Bojar, Z.
?
Zabokrtsk?y, O. Du?sek, P. Galu?s?c?akov?a,
M. Majli?s, D. Mare?cek, J. Mar?s??k, M. Nov?ak,
M. Popel, and A. Tamchyna. 2012. The joy of
parallelism with CzEng 1.0. In Proceedings of the
Eighth International Conference on Language Re-
sources and Evaluation, pages 3921?3928, Istanbul,
Turkey. European Language Resources Association.
N. Bouayad-Agha, D. R. Scott, and R. Power. 2000.
Integrating content and style in documents: A case
study of patient information leaflets. Information
Design Journal, 9(2?3):161?176.
W. Byrne, D. S. Doermann, M. Franz, S. Gustman,
J. Haji?c, D. W. Oard, et al. 2004. Automatic recog-
nition of spontaneous speech for access to multilin-
gual oral history archives. Speech and Audio Pro-
cessing, IEEE Transactions on, 12(4):420?435.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Translation.
In Proceedings of the Seventh Workshop on Statis-
tical Machine Translation, pages 10?51, Montr?eal,
Canada. ACL.
M. Carpuat, H. Daum?e III, A. Fraser, C. Quirk,
F. Braune, A. Clifton, et al. 2012. Domain adap-
tation in machine translation: Final report. In
2012 Johns Hopkins Summer Workshop Final Re-
port, pages 61?72. Johns Hopkins University.
M. R. Costa-juss`a, M. Farr?us, and J. Serrano Pons.
2012. Machine translation in medicine. A qual-
ity analysis of statistical machine translation in the
medical domain. In Proceedings of the 1st Virtual
International Conference on Advanced Research in
Scientific Areas, pages 1995?1998,
?
Zilina, Slovakia.
?
Zilinsk?a univerzita.
C. Dyer, V. Chahuneau, and N. A. Smith. 2013. A sim-
ple, fast, and effective reparameterization of IBM
model 2. In Proceedings of NAACL-HLT, pages
644?648.
M. Eck, S. Vogel, and A. Waibel. 2004a. Improv-
ing statistical machine translation in the medical do-
main using the Unified Medical Language System.
In COLING 2004: Proceedings of the 20th Inter-
national Conference on Computational Linguistics,
pages 792?798, Geneva, Switzerland. ACL.
226
M. Eck, S. Vogel, and A. Waibel. 2004b. Language
model adaptation for statistical machine translation
based on information retrieval. In Maria Teresa
Lino, Maria Francisca Xavier, F?atima Ferreira, Rute
Costa, and Raquel Silva, editors, Proceedings of the
International Conference on Language Resources
and Evaluation, pages 327?330, Lisbon, Portugal.
European Language Resources Association.
A. S. Hildebrand, M. Eck, S. Vogel, and A. Waibel.
2005. Adaptation of the translation model for statis-
tical machine translation based on information re-
trieval. In Proceedings of the 10th Annual Con-
ference of the European Association for Machine
Translation, pages 133?142, Budapest, Hungary.
European Association for Machine Translation.
A. Jimeno Yepes,
?
E. Prieur-Gaston, and A. N?ev?eol.
2013. Combining MEDLINE and publisher data to
create parallel corpora for the automatic translation
of biomedical text. BMC Bioinformatics, 14(1):1?
10.
J.-D Kim, T. Ohta, Y. Tateisi, and J. Tsujii. 2003. GE-
NIA corpus ? a semantically annotated corpus for
bio-textmining. Bioinformatics, 19(suppl 1):i180?
i182.
C. Knox, V. Law, T. Jewison, P. Liu, Son Ly, A. Frolkis,
A. Pon, K. Banco, C. Mak, V. Neveu, Y. Djoum-
bou, R. Eisner, A. C. Guo, and D. S. Wishart.
2011. DrugBank 3.0: a comprehensive resource for
?Omics? research on drugs. Nucleic acids research,
39(suppl 1):D1035?D1041.
P. Koehn and J. Schroeder. 2007. Experiments in do-
main adaptation for statistical machine translation.
In Proceedings of the Second Workshop on Statis-
tical Machine Translation, pages 224?227, Prague,
Czech Republic. ACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open Source Toolkit
for Statistical Machine Translation. In Proceedings
of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages
177?180, Praha, Czechia, June. ACL.
P. Koehn. 2005. Europarl: a parallel corpus for sta-
tistical machine translation. In Conference Proceed-
ings: the tenth Machine Translation Summit, pages
79?86, Phuket, Thailand. Asia-Pacific Association
for Machine Translation.
C. Kohlsch?utter, P. Fankhauser, and W. Nejdl. 2010.
Boilerplate detection using shallow text features. In
Proceedings of the Third ACM International Confer-
ence on Web Search and Data Mining, WSDM ?10,
pages 441?450, New York, NY, USA. ACM.
P. Langlais. 2002. Improving a general-purpose statis-
tical translation engine by terminological lexicons.
In COLING-02 on COMPUTERM 2002: second
international workshop on computational terminol-
ogy, volume 14, pages 1?7, Taipei, Taiwan. ACL.
M. Majli?s. 2012. Yet another language identifier. In
Proceedings of the Student Research Workshop at
the 13th Conference of the European Chapter of the
Association for Computational Linguistics, pages
46?54, Avignon, France. ACL.
S. Mansour, J. Wuebker, and H. Ney. 2011. Com-
bining translation and language model scoring for
domain-specific data filtering. In International
Workshop on Spoken Language Translation, pages
222?229, San Francisco, CA, USA. ISCA.
R. C. Moore and W. Lewis. 2010. Intelligent selection
of language model training data. In Proceedings of
the ACL 2010 Conference Short Papers, pages 220?
224, Uppsala, Sweden. ACL.
C. M?uller and I. Gurevych. 2006. Exploring the po-
tential of semantic relatedness in information re-
trieval. In LWA 2006 Lernen ? Wissensentdeck-
ung ? Adaptivit?at, 9.-11.10.2006, Hildesheimer In-
formatikberichte, pages 126?131, Hildesheim, Ger-
many. Universit?at Hildesheim.
P. Nakov. 2008. Improving English?Spanish statistical
machine translation: Experiments in domain adapta-
tion, sentence paraphrasing, tokenization, and recas-
ing. In Proceedings of the Third Workshop on Statis-
tical Machine Translation, pages 147?150, Colum-
bus, OH, USA. ACL.
F. J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In ACL ?03: Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics, pages 160?167, Morristown,
NJ, USA. ACL.
J. Pomik?alek. 2011. Removing Boilerplate and Du-
plicate Content from Web Corpora. PhD thesis,
Masaryk University, Faculty of Informatics, Brno.
B. Pouliquen and C. Mazenc. 2011. COPPA, CLIR
and TAPTA: three tools to assist in overcoming the
patent barrier at WIPO. In Proceedings of the Thir-
teenth Machine Translation Summit, pages 24?30,
Xiamen, China. Asia-Pacific Association for Ma-
chine Translation.
C. Rosse and Jos?e L. V. Mejino Jr. 2008. The foun-
dational model of anatomy ontology. In A. Burger,
D. Davidson, and R. Baldock, editors, Anatomy On-
tologies for Bioinformatics, volume 6 of Computa-
tional Biology, pages 59?117. Springer London.
G. Sanchis-Trilles and F. Casacuberta. 2010. Log-
linear weight optimisation via Bayesian adaptation
in statistical machine translation. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters, pages 1077?1085, Bei-
jing, China. ACL.
227
R. Sennrich. 2012. Perplexity minimization for trans-
lation model domain adaptation in statistical ma-
chine translation. In Proceedings of the 13th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, pages 539?549. ACL.
J. R. Smith, H. Saint-Amand, M. Plamada, P. Koehn,
C. Callison-Burch, and A. Lopez. 2013. Dirt cheap
web-scale parallel text from the common crawl. In
Proceedings of the 51st Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 1374?1383, Sofia, Bulgaria.
ACL.
A. Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proceedings of International
Conference on Spoken Language Processing, Den-
ver, Colorado, USA.
P. Thompson, S. Iqbal, J. McNaught, and Sophia Ana-
niadou. 2009. Construction of an annotated corpus
to support biomedical information extraction. BMC
bioinformatics, 10(1):349.
J. Tiedemann. 2009. News from OPUS ? a collection
of multilingual parallel corpora with tools and in-
terfaces. In Recent Advances in Natural Language
Processing, volume 5, pages 237?248, Borovets,
Bulgaria. John Benjamins.
U.S. National Library of Medicine. 2009. UMLS
reference manual. Metathesaurus. Bethesda, MD,
USA.
K. W?aschle and S. Riezler. 2012. Analyzing paral-
lelism and domain similarities in the MAREC patent
corpus. In M. Salampasis and B. Larsen, edi-
tors, Multidisciplinary Information Retrieval, vol-
ume 7356 of Lecture Notes in Computer Science,
pages 12?27. Springer Berlin Heidelberg.
H. Wu and H. Wang. 2004. Improving domain-specific
word alignment with a general bilingual corpus. In
Robert E. Frederking and Kathryn B. Taylor, editors,
Machine Translation: From Real Users to Research,
volume 3265 of Lecture Notes in Computer Science,
pages 262?271. Springer Berlin Heidelberg.
C. Wu, F. Xia, L. Deleger, and I. Solti. 2011. Statistical
machine translation for biomedical text: are we there
yet? AMIA Annual Symposium proceedings, pages
1290?1299.
D. Zeman. 2012. Data issues of the multilingual trans-
lation matrix. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 395?
400, Montr?eal, Canada. ACL.
228
