Combination of Machine Learning Methods
for Optimum Chinese Word Segmentation
Masayuki Asahara
Chooi-Ling Goh
Kenta Fukuoka
Yotaro Watanabe
Nara Institute of Science and Technology, Japan
E-mail: cje@is.naist.jp
Ai Azuma
Yuji Matsumoto
Takashi Tsuzuki
Matsushita Electric
Industrial Co., Ltd.
Abstract
This article presents our recent work for par-
ticipation in the Second International Chi-
nese Word Segmentation Bakeoff. Our
system performs two procedures: Out-of-
vocabulary extraction and word segmenta-
tion. We compose three out-of-vocabulary
extraction modules: Character-based tag-
ging with different classifiers ? maximum
entropy, support vector machines, and con-
ditional random fields. We also com-
pose three word segmentation modules ?
character-based tagging by maximum en-
tropy classifier, maximum entropy markov
model, and conditional random fields. All
modules are based on previously proposed
methods. We submitted three systems which
are different combination of the modules.
1 Overview
We compose three systems: Models a, b and c for the
closed test tracks on all four data sets.
For Models a and c, three out-of-vocabulary (OOV)
word extraction modules are composed: 1. Maximum
Entropy (MaxEnt) classifier-based tagging; 2. Max-
imum Entropy Markov Model (MEMM)-based word
segmenter with Conditional Random Fields (CRF)-
based chunking; 3. MEMM-based word segmenter
with Support Vector Machines (SVM)-based chunk-
ing. Two lists of OOV word candidates are constructed
either by voting or merging the three OOV word ex-
traction modules. Finally, a CRFs-based word seg-
menter produces the final results using either of the
voted list (Model a) or the merged list (Model c).
Most of the classifiers use surrounding words and
characters as the contextual features. Since word and
character features may cause data sparse problem, we
utilize a hard clustering algorithm (K-means) to define
word classes and character classes in order to over-
come the data sparse problem. The word classes are
used as the hidden states in MEMM and CRF-based
word segmenters. The character classes are used as the
features in character-based tagging, character-based
chunking and word segmentation.
Model b is our previous method proposed in (Goh
et al, 2004b): First, a MaxEnt classifier is used to per-
form character-based tagging to identify OOV words
in the test data. In-vocabulary (IV) word list together
with the extracted OOV word candidates is used in
Maximum Matching algorithm. Overlapping ambi-
guity is denoted by the different outputs from For-
ward and Backward Maximum Matching algorithm.
Finally, character-based tagging by MaxEnt classifier
resolves the ambiguity.
Section 2 describes Models a and c. Section 3 de-
scribes Model b. Section 4 discusses the differences
among the three models.
2 Models a and c
Models a and c use several modules. First, a hard
clustering algorithm is used to define word classes and
character classes. Second, three OOV extraction mod-
ules are trained with the training data. These modules,
then, extract the OOV words in the test data. Third,
the OOV word candidates produced by the three OOV
extraction modules are refined by voting (Model a) or
merging (Model c) them. The final word list is com-
posed by appending the OOV word candidates to the
IV word list. Finally, a CRF-based word segmenter
analyzes the sentence based on the new word list.
2.1 Clustering for word/character classes
We perform hard clustering for all words
and characters in the training data. K-
means algorithm is utilized. We use R 2.2.1
(http://www.r-project.org/) to perform
k-means clustering.
134
Since the word types are too large, we cannot run k-
means clustering on the whole data. Therefore, we di-
vide the word types into 4 groups randomly. K-means
clustering is performed for each group. Words in each
group are divided into 5 disjoint classes, producing 20
classes in total. Preceding and succeeding words in the
top 2000 rank are used as the features for the cluster-
ing. We define the set of the OOV words as the 21st
class. We also define two other classes for the begin-
of-sentence (BOS) and end-of-sentence (EOS). So, we
define 23 classes in total.
20 classes are defined for characters. K-means clus-
tering is performed for all characters in the training
data. Preceding and succeeding characters and BIES
position tags are used as features for the clustering:
?B? stands for ?the first character of a word?; ?I? stands
for ?an intermediate character of a word?; ?E? stands
for ?the last character of a word?; ?S? stands for ?the
single character word?. Characters only in the test data
are not assigned with any character class.
2.2 Three OOV extraction modules
In Models a and c, we use three OOV extraction mod-
ules.
First and second OOV extraction modules use
the output of a Maximam Entropy Markov Model
(MEMM)-based word segmenter (McCallum et al,
2000) (Uchimoto et al, 2001). Word list is composed
by the words appeared in 80% of the training data.
The words occured only in the remaining 20% of the
training data are regarded as OOV words. All word
candidates in a sentence are extracted to form a trel-
lis. Each word is assigned with a word class. The
word classes are used as the hidden states in the trellis.
In encoding, MaxEnt estimates state transition proba-
bilities based on the preceding word class (state) and
observed features such as the first character, last char-
acter, first character class, last character class of the
current word. In decoding, a simple Viterbi algorithm
is used.
The output of the MEMM-based word segmenter is
splitted character by character. Next, character-based
chunking is performed to extract OOV words. We use
two chunkers: based on SVM (Kudo and Matsumoto,
2001) and CRF (Lafferty et al, 2001). The chunker
annotates BIO position tags: ?B? stands for ?the first
character of an OOV word?; ?I? stands for ?other char-
acters in an OOV word?; ?O? stands for ?a character
outside an OOV word?.
The features used in the two chunkers are the char-
acters, the character classes and the information of
other characters in five-character window size. The
word sequence output by the MEMM-based word seg-
menter is converted into character sequence with BIES
position tags and the word classes. The position tags
with the word classes are also introduced as the fea-
tures.
The third one is a variation of the OOV module in
section 3 which is character-based tagging by MaxEnt
classifier. The difference is that we newly introduce
character classes in section 2.1 as the features.
In summary, we introduce three OOV word extrac-
tion modules: ?MEMM+SVM?, ?MEMM+CRF? and
?MaxEnt classifier?.
2.3 Voting/Merging the OOV words
The word list for the final word segmenter are com-
posed by voting or merging. Voting means the OOV
words which are extracted by two or more OOV word
extraction modules. Merging means the OOV words
which are extracted by any of the OOV word extrac-
tion modules. The model with the former (voting)
OOV word list is used in Model a, and the model with
the latter (merging) OOV word list is used in Model c.
2.4 CRF-based word segmenter
Final word segmentation is carried out by a CRF-based
word segmenter (Kudo and Matsumoto, 2004) (Peng
and McCallum, 2004). The word trellis is composed
by the similar method with MEMM-based word seg-
menter. Though state transition probabilities are esti-
mated in the case of MaxEnt framework, the proba-
bilities are normalized in the whole sentence in CRF-
based method. CRF-based word segmenter is robust to
length-bias problem (Kudo and Matsumoto, 2004) by
the global normalization. We will discuss the length-
bias problem in section 4.
2.5 Note on MSR data
Unfortunately, we could not complete Models a and
c for the MSR data due to time constraints. There-
fore, we submitted the following 2 fragmented mod-
els: Model a for MSR data is MEMM-based word
segmenter with OOV word list by voting; Model c for
MSR data is CRF-based word segmenter with no OOV
word candidate.
3 Model b
Model b uses a different approach. First, we extract the
OOV words using a MaxEnt classifier with only the
character as the features. We did not use the character
classes as the features. Each character is assigned with
BIES position tags. Word segmentation by character-
based tagging is firstly introduced by (Xue and Con-
verse, 2002). In encoding, we extract characters within
five-character window size for each character position
in the training data as the features for the classifier.
In decoding, the BIES position tag is deterministically
annotated character by character in the test data. The
135
words that appear only in the test data are treated as
OOV word candidates.
We can obtain quite high unknown word recall with
this model but the precision is a bit low. However,
the following segmentation model will try to elimi-
nate some false unknown words. In the next step, we
append OOV word candidates into the IV word list
extracted from the training data. The segmentation
model is similar to the OOV extraction method, except
that the features include the output from the Maximum
Matching (MaxMatch) algorithm. The algorithm runs
in both forward (FMaxMatch) and backward (BMax-
Match) directions using the final word list as the ref-
erences. The outputs of FMaxMatch and BMaxMatch
are also assigned with BIES tags. The differences be-
tween the FMaxMatch and BMaxMatch outputs indi-
cate the positions where the overlapping ambiguities
occur. The final word segmentation is carried out by
MaxEnt classifier again.
Note, both procedures in Model b use whole train-
ing data in the training phase. The dictionary used in
the MaxMatch algorithm is extracted from the training
data only during the training phase. So, the training of
segmentation model does not explicitly consider OOV
words. We did not use the word and character classes
as features in Model b unlike in the case of Models a
and c. The details of the model can be found in (Goh
et al, 2004b). The difference is that we do not pro-
vide character types here because it is forbidden in
this round. Besides, we also did not prune the OOV
words because this step involve the intervention of hu-
man knowledge.
4 Discussions and Conclusions
Table 1 summarizes the results of the three models.
The proposed systems employ purely corpus-based
statistical/machine learning method. Now, we discuss
what we observe in the three models. We remark two
problems in word segmentation: OOV word problem
and length-bias problem.
OOV word problem is that simple word-based
Markov Model family cannot analyze the words not
included in the word list. One of the solutions is
character-based tagging (Xue and Converse, 2002)
(Goh et al, 2004a). The simple character-based tag-
ging (Model b) achieved high ROOV but the precision
is low. We tried to refine OOV extraction by voting
and merging (Model a and c). However, the ROOV
of Models a and c are not as good as that of Model
b. Figure 1 shows type-precision and type-recall of
each OOV extraction modules. While voting helps to
make the precision higher, voting deteriorates the re-
call. Defining some hand written rules to prune false
OOV words will help to improve the IV word segmen-
tation (Goh et al, 2004b), because the precision of
OOV word extraction becomes higher. Other types of
OOV word extraction methods should be introduced.
For example, (Uchimoto et al, 2001) embeded OOV
models in MEMM-based word segmenter (with POS
tagging). Less than six-character substrings are ex-
tracted as the OOV word candidates in the word trel-
lis. (Peng and McCallum, 2004) proposed OOV word
extraction methods based on CRF-based word seg-
menter. Their CRF-based word segmenter can com-
pute a confidence in each segment. The high confi-
dent segments that are not in the IV word list are re-
garded as OOV word candidates. (Nakagawa, 2004)
proposed integration of word and OOV word position
tag in a trellis. These three OOV extraction method are
different from our methods ? character-based tagging.
Future work will include implementation of these dif-
ferent sorts of OOV word extraction modules.
Length bias problem means the tendency that the lo-
cally normalized Markov Model family prefers longer
words. Since choosing the longer words reduces the
number of words in a sentence, the state-transitions are
reduced. The less the state-transitions, the larger the
likelihood of the whole sentence. Actually, the length-
bias reflects the real distribution in the corpus. Still,
the length-bias problem is nonnegligible to achieve
high accuracy due to small exceptional cases. We used
CRF-based word segmenter which relaxes the prob-
lem (Kudo and Matsumoto, 2004). Actually, the CRF-
based word segmenter achieved high RIV .
We could not complete Model a and c for MSR.
After the deadline, we managed to complete Model
a (CRF + Voted Unk.) and c (CRF + Merged Unk.)
The result of Model a was precesion 0.976, recall
0.966, F-measure 0.971, OOV recall 0.570 and IV re-
call 0.988. The result of Model c was precesion 0.969,
recall 0.963, F-measure 0.966, OOV recall 0.571 and
IV recall 0.974. While the results are quite good, un-
fortunately, we could not submit the outputs in time.
While our results for the three data sets (AS,
CITYU, MSR) are fairly good, the result for the PKU
data is not as good. There is no correlation between
scores and OOV word rates. We investigate unseen
character distributions in the data set. There is no cor-
relation between scores and unseen character distribu-
tions.
We expected Model c (merging) to achieve higher
recall for OOV words than Model a (voting). How-
ever, the result was opposite. The noises in OOV
word candidates should have deteriorated the F-value
of overall word segmentation. One reason might be
that our CRF-based segmenter could not encode the
occurence of OOV words. We defined the 21st word
class for OOV words. However, the training data for
CRF-based segmenter did not contain the 21st class.
We should include the 21st class in the training data
136
Table 1: Our Three Models and Results: F-value/ROOV /RIV (Rank of F-value)
AS CITYU MSR PKU
Model a CRF + Voted Unk. CRF + Voted Unk. MEMM + Voted Unk. CRF + Voted Unk.
0.947/0.606/0.971 0.942/0.629/0.967 0.949/0.378/0.971 0.934/0.521/0.955
(2/11) (2/15) (16/29) (10/23)
Model b Char.-based tagging Char.-based tagging Char.-based tagging Char.-based tagging
0.952/0.696/0.963 0.941/0.736/0.953 0.958/0.718/0.958 0.941/0.760/0.941
(1/11) (3/15) (6/29) (7/23)
Model c CRF + Merged Unk. CRF + Merged Unk. CRF + No Unk. CRF + Merged Unk.
0.939/0.445/0.967 0.928/0.598/0.940 0.943/0.025/0.990 0.917/0.325/0.940
(7/11) (8/15) (21/29) (14/23)
150/764
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1727/2504=0.689Voted Recall = 1727/3226=0.535Merged Precision = 2532/6003=0.421Merged Recall = 2532/3226=0.784
69/599 586/2136
165/480 420/579
184/304
958/1141
AS
51/406
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1068/1714=0.623Voted Recall = 1068/1670=0.639Merged Precision = 1367/3531=0.387Merged Recall = 1367/1670=0.818
42/352 206/1059
87/439 114/188
109/196
758/891
CITYU
correctly extracted types(left side)
extracted types(right side)
57/555
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1196/1659=0.720Voted Recall = 1196/1991=0.600Merged Precision = 1628/4454=0.365Merged Recall = 1628/1991=0.817
40/330 335/1910
93/293 149/243
245/333
709/790
MSR
67/882
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1528/2827=0.540Voted Recall = 1528/2863=0.533Merged Precision = 2184/7064=0.309Merged Recall = 2184/2863=0.762
87/720 502/2635
181/727 217/424
201/407
929/1269
PKU
Figure 1: OOV Extraction Precision and Recall by Type
by regarding some words as pseudo OOV words.
We also found a bug in the CRF-based OOV word
extration module. The accuracy of the module might
be slightly better than the reported results. However,
the effect of the bug on overall F-value might be lim-
ited, since the module was only part of the OOV ex-
traction module combination ? voting and merging.
Acknowledgement
We would like to express our appreciation to Dr. Taku
Kudo who developed SVM-based chunker and gave us
several fruitful comments.
References
Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-
sumoto. 2004a. Chinese Word Segmentation by
Classification of Characters. In Proc. of Third
SIGHAN Workshop, pages 57?64.
Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-
sumoto. 2004b. Pruning False Unknown Words to
Improve Chinese Word Segmentation. In Proc. of
PACLIC-18, pages 139?149.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with Support Vector Machines. In Proc. of NAACL-
2001, pages 192?199.
Taku Kudo and Yuji Matsumoto. 2004. Applying
Conditional Random Fields to Japanese Morpho-
logical Analysis. In Proc. of EMNLP-2004, pages
230?237.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proc. of ICML-2001, pages 282?
289.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum Entropy Markov Mod-
els for Information Extraction and Segmentation. In
Proc. of ICML-2000, pages 591?598.
Tetsuji Nakagawa. 2004. Chinese and Japanese Word
Segmentation Using Word-Level and Character-
Level Information. In Proc. of COLING-2004,
pages 466?472.
Fuchun Peng and Andrew McCallum. 2004. Chinese
Segmentation and New Word Detection using Con-
ditional Random Fields. In Proc. of COLING-2004,
pages 562?568.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The Unknown Word Problem: a
Morphological Analysis of Japanese Using Maxi-
mum Entropy Aided by a Dictionary. In Proc. of
EMNLP-2001, pages 91?99.
Nianwen Xue and Susan P. Converse. 2002. Combin-
ing Classifiers for Chinese Word Segmentation. In
Proc. of First SIGHAN Workshop, pages 63?70.
137
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 628?637,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Multilayer Sequence Labeling
Ai Azuma Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology
Ikoma, Nara 630-0192, Japan
{ai-a,matsu}@is.naist.jp
Abstract
In this paper, we describe a novel approach to
cascaded learning and inference on sequences.
We propose a weakly joint learning model
on cascaded inference on sequences, called
multilayer sequence labeling. In this model,
inference on sequences is modeled as cas-
caded decision. However, the decision on a
sequence labeling sequel to other decisions
utilizes the features on the preceding results
as marginalized by the probabilistic models
on them. It is not novel itself, but our idea
central to this paper is that the probabilis-
tic models on succeeding labeling are viewed
as indirectly depending on the probabilistic
models on preceding analyses. We also pro-
pose two types of efficient dynamic program-
ming which are required in the gradient-based
optimization of an objective function. One
of the dynamic programming algorithms re-
sembles back propagation algorithm for mul-
tilayer feed-forward neural networks. The
other is a generalized version of the forward-
backward algorithm. We also report experi-
ments of cascaded part-of-speech tagging and
chunking of English sentences and show ef-
fectiveness of the proposed method.
1 Introduction
Machine learning approach is widely used to clas-
sify instances into discrete categories. In many
tasks, however, some set of inter-related labels
should be decided simultaneously. Such tasks are
called structured prediction. Sequence labeling is
the simplest subclass of structured prediction prob-
lems. In sequence labeling, the most likely one
among all the possible label sequences is predicted
for a given input. Although sequence labeling is
the simplest subclass, a lot of real-world tasks are
modeled as problems of this simplest subclass. In
addition, it might offer valuable insight and a toe-
hold for more general and complex structured pre-
diction problems. Many models have been proposed
for sequence labeling tasks, such as Hidden Markov
Models (HMM), Conditional Random Fields (CRF)
(Lafferty et al, 2001), Max-Margin Markov Net-
works (Taskar et al, 2003) and others. These models
have been applied to lots of practical tasks in natural
language processing (NLP), bioinformatics, speech
recognition, and so on. And they have shown great
success in recent years.
In real-world tasks, it is often needed to cascade
multiple predictions. A cascade of predictions here
means the situation in which some of predictions are
made based upon the results of other predictions.
Sequence labeling is not an exception. For exam-
ple, in NLP, we perform named entity recognition or
base-phrase chunking for given sentences based on
part-of-speech (POS) labels predicted by another se-
quence labeler. Natural languages are especially in-
terpreted to have a hierarchy of sequential structures
on different levels of abstraction. Therefore, many
tasks in NLP are modeled as a cascade of sequence
predictions.
If a prediction is based upon the result of another
prediction, we call the former upper stage and the
latter lower stage.
Methods pursued for a cascade of predictions ?
including sequence predictions, of course?, are de-
sired to perform certain types of capability. One de-
628
sired capability is rich forward information propa-
gation, that is, the learning and estimation on each
stage of predictions should utilize rich informa-
tion of the results of lower stages whenever pos-
sible. ?Rich information? here includes next bests
and confidence information of the results of lower
stages. Another is backward information propaga-
tion, that is, the rich annotated data on an upper stage
should affect the models on lower stages retroac-
tively.
Many current systems for a cascade of sequence
predictions adopt a simple 1-best feed-forward ap-
proach. They simply take the most likely output at
each prediction stage and transfer it to the next upper
stage. Such a framework can maximize reusability
of existing sequence labeling systems. On the other
hand, it exhibits a strong tendency to propagate er-
rors to upper labelers.
Typical improvement on the 1-best approach is
to keep k-best results in the cascade of predictions.
However, the larger k becomes, the more difficult it
is to enumerate and maintain the k-best results. It is
particularly prominent in sequence labeling.
The essence of this orientation is that the labeler
on an upper stage utilizes the information of all the
possible output candidates on lower stages. How-
ever, the size of the output space can become quite
large in sequence labeling. It effectively forbids ex-
plicit enumeration of all possible outputs, so it is
required to represent all the labeling possibilities
compactly or employ some approximation schemes.
Several studies are in this direction. In the method
proposed in Finkel et al (2006), a cascades of se-
quence predictions is viewed as a Bayesian network,
and sample sequences are drawn at each stage ac-
cording to the output distribution. The samples are
then used to estimate the entire distribution of the
cascade. In the method proposed in Bunescu (2008),
an upper labeler uses the probabilities marginalized
on the parts of the output sequences on lower stages
as weights for the features. The weighted features
are integrated in the model of the labeler on the
upper stage. A k-best approach (e.g., (Collins and
Duffy, 2002)) and the methods mentioned above are
effective to improve the forward information propa-
gation. However, they can never contribute on back-
ward information propagation.
To improve the both directions of information
propagation, Some studies propose the joint learning
of multiple sequence labelers. Sutton et al (2007)
proposes the joint learning method in case where
multiple labels are assigned to each time slice of
the input sequences. It enables simultaneous learn-
ing and estimation of multiple sequence labelings
on the same input sequences, where time slices of
the outputs of all the out sequences are regularly
aligned. However, it puts the distribution of states
into Bayesian networks with cyclic dependencies,
and exact inference is not tractable in such a model
in general. Therefore, it requires some approxi-
mate inference algorithms in learning or predictions.
Moreover, it only considers the cases where labels of
an input sequence and all output sequences are reg-
ularly aligned. It is not clear how to build a joint
labeling model which handles irregular output label
sequences like semi-Markov models (Sarawagi and
Cohen, 2005).
In this paper, we propose a middle ground for
a cascade of sequence predictions. The proposed
method adopts the basic idea of Bunescu (2008). We
first assume that the model on all the sequence la-
beling stages is probabilistic one. In modeling of an
upper stage, a feature is weighted by the marginal
probability of the fragment of the outputs from a
lower stage. However, this is not novel itself be-
cause it is just a paraphrase of Bunescu?s core idea.
Our intuition behind the proposed method is as fol-
lows. Features integrated in the model on each stage
are weighted by the marginal probabilities of the
fragments of the outputs on lower stages. So, if
the output distributions on lower stages change, the
marginal probabilities of any fragments also change,
and this in turn can change the value of the features
on the upper stage. In other words, the features on
an upper stage indirectly depend on the models on
the lower stages. Based on this intuition, the learn-
ing procedure of the model on an upper stage can
affect not only direct model parameters, but also the
weights of the features by changing the model on
the lower stages. Supervised learning based on an-
notated data on an upper stage may affect the model
or model parameters on the lower stages. It could
be said that the information of annotation data on
an upper stage is propagated back to the model on
lower stages.
In the next section, we describe the formal nota-
629
tion of our model. In Section 3, we propose an opti-
mization procedure according to the intuition noted
above. In Section 4, we report an experimental result
of our method. The proposed method shows some
improvements on a real-world task in comparison
with ordinary methods.
2 Formalization
In this section, we introduce the formal notation of
our model. Hereafter, for the sake of simplicity, we
only describe the simplest case in which there are
just two stages, one lower stage of sequence labeling
named L1 and one upper stage of sequence labeling
named L2. In L1, the most likely one among a set
of possible sequences is predicted for a given input
x. L2 is also a sequence labeling stage for the same
input x and the output of L1. No assumption is made
on the structure of x. The information of x is totally
encoded in feature functions. It is only assumed that
the output spaces of both L1 and L2 are conditioned
on the initial input x.
First of all, we describe the formalization of the
probabilistic model for L1. The model for L1 per
se is the same as ordinary ones for sequence label-
ing. For a given input x, consider a directed acyclic
graph (DAG) G1 = (V1, E1). A source of a DAG G
is a node whose in-degree is equal to zero. A sink
of a DAG G is nodes whose out-degree is equal to
zero. Let src(G), snk(G) denote the set of source
and sink nodes in G, respectively. A successful path
of a DAG G is defined as a directed path on G whose
starting node is a source and end node is a sink. If y
denotes a path on a DAG, let y also denote the set of
all the arcs appearing on y for the sake of shorthand.
We denote the set of all the possible successful paths
on G1 by Y1. The space of the output candidates for
L1 is exactly equal to Y1. For the modeling of L1, it
is assumed that features of the form f?1,k1,e1,x? ? R
(k1 ? K1, e1 ? E1) are allowed to be used. Here,
K1 is the index set of the feature types for L1. Such
a feature can capture an aspect of the correlation be-
tween adjacent nodes. We call this kind of features
input features for L1. This naming is used to distin-
guish them from another kind of features defined on
L1, which comes later. Although features on V1 can
be also defined, they are totally omitted in this paper
for brevity. Hereafter, if a symbol has subscripts,
then missing subscript indicates a set that range over
the omitted subscript. For example, f?1,e1,x?
def?
{
f?1,k1,e1,x?
}
k1?K1 , f?1,k1,x?
def?
{
f?1,k1,e1,x?
}
e1?E1 ,
f?1,x?
def?
{
f?1,k1,e1,x?
}
k1?K1,e1?E1 , and so on.
The probabilistic model on L1 forms the log-linear
model, that is,
P1(y1|x;?1)
def? 1Z1(x;?1)
exp
(
?1 ? F?1,y1,x?
)
(y1 ? Y1) ,
(1)
where ??1,k1? ? R (k1 ? K1) is the weight for the
feature of the same index k1, and the k1-th element
of F?1,y1,x?, F?1,k1,y1,x?
def? ?e1?y1 f?1,k1,e1,x?. Dot
operator (?) denotes the inner product with respect to
the subscripts commonly missing in both operands.
Z1 is the partition function for P1, defined as
Z1(x;?1)
def?
?
y1?Y1
exp
(
?1 ? F?1,y1,x?
)
. (2)
It is worth noting that this formalization subsumes
both directed and undirected linear-chain graphical
models, which are the most typical models for se-
quence labeling, including HMM and CRF. That is,
if the elements of V1 are aligned into regular time
slices, and the nodes in each time slice are associated
with possible assignments of labels for that time, we
obtain the representation equivalent to the ordinary
linear-chain graphical models, in which all possible
label assignments for each state are expanded. In
such configuration, all the possible successful paths
defined in our notation have strict one-to-one corre-
spondence to all the possible joint assignments of
labels in linear-chain graphical models. We pur-
posely employ this DAG-based notation because; it
is convenient to describe the models and algorithms
for our purpose, it allows for labels to stay in arbi-
trary time as in semi-Markov models, and it is easily
extended to models for a set of trees instead of se-
quences by replacing the graph-based notation with
hypergraph-based notation.
Next, we formalize the probabilistic model on the
upper stage L2. Like L1, consider a DAG G2 =
(V2, E2) conditioned on the input x, and the set of
all the possible successful paths on G2, denoted Y2.
The space of the output candidates for L2 becomes
Y2.
630
The form of the features available in designing the
probabilistic model for L2, denoted by P2, is the key
of this paper. A feature on an arc e2 ? E2 can ac-
cess local characteristics of the confidence-rated su-
perposition of the L1?s outputs, in addition to the
information of the input x. To formulate local char-
acteristics of the superposition of the L1?s outputs,
we first define output features of L1, denoted by
h?1,k?1,e1? ? R (k?1 ? K?1, e1 ? E1). Here, K?1 is
the index set of the output feature types of L1. Be-
fore the output features are integrated into the model
for L2, they all are confidence-rated with respect to
P1, that is, each output feature h?1,k?1,e1? is numer-
ically rated by the estimated probabilities summed
over the sequences emitting that feature. More for-
mally, all the L1?s output features are integrated in
features for P2 in the form of the marginalized out-
put features, which are defined as follows;
h??1,k?1,e1?(?1)
def? h?1,k?1,e1?P1(e1|x;?1)(
k?1 ? K?1, e1 ? E1
)
,
(3)
where
P1(e1|x;?1)
def?
?
y1?e1
P1(y1|x;?1)
=
?
y1?Y1
?e1?y1P1(y1|x;?1)
(e1 ? E1) .
(4)
Here, the notation
?
y1?e1 represents the sum-
mation over sequences consistent with an arc
e1 ? E1, that is, the summation over the set
{y1 ? Y1 | e1 ? y1}. ?P denotes the indicator
function for a predicate P . The input features for P2
on an arc e2 ? E2 are permitted to arbitrarily com-
bine the information of x and the L1?s marginalized
output features h?1, in addition to the local charac-
teristics of the arc at hand e2. In summary, an input
feature for L2 on an arc e2 ? E2 is of the form
f?2,k2,e2,x?
(
h?1(?1)
)
? R (k2 ? K2) , (5)
where K2 is the index set of the input feature types
for L2. To make the optimization procedure feasible,
smoothness condition on any L2?s input feature is
assumed with respect to all the L1?s output features,
that is, ?f?2,k2,e2,x??h??1,k?1,e1?
is always guaranteed to exist for
?k?1, e1, k2, e2. For example, additions and mul-
tiplications between some elements of h?1(?1) can
appear in the definition of L2?s input features. For
given input features f?2,x?
(
h?1(?1)
)
and parameters
??2,k2? ? R (k2 ? K2), the probabilistic model for
L2 is defined as follows;
P2(y2|x;?1,?2)
def? 1Z2(x;?1,?2)
exp
(
?2 ? F?2,y2,x?
(
h?1(?1)
))
(y2 ? Y2) ,
(6)
where F?2,k2,y2,x?
(
h?1(?1)
) def??
e2?y2 f?2,k2,e2,x?
(
h?1 (?1)
)
and Z2 is the par-
tition function of P2, defined by
Z2(x;?1,?2)
def?
?
y2?Y2
exp
(
?2 ? F?2,y2,x?
(
h?1(?1)
))
.
(7)
The definition of P2 (6) reveals one of the most im-
portant points in this paper. P2 is viewed not only
as the function of the ordinary direct parameters ?2
but also as the function of ?1, which represents the
parameters for the L1?s model, through the interme-
diate variables h?1. So optimization procedure on P2
may affect the determination of the values not only
of the direct parameters ?2 but also of the indirect
ones ?1.
If the result of L1 is reduced to the single golden
output y?1, i.e. P1(y1|x) = ?y1=y?1 , the definitions
above boil down to the formulation of the simple 1-
best feed forward architecture.
3 Optimization Algorithm
In this section, we describe optimization procedure
for the model formulated in the previous section.
Let D = {?x?, ?G1, y?1?, ?G2, y?2??m}m=1,2,??? ,M de-
note annotated data for the supervised learning of
the model. Here, ?G1, y?1? is a pair of a DAG and
correctly annotated successful sequence for L1. The
same holds for ?G2, y?2?. For given D, we can define
the conditional log-likelihood function on L1 and L2
respectively, that is,
L1 (?1;D)
def?
?
?x?,y?1??D
log (P1 (y?1|x?;?1)) ?
|?1|
2?12
(8)
631
   	  
   


 
Figure 1: Computation Graph of the Proposed Model
and
L2 (?1,?2;D)
def?
?
?x?,y?2??D
log (P2 (y?2|x?;?1,?2)) ?
|?2|
2?22
.
(9)
Here, ?12, ?22 are the variances of the prior distribu-
tions of the parameters. For the sake of simplicity,
we set the prior distribution as the zero-mean uni-
variance Gaussian. To optimize the both probabilis-
tic models P1 and P2 jointly, we also define the joint
conditional log-likelihood function
L (?1,?2;D)
def? L1 + L2 . (10)
The parameter values to be learned are the ones that
(possibly locally) maximize this objective function.
Note that this objective function is not guaranteed to
be globally convex.
We employ gradient-based parameter optimiza-
tion here. Optimization procedure repeatedly
searches a direction in the parameter space which
is ascendent with respect to the objective function,
and updates the parameter values into that direction
by small advances. Many existing optimization rou-
tines like steepest descent or conjugation gradient do
that job only by giving the objective value and gra-
dients on parameter values to be updated. So, the
optimization problem here boils down to the calcu-
lation of the objective value and gradients on given
parameter values.
Before entering the detailed description of the al-
gorithm for calculating the objective function and
gradients, we note the functional relations among
the objective function and previously defined vari-
ables. The diagram shown in Figure 1 illustrates
the functional relations among the parameters, input
and output feature functions, models, and objective
function. The variables at the head of a directed ar-
row in the figure is directly defined in terms of the
ones at the tail of the same arrow. The value of the
objective function on given parameter values can be
calculated in order of the arrows shown in the di-
agram. On the other hand, the parameter gradients
are calculated step-by-step in reverse order of the ar-
rows. The functional relations illustrated in the Fig-
ure 1 ensure some forms of the chain rule of dif-
ferentiation among the variables. The chain rule is
iteratively used to decompose the calculation of the
gradients into a divide-and-conquer fashion. These
two directions of stepwise computation are analo-
gous to the forward and back propagation for multi-
layer feedforward neural networks, respectively.
Algorithm 1 shows the whole picture of the
gradient-based optimization procedure for our
model. We first describe the flow to calculate the
objective value for a given parameters ?1 and ?2,
which is shown from line 2 through 4 in Algo-
rithm 1. The values of marginalized output features
h??1,x? can be calculated by (3). Because they are the
simple marginals of features, the ordinary forward-
backward algorithm (hereafter, abbreviated as ?F-
B?) on G1 offers an efficient way to calculate their
values. Although nothing definite about the forms
of the input features for L2 is presented in this pa-
per, f?2,x? can be calculated once the values of h??1,x?
have been obtained. Finally, L1, L2 and then L are
easy to calculate because they are no different from
the ordinary log-likelihood computation.
Now we describe the algorithm to calculate the
parameter gradients,
?L
??1
= ?L1??1
+ ?L2??1
, ?L??2
= ?L2??2
. (11)
Line 5 through line 7 in Algorithm 1 describe the
gradient computation. The terms ?L1??1 and
?L2
??2 in(11) become the same forms that appear in the ordi-
nary CRF optimization, i.e., the difference between
the empirical frequencies of the features and the
model expectations of them,
?L1
??1
= E?
[
F?1,y1,x?
]
? EP1
[
F?1,y1,x?
]
? |?1|?12
,
?L2
??2
= E?
[
F?2,y2,x?
]
? EP2
[
F?2,y2,x?
]
? |?2|?22
.
(12)
These calculations are performed by the ordinary F-
B on G1 and G2, respectively. Using the chain rule
of differentiation derived from the functional rela-
tions illustrated in Figure 1, the remaining term ?L2??1
632
Algorithm 1 Gradient-based optimization of the model parameters
Input: ?1, ?2
Output: argmax
??1,?2?
L
1: while ?1 or ?2 changes significantly do
2: calculate Z1 by (2), h?1 by (3) with the F-B on G1, and then L1 by (8)
3: calculate f?2,x? according to their definitions
4: calculate Z2 by (7) with the F-B on G2, and then L2 by (9) and L by (10)
5: calculate ?L1??1 and
?L2
??2 by (12) with the F-B on G1 and G2, respectively
6: calculate ?L?f?1,x? by (16) with the F-B on G2,
?f?1,x?
?h?1 , and them
?L2
?h?1 =
?L
?f?1,x? ?
?f?1,x?
?h?1
7: calculate ?L2??1 by (18) with Algorithm 2
8: ??1,?2? ? update-parameters
(
?1,?2,L, ?L??1 ,
?L
??2
)
9: end while
in (11) can be decomposed as follows;
?L2
??1
= ?L2?f?2,x?
?
?f?2,x?
??1
= ?L2?f?2,x?
?
?f?2,x?
?h?1
? ?h?1??1
.
(13)
Note that Leibniz?s notation here denotes a Jacobian
with the index sets omitted in the numerator and the
denominator, for example,
?f?2,x?
?h?1
def?
{
?f?2,k2,e2,x?
?h?1,k?1,e1?
}
k2?K2,e2?E2,k?1?K?1,e1?E1(14)
And also recall that dot operators here stand for the
inner product with respect to the index sets com-
monly omitted in both operands, for example,
?L2
?f2
? ?f2?h?1
=
?
k2?K2,e2?E2
?L2
?f?2,k2,e2,x?
?
?f?2,k2,e2,x?
?h?1
.
(15)
We describe the manipulation of each factor in
the right side of (13) in turn. Noting ?f?2,k2,e2,x??f?2,k`2,e`2,x? =
?k2=k`2?e2=e`2 , each element of the first factor of (13)
?L2
?f?2,x? can be transformed as follows;
?L2
?f?2,k2,e2,x?
= ??2,k2?
?
?x?,y?2??D
(
?e2?y?2 ? P2(e2|x?;?1,?2)
)
.
(16)
P2(e2|x?;?1,?2), the marginal probability on e2, can
be obtained as a by-product of the F-B for (12).
As described in the previous section, it is assumed
that the values of the second factor ?f?2,x??h?1 is guaran-
teed to exists for any given ?1, and the procedure for
calculating them is fixed in advance. The procedure
for some of concrete features is exemplified in the
previous section.
From the definition of h?1 (3), each element of the
third factor of (13) ?h?1??1 becomes
?h??1,k?1,e1?
???1,k1?
= h?1,k?1,e1?CovP1(y1|x)
[
?e1?y1 , F?1,k1,y1,x?
]
.
(17)
There exists efficient dynamic programming to cal-
culate the covariance value (17) (without goint into
that detail because it is very similar to the one shown
later in this paper), and of course we can run such
dynamic programming for ?k?1 ? K?1, e1 ? E1.
However, the size of the Jacobian ?h?1??1 is equal to
|K?1|?|E1|?|K1|. Since it is too large in many tasks
likely to arise in practice, we should avoid to calcu-
late all the elements of this Jacobian in a straight-
forward way. Instead of such naive computation, if
the values of ?L2?f?2,x? and
?f?2,x?
?h?1 are obtained, then we
can compute ?L2?h?1 =
?L2
?f?2,x? ?
?f?2,x?
?h?1 , and from (13)
633
and (17),
?L2
??1
= ?L2?h?1
? ?h?1??1
= EP1(y1|x)
[
H ??1,y1?F?1,y1,x?
]
? EP1(y1|x)
[
H ??1,y1?
]
EP1(y1|x)
[
F?1,y1,x?
]
,
(18)
where H ??1,y1?
def? ?e1?y1
?L2
?h??1,e1?
? h?1,e1?. In other
words, ?L2???1,k1? becomes the covariance between the
k1-th input feature for L1 and the hypothetical fea-
ture h??1,e1?
def? ?L2?h??1,e1? ? h?1,e1?.
The final problem is to derive an efficient way to
compute the first term of (18). The second term of
(18) can be calculated by the ordinary F-B because
it consists of the marginals of arc features. There are
two derivations of the algorithm for calculating the
first term. We describe briefly the both derivations.
One is a variant of the F-B on the expectation
semi-ring proposed in Li and Eisner (2009). First,
the F-B is generalized to the expectation semi-ring
with respect to the hypothetical feature h??1,e1?, and
by summing up the marginals of the feature vectors
f?1,e1,x? on all the arcs under the distribution of the
semi-ring, then we obtain the expectation of the fea-
ture vector f?1,e1,x? on the semi-ring potential. This
expectation is equal to the first term of (18). 1
Another derivation is to apply the automatic dif-
ferentiation (AD)(Wengert, 1964; Corliss et al,
2002) on the F-B calculating EP1
[
F?1,y1,x?
]
. It
exploits the fact that ???EP ?1
[
F?1,y1,x?
] ???
?=0
is
equal to the first term of (18), where ? ?
R is a dummy parameter, and P ?1(y1|x)
def?
1
Z1 exp
(
?1 ? F?1,y1,x? + ?H ??1,y1?
)
. It is easy
to derive the F-B for calculating the value
EP ?1
[
F?1,y1,x?
] ???
?=0
. AD transforms this F-B into
another algorithm for calculating the differentiation
w.r.t. ? evaluated at the point ? = 0. This trans-
formation is achieved in an automatic manner, by
replacing all appearances of ? in the F-B with a dual
number ? + ?. The dual number is a variant of the
complex number, with a kind of the imaginary unit
? with the property ?2 = 0. Like the usual complex
1For the detailed description, see Li and Eisner (2009) and
its references.
numbers, the arithmetic operations and the exponen-
tial function are generalized to the dual numbers,
and the ordinary F-B is also generalized to the dual
numbers. The imaginary part of the resulting values
is equal to the needed differentiation. 2 Anyway,
these two derivations lead to the same algorithm, and
the resulting algorithm is shown as Algorithm 2.
The final line in the loop of Algorithm 1 can be
implemented by various optimization routines and
line search algorithms.
The time and space complexity to compute the ob-
jective and gradient values for given parameter vec-
tors ?1,?2 is the same as that for that for Bunescu
(2008), up to a constant factor. Because the calcula-
tion of the objective function is essentially the same
as that for Bunescu (2008), and in gradient com-
putation, the time complexity of Algorithm 1 is the
same as that for the ordinary F-B (up to a constant
factor), and the proposed optimization procedure is
only required to store additional scalar values h??1,e1?
on each G1?s arc.
4 Experiment
We examined effectiveness of the method proposed
in this paper on a real task. The task is to annotate
the POS tags and to perform base-phrase chunking
on English sentences.
Base-phrase chunking is a task to classify con-
tinuous subsequences of words into syntactic cat-
egories. This task is performed by annotating a
chunking label on each word (Ramshaw and Mar-
cus, 1995). The types of chunking label consist of
?Begin-Category?, which represents the beginning
of a chunk, ?Inside-Category?, which represents the
inside of a chunk, and ?Other.? Usually, POS la-
beling runs first before base-phrase chunking is per-
formed. Therefore, this task is a typical interesting
case where a sequence labeling depends on the out-
put from other sequence labelers.
The data used for our experiment consist of En-
glish sentences from the Penn Treebank project
(Marcus et al, 1993) consisting of 10948 sentences
and 259104 words. We divided them into two
groups, training data consisting of 8936 sentences
and 211727 words and test data consisting of 2012
2For example, Berz (1992) gives a detailed description of
the reason why the dual number is used for this purpose.
634
Algorithm 2 Forward-backward Algorithm for Calculating Feature Covariances
Input: f?1,x?, ?e1
def? exp
(
?1 ? f?1,e1,x?
)
, h?e1
def? ?L2?h??1,e1? ? h?1,e1?
Output: qk1 = CovP(y1|x)
[
H ??1,y1?, F?1,k1,y1,x?
] (?k1 ? K1
)
1: for ?v1 ? src(G1), ?v1 ? 1, ??v1 ? 1
2: for all v1 ? V1 in a topological order do
3: prev ? {x ? V1 | (x, v1) ? E1}
4: ?v1 ?
?
x?prev
?(x,v1)?x, ??v1 ?
?
x?prev
?(x,v1)
(
h?(x,v1)?x + ?
?
x
)
5: end for
6: Z1 ?
?
x?snk(G1)
?x
7: for ?v1 ? snk(G1), ?v1 ? 1, ??v1 ? 1
8: for all v1 ? V1 in a reverse topological order do
9: next ? {x ? V1 | (v1, x) ? E1}
10: ?v1 ?
?
x?next
?(v1,x)?x, ??v1 ?
?
x?next
?(v1,x)
(
h?(v1,x)?x + ?
?
x
)
11: end for
12: for ?k1 ? K1, qk1 ? 0
13: for all (u1, v1) ? E1 do
14: p ? ?(u1,v1)
(
?u1??v1 + ??u1?v1
)
/Z1
15: for ?k1 ? K1, qk1 ? qk1 + pf?1,k1,e1,x?
16: end for
sentences and 47377 words. The number of the POS
label types is equal to 45. The number of the label
types used in base-phrase chunking is equal to 23.
We compare the proposed method to two exist-
ing sequence labeling methods as baselines. The
POS labeler is the same in all the three methods
used in this experiment. This labeler is a simple
CRF and learned by ordinary optimization proce-
dure. One baseline method is the 1-best pipeline
method. A simple CRF model is learned for the
chunking labeling, on the input sentences and the
most likely POS label sequences predicted by the
already learned POS labeler. We call this method
?CRF + CRF.? The other baseline method has a
CRF model for the chunking labeling, which uses
the marginalized features offered by the POS la-
beler. However, the parameters of the POS labeler
are fixed in the training of the chunking model.
This method corresponds to the method proposed
in Bunescu (2008). We call this baseline ?CRF +
CRF-MF? (?MF? for ?marginalized features?). The
proposed method is the same as ?CRF + CRF-MF?,
except that the both labelers are jointly trained by the
CRF CRF CRF
+ CRF + CRF-MF +CRF-BP
POS labeling 95.6 (95.6) 95.8
Base-phrase 92.1 92.7 93.1
chunking
Table 2: Experimental result (F-measure)
procedure described in Section 3. We call this pro-
posed method ?CRF + CRF-BP? (?BP? for ?back
propagation?).
In ?CRF + CRF-BP,? the objective function for
joint learning (10) is not guaranteed to be convex, so
optimization procedure is sensible to the initial con-
figuration of the model parameters. In this experi-
ment, we set the parameter values learned by ?CRF
+ CRF-MF? as the initial values for the training of
the ?CRF + CRF-BP? method. Feature templates
used in this experiment are listed in Table 1. Al-
though we only described the formalization and op-
timization procedure of the models with arc features,
We use node features in the experiment.
Table 2 shows the result of the methods we men-
635
=== Node feature templates ===
Node is source
Node is sink
Input word on the same time slice
Suffix of input word on the same time slice, n characters (n ? [1, 2, 3])
Initial word character is capitalized?
All word characters are capitalized?
Input word included in the vocabulary of POS T ? (T ? {(All possible POS labels)})
Input word contains numbers?
POS label?
=== Arc feature templates ===
Tail node is source
Head node is sink
Corresponding ordered pair of POS labels?
Table 1: List of feature templates. All node features are combined with the corresponding node label (POS or chunking
label) feature. All arc features are combined with the feature of the corresponding arc label pair. ? features are
instantiated on each time slice in five character window. ? features are not used in POS labeler, and marginalized as
output features for ?CRF + CRF-MF? and ?CRF + CRF-BP.?
tioned. In Table 2, bold numbers indicate significant
improvement over the baseline models with ? =
0.05. From Table 2, the proposed method signifi-
cantly outperforms two baseline methods on chunk-
ing performance. Although the improvement on
POS labeling performance by the proposed method
?CRF + CRF-BP? is not significant, it might show
that optimization procedure provides some form of
backward information propagation in comparison to
?CRF + CRF-MF.?
5 Conclusions
In this paper, we adopt the method to weight features
on an upper sequence labeling stage by the marginal-
ized probabilities estimated by the model on lower
stages. We also point out that the model on an upper
stage is considered to depend on the model on lower
stages indirectly. In addition, we propose optimiza-
tion procedure that enables the joint optimization of
the multiple models on the different level of stages.
We perform an experiment on a real-world task, and
our method significantly outperforms existing meth-
ods.
We examined the effectiveness of the proposed
method only on one task in comparison to just a few
existing methods. In the future, we hope to compare
our method to other competing methods like joint
learning approaches in terms of both accuracy and
computational efficiency, and perform extensive ex-
periments on various tasks.
References
M. Berz. 1992. Automatic differentiation as nonar-
chimedean analysis. In Computer Arithmetic and En-
closure, pages 439?450.
R.C. Bunescu. 2008. Learning with probabilistic fea-
tures for improved pipeline models. In Proceedings of
the 2008 Conference on Empirical Methods in Natural
Language Processing, pages 670?679.
M. Collins and N. Duffy. 2002. New ranking algorithms
for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 263?270. Association for
Computational Linguistics.
G.F. Corliss, C. Faure, and A. Griewank. 2002. Auto-
matic differentiation of algorithms: from simulation to
optimization. Springer Verlag.
J.R. Finkel, C.D. Manning, and A.Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
bayesian inference for linguistic annotation pipelines.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 618?
626.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
636
menting and labeling sequence data. In Proceedings of
the Eighteenth International Conference on Machine
Learning, pages 282?289.
Z. Li and J. Eisner. 2009. First-and second-order ex-
pectation semirings with applications to minimum-risk
training on translation forests. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing: Volume 1-Volume 1, pages 40?
51.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational linguistics,
19(2):330.
L.A. Ramshaw and M.P. Marcus. 1995. Text chunking
using transformation-based learning. In Proceedings
of the Third ACL Workshop on Very Large Corpora,
pages 82?94. Cambridge MA, USA.
S. Sarawagi and W.W. Cohen. 2005. Semi-markov
conditional random fields for information extraction.
Advances in Neural Information Processing Systems,
17:1185?1192.
C. Sutton, A. McCallum, and K. Rohanimanesh. 2007.
Dynamic conditional random fields: Factorized proba-
bilistic models for labeling and segmenting sequence
data. The Journal of Machine Learning Research,
8:693?723.
B. Taskar, C. Guestrin, and D. Koller. 2003. Max-margin
Markov networks. In Advances in Neural Information
Processing Systems 16.
RE Wengert. 1964. A simple automatic derivative
evaluation program. Communications of the ACM,
7(8):464.
637
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 139?144,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Construction of English MWE Dictionary and
its Application to POS Tagging
Yutaro Shigeto, Ai Azuma, Sorami Hisamoto, Shuhei Kondo, Tomoya Kose,
Keisuke Sakaguchi, Akifumi Yoshimoto, Frances Yung, Yuji Matsumoto
Nara Institute Science of Technology (NAIST)
Ikoma, Nara 630-0192 Japan
yutaro-s@is.naist.jp
Abstract
This paper reports our ongoing project for
constructing an English multiword expression
(MWE) dictionary and NLP tools based on
the developed dictionary. We extracted func-
tional MWEs from the English part of Wik-
tionary, annotated the Penn Treebank (PTB)
with MWE information, and conducted POS
tagging experiments. We report how the
MWE annotation is done on PTB and the re-
sults of POS and MWE tagging experiments.
1 Introduction
While there have been a great progress in POS
tagging and parsing of natural language sentences
thanks to the advancement of statistical and corpus-
based methods, there still remains difficulty in sen-
tence processing stemming from syntactic discrep-
ancies. One of such discrepancies is caused by mul-
tiword expressions (MWEs), which are known and
defined as expressions having ?idiosyncratic inter-
pretations that cross word boundaries (or spaces)?
(Sag et al, 2002).
Sag et al (2002) classifies MWEs largely into the
following categories:
? Lexicalized phrases
? fixed expressions: Those having fixed
word order and form (e.g. by and large).
? semi-fixed expressions: Those having
fixed word order with lexical variation
such as inflection, determiner selection,
etc. (e.g. come up with).
? syntactically flexible expressions: Those
having a wide range of syntactic variabil-
ity (e.g. phrasal verbs that take an NP ar-
gument between or following the verb and
the particle).
? Institutionalized phrases
? Phrases that are semantically and syntac-
tically compositional, such as collocations
(e.g. traffic light).
This paper reports our ongoing project for devel-
oping an English MWE dictionary of a broad cov-
erage and MWE-aware natural language processing
tools. The main contributions of this paper are as
follows:
1. Construction of an English MWE dictionary
(mainly consisting of functional expressions)
through extraction from Wiktionary1.
2. Annotation of MWEs in the Penn Treebank
(PTB).
3. Implementation of an MWE-aware POS tagger
and evaluation of its performance.
2 Related work
While there is a variety of MWE researches only a
few of them focus on MWE lexicon construction.
Though some examples, such as French adverb dic-
tionaries (Laporte and Voyatzi, 2008; Laporte et al,
2008), a Dutch MWE dictionary (Gre?goire, 2007)
and a Japanese MWE dictionary (Shudo et al, 2011)
have been constructed, there is no freely available
English MWE dictionary with a broad coverage.
Moreover, MWE-annotated corpora are only
available for a few languages, including French and
1https://en.wiktionary.org
139
Swedish. While the British National Corpus is anno-
tated with MWEs, its coverage is far from complete.
Considering this situation, we started construction
of an English MWE dictionary (with functional ex-
pressions first) and classified their occurrences in
PTB into MWE or literal usage, obtaining MWE-
annotated version of PTB.
The effect of MWE dictionaries have been re-
ported for various NLP tasks. Nivre and Nilsson
(2004) investigated the effect of recognizing MWEs
in syntactic dependency parsing of Swedish. Ko-
rkontzelos and Manandhar (2010) showed perfor-
mance improvement of base phrase chunking by an-
notating compound and proper nouns. Finlayson
and Kulkarni (2011) reported the effect of recogniz-
ing MWEs on word sense disambiguation.
Most of the previous approaches to MWE recog-
nition are based on frequency or collocation mea-
sures of words in large scale corpora. On the other
hand, some previous approaches tried to recognize
new MWEs using an MWE lexicon and MWE-
annotated corpora. Constant and Sigogne (2011)
presented MWE recognition using a Conditional
Random Fields (CRFs)-based tagger with the BIO
schema. Green et al (2011) proposed an MWE
recognition method using Tree Substitution Gram-
mars. Constant et al (2012) compared two phrase
structure analysis methods, one that uses MWE
recognition as preprocessing and the other that uses
a reranking method.
Although MWEs show a variety of flexibilities
in their appearance, most of the linguistic analyses
consider the fixed type of MWEs. For example, the
experiments by Nivre and Nilsson (2004) focus on
fixed expressions that fall into the following cate-
gories:
1. Multiword names
2. Numerical expressions
3. Compound function words
(a) Adverbs
(b) Prepositions
(c) Subordinating conjunctions
(d) Determiners
(e) Pronouns
Multiword names and numerical expressions be-
have as noun phrases and have limited syntactic
functionalities. On the other hand, compound func-
tion words have a variety of functionalities that may
affect language analyses such as POS tagging and
parsing. In this work, we extract compound func-
tional expressions from the English part of Wik-
tionary, and classify their occurrences in PTB into
either literal or MWE usages. We then build a POS
tagger that takes MWEs into account. In implement-
ing this, we use CRFs that can handle a sequence of
tokens as a single item (Kudo et al, 2004). We eval-
uate the performance of the tagger and compare it
with the method that uses the BIO schema for iden-
tifying MWE usages (Constant and Sigogne, 2011).
3 MWEs Extraction from Wiktionary
To construct an English MWE dictionary, we extract
entries from the English part of Wiktionary (as of
July 14, 2012) that include white spaces. We ex-
tract only fixed expressions that are categorized ei-
ther as adverbs, conjunctions, determiners, prepo-
sitions, prepositional phrases or pronouns. We ex-
clude compound nouns and phrasal verbs since the
former are easily recognized by an existing method
such as chunking and the latter need more sophis-
ticated analyzing methods because of their syntac-
tic flexibility. We also exclude multiword adjec-
tives since many of them are semi-fixed and behave
differently from lexical adjective, having predica-
tive usage only. Table 1 summarizes the numbers
of MWE entries in Wiktionary and the numbers of
them that appear at least once in PTB.
4 Annotation of MWEs in PTB
While it is usually not easy to identify the usage of
an MWE as either an MWE or a literal usage, we
initially thought that the phrase structure tree an-
notations in PTB would have enough information
to identify their usages. This assumption is cor-
rect in many cases (Figures 1(a) and 1(b)). The
MWE usage of ?a bit? in Figure 1(a) is analyzed as
?NP-ADV?, suggesting it is used as an adverb, and
the literal usage of ?a bit? in Figure 1(b) is labeled
as ?NP?, suggesting it is used literally. However,
there are a number of examples that are annotated
differently while their usages are the same. For ex-
ample, Figures 1(c), 1(d) and 1(e) all show RB us-
140
Table 1: Number of MWE types in Wiktionary and Penn Treebank
Adverb Conjunction Determiner Preposition Prepositional Phrase Pronoun
Wiktionary 1501 49 15 110 165 83
PTB 468 35 9 77 66 18
Examples after all as wll as a number of according to against the law no one
VP
VB
heat
PRT
up
NP-ADV
DT
a
NN
bit
(a) MWE usage as RB
ADVP
NP
DT
a
NN
bit
PP
IN
of
NP
NN
chromosome
CD
13
(b) Literal usage as NP
ADVP
NP-ADV
DT
a
RB
bit
JJR
smaller
(c) MWE usage as RB
ADVP
NP
DT
a
NN
bit
RBR
better
(d) MWE usage as RB
ADJP-PRD
NP
DT
a
RB
bit
JJR
isolated
(e) MWE usage as RB
Figure 1: Examples of phrase structures annotated to ?a bit?
age of ?a bit? while they are annotated differently 2.
Sometimes, the same structure tree is annotated to
instances of different usages (Figures 1(b) and 1(d)).
Therefore, for eachMWE candidate, we first clus-
ter its occurrences in PTB according to their phrase
tree structures. Some of the clusters clearly indi-
cate MWE usages (such as ?NP-ADV? trees in Fig-
ures 1(a) and 1(c)). In such cases, we regarded all in-
stances as MWE usages and annotated them as such.
For inconsistent or ambiguous cases (such as ?NP?
trees in Figures 1(b), 1(d) and 1(e)), we manually
classify each of them into either MWE or literal us-
age (some MWEs have multiple MWE usages). We
find a number of inconsistent POS annotations on
some internal words of MWEs (e.g. ?bit? in Fig-
ures 1(c) and 1(e) are annotated as RB while they
should be NN). We correct such inconsistent cases
(correction is only done on internal words of MWEs,
selecting the majority POS tags as correct). The total
number of POS tag corrections made on PTB (chap-
ter 00-24) was 1084.
2The POS tags in the trees are: RB(adverb), IN(preposition),
DT(determiner), NN(common noun) ...
5 Experiments of POS tagging and MWE
recognition
5.1 Experiment Setting
We conduct POS tagging experiments on the MWE-
annotated PTB, using sections 0-18 for training and
sections 22-24 for test as usual.
For the experiments, we use four versions of PTB
with the following POS annotations.
(a) Original: PTB with the original POS annota-
tion
(b) Revised: PTB with correction of inconsistent
POS tags
(c) BIO MWE: MWEs are annotated with the BIO
schema
(d) MWE: MWEs are annotated as single words
Concerning the MWE annotation in (c) and (d),
the total number of MWE tokens in PTB is 12131
(9417 in the training chapters, 1396 in the test
chapters, and 1319 for the remaining (development)
chapters).
Each word is annotated with the following in-
141
Figure 2: Example of lattice containing MWE (?about to/RB?) (correct path is marked with bold boxes.)
Table 2: Examples of MWE annotations in four versions
Version Word/POS
(a) Original about/RB to/TO
(b) Revised about/IN to/TO
(c) BIO MWE about/RB-B to/RB-I
(d) MWE about to/RB
formation: coarse-grained POS tag (CPOS), fine-
grained POS tag (FPOS) and surface form. Each
MWE is further annotated with its POS tag, surface
form, its internal words with their POS tags.
Table 2 shows sample annotations of MWE
?about to? in each of the four versions of PTB. In
(a), ?about/RB? is annotated incorrectly, which is
corrected in (b). In (c), ?-B? indicates the beginning
token of an MWE and ?-I? indicates an inside posi-
tion of an MWE. In (d), ?about to? is annotated as
an RB (we omit the POS tags for its internal words,
which are IN and TO).
We use a CRF-based tagger for training and test
on all the four PTB versions. Our CRF can han-
dle ?words with spaces? (e.g. ?about to? as a single
token as well as separated tokens) as shown in Fig-
ure 2. This extension is only relevant to the case of
the (d) MWE version.
Table 3 summarizes the set of feature templates
used in the experiments. In Table 3, ?Head POS?
means the POS tag of the beginning token of an
MWE. In the same way, ?Tail POS? means the POS
tag of the last token of an MWE. For example, for
?a lot of /DT?, its Head POS is DT and its Tail POS
is IN.
We evaluate POS tagging accuracy and MWE
recognition accuracy. In POS evaluation, each to-
ken receives a tag in the cases of (a), (b) and (c), so
the tagging accuracy is straightforwardly calculated.
Table 3: Feature templates used in CRF training
Unigram features
Surface form
FPOS, Surface form
CPOS, Surface form
Bigram features (left context / right context)
Surface form / FPOS, Surface form
FPOS, Surface form / Surface form
Tail POS, Surface form / Head POS, Surface form
Surface form / Head POS
Tail POS / Head POS
Tail POS / Surface form
In the case of (d), since MWEs are analyzed as sin-
gle words, they are expanded into the internal words
with their POS tags and the evaluated on the token
basis.
MWE recognition accuracy is evaluated for the
cases of (c) and (d). For the purpose of comparison,
we employ a simple baseline as well. This baseline
assigns each occurrence of an MWE its most fre-
quent usage in the training part of PTB. Evaluation
of MWE recognition accuracy is shown in precision,
recall and F-measure.
We use the standard set of features based on uni-
gram/bi-gram of words/POS. For our MWE version,
we add the word forms and POS tags of the first and
the last internal words of MWEs as shown in Ta-
ble 3.
5.2 Experimental Results
Table 4 shows the results of POS tagging. A slight
improvement is observed in (b) compared with (a)
because some of inconsistent tags are corrected.
Further improvement is achieved in (d). The exper-
iment on (c) does not show improvement even over
142
Figure 3: Example of errors: ?after all /RB? and ?a /DT bit /JJ.?
Table 4: Per token accuracy (precision)
Version Accuracy
(a) Original 97.54
(b) Revised 97.56
(c) BIO MWE 97.32
(d) split MWE 97.62
Table 5: Recognition performance of MWEs
Precision Recall F-measure
Baseline 78.79 80.26 79.51
(c) BIO 92.81 90.90 90.18
(d) MWE 95.75 97.16 96.45
(a). The reason may attribute to the data sparseness
caused by the increased size of POS tags.
Table 5 shows the results of MWE recognition.
Our MWE-aware CRF model (d) shows the best re-
sults. While the BIO model (c) significantly outper-
forms the baseline, it gives significantly lower re-
sults than our model.
We investigated errors in (d) and categorized them
into three types.
? False Positive: System finds an MWE, while it
is actually literal.
? False Negative: System misses to identify an
MWE.
? Misrecognition: System finds an MWE
wrongly (correct answer is another MWE).
Table 6 shows number of recognition errors of
MWEs.
An example of the False Positive is ?a bit /RB? in
Figure 3, which actually is a literal usage and should
be tagged as ?a /DT, bit /NN?.
An example of the False Negative is ?in black and
white /RB?, which is not recognized as an MWE.
One reason of this type of errors is low or zero fre-
quency of such MWEs in training data. ?after all
/RB? (in Figure 3) is another False Negative exam-
ple.
Table 6: Recognition error of MWEs
Error types # of errors
False Positives 33
False Negatives 19
Misrecognition 17
One example of Misrecognition errors stems from
ambiguous MWEs. For example, while ?how much?
only has MWE usages as RB, there are two RB
usages of ?how much? that have different POS
tag sequences for the internal words. Other ex-
amples of Misrecognition are due to zero or low
frequency MWEs, whose substrings also matches
shorter MWEs: ?quite/RB, a few/PRP? while cor-
rect analysis is ?quite a few/RB?, and ?the hell /RB,
out of /IN? while the correct analysis is ?the hell out
of /RB?.
6 Conclusion and Future work
This paper presented our ongoing project for con-
struction of an English MWE dictionary, and its ap-
plication to MWE-aware POS tagging. The exper-
imental results show that the MWE-aware tagger
achieved better performance on POS tagging and
MWE recognition. Although our current MWE dic-
tionary only covers fixed types of functional MWEs,
this dictionary and MWE annotation information on
PTB will be made publicly available.
We plan to handle a wider range of MWEs such as
phrasal verbs and other semi-fixed and syntactically
flexible MWEs, and to develop a POS tagger and a
syntactic parser on top of them.
References
Matthieu Constant and Anthony Sigogne. 2011. MWU-
Aware Part-of-Speech Tagging with a CRF Model and
Lexical Resources. In Proceedings of the Workshop on
Multiword Expressions: from Parsing and Generation
to the Real World, MWE ?11, pages 49?56.
143
Matthieu Constant, Anthony Sigogne, and Patrick Wa-
trin. 2012. Discriminative Strategies to Integrate Mul-
tiword Expression Recognition and Parsing. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics, ACL ?12, pages 204?
212.
Mark Alan Finlayson and Nidhi Kulkarni. 2011. De-
tecting Multi-Word Expressions improves Word Sense
Disambiguation. In Proceedings of the Workshop on
Multiword Expressions: from Parsing and Generation
to the Real World, MWE ?11, pages 20?24.
Spence Green, Marie-Catherine deMarneffe, John Bauer,
and Christopher D Manning. 2011. Multiword Ex-
pression Identification with Tree Substitution Gram-
mars: A Parsing tour de force with French. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ?11, pages
725?735.
Nicole Gre?goire. 2007. Design and Implementation of
a Lexicon of Dutch Multiword Expressions. In Pro-
ceedings of the Workshop on a Broader Perspective on
Multiword Expressions, MWE ?07, pages 17?24.
Ioannis Korkontzelos and Suresh Manandhar. 2010. Can
Recognising Multiword Expressions Improve Shallow
Parsing? In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 636?644.
Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004. Applying conditional random fields to japanese
morphological analysis. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?04, pages 230?237.
Eric Laporte and Stavroula Voyatzi. 2008. An Electronic
Dictionary of French Multiword Adverbs. In Lan-
guage Resources and Evaluation Conference. Work-
shop Towards a Shared Task for Multiword Expres-
sions, MWE ?08, pages 31?34.
Eric Laporte, Takuya Nakamura, and Stavroula Voy-
atzi. 2008. A French Corpus Annotated for Mul-
tiword Nouns. In Proceedings of the Language Re-
sources and Evaluation Conference. Workshop To-
wards a Shared Task on Multiword Expressions, MWE
?08, pages 27?30.
Joakim Nivre and Jens Nilsson. 2004. Multiword Units
in Syntactic Parsing. In Workshop on Methodologies
and Evaluation of Multiword Units in Real-World Ap-
plications, MEMURA ?04, pages 39?46.
Ivan A Sag, Timothy Baldwin, Francis Bond, Ann A
Copestake, and Dan Flickinger. 2002. Multiword Ex-
pressions: A Pain in the Neck for NLP. In Proceed-
ings of the Third International Conference on Com-
putational Linguistics and Intelligent Text Processing,
CICLing ?02, pages 1?15.
Kosho Shudo, Akira Kurahone, and Toshifumi Tanabe.
2011. A Comprehensive Dictionary of Multiword Ex-
pressions. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, HLT ?11, pages 161?
170.
144
