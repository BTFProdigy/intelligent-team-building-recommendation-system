The Parallel Grammar Project
Miriam Butt
Cent. for Computational Linguistics
UMIST
Manchester M60 1QD GB
mutt@csli.stanford.edu
Helge Dyvik
Dept. of Linguistics
University of Bergen
N5007 Bergen NORWAY
helge.dyvik@lili.uib.no
Tracy Holloway King
Palo Alto Research Center
Palo Alto, CA 94304 USA
thking@parc.com
Hiroshi Masuichi
Corporate Research Center
Fuji Xerox Co., Ltd.
Kanagawa 259-0157, JAPAN
hiroshi.masuichi@fujixerox.co.jp
Christian Rohrer
IMS Universita?t Stuttgart
D-70174 Stuttgart GERMANY
rohrer@ims.uni-stuttgart.de
Abstract
We report on the Parallel Grammar (ParGram)
project which uses the XLE parser and grammar
development platform for six languages: English,
French, German, Japanese, Norwegian, and Urdu.1
1 Introduction
Large-scale grammar development platforms are ex-
pensive and time consuming to produce. As such, a
desideratum for the platforms is a broad utilization
scope. A grammar development platform should be
able to be used to write grammars for a wide variety
of languages and a broad range of purposes. In this
paper, we report on the Parallel Grammar (ParGram)
project (Butt et al, 1999) which uses the XLE parser
and grammar development platform (Maxwell and
Kaplan, 1993) for six languages: English, French,
German, Japanese, Norwegian, and Urdu. All of
the grammars use the Lexical-Functional Gram-
mar (LFG) formalism which produces c(onstituent)-
structures (trees) and f(unctional)-structures (AVMs)
as the syntactic analysis.
LFG assumes a version of Chomsky?s Universal
Grammar hypothesis, namely that all languages are
structured by similar underlying principles. Within
LFG, f-structures are meant to encode a language
universal level of analysis, allowing for cross-
linguistic parallelism at this level of abstraction. Al-
though the construction of c-structures is governed
1We would like to thank Emily Bender, Mary Dalrymple,
and Ron Kaplan for help with this paper. In addition, we would
like to acknowledge the other grammar writers in the Par-
Gram project, both current: Stefanie Dipper, Jean-Philippe Mar-
cotte, Tomoko Ohkuma, and Victoria Rose?n; and past: Caroline
Brun, Christian Fortmann, Anette Frank, Jonas Kuhn, Veronica
Lux, Yukiko Morimoto, Mar??a-Eugenia Nin?o, and Fre?de?rique
Segond.
by general wellformedness principles, this level of
analysis encodes language particular differences in
linear word order, surface morphological vs. syntac-
tic structures, and constituency.
The ParGram project aims to test the LFG formal-
ism for its universality and coverage limitations and
to see how far parallelism can be maintained across
languages. Where possible, the analyses produced
by the grammars for similar constructions in each
language are parallel. This has the computational
advantage that the grammars can be used in simi-
lar applications and that machine translation (Frank,
1999) can be simplified.
The results of the project to date are encouraging.
Despite differences between the languages involved
and the aims and backgrounds of the project groups,
the ParGram grammars achieve a high level of paral-
lelism. This parallelism applies to the syntactic anal-
yses produced, as well as to grammar development
itself: the sharing of templates and feature decla-
rations, the utilization of common techniques, and
the transfer of knowledge and technology from one
grammar to another. The ability to bundle grammar
writing techniques, such as templates, into transfer-
able technology means that new grammars can be
bootstrapped in a relatively short amount of time.
There are a number of other large-scale gram-
mar projects in existence which we mention briefly
here. The LS-GRAM project (Schmidt et al, 1996),
funded by the EU-Commission under LRE (Lin-
guistic Research and Engineering), was concerned
with the development of grammatical resources for
nine European languages: Danish, Dutch, English,
French, German, Greek, Italian, Portuguese, and
Spanish. The project started in January 1994 and
ended in July 1996. Development of grammatical
resources was carried out in the framework of the
Advanced Language Engineering Platform (ALEP).
The coverage of the grammars implemented in LS-
GRAM was, however, much smaller than the cov-
erage of the English (Riezler et al, 2002) or Ger-
man grammar in ParGram. An effort which is closer
in spirit to ParGram is the implemention of gram-
mar development platforms for HPSG. In the Verb-
mobil project (Wahlster, 2000), HPSG grammars for
English, German, and Japanese were developed on
two platforms: LKB (Copestake, 2002) and PAGE.
The PAGE system, developed and maintained in the
Language Technology Lab of the German National
Research Center on Artificial Intelligence DFKI
GmbH, is an advanced NLP core engine that facili-
tates the development of grammatical and lexical re-
sources, building on typed feature logics. To evalu-
ate the HPSG platforms and to compare their mer-
its with those of XLE and the ParGram projects, one
would have to organize a special workshop, partic-
ularly as the HPSG grammars in Verbmobil were
written for spoken language, characterized by short
utterances, whereas the LFG grammars were devel-
oped for parsing technical manuals and/or newspa-
per texts. There are some indications that the Ger-
man and English grammars in ParGram exceed the
HPSG grammars in coverage (see (Crysmann et al,
2002) on the German HPSG grammar).
This paper is organized as follows. We first pro-
vide a history of the project. Then, we discuss how
parallelism is maintained in the project. Finally, we
provide a summary and discussion.
2 Project History
The ParGram project began in 1994 with three lan-
guages: English, French, and German. The gram-
mar writers worked closely together to solidify the
grammatical analyses and conventions. In addition,
as XLE was still in development, its abilities grew
as the size of the grammars and their needs grew.
After the initial stage of the project, more lan-
guages were added. Because Japanese is typolog-
ically very different from the initial three Euro-
pean languages of the project, it represented a chal-
lenging case. Despite this typological challenge, the
Japanese grammar has achieved broad coverage and
high performance within a year and a half. The
South Asian language Urdu also provides a widely
spoken, typologically distinct language. Although it
is of Indo-European origin, it shares many character-
istics with Japanese such as verb-finality, relatively
free word order, complex predicates, and the abil-
ity to drop any argument (rampant pro-drop). Nor-
wegian assumes a typological middle position be-
tween German and English, sharing different prop-
erties with each of them. Both the Urdu and the Nor-
wegian grammars are still relatively small.
Each grammar project has different goals, and
each site employs grammar writers with different
backgrounds and skills. The English, German, and
Japanese projects have pursued the goal of hav-
ing broad coverage, industrial grammars. The Nor-
wegian and Urdu grammars are smaller scale but
are experimenting with incorporating different kinds
of information into the grammar. The Norwegian
grammar includes a semantic projection; their anal-
yses produce not only c- and f-structures, but also
semantic structures. The Urdu grammar has imple-
mented a level of argument structure and is test-
ing various theoretical linguistic ideas. However,
even when the grammars are used for different pur-
poses and have different additional features, they
have maintained their basic parallelism in analysis
and have profited from the shared grammar writing
techniques and technology.
Table (1) shows the size of the grammars. The first
figure is the number of left-hand side categories in
phrase-structure rules which compile into a collec-
tion of finite-state machines with the listed number
of states and arcs.
(1)
Language Rules States Arcs
German 444 4883 15870
English 310 4935 13268
French 132 1116 2674
Japanese 50 333 1193
Norwegian 46 255 798
Urdu 25 106 169
3 Parallelism
Maintaining parallelism in grammars being devel-
oped at different sites on typologically distinct lan-
guages by grammar writers from different linguis-
tic traditions has proven successful. At project meet-
ings held twice a year, analyses of sample sentences
are compared and any differences are discussed; the
goal is to determine whether the differences are jus-
tified or whether the analyses should be changed
to maintain parallelism. In addition, all of the f-
structure features and their values are compared; this
not only ensures that trivial differences in naming
conventions do not arise, but also gives an overview
of the constructions each language covers and how
they are analyzed. All changes are implemented be-
fore the next project meeting. Each meeting also in-
volves discussion of constructions whose analysis
has not yet been settled on, e.g., the analysis of parti-
tives or proper names. If an analysis is agreed upon,
all the grammars implement it; if only a tentative
analysis is found, one grammar implements it and
reports on its success. For extremely complicated or
fundamental issues, e.g., how to represent predicate
alternations, subcommittees examine the issue and
report on it at the next meeting. The discussion of
such issues may be reopened at successive meetings
until a concensus is reached.
Even within a given linguistic formalism, LFG for
ParGram, there is usually more than one way to an-
alyze a construction. Moreover, the same theoreti-
cal analysis may have different possible implemen-
tations in XLE. These solutions often differ in effi-
ciency or conceptual simplicity and one of the tasks
within the ParGram project is to make design deci-
sions which favor one theoretical analysis and con-
comitant implementation over another.
3.1 Parallel Analyses
Whenever possible, the ParGram grammars choose
the same analysis and the same technical solution
for equivalent constructions. This was done, for
example, with imperatives. Imperatives are always
assigned a null pronominal subject within the f-
structure and a feature indicating that they are im-
peratives, as in (2).
(2) a. Jump! Saute! (French)
Spring! (German) Tobe! (Japanese)
Hopp! (Norwegian) kuudoo! (Urdu)
b. PRED jump SUBJ
SUBJ PRED pro
STMT-TYPE imp
Another example of this type comes from the
analysis of specifiers. Specifiers include many dif-
ferent types of information and hence can be ana-
lyzed in a number of ways. In the ParGram analysis,
the c-structure analysis is left relatively free accord-
ing to language particular needs and slightly vary-
ing theoretical assumptions. For instance, the Nor-
wegian grammar, unlike the other grammars, im-
plements the principles in (Bresnan, 2001) concern-
ing the relationship between an X -based c-structure
and the f-structure. This allows Norwegian speci-
fiers to be analyzed as functional heads of DPs etc.,
whereas they are constituents of NPs in the other
grammars. However, at the level of f-structure, this
information is part of a complex SPEC feature in
all the grammars. Thus parallelism is maintained
at the level of f-structure even across different the-
oretical preferences. An example is shown in (3)
for Norwegian and English in which the SPEC con-
sists of a QUANT(ifier) and a POSS(essive) (SPEC
can also contain information about DETerminers and
DEMONstratives).
(3) a. alle mine hester (Norwegian)
all my horses
?all my horses?
b. PRED horse
SPEC
QUANT PRED all
POSS
PRED pro
PERS 1
NUM sg
Interrogatives provide an interesting example be-
cause they differ significantly in the c-structures of
the languages, but have the same basic f-structure.
This contrast can be seen between the German ex-
ample in (4) and the Urdu one in (5). In German,
the interrogative word is in first position with the
finite verb second; English and Norwegian pattern
like German. In Urdu the verb is usually in final po-
sition, but the interrogative can appear in a number
of positions, including following the verb (5c).
(4) Was hat John Maria gegeben? (German)
what has John Maria give.PerfP
?What did John give to Mary??
(5) a. jon=nee marii=koo kyaa diiyaa? (Urdu)
John=Erg Mary=Dat what gave
?What did John give to Mary?
b. jon=nee kyaa marii=koo diiyaa?
c. jon=nee marii=ko diiyaa kyaa?
Despite these differences in word order and hence in
c-structure, the f-structures are parallel, with the in-
terrogative being in a FOCUS-INT and the sentence
having an interrogative STMT-TYPE, as in (6).
(6) PRED give SUBJ,OBJ,OBL
FOCUS-INT
PRED pro
PRON-TYPE int
SUBJ PRED John
OBJ [ ]
OBL PRED Mary
STMT-TYPE int
In the project grammars, many basic construc-
tions are of this type. However, as we will see in
the next section, there are times when parallelism is
not possible and not desirable. Even in these cases,
though, the grammars which can be parallel are;
so, three of the languages might have one analysis,
while three have another.
3.2 Justified Differences
Parallelism is not maintained at the cost of misrepre-
senting the language. This is reflected by the fact that
the c-structures are not parallel because word order
varies widely from language to language, although
there are naming conventions for the nodes. Instead,
the bulk of the parallelism is in the f-structure. How-
ever, even in the f-structure, situations arise in which
what seems to be the same construction in different
languages do not have the same analysis. An exam-
ple of this is predicate adjectives, as in (7).
(7) a. It is red.
b. Sore wa akai. (Japanese)
it TOP red
?It is red.?
In English, the copular verb is considered the syn-
tactic head of the clause, with the pronoun being the
subject and the predicate adjective being an XCOMP.
However, in Japanese, the adjective is the main pred-
icate, with the pronoun being the subject. As such,
these receive the non-parallel analyses seen in (8a)
for Japanese and (8b) for English.
(8) a. PRED red SUBJ
SUBJ PRED pro
b. PRED be XCOMP SUBJ
SUBJ PRED pro
XCOMP
PRED red SUBJ
SUBJ [ ]
Another situation that arises is when a feature
or construction is syntactically encoded in one lan-
guage, but not another. In such cases, the informa-
tion is only encoded in the languages that need it.
The equivalence captured by parallel analyses is not,
for example, translational equivalence. Rather, par-
allelism involves equivalence with respect to gram-
matical properties, e.g. construction types. One con-
sequence of this is that a typologically consistent
use of grammatical terms, embodied in the feature
names, is enforced. For example, even though there
is a tradition for referring to the distinction between
the pronouns he and she as a gender distinction in
English, this is a different distinction from the one
called gender in languages like German, French,
Urdu, and Norwegian, where gender refers to nom-
inal agreement classes. Parallelism leads to the sit-
uation where the feature GEND occurs in German,
French, Urdu, and Norwegian, but not in English
and Japanese. That is, parallelism does not mean
finding the same features in all languages, but rather
using the same features in the same way in all lan-
guages, to the extent that they are justified there. A
French example of grammatical gender is shown in
(9); note that determiner, adjective, and participle
agreement is dependent on the gender of the noun.
The f-structure for the nouns crayon and plume are
as in (10) with an overt GEND feature.
(9) a. Le petit crayon est casse?. (French)
the-M little-M pencil-M is broken-M.
?The little pencil is broken.?
b. La petite plume est casse?e. (French)
the-F little-F pen-F is broken-F.
?The little pen is broken.?
(10)
PRED crayon
GEND masc
PERS 3
PRED plume
GEND fem
PERS 3
F-structures for the equivalent words in English and
Japanese will not have a GEND feature.
A similar example comes from Japanese dis-
course particles. It is well-known that Japanese has
syntactic encodings for information such as honori-
fication. The verb in the Japanese sentence (11a)
encodes information that the subject is respected,
while the verb in (11b) shows politeness from the
writer (speaker) to the reader (hearer) of the sen-
tence. The f-structures for the verbs in (11) are as in
(12) with RESPECT and POLITE features within the
ADDRESS feature.
(11) a. sensei ga hon wo oyomininaru.
teacher Nom book Acc read-Respect
?The teacher read the book.? (Japanese)
b. seito ga hon wo yomimasu.
student Nom book Acc read-Polite
?The student reads the book.? (Japanese)
(12) a. PRED yomu SUBJ,OBJ
ADDRESS RESPECT +
b. PRED yomu SUBJ,OBJ
ADDRESS POLITE +
A final example comes from English progres-
sives, as in (13). In order to distinguish these two
forms, the English grammar uses a PROG feature
within the tense/aspect system. (13b) shows the f-
structure for (13a.ii).
(13) a. John hit Bill. i. He cried.
ii. He was crying.
b. PRED cry SUBJ
SUBJ PRED pro
TNS-ASP
TENSE past
PROG +
However, this distinction is not found in the other
languages. For example, (14a) is used to express
both (13a.i) and (13a.ii) in German.
(14) a. Er weinte. (German)
he cried
?He cried.?
b. PRED weinen SUBJ
SUBJ PRED pro
TNS-ASP TENSE past
As seen in (14b), the German f-structure is left un-
derspecified for PROG because there is no syntactic
reflex of it. If such a feature were posited, rampant
ambiguity would be introduced for all past tense
forms in German. Instead, the semantics will deter-
mine whether such forms are progressive.
Thus, there are a number of situations where hav-
ing parallel analyses would result in an incorrect
analysis for one of the languages.
3.3 One Language Shows the Way
Another type of situation arises when one language
provides evidence for a certain feature space or type
of analysis that is neither explicitly mirrored nor
explicitly contradicted by another language. In the-
oretical linguistics, it is commonly acknowledged
that what one language codes overtly may be harder
to detect for another language. This situation has
arisen in the ParGram project. Case features fall un-
der this topic. German, Japanese, and Urdu mark
NPs with overt case morphology. In comparison,
English, French, and Norwegian make relatively lit-
tle use of case except as part of the pronominal sys-
tem. Nevertheless, the f-structure analyses for all the
languages contain a case feature in the specification
of noun phrases.
This ?overspecification? of information expresses
deeper linguistic generalizations and keeps the f-
structural analyses as parallel as possible. In addi-
tion, the features can be put to use for the isolated
phenomena in which they do play a role. For exam-
ple, English does not mark animacy grammatically
in most situations. However, providing a ANIM +
feature to known animates, such as people?s names
and pronouns, allows the grammar to encode infor-
mation that is relevant for interpretation. Consider
the relative pronoun who in (15).
(15) a. the girl[ANIM +] who[ANIM +] left
b. the box[ANIM +] who[ANIM +] left
The relative pronoun has a ANIM + feature that is as-
signed to the noun it modifies by the relative clause
rules. As such, a noun modified by a relative clause
headed by who is interpreted as animate. In the case
of canonical inanimates, as in (15b), this will result
in a pragmatically odd interpretation, which is en-
coded in the f-structure.
Teasing apart these different phenomena crosslin-
guistically poses a challenge that the ParGram mem-
bers are continually engaged in. As such, we have
developed several methods to help maintain paral-
lelism.
3.4 Mechanics of Maintaining Parallelism
The parallelism among the grammars is maintained
in a number of ways. Most of the work is done dur-
ing two week-long project meetings held each year.
Three main activities occur during these meetings:
comparison of sample f-structures, comparison of
features and their values, and discussions of new or
problematic constructions.
A month before each meeting, the host site
chooses around fifteen sentences whose analysis is
to be compared at the meeting. These can be a ran-
dom selection or be thematic, e.g., all dealing with
predicatives or with interrogatives. The sentences
are then parsed by each grammar and the output is
compared. For the more recent grammars, this may
mean adding the relevant rules to the grammars, re-
sulting in growth of the grammar; for the older gram-
mars, this may mean updating a construction that has
not been examined in many years. Another approach
that was taken at the beginning of the project was to
have a common corpus of about 1,000 sentences that
all of the grammars were to parse. For the English,
French, and German grammars, this was an aligned
tractor manual. The corpus sentences were used for
the initial f-structure comparisons. Having a com-
mon corpus ensured that the grammars would have
roughly the same coverage. For example, they all
parsed declarative and imperative sentences. How-
ever, the nature of the corpus can leave major gaps
in coverage; in this case, the manual contained no in-
terrogatives.
The XLE platform requires that a grammar de-
clare all the features it uses and their possible val-
ues. Part of the Urdu feature table is shown in (16)
(the notation has been simplified for expository pur-
poses). As seen in (16) for QUANT, attributes which
take other attributes as their values must also be de-
clared. An example of such a feature was seen in
(3b) for SPEC which takes QUANT and POSS fea-
tures, among others, as its values.
(16) PRON-TYPE: pers poss null .
PROPER: date location name title .
PSEM: locational directional .
PTYPE: sem nosem .
QUANT: PRED QUANT-TYPE
QUANT-FORM .
The feature declarations of all of the languages are
compared feature by feature to ensure parallelism.
The most obvious use of this is to ensure that the
grammars encode the same features in the same way.
For example, at a basic level, one feature declaration
might have specified GEN for gender while the oth-
ers had chosen the name GEND; this divergence in
naming is regularized. More interesting cases arise
when one language uses a feature and another does
not for analyzing the same phenomena. When this is
noticed via the feature-table comparison, it is deter-
mined why one grammar needs the feature and the
other does not, and thus it may be possible to elim-
inate the feature in one grammar or to add it to an-
other.
On a deeper level, the feature comparison is use-
ful for conducting a survey of what constructions
each grammar has and how they are implemented.
For example, if a language does not have an ADE-
GREE (adjective degree) feature, the question will
arise as to whether the grammar analyzes compar-
ative and superlative adjectives. If they do not, then
they should be added and should use the ADEGREE
feature; if they do, then the question arises as to why
they do not have this feature as part of their analysis.
Finally, there is the discussion of problematic
constructions. These may be constructions that al-
ready have analyses which had been agreed upon in
the past but which are not working properly now that
more data has been considered. More frequently,
they are new constructions that one of the grammars
is considering adding. Possible analyses for the con-
struction are discussed and then one of the gram-
mars will incorporate the analysis to see whether it
works. If the analysis works, then the other gram-
mars will incorporate the analysis. Constructions
that have been discussed in past ParGram meet-
ings include predicative adjectives, quantifiers, par-
titives, and clefts. Even if not all of the languages
have the construction in question, as was the case
with clefts, the grammar writers for that language
may have interesting ideas on how to analyze it.
These group discussions have proven particularly
useful in extending grammar coverage in a parallel
fashion.
Once a consensus is reached, it is the responsi-
bility of each grammar to make sure that its anal-
yses match the new standard. As such, after each
meeting, the grammar writers will rename features,
change analyses, and implement new constructions
into their grammars. Most of the basic work has now
been accomplished. However, as the grammars ex-
pand coverage, more constructions need to be inte-
grated into the grammars, and these constructions
tend to be ones for which there is no standard analy-
sis in the linguistic literature; so, differences can eas-
ily arise in these areas.
4 Conclusion
The experiences of the ParGram grammar writers
has shown that the parallelism of analysis and imple-
mentation in the ParGram project aids further gram-
mar development efforts. Many of the basic deci-
sions about analyses and formalism have already
been made in the project. Thus, the grammar writer
for a new language can use existing technology to
bootstrap a grammar for the new language and can
parse equivalent constructions in the existing lan-
guages to see how to analyze a construction. This
allows the grammar writer to focus on more diffi-
cult constructions not yet encountered in the existing
grammars.
Consider first the Japanese grammar which was
started in the beginning of 2001. At the initial stage,
the work of grammar development involved imple-
menting the basic constructions already analyzed in
the other grammars. It was found that the grammar
writing techniques and guidelines to maintain par-
allelism shared in the ParGram project could be ef-
ficiently applied to the Japanese grammar. During
the next stage, LFG rules needed for grammatical is-
sues specific to Japanese have been gradually incor-
porated, and at the same time, the biannual ParGram
meetings have helped significantly to keep the gram-
mars parallel. Given this system, in a year and a half,
using two grammar writers, the Japanese grammar
has attained coverage of 99% for 500 sentences of a
copier manual and 95% for 10,000 sentences of an
eCRM (Voice-of-Customer) corpus.
Next consider the Norwegian grammar which
joined the ParGram group in 1999 and also empha-
sized slightly different goals from the other groups.
Rather than prioritizing large textual coverage from
the outset, the Norwegian group gave priority to the
development of a core grammar covering all major
construction types in a principled way based on the
proposals in (Bresnan, 2001) and the inclusion of a
semantic projection in addition to the f-structure. In
addition, time was spent on improving existing lexi-
cal resources ( 80,000 lemmas) and adapting them
to the XLE format. Roughly two man-years has been
spent on the grammar itself. The ParGram cooper-
ation on parallelism has ensured that the derived f-
structures are interesting in a multilingual context,
and the grammar will now serve as a basis for gram-
mar development in other closely related Scandina-
vian languages.
Thus, the ParGram project has shown that it is
possible to use a single grammar development plat-
form and a unified methodology of grammar writing
to develop large-scale grammars for typologically
different languages. The grammars? analyses show a
large degree of parallelism, despite being developed
at different sites. This is achieved by intensive meet-
ings twice a year. The parallelism can be exploited in
applications using the grammars: the fewer the dif-
ferences, the simpler a multilingual application can
be (see (Frank, 1999) on a machine-translation pro-
totype using ParGram).
References
Joan Bresnan. 2001. Lexical-Functional Syntax.
Blackwell.
Miriam Butt, Tracy Holloway King, Mar??a-Eugenia
Nin?o, and Fre?de?rique Segond. 1999. A Grammar
Writer?s Cookbook. CSLI Publications.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI Publications.
Berthold Crysmann, Anette Frank, Bernd Keifer, St.
Mu?ller, Gu?nter Neumann, Jakub Piskorski, Ulrich
Scha?fer, Melanie Siegel, Hans Uszkoreit, Feiyu
Xu, Markus Becker, and Hans-Ulrich Krieger.
2002. An integrated architecture for shallow and
deep parsing. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics, University of Pennsylvania.
Anette Frank. 1999. From parallel grammar devel-
opment towards machine translation. In Proceed-
ings of MT Summit VII, pages 134?142.
John T. Maxwell, III and Ron Kaplan. 1993. The
interface between phrasal and functional con-
straints. Computational Lingusitics, 19:571?589.
Stefan Riezler, Tracy Holloway King, Ronald Ka-
plan, Dick Crouch, John T. Maxwell, III, and
Mark Johnson. 2002. Parsing the wall street jour-
nal using a lexical-functional grammar and dis-
criminative estimation techniques. In Proceed-
ings of the Annual Meeting of the Association for
Computational Linguistics, University of Penn-
sylvania.
Paul Schmidt, Sibylle Rieder, Axel Theofilidis, and
Thierry Declerck. 1996. Lean formalisms, lin-
guistic theory, and applications: Grammar devel-
opment in alep. In Proceedings of COLING.
Wolfgang Wahlster, editor. 2000. Verbmobil:
Foundations of Speech-to-Speech Translation.
Springer.
Coling 2010: Poster Volume, pages 1426?1434,
Beijing, August 2010
Cross-Lingual Induction for Deep Broad-Coverage Syntax: A Case
Study on German Participles
Sina Zarrie? Aoife Cahill Jonas Kuhn Christian Rohrer
Institut fu?r Maschinelle Sprachverarbeitung (IMS), University of Stuttgart
{zarriesa,cahillae,jonas.kuhn,rohrer}@ims.uni-stuttgart.de
Abstract
This paper is a case study on cross-lingual
induction of lexical resources for deep,
broad-coverage syntactic analysis of Ger-
man. We use a parallel corpus to in-
duce a classifier for German participles
which can predict their syntactic category.
By means of this classifier, we induce a
resource of adverbial participles from a
huge monolingual corpus of German. We
integrate the resource into a German LFG
grammar and show that it improves pars-
ing coverage while maintaining accuracy.
1 Introduction
Parallel corpora are currently exploited in a wide
range of induction scenarios, including projection
of morphologic (Yarowsky et al, 2001), syntactic
(Hwa et al, 2005) and semantic (Pado? and Lap-
ata, 2009) resources. In this paper, we use cross-
lingual data to learn to predict whether a lexi-
cal item belongs to a specific syntactic category
that cannot easily be learned from monolingual re-
sources. In an application test scenario, we show
that this prediction method can be used to obtain
a lexical resource that improves deep, grammar-
based parsing.
The general idea of cross-lingual induction is
that linguistic annotations or structures, which are
not available or explicit in a given language, can
be inferred from another language where these an-
notations or structures are explicit or easy to ob-
tain. Thus, this technique is very attractive for
cheap acquisition of broad-coverage resources, as
is proven by the approaches cited above. More-
over, this induction process can be attractive for
the induction of deep (and perhaps specific) lin-
guistic knowledge that is hard to obtain in a mono-
lingual context. However, this latter perspective
has been less prominent in the NLP community
so far.
This paper investigates a cross-lingual induc-
tion method based on an exemplary problem aris-
ing in the deep syntactic analysis of German. This
showcase is the syntactic flexibility of German
participles, being morphologically ambiguous be-
tween verbal, adjectival and adverbial readings,
and it is instructive for several reasons: first, the
phenomenon is a notorious problem for linguistic
analysis and annotation of German, such that stan-
dard German resources do not represent the under-
lying analysis. Second, in Zarrie? et al (2010),
we showed that integrating the phenomenon of
adverbial participles in a naive way into a broad-
coverage grammar of German leads to significant
parsing problems, due to spurious ambiguities.
Third, it is completely straightforward to detect
adverbial participles in cross-lingual data since in
other languages, e.g. English or French, adverbs
are often morphologically marked.
In this paper, we use instances of adverbially
translated participles in a parallel corpus to boot-
strap a classifier that is able to identify an ad-
verbially used participle based on its monolingual
syntactic context. In contrast to what is commonly
assumed, we show that it is possible to detect ad-
verbial participles using only a relatively narrow
context window. This classifier enables us to iden-
tify an occurence of an adverbial participle inde-
pendently of its translation in a parallel corpus,
going far beyond the induction methodology in
Zarrie? et al (2010). By means of the participle
classifier, we can extract new types of adverbial
participles from a larger corpus of German news-
paper text and substantially augment the size of
the resource extracted only on Europarl data. Fi-
nally, we integrate this new resource into the Ger-
man LFG grammar and show that it improves cov-
erage without negatively affecting performance.
1426
The paper is structured as follows: in Sec-
tion 2, we describe the linguistic and computa-
tional problems related to the parsing of adver-
bial participles in German. Section 3 introduces
the general idea of using the translation data to
find instances of different participle categories. In
Section 4, we illustrate the training of the clas-
sifier, evaluating the impact of the context win-
dow and the quality of the training data obtained
from cross-lingual text. In Section 5, we apply the
classifier to new, monolingual data and describe
the extension of the resource for adverbial partici-
ples. Section 6 evaluates the extended resource by
means of parsing experiments using the German
LFG grammar.
2 The Problem
In German, past perfect participles are ambiguous
with respect to their morphosyntactic category. As
in other languages, they can be used as part of
the verbal complex (Example (1-a)) or as adjec-
tives (Example (1-b)). Since German adjectives
can generally undergo conversion into adverbs,
participles can also be used adverbially (Example
(1-c)). The verbal and adverbial participle forms
are morphologically identical.
(1) a. Sie haben das Experiment wiederholt.
?They have repeated the experiment.?
b. Das wiederholte Experiment war erfolgreich.
?The repeated experiment was succesful.?
c. Sie haben das Experiment wiederholt abge-
brochen.
?They cancelled the experiment repeatedly.?
Moreover, German adjectival modifiers can be
generally used as predicatives that can be either
selected by a verb (Example (2-a)) or that can oc-
cur as free predicatives (Example (2-b)).
(2) a. Er scheint begeistert von dem Experiment.
?He seems enthusiastic about the experiment.?
b. Er hat begeistert experimentiert.
?He has experimented enthusiastic.?
Since predicative adjectives are not inflected,
the surface form of a German participle is ambigu-
ous between a verbal, predicative or adverbial use.
2.1 Participles in the German LFG
In order to account for sentences like (1-c), an in-
tuitive approach would be to generally allow for
adverb conversion of participles in the grammar.
However, in Zarrie? et al (2010), we show that
such a rule can have a strong negative effect on
the overall performance of the parsing system, de-
spite the fact that it produces the desired syntac-
tic and semantic analysis for specific sentences.
This problem was illustrated using a German LFG
grammar (Rohrer and Forst, 2006) constructed as
part of the ParGram project (Butt et al, 2002).
The grammar is implemented in the XLE, a gram-
mar development environment which includes a
very efficient LFG parser and a stochastic dis-
ambiguation component which is based on a log-
linear probability model (Riezler et al, 2002).
In Zarrie? et al (2010), we found that the
naive implementation of adverbial participles in
the German LFG, i.e. in terms of a general gram-
mar rule that allows for participles-adverb conver-
sion, leads to spurious ambiguities that mislead
the disambiguation component of the grammar.
Moreover, the rule increases the number of time-
outs, i.e. sentences that cannot be parsed in a pre-
defined amount of time (20 seconds). Therefore,
we observe a drop in parsing accuracy although
grammar coverage is improved. As a solution, we
induced a lexical resource of adverbial participles
based on their adverbial translations in a paral-
lel corpus. This resource, comprising 46 partici-
ple types, restricts the adverb conversion such that
most of the spurious ambiguities are eliminated.
To assess the impact of specific rules in a broad-
coverage grammar, possibly targeting medium-to-
low frequency phenomena, we have established a
fine-grained evaluation methodology. The chal-
lenge posed by these low-frequent phenomena is
typically two-fold: on the one hand, if one takes
into account the disambiguation component of the
grammar and pursues an evaluation of the most
probable parses on a general test set, the new
grammr rule cannot be expected to show a positive
effect since the phenomenon is not likely to occur
very often in the test set. On the other hand, if one
is interested in a linguistically precise grammar,
it is very unsatisfactory to reduce grammar cov-
erage to statistically frequent phenomena. There-
fore, we combined a coverage-oriented evaluation
on specialised testsuites with a quantitative evalu-
ation including disambiguation, making sure that
1427
the increased coverage does not lead to an overall
drop in accuracy. The evaluation methodolgy will
also be applied to evaluate the impact of the new
participle resource, see Section 6.
2.2 The Standard Flat Analysis of Modifiers
The fact that German adjectival modifiers can gen-
erally undergo conversion into adverbs without
overt morphological marking is a notorious prob-
lem for the syntactic analysis of German: there
are no theoretically established tests to distinguish
predicative adjectives and adverbials, see Geuder
(2004). For this reason, the standard German tag
set assigns a uniform tag (?ADJD?) to modifiers
that are morphologically ambiguous between an
adjectival and adverbial reading. Moreover, in
the German treebank TIGER (Brants et al, 2002)
the resulting syntactic differences between the two
readings are annotated by the same flat structure
that does not disambiguate the sentence.
Despite certain theoretical problems related to
the analysis of German modifiers, their interpre-
tation in real corpus sentences is often unambigu-
ous for native speakers. As an example, consider
example (3) from the TIGER treebank. In the
sentence, the participle unterschrieben (signed)
clearly functions as a predicative modifier of the
sentence?s subject. The other, theoretically possi-
ble reading where the participle would modify the
verb send is semantically not acceptable. How-
ever, in TIGER, the participle is analysed as an
ADJD modifier attached under the VP node which
is the general analysis for adjectival and adverbial
modifiers.
(3) Die
It
sollte
should
unterschrieben
signed
an
to
die
the
Leitung
administration
zuru?ckgesandt
sent back
werden.
be.
?It should be sent back signed to the administation.?
Sentence (4) (also taken from TIGER) illus-
trates the case of an adverbial participle. In this
example, the reading where angemessen (ade-
quately) modifies the main verb is the only one
that is semantically plausible. In the treebank, the
participle is tagged as ADJD and analysed as a
modifier in the VP.
(4) Der
The
menschliche
human
Geist
mind
la??t
lets
sich
itself
rechnerisch
computationally
nicht
not
angemessen
adequately
simulieren.
simulate.
?The human mind cannot be adequately simulated in a
computational way.?
The flat annotation strategy adopted for modi-
fiers in the standard German tag set and in the tree-
bank TIGER entails that instances of adverbs (and
adverbial participles) cannot be extracted from au-
tomatically tagged, or parsed, text. Therefore,
it would be very hard to obtain training mate-
rial from German resources to train a system that
automatically identifies adverbially used partici-
ples. However, the intuition corroborated by the
examples presented in this section is that the struc-
tures can actually be disambiguated in many cor-
pus sentences.
In the following sections, we show how we ex-
ploit parallel text to obtain training material for
learning to predict occurences of adverbial par-
ticiples, without any manual effort. Moreover, by
means of this technique, we can substantially ex-
tend the grammatical resource for adverbial par-
ticiples compared to the resource that can be di-
rectly extracted from the parallel text.
3 Participles in the Parallel Corpus
The intuition of the cross-lingual induction ap-
proach is that adverbial participles can easily be
extracted from parallel corpora since in other lan-
guages (such as English or French) adverbs are
often morphologically marked and easily labelled
by statistical PoS taggers. As an example, con-
sider sentence (5) extracted from Europarl, where
the German participle versta?rkt is translated by an
English adverb (increasingly).
(5) a. Nicht
Not
ohne
without
Grund
reason
sprechen
speak
wir
we
versta?rkt
increasingly
vom
of a
Europa
Europe
der
of the
Regionen.
Regions.
b. It is not without reason that we increasingly speak
in terms of a Europe of the Regions.
The idea is to project specific morphological
information about adverbs which is overt in lan-
guages like English onto German where adverbs
cannot be directly extracted from tagged data.
While this idea might seem intuitively straightfor-
1428
ward, we also know that translation pairs in paral-
lel data are not always lingusitically parallel, and
as a consequence, word-alignment is not always
reliable. To assess the impact of non-parallelism
in adverbial translations of German participles,
we manually annotated a sample of 300 transla-
tions. This data also constitutes the basis for the
experiments reported in Section 4.
3.1 Data
Our experiments are based on the same data as in
(Zarrie? et al, 2010). For convenience, we pro-
vide a short description here.
We limit our investigations to non-lexicalised
participles occuring in the Europarl corpus and
not yet recorded as adverbs in the lexicon of the
German LFG grammar (5054 participle types in
total). Given the participle candidates, we ex-
tract the set of sentences that exhibit a word align-
ment between a German participle and an English,
French or Dutch adverb. The word alignments
have been obtained with GIZA++. The extrac-
tion yields 27784 German-English sentence pairs
considering all alignment links, and 5191 sen-
tence pairs considering only bidirectional align-
ments between a participle and an English adverb.
3.2 Systematic Non-Parallelism
For data exploration and evaluation, we anno-
tated 300 participle alignments out of the 5191
German-English sentences (with a bidirectional
participle-adverb alignment). We distinguish the
following annotation categories: (i) parallel trans-
lation, adverb information can be projected, (ii)
incorrect alignment, (iii) correct alignment, but
translation is a multi-word expression, (iv) correct
alignment, but translation is a paraphrase (possi-
bly involving a translation shift).
Parallel Cases In our annotated sample of En-
glish adverb - German participle pairs, 43%1 of
the translation instances are parallel in the sense
that the overt adverb information from the English
side can be projected onto the German participle.
This means that if we base the induction technique
1The diverging figures we report in Zarrie? et al (2010)
were due to a small bug in the script and it does not affect the
overall interpretation of the data.
on word-alignments alone, its precision would be
relatively low.
Non-Parallel Cases Taking a closer look at the
non-parallel cases in our sample (57% of the
translation pairs), we find that 47% of this set are
due to incorrect word alignments. The remain-
ing 53% thus reflect regular cases of non-parallel
translations. A typical configuration which makes
up 30% of the the non-parallel cases is exempli-
fied in (6) where the German main verb vorlegen
is translated by the English multiword expression
put forward.
(6) a. Wir haben eine Reihe von Vorschla?gen vorgelegt.
b. We have put forward a number of proposals.
An example for the general paraphrase or trans-
lation shift category is given in Sentence (7).
Here, the translational correspondence between
gekommen (arrived) and the adverb now is due
to language-specific, idiomatic realisations of an
identical underlying semantic concept. The para-
phrase translations make up 23% of the non-
parallel cases in the annotated sample.
(7) a. Die
That
Zeit
time
ist
is
noch
yet
nicht
not
gekommen
arrived.
.
b. That time is not now .
Furthermore, it is noticeable that the cross-
lingual approach seems to inherently factor out
the ambiguity between predicative and adverbial
participles. In our annotated sample, there are no
predicative participles that have been translated by
an English adverb.
3.3 Filtering Mechanisms
The data analysis in the previous section, show-
ing only 43% of parallel cases in English adverb
translations for German participles, mainly con-
firms other studies in annotation projection which
find that translational correspondences only allow
for projection of linguistic analyses in a more or
less limited proportion (Yarowsky et al, 2001;
Hwa et al, 2005; Mihalcea et al, 2007).
In previous studies on annotation projection,
quite distinct filtering methods have been pro-
posed: in Yarowsky et al (2001), projection er-
rors are mainly attributed to word alignment er-
rors and filtered based on translation probabilities.
1429
Hwa et al (2005) find that errors in the projec-
tion of syntactic relations are also due to system-
atic grammatical divergences between languages
and propose correcting these errors by means of
specific, manually designed filters. Bouma et al
(2008) make similar observations to Hwa et al
(2005), but try to replace manual correction rules
by filters from additional languages.
In Zarrie? et al (2010), we compared a num-
ber of filtering techniques on our participle data.
The 300 annotated translation instances are used
as a test set for evaluation. In particular, we
have established that a combination of syntactic
dependency-based filters and multilingual filters
can very accurately separate non-parallel transla-
tions from parallel ones where the adverb infor-
mation can be projected. In Section 4, we show
that these filtering techniques are also very useful
for removing noise from the training material that
we use to build a classifier.
4 Bootstrapping a German Participle
Classifier from Crosslingual Data
In the previous section, we have seen that German
adverbial participles can be easily found in cross-
lingual text by looking at their translations in a
language that morphologically marks adverbials.
In previous work, we exploited this observation
by directly extracting types of adverbial partici-
ples based on word alignment links and the filter-
ing mechanisms mentioned in Section 3. How-
ever, this method is very closely tied to data in
the parallel corpus, which only comprises around
5000 participle-adverb translations in total, which
results in 46 types of adverbial participles after fil-
tering. Thus, we have no means of telling whether
we would discover new types of adverbial partici-
ples in other corpora, from different domains to
Europarl. As this corpus is rather small and genre
specific, it even seems very likely that one could
find additional adverbial participles in a bigger
corpus. Moreover, we cannot be sure that certain
adverbial participles have systematically diverg-
ing translations in other languages, due to cross-
lingual lexicalisation differences. Generally, it is
not clear whether we have learned something gen-
eral about the syntactic phenomenon of adverbial
participles in German or whether we have just ex-
tracted a small, corpus-dependent subset of the
class of adverbial participles.
In this section, we use instances of adverbially
translated participles as training material for a
classifier that learns to predict adverbial partici-
ples based on their monolingual syntactic context.
Thus, we exploit the translations in the parallel
corpus as a means of obtaining ?annotated? or dis-
ambiguated training data without any manual ef-
fort. During training, we only consider the mono-
lingual context of the participle, such that the fi-
nal application of the classifier is not dependent
on cross-lingual data anymore.
4.1 Context-based Identification of
Adverbial Participles
Given the general linguistic problems related to
adverbial participles (see Section 2), one could
assume that it is very difficult to identify them
in a given context. To assess the general dif-
ficulty of this syntactic problem, we run a first
experiment comparing a grammar-based identifi-
cation method against a classifier that only con-
siders relatively narrow morpho-syntactic context.
For evaluation, we use the 300 annotated partici-
ple instances described in Section 3. This test
set divides into 172 negative instances, i.e. non-
adverbial participles, and 128 positive instances.
We report accuracy of the identification method,
as well as precision and recall relating to the num-
ber of correctly predicted adverbial participles.
For the grammar-based identification, we use
the German LFG which integrates the lexical
resource for adverbial participles established in
(Zarrie? et al, 2010). We parse the 300 Europarl
sentences and check whether the most probable
parse proposed by the grammar analyses the re-
spective participle as an adverb or not. The gram-
mar obtains a complete parse for 199 sentences
out of the test set and we only consider these in
the evaluation. The results are given in Table 1.
The high precision and accuracy of the
grammar-based identification of adverbial partici-
ples suggests that in a lot of sentences, the adver-
bial analysis is the only possible reading, i.e. the
only analysis that makes the sentence grammati-
cal. But of course, we have substantially restricted
the adverb participle-conversion in the grammar,
1430
Training Data Precision Recall Accuracy
Grammar 97.3 90.12 94.97
Classifier Unigram 87.10 84.38 87.92
Classifier Bigram 88.28 88.28 89.93
Classifier Trigram 89.60 87.5 90.27
Table 1: Evaluation on 300 participle instances
from Europarl
so that it does not propose adverbial analyses for
participles that are very unlikely to function as
modifiers of verbs.
For the classifier-based identification, we use
the adverbially translated participle tokens in our
Europarl data (5191 tokens in total) as training
material. We remove the 300 test instances from
this training set, and then divide it into a set of
positive and negative instances. To do this, we
use the filtering mechanisms already proposed in
Zarrie? et al (2010). These filters apply on the
type level, such that we first identify the positive
types (46 total) and then use all instances of these
types in the 4891 sentences as positive instances
of adverbial participles (1978 instances). The re-
maining sentences are used as negative instances.
For the training of the classifier, we use
maximum-entropy classification, which is also
commonly used for the general task of tagging
(Ratnaparkhi, 1996). In particular, we use the
open source TADM tool for parameter estimation
(Malouf, 2002). The tags of the words surround-
ing the participles are used as features in the clas-
sification task. We explore different sizes of the
context window, where the trigram window is the
most succesful (see Table 1). Beyond the trigram
window, the results of the classifier start decreas-
ing again, probably because of too many mislead-
ing features. Generally, this experiment shows
that the grammar-based identification is more pre-
cise, but that the classifier still performs surpris-
ingly well. Compared to the results from the
grammar-based identification, the high accuracy
of the classifier suggests that even the narrow syn-
tactic contexts of adverbial vs. non-adverbial par-
ticiples are quite distinct.
4.2 Designing Training Data for Participle
Classification
There are several questions related to the design
of the training data that we use to build our clas-
sifier. First, it is not clear how many negative
instances are helpful for learning the adverbial -
non-adverbial distinction. In the above experi-
ment, we simply use the instances that do not pass
the cross-lingual filters. In this section, we exper-
iment with an augmented set of negative instances
that was also obtained by extracting German par-
ticiple that are bi-directionally aligned to an En-
glish participle in Europarl. This is based on the
assumption that these participles are very likely
to be verbal. Second, it is not clear whether we
really need the filtering mechanisms proposed in
Zarrie? et al (2010) and whether we could im-
prove the classifier by training it on a larger set
of positive instances. Therefore, we also experi-
ment with two further sets of positive instances:
one where we used all participles (not necessarily
bidirectionally) aligned to an adverb, one where
we only use the bidirectional alignments. The re-
sults obtained for the different sizes of positive
and negative instance sets are given in Table 2.
The picture that emerges from the results in Ta-
ble 2 is very clear: the stricter the filtering of the
training material (i.e. the positive instances) is,
the better the performance of the classifier. The
fact that we (potentially) loose certain positive in-
stances in the filtering does not negatively impact
on the classifier which substantially benefits from
the fact that noise gets removed. Moreover, we
find that if the training material is appropriately
filtered, adding further negative instances does not
help improving the accuracy. By contrast, if we
train on a noisy set of positive instances, the clas-
sifier benefits from a larger set of negative in-
stances. However, the positive effect that we get
from augmenting the non-filtered training data is
still weaker than the positive effect we get from
the filtering.
5 Induction of Adverbial Participles on
Monolingual Data
Given the classifier from Section 4 that predicts
the syntactic category of a participle instance
1431
Training Data Pos. Instances Neg. Instances Precision Recall Accuracy
Non-Filtered Instances (all alignments) 27.184 10.000 43.10 100 43.10
Non-Filtered Instances (all alignments) 27.184 50.000 74.38 92.97 83.22
Non-Filtered Instances (symm. alignments) 4891 10.000 78.08 89.06 84.56
Non-Filtered Instances (symm. alignments) 4891 50.000 82.31 83.59 85.23
Filtered Instances 1978 10.000 91.60 85.16 90.27
Filtered Instances 1978 50.000 90.83 77.34 86.91
Table 2: Evaluation on 300 participle instances from Europarl
based on its monolingual syntactic context, we
can now detect new instances or types of adver-
bial participles in any PoS-tagged German corpus.
In this section, we investigate whether the classi-
fier can be used to augment the resource of ad-
verbial participles directly induced from Europarl
with new types.
5.1 Data Extraction
We run our extraction experiment on the Huge
German Corpus (HGC), a corpus of 200 million
words of newspaper and other text. This corpus
has been tagged with TreeTagger (Schmid, 1994).
For each of the 5054 participle candidates, we ex-
tract all instances from the HGC which have not
been tagged as finite verbs (at most 2000 tokens
per participle). For each participle token, we also
extract its syntactic context in terms of the 3 pre-
ceding and the 3 following tags. For classification,
we use only those participles that have more than
50 instances in the corpus (2953 types).
In contrast to the cross-lingual filtering mech-
anisms developed in Zarrie? et al (2010) which
operate on the type-level, the classifier makes a
prediction for every token of a given participle
candidate. Thus, for each of the participle can-
didates, we obtain a percentage of instances that
have been classified as adverbs. As we would ex-
pect, the percentage of adverbial instances is very
low for most of the participles in our candidate set:
for 75% of the 2953 types, the percentage is below
5%. This result confirms our initial intuition that
the property of being used as an adverb is strongly
lexically restricted to a certain class of participles.
5.2 Evaluation
Since we know that the classifier has an accu-
racy of 90% on the Europarl data, we only con-
sider participles as candidates for adverbs where
the classifier predicted more than 14% adverbial
instances. This leaves us with a set of 210 partici-
ples, which comprises 13 of the original 46 par-
ticiples extracted from Europarl, meaning we have
discovered 197 new adverbial participle types.
We performed a manual evaluation of 50 ran-
domly selected types out of the set of 197 new
participle types. Therefore, we looked at the in-
stances and their context which the classifier pre-
dicted to be adverbial. If there was at least one ad-
verbial instance among these, the participle type
was evaluated as correctly annotated by the clas-
sifier. By this means, we find that 76% of the par-
ticiples were correctly classified.
This evaluation suggests that the accuracy of
our classifier which we trained and tested on Eu-
roparl data is lower on the HGC data. The rea-
son for this drop in performance will be explained
in the following Section 5.3. However, assuming
an accuracy of 76%, we have discovered 150 new
types of adverbial participles. We argue that this is
a very satisfactory result given that we have not in-
vested any manual effort into the annotation or ex-
traction of adverbial participles. This results also
makes clear that the previous resource we induced
on Europarl data, comprising only 46 participle
types, was a very limited one.
5.3 Error Analysis
Taking a closer look at the 12 participle candi-
dates that the classifier incorrectly labels as adver-
bial, we observe that their adverbially classified
instances are mostly instances of a predicative use.
This means that our Europarl training data does
not contain enough evidence to learn the distinc-
tion between adverbial and predicative participles.
This is not surprising since the set of negative
instances used for training the classifier mainly
comprises verbal instances of participles. More-
over, the syntactic contexts and constructions in
which some predicatives and adverbials are used
1432
Grammar Prec. Rec. F-Sc. Time
in sec
46 Part-Adv 84.12 78.2 81.05 665
243 Part-Adv 84.12 77.67 80.76 665
Table 3: Evaluation on 371 TIGER sentences
are very similar. Thus, in future work, we will
have to include more data on predicatives (which
is more difficult to obtain) and analyse the syntac-
tic contexts in more detail.
6 Assessing the Impact of Resource
Coverage on Grammar-based Parsing
In this section, we evaluate the classifier-based in-
duction of adverbial participles from a grammar-
based perspective. We integrate the entire set of
induced adverbial participles (46 from Europarl
and 197 from the HGC) into the German LFG
grammar. As a consequence, the grammar al-
lows the adverb conversion for 243 lexical par-
ticiple types. We use the evaluation methodolgy
explained in Section 2.
First, we conduct an accuracy-oriented evalua-
tion on the standard TIGER test set. We compare
against the German LFG that only integrates the
small participle resource from Europarl. The re-
sults are given in Table 3. The difference between
the 46 Part-Adv and 243 Part-Adv resource is not
statistically signficant. Thus, the larger participle
resource has no overall negative effect on the pars-
ing performance. As established by an automatic
upperbound evaluation in Zarrie? et al (2010),
we cannot not expect to find a positive effect in
this evaluation because the phenomenon does not
occur in the standard test set.
To show that the augmented resource indeed
improves the coverage of the grammar, we built
a specialised testsuite of 1044 TIGER sentences
that contain an instance of a participle from the
resource. Since this testsuite comprises sen-
tences from the training set, we can only report
a coverage-oriented evaluation here, see Table 4.
The 243 Part-Adv increases the coverage by 8%
on the specialised testsuite.
Moreover, we manually evaluated 20 sentences
covered by the 243-Part-Adv grammar and not
by 46-Part-Adv as to whether they contain a cor-
rectly analysed adverbial participle. In two sen-
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
No Part-Adv 665 315 64 3033
46 Part-Adv 710 269 65 3118
243 Part-Adv 767 208 69 3151
Table 4: Performance on the specialised TIGER
test set (1044 sentences)
tences, the grammar obtained an adverbial analy-
sis for clearly predicative modifiers, based on the
enlarged resource. In three different sentences, it
was difficult to decide whether the participle acts
as an adverb or a predicative. In the remaining 15
sentences, the grammar established the the correct
analysis of a clearly adverbially used participle.
7 Conclusion
We have proposed a cross-lingual induction
method to automatically obtain data on adverbial
participles in German. We exploited this cross-
lingual data as training material for a classifier that
learns to predict the syntactic category of a partici-
ple from its monolingual syntactic context. Since
this category is usually not annotated in German
resources and hard to describe in theory, the find-
ing that adverbial participles can be predicted rel-
atively precisely is of general interest for theo-
retic and computational approaches to the syntac-
tic analysis of German.
We showed that, in order to obtain an accurate
participle classifier, the quality of the training ma-
terial induced from the parallel corpus is of crucial
importance. By applying the filtering techniques
from Zarrie? et al (2010), the accuracy of the
classifier increases between 5% and 7%. In future
work, we plan to include more data on predicative
participles to learn a more accurate distinction be-
tween predicative and adverbial participles.
Finally, we used the participle classifier to ex-
tract a lexical resource of adverbial participles for
the German LFG grammar. In comparison to the
relatively small resource of 46 types that can be
directly induced from Europarl, we discovered a
large number of new participle types (197 types
in total). In a parsing experiment, we showed that
this much bigger resource does not negatively im-
pact on parsing performance and improves gram-
mar coverage.
1433
References
Bouma, Gerlof, Jonas Kuhn, Bettina Schrader, and
Kathrin Spreyer. 2008. Parallel LFG Grammars
on Parallel Corpora: A Base for Practical Trian-
gulation. In Butt, Miriam and Tracy Holloway
King, editors, Proceedings of the LFG08 Confer-
ence, pages 169?189, Sydney, Australia. CSLI Pub-
lications, Stanford.
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Butt, Miriam, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project.
Geuder, Wilhelm. 2004. Depictives and transpar-
ent adverbs. In Austin, J. R., S. Engelbrecht,
and G. Rauh, editors, Adverbials. The Interplay of
Meaning, Context, and Syntactic Structure, pages
131?166. Benjamins.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Nat. Lang. Eng., 11(3):311?325.
Malouf, Robert. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Conference on Natural Lan-
guage Learning (CoNLL-2002), pages 49?55.
Mihalcea, Rada, Carmen Banea, and Jan Wiebe.
2007. Learning multilingual subjective language
via cross-lingual projections. In Proceedings of
the Association for Computational Linguistics (ACL
2007), pages 976?983, Prague.
Pado?, Sebastian and Mirella Lapata. 2009. Cross-
lingual annotation projection of semantic roles.
Journal of Artificial Intelligence Research, 36:307?
340.
Ratnaparkhi, Adwait. 1996. A maximum entropy
model for part-of-speech tagging. In Proceedings
of EMNLP 96, pages 133?142.
Riezler, Stefan, Tracy Holloway King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal
using a Lexical-Functional Grammar and Discrim-
inative Estimation Techniques . In Proceedings of
ACL 2002.
Rohrer, Christian and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG
for German. In Proceedings of LREC-2006.
Schmid, Helmut. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of In-
ternational Conference on New Methods in Lan-
guage Processing.
Yarowsky, David, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analy-
sis tools via robust projection across aligned cor-
pora. In Proceedings of HLT 2001, First Interna-
tional Conference on Human Language Technology
Research.
Zarrie?, Sina, Aoife Cahill, Jonas Kuhn, and Christian
Rohrer. 2010. A Cross-Lingual Induction Tech-
nique for German Adverbial Participles. In Pro-
ceedings of the 2010 Workshop on NLP and Lin-
guistics: Finding the Common Ground, ACL 2010,
pages 34?42, Uppsala, Sweden.
1434
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 33?40
Manchester, August 2008
Speeding up LFG Parsing Using C-Structure Pruning
Aoife Cahill? John T. Maxwell III? Paul Meurer? Christian Rohrer? Victoria Rose?n?
?IMS, University of Stuttgart, Germany, {cahillae, rohrer}@ims.uni-stuttgart.de
?Palo Alto Research Center, 3333 Coyote Hill Road, Palo Alto, CA 94304, maxwell@parc.com
?Unifob Aksis, Bergen, Norway, paul.meurer@aksis.uib.no
?Unifob Aksis and University of Bergen, Norway, victoria@uib.no
Abstract
In this paper we present a method for
greatly reducing parse times in LFG pars-
ing, while at the same time maintaining
parse accuracy. We evaluate the method-
ology on data from English, German and
Norwegian and show that the same pat-
terns hold across languages. We achieve
a speedup of 67% on the English data and
49% on the German data. On a small
amount of data for Norwegian, we achieve
a speedup of 40%, although with more
training data we expect this figure to in-
crease.
1 Introduction
Efficient parsing of large amounts of natural lan-
guage is extremely important for any real-world
application. The XLE Parsing System is a large-
scale, hand-crafted, deep, unification-based sys-
tem that processes raw text and produces both
constituent structures (phrase structure trees) and
feature structures (dependency attribute-value ma-
trices). A typical breakdown of parsing time
of XLE components with the English grammar
is Morphology (1.6%), Chart (5.8%) and Unifier
(92.6%). It is clear that the major bottleneck in
processing is in unification. Cahill et al (2007)
carried out a preliminary experiment to test the
theory that if fewer c-structures were passed to
the unifier, overall parsing times would improve,
while the accuracy of parsing would remain sta-
ble. Their experiments used state-of-the-art prob-
abilistic treebank-based parsers to automatically
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
mark certain constituents on the input sentences,
limiting the number of c-structures the XLE pars-
ing system would build. They achieved an 18%
speedup in parse times, while maintaining the ac-
curacy of the output f-structures. The experiments
presented in Cahill et al (2007) used the XLE sys-
tem as a black box and did not make any changes to
it. However, the results were encouraging enough
for a c-structure pruning mechanism to be fully in-
tegrated into the XLE system.
The paper is structured as follows: we present
the pruning model that has been integrated into the
XLE system (Section 2), and how it can be ap-
plied successfully to more than one language. We
present experiments for English (Section 3), Ger-
man (Section 4) and Norwegian (Section 5) show-
ing that for both German and English, a significant
improvement in speed is achieved, while the qual-
ity of the f-structures remains stable. For Norwe-
gian a speedup is also achieved, but more training
data is required to sustain the accuracy of the f-
structures. In Section 7 we present an error anal-
ysis on the German data. We then relate the work
presented in this paper to similar efficient parsing
strategies (Section 8) before concluding in Section
9.
2 XLE and the C-Structure Pruning
Mechanism
The XLE system is designed to deal with large
amounts of data in a robust manner. There are
several mechanisms which facilitate this, including
fragmenting and skimming. Fragmenting is called
when the grammar is unable to provide a complete
parse for the input sentence, and a fragment anal-
ysis of largest possible chunks is built. Skimming
is called when too much time or memory has been
used by XLE. Any constituents that have not been
33
fully processed are ?skimmed?, which means that
the amount of work carried out in processing the
constituent is limited. This guarantees that XLE
will finish processing the sentence in polynomial
time.
XLE uses a chart-based mechanism for build-
ing parses, and has been complemented with a c-
structure pruning mechanism to speed up parsing
time. During pruning, subtrees at a particular cell
in the chart are pruned if their probabilities are not
higher than a certain threshold. The chart pruner
uses a simple stochastic CFG model. The proba-
bility of a tree is the product of the probabilities
of each of the rules used to form the tree, includ-
ing the rules that lead to lexical items (such as N
? dog). The probability of a rule is basically the
number of times that that particular form of the
rule occurs in the training data divided by the num-
ber of times the rule?s category occurs in the train-
ing data, plus a smoothing term. This is similar
to the pruning described in Charniak and Johnson
(2005) where edges in a coarse-grained parse for-
est are pruned to allow full evaluation with fine-
grained categories.
The pruner prunes at the level of individual con-
stituents in the chart. It calculates the probabil-
ities of each of the subtrees of a constituent and
compares them. The probability of each subtree
is compared with the best subtree probability for
that constituent. If a subtree?s probability is lower
than the best probability by a given factor, then the
subtree is pruned. In practice, the threshold is the
natural logarithm of the factor used. So a value of
5 means that a subtree will be pruned if its prob-
ability is about a factor of 150 less than the best
probability.
If two different subtrees have different num-
bers of morphemes under them, then the proba-
bility model is biased towards the subtree that has
fewer morphemes (since there are fewer probabil-
ities multiplied together). XLE counteracts this by
normalizing the probabilities based on the differ-
ence in length.
To illustrate how this works, we give the follow-
ing example. The string Fruit flies like bananas has
two different analyses. Figures 1 and 2 give their
analyses along with hypothetical probabilities for
each rule.
These two analyses come together at the S con-
stituent that spans the whole sentence. The proba-
bility of the first analysis is 8.4375E-14. The prob-
S
NP
N
Fruit
N
flies
VP
V
like
NP
N
bananas
S ? NP VP 0.5000
NP ? N N 0.1500
N ? Fruit 0.0010
N ? flies 0.0015
VP ? V NP 0.2000
V ? like 0.0050
NP ? N 0.5000
N ? bananas 0.0015
8.4375E-14
Figure 1: Analysis (1) for the string Fruit flies like
bananas with hypothetical probabilities
S
NP
N
Fruit
VP
V
flies
PP
P
like
NP
N
bananas
S ? NP VP 0.5000
NP ? N 0.5000
N ? Fruit 0.0010
V ? flies 0.0025
VP ? V PP 0.1000
P ? like 0.0500
PP ? P NP 0.9000
NP ? bananas 0.0015
4.21875E-12
Figure 2: Analysis (2) for the string Fruit flies like
bananas with hypothetical probabilities
ability of the second analysis is 4.21875E-12. This
means that the probability of the second analysis
is 50 times higher than the probability of the first
analysis. If the threshold is less than the natural
logarithm of 50 (about 3.9), then the subtree of the
first analysis will be pruned from the S constituent.
3 Experiments on English
We carried out a number of parsing experiments to
test the effect of c-structure pruning, both in terms
of time and accuracy. We trained the c-structure
pruning algorithm on the standard sections of Penn
Treebank Wall Street Journal Text (Marcus et al,
1994). The training data consists of the original
WSJ strings, marked up with some of the Penn
34
Treebank constituent information. We marked up
NPs and SBARs as well as adjective and verbal
POS categories. This is meant to guide the train-
ing process, so that it does learn from parses that
are not compatible with the original treebank anal-
ysis. We evaluated against the PARC 700 Depen-
dency Bank (King et al, 2003), splitting it into 140
sentences as development data and the remaining
unseen 560 for final testing (as in Kaplan et al
(2004)). We experimented with different values
of the pruning cutoff on the development set; the
results are given in Table 1.
The results show that the lower the cutoff value,
the quicker the sentences can be parsed. Using
a cutoff of 4, the development sentences can be
parsed in 100 CPU seconds, while with a cutoff
of 10, the same experiment takes 182 seconds.
With no cutoff, the experiment takes 288 CPU sec-
onds. However, this increase in speed comes at a
price. The number of fragment parses increases,
i.e. there are more sentences that fail to be analyzed
with a complete spanning parse. With no pruning,
the number of fragment parses is 23, while with
the most aggressive pruning factor of 4, there are
39 fragment parses. There are also many more
skimmed sentences with no c-structure pruning,
which impacts negatively on the results. The ora-
cle f-score with no pruning is 83.07, but with prun-
ing (at all thresholds) the oracle f-score is higher.
This is due to less skimming when pruning is acti-
vated, since the more subtrees that are pruned, the
less likely the XLE system is to run over the time
or memory limits needed to trigger skimming.
Having established that a cutoff of 5 performs
best on the development data, we carried out the
final evaluation on the 560-sentence test set using
this cutoff. The results are given in Table 2. There
is a 67% speedup in parsing the 560 sentences, and
the most probable f-score increases significantly
from 79.93 to 82.83. The oracle f-score also in-
creases, while there is a decrease in the random f-
score. This shows that we are throwing away good
solutions during pruning, but that overall the re-
sults improve. Part of this again is due to the fact
that with no pruning, skimming is triggered much
more often. With a pruning factor of 5, there are
no skimmed sentences. There is also one sentence
that timed out with no pruning, which also lowers
the most probable and oracle f-scores.
Pruning Level None 5
Total Time 1204 392
Most Probable F-Score 79.93 82.83
Oracle F-Score 84.75 87.79
Random F-Score 75.47 74.31
# Fragment Parses 96 91
# Time Outs 1 0
# Skimmed Sents 33 0
Table 2: Results of c-structure pruning experi-
ments on English test data
4 Experiments on German
We carried out a similar set of experiments on
German data to test whether the methodology de-
scribed above ported to a language other than En-
glish. In the case of German, the typical time of
XLE components is: Morphology (22.5%), Chart
(3.5%) and Unifier (74%). As training data we
used the TIGER corpus (Brants et al, 2002). Set-
ting aside 2000 sentences for development and
testing, we used the remaining 48,474 sentences as
training data. In order to create the partially brack-
eted input required for training, we converted the
original TIGER graphs into Penn-style trees with
empty nodes and retained bracketed constituents of
the type NP, S, PN and AP. The training data was
parsed by the German ParGram LFG (Rohrer and
Forst, 2006). This resulted in 25,677 full parses,
21,279 fragmented parses and 1,518 parse fail-
ures.1 There are 52,959 features in the final prun-
ing model.
To establish the optimal pruning settings for
German, we split the 2,000 saved sentences into
371 development sentences and 1495 test sen-
tences for final evaluation. We evaluated against
the TiGer Dependency Bank (Forst et al, 2004)
(TiGerDB), a dependency-based gold standard for
German parsers that encodes grammatical rela-
tions similar to, though more fine-grained than,
the ones in the TIGER Treebank as well as mor-
phosyntactic features. We experimented with the
same pruning levels as in the English experiments.
The results are given in Table 3.
The results on the development set show a sim-
ilar trend to the English results. A cutoff of 4 re-
sults in the fastest system, however at the expense
1The reason there are more fragment parses than, for ex-
ample, the results reported in Rohrer and Forst (2006) is that
the bracketed input constrains the parser to only return parses
compatible with the bracketed input. If there is no solution
compatible with the brackets, then a fragment parse is re-
turned.
35
Pruning Level None 4 5 6 7 8 9 10
Oracle F-Score 83.07 84.50 85.47 85.75 85.57 85.57 85.02 84.10
Time (CPU seconds) 288 100 109 123 132 151 156 182
# Time Outs 0 0 0 0 0 0 0
# Fragments 23 39 36 31 29 27 27 24
# Skimmed Sents 8 0 0 1 1 1 1 3
Table 1: Results of c-structure pruning experiments on English development data
Pruning Level None 4 5 6 7 8 9 10
Oracle F-Score 83.69 83.45 84.02 82.86 82.82 82.95 83.03 82.81
Time (CPU seconds) 1313 331 465 871 962 1151 1168 1163
# Time Outs 6 0 0 5 5 5 5 6
# Fragments 65 104 93 81 74 73 73 68
Table 3: Results of c-structure pruning experiments on German development data
Pruning Level None 5
Total Time 3300 1655
Most Probable F-Score 82.63 82.73
Oracle F-Score 84.96 84.79
Random F-Score 73.58 73.72
# Fragment Parses 324 381
# Time Outs 2 2
Table 4: Results of c-structure pruning experi-
ments on German test data
of accuracy. A cutoff of 5 seems to provide the
best tradeoff between time and accuracy. Again,
most of the gain in oracle f-score is due to fewer
timeouts, rather than improved f-structures. In the
German development set, a cutoff of 5 leads to a
speedup of over 64% and a small increase in or-
acle f-score of 0.33 points. Therefore, for the fi-
nal evaluation on the unseen test-set, we choose a
cutoff of 5. The results are given in Table 4. We
achieve a speedup of 49% and a non-significant in-
crease in most probable f-score of 0.094. The time
spent by the system on morphology is much higher
for German than for English. If we only take the
unification stage of the process into account, the
German experiments show a speedup of 65.5%.
5 Experiments on Norwegian
As there is no treebank currently available for Nor-
wegian, we were unable to train the c-structure
pruning mechanism for Norwegian in the same
way as was done for English and German. There
is, however, some LFG-parsed data that has been
completely disambiguated using the techniques
described in Rose?n et al (2006). In total there
are 937 sentences from various text genres includ-
ing Norwegian hiking guides, Sophie?s World and
the Norwegian Wikipedia. We also use this dis-
ambiguated data as a gold standard for evaluation.
The typical time of XLE components with the Nor-
wegian grammar is: Morphology (1.6%), Chart
(11.2%) and Unifier (87.2%).
From the disambiguated text, we can automati-
cally extract partially bracketed sentences as input
to the c-structure pruning training method. We can
also extract sentences for training that are partially
disambiguated, but these cannot be used as part of
the test data. To do this, we extract the bracketed
string for each solution. If all the solutions pro-
duce the same bracketed string, then this is added
to the training data. This results in an average of
4556 features. As the data set is small, we do not
split it into development, training and test sections
as was done for English and German. Instead we
carry out a 10-fold cross validation over the entire
set. The results for each pruning level are given in
Table 5.
The results in Table 5 show that the pattern that
held for English and German does not quite hold
for Norwegian. While, as expected, the time taken
to parse the test set is greatly reduced when using
c-structure pruning, there is also a negative impact
on the quality of the f-structures. One reason for
this is that there are now sentences that could pre-
viously be parsed, and that now no longer can be
parsed, even with a fragment grammar.2 With c-
structure pruning, the number of fragment parses
increases for all thresholds, apart from 10. It is
also difficult to compare the Norwegian experi-
ment to the English and German, since the gold
standard is constrained to only consist of sentences
that can be parsed by the grammar. Theoretically
the oracle f-score for the experiment with no prun-
2With an extended fragment grammar, this would not hap-
pen.
36
Pruning Level None 4 5 6 7 8 9 10
Oracle F-Score 98.76 94.45 95.60 96.40 96.90 97.52 98.00 98.33
Time (CPU seconds) 218.8 106.2 107.4 109.3 112 116.2 124 130.7
# Time Outs 0 0 0 0 0 0 0 0
# Parse Failures 0.2 5.7 3.9 2 3.2 4.2 4.6 4.2
# Fragments 1.3 7.7 6.5 4.7 2.8 1.8 1.5 1.2
Table 5: Results of c-structure pruning 10-fold cross validation experiments on Norwegian data
55
60
65
70
75
80
85
90
95
None 4 5 6 7 8 9 10
Figure 3: The lower-bound results for each of the
10 cross validation runs across the thresholds
ing should be 100. The slight drop is due to a
slightly different morphological analyzer used in
the final experiments that treats compound nouns
differently. A threshold of 10 gives the best results,
with a speedup of 40% and a drop in f-score of 0.43
points. It is difficult to choose the ?best? thresh-
old, as the amount of training data is probably not
enough to get an accurate picture of the data. For
example, Figure 3 shows the lower-bound results
for each of the 10 runs. It is difficult to see a clear
pattern for all the runs, indicating that the amount
of training data is probably not enough for a reli-
able experiment.
6 Size of Training Data Corpus
The size of the Norwegian training corpus is con-
siderably smaller than the training corpora for En-
glish or German, so the question remains how
much training data we need in order for the c-
structure pruning to deliver reliable results. In or-
der to establish a rough estimate for the size of
training corpus required, we carried out an experi-
ment on the German TIGER training corpus.
We randomly divided the TIGER training cor-
pus into sets of 500 sentences. We plot the learn-
ing curve of the c-structure pruning mechanism in
Figure 4, examining the effect of increasing the
size of the training corpus on the oracle f-score on
the development set of 371 sentences. The curve
shows that, for the German data, the highest oracle
f-score of 84.98 was achieved with a training cor-
pus of 32,000 sentences. Although the curve fluc-
tuates, the general trend is that the more training
data, the better the oracle f-score.3
7 Error Analysis
Given that we are removing some subtrees during
parsing, it can sometimes happen that the desired
analysis gets pruned. We will take German as an
example, and look at some of these cases.
7.1 Separable particles vs pronominal
adverbs
The word dagegen (?against it?) can be a separable
prefix (VPART) or a pronominal adverb (PADV).
The verb protestieren (?to protest?) does not take
dagegen as separable prefix. The verb stimmen
(?to agree?) however does. If we parse the sen-
tence in (1) with the verb protestieren and activate
pruning, we do not get a complete parse. If we
parse the same sentence with stimmen as in (2) we
do get a complete parse. If we replace dagegen
by dafu?r, which in the current version of the Ger-
man LFG can only be a pronominal adverb, the
sentence in (3) gets a parse. We also notice that
if we parse a sentence, as in (4), where dagegen
occurs in a position where our grammar does not
allow separable prefixes to occur, we get a com-
plete parse for the sentence. These examples show
that the pruning mechanism has learned to prune
the separable prefix reading of words that can be
both separable prefixes and pronominal adverbs.
(1) Sie
they
protestieren
protest
dagegen.
against-it
?They protest against it.?
(2) Sie
they
stimmen
vote
dagegen.
against-it
?They vote against it.?
3Unexpectedly, the curve begins to decline after 32,000
sentences. However, the differences in f-score are not statis-
tically significant (using the approximate randomization test).
Running the same experiment with a different random seed
results in a similarly shaped graph, but any decline in f-score
when training on more data was not statistically significant at
the 99% level.
37
32000, 84.97698
84
84.1
84.2
84.3
84.4
84.5
84.6
84.7
84.8
84.9
85
50
0
20
00
35
00
50
00
65
00
80
00
95
00
11
00
0
12
50
0
14
00
0
15
50
0
17
00
0
18
50
0
20
00
0
21
50
0
23
00
0
24
50
0
26
00
0
27
50
0
29
00
0
30
50
0
32
00
0
33
50
0
35
00
0
36
50
0
38
00
0
39
50
0
41
00
0
42
50
0
44
00
0
45
50
0
47
00
0
48
50
0
Number of Training Sentences
F-
Sc
o
re
Figure 4: The effect of increasing the size of the training data on the oracle f-score
(3) Er
he
protestiert
protests
dafu?r.
for-it
?He protests in favour of it.?
(4) Dagegen
against-it
protestiert
protests
er.
he
?Against it, he protests.?
7.2 Derived nominal vs non-derived nominal
The word Morden can be the dative plural of the
noun Mord (?murder?) or the nominalized form of
the verb morden (?to murder?). With c-structure
pruning activated (at level 5), the nominalized
reading, as in (6), gets pruned, whereas the dative
plural reading is not (5). At pruning level 6, both
readings are assigned a full parse. We see simi-
lar pruning of nominalized readings as in (7). If
we look in more detail at the raw counts for re-
lated subtrees gathered from the training data, we
see that the common noun reading for Morden oc-
curs 156 times, while the nominalized reading only
occurs three times. With more training data, the c-
structure pruning mechanism could possibly learn
when to prune correctly in such cases.
(5) Er
he
redet
speaks
von
of
Morden.
murders
?He speaks of murders.?
(6) Das
the
Morden
murdering
will
wants
nicht
not
enden.
end
?The murdering does not want to end.?
(7) Das
the
Arbeiten
working
endet.
ends
?The operation ends.?
7.3 Personal pronouns which also function as
determiners
There are a number of words in German that can
function both as personal pronouns and determin-
ers. If we take, for example, the word ihr, which
can mean ?her?, ?their?, ?to-her?, ?you-pl? etc.,
the reading as a determiner gets pruned as well as
some occurrences as a pronoun. In example (8),
we get a complete parse for the sentence with the
dative pronoun reading of ihr. However, in ex-
ample (9), the determiner reading is pruned and
we fail to get a complete parse. In example (10),
we also fail to get a complete parse, but in exam-
ple (11), we do get a complete parse. There is a
parameter we can set that sets a confidence value
in certain tags. So, for example, we set the con-
fidence value of INFL-F BASE[det] (the tag given
to the determiner reading of personal pronouns) to
be 0.5, which says that we are 50% confident that
the tag INFL-F BASE[det] is correct. This results in
38
examples 8, 9 and 11 receiving a complete parse,
with the pruning threshold set to 5.
(8) Er
he
gibt
gives
es
it
ihr.
her
?He gives it to her.?
(9) Ihr
her/their
Auto
car
fa?hrt.
drives
?Her/Their car drives.
(10) Ihr
you(pl)
kommt.
come
?You come.?
(11) Er
he
vertraut
trusts
ihr.
her
?He trusts her.?
7.4 Coordination of Proper Nouns
Training the German c-structure pruning mecha-
nism on the TIGER treebank resulted in a pecu-
liar phenomenon when parsing coordinated proper
nouns. If we parse four coordinated proper nouns
with c-structure pruning activated as in (12), we
get a complete parse. However, as soon as we add
a fifth proper noun as in (13), we get a fragment
parse. This is only the case with proper nouns,
since the sentence in (14) which coordinates com-
mon nouns gets a complete parse. Interestingly, if
we coordinate n proper nouns plus one common
noun, we also get a complete parse. The reason for
this is that proper noun coordination is less com-
mon than common noun coordination in our train-
ing set.
(12) Hans, Fritz, Emil und Maria singen.
?Hans, Fritz, Emil and Maria sing.?
(13) Hans, Fritz, Emil, Walter und Maria sin-
gen.
?Hans, Fritz, Emil, Walter and Maria sing.?
(14) Hunde, Katzen, Esel, Pferde und Affen
kommen.
?Dogs, cats, donkeys, horses and apes
come.?
(15) Hans, Fritz, Emil, Walter, Maria und
Kinder singen.
?Hans, Fritz, Emil, Walter, Maria and chil-
dren sing.?
We ran a further experiment to test what effect
adding targeted training data had on c-structure
pruning. We automatically extracted a specialized
corpus of 31,845 sentences from the Huge Ger-
man Corpus. This corpus is a collection of 200
million words of newspaper and other text. The
sentences we extracted all contained examples of
proper noun coordination and had been automati-
cally chunked. Training on this sub-corpus as well
as the original TIGER training data did have the
desired effect of now parsing example (13) with
c-structure pruning activated.
8 Related Work
Ninomiya et al (2005) investigate beam threshold-
ing based on the local width to improve the speed
of a probabilistic HPSG parser. In each cell of a
CYK chart, the method keeps only a portion of the
edges which have higher figure of merits compared
to the other edges in the same cell. In particular,
each cell keeps the edges whose figure of merit is
greater than ?
max
- ?, where ?
max
is the high-
est figure of merit among the edges in the chart.
The term ?beam thresholding? is a little confusing,
since a beam search is not necessary ? instead, the
CYK chart is pruned directly. For this reason, we
prefer the term ?chart pruning? instead.
Clark and Curran (2007) describe the use of
a supertagger with a CCG parser. A supertag-
ger is like a tagger but with subcategorization in-
formation included. Chart pruners and supertag-
gers are conceptually complementary, since chart
pruners prune edges with the same span and the
same category, whereas supertaggers prune (lexi-
cal) edges with the same span and different cate-
gories. Ninomiya et al (2005) showed that com-
bining a chunk parser with beam thresholding pro-
duced better results than either technique alone. So
adding a supertagger should improve the results
described in this paper.
Zhang et al (2007) describe a technique to
selectively unpack an HPSG parse forest to ap-
ply maximum entropy features and get the n-best
parses. XLE already does something similar when
it applies maximum entropy features to get the
n-best feature structures after having obtained a
packed representation of all of the valid feature
structures. The current paper shows that pruning
the c-structure chart before doing (packed) unifica-
tion speeds up the process of getting a packed rep-
resentation of all the valid feature structures (ex-
cept the ones that may have been pruned).
39
9 Conclusions
In this paper we have presented a c-structure prun-
ing mechanism which has been integrated into the
XLE LFG parsing system. By pruning the number
of c-structures built in the chart, the next stage of
processing, the unifier, has considerably less work
to do. This results in a speedup of 67% for En-
glish, 49% for German and 40% for Norwegian.
The amount of training data for Norwegian was
much less than that for English or German, there-
fore further work is required to fully investigate
the effect of c-structure pruning. However, the re-
sults, even from the small training data, were en-
couraging and show the same general patterns as
English and German. We showed that for the Ger-
man training data, 32,000 sentences was the opti-
mal number in order to achieve the highest oracle
f-score. There remains some work to be done in
tuning the parameters for the c-structure pruning,
as our error analysis shows. Of course, with sta-
tistical methods one can never be guaranteed that
the correct parse will be produced; however we can
adjust the parameters to account for known prob-
lems. We have shown that the c-structure pruning
mechanism described is an efficient way of reduc-
ing parse times, while maintaining the accuracy of
the overall system.
Acknowledgements
The work presented in this paper was supported
by the COINS project as part of the linguistic
Collaborative Research Centre (SFB 732) at the
University of Stuttgart and by the Norwegian Re-
search Council through the LOGON and TREPIL
projects.
References
Brants, Sabine, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
Treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories, Sozopol, Bulgaria.
Cahill, Aoife, Tracy Holloway King, and John T.
Maxwell III. 2007. Pruning the Search Space of
a Hand-Crafted Parsing System with a Probabilistic
Parser. In ACL 2007 Workshop on Deep Linguistic
Processing, pages 65?72, Prague, Czech Republic,
June. Association for Computational Linguistics.
Charniak, Eugene and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 173?180, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Clark, Stephen and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Forst, Martin, Nu?ria Bertomeu, Berthold Crysmann,
Frederik Fouvry, Silvia Hansen-Schirra, and Valia
Kordoni. 2004. Towards a dependency-based gold
standard for German parsers ? The TiGer Depen-
dency Bank. In Proceedings of the COLING Work-
shop on Linguistically Interpreted Corpora (LINC
?04), Geneva, Switzerland.
Kaplan, Ronald M., John T. Maxwell, Tracy H. King,
and Richard Crouch. 2004. Integrating Finite-state
Technology with Deep LFG Grammars. In Pro-
ceedings of the ESSLLI 2004 Workshop on Combin-
ing Shallow and Deep Processing for NLP, Nancy,
France.
King, Tracy Holloway, Richard Crouch, Stefan Riezler,
Mary Dalrymple, and Ronald M. Kaplan. 2003. The
PARC 700 Dependency Bank. In Proceedings of the
EACL Workshop on Linguistically Interpreted Cor-
pora (LINC ?03), Budapest, Hungary.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Ninomiya, Takashi, Yoshimasa Tsuruoka, Yusuke
Miyao, and Jun?ichi Tsujii. 2005. Efficacy of Beam
Thresholding, Unification Filtering and Hybrid Pars-
ing in Probabilistic HPSG Parsing. In Proceed-
ings of the Ninth International Workshop on Pars-
ing Technology, pages 103?114, Vancouver, British
Columbia, October. Association for Computational
Linguistics.
Rohrer, Christian and Martin Forst. 2006. Improving
Coverage and Parsing Quality of a Large-scale LFG
for German. In Proceedings of the Language Re-
sources and Evaluation Conference (LREC-2006),
Genoa, Italy.
Rose?n, Victoria, Paul Meurer, and Koenraad de Smedt.
2006. Towards a Toolkit Linking Treebanking and
Grammar Development. In Hajic, Jan and Joakim
Nivre, editors, Proceedings of the Fifth Workshop
on Treebanks and Linguistic Theories, pages 55?66,
December.
Zhang, Yi, Stephan Oepen, and John Carroll. 2007.
Efficiency in Unification-Based N-Best Parsing. In
Proceedings of the Tenth International Conference
on Parsing Technologies, pages 48?59, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
40
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 34?42,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
A Cross-Lingual Induction Technique for German Adverbial Participles
Sina Zarrie? Aoife Cahill Jonas Kuhn Christian Rohrer
Institut fu?r Maschinelle Sprachverarbeitung (IMS)
University of Stuttgart
Stuttgart, Germany
{zarriesa,cahillae,jonas.kuhn,rohrer}@ims.uni-stuttgart.de
Abstract
We provide a detailed comparison of
strategies for implementing medium-to-
low frequency phenomena such as Ger-
man adverbial participles in a broad-
coverage, rule-based parsing system. We
show that allowing for general adverb con-
version of participles in the German LFG
grammar seriously affects its overall per-
formance, due to increased spurious am-
biguity. As a solution, we present a
corpus-based cross-lingual induction tech-
nique that detects adverbially used par-
ticiples in parallel text. In a grammar-
based evaluation, we show that the auto-
matically induced resource appropriately
restricts the adverb conversion to a limited
class of participles, and improves parsing
quantitatively as well as qualitatively.
1 Introduction
In German, past perfect participles are ambigu-
ous with respect to their morphosyntactic cate-
gory. As in other languages, they can be used
as part of the verbal complex (example (1-a)) or
as adjectives (example (1-b)). Since German ad-
jectives can generally undergo conversion into ad-
verbs, participles can also be used adverbially (ex-
ample (1-c)). All three participle forms in (1) are
morphologically identical.
(1) a. Das Experiment hat ihn begeistert.
?The experiment has enthused him.?
b. Er scheint von dem Experiment begeistert.
?He seems enthusiastic about the experiment.?
c. Er hat begeistert experimentiert.
?He has experimented in an enthusiastic way? or:
?He was enthusiastic when he experimented.?
This paper adresses the question of how to deal
with medium-to-low frequency phenomena such
as adverbial participles in a broad-coverage, rule-
based parsing system. In order to account for sen-
tences like (1-c), an intuitive approach would be to
generally allow for adverb conversion of partici-
ples in the grammar. However, on the basis of the
German LFG grammar (Rohrer and Forst, 2006),
we show that such a rule can have a strong negative
on the overall performance of the parsing system,
despite the fact that it produces the desired syntac-
tic and semantic analysis for specific sentences.
This trade-off between large-scale, statistical
and theoretically precise coverage is often en-
countered in engineering broad-coverage and, at
the same time, linguistically motivated parsing
systems: adding the analysis for a specific phe-
nomenon does not necessarily improve the overall
quality of the system since the rule might overgen-
erate and interact with completely different phe-
nomena in unpredicted ways.
In principle, there are two ways of dealing with
such an overgeneration problem in a grammar-
based framework: First, one could hand-craft
word lists or other linguistic constraints that re-
strict the adverb conversion to a certain set of par-
ticiples. Second, one could try to mine corpora for
this particular type of adverbs and integrate this
automatically induced knowledge into the gram-
mar (i.e. by means of pre-tagged input, word lists,
etc.). In the case of adverbial participles, both
ways are prone with difficulties. To our knowl-
edge, there has not been much theoretical work on
the linguistic properties of the participle adverb
conversion. Moreover, since the distinction be-
tween (predicative) adjectives and adverbs is the-
oretically hard to establish, the standard tag set
for German and, in consequence, annotated cor-
pora for German do not explicitly capture this phe-
nomenon. Thus, available statistical taggers and
parsers for German usually conflate the syntactic
structures underlying (1-b) and (1-c).
In this paper, we present a corpus-based ap-
proach to restricting the overgenerating adverb
conversion for participles in German, exploiting
34
parallel corpora and cross-lingual NLP induc-
tion techniques. Since adverbs are often overtly
marked in other languages (i.e. the ly-suffix in
English), adverbial participles can be straightfor-
wadly detected on word-aligned parallel text. We
describe the ingretation of the automatically in-
duced resource of adverbial participles into the
German LFG, and provide a detailed evaluation of
its effect on the grammar, see Section 5.
While the use of parallel resources is rather
familiar in a wide range of NLP domains, such
as statistical machine translation (Koehn, 2005)
or annotation projection (Yarowsky et al, 2001),
our work shows that they can be exploited for
very specific problems that arise in deep linguis-
tic analysis (see Section 4). In this way, high-
precision, data-oriented induction techniques can
clearly improve rule-based system development
through combining the benefits of high empirical
accuracy and little manual effort.
2 A Broad-Coverage LFG for German
Lexical Functional Grammar (LFG) (Bresnan,
2000) is a constraint-based theory of grammar. It
posits two levels of representation, c(onstituent)-
structure and f(unctional)- structure. C-structure
is represented by contextfree phrase-structure
trees, and captures surface grammatical configu-
rations. F-structures approximate basic predicate-
argument and adjunct structures.
The experiments reported in this paper use the
German LFG grammar constructed as part of the
ParGram project (Butt et al, 2002). The grammar
is implemented in the XLE, a grammar develop-
ment environment which includes a very efficient
LFG parser. Within the spectrum of appraoches
to natural language parsing, XLE can be consid-
ered a hybrid system combining a hand-crafted
grammar with a number of automatic ambiguity
management techniques: (i) c-structure pruning
where, based on information from statstically ob-
tained parses, some trees are ruled out before f-
structure unification (Cahill et al, 2007), (ii) an
Optimaly Theory-style constraint mechanism for
filtering and ranking competing analyses (Frank
et al, 2001), and (iii) a stochastic disambiguation
component which is based on a log-linear proba-
bility model (Riezler et al, 2002) and works on
the packed representations.
The German LFG grammar integrates a mor-
phological component which is a variant of
DMOR1 (Becker, 2001). This means that the (in-
ternal) lexicon does not comprise entries for sur-
face word forms, but entries for specific morpho-
logical tags, see (Dipper, 2003).
3 Participles in the German LFG
3.1 Analysis
The morphosyntactic ambiguity of German par-
ticiples presents a notorious difficulty for theoreti-
cal and computational analysis. The reason is that
adjectives (i.e. adjectival participles) do not only
occur as attributive modifiers (shown in (1-a)), but
can also be used as predicatives (see (2-b)). These
predicatives have exactly the same form as ver-
bal or adverbial participles (compare the three sen-
tences in (2)). Predicatives do appear either as ar-
guments of verbs like seem or as free adjuncts such
that they are not even syntactically distinguishable
from adverbs. The sentence in (2-c) is thus am-
biguous as to whether the participle is an adverb
modifying the main verb, or a predicative which
modifies the subject. Especially in the case of
modifiers refering to a psychological state, the two
underlying readings are hard to tell apart (Geuder,
2004). It is due to the lack of reliable semantic
tests that the standard German tag set (Schiller et
al., 1995) assigns the tag ?ADJD? to predicative
adjectives as well as adverbs.
(2) a. Das Experiment hat ihn begeistert.
?The experiment has enthused him.?
b. Er scheint von dem Experiment begeistert.
?He seems enthusiastic about the experiment.?
c. Er hat begeistert experimentiert.
?He has experimented in an enthusiastic way? or:
?He was enthusiastic when he experimented.?
For performance reasons, the German LFG does
not cover free predicatives at the moment. In the
context of our crosslingual induction approach,
the distinction between predicatives and adverbs
is rather straigtforward since we base our experi-
ments on languages that have morphologically dis-
tinct forms for these categories. In the follow-
ing, we will thus limit the discussion to adverbial
participles and ignore the complexities related to
predicative participles.
In the German LFG, the treatment of a given
participle form is closely tight to the morphologi-
cal analysis encoded in DMOR. In particular, ad-
verbial participles can have different degrees of
lexicalisation. For bestimmt (probably) in (3-a),
which is completely lexicalised, the morphology
35
proposes two analyses: (i) a participle tag of the
verbal lemma bestimmen (determine) and (ii) an
adverb tag for the lemma bestimmt. In this case,
the LFG parsing algorithm will figure out which
morphological analysis yields a syntactically well-
formed analysis. For gezielt (purposeful) in (3-b),
DMOR outputs, besides the participle analysis, an
adjective tag for the lemma. However, the gram-
mar can turn it into an adverb by a general ad-
verb conversion rule for adjectives. The difficult
case for the German LFG grammar is illustrated in
(3-c) by means of the adverbial participle wieder-
holt (repeatedly). This participle is neither lexi-
calised as an adverb nor as an adjective, but it still
can be used as an adverb.
(3) a. Bestimmt
Probably
ist
is
dieser
the
Mann
man
sehr
very
traurig.
sad.
b. Der
The
Mann
man
hat
has
gezielt
acted
gehandelt.
purposefully.
c. Der
The
Mann
man
hat
has
wiederholt
repeatedly
geweint.
cried.
To cover sentences like (3-c), the grammar
needs to include a rule that allows adverb conver-
sion for participles. Unfortunately, this rule is very
costly in terms of the overall performance of the
grammar, as is shown in the following section.
3.2 Assessing the Effect of Participle
Ambiguity on the German LFG
In this section, we want to illustrate the effect of
one specific grammar rule, i.e. the rule that gener-
ally allows for conversion of participles into ad-
verbs. We perform a contrastive evaluation of
two versions of the grammar: (i) the No-Part-Adv
version which does not allow for adverb conver-
sion (except for the lexicalised participles from
DMOR), (ii) the All-Part-Adv version which al-
lows every participle to be analysed as adverb.
Otherwise, the two versions of the grammar are
completely identical.
The comparison between the All-Part-Adv and
No-Part-Adv grammar version pursues two major
goals: On the one hand, we want to assess their
overall quantitative performance on representative
gold standard data, as it is common practice for
statistical parsing systems. On the other hand, we
are interested in getting a detailed picture of the
quality of the grammar for parsing adverbial par-
ticiples. These two goals do not necessarily go to-
gether since we know that the phenomenon is not
very frequent in the data which we use for evalu-
ation. Therefore, we do not only report accuracy
on gold standard data in the following, but also fo-
cus on error analysis and describe ways of qualti-
tatively assessing the grammar performance.
For evaluation, we use the TIGER treebank
(Brants et al, 2002). We report grammar per-
formance on the development set which consists
of the first 5000 TIGER sentences, and statistical
accuracy on the standard heldout set which com-
prises 371 sentences.
Quantitative Evaluation We first want to assess
the quantitative impact of the phenomenon of ad-
verbial participles in our evaluation data. We parse
the heldout set storing all possible analyses ob-
tained by both grammars, in order to compare the
upperbound score that the both versions can op-
timally achieve (i.e. independently of the disam-
biguation quality). Then, we run the XLE eval-
uation in the ?oracle? mode which means that the
disambiguation compares all system analyses for a
given sentence to its gold analysis, and chooses the
best system analysis for computing accuracy. The
upperbound f-score for both grammar versions is
almost identical (at about 83.6%). This suggests
that the phenomenon of adverbial participles does
not occur in the heldout set.
If we run the grammar versions on a larger
set of sentences, the difference in coverage be-
comes more obvious. In Table 1, we report the
absolute number of parsed sentences, starred sen-
tences (only receiving a partial or fragment parse),
and the timeouts 1 on our standard TIGER devel-
opment set. Not very surprisingly, the coverage
of the All-Part-Adv version seems to be broader.
However, this does not necessarily mean that the
40 additionally covered sentences all exhibit ad-
verbial participles (see below). Moreover, Table 2
gives a first indication of the fact that the extended
coverage comes at a price: the All-Part-Adv ver-
sion massively increases the number of ambigui-
ties per sentence. Related to this, in the All-Part-
Adv version, the number of timeouts increases by
16% and parsing speed goes down by 6% com-
pared to the No-Part-Adv version.
To assess the effect of the massively increased
ambiguity rate and the bigger proportion of time-
outs in All-Part-Adv, we perform a statistical eval-
uation of the two versions of the grammar against
the heldout set, i.e. we compute f-score based
1Sentences whose parsing can not be finished in prede-
fined amount of time, the maximally allowed parse time is
set to 20 seconds.
36
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
No-Part-Adv 4301 608 90 6853
All-Part-Adv 4339 555 105 7265
Table 1: Coverage-based evaluation on the TIGER
development set (sentences 1-5000), 4999 sen-
tences total
Sent. Av. ambiguities per sent. Av.
length No-Part-Adv All-Part-Adv Incr.
1-10 2.95 3.3 11%
11-20 24.99 36.09 44%
21-30 250.4 343.76 37%
31-40 1929.06 2972.847 54%
41-50 173970.0 663310.4 429%
Table 2: Average number of ambiguities per sen-
tence
on the parses that the XLE disambiguation selects
as the most probable parse. Both versions use
the same disambiguation model which results in
a slightly biased comparison but still reflects the
effect of increased ambiguity on the disambigua-
tion component. In Table 3, we can see that the
All-Part-Adv version performs significantly worse
than the grammar version which does not cap-
ture adverbial participles. The spurious ambigu-
ities and timeouts produced in All-Part-Adv have
such a strong negative impact on the disambigua-
tion component that it can not be outweighed by
the extended coverage of the grammar.
Qualitative Evaluation The fact that the All-
Part-Adv version generally increases parse ambi-
guity suggests that it produces a lot of undesired
analyses for constructions not related to adverbial
participles. To assess this assumption, we drew a
random sample of 20 sentences out of the addi-
tionally covered 41 sentences and checked manu-
ally whether these contained an adverbial partici-
ple: Only 40% of these sentences are actually cor-
rectly analysed. In all other cases, the grammar
lacks an analysis for a completely different phe-
Grammar Prec. Rec. F-Sc. Time
in sec
All-Part-Adv 83.80 76.71 80.1 666.55
No-Part-Adv 84.25 78.3 81.17 632.21
Table 3: Evaluation on the TIGER heldout set, 371
sentences total
nomenon (mostly related to coordination), but ob-
tains an (incorrect) analysis on the basis of the ad-
verb conversion rule.
As an example, Figure 1 presents two c-
structure analyses for the sentence in (4) in the
All-Part-Adv grammar. In the second c-structure
(CS2), the participle kritisiert (criticised) is anal-
ysed as adverb modifing the main verb haben
(have). This results in a very strange underlying f-
structure, meaning something like the Greens pos-
sess the SPD in a criticising manner.
(4) Die
The
Gru?nen
Greens
haben
have
die
the
SPD
SPD
kritisiert.
criticised.
?The Greens have criticised the SPD?
3.3 Interim Conclusion
This section has illustrated an exemplary dilemma
for parsing systems that aim broad-coverage and
linguisitically motivated analyses at the same time.
Since these systems need to explicitly address and
represent ambiguities that purely statistical sys-
tems are able to conflate or ignore, their perfor-
mance is not automatically improved by adding
a specific rule for a specific phenomenon. Inter-
estingly, the negative consequences affecting the
quantitative (statistical) as well as the qualitative
(linguistic) dimension of the grammar seem to be
closely related: The overgenerating adverb con-
version rule empirically leads to linguistically un-
motivated analyses which causes problems for the
disambiguation component. In the rest of the pa-
per, we show how the adverbial analysis of partici-
ples can be reasonably constrained on the basis of
a lexical resource induced from a parallel corpus.
4 Cross-Lingual Induction of Adverbial
Participles
The intuition of the cross-lingual induction ap-
proach is that adverbial participles can be easily
extracted from parallel corpora since in other lan-
guages (such as English or French) adverbs are
often morphologically marked and easily labelled
by statistical PoS taggers. As an example, con-
sider the sentence in (5), extracted from Europarl,
where the German participle versta?rkt is translated
by unambiguous adverbs in English and French
(increasingly and davantage).
(5) a. Nach der Osterweiterung stehen die Zeichen
versta?rkt auf Liberalisierung.
b. Following enlargement towards the east, the emphasis
is increasingly on liberalisation.
37
CS 1: ROOT:2543
CProot[std]:2536
DP[std]:984
DPx[std]:981
D[std]:616
die:34
NP:773
N[comm]:717
NAdj:714
Gr?nen:85
Cbar:2506
Vaux[haben,fin]:1054
haben:159
VP[v,part]:2080
DP[std]:1856
DPx[std]:2321
D[std]:1180
die:204
NP:1720
N[comm]:284
SPD:257
VC[v,part]:2009
V[v,part]:1593
Vx[v,part]:1590
kritisiert:348
PERIOD:418
.:410
CS 2: ROOT:2543
CProot[std]:2536
DP[std]:984
DPx[std]:981
D[std]:616
die:34
NP:773
N[comm]:717
NAdj:714
Gr?nen:85
Cbar:2506
V[v,fin]:2494
Vx[v,fin]:2491
haben:159
DP[std]:1856
DPx[std]:2321
D[std]:1180
die:204
NP:1720
N[comm]:284
SPD:257
ADVP[std]:1493
V[v,-infl]:1491
Vx[v,-infl]:1488
kritisiert:348
PERIOD:418
.:410
Figure 1: Two c-structures for sentence (4), obtained by the grammar All-Part-Adv - CS1 is correct, CS2
is semantically very strange
c. Apre`s l? e?largissement a` l? Est, la tendance sera da-
vantage a` la libe?ralisation.
In the following, we describe experiments on
Europarl where we automatically extract and fil-
ter adverbially translated German participles.
4.1 Data
We base our experiments on the German, En-
glish, French and Dutch part of the Europarl cor-
pus. We automatically word-aligned the German
part to each of the others with the GIZA++ tool
(Och and Ney, 2003). Note that, due to diver-
gences in sentence alignment and tokenisation,
the three word-alignments are not completely syn-
chronised. Moreover, each of the 4 languages has
been automatically PoS tagged using the TreeTag-
ger (Schmid, 1994). In addition, the German and
English parts have been parsed with MaltParser
(Nivre et al, 2006).
Since we want to limit our investigation to those
participles that are not already recorded as lexi-
calised adjective or adverb in the DMOR morphol-
ogy, we first have to generate the set of participle
candidates from the tagged Europarl data. We ex-
tract all distinct words (types) from the German
part that have been either tagged as ADJD (pred-
icative or adverbial modifier), 6089 types in total,
or as VVPP (past perfect participle), 5469 types
in total. We intersect this set of potential partici-
ples with the set of DMOR participles that only
have a verbal lemma. The resulting intersection
(5054 types in total) constitutes the set of all Ger-
man participles in Europarl that are not recorded
as lexicalised in the DMOR morphology .
Given the participle candidates, we now ex-
tract the set of sentences that exhibit a word
alignment between a German participle and an
English, French or Dutch adverb. The extrac-
tion yields 5191 German-English sentence pairs,
2570 German-French, and 4129 German-Dutch
sentence pairs. The German-English pairs com-
prise 1070 types of potentially adverbial partici-
ples. The types found in the German-French and
German-Dutch part form a proper subset of the
types extracted from the German-English pairs.
Thus, the additional languages will not increase
the recall of the induction. However, we will show
that they are extremely useful for filtering incor-
rect or uninteresting participle alignments.
For data exploration and evaluation, we anno-
tated 300 participle alignments out of the 5191
German-English sentences as to whether the En-
glish adverbial really points to an adverbial par-
ticiple on the German side (and/or the word-
alignment was correct). Throughout the entire set
of annotated sentences, this ratio between the par-
allel cases (where an English adverbial correctly
indicates a German adverbial) and all adverbially
translated participles is at about 30%. This means
that if we base the induction on word-alignments
alone, its precision would be relatively low.
The remaining 60% translation pairs do not only
reflect word alignment errors, but also cases where
we find a proper participle in the German sentence
that has a correct adverbial translation for other
reasons. A typical configuration is exemplified in
(6) where the German main verb vorlegen is trans-
lated as the verb-adverb combination put forward.
(6) a. Wir haben eine Reihe von Vorschla?gen vorgelegt.
b. We have put forward a number of proposals.
These sentence pairs are cases of free or para-
38
Figure 2: Type/token ratio for adverbial participles
phrasing translations. Ideally, we want our induc-
tion method to filter such type of configurations.
The 300 annotated sentences comprise 121 to-
ken instances of German adverbially used partici-
ples that have an adverbial translation in English.
However, these 121 tokens reduce to 24 partici-
ple types. The graph in Figure 2 displays the
type/token-ratio for an increasing number of in-
stances in our gold standard. The curve exponen-
tially decays from about 10 tokens onward and
suggests that from about 30 tokens onward, the
number of unseen types is relatively low. This can
be interpreted as evidence in favour of the hypoth-
esis that the number of adverbially used participles
is actually fairly limited and can be integrated into
the grammar in terms of a hard-coded resource.
4.2 Filtering
The data analysis in the previous section has
shown that approximately one third of the English
adverb alignments actually point to an adverbial
participle on the German side. This means that we
have to rigorously filter the data that we extract on
the basis of word-alignments in order to obtain a
high quality resource for our grammar. In this sec-
tion, we will investigate several filtering methods
and evaluate them on our annotated sentence pairs.
Frequency-based filtering As a first attempt,
we filtered the non-parallel cases in our set of
participle-adverb translations by means of the rel-
ative frequency of the adverb translations. For
each participle candidate, we counted the number
of tokens that exhibit an adverbial alignment on
the English side, and divided this number by its
total number of occurrences in the German Eu-
roparl. The best f-score of the ADV-FREQ filter
(see Table 4) is achieved by the 0.05 threshold, but
generally, the precision of the frequency filters is
too low for high-quality resource induction. The
reason for the poor performance of the frequency-
based filters seems to be that some German verbs
are systematically translated as verb - adverb com-
binations as in (6). For these participles, the rel-
ative frequency of adverbial alignments is not a
good indicator for their adverbial use in German.
Multilingual Filtering Similar to filters used
in annotation projection where noisy word-
alignments are ?cleaned? with the help of addi-
tional languages (Bouma et al, 2008), we have
implemented a filter that only selects those par-
ticiples as adverbials which also exhibit a certain
amount of adverbial translations in the French and
Dutch Europarl. We count the total number of
adverbial translations of a given participle on the
French side and divide it by the number of English
adverbial translations. For French, the best f-score
is achieved at a threshold of >0.1 (filter FR). For
Dutch, the best f-score is achieved at a threshold
of >0.05 (filter NL). The exact precision and re-
call values are given in Table 4.
Syntax-based Filtering The intuition behind
the filters presented in this section is that adver-
bial translations which are due to cross-lingual di-
vergences can be identified on the basis of their
syntactic contexts. Information about these con-
texts can be extracted from the dependency anal-
yses produced by MaltParser for the German and
English data. On the German side, we want to ex-
clude those participle instances for which the Ger-
man parser has found an auxiliary head, since this
configuration points to a normal partciple context
in German. The filter is called G-HEAD in Table
4. It filters all types which have an auxiliary head
in more than 40% of their adverbial translation
configurations. On the English side, we exclude
all translations where the adverb has a verbal head
which is also aligned to the German partciple. The
filter is called E-HEAD in Table 4. It excludes all
participle types which exhibit the E-HEAD con-
figuration in more than 50% of the cases.
39
filter prec. rec. f-sc.
ADV-FREQ 0.38 0.75 0.51
FR 0.48 0.76 0.58
NL 0.33 0.73 0.45
G-HEAD 0.65 0.8 0.71
E-HEAD 0.4 0.8 0.53
COMBINED-1 0.61 0.8 0.69
COMBINED-2 0.86 0.76 0.81
Table 4: Performance of filters on the set of gold
adverbial participle types
Combined Token-level Filtering So far, we
have shown that multilingual and syntactic in-
formation is useful to filter non-parallel partici-
ple translations. We have found that the pre-
cision of the syntactic filters can still be in-
creased by combining it with the multilingual fil-
ters. COMBINED-1 in Table 4 refers to the filter
which only includes those participle types which
have at least one adverbial translation on the En-
glish target side such that (i) the adverbial trans-
lation is paralleled on the French or Dutch target
side for the same German participle token and (ii)
the German participle token does not have an aux-
iliary head. If we combine this token-level filter-
ing with the syntactic type-level filtering G-HEAD
and E-HEAD (the filter called COMBINED-2 in
Table 4), the precision increases by about 25%
with little loss in recall.
4.3 Analysis
Based on the filtering techniques described in the
previous section, we can finally induce a list of 46
German adverbial participles from Europarl. The
fact that this participle class seems fairly delimited
in our data raises the theoretical question whether
the adverb conversion is licensed by any linguistic,
i.e. lexical-semantic, properties of these partici-
ples. However, we observe that the automatically
induced list comprises very diverse types of ad-
verbs, as well as very distinct types of underlying
verbs. Thus, besides adverbs that clearly modify
events (see sentence (5)), we also found adverbs
that are more likely to modify adjectives (sentence
(7-a)), or propositions (sentence (7-b)).
(7) a. Es ist eine verdammt gefa?hrliche Situation.
?It is a damned dangerous situation.?
b. Wir machen einen Bericht u?ber den Bericht des Rech-
nungshofes , zugegeben.
?We are drafting a report about the report of the Court
of Auditors , admittedly.?
A more fine-grained classification and analysis
of adverbial participles is left for future research.
5 Grammar-based Evaluation
The resource of participles licensing adverbial use,
whose induction was described in the previous
section, can be straightforwardly integrated into
the German LFG. By explicitly enumerating the
participles in the adverb lexicon, the grammar can
apply the standard adverb macros to them. To as-
sess the effect of the filtering, we built two new
versions of the grammar: (i) Euro-Part-Adv, its ad-
verb lexicon comprises all adverbially translated
participles found in Europarl (1091 types) and (ii)
Filt-Part-Adv, its adverb lexicon comprises only
the syntactically and multilingually filtered par-
ticiples found in Europarl (46 types).
Although we have seen in section 3.2 that adver-
bial participles do not seem to occur in the TIGER
heldout set, we also know that it is important to
assess the effect of ambiguity rate on the overall
grammar performance. Therefore, we computed
the accuracy of the most probable parses produced
by the Euro-Part-Adv and Filt-Part-Adv on the
heldout set. As is shown in Table 5, the Euro-Part-
Adv performs significantly worse than Filt-Part-
Adv. This suggests that the non-filtered participle
resource is not constrained enough and still pro-
duces a lot of spurious ambiguites that mislead the
disambiguation component. The coverage values
in Table 6 further corroborate the observation that
the unfiltered participle resource behaves similar
to the unrestricted adverb conversion in All-Part-
Adv (see Section 3.2). The coverage of the filtered
vs. the unfiltered version on the development set is
identical, however the timeouts in Euro-Part-Adv
increase by 17% and parsing time by 8%.
By contrast, there is no significant difference
in f-score between the No-Part-Adv version pre-
sented in Section 3.2 and the Filt-Part-Adv ver-
sion. Thus, we can, at least, assume that the fil-
tered participles resources has restricted the mas-
sive overgeneration caused by the general adverb
conversion rule such that the overall performance
of the original grammar is not negatively affected.
To evaluate the participle resource as to whether
it could have a positive qualtitative effect on pars-
ing TIGER at all, we built a specialised test-
suite which comprises only sentences containing
a non-lexicalised participle, which has an adver-
bial translation in Europarl and is tagged as ADJD
40
Grammar Prec. Rec. F-Sc. Time
in sec
Euro-Part-Adv 82.32 75.78 78.91 701
Filt-Part-Adv 84.12 78.2 81.05 665
Table 5: Evaluation on the TIGER heldout set, 371
sentences total
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
Euro-Part-Adv 4304 588 107 7359
Filt-Part-Adv 4304 604 91 6791
Table 6: Performance on the TIGER development
set (sentences 1-5000), 4999 sentences total
in TIGER. The sentences were extracted from the
whole TIGER corpus yielding a set of 139 sen-
tences. In this quality-oriented evaluation, we
only contrast the No-Part-Adv version with the
filtered Filt-Part-Adv version since the unfiltered
version leads to worse overall performance. As
can be seen in Table 7, the No-Part-Adv can only
completely cover 36% of the specialised testsuite
which is much lower than its average complete
coverage on the development set (86%). This sug-
gests that a substantial number of the extracted
ADJD participles are actually used as adverbial in
the specialised testsuite.
Similar to the qualitative evaluation procedure
in 3.2, we manually evaluated a random sample of
20 sentences covered by Filt-Part-Adv and not by
No-Part-Adv as to whether they contain an adver-
bial participle that has been correctly recognised.
This was the case for 90% of the sentences, the
remaining 2 sentences were cases of secondary
predications. An example of a relatively simple
TIGER sentence that the grammar could not cover
in the No-Part-Adv version is given in (8).
(8) Die Anti-Baby-Pillen stehen im Verdacht , vermehrt
Thrombosen auszulo?sen.
?The birth control pill is suspected to increasingly cause
thromboses.?
We also manually checked a random sample of
Grammar Parsed
Sent.
Starred
Sent.
Time-
outs
Time
in sec
No-Part-Adv 50 77 12 427
Filt-Part-Adv 92 39 8 366
Table 7: Performance on the specialised TIGER
test set, 139 sentences total
20 sentences that the Filt-Part-Adv grammar could
not cover, in order to see whether the grammar sys-
tematically misses certain cases of adverbial par-
ticiples. In this second random sample, the per-
centage of sentences containing a true adverbial
participle was again 90%. The grammar could
not correctly analyse these because of their spe-
cial syntax that is not covered by the general ad-
verb macro (or, of course, because of difficult con-
structions not related to adverbial participles). An
example for such a case is given in (9).
(9) Transitreisen junger Ma?nner vom Gaza-Streifen ins
Westjordanland und umgekehrt sind nicht gestattet.
?Transit travels from the Gaza Strip to the West Bank and
vice versa are not allowed for young men.?
The high proportion of true adverbial participle
instances in our specific testsuite suggests that the
data we induced from Europarl largely carries over
to TIGER (despite genre differences, for instance)
and constitutes a generally useful resource. Thus,
we can not only say that the filtered participle re-
source has no negative effect on the overall per-
formance of the German LFG, but also extends its
coverage for a less frequent phenomenon in a lin-
guistically precise way.
6 Conclusion
We have proposed an empirical account for detect-
ing adverbial participles in German. Since this
category is usually not annotated in German re-
sources and hard to describe in theory, we based
our method on multilingual parallel data. This
data suggests that only a fairly limited class of par-
ticiples actually undergo the conversion to adverbs
in free text. We have described a set of linguisti-
cally motivated filters which are necessary to in-
duce a high-precision resource for adverbial par-
ticiples from parallel data. This resource has been
integrated into the German LFG grammar. In con-
trast to the version of the grammar which does not
restrict the participle - adverb conversion, the re-
stricted version produces less spurious ambigui-
ties which leads to better f-score on gold standard
data. Moreover, by manually evaluating a spe-
cialised data set, we have established that the re-
stricted version also extends the coverage and pro-
duces the correct analyses which can be used for
further linguistic study.
41
References
Tanja Becker. 2001. DMOR: Handbuch. Technical
report, IMS, University of Stuttgart.
Gerlof Bouma, Jonas Kuhn, Bettina Schrader, and
Kathrin Spreyer. 2008. Parallel LFG Grammars
on Parallel Corpora: A Base for Practical Trian-
gulation. In Miriam Butt and Tracy Holloway
King, editors, Proceedings of the LFG08 Confer-
ence, pages 169?189, Sydney, Australia. CSLI Pub-
lications, Stanford.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The tiger
treebank. In Proceedings of the Workshop on Tree-
banks and Linguistic Theories.
Joan Bresnan. 2000. Lexical-Functional Syntax.
Blackwell, Oxford.
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
Parallel Grammar Project.
Aoife Cahill, John T. Maxwell III, Paul Meurer, Chris-
tian Rohrer, and Victoria Rose?n. 2007. Speeding
up LFG Parsing using C-Structure Pruning . In Col-
ing 2008: Proceedings of the workshop on Grammar
Engineering Across Frameworks, pages 33 ? 40.
Stefanie Dipper. 2003. Implementing and Document-
ing Large-Scale Grammars ? German LFG. Ph.D.
thesis, Universita?t Stuttgart, IMS.
Anette Frank, Tracy Holloway King, Jonas Kuhn, and
John T. Maxwell. 2001. Optimality Theory Style
Constraint Ranking in Large-Scale LFG Grammars
. In Peter Sells, editor, Formal and Empirical Issues
in Optimality Theoretic Syntax, page 367?397. CSLI
Publications.
Wilhelm Geuder. 2004. Depictives and transparent ad-
verbs. In J. R. Austin, S. Engelbrecht, and G. Rauh,
editors, Adverbials. The Interplay of Meaning, Con-
text, and Syntactic Structure, pages 131?166. Ben-
jamins.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT Summit 2005.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
Maltparser: A data driven parser-generator for de-
pendency parsing. In Proc. of LREC-2006.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Stefan Riezler, Tracy Holloway King, Ronald M. Ka-
plan, Richard Crouch, John T. Maxwell, and Mark
Johnson. 2002. Parsing the Wall Street Journal us-
ing a Lexical-Functional Grammar and Discrimina-
tive Estimation Techniques . In Proceedings of ACL
2002.
Christian Rohrer and Martin Forst. 2006. Improving
coverage and parsing quality of a large-scale LFG
for German. In Proceedings of LREC-2006.
Anne Schiller, Simone Teufel, and Christine Thielen.
1995. Guidelines fuer das Tagging deutscher Tex-
tkorpora mit STTS. Technical report, IMS, Univer-
sity of Stuttgart.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora. In
Proceedings of HLT 2001, First International Con-
ference on Human Language Technology Research.
42
