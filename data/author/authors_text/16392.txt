INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 100?104,
Utica, May 2012. c?2012 Association for Computational Linguistics
Working with Clinicians to Improve a Patient-Information NLG System
Saad Mahamood and Ehud Reiter
Department of Computing Science
University of Aberdeen
Aberdeen, Scotland, United Kingdom
{s.mahamood, e.reiter}@abdn.ac.uk
Abstract
NLG developers must work closely with do-
main experts in order to build good NLG sys-
tems, but relatively little has been published
about this process. In this paper, we describe
how NLG developers worked with clinicians
(nurses) to improve an NLG system which
generates information for parents of babies in
a neonatal intensive care unit, using a struc-
tured revision process. We believe that such a
process can significantly enhance the quality
of many NLG systems, in medicine and else-
where.
1 Introduction
Like other artificial intelligence (AI) systems, most
Natural Language Generation (NLG) systems incor-
porate domain knowledge (and domain communica-
tion knowledge (Kittredge et al, 1991)), either im-
plicitly or explicitly. Developers must work with do-
main experts to acquire such knowledge. Also like
software systems in general, applied NLG systems
must meet domain and application specific require-
ments in order to be useful; these again must come
from domain experts.
Since very few domain experts are familiar with
NLG, it is usually extremely difficult to acquire a
complete set of requirements, domain knowledge,
and domain communication knowledge at the be-
ginning of an NLG project. Especially, if no pre-
existing ?golden standard? corpus of domain texts
exists. Indeed, in many cases domain experts may
find it difficult to give detailed requirements and
knowledge until they can see a version of the NLG
system working on concrete examples. This sug-
gests that an iterative software development method-
ology should be used, where domain experts re-
peatedly try out an NLG system, revise underly-
ing domain (communication) knowledge and re-
quest changes to the system?s functionality, and wait
for developers to implement these changes before re-
peating the process.
We describe how we carried out this process on
BabyTalk-Family (Mahamood and Reiter, 2011), an
NLG system which generates summaries of clini-
cal data about a baby in a neonatal intensive care
unit (NICU), for the babys parents. Over a 6 month
period, this process enabled us to improve an ini-
tial version of the system (essentially the result of
a PhD) to the point where the system was good
enough to be deployable live in a hospital context.
We also describe how the feedback from the clini-
cians changed over the course of this period.
2 Previous Research
Reiter et al (2003) describe a knowledge acquisi-
tion strategy for building NLG systems which in-
cludes 4 stages: directly asking domain experts for
knowledge, structured knowledge acquisition activ-
ities with experts, corpus analysis, and revision with
experts. In this paper we focus on the last of these
phases, revision with experts. Reiter et al describe
this process in high-level qualitative terms; in this
paper our goal is to give a more detailed description
of the methodology, and also concrete data about
the comments received, and how they changed over
time.
The most similar previous work which we are
100
aware of is Williams and Reiter (2005), who de-
scribe a methodology for acquiring content selection
rules from domain experts, which is also based on
an iterative refinement process with domain experts.
Their process is broadly similar to what we describe
in this paper, but they focus just on content selection,
and do not give quantitative data about the revision
process.
In the wider software engineering community,
there has been a move to iterative development
methodologies, instead of the classic ?waterfall?
pipeline. In particular, agile methodologies (Mar-
tin, 2002) are based on rapid iterations and frequent
feedback from users; we are in a sense trying to ap-
ply some ideas from agile software engineering to
the task of building NLG systems. Our methodology
also can be considered to be a type of user-centred
design (Norman and Draper, 1986).
3 BabyTalk-Family
BabyTalk-Family (Mahamood and Reiter, 2011)
generates summaries of clinical data about babies in
a neonatal intensive care unit (NICU) for parents.
For more details about BabyTalk-Family, including
example outputs, please see Mahamood and Reiter.
BabyTalk-Family (BT-Family) was initially de-
veloped as part of a PhD project (Mahamood, 2010).
As such it was evaluated by showing output texts
(based on real NICU data) to people who had previ-
ously had a baby in NICU; the texts did not describe
the subject?s own baby (i.e., the subjects read texts
which summarised other people?s babies; they had
no previous knowledge of these babies). BT-Family
was also not rigorously tested from a software qual-
ity assurance perspective. The work presented here
arose from a followup project whose goal was to de-
ploy BT-Family live in a NICU, where parents who
currently had babies in NICU could read summaries
of their baby?s clinical data. Such a deployment re-
quired generated texts to be of much higher quality
(in terms of both content and language); we achieved
this quality using the revision process described in
this paper.
BT-Family is part of the BabyTalk family of sys-
tems (Gatt et al, 2009). All BabyTalk systems use
the same input data (NICU patient record), but they
produce different texts from this data; in particular
BT45 (Portet et al, 2009) produces texts which sum-
marise short periods to help real-time decision mak-
ing by clinicians, and BT-Nurse (Hunter et al, 2011)
produces summaries of 12 hours of data for nurses,
to support shift handover. BT-Nurse was also de-
ployed in the ward, to facilitate evaluation by nurses
who read reports about babies they were currently
looking after. To support this deployment, the BT-
Nurse developers spent about one month carrying
out a revision process with clinicians, in a somewhat
unstructured fashion. One outcome of the BT-Nurse
evaluation was that the system suffered because the
revision process was neither sufficiently well struc-
tured nor long enough; this was one of the motiva-
tions for the work presented here.
4 Revision Methodology
The revision process was carried out at the Neona-
tal Intensive Care Unit in conjunction with the hos-
pital Principal Investigator (PI) of our project and
two research nurses. We started with an initial fa-
miliarisation period for the nurses (the hospital PI
was already familiar with BT-Family), where we ex-
plained the goals of the project and asked the nurses
to examine some example BT-Family texts, which
we then discussed.
After the nurses were familiar with the project, we
conducted a number of revision cycles. Each cycle
followed the following procedure:
1. The clinicians (either the hospital PI or the research
nurses) choose between 3 and 11 scenarios (one
day?s worth of data from one baby). These scenar-
ios were chosen to test the system against a diverse
range of babies in different clinical conditions; sce-
narios were also chosen to check whether issues
identified in previous cycles had been addressed.
2. The nurses examined the texts generated by BT-
Family for the chosen scenarios. They both directly
commented on the texts (by writing notes on hard-
copy), and also (in some cases) edited the texts to
show what they would have liked to see.
3. The NLG developers analysed the comments and
revised texts; distilled from these a list of specific
change requests; prioritised the change requests on
the basis of importance and difficulty; and imple-
mented as many change requests as possible given
the time constraints of the cycle.
101
Figure 1: Example of marked up text annotated by a research nurse. The baby?s forename has been blacked out.
4. The scenarios were rerun through the updated sys-
tem, and the NLG developers checked that the is-
sues had been addressed. Clinicians did not usually
look at the revised texts, instead they would check
that the issues had been resolved in new scenarios
in the next cycle.
The above process was carried out 14 times over
a 6 month period with each cycle taking on average
11.28 days. A research fellow (Saad Mahamood)
was assigned to implement these changes working
full-time over this 6 month period. The length be-
tween each revision cycle was variable due to the
availability of the domain experts and the variable
level of complexity to implement identified changes
to the BT-Family system.
Figure 1 shows a extract from an early BT-Family
text generated in July 2011 that needed a lot of re-
vision. In this example, the nurse has identified the
following issues:
? Incorrect pronoun: He instead of His.
? Unnecessary phrase: Because XXXX was born ear-
lier than expected.
? Change in tense: is being instead of has been.
? Change in wording of time phrase: In the last 24
hours instead of Since yesterday.
? Incorrect content: incubator oxygen has increased,
it is not stable.
? Grammar mistake: were instead of was.
? Change in content: some (frequency) instead of
moderate (severity).
? Change in wording: self-resolving instead of self-
resolved.
5 Analysis of Feedback over Time
We extracted hand-written comments on BT-Family
texts (of the type shown in Figure 1) and annotated
the comments using a scheme similar to that used
by Hunter et al(2011) for analysing comments on
BT-Nurse texts. Two annotators were used with the
first annotating the entire set of 75 reports using a
pre-agreed classification scheme. The classification
scheme that was used consisted of three types of
categories: Content Errors, Language Errors, and
Comments with each containing specific categori-
sation labels as shown in Table 1. Content Errors
labels were used to annotate comments when there
were content based mistakes. Language error labels
were used to categorise the different types of lan-
guage based mistakes. Finally, comment labels were
used to classify different types of comments made
by the nurses. The second annotator annotated a
random partial subset of the reports independently
to check for the level of agreement between the first
and second annotators. By using Cohen?s kappa co-
efficient we found the level of inter-annotator agree-
ment was k=0.702.
Content errors were the most predominate type of
annotation (50.54%), followed by Language errors
(25.18%), and comments (24.27%). Positive com-
ments were unusual (only 5 in total), because the
clinicians were explicitly asked to focus on prob-
102
Content Errors Language Errors Comment
unnecessary (44.20%) spelling mistake (8.14%) positive (3.75%)
missing (28.26%) grammar mistake (22.22%) negative (0.75%)
wrong (22.82%) incorrect tense/aspect (18.51%) no agreement (1.50%)
should-be-elsewhere (4.71%) different word(s) required (35.55%) reformulation (12.78%)
unnecessary words (3.70%) observation (66.16%)
precision/vagueness (11.85%) question (15.03%)
Table 1: List of annotation categories and the labels within each category that was used. The frequency for each label
in it?s category is given in brackets.
Month Number of Avg. scenarios Avg. number of Avg. number of Avg. number of
revision cycles per cycle content errors language errors comments
June 1 5 1.8 4.2 1.2
July 2 8 4.93 5.5 1.87
August 2 5 4.8 4 5.8
September 2 4 6.37 8.5 4
October 3 7 2.95 1.57 6.42
November 3 5 1.6 1.6 3.6
December 1 5 0.8 0 0.4
Overall 14 5.7 6.92 3.62 3.32
Table 2: Summary table showing the average number of content errors, language errors, and comments per scenario.
lems. Table 2 shows statistics for the revision pro-
cess per month; the process started in the second half
of June, and ended in the first half of December.
From a qualitative perspective, the data suggests
that there were two phases to the revision process.
In the first phase (June to September), the number
of content and language errors in fact went up. We
believe this is because during this phase we were
adding around 16 new types of content to the re-
ports (based on requests from the clinicians) as well
as fixing problems with existing content (of the sort
shown in Figure 1); this additional content itself of-
ten needed to be revised in subsequent revision cy-
cles, which increased the error count for these cy-
cles. These additional errors from the addition of
new content may of arisen due to the complexity
and variation of clinical data. Additionally, our 3-
year old anonymised test set of clinical data may
not of been as representative as the live data due
to changes/additions in patient data. In the sec-
ond phase (October to December), requests for new
content diminished (around 4 requests) and we fo-
cused on fixing problems with existing content; in
this phase, the number of content and language er-
rors steadily decreased (that is, the system improved
from the clinician?s perspective), until we reached
the point in mid December when the clinicians were
satisfied that the quality of BT-Family texts was con-
sistently good from their perspective.
When the revision process ended, we started eval-
uating BT-Family texts directly with parents, by
showing parents texts about their babies. This work
is ongoing, but initial pilot results to date indicate
that parents are very happy with the texts, and do
not see major problems with either the language or
the content of the texts.
6 Discussion
The revision process had a major impact on the qual-
ity of BT-Family texts, as perceived by the clini-
cians. At the start of the process (June 2011), the
texts had so many mistakes that they were unusable;
the clinicians would not allow us to show parents
BT-Family texts about their babies, even in the con-
text of a pilot study. After 14 revision rounds over a
6 month period, text quality had improved dramati-
cally, to the point where clinicians allowed us to start
working directly with parents to get their feedback
and comments on BT-Family texts.
The fact that a new set of scenarios was used in
every iteration of the revision process was essen-
103
tial to giving clinicians confidence that text quality
would be acceptable in new cases; they would not
have had such confidence if we had focused on im-
proving the same set of texts.
The revision process took 6 months, which is a
considerable amount of time. This process would
have been shorter if BT-Family had undergone a
more rigorous testing and quality assurance (QA)
process ahead of time, which would for example
have addressed grammar mistakes, and (more im-
portantly) tested the system?s handling of boundary
and unusual cases. The process probably could also
have been further shortened in other ways, for ex-
ample by performing 3 revision cycles per month
instead of 2.
However, one reason the process took so long was
that the functionality of the system changed; as the
clinicians got a better idea of what BT-Family could
do and how it could help parents, they requested
new features, which we tried to add to the system
whenever possible. We also had to accommodate
changes in the input data (patient record), which
reflected changes in NICU procedures due to new
drugs, equipment, procedures, etc. So we were not
just tweaking the system to make it work better, we
were also enhancing its functionality and adapting it
to changing input data, which is a time consuming
process.
7 Conclusion
We have presented a methodology for improving
the quality and appropriateness of texts produced by
applied NLG systems, by repeatedly revising texts
based on feedback from domain experts. As we have
show in the results, the process is a time consuming
one, but appears to be quite effective in bringing an
NLG system to the required level of quality in a clin-
ical domain.
Acknowledgements
This work is funded by the UK Engineering and Physical Sciences
Council (EPSRC) and Digital Economy grant EP/H042938/1. Many
thanks to Dr. Yvonne Freer, Alison Young, and Joanne McCormick of
the Neonatal Intensive Care Unit at Simpson Centre for Reproductive
Health, Royal Infirmary of Edinburgh Hospital, for their help.
References
Albert Gatt, Francois Portet, Ehud Reiter, Jum Hunter,
Saad Mahamood, Wendy Moncur, and Somayajulu
Sripada. 2009. From data to text in the neonatal in-
tensive care unit: Using NLG technology for decision
support and information management. AI Communi-
cations, 22(3):153?186.
James Hunter, Yvonne Freer, Albert Gatt, Ehud Reiter,
Somayajulu Sripada, Cindy Sykes, and Dave Westwa-
ter. 2011. BT-Nurse: Computer generation of natu-
ral language shift summaries from complex heteroge-
neous medical data. Journal of the Americal Medical
Informatics Association, 18(5):621?624.
Richard Kittredge, Tanya Korelsky, and Owen Rambow.
1991. On the need for domain communication lan-
guage. Computational Intelligence, 7(4):305?314.
Saad Mahamood and Ehud Reiter. 2011. Generating
affective natural language for parents of neonatal in-
fants. In Proceedings of the 13th European Work-
shop on Natural Language Generation, pages 12?21,
Nancy, France, September. Association for Computa-
tional Linguistics.
Saad Mahamood. 2010. Generating Affective Natural
Language for Parents of Neonatal Infants. Ph.D. the-
sis, University of Aberdeen, Department of Comput-
ing Science.
Richard Martin. 2002. Agile Software Development,
Principles, Patterns, and Practices.
Donald A. Norman and Stephen W. Draper. 1986.
User Centered System Design; New Perspectives on
Human-Computer Interaction. L. Erlbaum Associates
Inc., Hillsdale, NJ, USA.
Franc?ois Portet, Ehud Reiter, Albert Gatt, Jim Hunter,
Somayajulu Sripada, Yvonne Freer, and Cindy Sykes.
2009. Automatic generation of textual summaries
from neonatal intensive care data. Artificial Intelli-
gence, 173(7-8):789?816.
Ehud Reiter, Somayajulu Sripada, and Roma Robertson.
2003. Acquiring correct knowledge for natural lan-
guage generation. Journal of Artificial Intelligence
Research, 18:491?516.
Sandra Williams and Ehud Reiter. 2005. Deriving con-
tent selection rules from a corpus of non-naturally oc-
curring documents for a novel NLG application. In
Proceedings of Corpus Linguistics workshop on using
Corpora for NLG.
104
Proceedings of the 8th International Natural Language Generation Conference, pages 123?127,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Generating Annotated Graphs using the NLG Pipeline Architecture
Saad Mahamood, William Bradshaw and Ehud Reiter
Arria NLG plc
Aberdeen, Scotland, United Kingdom
{saad.mahamood, william.bradshaw, ehud.reiter}@arria.com
Abstract
The Arria NLG Engine has been extended
to generate annotated graphs: data graphs
that contain computer-generated textual
annotations to explain phenomena in those
graphs. These graphs are generated along-
side text-only data summaries.
1 Introduction
Arria NLG1 develops NLG solutions, primarily in
the data-to-text area. These solutions are NLG
systems, which generate textual summaries of
large numeric data sets. Arria?s core product is
the Arria NLG Engine,2 which is configured and
customised for the needs of different clients.
Recently Arria has extended this core engine
so that it can automatically produce annotated
graphs, that is, data graphs that have textual an-
notations explaining phenomena in those graphs
(see example in Figure 1). This was developed af-
ter listening to one of our customers, whose staff
manually created annotated graphs and found this
process to be very time-consuming. The anno-
tated graph generation process is integrated into
the NLG pipeline, and is carried out in conjunc-
tion with the generation of a textual summary of a
data set.
In this short paper we summarise the relevant
research background, and briefly describe what we
have achieved in this area.
2 Background: Multimodality and NLG
Rich media such as web pages and electronic doc-
uments typically include several modalities in a
given document. A web page, for example, can
contain images, graphs, and interactive elements.
Because of this there has been an interest within
1Arria NLG plc (https://www.arria.com)
2For more information see: https://www.arria.
com/technology-A300.php
the NLG community in generating multimodal
documents. However, basic questions remain as
how best to combine and integrate multiple modal-
ities within a given NLG application.
2.1 Annotated Graphics
Sripada and Gao (2007) conducted a small study
where they showed scuba divers different possi-
ble outputs from their ScubaText system, including
text-only, graph-only and annotated graphs. They
found that divers preferred the annotated graph
presentation. The ScubaText software could not in
practice produce annotated graphs for arbitrary in-
put data sets and automatically set the scale based
on detected events, so this was primarily a study
of user preferences.
McCoy and colleagues have been developing
techniques to automatically generate textual sum-
maries of data graphics for visually impaired users
(Demier et al., 2008). This differs from our work
because their goal is to replace the graph, whereas
our goal is to generate an integrated text/graphics
presentation.
There were several early systems in the 1990s
(Wahlster et al., 1993; Feiner and McKeown,
1990, for example), which generated integrated
presentations of figures and texts, but these sys-
tems focused on annotating static pictures and dia-
grams, not data graphics. The WIP system, which
combined static diagram images and text, used a
deep planning approach to produce tightly inte-
grated multimodal documents; it is not clear how
robustly this approach handled new data sets and
contexts.
2.2 Embodied Conversational Agents
In recent years the challenge of combining mul-
tiple modalities such as text, speech, and/or ani-
mation has been addressed in the context of Em-
bodied Conversational Agents or ECAs. One ex-
ample is the NECA system (Krenn et al., 2002).
123
It allowed two embodied agents to converse with
each other via spoken dialogue while being able
to make gestures as well. From an architectural
perspective, NECA used a pipeline architecture in
some ways similar to the standard NLG data-to-
text pipeline (Reiter and Dale, 2000). Document
Planning is handled by the Scene Generator, which
selects the dialogue content. The ?Multi-modal
NLG? (M-NLG) component handles Microplan-
ning and Surface Realisation, and also deals with
specifying gestures, mood, and information struc-
ture. Thus the textual output generated by the sur-
face realiser in the NECA M-NLG component is
annotated with metadata for other modalities. In
particular, information on gestures, emotions, in-
formation structure, syntactic structure and dia-
logue structure (Krenn et al., 2002) are also in-
cluded to help inform the speech synthesis and
gesture assignment modules.
2.3 Background: Psychology
The question of whether information is best pre-
sented in text or graphics is in principle largely one
for cognitive psychology. Which type of presenta-
tion is most effective, and in which context? The
answer of course depends on the communicative
goal, the type of data being presented, the type of
user, the communication medium and other con-
textual factors.
In particular, a number of researchers (Petre,
1995, for example) have pointed out that graphical
presentations require expertise to interpret them
and hence may be more appropriate for experi-
enced users than for novices. Tufte (1983) points
out that statistical graphs can be very misleading
for people who are not used to interpreting them.
Alberdi et al (2001) report on a number of psy-
chological studies on effectiveness of data visual-
isations which were performed with clinicians in
a Neonatal Intensive Care Unit (NICU). At a high
level, these studies found that visualisations were
less effective and less used than had been hoped.
Detailed findings include the following:
? Although consultants, junior doctors, and
nurses all claimed in interviews to make
heavy use of the computer system which
displayed visualisations, when observed on-
ward only senior consultants actually did so;
junior doctors and nurses rarely looked at the
computer screen.
? Senior consultants were much better than ju-
nior staff at distinguishing real events from
noise (sensor artefacts).
? Even senior consultants could only identify
70% of key events purely from the visualisa-
tions.
Law et al (2005) followed up the above work by
explicitly comparing the effectiveness of visuali-
sations and textual summaries. The textual sum-
maries in the experiment were manually written,
but did not contain any diagnoses and instead fo-
cused on describing the data. Law et al found that
clinicians at all levels made better decisions when
showed the textual summaries; however at all lev-
els they preferred the visualisations.
A similar study with computer generated sum-
mary texts produced by the Babytalk BT45 sys-
tem (Portet et al., 2009), conducted by van der
Meulen et al (2008), found that decision quality
was best when clinicians were shown manually
written summaries; computer generated texts were
of similar effectiveness to visualisations. An er-
ror analysis of this study (Reiter et al., 2008) con-
cluded that computer generated texts were much
more effective in some contexts than in others.
An implication of the above studies is that in
many cases the ideal strategy is to produce both
text and graphics. This increases decision effec-
tiveness (since the modalities work best in differ-
ent situations), and also increases user satisfaction,
since users see the modality they like as well as the
one which is most effective for decision support.
2.4 Annotated Graphs in NLG Engine
We have extended our NLG Engine to generate an-
notated graphs as well as texts; an example output,
generated by a demonstration system, is shown in
figure 1. This example shows a very simple textual
output; examples of more complex textual output
are on the Arria website3.
This example output shows a comparison of
performance between the FTSE 100 and a given
stock portfolio. The value of the FTSE 100 is used
as a performance benchmark to see if a given stock
portfolio is performing better or worse than com-
pared to the stock market in general.
As can be seen in the graph in figure 1, the anno-
tations are small text fragments, which are placed
3A more detailed example is given here: https://
www.arria.com/case-study-oilgas-A231.php
124
Figure 1: Combined text and annotated graph detailing the stock portfolio performance
Figure 2: Graph illustrating stacking capabilities
when two annotations intersect each other
on top of the graph, and are linked to the relevant
events in that occur in the graph. Annotations can
also be placed at the bottom of graphs and at the
sides and can rearrange themselves depending on
the space available. In figure 1 the range annota-
tion used indicates the reason for the increase in
the value of a given stock portfolio over a particu-
lar time period. If one or more annotations collide
or intersect a stacking algorithm is used prior to
presentation to rearrange the placement of collid-
ing annotations as shown in figure 2.
Figure 3 illustrates the architecture that is used
by our NLG engine. The data analysis and data
interpretation modules analyse the input data and
produce a set of messages which can be communi-
cated to the user in the generated report. The doc-
ument planner decides on which messages should
be communicated overall, and where messages
should appear (for example, situational analysis
text, diagnosis text, impact text, graph annotation,
or a combination of these). The document planner
also decides on the type of graph used, and which
data channels it displays; these data channels must
include any channels which are annotated, but in
some cases other channels are displayed as well.
Once document planning is complete, the vi-
sualisation planning module generates the graph
design, including X and Y scale and the position
of the annotations on the graph. The time range
shown in the graph is largely determined by the
annotation messages. In other words, the decision
about what data to show on the graph is partially
driven by the annotations.
The annotation microplanner and realiser gener-
ate the actual annotation texts, using special rules
which are optimised for annotations (which need
125
Figure 3: Pipeline architecture of the Arria NLG
Engine
to be short and have different referring expres-
sions). After this has been completed, a renderer
produces the actual annotated graph. The final
task lies with the presenter module, which recom-
bines the separately generated summary text (gen-
erated by the NLG Microplanning and Realisation
modules) with the annotated graphs.
3 Conclusion
Annotated graphs are a very appealing mechanism
for combining text and data graphics into a sin-
gle multimodal information presentation; this is
shown both by the findings of Sripada and Gao
(2007) and by the experiences of our customers.
Amongst other benefits, we believe that annotated
graphs will address some of the deficiencies in
data graphics which were pointed out by Alberdi
et al (2001), by helping users (especially inexpe-
rienced ones) to more easily identify key events
in a graph and also to distinguish real events from
sensor artefacts and other noise.
We have developed software to create annotated
graphs, by modifying the standard NLG data-to-
text pipeline as described above. Our clients have
reacted very positively so far, and we are now ex-
ploring extensions, for example by making anno-
tated graphs interactive.
References
E. Alberdi, J. C. Becher, K. J. Gilhooly, J. R.W. Hunter,
R. H. Logie, A. Lyon, N. McIntosh, and J. Reiss.
2001. Expertise and the interpretation of comput-
erised physiological data: Implications for the de-
sign of computerised physiological monitoring in
neonatal intensive care. International Journal of
Human Computer Studies, 55(3):191?216.
S. Demier, S. Carberry, and K. F. McCoy. 2008. Gen-
erating textual summaries of bar charts. In Fifth
International Natural Language Generation Con-
ference (INLG 2008), pages 7?15. Association for
Computational Linguistics.
S. Feiner and K. R. McKeown. 1990. Generating Co-
ordinated Multimedia Explanations. In Sixth Con-
ference on Artificial Intelligence Applications, vol-
ume 290-296.
B. Krenn, H. Pirker, M. Grice, S. Baumann, P. Pi-
wek, K. van Deemter, M. Schroeder, M. Klesen, and
E. Gstrein. 2002. Generation of multi-modal di-
alogue for a net environment. In Proceedings of
KONVENS-02, Saarbruecken, Germany.
A. S. Law, Y. Freer, J. Hunter, R. H. Logie, N. McIn-
tosh, and J. Quinn. 2005. A comparison of graph-
ical and textual presentations of time series data to
support medical decision making in the neonatal in-
tensive care unit. Journal of Clinical Monitoring
and Computing, 19(3):183?194.
M. Petre. 1995. Why Looking isn?t always See-
ing: Readership Skills and Graphical Programming.
Communications of the ACM, 38:33?44.
F. Portet, E. Reiter, A. Gatt, J. Hunter, S. Sripada,
Y. Freer, and C. Sykes. 2009. Automatic gener-
ation of textual summaries from neonatal intensive
care data. Artificial Intelligence, 173(7-8):789?816.
E. Reiter and R. Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University
Press.
E. Reiter, A. Gatt, F. Portet, and M. van der Meulen.
2008. The Importance of Narrative and Other
Lessons from an Evaluation of an NLG System that
Summarises Clinical Data. Fifth International Natu-
ral Language Generation Conference (INLG 2008),
pages 147?155.
S. G. Sripada and F. Gao. 2007. Summarizing dive
computer data: A case study in integrating textual
and graphical presentations of numerical data. In
MOG 2007 Workshop on Multimodal Output Gen-
eration, pages 149?157.
126
E. Tufte. 1983. The Visual Display of Quantitative
Information. Graphics Press.
M. van der Meulen, R. Logie, Y. Freer, C. Sykes,
N. McIntosh, and J. Hunter. 2008. When a graph
is poorer than 100 words: A comparison of com-
puterised natural language generation, human gen-
erated descriptions and graphical displays in neona-
tal intensive care. Applied Cognitive Psychology,
24:77?89.
W. Wahlster, E. Andre?, W. Finkle, HJ. Profitlich, and
T. Rist. 1993. Plan-based integration of natural
language and graphics generation. Artificial Intel-
ligence, 63:387?427.
127
