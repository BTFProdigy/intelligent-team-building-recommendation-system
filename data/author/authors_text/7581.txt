Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 410?413,
Prague, June 2007. c?2007 Association for Computational Linguistics
UNT: SubFinder: Combining Knowledge Sources for
Automatic Lexical Substitution
Samer Hassan, Andras Csomai, Carmen Banea, Ravi Sinha, Rada Mihalcea?
Department of Computer Science and Engineering
University of North Texas
samer@unt.edu, csomaia@unt.edu, carmenb@unt.edu, rss0089@unt.edu, rada@cs.unt.edu
Abstract
This paper describes the University of North
Texas SUBFINDER system. The system is
able to provide the most likely set of sub-
stitutes for a word in a given context, by
combining several techniques and knowl-
edge sources. SUBFINDER has successfully
participated in the best and out of ten (oot)
tracks in the SEMEVAL lexical substitution
task, consistently ranking in the first or sec-
ond place.
1 Introduction
Lexical substitution is defined as the task of identify-
ing the most likely alternatives (substitutes) for a tar-
get word, given its context (McCarthy, 2002). Many
natural language processing applications can bene-
fit from the availability of such alternative words,
including word sense disambiguation, lexical ac-
quisition, machine translation, information retrieval,
question answering, text simplification, and others.
The task is closely related to the problem of word
sense disambiguation, with the substitutes acting as
synonyms for the input word meaning. Unlike word
sense disambiguation however, lexical substitution
is not performed with respect to a given sense inven-
tory, but instead candidate synonyms are generated
?on the fly? for a given word occurrence. Thus, lexi-
cal substitution can be regarded in a way as a hybrid
task that combines word sense disambiguation and
distributional similarity, targeting the identification
of semantically similar words that fit the context.
2 A system for lexical substitution
SUBFINDER is a system able to provide the most
likely set of substitutes for a word in a given context.
?Contact author.
In SUBFINDER, the lexical substitution task is car-
ried out as a sequence of two steps. First, candidates
are extracted from a variety of knowledge sources;
so far, we experimented with WordNet (Fellbaum,
1998), Microsoft Encarta encyclopedia, Roget, as
well as synonym sets generated from bilingual dic-
tionaries, but additional knowledge sources can be
integrated as well. Second, provided a list of candi-
dates, a number of ranking methods are applied in
a weighted combination, resulting in a final list of
lexical substitutes ranked by their semantic fit with
both the input target word and the context.
3 Candidate Extraction
Candidates are extracted using several lexical re-
sources, which are combined into a larger compre-
hensive resource.
WordNet: WordNet is a large lexical database of
English, with words grouped into synonym sets
called synsets. A problem we encountered with this
resource is that often times the only candidate in the
synset is the target word itself. Thus, to enlarge the
set of candidates, we use both the synonyms and the
hypernyms of the target word. We also remove the
target word from the synset, to ensure that only vi-
able candidates are considered.
Microsoft Encarta encyclopedia: The Microsoft
Encarta is an online encyclopedia and thesaurus re-
source, which provides for each word the part of
speech and a list of synonyms. Using the part of
speech as identified in the context, we are able to ex-
tract synsets for the target word. An important fea-
ture in the Encarta Thesaurus is that the first word
in the synset acts as a definition for the synset, and
therefore disambiguates the target word. This defi-
nition is maintained as a separate entry in the com-
410
prehensive resource, and it is also added to its corre-
sponding synset.
Other Lexical Resources: We have also experi-
mented with two other lexical resources, namely the
Roget thesaurus and a thesaurus built using bilingual
dictionaries. In evaluations carried out on the devel-
opment data set, the best results were obtained using
only WordNet and Encarta, and thus these are the
resources used in the final SUBFINDER system.
All these resources entail different forms of synset
clustering. In order to merge them, we use the
largest overlap among them. It is important to note
that the choice of the first resource considered has
a bearing on the way the synsets are clustered. In
experiments ran on the development data set, the
best results were obtained using a lexical resource
constructed starting with the Microsoft Encarta The-
saurus and then mapping the WordNet synsets to it.
4 Candidate Ranking
Several ranking methods are used to score the can-
didate substitutes, as described below.
Lexical Baseline (LB): In this approach we use
the pre-existing lexical resources to provide a rank-
ing over the candidate substitutes. We rank the can-
didates based on their occurrence in the two selected
lexical resources WordNet and Encarta, with those
occurring in both resources being assigned a higher
ranking. This technique emphasizes the resources
annotators? agreement that the candidates belong in-
deed to the same synset.
Machine Translation (MT): We use machine
translation to translate the test sentences back-and-
forth between English and a second language. From
the resulting English translation, we extract the re-
placement that the machine translation engine pro-
vides for the target word. To locate the translated
word we scan the translation for any of the can-
didates (and their inflections) as obtained from the
comprehensive resource, and score the candidate
synset accordingly.
We experimented with a range of languages such
as French, Italian, Spanish, Simplified Chinese, and
German, but the best results obtained on the devel-
opment data were based on the French translations.
This could be explained because French is part of
the Romance languages family and synonyms to En-
glish words often find their roots in Latin. If we
consider again the word bright, it was translated
into French as intelligent and then translated back
into English as intelligent for obvious reasons. In
one instance, intelligent was the best replacement
for bright in the trial data. Despite the fact that we
also used Italian and Spanish (which are both Latin-
based) we can only assume that French worked bet-
ter because translation engines are better trained on
French. From the resulting English translation, we
extract the replacement that the machine translation
engine provides for the target word. To locate the
translated word we scan the translation for any of the
candidates (and their inflections) as obtained from
the comprehensive resource, and score the candidate
synset accordingly. The translation process was car-
ried out using Google and AltaVista translation en-
gines resulting in two systems MTG and MTA re-
spectively. The translation systems feature high pre-
cision when a candidate is found (about 20% of the
time), at the cost of low recall. The lexical baseline
method is therefore used when no candidates are re-
turned by the translation method.
Most Common Sense (MCS): Another method
we use for ranking candidates is to consider the
first word appearing in the first synset returned by
WordNet. When no words other than the target
word are available in this synset, the method recur-
sively searches the next synset available for the tar-
get word. In order to guarantee a sufficient number
of candidates, we use the lexical baseline method as
a baseline.
Language Model (LM): We model the semantic
fit of a candidate substitute within the given context
using a language model, expressed using the condi-
tional probability:
P (c|g) = P (c, g)/P (g) ? Count(c, g) (1)
where c represents a possible candidate and g rep-
resents the context. The probability P (g) of the
context is the same for all the candidates, hence we
can ignore it and estimate P (c|g) as the N-gram fre-
quency of the context where the target word is re-
placed by the proposed candidate. To avoid skewed
counts that can arise from the different morpholog-
ical inflections of the target word or the candidate
and the bias that the context might have toward any
specific inflection, we generalize P (c|g) to take into
account all the inflections of the selected candidate
as shown in equation 2.
Pn(c|g) ?
n
?
i=1
Count(ci, g) (2)
where n is the number of possible inflections for the
candidate c.
We use the Google N-gram dataset to calculate the
term Count(ci g). The Google N-gram corpus is a
411
collection of English N-grams, ranging from one to
five N-grams, and their respective frequency counts
observed on the Web (Brants and Franz, 2006). In
order for the model to give high preference to the
longer N-grams, while maintaining the relative fre-
quencies of the shorter N-grams (typically more fre-
quent), we augment the counts of the higher order
N-grams with the maximum counts of the lower or-
der N-grams, hence guaranteeing that the score as-
signed to an N-gram of order N is higher than the
the score of an N-gram of order N ? 1.
Semantic Relatedness using Latent Semantic
Analysis (LSA): We expect to find a strong se-
mantic relationship between a good candidate and
the target context. A relatively simple and efficient
way to measure such a relatedness is the Latent Se-
mantic Analysis (Landauer et al, 1998). Documents
and terms are mapped into a 300 dimensional latent
semantic space, providing the ability to measure the
semantic relatedness between two words or a word
and a context. We use the InfoMap package from
Stanford University?s Center for the Study of Lan-
guage and Information, trained on a collection of
approximately one million Wikipedia articles. The
rank of a candidate is given by its semantic related-
ness to the entire context sentence.
Information Retrieval (IR): Although the Lan-
guage Model approach is successful in ranking the
candidates, it suffers from the small N-gram size im-
posed by using the Google N-grams corpus. Such
a restriction is obvious in the following 5-gram ex-
ample who was a bright boy in which the context
is not sufficient to disambiguate between happy and
smart as possible candidates. As a result, we adapt
an information retrieval approach which uses all the
content words available in the given context. Similar
to the previous models, the target word in the con-
text is replaced by all the generated inflections of
the selected candidate and then queried using a web
search engine. The resulting rank represents the sum
of the total number of pages in which the candidate
or any of its inflections occur together with the con-
text. This also reflects the semantic relatedness or
the relevance of the candidate to the context.
Word Sense Disambiguation (WSD): Since pre-
vious work indicated the usefulness of word sense
disambiguation systems in lexical substitution (Da-
gan et al, 2006), we use the SenseLearner word
sense disambiguation tool (Mihalcea and Csomai,
2005) to disambiguate the target word and, accord-
ingly, to propose its synonyms as candidates.
Final System: Our candidate ranking methods are
aimed at different aspects of what constitutes a good
candidate. On one hand, we measure the semantic
relatedness of a candidate with the original context
(the LSA and WSD methods fall under this cate-
gory). On the other hand, we also want to ensure
that the candidate fits the context and leads to a well
formed English sentence (e.g., the language model
method). Given that the methods described earlier
aim at orthogonal aspects of the problem, it is ex-
pected that a combination of these will provide a
better overall ranking.
We use a voting mechanism, where we consider
the reciprocal of the rank of each candidates as given
by one of the described methods. The final score of
a candidate is given by the decreasing order of the
weighted sum of the reciprocal ranks:
score (ci) =
?
m?rankings
?m
1
rmci
To determine the weight ? of each individual
ranking we run a genetic algorithm on the develop-
ment data, optimized for the mode precision and re-
call. Separate sets of weights are obtained for the
best and oot tasks. Table 1 shows the weights of
the individual ranking methods. As expected, for
the best task, the language model type of methods
obtain higher weights, whereas for the oot task, the
semantic methods seem to perform better.
5 Results and Discussion
The SUBFINDER system participated in the best and
the oot tracks of the lexical substitution task. The
best track calls for any number of best guesses,
with the most promising one listed first. The credit
for each correct guess is divided by the number of
guesses. The oot track allows systems to make up to
10 guesses, without penalizing, and without being of
any benefit if less than 10 substitutes are provided.
The ordering of guesses in the oot metric is unim-
portant.
For both tracks, the evaluation is carried out using
precision and recall, calculated based on the number
of matching responses between the system and the
human annotators, respectively. A ?mode? evalua-
tion is also conducted, which measures the ability of
the systems to capture the most frequent response
(the ?mode?) from the gold standard annotations.
For details, please refer to the official task descrip-
tion document (McCarthy and Navigli, 2007).
Tables 2 and 3 show the results obtained by SUB-
FINDER in the best and oot tracks respectively. The
tables also show a breakdown of the results based
412
on: only target words that were not identified as
multiwords (NMWT); only substitutes that were not
identified as multiwords (NMWS); only items with
sentences randomly selected from the Internet cor-
pus (RAND); only items with sentences manually se-
lected from the Internet corpus (MAN).
WSD LSA IR LB MCS MTA MTG LM
best 34 2 64 63 56 69 38 97
oot 6 82 7 28 46 14 32 68
Table 1: Weights of the individual ranking methods
P R Mode P Mode R
OVERALL 12.77 12.77 20.73 20.73
Further Analysis
NMWT 13.46 13.46 21.63 21.63
NMWS 13.79 13.79 21.59 21.59
RAND 12.85 12.85 20.18 20.18
MAN 12.69 12.69 21.35 21.35
Baselines
WORDNET 9.95 9.95 15.28 15.28
LIN 8.84 8.53 14.69 14.23
Table 2: BEST results
P R Mode P Mode R
OVERALL 49.19 49.19 66.26 66.26
Further Analysis
NMWT 51.13 51.13 68.03 68.03
NMWS 54.01 54.01 70.15 70.15
RAND 51.71 51.71 68.04 68.04
MAN 46.26 46.26 64.24 64.24
Baselines
WORDNET 29.70 29.35 40.57 40.57
LIN 27.70 26.72 40.47 39.19
Table 3: OOT results
Compared to other systems participating in this
task, our system consistently ranks on the first or
second place. SUBFINDER clearly outperforms all
the other systems for the ?mode? evaluation, show-
ing the ability of the system to find the substitute
most often preferred by the human annotators. In
addition, the system exceeds by a large margin all
the baselines calculated for the task, which select
substitutes based on existing lexical resources (e.g.,
WordNet or Lin distributional similarity).
Separate from the ?official? submission, we ran
a second experiment where we optimized the com-
bination weights targeting high precision and recall
(rather than high mode). An evaluation of the system
using this new set of weights yields a precision and
recall of 13.34 with a mode of 21.71 for the best task,
surpassing the best system according to the anony-
mous results report. For the oot task, the precision
and recall increased to 50.30, still maintaining sec-
ond place.
6 Conclusions
The lexical substitution task goes beyond simple
word sense disambiguation. To approach such a
task, we first need a good comprehensive and precise
lexical resource for candidate extraction. Secondly,
we need to semantically filter the highly diverse and
ambiguous set of candidates, while taking into ac-
count their fitness in the context in order to form
a proper linguistic expression. To accomplish this,
we built a system that incorporates lexical, semantic,
and probabilistic methods to capture both the seman-
tic similarity with the target word and the semantic
fit in the context. Compared to other systems partic-
ipating in this task, our system consistently ranks on
the first or second place. SUBFINDER clearly out-
performs all the other systems for the ?mode? eval-
uation, proving its ability to find the substitute most
often preferred by the human annotators.
Acknowledgments
This work was supported in part by the Texas Ad-
vanced Research Program under Grant #003594.
The authors are grateful to the Language and Infor-
mation Technologies research group at the Univer-
sity of North Texas for many useful discussions and
feedback on this work.
References
T. Brants and A. Franz. 2006. Web 1t 5-gram version 1.
Linguistic Data Consortium.
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein,
and C. Strapparava. 2006. Direct word sense match-
ing for lexical substitution. In Proceedings of the In-
ternational Conference on Computational Linguistics
ACL/COLING 2006.
C. Fellbaum. 1998. WordNet, An Electronic Lexical
Database. The MIT Press.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduc-
tion to latent semantic analysis. Discourse Processes,
25.
D. McCarthy and R. Navigli. 2007. The semeval English
lexical substitution task. In Proceedings of the ACL
Semeval workshop.
D. McCarthy. 2002. Lexical substitution as a task for
wsd evaluation. In Proceedings of the ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, Philadelphia.
R. Mihalcea and A. Csomai. 2005. Senselearner: Word
sense disambiguation for all words in unrestricted text.
In Proceedings of the 43nd Annual Meeting of the As-
sociation for Computational Linguistics, Ann Arbor,
MI.
413
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 76?81,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
SemEval-2010 Task 2: Cross-Lingual Lexical Substitution
Ravi Sinha
University of North Texas
ravisinha@unt.edu
Diana McCarthy
University of Sussex
dianam@sussex.ac.uk
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Abstract
In this paper we describe the SemEval-
2010 Cross-Lingual Lexical Substitution task,
which is based on the English Lexical Substi-
tution task run at SemEval-2007. In the En-
glish version of the task, annotators and sys-
tems had to find an alternative substitute word
or phrase for a target word in context. In this
paper we propose a task where the target word
and contexts will be in English, but the substi-
tutes will be in Spanish. In this paper we pro-
vide background and motivation for the task
and describe how the dataset will differ from
a machine translation task and previous word
sense disambiguation tasks based on parallel
data. We describe the annotation process and
how we anticipate scoring the system output.
We finish with some ideas for participating
systems.
1 Introduction
The Cross-Lingual Lexical Substitution task is
based on the English Lexical Substitution task run at
SemEval-2007. In the 2007 English Lexical Substi-
tution Task, annotators and systems had to find an al-
ternative substitute word or phrase for a target word
in context. In this cross-lingual task the target word
and contexts will be in English, but the substitutes
will be in Spanish.
An automatic system for cross-lingual lexical sub-
stitution would be useful for a number of applica-
tions. For instance, such a system could be used
to assist human translators in their work, by provid-
ing a number of correct translations that the human
translator can choose from. Similarly, the system
could be used to assist language learners, by pro-
viding them with the interpretation of the unknown
words in a text written in the language they are learn-
ing. Last but not least, the output of a cross-lingual
lexical substitution system could be used as input to
existing systems for cross-language information re-
trieval or automatic machine translation.
2 Background: The English Lexical
Substitution Task
The English Lexical substitution task (hereafter re-
ferred to as LEXSUB) was run at SemEval-2007 fol-
lowing earlier ideas on a method of testing WSD
systems without predetermining the inventory (Mc-
Carthy, 2002). The issue of which inventory is ap-
propriate for the task has been a long standing is-
sue for debate, and while there is hope that coarse-
grained inventories will allow for increased system
performance (Ide and Wilks, 2006) we do not yet
know if these will make the distinctions that will
most benefit practical systems (Stokoe, 2005) or re-
flect cognitive processes (Kilgarriff, 2006). LEXSUB
was proposed as a task which, while requiring con-
textual disambiguation, did not presuppose a spe-
cific sense inventory. In fact, it is quite possible to
use alternative representations of meaning (Schu?tze,
1998; Pantel and Lin, 2002).
The motivation for a substitution task was that it
would reflect capabilities that might be useful for
natural language processing tasks such as paraphras-
ing and textual entailment, while only focusing on
one aspect of the problem and therefore not requir-
ing a complete system that might mask system capa-
bilities at a lexical level and at the same time make
76
participation in the task difficult for small research
teams.
The task required systems to produce a substitute
word for a word in context. For example a substitute
of tournament might be given for the second oc-
currence of match (shown in bold) in the following
sentence:
The ideal preparation would be a light meal
about 2-2 1/2 hours pre-match, followed by a
warm-up hit and perhaps a top-up with extra fluid
before the match.
In LEXSUB, the data was collected for 201 words
from open class parts-of-speech (PoS) (i.e. nouns,
verbs, adjectives and adverbs). Words were selected
that have more than one meaning with at least one
near synonym. Ten sentences for each word were
extracted from the English Internet Corpus (Sharoff,
2006). There were five annotators who annotated
each target word as it occurred in the context of a
sentence. The annotators were each allowed to pro-
vide up to three substitutes, though they could also
provide a NIL response if they could not come up
with a substitute. They had to indicate if the target
word was an integral part of a multiword.
A development and test dataset were provided,
but no training data. Any system that relied on train-
ing data, such as sense annotated corpora, had to use
resources available from other sources. The task had
eight participating teams. Teams were allowed to
submit up to two systems and there were a total of
ten different systems. The scoring was conducted
using recall and precision measures using:
? the frequency distribution of responses from
the annotators and
? the mode of the annotators (the most frequent
response).
The systems were scored using their best guess as
well as an out-of-ten score which allowed up to 10
attempts. 1 The results are reported in McCarthy and
Navigli (2007) and in more detail in McCarthy and
Navigli (in press).
1The details are available at
http://nlp.cs.swarthmore.edu/semeval/tasks/
task10/task10documentation.pdf.
3 Motivation and Related Work
While there has been a lot of discussion on the rel-
evant sense distinctions for monolingual WSD sys-
tems, for machine translation applications there is
a consensus that the relevant sense distinctions are
those that reflect different translations. One early
and notable work was the SENSEVAL-2 Japanese
Translation task (Kurohashi, 2001) that obtained al-
ternative translation records of typical usages of a
test word, also referred to as a translation mem-
ory. Systems could either select the most appropri-
ate translation memory record for each instance and
were scored against a gold-standard set of annota-
tions, or they could provide a translation that was
scored by translation experts after the results were
submitted. In contrast to this work, we propose to
provide actual translations for target instances in ad-
vance, rather than predetermine translations using
lexicographers or rely on post-hoc evaluation, which
does not permit evaluation of new systems after the
competition.
Previous standalone WSD tasks based on parallel
data have obtained distinct translations for senses as
listed in a dictionary (Ng and Chan, 2007). In this
way fine-grained senses with the same translations
can be lumped together, however this does not fully
allow for the fact that some senses for the same
words may have some translations in common but
also others that are not. An example from Resnik
and Yarowsky (2000) (table 4 in that paper) is the
first two senses from WordNet for the noun interest:
WordNet sense Spanish Translation
monetary e.g. on loan intere?s, re?dito
stake/share intere?s,participacio?n
For WSD tasks, a decision can be made to lump
senses with such overlap, or split them using the dis-
tinctive translation and then use the distinctive trans-
lations as a sense inventory. This sense inventory is
then used to collect training from parallel data (Ng
and Chan, 2007). We propose that it would be in-
teresting to collect a dataset where the overlap in
translations for an instance can remain and that this
will depend on the token instance rather than map-
ping to a pre-defined sense inventory. Resnik and
Yarowsky (2000) also conducted their experiments
using words in context, rather than a predefined
77
sense-inventory as in (Ng and Chan, 2007; Chan and
Ng, 2005), however in these experiments the anno-
tators were asked for a single preferred translation.
We intend to allow annotators to supply as many
translations as they feel are equally valid. This will
allow us to examine more subtle relationships be-
tween usages and to allow partial credit to systems
which get a close approximation to the annotators?
translations. Unlike a full blown machine transla-
tion task (Carpuat and Wu, 2007), annotators and
systems will not be required to translate the whole
context but just the target word.
4 The Cross-Lingual Lexical Substitution
Task
Here we discuss our proposal for a Cross-Lingual
Lexical Substitution task. The task will follow LEX-
SUB except that the annotations will be translations
rather than paraphrases.
Given a target word in context, the task is to pro-
vide several correct translations for that word in a
given language. We will use English as the source
language and Spanish as the target language. Mul-
tiwords are ?part and parcel? of natural language.
For this reason, rather than try and filter multiwords,
which is very hard to do without assuming a fixed
inventory, 2 we will ask annotators to indicate where
the target word is part of a multiword and what that
multiword is. This way, we know what the substitute
translation is replacing.
We will provide both development and test sets,
but no training data. As for LEXSUB, any sys-
tems requiring data will need to obtain it from other
sources. We will include nouns, verbs, adjectives
and adverbs in both development and test data. Un-
like LEXSUB, the annotators will be told the PoS of
the current target word.
4.1 Annotation
We are going to use four annotators for our task, all
native Spanish speakers from Mexico, with a high
level of proficiency in English. The annotation in-
terface is shown in figure 1. We will calculate inter-
tagger agreement as pairwise agreement between
2The multiword inventories that do exist are far from com-
plete.
sets of substitutes from annotators, as was done in
LEXSUB.
4.2 An Example
One significant outcome of this task is that there
will not necessarily be clear divisions between us-
ages and senses because we do not use a predefined
sense inventory, or restrict the annotations to dis-
tinctive translations. This will mean that there can
be usages that overlap to different extents with each
other but do not have identical translations. An ex-
ample from our preliminary annotation trials is the
target adverb severely. Four sentences are shown in
figure 2 with the translations provided by one an-
notator marked in italics and {} braces. Here, all
the token occurrences seem related to each other in
that they share some translations, but not all. There
are sentences like 1 and 2 that appear not to have
anything in common. However 1, 3, and 4 seem to
be partly related (they share severamente), and 2, 3,
and 4 are also partly related (they share seriamente).
When we look again, sentences 1 and 2, though not
directly related, both have translations in common
with sentences 3 and 4.
4.3 Scoring
We will adopt the best and out-of-ten precision and
recall scores from LEXSUB. The systems can supply
as many translations as they feel fit the context. The
system translations will be given credit depending
on the number of annotators that picked each trans-
lation. The credit will be divided by the number of
annotator responses for the item and since for the
best score the credit for the system answers for an
item is also divided by the number of answers the
system provides, this allows more credit to be given
to instances where there is less variation. For that
reason, a system is better guessing the translation
that is most frequent unless it really wants to hedge
its bets. Thus if i is an item in the set of instances
I , and Ti is the multiset of gold standard translations
from the human annotators for i, and a system pro-
vides a set of answers Si for i, then the best score
for item i will be:
best score(i) =
?
s?Si frequency(s ? Ti)
|Si| ? |Ti|
(1)
78
Figure 1: The Cross-Lingual Lexical Substitution Interface
1. Perhaps the effect of West Nile Virus is sufficient to extinguish endemic birds already severely stressed
by habitat losses. {fuertemente, severamente, duramente, exageradamente}
2. She looked as severely as she could muster at Draco. {rigurosamente, seriamente}
3. A day before he was due to return to the United States Patton was severely injured in a road accident.
{seriamente, duramente, severamente}
4. Use market tools to address environmental issues , such as eliminating subsidies for industries that
severely harm the environment, like coal. {peligrosamente, seriamente, severamente}
5. This picture was severely damaged in the flood of 1913 and has rarely been seen until now. {altamente,
seriamente, exageradamente}
Figure 2: Translations from one annotator for the adverb severely
Precision is calculated by summing the scores for
each item and dividing by the number of items that
the system attempted whereas recall divides the sum
of scores for each item by |I|. Thus:
best precision =
?
i best score(i)
|i ? I : defined(Si)| (2)
best recall =
?
i best score(i)
|I| (3)
The out-of-ten scorer will allow up to ten system
responses and will not divide the credit attributed
to each answer by the number of system responses.
This allows the system to be less cautious and for
the fact that there is considerable variation on the
task and there may be cases where systems select a
perfectly good translation that the annotators had not
thought of. By allowing up to ten translations in the
out-of-ten task the systems can hedge their bets to
find the translations that the annotators supplied.
oot score(i) =
?
s?Si frequency(s ? Ti)
|Ti|
(4)
oot precision =
?
i oot score(i)
|i ? I : defined(Si)| (5)
79
oot recall =
?
i oot score(i)
|I| (6)
We will refine the scores before June 2009 when
we will release the development data for this cross-
lingual task. We note that there was an issue that the
original LEXSUB out-of-ten scorer allowed dupli-
cates (McCarthy and Navigli, in press). The effect
of duplicates is that systems can get inflated scores
because the credit for each item is not divided by
the number of substitutes and because the frequency
of each annotator response is used. McCarthy and
Navigli (in press) describe this oversight, identify
the systems that had included duplicates and explain
the implications. For our task there is an option for
the out-of-ten score. Either:
1. we remove duplicates before scoring or,
2. we allow duplicates so that systems can boost
their scores with duplicates on translations with
higher probability
We will probably allow duplicates but make this
clear to participants.
We may calculate additional best and out-of-ten
scores against the mode from the annotators re-
sponses as was done in LEXSUB, but we have not
decided on this yet. We will not run a multiword
task, but we will use the items identified as multi-
words as an optional filter to the scoring i.e. to see
how systems did without these items.
We will provide baselines and upper-bounds.
5 Systems
In the cross-lingual LEXSUB task, the systems will
have to deal with two parts of the problem, namely:
1. candidate collection
2. candidate selection
The first sub-task, candidate collection, refers to
consulting several resources and coming up with a
list of potential translation candidates for each tar-
get word and part of speech. We do not provide any
inventories, as with the original LEXSUB task, and
thus leave this task of coming up with the most suit-
able translation list (in contrast to the synonym list
required for LEXSUB) to the participants. As was
observed with LEXSUB, it is our intuition that the
quality of this translation list that the systems come
up with will determine to a large extent how well
the final performance of the system will be. Partici-
pants are free to use any ideas. However, a few pos-
sibilities might be to use parallel corpora, bilingual
dictionaries, a translation engine that only translates
the target word, or a machine translation system that
translates the entire sentences. Several of the bilin-
gual dictionaries or even other resources might be
combined together to come up with a comprehen-
sive translation candidate list, if that seems to im-
prove performance.
The second phase, candidate selection, concerns
fitting the translation candidates in context, and thus
coming up with a ranking as to which translations
are the most suitable for each instance. The highest
ranking candidate will be the output for best, and the
list of the top 10 ranking candidates will be the out-
put for out-of-ten. Again, participants are free to use
their creativity in this, while a range of possible al-
gorithms might include using a machine translation
system, using language models, word sense disam-
biguation models, semantic similarity-based tech-
niques, graph-based models etc. Again, combina-
tions of these might be used if they are feasible as
far as time and space are concerned.
We anticipate a minor practical issue to come up
with all participants, and that is the issue of different
character encodings, especially when using bilin-
gual dictionaries from the Web. This is directly re-
lated to the issue of dealing with characters with di-
acritics, and in our experience not all available soft-
ware packages and programs are able to handle dia-
critics and different character encodings in the same
way. This issue is inherent in all cross-lingual tasks,
and we leave it up to the discretion of the partici-
pants to effectively deal with it.
6 Post Hoc Issues
In LEXSUB a post hoc evaluation was conducted us-
ing fresh annotators to ensure that the substitutes
the systems came up with were not typically bet-
ter than those of the original annotators. This was
done as a sanity check because there was no fixed
inventory for the task and there will be a lot of varia-
80
tion in the task and sometimes the systems might do
better than the annotators. The post hoc evaluation
demonstrated that the post hoc annotators typically
preferred the substitutes provided by humans.
We have not yet determined whether we will run
a post hoc evaluation because of the costs of do-
ing this and the time constraints. Another option is
to reannotate a portion of our data using a new set
of annotators but restricting them to the translations
supplied by the initial set of annotations and other
translations from available resources. This would be
worthwhile but it could be done at any stage when
funds permit because we do not intend to supply a
set of candidate translations to the annotators since
we wish to evaluate candidate collection as well as
candidate selection.
7 Conclusions
In this paper we have outlined the cross-lingual lex-
ical substitution task to be run under the auspices
of SemEval-2010. The task will require annotators
and systems to find translations for a target word in
context. Unlike machine translation tasks, the whole
text is not translated and annotators are encouraged
to supply as many translations as fit the context. Un-
like previous WSD tasks based on parallel data, be-
cause we allow multiple translations and because we
do not restrict translations to those that provide clear
cut sense distinctions, we will be able to use the
dataset collected to investigate more subtle represen-
tations of meaning.
8 Acknowledgements
The work of the first and third authors has been partially
supported by a National Science Foundation CAREER
award #0747340. The work of the second author has been
supported by a Royal Society UK Dorothy Hodgkin Fel-
lowship.
References
Marine Carpuat and Dekai Wu. 2007. Improving statisti-
cal machine translation using word sense disambigua-
tion. In Proceedings of the Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL 2007), pages 61?72, Prague, Czech Republic,
June. Association for Computational Linguistics.
Yee Seng Chan and Hwee Tou Ng. 2005. Word sense
disambiguation with distribution estimation. In Pro-
ceedings of the 19th International Joint Conference on
Artificial Intelligence (IJCAI 2005), pages 1010?1015,
Edinburgh, Scotland.
Nancy Ide and Yorick Wilks. 2006. Making sense about
sense. In Eneko Agirre and Phil Edmonds, editors,
Word Sense Disambiguation, Algorithms and Applica-
tions, pages 47?73. Springer.
Adam Kilgarriff. 2006. Word senses. In Eneko
Agirre and Phil Edmonds, editors, Word Sense Disam-
biguation, Algorithms and Applications, pages 29?46.
Springer.
Sadao Kurohashi. 2001. SENSEVAL-2 japanese transla-
tion task. In Proceedings of the SENSEVAL-2 work-
shop, pages 37?44.
Diana McCarthy and Roberto Navigli. 2007. SemEval-
2007 task 10: English lexical substitution task. In Pro-
ceedings of the 4th International Workshop on Seman-
tic Evaluations (SemEval-2007), pages 48?53, Prague,
Czech Republic.
Diana McCarthy and Roberto Navigli. in press. The en-
glish lexical substitution task. Language Resources
and Evaluation Special Issue on Computational Se-
mantic Analysis of Language: SemEval-2007 and Be-
yond.
Diana McCarthy. 2002. Lexical substitution as a task for
wsd evaluation. In Proceedings of the ACL Workshop
on Word Sense Disambiguation: Recent Successes and
Future Directions, pages 109?115, Philadelphia, USA.
Hwee Tou Ng and Yee Seng Chan. 2007. SemEval-
2007 task 11: English lexical sample task via
English-Chinese parallel text. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-2007), pages 54?58, Prague, Czech Repub-
lic.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD
Conference on Knowledge Discovery and Data Min-
ing, pages 613?619, Edmonton, Canada.
Philip Resnik and David Yarowsky. 2000. Distinguish-
ing systems and distinguishing senses: New evaluation
methods for word sense disambiguation. Natural Lan-
guage Engineering, 5(3):113?133.
Hinrich Schu?tze. 1998. Automatic word sense discrimi-
nation. Computational Linguistics, 24(1):97?123.
Serge Sharoff. 2006. Open-source corpora: Using the
net to fish for linguistic data. International Journal of
Corpus Linguistics, 11(4):435?462.
Christopher Stokoe. 2005. Differentiating homonymy
and polysemy in information retrieval. In Proceedings
of the joint conference on Human Language Technol-
ogy and Empirical methods in Natural Language Pro-
cessing, pages 403?410, Vancouver, B.C., Canada.
81
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 9?14,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
SemEval-2010 Task 2: Cross-Lingual Lexical Substitution
Rada Mihalcea
University of North Texas
rada@cs.unt.edu
Ravi Sinha
University of North Texas
ravisinha@unt.edu
Diana McCarthy
Lexical Computing Ltd.
diana@dianamccarthy.co.uk
Abstract
In this paper we describe the SemEval-
2010 Cross-Lingual Lexical Substitution
task, where given an English target word
in context, participating systems had to
find an alternative substitute word or
phrase in Spanish. The task is based on
the English Lexical Substitution task run
at SemEval-2007. In this paper we pro-
vide background and motivation for the
task, we describe the data annotation pro-
cess and the scoring system, and present
the results of the participating systems.
1 Introduction
In the Cross-Lingual Lexical Substitution task, an-
notators and systems had to find an alternative
substitute word or phrase in Spanish for an En-
glish target word in context. The task is based
on the English Lexical Substitution task run at
SemEval-2007, where both target words and sub-
stitutes were in English.
An automatic system for cross-lingual lexical
substitution would be useful for a number of ap-
plications. For instance, such a system could be
used to assist human translators in their work, by
providing a number of correct translations that the
human translator can choose from. Similarly, the
system could be used to assist language learners,
by providing them with the interpretation of the
unknown words in a text written in the language
they are learning. Last but not least, the output
of a cross-lingual lexical substitution system could
be used as input to existing systems for cross-
language information retrieval or automatic ma-
chine translation.
2 Motivation and Related Work
While there has been a lot of discussion on the rel-
evant sense distinctions for monolingual WSD sys-
tems, for machine translation applications there is
a consensus that the relevant sense distinctions are
those that reflect different translations. One early
and notable work was the SENSEVAL-2 Japanese
Translation task (Kurohashi, 2001) that obtained
alternative translation records of typical usages of
a test word, also referred to as a translation mem-
ory. Systems could either select the most appro-
priate translation memory record for each instance
and were scored against a gold-standard set of an-
notations, or they could provide a translation that
was scored by translation experts after the results
were submitted. In contrast to this work, in our
task we provided actual translations for target in-
stances in advance, rather than predetermine trans-
lations using lexicographers or rely on post-hoc
evaluation, which does not permit evaluation of
new systems after the competition.
Previous standalone WSD tasks based on par-
allel data have obtained distinct translations for
senses as listed in a dictionary (Ng and Chan,
2007). In this way fine-grained senses with the
same translations can be lumped together, how-
ever this does not fully allow for the fact that some
senses for the same words may have some transla-
tions in common but also others that are not (Sinha
et al, 2009).
In our task, we collected a dataset which al-
lows instances of the same word to have some
translations in common, while not necessitating
a clustering of translations from a specific re-
source into senses (in comparison to Lefever and
Hoste (2010)). 1 Resnik and Yarowsky (2000) also
1Though in that task note that it is possible for a transla-
tion to occur in more than one cluster. It will be interesting to
9
conducted experiments using words in context,
rather than a predefined sense-inventory however
in these experiments the annotators were asked for
a single preferred translation. In our case, we al-
lowed annotators to supply as many translations
as they felt were equally valid. This allows us
to examine more subtle relationships between us-
ages and to allow partial credit to systems that
get a close approximation to the annotators? trans-
lations. Unlike a full blown machine translation
task (Carpuat and Wu, 2007), annotators and sys-
tems are not required to translate the whole context
but just the target word.
3 Background: The English Lexical
Substitution Task
The English Lexical substitution task (hereafter
referred to as LEXSUB) was run at SemEval-
2007 (McCarthy and Navigli, 2007; McCarthy and
Navigli, 2009). LEXSUB was proposed as a task
which, while requiring contextual disambiguation,
did not presuppose a specific sense inventory. In
fact, it is quite possible to use alternative rep-
resentations of meaning, such as those proposed
by Schu?tze (1998) and Pantel and Lin (2002).
The motivation for a substitution task was that
it would reflect capabilities that might be useful
for natural language processing tasks such as para-
phrasing and textual entailment, while not requir-
ing a complete system that might mask system ca-
pabilities at a lexical level and make participation
in the task difficult for small research teams.
The task required systems to produce a substi-
tute word for a word in context. The data was
collected for 201 words from open class parts-of-
speech (PoS) (i.e. nouns, verbs, adjectives and ad-
verbs). Words were selected that have more than
one meaning with at least one near synonym. Ten
sentences for each word were extracted from the
English Internet Corpus (Sharoff, 2006). There
were five annotators who annotated each target
word as it occurred in the context of a sentence.
The annotators were each allowed to provide up to
three substitutes, though they could also provide
a NIL response if they could not come up with a
substitute. They had to indicate if the target word
was an integral part of a multiword.
see the extent that this actually occurred in their data and the
extent that the translations that our annotators provided might
be clustered.
4 The Cross-Lingual Lexical
Substitution Task
The Cross-Lingual Lexical Substitution task fol-
lows LEXSUB except that the annotations are
translations rather than paraphrases. Given a tar-
get word in context, the task is to provide several
correct translations for that word in a given lan-
guage. We used English as the source language
and Spanish as the target language.
We provided both development and test sets, but
no training data. As for LEXSUB, any systems re-
quiring training data had to obtain it from other
sources. We included nouns, verbs, adjectives and
adverbs in both development and test data. We
used the same set of 30 development words as in
LEXSUB, and a subset of 100 words from the LEX-
SUB test set, selected so that they exhibit a wide
variety of substitutes. For each word, the same ex-
ample sentences were used as in LEXSUB.
4.1 Annotation
We used four annotators for the task, all native
Spanish speakers from Mexico, with a high level
of proficiency in English. As in LEXSUB, the an-
notators were allowed to use any resources they
wanted to, and were required to provide as many
substitutes as they could think of.
The inter-tagger agreement (ITA) was calcu-
lated as pairwise agreement between sets of sub-
stitutes from annotators, as done in LEXSUB. The
ITA without mode was determined as 0.2777,
which is comparable with the ITA of 0.2775 de-
termined for LEXSUB.
4.2 An Example
One significant outcome of this task is that there
are not necessarily clear divisions between usages
and senses because we do not use a predefined
sense inventory, or restrict the annotations to dis-
tinctive translations. This means that there can be
usages that overlap to different extents with each
other but do not have identical translations. An
example is the target adverb severely. Four sen-
tences are shown in Figure 1 with the translations
provided by one annotator marked in italics and
{} braces. Here, all the token occurrences seem
related to each other in that they share some trans-
lations, but not all. There are sentences like 1
and 2 that appear not to have anything in com-
mon. However 1, 3, and 4 seem to be partly re-
lated (they share severamente), and 2, 3, and 4 are
also partly related (they share seriamente). When
10
we look again, sentences 1 and 2, though not di-
rectly related, both have translations in common
with sentences 3 and 4.
4.3 Scoring
We adopted the best and out-of-ten precision and
recall scores from LEXSUB (oot in the equations
below). The systems were allowed to supply as
many translations as they feel fit the context. The
system translations are then given credit depend-
ing on the number of annotators that picked each
translation. The credit is divided by the number
of annotator responses for the item and since for
the best score the credit for the system answers
for an item is also divided by the number of an-
swers the system provides, this allows more credit
to be given to instances where there is less varia-
tion. For that reason, a system is better guessing
the translation that is most frequent unless it re-
ally wants to hedge its bets. Thus if i is an item
in the set of instances I , and T
i
is the multiset of
gold standard translations from the human annota-
tors for i, and a system provides a set of answers
S
i
for i, then the best score for item i is2:
best score(i) =
?
s?S
i
frequency(s ? T
i
)
|S
i
| ? |T
i
|
(1)
Precision is calculated by summing the scores
for each item and dividing by the number of items
that the system attempted whereas recall divides
the sum of scores for each item by |I|. Thus:
best precision =
?
i
best score(i)
|i ? I : defined(S
i
)|
(2)
best recall =
?
i
best score(i)
|I|
(3)
The out-of-ten scorer allows up to ten system
responses and does not divide the credit attributed
to each answer by the number of system responses.
This allows a system to be less cautious and for
the fact that there is considerable variation on the
task and there may be cases where systems select
a perfectly good translation that the annotators had
not thought of. By allowing up to ten translations
in the out-of-ten task the systems can hedge their
bets to find the translations that the annotators sup-
plied.
2NB scores are multiplied by 100, though for out-of-ten
this is not strictly a percentage.
oot score(i) =
?
s?S
i
frequency(s ? T
i
)
|T
i
|
(4)
oot precision =
?
i
oot score(i)
|i ? I : defined(S
i
)|
(5)
oot recall =
?
i
oot score(i)
|I|
(6)
We note that there was an issue that the origi-
nal LEXSUB out-of-ten scorer allowed duplicates
(McCarthy and Navigli, 2009). The effect of du-
plicates is that systems can get inflated scores be-
cause the credit for each item is not divided by the
number of substitutes and because the frequency
of each annotator response is used. McCarthy and
Navigli (2009) describe this oversight, identify the
systems that had included duplicates and explain
the implications. For our task, we decided to con-
tinue to allow for duplicates, so that systems can
boost their scores with duplicates on translations
with higher probability.
For both the best and out-of-ten measures, we
also report a mode score, which is calculated
against the mode from the annotators responses as
was done in LEXSUB. Unlike the LEXSUB task,
we did not run a separate multi-word subtask and
evaluation.
5 Baselines and Upper bound
To place results in perspective, several baselines as
well as the upper bound were calculated.
5.1 Baselines
We calculated two baselines, one dictionary-based
and one dictionary and corpus-based. The base-
lines were produced with the help of an on-
line Spanish-English dictionary3 and the Spanish
Wikipedia. For the first baseline, denoted by DICT,
for all target words, we collected all the Spanish
translations provided by the dictionary, in the or-
der returned on the online query page. The best
baseline was produced by taking the first transla-
tion provided by the online dictionary, while the
out-of-ten baseline was produced by taking the
first 10 translations provided.
The second baseline, DICTCORP, also ac-
counted for the frequency of the translations
within a Spanish dictionary. All the translations
3www.spanishdict.com
11
1. Perhaps the effect of West Nile Virus is sufficient to extinguish endemic birds already severely
stressed by habitat losses. {fuertemente, severamente, duramente, exageradamente}
2. She looked as severely as she could muster at Draco. {rigurosamente, seriamente}
3. A day before he was due to return to the United States Patton was severely injured in a road accident.
{seriamente, duramente, severamente}
4. Use market tools to address environmental issues , such as eliminating subsidies for industries that
severely harm the environment, like coal. {peligrosamente, seriamente, severamente}
5. This picture was severely damaged in the flood of 1913 and has rarely been seen until now.
{altamente, seriamente, exageradamente}
Figure 1: Translations from one annotator for the adverb severely
provided by the online dictionary for a given target
word were ranked according to their frequencies in
the Spanish Wikipedia, producing the DICTCORP
baseline.
5.2 Upper bound
The results for the best task reflect the inherent
variability as less credit is given where annotators
express differences. The theoretical upper bound
for the best recall (and precision if all items are
attempted) score is calculated as:
best
ub
=
?
i?I
freq
most freq substitute
i
|T
i
|
|I|
? 100
= 40.57 (7)
Note of course that this upper bound is theoretical
and assumes a human could find the most frequent
substitute selected by all annotators. Performance
of annotators will undoubtedly be lower than the
theoretical upper bound because of human vari-
ability on this task. Since we allow for duplicates,
the out-of-ten upper bound assumes the most fre-
quent word type in T
i
is selected for all ten an-
swers. Thus we would obtain ten times the best
upper bound (equation 7).
oot
ub
=
?
i?I
freq
most freq substitute
i
?10
|T
i
|
|I|
? 100
= 405.78 (8)
If we had not allowed duplicates then the out-
of-ten upper bound would have been just less than
100% (99.97). This is calculated by assuming the
top 10 most frequent responses from the annota-
tors are picked in every case. There are only a cou-
ple of cases where there are more than 10 transla-
tions from the annotators.
6 Systems
Nine teams participated in the task, and several
of them entered two systems. The systems used
various resources, including bilingual dictionar-
ies, parallel corpora such as Europarl or corpora
built from Wikipedia, monolingual corpora such
as Web1T or newswire collections, and transla-
tion software such as Moses, GIZA or Google.
Some systems attempted to select the substitutes
on the English side, using a lexical substitu-
tion framework or word sense disambiguation,
whereas some systems made the selection on the
Spanish side using lexical substitution in Spanish.
In the following, we briefly describe each par-
ticipating system.
CU-SMT relies on a phrase-based statistical ma-
chine translation system, trained on the Europarl
English-Spanish parallel corpora.
The UvT-v and UvT-g systems make use of k-
nearest neighbour classifiers to build one word ex-
pert for each target word, and select translations
on the basis of a GIZA alignment of the Europarl
parallel corpus.
The UBA-T and UBA-W systems both use can-
didates from Google dictionary, SpanishDict.com
and Babylon, which are then confirmed using par-
allel texts. UBA-T relies on the automatic trans-
lation of the source sentence using the Google
Translation API, combined with several heuristics.
The UBA-W system uses a parallel corpus auto-
matically constructed from DBpedia.
SWAT-E and SWAT-S use a lexical substitution
framework applied to either English or Spanish.
The SWAT-E system first performs lexical sub-
12
stitution in English, and then each substitute is
translated into Spanish. SWAT-S translates the
source sentences into Spanish, identifies the Span-
ish word corresponding to the target word, and
then it performs lexical substitution in Spanish.
TYO uses an English monolingual substitution
module, and then it translates the substitution can-
didates into Spanish using the Freedict and the
Google English-Spanish dictionary.
FCC-LS uses the probability of a word to be
translated into a candidate based on estimates ob-
tained from the GIZA alignment of the Europarl
corpus. These translations are subsequently fil-
tered to include only those that appear in a trans-
lation of the target word using Google translate.
WLVUSP determines candidates using the best
N translations of the test sentences obtained with
the Moses system, which are further filtered us-
ing an English-Spanish dictionary. USPWLV uses
candidates from an alignment of Europarl, which
are then selected using various features and a clas-
sifier tuned on the development data.
IRST-1 generates the best substitute using a PoS
constrained alignment of Moses translations of the
source sentences, with a back-off to a bilingual
dictionary. For out-of-ten, dictionary translations
are filtered using the LSA similarity between can-
didates and the sentence translation into Spanish.
IRSTbs is intended as a baseline, and it uses only
the PoS constrained Moses translation for best,
and the dictionary translations for out-of-ten.
ColEur and ColSlm use a supervised word sense
disambiguation algorithm to distinguish between
senses in the English source sentences. Trans-
lations are then assigned by using GIZA align-
ments from a parallel corpus, collected for the
word senses of interest.
7 Results
Tables 1 and 2 show the precision P and recall
R for the best and out-of-ten tasks respectively,
for normal and mode. The rows are ordered by
R. The out-of-ten systems were allowed to pro-
vide up to 10 substitutes and did not have any ad-
vantage by providing less. Since duplicates were
allowed so that a system can put more emphasis
on items it is more confident of, this means that
out-of-ten R and P scores might exceed 100%
because the credit for each of the human answers
is used for each of the duplicates (McCarthy and
Navigli, 2009). Duplicates will not help the mode
scores, and can be detrimental as valuable guesses
which would not be penalised are taken up with
Systems R P Mode R Mode P
UBA-T 27.15 27.15 57.20 57.20
USPWLV 26.81 26.81 58.85 58.85
ColSlm 25.99 27.59 56.24 59.16
WLVUSP 25.27 25.27 52.81 52.81
SWAT-E 21.46 21.46 43.21 43.21
UvT-v 21.09 21.09 43.76 43.76
CU-SMT 20.56 21.62 44.58 45.01
UBA-W 19.68 19.68 39.09 39.09
UvT-g 19.59 19.59 41.02 41.02
SWAT-S 18.87 18.87 36.63 36.63
ColEur 18.15 19.47 37.72 40.03
IRST-1 15.38 22.16 33.47 45.95
IRSTbs 13.21 22.51 28.26 45.27
TYO 8.39 8.62 14.95 15.31
DICT 24.34 24.34 50.34 50.34
DICTCORP 15.09 15.09 29.22 29.22
Table 1: best results
duplicates. In table 2, in the column marked dups,
we display the number of test items for which at
least one duplicate answer was provided. 4 Al-
though systems were perfectly free to use dupli-
cates, some may not have realised this. 5 Dupli-
cates help when a system is fairly confident of a
subset of its 10 answers.
We had anticipated a practical issue to come up
with all participants, which is the issue of different
character encodings, especially when using bilin-
gual dictionaries from the Web. While we were
counting on the participants to clean their files and
provide us with clean characters only, we ended up
with result files following different encodings (e.g,
UTF-8, ANSI), some of them including diacrit-
ics, and some of them containing malformed char-
acters. We were able to perform a basic cleaning
of the files, and transform the diacritics into their
diacriticless counterparts, however it was not pos-
sible to clean all the malformed characters without
a significant manual effort that was not possible
due to time constraints. As a result, a few of the
participants ended up losing a few points because
their translations, while being correct, contained
an invalid, malformed character that was not rec-
ognized as correct by the scorer.
There is some variation in rank order of the sys-
tems depending on which measures are used. 6
4Please note that any residual character encoding issues
were not considered by the scorer and so the number of du-
plicates may be slightly higher than if diacritics/different en-
codings had been considered.
5Also, note that some systems did not supply 10 transla-
tions. Their scores would possibly have improved if they had
done so.
6There is not a big difference between P and R because
13
Systems R P Mode R Mode P dups
SWAT-E 174.59 174.59 66.94 66.94 968
SWAT-S 97.98 97.98 79.01 79.01 872
UvT-v 58.91 58.91 62.96 62.96 345
UvT-g 55.29 55.29 73.94 73.94 146
UBA-W 52.75 52.75 83.54 83.54 -
WLVUSP 48.48 48.48 77.91 77.91 64
UBA-T 47.99 47.99 81.07 81.07 -
USPWLV 47.60 47.60 79.84 79.84 30
ColSlm 43.91 46.61 65.98 69.41 509
ColEur 41.72 44.77 67.35 71.47 125
TYO 34.54 35.46 58.02 59.16 -
IRST-1 31.48 33.14 55.42 58.30 -
FCC-LS 23.90 23.90 31.96 31.96 308
IRSTbs 8.33 29.74 19.89 64.44 -
DICT 44.04 44.04 73.53 73.53 30
DICTCORP 42.65 42.65 71.60 71.60 -
Table 2: out-of-ten results
UBA-T has the highest ranking on R for best. US-
PWLV is best at finding the mode, for best how-
ever the UBA-W and UBA-T systems (particularly
the former) both have exceptional performance for
finding the mode in the out-of-ten task, though
note that SWAT-S performs competitively given
that its duplicate responses will reduce its chances
on this metric. SWAT-E is the best system for out-
of-ten, as several of the items that were empha-
sized through duplication were also correct.
The results are much higher than for LEX-
SUB (McCarthy and Navigli, 2007). There are sev-
eral possible causes for this. It is perhaps easier
for humans, and machines to come up with trans-
lations compared to paraphrases. Though the ITA
figures are comparable on both tasks, our task con-
tained only a subset of the data in LEXSUB and we
specifically avoided data where the LEXSUB an-
notators had not been able to come up with a sub-
stitute or had labelled the instance as a name e.g.
measurements such as pound, yard or terms such
as mad in mad cow disease. Another reason for
this difference may be that there are many parallel
corpora available for training a system for this task
whereas that was not the case for LEXSUB.
8 Conclusions
In this paper we described the SemEval-2010
cross-lingual lexical substitution task, including
the motivation behind the task, the annotation pro-
cess and the scoring system, as well as the partic-
ipating systems. Nine different teams with a total
systems typically supplied answers for most items. However,
IRST-1 and IRSTbs did considerably better on precision com-
pared to recall since they did not cover all test items.
of 15 different systems participated in the task, us-
ing a variety of resources and approaches. Com-
parative evaluations using different metrics helped
determine what works well for the selection of
cross-lingual lexical substitutes.
9 Acknowledgements
The work of the first and second authors has been partially
supported by a National Science Foundation CAREER award
#0747340. The work of the third author has been supported
by a Royal Society UK Dorothy Hodgkin Fellowship. The
authors are grateful to Samer Hassan for his help with the
annotation interface.
References
Marine Carpuat and Dekai Wu. 2007. Improving statis-
tical machine translation using word sense disambigua-
tion. In Proceedings of the Joint Conference on Empir-
ical Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-CoNLL
2007), pages 61?72, Prague, Czech Republic, June. Asso-
ciation for Computational Linguistics.
Sadao Kurohashi. 2001. SENSEVAL-2 japanese translation
task. In Proceedings of the SENSEVAL-2 workshop, pages
37?44.
Els Lefever and Veronique Hoste. 2010. SemEval-2007 task
3: Cross-lingual word sense disambiguation. In Proceed-
ings of the 5th International Workshop on Semantic Eval-
uations (SemEval-2010), Uppsala, Sweden.
Diana McCarthy and Roberto Navigli. 2007. SemEval-2007
task 10: English lexical substitution task. In Proceedings
of the 4th International Workshop on Semantic Evalua-
tions (SemEval-2007), pages 48?53, Prague, Czech Re-
public.
Diana McCarthy and Roberto Navigli. 2009. The English
lexical substitution task. Language Resources and Eval-
uation Special Issue on Computational Semantic Analysis
of Language: SemEval-2007 and Beyond, 43(2):139?159.
Hwee Tou Ng and Yee Seng Chan. 2007. SemEval-2007 task
11: English lexical sample task via English-Chinese paral-
lel text. In Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007), pages 54?58,
Prague, Czech Republic.
Patrick Pantel and Dekang Lin. 2002. Discovering word
senses from text. In Proceedings of ACM SIGKDD Con-
ference on Knowledge Discovery and Data Mining, pages
613?619, Edmonton, Canada.
Philip Resnik and David Yarowsky. 2000. Distinguishing
systems and distinguishing senses: New evaluation meth-
ods for word sense disambiguation. Natural Language
Engineering, 5(3):113?133.
Hinrich Schu?tze. 1998. Automatic word sense discrimina-
tion. Computational Linguistics, 24(1):97?123.
Serge Sharoff. 2006. Open-source corpora: Using the net to
fish for linguistic data. International Journal of Corpus
Linguistics, 11(4):435?462.
Ravi Sinha, Diana McCarthy, and Rada Mihalcea. 2009.
Semeval-2010 task 2: Cross-lingual lexical substitution.
In Proceedings of the NAACL-HLT Workshop SEW-2009
- Semantic Evaluations: Recent Achievements and Future
Directions, Boulder, Colorado, USA.
14
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 493?496,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
UNT-SIMPRANK: Systems for Lexical Simplification Ranking
Ravi Sinha
University of North Texas
1155 Union Circle #311277
Denton, Texas
76203-5017
RaviSinha@my.unt.edu
Abstract
This paper presents three systems that took
part in the lexical simplification task at SE-
MEVAL 2012. Speculating on what the con-
cept of simplicity might mean for a word,
the systems apply different approaches to rank
the given candidate lists. One of the systems
performs second-best (statistically significant)
and another one performs third-best out of 9
systems and 3 baselines. Notably, the third-
best system is very close to the second-best,
and at the same time much more resource-light
in comparison.
1 Introduction
Lexical simplification (described in (Specia et al,
2012)) is a newer problem that has arisen follow-
ing a recent surge in interest in the related task of
lexical substitution (McCarthy et al, 2007). While
lexical substitution aims at making systems generate
suitable paraphrases for a target word in an instance,
which do not necessarily have to be simpler versions
of the original, it has been speculated that one pos-
sible use of the task could be lexical simplification,
in particular in the realm of making educational text
more readable for non-native speakers.
The task of lexical simplification, which thus
derives from lexical substitution, uses the same
data set, and has been introduced at the 6th
International Workshop on Semantic Evaluation
(SEMEVAL 2012), in conjunction with the First Joint
Conference on Lexical and Computational Seman-
tics (*SEM 2012). Instead of asking systems to pro-
vide substitutes, the task provides the systems with
all substitutes and asks them to be ranked.
The task provides several instances of triplets of a
context C, a target word T , and a set of gold stan-
dard substitutes S. The systems are supposed to
rank the substitutes si ? S from the simplest to the
most difficult, and match their predictions against
the provided human annotations. The organizers de-
fine simple loosely as words that can be understood
by a wide variety of people, regardless of their lit-
eracy and cognitive levels, age, and regional back-
grounds.
The task is novel in that so far most work has been
done on syntactic simplification and not on lexical
simplification. Carroll et. al. (Carroll et al, 1998)
seem to have pioneered some methodology and eval-
uation metrics in this field. Yatskar et. al. (Yatskar et
al., 2010) use an unsupervised learning method and
metadata from the Simple English Wikipedia.
2 Data
The data (trial and test, no training) have been
adopted from the original lexical substitution
task (McCarthy et al, 2007). The trial set has 300
examples, each with a context, a target word, and
a set of substitutions. The test set has 1710 exam-
ples. The organizers provide a scorer for the task,
the trial gold standard rankings, and three baselines.
The data is provided in XML format, with tags iden-
tifying the lemmas, parts of speech, instances, con-
texts and head words. The substitutions and gold
rankings are in plain text format.
493
3 Resources
Intuitively, a simple word is likely to have a high
frequency in a resource that is supposed to contain
simple words. Other factors that could intuitively in-
fluence simplicity would be the frequency in spoken
conversation, and whether the word is polysemous
or not. As such, the following resources have been
selected to contribute to the metric used in ranking
the substitutes.
3.1 Simple English Wikipedia
Simple English Wikipedia has been used before in
simplicity analysis, as described in (Yatskar et al,
2010). It is a publicly available, smaller Wikipedia
(298MB decompressed), which claims to only con-
sist of words that are somehow simple. For all the
substitute candidates, I count their frequencies of oc-
currence in this resource, and these counts serve as
a factor in computing the corresponding simplicity
scores (refer to Equation 1.)
3.2 Transcribed Spoken English Corpus
A set of spoken dialogues is also utilized in this
project to measure simplicity. Spoken language in-
tuitively contains more conversational words, and
has the same kind of resolution power as the Sim-
ple English Wikipedia when it comes to the relative
simplicity of a word. Frequency counts of all the
substitute candidates in a set of dialogue corpora is
computed, and used as another factor in the Equa-
tions 1 and 3.
3.3 WordNet
WordNet, as described in (Fellbaum, 1998), is a lex-
ical knowledge base that combines the properties of
a thesaurus with that of a semantic network. The ba-
sic entry in WordNet is a synset, which is defined as
a set of synonyms. I use WordNet 3.0, which has
over 150,000 unique words, over 110,000 synsets,
and over 200,000 word-sense pairs. For each substi-
tute, I extract the raw number of senses (for all parts
of speech possible) for that word present in Word-
Net. This count serves as yet another factor in the
proposed simplicity measure, under the hypothesis
that a simple word is used very frequently, and is
therefore polysemous.
3.4 Web1T Google N-gram Corpus
The Google Web 1T corpus (Brants and Franz,
2006) is a collection of English N-grams, ranging
from one to five N-grams, and their respective fre-
quency counts observed on the Web. The corpus was
generated from approximately 1 trillion tokens of
words from the Web, predominantly English. This
corpus is also used in both SIMPRANK and SALSA
systems, with the intuition that simpler words will
have higher counts on the Web taken as a whole.
3.5 SaLSA
SALSA (Stand-alone Lexical Substitution Ana-
lyzer) is an in-house application which accepts as in-
puts sentences with target words marked distinctly,
and then builds all possible 3-grams by substitut-
ing the target word with synonyms (and inflections
thereof). It then queries the Web1T corpus using an
in-house quick lookup application and gathers the
counts for all 3-grams. Finally, it sums the counts,
and assigns the aggregated scores to each corre-
sponding synonym and outputs a reverse-ranked list
of the synonyms. More detail about this method-
ology can be found in (Sinha and Mihalcea, 2009).
SALSA uses the exact same methodology described
in the paper, except that it is a stand-alone tool.
4 Experimental Setup
Figure 1 shows the general higher-level picture of
how the experiments have been performed. SIM-
PRANK uses five resources, including the unigram
frequency data, while SIMPRANKLIGHT does not
use the unigram frequencies.
I hypothesize that the simplicity of a word could
be represented as the Equation 1 (here cword() rep-
resents the frequency count of the word in a given
resource).
simplicity(word) =
1
len(word) + cword(SimpleWiki)
+ cword(Discourse) + cword(WordNet)
+ cword(Unigrams) (1)
This formula is very empirical in nature, in that it
has been found based on extensive experimentation
494
Figure 1: High-level schematic diagram of the experi-
ments
(Table 1). It intuitively makes sense that a simple
word is supposed to have high frequency counts in
lexical resources that are meant to be simple by de-
sign. Formally,
simplicity(word)
? frequency(SimpleResource)
?
1
length
(2)
Here, SimpleResource could be any resource
that contains simple words. Apart from frequency
counts, we could possibly also leverage morphology
for finding simplicity. Intuitively, a 3-letter word or
a 4-letter word would most likely be simpler than a
word that has a longer length. This accounts for the
length factor in the equations.
As Table 1 depicts, a lot of experiments were per-
formed where the components (counts) were mul-
tiplied instead of being added, normalized instead
of adding without normalization1, and also experi-
ments where subsets of the resources were selected.
The scores obtained using the gold standard and the
trial data are also shown in the table. The best com-
1The normalization is done by dividing by the maximum
value obtained for that particular resource
bination found (experiment 8 in the table) is outlined
in Equation 1.
Note however, that the Google Web1T corpus is
expensive in terms of money, computation time and
storage space. Thus, another set of experiments was
performed (listed as experiments 1a in Table 1 leav-
ing the unigram counts out, and it was found to work
almost just as well. This system has been labeled
SIMPRANKLIGHT and uses the formula in Equa-
tion 3.
simplicity(word) =
1
len(word) + cword(SimpleWiki)
+ cword(Discourse) + cword(WordNet)
(3)
The substitutes can then be sorted in the decreas-
ing order of simplicity scores. The substitute with
the highest simplicity score is hypothesized to be the
simplest.
Table 1: Variants of the experiments performed
SN System components Method Remarks Score
baseline no-change 0.05
baseline random 0.01
baseline unigram count (Web1T) 0.39
1 len, simplewiki, discourse, wordnet add normalize 0.20
1a len, simplewiki, discourse, wordnet add don?t normalize 0.37
2 len, simplewiki, discourse, wordnet add normalize, inc sort -0.20
3 len, simplewiki, discourse, wordnet multiply don?t normalize 0.25
4 simplewiki, discourse, wordnet add don?t normalize 0.36
4a simplewiki, discourse, wordnet add normalize 0.22
4b simplewiki, discourse, wordnet multiply don?t normalize 0.26
5 len, simplewiki, wordnet add don?t normalize 0.36
5a len, simplewiki, wordnet add normalize 0.19
5b len, simplewiki, wordnet multiply don?t normalize 0.26
6 len, discourse, wordnet add don?t normalize 0.31
6a len, discourse, wordnet add normalize 0.20
6b len, discourse, wordnet multiply don?t normalize 0.25
7 len, simplewiki, discourse add don?t normalize 0.37
7a len, simplewiki, discourse add normalize 0.22
7b len, simplewiki, discourse multiply don?t normalize 0.32
8 len, simplewiki, discourse, word-
net, unigrams
add don?t normalize 0.39
8a len, simplewiki, discourse, word-
net, unigrams
add normalize 0.22
8b len, simplewiki, discourse, word-
net, unigrams
multiply don?t normalize 0.26
9 SaLSA 0.36
Experiment 2 in Table 1 shows what happens
when an increasing-order ranking of the simplicity
scores is used. A negative score here underscores
the correctness of both the simplicity score as well
as that of the reverse-ranking.
The third system, SALSA (Stand-alone Lexical
Substitution Analyzer) is the only system out of the
495
three that takes advantage of the context provided
with the data set. It builds all possible 3-grams from
the context, replacing the target word one-by-one by
a substitute candidate (and inflections of the substi-
tute candidates). It then sums their frequency counts
in the Web1T corpus and assigns the sum to the sim-
plicity score of a particular synonym. The synonyms
can then be reverse-ranked.
5 System Standings and Discussion
For the test data, Table 2 depicts the system stand-
ings, separated by statistical significance.
Table 2: Test data system scores
Rank Team ID System ID Score
1 WLV-SHEF SimpLex 0.496
2 baseline Sim Freq 0.471
2 UNT SimpRank 0.471
2 annlor simple 0.465
3 UNT SimpRankL 0.449
4 EMNLPCPH ORD1 0.405
5 EMNLPCPH ORD2 0.393
6 SB mmSystem 0.289
7 annlor lmbing 0.199
8 baseline No Change 0.106
9 baseline Rand 0.013
10 UNT SaLSA -0.082
Surprisingly, the systems SIMPRANK and SIM-
PRANKLIGHT, which do not use the contexts pro-
vided, score much better than SALSA, which does
use the contexts. Apparently simplicity is rather a
statistical concept even for humans (the annotators
for the gold standard) and not a contextual one. Also
surprisingly, SIMPRANKLIGHT, which does not use
Google Web1T data, performs extremely well and
within 0.02 of the raw scores.
What is also surprising is the inability of all-but-
one systems to beat the baseline of using simple fre-
quency counts from Web1T, which is in turn based
entirely on statistical counts and does not take the
context into account.
A major contribution of this paper is the discovery
that other, lighter, free resources work just as well
as the expensive (in money, time and space) Web1T
data when it comes to identifying which word is sim-
ple and which one is not.
6 Future Work
I plan to extend this experiment by performing ab-
lation studies of all the individual features, play-
ing with new features, and also performing machine
learning experiments to see if supervised experi-
ments are a better way of solving the problem of
lexical simplicity ranking.
References
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Version 1.
John Carroll, Guido Minnen, Yvonne Canning, Siobhan
Devlin, and John Tait. 1998. Practical simplification
of english newspaper text to assist aphasic readers. In
In Proc. of AAAI-98 Workshop on Integrating Artificial
Intelligence and Assistive Technology, pages 7?10.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cambridge,
MA ; London, May.
Diana McCarthy, Falmer East Sussex, and Roberto Nav-
igli. 2007. Semeval-2007 task 10: English lexical
substitution task. In In Proceedings of the 4th work-
shop on Semantic Evaluations (SemEval-2007), pages
48?53.
Ravi Sinha and Rada Mihalcea. 2009. Combining lex-
ical resources for contextual synonym expansion. In
Proceedings of the International Conference RANLP-
2009, pages 404?410, Borovets, Bulgaria, September.
Association for Computational Linguistics.
Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea.
2012. Semeval-2012 task 1: English lexical simplifi-
cation. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-
Mizil, and Lillian Lee. 2010. For the sake of simplic-
ity: Unsupervised extraction of lexical simplifications
from Wikipedia. In Proceedings of the NAACL, pages
365?368.
496
