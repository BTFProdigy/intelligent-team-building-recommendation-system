Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 267?276, Prague, June 2007. c?2007 Association for Computational Linguistics
Exploiting Multi-Word Units in History-Based Probabilistic Generation
Deirdre Hogan, Conor Cafferkey, Aoife Cahill? and Josef van Genabith
National Centre for Language Technology
School of Computing, Dublin City University
Dublin 9, Ireland
dhogan,ccafferkey,josef@computing.dcu.ie
Abstract
We present a simple history-based model for
sentence generation from LFG f-structures,
which improves on the accuracy of previous
models by breaking down PCFG indepen-
dence assumptions so that more f-structure
conditioning context is used in the predic-
tion of grammar rule expansions. In addi-
tion, we present work on experiments with
named entities and other multi-word units,
showing a statistically significant improve-
ment of generation accuracy. Tested on sec-
tion 23 of the Penn Wall Street Journal Tree-
bank, the techniques described in this paper
improve BLEU scores from 66.52 to 68.82,
and coverage from 98.18% to 99.96%.
1 Introduction
Sentence generation, or surface realisation, is the
task of generating meaningful, grammatically cor-
rect and fluent text from some abstract semantic or
syntactic representation of the sentence. It is an im-
portant and growing field of natural language pro-
cessing with applications in areas such as transfer-
based machine translation (Riezler and Maxwell,
2006) and sentence condensation (Riezler et al,
2003). While recent work on generation in restricted
domains, such as (Belz, 2007), has shown promising
results there remains much room for improvement
particularly for broad coverage and robust genera-
tors, like those of Nakanishi et al (2005) and Cahill
? Now at the Institut fu?r Maschinelle Sprachverarbeitung,
Universita?t Stuttgart, Azenbergstrae 12, D-70174 Stuttgart,
Germany. aoife.cahill@ims.uni-stuttgart.de
and van Genabith (2006), which do not rely on hand-
crafted grammars and thus can easily be ported to
new languages.
This paper is concerned with sentence genera-
tion from Lexical-Functional Grammar (LFG) f-
structures (Kaplan, 1995). We present improve-
ments in previous LFG-based generation models
firstly by breaking down PCFG independence as-
sumptions so that more f-structure conditioning con-
text is included when predicting grammar rule ex-
pansions. This history-based approach has worked
well in parsing (Collins, 1999; Charniak, 2000) and
we show that it also improves PCFG-based genera-
tion.
We also present work on utilising named entities
and other multi-word units to improve generation
results for both accuracy and coverage. There has
been a limited amount of exploration into the use
of multi-word units in probabilistic parsing, for ex-
ample in (Kaplan and King, 2003) (LFG parsing)
and (Nivre and Nilsson, 2004) (dependency pars-
ing). We are not aware of any similar work on gen-
eration. In the LFG-based generation algorithm pre-
sented by Cahill and van Genabith (2006) complex
named entities (i.e. those consisting of more than
one word token) and other multi-word units can be
fragmented in the surface realization. We show that
the identification of such units may be used as a sim-
ple measure to constrain the generation model?s out-
put.
We take the generator of (Cahill and van Gen-
abith, 2006) as our baseline generator. When tested
on f-structures for all sentences from Section 23 of
the Penn Wall Street Journal (WSJ) treebank (Mar-
267
cus et al, 1993), the techniques described in this pa-
per improve BLEU score from 66.52 to 68.82. In
addition, coverage is increased from 98.18% to al-
most 100% (99.96%).
The remainder of the paper is structured as fol-
lows: in Section 2 we review related work on sta-
tistical sentence generation. Section 3 describes the
baseline generation model and in Section 4 we show
how the new history-based model improves over the
baseline. In Section 5 we describe the source of the
multi-word units (MWU) used in our experiments
and the various techniques we employ to make use
of these MWUs in the generation process. Section 6
gives experimental details and results.
2 Related Work on Statistical Generation
In (statistical) generators, sentences are generated
from an abstract linguistic encoding via the appli-
cation of grammar rules. These rules can be hand-
crafted grammar rules, such as those of (Langkilde-
Geary, 2002; Carroll and Oepen, 2005), created
semi-automatically (Belz, 2007) or, alternatively,
extracted fully automatically from treebanks (Ban-
galore and Rambow, 2000; Nakanishi et al, 2005;
Cahill and van Genabith, 2006).
Insofar as it is a broad coverage generator, which
has been trained and tested on sections of the WSJ
corpus, our generator is closer to the generators
of (Bangalore and Rambow, 2000; Langkilde-Geary,
2002; Nakanishi et al, 2005) than to those designed
for more restricted domains such as weather fore-
cast (Belz, 2007) and air travel domains (Ratna-
parkhi, 2000).
Another feature which characterises statistical
generators is the probability model used to select the
most probable sentence from among the space of all
possible sentences licensed by the grammar. One
generation technique is to first generate all possible
sentences, storing them in a word lattice (Langkilde
and Knight, 1998) or, alternatively, a generation for-
est, a packed represention of alternate trees proposed
by the generator (Langkilde, 2000), and then select
the most probable sequence of words via an n-gram
language model.
Increasingly syntax-based information is being
incorporated directly into the generation model. For
example, Carroll and Oepen (2005) describe a sen-
tence realisation process which uses a hand-crafted
HPSG grammar to generate a generation forest. A
selective unpacking algorithm allows the extraction
of an n-best list of realisations where realisation
ranking is based on a maximum entropy model. This
unpacking algorithm is used in (Velldal and Oepen,
2005) to rank realisations with features defined over
HPSG derivation trees. They achieved the best re-
sults when combining the tree-based model with an
n-gram language model.
Nakanishi et al (2005) describe a treebank-
extracted HPSG-based chart generator. Importing
techniques developed for HPSG parsing, they apply
a log linear model to a packed representation of all
alternative derivation trees for a given input. They
found that a model which included syntactic infor-
mation outperformed a bigram model as well as a
combination of bigram and syntax model.
The probability model described in this paper also
incorporates syntactic information, however, unlike
the discriminative HPSG models just described, it
is a generative history- and PCFG-based model.
While Belz (2007) and Humphreys et al (2001)
mention the use of contextual features for the rules
in their generation models, they do not provide de-
tails nor do they provide a formal probability model.
To the best of our knowledge this is the first paper
providing a probabilistic generative, history-based
generation model.
3 Surface Realisation from f-Structures
Cahill and van Genabith (2006) present a prob-
abilistic surface generation model for LFG (Ka-
plan, 1995). LFG is a constraint-based theory
of grammar, which analyses strings in terms of
c(onstituency)-structure and f(unctional)-structure
(Figure 1). C-structure is defined in terms of CFGs,
and f-structures are recursive attribute-value ma-
trices which represent abstract syntactic functions
(such as SUBJect, OBJect, OBLique, COMPlement
(sentential), ADJ(N)unct), agreement, control, long-
distance dependencies and some semantic informa-
tion (e.g. tense, aspect).
C-structures and f-structures are related in a pro-
jection architecture in terms of a piecewise corre-
spondence ?.1 The correspondence is indicated in
1Our formalisation follows (Kaplan, 1995).
268
S
?=?
NP VP
(? SUBJ)= ? ?=?
NNP V NP
?=? ?=? (? OBJ)= ?
Susan contacted PRP
(? PRED) = ?Susan? (? PRED) = ?contact? ?=?
(? NUM) = SG (? TENSE) = past
(? PERS) = 3 her
(? PRED) = ?pro?
(? NUM) = SG
(? PERS) = 3
f1:
?
?
?
?
?
?
PRED ?CONTACT?(?SUBJ)(?OBJ)??
SUBJ f2:
[
PRED ?SUSAN?
NUM SG
PERS 3
]
OBJ f2:
[
PRED ?PRO?
NUM SG
PERS 3
]
TENSE PAST
?
?
?
?
?
?
Figure 1: C- and f-structures with ? links for the sentence Susan contacted her.
terms of the curvy arrows pointing from c-structure
nodes to f-structure components in Figure 1. Given
a c-structure node ni, the corresponding f-structure
component fj is ?(ni). F-structures and the c-
structure/f-structure correspondence are described
in terms of functional annotations on c-structure
nodes (CFG grammar rules). An equation of the
form (?F) = ? states that the f-structure associated
with the mother of the current c-structure node (?)
has an attribute (grammatical function) (F), whose
value is the f-structure of the current node (?).
The up-arrows and down-arrows are shorthand for
?(M(ni)) = ?(ni) where ni is the c-structure node
annotated with the equation.2
Treebest := argmaxTreeP (Tree|F-Str) (1)
P (Tree|F-Str) :=
?
X ? Y in Tree
Feats = {ai|?vj(?(X))ai = vj}
P (X ? Y |X, Feats) (2)
The generation model of (Cahill and van Gen-
abith, 2006) maximises the probability of a tree
given an f-structure (Eqn. 1), and the string gener-
ated is the yield of the highest probability tree. The
generation process is guided by purely local infor-
mation in the input f-structure: f-structure annotated
CFG rules (LHS ? RHS) are conditioned on their
LHSs and on the set of features/attributes Feats =
{ai|?vj?(LHS)ai = vj}3 ?-linked to the LHS (Eqn.
2M is the mother function on CFG tree nodes.
3In words, Feats is the set of top level features/attributes
(those attributes ai for which there is a value vi) of the f-
structure ? linked to the LHS.
2). Table 1 shows a generation grammar rule and
conditioning features extracted from the example in
Figure 1. The probability of a tree is decomposed
into the product of the probabilities of the f-structure
annotated rules (conditioned on the LHS and local
Feats) contributing to the tree. Conditional proba-
bilities are estimated using maximum likelihood es-
timation.
grammar rule local conditioning features
S(?=?)? NP(?SUBJ=?) VP(?=?) S(?=?), {SUBJ,OBJ,PRED,TENSE}
Table 1: Example grammar rule (from Figure 1).
Cahill and van Genabith (2006) note that condi-
tioning f-structure annotated generation rules on lo-
cal features (Eqn. 2) can sometimes cause the model
to make inappropriate choices. Consider the follow-
ing scenario where in addition to the c-/f-structure in
Figure 1, the training set contains the c-/f-structure
displayed in Figure 2.
From Figures 1 and 2, the model learns (among
others) the generation rules and conditional proba-
bilities displayed in Tables 2 and 3.
F-Struct Feats Grammar Rules Prob
{SUBJ, OBJ, PRED} S(?=?) ? NP(?SUBJ=?) VP(?=?) 1
{SUBJ, OBJ, PRED} VP(?=?) ? V(?=?) NP(?OBJ=?) 1
{NUM, PER, GEN} NP(?SUBJ=?) ? NNP(?=?) 0.5
{NUM, PER, GEN} NP(?SUBJ=?) ? PRP(?=?) 0.5
{NUM, PER, GEN} NP(?OBJ=?) ? PRP(?=?) 1
Table 2: A sample of internal grammar rules ex-
tracted from Figures 1 and 2.
Given the input f-structure (for She
accepted) in Figure 3, (and assuming suit-
able generation rules for intransitive VPs and
accepted) the model would produce the inappro-
priate highest probability tree of Figure 4 with an
incorrect case for the pronoun in subject position.
269
S
?=?
NP VP
(? SUBJ)= ? ?=?
PRP V NP
?=? ?=? (? OBJ)= ?
She hired PRP
(? PRED) = ?pro? (? PRED) = ?hire? ?=?
(? NUM) = SG (? TENSE) = past
(? PERS) = 3 her
(? PRED) = ?pro?
(? NUM) = SG
(? PERS) = 3
f1 :
?
?
?
?
?
?
PRED ?HIRE?(?SUBJ)(?OBJ)??
SUBJ f2 :
[
PRED ?PRO?
NUM SG
PERS 3
]
OBJ f2 :
[
PRED ?PRO?
NUM SG
PERS 3
]
TENSE PAST
?
?
?
?
?
?
Figure 2: C- and f-structures with ? links for the sentence She hired her.
F-Struct Feats Grammar Rules Prob
{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP(?=?) ? she 0.33
{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP(?=?) ? her 0.66
Table 3: A sample of lexical item rules extracted
from Figures 1 and 2.
?
?
?
?
?
?
SUBJ
?
?
PRED pro
NUM sg
PERS 3
GEND fem
?
?
PRED accept
TENSE past
?
?
?
?
?
?
Figure 3: Input f-structure for She accepted.
To solve the problem, Cahill and van Gen-
abith (2006) apply an automatic generation gram-
mar transformation to their training data: they au-
tomatically label CFG nodes with additional case
information and the model now learns the new im-
proved generation rules of Tables 4 and 5. Note
how the additional case labelling subverts the prob-
lematic independence assumptions of the probabil-
ity model and communicates the fact that a subject
NP has to be realised as nominative case from the
S ? NP-nom VP production, via the intermediate
NP-nom ? PRP-nom, down to the lexical produc-
tion PRP-nom ? she. The labelling guarantees that,
given the example f-structure in Figure 3, the model
generates the correct string she accepted.
F-Struct Feats Grammar Rules
{SUBJ, OBJ, PRED} S(?=?) ? NP-nom(?SUBJ=?) VP(?=?)
{SUBJ, OBJ, PRED} VP(?=?) ? V(?=?) NP-acc(?OBJ=?)
{NUM, PER, GEN} NP-nom(?SUBJ=?) ? PRP-nom(?=?)
{NUM, PER, GEN} NP-nom(?SUBJ=?) ? NNP-nom(?=?)
{NUM, PER, GEN} NP-acc(?OBJ=?) ? PRP-acc(?=?)
Table 4: Internal grammar rules with case markings.
S
?=?
NP VP
(? SUBJ)= ? ?=?
PRP V
?=? ?=?
her accepted
(? PRED) = ?pro? (? PRED) = ?hire?
(? NUM) = SG (? TENSE) = past
(? PERS) = 3
Figure 4: Inappropriate output: her accepted.
F-Struct Feats Grammar Rules
{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP-nom(?=?) ? she
{PRED=PRO,NUM=SG PER=3, GEN=FEM} PRP-acc(?=?) ? her
Table 5: Lexical item rules with case markings
4 A History-Based Generation Model
The automatic generation grammar transform pre-
sented in (Cahill and van Genabith, 2006) provides
a solution to coarse-grained and (in fact) inappropri-
ate independence assumptions in the basic genera-
tion model. However, there is a sense in which the
proposed cure improves on the symptoms, but not
the cause of the problem: it weakens independence
assumptions by multiplying and hence increasing
the specificity of conditioning CFG category labels.
There is another option available to us, and that is
the option we will explore in this paper: instead of
applying a generation grammar transform, we will
improve the f-structure-based conditioning of the
generation rule probabilities. In the original model,
rules are conditioned on purely local f-structure con-
text: the set of features/attributes ?-linked to the
LHS of a grammar rule. As a direct consequence
of this, the conditioning (and hence the model) can-
not not distinguish between NP, PRP and NNP rules
270
appropriate to e.g. subject (SUBJ) or object con-
texts (OBJ) in a given input f-structure. However,
the required information can easily be incorporated
into the generation model by uniformly conditioning
generation rules on their parent (mother) grammati-
cal function, in addition to the local ?-linked feature
set. This additional conditioning has the effect of
making the choice of generation rules sensitive to
the history of the generation process, and, we argue,
provides a simpler, more uniform, general, intuitive
and natural probabilistic generation model obviating
the need for CFG-grammar transforms in the origi-
nal proposal of (Cahill and van Genabith, 2006).
In the new model, each generation rule is now
conditioned on the LHS rule CFG category, the set
of features ?-linked to LHS and the parent grammat-
ical function of the f-structure ?-linked to LHS. In a
given c-/f-structure pair, for a CFG node n, the par-
ent grammatical function of the f-structure ?-linked
to n is that grammatical function GF, which, if we
take the f-structure ?-linked to the mother M(n), and
apply it to GF, returns the f-structure ?-linked to n:
(?(M(n))GF) = ?(n).
The basic idea is best explained by way of an
example. Consider again Figure 1. The mother
grammatical function of the f-structure f2 asso-
ciated with node NP(?SUBJ=?) and its daughter
NNP(?=?) (via the ?=? functional annotation) is
SUBJ, as (?(M(n2))SUBJ) = ?(n2), or equivalently
(f1SUBJ) = f2.
Given Figures 1 and 2 as training set, the im-
proved model learns the generation rules (the mother
grammatical function of the outermost f-structure is
assumed to be a dummy TOP grammatical function)
of Tables 6 and 7.
F-Struct Feats Grammar Rules
{SUBJ, OBJ, PRED, TOP} S(?=?) ? NP(?SUBJ=?) VP(?=?)
{SUBJ, OBJ, PRED, TOP} VP(?=?) ? V(?=?) NP(?OBJ=?)
{NUM, PER, GEN, SUBJ} NP(?SUBJ=?) ? PRP(?=?)
{NUM, PER, GEN, OBJ} NP(?OBJ=?) ? PRP(?=?)
{NUM, PER, GEN, SUBJ} NP(?SUBJ=?) ? NNP(?=?)
Table 6: Grammar rules with extra feature extracted
from F-Structures.
Note, that for our example the effect of the uni-
form additional conditioning on mother grammat-
ical function has the same effect as the genera-
tion grammar transform of (Cahill and van Gen-
abith, 2006), but without the need for the gram-
F-Struct Feats Grammar Rules
{PRED=PRO,NUM=SG PER=3, GEN=FEM, SUBJ} PRP(?=?) ? she
{PRED=PRO,NUM=SG PER=3, GEN=FEM, OBJ} PRP(?=?) ? her
Table 7: Lexical item rules.
mar transform. Given the input f-structure in Fig-
ure 3, the model will generate the correct string
she accepted. In addition, uniform condition-
ing on mother grammatical function is more general
than the case-phenomena specific generation gram-
mar transform of (Cahill and van Genabith, 2006),
in that it applies to each and every sub-part of a
recursive input f-structure driving generation, mak-
ing available relevant generation history (context) to
guide local generation decisions.
The new history-based probabilistic generation
model is defined as:
P (Tree|F-Str) :=
?
X ? Y in Tree
Feats = {ai|?vj(?(X))ai = vj}
(?(M(X)))GF = ?(X)
P (X ? Y |X, Feats,GF) (3)
Note that the new conditioning feature, the f-
structure mother grammatical function, GF, is avail-
able from structure previously generated in the c-
structure tree. As such, it is part of the history of
the tree, i.e. it has already been generated in the top-
down derivation of the tree. In this way, the gen-
eration model resembles history-based models for
parsing (Black et al, 1992; Collins, 1999; Charniak,
2000). Unlike, say, the parent annotation for parsing
of (Johnson, 1998) the parent GF feature for a par-
ticular node expansion is not merely extracted from
the parent node in the c-structure tree, but is some-
times extracted from an ancestor node further up the
c-structure tree via intervening ?=? functional an-
notations.
Section 6 provides evaluation results for the new
model on section 23 of the Penn treebank.
5 Multi-Word Units
In another effort to improve generator accuracy over
the baseline model we explored the use of multi-
word units in generation. We expect that the identi-
fication of MWUs may be useful in imposing word-
order constraints and reducing the complexity of the
generation task. Take, for example, the following
271
??
?
?
?
?
?
?
APP
?
?
?
?
?
?
ADJUNCT
[
PRED ?New?
NUM sg
PERS 3
]
PRED ?York?
NUM sg
PERS 3
?
?
?
?
?
?
?
?
?
?
?
?
?
?
[
APP
[
PRED ?New York?
NUM sg
PERS 3
]]
?
?
?
?
?
?
?
?
APP
?
?
?
?
?
?
ADJUNCT
[
PRED ?New?/NE1 1
NUM sg
PERS 3
]
PRED ?York?/NE1 2
NUM sg
PERS 3
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Figure 5: Three different f-structure formats. From left to right: the original f-structure format; the MWU
chunk format; the MWU mark-up format.
two sentences which show the gold version of a sen-
tence followed by the version of the sentence pro-
duced by the generator:
Gold By this time , it was 4:30 a.m. in New York ,
and Mr. Smith fielded a call from a New York
customer wanting an opinion on the British
stock market , which had been having trou-
bles of its own even before Friday ?s New York
market break .
Test By this time , in New York , it was 4:30 a.m.
, and Mr. Smith fielded a call from New a
customer York , wanting an opinion on the
market British stock which had been having
troubles of its own even before Friday ?s New
York market break .
The gold version of the sentence contains a multi-
word unit, New York, which appears fragmented in
the generator output. If multi-word units were either
treated as one token throughout the generation pro-
cess, or, alternatively, if a constraint were imposed
on the generator such that multi-word units were al-
ways generated in the correct order, then this should
help improve generation accuracy. In Section 5.1
we describe the various techniques that were used
to incorporate multi-word units into the generation
process and in 5.2 we detail the different types and
sources of multi-word unit used in the experiments.
Section 6 provides evaluation results on test and de-
velopment sets from the WSJ treebank.
5.1 Incorporating MWUs into the Generation
Process
We carried out three types of experiment which, in
different ways, enabled the generation process to
respect the restrictions on word-order provided by
multi-word units. For the first experiments (type
1), the WSJ treebank training and test data were
altered so that multi-word units are concatenated
into single words (for example, New York becomes
New York). As in (Cahill and van Genabith, 2006) f-
structures are generated from the (now altered) tree-
bank and from this data, along with the treebank
trees, the PCFG-based grammar, which is used for
training the generation model, is extracted. Simi-
larly, the f-structures for the test and development
sets are created from Penn Treebank trees which
have been modified so that multi-word units form
single units. The leftmost and middle f-structures in
Figure 5 show an example of an original f-structure
format and a named-entity chunked format, respec-
tively. Strings output by the generator are then post-
processed so that the concatenated word sequences
are converted back into single words.
In the second experiment (type 2) only the test
data was altered with no concatenation of MWUs
carried out on the training data.
In the final experiments (type 3), instead of con-
catenating named entities, a constraint is introduced
to the generation algorithm which penalises the gen-
eration of sequences of words which violate the in-
ternal word order of named entities. The input is
marked-up in such a way that, although named en-
tities are no longer chunked together to form single
words, the algorithm can read which items are part
of named entities. See the rightmost f-structure in
Figure 5 for an example of an f-structure marked-
up in this way. The tag NE1 1, for example, indi-
cates that the sub-f-structure is part of a named iden-
tity with id number 1 and that the item corresponds
to the first word of the named entity. The baseline
generation algorithm, following Kay (1996)?s work
on chart generation, already contains the hard con-
straint that when combining two chart edges they
must cover disjoint sets of words. We added an ad-
ditional constraint which prevents edges from being
combined if this would result in the generation of
a string which contained a named entity which was
272
either incomplete or where the words in the named
entity were generated in the wrong order.
5.2 Types of MWUs used in Experiments
We carry out experiments with multi-word units
from three different sources. First, we use the output
of the maximum entropy-based named entity recog-
nition system of (Chieu and Ng, 2003). This sys-
tem identifies four types of named entity: person,
organisation, location, and miscellaneous. Addition-
ally we use a dictionary of candidate multi-word ex-
pressions based on a list from the Stanford Multi-
word Expression Project4. Finally, we also carry out
experiments with multi-word units extracted from
the BBN Pronoun Coreference and Entity Type Cor-
pus (Weischedel and Brunstein, 2005). This supple-
ments the Penn WSJ treebank?s one million words of
syntax-annotated Wall Street Journal text with addi-
tional annotations of 23 named entity types, includ-
ing nominal-type named entities such as person, or-
ganisation, location, etc. as well as numeric types
such as date, time, quantity and money. Since the
BBN corpus data is very comprehensive and is hand-
annotated we take this be be a gold standard, repre-
senting an upper bound for any gains that might be
made by identifying complex named entities in our
experiments.5 Table 8 gives examples of the various
types of MWUs identified by the three sources.
For our purposes we are not concerned with the
distinctions between different types of named enti-
ties; we are merely exploiting the fact that they may
be treated as atomic units in the generation model. In
all cases we disregard multi-word units that cross the
original syntactic bracketing of the WSJ treebank.
An overview of the various types of multi-word units
used in our experiments is presented in Table 9.
6 Experimental Evaluation
All experiments were carried out on the WSJ tree-
bank with sections 02-21 for training, section 24 for
development and section 23 for final test results. The
LFG annotation algorithm of (Cahill et al, 2004)
was used to produce the f-structures for develop-
ment, test and training sets.
4mwe.stanford.edu
5Although it is possible there are other types of MWUs that
may be more suitable to the task than the named entities identi-
fied by BBN, so further gains might be possible.
MWU type Examples
Names Martha Matthews
Yoshio Hatakeyama
Organisations Rolls-Royce Motor Cars Inc.
Washington State University
Locations New York City
New Zealand
Time expressions October 19th
two years ago
the 21st century
Quantities $2.7 million to $3 million
about 25 %
60 mph
Prepositional expressions in fact
at the time
on average
Table 8: Examples of some of the types of MWU
from the three different sources.
average number average length
(Chieu and Ng, 2003) 0.61 2.40
Stanford MWE Project 0.10 2.48
BBN Corpus 1.15 2.66
Table 9: Average number of MWUs per sentence
and average MWU length in the WSJ treebank
grouped by MWU source.
Table 10 shows the final results for section 23. For
each test we present BLEU score results as well as
String Edit Distance and coverage. We measure sta-
tistical significance using two different tests. First
we use a bootstrap resampling method, popular for
machine translation evaluations, to measure the sig-
nificance of improvements in BLEU scores, with a
resampling rate of 1000.6 We also calculated the
significance of an increase in String Edit Distance
by carrying out a paired t-test on the mean differ-
ence of the String Edit Distance scores. In Table 10,
? means significant at level 0.005. > means signif-
icant at level 0.05.
In Table 10, Baseline gives the results of the
generation algorithm of (Cahill and van Genabith,
2006). HB Model refers to the improved model
with the increased history context, as described in
Section 4. The results, where for example the
BLEU score rises from 66.52 to 67.24, show that
even increasing the conditioning context by a limited
6Scripts for running the bootstrapping method carried
out in our evaluation are available for download at projec-
tile.is.cs.cmu.edu/research/public/tools/bootStrap/tutorial.htm
273
Section 23 (2416 sentences)
Model BLEU StringEd Coverage BLEU Bootstrap Signif StringEd Paired T-Test
1. Baseline 66.52 68.69 98.18
2. HB Model 67.24 69.89 99.88 ? 1 ? 1
3. +MWU Best Automatic 67.81 70.36 99.92 ? 2 ? 2
4. MWU BBN 68.82 70.92 99.96 ? 3 > 3
Table 10: Results on Section 23 for all sentence lengths.
amount increases the accuracy of the system signif-
icantly for both BLEU and String Edit Distance. In
addition, coverage goes up from 98.18% to 99.88%.
+MWU Best Automatic displays our best results
using automatically identified named entities. These
were achieved using experiment type 2, described
in Section 5, with the MWUs produced by (Chieu
and Ng, 2003). Results displayed in Table 10 up
to this point are cumulative. The final row in Ta-
ble 10, MWU BBN, shows the best results with BBN
MWUs: the history-based model with BBN multi-
word units incorporated in a type 1 experiment.
We now discuss the various MWU experiments
in more detail. See Table 11 for a breakdown of
the MWU experiment results on the development
set, WSJ section 24. Our baseline for these exper-
iments is the history-based generator presented in
Section 4. For each experiment type described in
Section 5.1 we ran three experiments, varying the
source of MWUs. First, MWUs came from the auto-
matic NE recogniser of (Chieu and Ng, 2003), then
we added the MWUs from the Stanford list and fi-
nally we ran tests with MWUs extracted from the
BBN corpus.
Our first set of experiments (type 1), where both
training data and development set data were MWU-
chunked, produced the worst results for the automat-
ically chunked MWUs. BLEU score accuracy actu-
ally decreased for the automatically chunked MWU
experiments. In an error analysis of type 1 ex-
periments with (Chieu and Ng, 2003) concatenated
MWUs, we inspected those sentences where accu-
racy had decreased from the baseline. We found
that for over half (51.5%) of these sentences, the in-
put f-structures contained no multi-word units at all.
The problem for these sentences therefore lay with
the probabilistic grammar extracted from the MWU-
chunked training data. When the source of MWU
for the type 1 experiments was the BBN, however,
accuracy improved significantly over the baseline
and the result is the highest accuracy achieved over
all experiment types. One possible reason for the
low accuracy scores in the type 1 experiments with
the (Chieu and Ng, 2003) MWU chunked data could
be noisy MWUs which negatively affect the gram-
mar. For example, the named entity recogniser
of (Chieu and Ng, 2003) achieves an accuracy of
88.3% on section 23 of the Penn Treebank.
In order to avoid changing the grammar through
concatenation of MWU components (as in exper-
iment type 1) and thus risking side-effects which
cause some heretofore likely constructions become
less likely and vice versa, we ran the next set of ex-
periments (type 2) which leave the original grammar
intact and alter the input f-structures only. These
experiments were more successful overall and we
achieved an improvement over the baseline for both
BLEU and String Edit Distance scores with all
MWU types. As can be seen from Table 11 the
best score for automatically chunked MWUs are
with the (Chieu and Ng, 2003) MWUs. Accuracy
decreases marginally when we added the Stanford
MWUs. In our final set of experiments (type 3) al-
though the accuracy for all three types of MWUs
improves over the baseline, accuracy is a little be-
low the type 2 experiments.
It is difficult to compare sentence generators since
the information contained in the input varies greatly
between systems, systems are evaluated on different
test sets and coverage also varies considerably. In
order to compare our system with those of (Nakan-
ishi et al, 2005) and (Langkilde-Geary, 2002) we
report our best results with automatically acquired
MWUs for sentences of ? 20 words in length on
section 23: our system gets coverage of 100% and a
BLEU score of 71.39. For the same test set Nakan-
ishi et al (2005) achieved coverage of 90.75 and a
BLEU score of 77.33. Langkilde-Geary (2002) re-
274
Section 24 (1346 sentences)
Model MWUs BLEU StringEd Coverage
HB Model 65.85 69.93 99.93
type 1 (Chieu and Ng, 2003) 65.81 70.34 99.93
(training and test data chunked) +Stanford MWEs 64.81 69.67 99.93
BBN 67.24 71.46 99.93
type 2 (Chieu and Ng, 2003) 66.37 70.26 99.93
(test data chunked) +Stanford MWEs 66.28 70.21 99.93
BBN 66.84 70.74 99.93
type 3 (Chieu and Ng, 2003) 66.30 70.12 100
(internal generation constraint) +Stanford MWEs 66.07 70.02 99.93
BBN 66.45 70.14 99.93
Table 11: Results on Section 24, all sentence lengths.
ports 82.7% coverage and a BLEU score of 75.7%
on the same test set with the ?permute,no dir? type
input. Langkilde-Geary (2002) report results for ex-
periments with varying levels of linguistic detail in
the input given to the generator. As with Nakanishi
et al (2005) we find the ?permute,no dir? type of in-
put is most comparable to the level of information
contained in our input f-structures. Finally, the sym-
bolic generator of Callaway (2003) reports a Sim-
ple String Accuracy score of 88.84 and coverage of
98.7% on section 23 for all sentence lengths.
7 Conclusion and Future Work
We have presented techniques which improve the ac-
curacy of an already state-of-art surface generation
model. We found that a history-based model that
increases conditioning context in PCFG style rules
by simply including the grammatical function of the
f-structure parent, improves generator accuracy. In
the future we will experiment with increasing condi-
tioning context further and using more sophisticated
smoothing techniques to avoid sparse data problems
when conditioning is increased.
We have also demonstrated that automatically ac-
quired multi-word units can bring about moderate,
but significant, improvements in generator accuracy.
For automatically acquired MWUs, we found that
this could best be achieved by concatenating input
items when generating the f-structure input to the
generator, while training the input generation gram-
mar on the original (i.e. non-MWU concatenated)
sections of the treebank. Relying on the BBN cor-
pus as a source of multi-word units, we gave an up-
per bound to the potential usefulness of multi-word
units in generation and showed that automatically
acquired multi-word units, encouragingly, give re-
sults not far below the upper bound.
References
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proceedings of the 18th COLING.
Anja Belz. 2007. Probabilistic generation of wether fore-
cast texts. In Proceedings of NAACL-HLT.
Ezra Black, Fred Jelinek, John Lafferty, David M. Mager-
man, Robert Mercer, and Salim Roukos. 1992. To-
wards history-based grammars: Using richer models
for probabilistic parsing. In Proceeding of the 5th
DARPA Speech and Language Workshop.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
LFG approximations. In Proceedings of the 44th ACL.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef van
Genabith, and Andy Way. 2004. Long-distance de-
pendency resolution in automatically acquired wide-
coverage PCFG-based LFG approximations. In Pro-
ceedings of the 42nd ACL.
Charles B. Callaway. 2003. Evaluating coverage for
large symbolic NLG grammars. In In Proceedings of
the 18th IJCAI.
John A. Carroll and Stephan Oepen. 2005. High ef-
ficiency realization for a wide-coverage unification
grammar. In Proceedings of IJCNLP.
Eugene Charniak. 2000. A maximum entropy-inspired
parser. In Proceedings of the 1st NAACL.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach. In
Proceedings of the CoNLL.
275
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Kevin Humphreys, Mike Calcagno, and David Weise.
2001. Reusing a statistical language model for gen-
eration. In Proceedings of the 8th European Workshop
on Natural Language Generation (EWNLG).
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24.
Ronald M. Kaplan and Tracy Holloway King. 2003.
Low-level mark-up and large-scale LFG grammar pro-
cessing. In Proceedings of the Lexical Functional
Grammar Conference.
Ron Kaplan. 1995. The formal architecture of
lexical-functional grammar. In Dalrymple, Kaplan,
Maxwell, and Zaenen, editors, Formal Issues in
Lexical-Functional Grammar, pages 7?27. CSLI Pub-
lications.
Martin Kay. 1996. Chart generation. In Proceedings of
the 34th ACL.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proceedings of the 36th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 17th In-
ternational Conference on Computational Linguistics
(ACL-COLING).
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proceedings of the 2nd INLG.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of the 1st NAACL.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic models for disambiguation of an
HPSG-based chart generator. In Proceedings of the
9th IWPT.
Joakim Nivre and Jens Nilsson. 2004. Multiword units
in syntactic parsing. In Workshop on Methodologies
and Evaluation of Multiword Units in Real-World Ap-
plications.
Adwait Ratnaparkhi. 2000. Trainable methods for sur-
face natural language generation. In Proceedings of
the 1st NAACL.
Stefan Riezler and John T. Maxwell. 2006. Grammat-
ical machine translation. In Proceedings of the 6th
NAACL.
Stefan Riezler, Tracy H. King, Richard Crouch, and An-
nie Zaenen. 2003. Statistical sentence condensation
using ambiguity packing and stochastic disambigua-
tion methods for lexical-functional grammar. In Pro-
ceedings of the 3rd NAACL.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proceedings
of the MT-Summit.
Ralph Weischedel and Ada Brunstein, 2005. BBN pro-
noun coreference and entity type corpus. Technical
Report.
276
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 680?687,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Coordinate Noun Phrase Disambiguation in a Generative Parsing Model
Deirdre Hogan?
Computer Science Department
Trinity College Dublin
Dublin 2, Ireland
dhogan@computing.dcu.ie
Abstract
In this paper we present methods for im-
proving the disambiguation of noun phrase
(NP) coordination within the framework of a
lexicalised history-based parsing model. As
well as reducing noise in the data, we look at
modelling two main sources of information
for disambiguation: symmetry in conjunct
structure, and the dependency between con-
junct lexical heads. Our changes to the base-
line model result in an increase in NP coor-
dination dependency f-score from 69.9% to
73.8%, which represents a relative reduction
in f-score error of 13%.
1 Introduction
Coordination disambiguation is a relatively little
studied area, yet the correct bracketing of coordina-
tion constructions is one of the most difficult prob-
lems for natural language parsers. In the Collins
parser (Collins, 1999), for example, dependencies
involving coordination achieve an f-score as low as
61.8%, by far the worst performance of all depen-
dency types.
Take the phrase busloads of executives and their
wives (taken from the WSJ treebank). The coordi-
nating conjunction (CC) and and the noun phrase
their wives could attach to the noun phrase exec-
utives, as illustrated in Tree 1, Figure 1. Alterna-
tively, their wives could be incorrectly conjoined to
the noun phrase busloads of executives as in Tree 2,
Figure 1.
? Now at the National Centre for Language Technology,
Dublin City University, Ireland.
As with PP attachment, most previous attempts
at tackling coordination as a subproblem of parsing
have treated it as a separate task to parsing and it
is not always obvious how to integrate the methods
proposed for disambiguation into existing parsing
models. We therefore approach coordination disam-
biguation, not as a separate task, but from within the
framework of a generative parsing model.
As noun phrase coordination accounts for over
50% of coordination dependency error in our base-
line model we focus on NP coordination. Us-
ing a model based on the generative parsing model
of (Collins, 1999) Model 1, we attempt to improve
the ability of the parsing model to make the correct
coordination decisions. This is done in the context
of parse reranking, where the n-best parses output
from Bikel?s parser (Bikel, 2004) are reranked ac-
cording to a generative history-based model.
In Section 2 we summarise previous work on co-
ordination disambiguation. There is often a consid-
erable bias toward symmetry in the syntactic struc-
ture of two conjuncts and in Section 3 we introduce
new parameter classes to allow the model to prefer
symmetry in conjunct structure. Section 4 is con-
cerned with modelling the dependency between con-
junct head words and begins by looking at how the
different handling of coordination in noun phrases
and base noun phrases (NPB) affects coordination
disambiguation.1 We look at how we might improve
the model?s handling of coordinate head-head de-
pendencies by altering the model so that a common
1A base noun phrase, as defined in (Collins, 1999), is a noun
phrase which does not directly dominate another noun phrase,
unless that noun phrase is possessive.
680
1. NP
NP
NPB
busloads
PP
of NP
NP
NPB
executives
and NP
NPB
their wives
2. NP
NP
NPB
busloads
PP
of NP
NPB
executives
and NP
NPB
their wives
Figure 1: Tree 1. The correct noun phrase parse.
Tree 2. The incorrect parse for the noun phrase.
parameter class is used for coordinate word prob-
ability estimation in both NPs and NPBs. In Sec-
tion 4.2 we focus on improving the estimation of
this parameter class by incorporating BNC data, and
a measure of word similarity based on vector cosine
similarity, to reduce data sparseness. In Section 5 we
suggest a new head-finding rule for NPBs so that the
lexicalisation process for coordinate NPBs is more
similar to that of other NPs.
Section 6 examines inconsistencies in the annota-
tion of coordinate NPs in the Penn Treebank which
can lead to errors in coordination disambiguation.
We show how some coordinate noun phrase incon-
sistencies can be automatically detected and cleaned
from the data sets. Section 7 details how the model is
evaluated, presents the experiments made and gives
a breakdown of results.
2 Previous Work
Most previous attempts at tackling coordination
have focused on a particular type of NP coordination
to disambiguate. Both Resnik (1999) and Nakov and
Hearst (2005) consider NP coordinations of the form
n1 and n2 n3 where two structural analyses are pos-
sible: ((n1 and n2) n3) and ((n1) and (n2 n3)). They
aim to show more structure than is shown in trees
following the Penn guidelines, whereas in our ap-
proach we aim to reproduce Penn guideline trees.
To resolve the ambiguities, Resnik combines num-
ber agreement information of candidate conjoined
nouns, an information theoretic measure of semantic
similarity, and a measure of the appropriateness of
noun-noun modification. Nakov and Hearst (2005)
disambiguate by combining Web-based statistics on
head word co-occurrences with other mainly heuris-
tic information sources.
A probabilistic approach is presented in (Gold-
berg, 1999), where an unsupervised maximum en-
tropy statistical model is used to disambiguate coor-
dinate noun phrases of the form n1 preposition n2
cc n3. Here the problem is framed as an attachment
decision: does n3 attach ?high? to the first noun, n1,
or ?low? to n2?
In (Agarwal and Boggess, 1992) the task is to
identify pre-CC conjuncts which appear in text that
has been part-of-speech (POS) tagged and semi-
parsed, as well as tagged with semantic labels spe-
cific to the domain. The identification of the pre-
CC conjunct is based on heuristics which choose the
pre-CC conjunct that maximises the symmetry be-
tween pre- and post-CC conjuncts.
Insofar as we do not separate coordination dis-
ambiguation from the overall parsing task, our ap-
proach resembles the efforts to improve coordi-
nation disambiguation in (Kurohashi, 1994; Rat-
naparkhi, 1994; Charniak and Johnson, 2005).
In (Kurohashi, 1994) coordination disambiguation
is carried out as the first component of a Japanese
dependency parser using a technique which calcu-
lates similarity between series of words from the left
and right of a conjunction. Similarity is measured
based on matching POS tags, matching words and a
thesaurus-based measure of semantic similarity. In
both the discriminative reranker of Ratnaparkhi et
al. (1994) and that of Charniak and Johnson (2005)
features are included to capture syntactic parallelism
across conjuncts at various depths.
3 Modelling Symmetry Between Conjuncts
There is often a considerable bias toward symme-
try in the syntactic structure of two conjuncts, see
for example (Dubey et al, 2005). Take Figure 2: If
we take as level 0 the level in the coordinate sub-
681
NP1(plains)
NP2(plains)
NP3(plains)
DT6
the
JJ5
high
NNS4
plains
PP7(of)
IN8
of
NP9(T exas)
NNP10
Texas
CC11
and
NP11(states)
NP12(states)
DT15
the
JJ14
northern
NNS13
states
PP16(of)
IN17
of
NP18(Delta)
DT20
the
NNP19
Delta
Figure 2: Example of symmetry in conjunct structure in a lexicalised subtree.
tree where the coordinating conjunction CC occurs,
then there is exact symmetry in the two conjuncts in
terms of non-terminal labels and head word part-of-
speech tags for levels 0, 1 and 2. Learning a bias
toward parallelism in conjuncts should improve the
parsing model?s ability to correctly attach a coordi-
nation conjunction and second conjunct to the cor-
rect position in the tree.
In history-based models, features are limited to
being functions of the tree generated so far. The task
is to incorporate a feature into the model that cap-
tures a particular bias yet still adheres to derivation-
based restrictions. Parses are generated top-down,
head-first, left-to-right. Each node in the tree in
Figure 2 is annotated with the order the nodes are
generated (we omit, for the sake of clarity, the gen-
eration of the STOP nodes). Note that when the
decision to attach the second conjunct to the head
conjunct is being made (i.e. Step 11, when the CC
and NP(states) nodes are being generated) the sub-
tree rooted at NP(states) has not yet been generated.
Thus at the point that the conjunct attachment de-
cision is made it is not possible to use information
about symmetry of conjunct structure, as the struc-
ture of the second conjunct is not yet known.
It is possible, however, to condition on structure
of the already generated head conjunct when build-
ing the internal structure of the second conjunct. In
our model when the structure of the second conjunct
is being generated we condition on features which
are functions of the first conjunct. When generat-
ing a node Ni in the second conjunct, we retrieve
the corresponding node NipreCC in the first conjunct,
via a left to right traversal of the first conjunct. For
example, from Figure 2 the pre-CC node NP(Texas)
is the node corresponding to NP(Delta) in the post-
CC conjunct. From NipreCC we extract information,
such as its part-of-speech, for use as a feature when
predicting a POS tag for the corresponding node in
the post-CC conjunct.
When generating a second conjunct, instead of
the usual parameter classes for estimating the prob-
ability of the head label Ch and the POS label of a
dependent node ti, we created two new parameter
classes which are used only in the generation of sec-
ond conjunct nodes:
PccCh(Ch|?(headC), Cp, wp, tp, tgp, depth) (1)
Pccti(ti|?(headC), dir, Cp, wp, tp, dist, ti 1, ti 2, depth)
(2)
where ?(headC) returns the non-terminal label of
NipreCC for the node in question and ?(headC) re-
turns the POS tag of NipreCC . Both functions return
+NOMATCH+ if there is no NipreCC for the node.
Depth is the level of the post-CC conjunct node be-
ing generated.
4 Modelling Coordinate Head Words
Some noun pairs are more likely to be conjoined
than others. Take again the trees in Figure 1. The
two head nouns coordinated in Tree 1 are execu-
tives and wives, and in Tree 2: busloads and wives.
Clearly, the former pair of head nouns is more likely
and, for the purpose of discrimination, the model
would benefit if it could learn that executives and
wives is a more likely combination than busloads
and wives.
Bilexical head-head dependencies of the type
found in coordinate structures are a somewhat dif-
682
ferent class of dependency to modifier-head depen-
dencies. In the fat cat, for example, there is clearly
one head to the noun phrase: cat. In cats and dogs
however there are two heads, though in the parsing
model just one is chosen, somewhat arbitrarily, to
head the entire noun phrase.
In the baseline model there is essentially one pa-
rameter class for the estimation of word probabili-
ties:
Pword(wi|H(i)) (3)
where wi is the lexical head of constituent i and
H(i) is the history of the constituent. The history is
made up of conditioning features chosen from struc-
ture that has already been determined in the top-
down derivation of the tree.
In Section 4.1 we discuss how though the coordi-
nate head-head dependency is captured for NPs, it is
not captured for NPBs. We look at how we might
improve the model?s handling of coordinate head-
head dependencies by altering the model so that a
common parameter class in (4) is used for coordi-
nate word probability estimation in both NPs and
NPBs.
PcoordWord(wi|wp, H(i)) (4)
In Section 4.2 we focus on improving the estimation
of this parameter class by reducing data sparseness.
4.1 Extending PcoordWord to Coordinate NPBs
In the baseline model each node in the tree is an-
notated with a coordination flag which is set to true
for the node immediately following the coordinating
conjunction. For coordinate NPs the head-head de-
pendency is captured when this flag is set to true. In
Figure 1, discarding for simplicity the other features
in the history, the probability of the coordinate head
wives, is estimated in Tree 1 as:
Pword(wi = wives|coord = true, wp = executives, ...)
(5)
and in Tree 2:
Pword(wi = wives|coord = true, wp = busloads, ...) (6)
where wp is the head word of the node to which the
node headed by wi is attaching and coord is the co-
ordination flag.
Unlike NPs, in NPBs (i.e. flat, non-recursive NPs)
the coordination flag is not used to mark whether a
node is a coordinated head or not. This flag is always
set to false for NPBs. In addition, modifiers within
NPBs are conditioned on the previously generated
modifier rather than the head of the phrase.2 This
means that in an NPB such as (cats and dogs), the
estimate for the word cats will look like:
Pword(wi = cats|coord = false, wp = and, ...) (7)
In our new model, for NPs, when the coordination
flag is set to true, we use the parameter class in (4)
to estimate the probability of one lexical head noun,
given another. For NPBs, if a noun is generated di-
rectly after a CC then it is taken to be a coordinate
head, wi, and conditioned on the noun generated be-
fore the coordinating conjunction, which is chosen
as wp, and also estimated using (4).
4.2 Estimating the PcoordWord parameter class
Data for bilexical statistics are particularly sparse.
In order to decrease the sparseness of the coordinate
head noun data, we extracted from the BNC exam-
ples of coordinate head noun pairs. We extracted all
noun pairs occurring in a pattern of the form: noun
cc noun, as well as lists of any number of nouns
separated by commas and ending in cc noun.3 To
this data we added all head noun pairs from the WSJ
that occurred together in a coordinate noun phrase,
identified when the coordination flag was set to true.
Every occurrence ni CC nj was also counted as an
occurrence of nj CC ni. This further helps reduce
sparseness.
The probability of one noun, ni being coordinated
with another nj can be calculated simply as:
Plex(ni|nj) =
|ninj |
|nj |
(8)
Again to reduce data sparseness, we introduce a
measure of word similarity. A word can be rep-
resented as a vector where every dimension of the
vector represents another word type. The values of
the vector components, the term weights, are derived
from word co-occurrence counts. Cosine similar-
ity between two word vectors can then be used to
measure the similarity of two words. Measures of
2A full explanation of the handling of coordination in the
model is given in (Bikel, 2004).
3Extracting coordinate noun pairs from the BNC in such
a fashion follows work on networks of concepts described
in (Widdows, 2004).
683
similarity between words based on similarity of co-
occurrence vectors have been used before, for exam-
ple, for word sense disambiguation (Schu?tze, 1998)
and for PP-attachment disambiguation (Zhao and
Lin, 2004). Our measure resembles that of (Cara-
ballo, 99) where co-occurrence is also defined with
respect to coordination patterns, although the exper-
imental details in terms of data collection and vector
term weights differ.
We can now incorporate the similarity measure
into the probability estimate of (8) to give a new
k-NN style method of estimating bilexical statistics
based on weighting events according to the word
similarity measure:
Psim(ni|nj) =
?
nx?N(nj)
sim(nj , nx)|ninx|
?
nx?N(nj)
sim(nj , nx)|nx|
(9)
where sim(nj, nx) is a similarity score between
words nj and nx and N(nj) is the set of words in
the neighbourhood of nj . This neighbourhood can
be based on the k-nearest neighbours of nj , where
nearness is measured with the similarity function.
In order to smooth the bilexical estimate in (9) we
combine it with another estimate, trained from WSJ
data, by way of linear interpolation:
PcoordWord(ni|nj) =
?nj Psim(ni|nj) + (1? ?nj )PMLE(ni|ti) (10)
where ti is the POS tag of word ni, PMLE(ni|ti)
is the maximum-likelihood estimate calculated from
annotated WSJ data, and ?nj is calculated as in (11).
In (11) we adapt the Witten-Bell method for the
calculation of the weight ?, as used in the Collins
parser, so that it incorporates the similarity measure
for all words in the neighbourhood of nj .
?nj =
?
nx?N(nj )
sim(nj , nx)|nx|
?
nx?N(nj)
sim(nj , nx)(|nx| + CD(nx))
(11)
where C is a constant that can be optimised using
held-out data and D(nj) is the diversity of a word
nj: the number of distinct words with which nj has
been coordinated in the training set.
The estimate in (9) can be viewed as the estimate
with the more general history context than that of (8)
because the context includes not only nj but also
words similar to nj . The final probability estimate
for PcoordWord is calculated as the most specific es-
timate, Plex, combined via regular Witten-Bell inter-
polation with the estimate in (10).
5 NPB Head-Finding Rules
Head-finding rules for coordinate NPBs differ from
coordinate NPs.4 Take the following two versions
of the noun phrase hard work and harmony: (c) (NP
(NPB hard work and harmony)) and (d) (NP (NP
(NPB hard work)) and (NP (NPB harmony))). In the
first example, harmony is chosen as head word of the
NP; in example (d) the head of the entire NP is work.
The choice of head affects the various dependencies
in the model. However, in the case of two coordinate
NPs which, as in the above example, cover the same
span of words and differ only in whether the coordi-
nate noun phrase is flat as in (c) or structured as in
(d), the choice of head for the phrase is not particu-
larly informative. In both cases the head words be-
ing coordinated are the same and either word could
plausibly head the phrase; discrimination between
trees in such cases should not be influenced by the
choice of head, but rather by other, salient features
that distinguish the trees.5
We would like to alter the head-finding rules for
coordinate NPBs so that, in cases like those above,
the word chosen to head the entire coordinate noun
phrase would be the same for both base and non-
base noun phrases. We experiment with slightly
modified head-finding rules for coordinate NPBs. In
an NPB such as NPB? n1 CC n2 n3, the head rules
remain unchanged and the head of the phrase is (usu-
ally) the rightmost noun in the phrase. Thus, when
n2 is immediately followed by another noun the de-
fault is to assume nominal modifier coordination and
the head rules stay the same. The modification we
make to the head rules for NPBs is as follows: when
n2 is not immediately followed by a noun then the
noun chosen to head the entire phrase is n1.
6 Inconsistencies in WSJ Coordinate NP
Annotation
An inspection of NP coordination error in the base-
line model revealed inconsistencies in WSJ annota-
4See (Collins, 1999) for the rules used in the baseline model.
5For example, it would be better if discrimination was
largely based on whether hard modifies both work and harmony
(c), or whether it modifies work alone (d).
684
tion. In this section we outline some types of co-
ordinate NP inconsistency and outline a method for
detecting some of these inconsistencies, which we
later use to automatically clean noise from the data.
Eliminating noise from treebanks has been previ-
ously used successfully to increase overall parser ac-
curacy (Dickinson and Meurers, 2005).
The annotation of NPs in the Penn Treebank (Bies
et al, 1995) follows somewhat different guidelines
to that of other syntactic categories. Because their
interpretation is so ambiguous, no internal structure
is shown for nominal modifiers. For NPs with more
than one head noun, if the only unshared modifiers
in the constituent are nominal modifiers, then a flat
structure is also given. Thus in (NP the Manhattan
phone book and tour guide)6 a flat structure is given
because although the is a non-nominal modifier, it is
shared, modifying both tour guide and phone book,
and all other modifiers in the phrase are nominal.
However, we found that out of 1,417 examples
of NP coordination in sections 02 to 21, involving
phrases containing only nouns (common nouns or a
mixture of common and proper nouns) and the co-
ordinating conjunction, as many as 21.3%, contrary
to the guidelines, were given internal structure, in-
stead of a flat annotation. When all proper nouns are
involved this phenomenon is even more common.7
Another common source of inconsistency in co-
ordinate noun phrase bracketing occurs when a non-
nominal modifier appears in the coordinate noun
phrase. As previously discussed, according to the
guidelines the modifier is annotated flat if it is
shared. When the non-nominal modifier is un-
shared, more internal structure is shown, as in:
(NP (NP (NNS fangs)) (CC and) (NP (JJ pointed)
(NNS ears))). However, the following two struc-
tured phrases, for example, were given a com-
pletely flat structure in the treebank: (a) (NP (NP
(NN oversight))(CC and) (NP (JJ disciplinary)(NNS
procedures))), (b) (NP (ADJP (JJ moderate)(CC
and)(JJ low-cost))(NN housing)). If we follow the
guidelines then any coordinate NPB which ends
with the following tag sequence can be automat-
ically detected as incorrectly bracketed: CC/non-
nominal modifier/noun. This is because either the
6In this section we do not show the NPB levels.
7In the guidelines it is recognised however that proper names
are frequently annotated with internal structure.
non-nominal modifier, which is unambiguously un-
shared, is part of a noun phrase as (a) above, or it
conjoined with another modifier as in (b). We found
202 examples of this in the training set, out of a total
of 4,895 coordinate base noun phrases.
Finally, inconsistencies in POS tagging can also
lead to problems with coordination. Take the bi-
gram executive officer. We found 151 examples in
the training set of a base noun phrase which ended
with this bigram. 48% of the cases were POS tagged
JJ NN, 52% tagged NN NN. 8 This has repercussions
for coordinate noun phrase structure, as the presence
of an adjectival pre-modifier indicates a structured
annotation should be given.
These inconsistencies pose problems both for
training and testing. With a relatively large amount
of noise in the training set the model learns to give
structures, which should be very unlikely, too high
a probability. In testing, given inconsistencies in
the gold standard trees, it becomes more difficult
to judge how well the model is doing. Although it
would be difficult to automatically detect the POS
tagging errors, the other inconsistencies outlined
above can be detected automatically by simple pat-
tern matching. Automatically eliminating such ex-
amples is a simple method of cleaning the data.
7 Experimental Evaluation
We use a parsing model similar to that described
in (Hogan, 2005) which is based on (Collins, 1999)
Model 1 and uses k-NN for parameter estimation.
The n-best output from Bikel?s parser (Bikel, 2004)
is reranked according to this k-NN parsing model,
which achieves an f-score of 89.4% on section 23.
For the coordination experiments, sections 02 to 21
are used for training, section 23 for testing and the
remaining sections for validation. Results are for
sentences containing 40 words or less.
As outlined in Section 6, the treebank guide-
lines are somewhat ambiguous as to the appropriate
bracketing for coordinate NPs which consist entirely
of proper nouns. We therefore do not include, in the
coordination test and validation sets, coordinate NPs
where in the gold standard NP the leaf nodes consist
entirely of proper nouns (or CCs or commas). In do-
8According to the POS bracketing guidelines (Santorini,
1991) the correct sequence of POS tags should be NN NN.
685
ing so we hope to avoid a situation whereby the suc-
cess of the model is measured in part by how well
it can predict the often inconsistent bracketing deci-
sions made for a particular portion of the treebank.
In addition, and for the same reasons, if a gold
standard tree is inconsistent with the guidelines in
either of the following two ways the tree is not used
when calculating coordinate precision and recall of
the model: the gold tree is a noun phrase which ends
with the sequence CC/non-nominal modifier/noun;
the gold tree is a structured coordinate noun phrase
where each word in the noun phrase is a noun.9 Call
these inconsistencies type a and type b respectively.
This left us with a coordination validation set con-
sisting of 1064 coordinate noun phrases and a test
set of 416 coordinate NPs from section 23.
A coordinate phrase was deemed correct if the
parent constituent label, and the two conjunct node
labels (at level 0) match those in the gold subtree and
if, in addition, each of the conjunct head words are
the same in both test and gold tree. This follows the
definition of a coordinate dependency in (Collins,
1999). Based on these criteria, the baseline f-scores
for test and validation set were 69.1% and 67.1% re-
spectively. The coordination f-score for the oracle
trees on section 23 is 83.56%. In other words: if an
?oracle? were to choose from each set of n-best trees
the tree that maximised constituent precision and re-
call, then the resulting set of oracle trees would have
a NP coordination dependency f-score of 83.56%.
For the validation set the oracle trees coordination
dependency f-score is 82.47%.
7.1 Experiments and Results
We first eliminated from the training set al coordi-
nate noun phrase subtrees, of type a and type b de-
scribed in Section 7. The effect of this on the vali-
dation set is outlined in Table 1, step 2.
For the new parameter class in (1) we found that
the best results occurred when it was used only in
conjuncts of depth 1 and 2, although the case base
for this parameter class contained head events from
all post-CC conjunct depths. Parameter class (2) was
used for predicting POS tags at level 1 in right-of-
head conjuncts, although again the sample contained
9Recall from ?6 that for this latter case the noun phrase
should be flat - an NPB - rather than a noun phrase with internal
structure.
Model f-score significance
1. Baseline 67.1
2. NoiseElimination 68.7 ? 1
3. Symmetry 69.9 > 2,? 1
4. NPB head rule 70.6 NOT > 3, > 2,? 1
5. PcoordWord WSJ 71.7 NOT > 4, > 3,? 2
6. BNC data 72.1 NOT > 5, > 4,? 3
7. sim(wi, wp) 72.4 NOT > 6, NOT > 5,? 4
Table 1: Results on the Validation Set. 1064 coordi-
nate noun phrase dependencies. In the significance
column > means at level .05 and ? means at level
.005, for McNemar?s test of significance. Results are
cumulative.
events from all depths.
For the PcoordWord parameter class we extracted
9961 coordinate noun pairs from the WSJ train-
ing set and 815,323 pairs from the BNC. As pairs
are considered symmetric this resulted in a total of
1,650,568 coordinate noun events. The term weights
for the word vectors were dampened co-occurrence
counts, of the form: 1 + log(count). For the es-
timation of Psim(ni|nj) we found it too computa-
tionally expensive to calculate similarity measures
between nj and each word token collected. The best
results were obtained when the neighbourhood of nj
was taken to be the k-nearest neighbours of nj from
among the set of word that had previously occurred
in a coordination pattern with nj , where k is 1000.
Table 1 shows the effect of the PcoordWord parame-
ter class estimated from WSJ data only (step 5), with
the addition of BNC data (step 6) and finally with the
word similarity measure (step 7).
The result of these experiments, as well as that
involving the change in the head-finding heuristics,
outlined in Section 5, was an increase in coordinate
noun phrase f-score from 69.9% to 73.8% on the test
set. This represents a 13% relative reduction in co-
ordinate f-score error over the baseline, and, using
McNemar?s test for significance, is significant at the
0.05 level (p = 0.034). The reranker f-score for
all constituents (not excluding any coordinate NPs)
for section 23 rose slightly from 89.4% to 89.6%, a
small but significant increase in f-score.10
Finally, we report results on an unaltered coor-
dination test set, that is, a test set from which no
10Significance was calculated using the software available at
www.cis.upenn.edu/ dbikel/software.html.
686
noisy events were eliminated. The baseline coordi-
nation dependency f-score for all NP coordination
dependencies (550 dependencies) from section 23 is
69.27%. This rises to 72.74% when all experiments
described in Section 7 are applied, which is also a
statistically significant increase (p = 0.042).
8 Conclusion and Future Work
This paper outlined a novel method for modelling
symmetry in conjunct structure, for modelling the
dependency between noun phrase conjunct head
words and for incorporating a measure of word sim-
ilarity in the estimation of a model parameter. We
also demonstrated how simple pattern matching can
be used to reduce noise in WSJ noun phrase coor-
dination data. Combined, these techniques resulted
in a statistically significant improvement in noun
phrase coordination accuracy.
Coordination disambiguation necessitates in-
formation from a variety of sources. Another
information source important to NP coordinate
disambiguation is the dependency between non-
nominal modifiers and nouns which cross CCs
in NPBs. For example, modelling this type of
dependency could help the model learn that the
phrase the cats and dogs should be bracketed flat,
whereas the phrase the U.S. and Washington should
be given structure.
Acknowledgements We are grateful to the TCD
Broad Curriculum Fellowship scheme and to the
SFI Basic Research Grant 04/BR/CS370 for fund-
ing this research. Thanks to Pa?draig Cunningham,
Saturnino Luz, Jennifer Foster and Gerard Hogan
for helpful discussions and feedback on this work.
References
Rajeev Agarwal and Lois Boggess. 1992. A Simple but Useful
Approach to Conjunct Identification. In Proceedings of the
30th ACL.
Ann Bies, Mark Ferguson, Karen Katz and Robert MacIntyre.
1995. Bracketing Guidelines for Treebank II Style Penn
Treebank Project. Technical Report. University of Penn-
sylvania.
Dan Bikel. 2004. On The Parameter Space of Generative Lex-
icalized Statistical Parsing Models. Ph.D. thesis, University
of Pennsylvania.
Sharon Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Proceedings
of the 37th ACL.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-
best Parsing and MaxEnt Discriminative Reranking. In Pro-
ceedings of the 43rd ACL.
Michael Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. Ph.D. thesis, University of
Pennsylvania.
Markus Dickinson and W. Detmar Meurers. 2005. Prune dis-
eased branches to get healthy trees! How to find erroneous
local trees in a treebank and why it matters. In Proceedings
of the Fourth Workshop on Treebanks and Linguistic Theo-
ries (TLT).
Amit Dubey, Patrick Sturt and Frank Keller. 2005. Parallelism
in Coordination as an Instance of Syntactic Priming: Evi-
dence from Corpus-based Modeling. In Proceedings of the
HLT/EMNP-05.
Miriam Goldberg. 1999. An Unsupervised Model for Statis-
tically Determining Coordinate Phrase Attachment. In Pro-
ceedings of the 27th ACL.
Deirdre Hogan. 2005. k-NN for Local Probability Estimation
in Generative Parsing Models. In Proceedings of the IWPT-
05.
Sadao Kurohashi and Makoto Nagao. 1994. A Syntactic Anal-
ysis Method of Long Japanese Sentences Based on the De-
tection of Conjunctive Structures. In Computational Lin-
guistics, 20(4).
Preslav Nakov and Marti Hearst. 2005. Using the Web as an
Implicit Training Set: Application to Structural Ambiguity
Resolution. In Proceedings of the HLT/EMNLP-05.
Adwait Ratnaparkhi, Salim Roukos and R. Todd Ward. 1994. A
Maximum Entropy Model for Parsing. In Proceedings of the
International Conference on Spoken Language Processing.
Philip Resnik. 1999. Semantic Similarity in a Taxonomy: An
Information-Based Measure and its Application to Problems
of Ambiguity in Natural Language. In Journal of Artificial
Intelligence Research, 11:95-130, 1999.
Beatrice Santorini. 1991. Part-of-Speech Tagging Guidelines
for the Penn Treebank Project. Technical Report. University
of Pennsylvania.
Hinrich Schu?tze. 1998. Automatic Word Sense Discrimination.
Computational Linguistics, 24(1):97-123.
Dominic Widdows. 2004. Geometry and Meaning. CSLI Pub-
lications, Stanford, USA.
Shaojun Zhao and Dekang Lin. 2004. A Nearest-Neighbor
Method for Resolving PP-Attachment Ambiguity. In Pro-
ceedings of the IJCNLP-04.
687
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 149?152,
Prague, June 2007. c?2007 Association for Computational Linguistics
Empirical Measurements of Lexical Similarity in Noun Phrase Conjuncts
Deirdre Hogan?
Department of Computer Science
Trinity College Dublin
Dublin 2, Ireland
dhogan@computing.dcu.ie
Abstract
The ability to detect similarity in conjunct
heads is potentially a useful tool in help-
ing to disambiguate coordination structures
- a difficult task for parsers. We propose a
distributional measure of similarity designed
for such a task. We then compare several dif-
ferent measures of word similarity by testing
whether they can empirically detect similar-
ity in the head nouns of noun phrase con-
juncts in the Wall Street Journal (WSJ) tree-
bank. We demonstrate that several measures
of word similarity can successfully detect
conjunct head similarity and suggest that the
measure proposed in this paper is the most
appropriate for this task.
1 Introduction
Some noun pairs are more likely to be conjoined
than others. Take the follow two alternate brack-
etings: 1. busloads of ((executives) and (their
spouses)) and 2. ((busloads of executives) and
(their spouses)). The two head nouns coordinated
in 1 are executives and spouses, and (incorrectly)
in 2: busloads and spouses. Clearly, the former
pair of head nouns is more likely and, for the pur-
pose of discrimination, a parsing model would ben-
efit if it could learn that executives and spouses
is a more likely combination than busloads and
spouses. If nouns co-occurring in coordination pat-
terns are often semantically similar, and if a simi-
? Now at the National Centre for Language Technology,
Dublin City University, Ireland.
larity measure could be defined so that, for exam-
ple: sim(executives, spouses) > sim(busloads, spouses)
then it is potentially useful for coordination disam-
biguation.
The idea that nouns co-occurring in conjunc-
tions tend to be semantically related has been noted
in (Riloff and Shepherd, 1997) and used effec-
tively to automatically cluster semantically similar
words (Roark and Charniak, 1998; Caraballo, 1999;
Widdows and Dorow, 2002). The tendency for con-
joined nouns to be semantically similar has also
been exploited for coordinate noun phrase disam-
biguation by Resnik (1999) who employed a mea-
sure of similarity based on WordNet to measure
which were the head nouns being conjoined in cer-
tain types of coordinate noun phrase.
In this paper we look at different measures of
word similarity in order to discover whether they can
detect empirically a tendency for conjoined nouns to
be more similar than nouns which co-occur but are
not conjoined. In Section 2 we introduce a measure
of word similarity based on word vectors and in Sec-
tion 3 we briefly describe some WordNet similarity
measures which, in addition to our word vector mea-
sure, will be tested in the experiments of Section 4.
2 Similarity based on Coordination
Co-occurrences
The potential usefulness of a similarity measure de-
pends on the particular application. An obvious
place to start, when looking at similarity functions
for measuring the type of semantic similarity com-
mon for coordinate nouns, is a similarity function
based on distributional similarity with context de-
149
fined in terms of coordination patterns. Our mea-
sure of similarity is based on noun co-occurrence
information, extracted from conjunctions and lists.
We collected co-occurrence data on 82, 579 distinct
word types from the BNC and the WSJ treebank.
We extracted all noun pairs from the BNC which
occurred in a pattern of the form: noun cc noun1,
as well as lists of any number of nouns separated by
commas and ending in cc noun. Each noun in the list
is linked with every other noun in the list. Thus for
a list: n1, n2, and n3, there will be co-occurrences
between words n1 and n2, between n1 and n3 and
between n2 and n3. To the BNC data we added all
head noun pairs from the WSJ (sections 02 to 21)
that occurred together in a coordinate noun phrase.2
From the co-occurrence data we constructed word
vectors. Every dimension of a word vector repre-
sents another word type and the values of the com-
ponents of the vector, the term weights, are derived
from the coordinate word co-occurrence counts. We
used dampened co-occurrence counts, of the form:
1 + log(count), as the term weights for the word
vectors. To measure the similarity of two words, w1
and w2, we calculate the cosine of the angle between
the two word vectors, ~w1 and ~w2.
3 WordNet-Based Similarity Measures
We also examine the following measures of seman-
tic similarity which are WordNet-based.3 Wu and
Palmer (1994) propose a measure of similarity of
two concepts c1 and c2 based on the depth of con-
cepts in the WordNet hierarchy. Similarity is mea-
sured from the depth of the most specific node dom-
inating both c1 and c2, (their lowest common sub-
sumer), and normalised by the depths of c1 and
c2. In (Resnik, 1995) concepts in WordNet are
augmented by corpus statistics and an information-
theoretic measure of semantic similarity is calcu-
lated. Similarity of two concepts is measured
1It would be preferable to ensure that the pairs extracted are
unambiguously conjoined heads. We leave this to future work.
2We did not include coordinate head nouns from base noun
phrases (NPB) (i.e. noun phrases that do not dominate other
noun phrases) because the underspecified annotation of NPBs
in the WSJ means that the conjoined head nouns can not always
be easily identified.
3All of the WordNet-based similarity measure ex-
periments, as well as a random similarity measure,
were carried out with the WordNet::Similarity package,
http://search.cpan.org/dist/WordNet-Similarity.
by the information content of their lowest com-
mon subsumer in the is-a hierarchy of WordNet.
Both Jiang and Conrath (1997) and Lin (1998) pro-
pose extentions of Resnik?s measure. Leacock and
Chodorow (1998)?s measure takes into account the
path length between two concepts, which is scaled
by the depth of the hierarchy in which they re-
side. In (Hirst and St-Onge, 1998) similarity is
based on path length as well as the number of
changes in the direction in the path. In (Banerjee and
Pedersen, 2003) semantic relatedness between two
concepts is based on the number of shared words
in their WordNet definitions (glosses). The gloss
of a particular concept is extended to include the
glosses of other concepts to which it is related in the
WordNet hierarchy. Finally, Patwardhan and Peder-
son (2006) build on previous work on second-order
co-occurrence vectors (Schu?tze, 1998) by construct-
ing second-order co-occurrence vectors from Word-
Net glosses, where, as in (Banerjee and Pedersen,
2003), the gloss of a concept is extended so that it
includes the gloss of concepts to which it is directly
related in WordNet.
4 Experiments
We selected two sets of data from sections 00, 01,
22 and 24 of the WSJ treebank. The first consists
of all nouns pairs which make up the head words
of two conjuncts in coordinate noun phrases (again
not including coordinate NPBs). We found 601 such
coordinate noun pairs. The second data set consists
of 601 word pairs which were selected at random
from all head-modifier pairs where both head and
modifier words are nouns and are not coordinated.
We tested the 9 different measures of word similar-
ity just described on each data set in order to see if
a significant difference could be detected between
the similarity scores for the coordinate words sam-
ple and non-coordinate words sample.
Initially both the coordinate and non-coordinate
pair samples each contained 601 word pairs. How-
ever, before running the experiments we removed
all pairs where the words in the pair were identical.
This is because identical words occur more often in
coordinate head words than in other lexical depen-
dencies (there were 43 pairs with identical words in
the coordination set, compared to 3 such pairs in the
150
SimTest ncoord xcoord SDcoord nnonCoord xnonCoord SDnonCoord 95% CI p-value
coordDistrib 503 0.11 0.13 485 0.06 0.09 [0.04 0.07] 0.000
(Resnik, 1995) 444 3.19 2.33 396 2.43 2.10 [0.46 1.06] 0.000
(Lin, 1998) 444 0.27 0.26 396 0.19 0.22 [0.04 0.11] 0.000
(Jiang and Conrath, 1997) 444 0.13 0.65 395 0.07 0.08 [-0.01 0.11] 0.083
(Wu and Palmer, 1994) 444 0.63 0.19 396 0.55 0.19 [0.06 0.11] 0.000
(Leacock and Chodorow, 1998) 444 1.72 0.51 396 1.52 0.47 [0.13 0.27] 0.000
(Hirst and St-Onge, 1998) 459 1.599 2.03 447 1.09 1.87 [0.25 0.76] 0.000
(Banerjee and Pedersen, 2003) 451 114.12 317.18 436 82.20 168.21 [-1.08 64.92] 0.058
(Patwardhan and Pedersen, 2006) 459 0.67 0.18 447 0.66 0.2 [-0.02 0.03] 0.545
random 483 0.89 0.17 447 0.88 0.18 [-0.02 0.02] 0.859
Table 1: Summary statistics for 9 different word similarity measures (plus one random measure):ncoord
and nnonCoord are the sample sizes for the coordinate and non-coordinate noun pairs samples, respectively;
xcoord, SDcoord and xnonCoord, SDnonCoord are the sample means and standard deviations for the two sets.
The 95% CI column shows the 95% confidence interval for the difference between the two sample means.
The p-value is for a Welch two sample two-sided t-test. coordDistrib is the measure introduced in Section 2.
non-coordination set). If we had not removed them,
a statistically significant difference between the sim-
ilarity scores of the pairs in the two sets could be
found simply by using a measure which, say, gave
one score for identical words and another (lower)
score for all non-identical word pairs.
Results for all similarity measure tests on the data
sets described above are displayed in Table 1. In one
final experiment we used a random measure of sim-
ilarity. For each experiment we produced two sam-
ples, one consisting of the similarity scores given by
the similarity measure for the coordinate noun pairs,
and another set of similarity scores generated for the
non-coordinate pairs. The sample sizes, means, and
standard deviations for each experiment are shown
in the table. Note that the variation in the sample
size is due to coverage: the different measures did
not produce a score for all word pairs. Also dis-
played in Table 1 are the results of statistical signif-
icance tests based on the Welsh two sample t-test.
A 95% confidence interval for the difference of the
sample means is shown along with the p-value.
5 Discussion
For all but three of the experiments (excluding the
random measure), the difference between the mean
similarity measures is statistically significant. Inter-
estingly, the three tests where no significant differ-
ence was measured between the scores on the co-
ordination set and the non-coordination set (Jiang
and Conrath, 1997; Banerjee and Pedersen, 2003;
Patwardhan and Pedersen, 2006) were the three
top scoring measures in (Patwardhan and Pedersen,
2006), where a subset of six of the above WordNet-
based experiments were compared and the measures
evaluated against human relatedness judgements and
in a word sense disambiguation task. In another
comparative study (Budanitsky and Hirst, 2002) of
five of the above WordNet-based measures, evalu-
ated as part of a real-word spelling correction sys-
tem, Jiang and Conrath (1997)?s similarity score per-
formed best. Although performing relatively well
under other evaluation criteria, these three measures
seem less suited to measuring the kind of similar-
ity occurring in coordinate noun pairs. One possi-
ble explanation for the unsuitability of the measures
of (Patwardhan and Pedersen, 2006) for the coordi-
nate similarity task could be based on how context
is defined when building context vectors. Context
for an instance of the the word w is taken to be the
words that surround w in the corpus within a given
number of positions, where the corpus is taken as all
the glosses in WordNet. Words that form part of col-
locations such as disk drives or task force would then
tend to have very similar contexts, and thus such
word pairs, from non-coordinate modifier-head re-
lations, could be given too high a similarity score.
Although the difference between the mean simi-
larity scores seems rather slight in all experiments,
it is worth noting that not all coordinate head
words are semantically related. To take a cou-
ple of examples from the coordinate word pair set:
work/harmony extracted from hard work and har-
mony, and power/clause extracted from executive
power and the appropriations clause. We would
not expect these word pairs to get a high similar-
ity score. On the other hand, it is also possible that
151
some of the examples of non-coordinate dependen-
cies involve semantically similar words. For exam-
ple, nouns in lists are often semantically similar, and
we did not exclude nouns extracted from lists from
the non-coordinate test set.
Although not all coordinate noun pairs are se-
mantically similar, it seems clear, on inspection of
the two sets of data, that they are more likely to be
semantically similar than modifier-head word pairs,
and the tests carried out for most of the measures
of semantic similarity detect a significant difference
between the similarity scores assigned to coordinate
pairs and those assigned to non-coordinate pairs.
It is not possible to judge, based on the signifi-
cance tests alone, which might be the most useful
measure for the purpose of disambiguation. How-
ever, in terms of coverage, the distributional mea-
sure introduced in Section 2 clearly performs best4.
This measure of distributional similarity is perhaps
more suited to the task of coordination disambigua-
tion because it directly measures the type of simi-
larity that occurs between coordinate nouns. That
is, the distributional similarity measure presented in
Section 2 defines two words as similar if they occur
in coordination patterns with a similar set of words
and with similar distributions. Whether the words
are semantically similar becomes irrelevant. A mea-
sure of semantic similarity, on the other hand, might
find words similar which are quite unlikely to ap-
pear in coordination patterns. For example, Ceder-
berg and Widdows (2003) note that words appearing
in coordination patterns tend to be on the same onto-
logical level: ?fruit and vegetables? is quite likely to
occur, whereas ?fruit and apples? is an unlikely co-
occurrence. A WordNet-based measure of semantic
similarity, however, might give a high score to both
of the noun pairs.
In the future we intend to use the similarity mea-
sure outlined in Section 2 in a lexicalised parser to
help resolve coordinate noun phrase ambiguities.
Acknowledgements Thanks to the TCD Broad
Curriculum Fellowship and to the SFI Research
Grant 04/BR/CS370 for funding this research.
Thanks also to Pa?draig Cunningham, Saturnino Luz
and Jennifer Foster for helpful discussions.
4Somewhat unsurprisingly given it is part trained on data
from the same domain.
References
Satanjeev Banerjee and Ted Pedersen. 2003 Extended Gloss
Overlaps as a Measure of Semantic Relatedness. In Pro-
ceeding of the 18th IJCAI.
Alexander Budanitsky and Graeme Hirst. 2002 Semantic Dis-
tance in WordNet: An experimental, application-oriented
Evaluation of Five Measures In Proceedings of the 3rd CI-
CLING.
Sharon Caraballo. 1999 Automatic construction of a
hypernym-labeled noun hierarchy from text In Proceedings
of the 37th ACL.
Scott Cederberg and Dominic Widdows. 2003. Using LSA
and Noun Coordination Information to Improve the Preci-
sion and Recall of Automatic Hyponymy Extraction. In Pro-
ceedings of the 7th CoNLL.
G. Hirst and D. St-Onge 1998. Lexical Chains as repre-
sentations of context for the detection and correction of
malapropisms. WordNet: An electronic lexical database.
MIT Press.
J. Jiang and D. Conrath. 1997. Semantic similarity based on
corpus statistics and lexical taxonomy. In Proceedings of
the ROCLING.
C. Leacock and M. Chodorow. 1998. Combining local context
and WordNet similarity for word sense identification. Word-
Net: An electronic lexical database. MIT Press.
D. Lin. 1998. An information-theoretic definition of similarity.
In Proceedings of the 15th ICML.
Siddharth Patwardhan and Ted Pedersen. 2006. Using
WordNet-based Context Vectors to Estimate the Semantic
Relatedness of Concepts. In Proceedings of Making Sense of
Sense - Bringing Computational Linguistics and Psycholin-
guistics Together, EACL.
Philip Resnik. 1995. Using Information Content to Evaluate
Semantic Similarity. In Proceedings of IJCAI.
Philip Resnik. 1999. Semantic Similarity in a Taxonomy: An
Information-Based Measure and its Application to Problems
of Ambiguity in Natural Language. In Journal of Artificial
Intelligence Research, 11:95-130.
Ellen Riloff and Jessica Shepherd 1997. A Corpus-based Ap-
proach for Building Semantic Lexicon. In Proceedings of
the 2nd EMNLP.
Brian Roark and Eugene Charniak 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic semantic lexicon
construction. In Proceedings of the COLING-ACL.
Hinrich Schu?tze. 1998. Automatic Word Sense Discrimination.
Computational Linguistics, 24(1):97-123.
Dominic Widdows and Beate Dorow. 2002. A Graph Model
for Unsupervised Lexical Acquisition. In Proceedings of the
19th COLING.
Zhibiao Wu and Martha Palmer. 1994. Verb Semantics and
Lexical Selection. In Proceedings of the ACL.
152
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 202?203,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
k-NN for Local Probability Estimation in Generative Parsing Models 
Deirdre Hogan 
Department of Computer Science 
Trinity College Dublin 
Dublin 2, Ireland 
dhogan@cs.tcd.ie 
 
 
Abstract 
We describe a history-based generative 
parsing model which uses a k-nearest 
neighbour (k-NN) technique to estimate 
the model?s parameters.  Taking the 
output of a base n-best parser we use our 
model to re-estimate the log probability of 
each parse tree in the n-best list for 
sentences from the Penn Wall Street 
Journal treebank.  By further 
decomposing the local probability 
distributions of the base model, enriching 
the set of conditioning features used to 
estimate the model?s parameters, and 
using k-NN as opposed to the Witten-Bell 
estimation of the base model, we achieve 
an f-score of 89.2%, representing a 4% 
relative decrease in f-score error over the 
1-best output of the base parser. 
1 Introduction 
This paper describes a generative probabilistic 
model for parsing, based on Collins (1999), which 
re-estimates the probability of each parse generated 
by an initial base parser (Bikel, 2004) using 
memory-based techniques to estimate local 
probabilities.   
We used Bikel?s re-implementation of the 
Collins parser (Bikel, 2004) to produce the n-best 
parses of sentences from the Penn treebank.  We 
then recalculated the probability of each parse tree 
using a probabilistic model very similar to Collins 
(1999) Model 1.  In addition to the local estimation 
technique used, our model differs from Collins 
(1999) Model 1 in that we extend the feature sets 
used to predict parse structure to include more 
features from the parse history, and we further 
decompose some of the model?s parameter classes. 
2 Constraint Features for Training Set 
Restriction 
We use the same k-NN estimation technique as 
Toutonava et al(2003) however we also found that 
restricting the number of examples in the training 
set used in a particular parameter estimation helped 
both in terms of accuracy and speed.  We restricted 
the training sets by making use of constraint 
features whereby the training set is restricted to 
only those examples which have the same value for 
the constraint feature as the query instance.   
We carried out experiments using different 
sets of constraint features, some more restrictive 
than others.  The mechanism we used is as follows:  
if the number of examples in the training set, 
retrieved using a particular set of constraint 
features, exceeds a certain threshold value then use 
a higher level of restriction i.e. one which uses 
more constraint features.   If, using the higher level 
of restriction, the number of samples in the training 
set falls below a minimum threshold value then 
?back-off? to the less restricted set of training 
samples. 
3 Experiments 
Our model is trained on sections 2 to 21 inclusive 
of the Penn WSJ treebank and tested on section 23.  
We used sections 0, 1, 22 and 24 for validation. 
           We re-estimated the probability of each 
parse using our own baseline model, which is a 
replication of Collins Model 1.  We tested k-NN 
estimation on the head generation parameter class 
202
and the parameter classes for generating modifying 
nonterminals.  We further decomposed the two 
modifying nonterminal parameter classes.  Table 1 
outlines the parameter classes estimated using k-
NN in the final model settings and shows the 
feature sets used for each parameter class as well 
as the constraint feature settings.   
 
Parameter 
Class 
History Contraint 
Features 
P(CH |?) Cp, CH, wp, 
tp, tgp 
{Cp} 
P(ti |?) dir, Cp, CH,  
wp, tp, dist, ti-
1, t i-2, Cgp 
{dir, Cp}, {dir, Cp, 
CH } 
P(Ci |?) dir, ti, Cp CH, 
wp, tp, dist, ti-
1,ti-2, Cgp 
{dir,ti},{dir, ti, Cp} 
P(coord,punc|?) dir, Ci, ti, Cp, 
CH, wp, ,tp 
{dir, Ci, ti} 
P(Ci ti | Cp 
=NPB?) 
dir, CH, wp, 
Ci-2, wi-2, Ci-
3, wi-3, Cgp, 
Cggp, Cgggp 
{dir, CH } 
P(punc| Cp 
=NPB?) 
dir, ti, Ci, CH, 
wp,tp, t i-2, t i-3 
{dir, ti} 
 
Table 1:  The parameter classes estimated using k-NN in 
the final model. CH is the head child label, Cp the parent 
constituent label, wp the head word, tp the head part-of-
speech (POS) tag.  Ci, wi and ti are the modifier?s label, 
head word and head POS tag.  tgp  is the grand-parent 
POS tag, Cgp, Cggp, Cgggp are the labels of the grand-
parent, great-grandparent and great-great-grandparent 
nodes.   dir is a flag which indicates whether the 
modifier being generated is to the left or the right of the 
head child.  dist is the distance metric used in the 
Collins parser. coord, punc are the coordination and 
punctuation flags.  NPB stands for base noun phrase.    
 
We extend the original feature sets by increasing 
the order of both horizontal and vertical 
markovization.  From each constituent node in the 
vertical or horizontal history we chose features 
from among the constituent?s nonterminal label, its 
head word and the head word?s part-of-speech tag.   
We found for all parameter classes 000,10k  or 
000,20k  worked best.  Distance weighting 
function that worked best were the inverse distance 
weighting functions either (1/(d+1))6 or (1/(d+1))7.   
 
Model LR LP 
WB Baseline 88.2% 88.5% 
CO99 M1 87.9% 88.2% 
CO99 M2 88.5% 88.7% 
Bikel 1-best 88.7% 88.7% 
k-NN 89.1% 89.4% 
 
Table 2:  Results for sentences of less than or equal to 
40 words, from section 23 of the Penn treebank.  LP/LR 
=Labelled Precision/Recall.  CO99 M1 and M2 are 
(Collins 1999) Models 1 and 2 respectively.  Bikel 1-
best is (Bikel, 2004). k-NN is our final k-NN model.  
 
With our k-NN model we achieve LR/LR of 
89.1%/89.4% on sentences  40 words.  These 
results show an 8% relative reduction in f-score 
error over our Model 1 baseline and a 4% relative 
reduction in f-score error over the Bikel parser.   
We compared the results of our k-NN model 
against the Bikel 1-best parser results using the 
paired T test where the data points being compared 
were the scores of each parse in the two different 
sets of parses.  The 95% confidence interval for the 
mean difference between the scores of the paired 
sets of parses is [0.029, 0.159] with P< .005.     
Following (Collins 2000) the score of a parse takes 
into account the number of constituents in the gold 
standard parse for this sentence.   These results 
show that using the methods presented in this 
paper can produce significant improvements in 
parser accuracy over the baseline parser. 
References 
Daniel M. Bikel.  2004.  On the Parameter Space of 
Generative Lexicalized Statistical Parsing  Models.  
PhD thesis, University of Pennsylvania. 
Michael Collins.  1999.  Head-driven statistical models 
for natural language processing. PhD  thesis, 
University of Pennsylvania. 
Michael Collins.  2000.  Discriminative reranking for 
natural language parsing.  In Proceedings of the 7th 
ICML. 
Kristina Toutanova, Mark Mitchell and Christopher 
Manning. 2003.  Optimizing Local Probability 
Models for Statistical Parsing.  In Proceedings of 14th 
ECML. 
 
203
Parser-Based Retraining for Domain Adaptation of Probabilistic Generators
Deirdre Hogan, Jennifer Foster, Joachim Wagner and Josef van Genabith
National Centre for Language Technology
School of Computing
Dublin City University
Ireland
{dhogan, jfoster, jwagner, josef}@computing.dcu.ie
Abstract
While the effect of domain variation on Penn-
treebank-trained probabilistic parsers has been
investigated in previous work, we study its ef-
fect on a Penn-Treebank-trained probabilistic
generator. We show that applying the gener-
ator to data from the British National Corpus
results in a performance drop (from a BLEU
score of 0.66 on the standard WSJ test set to a
BLEU score of 0.54 on our BNC test set). We
develop a generator retraining method where
the domain-specific training data is automat-
ically produced using state-of-the-art parser
output. The retraining method recovers a sub-
stantial portion of the performance drop, re-
sulting in a generator which achieves a BLEU
score of 0.61 on our BNC test data.
1 Introduction
Grammars extracted from the Wall Street Journal
(WSJ) section of the Penn Treebank have been suc-
cessfully applied to natural language parsing, and
more recently, to natural language generation. It is
clear that high-quality grammars can be extracted
for the WSJ domain but it is not so clear how
these grammars scale to other text genres. Gildea
(2001), for example, has shown that WSJ-trained
parsers suffer a drop in performance when applied
to the more varied sentences of the Brown Cor-
pus. We investigate the effect of domain variation in
treebank-grammar-based generation by applying a
WSJ-trained generator to sentences from the British
National Corpus (BNC).
As with probabilistic parsing, probabilistic gener-
ation aims to produce the most likely output(s) given
the input. We can distinguish three types of prob-
abilistic generators, based on the type of probabil-
ity model used to select the most likely sentence.
The first type uses an n-gram language model, e.g.
(Langkilde, 2000), the second type uses a proba-
bility model defined over trees or feature-structure-
annotated trees, e.g. (Cahill and van Genabith,
2006), and the third type is a mixture of the first
and second type, employing n-gram and grammar-
based features, e.g. (Velldal and Oepen, 2005). The
generator used in our experiments is an instance of
the second type, using a probability model defined
over Lexical Functional Grammar c-structure and
f-structure annotations (Cahill and van Genabith,
2006; Hogan et al, 2007).
In an initial evaluation, we apply our probabilistic
WSJ-trained generator to BNC material, and show
that the generator suffers a substantial performance
degradation, with a drop in BLEU score from 0.66
to 0.54. We then turn our attention to the problem
of adapting the generator so that it can more accu-
rately generate the 1,000 sentences in our BNC test
set. The problem of adapting any NLP system to a
domain different from the domain upon which it has
been trained and for which no gold standard train-
ing material is available is a very real one, and one
which has been the focus of much recent research in
parsing. Some success has been achieved by training
a parser, not on gold standard hand-corrected trees,
but on parser output trees. These parser output trees
can by produced by a second parser in a co-training
scenario (Steedman et al, 2003), or by the same
parser with a reranking component in a type of self-
training scenario (McClosky et al, 2006). We tackle
165
the problem of domain adaptation in generation in
a similar way, by training the generator on domain
specific parser output trees instead of manually cor-
rected gold standard trees. This experiment achieves
promising results, with an increase in BLEU score
from 0.54 to 0.61. The method is generic and can be
applied to other probabilistic generators (for which
suitable training material can be automatically pro-
duced).
2 Background
The natural language generator used in our experi-
ments is the WSJ-trained system described in Cahill
and van Genabith (2006) and Hogan et al (2007).
Sentences are generated from Lexical Functional
Grammar (LFG) f-structures (Kaplan and Bresnan,
1982). The f-structures are created automatically
by annotating nodes in the gold standard WSJ trees
with LFG functional equations and then passing
these equations through a constraint solver (Cahill
et al, 2004). The generation algorithm is a chart-
based one which works by finding the most proba-
ble tree associated with the input f-structure. The
yield of the most probable tree is the output sen-
tence. An annotated PCFG, in which the non-
terminal symbols are decorated with functional in-
formation, is used to generate the most probable tree
from an f-structure. Cahill and van Genabith (2006)
attain 98.2% coverage and a BLEU score of 0.6652
on the standard WSJ test set (Section 23). Hogan
et al (2007) describe an extension to the system
which replaces the annotated PCFG selection model
with a more sophisticated history-based probabilis-
tic model. Instead of conditioning the righthand side
of a rule on the lefthand non-terminal and its asso-
ciated functional information alone, the new model
includes non-local conditioning information in the
form of functional information associated with an-
cestor nodes of the lefthand side category. This sys-
tem achieves a BLEU score of 0.6724 and 99.9%
coverage.
Other WSJ-trained generation systems include
Nakanishi et al (2005) and White et al (2007).
Nakanishi et al (2005) describe a generator trained
on a HPSG grammar derived from the WSJ Section
of the Penn Treebank. On sentences of ? 20 words
in length, their system attains coverage of 90.75%
and a BLEU score of 0.7733. White et al (2007)
describe a CCG-based realisation system which has
been trained on logical forms derived from CCG-
Bank (Hockenmaier and Steedman, 2005), achiev-
ing 94.3% coverage and a BLEU score of 0.5768 on
WSJ23 for all sentence lengths. The input structures
upon which these systems are trained vary in form
and specificity, but what the systems have in com-
mon is that their various input structures are derived
from Penn Treebank trees.
3 The BNC Test Data
The new English test set consists of 1,000 sentences
taken from the British National Corpus (Burnard,
2000). The BNC is a one hundred million word bal-
anced corpus of British English from the late twenti-
eth century. Ninety per cent of it is written text, and
the remaining 10% consists of transcribed sponta-
neous and scripted spoken language. The BNC sen-
tences in the test set are not chosen completely at
random. Each sentence in the test set has the prop-
erty of containing a word which appears as a verb
in the BNC but not in the usual training sections of
the Wall Street Journal section of the Penn Treebank
(WSJ02-21). Sentences were chosen in this way so
that the resulting test set would be a difficult one
for WSJ-trained systems. In order to produce in-
put f-structures for the generator, the test sentences
were manually parsed by one annotator, using as
references the Penn Treebank trees themselves and
the Penn Treebank bracketing guidelines (Bies et
al., 1995). When the two references did not agree,
the guidelines took precedence over the Penn Tree-
bank trees. Difficult parsing decisions were docu-
mented. Due to time constraints, the annotator did
not mark functional tags or traces. The context-free
gold standard parse trees were transformed into f-
structures using the automatic procedure of Cahill et
al. (2004).
4 Experiments
Experimental Setup In our first experiment, we
apply the original WSJ-trained generator to our
BNC test set. The gold standard trees for our BNC
test set differ from the gold standard Wall Street
Journal trees, in that they do not contain Penn-II
traces or functional tags. The process which pro-
166
duces f-structures from trees makes use of trace and
functional tag information, if available. Thus, to en-
sure that the training and test input f-structures are
created in the same way, we use a version of the
generator which is trained using gold standard WSJ
trees without functional tag or trace information.
When we test this system on the WSJ23 f-structures
(produced in the same way as the WSJ training ma-
terial), the BLEU score decreases slightly from 0.67
to 0.66. This is our baseline system.
In a further experiment, we attempt to adapt
the generator to BNC data by using BNC trees as
training material. Because we lack gold standard
BNC trees (apart from those in our test set), we
try instead to use parse trees produced by an accu-
rate parser. We choose the Charniak and Johnson
reranking parser because it is freely available and
achieves state-of-the-art accuracy (a Parseval f-score
of 91.3%) on the WSJ domain (Charniak and John-
son, 2005). It is, however, affected by domain vari-
ation ? Foster et al (2007) report that its f-score
drops by approximately 8 percentage points when
applied to the BNC domain. Our training size is
500,000 sentences. We conduct two experiments:
the first, in which 500,000 sentences are extracted
randomly from the BNC (minus the test set sen-
tences), and the second in which only shorter sen-
tences, of length ? 20 words, are chosen as training
material. The rationale behind the second experi-
ment is that shorter sentences are less likely to con-
tain parser errors.
We use the BLEU evaluation metric for our ex-
periments. We measure both coverage and full cov-
erage. Coverage measures the number of cases for
which the generator produced some kind of out-
put. Full coverage measures the number of cases for
which the generator produced a tree spanning all of
the words in the input.
Results The results of our experiments are shown
in Fig. 1. The first row shows the results we ob-
tain when the baseline system is applied to the f-
structures derived from the 1,000 BNC gold stan-
dard parse trees. The second row shows the results
on the same test set for a system trained on Charniak
and Johnson parser output trees for 500,000 BNC
sentences. The results in the final row are obtained
by training the generator on Charniak and Johnson
parser output trees for 500,000 BNC sentences of
length ? 20 words in length.
Discussion As expected, the performance of the
baseline system degrades when faced with out-of-
domain test data. The BLEU score drops from a
0.66 score for WSJ test data to a 0.54 score for
the BNC test data, and full coverage drops from
85.97% to 68.77%. There is a substantial improve-
ment, however, when the generator is trained on
BNC data. The BLEU score jumps from 0.5358
to 0.6135. There are at least two possible reasons
why a BLEU score of 0.66 is not obtained: The first
is that the quality of the f-structure-annotated trees
upon which the generator has been trained has de-
graded. For the baseline system, the generator is
trained on f-structure-annotated trees derived from
gold trees. The new system is trained on f-structure-
annotated parser output trees, and the performance
of Charniak and Johnson?s parser degrades when ap-
plied to BNC data (Foster et al, 2007). The second
reason has been suggested by Gildea (2001): WSJ
data is easier to learn than the more varied data in the
Brown Corpus or BNC. Perhaps even if gold stan-
dard BNC parse trees were available for training, the
system would not behave as well as it does for WSJ
material.
It is interesting to note that training on 500,000
shorter sentences does not appear to help. We hy-
pothesized that it would improve results because
shorter sentences are less likely to contain parser
errors. The drop in full coverage from 86.69% to
79.58% suggests that the number of short sentences
needs to be increased so that the size of the training
material stays constant.
5 Conclusion
We have investigated the effect of domain varia-
tion on a LFG-based WSJ-trained generation sys-
tem by testing the system?s performance on 1,000
sentences from the British National Corpus. Perfor-
mance drops from a BLEU score of 0.66 onWSJ test
data to 0.54 on the BNC test set. Encouragingly, we
have also shown that domain-specific training mate-
rial produced by a parser can be used to claw back
a significant portion of this performance degrada-
tion. Our method is general and could be applied
to other WSJ-trained generators (e.g. (Nakanishi et
167
Train BLEU Coverage Full Coverage
WSJ02-21 0.5358 99.1 68.77
BNC(500k) 0.6135 99.1 86.69
BNC(500k) ? 20 words 0.5834 99.1 79.58
Figure 1: Results for 1,000 BNC Sentences
al., 2005; White et al, 2007)). We intend to con-
tinue this research by training our generator on parse
trees produced by a BNC-self-trained version of the
Charniak and Johnson reranking parser (Foster et al,
2007). We also hope to extend the evaluation beyond
the BLEU metric by carrying out a human judge-
ment evaluation.
Acknowledgments
This research has been supported by the Enterprise
Ireland Commercialisation Fund (CFTD/2007/229),
Science Foundation Ireland (04/IN/I527) and the
IRCSET Embark Initative (P/04/232). We thank the
Irish Centre for High End Computing for providing
computing facilities.
References
Ann Bies, Mark Ferguson, Karen Katz, and Robert Mac-
Intyre. 1995. Bracketing guidelines for treebank
II style, Penn Treebank project. Technical Report
Tech Report MS-CIS-95-06, University of Pennsylva-
nia, Philadelphia, PA.
Lou Burnard. 2000. User reference guide for the British
National Corpus. Technical report, Oxford University
Computing Services.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
lfg approximations. In Proceedings of the 21st COL-
ING and the 44th Annual Meeting of the ACL, pages
1033?1040, Sydney.
Aoife Cahill, Michael Burke, Ruth O?Donovan, Josef
van Genabith, and Andy Way. 2004. Long-Distance
Dependency Resolution in Automatically Acquired
Wide-Coverage PCFG-Based LFG Approximations.
In Proceedings of the 42nd Meeting of the ACL, pages
320?327, Barcelona.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Ann Arbor.
Jennifer Foster, Joachim Wagner, Djame? Seddah, and
Josef van Genabith. 2007. Adapting WSJ-trained
parsers to the British National Corpus using in-domain
self-training. In Proceedings of the Tenth IWPT, pages
33?35, Prague.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of EMNLP, Pittsburgh.
Julia Hockenmaier and Mark Steedman. 2005. Ccgbank:
Users? manual. Technical report, Computer and Infor-
mation Science, University of Pennsylvania.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef
van Genabith. 2007. Exploiting multi-word units in
history-based probabilistic generation. In Proceedings
of the joint EMNLP/CoNLL, pages 267?276, Prague.
Ron Kaplan and Joan Bresnan. 1982. Lexical Functional
Grammar: a Formal System for Grammatical Repre-
sentation. In Joan Bresnan, editor, The Mental Repre-
sentation of Grammatical Relations, pages 173?281.
MIT Press.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proceedings of NAACL, Seattle.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of the Human Language Technology Conference
of the NAACL, Main Conference, pages 152?159, New
York City.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proceedings of the
Ninth IWPT, pages 93?102, Vancouver.
Mark Steedman, Miles Osbourne, Anoop Sarkar, Stephen
Clark, Rebecca Hwa, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Boot-
strapping statistical parsers from small datasets. In
Proceedings of EACL, pages 331?338, Budapest.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proceedings
of the MT-Summit, Phuket.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proceedings of the Workshop on
Using Corpora for NLG: Language Generation and
Machine Translation (UCNLG+MT), pages 267?276,
Copenhagen.
168
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 67?75,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Handling Unknown Words in Statistical Latent-Variable Parsing Models for
Arabic, English and French
Mohammed Attia, Jennifer Foster, Deirdre Hogan, Joseph Le Roux, Lamia Tounsi,
Josef van Genabith?
National Centre for Language Technology
School of Computing, Dublin City University
{mattia,jfoster,dhogan,jleroux,ltounsi,josef}@computing.dcu.ie
Abstract
This paper presents a study of the impact
of using simple and complex morphological
clues to improve the classification of rare and
unknown words for parsing. We compare
this approach to a language-independent tech-
nique often used in parsers which is based
solely on word frequencies. This study is ap-
plied to three languages that exhibit different
levels of morphological expressiveness: Ara-
bic, French and English. We integrate infor-
mation about Arabic affixes and morphotac-
tics into a PCFG-LA parser and obtain state-
of-the-art accuracy. We also show that these
morphological clues can be learnt automati-
cally from an annotated corpus.
1 Introduction
For a parser to do a reasonable job of analysing free
text, it must have a strategy for assigning part-of-
speech tags to words which are not in its lexicon.
This problem, also known as the problem of un-
known words, has received relatively little attention
in the vast literature on Wall-Street-Journal (WSJ)
statistical parsing. This is likely due to the fact that
the proportion of unknown words in the standard
English test set, Section 23 of the WSJ section of
Penn Treebank, is quite small. The problem mani-
fests itself when the text to be analysed comes from
a different domain to the text upon which the parser
has been trained, when the treebank upon which the
parser has been trained is limited in size and when
?Author names are listed in alphabetical order. For further
correspondence, contact L. Tounsi, D. Hogan or J. Foster.
the language to be parsed is heavily inflected. We
concentrate on the latter case, and examine the prob-
lem of unknown words for two languages which lie
on opposite ends of the spectrum of morphologi-
cal expressiveness and for one language which lies
somewhere in between: Arabic, English and French.
In our experiments we use a Berkeley-style latent-
variable PCFG parser and we contrast two tech-
niques for handling unknown words within the gen-
erative parsing model: one in which no language-
specific information is employed and one in which
morphological clues (or signatures) are exploited.
We find that the improvement accrued from look-
ing at a word?s morphology is greater for Arabic
and French than for English. The morphological
clues we use for English are taken directly from the
Berkeley parser (Petrov et al, 2006) and those for
French from recent work on French statistical pars-
ing with the Berkeley parser (Crabbe? and Candito,
2008; Candito et al, 2009). For Arabic, we present
our own set of heuristics to extract these signatures
and demonstrate a statistically significant improve-
ment of 3.25% over the baseline model which does
not employ morphological information.
We next try to establish to what extent these clues
can be learnt automatically by extracting affixes
from the words in the training data and ranking these
using information gain. We show that this automatic
method performs quite well for all three languages.
The paper is organised as follows: In Section 2
we describe latent variable PCFG parsing models.
This is followed in Section 3 by a description of our
three datasets, including statistics on the extent of
the unknown word problem in each. In Section 4, we
67
present results on applying a version of the parser
which uses a simple, language-agnostic, unknown-
word handling technique to our three languages. In
Section 5, we show how this technique is extended
to include morphological information and present
parsing results for English and French. In Section 6,
we describe the Arabic morphological system and
explain how we used heuristic rules to cluster words
into word-classes or signatures. We present parsing
results for the version of the parser which uses this
information. In Section 7, we describe our attempts
to automatically determine the signatures for a lan-
guage and present parsing results for the three lan-
guages. Finally, in Section 8, we discuss how this
work might be fruitfully extended.
2 Latent Variable PCFG Parsing
Johnson (1998) showed that refining treebank cate-
gories with parent information leads to more accu-
rate grammars. This was followed by a collection of
linguistically motivated propositions for manual or
semi-automatic modifications of categories in tree-
banks (Klein and Manning, 2003). In PCFG-LAs,
first introduced by Matsuzaki et al (2005), the re-
fined categories are learnt from the treebank us-
ing unsupervised techniques. Each base category
? and this includes part-of-speech tags ? is aug-
mented with an annotation that refines its distribu-
tional properties.
Following Petrov et al (2006) latent annotations
and probabilities for the associated rules are learnt
incrementally following an iterative process consist-
ing of the repetition of three steps.
1. Split each annotation of each symbol into n
(usually 2) new annotations and create rules
with the new annotated symbols. Estimate1 the
probabilities of the newly created rules.
2. Evaluate the impact of the newly created anno-
tations and discard the least useful ones. Re-
estimate probabilities with the new set of anno-
tations.
3. Smooth the probabilities to prevent overfitting.
We use our own parser which trains a PCFG-LA us-
ing the above procedure and parses using the max-
1Estimation of the parameters is performed by running Ex-
pectation/Maximisation on the training corpus.
rule parsing algorithm (Petrov et al, 2006; Petrov
and Klein, 2007). PCFG-LA parsing is relatively
language-independent but has been shown to be very
effective on several languages (Petrov, 2009). For
our experiments, we set the number of iterations to
be 5 and we test on sentences less than or equal to
40 words in length. All our experiments, apart from
the final one, are carried out on the development sets
of our three languages.
3 The Datasets
Arabic We use the the Penn Arabic Treebank
(ATB) (Bies and Maamouri, 2003; Maamouri and
Bies., 2004). The ATB describes written Modern
Standard Arabic newswire and follows the style and
guidelines of the English Penn-II treebank. We use
the part-of-speech tagset defined by Bikel and Bies
(Bikel, 2004). We employ the usual treebank split
(80% training, 10% development and 10% test).
English We use the Wall Street Journal section of
the Penn-II Treebank (Marcus et al, 1994). We train
our parser on sections 2-21 and use section 22 con-
catenated with section 24 as our development set.
Final testing is carried out on Section 23.
French We use the French Treebank (Abeille? et
al., 2003) and divide it into 80% for training, 10%
for development and 10% for final results. We fol-
low the methodology defined by Crabbe? and Can-
dito (2008): compound words are merged and the
tagset consists of base categories augmented with
morphological information in some cases2.
Table 1 gives basic unknown word statistics for
our three datasets. We calculate the proportion of
words in our development sets which are unknown
or rare (specified by the cutoff value) in the corre-
sponding training set. To control for training set
size, we also provide statistics when the English
training set is reduced to the size of the Arabic and
French training sets and when the Arabic training set
is reduced to the size of the French training set. In an
ideal world where training set sizes are the same for
all languages, the problem of unknown words will
be greatest for Arabic and smallest for English. It is
2This is called the CC tagset: base categories with verbal
moods and extraction features
68
language cutoff #train #dev #unk %unk language #train #dev #unk %unk
Arabic 0 594,683 70,188 3794 5.40 Reduced English 597,999 72,970 2627 3.60
- 1 - - 6023 8.58 (Arabic Size) - - 3849 5.27
- 5 - - 11,347 16.17 - - - 6700 9.18
- 10 - - 15,035 21.42 - - - 9083 12.45
English 0 950,028 72,970 2062 2.83 Reduced Arabic 266,132 70,188 7027 10.01
- 1 - - 2983 4.09 (French Size) - - 10,208 14.54
- 5 - - 5306 7.27 - - - 16,977 24.19
- 10 - - 7230 9.91 - - - 21,434 30.54
French 0 268,842 35,374 2116 5.98 Reduced English 265,464 72,970 4188 5.74
- 1 - - 3136 8.89 (French Size) - - 5894 8.08
- 5 - - 5697 16.11 - - - 10,105 13.85
- 10 - - 7584 21.44 - - - 13,053 17.89
Table 1: Basic Unknown Word Statistics for Arabic, French and English
reasonable to assume that the levels of inflectional
richness have a role to play in these differences.
4 A Simple Lexical Probability Model
The simplest method for handling unknown words
within a generative probabilistic parsing/tagging
model is to reserve a proportion of the lexical rule
probability mass for such cases. This is done by
mapping rare words in the training data to a spe-
cial UNKNOWN terminal symbol and estimating rule
probabilities in the usual way. We illustrate the pro-
cess with the toy unannotated PCFG in Figures 1
and 2. The lexical rules in Fig. 1 are the original
rules and the ones in Fig. 2 are the result of apply-
ing the rare-word-to-unknown-symbol transforma-
tion. Given the input sentence The shares recovered,
the word recovered is mapped to the UNKNOWN to-
ken and the three edges corresponding to the rules
NNS ? UNKNOWN, V BD ? UNKNOWN and
JJ ? UNKNOWN are added to the chart at this posi-
tion. The disadvantage of this simple approach is ob-
vious: all unknown words are treated equally and the
tag whose probability distribution is most dominated
by rare words in the training will be deemed the
most likely (JJ for this example), regardless of the
characteristics of the individual word. Apart from
its ease of implementation, its main advantage is its
language-independence - it can be used off-the-shelf
for any language for which a PCFG is available.3
One parameter along which the simple lexical
3Our simple lexical model is equivalent to the Berkeley sim-
pleLexicon option.
probability model can vary is the threshold used to
decide whether a word in the training data is rare or
?unknown?. When the threshold is set to n, a word
in the training data is considered to be unknown if it
occurs n or fewer times. We experiment with three
thresholds: 1, 5 and 10. The result of this experi-
ment for our three languages is shown in Table 2.
The general trend we see in Table 2 is that the
number of training set words considered to be un-
known should be minimized. For all three lan-
guages, the worst performing grammar is the one
obtained when the threshold is increased to 10. This
result is not unexpected. With this simple lexical
probability model, there is a trade-off between ob-
taining good guesses for words which do not occur
in the training data and obtaining reliable statistics
for words which do. The greater the proportion of
the probability mass that we reserve for the unknown
word section of the grammar, the more performance
suffers on the known yet rare words since these are
the words which are mapped to the UNKNOWN sym-
bol. For example, assume the word restructuring oc-
curs 10 times in the training data, always tagged as
a VBG. If the unknown threshold is less than ten and
if the word occurs in the sentence to be parsed, a
VBG edge will be added to the chart at this word?s
position with the probability 10/#VBG. If, however,
the threshold is set to 10, the word (in the training set
and the input sentence) will be mapped to UNKNOWN
and more possibilities will be explored (an edge for
each TAG ? UNKNOWN rule in the grammar). We
can see from Table 1 that at threshold 10, one fifth
69
VBD -> fell 50/153
VBD -> reoriented 2/153
VBD -> went 100/153
VBD -> latched 1/153
NNS -> photofinishers 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> centrist 4/24
DT -> the 170/170
Figure 1: The original toy PCFG
VBD -> fell 50/153
VBD -> UNKNOWN 3/153
VBD -> went 100/153
NNS -> UNKNOWN 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> UNKNOWN 4/24
DT -> the 170/170
Figure 2: Rare ? UNKNOWN
VBD -> fell 50/153
VBD -> UNK-ed 3/153
VBD -> went 100/153
NNS -> UNK-s 1/201
NNS -> shares 200/201
JJ -> financial 20/24
JJ -> UNK-ist 4/24
DT -> the 170/170
Figure 3: Rare ? UN-
KNOWN+SIGNATURE
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic
1 78.60 80.49 79.53 94.03
5 77.17 79.81 78.47 91.16
10 75.32 78.69 76.97 89.06
English
1 89.20 89.73 89.47 95.60
5 88.91 89.74 89.33 94.66
10 88.00 88.97 88.48 93.61
French
1 83.60 84.17 83.88 94.90
5 82.31 83.10 82.70 92.99
10 80.87 82.05 81.45 91.56
Table 2: Varying the Unknown Threshold with the Simple Lexical Probability Model
of the words in the Arabic and French development
sets are unknown, and this is reflected in the drop in
parsing performance at these thresholds.
5 Making use of Morphology
Unknown words are not all the same. We exploit this
fact by examining the effect on parsing accuracy of
clustering rare training set words using cues from
the word?s morphological structure. Affixes have
been shown to be useful in part-of-speech tagging
(Schmid, 1994; Tseng et al, 2005) and have been
used in the Charniak (Charniak, 2000), Stanford
(Klein and Manning, 2003) and Berkeley (Petrov et
al., 2006) parsers. In this section, we contrast the
effect on parsing accuracy of making use of such in-
formation for our three languages of interest.
Returning to our toy English example in Figures 1
and 2, and given the input sentence The shares re-
covered, we would like to use the fact that the un-
known word recovered ends with the past tense
suffix -ed to boost the probability of the lexical
rule V BD ? UNKNOWN. If we specialise the
UNKNOWN terminal using information from English
morphology, we can do just that, resulting in the
grammar in Figure 3. Now the word recovered is
mapped to the symbol UNK-ed and the only edge
which is added to the chart at this position is the one
corresponding to the rule V BD ? UNK-ed.
For our English experiments we use the unknown
word classes (or signatures) which are used in the
Berkeley parser. A signature indicates whether a
words contains a digit or a hyphen, if a word starts
with a capital letter or ends with one of the following
English suffixes (both derivational and inflectional):
-s, -ed, -ing, -ion, -er, -est, -ly, -ity, -y and -al.
For our French experiments we employ the same
signature list as Crabbe? and Candito (2008), which
itself was adapted from Arun and Keller (2005).
This list consists of (a) conjugation suffixes of regu-
70
lar verbs for common tenses (eg. -ons, -ez, -ent. . . )
and (b) derivational suffixes for nouns, adverbs and
adjectives (eg. -tion, -ment, -able. . . ).
The result of employing signature information
for French and English is shown in Table 3. Be-
side each f-score the absolute improvement over the
UNKNOWN baseline (Table 2) is given. For both
languages there is an improvement at all unknown
thresholds. The improvement for English is statis-
tically significant at unknown thresholds 1 and 10.4
The improvement is more marked for French and is
statistically significant at all levels.
In the next section, we experiment with signature
lists for Arabic.5
6 Arabic Signatures
In order to use morphological clues for Arabic we
go further than just looking at suffixes. We exploit
all the richness of the morphology of this language
which can be expressed through morphotactics.
6.1 Handling Arabic Morphotactics
Morphotactics refers to the way morphemes com-
bine together to form words (Beesley, 1998; Beesley
and Karttunen, 2003). Generally speaking, morpho-
tactics can be concatenative, with morphemes either
prefixed or suffixed to stems, or non-concatenative,
with stems undergoing internal alternations to con-
vey morphosyntactic information. Arabic is consid-
ered a typical example of a language that employs
non-concatenative morphotactics.
Arabic words are traditionally classified into three
types: verbs, nouns and particles. Adjectives take
almost all the morphological forms of, and share the
same templatic structures with, nouns. Adjectives,
for example, can be definite, and are inflected for
case, number and gender.
There are a number of indicators that tell us
whether the word is a verb or a noun. Among
4Statistical significance was determined using the strati-
fied shuffling method. The software used to perform the test
was downloaded from http://www.cis.upenn.edu/
?
dbikel/software.html.
5An inspection of the Berkeley Arabic grammar (available
at http://code.google.com/p/berkeleyparser/
downloads/list) shows that no Arabic-specific signatures
were employed. The Stanford parser uses 9 signatures for Ara-
bic, designed for use with unvocalised text. An immediate fu-
ture goal is to test this signature list with our parser.
these indicators are prefixes, suffixes and word tem-
plates. A template (Beesley and Karttunen, 2003) is
a kind of vocalization mould in which a word fits. In
derivational morphology Arabic words are formed
through the amalgamation of two tiers, namely, root
and template. A root is a sequence of three (rarely
two or four) consonants which are called radicals,
and the template is a pattern of vowels, or a com-
bination of consonants and vowels, with slots into
which the radicals of the root are inserted.
For the purpose of detection we use the reverse
of this information. Given that we have a word, we
try to extract the stem, by removing prefixes and suf-
fixes, and match the word against a number of verbal
and nominal templates. We found that most Ara-
bic templatic structures are in complementary dis-
tribution, i.e. they are either restricted to nominal
or verbal usage, and with simple regular expression
matching we can decide whether a word form is a
noun or a verb.
6.2 Noun Indicators
In order to detect that a word form is a noun (or ad-
jective), we employ heuristic rules related to Arabic
prefixes/suffixes and if none of these rules apply we
attempt to match the word against templatic struc-
tures. Using this methodology, we are able to detect
95% of ATB nouns.6
We define a list of 42 noun templates which are
used to indicate active/passive participle nouns, ver-
bal nouns, nouns of instrument and broken plural
nouns (see Table 4 for some examples). Note that
templates ending with taa marboutah ?ap? or start-
ing with meem madmoumah ?mu? are not consid-
ered since they are covered by our suffix/prefix rules,
which are as follows:
1- The definite article prefix ?  or in Buckwalter
transliteration ?Al?.
2- The tanween suffix

, 

,

 or ?N?, ?F?, ?K?, ?AF?.
3- The feminine plural suffix HA, or ?+At?.
4- The taa marboutah ending ? or ?ap? whether as a
6The heuristics we developed are designed to work on dia-
critized texts. Although diacritics are generally ignored in mod-
ern writing, the issue of restoring diacritics has been satisfac-
torily addressed by different researchers. For example, Nelken
and Shieber (2005) presented an algorithm for restoring diacrit-
ics to undiacritized MSA texts with an accuracy of over 90%
and Habasah et al (2009) reported on a freely-available toolkit
(MADA-TOKAN) an accuracy of over 96%.
71
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic
1 80.67 82.19 *81.42 (+ 1.89) 96.32
5 80.66 82.81 *81.72 (+ 3.25) 95.15
10 79.86 82.49 *81.15 (+ 4.18) 94.38
English
1 ***89.64 89.95 89.79 (+ 0.32) 96.44
5 89.16 89.80 89.48 (+ 0.15) 96.32
10 89.14 89.78 **89.46 (+ 0.98) 96.21
French
1 85.15 85.77 *85.46 (+ 1.58) 96.13
5 84.08 84.80 *84.44 (+ 1.74) 95.54
10 84.21 84.78 *84.49 (+ 3.04) 94.68
Table 3: Baseline Signatures for Arabic, French and English
statistically significant with *:p < 10?4, **: p < 10?3, ***: p < 0.004,
Template Name Regular Specification
Arabic Buckwalter Expression
?A

?
	
?

	
K 

{inofiEAl {ino.i.A. verbal noun (masdar)
?A

?
	
??

mifoEAl mi.o.A. noun instrument
??

	
?


J?

? musotafoEil musota.o.i. noun participle
?J


?

A

	
?

? mafAEiyl ma.A.iy. noun plural
?

?
	
?


J?

{isotafoEal {isota.o.a. verb
??

?

	
? fuwEil .uw.i. verb passive
Table 4: Sample Arabic Templatic Structures for Nouns and Verbs
feminine marker suffix or part of the word.
5- The genitive case marking kasrah 

, or ?+i?.
6- Words of length of at least five characters ending
with doubled yaa ?


or ?y??.
7- Words of length of at least six characters ending
with alif mamdoudah and hamzah Z  or ?A??.
8- Words of length of at least seven characters start-
ing with meem madmoumah ? or ?mu?.
6.3 Verb Indicators
In the same way, we define a list of 16 templates and
we combine them with heuristic rules related to Ara-
bic prefixes/suffixes to detect whether a word form
is exclusively a verb. The prefix/suffix heuristics are
as follows:
9-The plural marker suffix  ? or ?uwA? indicates a
verb.
10- The prefixes H , ?


,
	
?
,



,

? or ?sa?, ?>a?,
?>u?, ?na?, ?nu?, ?ya?, ?yu?, ?ta?, ?tu? indicate im-
prefective verb.
The verbal templates are less in number than the
noun templates yet they are no less effective in de-
tecting the word class (see Table 4 for examples).
Using these heuristics we are able to detect 85% of
ATB verbs.
6.4 Arabic Signatures
We map the 72 noun/verb classes that are identi-
fied using our hand-crafted heuristics into sets of
signatures of varying sizes: 4, 6, 14, 21, 25, 28
and 72. The very coarse-grained set considers just
4 signatures UNK-noun, UNK-verb, UNK-num,
and UNK and the most fine-grained set of 72 signa-
tures associates one signature per heuristic. In ad-
dition, we have evaluated the effect of reordering
rules and templates and also the effect of collating
all signatures satisfying an unknown word. The re-
sults of using these various signatures sets in parsing
72
UNK
NUM NOUN VERB
digits (see section 6.2) (see section 6.3)
Al definiteness tashkil At suffix ap suffix imperfect
rule 1 rules 2 and 5 rule 3 rule 4 rule 10
y? suffix A? suffix mu prefix verbal noun templates suffixes
rule 6 rule 7 rule 8 3 groupings dual/plural suffixes
plural templates participle active templates participle passive templates instrument templates passive templates
4 groupings
other templates verbal templates
5 groupings
Table 6: Arabic signatures
Cutoff 1 5 10
4 80.78 80.71 80.09
6 81.14 81.16 81.06
14 80.88 81.45 81.19
14 reorder 81.39 81.01 80.81
21 81.38 81.55 81.35
21 reorder 81.20 81.13 80.58
21 collect 80.94 80.56 79.63
25 81.18 81.25 81.26
28 81.42 81.72 (+ 3.25) 81.15
72 79.64 78.87 77.58
Table 5: Baseline Signatures for Arabic
our Arabic development set are presented in Table 5.
We achieve our best labeled bracketing f-score using
28 signatures with an unknown threshold of five. In
fact we get an improvement of 3.25% over using no
signatures at all (see Table 2). Table 3 describes in
more detail the scores obtained using the 28 signa-
tures present in Table 6. Apart from the set contain-
ing 72 signatures, all of the baseline signature sets in
Table 5 yield a statistically significant improvement
over the generic UNKNOWN results (p < 10?4).
7 Using Information Gain to Determine
Signatures
It is clear that dividing the UNKNOWN terminal into
more fine-grained categories based on morpholog-
ical information helps parsing for our three lan-
guages. In this section we explore whether useful
morphological clues can be learnt automatically. If
they can, it means that a latent-variable PCFG parser
can be adapted to any language without knowledge
of the language in question since the only language-
specific component in such a parser is the unknown-
signature specification.
In a nutshell, we extract affix features from train-
ing set words7 and then use information gain to rank
these features in terms of their predictive power in a
POS-tagging task. The features deemed most dis-
criminative are then used as signatures, replacing
our baseline signatures described in Sections 5 and
6. We are not going as far as actual POS-tagging,
but rather seeing whether the affixes that make good
features for a part-of-speech tagger also make good
unknown word signatures.
We experiment with English and French suffixes
of length 1-3 and Arabic prefixes and suffixes of var-
ious lengths as well as stem prefixes and suffixes of
length 2, 4 and 6. For each of our languages we
experiment with several information gain thresholds
on our development sets and we fix on an English
signature list containing 24 suffixes, a French list
containing 48 suffixes and an Arabic list containing
38 prefixes and suffixes.
Our development set results are presented in Ta-
ble 7. For all three languages, the information gain
signatures perform at a comparable level to the base-
line hand-crafted signatures (Table 3). For each
of the three unknown-word handling techniques, no
signature (UNKNOWN), hand-crafted signatures and
information gain signatures, we select the best un-
known threshold for each language?s development
set and apply these grammars to our test sets. The
f-scores are presented in Table 8, along with the up-
per bounds obtained by parsing with these grammars
in gold-tag mode. For French, the effect of tagging
accuracy on overall parse accuracy is striking. The
improvements that we get from using morphological
signatures are greatest for Arabic8 and smallest for
7We omit all function words and high frequency words be-
cause we are interested in the behaviour of words which are
likely to be similar to rare words.
8Bikel?s parser trained on the same Arabic data and tested
on the same input achieves an f-score of 76.50%. We trained
a 5-split-merge-iteration Berkeley grammar and parsed with the
73
Unknown Threshold Recall Precision F-Score Tagging Accuracy
Arabic IG
1 80.10 82.15 *81.11 (+ 1.58) 96.53
5 80.03 82.49 *81.32 (+ 2.85) 95.30
10 80.17 82.40 *81.27 (+ 4.3) 94.66
English IG
1 89.38 89.87 89.63 (+ 0.16) 96.45
5 89.54 90.22 ***89.88 (+ 0.55) 96.41
10 89.22 90.05 *89.63 (+ 1.15) 96.19
French IG
1 84.78 85.36 *85.07 (+ 1.19) 96.17
5 84.63 85.24 **84.93 (+ 2.23) 95.30
10 84.18 84.80 *84.49 (+ 3.09) 94.68
Table 7: Information Gain Signature Results
statistically significant with *:p < 10?4, **: p < 2 ? 10?4, ***: p < 0.005
Language No Sig Baseline Sig IG Sig
Arabic 78.34 *81.59 *81.33
Arabic Gold Tag 81.46 82.43 81.90
English 89.48 89.65 89.77
English Gold Tag 89.94 90.10 90.23
French 83.74 *85.77 **85.55
French Gold Tag 88.82 88.41 88.86
statistically significant with *: p < 10?4, **: p < 10?3
Table 8: F-Scores on Test Sets
English. The results for the information gain signa-
tures are promising and warrant further exploration.
8 Conclusion
We experiment with two unknown-word-handling
techniques in a statistical generative parsing model,
applying them to Arabic, French and English. One
technique is language-agnostic and the other makes
use of some morphological information (signatures)
in assigning part-of-speech tags to unknown words.
The performance differences from the two tech-
niques are smallest for English, the language with
the sparsest morphology of the three and the small-
est proportion of unknown words in its development
set. As a result of carrying out these experiments,
we have developed a list of Arabic signatures which
can be used with any statistical parser which does
Berkeley parser, achieving an f-score of 75.28%. We trained the
Berkeley parser with the -treebank SINGLEFILE option so that
English signatures were not employed.
its own tagging. We also present results which show
that signatures can be learnt automatically.
Our experiments have been carried out using gold
tokens. Tokenisation is an issue particularly for Ara-
bic, but also for French (since the treebank contains
merged compounds) and to a much lesser extent for
English (unedited text with missing apostrophes). It
is important that the experiments in this paper are re-
peated on untokenised text using automatic tokeni-
sation methods (e.g. MADA-TOKAN).
The performance improvements that we demon-
strate for Arabic unknown-word handling are obvi-
ously just the tip of the iceberg in terms of what can
be done to improve performance on a morpholog-
ically rich language. The simple generative lexical
probability model we use can be improved by adopt-
ing a more sophisticated approach in which known
and unknown word counts are combined when esti-
mating lexical rule probabilities for rare words (see
Huang and Harper (2009) and the Berkeley sophis-
ticatedLexicon training option). Further work will
also include making use of a lexical resource exter-
nal to the treebank (Goldberg et al, 2009; Habash,
2008) and investigating clustering techniques to re-
duce data sparseness (Candito and Crabbe?, 2009).
Acknowledgements
This research is funded by Enterprise Ireland
(CFTD/07/229 and PC/09/037) and the Irish Re-
search Council for Science Engineering and Tech-
nology (IRCSET). We thank Marie Candito and our
three reviewers for their very helpful suggestions.
74
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel,
2003. Treebanks: Building and Using Parsed
Corpora, chapter Building a Treebank for French.
Kluwer, Dordrecht.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In ACL. The Association for Computer Lin-
guistics.
Kenneth R. Beesley and Lauri Karttunen. 2003. Finite
State Morphology. CSLI studies in computational lin-
guistics.
Kenneth R. Beesley. 1998. Arabic morphology using
only finite-state operations. In The Workshop on Com-
putational Approaches to Semitic Languages.
Ann Bies and Mohammed Maamouri. 2003. Penn Ara-
bic Treebank guidelines. Technical Report TB-1-28-
03.
Dan Bikel. 2004. On the Parameter Space of Generative
Lexicalized Parsing Models. Ph.D. thesis, University
of Pennslyvania.
Marie Candito and Benoit Crabbe?. 2009. Improving gen-
erative statistical parsing with semi-supervised word
clustering. In Proceedings of IWPT?09.
Marie Candito, Beno??t Crabbe?, and Djame? Seddah. 2009.
On statistical parsing of French with supervised and
semi-supervised strategies. In Proceedings of the
EACL 2009 Workshop on Computational Linguis-
tic Aspects of Grammatical Inference, pages 49?57,
Athens, Greece, March.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the Annual Meeting of the
North American Association for Computational Lin-
guistics (NAACL-00), pages 132?139, Seattle, Wash-
ington.
Beno??t Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de TALN.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities.
In EACL, pages 327?335. The Association for Com-
puter Linguistics.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
Mada+tokan: A toolkit for Arabic tokenization, di-
acritization, morphological disambiguation, pos tag-
ging, stemming and lemmatization. In Proceedings of
the 2nd International Conference on Arabic Language
Resources and Tools (MEDAR).
Nizar Habash. 2008. Four techniques for online handling
of out-of-vocabulary words in arabic-english statistical
machine translation. In Proceedings of Association for
Computational Linguistics, pages 57?60.
Zhongqiang Huang and Mary Harper. 2009. Self-
training pcfg grammars with latent annotations across
languages. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing,
Singapore, August.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Dan Klein and Chris Manning. 2003. Accurate unlex-
icalised parsing. In Proceedings of the 41st Annual
Meeting of the ACL.
Mohammed Maamouri and Ann Bies. 2004. Developing
an Arabic Treebank: Methods, guidelines, procedures,
and tools. In Workshop on Computational Approaches
to Arabic Script-based Languages, COLING.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn Tree-
bank: Annotating predicate argument structure. In
Proceedings of the 1994 ARPA Speech and Natural
Language Workshop, pages 114?119, Princeton, New
Jersey.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 75?82, Ann Arbor, June.
Rani Nelken and Stuart M. Shieber. 2005. Arabic dia-
critization using weighted finite-state transducers. In
ACL-05 Workshop on Computational Approaches to
Semitic Languages.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Proceedings of HLT-
NAACL, Rochester, NY, April.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the ACL, Sydney,
Australia, July.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Berkeley, Berkeley, CA, USA.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing (NeMLaP-1), pages 44?49.
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morphological features help POS tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing.
75
Finding Common Ground: Towards a Surface Realisation Shared Task
Anja Belz
Natural Language Technology Group
Computing, Mathematical and Information Sciences
University of Brighton, Brighton BN2 4GJ, UK
a.s.belz@brighton.ac.uk
Mike White
Department of Linguistics
The Ohio State University
Columbus, OH, USA
mwhite@ling.osu.edu
Josef van Genabith and Deirdre Hogan
National Centre for Language Technology
School of Computing
Dublin City University
Dublin 9, Ireland
{dhogan,josef}@computing.dcu.ie
Amanda Stent
AT&T Labs Research, Inc.,
180 Park Avenue
Florham Park, NJ 07932, USA
stent@research.att.com
Abstract
In many areas of NLP reuse of utility tools
such as parsers and POS taggers is now
common, but this is still rare in NLG. The
subfield of surface realisation has perhaps
come closest, but at present we still lack
a basis on which different surface realis-
ers could be compared, chiefly because of
the wide variety of different input repre-
sentations used by different realisers. This
paper outlines an idea for a shared task in
surface realisation, where inputs are pro-
vided in a common-ground representation
formalism which participants map to the
types of input required by their system.
These inputs are derived from existing an-
notated corpora developed for language
analysis (parsing etc.). Outputs (realisa-
tions) are evaluated by automatic compari-
son against the human-authored text in the
corpora as well as by human assessors.
1 Background
When reading a paper reporting a new NLP sys-
tem, it is common these days to find that the
authors have taken an NLP utility tool off the
shelf and reused it. Researchers frequently reuse
parsers, POS-taggers, named entity recognisers,
coreference resolvers, and many other tools. Not
only is there a real choice between a range of dif-
ferent systems performing the same task, there are
also evaluation methodologies to help determine
what the state of the art is.
Natural Language Generation (NLG) has not
so far developed generic tools and methods for
comparing them to the same extent as Natural
Language Analysis (NLA) has. The subfield of
NLG that has perhaps come closest to developing
generic tools is surface realisation. Wide-coverage
surface realisers such as PENMAN/NIGEL (Mann
and Mathiesen, 1983), FUF/SURGE (Elhadad and
Robin, 1996) and REALPRO (Lavoie and Ram-
bow, 1997) were intended to be more or less off-
the-shelf plug-and-play modules. But they tended
to require a significant amount of work to adapt
and integrate, and required highly specific inputs
incorporating up to several hundred features that
needed to be set.
With the advent of statistical techniques in NLG
surface realisers appeared for which it was far sim-
pler to supply inputs, as information not provided
in the inputs could be added on the basis of like-
lihood. An early example, the Japan-Gloss sys-
tem (Knight et al, 1995) replaced PENMAN?s de-
fault settings with statistical decisions. The Halo-
gen/Nitrogen developers (Langkilde and Knight,
1998a) allowed inputs to be arbitrarily underspec-
ified, and any decision not made before the realiser
was decided simply by highest likelihood accord-
ing to a language model, automatically trainable
from raw corpora.
The Halogen/Nitrogen work sparked an interest
in statistical NLG which led to a range of surface
realisation methods that used corpus frequencies
in one way or another (Varges and Mellish, 2001;
White, 2004; Velldal et al, 2004; Paiva and Evans,
2005). Some surface realisation work looked at
directly applying statistical models during a lin-
guistically informed generation process to prune
the search space (White, 2004; Carroll and Oepen,
2005).
While statistical techniques have led to realisers
that are more (re)usable, we currently still have
no way of determining what the state of the art
is. A significant subset of statistical realisation
work (Langkilde, 2002; Callaway, 2003; Nakan-
ishi et al, 2005; Zhong and Stent, 2005; Cahill and
van Genabith, 2006; White and Rajkumar, 2009)
has recently produced results for regenerating the
Penn Treebank. The basic approach in all this
work is to remove information from the Penn Tree-
bank parses (the word strings themselves as well
as some of the parse information), and then con-
vert and use these underspecified representations
as inputs to the surface realiser whose task it is to
reproduce the original treebank sentence. Results
are typically evaluated using BLEU, and, roughly
speaking, BLEU scores go down as more informa-
tion is removed.
While publications of work along these lines do
refer to each other and (tentatively) compare BLEU
scores, the results are not in fact directly compara-
ble, because of the differences in the input repre-
sentations automatically derived from Penn Tree-
bank annotations. In particular, the extent to which
they are underspecified varies from one system to
the next.
The idea we would like to put forward with
this short paper is to develop a shared task in sur-
face realisation based on common inputs and an-
notated corpora of paired inputs and outputs de-
rived from various resources from NLA that build
on the Penn Treebank. Inputs are provided in a
common-ground representation formalism which
participants map to the types of input required by
their system. These inputs are automatically de-
rived from the Penn Treebank and the various lay-
ers of annotation (syntactic, semantic, discourse)
that have been developed for the documents in it.
Outputs (realisations) are evaluated by automatic
comparison against the human-authored text in the
corpora as well as by by human assessors.
In the short term, such a shared task would
make existing and new approaches directly com-
parable by evaluation on the benchmark data asso-
ciated with the shared task. In the long term, the
common-ground input representation may lead to
a standardised level of representation that can act
as a link between surface realisers and preceding
modules, and can make it possible to use alterna-
tive surface realisers as drop-in replacements for
each other.
2 Towards Common Inputs
One hugely challenging aspect in developing a
Surface Realisation task is developing a common
input representation that all, or at least a major-
ity of, surface realisation researchers are happy to
work with. While many different formalisms have
been used for input representations to surface re-
alisers, one cannot simply use e.g. van Genabith
et al?s automatically generated LFG f-structures,
White et als CCG logical forms, Nivre?s depen-
dencies, Miyao et al?s HPSG predicate-argument
structures or Copestake?s MRSs etc., as each of
them would introduce a bias in favour of one type
of system.
One possible solution is to develop a meta-
representation which contains, perhaps on multi-
ple layers of representation, all the information
needed to map to any of a given set of realiser in-
put representations, a common-ground representa-
tion that acts as a kind of interlingua for translating
between different input representations.
An important issue in deriving input repre-
sentations from semantically, syntactically and
discourse-annotated corpora is deciding what in-
formation not to include. A concern is that mak-
ing such decisions by committee may be difficult.
One way to make it easier might be to define sev-
eral versions of the task, where each version uses
inputs of different levels of specificity.
Basing a common input representation on what
can feasibly be obtained from non-NLG resources
would put everyone on reasonably common foot-
ing. If, moreover, the common input representa-
tions can be automatically derived from annota-
tions in existing resources, then data can be pro-
duced in sufficient quantities to make it feasible
for participants to automatically learn mappings
from the system-neutral input to their own input.
The above could be achieved by doing some-
thing along the lines of the CoNLL?08 shared task
on Joint Parsing of Syntactic and Semantic De-
pendencies, for which the organisers combined the
Penn Treebank, Propbank, Nombank and the BBN
Named Entity corpus into a dependency represen-
tation. Brief descriptions of these resources and
more details on this idea are provided in Section 4
below.
3 Evaluation
As many NLG researchers have argued, there is
usually not a single right answer in NLG, but var-
ious answers, some better than others, and NLG
tasks should take this into account. If a surface
realisation task is focused on single-best realiza-
tions, then it will not encourage research on pro-
ducing all possible good realizations, or multiple
acceptable realizations in a ranked list, etc. It
may not be the best approach to encourage sys-
tems that try to make a single, safe choice; in-
stead, perhaps one should encourage approaches
that can tell when multiple choices would be ok,
and if some would be better than others.
In the long term we need to develop task defi-
nitions, data resources and evaluation methodolo-
gies that properly take into account the one-to-
many nature of NLG, but in the short term it may be
more realistic to reuse existing non-NLG resources
(which do not provide alternative realisations) and
to adapt existing evaluation methodologies includ-
ing intrinsic assessment of Fluency, Clarity and
Appropriateness by trained evaluators, and auto-
matic intrinsic methods such as BLEU and NIST.
One simple way of adapting the latter, for exam-
ple, could be to calculate scores for the n best re-
alisations produced by a realiser and then to com-
pute a weighted average where scores for reali-
sations are weighted in inverse proportion to the
ranks given to the realisations by the realiser.
4 Data
There is a wide variety of different annotated re-
sources that could be of use in a shared task in sur-
face realisation. Many of these include documents
originally included in the Penn Treebank, and thus
make it possible in principle to combine the var-
ious levels of annotation into a single common-
ground representation. The following is a (non-
exhaustive) list of such resources:
1. Penn Treebank-3 (Marcus et al, 1999): one
million words of hand-parsed 1989 Wall
Street Journal material annotated in Treebank
II style. The Treebank bracketing style al-
lows extraction of simple predicate/argument
structure. In addition to Treebank-1 mate-
rial, Treebank-3 contains documents from the
Switchboard and Brown corpora.
2. Propbank (Palmer et al, 2005): This is a se-
mantic annotation of the Wall Street Journal
section of Penn Treebank-2. More specifi-
cally, each verb occurring in the Treebank has
been treated as a semantic predicate and the
surrounding text has been annotated for ar-
guments and adjuncts of the predicate. The
verbs have also been tagged with coarse
grained senses and with inflectional informa-
tion.
3. NomBank 1.0 (Meyers et al, 2004): Nom-
Bank is an annotation project at New York
University that provides argument structure
for common nouns in the Penn Treebank.
NomBank marks the sets of arguments that
occur with nouns in PropBank I, just as the
latter records such information for verbs.
4. BBN Pronoun Coreference and Entity Type
Corpus (Weischedel and Brunstein, 2005):
supplements the Wall Street Journal corpus,
adding annotation of pronoun coreference,
and a variety of entity and numeric types.
5. FrameNet (Johnson et al, 2002): 150,000
sentences annotated for semantic roles and
possible syntactic realisations. The annotated
sentences come from a variety of sources, in-
cluding some PropBank texts.
6. OntoNotes 2.0 (Weischedel et al, 2008):
OntoNotes 1.0 contains 674k words of Chi-
nese and 500k words of English newswire
and broadcast news data. OntoNotes follows
the Penn Treebank for syntax and PropBank
for predicate-argument structure. Its seman-
tic representation will include word sense
disambiguation for nouns and verbs, with
each word sense connected to an ontology,
and coreference. The current goal is to anno-
tate over a million words each of English and
Chinese, and half a million words of Arabic
over five years.
There are other resources which may be use-
ful. Zettelmoyer and Collins (2009) have man-
ually converted the original SQL meaning an-
notations of the ATIS corpus (et al, 1994)?
some 4,637 sentences?into lambda-calculus ex-
pressions which were used for training and testing
their semantic parser. This resource might make a
good out-of-domain test set for generation systems
trained on WSJ data.
FrameNet, used for semantic parsing, see for
example Gildea and Jurafsky (2002), identifies a
sentence?s frame elements and assigns semantic
roles to the frame elements. FrameNet data (Baker
and Sato, 2003) was used for training and test sets
in one of the SensEval-3 shared tasks in 2004 (Au-
tomatic Labeling of Semantic Roles). There has
been some work combining FrameNet with other
lexical resources. For example, Shi and Mihal-
cea (2005) integrated FrameNet with VerbNet and
WordNet for the purpose of enabling more robust
semantic parsing.
The Semlink project (http://verbs.colorado.
edu/semlink/) aims to integrate Propbank,
FrameNet, WordNet and VerbNet.
Other relevant work includes Moldovan and
Rus (Moldovan and Rus, 2001; Rus, 2002) who
developed a technique for parsing into logical
forms and used this to transform WordNet concept
definitions into logical forms. The same method
(with additional manual correction) was used to
produce the test set for another SensEval-3 shared
task (Identification of Logic Forms in English).
4.1 CoNLL 2008 Shared Task Data
Perhaps the most immediately promising resource
is is the CoNLL shared task data from 2008 (Sur-
deanu et al, 2008) which has syntactic depen-
dency annotations, named-entity boundaries and
the semantic dependencies model roles of both
verbal and nominal predicates. The data consist
of excerpts from Penn Treebank-3, BBN Pronoun
Coreference and Entity Type Corpus, PropBank I
and NomBank 1.0. In CoNLL ?08, the data was
used to train and test systems for the task of pro-
ducing a joint semantic and syntactic dependency
analysis of English sentences (the 2009 CoNLL
Shared Task extended this to multi-lingual data).
It seems feasible that we could reuse the CoNLL
data for a prototype Surface Realisation task,
adapting it and inversing the direction of the task,
i.e. mapping from syntactic-semantic dependency
representations to word strings.
5 Developing the Task
The first step in developing a Surface Realisa-
tion task could be to get together a working
group of surface realisation researchers to develop
a common-ground input representation automati-
cally derivable from a set of existing resources.
As part of this task a prototype corpus exempli-
fying inputs/outputs and annotations could be de-
veloped. At the end of this stage it would be use-
ful to write a white paper and circulate it and the
prototype corpus among the NLG (and wider NLP)
community for feedback and input.
After a further stage of development, it may be
feasible to run a prototype surface realisation task
at Generation Challenges 2011, combined with a
session for discussion and roadmapping. Depend-
ing on the outcome of all of this, a full-blown task
might be feasible by 2012. Some of this work will
need funding to be feasible, and the authors of this
paper are in the process of applying for financial
support for these plans.
6 Concluding Remarks
In this paper we have provided an overview of ex-
isting resources that could potentially be used for
a surface realisation task, and have outlined ideas
for how such a task might work. The core idea
is to develop a common-ground input representa-
tion which participants map to the types of input
required by their system. These inputs are derived
from existing annotated corpora developed for lan-
guage analysis. Outputs (realisations) are evalu-
ated by automatic comparison against the human-
authored text in the corpora as well as by by hu-
man assessors. Evaluation methods are adapted to
take account of the one-to-many nature of the re-
alisation mapping.
The ideas outlined in this paper began as a pro-
longed email exchange, interspersed with discus-
sions at conferences, among the authors. This pa-
per summarises our ideas as they have evolved so
far, to enable feedback and input from other re-
searchers interested in this type of task.
References
Colin F. Baker and Hiroaki Sato. 2003. The framenet
data and software. In Proceedings of ACL?03.
A. Cahill and J. van Genabith. 2006. Robust PCFG-
based generation using automatically acquired LFG
approximations. In Proc. ACL?06, pages 1033?44.
Charles Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proceedings of the
18th International Joint Conference on Artificial In-
telligence (IJCAI 2003), pages 811?817.
J. Carroll and S. Oepen. 2005. High efficiency
realization for a wide-coverage unification gram-
mar. In Proceedings of the 2nd International Joint
Conference on Natural Language Processing (IJC-
NLP?05), volume 3651, pages 165?176. Springer
Lecture Notes in Artificial Intelligence.
M. Elhadad and J. Robin. 1996. An overview of
SURGE: A reusable comprehensive syntactic real-
ization component. Technical Report 96-03, Dept
of Mathematics and Computer Science, Ben Gurion
University, Beer Sheva, Israel.
Deborah Dahl et al 1994. Expanding the scope of the
ATIS task: the ATIS-3 corpus. In Proceedings of the
ARPA HLT Workshop.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguis-
tics, 28(3):245?288.
C. Johnson, C. Fillmore, M. Petruck, C. Baker,
M. Ellsworth, J. Ruppenhoper, and E.Wood. 2002.
Framenet theory and practice. Technical report.
K. Knight, I. Chander, M. Haines, V. Hatzivassiloglou,
E. Hovy, M. Iida, S. Luk, R. Whitney, and K. Ya-
mada. 1995. Filling knowledge gaps in a broad-
coverage MT system. In Proceedings of the Four-
teenth International Joint Conference on Artificial
Intelligence (IJCAI ?95), pages 1390?1397.
I. Langkilde and K. Knight. 1998a. Generation
that exploits corpus-based statistical knowledge. In
Proc. COLING-ACL. http://www.isi.edu/licensed-
sw/halogen/nitro98.ps.
I. Langkilde. 2002. An empirical verification of cover-
age and correctness for a general-purpose sentence
generator. In Proc. 2nd International Natural Lan-
guage Generation Conference (INLG ?02).
B. Lavoie and O. Rambow. 1997. A fast and portable
realizer for text generation systems. In Proceedings
of the 5th Conference on Applied Natural Language
Processing (ANLP?97), pages 265?268.
W. Mann and C. Mathiesen. 1983. NIGEL: A sys-
temic grammar for text generation. Technical Re-
port ISI/RR-85-105, Information Sciences Institute.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, and Ann Taylor. 1999. Treebank-
3. Technical report, Linguistic Data Consortium,
Philadelphia.
Adam Meyers, Ruth Reeves, and Catherine Macleod.
2004. Np-external arguments a study of argument
sharing in english. In MWE ?04: Proceedings of
the Workshop on Multiword Expressions, pages 96?
103, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Dan I. Moldovan and Vasile Rus. 2001. Logic form
transformation of wordnet and its applicability to
question answering. In Proceedings of ACL?01.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic models for disambiguation of an
hpsg-based chart generator. In Proceedings of the
9th International Workshop on Parsing Technology
(Parsing?05), pages 93?102. Association for Com-
putational Linguistics.
D. S. Paiva and R. Evans. 2005. Empirically-based
control of natural language generation. In Proceed-
ings ACL?05.
M. Palmer, P. Kingsbury, and D. Gildea. 2005. The
proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Vasile Rus. 2002. Logic Form For WordNet Glosses
and Application to Question Answering. Ph.D. the-
sis.
Lei Shi and Rada Mihalcea. 2005. Putting pieces to-
gether: Combining framenet, verbnet and wordnet
for robust semantic parsing. In Proceedings of CI-
CLing?05.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL-2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL ?08:
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 159?177.
S. Varges and C. Mellish. 2001. Instance-based natu-
ral language generation. In Proceedings of the 2nd
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL ?01),
pages 1?8.
E. Velldal, S. Oepen, and D. Flickinger. 2004. Para-
phrasing treebanks for stochastic realization rank-
ing. In Proceedings of the 3rd Workshop on Tree-
banks and Linguistic Theories (TLT ?04), Tuebin-
gen, Germany.
Ralph Weischedel and Ada Brunstein. 2005. Bbn pro-
noun coreference and entity type corpus. Technical
report, Linguistic Data Consortium.
Ralph Weischedel et al 2008. Ontonotes release 2.0.
Technical report, Linguistic Data Consortium.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for ccg realisation. In Pro-
ceedings of the 2009 Conference on Empririal Meth-
ods in Natural Language Processing (EMNLP?09),
pages 410?419.
M. White. 2004. Reining in CCG chart realization. In
A. Belz, R. Evans, and P. Piwek, editors, Proceed-
ings INLG?04, volume 3123 of LNAI, pages 182?
191. Springer.
Luke Zettlemoyer and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical forms. In Proceedings of ACL-IJCNLP?09.
H. Zhong and A. Stent. 2005. Building surface
realizers automatically from corpora. In A. Belz
and S. Varges, editors, Proceedings of UCNLG?05,
pages 49?54.
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 14?19,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Decreasing lexical data sparsity in statistical syntactic parsing - experiments
with named entities
Deirdre Hogan, Jennifer Foster and Josef van Genabith
National Centre for Language Technology
School of Computing
Dublin City University
Dublin 9, Ireland
dhogan,jfoster,josef@computing.dcu.ie
Abstract
In this paper we present preliminary exper-
iments that aim to reduce lexical data spar-
sity in statistical parsing by exploiting infor-
mation about named entities. Words in the
WSJ corpus are mapped to named entity clus-
ters and a latent variable constituency parser
is trained and tested on the transformed cor-
pus. We explore two different methods for
mapping words to entities, and look at the ef-
fect of mapping various subsets of named en-
tity types. Thus far, results show no improve-
ment in parsing accuracy over the best base-
line score; we identify possible problems and
outline suggestions for future directions.
1 Introduction
Techniques for handling lexical data sparsity in
parsers have been important ever since the lexical-
isation of parsers led to significant improvements
in parser performance (Collins, 1999; Charniak,
2000). The original treebank set of non-terminal la-
bels is too general to give good parsing results. To
overcome this problem, in lexicalised constituency
parsers, non-terminals are enriched with lexical in-
formation. Lexicalisation of the grammar vastly
increases the number of parameters in the model,
spreading the data over more specific events. Statis-
tics based on low frequency events are not as reliable
as statistics on phenomena which occur regularly in
the data; frequency counts involving words are typi-
cally sparse.
Word statistics are also important in more re-
cent unlexicalised approaches to constituency pars-
ing such as latent variable parsing (Matsuzaki et al,
2005; Petrov et al, 2006). The basic idea of latent
variable parsing is that rather than enrich the non-
terminal labels by augmenting them with words, a
set of enriched labels which can encapsulate the syn-
tactic behaviour of words is automatically learned
via an EM training mechanism.
Parsers need to be able to handle both low fre-
quency words and words occurring in the test set
which were unseen in the training set (unknown
words). The problem of rare and unknown words is
particularly significant for languages where the size
of the treebank is small. Lexical sparseness is also
critical when running a parser on data that is in a dif-
ferent domain to the domain upon which the parser
was trained. As interest in parsing real world data
increases, a parsers ability to adequately handle out-
of-domain data is critical.
In this paper we examine whether clustering
words based on their named entity category can be
useful for reducing lexical sparsity in parsing. In-
tuitively word tokens in the corpus such as, say,
?Dublin? and ?New York? should play similar syn-
tactic roles in sentences. Likewise, it is difficult to
see how different people names could have differ-
ent discriminatory influences on the syntax of sen-
tences. This paper describes experiments at replac-
ing word tokens with special named entity tokens
(person names are mapped to PERSON tokens and
so on). Words in the original WSJ treebank are
mapped to entity types extracted from the BBN cor-
pus (Weischedel and Brunstein, 2005) and a latent
variable parser is trained and tested on the mapped
corpus. Ultimately, the motivation behind grouping
words together in this fashion is to make it easier for
14
the parser to recognise regularities in the data.1
The structure of paper is as follows: A brief sum-
mary of related work is given in Section 2. This
includes an outline of a common treatment of low
frequency and rare words in constituency parsing,
involving a mapping process that is similar to the
named entity mappings. Section 3 presents the ex-
periments carried out, starting with a short introduc-
tion of the named entity resource used in our exper-
iments and a description of the types of basic entity
mappings we examine. In ?3.1 and ?3.2 we describe
the two different types of mapping technique. Re-
sults are presented in Section 4, followed by a brief
discussion in Section 5 indicating possible problems
and avenues worth pursuing. Finally, we conclude.
2 Related Work
Much previous work on parsing and multiword units
(MWUs) adopts the words-with-spaces approach
which treats MWUs as one token (by concatenat-
ing the words together) (Nivre and Nilsson, 2004;
Cafferkey et al, 2007; Korkontzelos and Manand-
har, 2010). Alternative approaches are that of Finkel
and Manning (2009) on joint parsing and named en-
tity recognition and the work of (Wehrli et al, 2010)
which uses collocation information to rank compet-
ing hypotheses in a symbolic parser. Also related
is work on MWUs and grammar engineering, such
as (Zhang et al, 2006; Villavicencio et al, 2007)
where automatically detected MWUs are added to
the lexicon of a HPSG grammar to improve cover-
age.
Our work is most similar to the words-with-
spaces approach. Our many-to-one experiments
(see ?3.1) in particular are similar to previous
work on parsing words-with-spaces, except that we
map words to entity types rather than concatenated
words. Results are difficult to compare however, due
to different parsing methodologies, different types
of MWUs, as well as different evaluation methods.
Other relevant work is the integration of named
1It is true that latent variable parsers automatically induce
categories for similar words, and thus might be expected to
induce a category for say names of people if examples of
such words occurred in similar syntactic patterns in the data.
Nonetheless, the problem of data sparsity remains - it is diffi-
cult even for latent variable parsers to learn accurate patterns
based on words which only occur say once in the training set.
entity types in a surface realisation task by Rajku-
mar et al (2009) and the French parsing experiments
of (Candito and Crabbe?, 2009; Candito and Sed-
dah, 2010) which involve mapping words to clusters
based on morphology as well as clusters automati-
cally induced via unsupervised learning on a large
corpus.
2.1 Parsing unknown words
Most state-of-the-art constituency parsers (e.g.
(Petrov et al, 2006; Klein and Manning, 2003))
take a similar approach to rare and unknown words.
At the beginning of the training process very low
frequency words in the training set are mapped to
special UNKNOWN tokens. In this way, some
probability mass is reserved for occurrences of UN-
KNOWN tokens and the lexicon contains produc-
tions for such tokens (X ? UNKNOWN), with as-
sociated probabilities. When faced with a word in
the test set that the parser has not seen in its train-
ing set - the unknown word is mapped to the special
UNKNOWN token.
In syntactic parsing, rather than map all low fre-
quency words to one generic UNKNOWN type, it
is useful to have several different clusters of un-
known words, grouped according to morphologi-
cal and other ?surfacey? clues in the original word.
For example, certain suffixes in English are strong
predictors for the part-of-speech tag of the word
(e.g. ?ly?) and so all low frequency words end-
ing in ?ly? are mapped to ?UNKNOWN-ly?. As
well as suffix information, UNKNOWN words are
commonly grouped based on information on capi-
talisation and hyphenation. Similar techniques for
handling unknown words have been used for POS
tagging (e.g. (Weischedel et al, 1993; Tseng et
al., 2005)) and are used in the Charniak (Char-
niak, 2000), Berkeley (Petrov et al, 2006) and Stan-
ford (Klein and Manning, 2003) parsers, as well as
in the parser used for the experiments in this paper,
an in-house implementation of the Berkeley parser.
3 Experiments
The BBN Entity Type Corpus (Weischedel and
Brunstein, 2005) consists of sentences from the
Penn WSJ corpus, manually annotated with named
entities. The Entity Type corpus includes annota-
15
type count examples
PERSON 11254 Kim Cattrall
PER DESC 21451 president,chief executive officer,
FAC 383 office, Rockefeller Center
FAC DESC 2193 chateau ,stadiums, golf course
ORGANIZATION 24239 Securities and Exchange Commission
ORG DESC 15765 auto maker, college
GPE 10323 Los Angeles,South Africa
GPE DESC 1479 center, nation, country
LOCATION 907 North America,Europe, Hudson River
NORP 3269 Far Eastern
PRODUCT 667 Maxima, 300ZX
PRODUCT DESC 1156 cars
EVENT 296 Vietnam war,HUGO ,World War II
WORK OF ART 561 Revitalized Classics Take..
LAW 300 Catastrophic Care Act,Bill of Rights
LANGUAGE 62 Latin
CONTACT INFO 30 555 W. 57th St.
PLANT 172 crops, tree
ANIMAL 355 hawks
SUBSTANCE 2205 gold,drugs, oil
DISEASE 254 schizophrenia,alcoholism
GAME 74 football senior tennis and golf tours
Table 1: Name expression entity types (sections 02-21)
tion for three classes of named entity: name expres-
sions, time expressions and numeric expressions (in
this paper we focus on name expressions). These
are further broken down into types. Table 1 displays
name expression entity types, their frequency in the
training set (sections 02-21), as well as some illus-
trative examples from the training set data.
We carried out experiments with different subsets
of entity types. In one set of experiments, all name
expression entities were mapped, with no restriction
on the types (ALL NAMED). We also carried
out experiments on a reduced set of named entities
- where only entities marked as PERSON, ORGA-
NIZATION, or GPE and LOCATION were mapped
(REDUCED). Finally, we ran experiments where
only one type of named entity was mapped at a time.
In all cases the words in the named entities were re-
placed by their entity type.
3.1 Many-to-one Mapping
In the many-to-one mapping all words in a named
entity were replaced with one named entity type
token. This approach is distinct from the words-
with-spaces approach previously pursued in parsing
where, for example, ?New York? would be replaced
with ?New York?. Instead, in our experiments ?New
York? is replaced with ?GPE? (geo-political entity).
In both approaches, the parser is forced to respect
unk map NE map #unks f-score POS
generic
none (baseline 1) 2966 (4.08%) 88.69 95.57
ALL NAMED 1908 (2.73%) 89.21 95.49
REDUCED 2122 (3.02%) 89.43 96.08
Person 2671 (3.68%) 88.98 95.55
Organisation 2521 (3.55%) 89.38 95.92
Location 2945 (4.05%) 89.00 95.62
sigs
none (baseline 2) 2966 (4.08%) 89.72 96.51
ALL NAMED 1908 (2.73%) 89.67 95.99
REDUCED 2122 (3.02%) 89.53 96.65
Person 2671 (3.68%) 89.32 96.47
Organisation 2521 (3.55%) 89.53 96.64
Location 2945 (4.05%) 89.20 96.52
Table 2: Many-to-One Parsing Results.
the multiword unit boundary (and analyses which
contain constituents that cross the MWU boundary
will not be considered by the parser). Intuitively,
this should help parser accuracy and speed. The ad-
vantage of mapping the word tokens to their entity
type rather than to a words-with-spaces token is that
in addition we will be reducing data sparsity.
One issue with the many-to-one mapping is that
in evaluation exact comparison with a baseline re-
sult is difficult because the tokenisation of test and
gold sets is different. When named entities span
more than one word, we are reducing the number
of words in the sentences. As parsers tend to do bet-
ter on short sentences than on long sentences, this
could make parsing somewhat easier. However, we
found that the average number of words in a sen-
tence before and after this mapping does not change
by much. The average number of words in the devel-
opment set is 23.9. When we map words to named
entity tokens (ALL NAMED), the average drops
by just one word to 22.9.2
3.2 One-to-one Mapping
In the one-to-one experiments we replaced each
word in named entity with a named entity type to-
ken (e.g. Ada Lovelace ? pperson pperson).3 The
motivation was to measure the effect of reducing
word sparsity using named entities without altering
the original tokenisation of the data.4
2A related issue is that the resulting parse tree will lack an
analysis for the named entity.
3The entity type was given an extra letter where needed (e.g.
?pperson?) to avoid the conflation of a mapped entity token with
an original word (e.g. ?person?) in the corpus.
4Note, where there is punctuation as part of a named entity
we do not map the punctuation.
16
unk map NE map #unks f-score POS
generic
none (baseline 1) 2966 (4.08%) 88.69 95.57
ALL NAMED 1923 (2.64%) 89.28 94.99
REDUCED 2122 (2.90%) 88.76 95.76
Person 2654(3.65%) 88.95 95.57
Organisation 2521 (3.45%) 88.80 95.59
Location 2945 (4.04%) 88.88 95.66
sigs
none (baseline 2) 2966 (4.08%) 89.72 96.51
ALL NAMED 1923 (2.64%) 89.36 95.64
REDUCED 2122 (2.90%) 89.01 96.32
Person 2654(3.65%) 89.30 96.52
Organisation 2521 (3.45%) 89.29 96.30
Location 2945 (4.04%) 89.55 96.54
Table 3: One-to-One Parsing Results
In an initial experiment, where the mapping was
simply the word to the named entity type, many sen-
tences received no parse. This happened often when
a named entity consisted of three or more words and
resulted in a sentence such as ?But while the Oor-
ganization Oorganization Oorganization Oorganiza-
tion did n?t fall apart Friday?. We found that refining
the named entity by adding the number of the word
in the entity to the mapping resolved the coverage
problem. The example sentence is now: ?But while
the Oorganization1 Oorganization2 Oorganization3
Oorganization4 did n?t fall apart Friday?. See ?5 for
a possible explanation for the parser?s difficulty with
one-to-one mappings to coarse grained entity types.
4 Results
Table 2 and Table 3 give the results for the many-to-
one and one-to-one experiments respectively. Re-
sults are given against a baseline where unknowns
are given a ?generic? treatment (baseline 1) - i.e.
they are not clustered according to morphological
and surface information - and for the second baseline
(baseline 2), where morphological or surface feature
markers (sigs) are affixed to the unknowns.5
The results indicate that though lexical spar-
sity is decreasing, insofar as the number of un-
known words (#unks column) in the development
set decreases with all named entity mappings, the
named entity clusters are not informative enough
and parser accuracy falls short of the previous best
result. For all experiments, a pattern that emerges
5For all experiments, a split-merge cycle of 5 was used. Fol-
lowing convention, sections 02-21 were used for training. Sec-
tions 22 and 24 (sentences less than or equal to 100 words) were
used for the development set. As experiments are ongoing we
do not report results on a test set.
is that mapping words to named entities improves
results when low frequency words are mapped to
a generic UNKNOWN token. However, when low
frequency words are mapped to more fine-grained
UNKNOWN tokens, mapping words to named enti-
ties decreases accuracy marginally.
If a particular named entity occurs often in the text
then data sparsity is possibly not a problem for this
word. Rather than map all occurrences of a named
entity to its entity type, we experimented with map-
ping only low frequency entities. These named en-
tity mapping experiments now mirror more closely
the unknown words mappings - low frequency en-
tities are mapped to special entity types, then the
parser maps all remaining low frequency words to
UNKNOWN types. Table 4 shows the effect of map-
ping only entities that occur less than 10 times in the
training set, to the person type and the reduced set
of entity types. Results somewhat improve for all
but one of the one-to-one experiments, but nonethe-
less remain below the best baseline result. There is
still no advantage in mapping low frequency person
name words to, say, the person cluster, rather than to
an UNKNOWN-plus-signature cluster.
5 Discussion
Our results thus far suggest that clusters based on
morphology or surface clues are more informative
than the named entity clusters.
For the one-to-one mappings one obvious prob-
lem that emerged is that all words in entities (in-
cluding function words for example) get mapped to
a generic named entity token. A multi-word named
entity has its own internal syntactic structure, re-
flected for example in its sequence of part-of-speech
tags. By replacing each word in the entity with
the generic entity token we end up loosing informa-
tion about words, conflating words that take differ-
ent part-of-speech categories, and in fact make pars-
ing more difficult. The named entity clusters in this
case are too coarse-grained and words with different
syntactic properties are merged into the one cluster,
something we would like to avoid.
In future work, as well as avoiding mapping more
complex named entities, we will refine the named
entity clusters by attaching to the entity type signa-
tures similar to those attached to the UNKNOWN
17
unk map NE map one2one f-score many2one f-score
generic
Person 88.95 88.98
Person < 10 88.97 89.05
Reduced 88.76 89.43
Reduced < 10 89.51 88.85
sigs
Person 89.30 89.32
Person < 10 89.49 89.33
Reduced 89.01 89.53
Reduced < 10 89.42 89.15
Table 4: Measuring the effect of mapping only low fre-
quency named entities.
types. It would also be interesting to examine the ef-
fect of mapping other types of named entities, such
as dates and numeric expressions. Finally, we intend
trying similar experiments on out-of-domain data,
such as social media text where unknown words are
more problematic.
6 Conclusion
We have presented preliminary experiments which
test the novel technique of mapping word tokens to
named entity clusters, with the aim of improving
parser accuracy by reducing data sparsity. While our
results so far are disappointing, we have identified
possible problems and outlined future experiments,
including suggestions for refining the named entity
clusters so that they become more syntactically ho-
mogenous.
References
Conor Cafferkey, Deirdre Hogan, and Josef van Gen-
abith. 2007. Multi-word units in treebank-based prob-
abilistic parsing and generation. In Proceedings of the
10th International Conference on Recent Advances in
Natural Language Processing (RANLP-07), Borovets,
Bulgaria.
Marie Candito and Benoit Crabbe?. 2009. Improving gen-
erative statistical parsing with semi-supervised word
clustering. In Proceedings of the International Work-
shop on Parsing Technologies (IWPT-09).
Marie Candito and Djame? Seddah. 2010. Lemmatization
and statistical lexicalized parsing of morphologically-
rich languages. In Proceedings of the NAACL/HLT
Workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL).
Eugene Charniak. 2000. A maximum entropy-inspired
parser. In Proceedings of the 1st North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-2009).
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association of Computational
Linguistics (ACL).
Ioannis Korkontzelos and Suresh Manandhar. 2010. Can
recognising multiword expressions improve shallow
parsing? In Proceedings of the Conference of the
North American Chapter of the ACL (NAACL-10), Los
Angeles, California.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. In
Proceedings of the 43rd Annual Meeting of the ACL,
pages 75?82, Ann Arbor, June.
Joakim Nivre and Jens Nilsson. 2004. Multiword units
in syntactic parsing. In Workshop on Methodologies
and Evaluation of Multiword Units in Real-World Ap-
plications.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact and inter-
pretable tree annotation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the ACL, Sydney,
Australia, July.
Rajakrishnan Rajkumar, Michael White, and Dominic
Espinosa. 2009. Exploiting named entity classes in
ccg surface realisation. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-09).
Huihsin Tseng, Daniel Jurafsky, and Christopher Man-
ning. 2005. Morpholgical features help pos tagging
of unknown words across language varieties. In Pro-
ceedings of the Fourth SIGHAN Workshop on Chinese
Language Processing.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In Proceedings of
the Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
Eric Wehrli, Violeta Seretan, and Luke Nerima. 2010.
Sentence analysis and collocation identification. In
Proceedings of the Workshop on Multiword Expres-
sion: From Theory to Applications (MWE).
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus. In Tehcnical
Report.
18
Ralph Weischedel, Richard Schwartz, Jeff Palmucci,
Marie Meteer, and Lance Ramshaw. 1993. Coping
with ambiguity and unknown words through proba-
bilistic models. Computational Linguistics, 19(2).
Yi Zhang, Valia Kordoni, Aline Villavicencio, and Marco
Idiart. 2006. Automated multiword expression pre-
diction for grammar engineering. In Proceedings of
the Workshop on Multiword Expressions: Identifying
and Exploiting Underlying Properties.
19
