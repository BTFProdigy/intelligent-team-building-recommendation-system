Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1250?1259,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Semi-supervised Speech Act Recognition in Emails and Forums
Minwoo Jeong
??
Chin-Yew Lin
?
Gary Geunbae Lee
?
?
Pohang University of Science & Technology, Pohang, Korea
?
Microsoft Research Asia, Beijing, China
?
{stardust,gblee}@postech.ac.kr
?
cyl@microsoft.com
Abstract
In this paper, we present a semi-supervised
method for automatic speech act recogni-
tion in email and forums. The major chal-
lenge of this task is due to lack of labeled
data in these two genres. Our method
leverages labeled data in the Switchboard-
DAMSL and the Meeting Recorder Dia-
log Act database and applies simple do-
main adaptation techniques over a large
amount of unlabeled email and forum data
to address this problem. Our method uses
automatically extracted features such as
phrases and dependency trees, called sub-
tree features, for semi-supervised learn-
ing. Empirical results demonstrate that
our model is effective in email and forum
speech act recognition.
1 Introduction
Email and online forums are important social me-
dia. For example, thousands of emails and posts
are created daily in online communities, e.g.,
Usenet newsgroups or the TripAdvisor travel fo-
rum
1
, in which users interact with each other us-
ing emails/posts in complicated ways in discus-
sion threads. To uncover the rich interactions in
these email exchanges and forum discussions, we
propose to apply speech act recognition to email
and forum threads.
Despite extensive studies of speech act recogni-
tion in many areas, developing speech act recogni-
tion for online forms of conversation is very chal-
lenging. A major challenge is that emails and
forums usually have no labeled data for training
statistical speech act recognizers. Fortunately, la-
beled speech act data are available in other do-
mains (i.e., telephone and meeting conversations
?
This work was conducted during the author?s internship
at Microsoft Research Asia.
1
http://tripadvisor.com/
in this paper) and large unlabeled data sets can be
collected from the Web. Thus, we focus on the
problem of how to accurately recognize speech
acts in emails and forums by making maximum
use of data from existing resources.
Recently, there are increasing interests in
speech act recognition of online text-based con-
versations. Analysis of speech acts for online
chat and instant messages and have been studied
in computer-mediated communication (CMC) and
distance learning (Twitchell et al, 2004; Nastri et
al., 2006; Ros?e et al, 2008). In natural language
processing, Cohen et al (2004) and Feng et al
(2006) used speech acts to capture the intentional
focus of emails and discussion boards. However,
they assume that enough labeled data are available
for developing speech act recognition models.
A main contribution of this paper is that we ad-
dress the problem of learning speech act recog-
nition in a semi-supervised way. To our knowl-
edge, this is the first use of semi-supervised speech
act recognition in emails and online forums. To
do this, we make use of labeled data from spo-
ken conversations (Jurafsky et al, 1997; Dhillon
et al, 2004). A second contribution is that our
model learns subtree features that constitute dis-
criminative patterns: for example, variable length
n-grams and partial dependency structures. There-
fore, our model can capture both local features
such as n-grams and non-local dependencies. In
this paper, we extend subtree pattern mining to the
semi-supervised learning problem.
This paper is structured as follows. Section 2
reviews prior work on speech act recognition and
Section 3 presents the problem statement and our
data sets. Section 4 describes a supervised method
of learning subtree features that shows the effec-
tiveness of subtree features on labeled data sets.
Section 5 proposes semi-supervised learning tech-
niques for speech act recognition and Section 6
demonstrates our method applied to email and on-
1250
line forum thread data. Section 7 concludes this
paper with future work.
2 Related Work
Speech act theory is fundamental to many stud-
ies in discourse analysis and pragmatics (Austin,
1962; Searle, 1969). A speech act is an illo-
cutionary act of conversation and reflects shal-
low discourse structures of language. Recent re-
search on spoken dialog processing has investi-
gated computational speech act models of human-
human and human-computer conversations (Stol-
cke et al, 2000) and applications of these mod-
els to CMC and distance learning (Twitchell et al,
2004; Nastri et al, 2006; Ros?e et al, 2008).
Our work in this paper is closely related to prior
work on email and forum speech act recognition.
Cohen et al (2004) proposed the notion of ?email
speech act? for classifying the intent of an email
sender. They defined verb and noun categories
for email speech acts and used supervised learn-
ing to recognize them. Feng et al (2006) pre-
sented a method of detecting conversation focus
based on the speech acts of messages in discus-
sion boards. Extending Feng et al (2006)?s work,
Ravi and Kim (2007) applied speech act classifi-
cation to detect unanswered questions. However,
none of these studies have focused on the semi-
supervised speech act recognition problem and ex-
amined their methods across different genres.
The speech processing community frequently
employs two large-scale corpora for speech act
annotation: Switchboard-DAMSL (SWBD) and
Meeting Recorder Dialog Act (MRDA). SWBD is
an annotation scheme and collection of labeled di-
alog act
2
data for telephone conversations (Juraf-
sky et al, 1997). The main purpose of SWBD is
to acquire stochastic discourse grammars for train-
ing better language models for automatic speech
recognition. More recently, an MRDA corpus has
been adapted from SWBD but its tag set for la-
beling meetings has been modified to better reflect
the types of interaction in multi-party face-to-face
meetings (Dhillon et al, 2004). These two corpora
have been extensively studied, e.g., (Stolcke et al,
2000; Ang et al, 2005; Galley et al, 2004). We
also use these for our experiments.
2
A dialog act is the meaning of an utterance at the level
of illocutionary force (Austin, 1962), and broadly covers the
speech act and adjacency pair (Stolcke et al, 2000). In this
paper, we use only the term ?speech act? for clarity.
This paper focuses on the problem of semi-
supervised speech act recognition. The goal of
semi-supervised learning techniques is to use aux-
iliary data to improve a model?s capability to rec-
ognize speech acts. The approach in Tur et al
(2005) presented semi-supervised learning to em-
ploy auxiliary unlabeled data in call classification,
and is closely related to our work. However, our
approach uses the most discriminative subtree fea-
tures, which is particularly attractive for reducing
the model?s size. Our problem setting is closely re-
lated to the domain adaptation problem (Ando and
Zhang, 2005), i.e., we seek to obtain a model that
analyzes target domains (emails and forums) by
adapting a method that analyzes source domains
(SWBD and MRDA). Recently, this type of do-
main adaptation has become an important topic in
natural language processing.
3 Problem Definition
3.1 Problem Statement
We define speech act recognition to be the task
that, given a sentence, maps it to one of the speech
act types. Figure 1 shows two examples of our
email and forum speech act recognition. E1?6 are
all sentences in an email message. F1?3, F4?5,
and F6 are three posts in a forum thread. A sen-
tence interacts alone or with others, for example,
F6 agrees with the previous post (F4?5). To gain
insight into our work, it is useful to consider that
E2, 3 and F1, 4, 6 are summaries of two dis-
courses. In particular, F1 denotes a question and
F4 and F6 are corresponding answers. More re-
cently, using speech acts has become an appealing
approach in summarizing the discussions (Galley
et al, 2004; McKeown et al, 2007).
Next, we define speech act category based on
MRDA. Dhillon et al (2004) included definitions
of speech acts for colloquial style interactions
(e.g., backchannel, disruption, and floorgrabber),
but these are not applicable in emails and forums.
After removing these categories, we define 12 tags
(Table 1). Dhillon et al (2004) provides detailed
descriptions of each tag. We note that our tag set
definition is different from (Cohen et al, 2004;
Feng et al, 2006; Ravi and Kim, 2007) for two
reasons. First, prior work primarily interested in
the domain-specific speech acts, but our work use
domain-independent speech act tags. Second, we
focus on speech act recognition on the sentence-
level.
1251
E1: I am planning my schedule at CHI 2003 (http://www.chi2003.org/) S
E2: - will there be anything happening at the conference related to this W3C User interest group? QY
E3: I do not see anything on the program yet, but I suspect we could at least have an informal SIG S
E4: - a chance to meet others and bring someone like me up to speed on what is happening. S
E5: There will be many competing activities, so the sooner we can set this up the more likely I can attend. S
E6: Keith S
F1: If given a choice, should I choose Huangpu area, or should I choose Pudong area? QR
F2: Both location are separated by a Huangpu river, not sure which area is more convenient for sight seeing? QW
F3: Thanks in advance for reply! P
F4: Stay on the Puxi side of the Huangpu river and visit the Pudong side by the incredible tourist tunnel. AC
F5: If you stay on the Pudong side add half an hour to visit the majority of the tourist attractions. S
F6: I definitely agree with previous post. AA
Figure 1: Examples of speech act recognition in emails and online forums. Tags are defined in Table 1.
Table 1: Tags used to describe components of
speech acts
Tag Description
A Accept response
AA Acknowledge and appreciate
AC Action motivator
P Polite mechanism
QH Rhetorical question
QO Open-ended question
QR Or/or-clause question
QW Wh-question
QY Yes-no question
R Reject response
S Statement
U Uncertain response
The goal of semi-supervised speech act recogni-
tion is to learn a classifier using both labeled and
unlabeled data. We formally define our problem
as follows. Let x = {x
j
} be a forest, i.e., a set of
trees that represents a natural language structure,
for example, a sequence of words and a depen-
dency parse tree. We will describe this in more
detail in Section 4. Let y be a speech act. Then,
we define D
L
= {x
i
, y
i
}
n
i=1
as the set of labeled
training data, and D
U
= {x
i
}
l
i=n+1
as the set of
unlabeled training data where l = n+m and m is
the number of unlabeled data instances. Our goal
is to find a learning method to minimize the clas-
sification errors in D
L
and D
U
.
3.2 Data Preparation
In this paper, we separate labeled (D
L
) and un-
labeled data (D
U
). First we use SWBD
3
and
MRDA
4
as our labeled data. We automatically
3
LDC Catalog No. LDC97S62
4
http://www.icsi.berkeley.edu/?ees/dadb/
map original annotations in SWBD and MRDA to
one of the 12 speech acts.
5
Inter-annotator agree-
ment ? in both data sets is ? 0.8 (Jurafsky et al,
1997; Dhillon et al, 2004). For evaluation pur-
poses, we divide labeled data into three sets: train-
ing, development, and evaluation sets (Table 2).
Of the 1,155 available conversations in the SWBD
corpus, we use 855 for training, 100 for devel-
opment, and 200 for evaluation. Among the 75
available meetings in the MRDA corpus, we ex-
clude two meetings of different natures (btr001
and btr002). Of the remaining meetings, we use
59 for training, 6 for development, and 8 for eval-
uation. Then we merge multi-segments utterances
that belong to the same speaker and then divide all
data sets into sentences.
As stated earlier, our unlabeled data consists
of email (EMAIL) and online forum (FORUM)
data. For the EMAIL set, we selected 22,391
emails from Enron data
6
(discussion threads,
all documents, and calendar folders). For the FO-
RUM set, we crawled 11,602 threads and 55,743
posts from the TripAdvisor travel forum site (Bei-
jing, Shanghai, and Hongkong forums). As our
evaluation sets, we used 40 email threads of the
BC3 corpus
7
for EMAIL and 100 threads selected
from the same travel forum site for FORUM. Ev-
ery sentences was automatically segmented by the
MSRA sentence boundary detector (Table 2). An-
notation was performed by two human annotators,
and inter-annotator agreements were ? = 0.79 for
EMAIL and ? = 0.73 for FORUM.
Overall performance of automatic evaluation
measures usually depends on the distribution of
tags. In both labeled and unlabeled sets, the most
5
Our mapping tables are available at
http://home.postech.ac.kr/?stardust/acl09/.
6
http://www.cs.cmu.edu/?enron/
7
http://www.cs.ubc.ca/nest/lci/bc3.html
1252
Table 2: Number of sentences in labeled and unlabeled data
Set SWBD MRDA
Training 96,553 50,865
Development 12,299 8,366
Evaluation 24,264 10,492
Set EMAIL FORUM
Unlabeled 122,125 297,017
Evaluation 2,267 3,711
Figure 2: Distribution of speech acts in the evaluation sets. Tags are defined in Table 1.
frequent tag is the statement (S) tag (Figure 2).
Distributions of tags are similar in training and de-
velopment sets of SWBD and MRDA.
4 Speech Act Recognition
Previous work in speech act recognition used a
large set of lexical features, e.g., bag-of-words,
bigrams and trigrams (Stolcke et al, 2000; Co-
hen et al, 2004; Ang et al, 2005; Ravi and Kim,
2007). However, these methods create a large
number of lexical features that might not be nec-
essary for speech act identification. For example,
a Wh-question ?What site should we use to book a
Beijing-Chonqing flight?? can be predicted by two
discriminative features, ?(<s>, WRB) ? QW?
and ?(?, </s>) ? QW? where <s> and </s>
are sentence start and end symbols, and WRB is
a part-of-speech tag that denotes a Wh-adverb.
In addition, useful features could be of various
lengths, i.e. not fixed length n-grams, and non-
adjacent. One key idea of this paper is a novel use
of subtree features to model these for speech act
recognition.
4.1 Exploiting Subtree Features
To exploit subtree features in our model, we use
a subtree pattern mining method proposed by
Kudo and Matsumoto (2004). We briefly intro-
duce this algorithm here. In Section 3.1, we de-
fined x = {x
j
} as the forest that is a set of trees.
More precisely, x
j
is a labeled ordered tree where
each node has its own label and is ordered left-
to-right. Several types of labeled ordered trees
Figure 3: Representations of tree: (a) bag-of-
words, (b) n-gram, (c) word pair, and (d) depen-
dency tree. A node denotes a word and a directed
edge indicates a parent-and-child relationship.
are possible (Figure 3). Note that S-expression
can be used instead for computation, for example
(a(b(c(d)))) for the n-gram (Figure 3(b)).
Moreover, we employ a combination of multiple
trees as the input of the subtree pattern mining al-
gorithm.
We extract subtree features from the forest set
{x
i
}. A subtree t is a tree if t ? x. For exam-
ple, (a), (a(b)), and (b(c(d))) are subtrees
of Figure 3(b). We define the subtree feature as a
weak learner:
f(y, t,x) ,
{
+y t ? x,
?y otherwise,
(1)
where we assume a binary case y ? Y =
{+1,?1} for simplicity. Even though the ap-
proach in Kudo and Matsumoto (2004) and ours
are simiar, there are two clear distinctions. First,
our method employs multiple tree structures, and
uses different constraints to generate subtree can-
didates. In this paper, we only restrict generating
1253
the dependency subtrees which should have 3 or
more nodes. Second, our method is of interest
for semi-supervised learning problems. To learn
subtree features, Kudo and Matsumoto (2004) as-
sumed supervised data {(x
i
, y
i
)}. Here, we de-
scribe the supervised learning method and will de-
scribe our semi-supervised method in Section 5.
4.2 Supervised Boosting Learning
Given training examples, we construct a ensem-
ble learner F (x) =
?
k
?
k
f(y
k
, t
k
,x), where ?
k
is a coefficient for linear combination. A final
classifier h(x) can be derived from the ensemble
learner, i.e., h(x) , sgn (F (x)). As an optimiza-
tion framework (Mason et al, 2000), the objective
of boosting learning is to find F such that the cost
of functional
C(F ) =
?
i?D
?
i
C[y
i
F (x
i
)] (2)
is minimized for some non-negative and monoton-
ically decreasing cost function C : R ? R and
the weight ?
i
? R
+
. In this paper, we use the
AdaBoost algorithm (Schapire and Singer, 1999);
thus the cost function is defined as C(z) = e
?z
.
Constructing an ensemble learner requires that
the user choose a base learner, f(y, t,x), to
maximize the inner product ???C(F ), f? (Ma-
son et al, 2000). Finding f(y, t,x) to maxi-
mize ???C(F ), f? is equivalent to searching for
f(y, t,x) to minimize 2
?
i:f(y,t,x
i
)6=y
i
w
i
? 1,
where w
i
for i ? D
L
, is the empirical data dis-
tribution w
(k)
i
at step k. It is defined as:
w
(k)
i
= ?
i
? e
?y
i
F (x
i
)
. (3)
From Eq. 3, a proper base learner (i.e., subtree)
can be found by maximizing weighted gain, where
gain(t, y) =
?
i?D
L
y
i
w
i
f(y, t,x
i
). (4)
Thus, subtree mining is formulated as the prob-
lem of finding (
?
t, y?) = argmax
(t,y)?X?Y
gain(t, y). We
need to search with respect to a non-monotonic
score function (Eq. 4), thus we use the monotonic
bound, gain(t, y) ? ?(t), where
?(t) =max
?
?
2
?
w
i
{i|y
i
=+1,t?x
i
}
?
n
?
i=1
y
i
f(y, t,x
i
),
2
?
w
i
{i|y
i
=?1,t?x
i
}
+
n
?
i=1
y
i
f(y, t,x
i
)
?
?
. (5)
Table 3: Result of supervised learning experiment;
columns are micro-averaged F
1
score with macro-
averaged F
1
score in parentheses. MAXENT:
maximum entropy model; BOW: bag-of-words
model; NGRAM: n-gram model; +POSTAG,
+DEPTREE, +SPEAKER indicate that the com-
ponents were added individually onto NGRAM.
?
?
? indicates results significantly better than the
NGRAM model (p < 0.001).
Model SWBD MRDA
MAXENT 92.76 (63.54) 82.48 (57.19)
BOW 91.32 (54.47) 82.17 (55.42)
NGRAM 92.60 (58.43) 83.30 (57.53)
+POSTAG 92.69 (60.07) 83.60 (58.46)
+DEPTREE 92.67 (61.75)
?
83.57 (57.45)
+SPEAKER
?
92.86 (63.13) 83.40 (58.20)
ALL
?
92.87 (63.77) 83.49 (59.04)
The subtree set is efficiently enumerated using a
branch-and-bound procedure based on ?(t) (Kudo
and Matsumoto, 2004).
After finding an optimal base leaner, f(y?,
?
t,x),
we need to set the coefficient ?
k
to form a new en-
semble, F (x
i
) ? F (x
i
) + ?
k
f(
?
t, y?,x
i
). In Ad-
aBoost, we choose
?
k
=
1
2
log
(
1 + gain(
?
t, y?)
1? gain(
?
t, y?)
)
. (6)
After K iterations, the boosting algorithm returns
the ensemble learner F (x) which consists of a set
of appropriate base learners f(y, t,x).
4.3 Evaluation on Labeled Data
We verified the effectiveness of using subtree fea-
tures on the SWBD and MRDA data sets. For
boosting learning, one typically assumes ?
i
= 1.
In addition, the number of iterations, which relates
to the number of patterns, was determined by a
development set. We also used a one-vs.-all strat-
egy for the multi-class problem. Precision and re-
call were computed and combined into micro- and
macro-averaged F
1
scores. The significance of our
results was evaluated using the McNemar paired
test (Gillick and Cox, 1989), which is based on in-
dividual labeling decisions to compare the correct-
ness of two models. All experiments were imple-
mented in C++ and executed in Windows XP on a
PC with a Dual 2.1 GHz Intel Core2 processor and
2.0 Gbyte of main memory.
1254
Figure 4: Comparison of different trees (SWBD)
We show that use of subtree features is ef-
fective to solve the supervised speech act recog-
nition problem. We also compared our model
with the state-of-the-art maximum entropy classi-
fier (MAXENT). We used bag-of-words, bigram
and trigram features for MAXENT, which mod-
eled 702k (SWBD) and 460k (MRDA) parameters
(i.e., patterns), and produced micro-averaged F
1
scores of 92.76 (macro-averaged F
1
= 63.54) for
SWBD and 82.48 (macro-averaged F
1
= 57.19)
for MRDA. In contrast, our method generated ap-
proximately 4k to 5k patterns on average with sim-
ilar or greater F
1
scores (Table 3); hence, com-
pared to MAXENT, our model requires fewer cal-
culations and is just as accurate.
The n-gram model (NGRAM) performed signif-
icantly better than the bag-of-words model (Mc-
Nemar test; p < 0.001) (Table 3). Unlike MAX-
ENT, NGRAM automatically selects a relevant set
of variable length n-gram features (i.e., phrase
features). To this set, we separately added two
syntax type features, part-of-speech tag n-gram
(POSTAG) and dependency parse tree (DEPTREE)
automatically parsed by Minipar
8
, and one dis-
course type feature, speaker n-gram (SPEAKER).
Although some micro-averaged F
1
are not statisti-
cally significant between the original NGRAM and
the models that include POSTAG, DEPTREE or
SPEAKER, macro-averaged F
1
values indicate that
minor classes can take advantage of other struc-
tures. For example, in the result of SWBD (Fig-
ure 4), DEPTREE and SPEAKER models help to
predict uncertain responses (U), whereas NGRAM
and POSTAG cannot do this.
5 Semi-supervised Learning
Our goal is to eventually make maximum use
of existing resources in SWBD and MRDA for
8
http://www.cs.ualberta.ca/?lindek/minipar.htm
email/forum speech act recognition. We call the
model trained on the mixed data of these two cor-
pora BASELINE. We use ALL features in con-
structing the BASELINE for the semi-supervised
experiments. While this model gave promising re-
sults using SWBD and MRDA, language used in
emails and forums differs from that used in spo-
ken conversation. For example, ?thanx? is an ex-
pression commonly used as a polite mechanism
in online communications. To adapt our model to
understand this type of difference between spoken
and online text-based conversations, we should in-
duce new patterns from unlabeled email and fo-
rum data. We describe here two methods of semi-
supervised learning.
5.1 Method 1: Bootstrapping
First, we bootstrap the BASELINE model using au-
tomatically predicted unlabeled examples. How-
ever, using all of the unlabeled data results in noisy
models; therefore filtering or selecting data is very
important in practice. To this end, we only select
similar examples by criterion, d(x
i
,x
j
) < r or k
nearest neighbors where x
i
? D
L
and x
j
? D
U
.
In practice, r or k are fixed. In our method, exam-
ples are represented by trees; hence we use a ?tree
edit distance? for calculating d(x
i
,x
j
) (Shasha
and Zhang, 1990). Selected examples are evalu-
ated using BASELINE, and using subtree pattern
mining runs on the augmented data (i.e. unla-
beled). We call this method BOOTSTRAP.
5.2 Method 2: Semi-supervised Boosting
Our second method is based on a principle of
semi-supervised boosting learning (Bennett et al,
2002). Because we have no supervised guidance
for D
U
, our objective functional to find F is de-
fined as:
C(F ) =
?
i?D
L
?
i
C[y
i
F (x
i
)] +
?
i?D
U
?
i
C[|F (x
i
)|]
(7)
This cost functional is non-differentiable. To
solve it, we introduce pseudo-labels y? where y? =
sgn(F (x)) and |F (x)| = y?F (x). Using the same
derivation in Section 4.2, we obtain the following
1255
gain function and update rules:
gain(t, y) =
?
i?D
L
y
i
w
i
f(y, t,x
i
)
+
?
i?D
U
y?
i
w
i
f(y, t,x
i
), (8)
w
i
=
{
?
i
? e
?y
i
F (x
i
)
i ? D
L
,
?
i
? e
?y?
i
F (x
i
)
i ? D
U
.
(9)
Intuitively, an unlabeled example that has a
high-confidence |F (x)| at the current step, will
probably receive more weight at the next step.
That is, similar instances become more impor-
tant when learning and mining subtrees. This
semi-supervised boosting learning iteratively gen-
erates pseudo-labels for unlabeled data and finds
the value of F that minimizes training errors (Ben-
nett et al, 2002). Also, the algorithm infers new
features from unlabeled data, and these features
are iteratively re-evaluated by the current ensem-
ble learner. We call this method SEMIBOOST.
6 Experiment
6.1 Setting
We describe specific settings used in our exper-
iment. Because we have no development set,
we set the maximum number of iterations K at
10,000. At most K patterns can be extracted, but
this seldom happens because duplicated patterns
are merged. Typical settings for semi-supervised
boosting are ?
i
= 1 and ?
i
= 0.5, that is, we
penalize the weights for unlabeled data.
For efficiency, BASELINE model used 10% of
the SWBD and MRDA data, selected at random.
We observed that this data set does not degrade the
results of semi-supervised speech act recognition.
For BOOTSTRAP and SEMIBOOST, we selected
k = 100 nearest neighbors of unlabeled exam-
ples for each labeled example using tree edit dis-
tance, and then used 24,625 (SWBD) and 54,961
(MRDA) sentences for the semi-supervised set-
ting.
All trees were combined as described in Section
4.3 (ALL model). In EMAIL and FORUM data we
added different types of discourse features: mes-
sage type (e.g., initial or reply posts), authorship
(e.g., an identification of 2nd or 3rd posts written
by the same author), and relative position of a sen-
tence. In Figure 1, for example, F1?3 is an initial
post, and F4?5 and F6 are reply posts. Moreover,
F1, F4, and F6 are the first sentence in each post.
Table 4: Results of speech act recognition on on-
line conversations; columns are micro-averaged
F
1
score with macro-averaged scores in parenthe-
ses. ?
?
? indicates that the result is significantly bet-
ter than BASELINE (p < 0.001).
Model EMAIL FORUM
BASELINE 78.87 (37.44) 78.93 (35.57)
BOOTSTRAP
?
83.11 (44.90) 79.09 (44.38)
SEMIBOOST
?
82.80 (44.64)
?
81.76 (44.21)
SUPERVISED 90.95 (75.71) 83.67 (40.68)
These features do not occur in SWBD or MRDA
because these are utterance-by-utterance conver-
sations.
6.2 Result and Discussion
First, we show that our method of semi-supervised
learning can improve modeling of the speech
act of emails and forums. As our baseline,
BASELINE achieved a micro-averaged F
1
score
of ? 79 for both data sets. This implies that
SWBD and MRDA data are useful for our prob-
lem. Using unlabeled data, semi-supervised meth-
ods BOOTSTRAP and SEMIBOOST perform bet-
ter than BASELINE (Table 4; Figure 5). To verify
our claim, we evaluated the supervised speech act
recognition on EMAIL and FORUM evaluation
sets with 5-fold cross validation (SUPERVISED in
Table 4). In particular, our semi-supervised speech
act recognition is competitive with the supervised
model in FORUM data.
The difference in performance between super-
vised results in EMAIL and FORUM seems to
indicate that the latter is a more difficult data
set. However, our SEMIBOOST method were able
to come close to the supervised FORUM results
(81.76 vs. 83.67). This is also close to the range of
supervised MRDA data set (F
1
= 83.49 for ALL,
Table 3). Moreover, we analyzed a main reason of
why transfer results were competitive in the FO-
RUM but not in the EMAIL. This might be due
to the mismatch in the unlabeled data, that is, we
used different email collections, the BC3 corpus
(email communication of W3C on w3.org sites),
for evaluation while used Enron data for adaption.
We also conjecture that the discrepancy between
EMAIL and FORUM is probably due to the more
heterogeneous nature of the FORUM data where
anyone can post and reply while EMAIL (Enron or
1256
(a) EMAIL (b) FORUM
Figure 5: Result of the semi-supervised learning method
BC3) might have a more fix set of participants.
The improvement of less frequent tags is promi-
nent, for example 25% for action motivator (AC),
40% for polite mechanism (P), and 15% for rhetor-
ical question (QR) error rate reductions were
achieved in FORUM data (Figure 5(b)). There-
fore, the semi-supervised learning method is more
effective with small amounts of labeled data (i.e.,
less frequent annotations). We believe that despite
their relative rarity, these speech acts are more im-
portant than the statement (S) in some applica-
tions, e.g., summarization.
Next, we give a qualitative analysis for better
interpretation of our problem and results. Due to
limited space, we focus on FORUM data, which
can potentially be applied to many applications.
Of the top ranked patterns extracted by SEMI-
BOOST (Figure 6(a)), subtree patterns of n-gram,
part-of-speech, dependency parse trees are most
discriminative. The patterns from unlabeled data
have relatively lower ranks, but this is not surpris-
ing. This indicates that BASELINE model provides
the base knowledge for semi-supervised speech
act recognition. Also, unlabeled data for EMAIL
and FORUM help to induce new patterns or ad-
just the model?s parameters. As a result, the semi-
supervised method is better than the BASELINE
when an identical number of patterns is modeled
(Figure 6(b)). For this result, we conclude that our
method successfully transfers knowledge from a
source domain (i.e., SWBD and MRDA) to a tar-
get domain (i.e., EMAIL and FORUM); hence it
can be a solution to the domain adaption problem.
Finally, we determine the main reasons for error
(in SEMIBOOST), to gain insights that may allow
development of better models in future work (Fig-
ure 6(c)). We sorted speech act tags by their se-
mantics and partitioned the confusion matrix into
question type (Q*) and statement, which are two
high-level speech acts. Most errors occur in the
similar categories, that is, language usage in ques-
tion discourse is definitely distinct from that in
statement discourse. From this analysis, we be-
lieve that more advanced techniques (e.g. two-
stage classification and learning with hierarchy-
augmented loss) can improve our model.
7 Conclusion
Despite the increasing interest in online text-based
conversations, no study to date has investigated
semi-supervised speech act recognition in email
and forum threads. This paper has addressed the
problem of learning to recognize speech acts us-
ing labeled and unlabeled data. We have also con-
tributed to the development of a novel applica-
tion of boosting subtree mining. Empirical results
have demonstrated that semi-supervised learning
of speech act recognition with subtree features im-
proves the performance in email and forum data
sets. An attractive future direction is to exploit
prior knowledge for semi-supervised speech act
recognition. Druck et al (2008) described gen-
eralized expectation criteria in which a discrimi-
native model can employ the labeled features and
unlabeled instances. Using prior knowledge, we
expect that our model will effectively learn useful
patterns from unlabeled data.
As work progresses on analyzing online text-
based conversations such as emails, forums, and
online chats, the importance of developing models
for discourse without annotating much new data
will become more important. In the future, we
plan to explore other related problems such as ad-
jacency pairs (Levinson, 1983) and discourse pars-
ing (Soricut and Marcu, 2003) for large-scale on-
line forum data.
1257
(a) Example patterns
0 2000 4000 6000
20
25
30
35
40
45
Number of base leaners
Erro
r Ra
te(%)
BASELINEBOOTSTRAPSEMIBOOST
(b) Learning behavior (c) Confusion matrix
Figure 6: Analysis on FORUM data
Acknowledgement
We would like to thank to anonymous reviewers
for their valuable comments, and Yunbo Cao, Wei
Lai, Xinying Song, Jingtian Jing, and Wei Wu for
their help in preparing our data.
References
R. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and
unlabeled data. Journal of Machine Learning Re-
search, 6:1817?1853.
J. Ang, Y. Liu, and E. Shriberg. 2005. Automatic dia-
log act segmentation and classification in multiparty
meetings. In Proceedings of ICASSP, pages 1061?
106.
J. Austin. 1962. How to Do Things With Words. Har-
vard Univ. Press, Cambridge, MA.
K.P. Bennett, A. Demiriz, and R. Maclin. 2002. Ex-
ploiting unlabeled data in ensemble methods. In
Proceedings of ACM SIGKDD, pages 289?296.
W.W. Cohen, V.R. Carvalho, and T. Mitchell. 2004.
Learning to classify email into ?speech acts?. In
Proceedings of EMNLP, pages 309?316.
R. Dhillon, S. Bhagat, H. Carvey, and E. Shriberg.
2004. Meeting recorder project: Dialog act label-
ing guide. Technical report, International Computer
Science Institute.
G. Druck, G. Mann, and A. McCallum. 2008. Learn-
ing from labeled features using generalized expecta-
tion criteria. In Proceedings of ACM SIGIR, pages
595?602.
D. Feng, E. Shaw, J. Kim, and E. H. Hovy. 2006.
Learning to detect conversation focus of threaded
discussions. In Proceedings of HLT-NAACL, pages
208?215.
M. Galley, K. McKeown, J. Hirschberg, and
E. Shriberg. 2004. Identifying agreement and dis-
agreement in conversational speech: use of bayesian
networks to model pragmatic dependencies. In Pro-
ceedings of ACL.
L. Gillick and S. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proceedings of ICASSP, pages 532?535.
D. Jurafsky, E. Shriberg, and D. Biasca. 1997. Switch-
board SWBD-DAMSL labeling project coder?s
manual, draft 13. Technical report, Univ. of Col-
orado Institute of Cognitive Science.
T. Kudo and Y. Matsumoto. 2004. A boosting algo-
rithm for classification of semi-structured text. In
Proceedings of EMNLP, pages 301?308.
S. Levinson. 1983. Pragmatics. Cambridge Univ.
Press, Cambridge.
L. Mason, P. Bartlett, J. Baxter, and M. Frean. 2000.
Functional gradient techniques for combining hy-
potheses. In A.J. Smola, P.L. Bartlett, B. Sch?olkopf,
and D. Schuurmans, editors, Advances in Large
Margin Classifiers, pages 221?246. MIT Press,
Cambridge, MA.
K. McKeown, L. Shrestha, and O. Rambow. 2007. Us-
ing question-answer pairs in extractive summariza-
tion of email conversations. In Proceedings of CI-
CLing, volume 4394 of Lecture Notes in Computer
Science, pages 542?550.
J. Nastri, J. Pe na, and J. T. Hancock. 2006. The
construction of away messages: A speech act anal-
ysis. Journal of Computer-Mediated Communica-
tion, 11(4):article 7.
S. Ravi and J. Kim. 2007. Profiling student interac-
tions in threaded discussions with speech act classi-
fiers. In Proceedings of the AI in Education Confer-
ence.
1258
C. Ros?e, Y. Wang, Y. Cui, J. Arguello, K. Stegmann,
A. Weinberger, and F. Fischer. 2008. Analyzing
collaborative learning processes automatically: Ex-
ploiting the advances of computational linguistics in
computer-supported collaborative learning. Interna-
tional Journal of Computer-Supported Collabora-
tive Learning, 3(3):237?271.
R.E. Schapire and Y. Singer. 1999. Improved boosting
algorithms using confidence-rated predictions. Ma-
chine Learning, 37(3):297?336.
J. Searle. 1969. Speech Acts. Cambridge Univ. Press,
Cambridge.
D. Shasha and K. Zhang. 1990. Fast algorithms for the
unit cost editing distance between trees. Journal of
Algorithms, 11(4):581?621.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proceedings of NAACL-HLT, pages 149?
156.
A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates,
D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-
Dykema, and M. Meteer. 2000. Dialogue act
modeling for automatic tagging and recognition of
conversational speech. Computational Linguistics,
26(3):339?373.
G. Tur, D. Hakkani-T?ur, and R. E. Schapire. 2005.
Combining active and semi-supervised learning for
spoken language understanding. Speech Communi-
cation, 45(2):171?186.
D. P. Twitchell, J. F. Nunamaker, and J. K. Burgoon.
2004. Using speech act profiling for deception de-
tection. In Second Symposium on Intelligence and
Security Informatics, volume 3073 of Lecture Notes
in Computer Science, pages 403?410.
1259
Proceedings of NAACL HLT 2009: Short Papers, pages 169?172,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
A Local Tree Alignment-based Soft Pattern Matching Approach for
Information Extraction
Seokhwan Kim, Minwoo Jeong, and Gary Geunbae Lee
Department of Computer Science and Engineering
Pohang University of Science and Technology
San 31, Hyoja-dong, Nam-gu, Pohang, 790-784, Korea
{megaup, stardust, gblee}@postech.ac.kr
Abstract
This paper presents a new soft pattern match-
ing method which aims to improve the recall
with minimized precision loss in information
extraction tasks. Our approach is based on a
local tree alignment algorithm, and an effec-
tive strategy for controlling flexibility of the
pattern matching will be presented. The ex-
perimental results show that the method can
significantly improve the information extrac-
tion performance.
1 Introduction
The goal of information extraction (IE) is to ex-
tract structured information from unstructured natu-
ral language documents. Pattern induction to gener-
ate extraction patterns from a number of training in-
stances is one of the most widely applied approaches
for IE.
A number of pattern induction approaches have
recently been researched based on the dependency
analysis (Yangarber, 2003) (Sudo et al, 2001)
(Greenwood and Stevenson, 2006) (Sudo et al,
2003). The natural language texts in training in-
stances are parsed by dependency analyzer and con-
verted into dependency trees. Each subtree of a de-
pendency tree is considered as a candidate of ex-
traction patterns. An extraction pattern is gener-
ated by selecting the subtree which indicates the de-
pendency relationships of each labeled slot value
in the training instance and agrees on the selec-
tion criteria defined by each pattern representation
model. A number of dependency tree-based pat-
tern representation models have been proposed. The
predicate-argument (SVO) model allows subtrees
containing only a verb and its direct subject and
object as extraction pattern candidates (Yangarber,
2003). The chain model represents extraction pat-
terns as a chain-shaped path from each target slot
value to the root node of the dependency tree (Sudo
et al, 2001). A couple of chain model patterns shar-
ing the same verb are linked to each other and con-
struct a linked-chain model pattern (Greenwood and
Stevenson, 2006). The subtree model considers all
subtrees as pattern candidates (Sudo et al, 2003).
Regardless of the applied pattern representation
model, the methods have concentrated on extracting
only exactly equivalent subtrees of test instances to
the extraction patterns, which we call hard pattern
matching. While the hard pattern matching policy
is helpful to improve the precision of the extracted
results, it can cause the low recall problem. In or-
der to tackle this problem, a number of soft pattern
matching approaches which aim to improve recall
with minimized precision loss have been applied to
the linear vector pattern models by introducing a
probabilistic model (Xiao et al, 2004) or a sequence
alignment algorithm (Kim et al, 2008).
In this paper, we propose an alternative soft
pattern matching method for IE based on a local
tree alignment algorithm. While other soft pattern
matching approaches have been able to handle the
matching among linear vector instances with fea-
tures from tree structures only, our method aims to
directly solve the low recall problem of tree-to-tree
pattern matching by introducing the local tree align-
ment algorithm which is widely used in bioinformat-
ics to analyze RNA secondary structures. Moreover,
169
(a) Example pattern
(b) Dependency Tree of the example sentence
(c) Local alignment-based tree pattern matching
Figure 1: An example of local alignment-based tree pat-
tern matching
we present an effective policy for controlling degree
of flexibility in the pattern matching by setting the
optimal threshold values for each extracted pattern.
2 Methods
The low recall problem of information extraction
based on hard pattern matching is caused by lack
of flexibility in pattern matching. For example, the
tree pattern in Figure 1(a) cannot be matched with
the tree in Figure 1(b) by considering only exactly
equivalent subtrees, because the first tree has an ad-
ditional root node ?said? which is not in the second
one. However, the matching between two trees can
be performed by omitting just a node as shown in
Figure 1(c).
In order to improve and control the degree of flex-
ibility in tree pattern matching, we have adopted a
local tree alignment approach as the pattern match-
ing method instead of hard pattern matching strat-
egy. The local tree alignment problem is to find the
most similar subtree between two trees.
We have adopted the Hochsmann algorithm
(Hochsmann et al, 2003) which is a local tree align-
ment algorithm used in bioinformatics to analyze
RNA secondary structures. The goal of the Hochs-
mann algorithm is to find the local closed forest
alignment which maximizes the similarity score for
ordered trees. The algorithm can be implemented
by a dynamic programming approach which solves a
problem based on the previous results of its subprob-
lems. The main problem of Hochsmann algorithm
is to compute the similarity score between two sub-
forests according to the defined order from the sin-
gle node level to the entire tree level. The similarity
score is defined based on three tree edit operations
which are insertion, deletion, and replacement (Tai,
1979). For each pair of subforests, the maximum
similarity score among three edit operations is com-
puted, and the kind and the position of performed
edit operations are recorded.
The adaptation of Hochsmann algorithm to the IE
problem is performed by redefining the ?-function,
the similarity score function between two nodes, as
follows:
?(v,w) =
?
?????
?????
1 if lnk(v)=lnk(w),
and lbl(v)=lbl(w),
?(p(w), p(v)) if lbl(v)=<SLOT>,
0 otherwise.
where v and w are nodes to be compared, lnk(v) is
the link label of v, lbl(v) is the node label of v, and
p(v) denotes a parent node of v. While general local
tree alignment problems consider only node labels
to compute the node-level similarities, our method
considers not only node labels, but also link labels to
the head node, because the class of link to the head
node is important as the node label itself for depen-
dency trees. Moreover, the method should consider
the alignment of slot value nodes in the tree patterns
for adopting information extraction tasks. If the pat-
tern node v is a kind of slot value nodes, the similar-
ity score between v and w is inherited from parents
of both nodes.
After computing for all pairs of subforests, the
optimal alignment is obtained by trace-back based
on the recorded information of edit operation which
maximizes the similarity score for each subforest
pair. On the optimal alignment, the target node
aligned to a slot value node on the pattern is regarded
as an argument candidate of the extraction. Each ex-
170
traction candidate has its confidence score which is
computed from the alignment score, defined as:
score(TPTN, TTGT) = S(TPTN, TTGT)|TPTN|
where |T | denotes the total number of nodes in tree
T and S(T1, T2) is the similarity score of both trees
computed by Hochsmann algorithm.
Only the extraction candidates with alignment
score larger than the given threshold value, ?, are
accepted and regarded as extraction results. For the
simplest approach, the same threshold value, ?, can
be applied to all the patterns. However, we assumed
that each pattern has its own optimal threshold value
as its own confidence score, which is different from
other patterns? threshold values. The optimal thresh-
old value ?i and the confidence score confi for the
pattern Pi are defined as:
?i = argmax
0.5<??1.0
{evalfscore (Dtrain, Pi, ?)}
confi = max0.5<??1.0 {evalfscore (Dtrain, Pi, ?)}
where evalfscore(D,P, ?) is the evaluation result in
F-score of the extraction for the data set D using the
pattern P with the threshold value ?. For each pat-
tern, the threshold value which maximizes the eval-
uation result in F-score for the training data set and
the maximum evaluation result in F-score are as-
signed as the optimal threshold value and the con-
fidence score for the pattern respectively.
3 Experiment
In order to evaluate the effectiveness of our method,
we performed an experiment for the scenario tem-
plate extraction task on the management succession
domain in MUC-6. The task aims to extract sce-
nario template instances which consist of person-in,
person-out, position, organization slot values from
news articles about management succession events.
We used a modified version of the MUC-6 corpus
including 599 training documents and 100 test doc-
uments described by Soderland (1999). While the
scenario templates on the original MUC-6 corpus
are labeled on each document, this version has sce-
nario templates for each sentence.
All the sentences in both training and test
documents were converted into dependency trees
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
F-
sc
or
e
Proportion of Patterns Used
SOFT(SUBTREE)
SOFT(LINKED)
SOFT(CHAIN)
HARD(LINKED)
HARD(CHAIN)
HARD(SUBTREE)
SOFT/HARD(SVO)
Figure 2: Comparison of soft pattern matching strategy
with the hard pattern matching
by Berkeley Parser1 and LTH Constituent-to-
Dependency Conversion Tool2. From the depen-
dency trees and scenario templates on the training
data, we constructed pattern candidate sets for four
types of pattern representation models which are
SVO, chain, linked-chain, and subtree models. For
each pattern candidate, corresponding confidence
score and optimal threshold value were computed.
The pattern candidates for each pattern represen-
tation model were arranged in descending order of
confidence score. According to the arranged order,
each pattern was matched with test documents and
the extracted results were accumulated. Extracted
templates for test documents are evaluated by com-
paring with the answer templates on the test corpus.
The curves in Figure 2 show the relative perfor-
mance of the pattern matching strategies for each
pattern representation model. The results suggest
that soft pattern matching strategy with optimal
threshold values requires less number of patterns
for the performance saturation than the hard pat-
tern matching strategy for all pattern models except
the SVO model. For the SVO model, the result of
soft pattern matching strategy is equivalent to that
of hard pattern matching strategy. It is because most
of patterns represented in SVO model are relatively
shorter than those represented in other models.
In order to evaluate the flexibility controlling
strategy, we compared the result of optimally de-
termined threshold values with the cases of using
1http://nlp.cs.berkeley.edu/pages/Parsing.html
2http://nlp.cs.lth.se/pennconverter/
171
? SVO Chain Linked-Chain SubtreeP R F P R F P R F P R F
0.7 32.1 18.0 23.1 27.6 55.0 36.8 26.8 57.0 36.4 26.6 58.0 36.5
0.8 32.1 18.0 23.1 43.8 35.0 38.8 43.4 36.0 39.3 44.7 34.0 38.6
0.9 32.1 18.0 23.1 45.2 33.0 38.1 43.8 35.0 38.9 45.2 33.0 38.2
1.0 (hard) 32.1 18.0 23.1 45.2 33.0 38.1 43.8 35.0 38.9 45.2 33.0 38.2
optimal 32.1 18.0 23.1 36.0 49.0 41.5 40.7 48.0 44.0 43.0 46.0 44.4
Table 1: Experimental Results
various fixed threshold values. Table 1 represents
the final results for all pattern representation mod-
els and threshold values. For the SVO model, all
the results are equivalent regardless of the thresh-
old strategy because of extremely short length of the
patterns. For the other pattern models, precisions are
increased and recalls are decreased by increasing the
threshold. The maximum performances in F-score
are achieved by our optimal threshold determining
strategy for all pattern representation models. The
experimental results of our method show the better
recall than the cases of hard pattern matching and
controlled precision than the cases of extremely soft
pattern matching.
4 Conclusion
We presented a local tree alignment based soft pat-
tern matching approach for information extraction.
The softness of the pattern matching method is con-
trolled by the threshold value of the alignment score.
The optimal threshold values are determined by self-
evaluation on the training data. Experimental results
indicate that our soft pattern matching approach is
helpful to improve the pattern coverage and our
threshold learning strategy is effective to reduce the
precision loss followed by the soft pattern matching
method.
The goal of local tree alignment algorithm is to
measure the structural similarity between two trees.
It is similar to the kernel functions in the tree kernel
method which is another widely applied approach to
solve the IE problems. In the future, we plan to in-
corporate our alignment-based soft pattern matching
method into the tree kernel method for IE.
Acknowledgments
This work was supported by the Korea Science and
Engineering Foundation(KOSEF) grant funded by
the Korea government(MEST) (No. R01-2008-000-
20651-0)
References
Mark A. Greenwood and Mark Stevenson. 2006. Im-
proving semi-supervised acquisition of relation extrac-
tion patterns. In Proceedings of Workshop on Informa-
tion Extraction Beyond The Document, pp. 29?35.
Matthias Hochsmann, Thomas Toller, Robert Giegerich,
and Stefan Kurtz. 2003. Local similarity in rna sec-
ondary structures. In Proceedings of the IEEE Com-
puter Society Bioinformatics Conference , pp. 159?68.
Seokhwan Kim, Minwoo Jeong, and Gary Geunbae
Lee. 2008. An alignment-based pattern representa-
tion model for information extraction. In Proceedings
of the ACM SIGIR ?08, pp. 875?876.
Stephen Soderland. 1999. Learning information extrac-
tion rules for semi-structured and free text. Machine
Learning, 34(1):233?272.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2001. Automatic pattern acquisition for japanese in-
formation extraction. In Proceedings of the first inter-
national conference on Human language technology
research, pp. 1?7.
Kiyoshi Sudo, Satoshi Sekine, and Ralph Grishman.
2003. An improved extraction pattern representation
model for automatic ie pattern acquisition. In Pro-
ceedings of the ACL ?03, pp. 224?231.
Kuo-Chung Tai. 1979. The tree-to-tree correction prob-
lem. Journal of the ACM (JACM), 26(3):422?433.
Jing Xiao, Tat-Seng Chua, and Hang Cui. 2004. Cas-
cading use of soft and hard matching pattern rules
for weakly supervised information extraction. In Pro-
ceedings of COLING ?04, pp. 542?548.
Roman Yangarber. 2003. Counter-training in discovery
of semantic patterns. In Proceedings of the ACL ?03,
pp. 343?350.
172
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 412?419,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Exploiting Non-local Features for Spoken Language Understanding
Minwoo Jeong and Gary Geunbae Lee
Department of Computer Science & Engineering
Pohang University of Science and Technology,
San 31 Hyoja-dong, Nam-gu
Pohang 790-784, Korea
{stardust,gblee}@postech.ac.kr
Abstract
In this paper, we exploit non-local fea-
tures as an estimate of long-distance de-
pendencies to improve performance on the
statistical spoken language understanding
(SLU) problem. The statistical natural
language parsers trained on text perform
unreliably to encode non-local informa-
tion on spoken language. An alternative
method we propose is to use trigger pairs
that are automatically extracted by a fea-
ture induction algorithm. We describe a
light version of the inducer in which a sim-
ple modification is efficient and success-
ful. We evaluate our method on an SLU
task and show an error reduction of up to
27% over the base local model.
1 Introduction
For most sequential labeling problems in natural
language processing (NLP), a decision is made
based on local information. However, processing
that relies on the Markovian assumption cannot
represent higher-order dependencies. This long-
distance dependency problem has been considered
at length in computational linguistics. It is the key
limitation in bettering sequential models in vari-
ous natural language tasks. Thus, we need new
methods to import non-local information into se-
quential models.
There are two types of method for using non-
local information. One is to add edges to structure
to allow higher-order dependencies and another is
to add features (or observable variables) to encode
the non-locality. An additional consistent edge of
a linear-chain conditional random field (CRF) ex-
plicitly models the dependencies between distant
occurrences of similar words (Sutton and McCal-
lum, 2004; Finkel et al, 2005). However, this
approach requires additional time complexity in
inference/learning time and it is only suitable for
representing constraints by enforcing label consis-
tency. We wish to identify ambiguous labels with
more general dependency without additional time
cost in inference/learning time.
Another approach to modeling non-locality is
to use observational features which can capture
non-local information. Traditionally, many sys-
tems prefer to use a syntactic parser. In a language
understanding task, the head word dependencies
or parse tree path are successfully applied to learn
and predict semantic roles, especially those with
ambiguous labels (Gildea and Jurafsky, 2002). Al-
though the power of syntactic structure is impres-
sive, using the parser-based feature fails to encode
correct global information because of the low ac-
curacy of a modern parser. Furthermore the inac-
curate result of parsing is more serious in a spoken
language understanding (SLU) task. In contrast
to written language, spoken language loses much
information including grammar, structure or mor-
phology and contains some errors in automatically
recognized speech.
To solve the above problems, we present one
method to exploit non-local information ? the trig-
ger feature. In this paper, we incorporate trig-
ger pairs into a sequential model, a linear-chain
CRF. Then we describe an efficient algorithm to
extract the trigger feature from the training data it-
self. The framework for inducing trigger features
is based on the Kullback-Leibler divergence cri-
terion which measures the improvement of log-
likelihood on the current parameters by adding a
new feature (Pietra et al, 1997). To reduce the
cost of feature selection, we suggest a modified
412
version of an inducing algorithm which is quite ef-
ficient. We evaluate our method on an SLU task,
and demonstrate the improvements on both tran-
scripts and recognition outputs. On a real-world
problem, our modified version of a feature selec-
tion algorithm is very efficient for both perfor-
mance and time complexity.
2 Spoken Language Understanding as a
Sequential Labeling Problem
2.1 Spoken Language Understanding
The goal of SLU is to extract semantic mean-
ings from recognized utterances and to fill the
correct values into a semantic frame structure.
A semantic frame (or template) is a well-formed
and machine readable structure of extracted in-
formation consisting of slot/value pairs. An ex-
ample of such a reference frame is as follows.
<s> i wanna go from denver to new york on
november eighteenth </s>
FROMLOC.CITY NAME = denver
TOLOC.CITY NAME = new york
MONTH NAME = november
DAY NUMBER = eighteenth
This example from air travel data (CU-
Communicator corpus) was automatically gener-
ated by a Phoenix parser and manually corrected
(Pellom et al, 2000; He and Young, 2005). In this
example, the slot labels are two-level hierarchi-
cal; such as FROMLOC.CITY NAME. This hier-
archy differentiates the semantic frame extraction
problem from the named entity recognition (NER)
problem.
Regardless of the fact that there are some
differences between SLU and NER, we can
still apply well-known techniques used in NER
to an SLU problem. Following (Ramshaw
and Marcus, 1995), the slot labels are drawn
from a set of classes constructed by extending
each label by three additional symbols, Begin-
ning/Inside/Outside (B/I/O). A two-level hierar-
chical slot can be considered as an integrated flat-
tened slot. For example, FROMLOC.CITY NAME
and TOLOC.CITY NAME are different on this slot
definition scheme.
Now, we can formalize the SLU prob-
lem as a sequential labeling problem, y? =
argmaxy P (y|x). In this case, input word se-
quences x are not only lexical strings, but also
multiple linguistic features. To extract semantic
frames from utterance inputs, we use a linear-
chain CRF model; a model that assigns a joint
probability distribution over labels which is con-
ditional on the input sequences, where the distri-
bution respects the independent relations encoded
in a graph (Lafferty et al, 2001).
A linear-chain CRF is defined as follows. Let
G be an undirected model over sets of random
variables x and y. The graph G with parameters
? = {?, . . .} defines a conditional probability for
a state (or label) sequence y = y1, . . . , yT , given
an input x = x1, . . . , xT , to be
P?(y|x) = 1Zx exp
( T?
t=1
?
k
?kfk(yt?1, yt,x, t)
)
where Zx is the normalization factor that makes
the probability of all state sequences sum to one.
fk(yt?1, yt,x, t) is an arbitrary linguistic feature
function which is often binary-valued in NLP
tasks. ?k is a trained parameter associated with
feature fk. The feature functions can encode any
aspect of a state transition, yt?1 ? yt, and the
observation (a set of observable features), x, cen-
tered at the current time, t. Large positive val-
ues for ?k indicate a preference for such an event,
while large negative values make the event un-
likely.
Parameter estimation of a linear-chain CRF is
typically performed by conditional maximum log-
likelihood. To avoid overfitting, the 2-norm reg-
ularization is applied to penalize on weight vec-
tor whose norm is too large. We used a limited
memory version of the quasi-Newton method (L-
BFGS) to optimize this objective function. The
L-BFGS method converges super-linearly to the
solution, so it can be an efficient optimization
technique on large-scale NLP problems (Sha and
Pereira, 2003).
A linear-chain CRF has been previously applied
to obtain promising results in various natural lan-
guage tasks, but the linear-chain structure is defi-
cient in modeling long-distance dependencies be-
cause of its limited structure (n-th order Markov
chains).
2.2 Long-distance Dependency in Spoken
Language Understanding
In most sequential supervised learning prob-
lems including SLU, the feature function
fk(yt?1, yt,xt, t) indicates only local information
413
for practical reasons. With sufficient local context
(e.g. a sliding window of width 5), inference and
learning are both efficient.
However, if we only use local features, then
we cannot model long-distance dependencies.
Thus, we should incorporate non-local infor-
mation into the model. For example, figure
1 shows the long-distance dependency problem
in an SLU task. The same two word to-
kens ?dec.? should be classified differently,
DEPART.MONTH and RETURN.MONTH. The
dotted line boxes represent local information at the
current decision point (?dec.?), but they are ex-
actly the same in two distinct examples. More-
over, the two states share the same previous
sequence (O, O, FROMLOC.CITY NAME-B,
O, TOLOC.CITY NAME-B, O). If we cannot
obtain higher-order dependencies such as ?fly?
and ?return,? then the linear-chain CRF cannot
classify the correct labels between the two same
tokens. To solve this problem, we propose an ap-
proach to exploit non-local information in the next
section.
3 Incorporating Non-local Information
3.1 Using Trigger Features
To exploit non-local information to sequential la-
beling for a statistical SLU, we can use two ap-
proaches; a syntactic parser-based and a data-
driven approach. Traditionally, information ex-
traction and language understanding fields have
usually used a syntactic parser to encode global
information (e.g. parse tree path, governing cat-
egory, or head word) over a local model. In a se-
mantic role labeling task, the syntax and semantics
are correlated with each other (Gildea and Juraf-
sky, 2002), that is, the global structure of the sen-
tence is useful for identifying ambiguous semantic
roles. However the problem is the poor accuracy
of the syntactic parser with this type of feature. In
addition, recognized utterances are erroneous and
the spoken language has no capital letters, no ad-
ditional symbols, and sometimes no grammar, so
it is difficult to use a parser in an SLU problem.
Another solution is a data-driven method, which
uses statistics to find features that are approxi-
mately modeling long-distance dependencies. The
simplest way is to use identical words in history or
lexical co-occurrence, but we wish to use a more
general tool; triggering. The trigger word pairs
are introduced by (Rosenfeld, 1994). A trigger
pair is the basic element for extracting informa-
tion from the long-distance document history. In
language modeling, n-gram based on the Marko-
vian assumption cannot represent higher-order de-
pendencies, but it can automatically extract trigger
word pairs from data. The pair (A ? B) means
that word A and B are significantly correlated, that
is, when A occurs in the document, it triggers B,
causing its probability estimate to change.
To select reasonable pairs from arbitrary word
pairs, (Rosenfeld, 1994) used averaged mutual in-
formation (MI). In this scheme, the MI score of
one pair is MI(A;B) =
P (A,B) log P (B|A)P (B) + P (A, B?) log
P (B?|A)
P (B?) +
P (A?, B) log P (B|A?)P (B?) + P (A?, B?) log
P (B?|A?)
P (B?) .
Using the MI criterion, we can select corre-
lated word pairs. For example, the trigger pair
(dec.?return) was extracted with score 0.001179
in the training data1. This trigger word pair can
represent long-distance dependency and provide a
cue to identify ambiguous classes. The MI ap-
proach, however, considers only lexical colloca-
tion without reference labels y, and MI based se-
lection tends to excessively select the irrelevant
triggers. Recall that our goal is to find the signif-
icantly correlated trigger pairs which improve the
model. Therefore, we use a more appropriate se-
lection method for sequential supervised learning.
3.2 Selecting Trigger Feature
We present another approach to extract relevant
triggers and exploit them in a linear-chain CRF.
Our approach is based on an automatic feature in-
duction algorithm, which is a novel method to se-
lect a feature in an exponential model (Pietra et al,
1997; McCallum, 2003). We follow McCallum?s
work which is an efficient method to induce fea-
tures in a linear-chain CRF model. Following the
framework of feature inducing, we start the algo-
rithm with an empty set, and iteratively increase
the bundle of features including local features and
trigger features. Our basic assumption, however,
is that the local information should be included
because the local features are the basis of the de-
cision to identify the classes, and they reduce the
1In our experiment, the pair (dec.?fly) cannot be selected
because this MI score is too low. However, the trigger pair is
a binary type feature, so the pair (dec.?return) is enough to
classify the two cases in the previous example.
414
1999dec.onchicagotodenverfromfly... 10th
1999dec.onchicagotodenverfrom... 10threturn ...
...
DEPART.MONTH
RETURN.MONTH
Figure 1: An example of a long-distance dependency problem in spoken language understanding. In
this case, a word token ?dec.? with local feature set (dotted line box) is ambiguous for determining the
correct label (DEPART.MONTH or RETURN.MONTH).
mismatch between training and testing tasks. Fur-
thermore, this assumption leads us to faster train-
ing in the inducing procedure because we can only
consider additional trigger features.
Now, we start the inducing process with local
features rather than an empty set. After training
the base model ?(0), we should calculate the gains,
which measure the effect of adding a trigger fea-
ture, based on the local model parameter ?(0). The
gain of the trigger feature is defined as the im-
provement in log-likelihood of the current model
?(i) at the i-th iteration according to the following
formula:
G??(i)(g) = max? G?(i)(g, ?)
= max?
{
L?(i)+g,? ? L?(i)
}
where ? is a parameter of a trigger feature to
be found and g is a corresponding trigger feature
function. The optimal value of ? can be calculated
by Newton?s method.
By adding a new candidate trigger, the equation
of the linear-chain CRF model is changed to an
additional feature model as P?(i)+g,?(y|x) =
P?(i)(y|x) exp
(?T
t=1 ?g(yt?1, yt,x, t)
)
Zx(?(i), g, ?)
.
Note that Zx(?(i), g, ?) is the marginal sum over
all states of y?. Following (Pietra et al, 1997; Mc-
Callum, 2003), the mean field approximation and
agglomerated features allows us to treat the above
calculation as the independent inference problem
rather than sequential inference. We can evaluate
the probability of state y with an adding trigger
pair given observation x separately as follows.
P?(i)+g,?(y|x, t) =
P?(i)(y|x, t) exp (?g(yt,x, t))
Zx(?(i), g, ?)
Here, we introduce a second approximation. We
use the individual inference problem over the un-
structured maximum entropy (ME) model whose
state variable is independent from other states in
history. The background of our approximation is
that the state independent problem of CRF can
be relaxed to ME inference problem without the
state-structured model. In the result, we calculate
the gain of candidate triggers, and select trigger
features over a light ME model instead of a huge
computational CRF model2.
We can efficiently assess many candidate trig-
ger features in parallel by assuming that the old
features remain fixed while estimating the gain.
The gain of trigger features can be calculated on
the old model that is trained with the local and
added trigger pairs in previous iterations. Rather
than summing over all training instances, we only
need to use the mislabeled N tokens by the cur-
rent parameter ?(i) (McCallum, 2003). From mis-
classified instances, we generate the candidates of
trigger pairs, that is, all pairs of current words and
others within the sentence. With the candidate fea-
ture set, the gain is
G??(i)(g) = N??E?[g]
?
N?
j=1
log (E?(i) [exp(??g)|xj ])?
??2
2?2 .
Using the estimated gains, we can select a small
portion of all candidates, and retrain the model
with selected features. We iteratively perform the
selection algorithm with some stop conditions (ex-
cess of maximum iteration or no added feature up
to the gain threshold). The outline of the induction
2The ME model cannot represent the sequential structure
and the resulting model is different from CRF. Nevertheless,
we empirically prove that the effect of additional trigger fea-
tures on both ME and approximated CRF (without regarding
edge-state) are similar (see the experiment section).
415
Algorithm InduceLearn(x,y)
triggers ? {?} and i ? 0
while |pairs| > 0 and i < maxiter do
?(i) ? TrainME(x,y)
P (ye|xe) ? Evaluate(x,y,?(i))
c ? MakeCandidate(xe)
G?(i) ? EstimateGain(c, P (ye|xe))
pairs ? SelectTrigger(c, G?(i))
x ? UpdateObs(x, pairs)
triggers ? triggers ? pairs and i ? i+ 1
end while
?(i+1) ? TrainCRF(x,y)
return ?(i+1)
Figure 2: Outline of trigger feature induction al-
gorithm
algorithms is described in figure 2. In the next sec-
tion, we empirically prove the effectiveness of our
algorithm.
The trigger pairs introduced by (Rosenfeld,
1994) are just word pairs. Here, we can gen-
eralize the trigger pairs to any arbitrary pairs of
features. For example, the feature pair (of?B-
PP) is useful in deciding the correct answer
PERIOD OF DAY-I in ?in the middle of the day.?
Without constraints on generating the pairs (e.g.
at most 3 distant tokens), the candidates can be
arbitrary conjunctions of features3. Therefore we
can explore any features including local conjunc-
tion or non-local singleton features in a uniform
framework.
4 Experiments
4.1 Experimental Setup
We evaluate our method on the CU-Communicator
corpus. It consists of 13,983 utterances. The se-
mantic categories correspond to city names, time-
related information, airlines and other miscella-
neous entities. The semantic labels are automat-
ically generated by a Phoenix parser and manually
corrected. In the data set, the semantic category
has a two-level hierarchy: 31 first level classes
and 7 second level classes, for a total of 62 class
combinations. The data set is 630k words with
29k entities. Roughly half of the entities are time-
related information, a quarter of the entities are
3In our experiment, we do not consider the local conjunc-
tions because we wish to capture the effect of long-distance
entities.
city names, a tenth are state and country names,
and a fifth are airline and airport names. For
the second level hierarchy, approximately three
quarters of the entities are ?NONE?, a tenth are
?TOLOC?, a tenth are ?FROMLOC?, and the re-
maining are ?RETURN?, ?DEPERT?, ?ARRIVE?,
and ?STOPLOC.?
For spoken inputs, we used the open source
speech recognizer Sphinx2. We trained the recog-
nizer with only the domain-specific speech corpus.
The reported accuracy for Sphinx2 speech recog-
nition is about 85%, but the accuracy of our speech
recognizer is 76.27%; we used only a subset of the
data without tuning and the sentences of this sub-
set are longer and more complex than those of the
removed ones, most of which are single-word re-
sponses.
All of our results have averaged over 5-fold
cross validation with an 80/20 split of the data.
As it is standard, we compute precision and re-
call, which are evaluated on a per-entity basis and
combined into a micro-averaged F1 score (F1 =
2PR/(P+R)).
A final model (a first-order linear chain CRF)
is trained for 100 iterations with a Gaussian prior
variance of 20, and 200 or fewer trigger features
(down to a gain threshold of 1.0) for each round of
inducing iteration (100 iterations of L-BFGS for
the ME inducer and 10?20 iterations of L-BFGS
for the CRF inducer). All experiments are imple-
mented in C++ and executed on Linux with XEON
2.8 GHz dual processors and 2.0 Gbyte of main
memory.
4.2 Empirical Results
We list the feature templates used by our experi-
ment in figure 3. For local features, we use the
indicators for specific words at location i, or lo-
cations within five words of i (?2,?1, 0,+1,+2
words on current position i). We also use the part-
of-speech (POS) tags and phrase labels with par-
tial parsing. Like words, the two basic linguis-
tic features are located within five tokens. For
comparison, we exploit the two groups of non-
local syntax parser-based features; we use Collins
parser and extract this type of features from the
parse trees. The first consists of the head word
and POS-tag of the head word. The second group
includes governing category and parse tree paths
introduced by semantic role labeling (Gildea and
Jurafsky, 2002). Following the previous studies
416
Local feature templates
-lexical words
-part-of-speech (POS) tags
-phrase chunk labels
Grammar-based feature templates
-head word / POS-tag
-parse tree path and governing category
Trigger feature templates
-word pairs (wi ? wj), |i? j| > 2
-feature pairs between words, POS-tags, and
chunk labels (fi ? fj), |i? j| > 2
-null pairs (? ? wj)
Figure 3: Feature templates
of semantic role labeling, the parse tree path im-
proves the classification performance of semantic
role labeling. Finally, we use the trigger pairs that
are automatically extracted from the training data.
Avoiding the overlap of local features, we add the
constraint |i? j| > 2 for the target word wj . Note
that null pairs are equivalent to long-distance sin-
gleton word features wj .
To compute feature performance, we begin with
word features and iteratively add them one-by-one
so that we achieve the best performance. Table 1
shows the empirical results of local features, syn-
tactic parser-based features, and trigger features
respectively. The two F1 scores for text tran-
scripts (Text) and outputs recognized by an au-
tomatic speech recognizer (ASR) are listed. We
achieved F1 scores of 94.79 and 71.79 for Text and
ASR inputs using only word features. The perfor-
mance is decreased by adding the additional local
features (POS-tags and chunk labels) because the
pre-processor brings more errors to the system for
spoken dialog.
The parser-based and trigger features are added
to two baselines: word only and all local features.
The result shows that the trigger feature is more
robust to an SLU task than the features generated
from the syntactic parser. The parse tree path and
governing category show a small improvement of
performance over local features, but it is rather in-
significant (word vs. word+path, McNemar?s test
(Gillick and Cox, 1989); p = 0.022). In contrast,
the trigger features significantly improve the per-
formance of the system for both Text and ASR
inputs. The differences between the trigger and
the others are statistically significant (McNemar?s
test; p < 0.001 for both Text and ASR).
Table 1: The result of local features, parser-based
features and trigger features
Feature set F1 (Text) F1 (ASR)
word (w) 94.79 71.79
w + POStag (p) 94.57 71.61
w + chunk (c) 94.70 71.64
local (w+p+c) 94.41 71.60
w + head (h) 94.55 71.76
w + path (t) 95.07 72.17
w + h + t 94.84 72.09
local + head (h) 94.17 71.39
local + path (t) 94.80 71.89
local + h + t 94.51 71.67
w + trigger 96.18 72.95
local + trigger 96.04 72.72
Next, we compared the two trigger selection
methods; mutual information (MI) and feature in-
duction (FI). Table 2 shows the experimental re-
sults of the comparison between MI and FI ap-
proaches (with the local feature set; w+p+c). For
the MI-based approach, we should calculate an av-
eraged MI for each word pair appearing in a sen-
tence and cut the unreliable pairs (down to thresh-
old of 0.0001) before training the model. In con-
trast, the FI-based approach selects reliable trig-
gers which should improve the model in train-
ing time. Our method based on the feature in-
duction algorithm outperforms simple MI-based
methods. Fewer features are selected by FI, that
is, our method prunes the event pairs which are
highly correlated, but not relevant to models. The
extended feature trigger (fi ? fj) and null trig-
gers (? ? wj) improve the performance over word
trigger pairs (wi ? wj), but they are not statisti-
cally significant (vs. (fi ? fj); p = 0.749, vs.
({?, wi} ? wj); p = 0.294). Nevertheless, the
null pairs are effective in reducing the size of trig-
ger features.
Figure 4 shows a sample of triggers selected by
MI and FI approaches. For example, the trigger
?morning ? return? is ranked in first of FI but
66th of MI. Moreover, the top 5 pairs of MI are
not meaningful, that is, MI selects many functional
word pairs. The MI approach considers only lexi-
cal collocation without reference labels, so the FI
method is more appropriate to sequential super-
vised learning.
Finally, we wish to justify that our modified
417
Table 2: Result of the trigger selection methods
Method Avg. # triggers F1 (Text) F1 (ASR) McNemar?s test (vs. MI)
MI (wi ? wj) 1,713 95.20 72.12 -
FI (wi ? wj) 702 96.04 72.72 p < 0.001
FI (fi ? fj) 805 96.04 72.76 p < 0.001
FI ({?, wi} ? wj) 545 96.14 72.80 p < 0.001
Mutual Information Feature Induction
[1] from?like [1] morning?return
[2] on?to [2] morning?on
[3] to?i [3] morning?to
[4] on?from [4] afternoon?on
[5] from?i [5] afternoon?return
[41] afternoon?return [6] afternoon?to
[66] morning?return [15] morning?leaving
[89] morning?leaving [349] december?return
[1738] london?fly [608] illinois?airport
Figure 4: A sample of triggers extracted by two
methods
version of an inducing algorithm is efficient and
maintains performance without any drawbacks.
We proposed two approximations: starting with
local features (Approx. 1) and using an unstruc-
tured model on the selection stage (Approx. 2),
Table 3 shows the results of variant versions of
the algorithm. Surprisingly, the selection crite-
rion based on ME (the unstructured model) is bet-
ter than CRF (the structured model) not only for
time cost but also for the performance on our ex-
periment4. This result shows that local informa-
tion provides the fundamental decision clues. Our
modification of the algorithm to induce features
for CRF is sufficiently fast for practical usage.
5 Related Work and Discussion
The most relevant previous work is (He and
Young, 2005) who describes an generative ap-
proach ? hidden vector state (HVS) model. They
used 1,178 test utterances with 18 classes for 1st
level label, and published the resulting F1 score
of 88.07. Using the same test data and classes,
we achieved the 92.77 F1-performance, as well
4In our analysis, 10?20 iterations for each round of in-
ducing procedure are insufficient in optimizing the model in
CRF (empty) inducer. Thus, the resulting parameters are
under-fitted and selected features are infeasible. We need
more iteration to fit the parameters, but they require too much
learning time (> 1 day).
as 39% of error reduction compared to the previ-
ous result. Our system uses a discriminative ap-
proach, which directly models the conditional dis-
tribution, and it is sufficient for classification task.
To capture long-distance dependency, HVS uses a
context-free model, which increases the complex-
ity of models. In contrast, we use non-local trigger
features, which are relatively easy to use without
having additional complexity of models.
Trigger word pairs are introduced and success-
fully applied in a language modeling task. (Rosen-
feld, 1994) demonstrated that the trigger word
pairs improve the perplexity in ME-based lan-
guage models. Our method extends this idea to
sequential supervised learning problems. Our trig-
ger selection criterion is based on the automatic
feature inducing algorithm, and it allows us to gen-
eralize the arbitrary pairs of features.
Our method is based on two works of fea-
ture induction on an exponential model, (Pietra et
al., 1997) and (McCallum, 2003). Our induction
algorithm builds on McCallum?s method which
presents an efficient procedure to induce features
on CRF. (McCallum, 2003) suggested using only
the mislabeled events rather than the whole train-
ing events. This intuitional suggestion has offered
us fast training. We added two additional approx-
imations to reduce the time cost; 1) an inducing
procedure over a conditional non-structured infer-
ence problem rather than an approximated sequen-
tial inference problem, and 2) training with a local
feature set, which is the basic information to iden-
tify the labels.
In this paper, our approach describes how to
exploit non-local information to a SLU prob-
lem. The trigger features are more robust than
grammar-based features, and are easily extracted
from the data itself by using an efficient selection
algorithm.
418
Table 3: Comparison of variations in the induction algorithm (performed on one of the 5-fold validation
sets); columns are induction and total training time (h:m:s), number of trigger and total features, and
f-score on test data.
Inducer type Approx. Induction/total time # triggers/features F1 (Text) F1 (ASR)
CRF (empty) No approx. 3:55:01 / 5:27:13 682 / 2,693 90.23 67.60
CRF (local) Approx. 1 1:25:28 / 2:56:49 750 / 5,241 94.87 71.65
ME (empty) Approx. 2 20:57 / 1:54:22 618 / 2,080 94.85 71.46
ME (local) Approx. 1+2 6:30 / 1:36:14 608 / 5,099 95.17 71.81
6 Conclusion
We have presented a method to exploit non-local
information into a sequential supervised learning
task. In a real-world problem such as statistical
SLU, our model performs significantly better than
the traditional models which are based on syntac-
tic parser-based features. In comparing our se-
lection criterion, we find that the mutual informa-
tion tends to excessively select the triggers while
our feature induction algorithm alleviates this is-
sue. Furthermore, the modified version of the al-
gorithm is practically fast enough to maintain its
performance particularly when the local features
are offered by the starting position of the algo-
rithm.
In this paper, we have focused on a sequential
model such as a linear-chain CRF. However, our
method can also be naturally applied to arbitrary
structured models, thus the first alternative is to
combine our methods with a skip-chain CRF (Sut-
ton and McCallum, 2004). Applying and extend-
ing our approach to other natural language tasks
(which are difficult to apply a parser to) such as in-
formation extraction from e-mail data or biomed-
ical named entity recognition is a topic of future
work.
Acknowledgements
We thank three anonymous reviewers for helpful
comments. This research was supported by the
MIC (Ministry of Information and Communica-
tion), Korea, under the ITRC (Information Tech-
nology Research Center) support program super-
vised by the IITA (Institute of Information Tech-
nology Assessment). (IITA-2005-C1090-0501-
0018)
References
J. R. Finkel, T. Grenager, and C. Manning. 2005. In-
corporating non-local information into information
extraction systems by gibbs sampling. In Proceed-
ings of ACL?05, pages 363?370.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
L. Gillick and S. Cox. 1989. Some statistical issues in
the comparison of speech recognition algorithms. In
Proceedings of ICASSP, pages 532?535.
Y. He and S. Young. 2005. Semantic processing using
the hidden vector state model. Computer Speech &
Language, 19(1):85?106.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
of ICML, pages 282?289.
A. McCallum. 2003. Efficiently inducing features of
conditional random fields. In Proceedings of UAI,
page 403.
B. L. Pellom, W. Ward, and S. S. Pradhan. 2000. The
cu communicator: An architecture for dialogue sys-
tems. In Proceedings of ICSLP.
S. Della Pietra, V. J. Della Pietra, and J. Lafferty. 1997.
Inducing features of random fields. IEEE Trans.
Pattern Anal. Mach. Intell, 19(4):380?393.
L. A. Ramshaw and M. P. Marcus. 1995. Text chunk-
ing using transformation-based learning. In 3rd
Workshop on Very Large Corpora, pages 82?94.
R. Rosenfeld. 1994. Adaptive statistical language
modeling: A maximum entropy approach. Tech-
nical report, School of Computer Science Carnegie
Mellon University.
F. Sha and F. Pereira. 2003. Shallow parsing
with conditional random fields. In Proceedings of
HLT/NAACL?03.
C. Sutton and A. McCallum. 2004. Collective segmen-
tation and labeling of distant entities in information
extraction. In ICML Workshop on Statistical Rela-
tional Learning.
419
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 281?284,
Suntec, Singapore, 4 August 2009.
c
?2009 ACL and AFNLP
Efficient Inference of CRFs for Large-Scale Natural Language Data
Minwoo Jeong
??
Chin-Yew Lin
?
Gary Geunbae Lee
?
?
Pohang University of Science & Technology, Pohang, Korea
?
Microsoft Research Asia, Beijing, China
?
{stardust,gblee}@postech.ac.kr
?
cyl@microsoft.com
Abstract
This paper presents an efficient inference algo-
rithm of conditional random fields (CRFs) for
large-scale data. Our key idea is to decompose
the output label state into an active set and an
inactive set in which most unsupported tran-
sitions become a constant. Our method uni-
fies two previous methods for efficient infer-
ence of CRFs, and also derives a simple but
robust special case that performs faster than
exact inference when the active sets are suffi-
ciently small. We demonstrate that our method
achieves dramatic speedup on six standard nat-
ural language processing problems.
1 Introduction
Conditional random fields (CRFs) are widely used in
natural language processing, but extending them to
large-scale problems remains a significant challenge.
For simple graphical structures (e.g. linear-chain), an
exact inference can be obtained efficiently if the num-
ber of output labels is not large. However, for large
number of output labels, the inference is often pro-
hibitively expensive.
To alleviate this problem, researchers have begun to
study the methods of increasing inference speeds of
CRFs. Pal et al (2006) proposed a Sparse Forward-
Backward (SFB) algorithm, in which marginal distribu-
tion is compressed by approximating the true marginals
using Kullback-Leibler (KL) divergence. Cohn (2006)
proposed a Tied Potential (TP) algorithm which con-
strains the labeling considered in each feature function,
such that the functions can detect only a relatively small
set of labels. Both of these techniques efficiently com-
pute the marginals with a significantly reduced runtime,
resulting in faster training and decoding of CRFs.
This paper presents an efficient inference algorithm
of CRFs which unifies the SFB and TP approaches. We
first decompose output labels states into active and in-
active sets. Then, the active set is selected by feasible
heuristics and the parameters of the inactive set are held
a constant. The idea behind our method is that not all
of the states contribute to the marginals, that is, only a
?
Parts of this work were conducted during the author?s
internship at Microsoft Research Asia.
small group of the labeling states has sufficient statis-
tics. We show that the SFB and the TP are special cases
of our method because they derive from our unified al-
gorithm with a different setting of parameters. We also
present a simple but robust variant algorithm in which
CRFs efficiently learn and predict large-scale natural
language data.
2 Linear-chain CRFs
Many versions of CRFs have been developed for use
in natural language processing, computer vision, and
machine learning. For simplicity, we concentrate on
linear-chain CRFs (Lafferty et al, 2001; Sutton and
McCallum, 2006), but the generic idea described here
can be extended to CRFs of any structure.
Linear-chain CRFs are conditional probability dis-
tributions over label sequences which are conditioned
on input sequences (Lafferty et al, 2001). Formally,
x = {x
t
}
T
t=1
and y = {y
t
}
T
t=1
are sequences of in-
put and output variables. Respectively, where T is the
length of sequence, x
t
? X and y
t
? Y where X is the
finite set of the input observations and Y is that of the
output label state space. Then, a first-order linear-chain
CRF is defined as:
p
?
(y|x) =
1
Z(x)
T
?
t=1
?
t
(y
t
, y
t?1
,x), (1)
where ?
t
is the local potential that denotes the factor
at time t, and ? is the parameter vector. Z(x) is a
partition function which ensures the probabilities of all
state sequences sum to one. We assume that the poten-
tials factorize according to a set of observation features
{?
1
k
} and transition features {?
2
k
}, as follows:
?
t
(y
t
, y
t?1
,x) =?
1
t
(y
t
,x) ??
2
t
(y
t
, y
t?1
), (2)
?
1
t
(y
t
,x) =e
?
k
?
1
k
?
1
k
(y
t
,x)
, (3)
?
2
t
(y
t
, y
t?1
) =e
?
k
?
2
k
?
2
k
(y
t
,y
t?1
)
, (4)
where {?
1
k
} and {?
2
k
} are weight parameters which we
wish to learn from data.
Inference is significantly challenging both in learn-
ing and decoding CRFs. Time complexity is O(T |Y|
2
)
for exact inference (i.e., forward-backward and Viterbi
algorithm) of linear-chain CRFs (Lafferty et al, 2001).
The inference process is often prohibitively expensive
281
when |Y| is large, as is common in large-scale tasks.
This problem can be alleviated by introducing approx-
imate inference methods based on reduction of the
search spaces to be explored.
3 Efficient Inference Algorithm
3.1 Method
The key idea of our proposed efficient inference
method is that the output label state Y can be decom-
posed to an active set A and an inactive set A
c
. Intu-
itively, many of the possible transitions (y
t?1
? y
t
) do
not occur, or are unsupported, that is, only a small part
of the possible labeling set is informative. The infer-
ence algorithm need not precisely calculate marginals
or maximums (more generally, messages) for unsup-
ported transitions. Our efficient inference algorithm
approximates the unsupported transitions by assigning
them a constant value. When |A| < |Y|, both train-
ing and decoding times are remarkably reduced by this
approach.
We first define the notation for our algorithm. Let
A
i
be the active set and A
c
i
be the inactive set of output
label i where Y
i
= A
i
? A
c
i
. We define A
i
as:
A
i
= {j|?(y
t
= i, y
t?1
= j) > ?} (5)
where ? is a criterion function of transitions (y
t?1
?
y
t
) and ? is a hyperparameter. For clarity, we define the
local factors as:
?
1
t,i
, ?
1
t
(y
t
= i,x), (6)
?
2
j,i
, ?
2
t
(y
t?1
= j, y
t
= i). (7)
Note that we can ignore the subscript t at ?
2
t
(y
t?1
=
j, y
t
= i) by defining an HMM-like model, that is,
transition matrix ?
2
j,i
is independent of t.
As exact inference, we use the forward-backward
procedure to calculate marginals (Sutton and McCal-
lum, 2006). We formally describe here an efficient
calculation of ? and ? recursions for the forward-
backward procedure. The forward value ?
t
(i) is the
sum of the unnormalized scores for all partial paths that
start at t = 0 and converge at y
t
= i at time t. The
backward value ?
t
(i) similarly defines the sum of un-
normalized scores for all partial paths that start at time
t + 1 with state y
t+1
= j and continue until the end
of the sequences, t = T + 1. Then, we decompose the
equations of exact ? and ? recursions as follows:
?
t
(i) = ?
1
t,i
?
?
?
j?A
i
(
?
2
j,i
? ?
)
?
t?1
(j) + ?
?
?
, (8)
?
t?1
(j) =
?
i?A
j
?
1
t,i
(
?
2
j,i
? ?
)
?
t
(i) + ?
?
i?Y
?
1
t,i
?
t
(i),
(9)
where ? is a shared transition parameter value for set
A
c
i
, that is, ?
2
j,i
= ? if j ? A
c
i
. Note that
?
i
?
t
(i) = 1
(Sutton and McCallum, 2006). Because all unsup-
ported transitions in A
c
i
are calculated simultaneously,
the complexities of Eq. (8) and (9) are approximately
O(T |A
avg
||Y|) where |A
avg
| is the average number of
states in the active set, i.e.,
1
T
?
T
t=1
|A
i
|. The worst
case complexity of our ? and ? equations is O(T |Y|
2
).
Similarly, we decompose a ? recursion for the
Viterbi algorithm as follows:
?
t
(i) = ?
1
t,i
{
max
(
max
j?A
i
?
2
j,i
?
t?1
(j),max
j?Y
??
t?1
(j)
)}
,
(10)
where ?
t
(i) is the sum of unnormalized scores for the
best-scored partial path that starts at time t = 0 and
converges at y
t
= i at time t. Because ? is constant,
max
j?Y
?
t?1
(j) can be pre-calculated at time t ? 1.
By analogy with Eq. (8) and (9), the complexity is ap-
proximately O(T |A
avg
||Y|).
3.2 Setting ? and ?
To implement our inference algorithm, we need a
method of choosing appropriate values for the setting
function ? of the active set and for the constant value
? of the inactive set. These two problems are closely
related. The size of the active set affects both the com-
plexity of inference algorithm and the quality of the
model. Therefore, our goal for selecting ? and ? is
to make a plausible assumption that does not sacrifice
much accuracy but speeds up when applying large state
tasks. We describe four variant special case algorithms.
Method 1: We set ?(i, j) = Z(L) and ? = 0 where
L is a beam set, L = {l
1
, l
2
, . . . , l
m
} and the sub-
partition function Z(L) is approximated by Z(L) ?
?
t?1
(j). In this method, all sub-marginals in the inac-
tive set are totally excluded from calculation of the cur-
rent marginal. ? and ? in the inactive sets are set to 0
by default. Therefore, at each time step t the algorithm
prunes all states i in which ?
t
(i) < ?. It also generates
a subset L of output labels that will be exploited in next
time step t + 1.
1
This method has been derived the-
oretically from the process of selecting a compressed
marginal distribution within a fixed KL divergence of
the true marginal (Pal et al, 2006). This method most
closely resembles SFB algorithm; hence we refer an al-
ternative of SFB.
Method 2: We define ?(i, j) = |?
2
j,i
?1| and ? = 1.
In practice, unsupported transition features are not pa-
rameterized
2
; this means that ?
k
= 0 and ?
2
j,i
= 1
if j ? A
c
i
. Thus, this method estimates nearly-exact
1
In practice, dynamically selecting L increases the num-
ber of computations, and this is the main disadvantage of
Method 1. However, in inactive sets ?
t?1
(j) = 0 by de-
fault; hence, we need not calculate ?
t?1
(j). Therefore, it
counterbalances the extra computations in ? recursion.
2
This is a common practice in implementation of input
and output joint feature functions for large-scale problems.
This scheme uses only supported features that are used at
least once in the training examples. We call it the sparse
model. While a complete and dense feature model may per-
282
CRFs if the hyperparameter is ? = 0; hence this cri-
terion does not change the parameter. Although this
method is simple, it is sufficiently efficient for training
and decoding CRFs in real data.
Method 3: We define ?(i, j) = E
p?
??
2
k
(i, j)? where
E
p?
?z? is an empirical count of event z in training data.
We also assign a real value for the inactive set, i.e.,
? = c ? R, c 6= 0, 1. The value c is estimated in the
training phase; hence, c is a shared parameter for the
inactive set. This method is equivalent to TP (Cohn,
2006). By setting ? larger, we can achieve faster infer-
ence, a tradeoff exists between efficiency and accuracy.
Method 4: We define the shared parameter as a func-
tion of output label y in the inactive set, i.e., c(y). As in
Method 3, c(y) is estimated during the training phase.
When the problem expects different aspects of unsup-
ported transitions, this method would be better than us-
ing only one parameter c for all labels in inactive set.
4 Experiment
We evaluated our method on six large-scale natu-
ral language data sets (Table 1): Penn Treebank
3
for part-of-speech tagging (PTB), phrase chunk-
ing data
4
(CoNLL00), named entity recognition
data
5
(CoNLL03), grapheme-to-phoneme conversion
data
6
(NetTalk), spoken language understanding data
(Communicator) (Jeong and Lee, 2006), and fine-
grained named entity recognition data (Encyclopedia)
(Lee et al, 2007). The active set is sufficiently small in
Communicator and Encyclopedia despite their large
numbers of output labels. In all data sets, we selected
the current word, ?2 context words, bigrams, trigrams,
and prefix and suffix features as basic feature templates.
A template of part-of-speech tag features was added for
CoNLL00, CoNLL03, and Encyclopedia. In particu-
lar, all tasks except PTB and NetTalk require assigning
a label to a phrase rather than to a word; hence, we used
standard ?BIO? encoding. We used un-normalized log-
likelihood, accuracy and training/decoding times as our
evaluation measures. We did not use cross validation
and development set for tuning the parameter because
our goal is to evaluate the efficiency of inference algo-
rithms. Moreover, using the previous state-of-the-art
features we expect the achievement of better accuracy.
All our models were trained until parameter estima-
tion converged with a Gaussian prior variance of 4.
During training, a pseudo-likelihood parameter estima-
tion (Sutton and McCallum, 2006) was used as an ini-
tial weight (estimated in 30 iterations). We used com-
plete and dense input/output joint features for dense
model (Dense), and only supported features that are
used at least once in the training examples for sparse
form better, the sparse model performs well in practice with-
out significant loss of accuracy (Sha and Pereira, 2003).
3
Penn Treebank3: Catalog No. LDC99T42
4
http://www.cnts.ua.ac.be/conll2000/chunking/
5
http://www.cnts.ua.ac.be/conll2003/ner/
6
http://archive.ics.uci.edu/ml/
Table 1: Data sets: number of sentences in the train-
ing (#Train) and the test data sets (#Test), and number
of output labels (#Label). |A
?=1
avg
| denotes the average
number of active set when ? = 1, i.e., the supported
transitions that are used at least once in the training set.
Set #Train #Test #Label |A
?=1
avg
|
PTB 38,219 5462 45 30.01
CoNLL00 8,936 2,012 22 6.59
CoNLL03 14,987 3,684 8 4.13
NetTalk 18,008 2,000 51 22.18
Communicator 13,111 1,193 120 3.67
Encyclopedia 25,348 6,336 279 3.27
model (Sparse). All of our model variants were based
on Sparse model. For the hyper parameter ?, we empir-
ically selected 0.001 for Method 1 (this preserves 99%
of probability density), 0 for Method 2, and 4 for Meth-
ods 3 and 4. Note that ? for Methods 2, 3, and 4 indi-
cates an empirical count of features in training set. All
experiments were implemented in C++ and executed in
Windows 2003 with XEON 2.33 GHz Quad-Core pro-
cessor and 8.0 Gbyte of main memory.
We first show that our method is efficient for learning
CRFs (Figure 1). In all learning curves, Dense gener-
ally has a higher training log-likelihood than Sparse.
For PTB and Encyclopedia, results for Dense are not
available because training in a single machine failed
due to out-of-memory errors. For both Dense and
Sparse, we executed the exact inference method. Our
proposed method (Method 1?4) performs faster than
Sparse. In most results, Method 1 was the fastest, be-
cause it was terminated after fewer iterations. How-
ever, Method 1 sometimes failed to converge, for ex-
ample, in Encyclopedia. Similarly, Method 3 and 4
could not find the optimal solution in the NetTalk data
set. Method 2 showed stable results.
Second, we evaluated the accuracy and decoding
time of our methods (Table 2). Most results obtained
using our method were as accurate as those of Dense
and Sparse. However, some results of Method 1, 3,
and 4 were significantly inferior to those of Dense and
Sparse for one of two reasons: 1) parameter estimation
failed (NetTalk and Encyclopedia), or 2) approximate
inference caused search errors (CoNLL00 and Com-
municator). The improvements of decoding time on
Communicator and Encyclopedia were remarkable.
Finally, we compared our method with two open-
source implementations of CRFs: MALLET
7
and
CRF++
8
. MALLET can support the Sparse model, and
the CRF++ toolkit implements only the Dense model.
We compared them with Method 2 on the Commu-
nicator data set. In the accuracy measure, the re-
sults were 91.56 (MALLET), 91.87 (CRF++), and 91.92
(ours). Our method performs 5?50 times faster for
training (1,774 s for MALLET, 18,134 s for CRF++,
7
Ver. 2.0 RC3, http://mallet.cs.umass.edu/
8
Ver. 0.51, http://crfpp.sourceforge.net/
283
0 10000 20000 30000 40000?
1400
00
?
1000
00
Training time (sec)
Log?
likelih
ood
SparseMethod 1Method 2Method 3Method 4
(a) PTB
0 500 1500 2500?
1000
0
?
6000
Training time (sec)
Log?
likelih
ood
DenseSparseMethod 1Method 2Method 3Method 4
(b) CoNLL00
0 500 1000 1500?1
4000
?
1000
0?
6000
?
2000
Training time (sec)
Log?
likelih
ood
DenseSparseMethod 1Method 2Method 3Method 4
(c) CoNLL03
0 1000 3000 5000
?
4100
0
?
3900
0
Training time (sec)
Log?
likelih
ood
DenseSparseMethod 1Method 2Method 3Method 4
(d) NetTalk
0 1000 3000 5000?75
00
?
6500
?
5500
?
4500
Training time (sec)
Log?
likelih
ood
DenseSparseMethod 1Method 2Method 3Method 4
(e) Communicator
0 100000 200000 300000
?
3000
0?
2000
0?
1000
0
Training time (sec)
Log?
likelih
ood
SparseMethod 1Method 2Method 3Method 4
(f) Encyclopedia
Figure 1: Result of training linear-chain CRFs: Un-normalized training log-likelihood and training times are
compared. Dashed lines denote the termination of training step.
Table 2: Decoding result; columns are percent accuracy (Acc), and decoding time in milliseconds (Time) measured
per testing example. ?
?
? indicates that the result is significantly different from the Sparse model. N/A indicates
failure due to out-of-memory error.
Method
PTB CoNLL00 CoNLL03 NetTalk Communicator Encyclopedia
Acc Time Acc Time Acc Time Acc Time Acc Time Acc Time
Dense N/A N/A 96.1 0.89 95.8 0.26 88.4 0.49 91.6 0.94 N/A N/A
Sparse 96.6 1.12 95.9 0.62 95.9 0.21 88.4 0.44 91.9 0.83 93.6 34.75
Method 1 96.8 0.74 95.9 0.55
?
94.0 0.24
?
88.3 0.34 91.7 0.73
?
69.2 15.77
Method 2 96.6 0.92
?
95.7 0.52 95.9 0.21
?
87.4 0.32 91.9 0.30 93.6 4.99
Method 3 96.5 0.84
?
94.2 0.51 95.9 0.24
?
78.2 0.29
?
86.7 0.30 93.7 6.14
Method 4 96.6 0.85
?
92.1 0.51 95.9 0.24
?
77.9 0.30 91.9 0.29 93.3 4.88
and 368 s for ours) and 7?12 times faster for decod-
ing (2.881 ms for MALLET, 5.028 ms for CRF++, and
0.418 ms for ours). This result demonstrates that learn-
ing and decoding CRFs for large-scale natural language
problems can be efficiently solved using our method.
5 Conclusion
We have demonstrated empirically that our efficient in-
ference method can function successfully, allowing for
a significant speedup of computation. Our method links
two previous algorithms, the SFB and the TP. We have
also showed that a simple and robust variant method
(Method 2) is effective in large-scale problems.
9
The
empirical results show a significant improvement in
the training and decoding speeds especially when the
problem has a large state space of output labels. Fu-
ture work will consider applications to other large-scale
problems, and more-general graph topologies.
9
Code used in this work is available at
http://argmax.sourceforge.net/.
References
T. Cohn. 2006. Efficient inference in large conditional ran-
dom fields. In Proc. ECML, pages 606?613.
M. Jeong and G. G. Lee. 2006. Exploiting non-local fea-
tures for spoken language understanding. In Proc. of COL-
ING/ACL, pages 412?419, Sydney, Australia, July.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional
random fields: Probabilistic models for segmenting and
labeling sequence data. In Proc. ICML, pages 282?289.
C. Lee, Y. Hwang, and M. Jang. 2007. Fine-grained named
entity recognition and relation extraction for question an-
swering. In Proc. SIGIR Poster, pages 799?800.
C. Pal, C. Sutton, and A. McCallum. 2006. Sparse forward-
backward using minimum divergence beams for fast train-
ing of conditional random fields. In Proc. ICASSP.
F. Sha and F. Pereira. 2003. Shallow parsing with conditional
random fields. In Proc. of NAACL/HLT, pages 134?141.
C. Sutton and A. McCallum. 2006. An introduction to condi-
tional random fields for relational learning. In Lise Getoor
and Ben Taskar, editors, Introduction to Statistical Rela-
tional Learning. MIT Press, Cambridge, MA.
284
Using Higher-level Linguistic Knowledge for Speech Recognition Error
Correction in a Spoken Q/A Dialog
Minwoo Jeong
Department of Computer
Science and Engineering,
POSTECH, Pohang, Korea
stardust@postech.ac.kr
Byeongchang Kim
Division of Computer and
Multimedia Engineering,
Uiduk University,
Gyeongju, Korea
bckim@uiduk.ac.kr
Gary Geunbae Lee
Department of Computer
Science and Engineering,
POSTECH, Pohang, Korea
gblee@postech.ac.kr
Abstract
Speech interface is often required in many
application environments such as telephone-
based information retrieval, car navigation sys-
tems, and user-friendly interfaces, but the low
speech recognition rate makes it difficult to ex-
tend its application to new fields. Several ap-
proaches to increase the accuracy of the recog-
nition rate have been researched by error cor-
rection of the recognition results, but previ-
ous approaches were mainly lexical-oriented
ones in post error correction. We suggest
an improved syllable-based model and a new
semantic-oriented approach to correct both se-
mantic and lexical errors, which is also more
accurate for especially domain-specific speech
error correction. Through extensive experi-
ments using a speech-driven in-vehicle telem-
atics information retrieval, we demonstrate
the superior performance of our approach and
some advantages over previous lexical-oriented
approaches.
1 Introduction
New application environments such as telephone-based
retrieval, car navigation systems, and mobile information
retrieval, often require speech interface to conveniently
process user queries. In these environments, keyboard
input is inconvenient or sometimes impossible because
of spatial limitation on mobile devices and instability in
manipulating the devices.
However, because of the low recognition rate in current
speech recognition systems, the performance of speech
applications such as speech-driven information retrieval
(IR) and question answering (QA), and speech dialogue
systems is very low. The performance of the serially con-
nected spoken QA system, based on the QA system from
text input which has 76% performance and the output of
the ASR which operated at a 30% WER, was only 7%
(Harabagiu et al, 2002). (Harabagiu et al, 2002) ex-
poses several fundamental flaws of this simple combina-
tion of an automatic speech recognition (ASR) and QA
system, including the importance of named entity infor-
mation, and the inadequacies of current speech recogni-
tion technology based on n-gram language models.
The major problem of speech-driven IR and QA is the
decreasing of the performance due to the recognition er-
rors in ASR systems. Erroneously recognized spoken
queries drop the precision and recall of IR and QA sys-
tem. Some authors investigated the relation of ASR er-
rors and precision of IR (Barnett et al, 1997; Crestani,
2000). They evaluated the effectiveness of the IR systems
through various error rates using 35 queries of TREC.
Their researches show that the increasing word error rate
(WER) quickly decreases the precision of IR. Another
group investigated the performance of spoken queries in
NTCIR collections (Fujii et al, 2002A). They evaluated
a variety of speakers, and calculated the error rate with
respect to a query term, which is a keyword used for the
retrieval. They showed that the WER of the query terms
was generally higher than that of the general words ir-
respective of the speakers. In other words, recognition
of content words related to the IR and QA performance
was more difficult than that of normal words. So, they
introduced a method to improve the precision of speech-
driven IR by suggesting a new type of IR system tightly-
integrated with a speech input interface (Fujii et al,
2002B). In their system, document collection provides an
adaptation of the language model of the ASR, which re-
sults in a drop of the word error rate.
For this reason, some appropriate adaptation tech-
niques are required for overcoming speech recognition
errors such as post error correction. ASR error correc-
tion can be one of the domain adaptation techniques to
improve the recognition accuracy, and the primary advan-
Figure 1: Adaptation via Post Error Correction
tage of the error correction approach is its independence
of the specific speech recognizer. If the speech recog-
nizer can be regarded as a black-box, we can perform ro-
bust and flexible domain adaptation through the post error
correction process. Figure 1 shows the paradigm of this
post error correction approach.
One approach in post error correction, which is a
straightforward and intuitive method to robustly handle
many kinds of recognition errors, was rule-based ap-
proach (Kaki et al, 1998). (Kaki et al, 1998) collected
many lexical error patterns that occurred in a speech
translation system in Japanese. They could correct any
type of errors by matching the strings in the transcription
with lexical error patterns in the database. However, their
approach has a disadvantage in that the correction is only
feasible to the trained (or collected) lexical error patterns.
Another approach has been based on a statistical
method utilizing the probabilistic information of words
in a spoken dialogue situation and the language models
adapted to the application domain (Ringger and Allen,
1996). (Ringger and Allen, 1996) applied the noisy chan-
nel model to the correction of the errors in speech recog-
nition. They simplified a statistical machine translation
(MT) model called an IBM model (Brown et al, 1990),
and tried to construct a general post-processor that can
correct errors generated by any speech recognizer. The
model consists of two parts: a channel model, which ac-
counts for errors made by the ASR, and the language
model, which accounts for the likelihood of a sequence of
words being uttered. They trained the channel model and
the language model both using some transcriptions from
TRAINS-95 dialogue system which is a train traveling
planning system (Allen et al, 1996). Here, the channel
model has the distribution that an original word may be
recognized as an erroneous word. They use the proba-
bility of mistakenly recognized words, the co-occurrence
information extracted from the words and their neighbor-
ing words, and the tagged word bi-grams, which are all
lexical clues in error strings.
Such approaches based on lexical information of words
have shown some successful results, but they still have
major drawbacks; The performance of such systems de-
pends on the size and the quality of speech recognition
result, or on the database of collected error strings since
they are directly dependent on lexical items. The error
patterns constructed are available but not enough, be-
cause it is expensive to collect them; so in many cases,
they fail to recover the original strings from the lexical
specific error patterns. Also, since they are sensitive to
the error patterns, they occasionally mis-identify a cor-
rect word as an error word.
We suggest a more improved and robust semantic-
oriented error correction approach, which can be in-
tegrated into previous fragile lexical-based approaches.
In our approach, in addition to lexical information, we
use high level syntactic and semantic information of the
words in a speech transcription. We obtain semantic in-
formation from a knowledge base such as general the-
sauri and a special domain dictionary that we construct
by ourselves to contain some domain specific knowledge
to the target application.
In the next section, we first describe a general noisy
channel model for ASR error correction and discuss some
problems with them. We then introduce our improved
channel model especially for Korean language in section
3. We also propose a new high-level error correction
model using syntactic and semantic knowledge in section
4. We prove the feasibility of our approach through some
experiments in section 5, and draw some conclusions in
section 6.
2 Noisy Channel Error Correction Model
The noisy channel error correction framework has been
applied to a wide range of problems, such as spelling
correction, statistical machine translation, and ASR error
correction (Brill and Moore, 2000; Brown et al, 1990;
Ringger and Allen, 1996). The key idea of noisy chan-
nel model is that we can model some channel properties
through estimating the posterior probabilities.
The problem of ASR error correction can be stated
in this model as follows: For an input sentence, O =
o1, o2, . . . , on produced as the output sequence of ASR,
find the best word sequence,W? = w1, w2, . . . , wn, that
maximizes the posterior probability P (W |O). Then, ap-
plying Bayes? rule and dropping the constant denomina-
tor, we can rewrite as:
W? = arg max
W
P (W |O) = arg max
W
P (W )P (O|W ) (1)
Now, we have a noisy channel model for ASR er-
ror correction, with two components, the source model
P (W ) and the channel model P (O|W ). The probability
P(W) is given by the language model and can be decom-
posed as:
P (W ) =
?
i
P (wi|w1,i?1) (2)
Figure 2: Example of Word-based Channel Model
The distribution P (W ) can be defined using n-grams,
structured language model (Chelba, 1997), or any other
tool in the statistical language modeling.
Next, the conditional probability, P (O|W ) reflects the
channel characteristics of the ASR environment. If we as-
sume that the output word sequence produced under ASR
are independent of one another, we have the following
formula:
P (O|W ) =
?
i
P (o1,i|w1,i) =
?
i
P (oi|wi) (3)
So,
W? = arg max
W
P (W )P (O|W )
= arg max
W
(
?
i
P (wi|w1,i?1)
?
i
P (oi|wi))(4)
However, this simple one-to-one model is not suitable
to handling split or merged errors, which frequently ap-
pear in an ASR output, because we assume that the out-
put word sequence are independent of one another. For
example, 1figure 2 shows a split or a merged error prob-
lem. To solve this problem, Ringger and Allen used the
fertility of pre-channel word (Ringger and Allen, 1996).
Following (Brown et al, 1990), we refer to the num-
ber of post-channel words oi produced by a pre-channel
word wi as a fertility. They simplified the fertility model
of IBM statistical MT model-4, and permitted the fer-
tility within 2 windows such as P (oi?1, oi|wi) for two-
to-one channel probability, and P (oi|wi, wi+1) for one-
to-two channel probability. So, the fertility model can
deal with (TO LEAVE, TOLEDO) substitution. But this
improved fertility model only slightly increased the ac-
curacy in experiments (Ringger and Allen, 1996), and
we think the major reason is due to the data-sparseness
problem. Because substitution probability is based on the
whole word-level, this fertility model requires enormous
training data. We call the model a word-based channel
model, because this model is based on the word-to-word
transformation. The word-based model focused on inter-
word substitutions, so it requires enough results of ASR
and transcription pairs. Considering the cost of building
the enough amount of correction pairs, we need a smaller
unit than a word for overcoming the data-sparseness.
1This example is from (Ringger and Allen, 1996).
3 Syllable-based Channel Model
We suggest an improved channel model for smaller train-
ing data. If we can use smaller unit such as letter,
phoneme or syllable than word, relatively smaller training
set is needed. For dealing with intra-word transformation,
we suggest a syllable-based channel model, which can
deal with syllable-to-syllable transformation. This model
is especially reasonable for Korean. In some agglutina-
tive languages such as Korean, syllable is a basic unit of
written form like a Chinese character. In Korean, the av-
erage number of syllables in one word is about three or
four.
3.1 The Model
Suppose S = s1, s2, . . . , sn is a syllable sequence of
ASR output and W = w1, w2, . . . , wm is a source word
sequence, then our purpose is to find the best word se-
quence W? as follows:
W? = arg max
W
P (W |S) (5)
We can apply the same Bayes? rule and decompose the
syllable-to-word channel model into syllable-to-syllable
channel model.
P (w|s) = P (s|w)P (w)P (s) ? P (s|w)P (w)
? P (s|x)P (x|w)P (w) (6)
So, final formula can be written as:
W? = arg max
W
(P (W )P (X|W )P (S|X)) (7)
Here, P (S|X) is the probability of a syllable-to-
syllable transformation, where X = x1, x2, . . . , xn is
a source syllable sequence. P (X|W ) is a word model,
which can convert syllable lattice into word lattice. The
conversion can be done efficiently by dictionary look-up.
This model is similar to a standard hidden markov
model (HMM) of continuous speech recognition. In
speech recognition system, P (S|X) can be an acoustic
model in signal-to-phoneme level, and P (X|W ) can be
a pronunciation dictionary. Then, we applied the fertility
into our syllable-to-syllable channel model. We set the
maximum 2-fertility of syllable, which was determined
experimentally.
3.2 Training the Model
To train the model, we need a training data consisting
of {X,S} pairs which are manually transcribed strings
and ASR outputs. And, we align the pair based on mini-
mizing the edit distance between xi and si by dynamic
Figure 3: Example of Syllable-based Channel Model
programming. 2Figure 3 shows an alignment for the
syllable-model (For understanding, we use an English ex-
ample and a letter-to-letter alignment. In Korean, each
syllable is clearly distinguished much like a letter in En-
glish.). For example, (TO LEAVE, TOLEDO) pair in pre-
vious section can be divided into (TO, TO), (L, L), (EA,
E), and (VE, DO) with fertility 2.
We can then calculate the probability of each sub-
stitution P (si|xi) by Maximum-Likelihood Estimation
(MLE). Let C(xi) be the frequency of source syllable,
and C(xi, si) be the frequency of events where xi substi-
tute si. Then,
PMLE(si|xi) =
C(xi, si)
C(xi)
(8)
The total number of theoretical unique syllables is
about ten thousands in Korean, but the number of syl-
lables, which appeared at least one time, is about 2,300
in a corpus which has about 3 billion syllables. Thus, we
used Witten-Bell method for smoothing unseen substitu-
tions (Witten and Bell, 1991). Let T (xi) be the number
of substitution types, and N be the number of syllables in
a training data. For Witten-Bell discounting, we should
define Z(xi), which is the number of syllable xi with
count zero. Then, we can write as follows:
PWB(si|xi) =
T (xi)
Z(xi)(N + T (xi))
, if C(xi, si) = 0 (9)
3.3 Decoding the Model
Given a syllable sequence S, we want to find
arg maxW (P (W )P (X|W )P (S|X)). This will be to re-
turn an N-best list of candidates according to the models,
and then rescore these candidates by taking into account
the language model probabilities. To rescore the candi-
dates, we used Viterbi search algorithm to find the best
sequence. For implementation of candidate generation,
we store the syllable channel probabilities P (si|xi) as a
hash-table to pop them easily and fast. The system can
generate a candidate word sequence network using sylla-
ble channel model and a lexicon. And then, we can find
optimal sequence which has the best probability through
Viterbi decoding by including a language model.
2We omitted detail character-level match lines to simplify.
The whole word match is depicted in bold lines, while no-line
means character-level match errors.
Figure 4: Common semantic category values
4 Using Syntactic and Semantic
Knowledge
In some similar areas such as spelling error correction
or optical character recognition (OCR) error correction,
NLP researchers traditionally identified five levels of er-
rors in a text: (1) a lexical level, (2) a syntactic level,
(3) a semantic level, (4) a discourse structure level, and
(5) a pragmatic level (Kukich, 1992). In spelling cor-
rection and OCR error correction problem, correction
schemes mainly have focused on non-word errors at the
lexical level, which is an isolated word correction prob-
lem. However, errors of speech recognition tend to be
continuous word errors which should be better classi-
fied into syntactic and semantic level errors, because the
recognizer only produces word sequences existing in a
lexicon. So, this section presents a more syntax and
semantic-oriented approach to correct erroneous outputs
of a speech recognizer using a domain knowledge which
provides syntactic and semantic information. We fo-
cus on continuous word error detection and correction,
using syntactic and semantic knowledge, and pipeline
this high-level error correction method with the syllable-
based channel model.
4.1 Lexico-Semantic Pattern
A lexico-semantic pattern (LSP) is a structure where lin-
guistic entries and semantic types are used in combina-
tion to abstract certain sequences of the words in a text.
It has been used in the area of natural language interface
for database (NLIDB) (Jung et al, 2003) and a TREC
QA system for the purpose of matching the user query
with the appropriate answer types at syntax/semantic
level (Kim et al, 2001; Lee et al, 2001). In an LSP,
linguistic entries consist of words, phrases and part-of-
speech (POS) tags, such as ?YMCA,? ?Young Men?s
Christian Association,? and ?NNP.?3 Semantic types con-
3Part-of-speech tag denoting a proper noun which is used in
Penn TreeBank (Marcus et al, 1994).
Phrases LSP
Reading trainer
Fairy tale trainer %hobby @position
Recreation coach
Table 1: Example of a template abstracted by LSP
sist of common semantic classes and domain-specific (or
user-defined) semantic classes. The common semantic
tags again include attribute-values in databases, such as
?@corp? for a company name like ?IBM,? and pre-define
83 semantic category values, such as ?@location? for lo-
cation names like ?New York? (Jung et al, 2003). Fig-
ure 4 shows an example of predefined common semantic
category values which will be used in an ontology dictio-
nary.
In domain-specific application, well defined semantic
concepts are required, and the domain-specific seman-
tic classes represent these requirements. The domain-
specific semantic classes include special attribute names
in databases, such as ?%action? for ?active? and ?inac-
tive,? and semantic category names, such as ?%hobby? for
?reading? and ?recreation,? for which the user wants a spe-
cific meaning in the application domain. Moreover, we
used the classes to abstract out several synonyms into a
single concept. For example, a domain-specific semantic
class ?%question? represents some words, such as ?ques-
tion?, ?query?, ?asking?, and ?answer.?
The domain dictionary is a subset of the general se-
mantic category dictionary, and focuses only on the nar-
row extent of the knowledge it concerns, since it is im-
possible to cover all the knowledge of the world in imple-
menting an application. On the other hand, the ontology
dictionary for common semantic classes reflects the pure
general knowledge of the world; hence it performs a sup-
plementary role to extract semantic information. The do-
main dictionary provides the specific vocabulary which is
used in semantic representation tasks of a user query and
the template database.
4.2 Construction of a Domain Knowledge
For semantic-oriented error correction, we constructed a
domain knowledge, which consists of a domain dictio-
nary, an ontology dictionary, and template queries that
are similar to question types in a QA system (Lee et
al., 2001). Query sentences are semantically abstracted
by LSP?s and are automatically collected for the template
database.
Because Fujii et al (Fujii et al, 2002B) have shown
the importance of the language model which well de-
scribes the domain knowledge, we reflect the domain
information with a template database: database of tem-
plate queries of the source statements which are used
Figure 5: Process of Semantic-oriented Error Correction
for the actual error detection and correction task after
speech recognition. The template queries are automati-
cally acquired by the Query-to-LSP translation from the
source statements using two semantic category dictionar-
ies: domain dictionary and an ontology dictionary. As-
suming that some speech statements for a specific target
domain are predefined, a record of the template database
is composed of a fixed number of LSP elements, such as
POS tags, semantic tags, and domain-specific semantic
classes. Table 1 shows an example of template abstracted
by LSP conversion in a predefined domain of ?on-line ed-
ucation.?
Query-to-LSP translation transforms a given query into
a corresponding LSP, and the LSP?s enhance the cov-
erage of extraction by information abstraction through
many-to-one mapping between queries and an LSP. The
words in a query sentence are converted into the LSP
through several steps. First, a morphological analysis is
performed, which segments a sentence of words into mor-
phemes, and adds POS tags to the morphemes (Lee et al,
2002). NE recognition discovers all the possible seman-
tic types for each word by consulting a domain dictionary
and an ontology dictionary. NE tagging selects a seman-
tic type for each word so that a sentence can be mapped
into a suitable LSP sequence by searching several types
in the semantic dictionaries (An et al, 2003).
4.3 Semantic-oriented Error Correction Process
Now, we will show the working mechanism of post error
correction of a speech recognition result using the domain
knowledge of template database and domain-specific dic-
tionary. Figure 5 is a schematic diagram of the post error
correction process.
The overall process is divided into two stages: a syn-
tactic/semantic recovery and a lexical recovery stage. In
the semantic error detection stage, a recognized query is
converted into the corresponding LSP. The converted LSP
may be ill-formed depending on the errors in the rec-
ognized query. Semantic error correction is performed
by replacing these syntactic and/or semantic errors us-
ing a semantic confusion table. We used a pre-collected
template database to recover the semantic level errors,
and the technique for searching most similar templates
are based on a minimum edit distance dynamic program-
ming search, which has been used as a similarity search
in many areas such as spelling correction, OCR post cor-
rection, and DNA sequence analysis (Wagner and Fis-
cher, 1974). The semantic confusion table provides the
matching cost, which can be semantic similarity, to the
dynamic programming search process. The ?minimum
edit distance? between two words is originally defined as
the minimum number of deletions, insertions, and sub-
stitutions required to transform one word into the other.
We compute the minimum edit distances between the er-
roneous LSP?s and the template LSP?s in the template
database using the similarity cost functions at the seman-
tic level, and select, as the final template query, the one
which has the minimum distance among them. At this
stage, replaced LSP elements can provide some clues of
the recognition errors and the original query?s meaning
to the next lexical recovery stage. Moreover, candidate
error boundary can also be detected by this procedure.
After this procedure, lexical recovery is performed in
the next stage. Recovered semantic tags and the erro-
neous queries produced by ASR are the clues of lexi-
cal recovery. Erroneous query and recovered template
query are aligned by dynamic programming again, after
which some lexical candidates are generated by our im-
proved syllable-based channel model. Figure 6 4 shows
an example of semantic error correction process using the
same data in TRAIN-95 (Allen et al, 1996).
5 Experiments
5.1 Experimental Setup
We performed several experiments on the domain of in-
vehicle telematics IR related to navigation question an-
swering services. The speech transcripts used in the ex-
periments were composed of 462 queries, which were
collected by 1 male speaker in a real application. We
also used two Korean speech recognizers: a speech rec-
ognizer made by LG-Elite (LG Electronics Institute of
Technology) and a Korean commercial speech recog-
nizer, ByVoice (refer to http://www.voicetech.co.kr). For
4In corrected sentence, note that word ?A? is not recovered
because this word is meaningless functional word.
Figure 6: Example of Semantic-oriented Error Correction
our semantic-oriented error correction, we constructed a
domain knowledge for our target domain. We constructed
3,195 entries of domain dictionary, 13,154 entries of on-
tology dictionary, and 436 semantic templates generated
automatically using domain dictionary and ontology dic-
tionary.
We implemented both word-based and syllable-based
model for comparison, and combined the system of
syllable-based lexical correction with the LSP-based se-
mantic error correction. For experiments, we use trigrams
language model generated by SRILM toolkit (Stolcke,
2002), and a training program for channel model made
by ourselves. And, we divided the 462 queries into 6 dif-
ferent sets, and evaluated the results of 6-fold cross vali-
dation for each model.
5.2 Results
To measure error correction performance, we use word
error rate (WER) and term error rate (TER):
WER = |Sw| + |Iw| + |Dw||Wtruth|
(10)
TER = |St| + |It| + |Dt||Ttruth|
(11)
|Wtruth| is the number of original words, and |Ttruth|
is the number of query term (or keyword) in original
words, that is, an error rate of content words directly re-
lated to the performance of IR and QA system (Fujii et
al., 2002A).
Table 2, 3 present the experiments results of WER of
baseline ASR, word-based channel model, our syllable-
based channel model and combined syllable-based chan-
nel model with the LSP semantic correction model. The
performances of baseline systems were about 79% ?
81% on the utterances in in-vehicle telematics IR domain.
This result shows that the semantic error correction of
Test set 1 2 3 4 5 6 AVG.
Baseline 18.1% 21.6% 19.4% 22.8% 19.9% 19.2% 20.17%
Word-based 12.8% 20.3% 15.5% 17.5% 16.7% 17.7% 16.75%
Syllable-based 10.6% 16.0% 11.5% 16.1% 14.0% 10.6% 13.13%
Syllable + LSP 9.7% 14.9% 10.4% 15.3% 13.0% 10.7% 12.33%
Table 2: Result of LG-Elite Recognizer
Test set 1 2 3 4 5 6 AVG.
Baseline 20.5% 18.8% 19.8% 17.0% 16.9% 17.8% 18.47%
Word-based 19.9% 14.8% 18.4% 16.2% 15.3% 15.1% 16.75%
Syllable-based 16.7% 13.8% 17.0% 13.3% 12.7% 12.2% 14.28%
Syllable + LSP 15.3% 13.4% 15.8% 12.9% 11.3% 11.8% 13.42%
Table 3: Result of ByVoice
speech recognition result is a viable approach to improve
the performance.
Using both baseline ASR systems, we achieved 39%
and 27% of error reduction rate. In comparison with the
previous word-based model, our new approaches have
more accurate error correction performance in this do-
main. Table 4 shows the result of the experiments for
TER. The result of TER shows that baseline ASR systems
alone are not appropriate to process the user?s queries in
speech-driven IR, QA or dialog understanding system.
However, with a post error correction, the error reduc-
tion rate of TER is much higher than that of WER. And
we achieved better performance than word-based model.
With this result, our methods are considered to be more
appropriate in speech-driven IR and QA applications.
Compared with the word-based noisy channel model that
has been the best approach in the error correction so far,
our semantic-oriented error correction suggests alterna-
tive more successful methods for speech recognition error
correction.
Baseline Word-
based
Syllable-
based
Syllable
+ LSP
LG-Elite 56.4 % 31.5% 30.1% 26.7%
ByVoice 64.1% 34.1% 32.8% 27.6%
Table 4: Result of Term Error Rate
6 Conclusion and Future Works
We proposed an improved syllable-based noisy channel
model and combined higher level linguistic knowledge
for semantic-oriented approach in a speech recognition
error correction, which shows a superior performance in
domain-specific IR applications.
The previous works only focused on inter-word level
error correction, commonly depending on a large amount
of training corpus for the error correction model and the
language model. So, previous approaches require enor-
mous results of ASR and are dependent on specific speak-
ers and environments. On the other hand, our method
takes in far smaller training corpus, and it is possible to
implement the method easily and in a short time to ob-
tain the better error correction rate because it utilizes the
semantic information of the application domain.
And our semantic-oriented approach has more advan-
tages over lexical based ones, since it is less sensitive to
each error pattern. Also, the approach has a broader cov-
erage of error patterns, since several similar common er-
ror strings in the semantic ground can be reduced to one
semantic error pattern, which enables us to improve the
probability of recovering from erroneous recognition re-
sults.
And, because the LSP scheme transforms pure lexical
entries into abstract semantic categories, the size of the
error pattern database can be reduced remarkably, and
it also increases the coverage and robustness compared
with the previous pure lexical entries that can only deal
with the morphological variants.
With all these facts, the LSP correction has a high
possibility of generating semantically correct correction
due to the massive use of semantic contexts. Hence, it
shows a high performance, especially when combined
with domain-specific speech-driven natural language IR
and QA systems.
Future work should include the end-performance ex-
periments with IR or QA application for our error correc-
tion model.
7 Acknowledgements
This work was partly supported by Jungki KeoJeom
Project (MOCIE, ITEP), and by 21C Frontier Project
(MOST).
References
James F. Allen, Bradford W. Miller, Eric K. Ringger, and
Teresa Sikorski. 1996. A Robust System for Natural
Spoken Dialogue. In Proceedings of the 34th Annual
Meeting of the ACL
Juhui An, Seungwoo Lee, and Gary Geunbae Lee. 2003.
Automatic acquisition of Named Entity tagged corpus
from World Wide Web. In Proceedings of the 41st an-
nual meeting of the ACL (poster presentation).
J. Barnett, S. Anderson, J. Broglio, M. Singh, R. Hud-
son, and S.W. Kuo. 1997. Experiments in spoken
queries for documents retrieval. In Proceedings of Eu-
rospeech, (3):1323-1326.
Eric Brill and Robert C. Moore. 2000. An Improved
Error Model for Noisy Channel Spelling Correction.
ACL2000, 286-293.
P. F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della
Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P.
S. Roossin. 1990. A Statistical Approach to Machine
Translation. Computational Linguistics, 16(2):79-85
Ciprian Chelba. 1997. A Structured Language Model. In
Proceedings of the Thirty-Fifth Annual Meeting of the
ACL and Eighth Conference of the European Chapter
of the ACL, 498-503.
F. Crestani. 2000. Word recognition errors and relevance
feedback in spoken query processing In Proceedings
of the 2000 Flexible Query Answering Systems Confer-
ence, 267-281.
Atsushi Fujii, Katunobu Itou, and Tetsuya Ishikawa.
2002A. Speech-driven Text Retrieval: Using Target
IR Collections for Statistical Language Model Adapta-
tion in Speech Recognition. Anni R. Coden and Eric
W. Brown and Savitha Srinivasan (Eds.) Information
Retrieval Techniques for Speech Application (LNCS
2273), 94-104.
Atsushi Fujii, Katunobu Itou, and Tetsuya Ishikawa.
2002B. A method for open-vocabulary speech-driven
text retrieval. In Proceedings of the 2002 conference
on Empirical Methods in Natural Language Process-
ing, 188-195.
Sanda Harabagiu, Dan Moldovan, and Joe Picone. 2002.
Open-Domain Voice-Activated Question Answering.
COLING2002, (1):321-327, Taipei.
Hanmin Jung, Gary Geunbae Lee, Wonseug Choi,
KyungKoo Min, and Jungyun Seo. 2003. Multi-
lingual question answering with high portability on re-
lational databases. IEICE transactions on information
and systems, E-86D(2):306-315.
Satoshi Kaki, Eiichiro Sumita, and Hitoshi Iida. 1998.
A Method for Correcting Speech Recognition Using
the Statistical features of Character Co-occurrence.
COLING-ACL?98, 653-657.
Haksoo Kim, Kyungsun Kim, Gary Geunbae Lee, and
Jungyun Seo. 2001. MAYA: A Fast Question-
Answering System Based on a Predictive Answer In-
dexer. In Proceedings of the 39th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?01), Workshop on Open-Domain Question An-
swering
K. Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys,
24(4):377-439.
Geunbae Lee, Jungyun Seo, Seungwoo Lee, Hanmin
Jung, Bong-Hyun Cho, Changki Lee, Byung-Kwan
Kwak, Jeongwon Cha, Dongseok Kim, JooHui An,
Harksoo Kim, and Kyungsun Kim. 2001. SiteQ: Engi-
neering High Performance QA System Using Lexico-
Semantic Pattern Matching and Shallow NLP. In Pro-
ceedings of the 10th Text Retrieval Conference (TREC-
10), Washington D.C.
Gary Geunbae Lee, Jeongwon Cha, and Jong-Hyeok
Lee. 2002. Syllable pattern-based unknown mor-
pheme segmentation and estimation for hybrid part-of-
speech tagging of Korean. Computational Linguistics,
28(1):53-70.
Mitchell P. Marcus and Beatrice Santorini and Mary Ann
Marcinkiewicz. 1994. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313-330.
Eric K. Ringger and James F. Allen. 1996. A fertility
model for post correction of continuous speech recog-
nition ICSLP?96, 897-900.
Andreas Stolcke 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of Intl. Conf.
on Spoken Language Processing, (2):901-904, Denver,
Co. (http://www.speech.sri.com/projects/srilm/)
Robert A. Wagner and Michae J. Fischer. 1974. The
String-to-String Correction Problem. Journal of the
ACM, 21(1):168-173.
I. Witten and T. Bell. 1991. The Zero-Frequency Prob-
lem: Estimating the Probabilities of Novel Events in
Adaptive Text Compression. In IEEE Transactions on
Information Theory, 37(4).
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 564?571,
Beijing, August 2010
A Cross-lingual Annotation Projection Approach
for Relation Detection
Seokhwan Kim?, Minwoo Jeong?, Jonghoon Lee?, Gary Geunbae Lee?
?Department of Computer Science and Engineering,
Pohang University of Science and Technology
{megaup|jh21983|gblee}@postech.ac.kr
?Saarland University
m.jeong@mmci.uni-saarland.de
Abstract
While extensive studies on relation ex-
traction have been conducted in the last
decade, statistical systems based on su-
pervised learning are still limited because
they require large amounts of training data
to achieve high performance. In this pa-
per, we develop a cross-lingual annota-
tion projection method that leverages par-
allel corpora to bootstrap a relation detec-
tor without significant annotation efforts
for a resource-poor language. In order to
make our method more reliable, we intro-
duce three simple projection noise reduc-
tion methods. The merit of our method is
demonstrated through a novel Korean re-
lation detection task.
1 Introduction
Relation extraction aims to identify semantic re-
lations of entities in a document. Many rela-
tion extraction studies have followed the Rela-
tion Detection and Characterization (RDC) task
organized by the Automatic Content Extraction
project (Doddington et al, 2004) to make multi-
lingual corpora of English, Chinese and Ara-
bic. Although these datasets encourage the de-
velopment and evaluation of statistical relation
extractors for such languages, there would be a
scarcity of labeled training samples when learn-
ing a new system for another language such as
Korean. Since manual annotation of entities and
their relations for such resource-poor languages
is very expensive, we would like to consider in-
stead a weakly-supervised learning technique in
order to learn the relation extractor without sig-
nificant annotation efforts. To do this, we propose
to leverage parallel corpora to project the relation
annotation on the source language (e.g. English)
to the target (e.g. Korean).
While many supervised machine learning ap-
proaches have been successfully applied to the
RDC task (Kambhatla, 2004; Zhou et al, 2005;
Zelenko et al, 2003; Culotta and Sorensen, 2004;
Bunescu and Mooney, 2005; Zhang et al, 2006),
few have focused on weakly-supervised relation
extraction. For example, (Zhang, 2004) and (Chen
et al, 2006) utilized weakly-supervised learning
techniques for relation extraction, but they did
not consider weak supervision in the context of
cross-lingual relation extraction. Our key hypoth-
esis on the use of parallel corpora for learning
the relation extraction system is referred to as
cross-lingual annotation projection. Early stud-
ies of cross-lingual annotation projection were ac-
complished for lexically-based tasks; for exam-
ple part-of-speech tagging (Yarowsky and Ngai,
2001), named-entity tagging (Yarowsky et al,
2001), and verb classification (Merlo et al, 2002).
Recently, there has been increasing interest in ap-
plications of annotation projection such as depen-
dency parsing (Hwa et al, 2005), mention de-
tection (Zitouni and Florian, 2008), and semantic
role labeling (Pado and Lapata, 2009). However,
to the best of our knowledge, no work has reported
on the RDC task.
In this paper, we apply a cross-lingual anno-
tation projection approach to binary relation de-
tection, a task of identifying the relation between
two entities. A simple projection method propa-
gates the relations in source language sentences to
564
word-aligned target sentences, and a target rela-
tion detector can bootstrap from projected annota-
tion. However, this automatic annotation is unre-
liable because of mis-classification of source text
and word alignment errors, so it causes a critical
falling-off in annotation projection quality. To al-
leviate this problem, we present three noise reduc-
tion strategies: a heuristic filtering; an alignment
correction with dictionary; and an instance selec-
tion based on assessment, and combine these to
yield a better result.
We provide a quantitive evaluation of our
method on a new Korean RDC dataset. In our
experiment, we leverage an English-Korean par-
allel corpus collected from the Web, and demon-
strate that the annotation projection approach and
noise reduction method are beneficial to build an
initial Korean relation detection system. For ex-
ample, the combined model of three noise reduc-
tion methods achieves F1-scores of 36.9% (59.8%
precision and 26.7% recall), favorably comparing
with the 30.5% shown by the supervised base-
line.1
The remainder of this paper is structured as fol-
lows. In Section 2, we describe our cross-lingual
annotation projection approach to relation detec-
tion task. Then, we present the noise reduction
methods in Section 3. Our experiment on the pro-
posed Korean RDC evaluation set is shown in Sec-
tion 4 and Section 5, and we conclude this paper
in Section 6.
2 Cross-lingual Annotation Projection
for Relation Detection
The annotation projection from a resource-rich
language L1 to a resource-poor language L2 is
performed by a series of three subtasks: annota-
tion, projection and assessment.
The annotation projection for relation detection
can be performed as follows:
1) For a given pair of bi-sentences in parallel cor-
pora between a resource-rich language L1 and
a target language L2, the relation detection task
is carried out for the sentence in L1.
1The dataset and the parallel corpus are available on the
author?s website,
http://isoft.postech.ac.kr/?megaup/research/resources/.
2) The annotations obtained by analyzing the sen-
tence in L1 are projected onto the sentence in
L2 based on the word alignment information.
3) The projected annotations on the sentence in
L2 are utilized as resources to perform the re-
lation detection task for the language L2.
2.1 Annotation
The first step to projecting annotations from L1
onto L2 is obtaining annotations for the sentences
in L1. Since each instance for relation detection
is composed of a pair of entity mentions, the in-
formation about entity mentions on the given sen-
tences should be identified first. We detect the
entities in the L1 sentences of the parallel cor-
pora. Entity identification generates a number of
instances for relation detection by coupling two
entities within each sentence. For each instance,
the existence of semantic relation between entity
mentions is explored, which is called relation de-
tection. We assume that there exist available mod-
els or systems for all annotation processes, includ-
ing not only an entity tagger and a relation de-
tector themselves, but also required preprocessors
such as a part-of-speech tagger, base-phrase chun-
ker, and syntax parser for analyzing text in L1.
Figure 1 shows an example of annotation pro-
jection for relation detection of a bitext in En-
glish and Korean. The annotation of the sentence
in English shows that ?Jan Mullins? and ?Com-
puter Recycler Incorporated? are entity mentions
of a person and an organization, respectively. Fur-
thermore, the result indicates that the pair of en-
tities has a semantic relationship categorized as
?ROLE.Owner? type.
2.2 Projection
In order to project the annotations from the sen-
tences in L1 onto the sentences in L2, we utilize
the information of word alignment which plays
an important role in statistical machine transla-
tion techniques. The word alignment task aims
to identify translational relationships among the
words in a bitext and produces a bipartite graph
with a set of edges between words with transla-
tional relationships as shown in Figure 1. In the
same manner as the annotation in L1, entities are
565
????????
(keom-pyu-teo-ri-sa-i-keul-reo)
?
(ui)
??
(sa-jang)
?
(eun)
? ??
(ra-go)
???
(mal-haet-da)
Mullins, owner of Incorporated said that ...
?
(jan)
???
(meol-rin-seu)
Jan Computer Recycler
ROLE.Owner
PER ORG
ORG PER
ROLE.Owner
Figure 1: An example of annotation projection for relation detection of a bitext in English and Korean
considered as the first units to be projected. We as-
sume that the words of the sentences in L2 aligned
with a given entity mention in L1 inherit the infor-
mation about the original entity in L1.
After projecting the annotations of entity men-
tions, the projections for relational instances fol-
low. A projection is performed on a projected in-
stance in L2 which is a pair of projected entities
by duplicating annotations of the original instance
in L1.
Figure 1 presents an example of projection of a
positive relational instance between ?Jan Mullins?
and ?Computer Recycler Incorporated? in the
English sentence onto its translational counter-
part sentence in Korean. ?Jan meol-rin-seu? and
?keom-pyu-teo-ri-sa-i-keul-reo? are labeled as en-
tity mentions with types of a person?s name and an
organization?s name respectively. In addition, the
instance composed of the two projected entities is
annotated as a positive instance, because its orig-
inal instance on the English sentence also has a
semantic relationship.
As the description suggests, the annotation pro-
jection approach is highly dependant on the qual-
ity of word alignment. However, the results of au-
tomatic word alignment may include several noisy
or incomplete alignments because of technical dif-
ficulties. We present details to tackle the problem
by relieving the influence of alignment errors in
Section 3.
2.3 Assessment
The most important challenge for annotation pro-
jection approaches is how to improve the robust-
ness against the erroneous projections. The noise
produced by not only word alignment but also
mono-lingual annotations in L1 accumulates and
brings about a drastic decline in the quality of pro-
jected annotations.
The simplest policy of utilizing the projected
annotations for relation detection in L2 is to con-
sider that all projected instances are equivalently
reliable and to employ entire projections as train-
ing instances for the task without any filtering. In
contrast with this policy, which is likely to be sub-
standard, we propose an alternative policy where
the projected instances are assessed and only the
instances judged as reliable by the assessment are
utilized for the task. Details about the assessment
are provided in Section 3.
3 Noise Reduction Strategies
The efforts to reduce noisy projections are consid-
ered indispensable parts of the projection-based
relation detection method in a resource-poor lan-
guage. Our noise reduction approach includes the
following three strategies: heuristic-based align-
ment filtering, dictionary-based alignment correc-
tion, and assessment-based instance selection.
3.1 Heuristic-based Alignment Filtering
In order to improve the performance of annotation
projection approaches, we should break the bottle-
neck caused by the low quality of automatic word
alignment results. As relation detection is carried
out for each instance consisting of two entity men-
tions, the annotation projection for relation detec-
tion concerns projecting only entity mentions and
566
their relational instances. Since this is different
from other shallower tasks such as part-of-speech
tagging, base phrase chunking, and dependency
parsing which should consider projections for all
word units, we define and apply some heuristics
specialized to projections of entity mentions and
relation instances to improve robustness of the
method against erroneous alignments, as follows:
? A projection for an entity mention should
be based on alignments between contiguous
word sequences. If there are one or more
gaps in the word sequence in L2 aligned
with an entity mention in the sentence in
L1, we assume that the corresponding align-
ments are likely to be erroneous. Thus, the
alignments of non-contiguous words are ex-
cluded in projection.
? Both an entity mention in L1 and its projec-
tion in L2 should include at least one base
noun phrase. If no base noun phrase oc-
curs in the original entity mention in L1, it
may suggest some errors in annotation for
the sentence in L1. The same case for the
projected instance raises doubts about align-
ment errors. The alignments between word
sequences without any base noun phrase are
filtered out.
? The projected instance in L2 should sat-
isfy the clausal agreement with the original
instance in L1. If entities of an instance
are located in the same clause (or differ-
ent clauses), its projected instance should be
in the same manner. The instances without
clausal agreement are ruled out.
3.2 Dictionary-based Alignment Correction
The errors in word alignment are composed of
not only imprecise alignments but also incomplete
alignments. If an alignment of an entity among
two entities of a relation instance is not provided
in the result of the word alignment task, the pro-
jection for the corresponding instance is unavail-
able. Unfortunately, the above-stated alignment
filtering heuristics for improving the quality of
projections make the annotation loss problems
worse by filtering out several alignments likely to
be noisy.
In order to solve this problem, a dictionary-
based alignment correction strategy is incorpo-
rated in our method. The strategy requires a bilin-
gual dictionary for entity mentions. Each entry of
the dictionary is a pair of entity mention in L1 and
its translation or transliteration in L2. For each
entity to be projected from the sentence in L1,
its counterpart in L2 is retrieved from the bilin-
gual dictionary. Then, we seek the retrieved entity
mention from the sentence in L2 by finding the
longest common subsequence. If a subsequence
matched to the retrieved mention is found in the
sentence in L2, we make a new alignment between
it and its original entity on the L1 sentence.
3.3 Assessment-based Instance Selection
The reliabilities of instances projected via a series
of independent modules are different from each
other. Thus, we propose an assessment strategy
for each projected instance. To evaluate the reli-
ability of a projected instance in L2, we use the
confidence score of monolingual relation detec-
tion for the original counterpart instance in L1.
The acceptance of a projected instance is deter-
mined by whether the score of the instance is
larger than a given threshold value ?. Only ac-
cepted instances are considered as the results of
annotation projection and applied to solve the re-
lation detection task in target language L2.
4 Experimental Setup
To demonstrate the effectiveness of our cross-
lingual annotation projection approach for rela-
tion detection, we performed an experiment on
relation detection in Korean text with propagated
annotations from English resources.
4.1 Annotation
The first step to evaluate our method was annotat-
ing the English sentences in a given parallel cor-
pus. We use an English-Korean parallel corpus
crawled from an English-Korean dictionary on the
web. The parallel corpus consists of 454,315 bi-
sentence pairs in English and Korean 2. The En-
glish sentences in the parallel corpus were prepro-
2The parallel corpus collected and other resources are all
available in our website
http://isoft.postech.ac.kr/?megaup/research/resources/
567
cessed by the Stanford Parser 3 (Klein and Man-
ning, 2003) which provides a set of analyzed re-
sults including part-of-speech tag sequences, a de-
pendency tree, and a constituent parse tree for a
sentence.
The annotation for English sentences is di-
vided into two subtasks: entity mention recogni-
tion and relation detection. We utilized an off-
the-shelf system, Stanford Named Entity Recog-
nizer 4 (Finkel et al, 2005) for detecting entity
mentions on the English sentences. The total
number of English entities detected was 285,566.
Each pair of recognized entities within a sentence
was considered as an instance for relation detec-
tion.
A classification model learned with the train-
ing set of the ACE 2003 corpus which con-
sists of 674 documents and 9,683 relation in-
stances was built for relation detection in English.
In our implementation, we built a tree kernel-
based SVM model using SVM-Light 5 (Joachims,
1998) and Tree Kernel Tools 6 (Moschitti, 2006).
The subtree kernel method (Moschitti, 2006) for
shortest path enclosed subtrees (Zhang et al,
2006) was adopted in our model. Our rela-
tion detection model achieved 81.2/69.8/75.1 in
Precision/Recall/F-measure on the test set of the
ACE 2003 corpus, which consists of 97 docu-
ments and 1,386 relation instances.
The annotation of relations was performed by
determining the existence of semantic relations
for all 115,452 instances with the trained model
for relation detection. The annotation detected
22,162 instances as positive which have semantic
relations.
4.2 Projection
The labels about entities and relations in the En-
glish sentences of the parallel corpora were propa-
gated into the corresponding sentences in Korean.
The Korean sentences were preprocessed by our
part-of-speech tagger 7 (Lee et al, 2002) and a de-
pendency parser implemented by MSTParser with
3http://nlp.stanford.edu/software/lex-parser.shtml
4http://nlp.stanford.edu/software/CRF-NER.shtml
5http://svmlight.joachims.org/
6http://disi.unitn.it/?moschitt/Tree-Kernel.htm
7http://isoft.postech.ac.kr/?megaup/research/postag/
Filter Without assessing With assessing
none 97,239 39,203
+ heuristics 31,652 12,775
+ dictionary 39,891 17,381
Table 1: Numbers of projected instances
a model trained on the Sejong corpus (Kim, 2006).
The annotation projections were performed on
the bi-sentences of the parallel corpus followed
by descriptions mentioned in Section 2.2. The
bi-sentences were processed by the GIZA++ soft-
ware (Och and Ney, 2003) in the standard con-
figuration in both English-Korean and Korean-
English directions. The bi-direcional alignments
were joined by the grow-diag-final algorithm,
which is widely used in bilingual phrase extrac-
tion (Koehn et al, 2003) for statistical machine
translation. This system achieved 65.1/41.6/50.8
in Precision/Recall/F-measure in our evaluation
of 201 randomly sampled English-Korean bi-
sentences with manually annotated alignments.
The number of projected instances varied with
the applied strategies for reducing noise as shown
in Table 1. Many projected instances were fil-
tered out by heuristics, and only 32.6% of the in-
stances were left. However, several instances were
rescued by dictionary-based alignment correction
and the number of projected instances increased
from 31,652 to 39,891. For all cases of noise re-
duction strategies, we performed the assessment-
based instance selection with a threshold value ?
of 0.7, which was determined empirically through
the grid search method. About 40% of the pro-
jected instances were accepted by instance selec-
tion.
4.3 Evaluation
In order to evaluate our proposed method, we pre-
pared a dataset for the Korean RDC task. The
dataset was built by annotating the information
about entities and relations in 100 news docu-
ments in Korean. The annotations were performed
by two annotators following the guidelines for the
ACE corpus processed by LDC. Our Korean RDC
corpus consists of 835 sentences, 3,331 entity
mentions, and 8,354 relation instances. The sen-
568
Model w/o assessing with assessingP R F P R F
Baseline 60.5 20.4 30.5 - - -
Non-filtered 22.5 6.5 10.0 29.1 13.2 18.2
Heuristic 51.4 15.5 23.8 56.1 22.9 32.5
Heuristic + Dictionary 55.3 19.4 28.7 59.8 26.7 36.9
Table 2: Experimental Results
tences of the corpus were preprocessed by equiva-
lent systems used for analyzing Korean sentences
for projection. We randomly divided the dataset
into two subsets with the same number of in-
stances for use as a training set to build the base-
line system and for evaluation.
For evaluating our approach, training instance
sets to learn models were prepared for relation
detection in Korean. The instances of the train-
ing set (half of the manually built Korean RDC
corpufs) were used to train the baseline model.
All other sets of instances include these baseline
instances and additional instances propagated by
the annotation projection approach. The train-
ing sets with projected instances are categorized
into three groups by the level of applied strategies
for noise reduction. While the first set included
all projections without any noise reduction strate-
gies, the second included only the instances ac-
cepted by the heuristics. The last set consisted of
the results of a series of heuristic-based filtering
and dictionary-based correction. For each training
set with projected instances, an additional set was
derived by performing assessment-based instance
selection.
We built the relation detection models for all
seven training sets (a baseline set, three pro-
jected sets without assessing, and three pro-
jected sets with assessing). Our implementations
are based on the SVM-Light and Tree Kernel
Tools described in the former subsection. The
shortest path dependency kernel (Bunescu and
Mooney, 2005) implemented by the subtree kernel
method (Moschitti, 2006) was adopted to learn all
models.
The performance for each model was evaluated
with the predictions of the model on the test set,
which was the other half of Korean RDC corpus.
We measured the performances of the models on
true entity mentions with true chaining of coref-
erence. Precision, Recall and F-measure were
adopted for our evaluation.
5 Experimental Results
Table 2 compares the performances of the differ-
ent models which are distinguished by the applied
strategies for noise reduction. It shows that:
? The model with non-filtered projections
achieves extremely poor performance due to
a large number of erroneous instances. This
indicates that the efforts for reducing noise
are urgently needed.
? The heuristic-based alignment filtering helps
to improve the performance. However, it is
much worse than the baseline performance
because of a falling-off in recall.
? The dictionary-based correction to our pro-
jections increased both precision and recall
compared with the former models with pro-
jected instances. Nevertheless, it still fails to
achieve performance improvement over the
baseline model.
? For all models with projection, the
assessment-based instance selection boosts
the performances significantly. This means
that this selection strategy is crucial in
improving the performance of the models
by excluding unreliable instances with low
confidence.
? The model with heuristics and assessments
finally achieves better performance than the
baseline model. This suggests that the pro-
jected instances have a beneficial influence
569
on the relation detection task when at least
these two strategies are adopted for reducing
noises.
? The final model incorporating all proposed
noise reduction strategies outperforms the
baseline model by 6 in F-measure. This is
due to largely increased recall by absorbing
more useful features from the well-refined
set of projected instances.
The experimental results show that our pro-
posed techniques effectively improve the perfor-
mance of relation detection in the resource-poor
Korean language with a set of annotations pro-
jected from the resource-rich English language.
6 Conclusion
This paper presented a novel cross-lingual annota-
tion projection method for relation extraction in a
resource-poor language. We proposed methods of
propagating annotations from a resource-rich lan-
guage to a target language via parallel corpora. In
order to relieve the bad influence of noisy projec-
tions, we focused on the strategies for reducing the
noise generated during the projection. We applied
our methods to the relation detection task in Ko-
rean. Experimental results show that the projected
instances from an English-Korean parallel corpus
help to improve the performance of the task when
our noise reduction strategies are adopted.
We would like to introduce our method to the
other subtask of relation extraction, which is re-
lation categorization. While relation detection is
a binary classification problem, relation catego-
rization can be solved by a classifier for multi-
ple classes. Since the fundamental approaches
of the two tasks are similar, we expect that our
projection-based relation detection methods can
be easily adapted to the relation categorization
task.
For this further work, we are concerned about
the problem of low performance for Korean,
which was below 40 for relation detection. The re-
lation categorization performance is mostly lower
than detection because of the larger number of
classes to be classified, so the performance of
projection-based approaches has to be improved
in order to apply them. An experimental result
of this work shows that the most important factor
in improving the performance is how to select the
reliable instances from a large number of projec-
tions. We plan to develop more elaborate strate-
gies for instance selection to improve the projec-
tion performance for relation extraction.
Acknowledgement
This research was supported by the MKE (The
Ministry of Knowledge Economy), Korea, un-
der the ITRC (Information Technology Research
Center) support program supervised by the NIPA
(National IT Industry Promotion Agency) (NIPA-
2010-C1090-1031-0009).
References
Bunescu, Razvan C. and Raymond J. Mooney. 2005.
A shortest path dependency kernel for relation ex-
traction. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, page 724731.
Chen, Jinxiu, Donghong Ji, Chew Lim Tan, and
Zhengyu Niu. 2006. Relation extraction using la-
bel propagation based semi-supervised learning. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 129?136, Sydney, Australia. Associ-
ation for Computational Linguistics.
Culotta, Aron and Jaffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In Pro-
ceedings of ACL, volume 4.
Doddington, George, Alexis Mitchell, Mark Przy-
bocki, Lance Ramshaw, Stephanie Strassel, and
Ralph Weischedel. 2004. The automatic content
extraction (ACE) programtasks, data, and evalua-
tion. In Proceedings of LREC, volume 4, page
837840.
Finkel, Jenny R., Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
volume 43, page 363.
Hwa, Rebecca, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural Language Engineering, 11(03):311?325.
570
Joachims, Thorsten. 1998. Text categorization with
support vector machines: Learning with many rele-
vant features. In Proceedings of the European Con-
ference on Machine Learning, pages 137?142.
Kambhatla, Nanda. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the ACL 2004 on Interactive poster and demonstra-
tion sessions, page 22, Barcelona, Spain. Associa-
tion for Computational Linguistics.
Kim, Hansaem. 2006. Korean national corpus in the
21st century sejong project. In Proceedings of the
13th NIJL International Symposium, page 4954.
Klein, Dan and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, pages 423?430, Sap-
poro, Japan. Association for Computational Lin-
guistics.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, vol-
ume 1, pages 48?54.
Lee, Gary Geunbae, Jeongwon Cha, and Jong-Hyeok
Lee. 2002. Syllable pattern-based unknown mor-
pheme segmentation and estimation for hybrid part-
of-speech tagging of korean. Computational Lin-
guistics, 28(1):53?70.
Merlo, Paola, Suzanne Stevenson, Vivian Tsang, and
Gianluca Allaria. 2002. A multilingual paradigm
for automatic verb classification. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 207?214, Philadelphia,
Pennsylvania. Association for Computational Lin-
guistics.
Moschitti, Alessandro. 2006. Making tree kernels
practical for natural language learning. In Proceed-
ings of EACL06.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51,
March.
Pado, Sebastian and Mirella Lapata. 2009.
Cross-lingual annotation projection of semantic
roles. Journal of Artificial Intelligence Research,
36(1):307340.
Yarowsky, David and Grace Ngai. 2001. Inducing
multilingual POS taggers and NP bracketers via ro-
bust projection across aligned corpora. In Second
meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics on Language
technologies 2001, pages 1?8, Pittsburgh, Pennsyl-
vania. Association for Computational Linguistics.
Yarowsky, David, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the first international conference
on Human language technology research, pages 1?
8, San Diego. Association for Computational Lin-
guistics.
Zelenko, Dmitry, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. J. Mach. Learn. Res., 3:1083?1106.
Zhang, Min, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A composite kernel to extract relations be-
tween entities with both flat and structured features.
In Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, pages 825?832, Sydney, Australia. Associ-
ation for Computational Linguistics.
Zhang, Zhu. 2004. Weakly-supervised relation clas-
sification for information extraction. In Proceed-
ings of the thirteenth ACM international conference
on Information and knowledge management, pages
581?588, Washington, D.C., USA. ACM.
Zhou, Guodong, Jian Su, Jie Zhang, and Min Zhang.
2005. Exploring various knowledge in relation ex-
traction. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, page
434.
Zitouni, Imed and Radu Florian. 2008. Mention detec-
tion crossing the language barrier. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 600?609, Honolulu,
Hawaii. Association for Computational Linguistics.
571
Proceedings of the ACL 2010 Conference Short Papers, pages 151?155,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Unsupervised Discourse Segmentation
of Documents with Inherently Parallel Structure
Minwoo Jeong and Ivan Titov
Saarland University
Saarbru?cken, Germany
{m.jeong|titov}@mmci.uni-saarland.de
Abstract
Documents often have inherently parallel
structure: they may consist of a text and
commentaries, or an abstract and a body,
or parts presenting alternative views on
the same problem. Revealing relations be-
tween the parts by jointly segmenting and
predicting links between the segments,
would help to visualize such documents
and construct friendlier user interfaces. To
address this problem, we propose an un-
supervised Bayesian model for joint dis-
course segmentation and alignment. We
apply our method to the ?English as a sec-
ond language? podcast dataset where each
episode is composed of two parallel parts:
a story and an explanatory lecture. The
predicted topical links uncover hidden re-
lations between the stories and the lec-
tures. In this domain, our method achieves
competitive results, rivaling those of a pre-
viously proposed supervised technique.
1 Introduction
Many documents consist of parts exhibiting a high
degree of parallelism: e.g., abstract and body of
academic publications, summaries and detailed
news stories, etc. This is especially common with
the emergence of the Web 2.0 technologies: many
texts on the web are now accompanied with com-
ments and discussions. Segmentation of these par-
allel parts into coherent fragments and discovery
of hidden relations between them would facilitate
the development of better user interfaces and im-
prove the performance of summarization and in-
formation retrieval systems.
Discourse segmentation of the documents com-
posed of parallel parts is a novel and challeng-
ing problem, as previous research has mostly fo-
cused on the linear segmentation of isolated texts
(e.g., (Hearst, 1994)). The most straightforward
approach would be to use a pipeline strategy,
where an existing segmentation algorithm finds
discourse boundaries of each part independently,
and then the segments are aligned. Or, conversely,
a sentence-alignment stage can be followed by a
segmentation stage. However, as we will see in our
experiments, these strategies may result in poor
segmentation and alignment quality.
To address this problem, we construct a non-
parametric Bayesian model for joint segmenta-
tion and alignment of parallel parts. In com-
parison with the discussed pipeline approaches,
our method has two important advantages: (1) it
leverages the lexical cohesion phenomenon (Hal-
liday and Hasan, 1976) in modeling the paral-
lel parts of documents, and (2) ensures that the
effective number of segments can grow adap-
tively. Lexical cohesion is an idea that topically-
coherent segments display compact lexical distri-
butions (Hearst, 1994; Utiyama and Isahara, 2001;
Eisenstein and Barzilay, 2008). We hypothesize
that not only isolated fragments but also each
group of linked fragments displays a compact and
consistent lexical distribution, and our generative
model leverages this inter-part cohesion assump-
tion.
In this paper, we consider the dataset of ?En-
glish as a second language? (ESL) podcast1, where
each episode consists of two parallel parts: a story
(an example monologue or dialogue) and an ex-
planatory lecture discussing the meaning and us-
age of English expressions appearing in the story.
Fig. 1 presents an example episode, consisting of
two parallel parts, and their hidden topical rela-
tions.2 From the figure we may conclude that there
is a tendency of word repetition between each pair
of aligned segments, illustrating our hypothesis of
compactness of their joint distribution. Our goal is
1http://www.eslpod.com/
2Episode no. 232 post on Jan. 08, 2007.
151
I have a day job, but I recently started a small business on the side.
I didn't know anything about accounting and my friend, Roland, said that he would give me some advice.
Roland: So, the reason that you need to do your bookkeeping is so you can manage your cash flow.
This podcast is all about business vocabulary related to accounting. The title of the podcast is Business Bookkeeping. ... The story begins by Magdalena saying that she has a day job. A day job is your regular job that you work at from nine in the morning 'til five in the afternoon, for        example. She also has a small business on the side. ... Magdalena continues by saying that she didn't know anything about accounting and her friend,      Roland, said he would give her some advice. Accounting is the job of keeping correct records of the money you spend; it's very similar to      bookkeeping. ... Roland begins by saying that the reason that you need to do your bookkeeping is so you can       manage your cash flow. Cash flow, flow, means having enough money to run your business - to pay your bills. ... ...
Story Lecture transcript
...
Figure 1: An example episode of ESL podcast. Co-occurred words are represented in italic and underline.
to divide the lecture transcript into discourse units
and to align each unit to the related segment of the
story. Predicting these structures for the ESL pod-
cast could be the first step in development of an
e-learning system and a podcast search engine for
ESL learners.
2 Related Work
Discourse segmentation has been an active area
of research (Hearst, 1994; Utiyama and Isahara,
2001; Galley et al, 2003; Malioutov and Barzilay,
2006). Our work extends the Bayesian segmenta-
tion model (Eisenstein and Barzilay, 2008) for iso-
lated texts, to the problem of segmenting parallel
parts of documents.
The task of aligning each sentence of an abstract
to one or more sentences of the body has been
studied in the context of summarization (Marcu,
1999; Jing, 2002; Daume? and Marcu, 2004). Our
work is different in that we do not try to extract
the most relevant sentence but rather aim to find
coherent fragments with maximally overlapping
lexical distributions. Similarly, the query-focused
summarization (e.g., (Daume? and Marcu, 2006))
is also related but it focuses on sentence extraction
rather than on joint segmentation.
We are aware of only one previous work on joint
segmentation and alignment of multiple texts (Sun
et al, 2007) but their approach is based on similar-
ity functions rather than on modeling lexical cohe-
sion in the generative framework. Our application,
the analysis of the ESL podcast, was previously
studied in (Noh et al, 2010). They proposed a su-
pervised method which is driven by pairwise clas-
sification decisions. The main drawback of their
approach is that it neglects the discourse structure
and the lexical cohesion phenomenon.
3 Model
In this section we describe our model for discourse
segmentation of documents with inherently paral-
lel structure. We start by clarifying our assump-
tions about their structure.
We assume that a document x consists of K
parallel parts, that is, x = {x(k)}k=1:K , and
each part of the document consists of segments,
x(k) = {s(k)i }i=1:I . Note that the effective num-
ber of fragments I is unknown. Each segment can
either be specific to this part (drawn from a part-
specific language model ?(k)i ) or correspond to
the entire document (drawn from a document-level
language model ?(doc)i ). For example, the first
and the second sentences of the lecture transcript
in Fig. 1 are part-specific, whereas other linked
sentences belong to the document-level segments.
The document-level language models define top-
ical links between segments in different parts of
the document, whereas the part-specific language
models define the linear segmentation of the re-
maining unaligned text.
Each document-level language model corre-
sponds to the set of aligned segments, at most one
segment per part. Similarly, each part-specific lan-
guage model corresponds to a single segment of
the single corresponding part. Note that all the
documents are modeled independently, as we aim
not to discover collection-level topics (as e.g. in
(Blei et al, 2003)), but to perform joint discourse
segmentation and alignment.
Unlike (Eisenstein and Barzilay, 2008), we can-
not make an assumption that the number of seg-
ments is known a-priori, as the effective number of
part-specific segments can vary significantly from
document to document, depending on their size
and structure. To tackle this problem, we use
Dirichlet processes (DP) (Ferguson, 1973) to de-
152
fine priors on the number of segments. We incor-
porate them in our model in a similar way as it
is done for the Latent Dirichlet Allocation (LDA)
by Yu et al (2005). Unlike the standard LDA, the
topic proportions are chosen not from a Dirichlet
prior but from the marginal distribution GEM(?)
defined by the stick breaking construction (Sethu-
raman, 1994), where ? is the concentration param-
eter of the underlying DP distribution. GEM(?)
defines a distribution of partitions of the unit inter-
val into a countable number of parts.
The formal definition of our model is as follows:
? Draw the document-level topic proportions ?(doc) ?
GEM(?(doc)).
? Choose the document-level language model ?(doc)i ?
Dir(?(doc)) for i ? {1, 2, . . .}.
? Draw the part-specific topic proportions ?(k) ?
GEM(?(k)) for k ? {1, . . . ,K}.
? Choose the part-specific language models ?(k)i ?
Dir(?(k)) for k ? {1, . . . ,K} and i ? {1, 2, . . .}.
? For each part k and each sentence n:
? Draw type t(k)n ? Unif(Doc, Part).
? If (t(k)n = Doc); draw topic z
(k)
n ? ?(doc); gen-
erate words x(k)n ?Mult(?
(doc)
z(k)n
)
? Otherwise; draw topic z(k)n ? ?(k); generate
words x(k)n ?Mult(?
(k)
z(k)n
).
The priors ?(doc), ?(k), ?(doc) and ?(k) can be
estimated at learning time using non-informative
hyperpriors (as we do in our experiments), or set
manually to indicate preferences of segmentation
granularity.
At inference time, we enforce each latent topic
z(k)n to be assigned to a contiguous span of text,
assuming that coherent topics are not recurring
across the document (Halliday and Hasan, 1976).
It also reduces the search space and, consequently,
speeds up our sampling-based inference by reduc-
ing the time needed for Monte Carlo chains to
mix. In fact, this constraint can be integrated in the
model definition but it would significantly compli-
cate the model description.
4 Inference
As exact inference is intractable, we follow Eisen-
stein and Barzilay (2008) and instead use a
Metropolis-Hastings (MH) algorithm. At each
iteration of the MH algorithm, a new potential
alignment-segmentation pair (z?, t?) is drawn from
a proposal distribution Q(z?, t?|z, t), where (z, t)
(a) (b) (c)
Figure 2: Three types of moves: (a) shift, (b) split
and (c) merge.
is the current segmentation and its type. The new
pair (z?, t?) is accepted with the probability
min
(
1,
P (z?, t?,x)Q(z?, t?|z, t)
P (z, t,x)Q(z, t|z?, t?)
)
.
In order to implement the MH algorithm for our
model, we need to define the set of potential moves
(i.e. admissible changes from (z, t) to (z?, t?)),
and the proposal distribution Q over these moves.
If the actual number of segments is known and
only a linear discourse structure is acceptable, then
a single move, shift of the segment border (Fig.
2(a)), is sufficient (Eisenstein and Barzilay, 2008).
In our case, however, a more complex set of moves
is required.
We make two assumptions which are moti-
vated by the problem considered in Section 5:
we assume that (1) we are given the number of
document-level segments and also that (2) the
aligned segments appear in the same order in each
part of the document. With these assumptions in
mind, we introduce two additional moves (Fig.
2(b) and (c)):
? Split move: select a segment, and split it at
one of the spanned sentences; if the segment
was a document-level segment then one of
the fragments becomes the same document-
level segment.
? Merge move: select a pair of adjacent seg-
ments where at least one of the segments is
part-specific, and merge them; if one of them
was a document-level segment then the new
segment has the same document-level topic.
All the moves are selected with the uniform prob-
ability, and the distance c for the shift move is
drawn from the proposal distribution proportional
to c?1/cmax . The moves are selected indepen-
dently for each part.
Although the above two assumptions are not
crucial as a simple modification to the set of moves
would support both introduction and deletion of
document-level fragments, this modification was
not necessary for our experiments.
153
5 Experiment
5.1 Dataset and setup
Dataset We apply our model to the ESL podcast
dataset (Noh et al, 2010) of 200 episodes, with
an average of 17 sentences per story and 80 sen-
tences per lecture transcript. The gold standard
alignments assign each fragment of the story to a
segment of the lecture transcript. We can induce
segmentations at different levels of granularity on
both the story and the lecture side. However, given
that the segmentation of the story was obtained by
an automatic sentence splitter, there is no reason
to attempt to reproduce this segmentation. There-
fore, for quantitative evaluation purposes we fol-
low Noh et al (2010) and restrict our model to
alignment structures which agree with the given
segmentation of the story. For all evaluations, we
apply standard stemming algorithm and remove
common stop words.
Evaluationmetrics To measure the quality of seg-
mentation of the lecture transcript, we use two
standard metrics, Pk (Beeferman et al, 1999) and
WindowDiff (WD) (Pevzner and Hearst, 2002),
but both metrics disregard the alignment links (i.e.
the topic labels). Consequently, we also use the
macro-averaged F1 score on pairs of aligned span,
which measures both the segmentation and align-
ment quality.
Baseline Since there has been little previous re-
search on this problem, we compare our results
against two straightforward unsupervised base-
lines. For the first baseline, we consider the
pairwise sentence alignment (SentAlign) based
on the unigram and bigram overlap. The sec-
ond baseline is a pipeline approach (Pipeline),
where we first segment the lecture transcript with
BayesSeg (Eisenstein and Barzilay, 2008) and
then use the pairwise alignment to find their best
alignment to the segments of the story.
Our model We evaluate our joint model of seg-
mentation and alignment both with and without
the split/merge moves. For the model without
these moves, we set the desired number of seg-
ments in the lecture to be equal to the actual num-
ber of segments in the story I . In this setting,
the moves can only adjust positions of the seg-
ment borders. For the model with the split/merge
moves, we start with the same number of segments
I but it can be increased or decreased during in-
ference. For evaluation of our model, we run our
inference algorithm from five random states, and
Method Pk WD 1? F1
Uniform 0.453 0.458 0.682
SentAlign 0.446 0.547 0.313
Pipeline (I) 0.250 0.249 0.443
Pipeline (2I+1) 0.268 0.289 0.318
Our model (I) 0.193 0.204 0.254
+split/merge 0.181 0.193 0.239
Table 1: Results on the ESL podcast dataset. For
all metrics, lower values are better.
take the 100,000th iteration of each chain as a sam-
ple. Results are the average over these five runs.
Also we perform L-BFGS optimization to auto-
matically adjust the non-informative hyperpriors
after each 1,000 iterations of sampling.
5.2 Result
Table 1 summarizes the obtained results. ?Uni-
form? denotes the minimal baseline which uni-
formly draws a random set of I spans for each lec-
ture, and then aligns them to the segments of the
story preserving the linear order. Also, we con-
sider two variants of the pipeline approach: seg-
menting the lecture on I and 2I + 1 segments, re-
spectively.3 Our joint model substantially outper-
forms the baselines. The difference is statistically
significant with the level p < .01 measured with
the paired t-test. The significant improvement over
the pipeline results demonstrates benefits of joint
modeling for the considered problem. Moreover,
additional benefits are obtained by using the DP
priors and the split/merge moves (the last line in
Table 1). Finally, our model significantly outper-
forms the previously proposed supervised model
(Noh et al, 2010): they report micro-averaged F1
score 0.698 while our best model achieves 0.778
with the same metric. This observation confirms
that lexical cohesion modeling is crucial for suc-
cessful discourse analysis.
6 Conclusions
We studied the problem of joint discourse segmen-
tation and alignment of documents with inherently
parallel structure and achieved favorable results on
the ESL podcast dataset outperforming the cas-
caded baselines. Accurate prediction of these hid-
den relations would open interesting possibilities
3The use of the DP priors and the split/merge moves on
the first stage of the pipeline did not result in any improve-
ment in accuracy.
154
for construction of friendlier user interfaces. One
example being an application which, given a user-
selected fragment of the abstract, produces a sum-
mary from the aligned segment of the document
body.
Acknowledgment
The authors acknowledge the support of the
Excellence Cluster on Multimodal Computing
and Interaction (MMCI), and also thank Mikhail
Kozhevnikov and the anonymous reviewers for
their valuable comments, and Hyungjong Noh for
providing their data.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation.
Computational Linguistics, 34(1?3):177?210.
David M. Blei, Andrew Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. JMLR, 3:993?
1022.
Hal Daume? and Daniel Marcu. 2004. A phrase-based
hmm approach to document/abstract alignment. In
Proceedings of EMNLP, pages 137?144.
Hal Daume? and Daniel Marcu. 2006. Bayesian query-
focused summarization. In Proceedings of ACL,
pages 305?312.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Proceedings of
EMNLP, pages 334?343.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some non-parametric problems. Annals of Statistics,
1:209?230.
Michel Galley, Kathleen R. McKeown, Eric Fosler-
Lussier, and Hongyan Jing. 2003. Discourse seg-
mentation of multi-party conversation. In Proceed-
ings of ACL, pages 562?569.
M. A. K. Halliday and Ruqaiya Hasan. 1976. Cohe-
sion in English. Longman.
Marti Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of ACL, pages 9?16.
Hongyan Jing. 2002. Using hidden Markov modeling
to decompose human-written summaries. Computa-
tional Linguistics, 28(4):527?543.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In Pro-
ceedings of ACL, pages 25?32.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research. In
Proceedings of ACM SIGIR, pages 137?144.
Hyungjong Noh, Minwoo Jeong, Sungjin Lee,
Jonghoon Lee, and Gary Geunbae Lee. 2010.
Script-description pair extraction from text docu-
ments of English as second language podcast. In
Proceedings of the 2nd International Conference on
Computer Supported Education.
Lev Pevzner and Marti Hearst. 2002. A critique and
improvement of an evaluation metric for text seg-
mentation. Computational Linguistics, 28(1):19?
36.
Jayaram Sethuraman. 1994. A constructive definition
of Dirichlet priors. Statistica Sinica, 4:639?650.
Bingjun Sun, Prasenjit Mitra, C. Lee Giles, John Yen,
and Hongyuan Zha. 2007. Topic segmentation
with shared topic detection and alignment of mul-
tiple documents. In Proceedings of ACM SIGIR,
pages 199?206.
Masao Utiyama and Hitoshi Isahara. 2001. A statis-
tical model for domain-independent text segmenta-
tion. In Proceedings of ACL, pages 491?498.
Kai Yu, Shipeng Yu, and Vokler Tresp. 2005. Dirichlet
enhanced latent semantic analysis. In Proceedings
of AISTATS.
155
