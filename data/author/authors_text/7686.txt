TIPS: A Translingual Information Processing System
Y. Al-Onaizan, R. Florian, M. Franz, H. Hassan, Y. S. Lee, S. McCarley, K.
Papineni, S. Roukos, J. Sorensen, C. Tillmann, T. Ward, F. Xia
IBM T. J. Watson Research Center
Yorktown Heights
Abstract
Searching online information is
increasingly a daily activity for many
people. The multilinguality of online
content is also increasing (e.g. the
proportion of English web users, which
has been decreasing as a fraction the
increasing population of web users, dipped
below 50% in the summer of 2001). To
improve the ability of an English speaker
to search mutlilingual content, we built a
system that supports cross-lingual search
of an Arabic newswire collection and
provides on demand translation of Arabic
web pages into English. The cross-lingual
search engine supports a fast search
capability (sub-second response for typical
queries) and achieves state-of-the-art
performance in the high precision region
of the result list. The on demand statistical
machine translation uses the Direct
Translation model along with a novel
statistical Arabic Morphological Analyzer
to yield state-of-the-art translation quality.
The on demand SMT uses an efficient
dynamic programming decoder that
achieves reasonable speed for translating
web documents.
Overview
Morphologically rich languages like Arabic
(Beesley, K. 1996) present significant challenges
to many natural language processing applications
as the one described above because a word often
conveys complex meanings decomposable into
several morphemes (i.e. prefix, stem, suffix). By
segmenting words into morphemes, we can
improve the performance of natural language
systems including machine translation (Brown et
al. 1993) and information retrieval (Franz, M.
and McCarley, S. 2002). In this paper, we
present a cross-lingual English-Arabic search
engine combined with an on demand Arabic-
English statistical machine translation system
that relies on source language analysis for both
improved search and translation. We developed
novel statistical learning algorithms for
performing Arabic word segmentation (Lee, Y.
et al2003) into morphemes and morphological
source language (Arabic) analysis (Lee, Y. et al
2003b). These components improve both mono-
lingual (Arabic) search and cross-lingual
(English-Arabic) search and machine
translation. In addition, the system supports
either document translation or convolutional
models for cross-lingual search (Franz, M. and
McCarley, S. 2002).
The overall demonstration has the following
major components:
1. Mono-lingual search: uses Arabic word
segmentation and an okapi-like search
engine for document ranking.
2. Cross-lingual search: uses Arabic word
segmentation and morphological
analysis along with a statistical
morpheme translation matrix in a
convolutional model for document
ranking. The search can also use
document translation into English to
rank the Arabic documents. Both
approaches achieve similar precision in
the high precision region of retrieval.
The English query is also
morphologically analyzed to improve
performance.
3. OnDemand statistical machine
translation: this component uses both
analysis components along with a direct
channel translation model with a fast
dynamic programming decoder
(Tillmann, C. 2003). This system
                                                               Edmonton, May-June 2003
                                                              Demonstrations , pp. 1-2
                                                         Proceedings of HLT-NAACL 2003
achieves state-of-the-art Arabic-English
translation quality.
4. Arabic named entity detection and
translation: we have 31 categories of
Named Entities (Person, Organization,
etc.) that we detect and highlight in
Arabic text and provide the translation
of these entities into English. The
highlighted named entities help the user
to quickly assess the relevance of a
document.
All of the above functionality is available
through a web browser. We indexed the Arabic
AFP corpus about 330k documents for the
demonstration. The resulting search engine
supports sub-second query response. We also
provide an html detagging capability that allows
the translation of Arabic web pages while trying
to preserve the original layout as much as
possible in the on demand SMT component. The
Arabic Name Entity Tagger is currently run as an
offline process but we expect to have it online by
the demonstration time. We aslo include two
screen shots of the demonstration system.
Acknowledgments
This work was partially supported by the
Defense Advanced Research Projects Agency
and monitored by SPAWAR under contract No.
N66001-99-2-8916. The views and findings
contained in this material are those of the authors
and do not necessarily reflect the position of
policy of the Government and no official
endorsement should be inferred.
References
Beesley, K. 1996. Arabic Finite-State
Morphological Analysis and Generation.
Proceedings of COLING-96, pages 89? 94.
Brown, P., Della Pietra, S., Della Pietra, V., and
Mercer, R. 1993. The mathematics of statistical
machine translation: Parameter Estimation.
Computational Linguistics, 19(2): 263?311.
Franz, M. and McCarley, S. 2002. Arabic
Information Retrieval at IBM. Proceedings
of TREC 2002, pages 402?405.
Lee, Y., Papineni, K., Roukos, S.,
Emam, O., and Hassan, H. 2003. Language
Model Based Arabic Word Segmentation.
Submitted for publication.
Lee, Y., Papineni, K., Roukos, S., Emam,
O., and Hassan, H. 2003b. Automatic
Induction of Morphological Analysis for
Statistical Machine Translation. Manuscript in
preparation.
Tillmann, C., 2003. Word Reordering and a
DP Beam Search Algorithm for Statistical
Machine Translation. Computational
Linguistics, 29(1): 97-133.
Dependency Tree Kernels for Relation Extraction
Aron Culotta
University of Massachusetts
Amherst, MA 01002
USA
culotta@cs.umass.edu
Jeffrey Sorensen
IBM T.J. Watson Research Center
Yorktown Heights, NY 10598
USA
sorenj@us.ibm.com
Abstract
We extend previous work on tree kernels to estimate
the similarity between the dependency trees of sen-
tences. Using this kernel within a Support Vector
Machine, we detect and classify relations between
entities in the Automatic Content Extraction (ACE)
corpus of news articles. We examine the utility of
different features such as Wordnet hypernyms, parts
of speech, and entity types, and find that the depen-
dency tree kernel achieves a 20% F1 improvement
over a ?bag-of-words? kernel.
1 Introduction
The ability to detect complex patterns in data is lim-
ited by the complexity of the data?s representation.
In the case of text, a more structured data source
(e.g. a relational database) allows richer queries
than does an unstructured data source (e.g. a col-
lection of news articles). For example, current web
search engines would not perform well on the query,
?list all California-based CEOs who have social ties
with a United States Senator.? Only a structured
representation of the data can effectively provide
such a list.
The goal of Information Extraction (IE) is to dis-
cover relevant segments of information in a data
stream that will be useful for structuring the data.
In the case of text, this usually amounts to finding
mentions of interesting entities and the relations that
join them, transforming a large corpus of unstruc-
tured text into a relational database with entries such
as those in Table 1.
IE is commonly viewed as a three stage process:
first, an entity tagger detects all mentions of interest;
second, coreference resolution resolves disparate
mentions of the same entity; third, a relation extrac-
tor finds relations between these entities. Entity tag-
ging has been thoroughly addressed by many statis-
tical machine learning techniques, obtaining greater
than 90% F1 on many datasets (Tjong Kim Sang
and De Meulder, 2003). Coreference resolution is
an active area of research not investigated here (Pa-
Entity Type Location
Apple Organization Cupertino, CA
Microsoft Organization Redmond, WA
Table 1: An example of extracted fields
sula et al, 2002; McCallum and Wellner, 2003).
We describe a relation extraction technique based
on kernel methods. Kernel methods are non-
parametric density estimation techniques that com-
pute a kernel function between data instances,
where a kernel function can be thought of as a sim-
ilarity measure. Given a set of labeled instances,
kernel methods determine the label of a novel in-
stance by comparing it to the labeled training in-
stances using this kernel function. Nearest neighbor
classification and support-vector machines (SVMs)
are two popular examples of kernel methods (Fuku-
naga, 1990; Cortes and Vapnik, 1995).
An advantage of kernel methods is that they can
search a feature space much larger than could be
represented by a feature extraction-based approach.
This is possible because the kernel function can ex-
plore an implicit feature space when calculating the
similarity between two instances, as described in the
Section 3.
Working in such a large feature space can lead to
over-fitting in many machine learning algorithms.
To address this problem, we apply SVMs to the task
of relation extraction. SVMs find a boundary be-
tween instances of different classes such that the
distance between the boundary and the nearest in-
stances is maximized. This characteristic, in addi-
tion to empirical validation, indicates that SVMs are
particularly robust to over-fitting.
Here we are interested in detecting and classify-
ing instances of relations, where a relation is some
meaningful connection between two entities (Table
2). We represent each relation instance as an aug-
mented dependency tree. A dependency tree repre-
sents the grammatical dependencies in a sentence;
we augment this tree with features for each node
AT NEAR PART ROLE SOCIAL
Based-In Relative-location Part-of Affiliate, Founder Associate, Grandparent
Located Subsidiary Citizen-of, Management Parent, Sibling
Residence Other Client, Member Spouse, Other-professional
Owner, Other, Staff Other-relative, Other-personal
Table 2: Relation types and subtypes.
(e.g. part of speech) We choose this representation
because we hypothesize that instances containing
similar relations will share similar substructures in
their dependency trees. The task of the kernel func-
tion is to find these similarities.
We define a tree kernel over dependency trees and
incorporate this kernel within an SVM to extract
relations from newswire documents. The tree ker-
nel approach consistently outperforms the bag-of-
words kernel, suggesting that this highly-structured
representation of sentences is more informative for
detecting and distinguishing relations.
2 Related Work
Kernel methods (Vapnik, 1998; Cristianini and
Shawe-Taylor, 2000) have become increasingly
popular because of their ability to map arbitrary ob-
jects to a Euclidian feature space. Haussler (1999)
describes a framework for calculating kernels over
discrete structures such as strings and trees. String
kernels for text classification are explored in Lodhi
et al (2000), and tree kernel variants are described
in (Zelenko et al, 2003; Collins and Duffy, 2002;
Cumby and Roth, 2003). Our algorithm is similar
to that described by Zelenko et al (2003). Our
contributions are a richer sentence representation, a
more general framework to allow feature weighting,
as well as the use of composite kernels to reduce
kernel sparsity.
Brin (1998) and Agichtein and Gravano (2000)
apply pattern matching and wrapper techniques for
relation extraction, but these approaches do not
scale well to fastly evolving corpora. Miller et al
(2000) propose an integrated statistical parsing tech-
nique that augments parse trees with semantic la-
bels denoting entity and relation types. Whereas
Miller et al (2000) use a generative model to pro-
duce parse information as well as relation informa-
tion, we hypothesize that a technique discrimina-
tively trained to classify relations will achieve bet-
ter performance. Also, Roth and Yih (2002) learn a
Bayesian network to tag entities and their relations
simultaneously. We experiment with a more chal-
lenging set of relation types and a larger corpus.
3 Kernel Methods
In traditional machine learning, we are provided
a set of training instances S = {x1 . . . xN},
where each instance xi is represented by some d-
dimensional feature vector. Much time is spent on
the task of feature engineering ? searching for the
optimal feature set either manually by consulting
domain experts or automatically through feature in-
duction and selection (Scott and Matwin, 1999).
For example, in entity detection the original in-
stance representation is generally a word vector cor-
responding to a sentence. Feature extraction and
induction may result in features such as part-of-
speech, word n-grams, character n-grams, capital-
ization, and conjunctions of these features. In the
case of more structured objects, such as parse trees,
features may include some description of the ob-
ject?s structure, such as ?has an NP-VP subtree.?
Kernel methods can be particularly effective at re-
ducing the feature engineering burden for structured
objects. By calculating the similarity between two
objects, kernel methods can employ dynamic pro-
gramming solutions to efficiently enumerate over
substructures that would be too costly to explicitly
include as features.
Formally, a kernel function K is a mapping
K : X ? X ? [0,?] from instance space X
to a similarity score K(x, y) =
?
i ?i(x)?i(y) =
?(x) ? ?(y). Here, ?i(x) is some feature func-
tion over the instance x. The kernel function must
be symmetric [K(x, y) = K(y, x)] and positive-
semidefinite. By positive-semidefinite, we require
that the if x1, . . . , xn ? X, then the n ? n matrix
G defined by Gij = K(xi, xj) is positive semi-
definite. It has been shown that any function that
takes the dot product of feature vectors is a kernel
function (Haussler, 1999).
A simple kernel function takes the dot product of
the vector representation of instances being com-
pared. For example, in document classification,
each document can be represented by a binary vec-
tor, where each element corresponds to the presence
or absence of a particular word in that document.
Here, ?i(x) = 1 if word i occurs in document x.
Thus, the kernel function K(x, y) returns the num-
ber of words in common between x and y. We refer
to this kernel as the ?bag-of-words? kernel, since it
ignores word order.
When instances are more structured, as in the
case of dependency trees, more complex kernels
become necessary. Haussler (1999) describes con-
volution kernels, which find the similarity between
two structures by summing the similarity of their
substructures. As an example, consider a kernel
over strings. To determine the similarity between
two strings, string kernels (Lodhi et al, 2000) count
the number of common subsequences in the two
strings, and weight these matches by their length.
Thus, ?i(x) is the number of times string x contains
the subsequence referenced by i. These matches can
be found efficiently through a dynamic program,
allowing string kernels to examine long-range fea-
tures that would be computationally infeasible in a
feature-based method.
Given a training set S = {x1 . . . xN}, kernel
methods compute the Gram matrix G such that
Gij = K(xi, xj). Given G, the classifier finds a
hyperplane which separates instances of different
classes. To classify an unseen instance x, the classi-
fier first projects x into the feature space defined by
the kernel function. Classification then consists of
determining on which side of the separating hyper-
plane x lies.
A support vector machine (SVM) is a type of
classifier that formulates the task of finding the sep-
arating hyperplane as the solution to a quadratic pro-
gramming problem (Cristianini and Shawe-Taylor,
2000). Support vector machines attempt to find a
hyperplane that not only separates the classes but
also maximizes the margin between them. The hope
is that this will lead to better generalization perfor-
mance on unseen instances.
4 Augmented Dependency Trees
Our task is to detect and classify relations between
entities in text. We assume that entity tagging has
been performed; so to generate potential relation
instances, we iterate over all pairs of entities oc-
curring in the same sentence. For each entity pair,
we create an augmented dependency tree (described
below) representing this instance. Given a labeled
training set of potential relations, we define a tree
kernel over dependency trees which we then use in
an SVM to classify test instances.
A dependency tree is a representation that de-
notes grammatical relations between words in a sen-
tence (Figure 1). A set of rules maps a parse tree to
a dependency tree. For example, subjects are de-
pendent on their verbs and adjectives are dependent
Troops
Tikrit
advanced
near
t
t
t
t 0
1 2
3
Figure 1: A dependency tree for the sentence
Troops advanced near Tikrit.
Feature Example
word troops, Tikrit
part-of-speech (24 values) NN, NNP
general-pos (5 values) noun, verb, adj
chunk-tag NP, VP, ADJP
entity-type person, geo-political-entity
entity-level name, nominal, pronoun
Wordnet hypernyms social group, city
relation-argument ARG A, ARG B
Table 3: List of features assigned to each node in
the dependency tree.
on the nouns they modify. Note that for the pur-
poses of this paper, we do not consider the link la-
bels (e.g. ?object?, ?subject?); instead we use only
the dependency structure. To generate the parse tree
of each sentence, we use MXPOST, a maximum en-
tropy statistical parser1; we then convert this parse
tree to a dependency tree. Note that the left-to-right
ordering of the sentence is maintained in the depen-
dency tree only among siblings (i.e. the dependency
tree does not specify an order to traverse the tree to
recover the original sentence).
For each pair of entities in a sentence, we find
the smallest common subtree in the dependency tree
that includes both entities. We choose to use this
subtree instead of the entire tree to reduce noise
and emphasize the local characteristics of relations.
We then augment each node of the tree with a fea-
ture vector (Table 3). The relation-argument feature
specifies whether an entity is the first or second ar-
gument in a relation. This is required to learn asym-
metric relations (e.g. X OWNS Y).
Formally, a relation instance is a dependency tree
1http://www.cis.upenn.edu/?adwait/statnlp.html
T with nodes {t0 . . . tn}. The features of node ti
are given by ?(ti) = {v1 . . . vd}. We refer to the
jth child of node ti as ti[j], and we denote the set
of all children of node ti as ti[c]. We reference a
subset j of children of ti by ti[j] ? ti[c]. Finally, we
refer to the parent of node ti as ti.p.
From the example in Figure 1, t0[1] = t2,
t0[{0, 1}] = {t1, t2}, and t1.p = t0.
5 Tree kernels for dependency trees
We now define a kernel function for dependency
trees. The tree kernel is a function K(T1, T2) that
returns a normalized, symmetric similarity score in
the range (0, 1) for two trees T1 and T2. We de-
fine a slightly more general version of the kernel
described by Zelenko et al (2003).
We first define two functions over the features of
tree nodes: a matching function m(ti, tj) ? {0, 1}
and a similarity function s(ti, tj) ? (0,?]. Let the
feature vector ?(ti) = {v1 . . . vd} consist of two
possibly overlapping subsets ?m(ti) ? ?(ti) and
?s(ti) ? ?(ti). We use ?m(ti) in the matching
function and ?s(ti) in the similarity function. We
define
m(ti, tj) =
{
1 if ?m(ti) = ?m(tj)
0 otherwise
and
s(ti, tj) =
?
vq??s(ti)
?
vr??s(tj)
C(vq, vr)
where C(vq, vr) is some compatibility function
between two feature values. For example, in the
simplest case where
C(vq, vr) =
{
1 if vq = vr
0 otherwise
s(ti, tj) returns the number of feature values in
common between feature vectors ?s(ti) and ?s(tj).
We can think of the distinction between functions
m(ti, tj) and s(ti, tj) as a way to discretize the sim-
ilarity between two nodes. If ?m(ti) 6= ?m(tj),
then we declare the two nodes completely dissimi-
lar. However, if ?m(ti) = ?m(tj), then we proceed
to compute the similarity s(ti, tj). Thus, restrict-
ing nodes by m(ti, tj) is a way to prune the search
space of matching subtrees, as shown below.
For two dependency trees T1, T2, with root nodes
r1 and r2, we define the tree kernel K(T1, T2) as
follows:
K(T1, T2) =
?
??
??
0 if m(r1, r2) = 0
s(r1, r2)+
Kc(r1[c], r2[c]) otherwise
where Kc is a kernel function over children. Let
a and b be sequences of indices such that a is a
sequence a1 ? a2 ? . . . ? an, and likewise for b.
Let d(a) = an ? a1 +1 and l(a) be the length of a.
Then we have Kc(ti[c], tj [c]) =
?
a,b,l(a)=l(b)
?d(a)?d(b)K(ti[a], tj [b])
The constant 0 < ? < 1 is a decay factor that
penalizes matching subsequences that are spread
out within the child sequences. See Zelenko et al
(2003) for a proof that K is kernel function.
Intuitively, whenever we find a pair of matching
nodes, we search for all matching subsequences of
the children of each node. A matching subsequence
of children is a sequence of children a and b such
that m(ai, bi) = 1 (?i < n). For each matching
pair of nodes (ai, bi) in a matching subsequence,
we accumulate the result of the similarity function
s(ai, bj) and then recursively search for matching
subsequences of their children ai[c], bj [c].
We implement two types of tree kernels. A
contiguous kernel only matches children subse-
quences that are uninterrupted by non-matching
nodes. Therefore, d(a) = l(a). A sparse tree ker-
nel, by contrast, allows non-matching nodes within
matching subsequences.
Figure 2 shows two relation instances, where
each node contains the original text plus the features
used for the matching function, ?m(ti) = {general-
pos, entity-type, relation-argument}. (?NA? de-
notes the feature is not present for this node.) The
contiguous kernel matches the following substruc-
tures: {t0[0], u0[0]}, {t0[2], u0[1]}, {t3[0], u2[0]}.
Because the sparse kernel allows non-contiguous
matching sequences, it matches an additional sub-
structure {t0[0, ?, 2], u0[0, ?, 1]}, where (?) indi-
cates an arbitrary number of non-matching nodes.
Zelenko et al (2003) have shown the contiguous
kernel to be computable in O(mn) and the sparse
kernel in O(mn3), where m and n are the number
of children in trees T1 and T2 respectively.
6 Experiments
We extract relations from the Automatic Content
Extraction (ACE) corpus provided by the National
Institute for Standards and Technology (NIST). The
personnoun
NANA
verb
ARG_Bgeo?political
1
0
troops
advanced
noun
Tikrit
ARG_A
personnoun
forces
NANA
verbmoved
NANA
preptoward
ARG_B
t
t
t t
t
1
0
2 3
4
geo?politicalnoun
Baghdad
quickly
adverbNANA
ARG_A
nearprepNANA
2
3
u
u
u
u
Figure 2: Two instances of the NEAR relation.
data consists of about 800 annotated text documents
gathered from various newspapers and broadcasts.
Five entities have been annotated (PERSON, ORGA-
NIZATION, GEO-POLITICAL ENTITY, LOCATION,
FACILITY), along with 24 types of relations (Table
2). As noted from the distribution of relationship
types in the training data (Figure 3), data imbalance
and sparsity are potential problems.
In addition to the contiguous and sparse tree
kernels, we also implement a bag-of-words ker-
nel, which treats the tree as a vector of features
over nodes, disregarding any structural informa-
tion. We also create composite kernels by combin-
ing the sparse and contiguous kernels with the bag-
of-words kernel. Joachims et al (2001) have shown
that given two kernels K1, K2, the composite ker-
nel K12(xi, xj) = K1(xi, xj)+K2(xi, xj) is also a
kernel. We find that this composite kernel improves
performance when the Gram matrix G is sparse (i.e.
our instances are far apart in the kernel space).
The features used to represent each node are
shown in Table 3. After initial experimentation,
the set of features we use in the matching func-
tion is ?m(ti) = {general-pos, entity-type, relation-
argument}, and the similarity function examines the
Figure 3: Distribution over relation types in train-
ing data.
remaining features.
In our experiments we tested the following five
kernels:
K0 = sparse kernel
K1 = contiguous kernel
K2 = bag-of-words kernel
K3 = K0 + K2
K4 = K1 + K2
We also experimented with the function C(vq, vr),
the compatibility function between two feature val-
ues. For example, we can increase the importance
of two nodes having the same Wordnet hypernym2.
If vq, vr are hypernym features, then we can define
C(vq, vr) =
{
? if vq = vr
0 otherwise
When ? > 1, we increase the similarity of
nodes that share a hypernym. We tested a num-
ber of weighting schemes, but did not obtain a set
of weights that produced consistent significant im-
provements. See Section 8 for alternate approaches
to setting C.
2http://www.cogsci.princeton.edu/?wn/
Avg. Prec. Avg. Rec. Avg. F1
K1 69.6 25.3 36.8
K2 47.0 10.0 14.2
K3 68.9 24.3 35.5
K4 70.3 26.3 38.0
Table 4: Kernel performance comparison.
Table 4 shows the results of each kernel within
an SVM. (We augment the LibSVM3 implementa-
tion to include our dependency tree kernel.) Note
that, although training was done over all 24 rela-
tion subtypes, we evaluate only over the 5 high-level
relation types. Thus, classifying a RESIDENCE re-
lation as a LOCATED relation is deemed correct4.
Note also that K0 is not included in Table 4 because
of burdensome computational time. Table 4 shows
that precision is adequate, but recall is low. This
is a result of the aforementioned class imbalance ?
very few of the training examples are relations, so
the classifier is less likely to identify a testing in-
stances as a relation. Because we treat every pair
of mentions in a sentence as a possible relation, our
training set contains fewer than 15% positive rela-
tion instances.
To remedy this, we retrain each SVMs for a bi-
nary classification task. Here, we detect, but do not
classify, relations. This allows us to combine all
positive relation instances into one class, which pro-
vides us more training samples to estimate the class
boundary. We then threshold our output to achieve
an optimal operating point. As seen in Table 5, this
method of relation detection outperforms that of the
multi-class classifier.
We then use these binary classifiers in a cascading
scheme as follows: First, we use the binary SVM
to detect possible relations. Then, we use the SVM
trained only on positive relation instances to classify
each predicted relation. These results are shown in
Table 6.
The first result of interest is that the sparse tree
kernel, K0, does not perform as well as the con-
tiguous tree kernel, K1. Suspecting that noise was
introduced by the non-matching nodes allowed in
the sparse tree kernel, we performed the experi-
ment with different values for the decay factor ? =
{.9, .5, .1}, but obtained no improvement.
The second result of interest is that all tree ker-
nels outperform the bag-of-words kernel, K2, most
noticeably in recall performance, implying that the
3http://www.csie.ntu.edu.tw/?cjlin/libsvm/
4This is to compensate for the small amount of training data
for many classes.
Prec. Rec. F1
K0 ? ? ?
K0 (B) 83.4 45.5 58.8
K1 91.4 37.1 52.8
K1 (B) 84.7 49.3 62.3
K2 92.7 10.6 19.0
K2 (B) 72.5 40.2 51.7
K3 91.3 35.1 50.8
K3 (B) 80.1 49.9 61.5
K4 91.8 37.5 53.3
K4 (B) 81.2 51.8 63.2
Table 5: Relation detection performance. (B) de-
notes binary classification.
D C Avg. Prec. Avg. Rec. Avg. F1
K0 K0 66.0 29.0 40.1
K1 K1 66.6 32.4 43.5
K2 K2 62.5 27.7 38.1
K3 K3 67.5 34.3 45.3
K4 K4 67.1 35.0 45.8
K1 K4 67.4 33.9 45.0
K4 K1 65.3 32.5 43.3
Table 6: Results on the cascading classification. D
and C denote the kernel used for relation detection
and classification, respectively.
structural information the tree kernel provides is ex-
tremely useful for relation detection.
Note that the average results reported here are
representative of the performance per relation, ex-
cept for the NEAR relation, which had slightly lower
results overall due to its infrequency in training.
7 Conclusions
We have shown that using a dependency tree ker-
nel for relation extraction provides a vast improve-
ment over a bag-of-words kernel. While the de-
pendency tree kernel appears to perform well at the
task of classifying relations, recall is still relatively
low. Detecting relations is a difficult task for a ker-
nel method because the set of all non-relation in-
stances is extremely heterogeneous, and is therefore
difficult to characterize with a similarity metric. An
improved system might use a different method to
detect candidate relations and then use this kernel
method to classify the relations.
8 Future Work
The most immediate extension is to automatically
learn the feature compatibility function C(vq, vr).
A first approach might use tf-idf to weight each fea-
ture. Another approach might be to calculate the
information gain for each feature and use that as
its weight. A more complex system might learn a
weight for each pair of features; however this seems
computationally infeasible for large numbers of fea-
tures.
One could also perform latent semantic indexing
to collapse feature values into similar ?categories?
? for example, the words ?football? and ?baseball?
might fall into the same category. Here, C(vq, vr)
might return ?1 if vq = vr, and ?2 if vq and vr are
in the same category, where ?1 > ?2 > 0. Any
method that provides a ?soft? match between fea-
ture values will sharpen the granularity of the kernel
and enhance its modeling power.
Further investigation is also needed to understand
why the sparse kernel performs worse than the con-
tiguous kernel. These results contradict those given
in Zelenko et al (2003), where the sparse kernel
achieves 2-3% better F1 performance than the con-
tiguous kernel. It is worthwhile to characterize rela-
tion types that are better captured by the sparse ker-
nel, and to determine when using the sparse kernel
is worth the increased computational burden.
References
Eugene Agichtein and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text
collections. In Proceedings of the Fifth ACM In-
ternational Conference on Digital Libraries.
Sergey Brin. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extend-
ing Database Technology, EDBT?98.
M. Collins and N. Duffy. 2002. Convolution ker-
nels for natural language. In T. G. Dietterich,
S. Becker, and Z. Ghahramani, editors, Advances
in Neural Information Processing Systems 14,
Cambridge, MA. MIT Press.
Corinna Cortes and Vladimir Vapnik. 1995.
Support-vector networks. Machine Learning,
20(3):273?297.
N. Cristianini and J. Shawe-Taylor. 2000. An intro-
duction to support vector machines. Cambridge
University Press.
Chad M. Cumby and Dan Roth. 2003. On kernel
methods for relational learning. In Tom Fawcett
and Nina Mishra, editors, Machine Learning,
Proceedings of the Twentieth International Con-
ference (ICML 2003), August 21-24, 2003, Wash-
ington, DC, USA. AAAI Press.
K. Fukunaga. 1990. Introduction to Statistical Pat-
tern Recognition. Academic Press, second edi-
tion.
D. Haussler. 1999. Convolution kernels on dis-
crete structures. Technical Report UCS-CRL-99-
10, University of California, Santa Cruz.
Thorsten Joachims, Nello Cristianini, and John
Shawe-Taylor. 2001. Composite kernels for hy-
pertext categorisation. In Carla Brodley and An-
drea Danyluk, editors, Proceedings of ICML-
01, 18th International Conference on Machine
Learning, pages 250?257, Williams College, US.
Morgan Kaufmann Publishers, San Francisco,
US.
Huma Lodhi, John Shawe-Taylor, Nello Cristian-
ini, and Christopher J. C. H. Watkins. 2000. Text
classification using string kernels. In NIPS, pages
563?569.
A. McCallum and B. Wellner. 2003. Toward con-
ditional models of identity uncertainty with ap-
plication to proper noun coreference. In IJCAI
Workshop on Information Integration on the Web.
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel.
2000. A novel use of statistical parsing to ex-
tract information from text. In 6th Applied Nat-
ural Language Processing Conference.
H. Pasula, B. Marthi, B. Milch, S. Russell, and
I. Shpitser. 2002. Identity uncertainty and cita-
tion matching.
Dan Roth and Wen-tau Yih. 2002. Probabilistic
reasoning for entity and relation recognition. In
19th International Conference on Computational
Linguistics.
Sam Scott and Stan Matwin. 1999. Feature engi-
neering for text classification. In Proceedings of
ICML-99, 16th International Conference on Ma-
chine Learning.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 shared
task: Language-independent named entity recog-
nition. In Walter Daelemans and Miles Osborne,
editors, Proceedings of CoNLL-2003, pages 142?
147. Edmonton, Canada.
Vladimir Vapnik. 1998. Statistical Learning The-
ory. Whiley, Chichester, GB.
D. Zelenko, C. Aone, and A. Richardella. 2003.
Kernel methods for relation extraction. Jour-
nal of Machine Learning Research, pages 1083?
1106.
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 577?584,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Maximum Entropy Based Restoration of Arabic Diacritics
Imed Zitouni, Jeffrey S. Sorensen, Ruhi Sarikaya
IBM T.J. Watson Research Center
1101 Kitchawan Rd, Yorktown Heights, NY 10598
{izitouni, sorenj, sarikaya}@us.ibm.com
Abstract
Short vowels and other diacritics are not
part of written Arabic scripts. Exceptions
are made for important political and reli-
gious texts and in scripts for beginning stu-
dents of Arabic. Script without diacritics
have considerable ambiguity because many
words with different diacritic patterns ap-
pear identical in a diacritic-less setting. We
propose in this paper a maximum entropy
approach for restoring diacritics in a doc-
ument. The approach can easily integrate
and make effective use of diverse types of
information; the model we propose inte-
grates a wide array of lexical, segment-
based and part-of-speech tag features. The
combination of these feature types leads
to a state-of-the-art diacritization model.
Using a publicly available corpus (LDC?s
Arabic Treebank Part 3), we achieve a di-
acritic error rate of 5.1%, a segment error
rate 8.5%, and a word error rate of 17.3%.
In case-ending-less setting, we obtain a di-
acritic error rate of 2.2%, a segment error
rate 4.0%, and a word error rate of 7.2%.
1 Introduction
Modern Arabic written texts are composed of
scripts without short vowels and other diacritic
marks. This often leads to considerable ambigu-
ity since several words that have different diacritic
patterns may appear identical in a diacritic-less
setting. Educated modern Arabic speakers are able
to accurately restore diacritics in a document. This
is based on the context and their knowledge of the
grammar and the lexicon of Arabic. However, a
text without diacritics becomes a source of confu-
sion for beginning readers and people with learning
disabilities. A text without diacritics is also prob-
lematic for applications such as text-to-speech or
speech-to-text, where the lack of diacritics adds
another layer of ambiguity when processing the
data. As an example, full vocalization of text is
required for text-to-speech applications, where the
mapping from graphemes to phonemes is simple
compared to languages such as English and French;
where there is, in most cases, one-to-one relation-
ship. Also, using data with diacritics shows an
improvement in the accuracy of speech-recognition
applications (Afify et al, 2004). Currently, text-to-
speech, speech-to-text, and other applications use
data where diacritics are placed manually, which
is a tedious and time consuming excercise. A di-
acritization system that restores the diacritics of
scripts, i.e. supply the full diacritical markings,
would be of interest to these applications. It also
would greatly benefit nonnative speakers, sufferers
of dyslexia and could assist in restoring diacritics
of children?s and poetry books, a task that is cur-
rently done manually.
We propose in this paper a statistical approach
that restores diacritics in a text document. The
proposed approach is based on the maximum en-
tropy framework where several diverse sources of
information are employed. The model implicitly
learns the correlation between these types of infor-
mation and the output diacritics.
In the next section, we present the set of diacrit-
ics to be restored and the ambiguity we face when
processing a non-diacritized text. Section 3 gives
a brief summary of previous related works. Sec-
tion 4 presents our diacritization model; we ex-
plain the training and decoding process as well as
the different feature categories employed to restore
the diacritics. Section 5 describes a clearly defined
and replicable split of the LDC?s Arabic Treebank
Part 3 corpus, used to built and evaluate the sys-
tem, so that the reproduction of the results and
future comparison can accurately be established.
Section 6 presents the experimental results. Sec-
tion 7 reports a comparison of our approach to
the finite state machine modeling technique that
showed promissing results in (Nelken and Shieber,
2005). Finally, section 8 concludes the paper and
discusses future directions.
2 Arabic Diacritics
The Arabic alphabet consists of 28 letters that can
be extended to a set of 90 by additional shapes,
marks, and vowels (Tayli and Al-Salamah, 1990).
The 28 letters represent the consonants and long
577
vowels such as A, ? (both pronounced as /a:/),
?
  (pronounced as /i:/), and ? (pronounced as
/u:/). Long vowels are constructed by combin-
ing A, ?, ?
 , and ? with the short vowels. The
short vowels and certain other phonetic informa-
tion such as consonant doubling (shadda) are not
represented by letters, but by diacritics. A dia-
critic is a short stroke placed above or below the
consonant. Table 1 shows the complete set of Ara-
Diacritic Name Meaning/
on ? Pronunciation
Short vowels? fatha /a/
? damma /u/
? kasra /i/
Doubled case ending (?tanween?)? tanween al-fatha /an/
? tanween al-damma /un/
? tanween al-kasra /in/
Syllabification marks? shadda consonant
doubling? sukuun vowel
absence
Table 1: Arabic diacritics on the letter ? consonant
? ? (pronounced as /t/).
bic diacritics. We split the Arabic diacritics into
three sets: short vowels, doubled case endings, and
syllabification marks. Short vowels are written as
symbols either above or below the letter in text
with diacritics, and dropped all together in text
without diacritics. We find three short vowels:
? fatha: it represents the /a/ sound and is an
oblique dash over a consonant as in
? (c.f.
fourth row of Table 1).
? damma: it represents the /u/ sound and is
a loop over a consonant that resembles the
shape of a comma (c.f. fifth row of Table 1).
? kasra: it represents the /i/ sound and is an
oblique dash under a consonant (c.f. sixth row
of Table 1).
The doubled case ending diacritics are vowels used
at the end of the words to mark case distinction,
which can be considered as a double short vowels;
the term ?tanween? is used to express this phe-
nomenon. Similar to short vowels, there are three
different diacritics for tanween: tanween al-fatha,
tanween al-damma, and tanween al-kasra. They
are placed on the last letter of the word and have
the phonetic effect of placing an ?N? at the end
of the word. Text with diacritics contains also two
syllabification marks:
? shadda: it is a gemination mark placed above
the Arabic letters as in
?. It denotes the dou-
bling of the consonant. The shadda is usually
combined with a short vowel such as in
?.
? sukuun: written as a small circle as in
?. It is
used to indicate that the letter doesn?t contain
vowels.
Figure 1 shows an Arabic sentence transcribed with
and without diacritics. In modern Arabic, writing
scripts without diacritics is the most natural way.
Because many words with different vowel patterns
may appear identical in a diacritic-less setting,
considerable ambiguity exists at the word level.
The word I. J?, for example, has 21 possible forms
that have valid interpretations when adding dia-
critics (Kirchhoff and Vergyri, 2005). It may have
the interpretation of the verb ?to write? in I.
J

?
(pronounced /kataba/). Also, it can be interpreted
as ?books? in the noun form I.
J

? (pronounced /ku-
tubun/). A study made by (Debili et al, 2002)
shows that there is an average of 11.6 possible di-
acritizations for every non-diacritized word when
analyzing a text of 23,000 script forms.
. ?Q? 	Y?? @ ?
KQ? @ I. J?
. ? Q

?
	Y?? @ ?
K Q? @ I.
J

?
Figure 1: The same Arabic sentence without (up-
per row) and with (lower row) diacritics. The En-
glish translation is ?the president wrote the docu-
ment.?
Arabic diacritic restoration is a non-trivial task as
expressed in (El-Imam, 2003). Native speakers of
Arabic are able, in most cases, to accurately vo-
calize words in text based on their context, the
speaker?s knowledge of the grammar, and the lex-
icon of Arabic. Our goal is to convert knowledge
used by native speakers into features and incor-
porate them into a maximum entropy model. We
assume that the input text does not contain any
diacritics.
3 Previous Work
Diacritic restoration has been receiving increas-
ing attention and has been the focus of several
studies. In (El-Sadany and Hashish, 1988), a rule
based method that uses morphological analyzer for
578
vowelization was proposed. Another, rule-based
grapheme to sound conversion approach was ap-
peared in 2003 by Y. El-Imam (El-Imam, 2003).
The main drawbacks of these rule based methods is
that it is difficult to maintain the rules up-to-date
and extend them to other Arabic dialects. Also,
new rules are required due to the changing nature
of any ?living? language.
More recently, there have been several new stud-
ies that use alternative approaches for the diacriti-
zation problem. In (Emam and Fisher, 2004) an
example based hierarchical top-down approach is
proposed. First, the training data is searched hi-
erarchically for a matching sentence. If there is
a matching sentence, the whole utterance is used.
Otherwise they search for matching phrases, then
words to restore diacritics. If there is no match at
all, character n-gram models are used to diacritize
each word in the utterance.
In (Vergyri and Kirchhoff, 2004), diacritics in
conversational Arabic are restored by combining
morphological and contextual information with an
acoustic signal. Diacritization is treated as an un-
supervised tagging problem where each word is
tagged as one of the many possible forms provided
by the Buckwalter?s morphological analyzer (Buck-
walter, 2002). The Expectation Maximization
(EM) algorithm is used to learn the tag sequences.
Y. Gal in (Gal, 2002) used a HMM-based diacriti-
zation approach. This method is a white-space
delimited word based approach that restores only
vowels (a subset of all diacritics).
Most recently, a weighted finite state machine
based algorithm is proposed (Nelken and Shieber,
2005). This method employs characters and larger
morphological units in addition to words. Among
all the previous studies this one is more sophisti-
cated in terms of integrating multiple information
sources and formulating the problem as a search
task within a unified framework. This approach
also shows competitive results in terms of accuracy
when compared to previous studies. In their algo-
rithm, a character based generative diacritization
scheme is enabled only for words that do not occur
in the training data. It is not clearly stated in the
paper whether their method predict the diacritics
shedda and sukuun.
Even though the methods proposed for diacritic
restoration have been maturing and improving over
time, they are still limited in terms of coverage and
accuracy. In the approach we present in this paper,
we propose to restore the most comprehensive list
of the diacritics that are used in any Arabic text.
Our method differs from the previous approaches
in the way the diacritization problem is formulated
and because multiple information sources are inte-
grated. We view the diacritic restoration problem
as sequence classification, where given a sequence
of characters our goal is to assign diacritics to each
character. Our appoach is based on Maximum
Entropy (MaxEnt henceforth) technique (Berger
et al, 1996). MaxEnt can be used for sequence
classification, by converting the activation scores
into probabilities (through the soft-max function,
for instance) and using the standard dynamic pro-
gramming search algorithm (also known as Viterbi
search). We find in the literature several other
approaches of sequence classification such as (Mc-
Callum et al, 2000) and (Lafferty et al, 2001).
The conditional random fields method presented
in (Lafferty et al, 2001) is essentially a MaxEnt
model over the entire sequence: it differs from the
Maxent in that it models the sequence informa-
tion, whereas the Maxent makes a decision for each
state independently of the other states. The ap-
proach presented in (McCallum et al, 2000) com-
bines Maxent with Hidden Markov models to allow
observations to be presented as arbitrary overlap-
ping features, and define the probability of state
sequences given observation sequences.
We report in section 7 a comparative study be-
tween our approach and the most competitive dia-
critic restoration method that uses finite state ma-
chine algorithm (Nelken and Shieber, 2005). The
MaxEnt framework was successfully used to com-
bine a diverse collection of information sources and
yielded a highly competitive model that achieves a
5.1% DER.
4 Automatic Diacritization
The performance of many natural language pro-
cessing tasks, such as shallow parsing (Zhang et
al., 2002) and named entity recognition (Florian
et al, 2004), has been shown to depend on inte-
grating many sources of information. Given the
stated focus of integrating many feature types, we
selected the MaxEnt classifier. MaxEnt has the
ability to integrate arbitrary types of information
and make a classification decision by aggregating
all information available for a given classification.
4.1 Maximum Entropy Classifiers
We formulate the task of restoring diacritics as
a classification problem, where we assign to each
character in the text a label (i.e., diacritic). Be-
fore formally describing the method1, we introduce
some notations: let Y = {y1, . . . , yn} be the set of
diacritics to predict or restore, X be the example
space and F = {0, 1}m be a feature space. Each ex-
ample x ? X has associated a vector of binary fea-
tures f (x) = (f1 (x) , . . . , fm (x)). In a supervised
framework, like the one we are considering here, we
have access to a set of training examples together
with their classifications: {(x1, y1) , . . . , (xk, yk)}.
1This is not meant to be an in-depth introduction
to the method, but a brief overview to familiarize the
reader with them.
579
The MaxEnt algorithm associates a set of weights
(?ij)i=1...nj=1...m with the features, which are estimated
during the training phase to maximize the likeli-
hood of the data (Berger et al, 1996). Given these
weights, the model computes the probability dis-
tribution over labels for a particular example x as
follows:
P (y|x) = 1Z(x)
m
?
j=1
?fj (x)ij , Z(x) =
?
i
?
j
?fj (x)ij
where Z(X ) is a normalization factor. To esti-
mate the optimal ?j values, we train our Max-
Ent model using the sequential conditional gener-
alized iterative scaling (SCGIS) technique (Good-
man, 2002). While the MaxEnt method can nicely
integrate multiple feature types seamlessly, in cer-
tain cases it is known to overestimate its confidence
in especially low-frequency features. To overcome
this problem, we use the regularization method
based on adding Gaussian priors as described in
(Chen and Rosenfeld, 2000). After computing the
class probability distribution, the chosen diacritic
is the one with the most aposteriori probability.
The decoding algorithm, described in section 4.2,
performs sequence classification, through dynamic
programming.
4.2 Search to Restore Diacritics
We are interested in finding the diacritics of all
characters in a script or a sentence. These dia-
critics have strong interdependencies which can-
not be properly modeled if the classification is per-
formed independently for each character. We view
this problem as sequence classification, as con-
trasted with an example-based classification prob-
lem: given a sequence of characters in a sentence
x1x2 . . . xL, our goal is to assign diacritics (labels)
to each character, resulting in a sequence of diacrit-
ics y1y2 . . . yL. We make an assumption that dia-
critics can be modeled as a limited order Markov
sequence: the diacritic associated with the char-
acter i depends only on the diacritics associated
with the k previous diacritics, where k is usually
equal to 3. Given this assumption, and the nota-
tion xL1 = x1 . . . xL, the conditional probability of
assigning the diacritic sequence yL1 to the character
sequence xL1 becomes
p
(
yL1 |xL1
)
=
p
(
y1|xL1
)
p
(
y2|xL1 , y1
)
. . . p
(
yL|xL1 , yL?1L?k+1
)
(1)
and our goal is to find the sequence that maximizes
this conditional probability
y?L1 = arg max
yL1
p
(
yL1 |xL1
)
(2)
While we restricted the conditioning on the classi-
fication tag sequence to the previous k diacritics,
we do not impose any restrictions on the condition-
ing on the characters ? the probability is computed
using the entire character sequence xL1 .
To obtain the sequence in Equation (2), we create
a classification tag lattice (also called trellis), as
follows:
? Let xL1 be the input sequence of character and
S = {s1, s2, . . . , sm} be an enumeration of Yk
(m = |Y|k) - we will call an element sj a state.
Every such state corresponds to the labeling
of k successive characters. We find it useful
to think of an element si as a vector with k
elements. We use the notations si [j] for jth
element of such a vector (the label associated
with the token xi?k+j+1) and si [j1 . . . j2] for
the sequence of elements between indices j1
and j2.
? We conceptually associate every character
xi, i = 1, . . . , L with a copy of S, Si =
{
si1, . . . , sim
}
; this set represents all the possi-
ble labelings of characters xii?k+1 at the stage
where xi is examined.
? We then create links from the set Si to the
Si+1, for all i = 1 . . . L? 1, with the property
that
w
(
sij1 , s
i+1
j2
)
=
?
?
?
p
(
si+1j1 [k] |x
L
1 , si+1j2 [1..k ? 1]
)
if sij1 [2..k] = s
i+1
j2 [1..k ? 1]
0 otherwise
These weights correspond to probability of a
transition from the state sij1 to the state s
i+1
j2 .
? For every character xi, we compute recur-
sively2
?0 (sj) = 0, j = 1, . . . , k
?i (sj) = max
j1=1,...,M
?i?1 (sj1 ) + log w
(
si?1j1 , s
i
j
)
?i (sj) =
arg max
j1=1,...,M
?i?1 (sj1 ) + log w
(
si?1j1 , s
i
j
)
Intuitively, ?i (sj) represents the log-
probability of the most probable path through
the lattice that ends in state sj after i steps,
and ?i (sj) represents the state just before sj
on that particular path.
? Having computed the (?i)i values, the algo-
rithm for finding the best path, which corre-
sponds to the solution of Equation (2) is
1. Identify s?LL = arg maxj=1...L ?L (sj)
2. For i = L ? 1 . . . 1, compute
s?ii = ?i+1
(
s?i+1i+1
)
2For convenience, the index i associated with state
sij is moved to ?; the function ?i (sj) is in fact ?
(
sij
)
.
580
3. The solution for Equation (2) is given by
y? =
{
s?11[k], s?22[k], . . . , s?LL [k]
}
The runtime of the algorithm is ?
(
|Y|k ? L
)
, linear
in the size of the sentence L but exponential in the
size of the Markov dependency, k. To reduce the
search space, we use beam-search.
4.3 Features Employed
Within the MaxEnt framework, any type of fea-
tures can be used, enabling the system designer to
experiment with interesting feature types, rather
than worry about specific feature interactions. In
contrast, with a rule based system, the system de-
signer would have to consider how, for instance,
lexical derived information for a particular exam-
ple interacts with character context information.
That is not to say, ultimately, that rule-based sys-
tems are in some way inferior to statistical mod-
els ? they are built using valuable insight which
is hard to obtain from a statistical-model-only ap-
proach. Instead, we are merely suggesting that the
output of such a rule-based system can be easily
integrated into the MaxEnt framework as one of
the input features, most likely leading to improved
performance.
Features employed in our system can be divided
into three different categories: lexical, segment-
based, and part-of-speech tag (POS) features. We
also use the previously assigned two diacritics as
additional features.
In the following, we briefly describe the different
categories of features:
? Lexical Features: we include the charac-
ter n-gram spanning the curent character xi,
both preceding and following it in a win-
dow of 7: {xi?3, . . . , xi+3}. We use the cur-
rent word wi and its word context in a win-
dow of 5 (forward and backward trigram):
{wi?2, . . . , wi+2}. We specify if the character
of analysis is at the beginning or at the end
of a word. We also add joint features between
the above source of information.
? Segment-Based Features : Arabic blank-
delimited words are composed of zero or more
prefixes, followed by a stem and zero or more
suffixes. Each prefix, stem or suffix will be
called a segment in this paper. Segments are
often the subject of analysis when processing
Arabic (Zitouni et al, 2005). Syntactic in-
formation such as POS or parse information
is usually computed on segments rather than
words. As an example, the Arabic white-space
delimited word ?? D?K. A
? contains a verb ?K. A
?, a
third-person feminine singular subject-marker
H (she), and a pronoun suffix ?? (them); it
is also a complete sentence meaning ?she met
them.? To separate the Arabic white-space
delimited words into segments, we use a seg-
mentation model similar to the one presented
by (Lee et al, 2003). The model obtains an
accuracy of about 98%. In order to simulate
real applications, we only use segments gener-
ated by the model rather than true segments.
In the diacritization system, we include the
current segment ai and its word segment con-
text in a window of 5 (forward and backward
trigram): {ai?2, . . . , ai+2}. We specify if the
character of analysis is at the beginning or at
the end of a segment. We also add joint infor-
mation with lexical features.
? POS Features : we attach to the segment
ai of the current character, its POS: POS(ai).
This is combined with joint features that in-
clude the lexical and segment-based informa-
tion. We use a statistical POS tagging system
built on Arabic Treebank data with MaxEnt
framework (Ratnaparkhi, 1996). The model
has an accuracy of about 96%. We did not
want to use the true POS tags because we
would not have access to such information in
real applications.
5 Data
The diacritization system we present here is
trained and evaluated on the LDC?s Arabic Tree-
bank of diacritized news stories ? Part 3 v1.0: cata-
log number LDC2004T11 and ISBN 1-58563-298-8.
The corpus includes complete vocalization (includ-
ing case-endings). We introduce here a clearly de-
fined and replicable split of the corpus, so that the
reproduction of the results or future investigations
can accurately and correctly be established. This
corpus includes 600 documents from the An Nahar
News Text. There are a total of 340,281 words. We
split the corpus into two sets: training data and de-
velopment test (devtest) data. The training data
contains 288,000 words approximately, whereas the
devtest contains close to 52,000 words. The 90
documents of the devtest data are created by tak-
ing the last (in chronological order) 15% of docu-
ments dating from ?20021015 0101? (i.e., October
15, 2002) to ?20021215 0045? (i.e., December 15,
2002). The time span of the devtest is intention-
ally non-overlapping with that of the training set,
as this models how the system will perform in the
real world.
Previously published papers use proprietary cor-
pus or lack clear description of the training/devtest
data split, which make the comparison to other
techniques difficult. By clearly reporting the split
of the publicly available LDC?s Arabic Treebank
581
corpus in this section, we want future comparisons
to be correctly established.
6 Experiments
Experiments are reported in terms of word error
rate (WER), segment error rate (SER), and di-
acritization error rate (DER). The DER is the
proportion of incorrectly restored diacritics. The
WER is the percentage of incorrectly diacritized
white-space delimited words: in order to be
counted as incorrect, at least one character in the
word must have a diacritization error. The SER
is similar to WER but indicates the proportion of
incorrectly diacritized segments. A segment can
be a prefix, a stem, or a suffix. Segments are often
the subject of analysis when processing Arabic (Zi-
touni et al, 2005). Syntactic information such as
POS or parse information is based on segments
rather than words. Consequently, it is important
to know the SER in cases where the diacritization
system may be used to help disambiguate syntactic
information.
Several modern Arabic scripts contains the con-
sonant doubling ?shadda?; it is common for na-
tive speakers to write without diacritics except the
shadda. In this case the role of the diacritization
system will be to restore the short vowels, doubled
case ending, and the vowel absence ?sukuun?. We
run two batches of experiments: a first experiment
where documents contain the original shadda and
a second one where documents don?t contain any
diacritics including the shadda. The diacritization
system proceeds in two steps when it has to pre-
dict the shadda: a first step where only shadda is
restored and a second step where other diacritics
(excluding shadda) are predicted.
To assess the performance of the system under dif-
ferent conditions, we consider three cases based on
the kind of features employed:
1. system that has access to lexical features only;
2. system that has access to lexical and segment-
based features;
3. system that has access to lexical, segment-
based and POS features.
The different system types described above use the
two previously assigned diacritics as additional fea-
ture. The DER of the shadda restoration step is
equal to 5% when we use lexical features only, 0.4%
when we add segment-based information, and 0.3%
when we employ lexical, POS, and segment-based
features.
Table 2 reports experimental results of the diacriti-
zation system with different feature sets. Using
only lexical features, we observe a DER of 8.2%
and a WER of 25.1% which is competitive to a
True shadda Predicted shadda
WER SER DER WER SER DER
Lexical features
24.8 12.6 7.9 25.1 13.0 8.2
Lexical + segment-based features
18.2 9.0 5.5 18.8 9.4 5.8
Lexical + segment-based + POS features
17.3 8.5 5.1 18.0 8.9 5.5
Table 2: The impact of features on the diacriti-
zation system performance. The columns marked
with ?True shadda? represent results on docu-
ments containing the original consonant doubling
?shadda? while columns marked with ?Predicted
shadda? represent results where the system re-
stored all diacritics including shadda.
state-of-the-art system evaluated on Arabic Tree-
bank Part 2: in (Nelken and Shieber, 2005) a DER
of 12.79% and a WER of 23.61% are reported.
The system they described in (Nelken and Shieber,
2005) uses lexical, segment-based, and morpholog-
ical information. Table 2 also shows that, when
segment-based information is added to our sys-
tem, a significant improvement is achieved: 25%
for WER (18.8 vs. 25.1), 38% for SER (9.4 vs.
13.0), and 41% for DER (5.8 vs. 8.2). Similar be-
havior is observed when the documents contain the
original shadda. POS features are also helpful in
improving the performance of the system. They
improved the WER by 4% (18.0 vs. 18.8), SER by
5% (8.9 vs. 9.4), and DER by 5% (5.5 vs. 5.8).
Case-ending in Arabic documents consists of the
diacritic attributed to the last character in a white-
space delimited word. Restoring them is the most
difficult part in the diacritization of a document.
Case endings are only present in formal or highly
literary scripts. Only educated speakers of mod-
ern standard Arabic master their use. Technically,
every noun has such an ending, although at the
end of a sentence no inflection is pronounced, even
in formal speech, because of the rules of ?pause?.
For this reason, we conduct another experiment in
which case-endings were stripped throughout the
training and testing data without the attempt to
restore them.
We present in Table 3 the performance of the di-
acritization system on documents without case-
endings. Results clearly show that when case-
endings are omitted, the WER declines by 58%
(7.2% vs. 17.3%), SER is decreased by 52% (4.0%
vs. 8.5%), and DER is reduced by 56% (2.2% vs.
5.1%). Also, Table 3 shows again that a richer
set of features results in a better performance;
compared to a system using lexical features only,
adding POS and segment-based features improved
the WER by 38% (7.2% vs. 11.8%), the SER by
39% (4.0% vs. 6.6%), and DER by 38% (2.2% vs.
582
True shadda Predicted shadda
WER SER DER WER SER DER
Lexical features
11.8 6.6 3.6 12.4 7.0 3.9
Lexical + segment-based features
7.8 4.4 2.4 8.6 4.8 2.7
Lexical + segment-based + POS features
7.2 4.0 2.2 7.9 4.4 2.5
Table 3: Performance of the diacritization system
based on employed features. System is trained
and evaluated on documents without case-ending.
Columns marked with ?True shadda? represent re-
sults on documents containing the original con-
sonant doubling ?shadda? while columns marked
with ?Predicted shadda? represent results where
the system restored all diacritics including shadda.
3.6%). Similar to the results reported in Table 2,
we show that the performance of the system are
similar whether the document contains the origi-
nal shadda or not. A system like this trained on
non case-ending documents can be of interest to
applications such as speech recognition, where the
last state of a word HMM model can be defined to
absorb all possible vowels (Afify et al, 2004).
7 Comparison to other approaches
As stated in section 3, the most recent and ad-
vanced approach to diacritic restoration is the one
presented in (Nelken and Shieber, 2005): they
showed a DER of 12.79% and a WER of 23.61% on
Arabic Treebank corpus using finite state transduc-
ers (FST) with a Katz language modeling (LM) as
described in (Chen and Goodman, 1999). Because
they didn?t describe how they split their corpus
into training/test sets, we were not able to use the
same data for comparison purpose.
In this section, we want essentially to duplicate
the aforementioned FST result for comparison us-
ing the identical training and testing set we use for
our experiments. We also propose some new vari-
ations on the finite state machine modeling tech-
nique which improve performance considerably.
The algorithm for FST based vowel restoration
could not be simpler: between every pair of char-
acters we insert diacritics if doing so improves
the likelihood of the sequence as scored by a sta-
tistical n-gram model trained upon the training
corpus. Thus, in between every pair of charac-
ters we propose and score all possible diacritical
insertions. Results reported in Table 4 indicate
the error rates of diacritic restoration (including
shadda). We show performance using both Kneser-
Ney and Katz LMs (Chen and Goodman, 1999)
with increasingly large n-grams. It is our opinion
that large n-grams effectively duplicate the use of
a lexicon. It is unfortunate but true that, even for
a rich resource like the Arabic Treebank, the choice
of modeling heuristic and the effects of small sam-
ple size are considerable. Using the finite state ma-
chine modeling technique, we obtain similar results
to those reported in (Nelken and Shieber, 2005): a
WER of 23% and a DER of 15%. Better perfor-
mance is reached with the use of Kneser-Ney LM.
These results still under-perform those obtained
by MaxEnt approach presented in Table 2. When
all sources of information are included, the Max-
Ent technique outperforms the FST model by 21%
(22% vs. 18%) in terms of WER and 39% (9% vs.
5.5%) in terms of DER.
The SER reported on Table 2 and Table 3 are based
on the Arabic segmentation system we use in the
MaxEnt approach. Since, the FST model doesn?t
use such a system, we found inappropriate to re-
port SER in this section.
Katz LM Kneser-Ney LM
n-gram size WER DER WER DER
3 63 31 55 28
4 54 25 38 19
5 51 21 28 13
6 44 18 24 11
7 39 16 23 11
8 37 15 23 10
Table 4: Error Rate in % for n-gram diacritic
restoration using FST.
We propose in the following an extension to the
aforementioned FST model, where we jointly de-
termines not only diacritics but segmentation into
affixes as described in (Lee et al, 2003). Table 5
gives the performance of the extended FST model
where Kneser-Ney LM is used, since it produces
better results. This should be a much more dif-
ficult task, as there are more than twice as many
possible insertions. However, the choice of diacrit-
ics is related to and dependent upon the choice of
segmentation. Thus, we demonstrate that a richer
internal representation produces a more powerful
model.
8 Conclusion
We presented in this paper a statistical model for
Arabic diacritic restoration. The approach we pro-
pose is based on the Maximum entropy framework,
which gives the system the ability to integrate dif-
ferent sources of knowledge. Our model has the ad-
vantage of successfully combining diverse sources
of information ranging from lexical, segment-based
and POS features. Both POS and segment-based
features are generated by separate statistical sys-
tems ? not extracted manually ? in order to sim-
ulate real world applications. The segment-based
features are extracted from a statistical morpho-
logical analysis system using WFST approach and
the POS features are generated by a parsing model
583
True Shadda Predicted Shadda
n-gram size Kneser-Ney Kneser-Ney
WER DER WER DER
3 49 23 52 27
4 34 14 35 17
5 26 11 26 12
6 23 10 23 10
7 23 9 22 10
8 23 9 22 10
Table 5: Error Rate in % for n-gram dia-
critic restoration and segmentation using FST
and Kneser-Ney LM. Columns marked with ?True
shadda? represent results on documents contain-
ing the original consonant doubling ?shadda? while
columns marked with ?Predicted shadda? repre-
sent results where the system restored all diacritics
including shadda.
that also uses Maximum entropy framework. Eval-
uation results show that combining these sources of
information lead to state-of-the-art performance.
As future work, we plan to incorporate Buckwalter
morphological analyzer information to extract new
features that reduce the search space. One idea will
be to reduce the search to the number of hypothe-
ses, if any, proposed by the morphological analyzer.
We also plan to investigate additional conjunction
features to improve the accuracy of the model.
Acknowledgments
Grateful thanks are extended to Radu Florian for
his constructive comments regarding the maximum
entropy classifier.
References
M. Afify, S. Abdou, J. Makhoul, L. Nguyen, and B. Xi-
ang. 2004. The BBN RT04 BN Arabic System. In
RT04 Workshop, Palisades NY.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996.
A maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
T. Buckwalter. 2002. Buckwalter Arabic morpholog-
ical analyzer version 1.0. Technical report, Linguis-
tic Data Consortium, LDC2002L49 and ISBN 1-58563-
257-0.
Stanley F. Chen and Joshua Goodman. 1999. An
empirical study of smoothing techniques for language
modeling. computer speech and language. Computer
Speech and Language, 4(13):359?393.
Stanley Chen and Ronald Rosenfeld. 2000. A survey
of smoothing techniques for me models. IEEE Trans.
on Speech and Audio Processing.
F. Debili, H. Achour, and E. Souissi. 2002. De
l?etiquetage grammatical a? la voyellation automatique
de l?arabe. Technical report, Correspondances de
l?Institut de Recherche sur le Maghreb Contemporain
17.
Y. El-Imam. 2003. Phonetization of arabic: rules and
algorithms. Computer Speech and Language, 18:339?
373.
T. El-Sadany and M. Hashish. 1988. Semi-automatic
vowelization of Arabic verbs. In 10th NC Conference,
Jeddah, Saudi Arabia.
O. Emam and V. Fisher. 2004. A hierarchical ap-
proach for the statistical vowelization of Arabic text.
Technical report, IBM patent filed, DE9-2004-0006, US
patent application US2005/0192809 A1.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N Nicolov, and S Roukos. 2004.
A statistical model for multilingual entity detection
and tracking. In Proceedings of HLT-NAACL 2004,
pages 1?8.
Y. Gal. 2002. An HMM approach to vowel restora-
tion in Arabic and Hebrew. In ACL-02 Workshop on
Computational Approaches to Semitic Languages.
Joshua Goodman. 2002. Sequential conditional gener-
alized iterative scaling. In Proceedings of ACL?02.
K. Kirchhoff and D. Vergyri. 2005. Cross-dialectal
data sharing for acoustic modeling in Arabic speech
recognition. Speech Communication, 46(1):37?51, May.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data.
In ICML.
Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and
H. Hassan. 2003. Language model based Arabic word
segmentation. In Proceedings of the ACL?03, pages
399?406.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov models for
information extraction and segmentation. In ICML.
Rani Nelken and Stuart M. Shieber. 2005. Arabic
diacritization using weighted finite-state transducers.
In ACL-05 Workshop on Computational Approaches to
Semitic Languages, pages 79?86, Ann Arbor, Michigan.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Conference on
Empirical Methods in Natural Language Processing.
M. Tayli and A. Al-Salamah. 1990. Building bilingual
microcomputer systems. Communications of the ACM,
33(5):495?505.
D. Vergyri and K. Kirchhoff. 2004. Automatic dia-
critization of Arabic for acoustic modeling in speech
recognition. In COLING Workshop on Arabic-script
Based Languages, Geneva, Switzerland.
Tong Zhang, Fred Damerau, and David E. Johnson.
2002. Text chunking based on a generalization of Win-
now. Journal of Machine Learning Research, 2:615?
637.
Imed Zitouni, Jeff Sorensen, Xiaoqiang Luo, and Radu
Florian. 2005. The impact of morphological stemming
on Arabic mention detection and coreference resolu-
tion. In Proceedings of the ACL Workshop on Compu-
tational Approaches to Semitic Languages, pages 63?
70, Ann Arbor, June.
584
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 63?70,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Impact of Morphological Stemming on Arabic Mention
Detection and Coreference Resolution
Imed Zitouni, Jeff Sorensen, Xiaoqiang Luo, Radu Florian
{izitouni, sorenj, xiaoluo, raduf}@watson.ibm.com
IBM T.J. Watson Research Center
1101 Kitchawan Rd, Yorktown Heights, NY 10598, USA
Abstract
Arabic presents an interesting challenge to
natural language processing, being a highly
inflected and agglutinative language. In
particular, this paper presents an in-depth
investigation of the entity detection and
recognition (EDR) task for Arabic. We
start by highlighting why segmentation is
a necessary prerequisite for EDR, continue
by presenting a finite-state statistical seg-
menter, and then examine how the result-
ing segments can be better included into
a mention detection system and an entity
recognition system; both systems are statis-
tical, build around the maximum entropy
principle. Experiments on a clearly stated
partition of the ACE 2004 data show that
stem-based features can significantly im-
prove the performance of the EDT system
by 2 absolute F-measure points. The sys-
tem presented here had a competitive per-
formance in the ACE 2004 evaluation.
1 Introduction
Information extraction is a crucial step toward un-
derstanding and processing language. One goal of
information extraction tasks is to identify important
conceptual information in a discourse. These tasks
have applications in summarization, information re-
trieval (one can get al hits for Washington/person
and not the ones for Washington/state or Washing-
ton/city), data mining, question answering, language
understanding, etc.
In this paper we focus on the Entity Detection and
Recognition task (EDR) for Arabic as described in
ACE 2004 framework (ACE, 2004). The EDR has
close ties to the named entity recognition (NER) and
coreference resolution tasks, which have been the fo-
cus of several recent investigations (Bikel et al, 1997;
Miller et al, 1998; Borthwick, 1999; Mikheev et al,
1999; Soon et al, 2001; Ng and Cardie, 2002; Florian
et al, 2004), and have been at the center of evalu-
ations such as: MUC-6, MUC-7, and the CoNLL?02
and CoNLL?03 shared tasks. Usually, in computa-
tional linguistics literature, a named entity is an in-
stance of a location, a person, or an organization, and
the NER task consists of identifying each of these
occurrences. Instead, we will adopt the nomencla-
ture of the Automatic Content Extraction program
(NIST, 2004): we will call the instances of textual
references to objects/abstractions mentions, which
can be either named (e.g. John Mayor), nominal
(the president) or pronominal (she, it). An entity is
the aggregate of all the mentions (of any level) which
refer to one conceptual entity. For instance, in the
sentence
President John Smith said he has no com-
ments
there are two mentions (named and pronomial) but
only one entity, formed by the set {John Smith, he}.
We separate the EDR task into two parts: a men-
tion detection step, which identifies and classifies all
the mentions in a text ? and a coreference resolution
step, which combinines the detected mentions into
groups that refer to the same object. In its entirety,
the EDR task is arguably harder than traditional
named entity recognition, because of the additional
complexity involved in extracting non-named men-
tions (nominal and pronominal) and the requirement
of grouping mentions into entities. This is particu-
larly true for Arabic where nominals and pronouns
are also attached to the word they modify. In fact,
most Arabic words are morphologically derived from
a list of base forms or stems, to which prefixes and
suffixes can be attached to form Arabic surface forms
(blank-delimited words). In addition to the differ-
ent forms of the Arabic word that result from the
63
derivational and inflectional process, most preposi-
tions, conjunctions, pronouns, and possessive forms
are attached to the Arabic surface word. It is these
orthographic variations and complex morphological
structure that make Arabic language processing chal-
lenging (Xu et al, 2001; Xu et al, 2002).
Both tasks are performed with a statistical frame-
work: the mention detection system is similar to
the one presented in (Florian et al, 2004) and
the coreference resolution system is similar to the
one described in (Luo et al, 2004). Both systems
are built around from the maximum-entropy tech-
nique (Berger et al, 1996). We formulate the men-
tion detection task as a sequence classification prob-
lem. While this approach is language independent,
it must be modified to accomodate the particulars of
the Arabic language. The Arabic words may be com-
posed of zero or more prefixes, followed by a stem and
zero or more suffixes. We begin with a segmentation
of the written text before starting the classification.
This segmentation process consists of separating the
normal whitespace delimited words into (hypothe-
sized) prefixes, stems, and suffixes, which become the
subject of analysis (tokens). The resulting granular-
ity of breaking words into prefixes and suffixes allows
different mention type labels beyond the stem label
(for instance, in the case of nominal and pronominal
mentions). Additionally, because the prefixes and
suffixes are quite frequent, directly processing unseg-
mented words results in significant data sparseness.
We present in Section 2 the relevant particularities
of the Arabic language for natural language process-
ing, especially for the EDR task. We then describe
the segmentation system we employed for this task in
Section 3. Section 4 briefly describes our mention de-
tection system, explaining the different feature types
we use. We focus in particular on the stem n-gram,
prefix n-gram, and suffix n-gram features that are
specific to a morphologically rich language such as
Arabic. We describe in Section 5 our coreference
resolution system where we also describe the advan-
tage of using stem based features. Section 6 shows
and discusses the different experimental results and
Section 7 concludes the paper.
2 Why is Arabic Information
Extraction difficult?
The Arabic language, which is the mother tongue of
more than 300 million people (Center, 2000), present
significant challenges to many natural language pro-
cessing applications. Arabic is a highly inflected and
derived language. In Arabic morphology, most mor-
phemes are comprised of a basic word form (the root
or stem), to which many affixes can be attached to
form Arabic words. The Arabic alphabet consists
of 28 letters that can be extended to ninety by ad-
ditional shapes, marks, and vowels (Tayli and Al-
Salamah, 1990). Unlike Latin-based alphabets, the
orientation of writing in Arabic is from right to left.
In written Arabic, short vowels are often omitted.
Also, because variety in expression is appreciated
as part of a good writing style, the synonyms are
widespread. Arabic nouns encode information about
gender, number, and grammatical cases. There are
two genders (masculine and feminine), three num-
bers (singular, dual, and plural), and three gram-
matical cases (nominative, genitive, and accusative).
A noun has a nominative case when it is a subject,
accusative case when it is the object of a verb, and
genitive case when it is the object of a preposition.
The form of an Arabic noun is consequently deter-
mined by its gender, number, and grammatical case.
The definitive nouns are formed by attaching the
Arabic article ?

@ to the immediate front of the
nouns, such as in the word ??Q???

@ (the company).
Also, prepositions such as H. (by), and ? (to) can beattached as a prefix as in ??Q???? (to the company).
A noun may carry a possessive pronoun as a suffix,
such as in ?? D?Q?? (their company). For the EDR task,
in this previous example, the Arabic blank-delimited
word ?? D?Q?? should be split into two tokens: ??Q?? and
??. The first token ??Q?? is a mention that refers to
an organization, whereas the second token ?? is also
a mention, but one that may refer to a person. Also,
the prepositions (i.e., H. and ?) not be considered a
part of the mention.
Arabic has two kinds of plurals: broken plurals and
sound plurals (Wightwick and Gaafar, 1998; Chen
and Gey, 2002). The formation of broken plurals is
common, more complex and often irregular. As an
example, the plural form of the noun ?g. P (man) is
?A g. P (men), which is formed by inserting the infix
@. The plural form of the noun H. A
J? (book) is I. J?
(books), which is formed by deleting the infix @. The
plural form and the singular form may also be com-
pletely different (e.g. ?

@Q?@ for woman, but ZA
?	 for
women). The sound plurals are formed by adding
plural suffixes to singular nouns (e.g., IkAK. meaning
researcher): the plural suffix is H@ for feminine nouns
in grammatical cases (e.g., HA
JkAK.), 	?? for masculine
nouns in the nominative case (e.g., 	??JkAK.), and 	?K

for masculine nouns in the genitive and accusative
cases (e.g., 	?
JkAK.). The dual suffix is 	?@ for the nom-
inative case (e.g., 	?A
JkAK.), and 	?K
 for the genitive or
accusative (e.g., 	?
JkAK.).
Because we consider pronouns and nominals as men-
tions, it is essential to segment Arabic words into
these subword tokens. We also believe that the in-
64
formation denoted by these affixes can help with the
coreference resolution task1.
Arabic verbs have perfect and imperfect tenses (Ab-
bou and McCarus, 1983). Perfect tense denotes com-
pleted actions, while imperfect denotes ongoing ac-
tions. Arabic verbs in the perfect tense consist of a
stem followed by a subject marker, denoted as a suf-
fix. The subject marker indicates the person, gender,
and number of the subject. As an example, the verb
?K. A
? (to meet) has a perfect tense I?K. A
? for the third
person feminine singular, and @?

?K. A
? for the third per-
son masculine plural. We notice also that a verb with
a subject marker and a pronoun suffix can be by itself
a complete sentence, such us in the word ?? D?K. A
?: it
has a third-person feminine singular subject-markerH (she) and a pronoun suffix ?? (them). It is also
a complete sentence meaning ?she met them.? The
subject markers are often suffixes, but we may find
a subject marker as a combination of a prefix and a
suffix as in ???K. A
?K (she meets them). In this example,
the EDR system should be able to separate ???K. A
?K,
to create two mentions ( H and ??). Because the
two mentions belong to different entities, the EDR
system should not chain them together. An Arabic
word can potentially have a large number of vari-
ants, and some of the variants can be quite complex.
As an example, consider the word A ?D
JkAJ. ?? (and to
her researchers) which contains two prefixes and one
suffix ( A ? + ?

?kAK. + ? + ?).
3 Arabic Segmentation
Lee et al (2003) demonstrates a technique for seg-
menting Arabic text and uses it as a morphological
processing step in machine translation. A trigram
language model was used to score and select among
hypothesized segmentations determined by a set of
prefix and suffix expansion rules.
In our latest implementation of this algorithm, we
have recast this segmentation strategy as the com-
position of three distinct finite state machines. The
first machine, illustrated in Figure 1 encodes the pre-
fix and suffix expansion rules, producing a lattice of
possible segmentations. The second machine is a dic-
tionary that accepts characters and produces identi-
fiers corresponding to dictionary entries. The final
machine is a trigram language model, specifically a
Kneser-Ney (Chen and Goodman, 1998) based back-
off language model. Differing from (Lee et al, 2003),
we have also introduced an explicit model for un-
1As an example, we do not chain mentions with dif-
ferent gender, number, etc.
known words based upon a character unigram model,
although this model is dominated by an empirically
chosen unknown word penalty. Using 0.5M words
from the combined Arabic Treebanks 1V2, 2V2 and
3V1, the dictionary based segmenter achieves a exact
word match 97.8% correct segmentation.
SEP/epsilon
a/A#
epsilon/#
a/epsilon
a/epsilon
b/epsilon
b/B
UNK/epsilon
c/C
b/epsilon
c/BC
e/+E
epsilon/+
d/epsilon
d/epsilon
epsilon/epsilon
b/AB#
b/A#B#
e/+DE
c/epsilon d/BCD e/+D+E
Figure 1: Illustration of dictionary based segmenta-
tion finite state transducer
3.1 Bootstrapping
In addition to the model based upon a dictionary of
stems and words, we also experimented with models
based upon character n-grams, similar to those used
for Chinese segmentation (Sproat et al, 1996). For
these models, both arabic characters and spaces, and
the inserted prefix and suffix markers appear on the
arcs of the finite state machine. Here, the language
model is conditioned to insert prefix and suffix mark-
ers based upon the frequency of their appearance in
n-gram character contexts that appear in the train-
ing data. The character based model alone achieves
a 94.5% exact match segmentation accuracy, consid-
erably less accurate then the dictionary based model.
However, an analysis of the errors indicated that the
character based model is more effective at segment-
ing words that do not appear in the training data.
We seeked to exploit this ability to generalize to im-
prove the dictionary based model. As in (Lee et al,
2003), we used unsupervised training data which is
automatically segmented to discover previously un-
seen stems. In our case, the character n-gram model
is used to segment a portion of the Arabic Giga-
word corpus. From this, we create a vocabulary of
stems and affixes by requiring that tokens appear
more than twice in the supervised training data or
more than ten times in the unsupervised, segmented
corpus.
The resulting vocabulary, predominately of word
stems, is 53K words, or about six times the vo-
cabulary observed in the supervised training data.
This represents about only 18% of the total num-
ber of unique tokens observed in the aggregate
training data. With the addition of the automat-
ically acquired vocabulary, the segmentation accu-
racy achieves 98.1% exact match.
65
3.2 Preprocessing of Arabic Treebank Data
Because the Arabic treebank and the gigaword cor-
pora are based upon news data, we apply some
small amount of regular expression based preprocess-
ing. Arabic specific processing include removal of
the characters tatweel (), and vowels. Also, the fol-
lowing characters are treated as an equivalence class
during all lookups and processing: (1) ? ,?
 , and
(2)

@ , @ ,

@ ,

@. We define a token and introduce whites-
pace boundaries between every span of one or more
alphabetic or numeric characters. Each punctuation
symbol is considered a separate token. Character
classes, such as punctuation, are defined according
to the Unicode Standard (Aliprand et al, 2004).
4 Mention Detection
The mention detection task we investigate identifies,
for each mention, four pieces of information:
1. the mention type: person (PER), organiza-
tion (ORG), location (LOC), geopolitical en-
tity (GPE), facility (FAC), vehicle (VEH), and
weapon (WEA)
2. the mention level (named, nominal, pronominal,
or premodifier)
3. the mention class (generic, specific, negatively
quantified, etc.)
4. the mention sub-type, which is a sub-category
of the mention type (ACE, 2004) (e.g. OrgGov-
ernmental, FacilityPath, etc.).
4.1 System Description
We formulate the mention detection problem as a
classification problem, which takes as input seg-
mented Arabic text. We assign to each token in the
text a label indicating whether it starts a specific
mention, is inside a specific mention, or is outside
any mentions. We use a maximum entropy Markov
model (MEMM) classifier. The principle of maxi-
mum entropy states that when one searches among
probability distributions that model the observed
data (evidence), the preferred one is the one that
maximizes the entropy (a measure of the uncertainty
of the model) (Berger et al, 1996). One big advan-
tage of this approach is that it can combine arbitrary
and diverse types of information in making a classi-
fication decision.
Our mention detection system predicts the four la-
bels types associated with a mention through a cas-
cade approach. It first predicts the boundary and
the main entity type for each mention. Then, it uses
the information regarding the type and boundary in
different second-stage classifiers to predict the sub-
type, the mention level, and the mention class. Af-
ter the first stage, when the boundary (starting, in-
side, or outside a mention) has been determined, the
other classifiers can use this information to analyze
a larger context, capturing the patterns around the
entire mentions, rather than words. As an example,
the token sequence that refers to a mention will be-
come a single recognized unit and, consequently, lex-
ical and syntactic features occuring inside or outside
of the entire mention span can be used in prediction.
In the first stage (entity type detection and classifica-
tion), Arabic blank-delimited words, after segment-
ing, become a series of tokens representing prefixes,
stems, and suffixes (cf. section 2). We allow any
contiguous sequence of tokens can represent a men-
tion. Thus, prefixes and suffixes can be, and often
are, labeled with a different mention type than the
stem of the word that contains them as constituents.
4.2 Stem n-gram Features
We use a large set of features to improve the predic-
tion of mentions. This set can be partitioned into
4 categories: lexical, syntactic, gazetteer-based, and
those obtained by running other named-entity clas-
sifiers (with different tag sets). We use features such
as the shallow parsing information associated with
the tokens in a window of 3 tokens, POS, etc.
The context of a current token ti is clearly one of
the most important features in predicting whether ti
is a mention or not (Florian et al, 2004). We de-
note these features as backward token tri-grams and
forward token tri-grams for the previous and next
context of ti respectively. For a token ti, the back-
ward token n-gram feature will contains the previous
n ? 1 tokens in the history (ti?n+1, . . . ti?1) and the
forward token n-gram feature will contains the next
n ? 1 tokens (ti+1, . . . ti+n?1).
Because we are segmenting arabic words into
multiple tokens, there is some concern that tri-
gram contexts will no longer convey as much
contextual information. Consider the following
sentence extracted from the development set:
H. 	Qj?? ?
??A
J
??@ I. J?
??? Q
??? @ ?J??
 @
	Y? (transla-
tion ?This represents the location for Political
Party Office?). The ?Political Party Office? is
tagged as an organization and, as a word-for-word
translation, is expressed as ?to the Office of the
political to the party?. It is clear in this example
that the word Q?? (location for) contains crucial
information in distinguishing between a location
and an organization when tagging the token I. J?
?
66
(office). After segmentation, the sentence becomes:
+ I. J?
? + ?

@ + ? + Q?? + ?

@ + ?J? + ?
 + @
	Y?
.H. 	Qk + ?

@ + ? + ?
??A
J
? + ?

@
When predicting if the token I. J?
? (office) is the
beginning of an organization or not, backward and
forward token n-gram features contain only ?

@ + ?
(for the) and ?
??A
J
? + ?

@ (the political). This is
most likely not enough context, and addressing the
problem by increasing the size of the n-gram context
quickly leads to a data sparseness problem.
We propose in this paper the stem n-gram features as
additional features to the lexical set. If the current
token ti is a stem, the backward stem n-gram feature
contains the previous n ? 1 stems and the forward
stem n-gram feature will contain the following n? 1
stems. We proceed similarly for prefixes and suffixes:
if ti is a prefix (or suffix, respectively) we take the
previous and following prefixes (or suffixes)2. In the
sentence shown above, when the system is predict-
ing if the token I. J?
? (office) is the beginning of an
organization or not, the backward and forward stem
n-gram features contain Q?? ?J? (represent location
of) and H. 	Qk ?
??A
J
? (political office). The stem fea-
tures contain enough information in this example to
make a decision that I. J?
? (office) is the beginning of
an organization. In our experiments, n is 3, therefore
we use stem trigram features.
5 Coreference Resolution
Coreference resolution (or entity recognition) is de-
fined as grouping together mentions referring to the
same object or entity. For example, in the following
text,
(I) ?John believes Mary to be the best student?
three mentions ?John?, ?Mary?, ?student? are un-
derlined. ?Mary? and ?student? are in the same en-
tity since both refer to the same person.
The coreference system system is similar to the Bell
tree algorithm as described by (Luo et al, 2004).
In our implementation, the link model between a
candidate entity e and the current mention m is com-
puted as
PL(L = 1|e, m) ? maxmk?e P?L(L = 1|e, mk, m), (1)
2Thus, the difference to token n-grams is that the to-
kens of different type are removed from the streams, be-
fore the features are created.
where mk is one mention in entity e, and the basic
model building block P?L(L = 1|e, mk, m) is an ex-
ponential or maximum entropy model (Berger et al,
1996).
For the start model, we use the following approxima-
tion:
PS(S = 1|e1, e2, ? ? ? , et, m) ?
1 ? max
1?i?t
PL(L = 1|ei, m) (2)
The start model (cf. equation 2) says that the prob-
ability of starting a new entity, given the current
mention m and the previous entities e1, e2, ? ? ? , et, is
simply 1 minus the maximum link probability be-
tween the current mention and one of the previous
entities.
The maximum-entropy model provides us with a
flexible framework to encode features into the the
system. Our Arabic entity recognition system uses
many language-indepedent features such as strict
and partial string match, and distance features (Luo
et al, 2004). In this paper, however, we focus on the
addition of Arabic stem-based features.
5.1 Arabic Stem Match Feature
Features using the word context (left and right to-
kens) have been shown to be very helpful in corefer-
ence resolution (Luo et al, 2004). For Arabic, since
words are morphologically derived from a list of roots
(stems), we expected that a feature based on the
right and left stems would lead to improvement in
system accuracy.
Let m1 and m2 be two candidate mentions where
a mention is a string of tokens (prefixes, stems,
and suffixes) extracted from the segmented text.
In order to make a decision in either linking the
two mentions or not we use additional features
such as: do the stems in m1 and m2 match, do
stems in m1 match all stems in m2, do stems
in m1 partially match stems in m2. We proceed
similarly for prefixes and suffixes. Since prefixes and
suffixes can belong to different mention types, we
build a parse tree on the segmented text and we can
explore features dealing with the gender and number
of the token. In the following example, between
parentheses we make a word-for-word translations in
order to better explain our stemming feature. Let us
take the two mentions H. 	Qj?? ?
??A
J
??@ I. J?
???
(to-the-office the-politic to-the-party) and
?
G.
	Qm?'@ I. J?
? (office the-party?s) segmented as
H. 	Qk + ?

@ + ? + ?
??A
J
? + ?

@ + I. J?
? + ?

@ + ?
and ?
 + H. 	Qk + ?

@ + I. J?
? respectively. In our
67
development corpus, these two mentions are chained
to the same entity. The stemming match feature
in this case will contain information such us all
stems of m2 match, which is a strong indicator
that these mentions should be chained together.
Features based on the words alone would not help
this specific example, because the two strings m1
and m2 do not match.
6 Experiments
6.1 Data
The system is trained on the Arabic ACE 2003 and
part of the 2004 data. We introduce here a clearly
defined and replicable split of the ACE 2004 data,
so that future investigations can accurately and cor-
rectly compare against the results presented here.
There are 689 Arabic documents in LDC?s 2004 re-
lease (version 1.4) of ACE data from three sources:
the Arabic Treebank, a subset of the broadcast
(bnews) and newswire (nwire) TDT-4 documents.
The 178-document devtest is created by taking
the last (in chronological order) 25% of docu-
ments in each of three sources: 38 Arabic tree-
bank documents dating from ?20000715? (i.e., July
15, 2000) to ?20000815,? 76 bnews documents from
?20001205.1100.0489? (i.e., Dec. 05 of 2000 from
11:00pm to 04:89am) to ?20001230.1100.1216,? and
64 nwire documents from ?20001206.1000.0050? to
?20001230.0700.0061.? The time span of the test
set is intentionally non-overlapping with that of the
training set within each data source, as this models
how the system will perform in the real world.
6.2 Mention Detection
We want to investigate the usefulness of stem n-
gram features in the mention detection system. As
stated before, the experiments are run in the ACE?04
framework (NIST, 2004) where the system will iden-
tify mentions and will label them (cf. Section 4)
with a type (person, organization, etc), a sub-type
(OrgCommercial, OrgGovernmental, etc), a mention
level (named, nominal, etc), and a class (specific,
generic, etc). Detecting the mention boundaries (set
of consecutive tokens) and their main type is one of
the important steps of our mention detection sys-
tem. The score that the ACE community uses (ACE
value) attributes a higher importance (outlined by
its weight) to the main type compared to other sub-
tasks, such as the mention level and the class. Hence,
to build our mention detection system we spent a lot
of effort in improving the first step: detecting the
mention boundary and their main type. In this pa-
per, we report the results in terms of precision, recall,
and F-measure3.
Lexical features
Precision Recall F-measure
(%) (%) (%)
Total 73.3 58.0 64.7
FAC 76.0 24.0 36.5
GPE 79.4 65.6 71.8
LOC 57.7 29.9 39.4
ORG 63.1 46.6 53.6
PER 73.2 63.5 68.0
VEH 83.5 29.7 43.8
WEA 77.3 25.4 38.2
Lexical features + Stem
Precision Recall F-measure
(%) (%) (%)
Total 73.6 59.4 65.8
FAC 72.7 29.0 41.4
GPE 79.9 67.2 73.0
LOC 58.6 31.9 41.4
ORG 62.6 47.2 53.8
PER 73.8 64.6 68.9
VEH 81.7 35.9 49.9
WEA 78.4 29.9 43.2
Table 1: Performance of the mention detection sys-
tem using lexical features only.
To assess the impact of stemming n-gram features
on the system under different conditions, we consider
two cases: one where the system only has access to
lexical features (the tokens and direct derivatives in-
cluding standard n-gram features), and one where
the system has access to a richer set of information,
including lexical features, POS tags, text chunks,
parse tree, and gazetteer information. The former
framework has the advantage of being fast (making
it more appropriate for deployment in commercial
systems). The number of parameters to optimize in
the MaxEnt framework we use when only lexical fea-
tures are explored is around 280K parameters. This
number increases to 443K approximately when all in-
formation is used except the stemming feature. The
number of parameters introduced by the use of stem-
ming is around 130K parameters. Table 1 reports
experimental results using lexical features only; we
observe that the stemming n-gram features boost the
performance by one point (64.7 vs. 65.8). It is im-
portant to notice the stemming n-gram features im-
proved the performance of each category of the main
type.
In the second case, the systems have access to a large
amount of feature types, including lexical, syntac-
tic, gazetteer, and those obtained by running other
3The ACE value is an important factor for us, but its
relative complexity, due to different weights associated
with the subparts, makes for a hard comparison, while
the F-measure is relatively easy to interpret.
68
AllFeatures
Precision Recall F-measure
(%) (%) (%)
Total 74.3 64.0 68.8
FAC 72.3 36.8 48.8
GPE 80.5 70.8 75.4
LOC 61.1 35.4 44.8
ORG 61.4 50.3 55.3
PER 75.3 70.2 72.7
VEH 83.2 38.1 52.3
WEA 69.0 36.6 47.8
All-Features + Stem
Precision Recall F-measure
(%) (%) (%)
Total 74.4 64.6 69.2
FAC 68.8 38.5 49.4
GPE 80.8 71.9 76.1
LOC 60.2 36.8 45.7
ORG 62.2 51.0 56.1
PER 75.3 70.2 72.7
VEH 81.4 41.8 55.2
WEA 70.3 38.8 50.0
Table 2: Performance of the mention detection sys-
tem using lexical, syntactic, gazetteer features as well
as features obtained by running other named-entity
classifiers
named-entity classifiers (with different semantic tag
sets). Features are also extracted from the shal-
low parsing information associated with the tokens
in window of 3, POS, etc. The All-features system
incorporates all the features except for the stem n-
grams. Table 2 shows the experimental results with
and without the stem n-grams features. Again, Ta-
ble 2 shows that using stem n-grams features gave
a small boost to the whole main-type classification
system4. This is true for all types. It is interesting to
note that the increase in performance in both cases
(Tables 1 and 2) is obtained from increased recall,
with little change in precision. When the prefix and
suffix n-gram features are removed from the feature
set, we notice in both cases (Tables 1 and 2) a in-
significant decrease of the overall performance, which
is expected: what should a feature of preceeding (or
following) prepositions or finite articles captures?
As stated in Section 4.1, the mention detection sys-
tem uses a cascade approach. However, we were curi-
ous to see if the gain we obtained at the first level was
successfully transfered into the overall performance
of the mention detection system. Table 3 presents
the performance in terms of precision, recall, and F-
measure of the whole system. Despite the fact that
the improvement was small in terms of F-measure
(59.4 vs. 59.7), the stemming n-gram features gave
4The difference in performance is not statistically sig-
nificant
interesting improvement in terms of ACE value to
the hole EDR system as showed in section 6.3.
Precision Recall F-measure
(%) (%) (%)
All-Features 64.2 55.3 59.4
All-Features+Stem 64.4 55.7 59.7
Lexical 64.4 50.8 56.8
Lexical+Stem 64.6 52.0 57.6
Table 3: Performance of the mention detection sys-
tem including all ACE?04 subtasks
6.3 Coreference Resolution
In this section, we present the coreference results on
the devtest defined earlier. First, to see the effect of
stem matching features, we compare two coreference
systems: one with the stem features, the other with-
out. We test the two systems on both ?true? and
system mentions of the devtest set. ?True? men-
tions mean that input to the coreference system are
mentions marked by human, while system mentions
are output from the mention detection system. We
report results with two metrics: ECM-F and ACE-
Value. ECM-F is an entity-constrained mention F-
measure (cf. (Luo et al, 2004) for how ECM-F is
computed), and ACE-Value is the official ACE eval-
uation metric. The result is shown in Table 4: the
baseline numbers without stem features are listed un-
der ?Base,? and the results of the coreference system
with stem features are listed under ?Base+Stem.?
On true mention, the stem matching features im-
prove ECM-F from 77.7% to 80.0%, and ACE-value
from 86.9% to 88.2%. The similar improvement is
also observed on system mentions.The overall ECM-
F improves from 62.3% to 64.2% and the ACE value
improves from 61.9 to 63.1%. Note that the increase
on the ACE value is smaller than ECM-F. This is
because ACE-value is a weighted metric which em-
phasizes on NAME mentions and heavily discounts
PRONOUN mentions. Overall the stem features give
rise to consistent gain to the coreference system.
7 Conclusion
In this paper, we present a fully fledged Entity Detec-
tion and Tracking system for Arabic. At its base, the
system fundamentally depends on a finite state seg-
menter and makes good use of the relationships that
occur between word stems, by introducing features
which take into account the type of each segment.
In mention detection, the features are represented as
stem n-grams, while in coreference resolution they
are captured through stem-tailored match features.
69
Base Base+Stem
ECM-F ACEVal ECM-F ACEVal
Truth 77.7 86.9 80.0 88.2
System 62.3 61.9 64.2 63.1
Table 4: Effect of Arabic stemming features on coref-
erence resolution. The row marked with ?Truth?
represents the results with ?true? mentions while the
row marked with ?System? represents that mentions
are detected by the system. Numbers under ?ECM-
F? are Entity-Constrained-Mention F-measure and
numbers under ?ACE-Val? are ACE-values.
These types of features result in an improvement in
both the mention detection and coreference resolu-
tion performance, as shown through experiments on
the ACE 2004 Arabic data. The experiments are per-
formed on a clearly specified partition of the data, so
comparisons against the presented work can be cor-
rectly and accurately made in the future. In addi-
tion, we also report results on the official test data.
The presented system has obtained competitive re-
sults in the ACE 2004 evaluation, being ranked
amongst the top competitors.
8 Acknowledgements
This work was partially supported by the Defense
Advanced Research Projects Agency and monitored
by SPAWAR under contract No. N66001-99-2-8916.
The views and findings contained in this material are
those of the authors and do not necessarily reflect
the position of policy of the U.S. government and no
official endorsement should be inferred.
References
Peter F. Abbou and Ernest N. McCarus, editors. 1983.
Elementary modern standard Arabic. Cambridge Univer-
sity Press.
ACE. 2004. Automatic content extraction.
http://www.ldc.upenn.edu/Projects/ACE/.
Joan Aliprand, Julie Allen, Joe Becker, Mark Davis,
Michael Everson, Asmus Freytag, John Jenkins, Mike
Ksar, Rick McGowan, Eric Muller, Lisa Moore, Michel
Suignard, and Ken Whistler. 2004. The unicode stan-
dard. http://www.unicode.org/.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language process-
ing. Computational Linguistics, 22(1):39?71.
D. M. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-finder.
In Proceedings of ANLP-97, pages 194?201.
A. Borthwick. 1999. A Maximum Entropy Approach to
Named Entity Recognition. Ph.D. thesis, New York Uni-
versity.
Egyptian Demographic Center. 2000.
http://www.frcu.eun.eg/www/homepage/cdc/cdc.htm.
Aitao Chen and Fredic Gey. 2002. Building an arabic
stemmer for information retrieval. In Proceedings of the
Eleventh Text REtrieval Conference (TREC 2002), Na-
tional Institute of Standards and Technology, November.
S. F. Chen and J. Goodman. 1998. An empirical study
of smoothing techinques for language modeling. Techni-
cal Report TR-10-98, Center for Research in Comput-
ing Technology, Harvard University, Cambridge, Mas-
sachusettes, August.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, X. Luo, N Nicolov, and S Roukos. 2004. A statisti-
cal model for multilingual entity detection and tracking.
In Proceedings of HLT-NAACL 2004, pages 1?8.
Y.-S. Lee, K. Papineni, S. Roukos, O. Emam, and H. Has-
san. 2003. Language model based Arabic word segmen-
tation. In Proceedings of the ACL?03, pages 399?406.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based on
the bell tree. In Proc. of ACL?04.
A. Mikheev, M. Moens, and C. Grover. 1999. Named
entity recognition without gazetteers. In Proceedings of
EACL?99.
S. Miller, M. Crystal, H. Fox, L. Ramshaw, R. Schwarz,
R. Stone, and R. Weischedel. 1998. Bbn: Description of
the SIFT system as used for MUC-7. In MUC-7.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings of
the ACL?02, pages 104?111.
NIST. 2004. Proceedings of ace evaluation and pi meet-
ing 2004 workshop. Alexandria, VA, September. NIST.
W. M. Soon, H. T. Ng, and C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
R. Sproat, C. Shih, W. Gale, and N. Chang. 1996. A
stochastic finite-state word-segmentation algorithm for
Chinese. Computational Linguistics, 22(3).
M. Tayli and A. Al-Salamah. 1990. Building bilingual
microcomputer systems. Communications of the ACM,
33(5):495?505.
J. Wightwick and M. Gaafar. 1998. Arabic Verbs and
Essentials of Grammar. Passport Books.
J. Xu, A. Fraser, and R. Weischedel. 2001. Trec2001
cross-lingual retrieval at bbn. In TREC 2001, Gaithers-
burg: NIST.
J. Xu, A. Fraser, and R. Weischedel. 2002. Empirical
studies in strategies for arabic information retrieval. In
SIGIR 2002, Tampere, Finland.
70
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 87?93,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
An Integrated Approach for Arabic-English Named Entity Translation 
 
 
Hany Hassan Jeffrey Sorensen 
IBM Cairo Technology Development Center IBM T.J. Watson Research Center 
Giza - Egypt 
                Yorktown Heights 
P.O. Box 166 Al-Ahram  NY 10598 
hanyh@eg.ibm.com sorenj@us.ibm.com 
 
 
Abstract 
Translation of named entities (NEs), 
such as person names, organization names 
and location names is crucial for cross lin-
gual information retrieval, machine trans-
lation, and many other natural language 
processing applications. Newly named en-
tities are introduced on daily basis in 
newswire and this greatly complicates the 
translation task. Also, while some names 
can be translated, others must be transliter-
ated, and, still, others are mixed. In this 
paper we introduce an integrated approach 
for named entity translation deploying 
phrase-based translation, word-based trans-
lation, and transliteration modules into a 
single framework.  While Arabic based, 
the approach introduced here is a unified 
approach that can be applied to NE transla-
tion for any language pair. 
1 Introduction 
Named Entities (NEs) translation is crucial for ef-
fective cross-language information retrieval 
(CLIR) and for Machine Translation. There are 
many types of NE phrases, such as: person names, 
organization names, location names, temporal ex-
pressions, and names of events. In this paper we 
only focus on three categories of NEs: person 
names, location names and organization names, 
though the approach is, in principle, general 
enough to accommodate any entity type. 
NE identification has been an area of significant 
research interest for the last few years. NE transla-
tion, however, remains a largely unstudied prob-
lem. NEs might be phonetically transliterated (e.g. 
persons names) and might also be mixed between 
phonetic transliteration and semantic translation as 
the case with locations and organizations names. 
There are three distinct approaches that can be 
applied for NE translation, namely:  a 
transliteration approach, a word based translation 
approach and a phrase based translation approach. 
The transliteration approach depends on phonetic 
transliteration and is only appropriate for out of 
vocabulary and completely unknown words. For 
more frequently used words, transliteration does 
not provide sophisticated results. A word based 
approach depends upon traditional statistical 
machine translation techniques such as IBM 
Model1 (Brown et al, 1993) and may not always 
yield satisfactory results due to its inability to 
handle difficult many-to-many phrase translations. 
A phrase based approach could provide a good 
translation for frequently used NE phrases though 
it is inefficient for less frequent words. Each of the 
approaches has its advantages and disadvantages. 
In this paper we introduce an integrated ap-
proach for combining phrase based NE translation, 
word based NE translation, and NE transliteration 
in a single framework. Our approach attempts to 
harness the advantages of the three approaches 
while avoiding their pitfalls. We also introduce and 
evaluate a new approach for aligning NEs across 
parallel corpora, a process for automatically ex-
tracting new NEs translation phrases, and a new 
transliteration approach. As is typical for statistical 
MT, the system requires the availability of general 
parallel corpus and Named Entity identifiers for 
the NEs of interest. 
Our primary focus in this paper is on translating 
NEs out of context (i.e. NEs are extracted and 
translated without any contextual clues). Although 
87
this is a more difficult problem than translating 
NEs in context, we adopt this approach because it 
is more generally useful for CLIR applications. 
The paper is organized as follows, section 2 
presents related work, section 3 describes our inte-
grated NE translation approach, section 4 presents 
the word based translation module, the phrase 
based module, the transliteration module, and sys-
tem integration and decoding, section 5 provides 
the experimental setup and results and finally sec-
tion 6 concludes the paper. 
2 Related Work 
The Named Entity translation problem was previ-
ously addressed using two different approaches: 
Named Entity phrase translation (which includes 
word-based translation) and Named Entity translit-
eration. Recently, many NE phrase translation ap-
proaches have been proposed. Huang et al   
(Huang et al, 2003) proposed an approach to ex-
tract NE trans-lingual equivalences based on the 
minimization of a linearly combined multi-feature 
cost. However this approach used a bilingual dic-
tionary to extract NE pairs and deployed it itera-
tively to extract more NEs.  Moore (Moore, 2003), 
proposed an approach deploying a sequence of cost 
models. However this approach relies on ortho-
graphic clues, such as strings repeated in the 
source and target languages and capitalization, 
which are only suitable for language pairs with 
similar scripts and/or orthographic conventions.  
Most prior work in Arabic-related translitera-
tion has been developed for the purpose of ma-
chine translation and for Arabic-English 
transliteration in particular. Arbabi (Arbabi et al, 
1998) developed a hybrid neural network and 
knowledge-based system to generate multiple Eng-
lish spellings for Arabic person names. Stalls and 
Knight (Stalls and Knight, 1998) introduced an 
approach for Arabic-English back transliteration 
for names of English origin; this approach could 
only back transliterate to English the names that 
have an available pronunciation. Al-Onaizan and 
Knight (Al-Onaizan and Knight, 2002) proposed a 
spelling-based model which directly maps English 
letter sequences into Arabic letter sequences. Their 
model was trained on a small English Arabic 
names list without the need for English pronuncia-
tions. Although this method does not require the 
availability of English pronunciation, it has a seri-
ous limitation because it does not provide a mecha-
nism for inserting the omitted short vowels in 
Arabic names. Therefore it does not perform well 
with names of Arabic origin in which short vowels 
tend to be omitted. 
3 Integrated Approach for Named Entity 
Translation  
We introduce an integrated approach for Named 
Entity (NE) translation using phrase based transla-
tion, word based translation and transliteration ap-
proaches in a single framework. Our unified 
approach could handle, in principle, any NE type 
for any languages pair. 
The level of complication in NE translation de-
pends on the NE type, the original source of the 
names, the standard de facto translation for certain 
named entities and the presence of acronyms. For 
example persons names tend to be phonetically 
transliterated, but different sources might use dif-
ferent transliteration styles depending on the origi-
nal source of the names and the idiomatic 
translation that has been established. Consider the 
following two names: 
?
     
 : jAk $yrAk?  ?Jacques Chirac? 
? 	

   
  :jAk strw?  ?Jack Straw? 
Although the first names in both examples are the 
same in Arabic, their transliterations should be dif-
ferent.  One might be able to distinguish between 
the two by looking at the last names.  This example 
illustrates why transliteration may not be good for 
frequently used named entities.  Transliteration is 
more appropriate for unknown NEs. 
For locations and organizations, the translation 
can be a mixture of translation and transliteration. 
For example: 
 ffProceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1119?1127,
Beijing, August 2010
Syntax Based Reordering with Automatically Derived Rules for
Improved Statistical Machine Translation
Karthik Visweswariah
IBM Research
v-karthik@in.ibm.com
Jiri Navratil
IBM Research
jiri@us.ibm.com
Jeffrey Sorensen
Google, Inc.
sorenj@google.com
Vijil Chenthamarakshan
IBM Research
vijil.e.c@in.ibm.com
Nanda Kambhatla
IBM Research
kambhatla@in.ibm.com
Abstract
Syntax based reordering has been shown
to be an effective way of handling word
order differences between source and
target languages in Statistical Machine
Translation (SMT) systems. We present
a simple, automatic method to learn rules
that reorder source sentences to more
closely match the target language word or-
der using only a source side parse tree and
automatically generated alignments. The
resulting rules are applied to source lan-
guage inputs as a pre-processing step and
demonstrate significant improvements in
SMT systems across a variety of lan-
guages pairs including English to Hindi,
English to Spanish and English to French
as measured on a variety of internal test
sets as well as a public test set.
1 Introduction
Different languages arrange words in different or-
ders, whether due to grammatical constraints or
other conventions. Dealing with these word order
permutations is one of the fundamental challenges
of machine translation. Given an exceptionally
large training corpus, a phrase-based system can
learn these reordering on a case by case basis.
But, if our systems are to generalize to phrases not
seen in the training data, they must explicitly cap-
ture and model these reorderings. However, per-
mutations are difficult to model and impractical to
search.
Presently, approaches that handle reorderings
typically model word and phrase movements via
a distortion model and rely on the target language
model to produce words in the right order. Early
distortion models simply penalized longer jumps
more than shorter jumps (Koehn et al, 2003)
independent of the source or target phrases
in question. Other models (Tillman, 2004),
(Al-Onaizan and Papineni, 2006) generalize this
to include lexical dependencies on the source.
Another approach is to incorporate features,
based on the target syntax, during modeling and
decoding, and this is shown to be effective for var-
ious language pairs (Yamada and Knight, 2001),
(Zollmann and Venugopal, 2006). Hierarchical
phrase-based decoding (Chiang, 2005) also al-
lows for long range reordering without explic-
itly modeling syntax. While these approaches
have been shown to improve machine translation
performance (Zollmann et al, 2008) they usually
combine chart parsing with the decoding process,
and are significantly more computationally inten-
sive than phrase-based systems.
A third approach, one that has proved to be
useful for phrase-based SMT systems, is to re-
order each source-side sentence using a set of
rules applied to a parse tree of the source sen-
tence. The goal of these rules is to make the
word order of the source sentence more sim-
ilar to the expected target sentence word or-
der. With this approach, the reordering rules
are applied before training and testing with an
SMT system. The efficacy of these methods has
been shown on various language pairs including:
French to English (Xia and McCord, 2004), Ger-
man to English (Collins et al, 2005), English to
1119
Chinese, (Wang et al, 2007) and Hindi to English
(Ramanathan et al, 2008).
In this paper, we propose a simple model for re-
ordering conditioned on the source side parse tree.
The model is learned using a parallel corpus of
source-target sentence pairs, machine generated
word alignments, and source side parses. We ap-
ply the reordering model to both training and test
data, for four different language pairs: English
? Spanish, English ? French, English ? Hindi,
and English ? German. We show improvements
in machine translation performance for all of the
language pairs we consider except for English ?
German. We use this negative result to propose
extensions to our reordering model. We note that
the syntax based reordering we propose can be
combined with other approaches to handling re-
ordering and does not have to be followed by an
assumption of monotonicity. In fact, our phrase-
based model, trained upon reordered data, retains
its reordering models and search, but we expect
that these facilities are employed much more spar-
ingly with reordered inputs.
2 Related work
There is a significant quantity of work in syntax
based reordering employed to improve machine
translation systems. We summarize our contribu-
tions to be:
? Learning the reordering rules based on train-
ing data (without relying on linguistic knowl-
edge of the language pair)
? Requiring only source side parse trees
? Experimental results showing the efficacy for
multiple language pairs
? Using a lexicalized distortion model for our
baseline decoder
There have been several studies that have
demonstrated improvements with syntax
based reordering based upon hand-written
rules. There have also been studies inves-
tigating the sources of these improvements
(Zwarts and Dras, 2007). Hand-written rules
depend upon expert knowledge of the linguis-
tic properties of the particular language pair.
Initial efforts (Niessen and Ney, 2001) were
made at improving German-English translation
by handling two phenomena: question inver-
sion and detachable verb prefixes in German.
In (Collins et al, 2005), (Wang et al, 2007),
(Ramanathan et al, 2008), (Badr et al, 2009)
rules are developed for translation from Ger-
man to English, Chinese to English, English
to Hindi, and English to Arabic respectively.
(Xu et al, 2009) develop reordering rules based
upon a linguistic analysis of English and Korean
sentences and then apply those rules to trans-
lation from English into Korean and four other
languages: Japanese, Hindi, Urdu and Turkish.
Unlike this body of work, we automatically learn
the rules from the training data and show efficacy
on multiple language pairs.
There have been some studies that try to learn
rules from the data. (Habash, 2007) learns re-
ordering rules based on a dependency parse and
they report a negative result for Arabic to En-
glish translation. (Zhang et al, 2007) learn re-
ordering rules on chunks and part of speech
tags, but the rules they learn are not hierarchi-
cal and would require large amounts of training
data to learn rules for long sentences. Addition-
ally, we only keep a single best reordering (in-
stead of a lattice with possible reorderings) which
makes the decoding significantly more efficient.
(Xia and McCord, 2004) uses source and target
side parse trees to automatically learn rules to re-
order French sentences to match English order.
The requirement to have both source and target
side parse trees makes this method inapplicable
to any language that does not have adequate tree
bank resources. In addition, this work reports re-
sults using monotone decoding, since their exper-
iments using non-monotone decoding without a
distortion model were actually worse.
3 Reordering issues in specific languages
In this section we discuss the reordering issues
typical of translating between English and Hindi,
French, Spanish and German which are the four
language pairs we experiment on in this paper.
3.1 Spanish and French
Typical word ordering patterns common to these
two European languages relate to noun phrases in-
cluding groups of nouns and adjectives. In con-
1120
trast to English, French and Spanish adjectives
and adjunct nouns follow the main noun, i.e. we
typically observe a reversal of word order in noun
phrases, e.g., ?A beautiful red car? translates
into French as ?Une voiture rouge beau?, and as
?Un coche rojo bonito? into Spanish. Phrase-
based MT systems are capable of capturing these
patterns provided they occur with sufficient fre-
quency for each example in the training data. For
rare noun phrases, however, the MT may pro-
duce erroneous word order that can lead to seri-
ous distortions in the meaning. Particularly dif-
ficult are nominal phrases from specialized do-
mains that involve challenging terminology, for
example: ?group reference attribute? and ?valida-
tion checking code?. In both instances, the base-
line MT system generated translations with an in-
correct word order and, consequently, possibly a
different meaning. We will return to these two ex-
amples in Section 5.1 to compare the output of a
MT system with and without reordering.
3.2 German
Unlike French and Spanish, German poses a con-
siderably different challenge with respect to word
ordering. The most frequent reordering in German
relates to verbs, particularly verb groups consist-
ing of auxiliary and main verbs, as well as verbs
in relative clauses. Moreover, reordering patterns
between German and English tend to span large
portions of the sentence. We included German in
our investigations to determine whether our auto-
mated rule extraction procedure can capture such
long distance patterns.
3.3 Hindi
Hindi word order is significantly different than
English word order; the typical order followed
is Subject Object Verb (although Object Subject
Verb order can be used if nouns are followed by
appropriate case markers). This is in contrast to
English which has a Subject Verb Object order.
This can result in words that are close in English
moving arbitrarily far apart in Hindi depending on
the length of the noun phrase representing the ob-
ject and the length of the verb phrase. These long
range reorderings are generally hard for a phrase
based system to capture. Another way Hindi and
English differ is that prepositions in English be-
come postpositions in Hindi and appear after the
noun phrase. Again, this reordering can lead to
long distance movements of words. We include
Hindi in our investigation since it has significantly
different structure as compared to English.
4 Learning reordering rules
In this section we describe how we learn rules that
transform source parse trees so the leaf word order
is more like the target language. We restrict our-
selves to reorderings that can be obtained by per-
muting child nodes at various interior nodes in a
parse tree. With many reordering phenomena dis-
cussed in Section 3 this is a fairly strong assump-
tion about pairs of languages, and there are exam-
ples in English?Hindi where such an assumption
will not allow us to generate the right reordering.
As an example consider the English sentence ?I
do not want to play?. The sentence has a parse:
S
NP
PRP
I
VP
VBP
do
RB
not
VP
VB
want
S
VP
TO
to
VP
VB
play
The correct word order of the translation in Hindi
is ?I to play not want? In this case, the word not
breaks up the verb phrase want to play and hence
the right Hindi word order cannot be obtained by
the reordering allowed by our model. We found
such examples to be rare in English?Hindi, and
we impose this restriction for the simplicity of the
model. Experimental results on several languages
show benefits of reordering in spite of this simpli-
fying assumption.
Consider a source sentence s and its corre-
sponding constituency parse tree S1. We set up
the problem in a probabilistic framework, i.e. we
would like to build a probabilistic model P (T |S)
that assigns probabilities to trees such that the
1In this paper we work with constituency parse trees. Ini-
tial experiments, applying similar techniques to dependency
parse trees did not yield improvements.
1121
word order in trees T which are assigned higher
probability match the order of words in the target
language. A parse tree, S is a set of nodes. Inte-
rior nodes have an ordered list of children. Leaf
nodes in the tree are the words in the sentence
s, and interior nodes are labeled by the linguis-
tic constituent that they represent. Each word has
a parent node (with only one child) labeled by the
part-of-speech tag of the word.
Our model assigns non-zero probabilities to
trees that can be obtained by permuting the child
nodes at various interior nodes of the tree S. We
assume that children of a node are ordered inde-
pendently of all other nodes in the tree. Thus
P (T |S) =
?
n?I(S)
P (?(cn)|S, n, cn),
where I(S) is the set of interior nodes in the tree
S, cn is the list of children of node n and ? is a
permutation. We further assume that the reorder-
ing at a particular node is dependent only on the
labels of its children:
P (T |S) =
?
n?I(S)
P (?(cn)|cn).
We parameterize our model using a log-linear
model:
P (?(cn)|cn) =
1
Z(cn)
exp(?T f(?, cn)). (1)
We choose the simplest possible set of feature
functions: for each observed sequence of non-
terminals we have one boolean feature per per-
mutation of the sequence of non-terminals, with
the feature firing iff that particular sequence is ob-
served. Assuming, we have a training corpus C of
(T, S) tree pairs, we could optimize the parame-
ters of our model to maximize :
?
S?C P (T |S).
With the simple choice of feature functions de-
scribed above, this amounts to:
P (?(cn)|cn) =
count(?(cn))
count(cn)
,
where count(cn) is the number of times the se-
quences of nodes cn is observed in the training
data and count(?(cn)) is the number of times
that cn in S is permuted to ?(cn) in T . In Sec-
tion 6, we show considering more general fea-
ture functions and relaxing some of the indepen-
dence might yield improvements on certain lan-
guage pairs.
For each source sentence s with parse S we find
the tree T that makes the given alignment for that
sentence pair most monotone. For each node n in
the source tree S let Dn be the set of words that
are descendants of n. Let us denote by tpos(n) the
average position of words in the target sentence
that are aligned to words in Dn. Then
tpos(n) = 1|Dn|
?
w?Dn
a(w),
where a(w) is the index of the word on the target
side that w is aligned with. If a word w is not
aligned to any target word, we leave it out from
the mean position calculation above. If a word w
is aligned to many words we let a(w) be the mean
position of the words that w is aligned to. For each
node n in the tree we transform the tree by sorting
the list of children of n according to tpos. The
pairs of parse trees that we obtain (S, T ) in this
manner form our training corpus to estimate our
parameters.
In using our model, we once again go for the
simplest choice, we simply reorder the source side
sentences by choosing arg maxT P (T |S) both in
training and in testing; this amounts to reordering
each interior node based on the most frequent re-
ordering of the constituents seen in training. To
reduce the effect of noise in training alignments
we apply the reordering, only if we have seen the
constituent sequence often enough in our training
data (a count threshold parameter) and if the most
frequent reordering is sufficiently more frequent
than the next most frequent reordering (a signifi-
cance threshold).
5 Experiments
5.1 Results for French, Spanish, and German
In each language, the rule extraction was
performed using approximately 1.2M sen-
tence pairs aligned using a maxent aligner
(Ittycheriah and Roukos, 2005) trained using a
variety of domains (Europarl, computer manuals)
1122
and a maximum entropy parser for English
(Ratnaparkhi, 1999). With a significance thresh-
old of 1.2, we obtain about 1000 rules in the
eventual reordering process.
Phrase-based systems were trained for each lan-
guage pair using 11M sentence pairs spanning a
variety of publicly available (e.g. Europarl, UN
speeches) and internal corpora (IT technical and
news domains). The system phrase blocks were
extracted based on a union of HMM and max-
ent alignments with corpus-selective count prun-
ing. The lexicalized distortion model was used
as described in (Al-Onaizan and Papineni, 2006)
with a window width of up to 5 and a maximum
number of skipped (not covered) words during de-
coding of 2. The distortion model assigns a prob-
ability to a particular word to be observed with
a specific jump. The decoder uses a 5-gram in-
terpolated language model spanning the various
domains mentioned above. The baseline system
without reordering and a system with reordering
was trained and evaluated in contrastive experi-
ments. The evaluation was performed utilizing the
following (single-reference) test sets:
? News: 541 sentences from the news domain.
? TechA: 600 sentences from a computer-
related technical domain, this has been used
as a dev set.
? TechB: 1038 sentences from a similar do-
main as TechA used as a blind test.
? Dev09: 1026 sentences defined as the news-
dev2009b development set of the Workshop
on Statistical Machine Translation 2009 2.
This set provides a reference measurement
using a public data set. Previously published
results on this set can be found, for example,
in (Popovic et al, 2009).
In order to assess changes in word ordering pat-
terns prior to and after an application of the re-
ordering, we created histograms of word jumps
in the alignments obtained in the baseline as well
as in the reordered system. Given a source word
si at index i and the target word tj it is aligned
to at index j, a jump of 1 would correspond to
si+1 aligning to target word tj+1, while an align-
ment to tj?1 corresponds to a jump of -1, etc. A
2http://statmt.org/wmt09/
?8 ?6 ?4 ?2 0 2 4 6 8 10
?1
?0.5
0
0.5
1
1.5
2
x 105
C
nt
2 
? 
C
nt
1
Difference of histograms after and before reordering (EN?ES)
?8 ?6 ?4 ?2 0 2 4 6 8 10
?5000
0
5000
10000
15000
20000
Distance to next position
C
nt
2 
? 
C
nt
1
Difference of histograms after and before reordering (EN?FR)
Figure 1: Difference-histogram of word order
distortions for English?Spanish (upper), and
English?French (lower).
histogram over the jump values gives us a sum-
mary of word order distortion. If all of the jumps
were one, then there is no reordering between the
two languages. To gain insight into changes in-
troduced by our reordering we look at differences
of the two histograms i.e., counts after reordering
minus counts before reordering. We would hope
that after reordering most of the jumps are small
and concentrated around one. Figure 1 shows
such difference-histograms for the language pairs
English?Spanish and English?French, respec-
tively, on a sample of about 15k sentence pairs
held out of the system training data. Here, a pos-
itive difference value indicates an increased num-
ber after reordering. In both cases a consistent
trend toward monotonicity is observed, i.e more
jumps of size one and two, and fewer large jumps.
This confirms the intended reordering effect and
indicates that the reordering rules extracted gen-
eralize well.
Table 1 shows the resulting uncased BLEU
scores for English-Spanish and English-French.
In both cases the reordering has a consistent
positive effect on the BLEU scores across test sets.
In examining the sources of improvement, we no-
ticed that word order in several noun phrases that
1123
System News TechA TechB Dev09
Baseline 0.3849 0.3371 0.3483 0.2244
Sp
an
ish
Reordered 0.4031 0.3582 0.3605 0.2320
Baseline 0.5140 0.2971 0.3035 0.2014
Fr
en
ch
Reordered 0.5242 0.3152 0.3154 0.2092
Baseline 0.2580 0.1582 0.1697 0.1281
G
er
m
an
Reordered 0.2544 0.1606 0.1682 0.1271
Baseline 20.0
H
in
di
Reordered 21.7
Table 1: Uncased BLEU scores for phrase-based
machine translation.
were not common in the training data were fixed
by use of the reordering rules.
Table 1 shows the BLEU scores for the
English?German language pair, for which a
mixed result is observed. The difference-
histogram for English?German, shown in Figure
2, differs from those of the other languages with
several increases in jumps of large magnitude, in-
dicating failure of the extracted rules to general-
ize.
The failure of our simple method to gain con-
sistent improvements comparable to Spanish and
French, along with our preliminary finding that a
relatively few manually crafted reordering rules
(we describe these in Section 6.4) tend to outper-
form our method, leads us to believe that a more
refined approach is needed in this case and will be
subject of further discussion below.
5.2 Results for Hindi
Our Hindi-English experiments were run with
an internal parallel corpus of roughly 250k sen-
tence pairs (5.5M words) consisting of various
domains (including news). To learn reordering
rules we used HMM alignments and a maxent
parser (Ratnaparkhi, 1999), with a count thresh-
old of 100, and a significance threshold of 1.7
(these settings gave us roughly 200 rules). We also
experimented with other values of these thresh-
olds and found that the performance of our sys-
tems were not very sensitive to these thresholds.
We trained Direct Translation Model 2 (DTM)
?10 ?5 0 5 10
?600
?400
?200
0
200
400
600
800
1000
1200
Distance to next position
Cn
t2
 ?
 C
nt
1
Difference of histograms after and before reordering (EN?DE)
Figure 2: Difference-histogram of word order dis-
tortions for English?German.
systems (Ittycheriah and Roukos, 2007) with and
without source reordering and evaluated on a test
set of 357 sentences from the News domain.
We note that the DTM baseline includes features
(functions of target words and jump size) that al-
low it to model lexicalized reordering phenomena.
The reordering window size was set to +/- 8 words
for the baseline and system with reordered in-
puts. Table 1 shows the uncased BLEU scores for
English-Hindi, showing a gain from using the re-
ordering rules. For the reordered case, the HMM
alignments are rederived, but the accuracy of these
were no better than those of the unreordered in-
put and experiments showed that the gains in per-
formance were not due to the effect on the align-
ments.
Figure 3 shows difference-histograms for the
language pair English?Hindi, on a sample of
about 10k sentence pairs held out of the system
training data. The histogram indicates that our
reordering rules generalize and that the reordered
English is far more monotonic with respect to the
Hindi.
6 Analysis of errors and future
directions
In this section, we analyze some of the sources of
errors in reordering rules learned via our model, to
better understand directions for further improve-
ment.
1124
?8 ?6 ?4 ?2 0 2 4 6 8 10
?1.5
?1
?0.5
0
0.5
1
1.5
x 104
Distance to next position
Cn
t2
 ?
 C
nt
1
Difference of histograms after and before reordering (EN?HI)
Figure 3: Difference-histogram of word order dis-
tortions for English?Hindi.
6.1 Model weakness
In our initial experiments, we noticed that for the
most frequent reordering rules in English?Hindi
(e.g that IN NP or NP PP flips in Hindi) the prob-
ability of a reordering was roughly 65%. This
was concerning since it meant that on 35% of the
data we would be making wrong reordering deci-
sions by choosing the most likely reordering. To
get a better feel for whether we needed a stronger
model (e.g by lexicalization or by looking at larger
context in the tree rather than just the children),
we analyzed some of the cases in our training data
where (IN,NP), (NP, PP) pairs were left unaltered
in Hindi. In doing that analysis, we noticed exam-
ples involving negatives that our model does not
currently handle. The first issue was mentioned
in Section 4, where the assumption that we can
achieve the right word order by reordering con-
stituent phrases, is incorrect. The second issue
is illustrated by the following sentences: I have
some/no books, which have similar parse struc-
tures, the only difference being the determiner
some vs the determiner no. In Hindi, the order
of the fragments some books and the fragment
no books are different (in the first case the words
stay in order, in the second the flip). Handling
this example would need our model to be lexical-
ized. These issue of negatives requiring special
handling also came up in our analysis of German
(Section 6.4). Other than the negatives (which re-
quire a lexicalized model), the major reason for
the lack of sharpness of the reordering rule proba-
bility was alignment errors and parser issues. We
Aligner
Number of
Sentences fMeasure BLEU score
HMM 250k 62.4 21.7
MaxEnt 250k 76.6 21.4
Manual 5k - 21.3
Table 2: Using different alignments
look at these topics next.
6.2 Alignment accuracy
Since we rely on automatically generated align-
ments to learn the rules, low accuracy of
the alignments could impact the quality of
the rules learned. This is especially a con-
cern for English?Hindi since the quality of
HMM alignments are fairly low. To quan-
tify this effect, we learn reordering rules us-
ing three sets of alignments: HMM alignments,
alignments from a supervised MaxEnt aligner
(Ittycheriah and Roukos, 2005), and hand align-
ments. Table 2 summarizes our results using
aligners with differing alignment qualities for our
English?Hindi task and shows that quality of
alignments in learning the rules is not the driving
factor in affecting rule quality.
6.3 Parser accuracy
Accuracy of the parser in the source language is
a key requirement for our reordering method, be-
cause we choose the single best reordering based
on the most likely parse of the source sentence.
This would especially be an issue in translat-
ing from languages other than English, where the
parser would not be of quality comparable to the
English parser.
In examining some of the errors in reordering
we did observe a fair fraction attributable to
issues in parsing, as seen in the example sentence:
The rich of this country , corner almost 90% of
the wealth .
The second half of the sentence is parsed by the
Berkeley parser (Petrov et al, 2006) as:
FRAG
NP-SBJ
NN
corner
ADVP
RB
almost
NP-SBJ
NP
CD
90%
PP
IN
of
NP
DT
the
NN
wealth
1125
and by IBM?s maximum entropy
parser parser (Ratnaparkhi, 1999) as:
VP
VB
corner
NP
NP
QP
RB
almost
CD
90%
PP
IN
of
NP
DT
the
NN
wealth
With the first parse, we get the right Hindi order
for the second part of the sentence which is: the
wealth of almost 90% corner . To investigate the
effect of choice of parser we compared using the
Berkeley parser and the IBM parser for reorder-
ing, and we found the BLEU score essentially
unchanged: 21.6 for the Berkeley parser and
21.7 for the IBM parser. A potential source of
improvements might be to use alternative parses
(via different parsers or n-best parses) to generate
n-best reorderings both in training and at test.
6.4 Remarks on German reordering
Despite a common heritage, German word order is
distinct from English, particularly regarding verb
placement. This difference can be dramatic, if an
auxiliary (e.g. modal) verb is used in conjunction
with a full verb, or the sentence contains a subor-
dinate clause. In addition to our experiments with
automatically learned rules, a small set of hand-
crafted reordering rules was created and evalu-
ated. Our preliminary results indicate that the lat-
ter rules tend to outperform the automatically de-
rived ones by 0.5-1.0 BLEU points on average.
These rules are summarized as follows:
1. In a VP immediately following an NP, move
the negation particle to main verb.
2. Move a verb group away from a modal verb;
to the end the of a VP. Negation also moves
along with verb.
3. Move verb group to end of an embed-
ded/relative clause.
4. In a VP following a subject, move negation
to the end of VP (handling residual cases)
The above hand written rules show several weak-
nesses of our automatically learned rules for re-
ordering. Since our model is not lexicalized, nega-
tions are not handled properly as they are tagged
RB (along with other adverbs). Another limitation
apparent from the first rule above (the movement
of verbs in a verb phrase depends on the previous
phrase being a noun phrase) is that the automatic
reordering rule for a node?s children depends only
on the children of that node and not a larger con-
text. For instance, a full verb following a modal
verb is typically parsed as a VP child node of the
modal VP node, hence the automatic rule, as cur-
rently considered, will not take the modal verb
(being a sibling of the full-verb VP node) into ac-
count. We are currently investigating extensions
of the automatic rule extraction alorithm to ad-
dress these shortcomings.
6.5 Future directions
Based on our analysis of the errors and on the
hand designed German rules we would like to ex-
tend our model with more general feature func-
tions in Equation 1 by allowing features: that
are dependent on the constituent words (or head-
words), that examine a large context than just a
nodes children (see the first German rule above)
and that fire for all permutations when the con-
stituent X is moved to the end (or start). This
would allow us to generalize more easily to learn
rules of the type ?move X to the end of the
phrase?. Another direction that we feel should be
explored, is the use of multiple parses to obtain
multiple reorderings and combine these at a later
stage.
7 Conclusions
In this paper we presented a simple method to
automatically derive rules for reordering source
sentences to make it look more like target
language sentences. Experiments (on inter-
nal and public test sets) indicate performance
gains for English?French, English?Spanish,
and English?Hindi. For English?German we
did not see improvements with automatically
learned rules while a few hand designed rules did
give improvements, which motivated a few direc-
tions to explore.
1126
References
[Al-Onaizan and Papineni2006] Al-Onaizan, Yaser and
Kishore Papineni. 2006. Distortion models for sta-
tistical machine translation. In Proceedings of ACL.
[Badr et al2009] Badr, Ibrahim, Rabih Zbib, and
James Glass. 2009. Syntactic phrase reordering for
english-to-arabic statistical machine translation. In
Proceedings of EACL.
[Chiang2005] Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine transla-
tion. In Proceedings of ACL.
[Collins et al2005] Collins, Michael, Philipp Koehn,
and Ivona Kucerova. 2005. Clause restructuring
for statistical machine translation. In Proceedings
of ACL.
[Habash2007] Habash, Nizar. 2007. Syntactic prepro-
cessing for statistical machine translation. In MT
Summit.
[Ittycheriah and Roukos2005] Ittycheriah, Abraham
and Salim Roukos. 2005. A maximum entropy
word aligner for arabic-english machine translation.
In Proceedings of HLT/EMNLP.
[Ittycheriah and Roukos2007] Ittycheriah, Abraham
and Salim Roukos. 2007. Direct translation model
2. In Proceedings of HLT-NAACL, pages 57?64.
[Koehn et al2003] Koehn, Philipp, Franz Och, and
Daniel Marcu. 2003. Statistical phrase-based trans-
lation. In Proceedings of HLT-NAACL.
[Niessen and Ney2001] Niessen, Sonja and Hermann
Ney. 2001. Morpho-syntactic analysis for reorder-
ing in statistical machine translation. In Proc. MT
Summit VIII.
[Petrov et al2006] Petrov, Slav, Leon Barrett, Romain
Thibaux, and Dan Klein. 2006. Learning accu-
rate, compact, and interpretable tree annotation. In
COLING-ACL.
[Popovic et al2009] Popovic, Maja, David Vilar,
Daniel Stein, Evgeny Matusov, and Hermann Ney.
2009. The RWTH machine translation system for
WMT 2009. In Proceedings of WMT 2009.
[Ramanathan et al2008] Ramanathan, A., P. Bhat-
tacharyya, J. Hegde, R. M. Shah, and M. Sasikumar.
2008. Simple syntactic and morphological process-
ing can help english-hindi statistical machine trans-
lation. In Proceedings of International Joint Con-
ference on Natural Language Processing.
[Ratnaparkhi1999] Ratnaparkhi, Adwait. 1999. Learn-
ing to parse natural language with maximum en-
tropy models. Machine Learning, 34(1-3).
[Tillman2004] Tillman, Christoph. 2004. A unigram
orientation model for statistical machine translation.
In Proceedings of HLT-NAACL.
[Wang et al2007] Wang, Chao, Michael Collins, and
Philipp Koehn. 2007. Chinese syntactic reordering
for statistical machine translation. In Proceedings
of EMNLP-CoNLL.
[Xia and McCord2004] Xia, Fei and Michael McCord.
2004. Improving a statistical mt system with auto-
matically learned rewrite patterns. In Proceedings
of Coling.
[Xu et al2009] Xu, Peng, Jaeho Kang, Michael Ring-
gaard, and Franz Och. 2009. Using a dependency
parser to improve SMT for Subject-Object-Verb lan-
guages. In Proceedings of NAACL-HLT.
[Yamada and Knight2001] Yamada, Kenji and Kevin
Knight. 2001. A syntax-based statistical translation
model. In Proceedings of ACL.
[Zhang et al2007] Zhang, Yuqi, Richard Zens, and
Hermann Ney. 2007. Chunk-level reordering
of source language sentences with automatically
learned rules for statistical machine translation. In
NAACL-HLT AMTA Workshop on Syntax and Struc-
ture in Statistical Translation.
[Zollmann and Venugopal2006] Zollmann, Andreas
and Ashish Venugopal. 2006. Syntax augmented
machine translation via chart parsing. In Pro-
ceedings on the Workshop on Statistical Machine
Translation.
[Zollmann et al2008] Zollmann, Andreas, Ashish
Venugopal, Franz Och, and Jay Ponte. 2008. A
systematic comparison of phrase-based, hierar-
chical and syntax-augmented statistical MT. In
Proceedings of COLING.
[Zwarts and Dras2007] Zwarts, Simon and Mark Dras.
2007. Syntax-based word reordering in phrase-
based statistical machine translation: why does it
work? In Proc. MT Summit.
1127
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 61?66,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
The OpenGrm open-source finite-state grammar software libraries
Brian Roark? Richard Sproat?? Cyril Allauzen? Michael Riley? Jeffrey Sorensen? & Terry Tai?
?Oregon Health & Science University, Portland, Oregon ?Google, Inc., New York
Abstract
In this paper, we present a new collection
of open-source software libraries that pro-
vides command line binary utilities and library
classes and functions for compiling regular
expression and context-sensitive rewrite rules
into finite-state transducers, and for n-gram
language modeling. The OpenGrm libraries
use the OpenFst library to provide an efficient
encoding of grammars and general algorithms
for building, modifying and applying models.
1 Introduction
The OpenGrm libraries1 are a (growing) collec-
tion of open-source software libraries for build-
ing and applying various kinds of formal gram-
mars. The C++ libraries use the OpenFst library2
for the underlying finite-state representation, which
allows for easy inspection of the resulting grammars
and models, as well as straightforward combination
with other finite-state transducers. Like OpenFst,
there are easy-to-use command line binaries for fre-
quently used operations, as well as a C++ library
interface, allowing library users to create their own
algorithms from the basic classes and functions pro-
vided.
The libraries can be used for a range of com-
mon string processing tasks, such as text normal-
ization, as well as for building and using large sta-
tistical models for applications like speech recogni-
tion. In the rest of the paper, we will present each of
the two libraries, starting with the Thrax grammar
compiler and then the NGram library. First, though,
we will briefly present some preliminary (infor-
mal) background on weighted finite-state transduc-
ers (WFST), just as needed for this paper.
1http://www.opengrm.org/
2http://www.openfst.org/
2 Informal WFST preliminaries
A weighted finite-state transducer consists of a set
of states and transitions between states. There is an
initial state and a subset of states are final. Each tran-
sition is labeled with an input symbol from an input
alphabet; an output symbol from an output alpha-
bet; an origin state; a destination state; and a weight.
Each final state has an associated final weight. A
path in the WFST is a sequence of transitions where
each transition?s destination state is the next transi-
tion?s origin state. A valid path through the WFST is
a path where the origin state of the first transition is
an initial state, and the the last transition is to a final
state. Weights combine along the path according to
the semiring of the WFST.
If every transition in the transducer has the same
input and output symbol, then the WFST represents
a weighted finite-state automaton. In the OpenFst
library, there are a small number of special sym-
bols that can be used. The  symbol represents the
empty string, which allows the transition to be tra-
versed without consuming any symbol. The ? (or
failure) symbol on a transition also allows it to be
traversed without consuming any symbol, but it dif-
fers from  in only allowing traversal if the symbol
being matched does not label any other transition
leaving the same state, i.e., it encodes the semantics
of otherwise, which is useful for language models.
For a more detailed presentation of WFSTs, see Al-
lauzen et al (2007).
3 The Thrax Grammar Compiler
The Thrax grammar compiler3 compiles grammars
that consist of regular expressions, and context-
dependent rewrite rules, into FST archives (fars) of
weighted finite state transducers. Grammars may
3The compiler is named after Dionysius Thrax (170?
90BCE), the reputed first Greek grammarian.
61
be split over multiple files and imported into other
grammars. Strings in the rules may be parsed
in one of three different ways: as a sequence of
bytes (the default), as utf8 encodings, or accord-
ing to a user-provided symbol table. With the
--save symbols flag, the transducers can be
saved out into fars with appropriate symbol tables.
The Thrax libraries provide full support for dif-
ferent weight (semiring) classes. The command-line
flag --semiring allows one to set the semiring,
currently to one of: tropical (default), log or log64
semirings.
3.1 General Description
Thrax revolves around rules which, typically, con-
struct an FST based on a given input. In the simplest
case, we can just provide a string that represents a
(trivial) transducer and name it using the assignment
operator:
pear = "pear";
In this example, we have an FST consisting of the
characters ?p?, ?e?, ?a?, and ?r? in a chain, assigned
to the identifier pear:
This identifier can be used later in order to build
further FSTs, using built-in operators or using cus-
tom functions:
kiwi = "kiwi";
fruits = pear | kiwi; # union
In Thrax, string FSTs are enclosed by double-quotes
(") whereas simple strings (often used as pathnames
for functions) are enclosed in single-quotes (?).
Thrax provides a set of built-in functions that
aid in the construction of more complex expres-
sions. We have already seen the disjunction ?|? in
the previous example. Other standard regular op-
erations are expr*, expr+, expr? and expr{m,n},
the latter repeating expr between m and n times,
inclusive. Composition is notated with ?@? so
that expr1 @ expr2 denotes the composition of
expr1 and expr2. Rewriting is denoted with ?:?
where expr1 : expr2 rewrites strings that match
expr1 into expr2. Weights can be added to expres-
sions using the notation ?<>?: thus, expr<2.4>
adds weight 2.4 to expr. Various operations on
FSTs are also provided by built-in functions, includ-
ing Determinize, Minimize, Optimize and
Invert, among many others.
3.2 Detailed Description
A Thrax grammar consists of a set of one or more
source files, each of which must have the extension
.grm. The compiler compiles each source file to a
single FST archive with the extension .far. Each
grammar file has sections: Imports and Body, each
of which is optional. The body section can include
statements interleaved with functions, as specified
below. Comments begin with a single pound sign
(#) and last until the next newline.
3.2.1 Imports
The Thrax compiler compiles source files (with
the extension .grm) into FST archive files (with
the extension .far). FST archives are an Open-
Fst storage format for a series of one or more FSTs.
The FST archive and the original source file then
form a pair which can be imported into other source
files, allowing a Python-esque include system that is
hopefully familiar to many. Instead of working with
a monolithic file, Thrax allows for a modular con-
struction of the final rule set as well as sharing of
common elements across projects.
3.2.2 Functions
Thrax has extensive support for functions that can
greatly augment the capabilities of the language.
Functions in Thrax can be specified in two ways.
The first is inline via the func keyword within grm
files. These functions can take any number of input
arguments and must return a single result (usually an
FST) to the caller via the return keyword:
func DumbPluralize[fst] {
# Concatenate with "s"...
result = fst "s";
# ...and then return to caller.
return result;
}
Alternatively, functions can be written C++ and
added to the language. Regardless of the func-
tion implementation method (inline in Thrax or
subclassed in C++), functions are integrated into
the Thrax environment and can be called directly
by using the function name and providing the
necessary arguments. Thus, assuming someone has
written a function called NetworkPluralize
that retrieves the plural of a word from some web-
site, one could write a grammar fragment as follows:
62
apple = "apple";
plural_apple = DumbPluralize[apple];
plural_tomato = NetworkPluralize[
"tomato",
?http://server:port/...?];
3.2.3 Statements
Functions can be interleaved with grammar state-
ments that generate the FSTs that are exported to the
FST archive as output. Each statement consists of an
assignment terminating with a semicolon:
foo = "abc";
export bar = foo | "xyz";
Statements preceded with the export keyword will
be written to the final output archive. Statements
lacking this keyword define temporaries that be used
later, but are themselves not output.
The basic elements of any grammar are string
FSTs, which, as mentioned earlier, are defined by
text enclosed by double quotes ("), in contrast to
raw strings, which are enclosed by single quotes (?).
String FSTs can be parsed in one of three ways,
which are denoted using a dot (.) followed by ei-
ther byte, utf8, or an identifier holding a symbol ta-
ble. Note that within strings, the backslash character
(\) is used to escape the next character. Of partic-
ular note, ?\n? translates into a newline, ?\r? into
a line feed, and ?\t? into the tab character. Literal
left and right square brackets also need escaping, as
they are used to generate symbols (see below). All
other characters following the backslash are unin-
terpreted, so that we can use \? and \? to insert an
actual quote (double) quote symbol instead of termi-
nating the string.
Strings, by default, are interpreted as sequences
of bytes, each transition of the resulting FST
corresponding to a single 1-byte character of the
input. This can be specified either by leaving off the
parse mode ("abc") or by explicitly using the byte
mode ("abc".byte). The second way is to use
UTF8 parsing by using the special keyword, e.g.:
Finally, we can load a symbol table and split
the string using the fst field separator flag
(found in fst/src/lib/symbol-table.cc)
and then perform symbol table lookups. Symbol ta-
bles can be loaded using the SymbolTable built-in
function:
arctic_symbol_table =
SymbolTable[?/path/to/bears.symtab?];
pb = "polar bear".arctic_symbol_table;
One can also create temporary symbols on the
fly by enclosing a symbol name inside brackets
within an FST string. All of the text inside the
brackets will be taken to be part of the symbol
name, and future encounters of the same symbol
name will map to the same label. By default, la-
bels use ?Private Use Area B? of the unicode ta-
ble (0x100000 - 0x10FFFD), except that the last two
code points 0x10FFFC and 0x10FFFD are reserved
for the ?[BOS]? and ?[EOS]? tags discussed below.
cross_pos = "cross" ("" : "_[s_noun]");
pluralize_nouns = "_[s_noun]" : "es";
3.3 Standard Library Functions and
Operations
Built-in functions are provided that operate on FSTs
and perform most of the operations that are available
in the OpenFst library. These include: closure, con-
catenation, difference, composition and union. In
most cases the notation of these functions follows
standard conventions. Thus, for example, for clo-
sure, the following syntax applies: fst* (accepts fst
0 or more times); fst+ (accepts fst 1 or more times);
fst? (accepts fst 0 or 1 times) fst{x,y} (accepts fst at
least x but no more than y times).
The operator ?@? is used for composition: a @
b denotes a composed with b. A ?:? is used to de-
note rewrite, where a : b denotes a transducer
that deletes a and inserts b. Most functions can also
be expressed using functional notation:
b = Rewrite["abc", "def"];
The delimiters< and> add a weight to an expres-
sion in the chosen semiring: a<3> adds the weight
3 (in the tropical semiring by default) to a.
Functions lacking operators (hence only called
by function name) include: ArcSort, Connect,
Determinize, RmEpsilon, Minimize,
Optimize, Invert, Project and Reverse.
Most of these call the obvious underlying OpenFst
function.
One function in particular, CDRewrite is worth
further discussion. This function takes a transducer
and two context acceptors (and the alphabet ma-
chine), and generates a new FST that performs a
context dependent rewrite everywhere in the pro-
vided contexts. The context-dependent rewrite algo-
rithm used is that of Mohri and Sproat (1996), and
63
see also Kaplan and Kay (1994). The fourth argu-
ment (sigma star) needs to be a minimized ma-
chine. The fifth argument selects the direction of
rewrite; we can either rewrite left-to-right or right-
to-left or simultaneously. The sixth argument selects
whether the rewrite is optional.
CDRewrite[tau, lambda, rho,
sigma_star,
?ltr?|?rtl?|?sim?,
?obl?|?opt?]
For context-dependent rewrite rules, two built-in
symbols ?[BOS]? and ?[EOS]? have a special mean-
ing in the context specifications: they refer to the
beginning and end of string, respectively.
There are also built-in functions that perform
other tasks. In the interest of space we concentrate
here on the StringFile function, which loads a
file consisting of a list of strings, or tab-separated
pairs of strings, and compiles them to an acceptor
that represents the union of the strings.
StringFile[?strings_file?]
While it is equivalent to the union of the individual
string (pairs), StringFile uses an efficient algo-
rithm for constructing a prefix tree (trie) from the
list and can be significantly more efficient than com-
puting a union for large lists. If a line consists of a
tab-separated pair of strings a, b, a transducer equiv-
alent to Rewrite[a, b] is compiled.
The optional keywords byte (default), utf8 or
the name of a symbol table can be used to specify
the parsing mode for the strings. Thus
StringFile[?strings_file?, utf8, my_symtab]
would parse a sequence of tab-separated pairs, using
utf8 parsing for the left-hand string, and the symbol
table my symtab for the right-hand string.
4 NGram Library
The OpenGrm NGram library contains tools for
building, manipulating and using n-gram language
models represented as weighted finite-state trans-
ducers. The same finite-state topology is used to en-
code raw counts as well as smoothed models. Here
we briefly present this structure, followed by details
on the operations manipulating it.
An n-gram is a sequence of n symbols: w1 . . . wn.
Each state in the model represents a prefix history
of the n-gram (w1 . . . wn?1), and transitions in the
model represent either n-grams or backoff transi-
tions following that history. Figure 1 lists conven-
tions for states and transitions used to encode the
n-grams as a WFST.
This representation is similar to that used in other
WFST-based n-gram software libraries, such as the
AT&T GRM library (Allauzen et al, 2005). One
key difference is the implicit representation of <s>
and </s>, as opposed to encoding them as symbols
in the grammar. This has the benefit of including all
start and stop symbol functionality while avoiding
common pitfalls that arise with explicit symbols.
Another difference from the GRM library repre-
sentation is explicit inclusion of failure links from
states to their backoff states even in the raw count
files. The OpenGrm n-gram FST format is consis-
tent through all stages of building the models, mean-
ing that model manipulation (e.g., merging of two
Figure 1: List of state and transition conventions used to encode collection of n-grams in WFST.
An n-gram is a sequence of n symbols: w1 . . . wn. Its proper prefixes include all sequences w1 . . . wk for k < n.
? There is a unigram state in every model, representing the empty string.
? Every proper prefix of every n-gram in the model has an associated state in the model.
? The state associated with an n-gram w1...wn has a backoff transition (labeled with ) to the state associated
with its suffix w2...wn.
? An n-gram w1...wn is represented as a transition, labeled with wn, from the state associated with its prefix
w1...wn?1 to a destination state defined as follows:
? If w1...wn is a proper prefix of an n-gram in the model, then the destination of the transition is the state
associated with w1...wn
? Otherwise, the destination of the transition is the state associated with the suffix w2...wn.
? Start and end of the sequence are not represented via transitions in the automaton or symbols in the symbol
table. Rather
? The start state of the automaton encodes the ?start of sequence? n-gram prefix (commonly denoted<s>).
? The end of the sequence (often denoted </s>) is included in the model through state final weights, i.e.,
for a state associated with an n-gram prefix w1...wn, the final weight of that state represents the weight
of the n-gram w1...wn</s>.
64
(a)
?
?
?
a/0
a/-1.1
b/-1.1
b/0
b/-0.69
a/-0.69
0
0
(b)
?/0.69
?/0.916
a/0.6
a/0.