2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 315?326,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Getting More from Morphology in Multilingual Dependency Parsing
Matt Hohensee and Emily M. Bender
University of Washington
Department of Linguistics
Box 354340
Seattle WA 98195-4340, USA
{hohensee, ebender}@uw.edu
Abstract
We propose a linguistically motivated set of
features to capture morphological agreement
and add them to the MSTParser dependency
parser. Compared to the built-in morphologi-
cal feature set, ours is both much smaller and
more accurate across a sample of 20 morpho-
logically annotated treebanks. We find in-
creases in accuracy of up to 5.3% absolute.
While some of this results from the feature set
capturing information unrelated to morphol-
ogy, there is still significant improvement, up
to 4.6% absolute, due to the agreement model.
1 Introduction
Most data-driven dependency parsers are meant to
be language-independent. They do not use any
information that is specific to the language being
parsed, and they often rely heavily on n-grams, or
sequences of words and POS tags, to make parsing
decisions. However, designing a parser without in-
corporating any specific linguistic details does not
guarantee its language-independence; even linguis-
tically na??ve systems can involve design decisions
which in fact bias the system towards languages with
certain properties (Bender, 2011).
It is often taken for granted that using linguistic
information necessarily makes a system language-
dependent. But it is possible to design a linguisti-
cally intelligent parser without tuning it to a specific
language, by modeling at a high level phenomena
which appear cross-linguistically. Such a system is
still language-independent; it does not require any
knowledge or modeling of specific languages, but
it does use linguistic knowledge to make the most
of the available data. We present modifications to
an existing system, MSTParser (McDonald et al,
2006), to incorporate a very simple model of mor-
phological agreement. These modifications improve
parsing performance across a variety of languages
by making better use of morphological annotations.
2 Background and related work
2.1 Morphological marking of agreement
Most languages show some morphological agree-
ment via inflected noun, adjective, verb, and deter-
miner forms, although the degree to which this hap-
pens varies. At one end of the spectrum are analytic,
or ?morphologically impoverished?, languages. An
extreme example is Chinese, which shows no inflec-
tion at all; words do not take different forms de-
pending on features such as person or gender. En-
glish has some inflection, but is relatively morpho-
logically poor.
At the other end are synthetic or ?morphologi-
cally rich? languages such as Czech, which has, inter
alia, four genders and seven cases. In synthetic lan-
guages, words which are syntactically related in cer-
tain ways must agree: e.g., subject-verb agreement
for gender or determiner-noun agreement for case
(Corbett, 2006). Words participating in agreement
may be marked explicitly for the property in ques-
tion (via affixing or other morphological changes),
or may possess it inherently (with no specific affix
encoding the property). Treebanks are often anno-
tated to reflect some or all of these properties; the
level of detail depends on the annotation guidelines.
315
zahranic?n?? investice rostou
foreign investment grow
.F.PL.NOM .F.3RD.PL.NOM .3RD.PL.PRES
foreign investments grow
foreign investment grow
.3RD.PL .PL
Table 1: Sentence in Czech (Hajic?, 1998) and English
A sample sentence in English and Czech (Table 1)
demonstrates this contrast. In Czech, the adjective
and noun agree for gender, number, and case, and
the noun and verb agree for person and number. In
the English version, only the noun and verb agree.
Agreement can be very useful for data-driven de-
pendency parsing. A statistical parser can learn from
training data that, for example, a third-person singu-
lar noun is a likely dependent of a verb marked as
third-person singular. Similarly, it can learn that a
determiner showing genitive case and a noun show-
ing dative case are often not syntactically related.
It is often assumed that morphological complex-
ity correlates with degree of variation in word order.
This is because synthetic languages use inflection to
mark the roles of constituents, while analytic lan-
guages generally assign these roles to specific phrase
structural locations. Siewierska (1998) investigated
this empirically and found that it holds to a certain
extent: the absence of agreement and/or case mark-
ing predicts rigid word order, though their presence
is not particularly predictive of flexible word order.
Many parsers rely on word order to establish de-
pendencies, so they often perform best on languages
with more rigid word order. Making use of mor-
phological agreement could compensate for greater
variation in word order and help to bring parsing per-
formance on flexible-word-order languages up to par
with that on rigid-word-order languages.
2.2 MSTParser
The CoNLL-X (Buchholz and Marsi, 2006) and
CoNLL 2007 (Nivre et al, 2007) shared tasks fo-
cused on multilingual dependency parsing. Each
system was trained on treebanks in a variety of lan-
guages and predicted dependency arcs and labels for
POS-tagged data. The best performers in 2006 were
MSTParser (McDonald et al, 2006), which we use
here, and MaltParser (Nivre et al, 2006a).
MSTParser is a data-driven, graph-based parser
which creates a model from training data by learn-
ing weights for arc-level features. The feature set in-
cludes combinations of the word and POS tag of the
parent and child of each dependency arc; POS tags
of words between the parent and child; and POS tags
of the parent and child along with those of the pre-
ceding and following words. A similar feature set is
conjoined with arc labels in order to perform label-
ing, and an optional set of ?second-order? features
includes analogous information about siblings.
Morphological features for an arc are generated
by iterating over each pair in the cross product of
the parent and child tokens? lists of attributes. For
every such pair, thirteen groups of four features each
are generated. The thirteen groups represent combi-
nations of the head and child word forms/lemmas
and attributes. Each group contains subgroups dis-
tinguished by whether they use word forms or lem-
mas and by whether or not they encode the direc-
tion and distance of the dependency. These features
are summarized in Table 2. At run time, MSTParser
finds the highest-scoring parse for each sentence ac-
cording to the learned feature weights.
Decoding can be performed in projective or non-
projective mode, depending on the type of trees de-
sired. Projective trees are those in which every con-
stituent (head plus all dependents) forms a complete
subtree; non-projective parsing lacks this limitation.
2.3 Related work
The organizers of the CoNLL 2007 shared task
noted that languages with free word order and high
morphological complexity are the most difficult for
dependency parsing (Nivre et al, 2007). Most of the
participants took language-independent approaches
toward leveraging this complexity into better perfor-
mance: generating machine learning features based
on each item in a token?s list of morphological at-
tributes (Nivre et al, 2006b; Carreras et al, 2006);
using the entire list as an atomic feature (Chang et
al., 2006; Titov and Henderson, 2007); or generat-
ing features based on each pair of attributes in the
cross-product of the lists of a potential head and de-
pendent (McDonald et al, 2006; Nakagawa, 2007).
Language-specific uses of morphological infor-
mation have included using it to disambiguate func-
tion words (Bick, 2006) or to pick out finite verbs
316
<hdIdx>*<dpIdx>=<{hdForm|hdLemma}>(<dir+dist>)
<hdIdx>*<dpIdx>=<{dpForm|dpLemma}>(<dir+dist>)
<hdIdx>*<dpIdx>=<hdAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<dpAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><{dpForm|dpLemma}>(<dir+dist>)
<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><hdAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><dpAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{dpForm|dpLemma}><dpAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{dpForm|dpLemma}><hdAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<hdAtt><dpAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><hdAtt><dpAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{dpForm|dpLemma}><hdAtt><dpAtt>(<dir+dist>)
<hdIdx>*<dpIdx>=<{hdForm|hdLemma}><{dpForm|dpLemma}><hdAtt><dpAtt>(<dir+dist>)
Table 2: Original MSTParser feature templates. hdForm and dpForm are the head and dependent word forms;
hdLemma and dpLemma are the lemmas. hdAtt and dpAtt are the morphological attributes; hdIdx and dpIdx
are their indices. dir+dist is a string encoding the direction and length of the arc. Each line represents one feature.
Unlabeled
<attr>_agrees,head=<headPOS>,dep=<depPOS>
<attr>_disagrees,head=<headPOS>,dep=<depPOS>
head_<attr=value>,head=<headPOS>,dep=<depPOS>
dep_<attr=value>,head=<headPOS>,dep=<depPOS>
Labeled
<attr>_agrees&label=<label>,head=<headPOS>,dep=<depPOS>
<attr>_disagrees&label=<label>,head=<headPOS>,dep=<depPOS>
head_<attr=value>&label=<label>,head=<headPOS>,dep=<depPOS>
dep_<attr=value>&label=<label>,head=<headPOS>,dep=<depPOS>
Table 3: Agreement feature templates. headPOS and depPOS are the head and dependent coarse POS tags.
(Carreras et al, 2006). Schiehlen and Spranger
(2007) used language-specific rules to add detail to
other features, such as fine-grained POS tags or lem-
mas. Attardi et al (2007) modeled agreement ex-
plicitly, generating a morphological agreement fea-
ture whenever two tokens possess the same value
for the same linguistic attribute. The authors note
accuracy improvements of up to 0.5% for Italian
and 0.8% for Catalan using a transition-based parser.
A similar approach was used by Goldberg and El-
hadad (2010), who improved the accuracy of their
transition-based Hebrew parser by adding features
for gender and number agreement in noun phrases.
The potential of morphological information to im-
prove parsing performance has been documented in
numerous experiments using MaltParser and with
various morphological attributes as machine learn-
ing features, on several morphologically rich lan-
guages, including: Russian (Nivre et al, 2008);
Swedish (?vrelid and Nivre, 2007); Bangla, Tel-
ugu, and Hindi (Nivre, 2009); Turkish (Eryig?it et
al., 2008); and Basque (Bengoetxea and Gojenola,
2010). These experiments, however, did not include
any higher-level features such as agreement.
Goldberg and Elhadad (2009) found that using
morphological features increased the accuracy of
MSTParser on Hebrew only when the morpholog-
ical annotations were gold-standard; automatic an-
notations decreased accuracy, although MaltParser
showed improvement with both gold and automatic
annotations. The accuracy of MaltParser on Arabic
was improved by different types of morphological
features depending on whether gold or automatic an-
notations were used (Marton et al, 2010).
As far as we can tell, no language-independent
approaches to utilizing morphological data thus far
have taken advantage of agreement specifically. We
take a linguistically informed approach, maintain-
ing language-independence, by explicitly modeling
agreement between head and dependent morphol-
ogy.
3 Methodology
3.1 Modifications to parser
Our approach builds on the observation that there
are two kinds of information marked in morphol-
ogy: symmetric, recorded on both head and depen-
317
ID TOKEN CPOS MORPH HEAD REL Gloss
1 Vznikaj?? VERB num=PL|per=3 0 ROOT arise.3RD.PL
2 zbytec?ne? ADJ num=PL|gen=I|case=NOM 3 ATR unnecessary.PL.INAN.NOM
3 konflikty NOUN num=PL|gen=I|case=NOM 1 SBJ conflicts.PL.INAN.NOM
num_agrees,head=NOUN,dep=ADJ num_agrees,head=VERB,dep=NOUN
num_agrees&label=ATR,head=NOUN,dep=ADJ num_agrees&label=SBJ,head=VERB,dep=NOUN
gen_agrees,head=NOUN,dep=ADJ head_per=3,head=VERB,dep=NOUN
gen_agrees&label=ATR,head=NOUN,dep=ADJ head_per=3&label=SBJ,head=VERB,dep=NOUN
case_agrees,head=NOUN,dep=ADJ dep_gen=I,head=VERB,dep=NOUN
case_agrees&label=ATR,head=NOUN,dep=ADJ dep_gen=I&label=SBJ,head=VERB,dep=NOUN
dep_case=NOM,head=VERB,dep=NOUN
dep_case=NOM&label=SBJ,head=VERB,dep=NOUN
Table 4: Sample sentence (Hajic?, 1998) and agreement features generated
dent, and asymmetric, marked on only one or the
other. Symmetric information provides a natural,
effectively non-lossy type of back-off that parsers
can take advantage of; all that matters is whether
the information on the head and dependent match.1
Furthermore, we don?t need to know ahead of time
which types of morphological information are sym-
metric. This is extracted from the annotations.
In order to take advantage of this property of nat-
ural language, we devised a set of features which
model agreement. These allow the learner to op-
erate at a higher level, using agreement itself as a
feature rather than having to discover agreement and
forming generalizations about whether tokens which
agree (or disagree) in various ways are related. Since
agreement appears cross-linguistically, such features
are applicable to a diverse set of languages.
Since MSTParser breaks down every parse into a
set of arcs, our features are defined at the arc level.
Each arc is a head and dependent pair, and each of
those tokens has a list of morphological features in
the normalized form attribute=value. We com-
pare these lists and add, for every attribute which
is present in both, either an agreement or a disagree-
ment feature, depending on whether the head and de-
pendent have the same value for that attribute. This
feature encapsulates the attribute, but not the value,
as well as the coarse POS tags of the head and the
dependent. If an attribute is present in only one of
1If an attribute is marked on both head and dependent and
the value matches, the specific value should not affect the prob-
ability or possibility of the dependency relationship. If the same
attribute is marked on both elements but is independent (not a
matter of agreement) we risk losing information, but we hypoth-
esize that such information is unlikely to be very predictive.
the lists, we add a feature encapsulating whether the
token is the head or the dependent, the single mor-
phological feature (attribute and value), and the two
coarse POS tags. We also generate both types of fea-
tures conjoined with the arc label. Like the original
feature set, we include only first-order morphologi-
cal features. See Table 3 for a summary. A sample
sentence in a simplified CoNLL format and the fea-
tures it would trigger are shown in Table 4.2
We hypothesize that these agreement features will
function as a type of back-off, allowing the parser
to extract more information from the morphological
marking. For instance, they can capture case agree-
ment between a determiner and noun. We expect
that this would lead to higher parsing accuracy, espe-
cially when training on smaller datasets, where mor-
phological data might be sparse.
We made a slight modification to the parser so that
underscores used in the treebanks to indicate the ab-
sence of morphological annotation for a token were
not themselves treated as morphological informa-
tion. This was necessary to ensure that all feature
configurations performed identically on treebanks
with no morphological information. Depending on
the treebank, this increased or decreased the perfor-
mance of the system slightly (by less than 0.5%).
3.2 Data collection and preparation
We gathered a range of dependency treebanks, rep-
resenting as many language families as possible (Ta-
ble 5). Many of these used the CoNLL shared task
treebank format, so we adopted it as well, and con-
2A more complete description of the system, as well as
source code, can be found in (Hohensee, 2012).
318
Language ISO Treebank Num.
sents.
Ref.
size
Avg.
atts.
Reference
Hindi-Urdu hin HUTB 3,855 2,800 3.6 (Bhatt et al, 2009)
Hungarian hun Szeged DTB 92,176 9,000 3.3 (Vincze et al, 2010)
Czech ces PDT 1.0 73,068 9,000 2.8 (Hajic?, 1998)
Tamil tam TamilTB v0.1 600 600 2.8 (Ramasamy and Z?abokrtsky?, 2011)
Slovene slv SDT 1,998 1,500 2.6 (Dz?eroski et al, 2006)
Danish dan DDT 5,512 5,500 2.4 (Kromann, 2003)
Basque eus 3LB* 3,175 2,800 2.4 (Aduriz et al, 2003)
Dutch nld Alpino 13,735 9,000 2.4 (Van der Beek et al, 2002)
Latin lat LDT 3,423 2,800 2.4 (Bamman and Crane, 2006)
Bulgarian bul BulTreeBank 13,221 9,000 2.1 (Simov et al, 2004)
Greek (ancient) grc AGDT 21,104 9,000 2.1 (Bamman et al, 2009)
Finnish fin Turku 4,307 2,800 2.0 (Haverinen et al, 2010)
German deu NEGRA 3,427 2,800 2.0 (Brants et al, 1999)
Turkish tur METU-Sabanci 5,620 5,500 1.6 (Oflazer et al, 2003)
Catalan cat CESS-ECE* 3,512 2,800 1.5 (Mart? et al, 2007)
Arabic ara PADT 1.0 2,367 2,300 1.2 (Hajic et al, 2004)
Italian ita TUT 2,858 2,800 1.1 (Bosco et al, 2000)
Portuguese por Floresta 9,359 9,000 1.0 (Afonso et al, 2002)
Hebrew (modern) heb DepTB 6,214 5,500 0.9 (Goldberg, 2011)
English eng Penn* 49,208 9,000 0.4 (Marcus et al, 1993)
Chinese cmn Penn Chinese 28,035 9,000 0.0 (Xue et al, 2005)
*Acquired as part of NLTK (Bird et al, 2009)
Table 5: Language, ISO 639-2 code, treebank name, total number of sentences, reference size, average number of
morphological attributes per token, and reference for each treebank used, ordered by average number of attributes.
verted the other treebanks to the same. It includes
for each token: position in the sentence; the token
itself; a lemma (not present in all datasets); a coarse
POS tag; a fine POS tag; a list of morphological fea-
tures; the token?s head; and the label for the depen-
dency relation to that head.3 We retained all punctu-
ation and other tokens in the treebanks.
The POS tagsets used in the treebanks varied
widely. We normalized the coarse tags to the univer-
sal twelve-tag set suggested by Petrov et al (2011),
in order to ensure that every treebank had coarse tags
for use in the agreement features, and to make the
features easier to interpret. It is unlikely that infor-
mation was lost in this process: for treebanks with
one set of tags, information was added, and for those
with two, the universal tags aligned closely with the
coarse tags already in the data.
Two of the treebanks we used included no mor-
phological information. We included the Penn Chi-
nese Treebank as a representative of analytic lan-
guages.4 We also included part of the (English) Penn
3The original format also included two more fields, projec-
tive head and label; neither is used by MSTParser.
4Dependency trees were generated from the Penn Chinese
Treebank, converted to dependency trees. For this
data we generated morphological annotations based
on fine POS tags, consisting of person and number
information for nouns and verbs, and person, num-
ber, and case information for pronouns. The German
NEGRA corpus includes detailed morphological an-
notations for about 3,400 sentences (of 20,600), and
we used only that portion.
Note that the amount of morphological informa-
tion present in any given treebank is a function of the
morphological properties of the language as well as
the annotation guidelines: annotations do not nec-
essarily encode all of the morphological informa-
tion which is actually marked in a language. Fur-
thermore, the presence of a morphological feature
does not imply that it participates in an agreement
relationship; it merely encodes some piece of mor-
phological information about the token. Finally, an-
notation guidelines vary as to whether they provide
for the explicit marking of morphological proper-
ties which are inherent to a lemma (e.g., gender on
nouns) and not marked by separate affixes.
Treebank using the Penn2Malt converter: http://w3.msi.
vxu.se/?nivre/research/Penn2Malt.html.
319
We normalized all morphological annotations to
the form attribute=value (e.g., case=NOM). For
treebanks that provided values only, this involved
adding attribute names, obtained from the annota-
tion guidelines. The attributes person, number, gen-
der, and case appeared often; also included in some
data were verb tense, adjective degree, and pronoun
type (e.g., personal, possessive, or reflexive). We
normalized all features in the data, regardless of
whether they participate in any agreement relations.
Many of the treebanks include data from multiple
domains; to minimize the effects of this, we random-
ized the order of sentences in each treebank.
3.3 Experimental setup
All experiments were performed using 5-fold cross-
validation. Reported accuracies, run times, and fea-
ture counts are averages over all five folds. We
ran experiments on multiple cross-validation dataset
sizes in order to assess the performance of our model
when trained on different amounts of data. For each
treebank, we report results on a ?reference size?:
9,000 sentences or the largest size available (for tree-
banks of less than 9,000 sentences).
For evaluation, we used the module built into
MSTParser. We focused on the unlabeled accu-
racy score (percentage of tokens with correctly as-
signed heads, ignoring labels). We also looked at
labeled accuracies, but found they displayed trends
very similar, if not identical, to the unlabeled scores.
4 Results
We ran the system on each treebank at all dataset
sizes in projective and non-projective modes, using
no morphological features. For each language, sub-
sequent tests used the algorithm which performed
better (or non-projective in the case of a tie).
4.1 Overall results
We ran the parser on each treebank with each of
four feature configurations: one with no morpho-
logical features (no-morph); one with the original
morphological features (orig; Table 2); one using
the agreement features (agr; Table 3); and one us-
ing both feature sets (agr+orig).
Table 6 displays the unlabeled accuracy, run time,
and feature counts when parsing each treebank using
each feature configuration at the reference size, with
the highest accuracy highlighted. Excluding Chi-
nese, agr generated the best performance in all but
two cases, outperforming orig by margins ranging
from 0.8% (Arabic) to 5.3% (Latin) absolute. In the
other cases, agr+orig outperformed agr slightly.
In all cases, the total number of machine learning
features was approximately the same for no-morph
and agr, and for orig and agr+orig, because
the number of morphological features generated by
orig is very large compared to the number gener-
ated by agr. Performance was noticeably faster for
the two smaller feature configurations.
Figure 1 shows the error reduction of orig, agr,
and agr+orig relative to no-morph, at the refer-
ence size. Despite its relative lack of morphological
inflection, English shows a fairly high error reduc-
tion, because parsing performance on English was
already high. Similarly, error reduction on some of
the morphologically rich languages is lower because
baseline performance was low. Calculating the cor-
relation coefficient (Pearson?s r) between average
morphological attributes per token and error reduc-
tion gives r = 0.608 for orig, r = 0.560 for agr,
and r = 0.428 for agr+orig, with p < 0.01 for
the first two and p < 0.10 for the last, indicating
moderate correlations for all feature sets.
The strength of these correlations depends on sev-
eral factors. Languages differ in what information is
marked morphologically, and in number of agree-
ment relationships. Annotation schemes vary in
what morphological information they encode, and in
how relevant that information is to agreement. Some
morphologically complex languages have rigid word
order, leading to better performance with no mor-
phological features at all, and limiting the amount
of improvement that is possible. Finally, it is pos-
sible that a stronger correlation is obscured by other
effects due to feature set design, as we will find later.
4.2 Performance vs. dataset size
Figures 2 presents unlabeled accuracy when parsing
Czech with the orig and agr configurations. Im-
provement with agr is roughly uniform across all
dataset sizes; this was the general trend for all tree-
banks. This is somewhat unexpected; we had pre-
dicted that the agreement features would be more
helpful at smaller dataset sizes.
320
no-morph orig agr agr+orig
Lang. UAC time feats UAC ?time ?feats UAC ?time ?feats UAC ?time ?feats
hin 90.0 1.4k 1.6m 92.0 116% 893% 93.8 50% 1% 93.0 144% 893%
hun 87.9 4.6k 5.3m 88.7 201% 687% 90.3 10% 0% 89.9 159% 687%
ces 80.9 3.3k 4.8m 81.6 71% 454% 85.5 27% 0% 84.5 114% 454%
tam 79.0 0.1k 0.5m 79.7 237% 329% 82.1 64% 1% 81.1 279% 330%
slv 80.8 0.8k 1.0m 80.4 103% 352% 81.9 21% 1% 80.8 129% 353%
dan 87.7 2.0k 1.6m 88.4 71% 256% 89.3 24% 0% 89.3 86% 256%
lat 61.7 1.8k 1.6m 65.0 54% 306% 70.3 91% 0% 68.6 119% 306%
nld 88.2 2.0k 3.6m 89.0 83% 270% 90.5 16% 0% 90.3 98% 270%
eus 78.7 0.7k 1.7m 80.2 80% 229% 82.3 10% 0% 82.3 78% 230%
bul 89.9 1.7k 2.6m 90.1 60% 221% 93.0 14% 0% 92.5 54% 222%
grc 74.9 8.6k 3.8m 76.9 36% 314% 80.7 45% 0% 79.5 70% 314%
deu 90.0 0.9k 1.3m 90.8 33% 189% 92.0 1% 0% 91.7 50% 186%
fin 73.3 0.7k 2.4m 76.3 74% 244% 79.1 23% 1% 78.7 84% 245%
tur 80.2 1.2k 2.1m 81.5 13% 178% 81.6 ?2% 0% 81.7 29% 178%
cat 81.8 3.0k 2.5m 81.9 2% 142% 84.9 -9% 0% 84.0 ?2% 143%
ara 78.0 3.2k 2.0m 78.1 65% 94% 78.9 23% 0% 78.7 20% 94%
ita 88.3 4.2k 1.8m 88.9 ?3% 59% 90.2 9% 0% 90.3 6% 59%
por 88.1 6.4k 5.0m 88.1 18% 46% 89.0 ?3% 0% 88.9 27% 46%
heb 87.4 4.3k 3.1m 87.4 ?18% 31% 89.2 ?16% 0% 89.1 ?5% 31%
eng 88.1 5.2k 3.1m 88.0 5% 7% 90.6 3% 0% 90.6 ?9% 8%
cmn 82.4 7.5k 6.0m 82.4 37% 0% 82.4 16% 0% 82.4 23% 0%
Table 6: Unlabeled accuracy, run time in seconds, and number of features for all treebanks and feature configurations.
Run time and number of features for orig, agr, and agr+orig are given as percent change relative to no-morph
4.3 Gold vs. automatic tags
The Hebrew treebank includes both automatically
generated and gold standard POS and morphological
annotations. In order to test how sensitive the agree-
ment features are to automatically predicted mor-
phological information, tests were run on both ver-
sions at the reference size. These results are not di-
rectly comparable to those of Goldberg and Elhadad
(2009), because of the parser modifications, POS tag
normalization, and cross-validation described ear-
lier. Comparing results qualitatively, we find less
sensitivity to the automatic tags overall, and that the
orig features improve accuracy even when using
automatic tags.
Results appear in Table 7. Using the automatic
data affects all feature sets negatively by 2.1% to
2.9%. Since the no-morph parser was affected the
most, it appears that this decrease is due largely
to errors in the POS tags, rather than the morpho-
logical annotations. The orig features compensate
for this slightly (0.2%), and the agr features more
(0.8%); this indicates that including even automatic
morphological information can compensate for in-
correct POS tags, and that the agr feature configu-
ration is the most robust when given predicted tags.
Feature
configuration
Acc. on
gold data
Acc. on
auto data
Difference
no-morph 87.4 84.5 ?2.9
orig 87.4 84.7 ?2.7
agr 89.3 87.2 ?2.1
agr+orig 89.1 86.9 ?2.2
Table 7: Unlabeled accuracy on Hebrew dataset, with
gold and automatic POS and morphological annotations
4.4 PPL feature
Examining the feature weights from the first cross-
validation fold when running the agr feature config-
uration on the Czech dataset indicated that 323 of the
1,000 highest-weighted features are agreement fea-
tures. Of these, 79 are symmetric (?agrees? or ?dis-
agrees?) agr features, and 244 asymmetric. This
was unexpected, as the symmetric features would
seem to be more useful, and it suggested that the la-
beled asymmetric agr features might be important
for reasons other than their modeling of morpholog-
ical information. Careful analysis of the MSTParser
feature set revealed that it does not include a fea-
ture which incorporates head POS, dependent POS,
and dependency label. We hypothesized that the la-
beled asymmetric agr features were highly ranked
321
Figure 1: Error reduction relative to no-morph vs. language
Figure 2: Unlabeled accuracy vs. num. sentences, Czech
because they capture these three arc features, not be-
cause they include with morphological information.
To test this, we added a single feature template
to MSTParser which encapsulates head POS, de-
pendent POS, and dependency label (the POS-POS-
label, or PPL, feature). Running a subsequent ex-
periment on the Czech data and looking at feature
weights from the same cross-validation fold, 278 of
the 1,000 highest-weighted features were PPL fea-
tures, and 187 were asymmetric agr features. This
indicated that the improvement seen with agr fea-
tures was indeed due partly to their inclusion of fea-
tures combining label and head and dependent POS.
All feature configurations were run on all tree-
banks with the PPL feature included; results appear
in Table 8. Performance increases from orig to
agr are generally smaller, with a maximum of 4.6%
absolute. This is seen especially on languages with
less morphological information, such as English and
Hebrew; this indicates that for those languages, most
of the previous improvement was due not to agree-
ment modeling, but to the PPL effect.
Calculating Pearson?s r between morphological
features per token and the new error reduction data
gives a stronger correlation coefficient of 0.748 for
agr, with p < 0.01, demonstrating that improve-
ment due solely to agreement modeling correlates
strongly with quantity of morphological informa-
tion. The earlier error reduction data were likely
polluted by improvement due to capturing the PPL
information. Correlation for the other feature con-
figurations is still moderate (0.506 with p < 0.02
for orig and 0.621 with p < 0.01 for agr+orig).
5 Future work
In future work, we plan to experiment with more
careful normalization of treebanks. For instance,
if an adjective can agree with either a masculine
or a feminine noun, annotating it with both gen=M
322
no-morph orig agr agr+orig
Lang. UAC time feats UAC ?time ?feats UAC ?time ?feats UAC ?time ?feats
hin 90.0 1.4k 1.6 92.0 116% 893% 93.8 50% 1% 93.0 144% 893%
hun 87.9 4.6k 5.3 88.7 201% 687% 90.3 10% 0% 89.9 159% 687%
ces 80.9 3.3k 4.8 81.6 71% 454% 85.5 27% 0% 84.5 114% 454%
tam 79.0 0.1k 0.5 79.7 237% 329% 82.1 64% 1% 81.1 279% 330%
slv 80.8 0.8k 1.0 80.4 102% 352% 81.8 21% 0% 80.8 129% 353%
dan 87.8 2.0k 1.6 88.4 71% 256% 89.3 24% 0% 89.3 86% 256%
lat 61.7 1.8k 1.6 65.0 54% 306% 70.3 91% 0% 68.6 119% 306%
nld 88.2 2.0k 3.6 89.0 83% 270% 90.5 16% 0% 90.3 98% 270%
eus 78.7 0.7k 1.7 80.2 80% 229% 82.3 10% 0% 82.3 78% 230%
bul 89.9 1.7k 2.6 90.2 60% 221% 93.0 14% 0% 92.5 54% 222%
grc 74.9 8.6k 3.8 77.0 36% 314% 80.7 45% 0% 79.5 70% 314%
deu 90.0 0.9k 1.3 90.8 33% 189% 92.0 1% 0% 91.7 50% 186%
fin 73.3 0.7k 2.4 76.3 74% 244% 79.1 23% 0% 78.7 84% 245%
tur 80.2 1.2k 2.1 81.5 13% 178% 81.6 -2% 0% 81.7 29% 178%
cat 81.8 3.0k 2.5 81.9 1% 142% 84.9 -9% 0% 84.0 -2% 143%
ara 77.6 5.4k 1.8 77.7 20% 100% 78.2 -8% 0% 78.0 4% 100%
ita 88.4 4.2k 1.8 88.9 -2% 59% 90.2 9% 0% 90.3 6% 59%
por 88.1 6.4k 5.0 88.2 18% 46% 89.0 -3% 0% 88.9 27% 46%
heb 87.4 4.3k 3.1 87.4 -18% 31% 89.2 -16% 0% 89.1 -5% 31%
eng 88.1 5.2k 3.1 88.0 5% 7% 90.6 3% 0% 90.6 -9% 7%
cmn 82.4 7.5k 6.0 82.4 37% 0% 82.4 16% 0% 82.4 23% 0%
Table 8: Unlabeled accuracy, run time in seconds, and number of features with PPL feature included. Run time and
number of features for orig, agr, and agr+orig are given as percent change relative to no-morph.
and gen=F (rather than gen=X) would ensure that
agreement with a noun of either gender would be
captured by our features. Furthermore, we may ex-
periment with filtering morphological information
based on part-of-speech, on attribute, or on whether
the attribute participates in any agreement relation-
ships. We also intend to perform feature selection on
the original feature set, and investigate the impor-
tance of labeled morphological features, which are
included in agr but not in orig. Finally, we plan to
develop metrics to measure the degree of word or-
der flexibility in a treebank, in order to explore the
extent to which it correlates with the degree of im-
provement achieved by our system.
6 Conclusions
We developed a simple, language-independent
model of agreement to better leverage morphologi-
cal data in dependency parsing. Testing on treebanks
containing varying amounts of morphological infor-
mation resulted in substantial improvements in pars-
ing accuracy while reducing feature counts and run
times significantly. Although originally intended to
compensate for lower accuracy on morphologically
rich languages, the model improved performance on
all treebanks with any morphological information.
We acknowledge that because our model was
tested on treebanks which differ widely in annota-
tion guidelines, variables such as the amount of mor-
phological information included and the treatment
of non-projective parses and coordination could af-
fect parsing performance. We did not delve into
these factors. However, we believe this is part of the
strength of the approach: we were able to achieve
performance gains without any detailed knowledge
of the languages and treebanks used.
We hope these results will encourage similarly
linguistically motivated design in future systems.
This case study provides strong evidence that in-
corporating linguistic knowledge into NLP systems
does not preclude language independence, and in-
deed may enhance it, by leveling performance across
typologically differing languages.
Acknowledgments
We would like to thank everyone who assisted us in
gathering treebanks, particularly Maite Oronoz and
her colleagues at the University of the Basque Coun-
try and Yoav Goldberg, as well as three anonymous
reviewers for their comments.
323
References
I. Aduriz, M.J. Aranzabe, J.M. Arriola, A. Atutxa, A.D.
de Ilarraza, A. Garmendia, and M. Oronoz. 2003.
Construction of a Basque dependency treebank. In
Proc. of the Second Workshop on Treebanks and Lin-
guistic Theories (TLT 2003), pages 201?204.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002.
Floresta Sinta?(c)tica: A treebank for Portuguese. In
Proc. of the Third International Conference on Lan-
guage Resources and Evaluation (LREC 2002), page
1698.
G. Attardi, F. DellOrletta, M. Simi, A. Chanev, and
M. Ciaramita. 2007. Multilingual dependency pars-
ing and domain adaptation using DeSR. In Proc. of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL 2007), pages
1112?1118.
D. Bamman and G. Crane. 2006. The design and use
of a Latin dependency treebank. In Proc. of the Fifth
International Workshop on Treebanks and Linguistic
Theories (TLT 2006), pages 67?78.
D. Bamman, F. Mambrini, and G. Crane. 2009. An own-
ership model of annotation: The Ancient Greek De-
pendency Treebank. In Proc. of the Eighth Interna-
tional Workshop on Treebanks and Linguistic Theories
(TLT8), pages 5?15.
E.M. Bender. 2011. On achieving and evaluating
language-independence in NLP. Linguistic Issues in
Language Technology: Special Issue on Interaction of
Linguistics and Computational Linguistics, 6(3):1?26.
K. Bengoetxea and K. Gojenola. 2010. Application of
different techniques to dependency parsing of Basque.
In Proc. of the First Workshop on Statistical Parsing
of Morphologically Rich Languages (SPMRL 2010),
pages 31?39. Association for Computational Linguis-
tics.
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D.M.
Sharma, and F. Xia. 2009. A multi-representational
and multi-layered treebank for Hindi/Urdu. In Proc. of
the Third Linguistic Annotation Workshop (LAW III),
pages 186?189. Association for Computational Lin-
guistics.
E. Bick. 2006. LingPars, a linguistically inspired,
language-independent machine learner for depen-
dency treebanks. In Proc. of the Tenth Conference on
Computational Natural Language Learning (CoNLL-
X), pages 171?175. Association for Computational
Linguistics.
S. Bird, E. Klein, and E. Loper. 2009. Natural Language
Processing with Python. O?Reilly Media.
C. Bosco, V. Lombardo, D. Vassallo, and L. Lesmo.
2000. Building a treebank for Italian: a data-driven
annotation schema. In Proc. of the Second Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2000), pages 99?106.
T. Brants, W. Skut, and H. Uszkoreit. 1999. Syntactic
annotation of a German newspaper corpus. Treebanks:
Building and using parsed corpora, 20:73.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
the Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 149?164. Associa-
tion for Computational Linguistics.
X. Carreras, M. Surdeanu, and L. Marquez. 2006. Pro-
jective dependency parsing with perceptron. In Proc.
of the Tenth Conference on Computational Natural
Language Learning (CoNLL-X), pages 181?185. As-
sociation for Computational Linguistics.
M.W. Chang, Q. Do, and D. Roth. 2006. A pipeline
model for bottom-up dependency parsing. In Proc. of
the Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 186?190. Associa-
tion for Computational Linguistics.
G.G. Corbett. 2006. Agreement. Cambridge University
Press.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas,
Z. Z?abokrtsky, and A. Z?ele. 2006. Towards a Slovene
dependency treebank. In Proc. of the Fifth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC 2006).
G. Eryig?it, J. Nivre, and K. Oflazer. 2008. Depen-
dency parsing of Turkish. Computational Linguistics,
34(3):357?389.
Y. Goldberg and M. Elhadad. 2009. Hebrew de-
pendency parsing: Initial results. In Proc. of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 129?133. Association for Com-
putational Linguistics.
Y. Goldberg and M. Elhadad. 2010. Easy-first depen-
dency parsing of Modern Hebrew. In Proc. of the First
Workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL 2010), pages 103?107. As-
sociation for Computational Linguistics.
Yoav Goldberg. 2011. Automatic Syntactic Processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University.
J. Hajic, O. Smrz, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR Inter-
national Conference on Arabic Language Resources
and Tools, pages 110?117.
Jan Hajic?. 1998. Building a syntactically annotated
corpus: The Prague Dependency Treebank. In Eva
Hajic?ova?, editor, Issues of Valency and Meaning:
Studies in Honor of Jarmila Panevova?, pages 12?19.
Prague Karolinum, Charles University Press.
324
Katri Haverinen, Timo Viljanen, Veronika Laippala,
Samuel Kohonen, Filip Ginter, and Tapio Salakoski.
2010. Treebanking Finnish. In Proc. of the Ninth
International Workshop on Treebanks and Linguistic
Theories (TLT9, volume 9, pages 79?90.
M. Hohensee. 2012. It?s only morpho-logical: Model-
ing agreement in cross-linguistic dependency parsing.
Master?s thesis, University of Washington.
M.T. Kromann. 2003. The Danish Dependency Tree-
bank and the DTAG treebank tool. In Proc. of the Sec-
ond Workshop on Treebanks and Linguistic Theories
(TLT 2003), pages 217?220.
M.P. Marcus, M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguistics,
19(2):313?330.
M.A. Mart?, M. Taule?, L. Ma?rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel an-
notated corpus.
Y. Marton, N. Habash, and O. Rambow. 2010. Improv-
ing Arabic dependency parsing with lexical and inflec-
tional morphological features. In Proc. of the First
Workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL 2010), pages 13?21. Asso-
ciation for Computational Linguistics.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discrim-
inative parser. In Proc. of the Tenth Conference on
Computational Natural Language Learning (CoNLL-
X), pages 216?220. Association for Computational
Linguistics.
T. Nakagawa. 2007. Multilingual dependency pars-
ing using global features. In Proc. of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2007), pages 952?
956.
J. Nivre, J. Hall, and J. Nilsson. 2006a. Maltparser: A
data-driven parser-generator for dependency parsing.
In Proc. of the Fifth International Conference on Lan-
guage Resources and Evaluation (LREC 2006), vol-
ume 6, pages 2216?2219.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Mari-
nov. 2006b. Labeled pseudo-projective dependency
parsing with support vector machines. In Proc. of
the Tenth Conference on Computational Natural Lan-
guage Learning (CoNLL-X), pages 221?225. Associa-
tion for Computational Linguistics.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. CoNLL 2007 shared
task on dependency parsing. In Proc. of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL 2007). Association
for Computational Linguistics.
J. Nivre, I.M. Boguslavsky, and L.L. Iomdin. 2008. Pars-
ing the SynTagRus treebank of Russian. In Proc. of the
22nd International Conference on Computational Lin-
guistics (COLING 2008), volume 1, pages 641?648.
Association for Computational Linguistics.
J. Nivre. 2009. Parsing Indian languages with Malt-
Parser. In Proc. of the Seventh International Confer-
ence on Natural Language Processing (ICON 2009)
NLP Tools Contest, pages 12?18.
K. Oflazer, B. Say, D.Z. Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. Text, Speech, and Lan-
guage Technology, pages 261?277.
L. ?vrelid and J. Nivre. 2007. When word order and
part-of-speech tags are not enough?Swedish depen-
dency parsing with rich linguistic features. In Proc. of
the International Conference on Recent Advances in
Natural Language Processing (RANLP), pages 447?
451.
S. Petrov, D. Das, and R. McDonald. 2011. A
universal part-of-speech tagset. Arxiv preprint
ArXiv:1104.2086.
Loganathan Ramasamy and Zdene?k Z?abokrtsky?. 2011.
Tamil dependency parsing: Results using rule based
and corpus based approaches. In Proc. of the 12th
International Conference on Intelligent Text Process-
ing and Computational Linguistics (CICLING 2011),
volume 1, pages 82?95, Berlin, Heidelberg. Springer-
Verlag.
M. Schiehlen and K. Spranger. 2007. Global learn-
ing of labelled dependency trees. In Proc. of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL 2007), pages
1156?1160.
Anna Siewierska. 1998. Variation in major constituent
order: A global and a European perspective. In
Anna Siewierska, editor, Constituent Order in the
Languages of Europe, pages 475?551. Mouton De
Gruyter.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov.
2004. Design and implementation of the Bulgar-
ian HPSG-based treebank. Research on Language &
Computation, 2(4):495?522.
I. Titov and J. Henderson. 2007. Fast and robust mul-
tilingual dependency parsing with a generative latent
variable model. In Proc. of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL 2007), pages 947?951.
L. Van der Beek, G. Bouma, R. Malouf, and G. Van No-
ord. 2002. The Alpino dependency treebank. Lan-
guage and Computers, 45(1):8?22.
325
V. Vincze, D. Szauter, A. Alma?si, G. Mo?ra, Z. Alexin,
and J. Csirik. 2010. Hungarian dependency treebank.
In Proc. of the Seventh Conference on Language Re-
sources and Evaluation (LREC 2010).
N. Xue, F. Xia, F.D. Chiou, and M. Palmer. 2005. The
Penn Chinese Treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
11(2):207?238.
326
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 86?95,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Email Formality in the Workplace: A Case Study on the Enron Corpus
Kelly Peterson, Matt Hohensee, and Fei Xia
Linguistics Department
University of Washington
Seattle, WA 98195
{kellypet, hohensee, fxia}@uw.edu
Abstract
Email is an important way of communication
in our daily life and it has become the subject
of various NLP and social studies. In this pa-
per, we focus on email formality and explore
the factors that could affect the sender?s choice
of formality. As a case study, we use the En-
ron email corpus to test how formality is af-
fected by social distance, relative power, and
the weight of imposition, as defined in Brown
and Levinson?s model of politeness (1987).
Our experiments show that their model largely
holds in the Enron corpus. We believe that
the methodology proposed in the paper can be
applied to other social media domains and be
used to test other linguistic or social theories.
1 Introduction
Email has become an important way of communica-
tion in our daily life. Because of its wide usage,
it has been the subject of various studies such as
social network analysis (e.g., (Leuski, 2004; Dies-
ner et al, 2005; Carvalho et al, 2007)), deception
detection (e.g., (Zhou et al, 2004; Keila and Skill-
corn, 2005)), information extraction (e.g., (Culotta
et al, 2004; Minkov et al, 2005)), and topic discov-
ery (e.g., (McCallum et al, 2007)). In this study, we
focus on email formality in various social settings;
that is, we want to determine whether the choice of
formality in email communication is affected by fac-
tors such as the social distance and relative power
between the senders and the recipients.
While an early perspective of email communica-
tion held that email is a lean medium which lacks vi-
tal social cues (Daft and Lengel, 1986), other work
has shown that senders of email exhibit a wide range
of language and form choices which vary in differ-
ent social contexts (Orlikowski and Yates, 1994).
Through various theories of sociolinguistics, it is
proposed that these changes take place in a pre-
dictable manner.
Brown and Levinson (1987) have proposed a
model where in order to save the ?face? or public
self image of the hearer of a message, a speaker can
employ a range of verbal strategies. Their model
of politeness states that in social situations there
are three factors which are considered in a decision
whether or when to use communication techniques
such as formality:
1. The ?social distance? between the participants
as a symmetric relation
2. The relative ?power? between the participants
as an asymmetric relation
3. The weight of an imposition such as a request
Abdullah (2006) examines email interactions
from the perspective of Brown and Levinson?s po-
liteness model in a Malaysian corporation from over
180 participants and a corpus of 770 email mes-
sages. This work directly examines the factors men-
tioned previously which influence email formality.
Unfortunately, the methodology and data were not
provided for this study.
The goal of our work is to test whether Brown
and Levinson?s model holds in a real setting with a
much larger data set. In this study, we chose the En-
ron Email Corpus as our dataset. We first built two
classifiers: one labels an email as formal or informal
86
and the other determines whether an email contains
a request. Next, we used the classifiers to label ev-
ery email in the Enron corpus. Finally, we tested
whether the three factors in Brown and Levinson?s
theory indeed affect formality in email communica-
tion. While we consider the work a case study, we
believe that the methodology proposed in the paper
can be applied to other social media domains and be
used to test other linguistic or social theories.1
2 Overview of the Enron email corpus
The Enron email corpus, which consists of hundreds
of thousands of emails from over a hundred Enron
employees over a period of 3.5 years (1998 to 2002),
was made public during the US government?s legal
investigation of Enron. The corpus was first pro-
cessed and released by Klimt and Yang (2004) at
Carnegie Mellon University (CMU), and this CMU
dataset has later been re-processed by several other
research groups. In this section, we briefly introduce
the datasets that we used in our experiments.
2.1 The ISI dataset
The CMU dataset contains many duplicates. It was
later processed and cleaned by Shetty and Adibi
at ISI and released as a relational database. The
ISI database comprises 252,759 messages from the
email folders of 150 employees (Shetty and Adibi,
2004).2 We use the ISI dataset as the starting point
for all of our experiments except for the one in Sec-
tion 5.1.
2.2 The Sheffield dataset
The Enron email corpus contains both personal and
business emails. In 2006, Jabbari and his colleagues
at the University of Sheffield manually annotated
a subset of the emails in the CMU dataset with
?Business? or ?Personal? categories (Jabbari et al,
2006). The subset contains 14,818 emails and 3,598
of them (24.2%) are labeled as ?personal?.3 We use
this dataset in the personal vs. business experiment
1Our data including annotations and results can be found at
http://students.washington.edu/kellypet/enron-formality/
2The dataset can be downloaded from
http://www.isi.edu/?adibi/Enron/Enron.htm
3The dataset is available at
http://staffwww.dcs.shef.ac.uk/people/L.Guthrie/nlp/research.htm.
as described in Section 5.1.4
2.3 The ISI Enron employee position table
In addition to the ISI database, ISI also provided a
table of 161 employees and their positions in the
company.5 In Section 5.3, we study the effect of
seniority on the formality of a message, and we use
this table to determine the relative seniority between
senders and recipients of a given email.
3 Creating the gold standard
In this study, we build two classifiers: a formality
classifier that determines whether an email is formal,
and a request classifier that determines whether an
email contains a request. In order to train and eval-
uate the classifiers, 400 email messages were ran-
domly chosen from the Enron corpus and manually
labeled for formality and request.
3.1 Formality annotation
Formality is a concept which is difficult to define
precisely and human judgment on whether an email
is formal can be subjective. To determine how much
human annotators can agree on the concept, we
asked three annotators to label 100 out of the 400
emails with four labels: ?very formal?, ?somewhat
formal?, ?somewhat informal?, and ?very informal?.
Because formality is hard to define, we did not
give annotators a concrete definition. Instead, we
provided a few guidelines and asked annotators to
follow the guidelines and their intuition. One of
these guidelines was that the formality of an email
should not necessarily be dictated by the relationship
between the sender and the recipient if their rela-
tionship can be inferred from the message. Another
guideline stressed that the nature of an email being
business or personal should not necessarily dictate
its formality. Other than these guidelines, annota-
tors were asked to come up with their own criteria
for formality while doing the annotation.
Table 1 shows the agreement between each anno-
tator pair and the average score of the three pairs.
For agreement, we calculated the accuracy, which
4The ISI dataset and the Sheffield dataset contain significant
overlap as both were derived from the CMU dataset, but the
former is not necessarily a superset of the latter.
5We downloaded the table in January 2011 from
http://www.isi.edu/?adibi/Enron/Enron Employee Status.xls
87
Annotator 2-way 4-way
pair Agreement Agreement
(Acc/F1) (Acc)
A vs. B 87.3/77.8 53.7
A vs. C 85.4/77.2 40.6
B vs. C 84.5/72.9 36.1
Pairwise Ave 85.7/76.0 43.5
Table 1: Inter-annotator agreement for formality annota-
tion
is the percentage of emails that receive the same
label from the two annotators. 2-way agreement
means the agreement is calculated after the label
very formal has been merged with somewhat formal,
and very informal with somewhat informal; 4-way
agreement means that the agreement is calculated
with the four formality labels used by the annota-
tors. With the 2-way distinction (formal vs. infor-
mal), we also calculate the f-score for identifying
informal emails, treating one annotation as the gold
standard and the other as the system output. This
table shows that, although the concept of formality
is intuitive, the inter-annotator agreement on formal-
ity is pretty low (especially when making the 4-way
distinction).
Finally, Annotator A, who had the highest agree-
ment with other annotators, annotated the remain-
ing 300 emails, and his annotation was treated as the
gold standard for our formality classifier.
3.2 Request annotation
In order to train and evaluate our request classifier,
we asked two annotators to go over the same 400
emails and label each message as containing-request
or no-request. A message is considered to contain a
request if it is clear that the sender of the message
expects the recipient to take some action to respond
to the message. For instance, if a message includes
a question such as what do you think? or a request
such as please call me tomorrow, it should be la-
beled as containing-request as the sender expects the
recipient to call the sender or answer the question.
Our definition is slightly different from the defini-
tion of request used in speech acts, and it can be
seen as a synonym of require-action.
While some emails clearly contain requests and
others clearly do not, there is some gray area in be-
tween, which results in the disagreement between
the annotators. Many of the disagreed emails in-
clude sentences such as Let me know if you have
any questions. This very commonly used expression
is itself ambiguous between the meanings ?Let me
know whether you have any questions? and ?If you
have any questions, please inform me of that fact?.
Furthermore, this sentence often appears as a marker
of politeness or an offer to clarify further, rather than
a request for action. So the correct label of an email
containing this expression depends on the context.
For the 400 messages, the two annotators agreed
on 361 messages, for an inter-annotator agreement
of 90.3% and a F1-score of 87.9% for identifying
emails that contain requests.
4 Building classifiers
In this section, we discuss the feature sets used for
the two classifiers and report their performances.
4.1 Data pre-processing
Before forming the feature vectors for the classi-
fiers, we preprocessed all the emails in the ISI and
Sheffield dataset in several steps. First, we removed
any replied or forwarded message from the email
body as we want to use only the text written by the
sender. If the email body becomes empty after this
step, the email is excluded from the analysis con-
ducted in Section 5. After this step, the size of the
ISI dataset reduces from 252,759 to 232,815 emails,
and the size of the Sheffield dataset changes from
14,818 to 13,882 emails. Second, the email mes-
sages were segmented into sentences and tokenized
with tools in the NLTK package (Bird et al, 2009).
4.2 Formality classifier
For the formality classifier, we use two labels: for-
mal and informal.
4.2.1 Features for formality
During formality annotation, after the 100 emails
had been annotated, the three annotators were asked
to provide a few paragraphs describing their criteria
for formality. In these criteria, more cues are indi-
cators of informality (e.g., the use of vulgar words)
than indicators of formality. We use the follow-
ing features to capture the informal ?style? of the
88
emails:6
F1: Informal Word Features, which check the oc-
currences of informal words (see the next sec-
tion for detail)
F2: Punctuation Features:
? Exclamation Points (?!?)
? Absence of sentence final punctuation
? Frequency of ellipsis (?. . . ?)
F3: Case features:
? All lowercase Subject line
? Frequency of sentences which were en-
tirely lower case
? Frequency of sentences whose first word
is lower case
4.2.2 Informal words
We designed a simple heuristic method to ex-
tract a list of informal words from the Enron cor-
pus. First, we collect all the unigrams in the Enron
corpus. Second, we retrieve the information about
each unigram from Wordnik,7 a website that pro-
vides access to retrieve word definitions from mul-
tiple source dictionaries. Among the several dictio-
naries crawled by Wordnik, we find Wiktionary to be
the best source for our task since its labels on word
definitions such as ?informal?, ?offensive?, ?vulgar?,
?colloquial? and ?misspelling? were the most con-
sistent and relevant to our definition of ?formality?.
In addition to these labels, the part of speech cate-
gory for ?interjection? was also used to determine if
a word might be considered informal in email com-
munication. Third, we use the gathered word defini-
tions to determine whether a word is informal.
One issue with the last step is that words often
have multiple meanings and some meanings are in-
formal and others are not. For instance, the word
bloody can be formal or informal depending on
which meaning is used in an email. As word sense
disambiguation is out of the scope of this work, we
use some simple heuristics to determine whether
a word should be treated as informal or not. In
essence, the process treats a word as informal if a
6We did not use ngram features as they might be too specific
to the small training data we have and might not work well when
applied to other emails in the Enron corpus or emails in other
domain.
7http://www.wordnik.com
large percentage of definitions for the word have cer-
tain labels (e.g., vulgar, offensive, and misspelling)
or certain part-of-speech tags (e.g., interjection).8
4.2.3 Performance of the formality classifier
We trained a Maximum Entropy (MaxEnt) classi-
fier in the Mallet package (McCallum, 2002). Table
2 shows classification accuracy and precision, recall,
and F1-score for identifying informal emails. The
baseline system labels every email as formal because
62.7% of the emails in the dataset were annotated
as formal; its F1-score is zero as the recall is zero.
The numbers for the inter-annotator agreement row
are copied from the pairwise average of the 2-way
agreement in Table 1. The table shows that, with
very few features, the performance of the formal-
ity classifier is much better than the baseline and is
close to inter-annotator agreement. All three types
of features beat the baselines and combining them
provides additional improvement.
Acc Prec Rec F1
Baseline 62.7 - - -
Inter-annotator
agreement
85.7 89.5 66.8 76.0
F1: Informal words 69.2 75.0 26.7 39.3
F2: Punctuation 74.4 82.5 45.8 58.9
F3: Case features 69.7 80.0 26.5 39.8
F1+F2 76.4 77.3 51.1 61.5
F1+F3 72.8 74.3 39.4 51.5
F2+F3 80.3 85.2 59.7 70.2
F1+F2+F3 80.6 85.7 62.1 72.0
Table 2: Performance of the formality classifier. We use
10-fold cross validation on the 400 emails. Baseline: la-
bel every email as formal.
4.3 Request classifier
The request classifier uses two labels: containing-
request and no-request.
8We manually checked the list of informal words extracted
and estimated that the number the false positives is less than 1%.
However, the list is definitely not complete as many informal
words in the Enron corpus do not appear in the dictionaries used
by Wordnik.
89
0.60
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00
1-gram 2-gram 3-gram 4-gram 5-gram
Top 5000
Top 1000
At least 5
At least 10
Baseline
Figure 1: Accuracy of the request classifier with different
feature sets
4.3.1 Features for request
There has been considerable research into cate-
gorizing email messages by function. Cohen, Car-
valho, and Mitchell (2004) described the classifica-
tion of email into ?email speech acts?, building on
the speech act theory of Searle (1975). Carvalho and
Cohen (2006) achieved high-precision results cate-
gorizing messages into categories such as ?request?
and ?proposal? when preprocessing the text in cer-
tain ways and using unigram, bigram, and trigram
features only.
Unlike formality, which is more about the style of
the messages (e.g., whether the email is all in lower-
case), the content words are more relevant for iden-
tifying requests. Following the work in (Carvalho
and Cohen, 2006), we used word ngrams as features.
To prevent the features from being too specific to
the small training data, we experimented with two
ways of feature selection: by feature counts and by
chi-square scores. N-grams were extracted from the
email body only. For pre-processing, in addition to
the pre-processing step mentioned in 4.1, we also re-
placed some name entities (e.g., numbers and dates)
with special labels and lowercased the text.
4.3.2 Performance of the request classifier
We trained a MaxEnt classifier and ran 10-fold
cross validation on the 400-email dataset. Figure
1 shows the accuracy of the classifier with differ-
ent feature sets. The bottom dotted line is the base-
line result. In the 400 emails, 244 are labeled as no-
request, so a baseline system that labels everything
as no-request has an accuracy of 61%. The middle
two lines are the accuracy with features that occur no
fewer than 5 or 10 times. For the top two curves, fea-
tures are sorted according to the chi-square scores,
and the top one thousand or five thousand are kept.
X-axis shows the value of n for word ngrams; e.g.,
3-gram means features include word unigrams, bi-
grams, and trigrams. Figure 1 shows that chi-square
scores outperform feature counts for feature selec-
tion, and varying the value of n does not affect the
accuracy very much.
Table 3 shows classification accuracy and preci-
sion, recall, and F1-score for identifying request-
containing emails when n is set to 3. The table
shows that our classifier, regardless of methods used
for feature selection, greatly outperforms the base-
line system, and there is a small gap between the
performance of our classifier and the inter-annotator
agreement. For the rest of our experiment, we will
use 3-gram, Top5000 as the feature set for the re-
quest classifier.
Acc Prec Rec F1
Baseline 61.0 - - -
Inter-annotator
agreement
90.3 90.4 85.5 87.9
Using all features 79.5 76.8 68.0 72.1
At least 5 79.0 75.7 68.0 71.6
At least 10 79.3 75.9 68.6 72.1
Top1000 85.5 88.3 72.4 79.6
Top5000 85.5 88.3 72.4 79.6
Table 3: Performance of the request classifier with 3-
gram features: We use 10-fold cross validation on the 400
emails. Baseline: label every email as no-request.
5 Factors influencing formality
As mentioned in Section 1, Brown and Levinson
(1987) proposed three factors that influence commu-
nication choices such as formality: social distance,
relative power, and the weight of an imposition. In
this section, we test whether these factors indeed af-
fect formality in emails.
We measure social distance in two ways: one is
based on the nature of emails (personal vs. busi-
ness), and the other is based on the number of emails
sent from the sender to the recipient. While these
aspects do not directly define the social distance be-
tween individuals, they are employed to illustrate
90
related social properties in absence of data which
outlines the social distance of all Enron employees.
For relative power, we use the rank difference of the
positions that the sender and the recipient held in
Enron. Since relative power is complex to define
without more data, this definition of rank difference
serves as one dimension in which we can study rel-
ative power. For the weight of imposition, we com-
pare emails that contain requests and the ones that
do not.
5.1 Social distance: Personal vs. Business
In general, friends, family and other such personal
contacts are presumably closer in social distance
than business colleagues. Therefore, it is possible
that email messages of a personal nature will be
more likely to be informal than those of a business
nature. To test the hypothesis, we compare the de-
gree of formality in business vs. personal emails.
We use the Sheffield dataset, which contains 13,822
non-empty emails that have been manually labeled
as ?personal? or ?business?. We ran the formality
classifier on the data, and the results are in Table 4.
The first and second columns show the number of
emails that are labeled as formal or informal by our
formality classifier, and the last column shows the
percentage of emails in that row that are labeled in-
formal (a.k.a. the rate of informality).
The table demonstrates that the rate of informal-
ity in personal emails (56.0%) is indeed much higher
than that of business emails (21.3%). We have run
the Chi-square test and G test with the counts in the
table, and both tests indicate that formality (formal
vs. informal) is not independent from the business
nature of an email message (business vs. personal) at
p=0.0001. The same is true for formality and other
social factors that we tested in this section (see Ta-
bles 5, 7, 8, and 9).9
9There are two caveats for using these statistical tests to de-
termine whether two random variables (formality and a social
factor) are independent. First, the counts in the tables are based
on the output of the two classifiers, which could be different
from the real counts. Second, the data points in some experi-
ments were not chosen randomly from the whole email corpus;
for instance, the emails in Table 7 were from a small set of peo-
ple whose ranks at Enron were known.
Formal Informal Inf %
Personal 1410 1793 56.0%
Business 8409 2270 21.3%
Total 9819 4063 29.3%
Table 4: Formality in personal vs business emails, p <
0.0001
5.2 Social distance: Amount of contact
Besides the difference in personal and business mat-
ters, another way to measure social distance is the
amount of contact that two individuals have with
each other. People with more email exchange are
likely to be closer in social distance than those with
less email exchange, and are therefore likely to have
a higher rate of informality. To test this hypothesis,
we started with the ISI dataset and looked at the sub-
set of emails where an email has exactly one recipi-
ent, and both the sender and the recipient are in the
enron.com domain. The emails were then grouped
into several buckets based on the number of emails
from a sender to a recipient.
The results are in Table 5. The first column is
the range of the numbers of emails from a sender
to a recipient, and the last column is the number of
(sender, recipient) pairs where the number of emails
that the sender sent to the recipient is in the range
specified in the first column. The second column is
the total number of formal emails from the senders
to the recipients in those pairs. The third column is
defined similarly, and the 4th column is the rate of
informality. Note that the rates of informality in the
first two rows are about the same; it might be due to
the fact that the Enron corpus contains emails only
in a 3.5-year period. The rate of formality does go
up in the third and fourth rows.
Emails sent
from A to B
Formal Inf Inf % # of
pairs
1 to 10 23,423 7,566 24.4% 14,877
11 to 50 11,484 3,558 23.7% 737
51 to 100 3,236 1,363 29.6% 66
101 or more 2,114 1,271 37.5% 21
Total 40,257 13,758 25.5% 15,701
Table 5: Formality and the number of emails from the
sender to the recipient, p < 0.0001
91
5.3 Relative power: Rank difference
Another factor that could affect the sender?s choice
of formality is the relative difference in power or
rank between sender and recipient. For example, if
a manager sends an email to the CEO of an organi-
zation, the email is more likely to be formal than if
the recipient has a lower rank than the sender.
To investigate this, we started with the emails in
the ISI dataset whose senders are employees appear-
ing in the ISI Enron employee position table and re-
cipients are in the enron.com domain. We grouped
the emails by the sender?s position and calculated
the rate of informality in each group. The results are
in Table 6: the first two columns are the title and
the rank of the positions in Enron; the third column
is the number of employees with that position; the
fourth column is the total number of emails sent by
these employees; the fifth column is the rate of infor-
mality; the last column is the percentage of emails
that contain requests according to our request classi-
fier. It is interesting to see that the rates of informal-
ity and request vary a lot for different positions; for
instance, lawyers are more formal and make more
requests than traders.
Position Rank # of
emp
Emails
sent
Inf
%
Req
%
CEO 6 4 836 19.4% 21.7%
President 5 4 2,680 34.3% 19.3%
VP 4 28 11,425 22.2% 18.1%
Manag
Dir
3 6 4,953 14.0% 14.7%
Director 2 22 1,879 29.4% 15.2%
Manager 1 13 6,563 12.4% 25.3%
In-house
lawyer
0 3 1,548 7.0% 26.9%
Trader 0 12 1,743 33.1% 13.4%
Employee 0 38 11,770 19.1% 19.1%
Total - 130 43,397 22.0% 19.2%
Table 6: The set of Enron employees used in the formality
vs. rank study
To study the effect of rank difference on formal-
ity, we used the first six rows in Table 6 as the rel-
ative ranks of the next three rows are not so clearly
defined (Diesner et al, 2005). In total, there are 77
employees with rank 1-6, and we call this set of peo-
ple RankSet. We then extracted from the ISI dataset
only those emails that have exactly one recipient and
both sender and recipient are members of RankSet.
We grouped this small set, 3999 emails in total, ac-
cording to the rank difference (which is defined to
be the rank of the recipient minus the rank of the
sender). The results are in Table 7: the last column is
the number of (sender, recipient) pairs with that rank
difference. For instance, the -2 row indicates that,
among those messages addressed two ranks lower in
the organizational hierarchy, 24.7% are informal.
In general, Table 7 shows a lower rate of informal-
ity when an email is addressed to a recipient of su-
perior rank. For example, the informality rate of an
email addressed to someone 4 or more ranks higher
than the sender (15.6%) is less than half that of an
email addressed to someone 4 or more ranks lower
(31.6%). We do not know what causes the increase
of informality from +1 to +2; nevertheless, from
+2 to +4 (in emails addressing someone 2-4 ranks
higher), there is another decrease in informality rate.
Rank diff Formal Inf Inf % # of
pairs
-4 or less 39 18 31.6% 16
-3 84 32 27.6% 32
-2 226 74 24.7% 56
-1 499 141 22.0% 82
0 989 275 21.8% 190
+1 784 175 18.2% 95
+2 270 121 30.9% 58
+3 125 38 23.3% 46
+4 or more 92 17 15.6% 29
Total 3108 891 22.3% 604
Table 7: Formality and rank difference, p < 0.0001. Rank
diff is equal to recipient rank minus sender rank.
5.4 Weight of imposition: Requests
According to Brown and Levinson?s model of polite-
ness, the greater weight of an imposition, the greater
the usage of polite speech acts including formality.
In this model, a request is one of the most imposing
speech acts. Therefore, when a request is made, we
would expect a lower rate of informality.
To investigate this, we used the ISI dataset and
the results of our request classifier to determine the
92
rate of informality for request and no-request emails.
Table 8 shows that there is indeed a lower rate of
informality when a request is being made.
Formal Informal Inf %
Request 42,313 9,928 19.0%
No-request 128,958 51,616 28.6%
Total 171,271 61,544 26.4%
Table 8: Formality and request, p < 0.0001
5.5 Number of recipients
Another hypothesis we considered is the assumption
that a sender is less likely to be informal when there
are more recipients on an email since he does not
want to broadcast a style which is more personal
and could be perceived as unprofessional. To test
this hypothesis, we started with the ISI dataset and
looked at the subset of emails where an email has at
least one recipient.10 The emails were then grouped
based on the number of recipients in the emails.
Table 9 shows the rate of informality with differ-
ent numbers of recipients. For the most part in these
results, a greater number of recipients results in a
lower rate of informality. For instance, the rate of
informality is nearly cut in half when there are 3 to 5
recipients as opposed to a single recipient. However,
at the upper end of this scale, the rate of informality
rises again slightly. One possible explanation is that
when an email is addressed to a very large number of
recipients, the strategies employed (e.g., the model
of saving face) might differ from those employed in
an email addressed to a small audience.
6 Discussion
In this study, we explored the relation between for-
mality and five factors: (1) personal vs. business,
(2) amount of contact, (3) rank difference, (4) re-
quest, and (5) number of recipients. The experi-
ments show that the general patterns between the
rate of informality and the five factors are consistent
with Brown and Levinson?s model and our intuition;
10Some emails in the ISI dataset do not contain any recipi-
ent information. We suspect that the recipient information has
been somehow removed before the data was released to the pub-
lic. With the at-least-one-recipient requirement, the number of
non-empty emails in the ISI dataset is reduced from 232,815 to
180,757.
# of recipients Formal Inf Inf %
1 70,361 33,115 32.0%
2 5,807 1,914 24.8%
3-5 22,139 4,383 16.5%
6-10 12,903 2,626 16.9%
11 or greater 22,080 5,429 19.7%
Total 133,290 47,467 26.3%
Table 9: Formality and the number of recipients, p <
0.0001
for instance, an email tends to be more formal if it
is about business matter, it is sent to someone with
a higher rank, or it contains a request. But the ex-
periments did produce some unexpected results; for
instance, the rate of informality increased slightly
when the number of recipients is more than 10.
There are several possible reasons for the unex-
pected results. One is due to the limitation of our
dataset. For instance, the social interaction between
two people could easily go beyond the 3.5 years cov-
ered by the Enron corpus, and people could choose
other ways of communication besides email. There-
fore, the Enron corpus alone may not be sufficient
to capture the social distance between two people in
the corpus. Another possible reason is that the errors
made by our classifiers could contribute to some of
the unexpected results.
The third possible reason, the one that is most in-
teresting to us, is that there are indeed some inter-
esting phenomena which can explain away the un-
expectedness of the results. For instance, an email
sent to a large number of strangers (e.g., an adver-
tisement sent to a large mailing list) may choose to
use an informal and entertaining style in order to
catch the recipients? attention. Therefore, a theory
that intends to account for people?s email behavior
may need to distinguish emails sent to a large num-
ber of strangers from those sent to a small group of
friends. The benefit of a study like ours is that it
allows researchers to test a linguistic or social the-
ory on a large data set in a real setting. The study
can either provide supporting evidence for a theory
or reveal certain discrepancies between the predic-
tion made by the theory and the statistics in the real
data, which could lead to revision or refinement of
the theory.
While this case study has concentrated on email
93
communication, it would be interesting to study for-
mality behavior in other communication media such
as Facebook and Twitter. By applying our method-
ology to other media, it would be possible to deter-
mine whether there are other social factors that in-
fluence formality on these media. For example, it
would be useful to determine whether there is a dif-
ference in formality with respect to the number of
?friends? or ?followers? that a person has. Similarly,
it would be interesting to examine correlations on
the basis of whether a Facebook profile is config-
ured as ?public? or ?private? since the potential view-
ing audience would be reduced in the case of ?pri-
vate? profiles. Since Facebook also contains profiles
which are associated with both individuals and busi-
nesses, it would be interesting to compare these as
we did with personal and business emails. Finally, it
remains to be seen whether requests could be exam-
ined in these media but other social factors (includ-
ing whether posts related to personal matters, social
causes, or event promotion) could be explored to ex-
amine formality behavior.
7 Conclusions and Future Work
We believe that NLP techniques can be used to test
linguistic or social theories. As a case study, we
choose Brown and Levinson?s model of politeness
(1987), which states that three factors are considered
in a decision whether or when to use communication
techniques such as formality. We test the theory on
the Enron email corpus, and our experimental results
are largely consistent with the theory and human in-
tuition.
For future work, we plan to improve the perfor-
mance of our formality and request classifier by
adding additional features such as the ones that look
at the layout and zoning of an email (e.g., greetings
and signoffs). We also plan to apply our methodol-
ogy to other genres of data (e.g., blogs, Facebook,
Twitter) or to test other theories.
Another direction for future work is to explore
what communication techniques such as formality
can reveal about the culture of a particular social net-
work. For instance, among all the positions listed in
the ISI Enron employee position table, lawyers have
the lowest rate of informality (7.0%), compared to
other positions (e.g., 33.1% for traders). This im-
plies that the workplace behavior of lawyers (at least
with respect to emails) is very different from that
of traders. It will be interesting to compare the be-
haviors of people from different occupations or from
different social networks. Furthermore, if we could
define the norm of behavior within a social group,
we could then identify the outliers who might de-
serve special attention for various reasons.
Acknowledgment We would like to thank Todd Lin-
gren, Chris Rogers and three anonymous reviewers
for helpful comments and Katherine Coleman, Car-
men Harris and David Horton for providing email
annotation. Special thanks are extended to Drew
Marre` for his insight in application of this data.
References
Nor Azni Abdullah. 2006. Constructing Business
Email Messages: A Model of Writer?s Choice. ESP
Malaysia, 12:53?63.
Steven Bird, Edward Loper, and Ewan Klein. 2009. Nat-
ural Language Processing with Python. O?Reilly Me-
dia Inc.
Penelope Brown and Stephen C. Levinson. 1987. Po-
liteness: Some Universals in Language Usage. Cam-
bridge: Cambridge University Press.
Vitor R. Carvalho and William W. Cohen. 2006. Improv-
ing ?email speech acts? analysis via n-gram selection.
In Proceedings of the Analyzing Conversations in Text
and Speech (ACTS) Workshop at HLT-NAACL 2006,
pages 35?41, New York.
Vitor R. Carvalho, Wen Wu, and William W. Cohen.
2007. Discovering leadership roles in email work-
groups. In Proc. of the 4th Conference on Email and
Anti-Spam (CEAS 2007).
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Proceedings of the EMNLP-2004,
Barcelona, Spain.
A. Culotta, R. Bekkerman, and A. McCallum. 2004. Ex-
tracting social networks and contact information from
email and the web. In Proc. of the Conference on
Email and Anti-Spam (CEAS 2004).
Richard L. Daft and Robert H. Lengel. 1986. Organi-
sational Information Requirements, Media Richness,
and Structural Determinants. Management Science,
32:554?571.
Jana Diesner, Terill Frantz, and Kathleen Carley. 2005.
Communication networks from the enron email corpus
?it?s always about the people. enron is no different?.
94
Computational & Mathematical Organization Theory,
11(3):201?228.
Sanaz Jabbari, Ben Allison, David Guthrie, and Louise
Guthrie. 2006. Towards the Orwellian nightmare:
separation of business and personal emails. In Pro-
ceedings of the COLING/ACL 2006 Main Conference
Poster Sessions, pages 407?411.
Parambir S. Keila and David B. Skillcorn. 2005. Detect-
ing unusual and deceptive communication in email.
Technical report, Queens University, Ontario, Canada.
Brian Klimt and Yiming Yang. 2004. Enron corpus: A
new data set for email classification research. Techni-
cal report, Carnegie Mellon University.
Anton Leuski. 2004. Email is a stage: Discovering peo-
ple roles from email archives. In Proc. of the 27th
Annual International ACM SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR 2004), pages 502?503.
Andrew McCallum, Xuerui Wang, and Andres Corrada-
Emmanuel. 2007. Topic and role discovery in so-
cial networks with experiments on enron and academic
email. Journal of Artificial Intelligence Research,
30:249?272.
Andrew McCallum. 2002. Mallet: A machine learning
for language toolkit. http://mallet.cs.umass.edu.
Einat Minkov, Richard C. Wang, and WilliamW. Cohen.
2005. Extracting personal names from email: Apply-
ing named entity recognition to informal text. In Proc.
of EMNLP-2005.
Wanda Orlikowski and JoAnne Yates. 1994. Genre
repertoire: The structuring of communicative practices
in organizations. Administrative Science Quarterly,
39(4):541?574.
John R. Searle. 1975. A taxonomy of illocutionary acts.
In K. Gunderson, editor, Language, Mind, and Knowl-
edge, pages 344?369. Minneapolis.
Jitesh Shetty and Jafar Adibi. 2004. The enron email
dataset database schema and brief statistical report.
Technical report, Information Sciences Institute at
University of South California.
L. Zhou, J.K. Burgoon, J.F. Nunamaker Jr, and
D. Twitchel. 2004. Automating linguistics-based cues
for detecting deception in text-based asynchronous
computer-mediated communication. Group Decision
and Negotiation, 13:81?106.
95
