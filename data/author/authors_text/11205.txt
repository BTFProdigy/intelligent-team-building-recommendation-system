BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 118?119,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Raising the Compatibility of Heterogeneous Annotations:
A Case Study on Protein Mention Recognition
Yue Wang? Kazuhiro Yoshida? Jin-Dong Kim? Rune S?tre? Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo
?School of Informatics, University of Manchester
?National Center for Text Mining
Hongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 JAPAN
{wangyue, kyoshida, jdkim, rune.saetre, tsujii}@is.s.u-tokyo.ac.jp
Abstract
While there are several corpora which claim
to have annotations for protein references,
the heterogeneity between the annotations is
recognized as an obstacle to develop expen-
sive resources in a synergistic way. Here we
present a series of experimental results which
show the differences of protein mention an-
notations made to two corpora, GENIA and
AImed.
1 Introduction
There are several well-known corpora with protein
mention annotations. It is a natural request to bene-
fit from the existing annotations, but the heterogene-
ity of the annotations remains an obstacle. The het-
erogeneity is caused by different definitions of ?pro-
tein?, annotation conventions, and so on.
It is clear that by raising the compatibility of an-
notations, we can reduce the performance degrada-
tion caused by the heterogeneity of annotations.
In this work, we design several experiments to
observe the effect of removing or relaxing the het-
erogeneity between the annotations in two corpora.
The experimental results show that if we understand
where the difference is, we can raise the compati-
bility of the heterogeneous annotations by removing
the difference.
2 Corpora and protein mention recognizer
We used two corpora: the GENIA corpus (Kim
et al, 2003), and the AImed corpus (Bunescu and
Mooney, 2006). There are 2,000 MEDLINE ab-
stracts and 93,293 entities in the GENIA corpus.
?
??
??
??
??
??
??
??
??
??
?? ?? ?? ?? ??? ??? ??? ???????
???????????????????
???
????
Proceedings of the Workshop on BioNLP, pages 106?107,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Incorporating GENETAG-style annotation to GENIA corpus
Tomoko Ohta? and Jin-Dong Kim? and Sampo Pyysalo? and Yue Wang? and Jun?ichi Tsujii???
?Department of Computer Science, University of Tokyo, Tokyo, Japan
?School of Computer Science, University of Manchester, Manchester, UK
?National Centre for Text Mining, University of Manchester, Manchester, UK
{okap,jdkim,smp,wangyue,tsujii}@is.s.u-tokyo.ac.jp
1 Introduction
Proteins and genes are the most important entities in
molecular biology, and their automated recognition
in text is the most widely studied task in biomed-
ical information extraction (IE). Several corpora
containing annotation for these entities have been
introduced, GENIA (Kim et al, 2003; Kim et al,
2008) and GENETAG (Tanabe et al, 2005) being
the most prominent and widely applied. While both
aim to address protein/gene annotation, their an-
notation principles differ notably. One key differ-
ence is that GENETAG annotates the conceptual en-
tity, gene, which is often associated with a function,
while GENIA concentrates on the physical forms of
gene, i.e. protein, DNA and RNA. The difference
has caused serious problems relating to the compat-
ibility and comparability of the annotations. In this
work, we present an extension of GENIA annotation
which integrates GENETAG-style gene annotation.
The new version of the GENIA corpus is the first to
bring together these two types of entity annotation.
2 GGP Annotation
Gene is the basic unit of heredity, which is encoded
in the coding region of DNA. Its physical manifes-
tations as RNA and Protein are often called its prod-
ucts. In our view of these four entity types, gene is
taken as an abstract entity whereas protein, DNA and
RNA are physical entities. While the three physical
entity types are disjoint, the abstract concept, gene,
is defined from a different perspective and is realized
in, not disjoint from, the physical entity types.
The latest public version of GENIA corpus (here-
after ?old corpus?) contains annotations for gene-
Protein DNA RNA GGP
Old Annotation 21,489 8,653 876 N/A
New Annotation 15,452 7,872 863 12,272
Table 1: Statistics on annotation for gene-related entities
related entities, but they are classified into only
physical entity types: Protein, DNA and RNA. The
corpus revisions described in this work are two-fold.
First, annotation for the abstract entity, gene, were
added (Table 1, GGP). To emphasize the character-
istics of the new entity type, which does not dis-
tinguish a gene and its products, we call it GGP
(gene or gene product). Second, the addition of GGP
annotation triggered large-scale removal of Protein,
DNA and RNA annotation instances for cases where
the physical form of the gene was not referred to
(Due to space limitations, we omit RNA from now
on). The time cost involved with this revision was
approximately 500 person-hours.
3 Quality Assessment
To measure the effect of revision, we performed
NER experiments with old and new annotation (Ta-
bles 2 and 3). We split the corpus into disjoint 90%
and 10% parts for use in training and test, respec-
tively. We used the BANNER (Leaman and Gonza-
lez, 2008) NE tagger and created a separate single-
class NER problem for each entity type.
In the old annotation, consistency is moderate
for protein (77.70%), while DNA is problematic
(58.03%). The new GGP annotation has been
achieved in a fairly consistent way (81.44%). How-
ever, the removal of annotation for entities previ-
ously marked as protein or DNA had opposite effects
on the two: better performance for DNA (64.06%),
106
Precision Recall F-score
Protein 80.78 74.84 77.70
DNA 64.90 52.48 58.03
Table 2: NER performance before GGP annotation
Precision Recall F-score
Protein 71.20 56.61 63.08
DNA 69.59 59.35 64.06
GGP 86.86 76.65 81.44
Protein+ 83.22 78.20 80.63
Table 3: NER performance after GGP annotation
Phosphorylation Gene expression
GGP in protein 70% GGP abstract 34%
Protein 25% Protein 24%
GGP abstract 3% GGP in Protein 17%
Peptide 1% GGP in DNA 9%
Table 4: Distribution of theme entity types in GENIA
implying annotation consistency improved with the
removals, but worse for Protein (63.08%).
We find the primary explanation for this effect in
the statistics in Table 1: in the revision, a large num-
ber of protein annotations (6,037) but only a small
number of DNA annotations (780) were replaced
with GGP. To distinguish such GGPs from those em-
bedded in Protein or DNA annotations, we call them
?abstract? GGPs, as they appear in text without in-
formation on their physical form. Nevertheless, in
the old annotation, they had to be annotated as either
protein or DNA, which might have caused inconsis-
tent annotation. However, the statistics show a clear
preference for choosing Protein over DNA. The rad-
ical drop of performance in protein recognition can
then be explained in part as a result of removing this
systematic preference.
Aside from the discussion on whether the pref-
erence is general or specific, we interpret the pref-
erence as a need for ?potential? proteins to be re-
trieved together with ?real? proteins, which was an-
swered by the old protein annotation. To reproduce
this class in the new annotation, we added abstract
GGPs to the Protein annotation and performed an
NER experiment. The result (Table 3, Protein+)
shows a clear improvement over the comparable re-
sult for the old protein annotation.
In conclusion, we argue, the revision of the GE-
NIA annotation, in addition to introducing a new en-
tity class, has led to a significant improvement of
overall consistency.
4 Discussion
Although there are already corpora such as GENE-
TAG with annotation similar to GGPs, we expect
this newly introduced class of annotation to support
existing annotations of GENIA, such as event and
co-reference annotation, opening up new possibili-
ties for application. The quality of entity annota-
tion should be closely related to that of other seman-
tic annotation, e.g. events. For example, the event
type Phosphorylation is about a change on physi-
cal entities, e.g. proteins and peptides, and as such,
it is expected that themes of these events would be
physical entities. On the other hand, the event type
Gene expression is about the manifestation of an ab-
stract entity (gene) as a physical entity (protein) and
would thus be expected to involve both abstract and
physical entities. Statistics from GENIA (Table 4)
show that the theme selection made in event anno-
tation well reflects these characteristics of the two
event types. The observation suggests that there is a
good likelihood that improvement of the entity an-
notation can be further transferred to other semantic
annotation, which is open for future work.
Acknowledgments
This work was partially supported by Grant-in-Aid
for Specially Promoted Research (MEXT, Japan)
and Genome Network Project (MEXT, Japan).
References
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2003. GENIA corpus - a semantically an-
notated corpus for bio-textmining. Bioinformatics,
19(suppl. 1):i180?i182.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
lterature. BMC Bioinformatics, 9(1):10.
R. Leaman and G. Gonzalez. 2008. Banner: an exe-
cutable survey of advances in biomedical named en-
tity recognition. Pacific Symposium on Biocomputing,
pages 652?663.
Lorraine Tanabe, Natalie Xie, Lynne Thom, Wayne Mat-
ten, and W John Wilbur. 2005. Genetag: a tagged cor-
pus for gene/protein named entity recognition. BMC
Bioinformatics, 6(Suppl 1):S3.
107
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 603?612,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Study of Concept-based Weighting Regularization
for Medical Records Search
Yue Wang, Xitong Liu, Hui Fang
Department of Electrical & Computer Engineering,
University of Delaware, USA
{wangyue,xtliu,hfang}@udel.edu
Abstract
An important search task in the biomedical
domain is to find medical records of pa-
tients who are qualified for a clinical trial.
One commonly used approach is to apply
NLP tools to map terms from queries and
documents to concepts and then compute
the relevance scores based on the concept-
based representation. However, the map-
ping results are not perfect, and none of
previous work studied how to deal with
them in the retrieval process. In this pa-
per, we focus on addressing the limitations
caused by the imperfect mapping results
and study how to further improve the re-
trieval performance of the concept-based
ranking methods. In particular, we ap-
ply axiomatic approaches and propose two
weighting regularization methods that ad-
just the weighting based on the relations
among the concepts. Experimental results
show that the proposed methods are effec-
tive to improve the retrieval performance,
and their performances are comparable to
other top-performing systems in the TREC
Medical Records Track.
1 Introduction
With the increasing use of electronic health
records, it becomes urgent to leverage this rich
information resource about patients? health condi-
tions to transform research in health and medicine.
As an example, when developing a cohort for
a clinical trial, researchers need to identify pa-
tients matching a set of clinical criteria based on
their medical records during their hospital visits
(Safran et al, 2007; Friedman et al, 2010). This
selection process is clearly a domain-specific re-
trieval problem, which searches for relevant medi-
cal records that contain useful information about
their corresponding patients? qualification to the
criteria specified in a query, e.g., ?female patient
with breast cancer with mastectomies during ad-
mission?.
Intuitively, to better solve this domain-specific
retrieval problem, we need to understand the re-
quirements specified in a query and identify the
documents satisfying these requirements based on
their semantic meanings. In the past decades,
significant efforts have been put on constructing
biomedical knowledge bases (Aronson and Lang,
2010; Lipscomb, 2000; Corporation, 1999) and
developing natural language processing (NLP)
tools, such as MetaMap, to utilize the informa-
tion from the knowledge bases (Aronson, 2001;
McInnes et al, 2009). These efforts make it pos-
sible to map free text to concepts and use these
concepts to represent queries and documents.
Indeed, concept-based representation is one
of the commonly used approaches that leverage
knowledge bases to improve the retrieval perfor-
mance (Limsopatham et al, 2013d; Limsopatham
et al, 2013b). The basic idea is to represent
both queries and documents as ?bags of concepts?,
where the concepts are identified based on the in-
formation from the knowledge bases. This method
has been shown to be more effective than tra-
ditional term-based representation in the medical
record retrieval because of its ability to handle the
ambiguity in the medical terminology. However,
this method also suffers the limitation that its ef-
fectiveness depends on the accuracy of the concept
mapping results. As a result, directly applying
existing weighting strategies might lead to non-
optimal retrieval performance.
In this paper, to address the limitation caused by
the inaccurate concept mapping results, we pro-
pose to regularize the weighting strategies in the
concept-based representation methods. Specifi-
cally, by applying the axiomatic approaches (Fang
and Zhai, 2005), we analyze the retrieval func-
603
tions with concept-based representation and find
that they may violate some reasonable retrieval
constraints. We then propose two concept-based
weighting regularization methods so that the reg-
ularized retrieval functions would satisfy the re-
trieval constraints and achieve better retrieval per-
formance. Experimental results over two TREC
collections show that both proposed concept-
based weighting regularization methods can im-
prove the retrieval performance, and their perfor-
mance is comparable with the best systems of
the TREC Medical Records tracks (Voorhees and
Tong, 2011; Voorhees and Hersh, 2012).
Many NLP techniques have been developed to
understand the semantic meaning of textual in-
formation, and are often applied to improve the
search accuracy. However, due to the inherent am-
biguity of natural languages, the results of NLP
tools are not perfect. One of our contributions is
to present a general methodology that can be used
to adjust existing IR techniques based on the inac-
curate NLP results.
2 Related Work
The Medical Records track of the Text REtrieval
Conference (TREC) provides a common platform
to study the medical records retrieval problem
and evaluate the proposed methods (Voorhees and
Tong, 2011; Voorhees and Hersh, 2012).
Concept-based representation has been studied
for the medical record retrieval problem (Lim-
sopatham et al, 2013d; Limsopatham et al,
2013b; Limsopatham et al, 2013a; Qi and La-
querre, 2012; Koopman et al, 2011; Koopman et
al., 2012). For example, Qi and Laquerre used
MetaMap to generate the concept-based repre-
sentation and then apply a vector space retrieval
model for ranking, and their results are one of
the top ranked runs in the TREC 2012 Medi-
cal Records track (Qi and Laquerre, 2012). To
further improve the performance, Limsopatham
et al proposed a task-specific representation,
i.e., using only four types of concepts (symp-
tom, diagnostic test, diagnosis and treatment) in
the concept-based representation and a query ex-
pansion method based on the relationships among
the medical concepts (Limsopatham et al, 2013d;
Limsopatham et al, 2013a). Moreover, they also
proposed a learning approach to combine both
term-based and concept-based representation to
further improve the performance (Limsopatham et
Figure 1: Example of MetaMap result for a query.
al., 2013b).
Our work is also related to domain-specific
IR (Yan et al, 2011; Lin and Demner-Fushman,
2006; Zhou et al, 2007). For example, Yan et
al. proposed a granularity-based document rank-
ing model that utilizes ontologies to identify doc-
ument concepts. However, none of the previous
work has studied how to regularize the weight of
concepts based on their relations.
It is well known that the effectiveness of a re-
trieval function is closely related to the weight-
ing strategies (Fang and Zhai, 2005; Singhal et
al., 1996). Various term weighting strategies have
been proposed and studied for the term-based
representation (Amati and Van Rijsbergen, 2002;
Singhal et al, 1996; Robertson et al, 1996).
However, existing studies on concept-based rep-
resentation still used weighting strategies devel-
oped for term-based representation such as vector
space models (Qi and Laquerre, 2012) and diver-
gence from randomness (DFR) (Limsopatham et
al., 2013a) and did not take the inaccurate con-
cept mapping results into consideration. Com-
pared with previous work, we focus on address-
ing the limitation caused by the inaccurate con-
cept mapping. Note that our efforts are orthogonal
to existing work, and it is expected to bring addi-
tional improvement to the retrieval performance.
3 Concept-based Representation for
Medical Records Retrieval
3.1 Problem Formulation
We follow the problem setup used in the TREC
medical record track (Voorhees and Tong, 2011;
Voorhees and Hersh, 2012). The task is to retrieve
relevant patient visits with respect to a query.
Since each visit can be associated with multiple
medical records, the relevance of a visit is related
to the relevance of individual associated medical
records. Existing studies computed the relevance
604
scores at either visit-level, where all the medical
records of a visit are merged into a visit document
(Demner-Fushman et al, 2012; Limsopatham et
al., 2013c), or record-level, where we can first
compute the relevance score of individual records
and then aggregate their scores as the relevance
score of a visit (Limsopatham et al, 2013c; Zhu
and Carterette, 2012; Limsopatham et al, 2013d).
In this paper, we focus on the visit-level relevance
because of its simplicity. In particular, given a pa-
tient?s visit, all the medical records generated from
this visit are merged as a document. Note that our
proposed concept-weighting strategies can also be
easily applied to record-level relevance modeling.
Since the goal is to retrieve medical records of
patients that satisfying requirements specified in a
query, the relevance of medical records should be
modeled based on how well they match all the re-
quirements (i.e., aspects) specified in the queries.
3.2 Background: UMLS and MetaMap
Unified Medical Language System (UMLS) is a
metathesaurus containing information from more
than 100 controlled medical terminologies such as
the Systematized Nomenclature of Medicine Clin-
ical Terms (SNOMED-CT) and Medical Subject
Headings (MeSH). Specifically, it contains the in-
formation about over 2.8 million biomedical con-
cepts. Each concept is labeled with a Concept
Unique Identifier (CUI) and has a preferred name
and a semantic type.
Moreover, NLP tools for utilizing the informa-
tion from UMLS have been developed. In partic-
ular, MetaMap (Aronson, 2001) can take a text
string as the input, segment it into phrases, and
then map each phrase to multiple UMLS CUIs
with confidence scores. The confidence score is
an indicator of the quality of the phrase-to-concept
mapping by MetaMap. It is computed by four met-
rics: centrality, variation, coverage and cohesive-
ness (Aronson, 2001). These four measures try to
evaluate the mapping from different angles, such
as the involvement of the central part, the distance
of the concept to the original phrase, and how well
the concept matches the phrase. The maximum
confidence in MetaMap is 1000.
Figure 1 shows the MetaMap results for an ex-
ample query ?children with dental caries?. Two
query aspects, i.e., ?children? and ?dental caries?,
are identified. Each of them is mapped to multiple
concepts, and each concept is associated with the
confidence score as well as more detailed informa-
tion about this concept.
3.3 Concept-based Representation
Traditional retrieval models are based on ?bag of
terms? representation. One limitation of this rep-
resentation is that relevance scores are computed
based on the matching of terms rather than the
meanings. As a result, the system may fail to re-
trieve the relevant documents that do not contain
any query terms.
To overcome this limitation, concept-based rep-
resentation has been proposed to bridge the vo-
cabulary gap between documents and queries
(Qi and Laquerre, 2012; Limsopatham et al,
2013b; Koopman et al, 2012). In particular,
MetaMap is used to map terms from queries
and documents (e.g., medical records) to the
semantic concepts from biomedical knowledge
bases such as UMLS. Within the concept-based
representation, the query can then be repre-
sented as a bag of all the generated CUIs
in the MetaMap results. For example, the
query from Figure 1 can be represented as
{C0008059, C0680063, C0011334, C0333519,
C0226984}. Documents can be represented in a
similar way.
After converting both queries and documents
to concept-based representations using MetaMap,
previous work applied existing retrieval functions
such as vector space models (Singhal et al, 1996)
to rank the documents. Note that when referring
to existing retrieval functions in the paper, they
include traditional keyword matching based func-
tions such as pivoted normalization (Singhal et
al., 1996), Okapi (Robertson et al, 1996), Dirich-
let prior (Zhai and Lafferty, 2001) and basic ax-
iomatic functions (Fang and Zhai, 2005).
4 Weighting Strategies for
Concept-based Representation
4.1 Motivation
Although existing retrieval functions can be di-
rectly applied to concept-based representation,
they may lead to non-optimal performance. This
is mainly caused by the fact that MetaMap may
generate more than one mapped concepts for an
aspect, i.e., a semantic unit in the text.
Ideally, an aspect will be mapped to only one
concept, and different concepts would represent
different semantic meanings. Under such a situ-
605
                                                                                                                                             
Figure 2: Exploratory data analysis (From left to right are choosing minimum, average and maximum
IDF concepts as the representing concepts, respectively. The removed concepts are highlighted in the
figures.).
ation, traditional retrieval functions would likely
work well and generate satisfying retrieval per-
formance since the relations among concepts are
independent which is consistent with the assump-
tions made in traditional IR (Manning et al, 2008).
However, the mapping results generated by
MetaMap are not perfect. Although MetaMap is
able to rank all the candidate concepts with the
confidence score and pick the most likely one,
the accuracy is not very high. In particular, our
preliminary results show that turning on the dis-
ambiguation functionality provided by MetaMap
(i.e., returning only the most likely concept for
each query) could lead to worse retrieval per-
formance than using all the candidate mappings.
Thus, we use the one-to-many mapping results
generated by MetaMap, in which each aspect can
be mapped to multiple concepts.
Unfortunately, such one-to-many concept map-
pings could hinder the retrieval performance in the
following two ways.
? The multiple concepts generated from the
same aspect are related, which is inconsis-
tent with the independence assumption made
in the existing retrieval functions (Manning
et al, 2008). For example, as shown in Fig-
ure 1, ?dental caries? is mapped to three con-
cepts. It is clear that the concepts are related,
but existing retrieval functions are unable to
capture their relations and would compute the
weight of each concept independently.
? The one-to-many mapping results generated
by MetaMap could arbitrarily inflate the
weights of some query aspects. For exam-
ple, as shown in Figure 1, query aspect ?chil-
dren? is mapped to 2 concepts while ?den-
tal caries? is mapped to 3 concepts. In the
existing retrieval functions, term occurrences
are important relevance signals. However,
when converting the text to concepts repre-
sentation using MetaMap, the occurrences of
the concepts are determined by not only the
original term occurrences, a good indicator
of relevance, but also the number of mapped
concepts, which is determined by MetaMap
and has nothing to do with the relevance sta-
tus. As a result, the occurrences of concepts
might not be a very accurate indicator of im-
portance of the corresponding query aspect.
To address the limitations caused by the inac-
curate mapping results, we propose to apply ax-
iomatic approaches (Fang and Zhai, 2005) to reg-
ularize the weighting strategies for concept-based
representation methods. In particular, we first
formalize retrieval constraints that any reasonable
concept-based representation methods should sat-
isfy and then discuss how to regularize the existing
weighting strategies to satisfy the constraints and
improve the retrieval performance.
We first explain the notations used in this sec-
tion. Q and D denote a query and a document
with the concept-based representation. S(Q,D)
is the relevance score of D with respect to Q. e
i
denotes a concept, and A(e) denotes the query
aspect associated with e, i.e., a set of concepts
that are mapped to the same phrases as e by us-
ing MetaMap. i(e) is the normalized confidence
score of the mapping for concept e generated by
MetaMap. c(e,D) denotes the occurrences of
concept e in document D, df(e) denotes the num-
ber of documents containing e. |D| is the docu-
ment length of D. Imp
c
(e) is the importance of
the concept such as the concept IDF value, and
Imp
A
(A) is the importance of the aspect.
606
4.2 Unified concept weighting regularization
We now discuss how to address the first challenge,
i.e,. how to regularize the weighting strategy so
that we can take into consideration the fact that
concepts associated with the same query aspect are
not independent. We call a concept is a variant of
another one if both of them are associated with the
same aspect.
Intuitively, given a query with two aspects, a
document covering both aspects should be ranked
higher than those covering only one aspect. We
can formalize the intuition in the concept-based
representation as the following constraint.
Unified Constraint: Let query be Q =
{e
1
, e
2
, e
3
}, and we know that e
2
is a variant of
e
3
. Assume we have two documents D
1
and D
2
with the same document length, i.e., |D
1
| = |D
2
|.
If we know that c(e
1
, D
1
) = c(e
3
, D
2
) > 0,
c(e
1
, D
2
) = c(e
3
, D
1
) = 0 and c(e
2
, D
1
) =
c(e
2
, D
2
) > 0, then S(Q,D
1
) > S(Q,D
2
).
It is clear that existing retrieval functions would
violate this constraint since they ignore the rela-
tions among concepts.
One simple strategy to fix this problem is to
merge all the concept variants as a single concept
and select one representative concept to replace all
occurrences of other variants in both queries and
documents. By merging the concepts together, we
are aiming to purify the concepts and make the
similar concepts centralized so that the assumption
that all the concepts are independent would hold.
Formally, the adjusted occurrences of a concept
e in a document D is shown as follows:
c
mod
(e,D)=
{
?
e
?
?EC(e)
c(e
?
, D) e=Rep(EC(e))
0 e 6=Rep(EC(e))
(1)
where c(e,D) is the original occurrence of con-
cept e in document D, EC(e) denotes a set of
all the variants of e including itself (i.e., all the
concepts with the same preferred name as e), and
Rep(EC(e)) denotes the representative concept
from EC(e).
It is trivial to prove that, with such changes, ex-
isting retrieval functions would satisfy the above
constraint since the constraint implies TFC2 con-
straint defined in the previous study (Fang et al,
2004).
Now the remaining question is how to select the
representative concept from all the variants. There
are three options: select the concept with the maxi-
mum IDF, average IDF, or minimum IDF. We con-
duct exploratory data analysis on these three op-
tions. In particular, for each option, we generate
a plot indicating the correlation between the IDF
value of a concept and the relevance probability of
the concept (i.e., the probability that a document
containing the concept is relevant). Note that both
original and replaced IDF values are shown in the
plot for each option. Figure 2 shows the results. It
is clear that the right plot (i.e., selecting the con-
cept with the maximum IDF as the representative
concept) is the best choice since the changes make
the points less scattered. In fact, this can also be
confirmed by experimental results as reported in
Table 5. Thus, we use the concept with the max-
imum IDF value as the representative concept of
all the variants.
4.3 Balanced concept weighting
regularization
We now discuss how to address the second chal-
lenge, i.e., how to regularize the weighting strat-
egy to deal with the arbitrarily inflated statistics
caused by the one-to-many mappings.
The arbitrary inflation could impact the impor-
tance of the query aspects. For example, as shown
in Figure 1, one aspect is mapped to two con-
cepts while the other is mapped to three. More-
over, it could also impact the accuracy of the con-
cept IDF values. Consider ?colonoscopies? and
?adult?, it is clear that the first term is more im-
portant than the second one, which is consistent
with their term IDF values, i.e., 7.52 and 2.92, re-
spectively. However, with the concept-based rep-
resentation, the IDF value of the concept ?colono-
scopies?(C0009378) is 2.72, which is even smaller
than that of concept ?adult? (C1706450), i.e., 2.92.
To fix the negative impact on query aspects, we
could leverage the findings in the previous study
(Zheng and Fang, 2010) and regularize the weight-
ing strategy based on the length of query aspects
to favor documents covering more query aspects.
Since each concept mapping is associated with a
confidence score, we can incorporate them into the
regularization function as follows:
f(e,Q) = (1? ?) + ? ?
(
?
e
?
?Q
i(e
?
)
?
e
??
?A(e)
i(e
??
)
)
, (2)
where i(e) is the normalized confidence score of
concept e generated by MetaMap, and ? is a pa-
rameter between 0 and 1 to control the effect of the
regularization. When ? is set to 0, there is no reg-
ularization. This regularization function aims to
607
penalize the weight of concept e based on its vari-
ants as well as the concepts from other aspects. In
particular, a concept would receive more penalty
(i.e., its weight will be decreased more) when it
has more variants and the mappings of these vari-
ants are more accurate.
To fix the negative impact on the concept IDF
values, we propose to regularize the weighting
based on the importance of the query aspect. This
regularization can be formalized as the following
constraint.
Balanced Constraint: Let Q be a query
with two concepts and the concepts are associ-
ated with different aspects, i.e., Q = {e
1
, e
2
},
and A(e
1
) 6= A(e
2
). Assume D
1
and D
2
are two documents with the same length, i.e.,
|D
1
| = |D
2
|, and they cover different concepts
with the same occurrences, i.e., c(e
1
, D
1
) =
c(e
2
, D
2
) > 0 and c(e
2
, D
1
) = c(e
1
, D
2
) =
0. If we know Imp
c
(e
1
) = Imp
c
(e
2
) and
Imp
A
(A(e
1
)) < Imp
A
(A(e
2
)), then we have
S(Q,D
1
) < S(Q,D
2
).
This constraint requires that the relevance score
of a document should be affected by not only the
importance of the concepts but also the importance
of the associated query aspect. In a way, the con-
straint aims to counteract the arbitrary statistics in-
flation caused by MetaMap results and balance the
weight among concepts based on the importance
of the associated query aspects. And it is not dif-
ficult to show that existing retrieval functions vio-
late this constraint.
Now the question is how to revise the retrieval
functions to make them satisfy this constraint. We
propose to incorporate the importance of query as-
pect into the previous regularization function in
Equation (2) as follows:
f(e,Q) = (1??)+? ?
(
?
e
?
?Q
i(e
?
)
?
e
??
?A(e)
i(e
??
)
)
?Imp
A
(A(e)).
(3)
Note that Imp
A
(A(e)) is the importance of a
query aspect and can be estimated based on the
terms from the query aspect. In this paper, we
use the maximum term IDF value from the aspect
to estimate the importance, which performs better
than using minimum and average IDF values as
shown in the experiments (i.e., Table 6). We plan
to study other options in the future work.
4.4 Discussions
Both proposed regularization methods can be
combined with any existing retrieval functions. In
this paper, we focus on one of the state of the
art weighting strategies, i.e., F2-EXP function de-
rived from axiomatic retrieval model (Fang and
Zhai, 2005), and explain how to incorporate the
regularization methods into the function.
The original F2-EXP retrieval function is shown
as follows:
S(Q,D) =
?
e?Q?D
c(e,Q) ? (
N
df(e)
)
0.35
?
c(e,D)
c(e,D) + b +
b?|D|
avdl
(4)
where b is a parameter control the weight of the
document length normalization.
With the unified concept weighting regulariza-
tion, the revised function based on F2-EXP func-
tion, i.e., Unified, is shown as follows:
S(Q,D)=
?
e?Q?D
c
mod
(e,Q)?(
N
df(t)
)
0.35
?
c
mod
(e,D)
c
mod
(e,D)+b+
b?|D|
avdl
(5)
where c
mod
(e,D) and c
mod
(e,Q) denote the
modified occurrences as shown in Equation (1). It
can be shown that this function satisfies the unified
constraint but violates the balanced constraint.
Following the similar strategy used in the previ-
ous study (Zheng and Fang, 2010), we can further
incorporate the regularization function proposed
in Equation (3) to the above function to make it
satisfy the balanced constraint as follows:
S(Q,D) =
?
e?Q?D
c
mod
(e,Q)?(
N
df(t)
)
0.35
?f(e,Q) (6)
?
c
mod
(e,D)
c
mod
(e,D)+b+
b?|D|
avdl
where f(e,Q) is the newly proposed regular-
ization function as shown in Equation (3). This
method is denoted as Balanced, and can be shown
that it satisfies both constraints.
Table 1: Statistics of collections.
# of unique tokens AvgDL AvgQL11 AvgQL12
Term 263,356 2,659 10.23 8.82
Concept 58,192 2,673 8.79 7.81
5 Experiments
5.1 Experiment Setup
We conduct experiments using two data sets from
the TREC Medical Records track 2011 and 2012.
608
Table 2: Description of Methods
Name Representation Ranking strategies
Term-BL Term F2-EXP (i.e., Equation (4))
Concept-BL Concept (i.e., Section 3.3) F2-EXP (i.e., Equation (4))
TSConcept-BL Task specific concept ((Limsopatham et al, 2013d)) F2-EXP (i.e., Equation (4))
Unified Concept (i.e., Section 4.2) F2-EXP + Unified (i.e., Equation (5))
Balanced Concept (i.e., Section 4.3) F2-EXP + Balanced (i.e., Equation (6))
Table 3: Performance under optimized parameter settings
Med11 Med12
MAP bpref infNDCG infAP
Term-BL 0.3474 0.4727 0.4695 0.2106
Concept-BL 0.3967 0.5476 0.5243 0.2497
TSConcept-BL 0.3964 0.5329 0.5283 0.2694
Unified 0.4235
T
0.5443
T
0.5416
T
0.2586
T
Balanced 0.4561
T ,C ,TS
0.5697
T ,C ,TS
0.5767
T ,C ,TS
0.2859
T ,C ,TS
The data sets are denoted as Med11 and Med12.
Both data sets used the same document collection
with 100,866 medical records, each of which is as-
sociated with a unique patient visit to the hospi-
tal or emergency department. Since the task is to
retrieve relevant visits, we merged all the records
from a visit to form a single document for the visit,
which leads to 17,198 documents in the collection.
There are 34 queries in Med11 and 47 in Med12.
These queries were developed by domain experts
based on the ?inclusion criteria? of a clinical study
(Voorhees and Tong, 2011; Voorhees and Hersh,
2012).
After applying MetaMap to both documents and
queries, we can construct a concept-based collec-
tion. Since documents are often much longer, we
can first segment them into sentences, get the map-
ping results for each sentence, and then merge
them together to generate the concept-based rep-
resentation for the documents.
Table 1 compares the statistics of the term-
based and the concept-based collections, including
the number of unique tokens in the collection (i.e.,
the number of terms for term-based representa-
tion and the number of concepts for concept-based
representation), the average number of tokens in
the documents (AvgDL) and the average number
of tokens in the queries for these two collections
(AvgQL11 and AvgQL12). It is interesting to see
that the number of unique tokens is much smaller
when using the concept-based indexing. This is
expected since terms are semantically related and
a group of related terms would be mapped to one
semantic concept. Moreover, we observe that the
document length and query length are similar for
both collections. This is caused by the fact that
concepts are related and the MetaMap would map
an aspect to multiple related concepts.
Table 2 summarizes the methods that we com-
pare in the experiments. Following the evalua-
tion methodology used in the medical record track,
we use MAP@1000 as the primary measure for
Med11 and also report bpref. For Med12, we take
infNDCG@100 as the primary measure and also
report infAP@100. Different measures were cho-
sen for these two sets mainly because different
pooling strategies were used to create the judg-
ment pools (Voorhees and Hersh, 2012).
5.2 Performance Comparison
Table 3 shows the performance under optimized
parameter settings for all the methods over both
data sets. The performance is optimized in terms
of MAP in Med11, and infNDCG in Med12, re-
spectively. ? and b are tuned from 0 to 1 with the
step 0.1. Note that
T
,
C
and
TS
indicate improve-
ment over Term-BL, Concept-BL and TSConcept-
BL is statistically significant at 0.05 level based on
Wilcoxon signed-rank test, respectively.
Results show that Balanced method can signifi-
cantly improve the retrieval performance over both
collections. Unified method outperforms the base-
line methods in terms of the primary measure on
both collections, although it fails to improve the
infAP on Med12 for one baseline method. It is not
surprising to see that Balanced method is more ef-
fective than Unified since the former satisfies both
of the proposed retrieval constraints while the lat-
609
Table 4: Testing Performance
Trained on Med12 Med11
Tested on Med11 Med12
Measures MAP bpref infNDCG infAP
Term-BL 0.3451 0.4682 0.4640 0.2040
Concept-BL 0.3895 0.5394 0.5194 0.2441
TSConcept-BL 0.3901 0.5286 0.5208 0.2662
Unified 0.4176
T,C
0.5391
T
0.5346
T
0.2514
T
Balanced 0.4497
T ,C ,TS
0.5627
T ,C ,TS
0.5736
T ,C ,TS
0.2811
T ,C ,TS
ter satisfies only one. Finally, we noticed that
the performance difference between TSConcept-
BL and Concept-BL is not as significant as the
ones reported in the previous study (Limsopatham
et al, 2013d), which is probably caused by the
difference of problem set up (i.e., record-level vs.
visit-level as discussed in Section 3.1).
We also conduct experiments to train parame-
ters on one collection and compare the testing per-
formance on the other collection. The results are
summarized in Table 4. Clearly, Balanced is still
the most effective regularization method. The test-
ing performance is very close to the optimal per-
formance, which indicates that the proposed meth-
ods are robust with respect to the parameter set-
ting.
Moreover, we would like to point out that the
testing performance of Balanced is comparable
to the top ranked runs from the TREC Medical
records track. For example, the performance of
the best automatic system in Med11 (e.g., Cen-
gageM11R3) is 0.552 in terms of bpref, while
the performance of the best automatic system
in Med12 (e.g., udelSUM) is 0.578 in terms of
infNDCG. Note that the top system of Med12 used
multiple external resources such as Wikipedia and
Web, while we did not use such resources. More-
over, our performance might be further improved
if we apply the result filtering methods used by
many TREC participants (Leveling et al, 2012).
Table 5: Selecting representative concepts
MAP bpref
Unified (i.e., Unified-max) 0.4235 0.5443
Unified-min 0.3894 0.5202
Unified-avg 0.4164 0.5303
5.3 More Analysis
In the Unified method, we chose the concept with
the maximum IDF as the representative concept
Table 6: Estimating query aspect importance
MAP bpref
Balanced (i.e., Balanced-max) 0.4561 0.5697
Balanced-min 0.4216 0.5484
Balanced-avg 0.4397 0.5581
Table 7: Regularization components in Balanced
MAP bpref
Balanced 0.4561 0.5697
Confidence only 0.4294 0.5507
Importance only 0.4373 0.5598
among all the variants. We now conduct experi-
ments on Med11 to compare its performance with
those of using average IDF and minimum IDF
ones as the representative concept. The results are
shown in Table 5. It is clear that using maximum
IDF is the best choice, which is consistent with
our observation from the data exploratory analysis
shown in Figure 2.
In the Balanced method, we used the maximum
IDF value to estimate the query importance. We
also conduct experiments to compare its perfor-
mance with those using the minimum and aver-
age IDF values. Table 6 summarizes the results,
and shows that using the maximum IDF value per-
forms better than the other choices.
As shown in Equation (3), the Balanced method
regularizes the weights through two components:
(1) normalized confidence score of each aspect,
i.e.,
?
e
?
?Q
i(e
?
)
?
e
??
?A(e)
i(e
??
)
; and (2) the importance of the
query aspect, i.e., Imp
A
(A(e)). To examine the
effectiveness of each component, we conduct ex-
periments using the modified Balanced method
with only one of the components. The results are
shown in Table 7. It is clear that both components
are essential to improve the retrieval performance.
Finally, we report the performance improve-
ment of the proposed methods over the Concept-
BL for each query in Figure 3. Clearly, both of the
610
-0.2
-0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
101 106 111 116 121 126 131 136 141 146 151 156 161 165 171 176 181 185
Pe
rfo
rm
an
ce
 D
iff
er
en
ce
Query ID
Improvement(Unified)
Improvement(Balanced)
Figure 3: Improvement of proposed methods (Compared with the Concept-BL method).
proposed methods can improve the effectiveness
of most queries, and the Balanced method is more
robust than the Unified method.
6 Conclusions and Future Work
Medical record retrieval is an important domain-
specific IR problem. Concept-based representa-
tion is an effective approach to dealing with am-
biguity terminology in medical domain. How-
ever, the results of the NLP tools used to gen-
erate the concept-based representation are often
not perfect. In this paper, we present a general
methodology that can use axiomatic approaches
as guidance to regularize the concept weighting
strategies to address the limitations caused by the
inaccurate concept mapping and improve the re-
trieval performance. In particular, we proposed
two weighting regularization methods based on
the relations among concepts. Experimental re-
sults show that the proposed methods can signif-
icantly outperform existing retrieval functions.
There are many interesting directions for our fu-
ture work. First, we plan to study how to automat-
ically predict whether to use concept-based index-
ing based on the quality of MetaMap results, and
explore whether the proposed methods are appli-
cable for other entity linking methods. Second,
we will study how to leverage other information
from knowledge bases to further improve the per-
formance. Third, more experiments could be con-
ducted to examine the effectiveness of the pro-
posed methods when using other ranking strate-
gies. Finally, it would be interesting to study how
to follow the proposed methodology to study other
domain-specific IR problems.
References
Gianni Amati and Cornelis Joost Van Rijsbergen.
2002. Probabilistic models of information retrieval
based on measuring the divergence from random-
ness. ACM TOIS.
Alan R. Aronson and Franc?ois-Michel Lang. 2010. An
overview of metamap: historical perspective and re-
cent advances. JAMIA, 17(3):229?236.
Alan R. Aronson. 2001. Effective mapping of biomed-
ical text to the UMLS Metathesaurus: the MetaMap
program. In Proceedings of AMIA Symposium.
Practice Management Information Corporation. 1999.
ICD-9-CM: International Classification of Dis-
eases, 9th Revision, Clinical Modification, 5th Edi-
tion. Practice Management Information Corpora-
tion.
Dina Demner-Fushman, Swapna Abhyankar, Anto-
nio Jimeno-Yepes, Russell Loane, Francois Lang,
James G. Mork, Nicholas Ide, and Alan R. Aron-
son. 2012. NLM at TREC 2012 Medical Records
Track. In Proceedings of TREC 2012.
Hui Fang and ChengXiang Zhai. 2005. An exploration
of axiomatic approaches to information retrieval. In
Proceedings of SIGIR?05.
Hui Fang, Tao Tao, and ChengXiang Zhai. 2004. A
formal study of information retrieval heuristics. In
Proceedings of SIGIR?04.
Charles P. Friedman, Adam K. Wong, and David Blu-
menthal. 2010. Achieving a nationwide learning
health system. Science Translational Medicine.
Beval Koopman, Michael Lawley, and Peter Bruza.
2011. AEHRC & QUT at TREC 2011 Medical
Track : a concept-based information retrieval ap-
proach. In Proceedings of TREC?11.
Bevan Koopman, Guido Zuccon, Anthony Nguyen,
Deanne Vickers, Luke Butt, and Peter D. Bruza.
611
2012. Exploiting SNOMED CT Concepts & Re-
lationships for Clinical Information Retrieval: Aus-
tralian e-Health Research Centre and Queensland
University of Technology at the TREC 2012 Med-
ical Track. In Proceedings of TREC?12.
Johannes Leveling, Lorraine Goeuriot, Liadh Kelly,
and Gareth J. F. Jones. 2012. DCU@TRECMed
2012: Using adhoc Baselines for Domain-Specific
Retrieval. In Proceedings of TREC 2012.
Nut Limsopatham, Craig Macdonald, and Iadh Ou-
nis. 2013a. Inferring conceptual relationships to
improve medical records search. In Proceedings of
OAIR?13.
Nut Limsopatham, Craig Macdonald, and Iadh Ou-
nis. 2013b. Learning to combine representations
for medical records search. In Proceedings of SI-
GIR?13.
Nut Limsopatham, Craig Macdonald, and Iadh Ounis.
2013c. Learning to selectively rank patients? medi-
cal history. In Proceedings of CIKM?13.
Nut Limsopatham, Craig Macdonald, and Iadh Ounis.
2013d. A task-specific query and document repre-
sentation for medical records search. In Proceedings
of ECIR?13.
Jimmy Lin and Dina Demner-Fushman. 2006. The
role of knowledge in conceptual retrieval: a study
in the domain of clinical medicine. In Proceedings
of the 29th annual international ACM SIGIR confer-
ence on Research and development in information
retrieval, SIGIR ?06, pages 99?106, New York, NY,
USA. ACM.
Carolyn E Lipscomb. 2000. Medical Subject Headings
(MeSH). The Medical Library Association.
Christopher D. Manning, P. Raghavan, and H. Schutze.
2008. Introduction to Information Retrieval. Cam-
bridge University Press.
Bridget T. McInnes, Ted Pedersen, and Serguei V. S.
Pakhomov. 2009. UMLS-Interface and UMLS-
Similarity : Open Source Software for Measuring
Paths and Semantic Similarity. In Proceedings of
AMIA Symposium.
Yanjun Qi and Pierre-Francois Laquerre. 2012. Re-
trieving Medical Records: NEC Labs America at
TREC 2012 Medical Record Track. In Proceedings
of TREC 2012.
S.E. Robertson, S. Walker, S. Jones, M.M. Hancock-
Beaulieu, and M. Gatford. 1996. Okapi at TREC-3.
pages 109?126.
Charles Safran, Meryl Bloomrosen, W. Edward
Hammond, Steven Labkoff, Suzanne Markel-Fox,
Paul C. Tang, and Don E. Detmer. 2007. White pa-
per: Toward a national framework for the secondary
use of health data: An american medical informatics
association white paper. JAMIA, 14(1):1?9.
Amit Singhal, Chris Buckley, and Mandar Mitra. 1996.
Pivoted document length normalization. In Pro-
ceedings of SIGIR?96.
Ellen M. Voorhees and William Hersh. 2012.
Overview of the TREC 2012 Medical Records
Track. In Proceedings of TREC 2012.
Ellen M. Voorhees and Richard M. Tong. 2011.
Overview of the TREC 2011 Medical Records
Track. In Proceedings of TREC 2011.
Xin Yan, Raymond Y.K. Lau, Dawei Song, Xue Li,
and Jian Ma. 2011. Toward a semantic granular-
ity model for domain-specific information retrieval.
ACM TOIS.
Chengxiang Zhai and John Lafferty. 2001. A study
of smoothing methods for language models applied
to Ad Hoc information retrieval. In Proceedings of
SIGIR?01.
Wei Zheng and Hui Fang. 2010. Query aspect
based term weighting regularization in information
retrieval. In Proceedings of ECIR?10.
Wei Zhou, Clement Yu, Neil Smalheiser, Vetle Torvik,
and Jie Hong. 2007. Knowledge-intensive concep-
tual retrieval and passage extraction of biomedical
literature. In Proceedings of SIGIR?07.
Dongqing Zhu and Ben Carterette. 2012. Combining
multi-level evidence for medical record retrieval. In
Proceedings of SHB?12.
612
Proceedings of BioNLP Shared Task 2011 Workshop, pages 7?15,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Overview of Genia Event Task in BioNLP Shared Task 2011
Jin-Dong Kim
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku, Tokyo
jdkim@dbcls.rois.ac.jp
Yue Wang
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku, Tokyo
wang@dbcls.rois.ac.jp
Toshihisa Takagi
University of Tokyo
5-1-5 Kashiwa-no-ha, Kashiwa, Chiba
tt@k.u-tokyo.ac.jp
Akinori Yonezawa
Database Center for Life Science
2-11-16 Yayoi, Bunkyo-ku, Tokyo
yonezawa@dbcls.rois.ac.jp
Abstract
The Genia event task, a bio-molecular event
extraction task, is arranged as one of the main
tasks of BioNLP Shared Task 2011. As its sec-
ond time to be arranged for community-wide
focused efforts, it aimed to measure the ad-
vance of the community since 2009, and to
evaluate generalization of the technology to
full text papers. After a 3-month system de-
velopment period, 15 teams submitted their
performance results on test cases. The re-
sults show the community has made a sig-
nificant advancement in terms of both perfor-
mance improvement and generalization.
1 Introduction
The BioNLP Shared Task (BioNLP-ST, hereafter)
is a series of efforts to promote a community-
wide collaboration towards fine-grained informa-
tion extraction (IE) in biomedical domain. The
first event, BioNLP-ST 2009, introducing a bio-
molecular event (bio-event) extraction task to the
community, attracted a wide attention, with 42 teams
being registered for participation and 24 teams sub-
mitting final results (Kim et al, 2009).
To establish a community effort, the organizers
provided the task definition, benchmark data, and
evaluations, and the participants competed in devel-
oping systems to perform the task. Meanwhile, par-
ticipants and organizers communicated to develop a
better setup of evaluation, and some provided their
tools and resources for other participants, making it
a collaborative competition.
The final results enabled to observe the state-of-
the-art performance of the community on the bio-
event extraction task, which showed that the auto-
matic extraction of simple events - those with unary
arguments, e.g. gene expression, localization, phos-
phorylation - could be achieved at the performance
level of 70% in F-score, but the extraction of com-
plex events, e.g. binding and regulation, was a lot
more challenging, having achieved 40% of perfor-
mance level.
After BioNLP-ST 2009, all the resources from the
event were released to the public, to encourage con-
tinuous efforts for further advancement. Since then,
several improvements have been reported (Miwa et
al., 2010b; Poon and Vanderwende, 2010; Vlachos,
2010; Miwa et al, 2010a; Bjo?rne et al, 2010).
For example, Miwa et al (Miwa et al, 2010b)
reported a significant improvement with binding
events, achieving 50% of performance level.
The task introduced in BioNLP-ST 2009 was re-
named to Genia event (GE) task, and was hosted
again in BioNLP-ST 2011, which also hosted four
other IE tasks and three supporting tasks (Kim et al,
2011). As the sole task that was repeated in the two
events, the GE task was referenced during the devel-
opment of other tasks, and took the role of connect-
ing the results of the 2009 event to the main tasks of
2011. The GE task in 2011 received final submis-
sions from 15 teams. The results show the commu-
nity made a significant progress with the task, and
also show the technology can be generalized to full
papers at moderate cost of performance.
This paper presents the task setup, preparation,
and discusses the results.
7
Event Type Primary Argument Secondary Argument
Gene expression Theme(Protein)
Transcription Theme(Protein)
Protein catabolism Theme(Protein)
Phosphorylation Theme(Protein) Site(Entity)
Localization Theme(Protein) AtLoc(Entity), ToLoc(Entity)
Binding Theme(Protein)+ Site(Entity)+
Regulation Theme(Protein/Event), Cause(Protein/Event) Site(Entity), CSite(Entity)
Positive regulation Theme(Protein/Event), Cause(Protein/Event) Site(Entity), CSite(Entity)
Negative regulation Theme(Protein/Event), Cause(Protein/Event) Site(Entity), CSite(Entity)
Table 1: Event types and their arguments for Genia event task. The type of each filler entity is specified in parenthesis.
Arguments that may be filled more than once per event are marked with ?+?.
2 Task Definition
The GE task follows the task definition of BioNLP-
ST 2009, which is briefly described in this section.
For more detail, please refer to (Kim et al, 2009).
Table 1 shows the event types to be addressed in
the task. For each event type, the primary and sec-
ondary arguments to be extracted with an event are
defined. For example, a Phosphorylation event is
primarily extracted with the protein to be phospho-
rylated. As secondary information, the specific site
to be phosphorylated may be extracted.
From a computational point of view, the event
types represent different levels of complexity. When
only primary arguments are considered, the first five
event types in Table 1 are classified as simple event
types, requiring only unary arguments. The Bind-
ing and Regulation types are more complex: Bind-
ing requires detection of an arbitrary number of ar-
guments, and Regulation requires detection of recur-
sive event structure.
Based on the definition of event types, the entire
task is divided to three sub-tasks addressing event
extraction at different levels of specificity:
Task 1. Core event extraction addresses the ex-
traction of typed events together with their pri-
mary arguments.
Task 2. Event enrichment addresses the extrac-
tion of secondary arguments that further spec-
ify the events extracted in Task 1.
Task 3. Negation/Speculation detection
addresses the detection of negations and
speculations over the extracted events.
Task 1 serves as the backbone of the GE task and is
mandatory for all participants, while the other two
are optional.
The failure of p65 translocation to the nucleus ?
Protein Localization Location
theme ToLoc
Negated
Figure 1: Event annotation example
Figure 1 shows an example of event annotation.
The event encoded in the text is represented in a
standoff-style annotation as follows:
T1 Protein 15 18
T2 Localization 19 32
T3 Entity 40 46
E1 Localization:T2 Theme:T1 ToLoc:T1
M1 Negation E1
The annotation T1 identifies the entity referred
to by the string (p65) between the character offsets,
15 and 18 to be a Protein. T2 identifies the string,
translocation, to refer to a Localization event. Enti-
ties other than proteins or event type references are
classified into a default class Entity, as in T3. E1
then represents the event defined by the three enti-
ties, as defined in Table 1. Note that for Task 1, the
entity, T3, does not need to be identified, and the
event, E1, may be identified without specification of
the secondary argument, ToLoc:T1:
E1? Localization:T2 Theme:T1
Finding the full representation of E1 is the goal of
Task 2. In the example, the localization event, E1,
is negated as expressed in the failure of . Finding the
negation, M1 is the goal of Task 3.
8
Training Devel Test
Item Abs. Full Abs. Full Abs. Full
Articles 800 5 150 5 260 4
Words 176146 29583 33827 30305 57256 21791
Proteins 9300 2325 2080 2610 3589 1712
Events 8615 1695 1795 1455 3193 1294
Gene expression 1738 527 356 393 722 280
Transcription 576 91 82 76 137 37
Protein catabolism 110 0 21 2 14 1
Phosphorylation 169 23 47 64 139 50
Localization 265 16 53 14 174 17
Binding 887 101 249 126 349 153
Regulation 961 152 173 123 292 96
Positive regulation 2847 538 618 382 987 466
Negative regulation 1062 247 196 275 379 194
Table 2: Statistics of annotations in training, development, and test sets
3 Data preparation
The data sets are prepared in two collections: the
abstract and the full text collections. The abstract
collection includes the same data used for BioNLP-
ST 2009, and is meant to be used to measure the
progress of the community. The full text collection
includes full papers which are newly annotated, and
is meant to be used to measure the generalization
of the technology to full papers. Table 2 shows the
statistics of the annotations in the GE task data sets.
Since the training data from the full text collection is
relatively small despite of the expected rich variety
of expressions in full text, it is expected that ?gener-
alization? of a model from the abstract collection to
full papers would be a key technique to get a reason-
able performance.
A full paper consists of several sections includ-
ing the title, abstract, introduction, results, conclu-
sion, methods, and so on. Different sections would
be written with different purposes, which may af-
fect the type of information that are found in the sec-
tions. Table 3 shows the distribution of annotations
in different sections. It indicates that event men-
tions, according to the event definition in Table 1, in
Methods and Captions are much less frequent than
in the other TIAB, Intro. and R/D/C sections. Fig-
ure 2 illustrates the different distribution of anno-
tated event types in the five sections. It is notable
that the Methods section (depicted in blue) shows
very different distribution compared to others: while
Gene_expression
Transcrip.
Binding
Regulation
Pos_regul.
Neg_regul.
TIAB Intro. R/D/C Methods Caption
Figure 2: Event distribution in different sections
Regulation and Positive regulation events are not as
frequent as in other sections, Negative regulation is
relatively much more frequent. It may agree with
an intuition that experimental devices, which will be
explained in Methods sections, often consists of ar-
tificial processes that are designed to cause a nega-
tive regulatory effect, e.g. mutation, addition of in-
hibitor proteins, etc. This observation suggests a dif-
ferent event annotation scheme, or a different event
extraction strategy would be required for Methods
sections.
9
Full Paper
Item Abstract Whole TIAB Intro. R/D/C Methods Caption
Words 267229 80962 3538 7878 43420 19406 6720
Proteins 14969 6580 336 597 3980 916 751
(Density: P / W) (5.60%) (8.13%) (9.50%) (7.58%) (9.17%) (4.72%) (11.18%)
Events 13603 4436 272 427 3234 198 278
(Density: E / W) (5.09%) (5.48%) (7.69%) (5.42%) (7.51%) (1.02%) (4.14%)
(Density: E / P) (90.87%) (67.42%) (80.95%) (71.52%) (81.93%) (21.62%) (37.02%)
Gene expression 2816 1193 62 98 841 80 112
Transcription 795 204 7 7 140 30 20
Protein catabolism 145 3 0 0 3 0 0
Phosphorylation 355 137 12 12 101 10 2
Localization 492 47 3 15 22 7 0
Binding 1485 380 16 74 266 6 18
Regulation 1426 371 35 30 281 4 21
Positive regulation 4452 1385 98 131 1087 15 54
Negative regulation 1637 716 39 60 520 46 51
Table 3: Statistics of annotations in different sections of text: the Abstract column is of the abstraction collection
(1210 titles and abstracts), and the following columns are of full paper collection (14 full papers). TIAB = title and
abstract, Intro. = introduction and background, R/D/C = results, discussions, and conclusions, Methods = methods,
materials, and experimental procedures. Some minor sections, supporting information, supplementary material, and
synopsis, are ignored. Density = relative density of annotation (P/W = Protein/Word, E/W = Event/Word, and E/P =
Event/Protein).
4 Participation
In total, 15 teams submitted final results. All 15
teams participated in the mandatory Task 1, four
teams in Task 2, and two teams in Task 3. Only one
team, UTurku, completed all the three tasks.
Table 4 shows the profile of the teams, except-
ing three who chose to remain anonymous. A brief
examination on the team organization (the People
column) suggests the importance of a computer sci-
ence background, C and BI, to perform the GE task,
which agrees with the same observation made in
2009. It is interpreted as follows: the role of com-
puter scientists may be emphasized in part due to
the fact that the task requires complex computational
modeling, demanding particular efforts in frame-
work design and implementation and computational
resources. The ?09 column suggests that previous
experience in the task may have affected to the per-
formance of the teams, especially in a complex task
like the GE task.
Table 5 shows the profile of the systems. A
notable observation is that four teams developed
their systems based on the model of UTurku09
(Bjo?rne et al, 2009) which was the winning sys-
tem of BioNLP-ST 2009. It may show an influence
of the BioNLP-ST series in the task. For syntac-
tic analyses, the prevailing use of Charniak John-
son re-ranking parser (Charniak and Johnson, 2005)
using the self-trained biomedical model from Mc-
Closky (2008) (McCCJ) which is converted to Stan-
ford Dependency (de Marneffe et al, 2006) is no-
table, which may also be an influence from the re-
sults of BioNLP-ST 2009. The last two teams,
XABioNLP and HCMUS, who did not use syntactic
analyses could not get a performance comparable to
the others, which may suggest the importance of us-
ing syntactic analyses for a complex IE task like GE
task.
5 Results
5.1 Task 1
Table 6 shows the final evaluation results of Task 1.
For reference, the reported performance of the two
systems, UTurku09 and Miwa10 is listed in the
top. UTurku09 was the winning system of Task 1
in 2009 (Bjo?rne et al, 2009), and Miwa10 was
the best system reported after BioNLP-ST 2009
(Miwa et al, 2010b). Particularly, the latter made
10
Team ?09 Task People reference
FAUST
?
12- 3C (Riedel et al, 2011)
UMASS
?
12- 1C (Riedel and McCallum, 2011)
UTurku
?
123 1BI (Bjrne and Salakoski, 2011)
MSR-NLP 1-- 4C (Quirk et al, 2011)
ConcordU
?
1-3 2C (Kilicoglu and Bergler, 2011)
UWMadison
?
1-- 2C (Vlachos and Craven, 2011)
Stanford 1-- 3C+1.5L (McClosky et al, 2011)
BMI@ASU
?
12- 3C (Emadzadeh et al, 2011)
CCP-BTMG
?
1-- 3BI (Liu et al, 2011)
TM-SCS 1-- 1C (Bui and Sloot, 2011)
XABioNLP 1-- 4C (Casillas et al, 2011)
HCMUS 1-- 6L (Minh et al, 2011)
Table 4: Team profiles: The ?09 column indicates whether at least one team member participated in BioNLP-ST 2009.
In People column, C=Computer Scientist, BI=Bioinformatician, B=Biologist, L=Linguist
NLP Task Other resources
Team Lexical Proc. Syntactic Proc. Trig. Arg. group Dictionary Other
FAUST SnowBall, CNLP McCCJ+SD Stacking (UMASS + Stanford)
UMASS SnowBall, CNLP McCCJ+SD Joint infer., Dual Decomposition
UTurku Porter McCCJ+SD SVM SVM SVM S. cues
MSR-NLP Porter McCCJ+SD, Enju SVM MaxEnt rules Coref(Hobbs)
ConcordU - McCCJ+SD dic rules rules S./N. cues
UWMadison Morpha, Porter MCCCJ+SD Joint infer., SEARN
Stanford Morpha, CNLP McCCJ+SD MaxEnt MSTParser word clusters
BMI@ASU Porter, WordNet Stanford+SD SVM SVM - MeSH
CCP-BTMG Porter, WordNet Stanford+SD Subgraph Isomorphism
TM-SCS Stanford Stanford dic rules rules
XABioNLP KAF - rules
HCMUS OpenNLP - dic, rules rules UIMA
Table 5: System profiles: SnowBall=SnowBall Stemmer, CNLP=Stanford CoreNLP (tokenization), KAF=Kyoto An-
notation Format McCCJ=McClosky-Charniak-Johnson Parser, Stanford=Stanford Parser, SD=Stanford Dependency
Conversion, S.=Speculation, N.=Negation
an impressive improvement with Binding events
(44.41%?52.62%).
The best performance in Task 1 this time is
achieved by the FAUST system, which adopts a
combination model of UMass and Stanford. Its
performance on the abstract collection, 56.04%,
demonstrates a significant improvement of the com-
munity in the repeated GE task, when compared to
both UTurku09, 51.95% and Miwa10, 53.29%.
The biggest improvement is made to the Regulation
events (40.11%?46.97%) which requires a com-
plex modeling for recursive event structure - an
event may become an argument of another event.
The second ranked system, UMass, shows the best
performance on the full paper collection. It suggests
that what FAUST obtained from the model combi-
nation might be a better optimization to abstracts.
The ConcordU system is notable as it is the sole
rule-based system that is ranked above the average.
It shows a performance optimized for precision with
relatively low recall. The same tendency is roughly
replicated by other rule-based systems, CCP-BTMG,
TM-SCS, XABioNLP, and HCMUS. It suggests that
a rule-based system might not be a good choice if a
high coverage is desired. However, the performance
of ConcordU for simple events suggests that a high
precision can be achieved by a rule based system
with a modest loss of recall. It might be more true
when the task is less complex.
This time, three teams achieved better results than
Miwa10, which indicates some role of focused ef-
forts like BioNLP-ST. The comparison between the
11
performance on abstract and full paper collections
shows that generalization to full papers is feasible
with very modest loss in performance.
5.2 Task 2
Tables 7 shows final evaluation results of Task 2.
For reference, the reported performance of the task-
winning system in 2009, UT+DBCLS09 (Riedel et
al., 2009), is shown in the top. The first and second
ranked system, FAUST and UMass, which share a
same author with Riedel09, made a significant
improvement over Riedel09 in the abstract col-
lection. UTurku achieved the best performance in
finding sites arguments but did not produce location
arguments. In table 7, the performance of all the
systems in full text collection suggests that finding
secondary arguments in full text is much more chal-
lenging.
In detail, a significant improvement was made for
Location arguments (36.59%?50.00%). A further
breakdown of the results of site extraction, shown
in table 8, shows that finding site arguments for
Phosphorylation, Binding and Regulation events are
all significantly improved, but in different ways.
The extraction of protein sites to be phosphory-
lated is approaching a practical level of performance
(84.21%), while protein sites to be bound or to be
regulated remains challenging to be extracted.
5.3 Task 3
Table 9 shows final evaluation results of Task 3.
For reference, the reported performance of the task-
winning system in 2009, Kilicoglu09(Kilicoglu
and Bergler, 2009), is shown in the top. Among the
two teams participated in the task, UTurku showed
a better performance in extracting negated events,
while ConcordU showed a better performance in
extracting speculated events.
6 Conclusions
The Genia event task which was repeated for
BioNLP-ST 2009 and 2011 took a role of measur-
ing the progress of the community and generaliza-
tion IE technology to full papers. The results from
15 teams who made their final submissions to the
task show that a clear advance of the community in
terms of the performance on a focused domain and
also generalization to full papers. To our disappoint-
ment, however, an effective use of supporting task
results was not observed, which thus remains as fu-
ture work for further improvement.
Acknowledgments
This work is supported by the ?Integrated Database
Project? funded by the Ministry of Education, Cul-
ture, Sports, Science and Technology of Japan.
References
Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,
Tapio Pahikkala, and Tapio Salakoski. 2009. Extract-
ing complex biological events with rich graph-based
feature sets. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages 10?
18, Boulder, Colorado, June. Association for Compu-
tational Linguistics.
Jari Bjo?rne, Filip Ginter, Sampo Pyysalo, Jun?ichi Tsujii,
and Tapio Salakoski. 2010. Complex event extraction
at PubMed scale. Bioinformatics, 26(12):i382?390.
Jari Bjrne and Tapio Salakoski. 2011. Generaliz-
ing Biomedical Event Extraction. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Quoc-Chinh Bui and Peter. M.A. Sloot. 2011. Extracting
biological events from text using simple syntactic pat-
terns. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Arantza Casillas, Arantza Daz de Ilarraza, Koldo Go-
jenola, Maite Oronoz, and German Rigau. 2011. Us-
ing Kybots for Extracting Events in Biomedical Texts.
In Proceedings of the BioNLP 2011 Workshop Com-
panion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proceedings of the Fifth International Conference
on Language Resources and Evaluation (LREC?06),
pages 449?454.
Ehsan Emadzadeh, Azadeh Nikfarjam, and Graciela
Gonzalez. 2011. Double Layered Learning for Bi-
ological Event Extraction from Text. In Proceedings
12
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2009. Syntactic de-
pendency based heuristics for biological event extrac-
tion. In Proceedings of the BioNLP 2009 Workshop
Companion Volume for Shared Task, pages 119?127,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Halil Kilicoglu and Sabine Bergler. 2011. Adapting a
General Semantic Interpretation Approach to Biolog-
ical Event Extraction. In Proceedings of the BioNLP
2011 Workshop Companion Volume for Shared Task,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011. Overview of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Haibin Liu, Ravikumar Komandur, and Karin Verspoor.
2011. From graphs to events: A subgraph matching
approach for information extraction from biomedical
text. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
David McClosky and Eugene Charniak. 2008. Self-
Training for Biomedical Parsing. In Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics - Human Language Technolo-
gies (ACL-HLT?08), pages 101?104.
David McClosky, Mihai Surdeanu, and Christopher Man-
ning. 2011. Event Extraction as Dependency Parsing
for BioNLP 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Quang Le Minh, Son Nguyen Truong, and Quoc Ho Bao.
2011. A pattern approach for Biomedical Event Anno-
tation . In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, and
Jun?ichi Tsujii. 2010a. A comparative study of syn-
tactic parsers for event extraction. In Proceedings of
BioNLP?10, pages 37?45.
Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010b. Event extraction with complex event
classification using rich features. Journal of Bioinfor-
matics and Computational Biology (JBCB), 8(1):131?
146, February.
Hoifung Poon and Lucy Vanderwende. 2010. Joint infer-
ence for knowledge extraction from biomedical litera-
ture. In Proceedings of NAACL-HLT?10, pages 813?
821.
Chris Quirk, Pallavi Choudhury, Michael Gamon, and
Lucy Vanderwend. 2011. MSR-NLP Entry in
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel and Andrew McCallum. 2011. Robust
Biomedical Event Extraction with Dual Decomposi-
tion and Minimal Domain Adaptation. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,
and Jun?ichi Tsujii. 2009. A markov logic approach
to bio-molecular event extraction. In Proceedings of
the BioNLP 2009 Workshop Companion Volume for
Shared Task, pages 41?49, Boulder, Colorado, June.
Association for Computational Linguistics.
Sebastian Riedel, David McClosky, Mihai Surdeanu, An-
drew McCallum, and Christopher Manning. 2011.
Model Combination for Event Extraction in BioNLP
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Andreas Vlachos and Mark Craven. 2011. Biomedical
Event Extraction from Abstracts and Full Papers using
Search-based Structured Prediction. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Andreas Vlachos. 2010. Two strong baselines for the
bionlp 2009 event extraction task. In Proceedings of
BioNLP?10, pages 1?9.
13
Team Simple Event Binding Regulation All
UTurku09 A 64.21 / 77.45 / 70.21 40.06 / 49.82 / 44.41 35.63 / 45.87 / 40.11 46.73 / 58.48 / 51.95
Miwa10 A 70.44 52.62 40.60 48.62 / 58.96 / 53.29
W 68.47 / 80.25 / 73.90 44.20 / 53.71 / 48.49 38.02 / 54.94 / 44.94 49.41 / 64.75 / 56.04
FAUST A 66.16 / 81.04 / 72.85 45.53 / 58.09 / 51.05 39.38 / 58.18 / 46.97 50.00 / 67.53 / 57.46
F 75.58 / 78.23 / 76.88 40.97 / 44.70 / 42.75 34.99 / 48.24 / 40.56 47.92 / 58.47 / 52.67
W 67.01 / 81.40 / 73.50 42.97 / 56.42 / 48.79 37.52 / 52.67 / 43.82 48.49 / 64.08 / 55.20
UMass A 64.21 / 80.74 / 71.54 43.52 / 60.89 / 50.76 38.78 / 55.07 / 45.51 48.74 / 65.94 / 56.05
F 75.58 / 83.14 / 79.18 41.67 / 47.62 / 44.44 34.72 / 47.51 / 40.12 47.84 / 59.76 / 53.14
W 68.22 / 76.47 / 72.11 42.97 / 43.60 / 43.28 38.72 / 47.64 / 42.72 49.56 / 57.65 / 53.30
UTurku A 64.97 / 76.72 / 70.36 45.24 / 50.00 / 47.50 40.41 / 49.01 / 44.30 50.06 / 59.48 / 54.37
F 78.18 / 75.82 / 76.98 37.50 / 31.76 / 34.39 34.99 / 44.46 / 39.16 48.31 / 53.38 / 50.72
W 68.99 / 74.30 / 71.54 42.36 / 40.47 / 41.39 36.64 / 44.08 / 40.02 48.64 / 54.71 / 51.50
MSR-NLP A 65.99 / 74.71 / 70.08 43.23 / 44.51 / 43.86 37.14 / 45.38 / 40.85 48.52 / 56.47 / 52.20
F 78.18 / 73.24 / 75.63 40.28 / 32.77 / 36.14 35.52 / 41.34 / 38.21 48.94 / 50.77 / 49.84
W 59.99 / 85.53 / 70.52 29.33 / 49.66 / 36.88 35.72 / 45.85 / 40.16 43.55 / 59.58 / 50.32
ConcordU A 56.51 / 84.56 / 67.75 29.97 / 49.76 / 37.41 36.24 / 47.09 / 40.96 43.09 / 60.37 / 50.28
F 70.65 / 88.03 / 78.39 27.78 / 49.38 / 35.56 34.58 / 43.22 / 38.42 44.71 / 57.75 / 50.40
W 59.67 / 80.95 / 68.70 29.33 / 49.66 / 36.88 34.10 / 49.46 / 40.37 42.56 / 61.21 / 50.21
UWMadison A 54.99 / 79.85 / 65.13 34.87 / 56.81 / 43.21 34.54 / 50.67 / 41.08 42.17 / 62.30 / 50.30
F 74.03 / 83.58 / 78.51 15.97 / 29.87 / 20.81 33.11 / 46.87 / 38.81 43.53 / 58.73 / 50.00
W 65.79 / 76.83 / 70.88 39.92 / 49.87 / 44.34 27.55 / 48.75 / 35.21 42.36 / 61.08 / 50.03
Stanford A 62.61 / 77.57 / 69.29 42.36 / 54.24 / 47.57 28.25 / 49.95 / 36.09 42.55 / 62.69 / 50.69
F 75.58 / 75.00 / 75.29 34.03 / 40.16 / 36.84 26.01 / 46.08 / 33.25 41.88 / 57.36 / 48.41
W 62.09 / 76.55 / 68.57 27.90 / 44.92 / 34.42 22.30 / 40.26 / 28.70 36.91 / 56.63 / 44.69
BMI@ASU A 58.71 / 78.51 / 67.18 26.22 / 47.40 / 33.77 22.99 / 40.47 / 29.32 36.61 / 57.82 / 44.83
F 72.47 / 72.09 / 72.28 31.94 / 40.71 / 35.80 20.78 / 39.74 / 27.29 37.65 / 53.93 / 44.34
W 53.61 / 75.13 / 62.57 22.61 / 49.12 / 30.96 19.01 / 43.80 / 26.51 31.57 / 58.99 / 41.13
CCP-BTMG A 50.93 / 74.50 / 60.50 25.65 / 53.29 / 34.63 19.54 / 43.47 / 26.96 31.87 / 59.02 / 41.39
F 61.82 / 76.77 / 68.49 15.28 / 37.29 / 21.67 17.83 / 44.63 / 25.48 30.82 / 58.92 / 40.47
W 57.33 / 71.34 / 63.57 34.01 / 44.77 / 38.66 16.39 / 25.37 / 19.91 32.73 / 45.84 / 38.19
TM-SCS A 53.65 / 71.66 / 61.36 36.02 / 49.41 / 41.67 18.29 / 27.07 / 21.83 33.36 / 47.09 / 39.06
F 68.57 / 70.59 / 69.57 29.17 / 35.00 / 31.82 12.20 / 21.02 / 15.44 31.14 / 42.83 / 36.06
W 43.71 / 47.18 / 45.38 05.30 / 50.00 / 09.58 05.79 / 26.94 / 09.54 19.07 / 42.08 / 26.25
XABioNLP A 39.76 / 45.90 / 42.61 06.34 / 56.41 / 11.40 04.72 / 23.21 / 07.84 17.91 / 40.74 / 24.89
F 55.84 / 50.23 / 52.89 02.78 / 30.77 / 05.10 08.18 / 33.89 / 13.17 21.96 / 45.09 / 29.54
W 24.82 / 35.14 / 29.09 04.68 / 12.92 / 06.88 01.63 / 10.40 / 02.81 10.12 / 27.17 / 14.75
HCMUS A 22.42 / 37.38 / 28.03 04.61 / 10.46 / 06.40 01.69 / 10.37 / 02.91 09.71 / 27.30 / 14.33
F 32.21 / 31.16 / 31.67 04.86 / 28.00 / 08.28 01.47 / 10.48 / 02.59 11.14 / 26.89 / 15.75
Table 6: Evaluation results (recall / precision / f-score) of Task 1 in (W)hole data set, (A)bstracts only, and (F)ull
papers only. Some notable figures are emphasized in bold.
14
Team Sites (222) Locations (66) All (288)
UT+DBCLS09 A 23.08 / 88.24 / 36.59 32.14 / 72.41 / 44.52
W 32.88 / 70.87 / 44.92 36.36 / 75.00 / 48.98 33.68 / 71.85 / 45.86
FAUST A 43.51 / 71.25 / 54.03 36.92 / 77.42 / 50.00 41.33 / 72.97 / 52.77
F 17.58 / 69.57 / 28.07 - 17.39 / 66.67 / 27.59
W 31.98 / 71.00 / 44.10 36.36 / 77.42 / 49.48 32.99 / 72.52 / 45.35
UMass A 42.75 / 70.00 / 53.08 36.92 / 77.42 / 50.00 40.82 / 72.07 / 52.12
F 16.48 / 75.00 / 27.03 - 16.30 / 75.00 / 26.79
W 32.88 / 62.93 / 43.20 22.73 / 83.33 / 35.71 30.56 / 65.67 / 41.71
BMI@ASU A 37.40 / 67.12 / 48.04 23.08 / 83.33 / 36.14 32.65 / 70.33 / 44.60
F 26.37 / 55.81 / 35.82 - 26.09 / 55.81 / 35.56
W 40.09 / 65.44 / 49.72 00.00 / 00.00 / 00.00 30.90 / 65.44 / 41.98
UTurku A 48.09 / 69.23 / 56.76 00.00 / 00.00 / 00.00 32.14 / 69.23 / 43.90
F 28.57 / 57.78 / 38.24 - 28.26 / 57.78 / 37.96
Table 7: Evaluation results of Task 2 in (W)hole data set, (A)bstracts only, and (F)ull papers only
Team Phospho. (67) Binding (84) Reg. (71)
Riedel?09 A 71.43 / 71.43 / 71.43 04.76 / 50.00 / 08.70 12.96 / 58.33 / 21.21
W 71.64 / 84.21 / 77.42 05.95 / 38.46 / 10.31 28.17 / 60.61 / 38.46
FAUST A 71.43 / 81.63 / 76.19 04.76 / 14.29 / 07.14 29.63 / 66.67 / 41.03
F 72.73 / 100.0 / 84.21 06.35 / 66.67 / 11.59 23.53 / 44.44 / 30.77
W 76.12 / 79.69 / 77.86 04.76 / 36.36 / 08.42 22.54 / 64.00 / 33.33
UMass A 76.79 / 76.79 / 76.79 04.76 / 14.29 / 07.14 22.22 / 70.59 / 33.80
F 72.73 / 100.0 / 84.21 04.76 / 75.00 / 08.96 23.53 / 50.00 / 32.00
W 52.24 / 97.22 / 67.96 20.24 / 53.12 / 29.31 29.58 / 43.75 / 35.29
BMI@ASU A 53.57 / 96.77 / 68.97 09.52 / 22.22 / 13.33 31.48 / 51.52 / 39.08
F 45.45 / 100.0 / 62.50 23.81 / 65.22 / 34.88 23.53 / 26.67 / 25.00
W 76.12 / 91.07 / 82.93 21.43 / 51.43 / 30.25 28.17 / 44.44 / 34.48
UTurku A 78.57 / 89.80 / 83.81 09.52 / 18.18 / 12.50 31.48 / 54.84 / 40.00
F 63.64 / 100.0 / 77.78 25.40 / 66.67 / 36.78 17.65 / 21.43 / 19.35
Table 8: Evaluation results of Site information for different event types in (A)bstracts
Team Negation Speculation All
Kilicoglu09 A 14.98 / 50.75 / 23.13 16.83 / 50.72 / 25.27 15.86 / 50.74 / 24.17
W 22.87 / 48.85 / 31.15 17.86 / 32.54 / 23.06 20.30 / 39.67 / 26.86
UTurku A 22.03 / 49.02 / 30.40 19.23 / 38.46 / 25.64 20.69 / 43.69 / 28.08
F 25.76 / 48.28 / 33.59 15.00 / 23.08 / 18.18 19.28 / 30.85 / 23.73
W 18.77 / 44.26 / 26.36 21.10 / 38.46 / 27.25 19.97 / 40.89 / 26.83
ConcordU A 18.06 / 46.59 / 26.03 23.08 / 40.00 / 29.27 20.46 / 42.79 / 27.68
F 21.21 / 38.24 / 27.29 17.00 / 34.69 / 22.82 18.67 / 36.14 / 24.63
Table 9: Evaluation results of Task 3 in (W)hole data set, (A)bstracts only, and (F)ull papers only
15
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 202?205,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
PubAnnotation - a persistent and sharable corpus and annotation repository
Jin-Dong Kim and Yue Wang
Database Center for Life Science (DBCLS),
Research Organization of Information and Systems (ROIS),
2-11-16, Yayoi, Bunkyo-ku, Tokyo, 113-0032, Japan
{jdkim|wang}@dbcls.rois.ac.jp
Abstract
There has been an active development of cor-
pora and annotations in the BioNLP commu-
nity. As those resources accumulate, a new
issue arises about the reusability. As a solu-
tion to improve the reusability of corpora and
annotations, we present PubAnnotation, a per-
sistent and sharable repository, where various
corpora and annotations can be stored together
in a stable and comparable way. As a position
paper, it explains the motivation and the core
concepts of the repository and presents a pro-
totype repository as a proof-of-concept.
1 Introduction
Corpora with high-quality annotation is regarded in-
dispensable for the development of language pro-
cessing technology (LT), e.g. natural language pro-
cessing (NLP) or textmining. Biology is one of the
fields which have strong needs for LT, due to the
high productivity of new information, most of which
is published in literature. There have been thus an
active development of corpora and annotations for
the NLP for biology (BioNLP). Those resources are
certainly an invaluable asset of the community.
As those resources accumulate, however, a new
issue arises about the reusability: the corpora and
annotations need to be sharable and comparable. For
example, there are a number of corpora that claim to
have annotations for protein or gene names, e.g, Ge-
nia (Kim et al, 2003), Aimed (Bunescu et al, 2004),
and Yapex (Franze?n et al, 2002). To reuse them, a
user needs to be able to compare them so that they
can devise a strategy on how to use them. It is how-
ever known that often the annotations in different
corpora are incompatible to each other (Wang et al,
2010): while one is considered as a protein name in
a corpus, it may not be the case in another.
A comparison of annotations in different corpora
could be made directly or indirectly. If there is an
overlap between two corpora, a direct comparison
of them would be possible. For example, there are
one1, two2 and three3 PubMed abstracts overlapped
between Genia - Yapex, Genia - Aimed, and Yapex
- Aimed corpora, respectively. When there is no or
insufficient overlap, an indirect comparison could be
tried (Wang et al, 2010). In any case, there are a
number of problems that make it costly and trouble-
some, though not impossible, e.g. different formats,
different ways of character encoding, and so on.
While there have been a few discussions about
the reusability of corpora and annotations (Cohen et
al., 2005; Johnson et al, 2007; Wang et al, 2010;
Campos et al, 2012), as a new approach, we present
PubAnnotation, a persistent and sharable storage or
repository, where various corpora and annotations
can be stored together in a stable and comparable
way. In this position paper, after the motivation and
background are explained in section 1, the initial de-
sign and a prototype implementation of the storage
are presented in section 2 and 3, respectively and fu-
ture works are discussed in section 4.
2 Design
Figure 1 illustrates the current situation of cor-
pus annotation in the BioNLP community, which
we consider problematic. In the community, there
1PMID-10357818
2PMID-8493578, PMID-8910398
3PMID-9144171, PMID-10318834, PMID-10713102
202
Figure 1: Usual setup of PubMed text annotation
are several central sources of texts, e.g. PubMed,
PubMed Central (PMC), and so on. In this work,
we consider only PubMed as the source of texts for
brevity, but the same concept should be applicable
to other sources. Texts from PubMed are mostly the
title and abstract of literature indexed in PubMed.
For an annotation project, text pieces from a source
database (DB) are often copied in a local storage and
annotations are attached to them.
Among others, the problem we focus on in this
situation is the variations that are made to the texts.
Suppose that there are two groups who happen to
produce annotations to a same PubMed abstract.
The abstract will be copied to the local storages of
the two groups (illustrated as the local storage 1 and
2 in the figure). There are however at least two rea-
sons that may cause the local copies to be different
from the abstract in PubMed, and also to be different
from each other even though they are copies of the
same PubMed abstract:
Versioning This variation is made by PubMed. The
text in PubMed is changed from time to time
for correction, change of policy, and so on. For
example, Greek letters, e.g., ?, are spelled out,
e.g., alpha, in old entries, but in recent entries
they are encoded as they are in Unicode. For
the reason, there is a chance that copies of the
same entry made at different times (snapshots,
hereafter) may be different from each other.
Conversion This variation is made by individual
groups. The texts in a local storage are some-
times changed for local processing. For exam-
ple, most of the currently available NLP tools
(for English), e.g., POS taggers and parsers that
Figure 2: Persistent text/annotation repository
Figure 3: Text/annotation alignment for integration
are developed based on Penn Treebank, can-
not treat Unicode characters appropriately. For
such NLP tools to be used, all the Unicode
characters need to be converted to ASCII char-
acter sequences in local copies. Sometimes, the
result of some pre-processing, e.g. tokeniza-
tion, also remains in local copies.
The problem of text variation may not be such a
problem that makes the reuse of corpora and anno-
tations extremely difficult, but a problem that makes
it troublesome, raising the cost of the entire commu-
nity substantially.
To remedy the problem, we present, a persistent
and sharable storage of corpora and annotations,
which we call PubAnnotation. Figure 2 illustrates
an improved situation we aim at with PubAnnota-
tion. The key idea is to maintain all the texts in
PubAnnotation in their canonical form, to which all
the corresponding annotations are to be aligned. For
texts from PubMed, the canonical form is defined to
be exactly the same as in PubMed. With the defini-
tion, a text entry in PubAnnotation needs to be up-
dated (uptodate in the figure) as the corresponding
text in PubMed changes (versioning). Accordingly,
the annotations belonging to the entry also need to
be re-aligned (alignment).
There also would be a situation where a variation
of a text entry is required for some reason, e.g. for
203
Figure 4: Text/annotation alignment example
application of an NLP tool that cannot handle Uni-
code characters. Figure 3 illustrates a required pro-
cess to cope with such a situation: first, the text is
exported in a desired form (conversion in the fig-
ure); second, annotations are made to the text; and
third, the annotations are aligned back to the text in
its canonical form in the repository.
Figure 4 shows an example of text conversion
and annotation alignment that are required when the
Enju parser (Miyao and Tsujii, 2008) needs to be
used for the annotation of protein names. The ex-
ample text includes a Greek letter, ?, which Enju
cannot properly handle. As Enju expects Greek
letters to be spelled out with double equal signs
on both sides, the example text is converted as so
when it is exported into a local storage. Based
on the pre-processing by Enju, the two text spans,
CD==epsilon== and CD4, are annotated as pro-
tein names. When they are imported back to PubAn-
notation, the annotations are re-aligned to the canon-
ical text in the repository. In this way, the texts
and annotations can be maintained in their canon-
ical form and in alignment respectively in PubAn-
notation. In the same way, existing annotations, e.g.
Genia, Aimed, Yapex, may be imported in the repos-
itory, as far as their base texts are sufficiently similar
to the canonical entries so that they can be aligned
reliably. In this way, various existing annotations
may be integrated in the repository,
To enable all the processes described so far, any
two versions of the same text need to be aligned, so
that the places of change can be detected. Text align-
ment is therefore a key technology of PubAnnota-
tion. In our implementation of the prototype repos-
itory, the Hunt-McIlroy?s longest common subse-
quence (LCS) algorithm (Hunt and McIlroy, 1976),
as implemented in the diff-lcs ruby gem pack-
age, is used for the alignment.
Figure 5: DB schema of persistent annotation repository
3 Prototype implementation
As a proof-of-concept, a prototype repository has
been implemented. One aspect considered seriously
is the scalability, as repository is intended to be ?per-
sistent?. Therefore it is implemented on a relational
database (Ruby on Rails with PostgreSQL 9.1.3), in-
stead of relying on a plain file system.
Figure 5 shows the database schema of the reposi-
tory.4 Three tables are created for documents, anno-
tations, and (annotation) contexts, respectively. The
annotations are stored in a stand-off style, each of
which belongs to a document and also to an anno-
tation context (context, hereafter). A context rep-
resents a set of annotations sharing the same set of
meta-data, e.g., the type of annotation and the an-
notator. For brevity, we only considered PubMed as
the source DB, and named entity recognition (NER)-
type annotations, which may be simply represented
by the attributes, begin, end, and label.
The prototype repository provides a RESTful in-
terface. Table 1 shows some example which can be
accessed with the standard HTTP GET method. A
new entry can be created in the repository using a
HTTP POST method with data in JSON format. Fig-
ure 6 shows an example of JSON data for the cre-
ation of annotations in the repository. Note that, the
base text of the annotations needs to be passed to-
gether with the annotations, so that the text can be
compared to the canonical one in the repository. If a
difference is detected, the repository will try to align
the annotations to the text in the repository.
4Although not shown in the figure, all the records are stored
with the date of creation.
204
http://server url/pmid/8493578
to retrieve the document record of a specific PMID
http://server url/pmid/8493578.ascii
same as above, but in US-ASCII encoding (Unicode characters are converted to HTML entities).
http://server url/pmid/8493578/annotations
to retrieve all the annotations to the specific document.
http://server url/pmid/8493578/contexts
to retrieve all the annotation contexts created to the specific document.
http://server url/pmid/8493578/annotations?context=genia-protein
to retrieve all the annotations that belong to genia-protein context.
http://server url/pmid/8493578/annotations.json?context=genia-protein
the same as above, but in JSON format.
Table 1: Examples of RESTful interface of the prototype repository
{
"document":
{"pmid":"8493578",
"text":"Regulation ..."},
"context":
{"name":"genia-protein"},
"annotations":
[
{"begin":51,"end":56,
"label":"Protein",
{"begin":75,"end":97,
"label":"Protein",
]
}
Figure 6: The JSON-encoded data for the creation of two
protein annotations to the document of PMID:8493578.
4 Discussions and conclusions
The current state of the design and the prototype
implementation are largely incomplete, and there is
a much room for improvement. For example, the
database schema has to be further developed to store
texts from various source DBs, e.g., PMC, and to
represent various types of annotations, e.g., relations
and events. The issue of governance is yet to be
discussed. We, however, hope the core concepts
presented in this position paper to facilitate discus-
sions and collaborations of the community and the
remaining issues to be addressed in near future.
Acknowledgments
This work was supported by the ?Integrated
Database Project? funded by the Ministry of Edu-
cation, Culture, Sports, Science and Technology of
Japan.
References
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M.
Marcotte, Raymond J. Mooney, Arun K. Ramani, and
Yuk Wah Wong. 2004. Comparative experiments
on learning information extractors for proteins and
their interactions. Artificial Intelligence in Medicine,
33(2):139?155.
David Campos, Srgio Matos, Ian Lewin, Jos Lus Oliveira,
and Dietrich Rebholz-Schuhmann. 2012. Harmoniza-
tion of gene/protein annotations: towards a gold stan-
dard medline. Bioinformatics, 28(9):1253?1261.
K. Bretonnel Cohen, Philip V Ogren, Lynne Fox, and
Lawrence Hunter. 2005. Empirical data on corpus de-
sign and usage in biomedical natural language process-
ing. In AMIA annual symposium proceedings, pages
156?160.
Kristofer Franze?n, Gunnar Eriksson, Fredrik Olsson, Lars
Asker, Per Lide?n, and Joakim Co?ster. 2002. Protein
names and how to find them. International Journal of
Medical Informatics, 67(13):49 ? 61.
James W. Hunt and M. Douglas McIlroy. 1976. An Al-
gorithm for Differential File Comparison. Technical
Report 41, Bell Laboratories Computing Science, July.
Helen Johnson, William Baumgartner, Martin Krallinger,
K Bretonnel Cohen, and Lawrence Hunter. 2007.
Corpus refactoring: a feasibility study. Journal of
Biomedical Discovery and Collaboration, 2(1):4.
Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, and Jun?ichi
Tsujii. 2003. GENIA corpus - a semantically an-
notated corpus for bio-textmining. Bioinformatics,
19(suppl. 1):i180?i182.
Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest
models for probabilistic hpsg parsing. Computational
Linguistics, 34(1):35?80, March.
Yue Wang, Jin-Dong Kim, Rune S?tre, Sampo Pyysalo,
Tomoko Ohta, and Jun?ichi Tsujii. 2010. Improving
the inter-corpora compatibility for protein annotations.
Journal of Bioinformatics and Computational Biology,
8(5):901?916.
205
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 240?243,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Boosting the protein name recognition performance
by bootstrapping on selected text
Yue Wang and Jin-Dong Kim
Database Center for Life Science,
Research Organization of Information and Systems
2-11-16 Yayoi, Bunkyo-ku, Tokyo, Japan 113-0032
{wang,jdkim}@dbcls.rois.ac.jp
Abstract
When only a small amount of manually anno-
tated data is available, application of a boot-
strapping method is often considered to com-
pensate for the lack of sufcient training ma-
terial for a machine-learning method. The
paper reports a series of experimental results
of bootstrapping for protein name recogni-
tion. The results show that the performance
changes signicantly according to the choice
of text collection where the training samples
to bootstrap, and that an improvement can be
obtained only with a well chosen text collec-
tion.
1 Introduction
While machine learning-based approaches are be-
coming more and more popular for the development
of natural language processing (NLP) systems, cor-
pora with annotation are regarded as a critical re-
source for the training process. Nonetheless, the cre-
ation of corpus annotation is an expensive and time-
consuming work (Cohen et al, 2005), and it is of-
ten the case that lack of sufcient annotation hinders
the development of NLP systems. Bootstrapping
method (Becker et al, 2005; Vlachos and Gasperin,
2006) can be considered as a way to automatically
inate the amount of corpus annotation to comple-
ment the lack of sufcient annotation.
In this study, we report the experimental results on
the effect of bootstrapping for the training of protein
name recognizers, particularly in the situation when
we have only a small amount of corpus annotations.
In summary, we begin with a small corpus with
manual annotation for protein names. A named en-
tity tagger trained on the small corpus is applied to
a big collection of text, to obtain more annotation.
We hope the newly created annotation to be precise
enough so that the training of a protein tagger can
benet from the increased training material.
We assume that the accuracy of a bootstrapping
method (Ng, 2004) depends on two factors: the ac-
curacy of the bootstrap tagger itself and the similar-
ity of the text to the original corpus. While accuracy
of the bootstrap tagger may be maximized by nd-
ing the optimal parameters of the applied machine
learning method, the choice of text where the origi-
nal annotations will bootstrap may also be a critical
factor for the success of the bootstrapping method.
Experimental results presented in this paper con-
rm that we can get a improvement by using a boot-
strapping method with a well chosen collection of
texts.
The paper is organized as follows. Section 2 intro-
duces the two datasets used in this paper. Following
that, in Section 3, we briey introduce the experi-
ments performed in our research. The experimental
results are demonstrated in Section 4. The research
is concluded in Section 5 and in the meanwhile, fu-
ture work is discussed.
2 Datasets
2.1 The cyanobacteria genome database
Cyanobacteria are prokaryotic organisms that have
served as important model organisms for studying
oxygenic photosynthesis and have played a signi-
240
cant role in the Earthfs history as primary producers
of atmospheric oxygen (Nakao et al, 2010).
The cyanobacteria genome database (abbreviated
to CyanoBase
1
) includes the annotations to the
PubMed text. In total, 39 species of the cyanobacte-
ria are covered in the CyanoBase.
In our cyanobacteria data (henceforth, the Kazusa
data for short), 270 abstracts were annotated by two
independent annotators. We take the entities, about
which both of the annotators agreed with each other.
In total, there are 1,101 entities in 2,630 sentences.
The Kazusa data was split equally into three sub-
sets and the subsets were used in turn as the training,
development and testing sets in the experiments.
2.2 The BioCreative data
The BioCreative data, which was used for the
BioCreative II gene mention task
2
, is described as
the tagged gene/protein names in the PubMed text.
The training set is used in the research, and totally
there are 15,000 sentences in the dataset.
Unlike other datasets, the BioCreative data was
designed to contain sentences both with and without
protein names, in a variety of contexts. Since the
collection is made to explicitly compile positive and
negative examples for protein recognition, there is a
chance that the sample of text is not comprehensive,
and gray-zone expressions may be missed.
The reason that we chose the BioCreative data
for the bootstrapping is that, the BioCreative data
(henceforth, the BC2 data for short) is the collection
for the purpose of training and evaluation of protein
name taggers.
3 Experiment summary
In the following experiments, the NERSuite
3
, a
named entity tagger based on Conditional Random
Fields (CRFs) (Lafferty et al, 2001; Sutton and Mc-
Callum, 2007), is used. The NERSuite is executable
open-source and serves as a machine learning sys-
tem for named entity recognition (NER). The sigma
value for the L2-regularization is optimizable and in
our experiments, we tune the sigma value between
10?1 to 104.
1
http://genome.kazusa.or.jp/cyanobase
2
http://www.biocreative.org/
3
http://nersuite.nlplab.org/
As mentioned in Section 2.1, the three subsets of
Kazusa data are used for training, tuning and testing
purposes, in turn. We experimented with all the six
combinations.
Experiments were performed to compare three
different strategies. First, with the baseline strat-
egy, the protein tagger is trained only on the Kazusa
training set. The sigma value is optimized on the
tuning set, and the performance is evaluated on the
test set. It is the most typical strategy particularly
when it is believed there is a sufcient training ma-
terial.
Second, with the bootstrapping strategy, the
Kazusa training set is used as the seed data. A tag-
ger for bootstrapping (bootstrap tagger, hereafter) is
trained on the seed data, and applied to the BC2 data
to bootstrap the training examples. Another pro-
tein tagger (application tagger) is then trained on the
bootstrapped BC2 data together with the seed data.
The Kazusa tuning set is used to optimize the two
sigma values for the two protein taggers, and the
performance is evaluated on the test set. With this
strategy, we wish the bootstrapped examples com-
plement the lack of sufcient training examples.
Experiment Seed BT BT+SS
E1 368 647 647 (1,103)
E2 368 647 647 (1,103)
E3 366 759 759 (1,200)
E4 366 769 590 (1,056)
E5 367 882 558 (1,068)
E6 367 558 558 (1,068)
Table 1: The number of positive examples used in each
experiment. The ?BT? column shows the number of posi-
tive examples obtained by the bootstrapping in the 15,000
BC2 sentences. In the last column, the gures in paren-
theses are the number of the selected sentences.
Third, the bootstrapping with sentence selection
strategy is almost the same with the bootstrapping
strategy, except that the second tagger is trained after
the non-relevant sentences are ltered out from the
BC2 data. Here, non-relevant sentences mean those
that are not tagged by the the bootstrap tagger. With
this strategy, we wish an improvement with the boot-
strapping by removing noisy data. Table 1 shows the
number of the seed and bootstrapped examples used
for the three strategies. It is observed that the seed
241
Training Tuning Testing Baseline BT BT+SS
E1 A B C 63.7/29.2/40.0 [102] 61.3/25.9/36.4 [104-101] 61.7/38.2/47.1 [104-104]
E2 A C B 65.2/36.9/47.1 [103] 67.7/35.0/46.1 [104-101] 61.7/46.7/53.2 [104-104]
E3 B C A 75.3/36.4/49.1 [102] 75.2/31.3/44.2 [102-101] 67.1/40.0/50.1 [102-101]
E4 B A C 68.5/33.8/45.3 [102] 70.2/28.9/40.9 [104-101] 66.7/36.5/47.2 [101-102]
E5 C B A 77.7/35.1/48.3 [101] 71.8/27.7/40.0 [104-102] 70.9/38.3/49.7 [100-101]
E6 C A B 73.0/39.1/50.9 [101] 76.1/32.2/45.3 [100-102] 67.7/41.8/51.7 [100-102]
Table 2: Experimental results of using the Kazusa and BC2 data (Precision/Recall/F-score). ?BT? and ?SS? represent
the bootstrapping and sentence selection strategies, respectively. The gures in square brackets are the sigma values
optimized in the experiments.
annotation bootstrap only on a small portion of the
BC2 data set, e.g., 1,103 vs. 15,000 sentences in the
case of E1 (less than 10%), suggesting that a large
portion of the data set may be irrelevant to the origi-
nal data set.
4 Experimental results
The experimental results of all the six combinations
are shown in Table 2. The use of the three subsets,
denoted by A, B, C, of the Kazusa data set for train-
ing, tuning and testing in each experiment is spec-
ied in ?training?, ?tuning? and ?testing? columns.
The results of the baseline strategy that uses only
the Kazusa data are shown in the ?baseline? column,
whereas the results with the bootstrapping methods
with and without sentence selection are shown in the
last two columns. As explained in Section 3, the
sigma values are optimized using the tuning set for
each experiment. Note that for bootstrapping, we
need two sigma values for the bootstrapping tagger
and the application tagger. See section 3.
The performance of named entity recognition is
measured in terms of precision, recall and F-score.
For matching criterion, in order to avoid underesti-
mation, instead of the exact matching, system per-
formance is evaluated under a soft matching, the
overlapping matching criterion. That is, if any part
of the annotated protein/gene names is recognized
by the NER tagger, we will regard that as a correct
answer.
4.1 Results with the bootstrapping strategy
Comparing the two columns, ?baseline? and ?BT?,
we observe that the use of bootstrapping may lead
to a degradation of the performance. Note that the
sigma values are optimized on the development set
for each experiment, and the text for bootstrapping
is BC2 corpus which is expected to be similar to the
Kazusa corpus, but still it is observed that the boot-
strapping does not work, suggesting that the text col-
lection may not yet similar enough.
4.2 Results with bootstrapping with sentence
selection
Comparing the last column (the ?BT+SS? column)
to the ?baseline? column, we observe that the appli-
cation of the bootstrapping method with sentence se-
lection consistently improves the performance. The
improvement is sometimes signicant, e.g., 7.1% of
difference in F-score in the case of E1, but some-
times not, e.g., only 0.8% in the case of E6, but the
performance is improved in the every experiments.
The results conrm our assumption that the choice
of text for bootstrapping is important, and that the
sentence selection is a stable method for the choice
of text.
5 Conclusion and future work
In order to compensate for the lack of sufcient
training data for a CRF-based protein name recog-
nizer, the potential of a bootstrapping method has
been explored through a series of experiments. The
BC2 data was chosen for the bootstrapping as the
data set was one collected for protein name recogni-
tion.
Our initial experiment showed that the seed anno-
tations bootstrapped only on a very small portion of
the BC2 data set, suggesting that a big portion of the
data set might be less relevant to the seed corpus.
From a series of experiments, it was observed that
the performance of protein name recognition was al-
ways improved with bootstrapping by selecting only
242
the sentences where the seed annotations bootstrap,
and by using them as an additional training data.
The goal was to be able to predict more possible
protein mentions (recall) at a relatively satisfactory
level of the quality (precision). The experimental
results suggest us, in order to achieve the goal, the
choice of text collection is important for the success
of the use of a bootstrapping method.
For the future work, we would like to take use of
the original annotations in the BC2 data. A ltering
strategy (Wang, 2010) will be performed. Instead of
completely using the output of the Kazusa-trained
tagger, we compare the output of the Kazusa-trained
tagger with the BioCreative annotations. If the en-
tity is recognized by the tagger and also annotated
in the BioCreative data, then the annotation to this
entity will be kept. The entity will be regarded as
a true positive according to the BioCreative annota-
tions. Otherwise, we will remove the annotation to
the entity from the BioCreative annotations.
Further, we also would like to combine the boot-
strapping with the ltering. Besides keeping the true
positives, we also want to include some false pos-
itives from the bootstrapping. Because these false
positives helps in improving the recall, when the tag-
ger is applied to the Kazusa testing subset. To dis-
criminate this strategy from the bootstrapping and
ltering strategies, different sigma value should be
used.
Acknowledgement
We thank Shinobu Okamoto for providing the
Kazusa data and for many useful discussion. This
work was supported by the ?Integrated Database
Project? funded by the Ministry of Education, Cul-
ture, Sports, Science and Technology (MEXT) of
Japan.
References
K. Bretonnel Cohen, Lynne Fox, Philip Ogren and
Lawrence Hunter. 2005. Empirical data on corpus
design and usage in biomedical natural language pro-
cessing. Proceedings of the AMIA Annual Symposium,
38?45.
Markus Becker, Ben Hachey, Beatrice Alex, Claire
Grover. 2005. Optimising Selective Sampling for
Bootstrapping Named Entity Recognition. Proceed-
ings of the Workshop on Learning with Multiple Views,
5?11.
Andreas Vlachos and Caroline Gasperin. 2006. Boot-
strapping and Evaluating Named Entity Recognition
in the Biomedical domain. Proceedings of the BioNLP
Workshop, 138?145.
Andrew Ng. 2004. Feature selection, L1 vs. L2 regu-
larization, and rotational invariance. Proceedings of
the 21st International Conference on Machine Learn-
ing (ICML).
Mitsuteru Nakao, Shinobu Okamoto, Mitsuyo Kohara,
Tsunakazu Fujishiro, Takatomo Fujisawa, Shusei
Sato, Satoshi Tabata, Takakazu Kaneko and Yasukazu
Nakamura. 2010. CyanoBase: the cyanobacteria
genome database update 2010. Nucleic Acids Re-
search, 38:D379?D381.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional Random Fields: Probabilistic Mod-
els for Segmenting and Labeling Sequence Data. Pro-
ceedings of the 18th International Conference on Ma-
chine Learning, 282?289.
Charles Sutton and Andrew McCallum. 2007. An Intro-
duction to Conditional Random Fields for Relational
Learning. Introduction to Statistical Relational Learn-
ing, MIT Press.
Yue Wang. 2010. Developing Robust Protein Name
Recognizers Based on a Comparative Analysis of Pro-
tein Annotations in Different Corpora. University of
Tokyo, Japan, PhD Thesis.
243
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 8?15,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
The Genia Event Extraction Shared Task, 2013 Edition - Overview
Jin-Dong Kim and Yue Wang and Yamamoto Yasunori
Database Center for Life Science (DBCLS)
Research Organization of Information and Systems (ROIS)
{jdkim|wang|yy}@dbcls.rois.ac.jp
Abstract
The Genia Event Extraction task is orga-
nized for the third time, in BioNLP Shared
Task 2013. Toward knowledge based con-
struction, the task is modified in a num-
ber of points. As the final results, it re-
ceived 12 submissions, among which 2
were withdrawn from the final report. This
paper presents the task setting, data sets,
and the final results with discussion for
possible future directions.
1 Introduction
Among various resources of life science, litera-
ture is regarded as one of the most important types
of knowledge base. Nevertheless, lack of explicit
structure in natural language texts prevents com-
puter systems from accessing fine-grained infor-
mation written in literature. BioNLP Shared Task
(ST) series (Kim et al, 2009; Kim et al, 2011a)
is one of the community-wide efforts to address
the problem. Since its initial organization in 2009,
BioNLP-ST series has published a number of fine-
grained information extraction (IE) tasks moti-
vated for bioinformatics projects. Having solicited
wide participation from the community of natural
language processing, machine learning, and bioin-
formatics, it has contributed to the production of
rich resources for fine-grained BioIE, e.g., TEES1
(Bjo?rne and Salakoski, 2011), SBEP2 (McClosky
et al, 2011) and EVEX3 (Van Landeghem et al,
2011).
The Genia Event Extraction (GE) task is a sem-
inal task of BioNLP-ST. It was first organized as
the sole task of the initial 2009 edition of BioNLP-
ST. The task was originally designed and imple-
mented based on the Genia event corpus (Kim et
1https://github.com/jbjorne/TEES/wiki
2http://nlp.stanford.edu/software/eventparser.shtml
3http://www.evexdb.org/
al., 2008b) which represented domain knowledge
around NF?B proteins. There were also some ef-
forts to explore the possibility of literature mining
for pathway construction (Kim et al, 2008a; Oda
et al, 2008). The GE task was designed to make
such an effort a community-driven one by sharing
available resources, e.g., benchmark data sets, and
evaluation tools, with the community.
In its second edition (Kim et al, 2011b) orga-
nized in BioNLP-ST 2011 (Kim et al, 2011a), the
data sets were extended to include full text articles.
The data sets consisted of two collections. The ab-
stract collection, that had come from the first edi-
tion, was used again to measure the progress of the
community between 2009 and 2011 editions, and
the full text collection, that was newly created, was
used to measure the generalization of the technol-
ogy to full text papers.
In its third edition this year, while succeeding
the fundamental characteristics from its previous
editions, the GE task tries to evolve with the goal
to make it a more ?real? task toward knowledge
base construction. The first design choice to ad-
dress the goal is to construct the data sets fully
with recent full papers, so that the extracted pieces
of information can represent up-to-date knowl-
edge of the domain. The abstract collection, that
had been already used twice (in 2009 and 2011), is
removed from official evaluation this time4. Sec-
ond, GE task subsumes the coreference task which
has long been considered critical for improvement
of event extraction performance. It is implemented
by providing coreference annotation in integration
with event annotation in the data sets.
The paper explains the task setting and data sets,
presents the final results of participating systems,
and discusses notable observations with conclu-
sions.
4However, if necessary, the online evaluation for the pre-
vious editions of GE task may be used, which is available at
http://bionlp-st.dbcls.jp/GE/.
8
Event Type Primary Argument Secondary Argument
Gene expression Theme(Protein)
Transcription Theme(Protein)
Localization Theme(Protein) Loc(Entity)?
Protein catabolism Theme(Protein)
Binding Theme(Protein)+ Site(Entity)*
Protein modification Theme(Protein), Cause(Protein/Event)? Site(Entity)?
Phosphorylation Theme(Protein), Cause(Protein/Event)? Site(Entity)?
Ubiquitination Theme(Protein), Cause(Protein/Event)? Site(Entity)?
Acetylation Theme(Protein), Cause(Protein/Event)? Site(Entity)?
Deacetylation Theme(Protein), Cause(Protein/Event)? Site(Entity)?
Regulation Theme(Protein/Event), Cause(Protein/Event)? Site(Entity)?, CSite(Entity)?
Positive regulation Theme(Protein/Event), Cause(Protein/Event)? Site(Entity)?, CSite(Entity)?
Negative regulation Theme(Protein/Event), Cause(Protein/Event)? Site(Entity)?, CSite(Entity)?
Table 1: Event types and their arguments for Genia Event Extraction task. The type of each filler entity
is specified in parenthesis. Arguments that may be filled more than once per event are marked with ?+?,
and optional arguments are with ???.
2 Task setting
This section explains the task setting of the 2013
edition of the GE task with a focus on changes to
previous editions. For comprehensive explanation,
readers are referred to Kim et al (2009).
The changes made to the task setting are three-
folds, among which two are about event types
to be extracted. Table 1 shows the event types
and their arguments targeted in the 2013 edition.
First, four new event types are added to the target
of extraction; the Protein modification
type and its three sub-types, Ubiquitination,
Acetylation, Deacetylation. Second,
The Protein modification types are modi-
fied to be directly linked to causal entities, which
was only possible through Regulation events
in previous editions.
The modifications were made based on analy-
sis on preliminary annotation during preparation
of the data sets: in recent papers on NF?B, dis-
cussions on protein modification were observed
with non-trivial frequency. However, in the end,
it turned out that the influence of the above modi-
fications was trivial in terms of the number of an-
notated instances in the final data sets, as shown
in section 3, after filtering out events on non-
individual proteins, e.g., protein families, protein
complexes.
Third change made to the task setting is addition
of coreference and part-of annotations to the data
sets. It is to address the observation from 2009
edition that coreference structures and entity rela-
tions often hide the syntactic paths between event
triggers and their arguments, restricting the perfor-
mance of event extraction. In 2011, the Protein
coreference task and Entity Relation were orga-
nized as sub-tasks, to explicitly address the prob-
lem, but this time, coreference and part-of anno-
tations are integrated in the GE task, to encour-
age an integrative use of them for event extrac-
tion. Figure 1 shows an example of annotation
with coreference and part-of annotations5. Note
that the event representation in the figure is re-
lation centric6, which is different from the event
centric representation of the default BioNLP-ST
format. The two representations are interchange-
able, and the GE task provides data sets in both
formats, together with an automatic converter be-
tween them. Below is the corresponding annota-
tion in the BioNLP-ST format:
T8 Protein 933 938 TRAF1
T9 Protein 940 945 TRAF2
T10 Protein 947 952 TRAF3
T11 Protein 958 963 TRAF6
T12 Protein 1038 1042 CD40
T41 Anaphora 1058 1072 These proteins
T48 Binding 1112 1119 binding
T49 Entity 1127 1143 cytoplasmic tail
T13 Protein 1147 1151 CD40
R1 Coreference Subject:T41 Object:T8
R2 Coreference Subject:T41 Object:T9
R3 Coreference Subject:T41 Object:T10
R4 Coreference Subject:T41 Object:T11
E4 Binding:T48 Theme:T8 Theme2:T13 Site2:T49
E5 Binding:T48 Theme:T9 Theme2:T13 Site2:T49
E6 Binding:T48 Theme:T10 Theme2:T13 Site2:T49
E7 Binding:T48 Theme:T11 Theme2:T13 Site2:T49
In the example, the event trigger, binding, de-
notes four binding events, in which the four pro-
teins, TRAF1, TRAF2, TRAF3, and TRAF6, bind
to the protein, CD40, respectively, through the
site, cytoplasmic tail. The links between the four
5The example is taken from the file, PMC-3148254-01-
Introduction.
6PubAnnotation (http://pubannotation.org) format.
9
Figure 1: Annotation example with coreferences and part-of relationship
proteins and the event trigger are however very
hard to find, without being bridged by the demon-
strative noun phrase (NP), These proteins. In the
case, if the link between the demonstrative NP,
These proteins and its four antecedents, TRAF1,
TRAF2, TRAF3, and TRAF6, can be somehow de-
tected, the remaining link, between the demonstra-
tive NP and the trigger, may be detected by their
syntactic connection. A key point here is the dif-
ferent characteristics of the two step links: de-
tecting the former is rather semantic or discour-
sal while the latter may be a more syntactic prob-
lem. Then, solving them using different processes
would make a sense. To encourage an exploration
into the hypothesis, the coreference annotation is
provided in the training and development data sets.
Based on the definition of event types, the en-
tire task is divided into three sub-tasks addressing
event extraction at different levels of specificity:
Task 1. Core event extraction addresses the ex-
traction of typed events together with their
primary arguments.
Task 2. Event enrichment addresses the extrac-
tion of secondary arguments that further
specify the events extracted in Task 1.
Task 3. Negation/Speculation detection
addresses the detection of negations and
speculations over the extracted events.
For more detail of the subtasks, readers are re-
ferred to Kim et al (2011b).
Item Training Devel Test
Articles 10 10 14
Words 54938 57907 75144
Proteins 3571 4138 4359
Entities 121 314 327
Events 2817 3199 3348
Gene expression 729 591 619
Transcription 122 98 101
Localization 44 197 99
Protein catabolism 23 30 14
Binding 195 376 342
Protein modification 8 1 1
Phosphorylation 117 197 161
Ubiquitination 4 2 30
Acetylation 0 3 0
Deacetylation 0 5 0
Regulation 299 284 299
Positive regulation 780 883 1144
Negative regulation 496 532 538
Coreferences 178 160 197
to Protein 152 123 169
to Entity 5 6 6
to Event 18 27 13
to Anaphora 3 4 9
Table 2: Statistics of annotations in training, de-
velopment, and test sets
3 Data Preparation
As discussed in section 1, for the 2013 edition, the
data sets are constructed fully with full text pa-
pers. Table 2 shows statistics of three data sets for
training, development and test. The data sets con-
sist of 34 full text papers from the Open Access
subset of PubMed Central. The papers were re-
trieved using lexical variants of the term, ?NF?B?
as primary keyword, and ?pathway? and ?regula-
tion? as secondary keywords. The retrieved papers
were given to the annotators with higher priority
10
Item TIAB Intro. R/D/C Methods Caption all
Words 10483 25543 125172 59612 29085 263133
Proteins 816 1507 9060 1797 2169 16427
(Density: P / W) (7.78%) (5.90%) (7.24%) (3.01%) (7.46%) (6.24%)
Prot. Coreferences 18 89 267 5 33 445
(Density: C / P) (2.21%) (5.91%) (2.95%) (0.28%) (1.52%) (2.71%)
Events 510 902 6391 311 892 9364
(Density: E / W) (4.87%) (3.53%) (5.11%) (0.52%) (3.07%) (3.56%)
(Density: E / P) (62.50%) (59.85%) (70.54%) (17.31%) (41.12%) (57.00%)
Gene expression 101 152 1265 125 220 1939
Transcription 10 18 209 36 47 321
Localization 19 47 191 8 41 340
Protein catabolism 0 3 49 0 8 67
Binding 29 158 572 15 92 913
Protein modification 1 1 7 0 0 10
Phosphorylation 27 38 347 19 35 475
Ubiquitination 0 2 8 0 10 36
Acetylation 0 3 0 0 0 3
Deacetylation 0 5 0 0 0 5
Regulation 67 76 625 7 66 882
Positive regulation 167 286 2045 19 203 2807
Negative regulation 89 113 1073 69 170 1566
Table 3: Statistics of annotations in different sections of text: the Abstract column is of the abstraction
collection (1210 titles and abstracts), and the following columns are of full paper collection (14 full
papers). TIAB = title and abstract, Intro. = introduction and background, R/D/C = results, discussions,
and conclusions, Methods = methods, materials, and experimental procedures. Some minor sections,
supporting information, supplementary material, and synopsis, are ignored. Density = relative density of
annotation (P/W = Protein/Word, E/W = Event/Word, and E/P = Event/Protein).
Figure 2: Event distribution in different sections
to newer ones. Note that among 34 papers, 14
were from the full text collection of 2011 edition
data sets, and 20 were newly collected this time.
The annotation to the all 34 papers were produced
by the same annotators who also produced anno-
tations for the previous editions of GE task.
The annotated papers are divided into the train-
ing, development, and test data sets; 10, 10, and
14, respectively. Note that the size of the training
data set is much smaller than previous editions,
in terms of number of words and events, while
the size of the development and test data sets are
comparable to previous editions. It is the conse-
quence of a design choice of the organizers with
the notion that (1) relevant resources are substan-
tially accumulated through last two editions, and
that (2) therefore the importance of training data
set may be reduced while the importance of devel-
opment and test data sets needs to be kept. Instead,
participants may utilize, for example, the abstract
collection of the 2011 edition, of which the anno-
tation was produced by the same annotators with
almost same principles. As another example, the
data sets of the EPI task (Ohta et al, 2011) also
may be utilized for the newly added protein modi-
fication events.
Table 3 shows the statistics of annotated event
types in different sections of the full papers in the
data sets. For the analysis, the sections are classi-
fied to five groups as follows:
? The TIAB group includes the titles and
abstracts. In the GE-2011 data sets,
the corresponding files match the pattern,
PMC-*TIAB*.txt.
? The Intro group includes sections
for introduction, and background. The
corresponding files match the pattern,
PMC-*@(-|._)@(I|Back)*.txt.
11
Team ?09 ?11 Task Expertise
EVEX UTurku 123 2C+2BI+1B
TEES-2.1 UTurku 123 2BI
BioSEM TM-SCS 1-- 1C+1BI
NCBI CCP-BTMG 1-- 3BI
DlutNLP 1-- 3C
HDS4NLP 1-- 3C
NICTANLM CCP-BTMG 1-3 6C
USheff 1-- 2C
UZH UZurich 1-- 6C
HCMUS HCMUS 1-- 4C
Table 4: Team profiles: The ?09 and ?11 columns
show the predecessors in 2009 and 2011 editions.
In Expertise column, C=Computer Scientist,
BI=Bioinformatician, B=Biologist, L=Linguist
? The R/D/C group includes sections
on results, discussions, and conclu-
sions. The files match the pattern,
PMC-*@(-|._)@(R|D|Conc)*.txt
? The Methods group includes sections on
methods, materials, and experimental pro-
cedures. The files match the pattern,
PMC-*@(-|._)@(Met|Mat|MAT|E)*.txt
? The Caption group includes the captions of
tables and figures. The corresponding files
math the pattern, PMC-*aption*.txt.
Figure 2 illustrates the different distribution of
annotated event types in the five section groups.
It shows that the Methods group has signifi-
cantly different distribution of annotated events,
confirming a similar observation reported in Kim
et al (2011b).
4 Participation
The GE task received final submissions from 12
teams, among which 2 were withdrawn from final
report. Table 4 summarizes the teams. Unfortu-
nately, the subtasks 2 and 3 did not met a large
participation.
Table 5 profiles the participating systems. The
systems are roughly grouped into SVM-based
pipeline (EVEX, TEES-2.1, and DlutNLP),
rule-based pipeline (BioSEM and UZH), mixed
pipeline (USheff and HCMUS), joint pattern
matching (NCBI and NICTANLM), and joint SVM
(HDS4NLP) systems. In terms of use of ex-
ternal resources, 5 teams (EVEX, TEES-2.1,
NCBI, DlutNLP, and USheff) utilized data sets
from 2011 edition, and two teams (HDS4NLP and
NICTANLM) utilized independent resources, e.g.,
UniProt (Bairoch et al, 2005), IntAct (Kerrien et
al., 2012), and CRAFT (Verspoor et al, 2012).
5 Results and Discussions
Table 6 shows the final results of subtask 1. Over-
all EVEX, TEES-2.1, and BioSEM show the
best performance only with marginal difference
between them. In detail, the performance of
BioSEM is significantly different from EVEX and
TEES-2.1: (1) while BioSEM show the best per-
formance with Binding and Protein modification
events, EVEX and TEES-2.1 show the best per-
formance with Regulation events which takes the
largest portion of annotation in data sets; and (2)
while the performance of EVEX and TEES-2.1
is balanced over recall and precision, BioSEM is
biased for precision, which is a typical feature of
rule-based systems. It is also notable that BioSEM
has achieved a near best performance using only
shallow parsing. Although it is not shown in the
table, NCBI is the only system which produced
Ubiquitination events, which is interpreted
as a result of utilizing 2011-EPI data sets (Ohta et
al., 2011) for the system development.
Table 7 shows subtask 1 final results only within
TIAB sections. It shows that the systems de-
veloped utilizing previous resources, e.g., 2011
data sets, and EVEX, perform better for titles and
abstracts, which makes sense because those re-
sources are title and abstract-centric.
Tables 8 and 9 show evaluation results within
Methods and Captions section groups, respec-
tively. All the systems show their worst per-
formance in the two section groups. Especially
the drop of performance with regulation events is
huge. Note the two section groups also show sig-
nificantly different event distribution compared to
other section groups (see section 3). It suggests
that language expression in the two section groups
may be quite different from other sections, and an
extensive examination is required to get a reason-
able performance in the sections.
Table 10 and 11 show final results of Task 2
(Event enrichment) and 3 (Negation/Speculation
detection), respectively, which unfortunately did
not meet a large participation.
6 Conclusions
In its third edition, the GE task is fully changed
to a full text paper centric task, while the online
evaluation service on the abstract-centric data sets
12
NLP Task Other resources
Team Lexical Proc. Syntactic Proc. Trig. Arg. group Dic. Other
EVEX Porter McCCJ SVM SVM SVM S. cues EVEX
TEES-2.1 Porter McCCJ SVM SVM SVM S. cues
BioSEM OpenNLP, LingPipe OpenNLP(shallow) dic rules rules
NCBI MedPost, BioLemm McCCJ Subgraph Isomorphism rules 2011 GE / EPI
DlutNLP Porter, GTB-tok McCCJ SVM SVM rules 2011 GE
HDS4NLP CNLP, Morpha McCCJ SVM SVM UniProt, IntAct
NICTANLM ClearParser Subgraph Isomorphism rules CRAFT, EVEX
USheff Porter, LingPipe Stanford dic SVM SVM, rules 2011 GE
UZH Porter, Morpha, LingPipe LTT2, Pro3Gres dic. MaxEnt rules rules
HCMUS SnowBall McCCJ dic, SVM rules, SVM rules
Table 5: System profiles: SnowBall=SnowBall Stemmer, CNLP=Stanford CoreNLP (tokenization),
McCCJ=McClosky-Charniak-Johnson Parser, Stanford=Stanford Parser, S.=Speculation, N.=Negation
Team Simple Event Binding Prot-Mod. Regulation All
EVEX 73.83 / 79.56 / 76.59 41.14 / 44.77 / 42.88 61.78 / 69.41 / 65.37 32.41 / 47.16 / 38.41 45.44 / 58.03 / 50.97
TEES-2.1 74.19 / 79.64 / 76.82 42.34 / 44.34 / 43.32 63.87 / 69.32 / 66.49 33.08 / 44.78 / 38.05 46.17 / 56.32 / 50.74
BioSEM 67.71 / 86.90 / 76.11 47.45 / 52.32 / 49.76 69.11 / 80.49 / 74.37 28.19 / 49.06 / 35.80 42.47 / 62.83 / 50.68
NCBI 72.99 / 72.12 / 72.55 37.54 / 41.81 / 39.56 64.92 / 77.02 / 70.45 24.74 / 55.61 / 34.25 40.53 / 61.72 / 48.93
DlutNLP 69.15 / 80.56 / 74.42 40.84 / 44.16 / 42.43 62.83 / 77.42 / 69.36 26.49 / 43.46 / 32.92 40.81 / 57.00 / 47.56
HDS4NLP 75.27 / 83.27 / 79.07 41.74 / 33.74 / 37.32 70.68 / 75.84 / 73.17 16.67 / 30.86 / 21.64 37.11 / 51.19 / 43.03
NICTANLM 73.59 / 57.67 / 64.66 32.13 / 31.10 / 31.61 42.41 / 72.97 / 53.64 21.60 / 47.14 / 29.63 36.99 / 50.68 / 42.77
USheff 54.50 / 80.07 / 64.86 31.53 / 46.88 / 37.70 39.79 / 92.68 / 55.68 21.14 / 52.69 / 30.18 31.69 / 63.28 / 42.23
UZH 60.26 / 77.47 / 67.79 22.22 / 28.03 / 24.79 62.30 / 70.83 / 66.30 11.06 / 31.02 / 16.31 27.57 / 51.33 / 35.87
HCMUS 67.47 / 60.24 / 63.65 38.74 / 26.99 / 31.81 64.92 / 57.67 / 61.08 19.60 / 19.93 / 19.76 36.23 / 33.80 / 34.98
Table 6: Evaluation results (recall / precision / f-score) of Task 1. Some notable figures are emphasized
in bold.
is kept maintained. Unfortunately, the corefer-
ence annotation, which has been integrated in the
event annotation in the data sets, was not exploited
by the participants, during the official shared task
period. An analysis shows that the performance
of systems significantly drops in the Methods and
Captions sections, suggesting for an extensive ex-
amination in the sections.
As usual, after the official shared task period,
the GE task is maintaining an online evaluation
that can be freely accessed by anyone but with
a time limitation; once in 24 hours per a per-
son. With a few new features that are introduced
in 2013 editions but are not fully exploited by
the participants, the organizers solicit participants
to continuously explore the task using the online
evaluation. The organizers are also planning to
provide more resources to the participants, based
on the understanding that interactive communica-
tion between organizers and participants is impor-
tant for progress of the participating systems and
also the task itself.
References
Amos Bairoch, Rolf Apweiler, Cathy H. Wu,
Winona C. Barker, Brigitte Boeckmann, Serenella
Ferro, Elisabeth Gasteiger, Hongzhan Huang, Ro-
drigo Lopez, Michele Magrane, Maria J. Mar-
tin, Darren A. Natale, Claire O?Donovan, Nicole
Redaschi, and Lai-Su L. Yeh. 2005. The universal
protein resource (uniprot). Nucleic Acids Research,
33(suppl 1):D154?D159.
Jari Bjo?rne and Tapio Salakoski. 2011. Generaliz-
ing biomedical event extraction. In Proceedings of
BioNLP Shared Task 2011 Workshop, pages 183?
191, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Samuel Kerrien, Bruno Aranda, Lionel Breuza, Alan
Bridge, Fiona Broackes-Carter, Carol Chen, Mar-
garet Duesbury, Marine Dumousseau, Marc Feuer-
mann, Ursula Hinz, Christine Jandrasits, Rafael C.
Jimenez, Jyoti Khadake, Usha Mahadevan, Patrick
Masson, Ivo Pedruzzi, Eric Pfeiffenberger, Pablo
Porras, Arathi Raghunath, Bernd Roechert, Sandra
Orchard, and Henning Hermjakob. 2012. The in-
tact molecular interaction database in 2012. Nucleic
Acids Research, 40(D1):D841?D846.
Jin-Dong Kim, Tomoko Ohta, Kanae Oda, and Jun?ichi
Tsujii. 2008a. From text to pathway: corpus annota-
tion for knowledge acquisition from biomedical lit-
erature. In Proceedings of the 6th Asia Pacific Bioin-
formatics Conference, Series on Advances in Bioin-
13
formatics and Computational Biology, pages 165?
176. Imperial College Press.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008b. Corpus annotation for mining biomedical
events from lterature. BMC Bioinformatics, 9(1):10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing
in Biomedicine (BioNLP) NAACL 2009 Workshop,
pages 1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association
for Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia
Event task in BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
David McClosky, Mihai Surdeanu, and Christopher
Manning. 2011. Event extraction as dependency
parsing. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies, pages 1626?
1635, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Kanae Oda, Jin-Dong Kim, Tomoko Ohta, Daisuke
Okanohara, Takuya Matsuzaki, Yuka Tateisi, and
Jun?ichi Tsujii. 2008. New challenges for text min-
ing: Mapping between text and manually curated
pathways.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsu-
jii. 2011. Overview of the Epigenetics and Post-
translational Modifications (EPI) task of BioNLP
Shared Task 2011. In Proceedings of the BioNLP
2011 Workshop Companion Volume for Shared Task,
Portland, Oregon, June. Association for Computa-
tional Linguistics.
Sofie Van Landeghem, Filip Ginter, Yves Van de Peer,
and Tapio Salakoski. 2011. Evex: A pubmed-scale
resource for homology-based generalization of text
mining predictions. In Proceedings of BioNLP 2011
Workshop, pages 28?37, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Karin Verspoor, Kevin Cohen, Arrick Lanfranchi,
Colin Warner, Helen Johnson, Christophe Roeder,
Jinho Choi, Christopher Funk, Yuriy Malenkiy,
Miriam Eckert, Nianwen Xue, William Baumgart-
ner, Michael Bada, Martha Palmer, and Lawrence
Hunter. 2012. A corpus of full-text journal articles
is a robust evaluation tool for revealing differences
in performance of biomedical natural language pro-
cessing tools. BMC Bioinformatics, 13(1):207.
14
Team Simple Event Binding Prot-Mod. Regulation All
EVEX 91.67 / 88.00 / 89.80 55.56 / 62.50 / 58.82 85.71 / 75.00 / 80.00 51.18 / 59.09 / 54.85 62.83 / 68.18 / 65.40
TEES-2.1 91.67 / 88.00 / 89.80 55.56 / 62.50 / 58.82 85.71 / 75.00 / 80.00 51.18 / 57.02 / 53.94 62.83 / 66.67 / 64.69
NCBI 81.25 / 79.59 / 80.41 55.56 / 45.45 / 50.00 85.71 / 66.67 / 75.00 37.01 / 67.14 / 47.72 50.79 / 69.78 / 58.79
BioSEM 83.33 / 88.89 / 86.02 66.67 / 66.67 / 66.67 85.71 / 75.00 / 80.00 35.43 / 54.22 / 42.86 50.79 / 66.90 / 57.74
DlutNLP 87.50 / 93.33 / 90.32 44.44 / 50.00 / 47.06 85.71 / 85.71 / 85.71 37.01 / 51.09 / 42.92 51.83 / 65.13 / 57.73
USheff 81.25 / 88.64 / 84.78 44.44 / 57.14 / 50.00 71.43 / 71.43 / 71.43 29.13 / 56.06 / 38.34 44.50 / 68.55 / 53.97
NICTANLM 93.75 / 57.69 / 71.43 22.22 / 25.00 / 23.53 42.86 /100.00 / 60.00 29.92 / 49.35 / 37.25 46.07 / 53.01 / 49.30
HDS4NLP 93.75 / 90.00 / 91.84 66.67 / 54.55 / 60.00 85.71 / 85.71 / 85.71 19.69 / 31.65 / 24.27 42.93 / 55.78 / 48.52
HCMUS 93.75 / 69.23 / 79.65 33.33 / 27.27 / 30.00 71.43 / 41.67 / 52.63 27.56 / 25.36 / 26.42 46.07 / 38.94 / 42.21
UZH 72.92 / 79.55 / 76.09 44.44 / 57.14 / 50.00 71.43 / 71.43 / 71.43 11.02 / 32.56 / 16.47 30.37 / 57.43 / 39.73
Table 7: Evaluation results (recall / precision / f-score) of Task 1 in titles and abstracts. Some notable
figures are emphasized in bold.
Team Simple Event Binding Prot-Mod. Regulation All
BioSEM 70.83 / 90.44 / 79.44 48.24 / 53.93 / 50.93 74.17 / 82.41 / 78.07 28.74 / 51.25 / 36.83 42.97 / 64.90 / 51.70
EVEX 73.51 / 83.26 / 78.08 43.72 / 47.80 / 45.67 66.67 / 66.12 / 66.39 32.79 / 46.79 / 38.56 45.29 / 58.05 / 50.88
TEES-2.1 74.09 / 83.37 / 78.46 43.72 / 47.80 / 45.67 66.67 / 65.04 / 65.84 33.24 / 44.48 / 38.04 45.70 / 56.34 / 50.46
NCBI 74.28 / 75.59 / 74.93 38.19 / 45.24 / 41.42 67.50 / 81.82 / 73.97 24.69 / 55.46 / 34.17 40.01 / 63.56 / 49.11
DlutNLP 70.06 / 84.49 / 76.60 39.20 / 44.32 / 41.60 67.50 / 74.31 / 70.74 27.78 / 43.23 / 33.83 41.01 / 56.70 / 47.60
NICTANLM 75.24 / 57.14 / 64.95 35.68 / 41.76 / 38.48 52.50 / 76.83 / 62.38 22.33 / 46.83 / 30.24 37.73 / 52.30 / 43.84
USheff 56.81 / 80.43 / 66.59 32.66 / 48.15 / 38.92 45.00 / 94.74 / 61.02 21.67 / 53.55 / 30.85 32.27 / 63.93 / 42.89
HDS4NLP 76.20 / 84.65 / 80.20 41.21 / 38.14 / 39.61 75.83 / 75.21 / 75.52 16.58 / 30.16 / 21.40 36.19 / 51.26 / 42.42
UZH 63.53 / 78.25 / 70.13 23.12 / 28.75 / 25.63 66.67 / 74.07 / 70.18 10.61 / 29.39 / 15.59 27.36 / 50.89 / 35.58
HCMUS 67.18 / 62.84 / 64.94 38.19 / 28.15 / 32.41 67.50 / 61.83 / 64.54 19.45 / 20.11 / 19.78 35.09 / 33.95 / 34.51
Table 8: Evaluation results (recall / precision / f-score) of Task 1 in Methods section group. Some notable
figures are emphasized in bold.
Team Simple Event Binding Prot-Mod. Regulation All
TEES-2.1 76.67 / 67.65 / 71.88 53.19 / 46.30 / 49.50 60.61 / 76.92 / 67.80 22.68 / 39.29 / 28.76 43.41/53.74 / 48.02
BioSEM 60.00 / 78.26 / 67.92 68.09 / 58.18 / 62.75 69.70 / 82.14 / 75.41 23.20 / 34.35 / 27.69 42.31/54.42 / 47.60
EVEX 76.67 / 67.65 / 71.88 53.19 / 46.30 / 49.50 48.48 / 72.73 / 58.18 21.13 / 39.81 / 27.61 41.48/53.74 / 46.82
DlutNLP 70.00 / 67.02 / 68.48 55.32 / 48.15 / 51.49 57.58 / 79.17 / 66.67 18.04 / 46.67 / 26.02 39.29/57.89 / 46.81
NCBI 80.00 / 58.54 / 67.61 40.43 / 41.30 / 40.86 66.67 / 70.97 / 68.75 14.95 / 44.62 / 22.39 39.01/53.58 / 45.15
HDS4NLP 78.89 / 78.02 / 78.45 48.94 / 29.49 / 36.80 66.67 / 68.75 / 67.69 06.19 / 14.63 / 08.70 35.16/45.23 / 39.57
UZH 57.78 / 68.42 / 62.65 23.40 / 26.19 / 24.72 69.70 / 74.19 / 71.88 12.89 / 43.10 / 19.84 30.49/53.62 / 38.88
USheff 47.78 / 74.14 / 58.11 36.17 / 45.95 / 40.48 30.30 /100.00 / 46.51 13.40 / 45.61 / 20.72 26.37/59.26 / 36.50
NICTANLM 75.56 / 53.12 / 62.39 40.43 / 27.94 / 33.04 18.18 / 54.55 / 27.27 11.34 / 36.67 / 17.32 31.59/43.07 / 36.45
HCMUS 73.33 / 52.80 / 61.40 53.19 / 25.51 / 34.48 63.64 / 53.85 / 58.33 15.46 / 17.96 / 16.62 39.01/33.10 / 35.81
Table 9: Evaluation results (recall / precision / f-score) of Task 1 in Captions section group. Some notable
figures are emphasized in bold.
Team Site-Binding Site-Phosphorylation Loc-Localization Total
TEES-2.1 31.37 / 56.14 / 40.25 37.21 / 82.05 / 51.20 36.67 / 78.57 / 50.00 22.03 / 61.90 / 32.50
EVEX 31.37 / 56.14 / 40.25 32.56 / 80.00 / 46.28 36.67 / 78.57 / 50.00 20.90 / 61.67 / 31.22
Table 10: Evaluation results (recall / precision / f-score) of Task 2
Team Negation Speculation Total
TEES-2.1 21.68 / 36.84 / 27.30 18.46 / 33.96 / 23.92 19.53 / 35.59 / 25.22
EVEX 20.98 / 38.03 / 27.04 18.46 / 32.73 / 23.61 19.82 / 34.41 / 25.15
NICTANLM 15.38 / 32.76 / 20.94 14.36 / 34.15 / 20.22 14.79 / 33.57 / 20.54
Table 11: Evaluation results (recall / precision / f-score) of Task 3
15
