Automatic Thesaurus Generation through Multiple Filtering 
Kyo KAGEURA t, Keita TSUJI*, and Akiko, N. AIZAWA * 
? National Inst itute of hffonnatics 
2-1.-2 Hitotsubashi, Chiyoda-ku, Tokyo, 101-8430 Japan 
E-Mail: {kyo,akiko} @nii.ac.jp 
tGraduate School of Education, University of Tokyo, 
7-3-1 Hongo, Bunkyo-ku, Tokyo, 113 Japan 
E-Mail: i34188{hn-unix.cc.u-tokyo.ac.jp 
Abstract; 
11, this paper, we propose a method of gen- 
(',rating bilingual keyword eh.lsters or thesauri 
from parallel or comi.m, able bilingual corpora. 
The method combines nmrphological nd lex- 
ical processing, bilingual word aligmnent, and 
graph-theoretic cluster generation. An experi- 
ment shows that the method is promising. 
1 In t roduct ion  
In this paper, we propose a method of auto- 
matte bilingual thesaurus generation by a com- 
bination of methods or multiple tiltering. The 
procedure consists of three modules: (i) a mor- 
phological and lexical processing module, (it) a 
translation pair extraction module, and (iii) a 
cluster generation module. The method takes 
parallel or comparable corpora as input, and 
produces as outlmt bilingual keyword clusters 
with a reasonable computational cost. 
Our aim is to construct domain-oriented 
bilingual thesauri, which are much in need both 
for cross-language IR and tbr technical tr~msla- 
tors. We assume domain-dependent parallel or 
comparM)le corpora as a source of inibrmation, 
which are. abundant in case of Japanese and En- 
glish. 
The techniques used in each module are 
reasonably well developed, including statistical 
word alignment methods. Itowever, there re- 
main at le.ast three problems: (i) ambiguity of 
multiple hNmx combinations ill an aligmnent, 
which cannot be resolved by purely statistical 
methods, (it) syntagmatie unit mismatches, es- 
pecially in such cases as English and Jal)anese, 
and (iii) difficulty ill final cleaning-up 1 .
In this paper, we show that the proper com- 
bination of the above modules can be useful es- 
pecially for resolving the cleaning-up roblem 
and can produce good results in bilingual ellis- 
ter or thesaurus generation. 
2 Method  
The procedure for thesaurus generation con- 
sists of the following three main nlodules. 
(1) Morphological and lexical processing mod- 
ule: keyword milts 2 for English and Japanese 
are extracted separately. 
(2) Translation pair extraction module: statis- 
tical weighting is applie.d to a corpus which has 
been through the morl}hological nd lexical pro- 
cessing module. The ailn of this stage is not to 
determine mdque translation pairs, but to re- 
strict translation candidates to a reasonable ex- 
tent. 
(3) Cleaning and cluster generation module: 
a bilingual keyword graph is constructed on 
the basis of" the pairs extracted at translation 
pair extraction module, and a graph-theoretic 
method is applied to the keyword graph, to gen- 
erate proper keyword clusters by removing er- 
roneous links. 
If we want to obtain a clean lexicon, minor trans- 
lation variations tend to be omitted, while many errors 
would be included if we want to retain minor variations. 
2 The word 'keyword' implies words that are impof  
tant with respect to documents or domains. In this pa- 
per, we use the word for convenience, roughly in the 
81une se~lse as "content-bearing words". If necessary, a
module of keyword or terin weighting (e.g. Fl'antzi $? 
Ananiadou 1995; Nakagawa & Mort 1998) can be incor- 
porated easily. 
397 
2.1 Morphological gz lexical processing 
At this stage, basic lexical units or keyword 
candidates are extracted. We separately extract 
minimum or shortest units and maxinnlm or 
longest complex units as syntagmatic units for 
keyword candidates. So two outputs are pro- 
duccd from this module, i.e. a bilingual key- 
word corpora of minimum units and another of 
maximum units. 
The processing proceeds as follows: 
(a) Morphological analysis 
First, the cortms is morphologically anal- 
ysed and POS-tagged. Currently, JUMAN3.5 
(Kurohashi ~z Nagao 1998) is used for Japanese 
and LT_POS/LT_CHUNK (Mikheev 1996) is 
used for English. 
(bl) Ext raet lon  of min imum units 
Minimum units in English are simply de- 
fined as non-flmctional simple words extracted 
from the output of LT_POS. Minimum mean- 
ingful units in Japanese are defined as: 
C_Pref ix* (C_AdvlC_AdjlN) C Suf f ix*  
where C_ indicates that the unit should consist 
of either Chinese characters or Katakana 3 . 
(b2) Extraction of maximum units 
Maximum complex units for English are the 
units extracted by LT_CHUNK, with some ad- 
hoc modifications. 
Maximum complex units fin' Japanese are 
defined by the following basic pattern, 
^C_Adj * (C_Affix l C_tdv l C_Adj \[ N) + 
where ^ C means that the unit should begin with 
either Chinese character or Katakana. The pat- 
tern remains deliberately coarse, to absorb er- 
rors by JUMAN. Coarse patterns with simple 
character type restrictions produce better re- 
sults than grammatically well-defined syntag- 
matic patterns. A separate stop word list for 
affixes is also prepared together with an excep- 
tional treatment routine, to make the Japanese 
units better corresl)ond to English units 4 . 
After these processes, two corpora, one con- 
sisting of minimum units and the other of max- 
3 In addition, we have made a few ad-hoc rules to 
screen out some consistent errors produced by the mor- 
phological analysers. 
4 For instance, the Japanese suffix 'Ill' is eliminated 
because it corresponds inmost cases to the English word 
'for', which tends to be excluded fi'om chunks made by 
LT_CHUNK. 
imum units, are created. 
Intermediate constituent units are not ex- 
tracted, because their inter-lingnal unit corre- 
spondence is less reliable. Also, many impor- 
tant intermediate units of longer complex units 
appear themselves as an independent complex 
unit in a large domain-specific corpus, and, even 
if they do not, intermediate units can be ex- 
tracted on the basis of minimum and maximum 
translation pairs if necessary. 
2.2 Extraction of translation candidates 
The module for extracting translation can- 
didate pairs consists of statistical weighting and 
postprocessing. These are applied to the data of 
nfinimum units and maximum units separately. 
After that, the two data are merged to make 
input for the cluster generation module. 
(a) Statistical weighting 
Many methods of extracting lexical transla- 
tion pairs have been proposed (Daille, Gaussier 
& Langd 1994; Eijk t993; Fung 1995; Gale ?~ 
Church 1991; Hiemstra 1996; ltull 1998; Ku- 
piec 1993; Melamed 1996; Smadja, McKeown & 
Hatzivmssiloglou 1996). Though it, is ditficult to 
evaluate the performance of existing methods as 
they use ditferent corpora for evaluation 5 , the 
performance does not seem to be radically dif- 
ferent. We adopted log-likelihood ratio (Dan- 
ning 1993), which gave the best pertbrmance 
among crude non-iterative methods in our test 
experiments 6 .
(b) Postproeessing f i lter 
As the output of statistical weighting is sim- 
ply a weighted list of all English and Japanese 
co-occurring pairs, it; is necessary to restrict 
translation candidates o that they can be ef- 
t~ctively used in the graph-theoretic cluster gen- 
eration module. In addition to restricting pos- 
sible translation pairs, it is necessary to deter- 
mine unique translation pairs for hapax legom- 
ena. We use both macro- and micro-filtering 
heuristics to restrict translation candidates. 
'~ A common testbed exists for French-English align- 
ment (Veronis 1996-99) but not for Japanese-English. 
6 At the time of writing this paper, we have finished a 
preliminary comparative xperiments of various meth- 
ods, among which the method proposed by Melamed 
(1996) gave by far the best result. We are thus plan- 
ning to replace this module with the method proposed 
by Melamed (1996). 
398 
Two macro heuristics, applied to the over- 
all list of pairs, are defined, i.e. (i) a proper 
translation should have a statistical score higher 
than the threshold Xs,  and (ii) a keyword 
should have maximal ly Xc translations or Xp  x 
token frequertcy when the frequency is less 
than Xc.  
Micro heuristics uses the information within 
each alignment; we assume that a keyword in 
one language only has one translation within 
an aligninent r . Selecting unique pairs in each 
al ignment is achieved by recursively taking a 
pair with tile highest score within an alignment, 
ead~ time deleting other pairs which have the 
same English or Japanese elements 8 . 
After this process, the data  of nl ininmm 
units and maximum units are merged, which 
constitutes input for the, next stage. 
2.3  Graph- theoret i c  c lus ter  generat ion  
Up to this stage, the cooccurrence inforlna- 
tion used to extract pairs has the depth of only 
one. In order to el iminate erroneous transla- 
tions, we re-organise the extracted pairs into 
graph and use multi-level topological informa- 
tion by applying tile graph-theoret ic method. 
For exi)lanation , let us assume that we obtained 
the data  in Table 1 fi'om the previous module 
~us an input to this module. 
Firstly, the initial ke!jword graph is con- 
structed, where each node represents an English 
or JaI)anese keyword, and a link or edge repre- 
se.nts the pairwise relation of corresponding key- 
words. W(' define the capacit~j or strength of a 
link by the frequency of occurrence of the pair 
in the corpus, i.e.. the nmnber of al ignments 
in which the pair occurs 9 . Figure 1 shows the 
This is not true for longer alignment units such as 
full texts. However, this will apply to parallel titles and 
abstracts which are readily available. Many lexical align- 
ment methods tarting fi'om sentence-levd aligmnents 
assume this or some variations of this. 
Many maximum unit pairs in fact have the same 
score. We used the arithmetic mean of the constituent 
minimum units to resolve aligmnent ambiguity. 
9 The score of likelihood ratio is a possible alternative 
for link eai)acity, but the result of a preliminary experi- 
ment was no better. In addition, after selecting pairs by 
threshold, whether a pair constitutes a proper transla- 
tion or not is not a matter of weight, because threshold 
setting implies that all pairs above that are regarded as 
correct. So we adopt simple frequency as the link ca- 
pacity. Itowever, we notice a lack of atfinity/)etween the 
Japanese keywords English keywords frequency 
- U -- b ~ information retrieval 1 
g- -- q -- b ? keyword 39 
5- --~ .7. b }~..~ information retrieval 1 
5- 4 ~ X b ~,.'~'~ text retrieval 6 
5- 4- X I- ~'~.~ text search 3 
J}~ >1 t~. ~rll/-~ iiii" keyword 1 
~ J."~.l'~ {~.~. "~"~ information retrieval 1 
'\]~i{fi}'~ information gathering 4 
'l'~ N ~  information retreival t 
i'~ ~ '~ information retrieval 320 
'1~ N~'5.'~ information search 5 
t~ ~f{l\[?{ ~JS inibrmation gathering 6 
' \] '~11~ information retrieval 1 
~i~t;~'~ bibliographic search 1 
~i}J~tt~ document retrieval 11 
~ ~: ~,~ "-.4~ document retrieval 19 
~ '~ text retrieval 1 
Table 1. Input exanlple for cluster generation 
iifitial keyword graph made from t, he data in 
Table 1. The task now is to detect and re- 
move erroneous links to generate independent 
graphs or clusters consisting of I)roper transla- 
tion pairs 1? . 
infornlalion galhcrillg O,ihlie,t, llqphic r,'uie~td) 
Figure I. Example of initial keyword graph 
The detect ion algorithm is based on the sim- 
ple principle that sets of links, which decompose 
a connected keyword cluster into disjoint sub- 
clusters when they are removed fronl tile origi- 
nal cluster, are candidates for improt)er transla- 
tions. In graph theory, such a link set is called 
an edge cut and the edge cut with the min- 
imal total  capacity is called a m, inimum edge 
cut. A min imum edge cut does not necessarily 
imply a single translat ion error. An efficient al- 
statistical alignment method we used here and the deft- 
nition of link cat)acity, which is currently under exami- 
nation and will be iml)roved by renewing the alignment 
module. 
m This approach is radically different fl'om statisti- 
cally oriented word clustering (Deerwester t. al 1990; 
Finch 1993; Grefenstette 1994; Schiitze & Pedersen 1997; 
Strzalkowski 1994); this is why we use the word '(:luster 
generation' instead of 'clustering'. 
399 
I~eworU t2~,f%1: kyewo~rd .~)/;/#o;'.','} 
i I ~, , /~  0 toxt search =F - ' / - -  4"~J " "~" z / 
, t , , , , , , ,~{D/  /:r~z, k t~? ~ . . . .  core  cl I~te 
v~.~ I (le'tl relR~i(ID ~Y'v"t 
g '~ l ,~t#t~' . .~ ; J * \ / \  \ x .  . ~',.J.Y~??;t,' 
(Ivld.'-,r?,kJ I ~lnfoll~latlon rotr~lva/ ? ) Id~.~u.lent 
inJorln(ttion . I~.  ~20~. \ .A~ 1 ~;'etrt~:val 
retrievtdl /V xl \~f~l ~\ 19 
r. / i , , ,~.~" , ~u~nl  rotfiova, 
x \  6 ~ifo a h . / , I 
I \ I thihho r.pldcTl~iz't~fl* :':-... "6../. \ ." ,  f :. infon'hzdiongalh'el~ng ~ \ ~/  ~ '\[~ 
~ / bibl~graphic sea r-ch 
'u!  
(a) (b) 
Figure 2. Steps of graph-theoretic cluster generation 
(c) 
gorithm exists for minimum edge cut detection 
(Nagamochi 1993). 
Our procedure first checks links that should 
not be eliminated, using the conditions: (i) the 
frequency is no less than Na, (ii) the Japanese 
and English notations are identical, or (iii) ei- 
ther of the Japanese or English expressions have 
only one corresponding translation (Figure 2 
(a); it is assumed that N~ = N/~ = Ne = 3). 
Secondly, core keywords whose fi'equency is no 
less than NZ are checked (Figure 2 (b)). This is 
used for the restriction that each cluster should 
include at le~t one core keyword. Lastly, edge 
cuts with a total capacity of less than Ne are 
detected and removed (Figure 2 (c)). This pro- 
cedure is repeated recursivety until no fllrther 
application is possible. Figure 3 shows the state 
after these steps are applied. 
. /  
/ 
! 
I .  
" ' "  . . . . . . . . . .  :L . . . . .  ~ . . . . . . . . . . .  2 . . . . . . . . .  ~'- 
Figure 3. Generated clusters 
3 Exper iment  
3.1 Settings and procedures 
We applied the method to Japanese and 
English bilingual parallel corpus consisting of 
25534 title pairs in the field of computer sci- 
ence. Table 2 shows the basic quantitative infor- 
mation after morphological nd lexical filtering 
was applied. 
Mininmm units 
Japanese Token: 178091 Type: 14938 
English Token: 154554 Type: 12634 
Maximum units 
Japanese Token: 89742 Type: 38813 
English Token: 80018 Type: 41693 
Table 2. Basic quantity of the data 
In the pair extraction module, the threshold 
Xs' was set to 1011 . The parameter X c w~s 
set to 10 and Xp to 0.5. As a result, 28905 
translation candidate pairs were obtained, with 
24855 Japanese and 23430 English keywords. 
Of these, 20071 pairs occurred only once and 
3581 only twice. The most frequent pair oc- 
curred 3196 times in the corpus. 8242 (28.5%) 
were minimum unit pairs, and 20663 (71.5%) 
were maximum unit pairs. 
Table 3 shows the number of keywords which 
had N translations. On average, a Japanese 
keyword had 1.16 English translations, while 
an English keyword had 1.23 Japanese trans- 
lations. 
N Jap. Eng. N ,lap. En. 
1 21796 19778 5 62 157 
2 2409 2693 6 10 59 
3 412 437 7 7 17 
4 159 285 8 0 4 
Table 3. Number of translations 
ix This is purely heuristic. Minimum units and maxi- 
mum units are given ditferent scores. But only 3 pairs 
below this threshold were proper translation pairs in 100 
random samples of minimum unit pairs, and 5 in 100 
samples of maxinmn~ units. 
400 
Evaluating recall and precision on the ba- 
sis of 100 randonfly selected title pairs, which 
consisted of 778 keyword token pairs, the pre- 
cision tokenwise was 84.06% (654 correct trans- 
lations) and the recall was 87.08% (654 of 751 
correct pairs). Typewise precision was 81.65% 
(543 correct of 665 pairs). 
The initial keyword graph generated fi'om 
these 28905 translation candidates consisted of 
19527 independent subgraphs, with the largest 
cluster containing 2701 pairs (i.e. 9.3% of all 
the pairs). The cluster generation method was 
applied with parameters Na =: 4, Ne = 10 
and N/~ = 1) 2 . As a result, 893 translation 
pairs were removed, and 20357 bilingual clus- 
ters were generated. The maximum cluster now 
contained only 64 pairs. Table 4 :shows the num- 
ber of clusters by size given by number of pairs. 
size no. of clusters size no. of (:lusters 
1 16693 5-9 322 
2 2354 10-19 52 
3 504 20-64 22 
4 410 
Table 4. Number of chlsters by size 
3.2 Overal l  evaluation 
The result was manually evaluated fi'om two 
points of view, i.e. consistency of clusters and 
cocrectness of link removal ~3 . 
(1) rib check the internal consistency, clusters 
were classified into three groul)S by size, and 
were separately evaluated. 2000 'small' clusters, 
consisting of only one pair, were randomly sam- 
pled and evaluated as 'correct' (c), 'more or less 
correct' (m) or ~wrong' (w). 4t}0 medimn size 
clusters consisting of 2-9 pairs and all the 74 
large clusters consisting of 10 or more pairs were 
evaluated as 'consistent' (c: consisting only of 
closely related keywords), 'mostly consistent' 
(Ill: consisting mostly of related keywords), 'hy- 
brid' (t1: consisting of two or more different key- 
word groups: 11) or q)ad' (w). Table 5 shows 
the result of the evaluation. The general per- 
formance is very good, with more or less 80% of 
the clusters being meaningflfl. 
12 This is again determined heuristically. For an exami- 
nation of the effect of parameters, ee Aizawa & Kageura 
(to apl)ear). 
~3 The evaluation was done by the first author. Cur- 
rently no cross-checking has been carried out. 
For small clusters, the performance was sep- 
arately evaluated for minimuln and maximuln 
refit pairs. Note that the ratio of maximum 
unit pairs is comparatively higher in the small 
cluster than the overall average. Most pairs 
ewfluated as partially correct, as well as some 
wrong pairs, suffered from mismatch of the syn- 
tagmatic units. 
c m w total 
Small 1389 370 241 2000 
(69.5%) (18.5%) (12.1%) (100%) 
milfimum 288 26 69 383 
(75.2%) (6.8%) (18.0%) 19.2% 
maximum 1101 344 172 1617 
(68.1%) (21.3%) (10.6%) 80.9% 
c m h w 
Medium 116 148 32 104 
(29.0%) (37.0%) (8.0%) (26.0%) 
Large 8 18 43 5 
(lo.8%) (24.3%) (58.1%) (&8%) 
Table 5. Evaluation of internal consistency 
73% of tile medium sized clusters were 'cor- 
reel), 'mostly correct' or 'hybrid'. Among the 
'lnostly con'ect' and 'hybrid' clusters, 97 (91 
and 6 respectively) were mainly caused by the 
mismatch of the units. For instance, in the 
case: { Kid, i~iN'fL, ~i~@, optimization, opti- 
mal, optimisation, optimum, network optimiza- 
tion }, the last English keyword has the excess 
unit 'network'. Other 'mostly correct' and 'hy- 
brid' chtsters were due to the l)roblem of corpus 
frequencies. 
Among the large clusters, more than half 
were qlybrid '14 . Among the hnostly correct' 
and qlybrid' large (;lusters, only 8 (3--t-5) were 
due to unit mismatch, while 53 (15+38) were 
due to quantitative factors. This shows a strik- 
ing contrast o the medium sized clusters. Large 
hybrid clusters tended to include lnany common 
word pairs which occur fi'equently. For instance, 
in the largest chlster, ' )  x ? .z, system' (3196), 
'lJ~l~} development' (1097), '~tki~\] design' (1073), 
and 'NiL enviromnent' (890) are included due 
to indirect associations. The tbllowing are two 
examples of hybrid clusters, whose hybridness 
comes fi:om quantitative factors and unit mis- 
matches respectively: 
Example  1: ~fg2:/~.{C6/~tJ/4) -x" ')/overview/outline/ 
summary/smmnarization/overall 
14 And most of the sub-clusters in these hybrid clusters 
are 'mostly correct'. 
401 
/pattern/patterns/patten/patterm matching 
In the first case, the 'overall' group and the 
'summary' group are mixed up. In the sec- 
ond case, the mismatch of syntagmatic units is 
caused by borrowed words. In fact, many errors 
caused by the mismatch of syntagmatic units in- 
volve borrowed words written in Katakana. 
(2) To look at the perfbrmance of graph- 
theoretic cluster generation, we exanfined the 
removed pairs fl'om two points of view, i.e. the 
correctness of link removal and the internal con- 
sistency of clusters generated by link remowfl. 
For the former, we introduced three categories 
for evaluation: mismatched pairs correctly re- 
moved (c), proper translation pairs wrongly re- 
moved (w), and pairs of related meaning re- 
moved (p). The consistency of newly generated 
clusters were evaluated in the same manner as 
above. 
c p w total 
cc 90 (10.1) 53 (5.9) 39 (4.4) 182 (20.4) 
cm 148 (16.6) 56 (6.6) 32 (3.6) 236 (26.4) 
ch 96 (10.8) 20 (2.2) 6 (0.7) 122 (13.7) 
mm 44 (4.9) 29 (3.3) 30 (3.4) 103 (tl.5) 
mh 52 (5.8) lS (1.5) 5 (0.6) 70 (7.8) 
hh 30 (3.4) 3 (0.3) 3 (0.3) 36 (4.0) 
xc 42 (4.7) 9 (1.0) 9 (1.0) 60 (6.7) 
xm 28 (3.t) 8 (0.9) 20 (2.2) 56 (6.3) 
xh 8 (0.9) 2 (0.3) 5 (0.6) 15 (1.7) 
xx 4 (0.5) 1 (0.1) 8 (0.9) 13 (1.5) 
all 542 (60.7) 194 (21.7) 157 (17.6) 893 (100) 
Table 6. Evaluation of removed links 
Table 6 shows the result of evaluation of all 
the 893 removed pairs. 'c' 'p' and 'w' in the top 
row indicate types of removed links, and 'cc', 
'cm' etc. in the leftmost column indicate inter- 
nal consistencies of two clusters generated by 
link removal. A total of 157 (17.6%) of the re- 
moved links were correct links wrongly removed, 
but among them, 115 links did not produce 
'bad' clusters. If we consider them to be toler- 
able, only 42 removals (4.7%) were fatal errors. 
By exanfining the renloved links, wc found 
that the links removed at the higher edge capac- 
ity included more wrongly removed pairs. For 
instance, among 142 edges removed at capacity 
4 (which is the maximum deletable value set by 
N,~), 41 or 28.9% were wrongly removed correct 
translations, while among 288 links removed at 
capacity l, only 15 or 5.2 % were correct trans- 
lations. 
4 Discuss ion 
From the experiment, we have found some 
factors that affect performance. 
(1) Many errors were produced at the stage of 
extracting keyword milts, by syntagmatic mis- 
match. A substantial nmnber of them involved 
Japanese Katakana keywords. Thereibre, in ad- 
dition to the general refinement of the morpho- 
logical processing module, the perfbrmance will 
be improved if we use string proxinfity informa- 
tion to determine syntagmatic units 15 . 
(2) We expect that some errors produced by 
statistical weighting and filtering could be re- 
moved by applying stemming and orthographic 
normalisations, which are not flflly exploited in 
the current implementation. Looking back from 
the cluster generation stage, frequently occur- 
ring keywords tend to cause problems due to 
indirect associations. At the time of writing, we 
are radically changing the statistical alignment 
module based on Melamed (1996) and incorpo- 
rating iterative alignment anchoring routine so 
that the method can be applied not only to titles 
but also to abstracts, etc. Used in conjunction 
with string proximity and stemming inforina- 
tion, we might be able to retain nfinor va.riations 
properly. 
(3) At the cluster generation stage, we observed 
that correct links tend to be wrongly removed 
for higher capacities of edge cut. In the cur- 
rent implementation, the parameter values re- 
main the same for all the clusters. Performance 
will be improved by introducing a method of 
dynamically changing the parameter w-dues ac- 
cording to the cluster size and the frequencies 
of their constituent pairs. 
5 Conclus ion 
We have proposed a method of constructing 
bilingual thesauri automatically, fl'om parallel 
or comparable corpora. The experiment showed 
that the performance is fairly good. We are cur- 
rently improving the method further, along the 
lines discussed in the previous ection. Further 
experiments are currently being carried out, us- 
ing the data of narrower domains (e.g. artificial 
ls This can also be used for resolving hapax ambiguity. 
402 
intelligence) as well as abstracts instead of ti- 
tles. 
At the next stag(.', we are 1)lanning to eval- 
uate the method fi'om the point of view of per- 
formance of generated clusters in practical ap- 
plications. We are currently planning to apply 
the generated clusters to query expansion and 
user navigation in cross-lingual Il ., as well as to 
on-line dictionary lookup systems used as trans- 
lation aids. 
Acknowledgement  
This research is a part of the research 
project "A Study on Ubiquitous Information 
Systems tbr Utilization of Highly Distributed 
Information FLesources", fimded by the Japan 
Society for the Promotion of Science. 
Re ferences  
\[1\] Aizawa, A. N. and Kageura, K. (to appear) "A 
grai)h-/)ased al)proach to the autoinatic gen- 
eration of multilingual keyword clusters." In: 
Bouligmflt, D., Jacquemin, C. and l'tIomme, 
M-C. (eds.) Recent Advances in Computational 
7~rminology. Amsterdam: John Benjanfins. 
\[2\] Dagan, I. and Church, K. (1994) "Termight: 
Identifying and translating technical terminol- 
ogy. " Prec. of the Fourth ANLP. p.34 40. 
\[3\] Daille, B., Gaussier, E. and Langd, J. M. (t994) 
"Towards automatic extraction of monolingual 
and bilingual terminology." COLING'9~. p. 
515-.521. 
\[4\] Deerwester, S., Dumais, S. T., Furnas, G. W., 
Landauer, T. K. and Harshman, R. (1990) "In- 
dexing by latent semantic analysis." JASIS. 
41(6), p. 391 407. 
\[5\] Dunning, T. (1993) "Accurate reel,hods for the 
statistics of surprise and coincidence." Compu- 
tational Lin.quistics. 19(1), p. 61 74. 
\[6\] Eijk, van der P. (1993) "Automating the acqui- 
sition of bilingual terminology." Prec. of the 6th 
EACL. p. 11.3-119. 
\[7\] Finch, S. P. (1993) Finding Structure in Lan- 
9ua.qe. PhD Thesis. Edinbourgh: University of 
Edinbourgh. 
\[8\] Frantzi, K. T. and Ananiadou, S. (1995) "Sta- 
tistical measures for terminological extraction." 
Proc. of 3rd Int'l Conf. on Statistical Analysis 
of Textual Data. p. 297-308. 
\[9\] Fung, P. (t995) "A t)attcrn matching method 
for finding noun and proper noun translations 
fi'om noisy parallel cort)ora.." Proe. of 33rd 
A CL. p. 233 236. 
\[10\] Gale, W. A. and Church, K. W. (1991.) "Idem 
tifying word correspondences in parallel texts." 
Proc. of DARPA &~eech. and Natural Lan.quwe 
Workshop. p. 152-157. 
\[11\] Grefenstette, G. (1994) Explorations in Auto- 
matic Thesaurus Discovery. Boston: Kluwer 
Academic. 
\[12\] tfiemstra, D. (1996) Using Statistical Methods 
to Creat a Bilingual Dictionary. MSc Thesis, 
Twcnte University. 
\[13\] Ifull, D. A. (1998) "A practical approach to ter- 
minology aligmnent." Computerm'98. p. 1---7. 
\[14\] Kitamura, M. and Matsumoto, Y. (1997) "Au- 
tomatic Extraction of Translation Patterns in 
Parallel Corpora." Transactions of IPSJ. 38(4), 
p. 727- 735. 
\[15\] Kupiec, J. (1993) "An algorithm for finding 
noun phrase correspondences in bilingual col  
pora." 15"oc. of 31st ACL. p.17--22. 
\[16\] Kurohashi, S. and Nagao, M. (1998) Japanese 
Morphological Analysis System .luman versioT~ 
3.5 User's Mawaal. Kyoto: Kyoto University." 
\[17\] Melamed, I. D. (1996) "Automatic onstruction 
of clean broad-coverage translation lexicons." 
2nd Conference of the Association for Mach, ine 
Translation in the Americas. p. 125-134. 
\[18\] Mikheev, A. (1996) '%earning pro:t-of-speech 
guessing rules from lexicon." COLING'96, p. 
770-775. 
\[19\] Nagamochi, H. (1993) "Minimum cut, in a 
graph." In: Fujisige, S. (ed.) Discrete Struc- 
ture and Algorithms H (Chapter 4). Tokyo: 
Kindaikagakusha. 
\[20\] Nal~Gawa , H. and Mori, T. (1998) "Nested col 
location and COml)ound noun for term extrac- 
tion." Computerm'98. p 64 70. 
\[21\] Sch{itze, It. and Pedersen, J.O. (1997) "A 
cooccurrence-based thesaurus and two appli- 
cations to information retrieval." Information 
Processing and Management. 33(3), I).307-318. 
\[22\] Smadja, F., MeKeown, K. R. and Hatzivas- 
siloglou, V. (1996) "Translating collocations 
for bilingual exicons: A statistical apt)roach." 
Computational Linguistics. 22(1), p. \]-38. 
\[23\] Strzalkowski, T. (1994) "Building a lexicM do- 
main map from text corpora." COLING'94, 
t).604-610. 
\[24\] Veronis, J. (1996-) "ARCADE: Evaluation of 
parallel text alignment systems." 
ht tl)://www.lpl.univ-aix.fi'/projects/arcade/ 
\[25\] Yonezawa, K. and Matsumoto, Y. (1998) 
"Zoshinteki taiouzuke ni yoru taiyaku tekisuto 
lmra no hol?yaku hyougen o cyusyutu." Proe 
of the \]#h, Annual Meeting of th.e Association 
for NLP. p. 576-579. 
403 
A Method of Cluster-Based Indexing of Textual Data
Akiko Aizawa
National Institute of Informatics
akiko@nii.ac.jp
Abstract
This paper presents a framework for cluster-
ing in text-based information retrieval systems.
The prominent feature of the proposed method
is that documents, terms, and other related el-
ements of textual information are clustered si-
multaneously into small overlapping clusters. In
the paper, the mathematical formulation and
implementation of the clustering method are
briefly introduced, together with some experi-
mental results.
1 Introduction
This paper is an attempt to provide a view of
indexing as a process of generating many small
clusters overlapping with each other. Individ-
ual clusters, referred to as micro-clusters in this
paper, contain multiple subsets of associated el-
ements, such as documents, terms, authors, key-
words, and other related attribute sets. For ex-
ample, a cluster in Figure 1 represents ?a set of
documents written by a specific community of
authors related to a subject represented by a set
of terms?.
Our motivations for considering such clusters
are that (i) the universal properties of text-
based information spaces, namely large scale,
sparseness, and local redundancy (Joachims,
2001), may be better manipulated by focusing
on only limited sub-regions of the space; and
also that (ii) the multiple viewpoints of infor-
mation contents, which a conventional retrieval
system provides, can be better utilized by con-
sidering not only the relations between ?doc-
uments? and ?terms? but also associations be-
tween other attributes such as ?authors? within
the same unified framework.
Based on the background, this paper presents
a framework of micro-clustering, within which
we adopt a probabilistic formulation of co-
ST : a subset of terms
SD: a subset of documents
SA: a subset 
of authors
A ?cluster? 
that represents
associations between
ST, SA, and SD.
Term space
Document space
Author space
Figure 1: Cluster-based indexing of information
spaces.
occurrences of textual elements. For simplic-
ity, we focus primarily on the co-occurrences
between ?documents? and ?terms? in our expla-
nation, but the presented framework is directly
applicable to more general cases with more than
two attributes.
2 Background Issues
A view from indexing
In information retrieval research, matrix
transformation-based indexing methods such as
Latent Semantic Indexing (LSI) (Deerwester
et al, 1990) have recently become quite com-
mon. These methods can be viewed as an es-
tablished basis for exposing hidden associations
between documents and terms. However, their
objective is to generate a compact representa-
tion of the original information space, and it is
likely in consequence that the resulting orthog-
onal vectors are dense with many non-zero ele-
ments (Dhillon and Modha, 1999). In addition,
because the reduction process is globally op-
timized, matrix transformation-based methods
become computationally infeasible when deal-
ing with high-dimensional data.
A view from clustering
The document-clustering problem has also
been extensively studied in the past (Iwayama
and Tokunaga, 1995; Steinbach et al, 2000).
The majority of the previous approaches to clus-
tering construct either a partition or a hierarchy
of target documents, where the generated clus-
ters are either exclusive or nested. However,
generating mutually exclusive or tree-structured
clusters in general is a hard-constrained prob-
lem and thus is likely to suffer high computa-
tional costs when dealing with large-scale data.
Also, such a constraint is not necessarily re-
quired in actual applications, because ?topics?
of documents, or rather ?indices? in our context,
are arbitrarily overlapped in nature (Zamir and
Etzioni, 1998).
Basic Strategy:
Based on the above observations, our basic
strategy is as follows:
? Instead of generating component vectors with
many non-zero elements, produce only limited
subsets of elements, i.e., micro-clusters, with
significance weights.
? Instead of transforming the entire co-
occurrence matrix into a different feature
space, extract tightly associated sub-structures
of the elements on the graphical representation
of the matrix.
?Use entropy-based criteria for cluster evalua-
tion so that the sizes of the generated clusters
can be determined independently of other ex-
isting clusters.
?Allow the generated clusters to overlap with
each other. By assuming that each element
can be categorized into multiple clusters, we
can reduce the problem to a feasible level where
the clusters are processed individually.
Related studies:
Another important aspect of the proposed
micro-clustering scheme is that the method em-
ploys simultaneous clustering of its composing
elements. This not only enables us to com-
bine issues in term indexing and document clus-
tering, as mentioned above, but also is useful
for connecting matrix-based and graph-based
notions of clustering; the latter is based on
the association networks of the elements ex-
tracted from the original co-occurrence matri-
ces. Some recent topics dealing with this sort
of duality and/or graphical views include: the
Information Bottleneck Method (Slonim and
Tishby, 2000), Conceptual Indexing (Dhillon
and Modha, 1999; Karypis and Han, 2000), and
Bipartite Spectral Graph Partitioning (Dhillon,
2001), although each of these follows its own
mathematical formulation.
3 The Clustering Method
3.1 Definition of Micro-Clusters
Let D = {d
1
, ? ? ? , dN} be a collection of N tar-
get documents, and let SD be a subset of doc-
uments such that SD ? D. Likewise, let T =
{t
1
, ? ? ? , tM} be a set of M distinct terms that
appear in the target document collection, and
let ST be a subset of terms such that ST ? T .
A cluster, denoted as c, is defined as a combi-
nation of ST and SD:
c = (ST , SD). (1)
The co-occurrences of terms and documents can
be expressed as a matrix of size M ?N in which
the (i, j)-th cell indicates that ti (? T ) appears
in dj (? D). We make the value of the (i, j)-th
cell equal to freq(ti, dj). Although we primarily
assume the value is either ?1? (exist) or ?0? (not
exist) in this paper, our formulation could eas-
ily be extended to the cases where freq(ti, dj)
represents the actual number of times that ti
appears in dj .
The observed total frequency of ti over all the
documents in D is denoted as freq(ti,D). Simi-
larly, the observed total frequency of dj , i.e. the
total number of terms contained in dj , is de-
noted as freq(T, dj). These values correspond
to summations of the columns and the rows of
the co-occurrence matrix. The total frequency
of all the documents is denoted as freq(T,D).
Thus,
freq(T,D) =
?
ti?T
freq(ti ,D) =
?
dj?D
freq(T, dj )
=
?
ti?T
?
dj?D
freq(ti , dj). (2)
We sometimes use freq(ti) for freq(ti,D),
freq(dj) for freq(T, dj) and F for freq(T,D).
D
oc
u
m
en
ts
 
Terms 
A representation 
of a cluster that 
is composed of 
subsets of 
documents and
terms
SD
ST
c(ST,SD)
freq(ti, dj)=1 freq(ST, SD)=7
0
0
1 1
1
1
1
1
|ST|=3
|
S
D
|
=
3
D
oc
u
m
en
ts
 
Terms
7
1
Before agglomeration After agglomeration
|
S
D
|
=
3
|ST|=3
ST
SD
ST
SD
Figure 2: Example of a cluster defined on a co-occurrence matrix.
When a cluster c is being considered, T and D
in the above definitions are changed to ST and
SD. In this case, freq(ti, SD) and freq(ST , dj)
represent the frequencies of ti and dj within
c = (ST , SD), respectively. In the co-occurrence
matrix, a cluster is expressed as a ?rectangular?
region if terms and documents are so permuted
(Figure 2).
3.2 Probabilistic Formulation
The view of the co-occurrence matrix can be
further extended by assigning probabilities to
each cell. With the probabilistic formulation,
ti and dj are considered as independently ob-
served events, and their combination as a sin-
gle co-occurrence event (ti, dj). Then, a cluster
c = (ST , SD) is also considered as a single co-
occurrence event of observing one of ti ? ST
within one of dj ? SD.
In estimating the probability of each event,
we use a simple discounting method similar to
the absolute discounting in probabilistic lan-
guage modeling studies (Baayen, 2001). The
method subtracts a constant value ?, called a
discounting coefficient, from all the observed
term frequencies and estimates the probability
of ti as:
P (ti) =
freq(ti) ? ?
F
. (3)
Note that the discounting effect is stronger for
low-frequency terms. For high-frequency terms,
P (ti) ? freq(ti)/F . In the original definition,
the value of ? was uniquely determined, for ex-
ample as ? = m(1)M with m(1) being the number
of terms that appear exactly once in the text.
However, we experimentally vary the value of ?
in our study, because it is an essential factor for
controlling the size and quality of the generated
clusters.
Assuming that the probabilities assigned to
documents are not affected by the discounting,
P (dj |ti) = freq(ti, dj) / freq(ti). Then, apply-
ing P (ti, dj) = P (dj |ti)P (ti), the co-occurrence
probability of ti and dj is given as:
P (ti, dj) =
freq(ti) ? ?
freq(ti)
? freq(ti , dj)
F
. (4)
Similarly, the co-occurence probability of ST
and SD is given as:
P (ST , SD) =
freq(ST ) ? ?
freq(ST )
? freq(ST , SD)
F
. (5)
3.3 Criteria for Cluster Evaluation
The evaluation is based on the information the-
oretic view of the retrieval systems (Aizawa,
2000). Let T and D be two random vari-
ables corresponding to the events of observ-
ing a term and a document, respectively. De-
note their occurrence probabilities as P (T ) and
P (D), and their co-occurrence probability as a
joint distribution P (T ,D). By the general defi-
nition of traditional information theory, the mu-
tual information between T and D, denoted as
I(T ,D), is calculated as:
I(T ,D) =
?
ti?T
?
dj?D
P (ti, dj)log
P (ti, dj)
P (ti)P (dj)
, (6)
where the values of P (ti, dj) and P (ti) are cal-
culated using Eqs. (3) and (4). P (dj) is deter-
mined by P (dj) =
?
ti?T P (ti, dj), or approx-
imated simply by P (dj) = freq(dj)/F . Next,
the mutual information after agglomerating ST
and SD into a single cluster (Figure 2) is calcu-
lated as:
I ?(T ,D) =
?
ti /?ST
?
dj /?SD
P (ti, dj)log
P (ti, dj)
P (ti)P (dj)
+P (ST , SD)log
P (ST , SD)
P (ST )P (SD)
, (7)
where P (ST ) =
?
ti?ST P (ti) and P (SD) =
?
dj?SD P (dj).
The fitness of a cluster, denoted as
?I(ST , SD), is defined as the difference of the
two information values given by Eqs.(6) and (7):
?I(ST , SD) = I ?(T ,D) ? I(T ,D)
= P (ST , SD)log
P (ST , SD)
P (ST )P (SD)
?
?
ti?ST
?
dj?SD
P (ti, dj)log
P (ti, dj)
P (ti)P (dj)
. (8)
Without discounting, the value of ?I(ST , SD) in
the above equation is always negative or zero.
However, with discounting, the value becomes
positive for uniformly dense clusters, because
the frequencies of individual cells are always
smaller than their agglomeration and so the dis-
counting effect is stronger for the former.
Using the same formula, we calculated the
significance weights ti in c = (ST , SD) as:
?I(ti, SD) =
?
dj?SD
P (ti, dj)log
P (ti, dj)
P (ti)P (dj)
, (9)
and the significance weights of dj as:
?I(ST , dj) =
?
ti?ST
P (ti, dj)log
P (ti, dj)
P (ti)P (dj)
. (10)
In other words, all the terms and documents in a
cluster can be jointly ordered according to their
contribution in the entropy calculation given by
Eq. (7).
To summarize, the proposed probabilistic
formulation has the following two major fea-
tures. First, clustering is generally defined as
an operation of agglomerating a group of cells
in the contingency table. Such an interpreta-
tion is unique because existing probabilistic ap-
proaches, including those with a duality view,
agglomerate entire rows or columns of the con-
tingency table all at once. Second, the estima-
tion of the occurrence probability is not simply
in proportion to the observed frequency. The
discounting scheme enables us to trade off (i)
the loss of averaging probabilities in the ag-
glomerated clusters, and (ii) the improvement
of probability estimations by using larger sam-
ples sizes after agglomeration.
It should be noted that although we have re-
stricted our focus to one-to-one correspondences
between terms and documents, the proposed
framework can be directly applicable to more
general cases with k(? 2) attributes. Namely,
given k random variables X
1
, ? ? ? ,Xk, Eq. (8)
can be extended as:
?I(SX
1
, ? ? ? , SXk)
= P (SX
1
, ? ? ? , SXk )log
P (SX
1
, ? ? ? , SXk )
P (SX
1
) ? ? ?P (SXk )
(11)
?
?
x
1
?SX
1
? ? ?
?
xk?SXk
P (x
1
, ? ? ? , xk)log
P (x
1
, ? ? ? , xk)
P (x
1
) ? ? ?P (xk)
.
3.4 Cluster Generation Procedure
The cluster generation process is defined as the
repeated iterations of cluster initiation and clus-
ter improvement steps (Aizawa, 2002).
First, in the cluster initiation step, a single
term ti is selected, and an initial cluster is then
formulated by collecting documents that con-
tain ti and terms that co-occur with ti within
the same document. The collected subsets,
respectively, become SD and ST of the initi-
ated cluster. On the bipartite graph of terms
and documents (Figure 2), the process can be
viewed as a two-step expansion starting from ti.
Next, in the cluster improvement step, all the
terms and documents in the initial cluster are
tested for elimination in the order of increas-
ing significance weights given by Eqs. (9) and
(10). If the performance of the target cluster is
improved after the elimination, then the corre-
sponding term or document is removed. When
finished with all the terms and documents in the
cluster, the newly generated cluster is tested to
see whether the evaluation value given by Eq.
(8) is positive. Clusters that do not satisfy this
condition are discarded. Note that the resulting
cluster is only locally optimized, as the improve-
ment depends on the order of examining terms
and documents for elimination.
At the initiation step, instead of randomly
selecting an initiating term, our current im-
plementation enumerates all the existing terms
ti ? T . We also limit the sizes of ST and SD
to kmax = 50 to avoid explosive computation
caused by high frequency terms. Except for
kmax, the discounting coefficient ? is the only
parameter that controls the sizes of the gener-
ated clusters. The effect of ? is examined in
detail in the following experiments.
4 Experimental Results
4.1 The Data Set
In our experiments, we used NTCIR-J11, a
Japanese text collection for retrieval tasks that
is composed of abstracts of conference papers
organized by Japanese academic societies. In
preparing the data for the experiments, we first
selected 52,867 papers from five different so-
cieties: 23,105 from the Society of Polymer
Science, Japan (SPSJ), 20,482 from the Japan
Society of Civil Engineers (JSCE), 4,832 from
the Japan Society for Precision Engineering
(JSPE), 2,434 from the Ecological Society of
Japan (ESJ), and 2,014 from the Japanese So-
ciety for Artificial Intelligence (JSAI).
The papers were then analyzed by the mor-
phological analyzer ChaSen Ver.2.02 (Mat-
sumoto et al, 1999) to extract nouns and com-
pound nouns using the Part-Of-Speech tags.
Next, the co-occurrence frequencies between
documents and terms were collected. After pre-
processing, the number of distinctive terms was
772,852 for the 52,867 documents.
4.2 Clustering Results
In our first experiments, we used a framework
of unsupervised text categorization, where the
quality of the generated clusters was evaluated
1
http://research.nii.ac.jp/ntcir/
by the goodness of the separation between dif-
ferent societies. To investigate the effect of the
discounting parameter, it was given the values
? = 0.1,0.3,0.5,0.7, 0.9, 0.95.
Table 1 compares the total number of gener-
ated clusters (c), the average number of docu-
ments per cluster (sd), and the average number
of terms per cluster (st), for different values of
?. We also examined the ratio of unique clus-
ters that consist only of documents from a sin-
gle society (rs), and an inside-cluster ratio that
is defined as the average relative weight of the
dominant society for each cluster (ri). Here, the
weight of each society within a cluster was cal-
culated as the sum of the significance weights of
its component documents given by Eq. (10).
The results shown in Table 1 indicate that re-
ducing the value of ? improves the quality of the
generated clusters: with smaller ?, the single so-
ciety ratio and the inside-cluster ratio becomes
higher, while the number of generated clusters
becomes smaller.
Table 1: Summary of clustering results.
? c sd st rs ri
0.10 136,832 3.25 9.3 0.953 0.983
0.30 187,079 3.94 29.4 0.896 0.960
0.50 196,208 4.81 39.7 0.866 0.951
0.70 196,911 5.39 44.4 0.851 0.948
0.90 197,164 5.81 46.3 0.841 0.945
0.95 197,193 5.89 46.6 0.839 0.944
4.3 Categorization Results
In our second experiment, we used a frame-
work of supervised text categorization, where
the generated clusters were used as indices for
classifying documents between the existing so-
cieties, and the categorization performance was
examined.
For this purpose, the documents were first di-
vided into a training set of 50,182 documents
and a test set of 2,641 documents. Then, assum-
ing that the originating societies of the training
documents are known, the significance weights
of the five societies were calculated for each
cluster generated in the previous experiments.
Next, the test documents were assigned to one
of the five societies based on the membership
of the multiple clusters to which they belong.
For comparison, two supervised text categoriza-
tion methods, naive Bayes and Support Vector
Machine (SVM), were also applied to the same
training and test sets.
The results are shown in Table 2. In this
case, the performance was better for larger ?,
indicating that the major factor determining
the categorization performance was the num-
ber of clusters rather than their quality. For
? = 0.5 ? 0.95, each tested document appeared
in at least one of the generated clusters, and the
performance was almost comparable to the per-
formance of standard text categorization meth-
ods: slightly better than naive Bayes, but not
so good as SVM. We also compared the perfor-
mance for varied sizes of training sets and also
using different combination of societies, but the
tendency remained the same.
Table 2: Summary of categorization results.
? correct judge F-value
0.10 2,370 2,446 0.932
0.30 2,520 2,623 0.957
0.50 2,575 2,641 0.975
0.70 2,583 2,641 0.978
0.90 2,584 2,641 0.978
0.95 2,583 2,641 0.978
naive Bayes 2,579 2,641 0.977
SVM 2,602 2,641 0.985
4.4 Further Analysis
Analysis of categorization errors
Table 3 compares the patterns of misclassi-
fication, where the columns and rows repre-
sent the classified and the real categories, re-
spectively. It can be seen that as far as mi-
nor categories such as ESJ and JSAI are con-
cerned, the proposed micro-clustering method
performed slightly better than SVM. The rea-
son may be that the former method is based on
locally conformed clusters and less affected by
the skew of the distribution of category sizes.
However, the details are left for further investi-
gation.
In addition, by manually analyzing the indi-
vidual misclassified documents, it can be con-
firmed that most of them dealt with inter-
domain topics. For example, nine out of the ten
JSCE documents misclassified as ESJ were re-
lated to environmental issues; six out of the 14
JSPE documents misclassified as JSCE, as well as
all seven JSPE documents misclassified as JSAI,
were related to the application of artificial intel-
ligence techniques. These were the major causes
of the performance difference of the two meth-
ods.
Table 3: Analysis of miss-classification.
(a) Micro-clustering results
j u d g e
SPSJ JSCE JSPE ESJ JSAI
r SPSJ 1146 7 2 0 0
e JSCE 5 1007 1 10 1
a JSPE 3 14 216 1 7
l ESJ 0 1 0 120 0
JSAI 0 3 1 1 95
(b) Text categorization results
j u d g e
SPSJ JSCE JSPE ESJ JSAI
r SPSJ 1150 2 3 0 0
e JSCE 2 1017 1 2 2
a JSPE 5 9 226 1 0
l ESJ 0 2 0 119 0
JSAI 1 3 6 0 90
Effect of local improvement:
We also tested the categorization perfor-
mance without local improvement where the top
50 terms at most survive unconditionally after
forming the initial clusters. In this case, the
clustering works similarly to the automatic rel-
evance feedback in information retrieval. Us-
ing the same data set, the result was 2,564 cor-
rect judgments (F-value 0.971), which shows the
effectiveness of local improvement in reducing
noise in automatic relevance feedback.
Effect of cluster duplication check:
Because we do not apply any duplication
check in our generation step, the same cluster
may appear repeatedly in the resulting cluster
set. We have also tested the other case where
clusters with terms or document sets identi-
cal to existing better-performing clusters were
eliminated. The obtained categorization per-
formance was slightly worse than the one with-
out elimination. For example, the best perfor-
mance obtained for ? = 0.9 was 2,582 correct
judgments (F-value 0.978) with 137,867 (30%
reduced) clusters.
The results indicate that the system does not
necessarily require expensive redundancy checks
for the generated clusters as a whole. Such con-
sideration becomes necessary when the formu-
lated clusters are presented to users, in which
case, the duplication check can be applied only
locally.
5 Discussion
In this paper, we reported a method of gener-
ating overlapping micro-clusters in which doc-
uments, terms, and other related elements of
text-based information are grouped together.
Comparing the proposed micro-clustering
method with existing text categorization meth-
ods, the distinctive feature of the former is that
the documents on borders are readily viewed
and examined. In addition, the terms in the
cluster can be further utilized in digesting the
descriptions of the clustered documents. Such
properties of micro-clustering may be particu-
larly important when the system actually inter-
acts with its users.
For comparison purposes, we have used only
the conventional documents-and- terms feature
space in our experiments. However, the pro-
posed micro-clustering framework can be ap-
plied more flexibly to other cases as well. For
example, we have also generated clusters using
the co-occurrences of the triple of documents,
terms, and authors. Although the performance
was not much different in terms of text catego-
rization (2,584 correct judgments out of 2,639
judgments, the precision slightly improved), we
can confirm that many of the highly ranked clus-
ters contain documents produced by the same
group of authors, emphasizing the characteris-
tics of such generated clusters.
Future issues include: (i) enhancing the prob-
abilistic models considering other discounting
techniques in linguistic studies; (ii) developing
a strategy for initiating clusters by combining
different attribute sets, such as documents or
authors; and also (iii) establishing a method
of evaluating overlapping clusters. We are also
looking into the possibility of applying the pro-
posed framework to Web document clustering
problems.
References
A. Aizawa. 2000. The feature quantity: An informa-
tion theoretic perspective of tfidf-like measures.
In Proc. of ACM SIGIR 2000, pages 104?111.
A. Aizawa. 2002. An approach to microscopic clus-
tering of terms and documents. In Proc. of the 7th
Pacific Rim Conference on Artificial Intelligence
(to appear).
R. H. Baayen. 2001. Word frequency distributions.
Kluwer Academic Publishers.
S. Deerwester, S. T. Dumais, G. W. Furnas, T. K.
Landauer, and R. Harshman. 1990. Indexing by
latent semantic analysis. Journal of American So-
ciety of Information Science, 41:391?407.
I. S. Dhillon and D. S. Modha. 1999. Concept
decomposition for large sparse text data using
clustering. Technical Report Research Report RJ
10147, IBM Almaden Research Center.
I. S. Dhillon. 2001. Co-clustering documents and
words using bipartite spectral graph partitioning.
Technical Report 2001-05, UT Austin CS Dept.
M. Iwayama and T. Tokunaga. 1995. Cluster-based
text categorization: a comparison of category
search strategies. In Proc. of ACM SIGIR?95,
pages 273?281.
T. Joachims. 2001. A statistical learning model of
text classification for support vector machines. In
Proc. of ACM SIGIR 2001, pages 128?136.
G. Karypis and E.-H. Han. 2000. Fast supervised
dimensionality reduction algorithm with applica-
tions to document categorization and retrieval. In
Proc. of the 9th ACM International Conference on
Information and Knowledge Management, pages
12?19.
Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Hi-
rano, K. Matsuda, and M. Asahara. 1999. Mor-
phological analysis system chasen 2.0.2 users
manual. NAIST Technical Report NAIST-IS-
TR99012, Nara Institute of Science and Technol-
ogy.
N. Slonim and N. Tishby. 2000. Document cluster-
ing using word clusters via the information bot-
tleneck method. In Proc. of ACM SIGIR 2000,
pages 2008?2015.
M. Steinbach, G. Karypis, and V. Kumar. 2000. A
comparison of document clustering techniques. In
KDD Workshop on Text Mining.
O. Zamir and O. Etzioni. 1998. Web document clus-
tering: A feasibility demonstration. In Proc. of
ACM SIGIR?98, pages 46?54.
 
		Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 686?695,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Framework of Semantic Role Assignment based on Extended Lexical
Conceptual Structure: Comparison with VerbNet and FrameNet
Yuichiroh Matsubayashi? Yusuke Miyao? Akiko Aizawa?
?, National Institute of Informatics, Japan
{y-matsu,yusuke,aizawa}@nii.ac.jp
Abstract
Widely accepted resources for semantic
parsing, such as PropBank and FrameNet,
are not perfect as a semantic role label-
ing framework. Their semantic roles are
not strictly defined; therefore, their mean-
ings and semantic characteristics are un-
clear. In addition, it is presupposed that
a single semantic role is assigned to each
syntactic argument. This is not necessarily
true when we consider internal structures of
verb semantics. We propose a new frame-
work for semantic role annotation which
solves these problems by extending the the-
ory of lexical conceptual structure (LCS).
By comparing our framework with that of
existing resources, including VerbNet and
FrameNet, we demonstrate that our ex-
tended LCS framework can give a formal
definition of semantic role labels, and that
multiple roles of arguments can be repre-
sented strictly and naturally.
1 Introduction
Recent developments of large semantic resources
have accelerated empirical research on seman-
tic processing (Ma`rquez et al 2008). Specif-
ically, corpora with semantic role annotations,
such as PropBank (Kingsbury and Palmer, 2002)
and FrameNet (Ruppenhofer et al 2006), are in-
dispensable resources for semantic role labeling.
However, there are two topics we have to carefully
take into consideration regarding role assignment
frameworks: (1) clarity of semantic role meanings
and (2) the constraint that a single semantic role
is assigned to each syntactic argument.
While these resources are undoubtedly invalu-
able for empirical research on semantic process-
Sentence [John] threw [a ball] [from the window] .
Affection Agent Patient
Movement Source Theme Source/Path
PropBank Arg0 Arg1 Arg2
VerbNet Agent Theme Source
FrameNet Agent Theme Source
Table 1: Examples of single role assignments with ex-
isting resources.
ing, current usage of semantic labels for SRL sys-
tems is questionable from a theoretical viewpoint.
For example, most of the works on SRL have
used PropBank?s numerical role labels (Arg0 to
Arg5). However, the meanings of these numbers
depend on each verb in principle and PropBank
does not expect semantic consistency, namely on
Arg2 to Arg5. Moreover, Yi et al(2007) explic-
itly showed that Arg2 to Arg5 are semantically
inconsistent. The reason why such labels have
been used in SRL systems is that verb-specific
roles generally have a small number of instances
and are not suitable for learning. However, it is
necessary to avoid using inconsistent labels since
those labels confuse machine learners and can be
a cause of low accuracy in automatic process-
ing. In addition, clarity of the definition of roles
are particularly important for users to rationally
know how to use each role in their applications.
For this reasons, well-organized and generalized
labels grounded in linguistic characteristics are
needed in practice. Semantic roles of FrameNet
and VerbNet (Kipper et al 2000) are used more
consistently to some extent, but the definition of
the roles is not given in a formal manner and their
semantic characteristics are unclear.
Another somewhat related problem of existing
annotation frameworks is that it is presupposed
686
that a single semantic role is assigned to each syn-
tactic argument.1In fact, one syntactic argument
can play multiple roles in the event (or events) ex-
pressed by a verb. For example, Table 1 shows a
sentence containing the verb ?throw? and seman-
tic roles assigned to its arguments in each frame-
work. The table shows that each framework as-
signs a single role, such as Arg0 and Agent, to
each syntactic argument. However, we can ac-
quire information from this sentence that John
is an agent of the throwing event (the ?Affec-
tion? row), as well as a source of the movement
event of the ball (the ?Movement? row). Existing
frameworks of assigning single roles simply ig-
nore such information that verbs inherently have
in their semantics. We believe that giving a clear
definition of multiple argument roles would be
beneficial not only as a theoretical framework but
also for practical applications that require detailed
meanings derived from secondary roles.
This issue is also related to fragmentation and
the unclear definition of semantic roles in these
frameworks. As we exemplify in this paper, mul-
tiple semantic characteristics are conflated in a
single role label in these resources due to the man-
ner of single-role assignment. This means that se-
mantic roles of existing resources are not mono-
lithic and inherently not mutually independent,
but they share some semantic characteristics.
The aim of this paper is more on theoreti-
cal discussion for role-labeling frameworks rather
than introducing a new resource. We developed
a framework of verb lexical semantics, which is
an extension of the lexical conceptual structure
(LCS) theory, and compare it with other exist-
ing frameworks which are used in VerbNet and
FrameNet, as an annotation scheme of SRL. LCS
is a decomposition-based approach to verb se-
mantics and describes a meaning by composing
a set of primitive predicates. The advantage of
this approach is that primitive predicates and their
compositions are formally defined. As a result,
we can give a strict definition of semantic roles
by grounding them to lexical semantic structures
of verbs. In fact, we define semantic roles as ar-
gument slots in primitive predicates. With this ap-
1To be precise, FrameNet permits multiple-role assign-
ment, while it does not perform this systematically as we
show in Table 1. It mostly defines a single role label for a
corresponding syntactic argument, that plays multiple roles
in several sub-events in a verb.
proach, we demonstrate that some sort of seman-
tic characteristics that VerbNet and FrameNet in-
formally/implicitly describe in their roles can be
given formal definitions and that multiple argu-
ment roles can be represented strictly and natu-
rally by extending the LCS theory.
In the first half of this paper, we define our ex-
tended LCS framework and describe how it gives
a formal definition of roles and solves the problem
of multiple roles. In the latter half, we discuss
the analysis of the empirical data we collected
for 60 Japanese verbs and also discuss theoreti-
cal relationships with the frameworks of existing
resources. We discuss in detail the relationships
between our role labels and VerbNet?s thematic
roles. We also describe the relationship between
our framework and FrameNet, with regards to the
definitions of the relationships between semantic
frames.
2 Related works
There have been several attempts in linguistics
to assign multiple semantic properties to one ar-
gument. Gruber (1965) demonstrated the dis-
pensability of the constraint that an argument
takes only one semantic role, with some concrete
examples. Rozwadowska (1988) suggested an
approach of feature decomposition for semantic
roles using her three features of change, cause,
and sentient, and defined typical thematic roles
by combining these features. This approach made
it possible for us to classify semantic properties
across thematic roles. However, Levin and Rap-
paport Hovav (2005) argued that the number of
combinations using defined features is usually
larger than the actual number of possible com-
binations; therefore, feature decomposition ap-
proaches should predict possible feature combi-
nations.
Culicover and Wilkins (1984) divided their
roles into two groups, action and perceptional
roles, and explained that dual assignment of roles
always involves one role from each set. Jackend-
off (1990) proposed an LCS framework for rep-
resenting the meaning of a verb by using several
primitive predicates. Jackendoff also stated that
an LCS represents two tiers in its structure, action
tier and thematic tier, which are similar to Culi-
cover and Wilkins?s two sets. Essentially, these
two approaches distinguished roles related to ac-
tion and change, and successfully restricted com-
687
26
6
4
cause(affect(i,j), go(j,
2
6
4
from(locate(in(i)))
fromward(locate(at(k)))
toward(locate(at(l)))
3
7
5
))
3
7
7
5
Figure 1: LCS of the verb throw.
binations of roles by taking a role from each set.
Dorr (1997) created an LCS-based lexical re-
source as an interlingual representation for ma-
chine translation. This framework was also used
for text generation (Habash et al 2003). How-
ever, the problem of multiple-role assignment was
not completely solved on the resource. As a
comparison of different semantic structures, Dorr
(2001) and Hajic?ova? and Kuc?erova? (2002) ana-
lyzed the connection between LCS and PropBank
roles, and showed that the mapping between LCS
and PropBank roles was many to many correspon-
dence and roles can map only by comparing a
whole argument structure of a verb. Habash and
Dorr (2001) tried to map LCS structures into the-
matic roles by using their thematic hierarchy.
3 Multiple role expression using lexical
conceptual structure
Lexical conceptual structure is an approach to de-
scribe a generalized structure of an event or state
represented by a verb. A meaning of a verb is rep-
resented as a structure composed of several prim-
itive predicates. For example, the LCS structure
for the verb ?throw? is shown in Figure 1 and
includes the predicates cause, affect, go, from,
fromward, toward, locate, in, and at. The argu-
ments of primitive predicates are filled by core ar-
guments of the verb. This type of decomposition
approach enables us to represent a case that one
syntactic argument fills multiple slots in the struc-
ture. In Figure 1, the argument i appears twice in
the structure: as the first argument of affect and
the argument in from.
The primitives are designed to represent a full
or partial action-change-state chain, which con-
sists of a state, a change in or maintaining of a
state, or an action that changes/maintains a state.
Table 2 shows primitives that play important roles
to represent that chain. Some primitives embed
other primitives as their arguments and the seman-
tics of the entire structure of an LCS structure
is calculated according to the definition of each
primitive. For instance, the LCS structure in Fig-
Predicates Semantic Functions
state(x, y) First argument is in state specified by
second argument.
cause(x, y) Action in first argument causes change
specified in second argument.
act(x) First argument affects itself.
affect(x, y) First argument affects second argument.
react(x, y) First argument affects itself, due to the
effect from second argument.
go(x, y) First argument changes according to the
path described in the second argument.
from(x) Starting point of certain change event.
fromward(x) Direction of starting point.
via(x) Pass point of certain change event.
toward(x) Direction of end point.
to(x) End point of certain change event.
along(x) Linear-shaped path of change event.
Table 2: Major primitive predicates and their semantic
functions.
ure 1 represents the action changing the state of j.
The inner structure of the second argument of go
represents the path of the change.
The overall definition of our extended LCS
framework is shown in Figure 2.2 Basically, our
definition is based on Jackendoff?s LCS frame-
work (1990), but performed some simplifications
and added extensions. The modification is per-
formed in order to increase strictness and gen-
erality of representation and also a coverage for
various verbs appearing in a corpus. The main
differences between the two LCS frameworks are
as follows. In our extended LCS framework, (i)
the possible combinations of cause, act, affect,
react, and go are clearly restricted, (ii) multiple
actions or changes in an event can be described
by introducing a combination function (comb for
short), (iii) GO, STAY and INCH in Jackendoff?s
theory are incorporated into one function go, and
(iv) most of the change-of-state events are repre-
sented as a metaphor using a spatial transition.
The idea of a comb function comes from a nat-
ural extension of Jackendoff?s EXCH function.
In our case, comb is not limited to describing
a counter-transfer of the main event but can de-
scribe subordinate events occurring in relation to
the main event.3 We can also describe multiple
2Here we omitted the attributes taken by each predicate,
in order to simplify the explanation. We also omitted an
explanation for lower level primitives, such as STATE and
PLACE groups, which are not necessarily important for the
topic of this paper.
3In our extended LCS theory, we can describe multiple
688
LCS =
2
4
EVENT+
comb
h
EVENT
i
*
3
5
STATE =
8
>
>
>
<
>
>
>
:
be
locate(PLACE)
orient(PLACE)
extent(PLACE)
connect(arg)
9
>
>
>
=
>
>
>
;
EVENT =
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
8
>
>
>
<
>
>
>
:
state(arg, STATE)
go(arg, PATH)
cause(act(arg1), go(arg1, PATH))
cause(affect(arg1, arg2), go(arg2, PATH))
cause(react(arg1, arg2), go(arg1, PATH))
9
>
>
>
=
>
>
>
;
manner(constant)?
mean(constant)?
instrument(constant)?
purpose(EVENT)*
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
PLACE =
8
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
:
in(arg)
on(arg)
cover(arg)
fit(arg)
inscribed(arg)
beside(arg)
around(arg)
near(arg)
inside(arg)
at(arg)
9
>
>
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
>
;
PATH=
2
6
6
6
6
6
6
6
6
4
from(STATE)?
fromward(STATE)?
via(STATE)?
toward(STATE)?
to(STATE)?
along(arg)?
3
7
7
7
7
7
7
7
7
5
Figure 2: Description system of our LCS. Operators
+, ?, ? follow the basic regular expression syntax. {}
represents a choice of the elements.
main events if the agent does more than two ac-
tions simultaneously and all the actions are the
focus (e.g., John exchanges A with B). This ex-
tension is simple, but essential for creating LCS
structures of predicates appearing in actual data.
In our development of 60 Japanese predicates
(verb and verbal noun) frequently appearing in
Kyoto University Text Corpus (KTC) (Kurohashi
and Nagao, 1997) , 37.6% of the frames included
multiple events. By using the comb function, we
can express complicated events with predicate de-
composition and prevent missing (multiple) roles.
A key point for associating LCS framework
with the existing frameworks of semantic roles is
that each primitive predicate of LCS represents
a fundamental function in semantics. The func-
events in the semantic structure of a verb. However, gener-
ally, a verb focuses on one of those events and this makes
a semantic variation among verbs such as buy, sell, and pay
as well as difference of syntactic behavior of the arguments.
Therefore, focused event should be distinguished from the
others as lexical information. We expressed focused events
as main formulae (formulae that are not surrounded by a
comb function).
Role Description
Protagonist Entity which is viewpoint of verb.
Theme Entity in which its state or change of state
is mentioned.
State Current state of certain entity.
Actor Entity which performs action that
changes/maintains its state.
Effector Entity which performs action that
changes/maintains a state of another entity.
Patient Entity which is changed/maintained its
state by another entity.
Stimulus Entity which is cause of the action.
Source Starting point of certain change event.
Source dir Direction of starting point.
Middle Pass point of certain change event.
Goal End point of certain change event.
Goal dir Direction of end point.
Route Linear-shaped path of certain change event.
Table 3: Semantic role list for proposing extended LCS
framework.
tions of the arguments of the primitive predicates
can be explained using generalized semantic roles
such as typical thematic roles. In order to sim-
ply represent the semantic functions of the ar-
guments in the LCS primitives or make it eas-
ier to compare our extended LCS framework with
other SRL frameworks, we define a semantic role
set that corresponds to the semantic functions of
the primitive predicates in the LCS structure (Ta-
ble 3). We employed role names similarly to typ-
ical thematic roles in order to easily compare the
role sets, but the definition is different. Also, due
to the increase of the generality of LCS represen-
tation, we obtained clearer definition to explain a
correspondence between LCS primitives and typ-
ical thematic roles than the Jackendoff?s predi-
cates. Note that the core semantic information of
a verb represented by a LCS framework is em-
bodied directly in its LCS structure and the in-
formation decreases if the structure is mapped to
the semantic roles. The mapping is just for con-
trasting thematic roles. Each role is given an ob-
vious meaning and designed to fit to the upper-
level primitives of the LCS structure, which are
the arguments of EVENT and PATH functions. In
Table 4, we can see that these roles correspond al-
most one-to-one to the primitive arguments. One
special role is Protagonist, which does not match
an argument of a specific primitive. The Pro-
tagonist is assigned to the first argument in the
main formula to distinguish that formula from the
sub formulae. There are 13 defined roles, and
689
Predicate 1st arg 2nd arg
state Theme State
act Actor ?
affect Effector Patient
react Actor Stimulus
go Theme PATH
from Source ?
fromward Source dir ?
via Middle ?
toward Goal dir ?
to Goal ?
along Route ?
Table 4: Correspondence between semantic roles and
arguments of LCS primitives
this number is comparatively smaller than that in
VerbNet. The discussion with regard to this num-
ber is described in the next section.
Essentially, the semantic functions of the ar-
guments in LCS primitives are similar to those
of traditional, or basic, thematic roles. However,
there are two important differences. Our extended
LCS framework principally guarantees that the
primitive predicates do not contain any informa-
tion concerning (i) selectional preference and (ii)
complex structural relation of arguments. Primi-
tives are designed to purely represent a function
in an action-change-state chain, thus the informa-
tion of selectional preference is annotated to a dif-
ferent layer; specifically, it is directly annotated to
core arguments (e.g., we can annotate i with sel-
Pref(animate ? organization) in Figure 1). Also,
the semantic function is already decomposed and
the structural relation among the arguments is rep-
resented as a structure of primitives in LCS rep-
resentation. Therefore, each argument slot of
the primitive predicates does not include compli-
cated meanings and represents a primitive seman-
tic property which is highly functional. These
characteristics are necessary to ensure clarity of
the semantic role meanings. We believe that even
though there surely exists a certain type of com-
plex semantic role, it is reasonable to represent
that role based on decomposed properties.
In order to show an instance of our extended
LCS theory, we constructed a dictionary of LCS
structures for 60 Japanese verbs (including event
nouns) using our extended LCS framework. The
60 verbs were the most frequent verbs in KTC af-
ter excluding 100 most frequent ones.4 We cre-
4We omitted top 100 verbs since these most frequent ones
Role Single Multiple Grow (%)
Theme 21 108 414
State 1 1 0
Actor 12 13 8.3
Effector 73 92 26
Patient 77 79 2.5
Stimulus 0 0 0
Source 11 44 300
Source dir 4 4 0
Middle 1 8 700
Goal 42 81 93
Goal dir 2 3 50
Route 2 2 0
w/o Theme 225 327 45
Total 246 435 77
Table 5: Number of appearances of each role
ated the dictionary looking at the instances of
the target verbs in KTC. To increase the cover-
age of senses and case frames, we also consulted
the online Japanese dictionary Digital Daijisen5
and Kyoto university case frames (Kawahara and
Kurohashi, 2006) which is a compilation of case
frames automatically acquired from a huge web
corpus. There were 97 constructed frames in the
dictionary.
Then we analyzed how many roles are addi-
tionally assigned by permitting multiple role as-
signment (see Table 5). The numbers of assigned
roles for single role are calculated by counting
roles that appear first for each target argument in
the structure. Table 5 shows that the total number
of assigned roles is 1.77 times larger than single-
role assignment. The main reason is an increase in
Theme. For single-role assignment, Theme, in our
sense, in action verbs is always duplicated with
Actor/Patient. On the other hand, LCS strictly
divides a function for action and change; there-
fore the duplicated Theme is correctly annotated.
Moreover, we obtained a 45% increase even when
we did not count duplicated Theme. Most of in-
crease are a result from the increase in Source
and Goal. For example, Effectors of transmission
verbs are also annotated with a Source, and Effec-
tors of movement verbs are sometimes annotated
with Source or Goal.
contain a phonogram form (Hiragana form) of a certain verb
written with Kanji characters, and that phonogram form gen-
erally has a huge ambiguity because many different verbs
have same pronunciation in Japanese.
5Available at http://dictionary.goo.ne.jp/jn/.
690
Resource Frame-independent # of roles
LCS yes 13
VerbNet (v3.1) yes 30
FrameNet (r1.4) no 8884
Table 6: Number of roles in each resource.
4 Comparison with other resources
4.1 Number of semantic roles
The number of roles is related to the number of se-
mantic properties represented in a framework and
to the generality of that property. Table 6 lists the
number of semantic roles defined in our extended
LCS framework, VerbNet and FrameNet.
There are two ways to define semantic roles.
One is frame specific, where the definition of each
role depends on a specific lexical entry and such
a role is never used in the other frames. The other
is frame independent, which is to construct roles
whose semantic function is generalized across
all verbs. The number of roles in FrameNet is
comparatively large because it defines roles in a
frame-specific way. FrameNet respects individual
meanings of arguments rather than generality of
roles.
Compared with VerbNet, the number of roles
defined in our extended LCS framework is less
than half. However, this fact does not mean
that the representation ability of our framework is
lower than VerbNet. We manually checked and
listed a corresponding representation in our ex-
tended LCS framework for each thematic role in
VerbNet in Table 6. This table does not provide a
perfect or complete mapping between the roles in
these two frameworks because the mappings are
not based on annotated data. However, we can
roughly say that the VerbNet roles combine three
types of information, a function of the argument
in the action-change-state chain, selectional pref-
erence, and structural information of arguments,
which are in different layers in LCS representa-
tion. VerbNet has many roles whose functions in
the action-change-state chain are duplicated. For
example, Destination, Recipient, and Beneficiary
have the same property end-state (Goal in LCS)
of a changing event. The difference between such
roles comes from a specific sub-type of a chang-
ing event (possession), selectional preference, and
structural information among the arguments. By
distinguishing such roles, VerbNet roles may take
into account specific syntactic behaviors of cer-
tain semantic roles. Packing such complex infor-
mation to semantic roles is useful for analyzing
argument realization. However, from the view-
point of semantic representation, the clarity for
semantic properties provided using a predicate de-
composition approach is beneficial. The 13 roles
for the LCS approach is sufficient for obtaining
a function in the action-change-state chain. In
our LCS framework, selectional preference can
be assigned to arguments in an individual verb or
verb class level instead of role labels themselves
to maintain generality of semantic functions. In
addition, our extended LCS framework can easily
separate complex structural information from role
labels because LCS directly represents a structure
among the arguments. We can calculate the infor-
mation from the LCS structure instead of coding
it into role labels. As a result, our extended LCS
framework maintains generality of roles and the
number of roles is smaller than other frameworks.
4.2 Clarity of role meanings
We showed that an approach of predicate decom-
position used in LCS theory clarified role mean-
ings assigned to syntactic arguments. Moreover,
LCS achieves high generality of roles by separat-
ing selectional preference or structural informa-
tion from role labels. The complex meaning of
one syntactic argument is represented by multi-
ple appearances of the argument in an LCS struc-
ture. For example, we show an LCS structure
and a frame in VerbNet with regard to the verb
?buy? in Figure 3. The LCS structure consists
of four formulae. The first one is the main for-
mula and the others are sub-formulae that rep-
resent co-occurring actions. The semantic-role-
like representation of the structure is given in Ta-
ble 4: i = {Protagonist, Effector, Source, Goal},
j = {Patient,Theme}, k = {Effector, Source,
Goal}, and l = {Patient,Theme}. Selectional
preference is annotated to each argument as i:
selPref(animate ? organization), j: selPref(any),
k: selPref(animate ? organization), and l: sel-
Pref(valuable entity). If we want to represent the
information, such as ?Source of what??, then we
can extend the notation as Source(j) to refer to a
changing object.
On the other hand, VerbNet combines mul-
tiple types of information into a single role as
mentioned above. Also, the meaning of some
691
VerbNet role (# of uses) Representation in LCS
Actor (9), Actor1 (9), Actor2 (9) Actor or Effector in symmetric formulas in the structure
Agent (212) (Actor ? Effector) ? Protagonist
Asset (6) Theme ? Source of the change is (locate(in()) ? Protagonist) ?
selPref(valuable entity)
Beneficiary (9) (peripheral role ? (Goal ? locate(in()))) ? selPref(animate ? organization)
? ?(Actor ? Effector) ? a transferred entity is something beneficial
Cause (21) ((Effector ? selPref(?animate ? ?organization)) ? Stimulus ? peripheral role)
Destination (32) Goal
Experiencer (24) Actor of react()
Instrument (25) ((Effector ? selPref(?animate ? ?organization)) ? peripheral role)
Location (45) (Theme ? PATH roles ? peripheral role) ? selPref(location)
Material (6) Theme ? Source of a change ? The Goal of the change is locate(fit()) ?
the Goal fullfills selPref(physical object)
Patient (59), Patient 1(11) Patient ? Theme
Patient2 (11) (Source ? Goal) ? connect()
Predicate (23) Theme ? (Goal ? locate(fit())) ? peripheral role
Product (7) Theme ? (Goal ? locate(fit()) ? selPref(physical object))
Proposition (11) Theme
Recipient (33) Goal ? locate(in()) ? selPref(animate ? organization)
Source (34) Source
Theme (162) Theme
Theme1 (13), Theme2 (13) Both of the two is Theme ? Theme1 is Theme and Theme2 is State
Topic (18) Theme ? selPref(knowledge ? infromation)
Table 7: Relationship of roles between VerbNet and our LCS framework. VerbNet roles that appears more than
five times in frame definition are analyzed. Each relationship shown here is only a partial and consistent part of
the complete correspondence table. Note that complete table of mapping highly depends on each lexical entry
(or verb class). Here, locate(in()) generally means possession or recognizing.
roles depends more on selectional preference or
the structure of the arguments than a primitive
function in the action-change-state chain. Such
VerbNet roles are used for several different func-
tions depending on verbs and their alternations,
and it is therefore difficult to capture decomposed
properties from the role label without having spe-
cific lexical knowledge. Moreover, some seman-
tic functions, such as Mary is a Goal of the money
in Figure 3, are completely discarded from the
representation at the level of role labels.
There is another representation related to the
argument meanings in VerbNet. This representa-
tion is a type of predicate decomposition using its
original set of predicates, which are referred to as
semantic predicates. For example, the verb ?buy?
in Figure 3 has the predicates has possession,
transfer and cost for composing the meaning of
its event structure. The thematic roles are fillers
of the predicates? arguments, thus the semantic
predicates may implicitly provide additional func-
tions to the roles and possibly represent multiple
roles. Unfortunately, we cannot discover what
each argument of the semantic predicates exactly
means since the definition of each predicate is not
Example: ?John bought a book from Mary for $10.?
VerbNet: Agent V Theme {from} Source {for} Asset.
has possession(start(E), Source, Theme),
has possession(end(E), Agent, Theme),
transfer(during(E), Theme), cost(E, Asset)
LCS:
2
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
4
cause(aff(i:John, j:a book), go(j,
h
to(loc(in(i)))
i
))
comb
2
4cause(aff(i,l:$10), go(l,
"
from(loc(in(i)))
to(loc(at(k:Mary)))
#
))
3
5
comb
2
4cause(aff(k,j), go(j,
"
from(loc(in(k)))
to(loc(at(i)))
#
))
3
5
comb
?
cause(aff(k,l), go(l,
h
to(loc(in(k)))
i
))
?
3
7
7
7
7
7
7
7
7
7
7
7
7
7
7
7
5
Figure 3: Comparison between the semantic predicate
representation and the LCS structure of the verb buy.
publicly available. A requirement for obtaining
implicit semantic functions from these semantic
predicates is clearly defining how the roles (or
functions) are calculated from these complex re-
lations of semantic predicates.
FrameNet does not use semantic roles general-
ized among all verbs or does not represent seman-
692
i: selPref(animate ? organization), j: selPref(any), k: selPref(animate ? organization), l:
selPref(valuable entity)
Figure 4: LCS of the verbs get, buy, sell, pay, and collect and their relationships calculated from the structures.
tic properties of roles using a predicate decom-
position approach, but defines specific roles for
each conceptual event/state to represent a specific
background of the roles in the event/state. How-
ever, at the same time, FrameNet defines several
types of parent-child relations between most of
the frames and between their roles; therefore, we
may say FrameNet implicitly describes a sort of
decomposed property using roles in highly gen-
eral or abstract frames and represents the inher-
itance of these semantic properties. One advan-
tage of this approach is that the inheritance of a
meaning between roles is controlled through the
relations, which are carefully maintained by hu-
man efforts, and is not restricted by the represen-
tation ability of the decomposition system. On the
other hand, the only way to represent generalized
properties of a certain semantic role is enumerat-
ing all inherited roles by tracing ancestors. Also,
a semantic relation between arguments in a cer-
tain frame, which is given by LCS structure and
semantic predicates of VerbNet, is only defined
by a natural language description for each frame
in FrameNet. From a CL point of view, we con-
sider that, at least, a certain level of formalization
of semantic relation of arguments is important for
utilize this information for application. LCS ap-
proach, or an approach using a well-defined pred-
icate decomposition, can explicitly describe se-
mantic properties and relationships between argu-
Figure 5: The frame relations among the verbs get,
buy, sell, pay, and collect in FrameNet.
ments in a lexical structure. The primitive proper-
ties can be clearly defined, even though the repre-
sentation ability is restricted under the generality
of roles.
In addition, the frame-to-frame relations in
FrameNet may be a useful resource for some ap-
plication tasks such as paraphrasing and entail-
ment. We argue that some types of relationships
between frames are automatically calculated us-
ing the LCS approach. For example, one of the
relations is based on an inclusion relation of two
LCS structures. Figure 4 shows automatically
calculated relations surrounding the verb ?buy?.
Note that we chose a sense related to a com-
mercial transaction, which means a exchange of
a goods and money, for each word in order to
compare the resulted relation graph with that of
FrameNet. We call relations among ?buy?, ?sell?,
?pay? and ?collect? as different viewpoints since
693
they contain exactly the same formulae, and the
only difference is the main formula. The rela-
tion between ?buy? and ?get? is defined as in-
heritance; a part of the child structure exactly
equals the parent structure. Interestingly, the re-
lations surrounding the ?buy? are similar to those
in FrameNet (see Figure 5). We cannot describe
all types of the relations we considered due to
space limitations. However, the point is that these
relationships are represented as rewriting rules
between the two LCS representations and thus
they are automatically calculated. Moreover, the
grounds for relations maintain clarity based on
concrete structural relations. A semantic relation
construction of frames based on structural rela-
tionships is another possible application of LCS
approaches that connects traditional LCS theo-
ries with resources representing a lexical network
such as FrameNet.
4.3 Consistency on semantic structures
Constructing a LCS dictionary is generally a dif-
ficult work since LCS has a high flexibility for
describing structures and different people tend to
write different structures for a single verb. We
maintained consistency of the dictionary by tak-
ing into account a similarity of the structures be-
tween the verbs that are in paraphrasing or entail-
ment relations. This idea was inspired by auto-
matic calculation of semantic relations of lexicon
as we mentioned above. We created a LCS struc-
ture for each lexical entry as we can calculate se-
mantic relations between related verbs and main-
tained high-level consistency among the verbs.
Using our extended LCS theory, we success-
fully created 97 frames for 60 predicates without
any extra modification. From this result, we be-
lieve that our extended theory is stable to some
extent. On the other hand, we found that an extra
extension of the LCS theory is needed for some
verbs to explain the different syntactic behaviors
of one verb. For example, a condition for a cer-
tain syntactic behavior of a verb related to re-
ciprocal alteration (see class 2.5 of Levin (Levin,
1993)) such as???? (connect) and?? (in-
tegrate) cannot be explained without considering
the number of entities in some arguments. Also,
some verbs need to define an order of the internal
events. For example, the Japanese verb ???
? (shuttle) means that going is a first action and
coming back is a second action. These are not
the problems that are directly related to a seman-
tic role annotation on that we focus in this paper,
but we plan to solve these problems with further
extensions.
5 Conclusion
We discussed the two problems in current labeling
approaches for argument-structure analysis: the
problems in clarity of role meanings and multiple-
role assignment. By focusing on the fact that an
approach of predicate decomposition is suitable
for solving these problems, we proposed a new
framework for semantic role assignment by ex-
tending Jackendoff?s LCS framework. The statis-
tics of our LCS dictionary for 60 Japanese verbs
showed that 37.6% of the created frames included
multiple events and the number of assigned roles
for one syntactic argument increased 77% from
that in single-role assignment.
Compared to the other resources such as Verb-
Net and FrameNet, the role definitions in our ex-
tended LCS framework are clearer since the prim-
itive predicates limit the meaning of each role to
a function in the action-change-state chain. We
also showed that LCS can separate three types of
information, the functions represented by primi-
tives, the selectional preference and structural re-
lation of arguments, which are conflated in role la-
bels in existing resources. As a potential of LCS,
we demonstrated that several types of frame re-
lations, which are similar to those in FrameNet,
are automatically calculated using the structural
relations between LCSs. We still must perform a
thorough investigation for enumerating relations
which can be represented in terms of rewriting
rules for LCS structures. However, automatic
construction of a consistent relation graph of se-
mantic frames may be possible based on lexical
structures.
We believe that this kind of decomposed analy-
sis will accelerate both fundamental and applica-
tion research on argument-structure analysis. As a
future work, we plan to expand the dictionary and
construct a corpus based on our LCS dictionary.
Acknowledgment
This work was partially supported by JSPS Grant-
in-Aid for Scientific Research #22800078.
694
References
P.W. Culicover and W.K. Wilkins. 1984. Locality in
linguistic theory. Academic Press.
Bonnie J. Dorr. 1997. Large-scale dictionary con-
struction for foreign language tutoring and inter-
lingual machine translation. Machine Translation,
12(4):271?322.
Bonnie J. Dorr. 2001. Lcs database. http://www.
umiacs.umd.edu/?bonnie/LCS Database Document
ation.html.
Jeffrey S Gruber. 1965. Studies in lexical relations.
Ph.D. thesis, MIT.
N. Habash and B. Dorr. 2001. Large scale language
independent generation using thematic hierarchies.
In Proceedings of MT summit VIII.
N. Habash, B. Dorr, and D. Traum. 2003. Hybrid
natural language generation from lexical conceptual
structures. Machine Translation, 18(2):81?128.
Eva Hajic?ova? and Ivona Kuc?erova?. 2002. Argu-
ment/valency structure in propbank, lcs database
and prague dependency treebank: A comparative
pilot study. In Proceedings of the Third Inter-
national Conference on Language Resources and
Evaluation (LREC 2002), pages 846?851.
Ray Jackendoff. 1990. Semantic Structures. The MIT
Press.
D. Kawahara and S. Kurohashi. 2006. Case frame
compilation from the web using high-performance
computing. In Proceedings of LREC-2006, pages
1344?1347.
Paul Kingsbury and Martha Palmer. 2002. From Tree-
bank to PropBank. In Proceedings of LREC-2002,
pages 1989?1993.
Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000. Class-based construction of a verb lexicon.
In Proceedings of the National Conference on Arti-
ficial Intelligence, pages 691?696. Menlo Park, CA;
Cambridge, MA; London; AAAI Press; MIT Press;
1999.
Sadao Kurohashi and Makoto Nagao. 1997. Kyoto
university text corpus project. Proceedings of the
Annual Conference of JSAI, 11:58?61.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment realization. Cambridge University Press.
Beth Levin. 1993. English verb classes and alter-
nations: A preliminary investigation. University of
Chicago Press.
Llu??s Ma`rquez, Xavier Carreras, Kenneth C.
Litkowski, and Suzanne Stevenson. 2008. Se-
mantic role labeling: an introduction to the special
issue. Computational linguistics, 34(2):145?159.
B. Rozwadowska. 1988. Thematic restrictions on de-
rived nominals. In W Wlikins, editor, Syntax and
Semantics, volume 21, pages 147?165. Academic
Press.
J. Ruppenhofer, M. Ellsworth, M.R.L. Petruck, C.R.
Johnson, and J. Scheffczyk. 2006. FrameNet II:
Extended Theory and Practice. Berkeley FrameNet
Release, 1.
Szu-ting Yi, Edward Loper, and Martha Palmer. 2007.
Can semantic roles generalize across genres? In
Proceedings of HLT-NAACL 2007, pages 548?555.
695
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 473?478,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Clustering Comparable Corpora For Bilingual Lexicon Extraction
Bo Li, Eric Gaussier
UJF-Grenoble 1 / CNRS, France
LIG UMR 5217
firstname.lastname@imag.fr
Akiko Aizawa
National Institute of Informatics
Tokyo, Japan
aizawa@nii.ac.jp
Abstract
We study in this paper the problem of enhanc-
ing the comparability of bilingual corpora in
order to improve the quality of bilingual lexi-
cons extracted from comparable corpora. We
introduce a clustering-based approach for en-
hancing corpus comparability which exploits
the homogeneity feature of the corpus, and
finally preserves most of the vocabulary of
the original corpus. Our experiments illus-
trate the well-foundedness of this method and
show that the bilingual lexicons obtained from
the homogeneous corpus are of better quality
than the lexicons obtained with previous ap-
proaches.
1 Introduction
Bilingual lexicons are an important resource in mul-
tilingual natural language processing tasks such as
statistical machine translation (Och and Ney, 2003)
and cross-language information retrieval (Balles-
teros and Croft, 1997). Because it is expensive to
manually build bilingual lexicons adapted to dif-
ferent domains, researchers have tried to automat-
ically extract bilingual lexicons from various cor-
pora. Compared with parallel corpora, it is much
easier to build high-volume comparable corpora, i.e.
corpora consisting of documents in different lan-
guages covering overlapping information. Several
studies have focused on the extraction of bilingual
lexicons from comparable corpora (Fung and McK-
eown, 1997; Fung and Yee, 1998; Rapp, 1999;
De?jean et al, 2002; Gaussier et al, 2004; Robitaille
et al, 2006; Morin et al, 2007; Garera et al, 2009;
Yu and Tsujii, 2009; Shezaf and Rappoport, 2010).
The basic assumption behind most studies on lex-
icon extraction from comparable corpora is a dis-
tributional hypothesis, stating that words which are
translation of each other are likely to appear in simi-
lar context across languages. On top of this hypoth-
esis, researchers have investigated the use of better
representations for word contexts, as well as the use
of different methods for matching words across lan-
guages. These approaches seem to have reached a
plateau in terms of performance. More recently, and
departing from such traditional approaches, we have
proposed in (Li and Gaussier, 2010) an approach
based on improving the comparability of the cor-
pus under consideration, prior to extracting bilingual
lexicons. This approach is interesting since there is
no point in trying to extract lexicons from a corpus
with a low degree of comparability, as the probabil-
ity of finding translations of any given word is low
in such cases. We follow here the same general idea
and aim, in a first step, at improving the compara-
bility of a given corpus while preserving most of
its vocabulary. However, unlike the previous work,
we show here that it is possible to guarantee a cer-
tain degree of homogeneity for the improved corpus,
and that this homogeneity translates into a signifi-
cant improvement of both the quality of the resulting
corpora and the bilingual lexicons extracted.
2 Enhancing Comparable Corpora: A
Clustering Approach
We first introduce in this section the comparability
measure proposed in former work, prior to describ-
ing the clustering-based algorithm to improve the
473
quality of a given comparable corpus. For conve-
nience, the following discussion will be made in the
context of the English-French comparable corpus.
2.1 The Comparability Measure
In order to measure the degree of comparability of
bilingual corpora, we make use of the measure M
developed in (Li and Gaussier, 2010): Given a com-
parable corpus P consisting of an English part Pe
and a French part Pf , the degree of comparability of
P is defined as the expectation of finding the trans-
lation of any given source/target word in the tar-
get/source corpus vocabulary. Let ? be a function
indicating whether a translation from the translation
set Tw of the word w is found in the vocabulary Pv
of a corpus P , i.e.:
?(w,P) =
{
1 iff Tw ? Pv 6= ?
0 else
and letD be a bilingual dictionary withDve denoting
its English vocabulary andDvf its French vocabulary.
The comparability measure M can be written as:
M(Pe,Pf ) (1)
=
?
w?Pe?Dve
?(w,Pf ) +
?
w?Pf?Dvf
?(w,Pe)
#w(Pe ? Dve ) + #w(Pf ? D
v
f )
where #w(P) denotes the number of different
words present in P . One can find from equa-
tion 1 that M directly measures the proportion of
source/target words translated in the target/source
vocabulary of P .
2.2 Clustering Documents for High Quality
Comparable Corpora
If a corpus covers a limited set of topics, it is more
likely to contain consistent information on the words
used (Morin et al, 2007), leading to improved bilin-
gual lexicons extracted with existing algorithms re-
lying on the distributional hypothesis. The term ho-
mogeneity directly refers to this fact, and we will say,
in an informal manner, that a corpus is homogeneous
if it covers a limited set of topics. The rationale for
the algorithm we introduce here to enhance corpus
comparability is precisely based on the concept of
homogeneity. In order to find document sets which
are similar with each other (i.e. homogeneous), it
is natural to resort to clustering techniques. Further-
more, since we need homogeneous corpora for bilin-
gual lexicon extraction, it will be convenient to rely
on techniques which allows one to easily prune less
relevant clusters. To perform all this, we use in this
work a standard hierarchical agglomerative cluster-
ing method.
2.2.1 Bilingual Clustering Algorithm
The overall process retained to build high quality,
homogeneous comparable corpora relies on the fol-
lowing steps:
1. Using the bilingual similarity measure defined
in Section 2.2.2, cluster English and French
documents so as to get bilingual dendrograms
from the original corpus P by grouping docu-
ments with related content;
2. Pick high quality sub-clusters by threshold-
ing the obtained dendrograms according to the
node depth, which retains nodes far from the
roots of the clustering trees;
3. Combine all these sub-clusters to form a new
comparable corpus PH , which thus contains
homogeneous, high-quality subparts;
4. Use again steps (1), (2) and (3) to enrich the
remaining subpart of P (denoted as PL, PL =
P \ PH ) with external resources.
The first three steps aim at extracting the most com-
parable and homogeneous subpart of P . Once this
has been done, one needs to resort to new corpora
if one wants to build an homogeneous corpus with
a high degree of comparability from PL. To do so,
we simply perform, in step (4), the clustering and
thresholding process defined in (1), (2) and (3) on
two comparable corpora: The first one consists of
the English part of PL and the French part of an ex-
ternal corpus PT ; The second one consists of the
French part of PL and the English part of PT . The
two high quality subparts obtained from these two
new comparable corpora in step (4) are then com-
bined with PH to constitute the final comparable
corpus of higher quality.
474
2.2.2 Similarity Measure
Let us assume that we have two document sets (i.e.
clusters) C1 and C2. In the task of bilingual lexi-
con extraction, two document sets are similar to each
other and should be clustered if the combination of
the two can complement the content of each single
set, which relates to the notion of homogeneity. In
other words, both the English part Ce1 of C1 and the
French part Cf1 of C1 should be comparable to their
counterparts (respectively the same for the French
part Cf2 of C2 and the English part C
e
2 of C2). This
leads to the following similarity measure for C1 and
C2:
sim(C1, C2) = ? ?M(Ce1, C
f
2 )+ (1??) ?M(C
e
2, C
f
1 )
where ? (0 ? ? ? 1) is a weight controlling the
importance of the two subparts (Ce1 , C
f
2 ) and (C
e
2 ,
Cf1 ). Intuitively, the larger one, containing more in-
formation, of the two comparable corpora (Ce1 , C
f
2 )
and (Ce2 , C
f
1 ) should dominate the overall similar-
ity sim(C1, C2). Since the content relatedness in the
comparable corpus is basically reflected by the re-
lations between all the possible bilingual document
pairs, we use here the number of document pairs to
represent the scale of the comparable corpus. The
weight ? can thus be defined as the proportion of
possible document pairs in the current comparable
corpus (Ce1 , C
f
2 ) to all the possible document pairs,
which is:
? =
#d(Ce1) ?#d(C
f
2 )
#d(Ce1) ?#d(C
f
2 ) + #d(C
e
2) ?#d(C
f
1 )
where #d(C) stands for the number of documents in
C. However, this measure does not integrate the rel-
ative length of the French and English parts, which
actually impacts the performance of bilingual lexi-
con extraction. If a 1-to-1 constraint is too strong
(i.e. assuming that all clusters should contain the
same number of English and French documents),
having completely unbalanced corpora is also not
desirable. We thus introduce a penalty function ?
aiming at penalizing unbalanced corpora:
?(C) =
1
(1 + log(1 + |#d(C
e)?#d(Cf )|
min(#d(Ce)),#d(Cf ))
)
(2)
The above penalty function leads us to a new simi-
larity measure siml which is the one finally used in
the above algorithm:
siml(C1, C2) = sim(C1, C2) ? ?(C1 ? C2) (3)
3 Experiments and Results
The experiments we have designed in this paper aim
at assessing (a) whether the clustering-based algo-
rithm we have introduced yields corpora of higher
quality in terms of comparability scores, and (b)
whether the bilingual lexicons extracted from such
corpora are of higher quality. Several corpora were
used in our experiments: the TREC1 Associated
Press corpus (AP, English) and the corpora used
in the CLEF2 campaign including the Los Ange-
les Times (LAT94, English), the Glasgow Herald
(GH95, English), Le Monde (MON94, French), SDA
French 94 (SDA94, French) and SDA French 95
(SDA95, French). In addition, two monolingual cor-
pora Wiki-En and Wiki-Fr were built by respectively
retrieving all the articles below the category Society
and Socie?te? from the Wikipedia dump files3. The
bilingual dictionary used in the experiments is con-
structed from an online dictionary. It consists of
33k distinct English words and 28k distinct French
words, constituting 76k translation pairs. In our ex-
periments, we use the method described in this pa-
per, as well as the one in (Li and Gaussier, 2010)
which is the only alternative method to enhance cor-
pus comparability.
3.1 Improving Corpus Quality
In this subsection, the clustering algorithm described
in Section 2.2.1 is employed to improve the quality
of the comparable corpus. The corpora GH95 and
SDA95 are used as the original corpus P0 (56k En-
glish documents and 42k French documents). We
consider two external corpora: P1T (109k English
documents and 87k French documents) consisting of
the corpora LAT94, MON94 and SDA94; P2T (368k
English documents and 378k French documents)
consisting of Wiki-En and Wiki-Fr.
1http://trec.nist.gov
2http://www.clef-campaign.org
3The Wikipedia dump files can be downloaded at
http://download.wikimedia.org. In this paper, we use the En-
glish dump file on July 13, 2009 and the French dump file on
July 7, 2009.
475
P0 P1? P2? P1 P2 P1 > P0 P2 > P0
Precision 0.226 0.277 0.325 0.295 0.461 0.069, 30.5% 0.235, 104.0%
Recall 0.103 0.122 0.145 0.133 0.212 0.030, 29.1% 0.109, 105.8%
Table 1: Performance of the bilingual lexicon extraction from different corpora (best results in bold)
After the clustering process, we obtain the result-
ing corpora P1 (with the external corpus P1T ) and
P2 (with P2T ). As mentioned before, we also used
the method described in (Li and Gaussier, 2010)
on the same data, producing resulting corpora P1?
(with P1T ) and P
2? (with P2T ) from P
0. In terms
of lexical coverage, P1 (resp. P2) covers 97.9%
(resp. 99.0%) of the vocabulary of P0. Hence, most
of the vocabulary of the original corpus has been
preserved. The comparability score of P1 reaches
0.924 and that of P2 is 0.939. Both corpora are
more comparable than P0 of which the comparabil-
ity is 0.881. Furthermore, both P1 and P2 are more
comparable than P1? (comparability 0.912) and P2?
(comparability 0.915), which shows homogeneity is
crucial for comparability. The intrinsic evaluation
shows the efficiency of our approach which can im-
prove the quality of the given corpus while preserv-
ing most of its vocabulary.
3.2 Bilingual Lexicon Extraction Experiments
To extract bilingual lexicons from comparable cor-
pora, we directly use here the method proposed by
Fung and Yee (1998) which has been referred to
as the standard approach in more recent studies
(De?jean et al, 2002; Gaussier et al, 2004; Yu and
Tsujii, 2009). In this approach, each word w is rep-
resented as a context vector consisting of the words
co-occurring with w in a certain window in the cor-
pus. The context vectors in different languages are
then bridged with an existing bilingual dictionary.
Finally, a similarity score is given to any word pair
based on the cosine of their respective context vec-
tors.
3.2.1 Experiment Settings
In order to measure the performance of the lexi-
cons extracted, we follow the common practice by
dividing the bilingual dictionary into 2 parts: 10%
of the English words (3,338 words) together with
their translations are randomly chosen and used as
the evaluation set, the remaining words being used
to compute the similarity of context vectors. En-
glish words not present in Pe or with no translation
in Pf are excluded from the evaluation set. For each
English word in the evaluation set, all the French
words in Pf are then ranked according to their sim-
ilarity with the English word. Precision and recall
are then computed on the first N translation candi-
date lists. The precision amounts in this case to the
proportion of lists containing the correct translation
(in case of multiple translations, a list is deemed to
contain the correct translation as soon as one of the
possible translations is present). The recall is the
proportion of correct translations found in the lists
to all the translations in the corpus. This evaluation
procedure has been used in previous studies and is
now standard.
3.2.2 Results and Analysis
In a first series of experiments, bilingual lexicons
were extracted from the corpora obtained by our ap-
proach (P1 and P2), the corpora obtained by the
approach described in (Li and Gaussier, 2010) (P1?
and P2?) and the original corpus P0, with the fixed
N value set to 20. Table 1 displays the results ob-
tained. Each of the last two columns ?P1 > P0?
and ?P2 > P0? contains the absolute and the rel-
ative difference (in %) w.r.t. P0. As one can note,
the best results (in bold) are obtained from the cor-
pora P2 built with the method we have described in
this paper. The lexicons extracted from the enhanced
corpora are of much higher quality than the ones ob-
tained from the original corpus . For instance, the
increase of the precision is 6.9% (30.5% relatively)
in P1 and 23.5% (104.0% relatively) in P2, com-
pared with P0. The difference is more remarkable
withP2, which is obtained from a large external cor-
pus P2T . Intuitively, one can expect to find, in larger
corpora, more documents related to a given corpus,
an intuition which seems to be confirmed by our re-
sults. One can also notice, by comparing P2 and
P2? as well as P1 and P1?, a remarkable improve-
ment when considering our approach and the early
476
methodology.
Intuitively, the value N plays an important role
in the above experiments. In a second series of ex-
periments, we let N vary from 1 to 300 and plot the
results obtained with different evaluation measure in
Figure 1. In Figure 1(a) (resp. Figure 1(b)), the x-
axis corresponds to the values taken by N, and the y-
axis to the precision (resp. recall) scores for the lexi-
cons extracted on each of the 5 corporaP0,P1?,P2?,
P1 and P2. A clear fact from the figure is that both
the precision and the recall scores increase accord-
ing to the increase of the N values, which coincides
with our intuition. As one can note, our method con-
sistently outperforms the previous work and also the
original corpus on all the values considered for N .
0 100 200 300
0.0
0.2
0.4
0.6
0.8
N
Precision
P
2
P
2'
P
1
P
1'
P
0
(a) Precision
0 100 200 300
0.0
0.1
0.2
0.3
0.4
N
Recall
P
2
P
2'
P
1
P
1'
P
0
(b) Recall
Figure 1: Performance of bilingual lexicon extraction
from different corpora with varied N values from 1 to
300. The five lines from the top down in each subfigure
are corresponding to the results for P2, P2?, P1, P1? and
P0 respectively.
4 Discussion
As previous studies on bilingual lexicon extrac-
tion from comparable corpora radically differ on
resources used and technical choices, it is very
difficult to compare them in a unified framework
(Laroche and Langlais, 2010). We compare in this
section our method with some ones in the same vein
(i.e. enhancing bilingual corpora prior to extract-
ing bilingual lexicons from them). Some works like
(Munteanu et al, 2004) and (Munteanu and Marcu,
2006) propose methods to extract parallel fragments
from comparable corpora. However, their approach
only focuses on a very small part of the original cor-
pus, whereas our work aims at preserving most of
the vocabulary of the original corpus.
We have followed here the general approach in
(Li and Gaussier, 2010) which consists in enhancing
the quality of a comparable corpus prior to extract-
ing information from it. However, despite this latter
work, we have shown here a method which ensures
homogeneity of the obtained corpus, and which fi-
nally leads to comparable corpora of higher quality.
In turn such corpora yield better bilingual lexicons
extracted.
Acknowledgements
This work was supported by the French National Re-
search Agency grant ANR-08-CORD-009.
References
Lisa Ballesteros and W. Bruce Croft. 1997. Phrasal
translation and query expansion techniques for cross-
language information retrieval. In Proceedings of the
20th ACM SIGIR, pages 84?91, Philadelphia, Pennsyl-
vania, USA.
Herve? De?jean, Eric Gaussier, and Fatia Sadat. 2002.
An approach based on multilingual thesauri and model
combination for bilingual lexicon extraction. In Pro-
ceedings of the 19th International Conference on
Computational Linguistics, pages 1?7, Taipei, Taiwan.
Pascale Fung and Kathleen McKeown. 1997. Find-
ing terminology translations from non-parallel cor-
pora. In Proceedings of the 5th Annual Workshop on
Very Large Corpora, pages 192?202, Hong Kong.
Pascale Fung and Lo Yuen Yee. 1998. An IR approach
for translating new words from nonparallel, compara-
ble texts. In Proceedings of the 17th international con-
477
ference on Computational linguistics, pages 414?420,
Montreal, Quebec, Canada.
Nikesh Garera, Chris Callison-Burch, and David
Yarowsky. 2009. Improving translation lexicon induc-
tion from monolingual corpora via dependency con-
texts and part-of-speech equivalences. In CoNLL 09:
Proceedings of the Thirteenth Conference on Compu-
tational Natural Language Learning, pages 129?137,
Boulder, Colorado.
E. Gaussier, J.-M. Renders, I. Matveeva, C. Goutte, and
H. De?jean. 2004. A geometric view on bilingual
lexicon extraction from comparable corpora. In Pro-
ceedings of the 42nd Annual Meeting of the Associ-
ation for Computational Linguistics, pages 526?533,
Barcelona, Spain.
Audrey Laroche and Philippe Langlais. 2010. Revisiting
context-based projection methods for term-translation
spotting in comparable corpora. In Proceedings of
the 23rd International Conference on Computational
Linguistics (Coling 2010), pages 617?625, Beijing,
China, August.
Bo Li and Eric Gaussier. 2010. Improving corpus
comparability for bilingual lexicon extraction from
comparable corpora. In Proceedings of the 23rd In-
ternational Conference on Computational Linguistics,
pages 644?652, Beijing, China.
Emmanuel Morin, Be?atrice Daille, Koichi Takeuchi, and
Kyo Kageura. 2007. Bilingual terminology mining -
using brain, not brawn comparable corpora. In Pro-
ceedings of the 45th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 664?671,
Prague, Czech Republic.
Dragos Stefan Munteanu and Daniel Marcu. 2006. Ex-
tracting parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Compu-
tational Linguistics, pages 81?88, Sydney, Australia.
Dragos Stefan Munteanu, Alexander Fraser, and Daniel
Marcu. 2004. Improved machine translation perfor-
mance via parallel sentence extraction from compara-
ble corpora. In Proceedings of the HLT-NAACL 2004,
pages 265?272, Boston, MA., USA.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Reinhard Rapp. 1999. Automatic identification of word
translations from unrelated English and German cor-
pora. In Proceedings of the 37th Annual Meeting of
the Association for Computational Linguistics, pages
519?526, College Park, Maryland, USA.
Xavier Robitaille, Yasuhiro Sasaki, Masatsugu Tonoike,
Satoshi Sato, and Takehito Utsuro. 2006. Compil-
ing French-Japanese terminologies from the web. In
Proceedings of the 11st Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 225?232, Trento, Italy.
Daphna Shezaf and Ari Rappoport. 2010. Bilingual lex-
icon generation using non-aligned signatures. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 98?107, Up-
psala, Sweden.
Kun Yu and Junichi Tsujii. 2009. Extracting bilingual
dictionary from comparable corpora with dependency
heterogeneity. In Proceedings of HLT-NAACL 2009,
pages 121?124, Boulder, Colorado, USA.
478
Proceedings of the Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages 69?74,
Beijing, August 2010
Mining coreference relations between formulas and text using Wikipedia
Minh Nghiem Quoc 1, Keisuke Yokoi 2, Yuichiroh Matsubayashi 3 Akiko Aizawa 1 2 3
1 Department of Informatics, The Graduate University for Advanced Studies
2 Department of Computer Science, University of Tokyo
3 National Institute of Informatics
{nqminh, kei-yoko, y-matsu, aizawa}@nii.ac.jp
Abstract
In this paper, we address the problem of
discovering coreference relations between
formulas and the surrounding text. The
task is different from traditional coref-
erence resolution because of the unique
structure of the formulas. In this paper, we
present an approach, which we call ?CDF
(Concept Description Formula)?, for min-
ing coreference relations between formu-
las and the concepts that refer to them.
Using Wikipedia articles as a target cor-
pus, our approach is based on surface level
text matching between formulas and text,
as well as patterns that represent relation-
ships between them. The results showed
the potential of our approach for formulas
and text coreference mining.
1 Introduction
1.1 Motivation
Mathematical content is a valuable information
source for many users: teachers, students, re-
searchers need access to mathematical resources
for teaching, studying, or obtaining updated infor-
mation for research and development. Although
more and more mathematical content is becom-
ing available on the Web nowadays, conventional
search engines do not provide direct search of
mathematical formulas. As such, retrieving math-
ematical content remains an open issue.
Some recent studies proposed mathematical re-
trieval systems that were based on structural sim-
ilarity of equations (Adeel and Khiyal, 2008;
Yokoi and Aizawa, 2009; Nghiem et al, 2009).
However, in these studies, the semantics of the
equations is still not taken into account. As
mathematical equations follow highly abstract and
also rewritable representations, structural similar-
ity alone is insufficient as a metric for semantic
similarity.
Based on this observation, the primary goal of
this paper is to establish a method for extracting
implicit connections between mathematical for-
mulas and their names together with the descrip-
tions written in natural language text. This en-
ables keywords to be associated with the formu-
las and makes mathematical search more power-
ful. For example, it is easier for people searching
and retrieving mathematical concepts if they know
the name of the equation ?a2 + b2 = c2? is
the ?Pythagorean Theorem?. It could also make
mathematics more understandable and usable for
users.
While many studies have presented corefer-
ence relations among texts (Ponzetto and Poesio,
2009), no work has ever considered the corefer-
ence relations between formulas and texts. In this
paper, we use Wikipedia articles as a target cor-
pus. We chose Wikipedia for these reasons: (1)
Wikipedia uses a subset of TEX markup for math-
ematical formulas. That way, we can analyze the
content of these formulas using TEX expressions
rather than analyzing the images. (2) Wikipedia
provides a wealth of knowledge and the content
of Wikipedia is much cleaner than typical Web
pages, as explained in Giles (2005).
69
1.2 Related Work
Ponzetto and Poesio (2006) attempted to include
semantic information extracted from WordNet
and Wikipedia into their coreference resolution
model. Shnarch et al (2009) presented the ex-
traction of a large-scale rule base from Wikipedia
designed to cover a wide scope of the lexical
reference relations. Their rule base has compa-
rable performance with WordNet while provid-
ing largely complementary information. Yan et
al. (2009) proposed an unsupervised relation ex-
traction method for discovering and enhancing
relations associated with a specified concept in
Wikipedia. Their work combined deep linguis-
tic patterns extracted from Wikipedia with surface
patterns obtained from the Web to generate vari-
ous relations. The results of these studies showed
that Wikipedia is a knowledge-rich and promising
resource for extracting relations between repre-
sentative terms in text. However, these techniques
are not directly applicable to the coreference res-
olution between formulas and texts as we mention
in the next section.
1.3 Challenges
There are two key challenges in solving the coref-
erence relations between formulas and texts using
Wikipedia articles.
? First, formulas have unique structures such
as prior operators and nested functions. In
addition, features such as gender, plural, part
of speech, and proper name, are unavail-
able with formulas for coreference resolu-
tion. Therefore, we cannot apply standard
natural language processing methods to for-
mulas.
? Second, no labeled data are available for
the coreference relations between formu-
las and texts. This means we cannot ap-
ply commonly used machine learning-based
techniques without expensive human annota-
tions.
1.4 Our Approach and Key Contributions
In this paper, we present an approach, which
we call CDF (Concept Description Formula), for
mining coreference relations between mathemat-
ical Formulas and Concepts using Wikipedia ar-
ticles. In order to address the previously men-
tioned challenges, the proposed CDF approach is
featured as follows:
? First, we consider not only the concept-
formula pairs but extend the relation with de-
scriptions of the concept. Note that a ?con-
cept? in our study corresponds to a ?name? or
a ?title? of a formula, which is usually quite
short. By additionally considering words ex-
tracted from the descriptions, we have a bet-
ter chance of detecting keywords, such as
mathematical symbols, and function or vari-
able names, used in the equations.
? Second, we apply an unsupervised frame-
work in our approach. Initially, we extract
highly confident coreference pairs using sur-
face level text matching. Next, we collect
promising syntactic patterns from the de-
scriptions and then use the patterns to extract
coreference pairs. The process enables us to
deal with cases where there exist no common
words between the concepts and the formu-
las.
The remainder of this paper is organized as fol-
lows: In section 2, we present our method. We
then describe the experiments and results in sec-
tion 3. Section 4 concludes the paper and gives
avenues for future work.
2 Method
2.1 Overview of the Method
In this section, we first explain the terms used in
our approach. We then provide a framework of
our method and the functions of the main mod-
ules.
Given a set of Wikipedia articles as input, our
system outputs a list of formulas along with their
names and descriptions. Herein
? Concept: A concept C is a phrase that repre-
sents a name of a mathematical formula. In
Wikipedia, we extract candidate concepts as
noun phrases (NPs) that are either the titles of
70
Wikipedia articles, section headings, or writ-
ten in bold or italic. Additional NPs that con-
tain at least one content word are also consid-
ered.
? Description: A description D is a phrase
that describes the concept. In Wikipedia, de-
scriptions often follow a concept after the
verb ?be?.
? Formula: A formula F is a mathematical
formula. In Wikipedia extracted XML files,
formulas occur between the < math > and
< /math > tags. They are encoded in TEX
format.
? Candidate: A candidate is a triple of con-
cept, description and formula. Our system
will judge if the candidate is qualified, which
means the concept is related to the formula.
Figure 1 shows a section of a Wikipedia article
and the concepts, descriptions and formulas in this
section. Table 1 shows the extracted candidates.
Details of how to extract the concepts, descrip-
tions and formulas and how to form candidates are
described in the next sections.
Sine, cosine and tangent
The sine of an angle is the ratio of the length of the opposite side to the length of the hypotenuse. In our case
sin A= oppositehypotenuse=ah
Note that this ratio does not depend on size of the particular right triangle chosen, as long as it contains the angle A, since all such triangles are similar.
The cosine of an angle is the ratio of the length of the adjacent side to the length of the hypotenuse. In our case
cos A= adjacenthypotenuse=bh
The tangent of an angle is the ratio of the length of the opposite side to the length of the adjacent side (called so because it can be represented as a line segment tangent to the circle). In our case
tan A= oppositeadjacent =ab
The acronym "SOHCAHTOA" is a useful mnemonic for these ratios.
TITLE
PARAGRAPH
PARAGRAPH
PARAGRAPH
FORMULA
FORMULA
FORMULA
CONCEPT DESCRIPTION
DESCRIPTIONCONCEPT
DESCRIPTION
CONCEPT
Figure 1: Examples of extracted paragraphs
The framework of the system is shown in Fig-
ure 2. The system has four main modules.
? Text Preprocessor: processes Wikipedia ar-
ticles to extract CDF (Concept Description
Formula) candidates.
Input: Wikipedia articles
Preprocessor
Pattern Matching Text Matching
Output: equation's references
Pattern Generation
Concept Description FormulaThe sine of an angle the ratio of the length of the opposite side to the length of the hypotenuse a quadratic equation a polynomial equation of the second degree
sin A= oppositehypotenuse=ahax2?bx?c=0
Figure 2: Framework of the proposed approach
? Text Matching: extracts reliable and qual-
ified candidates using surface level text
matching.
? Pattern Generation: generates patterns
from qualified candidates.
? Pattern Matching: extends the candidate
list using the generated patterns.
2.2 Text Preprocessor
This module preprocesses the text of the
Wikipedia article to extract CDF candidates.
Based on the assumption that concepts, their de-
scriptions and formulas are in the same paragraph,
we split the text into paragraphs and select para-
graphs that contain at least one formula.
On these selected paragraphs, we run Sentence
Boundary Detector, Tokenizer and Parser from
OpenNLP tools. 1 Based on the parse trees, we
extract the noun phrases (NPs) and identify NPs
representing concepts or descriptions using the
definitions in Section 2.1.
Following the general idea in Shnarch et al
(2009), we use the ?Be-Comp? rule to identify the
description of a concept in the definition sentence.
In a sentence, we extract nominal complements of
the verb ?to be?, assign the NP that occurs after
the verb ?to be? as the description of the NP that
occurs before the verb. Note that some concepts
have descriptions while others do not.
1http://opennlp.sourceforge.net/
71
Table 1: Examples of candidates
Concept Description Formula
the sine of an angle the ratio of the length of the opposite side to sinA = oppositehypotenuse = ah
the length of the hypotenuse
the cosine of an angle the ratio of the length of the adjacent side to cosA = adjacenthypotenuse = bh
the length of the hypotenuse
a quadratic equation a polynomial equation of the second degree ax2 + bx+ c = 0
the quadratic formula x = ?b?
?
b2?4ac
2a
the complex number i i2 = ?1
the Cahen?Mellin integral e?y = 12pii
? c+i?
c?i? ?(s)y?s ds
The ?Be-Comp? rule can also identify if a for-
mula is related to the concept.
After that, we group each formula F in the
same paragraph with concept C and its descrip-
tion D to form a candidate (C, D, F ). Table 1
presents candidate examples. Because we only
choose paragraphs that contain at least one for-
mula, every concept has a formula attached to it.
In order to judge the correctness of candidates,
we use the text-matching module, described in the
next section.
2.3 Text Matching
In this step, we classify candidates using surface
text. Given a list of candidates of the form (C, D,
F ), this module judges if a candidate is qualified
by using the surface text in concept, description
and formula. Because many formulas share the
same variable names or function names (or part of
these names) with their concepts (e.g. the first two
candidates in Table 1), we filter these candidates
using surface text matching.
We define the similarity between concept C,
description D and formula F by the number of
overlapped words, as in Eq. 1.
sim(F,CD) = |TF ? TC|min{|TC|, |TF|}
+ |TF ? TD|min{|TD|, |TF|}(1)
TF , TC and TD are sets of words extracted from
F , C and D, respectively.
Candidates with sim(F,CD) no larger than a
threshold ?1 (1/3 in this study) are grouped into
the group Ctrue. The rest are filtered and stored in
C0. In this step, function words such as articles,
pronouns, conjunctions and so on in concepts and
descriptions are ignored. Common operators in
formulas are also converted to text, such as ?+?
?plus?, ??? ?minus?, ?\frac? ?divide?.
Using only concepts for text matching with for-
mulas might leave out various important relations.
For example, from the description of the first and
second formula in Table 1, we could extract the
variable names ?opposite?, ?adjacent? and ?hy-
potenuse?.
By adding the description, we could get a more
accurate judgment of whether the concept and
the formula are coreferent. In this case, we can
consider the concept, description and the formula
form a coreference chain.
After this step, we have two categories, Ctrue
and C0. Ctrue contains qualified candidates while
C0 contains candidates that cannot be determined
by text matching. The formulas in C0 have little
or no text relation with their concepts and descrip-
tions. Thus, we can only judge the correctness of
these candidates by using the text around the con-
cepts, descriptions and formulas. The surrounding
text can be formed into patterns and are generated
in the next step.
2.4 Pattern Generation
One difficulty in judging the correctness of a can-
didate is that the formula does not share any re-
lation with its concept and description. The third
candidate in Fig. 1 is an example. It should be
classified as a qualified instance but is left behind
in C0 after the ?text matching? step.
72
In this step, we use the qualified instances in
Ctrue to generate patterns. These patterns are used
in the next step to judge the candidates in C0. Pat-
terns are generated as follows. First, the concept,
description and formula are replaced by CONC,
DESC and FORM, respectively. We then simply
take the entire string between the first and the last
appearance of CONC, DESC and FORM.
Table 2 presents examples of patterns extracted
from group Ctrue.
Table 2: Examples of extracted patterns
Pattern
CONC is DESC: FORM
CONC is DESC. In our case FORM
CONC is DESC. So, ..., FORM
CONC FORM
CONC is denoted by FORM
CONC is given by ... FORM
CONC can be written as ... : FORM
FORM where CONC is DESC
FORM satisfies CONC
Using a window surrounding the concepts and
formulas often leads to exponential growth in pat-
terns, so we limit our patterns to those between
any concept C, description D or formula F .
The patterns we obtained above are exactly the
shortest paths from the C nodes to their F node in
the parse tree. Figure 3 presents examples of these
patterns in parse trees.
I
np
u
tp
: W
ikke
np
u
tp
: np
W d a
ikke
pp
rc np
kls Po M a
Figure 3: Examples of extracted patterns
2.5 Pattern Matching
In this step, we use patterns obtained from the
previous step to classify more candidates in C0.
We use the string distance between the patterns,
where candidates? patterns having a string dis-
tance to any of the patterns extracted in the previ-
ous step no larger than the threshold ?2 are added
into Ctrue.
3 Experiments
3.1 Data
We collected a total of 16,406 mathematical doc-
uments from the Wikipedia Mathematics Portal.
After the preprocessing step, we selected 72,084
paragraphs that contain at least one formula. From
these paragraphs, we extracted 931,716 candi-
dates.
Because no labeled data are available for use
in this task, we randomly chose 100 candidates:
60 candidates from Ctrue after the text matching
step, 20 candidates added to Ctrue after pattern
matching with ?2 = 0, and 20 candidates added
to Ctrue after pattern matching with ?2 = 0.25 for
our evaluation. These candidates were annotated
manually. The sizes of the sample sets for human
judgment (60, 20 and 20) were selected approx-
imately proportional to the sizes of the obtained
candidate sets.
3.2 Results
After the text matching step, we obtained 138,285
qualified candidates in the Ctrue group and
793,431 candidates in C0. In Ctrue, we had 6,129
different patterns. Applying these patterns to C0
by exact pattern matching (?2 = 0), we obtained a
further 34,148 qualified candidates. We obtained
an additional 30,337 qualified candidates when
we increased the threshold ?2 to 0.25.
For comparison, we built a baseline system.
The baseline automatically groups nearest for-
mula and concept. It had 51 correctly qualified
candidates. The results?displayed in Table 3
and depicted in Figure 4?show that our proposed
method is significantly better than the baseline in
terms of accuracy.
As we can see from the results, when we lower
the threshold, more candidates are added to Ctrue,
which means we get more formulas and formula
names; but it also lowers the accuracy. Although
the performance is not as high as other existing
coreference resolution techniques, the proposed
73
Table 3: Results of the system
Module No. correct/ No. of
total CDF found
Text Matching 41 / 60 138,285
Pattern Matching 52 / 80 172,433
?2 = 0
Pattern Matching 56 / 100 202,270
?2 = 0.25
method is a promising starting point for solving
coreference relations between formulas and sur-
rounding text.
4 Conclusions
In this paper, we discuss the problem of discov-
ering coreference relations between formulas and
the surrounding texts. Although we could only
use a small number of annotated data for the eval-
uation in this paper, our preliminary experimental
results showed that our approach based on sur-
face text-based matching between formulas and
text, as well as patterns representing relationships
between them showed promise for mining math-
ematical knowledge from Wikipedia. Since this
is the first attempt to extract coreference rela-
tions between formulas and texts, there is room
for further improvement. Possible improvements
include: (1) using advanced technology for pat-
tern matching to improve the coverage of the re-
sult and (2) expanding the work by mining knowl-
edge from the Web.
References
Eyal Shnarch, Libby Barak and Ido Dagan. 2009.
Extracting Lexical Reference Rules from Wikipedia
Proceedings of the 47th Annual Meeting of the ACL
and the 4th IJCNLP of the AFNLP, pages 450?458
Yulan Yan, Naoaki Okazaki, Yutaka Matsuo, Zhenglu
Yang and Mitsuru Ishizuka. 2009. Unsupervised
Relation Extraction by Mining Wikipedia Texts Us-
ing Information from the Web Proceedings of the
47th Annual Meeting of the ACL and the 4th IJC-
NLP of the AFNLP, pages 1021?1029
Simone Paolo Ponzetto and Massimo Poesio. 2009.
State-of-the-art NLP Approaches to Coreference
Input: uWiked
a uunret: uWikedtc
a uunret: uWikedtclsP
oc
oP
Pc
PP
Mc
MP
hc gWWTr Wx
Input: uWiked
a uunret: uWikedtc
a uunret: uWikedtclsP
Occccc
Oscccc
Oocccc
OMcccc
Oqcccc
sccccc
sscccc
'fltfGt???t
GfTe?
? ?n?ken
Figure 4: Results of the system
Resolution: Theory and Practical Recipes Tutorial
Abstracts of ACL-IJCNLP 2009, page 6
Minh Nghiem, Keisuke Yokoi and Akiko Aizawa.
2009. Enhancing Mathematical Search with Names
of Formulas The Workshop on E-Inclusion in Math-
ematics and Science 2009, pages 22?25
Keisuke Yokoi and Akiko Aizawa. 2009. An Ap-
proach to Similarity Search for Mathematical Ex-
pressions using MathML 2nd workshop Towards a
Digital Mathematics Library, pages 27?35
Hui Siu Cheung Muhammad Adeel and Sikandar
Hayat Khiyal. 2008. Math Go! Prototype of a
Content Based Mathematical Formula Search En-
gine Journal of Theoretical and Applied Informa-
tion Technology, Vol. 4, No. 10, pages 1002?1012
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting Semantic Role Labeling, WordNet and
Wikipedia for Coreference Resolution In Proceed-
ings of HLT-NAACL-06, pages 192?199
Jim Giles. 2005. Internet Encyclopaedias Go Head
to Head Nature Volume: 438, Issue: 7070, pages
900?901
World Wide Web Consortium. Mathematical Markup
Language (MathML) version 2.0 (second edition)
http://www.w3.org/TR/MathML2/
74
Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 140?148,
Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational Linguistics
Relation Annotation for Understanding Research Papers
Yuka Tateisi? Yo Shidahara? Yusuke Miyao? Akiko Aizawa?
?National Institute of Informatics, Tokyo, Japan
{yucca,yusuke,aizawa}@nii.ac.jp
?Freelance Annotator
yo.shidahara@gmail.com
Abstract
We describe a new annotation scheme for
formalizing relation structures in research
papers. The scheme has been developed
through the investigation of computer sci-
ence papers. Using the scheme, we are
building a Japanese corpus to help develop
information extraction systems for digital
libraries. We report on the outline of the
annotation scheme and on annotation ex-
periments conducted on research abstracts
from the IPSJ Journal.
1 Introduction
Present day researchers need services for search-
ing research papers. Search engines and pub-
lishing companies provide specialized search ser-
vices, such as Google Scholar, Microsoft Aca-
demic Search, and Science Direct. Academic so-
cieties provide archives of journal articles and/or
conference proceedings such as the ACL Anthol-
ogy. These services focus on simple keyword-
based searches as well as extralinguistic relations
among research papers, authors, and research top-
ics. However, because contemporary research is
becoming increasingly complicated and interre-
lated, intelligent content-based search systems are
desired (Banchs, 2012). A typical query in compu-
tational linguistics could be what tasks have CRFs
been used for?, which includes the elements of
a typical schema for searching research papers;
researchers want to find relationships between a
technique and its applications (Gupta and Man-
ning, 2011). Answers to this query can be found
in various forms in published papers, for example,
(1) CRF-based POS tagging has achieved state-of-
the-art accuracy.
(2) CRFs have been successfully applied to se-
quence labeling problems including POS tagging
and named entity recognition.
(3) We apply feature reduction to CRFs and show
its effectiveness in POS tagging.
(4) This study proposes a new method for the ef-
ficient training of CRFs. The proposed method is
evaluated for POS tagging tasks.
Note that the same semantic relation, i.e., the
use of CRFs for POS tagging, is expressed by var-
ious syntactic constructs: internal structures of the
phrase in (1), clause-level structures in (2), inter-
clause structures in (3), and discourse-level struc-
tures in (4). This implies that an integrated frame-
work is required to represent semantic relations for
phrase-level, clause-level, inter-clause level, and
discourse-level structures. Another interesting fact
is that we can recognize various fragments of in-
formation from single texts. For example, from
sentence (1), we can identify CRF is applied to
POS tagging, state-of-the-art accuracy is achieved
for POS tagging, and CRFs achieve high POS tag-
ging accuracy, all of which is valuable content for
different search requests. This indicates that we
need a framework that can cover (almost) all con-
tent in a text.
In this paper we describe a new annotation
scheme for formalizing typical schemas for repre-
senting relations among concepts in research pa-
pers, such as techniques, resources, and effects.
Our study aims to establish a framework for rep-
resenting the semantics of research papers to help
construct intelligent search systems. In particular,
we focus on the formalization of typical schemas
that we believe exemplify common query charac-
teristics.
From the above observations, we have de-
veloped the following criteria for our proposed
framework: use the same scheme for annotating
contents in all levels of linguistic structures, an-
notate (almost) all contents presented in texts, and
capture relations necessary for surveying research
papers. We investigated 71 computer science ab-
stracts (498 sentences) and defined an annotation
140
scheme comprising 16 types of semantic relations.
Computer science is particularly suitable for our
purpose because it is primarily concerned with ab-
stract concepts rather than concrete entities, which
are typically the primary focus of empirical sci-
ences such as physics and biology. In addition,
computer and computational methods can be ap-
plied to an extraordinarily wide range of top-
ics; computer science papers might discuss a bus
timetable (for automatic optimization), a person?s
palm (as a device for projecting images), or look-
ing over another person!Gs shoulder (to obtain pass-
words). Therefore, to annotate all computer sci-
ence papers, we cannot develop predefined entity
ontologies, which is the typical approach taken in
biomedical text mining (Kim et al, 2011).
However, most computer science papers have
characteristic schemata: the papers describe a
problem, postulate a method, apply the method to
the problem using particular data or devices, and
perform experiments to evaluate the method. The
typical schemata clearly represent the structure of
interests in this research field. Therefore, we can
focus on typical schemata, such as application of
a method to a problem and evaluation of a method
for a task. As we will demonstrate in this paper,
the proposed annotation scheme can cover almost
all content, from phrase levels to discourse levels,
in computer science papers.
Note that this does not necessarily mean that our
framework can only be applied to computer sci-
ence literature. The characteristics of the schemata
described above are universal in contemporary sci-
ence and engineering, and many other activities in
human society. Thus, the framework presented in
this study can be viewed as a starting point for re-
search focusing on representative schemata of hu-
man activities.
2 Related Work
Traditionally, research on searching research pa-
pers has focused more on the social aspects of
papers and their authors, such as citation links
and co-authorship analysis implemented in the
aforementioned services. Recently, research on
content-based analysis of research papers has been
emerging.
For example, methods of document zoning have
been proposed for research papers in biomedicine
(Mizuta et al, 2006; Agarwal and Yu, 2009; Li-
akata et al, 2010; Guo et al, 2011; Varga et
al., 2012), and chemistry and computational lin-
guistics (Teufel et al, 2009). Zoning provides
a sentence-based information structure of papers
to help identify the components such as the pro-
posed method and the results obtained in the study.
As such, zoning can narrow down the sections of
a paper in which the answer to a query can be
found. However, zoning alone cannot always cap-
ture the relation between the concepts described in
the sections as it focuses on relation at a sentence
level. For example, the examples (1), (2), (3) in the
previous section require intra-sentence analysis to
capture the relation between CRF and POS tag-
ging. Our annotation scheme, which can be seen
as conplementary to zoning, attempts to provide
a structure for capturing the relationship between
concepts at a finer-grained level than a sentence.
Establishing semantic relations among scien-
tific papers has also been studied. For example,
the ACL Anthology Searchbench (Scha?fer et al,
2011) provides querying by predicate-argument
relations. The system accepts specifications of
subject, predicate, and object, and searches for
texts that semantically match the query using the
results from an HPSG parser. It can also search
by topics automatically extracted from the papers.
Gupta and Manning (2011) proposed a method for
extracting Focus, Domain, and Technique from pa-
pers in the ACL anthology: Focus is a research
article?s main contribution, Domain is an applica-
tion domain, and Technique is a method or a tool
used to achieve the Focus. The change in these as-
pects over time is traced to measure the influence
of research communities on each other. Fukuda et
al. (2012) developed a method of technical trend
analysis that can be applied to both patent appli-
cations and academic papers, using the distribu-
tion of named entities. However, as processes and
functions are key concepts in computer science,
elements are often described in a unit with its own
internal structures which include data, systems,
and other entities as substructures. Thus, tech-
nical concepts such as technique cannot be cap-
tured fully by extracting named entities. Gupta
and Manning (2011) analyzed the internal struc-
tures of concepts syntactically using a dependency
parser, but did not further investigate the structure
semantically.
In addition to the methodological aspects of re-
search, i.e., what techniques are applied to what
domain, a research paper can include other infor-
141
mation that we also want to capture, such as how
the author evaluates current systems and methods
or the previous efforts of others. An attempt to
identify the evaluation and other meta-aspects of
scientific papers was made by Thompson et al
(2011), which, on top of the biomedical events
annotated in the GENIA event corpus (Kim et
al., 2008), annotated meta-knowledge such as the
certainty level of the author, polarity (positive?
negative), and manner (strong?weak) of events, as
well as source (whether the event is attributed to
the current study or previous studies), along with
the clue mentioned in the text. For in-domain
relations within and between the events, they re-
lied on the underlying GENIA annotation, which
maps events and their participants to a subset of
Gene Ontology (The Gene Ontology Consortium,
2000), a standard ontology in genome science.
We cannot assume the existence of standard do-
main ontology in the variety of domains to which
computer systems are applied, as was mentioned
in Section 1. On the other hand, using domain-
general linguistic frameworks, such as FrameNet
(Ruppenhofer et al, 2006) or the Lexical Concep-
tual Structure (Jackendoff, 1990) is also not sat-
isfactory for our purpose. These frameworks at-
tempt to identify the relations lexicalized by verbs
and their case arguments; however, they do not
consider discourse or other levels of linguistic rep-
resentation. In addition, relying on a linguistic the-
ory requires that annotators understand linguistics.
Most computer scientists, the best candidates for
performing the annotation task, would not have the
necessary knowledge of linguistics and would re-
quire training, which would increase costs for cor-
pus annotation.
3 Annotation Scheme
The principle is to employ a uniform structure to
represent semantic relations in scientific papers
in phrase-level, clause-level, inter-clause level,
and discourse-level structures. For this purpose,
a bottom-up strategy that identifies relations be-
tween the entities mentioned is used. This strat-
egy is similar to dependency parsing/annotation,
which identifies the relations between constituents
to find the overall structure of sentences.
We did not want the relations to be uncondi-
tionally concrete and domain-specific, because, as
mentioned in the previous section, new concepts
and relations that may not be expressed by pre-
In this paper, we propose a novel strategy for
parallel preconditioning of large scale linear
systems by means of a two-level approximate
inverse technique with AISM method. Accord-
ing to the numerical results on an origin 2400 by
using MPI, the proposed parallel technique of
computing the approximate inverse makes the
speedup of about 136.72 times with 16 proces-
sors.
Figure 1: Sample Abstract
defined (concrete, domain-specific) concepts and
relations may be created. For the same reason,
we did not set specific entity types on the basis of
domain ontology. We simply classified entities as
?general object,? ?specific object,? and ?measure-
ment.?
To illustrate our scheme, consider the two-
sentence abstract1 shown in Figure 12.
In the first sentence, we can read that a method
called two-level approximate inverse is used for
parallel preconditioning (1), the preconditioning
is applied to large-scale linear systems, the AISM
method is a subcomponent or a substage of the
two-level technique, and the author claims that the
use of two-level approximate inverse is a novel
strategy.
In the second sentence, we can read that the
author has conducted a numerical experiment,
the experiment was conducted on an origin 2400
(a computer system), message Passing Interface
(MPI, a standardized method for message passing)
was used in the experiment, the proposed parallel
technique was 136.72 times quicker than existing
methods, and the speedup was achieved using 16
processors.
In addition, by comparing the two sentences, we
can determine that the proposed parallel technique
in the second sentence refers to the parallel pre-
conditioning using two-level approximate inverse
mentioned in the first sentence. Consequently, we
can infer the author?s claim that the parallel pre-
conditioning using two-level approximate inverse
achieved 136.72 times speedup.
We define binary relations including
APPLY TO(A, B) (A method A is applied
to achieve the purpose B or used for do-
ing B), EVALUATE(A, B) (A is evaluated as
1Linjie Zhang, Kentaro Moriya and Takashi Nodera.
2008. Two-level Parallel Computation for Approximate In-
verse with AISM Method. IPSJ Journal, 48 (6): 2164-2168.
2Although the annotation was done for abstracts in
Japanese, we present examples in English except where we
discuss issues that we believe are specific to Japanese.
142
APPLY TO(two-level approximate inverse, parallel preconditioning)
APPLY TO(parallel preconditioning, large scale linear systems)
SUBCONCEPT(AISM method, two-level approximate inverse)
EVALUATE(two-level approximate inverse, novel)
RESULT(numerical results, 136.72 times speedup)
CONDITION(origin 2400, 136.72 times speedup)
APPLY TO(MPI, numerical results)
EVALUATE(the proposed parallel technique, 136.72 times speedup)
CONDITION(16 processors, 136.72 times speedup)
EQUIVALENCE(the proposed parallel technique, two-level approximate inverse)
Figure 2: Relations Found in the Sentences in Figure 1
B), SUBCONCEPT(A, B) (A is a part of B),
RESULT(A, B) (The result of experiment A is B),
CONDITION(A, B) (The condition A holds in
situation B), and EQUIVALENCE(A, B) (A and
B refer to the same entity), with which we can
express the relations mentioned in the example, as
shown in Figure 2.
Note that it is the use of two-level approximate
inverse for parallel preconditioning(A) that the au-
thor claims to be novel. However, the relation in A
is already represented by the first APPLY TO rela-
tion. Consequently, it is sufficient to annotate the
EVALUATE relation between two-level approxi-
mate inverse and novel. This is approximately
equivalent to paraphrasing the use of two-level ap-
proximate inverse for parallel preconditioning is
novel as two-level approximate inverse used for
parallel preconditioning is novel. The same holds
for the equivalence relation involving the proposed
method.
Expressing the content as the set of relations fa-
cilitates discovery of a concept that plays a par-
ticular role in the work. For example, if a reader
wants to know the method for achieving paral-
lel preconditioning, X, which satisfies the relation
APPLY TO(X, parallel preconditioning) must be
searched for. By using the APPLY TO relations
mentioned in Figure 2 and inference on an is-a re-
lation expressed by the SUBCONCEPT, we can ob-
tain the result that AISM method is used for paral-
lel preconditioning.
After a series of trial annotations on 71 abstracts
from the IPSJ Journal (a monthly peer-reviewed
journal published by the Information Processing
Society of Japan), the following tag set was fixed.
The annotation was conducted by the two of the
authors of this paper.
3.1 Entity and Relation Types
The current tag set has 16 relation types and three
entity types. An entity is whatever can be an argu-
Type Definition Example
OBJECT the name of concrete entities such as
a system, a person, and a company
Origin
2400, SGI
MEASURE value, measurement, necessity, obli-
gation, expectation, and possibility
novel,
136.72
TERM any other
Table 1: Entity Tags
ment or a participant in a relation. Entity types
are OBJECT, MEASURE, or TERM, as shown in
Table 1. Note that, unlike most schemes where
the term entity refers to a nominal (named entity),
in our scheme, almost all syntactic types of con-
tent words can be an entity, including numbers,
verbs, adjectives, adverbs, and even some auxil-
iaries. The 16 types of relations are shown in Ta-
ble 2. They are binary relations are directed from
A to B.
All relations except EVALUATE COMPARE, and
ATTRIBUTE can hold between any types of en-
tity. EVALUATE and COMPARE relations hold
between an entity (of any type) and an entity
of the MEASURE type. The entities involved
in an ATTRIBUTE relation must not be of the
MEASURE type.
The INPUT and OUTPUT relations were intro-
duced to deal with the distinction between the data
and method used in computer systems. We ex-
tend the use of the scheme to annotate the in-
ner structure of sentences and predicates, by es-
tablishing the relations between verbs and their
case elements. For example, in automatically
generated test data, obviously test data is an
output of the action of generate, and automati-
cally is the manner of generation. We annotate
the test data as an OUTPUT and automatically
as an ATTRIBUTE of generate. In another ex-
ample, a protocol that combines biometrics and
zero-knowledge proof, the protocol is the product
of an action of combining biometrics and zero-
143
Type Definition Example
APPLY TO(A, B) A method A is applied to achieve the purpose B or used for
conducting B
CRFA-based taggerB
RESULT(A, B) A results in B in the sense that B is either an experimental
result, a logical conclusion, or a side effect of A
experimentA shows the increaseB in F-
score compared to the baseline
PERFORM(A, B) A is the agent of an intentional action B a frustrated playerA of a gameB
INPUT(A, B) A is the input of a system or a process B, A is something
obtained for B
corpusA for trainingB
OUTPUT(A, B) A is the output of a system or a processB, A is something
generated from B
an imagea displayedB on a palm
TARGET(A, B) Ais the target of an action B, which does not suffer alteration to driveB a busA
ORIGIN(A, B) A is the starting point of action B to driveB from ShinjukuA
DESTINATION(A, B) A is the ending point of action B an image displayedB on a palmA
CONDITION(A, B) The condition A holds in situation B, e.g, time, location, ex-
perimental condition
a surveyB conducted in Indiaa
ATTRIBUTE(A, B) A is an attribute or a characteristic of B accuracyA of the taggerB
STATE(A, B) A is the sentiment of a person B other than the author, e.g. a
user of a computer system or a player of a game
a frustratedA playerB of a game
EVALUATE(A, B) A is evaluated as B in comparison to C experiment shows an increaseB
COMPARE(C, B) in F?scoreA compared to the baselineC
SUBCONCEPT(A, B) A is-a, or is a part-of B a corpusA such as PTBa
EQUIVALENCE(A, B) terms A and B refer to the same entity: definition, abbrevia-
tion, or coreference
DoSB (denial ? of ? serviceA) attack
SPLIT(A, B) a term is split by parenthesical expressions into A and B DoSB (denial-of-service) attackA
Table 2: Relation Tags
knowledge proof. Therefore, both biometrics and
zero-knowledge proof are annotated as INPUTs of
combines, and protocol is annotated as OUTPUT
of combines. This scheme is not only used for
computer-related verbs, but is further extended
to any verb phrases or phrases with nominalized
verbs. In change in a situation, situation is an-
notated as both INPUT and OUTPUT of change.
It is as if we regard change as a machine that
changes something, and when we input a situa-
tion, the change-machine processes it and output
a different situation. Similarly, in evolution of mo-
bile phones, mobile phones is annotated as both
INPUT and OUTPUT of evolution. Here we re-
gard evolution as a machine, and when we input
(old-style) mobile phones, the evolution-machine
processes them and outputs (new-style) mobile
phones. We have found that a wide variety of pred-
icates can be interpreted using these relations.
3.2 Other Features
Although we aim to annotate all possible relations
mentioned, some conventions are introduced to re-
duce the workload.
First, we do not annotate the structure within
entities. No nested entities are allowed, and com-
pound words are treated as a single word. In ad-
dition, polarity (negation) is not expressed as a re-
lation but as a part of an entity. We assume that
the internal structure of entities can be analyzed
by mechanisms such as technical term recognition.
On the other hand, nested and crossed relations are
allowed.
Second, we do not annotate words that indicate
the existence of relations. This is because the re-
lations are usually indicated by case markers and
punctuation 3 and marking them up was found to
be a considerable mental workload. In addition,
words and phrases that directly represent the re-
lations themselves are not annotated as entities.
For example, in CG iteration was applied to the
problem, we directly CG relation and the problem
directly with APPLY TO and skip the phrase was
applied to.
Third, relations other than EQUIVALENCE and
SUBCONCEPT are annotated within a sentence.
We assume that the discourse-level relation can be
inferred by the composition of relations.
In addition, the annotation of frequent verbs and
their case elements was examined in the trial pro-
cess. Verbs were classified, according to the pat-
tern of the annotated relation with the case ele-
ments. For example, verbs semantically similar to
assemble and compile form a class. The semantic
role of the direct object of these verbs varies by
context. For example, the materials in phrases like
compile source codes or the product in phrases like
3This is in the case with Japanese. In languages such as
English, there may be no trigger words, as the semantic rela-
tions are often expressed by the structure of sentences.
144
compile the driver from the source codes. In our
scheme, the former is the INPUT of the verb, and
the latter is the OUTPUT of the verb. Another ex-
ample is the class of verbs that includes learn and
obtain. The direct object (what is learned) is the
INPUT to the system but is also the result or an
output of the learning process. In such cases, we
decided that both INPUT and OUTPUT should be
annotated between the verb and its object.
Other details of annotation fixed in the process
of trial annotation include:
1) The span of entities, which is determined to be
the longest possible sequences delimited by case
suffix (-ga,-wo, etc.) in the case of nominals and to
separate the -suru suffix of verbs and the -da suffix
of adjectives but retain other conjugation suffixes;
2) How to annotate evaluation sentences involv-
ing nouns derived from adjectives that imply eval-
uation and measurement, such as necessity, diffi-
culty, and length. The initial agreement was that
we would consider that they lose MEASURE-ness
when nominalized; however, with the similarity of
Japanese expressions hitsuyou/mondai de aru (is
necessary/problematic) and hitsuyou/mondai ga
aru(there is a necessity/problem), there was con-
fusion about which word should be the MEASURE
argument necessary for the EVALUATE relation.
It was determined that, for example, in hit-
suyou/mondai de aru, de aru, a copula, is ig-
nored and hitsuyou/mondai is the MEASURE. In
hitsuyou/mondai ga aru, aru is the MEASURE;
3) How to annotate phrases like the tagger was
better in precision, where it can be understood that
the system is evaluated as being better in precision.
While what is actually measured in the evaluation
process described in the paper is the precision (an
attribute) of the tagger and the sentence has almost
the same meaning as the tagger?s precision was
better, the surface (syntactic) subject of is better
is the tagger. This can lead to two possibilities
for the target of the EVALUATE relation. We de-
cided that the EVALUATE relation holds between
precision and better, and the ATTRIBUTE relation
holds between precision and tagger, as illustrated
in Figure 3.
A set of annotation guidelines was compiled as
the result of the trial annotation, including the clas-
sifications and the pattern of annotation on fre-
quent verbs and their arguments.
Figure 3: Annotation of the tagger was better in
precision
Entity Relation
Conunt % Conunt %
Total 1895 100.0 Total 2269 100.0
OK 1658 87.5 OK 1110 48.9
Type 56 3.0 Type 250 11.0
Span 67 3.5 Direction 6 0.3
Direction+Type 106 4.7
None 114 6.0 None 797 35.1
Table 3: Tag Counts
4 Annotation Experiment
We conducted an experiment on another 30 ab-
stracts (197 sentences) from the IPSJ Journal. The
two annotators who participated in the develop-
ment of the guidelines annotated the abstracts in-
dependently, and inter-annotator discrepancy was
checked. The annotation was performed man-
ually using the brat annotation tool(Stenetorp et
al., 2012). No automatic preprocessing was per-
formed. Figure 4 shows the annotation results for
the abstract shown in Figure 1. The 30 pairs of an-
notation results were aligned automatically; The
results are shown in Tables 3, 4, and 5.
Table 3 shows the matches between the two
annotators. ?Total? denotes the count of enti-
ties/relations that at least one annotator found,
?OK? denotes complete matches, ?Type? denotes
cases where two annotations on the same span
have different entity/relation types, ?Span? de-
notes entities where two annotations partially
overlap, ?Direction? denotes the count of relations
where (only) the direction is different, and ?Direc-
tion+Type?denotes relations where the same pair
of entities were in different types of relation and
in opposite directions, and ?None? denotes cases
where no counterpart was found in the other re-
sult.
Tables 4 and 5 are the confusion matrices for
entity type and relation type, respectively. The
differences in the span and direction are ignored.
Agreement in F-score calculated in the same man-
ner as in Brants (2000) for each relation is shown
in column F, with the overall (micro-average) F-
score shown in the bottom row of column F.
If we assume the number of cases that none of
145
Figure 4: Annotation Results with brat
TERM OBJECT MEASURE NONE Total F(%)
TERM 1458 2 38 14 1512 94.9
OBJECT 0 17 0 0 17 94.4
MEASURE 28 0 238 18 284 83.8
None 74 0 8 X 82
Total 1560 19 284 32 93.0
Table 4: Confusion Matrix for Entity
the annotators recognized (the value of the cell
X in the tables) to be zero, the observed agree-
ment and Cohen?s ? coefficient are 90.3% and
70.0% for entities, and 49.3% and 43.5% for re-
lations, respectively. If we ignore the count for the
cases where one annotator did not recognize the
entity/relation (?None? rows and columns in the
tables), the observed agreement and ? are 96.1%
and 89.3% for entities, and 76.1% and 74.3% for
relations, respectively. The latter statistics indi-
cate the agreement on types for entities/relations
that both annotators recognized.
These results show that entity annotation was
consistent between the annotators but the agree-
ment for relation annotation varied, depending on
the relation type. Table 5 shows that agreement
for DESTINATION, ORIGIN, EVALUATE, and
SPLIT was reasonably high, but was low for
CONDITION and TARGET. The rise in agreement
(simple and ?) by excluding cases where only one
annotator recognized the relation indicate that the
problem is recognition, rather than classification,
of relations4.
From the investigation of the annotated text, the
following was found:
(1) ATTRIBUTE/CONDITION decision was in-
consistent in phrases involving EVALUATE rela-
tion, such as the disk space is smaller for the im-
age (Figure 5). The EVALUATE relation between
the disk space and smaller was agreed; however,
the two annotators recognized different relations
between the image and other words. One annota-
4The same observation was true for entities
tor recognized the ATTRIBUTE relation between
the disk space and the image (?the disk space as a
feature of the image is smaller?). The other recog-
nized the CONDITION relation between the image
and smaller (?the disk space is smaller in the case
of the image?).
(2) We were not in complete agreement about
skipping phrases that directly represent a relation.
The expressions to be skipped in the 71 trial ab-
stracts were listed in the guidelines; however, it is
difficult to exhaust all such expressions.
(3) In the case of some verbs, an argument can
be INPUT and OUTPUT simultaneously (Section
3.1). We agreed that an object that undergoes alter-
ation in a process should be tagged as both INPUT
and OUTPUT but one that does not undergo al-
teration or which is just moved is the TARGET.
Conflicts occurred for verbs that denote preven-
tion of some situations such as prevent, avoid, and
suppress, as illustrated in Figure 6. One annota-
tor claimed that the possibility of DoS attacks is
reduced to zero; hence the argument of the verb
should be annotated with INPUT and OUTPUT.
The other claims that since the DoS attack itself
does not change, it is a TARGET.
(4) In a coordination expression, logical inference
may be implicitly stated. For example, in it re-
quires the linguistic knowledge and is costly, the
reason for costly is likely to be the need for lin-
guistic knowledge, i.e., employment of an expert
linguist. However, the relation is not readily ap-
parent. We wanted to capture the relation in such
cases, but the disagreement shows that it is diffi-
cult to judge such a relation consistently.
(5) The decision on whether to split expressions
like XX dekiru and XX kanou (can/able to XX) was
also problematic. The guideline was to split them.
This contradicts the decision for the compound
words in general that we do not split them; how-
ever, we determined that dekiru/kanou cases had
146
APP ATT COMP COND DEST EQU EVAL IN ORIG OUT PER RES SPL STA SUB TAR None Total F(%)
APPLY TO 136 9 0 2 1 1 2 10 1 0 0 3 0 0 1 0 65 231 53.0
ATTRIBUTE 14 154 0 19 6 0 9 5 1 0 7 1 0 0 3 0 28 247 59.7
COMPARE 0 0 6 1 0 0 0 0 0 0 0 0 0 0 0 0 4 11 54.5
CONDITION 4 11 1 77 0 0 1 4 0 0 0 5 0 0 0 0 49 152 48.7
DESTINATION 6 0 0 0 39 0 0 0 0 1 0 0 0 0 0 0 4 50 77.2
EQUIVALENCE 4 1 0 1 0 54 0 0 0 0 0 0 0 0 4 0 23 87 60.0
EVALUATE 0 11 0 0 0 0 215 3 0 9 0 0 0 0 0 1 41 280 76.1
INPUT 12 2 0 0 0 1 4 96 0 11 0 0 0 0 0 9 15 150 58.7
ORIGIN 0 0 0 0 0 0 0 0 16 0 0 0 0 0 0 0 2 18 78.0
OUTPUT 2 1 0 3 0 0 4 23 0 141 0 0 0 0 0 18 37 229 56.5
PERFORM 1 0 0 0 0 0 0 0 0 0 19 0 0 0 0 0 2 22 74.5
RESULT 8 1 0 0 0 0 1 1 0 0 0 38 0 0 0 0 22 71 54.3
SPLIT 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 80.0
STATE 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
SUBCONCEPT 14 10 0 3 0 4 5 0 0 2 0 0 0 0 81 0 34 153 58.1
TARGET 6 2 1 3 2 0 7 12 0 14 1 0 0 0 0 42 6 96 47.7
None 75 67 3 55 3 33 37 23 5 92 2 22 1 0 37 10 X 465
Total 282 269 11 164 51 93 285 177 23 270 29 69 3 0 126 80 332 59.8
Table 5: Confusion Matrix for Relation
Figure 5: ATTRIBUTE/CONDITION Disagreement
Figure 6: INPUT/OUTPUT/TARGET Disagreement
to be exceptions because the possibility of XX is
expressed by dekiru/kanou and it seemed natural
to relate XX and dekiru/kanou with EVALUATE.
Unfortunately, confusion about splitting them re-
mains.
5 Conclusions
We set up a scheme to annotate the content of re-
search papers comprehensively. Sixteen semantic
relations were defined, and guidelines for anno-
tating semantic relations between concepts using
the relations were established. The experimen-
tal results on 30 abstracts show that fairly good
agreement was achieved, and that while entity-
and relation-type determination can be performed
consistently, determining whether a relation exists
between particular pairs of entities remains prob-
lematic. We also found several discrepancy pat-
terns that should be resolved and included in a fu-
ture revision of the guidelines.
Traditionally, in semantic annotation of texts
in the science/engineering domains, corpus cre-
ators focus on specific types of entities or events
in which they are interested. On the other hand,
we did not assume such specific types of entities
or events, and we attempted to design a scheme
that annotates more general relations in computer
science/engineering domain.
Although the annotation is conducted for com-
puter science abstracts in Japanese, we believe the
scheme can be used for other languages, or for
the broader science/engineering domains. The an-
notated corpus can provide data for constructing
comprehensive semantic relation extraction sys-
tems. This would be challenging but worthwhile
since such systems are in great demand. Such
relation extraction systems will be the basis for
content-based retrieval and other applications, in-
cluding paraphrasing and translation.
The abstracts annotated in the course of the ex-
periment have been cleaned up and are available
on request. We are planning to increase the vol-
ume and make the corpus widely available.
In the future, we will assess machine-learning
performance and incorporate the relation extrac-
tion mechanisms into search systems. Comparison
of the annotated structure and the structures that
can be given by existing semantic theories could
be an interesting theoretical subject for future re-
search.
Acknowledgments
This study was partially supported by the Japan
Ministry of Education, Culture, Sports, Science
and Technology Grant-in-Aid for Scientific Re-
search (B) No. 22300031.
147
References
Shashank Agarwal and Hong Yu. 2009. Automatically
classifying sentences in full-text biomedical articles
into introduction, methods, results and discussion.
Bioinformatics, 25(23):3174?3180.
Rafael E. Banchs, editor. 2012. Proceedings of the
ACL-2012 Special Workshop on Rediscovering 50
Years of Discoveries. Association for Computational
Linguistics.
Thorsten Brants. 2000. Inter-annotator agreement for
a German newspaper corpus. In Proceedings of the
Second International Conference on Language Re-
sources and Evaluation.
Satoshi Fukuda, Hidetsugu Nanba, and Toshiyuki
Takezawa. 2012. Extraction and visualization of
technical trend information from research papers
and patents. In Proceedings of the 1st International
Workshop on Mining Scientific Publications.
Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011. A weakly-supervised approach to argumen-
tative zoning of scientific documents. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 273?283.
Sonal Gupta and Christopher D Manning. 2011. An-
alyzing the dynamics of research by extracting key
aspects of scientific papers. In Proceedings of 5th
IJCNLP.
Ray Jackendoff. 1990. Semantic Structures. The MIT
Press.
Jin-Dong Kim, Tomoko Ohta, and Jun ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011.
Overview of bionlp shared task 2011. In Proceed-
ings of BioNLP Shared Task 2011 Workshop, pages
1?6.
Maria Liakata, Simone Teufel, Advaith Siddharthan,
and Colin Batchelor. 2010. Corpora for concep-
tualisation and zoning of scientific papers. In Pro-
ceedings of LREC 2010.
Yoko Mizuta, Anna Korhonen, Tony Mullen, and Nigel
Collier. 2006. Zone analysis in biology articles as a
basis for information extraction. International Jour-
nal of Medical Informatics, 75(6):468?487.
Josef Ruppenhofer, Michael Ellsworth, Miriam R.L.
Petruck, Christopher R. Johnson, and Jan Schef-
fczyk. 2006. FrameNet II: Extended Theory and
Practice. International Computer Science Institute.
Ulrich Scha?fer, Bernd Kiefer, Christian Spurk, Jo?rg
Steffen, and Rui Wang. 2011. The ACL anthology
searchbench. In Proceedings of the ACL-HLT 2011
System Demonstrations, pages 7?13.
Pontus Stenetorp, Sampo Pyysalo, Goran Topic?,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a web-based tool for NLP-assisted
text annotation. In Proceedings of the Demonstra-
tions Session at EACL.
Simone Teufel, Advaith Siddharthan, and Colin Batch-
elor. 2009. Towards discipline-independent ar-
gumentative zoning: evidence from chemistry and
computational linguistics. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 1493?1502.
The Gene Ontology Consortium. 2000. Gene ontol-
ogy: tool for the unification of biology. Nature Ge-
netics, 25(1):25?29.
Paul Thompson, Raheel Nawaz, John McNaught, and
Sophia Ananiadou. 2011. Enriching a biomedi-
cal event corpus with meta-knowledge annotation.
BMC Bioinformatics, 12.
Andrea Varga, Daniel Preotiuc-Pietro, and Fabio
Ciravegna. 2012. Unsupervised document zone
identification using probabilistic graphical models.
In Proceedings of LREC 2012, pages 1610?1617.
148
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 49?58,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Modeling Comma Placement in Chinese Text for Better Readability using
Linguistic Features and Gaze Information
Tadayoshi Hara1 Chen Chen2? Yoshinobu Kano3,1 Akiko Aizawa1
1National Institute of Informatics, Japan 2The University of Tokyo, Japan
3PRESTO, Japan Science and Technology Agency
{harasan, kano, aizawa}@nii.ac.jp
Abstract
Comma placements in Chinese text are
relatively arbitrary although there are
some syntactic guidelines for them. In this
research, we attempt to improve the read-
ability of text by optimizing comma place-
ments through integration of linguistic fea-
tures of text and gaze features of readers.
We design a comma predictor for gen-
eral Chinese text based on conditional ran-
dom field models with linguistic features.
After that, we build a rule-based filter for
categorizing commas in text according to
their contribution to readability based on
the analysis of gazes of people reading text
with and without commas.
The experimental results show that our
predictor reproduces the comma distribu-
tion in the Penn Chinese Treebank with
78.41 in F1-score and commas chosen by
our filter smoothen certain gaze behaviors.
1 Introduction
Chinese is an ideographic language, with no natu-
ral apparent word boundaries, little morphology,
and no case markers. Moreover, most Chinese
sentences are quite long. These features make it
especially difficult for Chinese learners to identify
composition of a word or a clause in a sentence.
Punctuation marks, especially commas, are al-
lowed to be placed relatively arbitrarily to serve as
important segmentation cues (Yue, 2006) for pro-
viding syntactic and prosodic boundaries in text;
commas indicate not only phrase or clause bound-
aries but also sentence segmentations, and they
capture some of the major aspects of a writer?s
prosodic intent (Chafe, 1988). The combination
of both aspects promotes cognition when reading
text (Ren and Yang, 2010; Walker et al, 2001).
?The Japan Research Institute, Ltd. (from April, 2013)
Linguistic FeaturesCRF modelCRF model-based Comma Predictor
Gaze FeaturesHuman AnnotationRule-based Comma Filter Text with/without Commas
Parse Tree
Treebank
Comma Distribution for Readability
Comma Distribution in General Text
Input (Comma-less) Text
+
+
Figure 1: Our approach
However, although there are guidelines and re-
search on the syntactic aspects of comma place-
ment, prosodic aspects have not been explored,
since they are more related with cognition. It is
as yet unclear how comma placement should be
optimized for reading, and it has thus far been up
to the writer (Huang and Chen, 2011).
In this research, we attempt to optimize comma
placements by integrating the linguistic features of
text and the gaze features of readers. Figure 1 il-
lustrates our approach. First, we design a comma
predictor for general Chinese text based on con-
ditional random field (CRF) models with various
linguistic features. Second, we build a rule-based
filter for classifying commas in text into ones fa-
cilitating or obstructing readability, by comparing
the gaze features of persons reading text with and
without commas. These two steps are connected
by applying our rule-based filter to commas pre-
dicted by our comma predictor. The experimental
results for each step validate our approach.
Related work is described in Section 2. The
functions of Chinese commas are described in
Section 3. Our CRF model-based comma predic-
tor is examined in Section 4, and our rule-based
comma filter is constructed and examined in Sec-
tion 5 and 6. Section 7 contains a summary and
outlines future directions of this research.
49
[Case 1] When a pause between a subject and a predicate is needed. (? (,) means the original or comparative position of the comma in Chinese text.)
e.g. ????????????????????????(The stars we can see (,)? are mostly fixed stars that are far away from the earth.)
[Case 2] When a pause between an inner predicate and an object of a sentence is needed.
e.g. ?????????????????????(We should see that (,) science needs a person to devote all his/her life to it.)
[Case 3] When a pause after an inner (adverbial, prepositional, etc.) modifier of a sentence is needed.
e.g. ?????????????(He is no stranger (,) to this city.) (The order of the modifier and the main clause is opposite in the English translation.)
[Case 4] When a pause between clauses in a complex sentence is needed, besides the use of semicolon (?).
e.g. ??????????????????????(It is said that there are more than 100 Suzhou traditional gardens, (,) no more than 10 of which I
have been to.)
[Case 5] When a pause between phrases of the same syntactic type is needed.
e.g. ??????????????? (The students prefer young (,) and energetic teachers.)
Table 1: Five main usages of commas in Chinese text
(a) Screenshot of a material
Display PC Monitor SubjectEye TrackerHost PC Monitor
(b) Scene of the experiment (c) Window around a gaze point
Figure 3: Settings for eye-tracking experiments
WS Word surface
POS POS tag
DIP Depth of a word in the parse tree
STAG Syntactic tag
OIC Order of the clause in a sentence that a word belongs to
WL Word length
LOD Length of fragment with specific depth in a parsing tree
Table 2: Features used in our CRF model
2 Related Work
Previous work on Chinese punctuation prediction
mostly focuses on sentence segmentation in au-
tomatic speech recognition (Shriberg et al, 2000;
Huang and Zweig, 2002; Peitz et al, 2011).
Jin et al (2002) classified commas for sentence
segmentation and succeeded in improving pars-
ing performance. Lu and Ng (2010) proposed
an approach built on a dynamic CRF for predict-
ing punctuations, sentence boundaries, and sen-
tence types of speech utterances without prosodic
cues. Zhang et al (2006) suggested that a cascade
CRF-based approach can deal with ancient Chi-
nese prose punctuation better than a single CRF.
Guo et al (2010) implemented a three-tier max-
imum entropy model incorporating linguistically
motivated features for generating commonly used
Chinese punctuation marks in unpunctuated sen-
tences output by a surface realizer.
(a)
WS|POS|STAG|DIP|OIC|WL|LOD|IOB-tag
(b)
Figure 2: Example of a parse tree (a) and its cor-
responding training data (b) with the features
3 Functions of Chinese Commas
There are five main uses of commas in Chinese
text, as shown in Table 1. Cases 1 to 4 are from
ZDIC.NET (2005), and Case 5 obviously exists in
Chinese text. The first three serve the function of
emphasis, while the latter two indicate coordinat-
ing or subordinating clauses or phrases.
In Cases 1 and 2, a comma is inserted as a
kind of pause between a short subject and a long
predicate, or between a short remainder predicate,
such as?? (see/know),??/?? (indicate),?
50
Feature F1 (P/R) A
WS 59.32 (72.67/50.12) 95.45
POS 32.51 (69.06/21.26) 94.08
DIP 34.14 (68.65/22.72) 94.13
STAG 22.44 (64.00/13.60) 93.67
OIC 9.27 (66.56/ 4.98) 93.42
WL 10.70 (75.24/ 5.76) 93.52
LOD 35.32 (59.20/25.17) 93.81
WS+POS 63.75 (79.93/53.01) 96.03
WS +DIP 70.06 (83.27/60.47) 96.61
WS +STAG 57.42 (81.94/44.19) 95.67
WS +OIC 60.35 (77.98/49.22) 95.73
WS +WL 60.90 (76.39/50.63) 95.71
WS +LOD 70.85 (78.87/64.31) 96.53
WS+POS+DIP 73.41 (84.62/64.82) 96.93
WS+POS+DIP+STAG 74.58 (83.66/67.27) 97.01
WS+POS+DIP +OIC 76.87 (84.29/70.65) 97.23
WS+POS+DIP +WL 70.18 (83.33/60.62) 96.63
WS+POS+DIP +LOD 76.61 (82.61/71.43) 97.16
WS+POS+DIP+STAG+OIC 76.62 (84.48/70.09) 97.21
WS+POS+DIP+STAG +WL 74.12 (84.00/66.33) 96.98
WS+POS+DIP+STAG +LOD 77.64 (85.11/71.38) 97.33
WS+POS+DIP +OIC+WL 75.43 (84.76/67.95) 97.11
WS+POS+DIP +OIC +LOD 78.23 (84.23/73.03) 97.36
WS+POS+DIP +WL+LOD 74.01 (85.80/65.06) 97.02
WS+POS+DIP+STAG+OIC+WL 77.25 (83.97/71.53) 97.26
WS+POS+DIP+STAG+OIC +LOD 77.31 (86.36/69.97) 97.33
WS+POS+DIP+STAG +WL+LOD 76.55 (85.24/69.46) 97.23
WS+POS+DIP +OIC+WL+LOD 77.60 (84.30/71.89) 97.30
WS+POS+DIP+STAG+OIC+WL+LOD 78.41 (83.97/73.54) 97.36
F1: F1-Score, P: precision (%), R: recall (%), A: accuracy (%)
Table 3: Performance of the comma predictor
(A) #Characters,
Article (B) #Punctuations, (C) / (A) (C) / (B) Subjects
ID (C) #Commas
6 692 49 28 4.04% 57.14% L, T, C
7 335 30 15 4.48% 50.00% L, T, C
10 346 18 7 2.02% 38.89% L, T, C, Z
12 221 18 7 3.17% 38.89% L, T, C
14 572 33 14 2.45% 42.42% L, T, C
18 471 36 13 2.76% 36.11% C, Z
79 655 53 28 4.27% 52.83% Z
82 471 30 13 2.76% 43.33% Z
121 629 41 19 3.02% 46.34% Z
294 608 50 24 3.95% 48.00% Z
401 567 43 21 3.70% 48.84% L, T, C
406 558 39 18 3.23% 46.15% Z
413 552 52 22 3.99% 42.31% T, C, Z
423 580 49 26 4.48% 53.06% L, C, Z
438 674 46 28 4.15% 60.87% Z
Average 528.73 39.13 18.87 3.57% 48.22% -
Table 4: Materials assigned to each subject
? (find) etc., and following long clause-style ob-
jects. English commas, on the other hand, sel-
dom have such usages (Zeng, 2006). In Cases 3
and 4, commas instead of conjunctions sometimes
connect two clauses in a relation of either coordi-
nation or subordination. English commas, on the
other hand, are only required between independent
clauses connected by conjunctions (Zeng, 2006).
Liu et al (2010) proved that Chinese commas
can change the syntactic structures of sentences
by playing lexical or syntactic roles. Ren and
Yang (2010) claimed that inserting commas as
clause boundaries shortens the fixation time in
post-comma regions. Meanwhile, in computa-
tional linguistics, Xue and Yang (2011) showed
Figure 4: Obtained eye-movement trace map
0
100,000
200,000
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10
ith commas Without commas
Tota
l vie
win
g 
time
 
(sec
.)
0
1 0
2 0
Figure 5: Total viewing time
that Chinese sentence segmentation can be viewed
as detecting loosely coordinated clauses separated
by commas.
4 CRF Model-based Comma Predictor
We first predict comma placements in existing
text. The prediction is formalized as a task to an-
notate each word in a word sequence with an IOB-
style tag such as I-Comma (following a comma),
B-Comma (preceding a comma) or O (neither I-
Comma nor B-Comma). We utilize a CRF model
for this sequential labeling (Lafferty et al, 2001).
4.1 CRF Model for Comma Prediction
A conditional probability assigned to a label se-
quence Y for a particular sequence of words X in
a first-order linear-chain CRF is given by:
P?(Y |X) =
exp(
?n
w
?k
i ?ifi(Yw?1, Yw, X,w))
Z0(X)
where w is a word position in X , fi is a binary
function describing a feature for Yw?1, Yw, X , and
w, ?i is a weight for that feature, and Z0 is a nor-
malization factor over all possible label sequences.
The weight ?i for each fi is learned on training
data. For fi, the linguistic features shown in Ta-
ble 2 are derived from a syntactic parse of a sen-
tence1. The first three were used initially; the rest
were added after we got feedback from construc-
tion of our rule-based filters (see Section 5). Fig-
ure 2 shows an example of a parsing tree and its
corresponding training data.
1Some other features or tag formats which worked well in
the previous research, such as bi-/tri-gram, a preceding word
(L-1) or its POS (POS-1), and IO-style tag (Leaman and Gon-
zalez, 2008) were also examined, but they did not work that
well, probably because of the difference in task settings.
51
0
1,000
2,000
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10Fixa
tion
 
time
 
/
com
ma
 
(sec
.)
0.0
1.0
2.0 ith commas Without commas
Figure 6: Fixation time per comma
0.01.0
2.03.0
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10#reg
res
sion
s /
com
ma 0
ith co mas Without commas
12
3
Figure 7: Number of regressions per comma
4.2 Experimental Settings
The Penn Chinese Treebank (CTB) 7.0 (Nai-
wen Xue and Palmer, 2005) consists of 2,448
articles in five genres. It contains 1,196,329
words, and all sentences are annotated with parse
trees. We selected four genres for written Chi-
nese (newswire, news magazine, broadcast news
and newsgroups/weblogs) from this corpus as our
dataset. These were randomly divided into train-
ing (90%) and test data (10%). We also corrected
errors in tagging and inconsistencies in the dataset,
mainly by solving problems around strange char-
acters tagged as PU (punctuation). The commas
and characters after this preprocessing numbered
63,571 and 1,533,928 in the training data and
4,116 and 111,172 in the test data.
MALLET (McCallum, 2002) and its applica-
tion ABNER (Settles, 2005) were used to train the
CRF model. We evaluated the results in terms
of precision (P = tp/(tp + fp)), recall (R =
tp/(tp+fn)), F1-score (F1 = 2PR/(P+R)), and
accuracy (A = (tp + tn)/(tp + tn + fp + fn)),
where tp, tn, fp and fn are respectively the num-
ber of true positives, true negatives, false positives
and false negatives, based on whether the model
and the corpus provided commas at each location.
4.3 Performance of the CRF Model
Table 3 shows the performance of our CRF
model2. We can see that WS contributed much
more to the performance than other features, prob-
ably because a word surface itself has a lot of
information on both prosodic and syntactic func-
tions. Combining WS with other features greatly
improved performance, and as a result, with all
2Precision, recall, F1-score, and accuracy with WS + POS
+ DIP + L-1 + POS-1 were 82.96%, 65.04%, 72.91 and
96.84%, respectively (lower than those with WS+POS+DIP).
4080
120160
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Comma
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z10Sac
cad
e le
ngth
(1) / 
com
ma
ith co mas Without commas
4080
120160
(pixel)
Figure 8: Saccade length (1) per comma
30
60
90
L1 L2 L3 L4 L5 L6 L7 T1 T2 T3 T4 T5 T6 T8 C1 C2 C3 C4 C5 C6 C7 C8 Z1 Z2 Z3 Z4 Z5 Z6 Z7 Z8 Z9 Z10Trials (?Subject? + ?Trial No.?)
With Comma No Commaith co mas Without commas
L1 ? L7 T1 ? T7 1 ? 8 Z1 ? Z1030Sac
cad
e le
ngth
(2) / 
com
ma 60
90(pixel)
Figure 9: Saccade length (2) per comma
features (WS + POS + STAG + DIP + OIC + LOD
+ WL), precision, recall, F1-score and accuracy
were 83.97%, 73.54%, 78.41 and 97.36%.
We also found that a large number of false pos-
itives seemed helpful according to native speakers
(see the description of the subjects in Section 5 and
6). Although these commas do not appear in the
CTB text, they might smoothen the reading expe-
rience. We constructed a rule-based filter in order
to pick out such commas.
5 Rule-based Comma Filter
We constructed a rule-based comma filter for clas-
sifying commas in text into ones facilitating (pos-
itive) or obstructing (negative) the reading process
as follows:
[Step 1]: Collect gaze data from persons reading
text with or without commas (Section 5.1).
[Step 2]: Compare gaze features around commas
to find those features that reflect the effect of
comma placement. (Section 5.2).
[Step 3]: Annotate commas with categories based
on the obtained features (Section 5.3), and devise
rules to explain the annotation (Section 5.4).
5.1 Collecting Human Eye-movement Data
Eye-movements during reading contain rich infor-
mation on how the document is being read, what
the reader is interested in, where difficulties hap-
pen, etc. The movements are characterized by fix-
ations (short periods of steadiness), saccades (fast
movements), and regressions (backward saccades)
(Rayner, 1998). In order to analyze the effect of
commas on reading through the features, we col-
lected gaze data from subjects reading text in the
following settings.
[Subjects and Materials] Four native Man-
52
Categories Effect on readability Outward manifestation
Positive (?) Can improve readability. Presence would cause GF+.
Semi-positive (?) Might be necessary for readability, but the importance is not as obvious as a positive comma. Absence might cause GF-.
Semi-negative (2) Might be negative, but its severity is not as obvious as a negative comma. Absence might cause GF+.
Negative (?) Thought to reduce a document?s readability. Presence would cause GF-.
GF+/GF-: values of eye-tracking features that represent good/poor readability
Table 5: Comma categories
Subject Positive (?) Semi-positive (?) Semi-negative (2) Negative (?) Adjustment formula
L ?FT?>800 500<?FT??800 -100<?FT??500 ?FT?<-100 ?FT? = ?FT ? ?RT ? 200
C ?FT?>900 600<?FT??900 -200<?FT??600 ?FT?<-200 ?FT? = ?FT ? ?RT ? 275
T ?FT?>600 300<?FT??600 -300<?FT??300 ?FT?<-300 ?FT? = ?FT ? ?RT ? 250
Z ?FT?>650 350<?FT??650 -250<?FT??350 ?FT?<-250 ?FT? = ?FT ? ?RT ? 250
?FT = [ fixation time (without commas) [ms]]? [ fixation time (with commas) [ms]]
?RT = [ #regressions (without commas) ]? [ #regressions (with commas) ]
Table 6: Estimation formula for judging the contribution of commas to readability
ID ? ? 2 ?
6 13 6 4 5
7 8 6 1 0
10 5 0 1 1
12 1 4 2 0
14 4 4 5 1
18 5 1 4 3
79 11 4 9 4
82 5 6 2 0
ID ? ? 2 ?
121 11 2 6 0
294 9 9 4 1
401 10 7 2 2
406 5 6 5 2
413 8 5 6 3
423 11 4 7 4
438 6 16 6 0
Total 112 80 64 26
Table 7: Categories of annotated commas
darin Chinese speakers (graduate students and re-
searchers) read 15 newswire articles selected from
CTB 7.0 (included in the test data in Section 4.2).
Table 4 and Figure 3(a) show the materials as-
signed to each subject and a screenshot of one ma-
terial. Each article was presented in 12-15 points
of bold-faced Fang-Song font occupying 13?13,
14?15, 15?16 or 16?16 pixels along with a line
spacing of 5-10 pixels3.
[Apparatus] Figure 3(b) shows a scene of the
experiment. An EyeLink 1000 eye tracker (SR
Research Ltd., Toronto, Canada) with a desktop
mount monitored the movements of a right eye at
1,000 Hz. The subject?s head was supported at the
chin and forehead. The distance between the eyes
and the monitor was around 55 cm, and each Chi-
nese character subtended a visual angle 1?. Text
was presented on a 19? monitor at a resolution
of 800?600 pixels, with the brightness adjusted
to a comfortable level. The displayed article was
masked except for the area around a gaze point
(see Figure 3(c)) in order to confirm that the gaze
point was correctly detected and make the subject
concentrate on the area (adjusted for him/her).
[Procedure] Each article was presented twice
(once with/once without commas) to each subject.
3These values, as well as the screen position of the article,
were adjusted for each subject.
The one without commas was presented first4 (not
necessarily in a row). We did not give any compre-
hension test after reading; we just asked the sub-
jects to read carefully and silently at their normal
or lower speed, in order to minimize the effect of
the first reading on the second. The subjects were
informed of the presence or absence of commas
beforehand. The apparatus was calibrated before
the experiment and between trials. The experi-
ment lasted around two hours for each subject.
[Alignment of eye-tracking data to text] Figure 4
shows an example of the obtained eye-movement
trace map, where circles and lines respectively
mean fixation points and saccades, and color depth
shows their duration. The alignment of the data to
the text is a critical task, and although automatic
approaches have been proposed (Mart??nez-Go?mez
et al, 2012a; Mart??nez-Go?mez et al, 2012b), they
do not seem robust enough for our purpose. Ac-
cordingly, we here just compared the entire layout
of the gaze point distribution and that of the actual
text, and adjusted them to have relatively coherent
positions on the x-axis; i.e., the beginning and end
of the gaze point sequence in a line were made as
close as possible to those of the line in the text.
5.2 Analysis of Eye-movement Data
The gaze data were analyzed by focusing on re-
gions around each comma or where each one
should be (three characters left and right to the
comma5).
4If we had used the reversed order, the subject would have
knowledge about original comma distribution, and this would
cause abnormally quick reading of the text without commas.
With the order we set, conflicts between false segmentations
(made in first reading) and correct ones might bother the sub-
ject, which is trade-off (though minor) in the second reading.
5When a comma appeared at the beginning of a line, two
characters to the left and right of the comma and one charac-
53
1. If L Seg and R Seg are both very long, a comma must be put between them.
2. If two ? appear serially, one is necessary whereas the other might be optional or judged negative, but it still depends on the lengths of the siblings.
3. If two neighboring commas appear very close to each other, one of them is judged as negative whereas judgment on the other one is reserved.
4. If several (more than 2) ?s appear continually, one or more ?s might be reserved in consideration of the global condition.
5. A comma is always needed after a long sentence or clause without any syntactically significant punctuation with the function of segmentation.
6. If a ? appears near a ?, it might be judged as negative with a high probability. However, the judgment process is always from the bottom up, which
means ? ? 2? ???. For example, if a 2 appears near a ?, we judge 2 first (to be positive or negative), then judge the ? in the condition
with or without the comma of 2.
Table 8: General rules for reference
Figure 5, 6 and 7 respectively show the total
viewing time, fixation time (duration for all fix-
ations and saccades in a target region) per comma,
and number of regressions per comma6 for each
trial. We can see a general trend wherein the for-
mer two were shorter and the latter was smaller for
the articles with commas than without. The diver-
sity of the subjects was also observed in Figure 6.
Figure 8 and 9 show the saccade length per
comma for different measures. The former (lat-
ter) figure considers a saccade in which at least
one edge (both edges) was in the region. We can-
not see any global trend, probably because of the
difference in global layout of materials brought by
the presence or absence of commas.
5.3 Categorization of Commas
Using the features shown to be effective to repre-
sent the effect of comma placement, we analyzed
the statistics for each comma in order to manu-
ally construct an estimation formula for judging
the contribution of each comma to readability. The
contribution was classified into four categories
(Table 5), and the formula is described in Table 67.
The adjustment formula was based on our obser-
vation that the number of regressions could only
be regarded as an aid. For example, for subject
C, if ?FT=200ms and ?RT =?2, ?FT?=?350,
and therefore, the comma is annotated as negative.
All parameters were decided empirically and man-
ually checked twice (self-judgment and feedback
from the subjects).
On the basis of this estimation formula, all arti-
cles in Table 4 were manually annotated. Table 7
shows the distribution of the assigned categories8.
ter to the left and right of the final character of the last line
were analyzed.
6Calculated by counting the instances where the x-
position of [a fixation / end point of a saccade ] was ahead
of [the former fixation / its start point]. Although the counts
of these two types were almost the same, by counting both of
them, we expected to cover any possible regression.
7One or two features are used to judge the category of a
comma. We will explore more features in the future.
8In the case of severe contradictions, the annotators dis-
cussed them and resolved them by voting.
5.4 Implementation of Rule-based Filter
The annotated commas were classified into Cases
1 to 5 in Table 1, based on the types of left and
right segment conjuncts (L Seg and R Seg, which
were obtained from the parse trees in CTB). For
each of the five cases, the reason for the assign-
ment of a category (?, ?, 2 or ?) to each
comma was explained by a manually constructed
rule which utilized information about L Seg and
R Seg. The rules were constructed so that they
would cover as many instances as possible. Ta-
ble 8 shows the general rules utilized as a refer-
ence, and Table 9 shows the finally obtained rules.
The rightmost column in this table shows the num-
ber of commas matching each rule. These rules
were then implemented as a filter for classifying
commas in a given text.
For several rules (?10, 28, 210, 211 and
212), there were only single instances. In addi-
tion, although our rules were built carefully, a few
exceptions to the detailed threshold were found.
Collecting and investigating more gaze data would
help to make our rules more sophisticated.
6 Performance of the Rule-based Filter
We assumed that our comma predictor provides a
CTB text with the same distribution as the origi-
nal one in CTB (see Figure 1). Accordingly, we
examined the quality of the comma categorization
by our rule-based filter through gaze experiments.
6.1 Experimental Settings
Another five native Mandarin Chinese speakers
were invited as test subjects. The CTB articles as-
signed to the subjects are listed in Table 10. These
articles were selected from the test data in Sec-
tion 4.2 in such a way that 520<#characters<700,
#commas>17, #commas/#punctuations>38%,
and #commas/#characters>3.1%, since we
needed articles of appropriate length with a fair
number of commas. After that, we manually
chose articles that seemed to attract the subjects?
interest from those that satisfied the conditions.
54
Case 1: L Subject + R Predicate #commas
?6 L IP-SBJ + R VP (length both<14 (In Seg Len)) 2
?7 L IP-SBJ/NP-SBJ (Org Len>13, Ttl Len>15) 7
?6 L NP-SBJ/IP-SBJ (<14) + R VP (?25) 2
Case 2: L Predicate + R Object #commas
?9 Long frontings (Modifier/Subject, >7) + short L predicate (VV/VRD/VSB? ? ? , ?3) + Longer R object (IP-OBJ, >28) 6
?8 Short frontings (<5) + short L predicate (<3) + moderate-length R object (IP-SBJ, <20) 4
26 Short frontings (<6) + short L predicate (?3) + long R object (IP-SBJ, >23) 9
Case 3: L Modifier #commas
?3 Short frequently used L modifier (2-3,??,??, etc.) + moderate-length/long R SPO (?w18p10) 13
?7 Short L (PP/LCP)-TMP (5, 6) + long R NP (?10) 4
?10 Long L CP-CND (e.g.,??, >18) + moderate-length R Seg (SPO, IP, etc. <18) 1
?1 Long L modifier (PP(-XXX, P+Long NP/IP), IP-ADV, ?17) 6
?4 Moderate-length/short L modifier (PP(-XXX, P+IP, There is IP inside, >6<15, cf. 26 (NP)) 9
?9 Long L (PP/LCP)-TMP (Ttl Len?10), short R Seg (NP/ADVP, <3) 4
?10 Short L (LCP/PP)-LOC (<8) 2
22 Long L LOC (or there is LCP inside PP, >10) 5
23 Very short frequently used L ADVP/ADV (2) 8
25 Short L (PP/LCP/NP)-TMP (4;5-6, when R Seg is short (<10)) 12
24 Moderate-length PP(-XXX, P+NP, >8 ?13) + R Seg (SPO, IP, VO, MSPO, etc.) 6
28 Short L IP-CND (<8) 1
211 Long L PP-DIR (>20) + short R VO (?10) 1
?2 Very short L (QP/NP/LCP)-TMP (?3) 8
?5 Short frequently used L modifier (as in ?3, ?3) + short/moderate-length R Seg (SPO etc., <c20w9) 1
Case 4: L c + R c #commas
?2 L c & R c are both long (In Seg Len?15; or one>13, the other near 20) 39
?8 L c is the summary of R c 2
?2 Moderate-length L c + R c (both ?10?15; or one?17, the other?12) 25
?3 Moderate-length clause (>10), but connected with familiar CC or ADVP 6
?5 Three or more consecutive moderate-length clauses (all<15, and at least one ?10) 12
?7 Very short L c + R c (both <5), something like slogan) 1
Case 5: L p + R p #commas
?1 Short coordinate modifiers (Both side <5) 4
?4 Short L p+R p (both<c15w5, and at least one <10), but pre-L p (e.g., SBJ) is too long (>18) 2
?5 Between two moderate-length/long phrases (both ?15; or L p?17, R p=10-14; Or L p=10-14, R p>20) 39
?11 Long pre-L p (SBJ /ADV, etc. >16) + short L p (?5) + long R p (?18) 2
(?3 Moderate-length phrase (>10), but connected with familiar CC or ADVP) (6)
?6 Three or more consecutive short/moderate-length phrases (both<15, at least one<8) 5
21 Between short phrases (both ?c13w5), and pre-L p (SBJ/ADV, etc.) is short/moderate-length (<11) 13
27 Coordinate VPs, and L VP is a moderate-length VP (PP-MNR VP) 4
29 Phrasal coordination between a long (?18) and a short (<10) phrase 3
210 Moderate-length coordinate VPs (>10<15), and R VP has the structure like VP (MSP VP) 1
212 Between two short/moderate-length NP phrases (both ?15, e.g., L NP-TPC+R NP-SBJ) 1
?1 Moderate-length/short phrase ((i) c:one>10<18, The other >5?10, w:one?5, the other>5?10; (ii) c:both?10<15, 13
w:both>5?7), and pre-L p (SBJ/ADV, etc.) is short (?5)
? L x/R x: the left/right segment of a target comma which is x.
(x can be ?p? (phrase) / ?c? (clause), syntactic tags (with function tags) such as ?VP? and ?IP-SBJ?, or general functions such as ?Subject? and ?Predicate?.)
? Org Len: the number of characters in a segment (including other commas or punctuation inside).
? In Seg Len/Ttl Len: the number of characters between the comma and nearest punctuation (inside a long/outside a short target segment).
? SPO: subject + predicate + object, belonging to the outermost sentence. The length is defined in the similar way as In Seg Len.
? MSPO: modifier + subject + predicate + object. The length is defined in the similar way as In Seg Len.
? -XX or -XXX: arbitrary type of possible functional tag (or without any functional tag) connected with the former syntactic tag.
? ?ciwj: #characters?i and #words?j.
? In some cases (in Case 3, 4 and 5), the length is calculated after negative (or judged negative) commas are eliminated.
? The rules related with TMP are applied faster than ones related with LCP (in Case 3).
? ?3 appears in both Case 4 (clause) and Case 5 (phrase). The number of commas is given by the sum of those in both cases.
Table 9: Entire classification of rules based on traditional comma categories
(A) #Characters,
Article (B) #Punctuations, (C) / (A) (C) / (B) Subjects
ID (C) #Commas
6 692 49 28 4.04% 57.14% L, S, H
11 672 48 21 3.13% 43.75% L, S, F
15 674 67 26 3.86% 38.81% L, S, H
16 547 43 22 4.02% 51.16% L, S, F
56 524 43 18 3.44% 41.86% L, H, M
73 595 46 28 4.71% 60.87% S, H, F, M
79 655 53 28 4.27% 52.83% H, F, M
99 671 55 24 3.58% 43.64% F, M
Average 628.75 50.50 24.38 3.88% 48.27% -
Table 10: Materials assigned to each subject
Our rule-based filter was applied to the commas
of each article9, and the commas were classified
9Instances of incoherence among the applied rules were
0
40,000
80,000
120,000
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)Positive distribution Negative distribution
040
8012
Tot
al v
iew
ing
tim
e (se
c.)
Figure 10: Total viewing time for two distributions
into two distributions: a positive one (positive +
semi-positive commas) and a negative one (nega-
tive + semi-negative commas). Two types of ma-
terials were thus generated by leaving the commas
in one distribution and removing the others.
manually checked and corrected.
55
20
40
60
80
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)
EM
FFT
(10
0) Positive distribution Negative distribution
Figure 11: EMFFT for two distributions
46
810
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)
EM
FT
(80
0) Positive distribution Negative distribution
Figure 12: EMFT for two distributions
The apparatus and procedure were almost the
same as those in Section 5.1, whereas, on the ba-
sis of the feedback from the previous experiments,
the font size, number of characters in a line, and
line spacing were fixed to single optimized values,
respectively, 14-point Fang-Song font occupying
15?16 pixels, 33 characters and 7 pixels.
6.2 Evaluation Metrics
We examined whether our positive/negative distri-
butions really facilitated/obstructed the subjects?
reading process by using the following metrics:
TT, EMFFT = FFTFT
10
, EMFT = FTCN?TT
11
,
EMRT = RT2?CN
12
, EMSLO = SLO2?TT ,
where TT, FT, RT and CN are total viewing time,
fixation time, number of regressions, and num-
ber of commas respectively, as described in Sec-
tion 5.2. FFT and SLO are additionally introduced
metrics respectively for the ?total duration for all
first-pass fixations in a target region that exclude
any regressions? and for the ?length of saccades
from inside a target region to the outside?13. All of
the areas around commas appearing in the original
article were considered target areas for the metrics.
The other settings were the same as in Section 5.
6.3 Contribution of Categorized Commas
Figure 10, 11, 12, 13 and 14 respectively show TT,
EMFFT , EMFT , EMRT and EMSLO for two types
of comma distributions in each trial.
10Ratio to the total fixation time in the target areas (FT).
11Normalized by the total viewing time (TT).
12Two types of RT count (see Section 5.2) were averaged.
13Respectively to reflect ?the early-stage processing of the
region? and ?the information processed for a fixation and a
decision of the next fixation point? (Hirotani et al, 2006).
0
5
10
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)
EM
RT
(1
0) Positive distribution Nega ive distribution
Figure 13: EMRT for two distributions
0
5
10
F79 F11 F16 F73 F99 H73 H06 H15 H79 H56 L06 L11 L15 L16 L56 M99 M79 M73 M56 S06 S11 S15 S16 S73Trials (?Subject? + ?Article ID?)
Distribution(G) Distribution(B)Positive distribution Negative distribution
EM
SLO
(10
0)
Figure 14: EMSLO for two distributions
For TT, we cannot see any general trend, mainly
because this time, the reading order of the text
was random, which spread out the second reading
effect evenly between the two distributions. For
EMFFT , we cannot reach a conclusion either. In
contrast, in more than half of the trials, EMFFT
was larger for positive distributions, which would
imply that the positive commas helped to prevent
the reader?s gaze from revisiting the target regions.
For most trials, except for subject S whose cal-
ibration was poor and reading process was poor
in M56, EMFT and EMRT decreased and EMSLO
increased for positive distributions, which implies
that the positive commas smoothed the reading
process around the target regions.
7 Conclusion
We proposed an approach for modeling comma
placement in Chinese text for smoothing reading.
In our approach, commas are added to the text on
the basis of a CRF model-based comma predic-
tor trained on the treebank, and a rule-based filter
then classifies the commas into ones facilitating or
obstructing reading. The experimental results on
each part of this approach were encouraging.
In our future work, we would like see how com-
mas affect reading by using much more material,
and thereby refine our framework in order to bring
a better reading experience to readers.
Acknowledgments
This research was partially supported by Kakenhi,
MEXT Japan [23650076] and JST PRESTO.
56
References
Wallace Chafe. 1988. Punctuation and the prosody of
written language. Written Communication, 5:396?
426.
Yuqing Guo, Haifeng Wang, and Josef van Genabith.
2010. A linguistically inspired statistical model
for Chinese punctuation generation. ACM Trans-
actions on Asian Language Information Processing,
9(2):6:1?6:27, June.
Masako Hirotani, Lyn Frazier, and Keith Rayner. 2006.
Punctuation and intonation effects on clause and
sentence wrap-up: Evidence from eye movements.
Journal of Memory and Language, 54(3):425?443.
Hen-Hsen Huang and Hsin-Hsi Chen. 2011. Pause and
stop labeling for Chinese sentence boundary detec-
tion. In Proceedings of Recent Advances in Natural
Language Processing, pages 146?153.
Jing Huang and Geoffrey Zweig. 2002. Maximum en-
tropy model for punctuation annotation from speech.
In Proceedings of the International Conference on
Spoken Language Processing, pages 917?920.
Mei xun Jin, Mi-Young Kim, Dongil Kim, and Jong-
Hyeok Lee. 2002. Segmentation of Chinese
long sentences using commas. In Proceedings of
the Third SIGHAN Workshop on Chinese Language
Processing, pages 1?8.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
?01, pages 282?289, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
Robert Leaman and Graciela Gonzalez. 2008. BAN-
NER: An executable survery of advances in biomed-
ical named entity recognition. In Pacific Symposium
on Biocomputing (PSB?08), pages 652?663.
Baolin Liu, Zhongning Wang, and Zhixing Jin. 2010.
The effects of punctuations in Chinese sentence
comprehension: An erp study. Journal of Neurolin-
guistics, 23(1):66?68.
Wei Lu and Hwee Tou Ng. 2010. Better punctuation
prediction with dynamic conditional random fields.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP
?10), pages 177?186.
Pascual Mart??nez-Go?mez, Chen Chen, Tadayoshi Hara,
Yoshinobu Kano, and Akiko Aizawa. 2012a. Image
registration for text-gaze alignment. In Proceedings
of the 2012 ACM international conference on Intel-
ligent User Interfaces (IUI ?12), pages 257?260.
Pascual Mart??nez-Go?mez, Tadayoshi Hara, Chen
Chen, Kyohei Tomita, Yoshinobu Kano, and Akiko
Aizawa. 2012b. Synthesizing image representa-
tions of linguistic and topological features for pre-
dicting areas of attention. In Patricia Anthony, Mit-
suru Ishizuka, and Dickson Lukose, editors, PRICAI
2012: Trends in Artificial Intelligence, pages 312?
323. Springer.
Andrew Kachites McCallum. 2002. MALLET: A ma-
chine learning for language toolkit.
Fu-dong Chiou Naiwen Xue, Fei Xia and Marta
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Stephan Peitz, Markus Freitag, Arne Mauser, and Her-
mann Ney. 2011. Modeling punctuation prediction
as machine translation. In Proceedings of Interna-
tional Workshop on Spoken Language Translation,
pages 238?245.
Keith Rayner. 1998. Eye movements in reading and
information processing: 20 years of research. Psy-
chological Bulletin, 124(3):372?422.
Gui-Qin Ren and Yufang Yang. 2010. Syntac-
tic boundaries and comma placement during silent
reading of Chinese text: evidence from eye move-
ments. Journal of Research in Reading, 33(2):168?
177.
Burr Settles. 2005. ABNER: an open source tool
for automatically tagging genes, proteins, and other
entity names in text. Bioinformatics, 21(14):3191?
3192.
Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-
Tu?r, and Go?khan Tu?r. 2000. Prosody-based au-
tomatic segmentation of speech into sentences and
topics. Speech Communication, 32(1-2):127?154.
Judy Perkins Walker, Kirk Fongemie, and Tracy
Daigle. 2001. Prosodic facilitation in the resolu-
tion of syntactic ambiguities in subjects with left
and right hemisphere damage. Brain and Language,
78(2):169?196.
Nianwen Xue and Yaqin Yang. 2011. Chinese sen-
tence segmentation as comma classification. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics:shortpapers,
pages 631?635.
Ming Yue. 2006. Discursive usage of six Chinese
punctuation marks. In Proceedings of the COL-
ING/ACL 2006 Student Research Workshop, pages
43?48.
ZDIC.NET. 2005. Commonly used Chinese punctua-
tion usage short list. Long Wiki, Retrieved Dec 10,
2012, from http://www.zdic.net/appendix/f3.htm.
(in Chinese).
X. Y. Zeng. 2006. The comparison and the use
of English and Chinese comma. College English,
3(2):62?65. (in Chinese).
57
Kaixu Zhang, Yunqing Xia, and Hang Yu. 2006.
CRF-based approach to sentence segmentation and
punctuation for ancient Chinese prose. Jour-
nal of Tsinghua Univ (Science and Technology),
49(10):1733?1736. (in Chinese).
58
Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 44?52,
Dublin, Ireland, August 23rd 2014.
Significance of Bridging Real-world Documents and NLP Technologies
Tadayoshi Hara Goran Topic? Yusuke Miyao Akiko Aizawa
National Institute of Informatics
2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan
{harasan, goran topic, yusuke, aizawa}@nii.ac.jp
Abstract
Most conventional natural language processing (NLP) tools assume plain text as their input,
whereas real-world documents display text more expressively, using a variety of layouts, sentence
structures, and inline objects, among others. When NLP tools are applied to such text, users
must first convert the text into the input/output formats of the tools. Moreover, this awkwardly
obtained input typically does not allow the expected maximum performance of the NLP tools to
be achieved. This work attempts to raise awareness of this issue using XML documents, where
textual composition beyond plain text is given by tags. We propose a general framework for
data conversion between XML-tagged text and plain text used as input/output for NLP tools and
show that text sequences obtained by our framework can be much more thoroughly and efficiently
processed by parsers than naively tag-removed text. These results highlight the significance of
bridging real-world documents and NLP technologies.
1 Introduction
Recent advances in natural language processing (NLP) technologies have allowed us to dream about
applying these technologies to large-scale text, and then extracting a wealth of information from the text
or enriching the text itself with various additional information. When actually considering the realization
of this dream, however, we are faced with an inevitable problem. Conventional NLP tools usually assume
an ideal situation where each input text consists of a plain word sequence, whereas real-world documents
display text more expressively using a variety of layouts, sentence structures, and inline objects, among
others. This means that obtaining valid input for a target NLP tool is left completely to the users, who
have to program pre- and postprocessors for each application to convert their target text into the required
format and integrate the output results into their original text. This additional effort reduces the viability
of technologies, while the awkwardly obtained input does not allow the expected maximum benefit of
the NLP technologies to be realized.
In this research, we raise awareness of this issue by developing a framework that simplifies this con-
version and integration process. We assume that any textual composition beyond plain text is captured by
tags in XML documents, and focus on the data conversion between XML-tagged text and the input/output
formats of NLP tools. According to our observations, the data conversion process is determined by the
textual functions of the XML-tags utilized in the target text, of which there seem to be only four types.
We therefore devise a conversion strategy for each of the four types. After all tags in the XML tagset of
the target text have been classified by the user into the four types, data conversion and integration can be
executed automatically using our strategies, regardless of the size of the text (see Figure 1).
In the experiments, we apply our framework to several types of XML documents, and the results show
that our framework can extract plain text sequences from the target XML documents by classifying only
20% or fewer of the total number of tag types. Furthermore, with the obtained sequences, two typical
parsers succeed in processing entire documents with a much greater coverage rate and using much less
parsing time than with naively tag-removed text.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/44
Formatted Input(e.g. plain text)
Output analysis(e.g. parse trees)
<p> A user task is a scenario of use of the UI, ? the UML 
notation.  In our case, we use the CTT (Concur Task Tree) 
<cite>[ <bibref bibrefs="paterno-ctte-2001,paterno-ctt-2001" separator="," show="Number" yyseparator=","/>]</cite></p>
A user task is a scenario of use of the UI, ? the UML notation.In our case, we use the CTT (Concur Task Tree) [1]
<sentence id=?s0?><cons>?</cons></sentence>
<sentence id=?s1?>?<cons><tok>[</tok>
</cons><cons><cons><tok>1</tok></cons> ? 
<cons><tok>]</tok></cons>?</sentence>
Independent
Decoration
Object
Meta-info
Tag1
Tag2
?
(Classifyinto 4 types)
TextstructuredbyXML
TextstructuredbyXML
Textstructuredby XML
Tagset
Dataconversion NLP tools(Parser)
Figure 1: Proposed data conversion framework for applying NLP tools to text structured as XML
Tag type Criteria for classification Strategies for data conversion
Independent To represent a region syntactically Remove the tag and tagged region
independent from the surrounding text ? (apply tools to the tagged region independently)
? recover the (analyzed) tagged region after applying the tools
Decoration To set the display style of Remove only the tag
the tagged region at the same level as ? recover the tag after applying the tools
the surrounding text
Object To represent the minimal object Replace the tag (and the tagged region) with a plain word
unit that should be handled in the ? (do not process the tagged region further)
the same level as the surrounding text ? recover the tag (and region) after applying the tools
Meta-info To describe the display style Remove the tag and tagged region
setting or additional information ? (do not process the tagged region further)
? recover the tag and region after applying the tools
Table 1: Four types of tags and the data conversion strategy for each type
The contribution of this work is to demonstrate the significance of bridging real-world documents and
NLP technologies in practice. We show that, if supported by a proper framework, conventional NLP tools
already have the ability to process real-world text without significant loss of performance. We expect the
demonstration to promote further discussion on real-world document processing.
In Section 2, some related research attempts are introduced. In Section 3, the four types of textual
functions for XML tags and our data conversion strategies for each of these are described and imple-
mented. In Section 4, the efficiency of our framework and the adequacy of the obtained text sequences
for use in NLP tools are examined using several types of documents.
2 Related Work
To the best of our knowledge, no significant work on a unified methodology for data conversion between
target text and the input/output formats of NLP tools has been published. Some NLP tools provide
scripts for extracting valid input text for the tools from real-world documents; however, even these scripts
assume specific formats for the documents. For example, deep syntactic parsers such as the C&C Parser
(Clark and Curran, 2007) and Enju (Ninomiya et al., 2007) assume POS-tagged sentences as input, and
therefore the distributed packages for the parsers1 contain POS-taggers together with the parsers. The
POS-taggers assume plain text sentences as their input.
As the work most relevant to our study, UIMA (Ferruci et al., 2006) deals with various annotations
in an integrated framework. However, in this work, the authors merely proposed the framework and did
1[C&C Parser]: http://svn.ask.it.usyd.edu.au/trac/candc/wiki / [Enju]: http://kmcs.nii.ac.jp/enju/45
New UI is shown. The UI is more useful  than XYZ          , and ... .
text indexmark
Cite1
Notice that ? . notecite
<text>New UI</text> is shown. The UI is more useful than XYZ <indexmark>
? </indexmark> in <cite>[ ? ]</cite><note>Notice that ? .</note>, and ? .
sentence sentence
New UI is shown. The UI is more useful  than XYZ          , and ... .Cite1 sentence
<sentence><text>New UI</text> is shown.</sentence> <sentence>The UI is more useful  than XYZ<indexmark> 
? </indexmark> in <cite>[?]</cite><note><sentence>Notice that ? . </sentence></note>, and ? .</sentence> 
(a)
(c)
(d)
(b)
text indexmark notecite Notice that ? . 
Meta-infoDecoration
IndependentObject
Figure 2: Example of executing our strategy
not explain how the given text can be used in a target annotation process such as parsing. Some projects
based on the UIMA framework, such as RASP4UIMA (Andersen et al., 2008), U-compare (Kano et
al., 2011), and Kachako (Kano, 2012)2, have developed systems where the connections between various
documents and various tools are already established. Users, however, can utilize only the text and tool
pairs that have already been integrated into the systems. GATE (Cunningham et al., 2013) is based on
a similar concept to UIMA; it supports XML documents as its input, while the framework also requires
integration of tools into the systems.
In our framework, although availability of XML documents is assumed, a user can apply NLP tools to
the documents without modifying the tools; instead, this is achieved by merely classifying the XML-tags
in the documents into a small number of functional types.
3 Data Conversion Framework
We designed a framework for data conversion between tagged text and the input/output formats of NLP
tools based on the four types of textual functions of tags. First, we introduce the four tag types and
the data conversion strategy for each. Then, we introduce the procedure for managing the entire data
conversion process using the strategies.
3.1 Strategies for the Four Tag Types
The functions of the tags are classified into only four types, namely, Independent, Decoration, Object,
and Meta-info, and for each of these types, a strategy for data conversion is described, as given in Table
1. This section explains the types and their strategies using a simple example where we attempt to apply
a sentence splitter to the text given in Figure 2(a). The target text has four tags, ?<note>?, ?<text>?,
?<cite>?, and ?<indexmark>?, denoting, respectively, Independent, Decoration, Object, and Meta-
info tags. We now describe each of the types.
Regions enclosed by Independent tags contain syntactically independent text, such as titles, sections,
and so on. In some cases, a region of this type is inserted into the middle of another sentence, like
the ?<note>? tags in the example, which represent footnote text. The data conversion strategy for text
containing these tags is to split the enclosed region into multiple subregions and apply the NLP tools
separately to each subregion.
2[U-compare]: http://u-compare.org/ / [Kachako]: http://kachako.org/kano/46
<?xml ?><document ?><title>Formal approaches ? </title><creator> ? </creator><abstract>This research ? </abstract><section><title>Introduction</title><para><p><text>New UI</text> is shown. The UI is more useful than XYZ<indexmark> ? </indexmark> in <cite>[ ? ]</cite><note>Notice that ? </note> and ? .</p></para></section><bibliography> ? </bibliography></document>
(a) XML document
?xml document
creator section bibliographytitle abstractThis research ? Formal ?
paratitleIntroduction
p
textNew UI indexmark? cite[?] noteNotice ? . and ? .inis ? XYZ
(b) Structure of XML document
Figure 3: Example XML document
Decoration tags, on the other hand, do not necessarily guarantee the independence of the enclosed
text regions, and are utilized mainly for embossing the regions visually, such as changing the font or
color of the text (?<text>? in the example), paragraphing sections3, and so on. The data conversion
strategy for text containing these tags is to remove the tags before inputting the text into the NLP tools,
and then to recover the tags afterwards4.
Regions enclosed by Object tags contain special descriptions for representing objects treated as single
syntactic components in the surrounding text. The regions do not consist of natural language text, and
therefore cannot be analyzed by NLP tools5. The data conversion strategy for text containing this tag is
to replace the enclosed region with some proper character sequence before inputting the text into NLP
tools, and then to recover the replaced region afterwards.
Regions enclosed by Meta-info tags are not targets of NLP tools, mainly because the regions are not
displayed, but utilized for other purposes, such as creating index pages (like this ?<indexmark>?)6. The
data conversion strategy for text containing these tags is to delete the tagged region before inputting the
text into NLP tools, and then to recover the region afterwards.
According to the above strategies, which are summarized in Table 1, conversion of the example text
in Figure 2(a) is carried out as follows. In the first step, tags are removed from the text, whilst retaining
their offsets in the resulting tag-less sequence shown in (b). ?Cite1? in the sequence is a plain word
utilized to replace the ?<cite>? tag region. For the ?<note>? tag, we recursively apply our strategies
to its inner region, with two plain text regions ?New UI ...? and ?Notice that ...? consequently input into
the sentence splitter. Thereafter, sentence boundary information is returned as shown in (c), and finally,
using the retained offset information of the tags, the obtained analysis and original tag information are
integrated to produce the XML-tagged sequence shown in (d).
3.2 Procedure for Efficient Tag Classification and Data Conversion
In actual XML documents as shown in Figure 3(a), a number of tags are introduced and tagged regions
are multi-layered as illustrated in Figure 3(b) (where black and white boxes represent, respectively, XML
tags and plain text regions, and regions enclosed by tags are placed below the tags in the order they
appear.). We implemented a complete data conversion procedure for efficiently classifying tags in text
documents into the four types and simultaneously obtaining plain text sequences from such documents,
3In some types of scientific articles, one sentence can be split into two paragraph regions. It depends on the target text
whether a paragraph tag is classified as Independent or Decoration.
4The tags may imply that the enclosed regions constitute chunks of text, which may be suitable for use in NLP tools.
5In some cases, natural language text is used for parts of the descriptions, for example, itemization or tables in scientific
articles. How the inner textual parts are generally associated with the surrounding text would be discussed in our future work.
For the treatment of list structures, we can learn more from A??t-Mokhtar et al. (2003).
6If the tagged region contains analyzable text, it depends on the user policy whether NLP tools should be applied to the
region, that is, whether to classify the tag as Independent.47
@plain_text_sequences = ();   # plain text sequences input to NLP tools@recovery_info = ();                 # information for recovering original document after applying NLP tools@unknown = ();                        # unknown tags
function data_convert ($target_sequence, $seq_ID) {
if ($target_sequence contains any tags) {   # process one instance of tag usage in a target sequence$usage = (pick one instance of top-level tag usage in $target_sequence);$tag = (name of the top-level tag in $usage);@attributes = (attributes and their values for the top-level tag in $usage);$region = (region in $target_sequence enclosed by tag $tag in $usage); $tag_and_region =  (region in $target_sequence consisting of $region & tag $tag enclosing it);
if ($tag ?@independent)             { remove $tag_and_region from $target_sequence;add [?independent?, $tag, @attributes,  $seq_ID, $seq_ID + 1,(offset in $target_sequence where $tag_and_region should be inserted) ] to @recovery_info;data_convert($region,  $seq_ID + 1);  } # process the tagged region separatelyelse if ($tag ?@decoration)         { remove only tag $tag enclosing $region from $target_sequence; add [?decoration?, $tag, @attributes,(offsets in $target_sequence where $region begans and ends)] to @recovery_info; }else if ($tag ?@object)                 { replace $tag_and_region in $target_sequence with a unique plain word $uniq;add [?object?, $uniq, $tag_and_region] to @recovery_info; }else if ($tag ?@meta_info)          { remove $tag_and_region from $target_sequence;add [?meta_info?, $tag_and_region,(offset in $target_sequence where $tag_and_region should be inserted)] to @recovery_info; }else                                                     { replace $tag_and_region in $target_sequence with a unique plain word $uniq;add [?unknown?, $uniq, $tag_and_region] to @recovery_info;if ($tag ?@unknown) { add $tag to @unknown; }  }
data_convert($target_sequence, $seq_ID);  # process the remaining tags}else {  # a plain text sequence is obtainedadd [$seq_id,  $target_sequence] to @plain_text_sequences;} }
function main ($XML_document) {data_convert ($XML_document, 0);return @plain_text_sequences, @recovery_info, @unknown;}
Figure 4: Pseudo-code algorithm for data conversion from XML text to plain text for use in NLP tools
as given by the pseudo-code algorithm in Figure 4. In the remainder of this section, we explain how the
algorithm works.
Our data conversion procedure applies the strategies for the four types of tags recursively from the
top-level tags to the lower tags. The @independent, @decoration, @object and @meta info lists
contain, respectively, Independent, Decoration, Object, and Meta-info tags, which have already been
classified by the user. When applied to a target document, the algorithm uses the four lists and strategies
given in the previous section in its first attempt at converting the document into plain text sequences,
storing unknown (and therefore unprocessed) tags, if any, in @unknown. After the entire document has
been processed for the first time, the user classifies any reported unknown tags. This process is repeated
until no further unknown tags are encountered.
In the first iteration of processing the document in Figure 3(a) the algorithm is applied to the target
document with the four tag lists empty. In the function ?data convert?, top-level tags in the document,
?<?xml>? and ?<document>?, are detected as yet-to-be classified tags and added to @unknown.
The tags and their enclosed regions in the target document are replaced with unique plain text such as
?UN1? and ?UN2?, and the input text thus becomes a sequence consisting of only plain words like ?UN1
UN2?. The algorithm then adds the sequence to @plain text sequences and terminates. The user then
classifies the reported yet-to-be classified tags in @unknown into the four tag lists, and the algorithm48
Article # ar- # total tags # classified tags (# types) #obtain-
type ticles (# types) I D O M Total ed seq.
PMC 1,000 1,357,229( 421) 32,109(12) 62,414( 8) 48205( 9) 33,953(56) 176,681( 85) 25,679
ArX. 300 1,969,359(210?) 5,888(15) 46,962(12) 60,194( 8) 7,960(17) 121,004( 52) 4,167
ACL 67 130,861( 66?) 3,240(24) 14,064(29) 4,589(15) 2,304(19) 24,197( 87) 2,293
Wiki. 300 223,514( 60?) 3,530(12) 11,197( 8) 1,470(28) 11,360(67) 27,557(115) 2,286
(ArX.: arXiv.org, Wiki.: Wikipedia, I: Independent, D: Decoration, O: Object, M: Meta-info)
Table 2: Classified tags and obtained sequences for each type of article
Treat- Parsing with Enju parser Parsing with Stanford parserArticle ed tag * # sen- ** Time Avg. # failures * # sen- ** Time Avg. # failures
type classes tences (s) (**/*) (rate) tences (s) (**/*) (rate)
None 159,327 209,783 1.32 4,721 ( 2.96%) 170,999 58,865 0.39 18,621 (10.89%)
PMC O/M 112,285 135,752 1.21 810 ( 0.72%) 126,176 50,741 0.44 11,881 ( 9.42%)
All 126,215 132,250 1.05 699 ( 0.55%) 139,805 63,295 0.49 11,338 ( 8.11%)
None 74,762 108,831 1.46 2,047 ( 2.74%) 75,672 27,970 0.43 10,590 (13.99%)
ArX. O/M 41,265 89,200 2.16 411 ( 1.00%) 48,666 24,630 0.57 5,457 (11.21%)
All 43,208 87,952 2.04 348 ( 0.81%) 50,504 26,360 0.58 5,345 (10.58%)
None 19,571 15,142 0.77 115 ( 0.59%) 17,166 5,047 0.29 1,095 ( 6.38%)
ACL O/M 9,819 9,481 0.97 63 ( 0.64%) 11,182 4,157 0.37 616 ( 5.51%)
All 11,136 8,482 0.76 39 ( 0.35%) 12,402 4,871 0.39 587 ( 4.73%)
None 10,561 14,704 1.39 1,161 (10.99%) 14,883 3,114 0.24 1,651 (11.09%)
Wiki. O/M 5,026 6,743 1.34 67 ( 1.33%) 6,173 2,248 0.38 282 ( 4.57%)
All 6,893 6,058 0.88 61 ( 0.88%) 8,049 2,451 0.31 258 ( 3.21%)
(ArX.: arXiv, Wiki.: Wikipedia, O/M: Object and Meta-info)
Table 3: Impact on parsing performance of plain text sequences extracted using classified tags
starts its second iteration7.
In the case of Independent/Decoration tags, the algorithm splits the regions enclosed by the
tags/removes only the tags from the target text, and recursively processes the obtained text sequence(s)
according to our strategies. In the splitting/removal operation, the algorithm stores in @recovery info,
the locations (offsets) in the obtained text where the tags should be inserted in order to recover the tags
and textual structures after applying the NLP tools. In the case of Object/Meta-info tags, regions en-
closed by these tags are replaced with unique plain text/omitted from the target text, which means that the
inner regions are not unpacked and processed (with relevant information about the replacement/omitting
process also stored in @recovery info). This avoids unnecessary classification tasks for tags that are
utilized only in the regions, and therefore minimizes user effort.
When no further unknown tags are reported, sufficient tag classification has been done to obtain plain
text sequences for input into NLP tools, with the sequences already stored in @plain text sequences.
After applying NLP tools to the obtained sequences, @recovery info is used to integrate the anno-
tated output from the tools into the original XML document by merging the offset information8, and
consequently to recover the structure of the original document.
4 Experiments
We investigated whether the algorithm introduced in Section 3.2 is robustly applicable to different types
of XML documents and whether the obtained text sequences are adequate for input into NLP tools. The
results of this investigation highlight the significance of bridging real-world text and NLP technologies.
4.1 Target Documents
Our algorithm was applied to four types of XML documents: three types of scientific ar-
ticles, examples of which were, respectively, downloaded from PubMed Central (PMC)
(http://www.ncbi.nlm.nih.gov/pmc/tools/ftp/), arXiv.org (http://arxiv.org/) and ACL Anthology
7The user can delay the classification for some tags to later iterations.
8When crossover of tag regions occur, the region in the annotated output is divided into subregions at the crossover point.49
(http://anthology.aclweb.org/)9, and a web page type, examples of which were downloaded from
Wikipedia (http://www.wikipedia.org/). The articles obtained from PMC were originally given in an
XML format, while those from arXiv.org and ACL Anthology were given in XHTML (based on XML),
and those from Wikipedia were given in HTML, with the HTML articles generated via intermediate
XML files. These four types of articles were therefore more or less based on valid XML (or XML-like)
formats. For our experiments, we randomly selected 1,000 PMC articles, randomly selected 300
arXiv.org articles, collected 67 (31 long and 36 short) ACL 2014 conference papers without any
conversion errors (see Footnote 9), and randomly downloaded 300 Wikipedia articles.
Each of the documents contained a variety of textual parts; we decided to apply the NLP tools to the
titles of the articles and sections, abstracts, and body text of the main sections in the scientific articles,
and to the titles of the articles, body text headings, and the actual body text of the Wikipedia articles.
According to these policies, we classified the tags appearing in all articles of each type.
4.2 Efficiency of Tag Classification
Table 2 summarizes the classified tags and obtained sequences for each type of document. The second
to ninth columns give the numbers of utilized articles, tags (in tokens and types) in the documents, each
type of tag actually classified and processed, and obtained text sequences, respectively10. Using simple
regular-expression matching, we found no remaining tagged regions in the obtained sequences. From
this we concluded that our framework at least succeeded in converting XML-tagged text into plain text.
For the PMC articles, we obtained plain text sequences by classifying only a fifth or less of the total
number of tag types, that is, focusing on less than 15% of the total tag occurrences in the documents
(comparing the third and eighth columns). This is because the tags within the regions enclosed by
Object and Meta-info tags were not considered by our procedure. For each of the arXiv.org, ACL and
Wikipedia articles, a similar effect was implied by the fact that the number of classified tags was less
than 20% of the total occurrences of all tags.
4.3 Adequacy of Obtained Sequences for Use in NLP Tools
We randomly selected several articles from each article type, and confirmed that the obtained text se-
quences consisted of valid sentences, which could be directly input into NLP tools and which thoroughly
covered the content of the original articles. Then, to evaluate the impact of this adequacy in a more prac-
tical situation, we input the obtained sequences (listed in Table 2) into two typical parsers, namely, the
Enju parser for deep syntactic/semantic analysis, and the Stanford parser (de Marneffe et al., 2006)11 for
phrase structure and dependency analysis12. Table 3 compares the parsing performance on three types
of plain text sequences obtained by different strategies: simply removing all tags, processing Object
and Meta-info tags using our framework and removing the remaining tags, and processing all the tags
using our framework. For each combination of parser and article type, we give the number of detected
sentences13, the total parsing time, the average parsing time per sentence14, and the number/ratio of
sentences that could not be parsed15.
For all article types, the parsers, especially the Enju parser, succeeded in processing the entire article
with much higher coverage (see the fourth column for each parser) and in much less time (see the third
9The XHTML version of 178 ACL 2014 conference papers were available at ACL Anthology. Each of the XHTML files
was generated by automatic conversion of the original article using LaTeXML (http://dlmf.nist.gov/LaTeXML/).
10For Wikipedia, arXiv.org and ACL articles, since HTML/XHTML tag names represent more abstract textual functions,
the number of different tag types was much smaller than for PMC articles (see ? in the table). To better capture the textual
functions of the tagged regions, we used the combination of the tag name and its selected attributes as a single tag. The number
of classified tags for Wikipedia, arXiv.org and ACL given in the table reflects this decision.
11http://nlp.stanford.edu/software/lex-parser.shtml
12The annotated output from the parsers was integrated into the original XML documents by merging the offset information,
and the structures of the original documents were consequently recovered. The recovered structures were input to xmllint, a
UNIX tool for parsing XML documents, and the tool succeeded in parsing the structures without detecting any error.
13For the Enju parser, we split each text sequence into sentences using GeniaSS [http://www.nactem.ac.uk/y-matsu/geniass/].
14For the Enju parser, the time spent parsing failed sentences was also considered.
15For the Stanford parser, the maximum sentence length was limited to 50 words using the option settings because several
sentences caused parsing failures, even after increasing the memory size from 150 MB to 2 GB, which terminated the whole
process. 50
column for each parser) using the text sequences obtained by treating some (Object and Meta-info)
or all tags with our framework than with those sequences obtained by merely removing the tags. This
is mainly because the text sequences obtained by merely removing the tags contained some embedded
inserted sentences (specified by Independent tags), bare expressions consisting of non natural language
(non-NL) principles (specified by Object tags), and sequences not directly related to the displayed text
(specified by Meta-info tags), which confused the parsers. In particular, treating Object and Meta-info
tags drastically improved parsing performance, since non-NL tokens were excluded from the analysis.
Compared with treating Object/Meta-info tags, treating all tags, that is, additionally treating Inde-
pendent tags and removing the remaining tags as Decoration tags, increased the number of detected
sentences. This is because Independent tags provide solid information for separating text sequences
into shorter sequences and thus prompting the splitting of sequences into shorter sentences, which de-
creased parsing failure by preventing a lack of search space for the Enju parser and by increasing target
(? 50 word) sentences for the Stanford parser. Treating all tags increased the total time for the Stanford
parser since a decrease in failed (> 50 word) sentences directly implied an increase in processing cost,
whereas, for the Enju parser, the total time decreased since the shortened sentences drastically narrowed
the required search space.
4.4 Significance of Bridging Real-world Documents and NLP Technologies
As demonstrated above, the parsers succeeded in processing the entire article with much higher coverage
and in much less time with the text sequences obtained by our framework than with those sequences
obtained by merely removing the tags. Then, what does such thorough and efficient processing bring
about? If our target is shallow analysis of documents which can be achieved by simple approaches such
as counting words, removing tags will suffice; embedded sentences do not affect word count, and non-NL
sequences can be canceled by a large amount of valid sequences in the target documents.
Such shallow approaches, however, cannot satisfy the demands on more detailed or precise analysis
of documents: discourse analysis, translation, grammar extraction, and so on. In order to be sensitive to
subtle signs from the documents, information uttered even in small parts of text cannot be overlooked,
under the condition that sequences other than body text are excluded.
This process of plain text extraction is a well-established procedure in NLP research; in order to
concentrate on precise analysis of natural language phenomena, datasets have been arranged in the format
of plain text sequences, and, using those datasets, plenty of remarkable achievements have been reported
in various NLP tasks while brand-new tasks have been found and tackled.
But what is the ultimate goal of these challenges? Is it to just parse carefully arranged datasets? We all
know this to be just a stepping stone to the real goal: to parse real-world, richly-formatted documents. As
we demonstrated, if supported by a proper framework, conventional NLP tools already have the ability
to process real-world text without significant loss of performance. Adequately bridging target real-world
documents and NLP technologies is thus a crucial task for taking advantage of full benefit brought by
NLP technologies in ubiquitous application of NLP.
5 Conclusion
We proposed a framework for data conversion between XML-tagged text and input/output formats of
NLP tools. In our framework, once each tag utilized in the XML-tagged text has been classified as one
of the four types of textual functions, the conversion is automatically done according to the classification.
In the experiments, we applied our framework to several types of documents, and succeeded in obtaining
plain text sequences from these documents by classifying only a fifth of the total number of tag types
in the documents. We also observed that with the obtained sequences, the target documents were much
more thoroughly and efficiently processed by parsers than with naively tag-removed text. These results
emphasize the significance of bridging real-world documents and NLP technologies.
We are now ready for public release of a tool for conversion of XML documents into plain text se-
quences utilizing our framework. We would like to share further discussion on applying NLP tools to
various real-world documents for increased benefits from NLP.51
Acknowledgements
This research was partially supported by ?Data Centric Science: Research Commons? at the Research
Organization of Information and Systems (ROIS), Japan.
References
Salah A??t-Mokhtar, Veronika Lux, and ?Eva Ba?nik. 2003. Linguistic parsing of lists in structured documents. In
Proceedings of Language Technology and the Semantic Web: 3rd Workshop on NLP and XML (NLPXML-2003),
Budapest, Hungary, April.
?. Andersen, J. Nioche, E.J. Briscoe, and J. Carroll. 2008. The BNC parsed with RASP4UIMA. In Proceedings
of the 6th Language Resources and Evaluation Conference (LREC 2008), pages 865?869, Marrakech, Morocco,
May.
Stephen Clark and James R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4):493?552.
H. Cunningham, V. Tablan, A. Roberts, and K. Bontcheva. 2013. Getting more out of biomedical documents with
GATE?s full lifecycle open source text analytics. PLoS Comput Biol, 9(2).
Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency
parses from phrase structure parses. In Proceedings of the 5th Language Resources and Evaluation Conference
(LREC 2006), pages 449?454, Genoa, Italy, May.
David Ferruci, Adam Lally, Daniel Gruhl, Edward Epstein, Marshall Schor, J. William Murdock, Andy Frenkiel,
Eric W. Brown, Thomas Hampp, Yurdaer Doganata, Christopher Welty, Lisa Amini, Galina Kofman, Lev Koza-
kov, and Yosi Mass. 2006. Towards an interoperability standard for text and multi-modal analytics. Technical
Report RC24122, IBM Research Report.
Yoshinobu Kano, Makoto Miwa, Kevin Cohen, Larry Hunter, Sophia Ananiadou, and Jun?ichi Tsujii. 2011.
U-Compare: a modular NLP workflow construction and evaluation system. IBM Journal of Research and
Development, 55(3):11:1?11:10.
Yoshinobu Kano. 2012. Kachako: a hybrid-cloud unstructured information platform for full automation of service
composition, scalable deployment and evaluation. In Proceedings in the 1st International Workshop on Analyt-
ics Services on the Cloud (ASC), the 10th International Conference on Services Oriented Computing (ICSOC
2012), Shanghai, China, November.
Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii. 2007. A log-linear model with an
n-gram reference distribution for accurate HPSG parsing. In Proceedings of the 10th International Conference
on Parsing Technologies (IWPT?07), Prague, Czech Republic, June.
52
