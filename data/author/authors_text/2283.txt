Proceedings of the 12th Conference of the European Chapter of the ACL, pages 238?245,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Effects of Word Confusion Networks on Voice Search
Junlan Feng, Srinivas Bangalore
AT&T Labs-Research
Florham Park, NJ, USA
junlan,srini@research.att.com
Abstract
Mobile voice-enabled search is emerging
as one of the most popular applications
abetted by the exponential growth in the
number of mobile devices. The automatic
speech recognition (ASR) output of the
voice query is parsed into several fields.
Search is then performed on a text corpus
or a database. In order to improve the ro-
bustness of the query parser to noise in the
ASR output, in this paper, we investigate
two different methods to query parsing.
Both methods exploit multiple hypotheses
from ASR, in the form of word confusion
networks, in order to achieve tighter cou-
pling between ASR and query parsing and
improved accuracy of the query parser. We
also investigate the results of this improve-
ment on search accuracy. Word confusion-
network based query parsing outperforms
ASR 1-best based query-parsing by 2.7%
absolute and the search performance im-
proves by 1.8% absolute on one of our data
sets.
1 Introduction
Local search specializes in serving geographi-
cally constrained search queries on a structured
database of local business listings. Most text-
based local search engines provide two text fields:
the ?SearchTerm? (e.g. Best Chinese Restau-
rant) and the ?LocationTerm? (e.g. a city, state,
street address, neighborhood etc.). Most voice-
enabled local search dialog systems mimic this
two-field approach and employ a two-turn dia-
log strategy. The dialog system solicits from the
user a LocationTerm in the first turn followed by a
SearchTerm in the second turn (Wang et al, 2008).
Although the two-field interface has been
widely accepted, it has several limitations for mo-
bile voice search. First, most mobile devices are
location-aware which obviates the need to spec-
ify the LocationTerm. Second, it?s not always
straightforward for users to be aware of the dis-
tinction between these two fields. It is com-
mon for users to specify location information in
the SearchTerm field. For example, ?restaurants
near Manhattan? for SearchTerm and ?NY NY?
for LocationTerm. For voice-based search, it is
more natural for users to specify queries in a sin-
gle utterance1. Finally, many queries often con-
tain other constraints (assuming LocationTerm is a
constraint) such as that deliver in restaurants that
deliver or open 24 hours in night clubs open 24
hours. It would be very cumbersome to enumerate
each constraint as a different text field or a dialog
turn. An interface that allows for specifying con-
straints in a natural language utterance would be
most convenient.
In this paper, we introduce a voice-based search
system that allows users to specify search requests
in a single natural language utterance. The out-
put of ASR is then parsed by a query parser
into three fields: LocationTerm, SearchTerm,
and Filler. We use a local search engine,
http://www.yellowpages.com/, which accepts the
SearchTerm and LocationTerm as two query fields
and returns the search results from a business list-
ings database. We present two methods for pars-
ing the voice query into different fields with par-
ticular emphasis on exploiting the ASR output be-
yond the 1-best hypothesis. We demonstrate that
by parsing word confusion networks, the accuracy
of the query parser can be improved. We further
investigate the effect of this improvement on the
search task and demonstrate the benefit of tighter
coupling of ASR and the query parser on search
accuracy.
The paper outline is as follows. In Section 2, we
discuss some of the related threads of research rel-
evant for our task. In Section 3, we motivate the
need for a query parsing module in voice-based
search systems. We present two different query
parsing models in Section 4 and Section 5 and dis-
cuss experimental results in Section 6. We sum-
marize our results in Section 7.
1Based on the returned results, the query may be refined
in subsequent turns of a dialog.
238
2 Related Work
The role of query parsing can be considered as
similar to spoken language understanding (SLU)
in dialog applications. However, voice-based
search systems currently do not have SLU as a
separate module, instead the words in the ASR
1-best output are directly used for search. Most
voice-based search applications apply a conven-
tional vector space model (VSM) used in infor-
mation retrieval systems for search. In (Yu et al,
2007), the authors enhanced the VSM by deem-
phasizing term frequency in Listing Names and
using character level instead of word level uni/bi-
gram terms to improve robustness to ASR errors.
While this approach improves recall it does not
improve precision. In other work (Natarajan et
al., 2002), the authors proposed a two-state hidden
Markov model approach for query understanding
and speech recognition in the same step (Natarajan
et al, 2002).
There are two other threads of research liter-
ature relevant to our work. Named entity (NE)
extraction attempts to identify entities of interest
in speech or text. Typical entities include loca-
tions, persons, organizations, dates, times mon-
etary amounts and percentages (Kubala et al,
1998). Most approaches for NE tasks rely on ma-
chine learning approaches using annotated data.
These algorithms include a hidden Markov model,
support vector machines, maximum entropy, and
conditional random fields. With the goal of im-
proving robustness to ASR errors, (Favre et al,
2005) described a finite-state machine based ap-
proach to take as input ASR n-best strings and ex-
tract the NEs. Although our task of query segmen-
tation has similarity with NE tasks, it is arguable
whether the SearchTerm is a well-defined entity,
since a user can provide varied expressions as they
would for a general web search. Also, it is not
clear how the current best performing NE methods
based on maximum entropy or conditional ran-
dom fields models can be extended to apply on
weighted lattices produced by ASR.
The other related literature is natural language
interface to databases (NLIDBs), which had been
well-studied during 1960s-1980s (Androutsopou-
los, 1995). In this research, the aim is to map
a natural language query into a structured query
that could be used to access a database. However,
most of the literature pertains to textual queries,
not spoken queries. Although in its full general-
1?bestWCN Query
ParsedQueryParserSpeech SearchASR
Figure 1: Architecture of a voice-based search sys-
tem
ity the task of NLIDB is significantly more ambi-
tious than our current task, some of the challeng-
ing problems (e.g. modifier attachment in queries)
can also be seen in our task as well.
3 Voice-based Search System
Architecture
Figure 1 illustrates the architecture of our voice-
based search system. As expected the ASR and
Search components perform speech recognition
and search tasks. In addition to ASR and Search,
we also integrate a query parsing module between
ASR and Search for a number of reasons.
First, as can be expected the ASR 1-best out-
put is typically error-prone especially when a user
query originates from a noisy environment. How-
ever, ASR word confusion networks which com-
pactly encode multiple word hypotheses with their
probabilities have the potential to alleviate the er-
rors in a 1-best output. Our motivation to intro-
duce the understanding module is to rescore the
ASR output for the purpose of maximizing search
performance. In this paper, we show promising
results using richer ASR output beyond 1-best hy-
pothesis.
Second, as mentioned earlier, the query parser
not only provides the search engine ?what? and
?where? information, but also segments the query
to phrases of other concepts. For the example we
used earlier, we segment night club open 24 hours
into night club and open 24 hours. Query seg-
mentation has been considered as a key step to
achieving higher retrieval accuracy (Tan and Peng,
2008).
Lastly, we prefer to reuse an existing local
search engine http://www.yellowpages.com/, in
which many text normalization, task specific tun-
ing, business rules, and scalability issues have
been well addressed. Given that, we need a mod-
ule to translate ASR output to the query syntax that
the local search engine supports.
In the next section, we present our proposed ap-
proaches of how we parse ASR output including
ASR 1-best string and lattices in a scalable frame-
work.
239
4 Text Indexing and Search-based Parser
(PARIS)
As we discussed above, there are many potential
approaches such as those for NE extraction we can
explore for parsing a query. In the context of voice
local search, users expect overall system response
time to be similar to that of web search. Con-
sequently, the relatively long ASR latency leaves
no room for a slow parser. On the other hand,
the parser needs to be tightly synchronized with
changes in the listing database, which is updated
at least once a day. Hence, the parser?s training
process also needs to be quick to accomodate these
changes. In this section, we propose a probabilis-
tic query parsing approach called PARIS (parsing
using indexing and search). We start by presenting
a model for parsing ASR 1-best and extend the ap-
proach to consider ASR lattices.
4.1 Query Parsing on ASR 1-best output
4.1.1 The Problem
We formulate the query parsing task as follows.
A 1-best ASR output is a sequence of words:
Q = q1, q2, . . . , qn. The parsing task is to
segment Q into a sequence of concepts. Each
concept can possibly span multiple words. Let
S = s1, s2, . . . , sk, . . . , sm be one of the possible
segmentations comprising of m segments, where
sk = qij = qi, . . . qj , 1 ? i ? j ? n + 1. The
corresponding concept sequence is represented as
C = c1, c2, . . . , ck, . . . , cm.
For a given Q, we are interested in searching
for the best segmentation and concept sequence
(S?, C?) as defined by Equation 1, which is rewrit-
ten using Bayes rule as Equation 2. The prior
probability P (C) is approximated using an h-
gram model on the concept sequence as shown
in Equation 3. We model the segment sequence
generation probability P (S|C) as shown in Equa-
tion 4, using independence assumptions. Finally,
the query terms corresponding to a segment and
concept are generated using Equations 5 and 6.
(S?, C?) = argmax
S,C
P (S,C) (1)
= argmax
S,C
P (C) ? P (S|C) (2)
P (C) = P (c1) ?
m?
i
P (ci|c
i?h+1
i?1 ) (3)
P (S|C) =
m?
k=1
P (sk | ck) (4)
P (sk|ck) = P (q
i
j |ck) (5)
P (qij |ck) = Pck(qi) ?
j?
l=i+1
Pck(ql | q
l?k+1
l?1 ) (6)
To train this model, we only have access to text
query logs from two distinct fields (SearchTerm,
LocationTerm) and the business listing database.
We built a SearchTerm corpus by including valid
queries that users typed to the SearchTerm field
and all the unique business listing names in the
listing database. Valid queries are those queries
for which the search engine returns at least one
business listing result or a business category. Sim-
ilarly, we built a corpus for LocationTerm by con-
catenating valid LocationTerm queries and unique
addresses including street address, city, state, and
zip-code in the listing database. We also built a
small corpus for Filler, which contains common
carrier phrases and stop words. The generation
probabilities as defined in 6 can be learned from
these three corpora.
In the following section, we describe a scalable
way of implementation using standard text indexer
and searcher.
4.1.2 Probabilistic Parsing using Text Search
We use Apache-Lucene (Hatcher and Gospod-
netic, 2004), a standard text indexing and search
engines for query parsing. Lucene is an open-
source full-featured text search engine library.
Both Lucene indexing and search are efficient
enough for our tasks. It takes a few milliseconds
to return results for a common query. Indexing
millions of search logs and listings can be done
in minutes. Reusing text search engines allows
a seamless integration between query parsing and
search.
We changed the tf.idf based document-term
relevancy metric in Lucene to reflect P (qij |ck) us-
ing Relevancy as defined below.
P (qij |ck) = Relevancy(q
i
j , dk) =
tf(qij , dk) + ?
N
(7)
where dk is a corpus of examples we collected for
the concept ck; tf(qij , dk) is referred as the term
frequency, the frequency of qij in dk;N is the num-
ber of entries in dk; ? is an empirically determined
smoothing factor.
240
0 1
gary/0.323
cherry/4.104
dairy/1.442
jerry/3.956
2
crites/0.652
christ/2.857
creek/3.872
queen/1.439
kreep/4.540
kersten/2.045
3springfield/0.303in/1.346 4
springfield/1.367
_epsilon/0.294 5/1
missouri/7.021
Figure 2: An example confusion network for ?Gary crities Springfield Missouri?
Inputs:
? A set of K concepts:C = c1, c2, . . . , cK ,
in this paper, K = 3, c1 =
SearchTerm, c2 = LocationTerm,
c3 = Filler
? Each concept ck associates with a text
corpus: dk. Corpora are indexed using
Lucene Indexing.
? A given query: Q = q1, q2, . . . , qn
? A given maximum number of words in a
query segment: Ng
Parsing:
? Enumerate possible segments in Q up to
Ng words long: qij = qi, qi+1, . . . , qj ,
j >= i, |j ? i| < Ng
? Obtain P (qij |ck)) for each pair of ck and
qij using Lucene Search
? Boost P (qij |ck)) based on the position of
qij in the query P (q
i
j |ck) = P (q
i
j |ck) ?
boostck(i, j, n)
? Search for the best segment sequence
and concept sequence using Viterbi
search
Fig.3. Parsing procedure using Text Indexer and
Searcher
pck(q
i
j) =
tf(qii ? dis(i, j), dk) + ?
N ? shift
(8)
When tf(qij , dk) is zero for all concepts, we
loosen the phrase search to be proximity search,
which searches words in qij within a specific dis-
tance. For instance, ?burlington west virginia? ?
5 will find entries that include these three words
within 5 words of each other. tf(qij , dk) is dis-
counted for proximity search. For a given qij , we
allow a distance of dis(i, j) = (j ? i + shift)
words. shift is a parameter that is set empirically.
The discounting formula is given in 8.
Figure 3 shows the procedure we use for pars-
ing. It enumerates possible segments qij of a given
Q. It then obtains P (qij |ck) using Lucene Search.
We boost pck(q
i
j)) based on the position of q
i
j in
Q. In our case, we simply set: boostck(i, j, n) = 3
if j = n and ck = LocationTerm. Other-
wise, boostck(i, j, n) = 1. The algorithm searches
for the best segmentation using the Viterbi algo-
rithm. Out-of-vocabulary words are assigned to c3
(Filler).
4.2 Query Parsing on ASR Lattices
Word confusion networks (WCNs) is a compact
lattice format (Mangu et al, 2000). It aligns a
speech lattice with its top-1 hypothesis, yielding
a ?sausage?-like approximation of lattices. It has
been used in applications such as word spotting
and spoken document retrieval. In the following,
we present our use of WCNs for query parsing
task.
Figure 2 shows a pruned WCN example. For
each word position, there are multiple alternatives
and their associated negative log posterior proba-
bilities. The 1-best path is ?Gary Crites Spring-
field Missouri?. The reference is ?Dairy Queen
in Springfield Missouri?. ASR misrecognized
?Dairy Queen? as ?Gary Crities?. However, the
correct words ?Dairy Queen? do appear in the lat-
tice, though with lower probability. The challenge
is to select the correct words from the lattice by
considering both ASR posterior probabilities and
parser probabilities.
The hypotheses in WCNs have to be reranked
241
by the Query Parser to prefer those that have
meaningful concepts. Clearly, each business name
in the listing database corresponds to a single con-
cept. However, the long queries from query logs
tend to contain multiple concepts. For example, a
frequent query is ?night club for 18 and up?. We
know ?night club? is the main subject. And ?18
and up? is a constraint. Without matching ?night
club?, any match with ?18 and up? is meaning-
less. The data fortunately can tell us which words
are more likely to be a subject. We rarely see ?18
and up? as a complete query. Given these observa-
tions, we propose calculating the probability of a
query term to be a subject. ?Subject? here specif-
ically means a complete query or a listing name.
For the example shown in Figure 2, we observe the
negative log probability for ?Dairy Queen? to be a
subject is 9.3. ?Gary Crites? gets 15.3. We refer
to this probability as subject likelihood. Given a
candidate query term s = w1, w2, ..wm, we repre-
sent the subject likelihood as Psb(s). In our exper-
iments, we estimate Psb using relative frequency
normorlized by the length of s. We use the follow-
ing formula to combine it with posterior probabil-
ities in WCNs Pcf (s):
P (s) = Pcf (s) ? Psb(s)
?
Pcf (s) =
?
j=1,...,nw
Pcf (wi)
where ? is used to flatten ASR posterior proba-
bilities and nw is the number of words in s. In
our experiments, ? is set to 0.5. We then re-rank
ASR outputs based on P (s). We will report ex-
perimental results with this approach. ?Subject?
is only related to SearchTerm. Considering this,
we parse the ASR 1-best out first and keep the
Location terms extracted as they are. Only word
alternatives corresponding to the search terms are
used for reranking. This also improves speed,
since we make the confusion network lattice much
smaller. In our initial investigations, such an ap-
proach yields promising results as illustrated in the
experiment section.
Another capability that the parser does for both
ASR 1-best and lattices is spelling correction. It
corrects words such as restaurants to restaurants.
ASR produces spelling errors because the lan-
guage model is trained on query logs. We need
to make more efforts to clean up the query log
database, though progresses had been made.
5 Finite-state Transducer-based Parser
In this section, we present an alternate method for
parsing which can transparently scale to take as in-
put word lattices from ASR. We encode the prob-
lem of parsing as a weighted finite-state transducer
(FST). This encoding allows us to apply the parser
on ASR 1-best as well as ASR WCNs using the
composition operation of FSTs.
We formulate the parsing problem as associat-
ing with each token of the input a label indicating
whether that token belongs to one of a business
listing (bl), city/state (cs) or neither (null). Thus,
given a word sequence (W = w1, . . . , wn) output
from ASR, we search of the most likely label se-
quence (T = t1, . . . , tn), as shown in Equation 9.
We use the joint probability P (W,T ) and approx-
imate it using an k-gram model as shown in Equa-
tions 10,11.
T ? = argmax
T
P (T |W ) (9)
= argmax
T
P (W,T ) (10)
= argmax
T
n?
i
P (wi, ti | w
i?k+1
i?1 , t
i?k+1
i?1 )
(11)
A k-gram model can be encoded as a weighted
finite-state acceptor (FSA) (Allauzen et al, 2004).
The states of the FSA correspond to the k-gram
histories, the transition labels to the pair (wi, ti)
and the weights on the arcs are ?log(P (wi, ti |
wi?k+1i?1 , t
i?k+1
i?1 )). The FSA also encodes back-off
arcs for purposes of smoothing with lower order k-
grams. An annotated corpus of words and labels is
used to estimate the weights of the FSA. A sample
corpus is shown in Table 1.
1. pizza bl hut bl new cs york cs new cs
york cs
2. home bl depot bl around null
san cs francisco cs
3. please null show null me null indian bl
restaurants bl in null chicago cs
4. pediatricians bl open null on null
sundays null
5. hyatt bl regency bl in null honolulu cs
hawaii cs
Table 1: A Sample set of annotated sentences
242
The FSA on the joint alphabet is converted into
an FST. The paired symbols (wi, ti) are reinter-
preted as consisting of an input symbol wi and
output symbol ti. The resulting FST (M ) is used
to parse the 1-best ASR (represented as FSTs
(I)), using composition of FSTs and a search for
the lowest weight path as shown in Equation 12.
The output symbol sequence (pi2) from the lowest
weight path is T ?.
T ? = pi2(Bestpath(I ?M)) (12)
Equation 12 shows a method for parsing the 1-
best ASR output using the FST. However, a simi-
lar method can be applied for parsing WCNs. The
WCN arcs are associated with a posterior weight
that needs to be scaled suitably to be comparable
to the weights encoded in M . We represent the re-
sult of scaling the weights in WCN by a factor of
? asWCN?. The value of the scaling factor is de-
termined empirically. Thus the process of parsing
a WCN is represented by Equation 13.
T ? = pi2(Bestpath(WCN
? ?M)) (13)
6 Experiments
We have access to text query logs consisting of 18
million queries to the two text fields: SearchTerm
and LocationTerm. In addition to these logs, we
have access to 11 million unique business listing
names and their addresses. We use the combined
data to train the parameters of the two parsing
models as discussed in the previous sections. We
tested our approaches on three data sets, which in
total include 2686 speech queries. These queries
were collected from users using mobile devices
from different time periods. Labelers transcribed
and annotated the test data using SearchTerm and
LocationTerm tags.
Data Sets Number of WACC
Speech Queries
Test1 1484 70.1%
Test2 544 82.9%
Test3 658 77.3%
Table 2: ASR Performance on three Data Sets
We use an ASR with a trigram-based language
model trained on the query logs. Table 2 shows the
ASR word accuracies on the three data sets. The
accuracy is the lowest on Test1, in which many
users were non-native English speakers and a large
percentage of queries are not intended for local
search.
We measure the parsing performance in terms
of extraction accuracy on the two non-filler slots:
SearchTerm and LocationTerm. Extraction accu-
racy computes the percentage of the test set where
the string identified by the parser for a slot is ex-
actly the same as the annotated string for that slot.
Table 3 reports parsing performance using the
PARIS approach for the two slots. The ?Tran-
scription? columns present the parser?s perfor-
mances on human transcriptions (i.e. word ac-
curacy=100%) of the speech. As expected, the
parser?s performance heavily relies on ASR word
accuracy. We achieved lower parsing perfor-
mance on Test1 compared to other test sets due
to lower ASR accuracy on this test set. The
promising aspect is that we consistently improved
SearchTerm extraction accuracy when usingWCN
as input. The performance under ?Oracle path?
column shows the upper bound for the parser us-
ing the oracle path2 from the WCN. We pruned
the WCN by keeping only those arcs that are
within cthresh of the lowest cost arc between
two states. Cthresh = 4 is used in our experi-
ments. For Test2, the upper bound improvement
is 7.6% (82.5%-74.9%) absolute. Our proposed
approach using pruned WCN achieved 2.7% im-
provement, which is 35% of the maximum poten-
tial gain. We observed smaller improvements on
Test1 and Test3. Our approach did not take advan-
tage of WCN for LocationTerm extraction, hence
we obtained the same performance with WCNs as
using ASR 1-best.
In Table 4, we report the parsing performance
for the FST-based approach. We note that the
FST-based parser on a WCN also improves the
SearchTerm and LocationTerm extraction accu-
racy over ASR 1-best, an improvement of about
1.5%. The accuracies on the oracle path and the
transcription are slightly lower with the FST-based
parser than with the PARIS approach. The per-
formance gap, however, is bigger on ASR 1-best.
The main reason is PARIS has embedded a module
for spelling correction that is not included in the
FST approach. For instance, it corrects nieman to
neiman. These improvements from spelling cor-
rection don?t contribute much to search perfor-
2Oracle text string is the path in the WCN that is closest
to the reference string in terms of Levenshtein edit distance
243
Data Sets SearchTerm Extraction Accuracy LocationTerm Extraction Accuracy
Input ASR WCN Oracle Transcription ASR WCN Oracle Transcription
1-best Path 4 1best Path 4
Test1 60.0% 60.7% 67.9% 94.1% 80.6% 80.6% 85.2% 97.5%
Test2 74.9% 77.6% 82.5% 98.6% 89.0% 89.0% 92.8% 98.7%
Test3 64.7% 65.7% 71.5% 96.7% 88.8% 88.8% 90.5% 97.4%
Table 3: Parsing performance using the PARIS approach
Data Sets SearchTerm Extraction Accuracy LocationTerm Extraction Accuracy
Input ASR WCN Oracle Transcription ASR WCN Oracle Transcription
1-best Path 4 1best Path 4
Test1 56.9% 57.4% 65.6% 92.2% 79.8% 79.8% 83.8% 95.1%
Test2 69.5% 71.0% 81.9% 98.0% 89.4% 89.4% 92.7% 98.5%
Test3 59.2% 60.6% 69.3% 96.1% 87.1% 87.1% 89.3% 97.3%
Table 4: Parsing performance using the FST approach
mance as we will see below, since the search en-
gine is quite robust to spelling errors. ASR gen-
erates spelling errors because the language model
is trained using query logs, where misspellings are
frequent.
We evaluated the impact of parsing perfor-
mance on search accuracy. In order to measure
search accuracy, we need to first collect a ref-
erence set of search results for our test utter-
ances. For this purpose, we submitted the hu-
man annotated two-field data to the search engine
(http://www.yellowpages.com/ ) and extracted the
top 5 results from the returned pages. The re-
turned search results are either business categories
such as ?Chinese Restaurant? or business listings
including business names and addresses. We con-
sidered these results as the reference search results
for our test utterances.
In order to evaluate our voice search system, we
submitted the two fields resulting from the query
parser on the ASR output (1-best/WCN) to the
search engine. We extracted the top 5 results from
the returned pages and we computed the Precision,
Recall and F1 scores between this set of results
and the reference search set. Precision is the ra-
tio of relevant results among the top 5 results the
voice search system returns. Recall refers to the
ratio of relevant results to the reference search re-
sult set. F1 combines precision and recall as: (2
* Recall * Precision) / (Recall + Precision) (van
Rijsbergen, 1979).
In Table 5 and Table 6, we report the search per-
formance using PARIS and FST approaches. The
overall improvement in search performance is not
Data Sets Precision Recall F1
ASR Test1 71.8% 66.4% 68.8%
1-best
Test2 80.7% 76.5% 78.5%
Test3 72.9% 68.8% 70.8%
WCN
Test1 70.8% 67.2% 69.0%
Test2 81.6% 79.0% 80.3%
Test3 73.0% 69.1% 71.0%
Table 5: Search performances using the PARIS ap-
proach
Data Sets Precision Recall F1
ASR Test1 71.6% 64.3% 67.8%
1-best
Test2 79.6% 76.0% 77.7%
Test3 72.9% 67.2% 70.0%
WCN
Test1 70.5% 64.7% 67.5%
Test2 80.3% 77.3% 78.8%
Test3 72.9% 68.1% 70.3%
Table 6: Search performances using the FST ap-
proach
as large as the improvement in the slot accura-
cies between using ASR 1-best and WCNs. On
Test1, we obtained higher recall but lower preci-
sion with WCN resulting in a slight decrease in
F1 score. For both approaches, we observed that
using WCNs consistently improves recall but not
precision. Although this might be counterintu-
itive, given that WCNs improve the slot accuracy
overall. One possible explanation is that we have
observed errors made by the parser using WCNs
are more ?severe? in terms of their relationship to
the original queries. For example, in one particular
244
case, the annotated SearchTerm is ?book stores?,
for which the ASR 1-best-based parser returned
?books? (due to ASR error) as the SearchTerm,
while the WCN-based parser identified ?banks?
as the SearchTerm. As a result, the returned re-
sults from the search engine using the 1-best-based
parser were more relevant compared to the results
returned by the WCN-based parser.
There are few directions that this observation
suggests. First, the weights on WCNs may need
to be scaled suitably to optimize the search per-
formance as opposed to the slot accuracy perfor-
mance. Second, there is a need for tighter cou-
pling between the parsing and search components
as the eventual goal for models of voice search is
to improve search accuracy and not just the slot
accuracy. We plan to investigate such questions in
future work.
7 Summary
This paper describes two methods for query pars-
ing. The task is to parse ASR output including 1-
best and lattices into database or search fields. In
our experiments, these fields are SearchTerm and
LocationTerm for local search. Our first method,
referred to as PARIS, takes advantage of a generic
search engine (for text indexing and search) for
parsing. All probabilities needed are retrieved on-
the-fly. We used keyword search, phrase search
and proximity search. The second approach, re-
ferred to as FST-based parser, which encodes the
problem of parsing as a weighted finite-state trans-
duction (FST). Both PARIS and FST successfully
exploit multiple hypotheses and posterior proba-
bilities from ASR encoded as word confusion net-
works and demonstrate improved accuracy. These
results show the benefits of tightly coupling ASR
and the query parser. Furthermore, we evaluated
the effects of this improvement on search perfor-
mance. We observed that the search accuracy im-
proves using word confusion networks. However,
the improvement on search is less than the im-
provement we obtained on parsing performance.
Some improvements the parser achieves do not
contribute to search. This suggests the need of
coupling the search module and the query parser
as well.
The two methods, namely PARIS and FST,
achieved comparable performances on search.
One advantage with PARIS is the fast training
process, which takes minutes to index millions
of query logs and listing entries. For the same
amount of data, FST needs a number of hours to
train. The other advantage is PARIS can easily
use proximity search to loosen the constrain of N-
gram models, which is hard to be implemented
using FST. FST, on the other hand, does better
smoothing on learning probabilities. It can also
more directly exploit ASR lattices, which essen-
tially are represented as FST too. For future work,
we are interested in ways of harnessing the bene-
fits of the both these approaches.
References
C. Allauzen, M. Mohri, M. Riley, and B. Roark. 2004.
A generalized construction of speech recognition
transducers. In ICASSP, pages 761?764.
I. Androutsopoulos. 1995. Natural language interfaces
to databases - an introduction. Journal of Natural
Language Engineering, 1:29?81.
B. Favre, F. Bechet, and P. Nocera. 2005. Robust
named entity extraction from large spoken archives.
In Proceeding of HLT 2005.
E. Hatcher and O. Gospodnetic. 2004. Lucene in Ac-
tion (In Action series). Manning Publications Co.,
Greenwich, CT, USA.
F. Kubala, R. Schwartz, R. Stone, and R. Weischedel.
1998. Named entity extraction from speech. In in
Proceedings of DARPA Broadcast News Transcrip-
tion and Understanding Workshop, pages 287?292.
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding con-
sensus in speech recognition: Word error minimiza-
tion and other applications of confusion networks.
Computation and Language, 14(4):273?400, Octo-
ber.
P. Natarajan, R. Prasad, R.M. Schwartz, and
J. Makhoul. 2002. A scalable architecture for di-
rectory assistance automation. In ICASSP 2002.
B. Tan and F. Peng. 2008. Unsupervised query seg-
mentation using generative language models and
wikipedia. In Proceedings of WWW-2008.
C.V. van Rijsbergen. 1979. Information Retrieval.
Boston. Butterworth, London.
Y. Wang, D. Yu, Y. Ju, and A. Alex. 2008. An intro-
duction to voice search. Signal Processing Magzine,
25(3):29?38.
D. Yu, Y.C. Ju, Y.Y. Wang, G. Zweig, and A. Acero.
2007. Automated directory assistance system - from
theory to practice. In Interspeech.
245
Proceedings of the Interactive Question Answering Workshop at HLT-NAACL 2006, pages 33?40,
New York City, NY, USA. June 2006. c?2006 Association for Computational Linguistics
A Data Driven Approach to Relevancy Recognition for
Contextual Question Answering
Fan Yang?
OGI School of Science & Engineering
Oregon Health & Science University
fly@cslu.ogi.edu
Junlan Feng and Giuseppe Di Fabbrizio
AT&T Labs - Research
180 Park Avenue, Florham Park, NJ, 07932 - USA
junlan@research.att.com, pino@research.att.com
Abstract
Contextual question answering (QA), in
which users? information needs are satis-
fied through an interactive QA dialogue,
has recently attracted more research atten-
tion. One challenge of engaging dialogue
into QA systems is to determine whether
a question is relevant to the previous inter-
action context. We refer to this task as rel-
evancy recognition. In this paper we pro-
pose a data driven approach for the task
of relevancy recognition and evaluate it
on two data sets: the TREC data and the
HandQA data. The results show that we
achieve better performance than a previ-
ous rule-based algorithm. A detailed eval-
uation analysis is presented.
1 Introduction
Question Answering (QA) is an interactive
human-machine process that aims to respond
to users? natural language questions with exact
answers rather than a list of documents. In the
last few years, QA has attracted broader research
attention from both the information retrieval
(Voorhees, 2004) and the computational linguistic
fields (http://www.clt.mq.edu.au/Events/
Conferences/acl04qa/). Publicly ac-
cessible web-based QA systems, such as
AskJeeves (http://www.ask.com/) and START
(http://start.csail.mit.edu/), have scaled up
?The work was done when the first author was visiting
AT&T Labs - Research.
this technology to open-domain solutions. More
task-oriented QA systems are deployed as virtual
customer care agents addressing questions about
specific domains. For instance, the AT&T Ask
Allier agent (http://www.allie.att.com/) is
able to answer questions about the AT&T plans
and services; and the Ikea ?Just Ask Anna!? agent
(http://www.ikea.com/ms/en US/) targets ques-
tions pertaining the company?s catalog. Most of
these QA systems, however, are limited to answer
questions in isolation. The reality is that users often
ask questions naturally as part of contextualized
interaction. For instance, a question ?How do I
subscribe to the AT&T CallVantager service?? is
likely to be followed by other related questions
like ?How much will the basic plan cost?? and
so on. Furthermore, many questions that users
frequently want answers for cannot be satisfied with
a simple answer. Some of them are too complicated,
broad, narrow, or vague resulting that there isn?t a
simple good answer or there are many good answer
candidates, which entails a clarification procedure
to constrain or relax the search. In all these cases,
a question answering system that is able to answer
contextual questions is more favored.
Contextual question answering as a research chal-
lenge has been fostered by TREC (Text Retrieval
Conference) since 2001. The TREC 2001 QA track
made the first attempt to evaluate QA systems? abil-
ity of tracking context through a series of questions.
The TREC 2004 re-introduced this task and orga-
nized all questions into 64 series, with each series
focusing on a specific topic. The earlier questions
in a series provide context for the on-going ques-
tion. However, in reality, QA systems will not be
33
informed about the boundaries between series in ad-
vance.
One challenge of engaging dialogue into QA sys-
tems is to determine the boundaries between topics.
For each question, the system would need to deter-
mine whether the question begins a new topic or it
is a follow-up question related to the current exist-
ing topic. We refer to this procedure as relevancy
recognition. If a question is recognized as a follow-
up question, the next step is to make use of context
information to interpret it and retrieve the answer.
We refer to this procedure as context information fu-
sion. Relevancy recognition is similar to text seg-
mentation (Hearst, 1994), but relevancy recognition
focuses on the current question with the previous
text while text segmentation has the full text avail-
able and is allowed to look ahead.
De Boni and Manandhar (2005) developed a rule-
based algorithm for relevancy recognition. Their
rules were manually deduced by carefully analyzing
the TREC 2001 QA data. For example, if a question
has no verbs, it is a follow-up question. This rule-
based algorithm achieves 81% in accuracy when rec-
ognizing the question relevance in the TREC 2001
QA data set. The disadvantage of this approach is
that it involves a good deal of human effort to re-
search on a specific data set and summarize the rules.
For a new corpus from a different domain, it is very
likely that one would have to go over the data set and
modify the rules, which is time and human-effort
consuming. An alternative is to pursue a data driven
approach to automatically learn the rules from a data
set. In this paper, we describe our experiments of
using supervised learning classification techniques
for the task of relevancy recognition. Experiments
show that machine learning approach achieves better
recognition accuracy and can also be easily applied
to a new domain.
The organization of this paper is as follows. In
Section 2, we summarize De Boni and Manandhar?s
rule-based algorithm. We present our learning ap-
proach in Section 3. We ran our experiments on
two data sets, namely, the TREC QA data and the
HandQA data, and give the results in Section 4. In
section 5, we report our preliminary study on con-
text information fusion. We conclude this paper in
Section 6.
2 Rule-Based Approach
De Boni and Manandhar (2005) observed the fol-
lowing cues to recognize follow-up questions:
? Pronouns and possessive adjectives. For exam-
ple, if a question has a pronoun that does not re-
fer to an entity in the same sentence, this ques-
tion could be a follow-up question.
? Cue words, such as ?precisely? and ?exactly?.
? Ellipsis. For example, if a question is not syn-
tactically complete, this question could be a
follow-up question.
? Semantic Similarity. For example, if a ques-
tion bears certain semantic similarity to previ-
ous questions, this question might be a follow-
up question.
De Boni and Manandhar (2005) proposed an
algorithm of calculating the semantic similar-
ity between the current question Q and a pre-
vious question Q?. Supposed Q consists of a
list of words (w1, w2, ..., wn) and Q? consists
of (w?1, w?2, ..., w?m):
SentenceSimilarity(Q, Q?) (1)
=
?
1?j?n
( max
1?i?m
WordSimilarity(wj , w?i))
The value of WordSimilarity(w, w?) is the
similarity between two words, calculated from
WordNet (Fellbaum, 1998). It returns a value
between 0 (w and w? have no semantic rela-
tions) and 1 (w and w? are the same).
Motivated by these observations, De Boni and
Manandhar (2005) proposed the rule-based algo-
rithm for relevancy recognition given in Figure 1.
This approach can be easily mapped into an hand-
crafted decision tree. According to the algorithm,
a question follows the current existing topic if it (1)
contains reference to other questions; or (2) contains
context-related cue words; or (3) contains no verbs;
or (4) bears certain semantic similarity to previous
questions or answer. Evaluated on the TREC 2001
QA context track data, the recall of the algorithm
is 90% for recognizing first questions and 78% for
follow-up questions; the precision is 56% and 76%
respectively. The overall accuracy is 81%.
34
Given the current question Qi and a sequence of history ques-
tions Qi?n, ..., Qi?1:
1. If Qi has a pronoun or possessive adjective which has
no references in the current question, Qi is a follow-up
question.
2. If Qi has cue words such as ?precisely? or ?exactly?, Qi
is a follow-up question.
3. If Qi does not contain any verbs, Qi is a follow-up ques-
tion.
4. Otherwise, calculate the semantic similarity measure of
Qi as
SimilarityMeasure(Qi)
= max
1?j?n
f(j) ? SentenceSimilarity(Qi, Qi?j)
Here f(j) is a decay function. If the similarity measure is
higher than a certain threshold, Qi is a follow-up ques-
tion.
5. Otherwise, if answer is available, calculate the semantic
distance between Qi and the immediately previous an-
swer Ai?1: SentenceSimilarity(Qi, Ai?1). If it is
higher than a certain threshold, Qi is a follow-up ques-
tion that is related to the previous answer.
6. Otherwise, Qi begins a new topic.
Figure 1: Rule-based Algorithm
3 Data Driven Approach
3.1 Decision Tree Learning
As a move away from heuristic rules, in this paper,
we make an attempt towards the task of relevancy
recognition using machine learning techniques. We
formulate it as a binary classification problem: a
question either begins a new topic or follows the
current existing topic. This classification task can
be approached with a number of learning algorithms
such as support vector machines, Adaboost and arti-
ficial neural networks. In this paper, we present our
experiments using Decision Tree. A decision tree
is a tree in which each internal node represents a
choice between a number of alternatives, and each
leaf node represents a decision. Learning a decision
tree is fairly straightforward. It begins from the root
node which consists of all the training data, growing
the tree top-down by recursively splitting each node
based on maximum information gain until certain
criteria is met. Although the idea is simple, decision
tree learning is often able to yield good results.
3.2 Feature Extraction
Inspired by De Boni and Manandhar?s (2005) work,
we selected two categories of features: syntactic fea-
tures and semantic features. Syntactic features cap-
ture whether a question has certain syntactic compo-
nents, such as verbs or pronouns. Semantic features
characterize the semantic similarity between the cur-
rent question and previous questions.
3.2.1 Syntactic Features
As the first step, we tagged each question with
part-of-speech tags using GATE (Cunningham et al,
2002), a software tool set for text engineering. We
then extracted the following binary syntactic fea-
tures:
PRONOUN: whether the question has a pronoun
or not. A more useful feature would be to la-
bel whether a pronoun refers to an entity in the
previous questions or in the current question.
However, the performances of currently avail-
able tools for anaphora resolution are quite lim-
ited for our task. The tools we tried, includ-
ing GATE (Cunningham et al, 2002), Ling-
Pipe (http://www.alias-i.com/lingpipe/)
and JavaRAP (Qiu et al, 2004), tend to use
the nearest noun phrase as the referents for pro-
nouns. While in the TREC questions, pronouns
tend to refer to the topic words (focus). As a
result, unsupervised anaphora resolution intro-
duced more noise than useful information.
ProperNoun: whether the question has a proper
noun or not.
NOUN: whether the question has a noun or not.
VERB: whether the question has a verb or not.
DefiniteNoun: if a question has a definite noun
phrase that refers to an entity in previous ques-
tions, the question is very likely to be a follow-
up question. However, considering the diffi-
culty in automatically identifying definite noun
phrases and their referents, we ended up not us-
ing this feature in our training because it in fact
introduced misleading information.
3.3 Semantic Features
To compute the semantic similarity between two
questions, we modified De Boni and Manandhar?s
formula with a further normalization by the length
of the questions; see formula (2).
35
SentenceSimilarity(Q, Q?) (2)
= 1n
?
1?j?n
( max
1?i?m
WordSimilarity(wj , w?i))
This normalization has pros and cons. It removes
the bias towards long sentences by eliminating the
accumulating effect; but on the other hand, it might
cause the system to miss a related question, for ex-
ample, when two related sentences have only one
key word in common.1
Formula (2) shows that sentence level similarity
depends on word-word similarity. Researchers have
proposed a variety of ways in measuring the seman-
tic similarity or relatedness between two words (to
be exact, word senses) based on WordNet. For ex-
ample, the Path (path) measure is the inverse of
the shortest path length between two word senses
in WordNet; the Wu and Palmer?s (wup) measure
(Wu and Palmer, 1994) is to find the most spe-
cific concept that two word senses share as ances-
tor (least common subsumer), and then scale the
path length of this concept to the root note (sup-
posed that there is a virtual root note in WordNet)
by the sum of the path lengths of the individual
word sense to the root node; the Lin?s (lin) mea-
sure (Lin, 1998) is based on information content,
which is a corpus based measure of the specificity of
a word; the Vector (vector) measure associates each
word with a gloss vector and calculates the similar-
ity of two words as the cosine between their gloss
vectors (Patwardhan, 2003). It was unclear which
measure(s) would contribute the best information to
the task of relevancy recognition, so we just exper-
imented on all four measures, path, wup, lin, and
vector, in our decision tree training. We used Peder-
sen et al?s (2004) tool WordNet::Similarity to com-
pute these four measures. WordNet::Similarity im-
plements nine different measures of word similar-
ity. We here only used the four described above be-
cause they return a value between 0 and 1, which
is suitable for using formula (2) to calculate sen-
tence similarity, and we leave others as future work.
Notice that the WordNet::Similarity implementation
1Another idea is to feed the decision tree training both the
normalized and non-normalized semantic similarity informa-
tion and see what would come out. We tried it on the TREC data
and found out that the normalized features actually have higher
information gain (i.e. appear at the top levels of the learned tree.
can only measure path, wup, and lin between two
nouns or between two verbs, while it uses all the
content words for the vector measure. We thus have
the following semantic features:
path noun: sentence similarity is based on the
nouns2 similarity using the path measure.
path verb: sentence similarity is based on the non-
trivial verbs similarity using the path measure.
Trivial verbs include ?does, been, has, have,
had, was, were, am, will, do, did, would, might,
could, is, are, can, should, shall, being?.
wup noun: sentence similarity is based on the
nouns similarity using the Wu and Palmer?s
measure.
wup verb: sentence similarity is based on the
non-trivial verbs similarity using the Wu and
Palmer?s measure.
lin noun: sentence similarity is based on the nouns
similarity using the Lin?s measure.
lin verb: sentence similarity is based on the non-
trivial verbs similarity using the Lin?s measure.
vector: sentence similarity is based on all content
words (nouns, verbs, and adjectives) similarity
using the vector measure.
4 Results
We ran the experiments on two sets of data: the
TREC QA data and the HandQA data.
4.1 Results on the TREC data
TREC has contextual questions in 2001 context
track and 2004 (Voorhees, 2001; Voorhees, 2004).
Questions about a specific topic are organized into a
session. In reality, the boundaries between sessions
are not given. The QA system would have to rec-
ognize the start of a new session as the first step of
question answering. We used the TREC 2004 data
as training and the TREC 2001 context track data as
testing. The training data contain 286 factoid and list
questions in 65 sessions3; the testing data contain 42
questions in 10 sessions. Averagely each session has
about 4-5 questions. Figure 2 shows some example
questions (the first three sessions) from the TREC
2001 context track data.
2This is to filter out all other words but nouns from a sen-
tence for measuring semantic similarity.
3In the TREC 2004 data, each session of questions is as-
signed a phrase as the topic, and thus the first question in a ses-
sion might have pronouns referring to this topic phrase. In such
cases, we manually replaced the pronouns by the topic phrase.
36
CTX1a Which museum in Florence was damaged by a
major bomb explosion in 1993?
CTX1b On what day did this happen?
CTX1c Which galleries were involved?
CTX1d How many people were killed?
CTX1e Where were these people located?
CTX1f How much explosive was used?
CTX2a Which industrial sector supplies the most
jobs in Toulouse?
CTX2b How many foreign companies were based there
in 1994?
CTX2c Name a company that flies there.
CTX3a What grape variety is used in Chateau Petrus
Bordeaus?
CTX3b How much did the future cost for the 1989
Vintage?
CTX3c Where did the winery?s owner go to college?
CTX3d What California winery does he own?
Figure 2: Example TREC questions
4.1.1 Confusion Matrix
Table 1 shows the confusion matrix of the deci-
sion tree learning results. On the testing data, the
learned model performs with 90% in recall and 82%
in precision for recognizing first questions; for rec-
ognizing follow-up questions, the recall is 94% and
precision is 97%. In contrast, De Boni and Man-
andhar?s rule-based algorithm has 90% in recall and
56% in precision for recognizing first questions; for
follow-up questions, the recall is 78% and precision
is 96%. The recall and precision of our learned
model to recognize first questions and follow-up
questions are all better than or at least the same
as the rule-based algorithm. The accuracy of our
learned model is 93%, about 12% absolute improve-
ment from the rule-based algorithm, which is 81% in
accuracy. Although the size of the data is too small
to draw a more general conclusion, we do see that
the data driven approach has better performance.
Training Data
Predicted Class
True Class First follow-up Total
First 63 2 65
follow-up 1 220 221
Total 64 222 286
Testing Data
Predicted Class
True Class First follow-up Total Recall
First 9 1 10 90%
follow-up 2 30 32 94%
Total 11 31 42
Precision 82% 97%
Table 1: Confusion Matrix for TREC Data
4.1.2 Trained Tree
Figure 3 shows the first top two levels of the tree
learned from the training data. Not surprisingly,
PRONOUN turns out to be the most important fea-
ture which has the highest information gain. In the
TREC data, when there is a pronoun in a question,
the question is very likely to be a follow-up ques-
tion. In fact, in the TREC 2004 data, the referent
of pronouns very often is the topic phrase. The fea-
ture path noun, on the second level of the trained
tree, turns out to contribute most information in this
recognition task among the four different semantic
similarity measures. The similarity measures using
wup, wup noun and wup verb, and the vector mea-
sure do not appear in any node of the trained tree.
Figure 3: Trained Tree on TREC Data
The following are rules generated from the train-
ing data whose confidence is higher than 90%. Con-
fidence is defined as out of the training records for
which the left hand side of the rule is true, the per-
centage of records for which the right hand side is
also true. This measures the accuracy of the rule.
- If PRONOUN=1 then follow-up question
- If path noun?0.31 then follow-up question
- If lin noun?0.43 then follow-up question
- If path noun<0.15 and PRONOUN=0 then first question
De Boni and Manandhar?s algorithm has this
rule:?if a question has no verb, the question is
follow-up question?. However, we did not learn this
rule from the data, nor the feature VERB appears in
any node of the trained tree. One possible reason
is that this rule has too little support in the training
set (support is defined as the percentage of which
the left hand side of the rule is true). Another pos-
sible reason is that this rule is not needed because
the combination of other features is able to provide
enough information for recognizing follow-up ques-
tions. In any case, the decision tree learns a (local)
37
optimized combination of features which captures
most cases, and avoids redundant rules.
4.1.3 Error Analysis
The trained decision tree has 3 errors in the test-
ing data. Two of the errors are mis-recognition of
follow-up questions to be first questions, and one is
the vice versa.
The first error is failure to recognize the ques-
tion ?which galleries were involved?? (CTX1c) as
a follow-up question (see Figure 2 for context). It
is a syntactically complete sentence, and there is no
pronoun or definite noun in the sentence. Seman-
tic features are the most useful information to rec-
ognize it as a follow-up question. However, the se-
mantic relatedness in WordNet between the words
?gallery? in the current question and ?museum? in
the first question of this session (CTX1a in Figure 2)
is not strong enough for the trained decision tree to
relate the two questions together.
The second error is failure to recognize the ques-
tion ?Where did the winery?s owner go to college??
(CTX3c) as a follow-up question. Similarly, part
of the reason for this failure is due to the insuffi-
cient semantic relatedness between the words ?win-
ery? and ?grape? (in CTX3a) to connect the ques-
tions together. However, this question has a definite
noun phrase ?the winery? which refers to ?Chateau
Petrus Bordeaux? in the first question in this session.
We did not make use of the feature DefiniteNoun in
our training, because it is not easy to automatically
identify the referents of a definite noun phrase, or
even whether it has a referent or not. A lot of def-
inite noun phrases, such as ?the sun?, ?the trees in
China?, ?the first movie?, and ?the space shuttles?,
do not refer to any entity in the text. This does not
mean that the feature DefiniteNoun is not important,
but instead that we just leave it as our future work to
better incorporate this feature.
The third error, is failure to recognize the question
?What does transgenic mean?? as the first question
that opens a session. This error is due to the over-
fitting of decision tree training.
4.1.4 Boosting
We tried another machine learning approach, Ad-
aboost (Schapire and Singer, 2000), which is resis-
tant (but not always) to over-fitting. It calls a given
weak learning algorithm repeatedly in a series of
rounds t = 1, ..., T . Each time the weak learning
algorithm generates a rough ?rule of thumb?, and
after many rounds Adaboost combines these weak
rules into a single prediction rule that, hopefully,
will be more accurate than any one of the weak
rules. Figure 2 shows the confusion matrix of Ad-
aboost learning results. It shows that Adaboost is
able to correctly recognize ?What does transgenic
mean?? as beginning a new topic. However, Ad-
aboost has more errors in recognizing follow-up
questions, which results in an overall accuracy of
88%, slightly lower than decision tree learning.
Training Data
Predicted Class
True Class First follow-up Total
First 64 1 65
follow-up 1 220 221
Total 65 221 286
Testing Data
Predicted Class
True Class First follow-up Total Recall
First 10 0 10 100%
follow-up 5 27 32 84%
Total 15 27 42
Precision 67% 100%
Table 2: Confusion Matrix Using Adaboosting
4.2 Results on the HandQA data
We also conducted an experiment using real-world
customer-care related questions. We selected our
test data from the chat logs of a deployed online
QA system. We refer to this system as HandQA.
HandQA is built using a telecommunication ontol-
ogy database and 1600 pre-determined FAQ-answer
pairs. For every submitted customer question,
HandQA chooses one of these 1600 answers as the
response. Each chat session contains about 3 ques-
tions. We assume the questions in a session are
context-related.
The HandQA data are different from the TREC
data in two ways. First, HandQA questions are real
typed questions from motivated users. The HandQA
data contain some noisy information, such as typos
and bad grammars. Some users even treated this
system as a search engine and simply typed in the
keywords. Second, questions in a chat session ba-
sically asked for the same information. Very often,
when the system failed to get the correct answer to
38
the user?s question, the user would repeat or rephrase
the same question, until they gave up or the system
luckily found the answer. As an example, Figure 4
shows two chat sessions. Again, we did not use the
system?s answer in our relevancy recognition.
How to make number non published?
Non published numbers
How to make number non listed?
Is my number switched to Call Vantage yet?
When will my number be switched?
When is number transferred?
Figure 4: Example questions in HandQA
A subset of the HandQA data, 5908 questions in
2184 sessions are used for training and testing the
decision tree. The data were randomly divided into
two sets: 90% for training and 10% for testing.
4.2.1 Confusion Matrix
Table 3 shows the confusion matrix of the deci-
sion tree learning results. For recognizing first ques-
tions, the learned model has 73% in recall and 62%
in precision; for recognizing follow-up questions,
the recall is 75% and precision is 84%. The accuracy
is 74%. A base line model is to have all questions
except the first one as following up questions, which
results in the accuracy of 64% (380/590). Thus the
learned decision tree yields an absolute improve-
ment of 10%. However, the results on this data set
are not as good as those on the TREC data.
Training Data
Predicted Class
True Class First follow-up Total
First 1483 490 1973
follow-up 699 2646 3345
Total 2182 3136 5318
Testing Data
Predicted Class
True Class First follow-up Total Recall
First 153 58 211 73%
follow-up 93 286 379 75%
Total 246 344 590
Precision 62% 84%
Table 3: Confusion Matrix for HandQA Data
4.2.2 Trained Tree
Table 5 shows the top two levels of the tree
learned from the training data, both of which are
on the semantic measure path. This again confirms
that path best fits the task of relevancy recognition
among the four semantic measures.
No syntactical features appear in any node of the
learned tree. This is not surprising because syntac-
tic information is noisy in this data set. Typos, bad
grammars, and mis-capitalization affect automatic
POS tagging. Keywords input also results in incom-
plete sentences, which makes it unreliable to recog-
nize follow-up questions based on whether a ques-
tion is a complete sentence or not. Furthermore,
because questions in a session rarely refer to each
other, but just repeat or rephrase each other, the fea-
ture PRONOUN does not help either. All these make
syntactic features not useful. Semantic features turn
out to be more important for this data set.
Figure 5: Trained Tree on HandQA Data
4.2.3 Error Analysis
There are two reasons for the decreased perfor-
mance in this data set. The first reason, as we ana-
lyzed above, is that syntactical features do not con-
tribute to the recognition task. The second reason is
that consecutive chat sessions might ask for the same
information. In the handQA data set, questions are
basically all about telecommunication service, and
questions in two consecutive chat sessions, although
by different users, could be on very similar topics or
even have same words. Thus, questions, although in
two separate chat sessions, could have high semantic
similarity measure. This would introduce confusing
information to the decision tree learning.
5 Making Use of Context Information
Relevancy recognition is the first step of contextual
question answering. If a question is recognized as
following the current existing topic, the next step is
to make use of the context information to interpret it
39
and retrieve the answers. To explore how context in-
formation helps answer retrieval, we conducted pre-
liminary experiments with the TREC 2004 QA data.
We indexed the TREC documents using the Lucene
search engine (Hatcher and Gospodnetic, 2004) for
document retrieval. The Lucene search engine takes
as input a query (a list of keywords), and returns a
ranked list of relevant documents, of which the first
50 were taken and analyzed in our experiments. We
tried different strategies for query formulation. Sim-
ply using the questions as the query, only 20% of
the follow-up questions find their answers in the first
50 returned documents. This percentage went up
to 85% when we used the topic words, provided in
TREC data for each section, as the query. Because
topic words are usually not available in real world
applications, to be more practical, we tried using the
noun phrases in the first question as the query. In
this case, 81% of the questions are able to find the
answers in the returned documents. When we com-
bined the (follow-up) question with the noun phrases
in the first question as the query, the retrieved rate
increases to 84%. Typically, document retrieval is a
crucial step for QA systems. These results suggest
that context information fusion has a big potential to
improve the performance of answer retrieval. How-
ever, we leave the topic of how to fuse context infor-
mation into the follow-up questions as future work.
6 Conclusion
In this paper, we present a data driven approach, de-
cision tree learning, for the task of relevancy recog-
nition in contextual question answering. Experi-
ments show that this approach achieves 93% accu-
racy on the TREC data, about 12% improvement
from the rule-based algorithm reported by De Boni
and Mananhar (2005). Moreover, this data driven
approach requires much less human effort on inves-
tigating a specific data set and less human exper-
tise to summarize rules from the observation. All
the features we used in the training can be automat-
ically extracted. This makes it straightforward to
train a model in a new domain, such as the HandQA.
Furthermore, decision tree learning is a white-box
model and the trained tree is human interpretable. It
shows that the path measure has the best information
gain among the other semantic similarity measures.
We also report our preliminary experiment results on
context information fusion for question answering.
7 Acknowledgement
The authors thank Srinivas Bangalore and Mazin E.
Gilbert for helpful discussion.
References
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust nlp tools and applications. In Proceedings
of the 40th ACL.
Marco De Boni and Suresh Manandhar. 2005. Imple-
menting clarification dialogues in open domain ques-
tion answering. Natural Language Engineering. Ac-
cepted.
Christiane Fellbaum. 1998. WordNet:An Electronic Lex-
ical Database. MIT Press, Cambridge, MA.
Erik Hatcher and Otis Gospodnetic. 2004. Lucene in
Action. Manning.
Marti A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In Proceedings of 32nd ACL, pages
9?16.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of the International Con-
ference on Machine Learning.
Siddharth Patwardhan. 2003. Incorporating dictionary
and corpus information into a context vector measure
of semantic relatedness. master?s thesis, University of
Minnesota, Duluth.
Ted Pederson, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. WordNet::Similarity - measuring the re-
latedness of concepts. In Proceedings of the 9th AAAI.
Intelligent Systems Demonstration.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2004. A
public reference implementation of the rap anaphora
resolution algorithm. In Proceedings of LREC, pages
291?294.
Robert E. Schapire and Yoram Singer. 2000. BoosTex-
ter: A boosting-based system for text categorization.
Machine Learning, 39:135?168.
Ellen M. Voorhees. 2001. Overview of the TREC 2001
question answering track. In Proceedings of TREC-10.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In Proceedings of TREC-13.
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In Proceedings of 32nd ACL,
pages 133?138.
40
Coling 2010: Poster Volume, pages 36?44,
Beijing, August 2010
Robust Sentiment Detection on Twitter from Biased and Noisy Data
Luciano Barbosa
AT&T Labs - Research
lbarbosa@research.att.com
Junlan Feng
AT&T Labs - Research
junlan@research.att.com
Abstract
In this paper, we propose an approach to
automatically detect sentiments on Twit-
ter messages (tweets) that explores some
characteristics of how tweets are written
and meta-information of the words that
compose these messages. Moreover, we
leverage sources of noisy labels as our
training data. These noisy labels were
provided by a few sentiment detection
websites over twitter data. In our experi-
ments, we show that since our features are
able to capture a more abstract represen-
tation of tweets, our solution is more ef-
fective than previous ones and also more
robust regarding biased and noisy data,
which is the kind of data provided by these
sources.
1 Introduction
Twitter is one of the most popular social network
websites and has been growing at a very fast pace.
The number of Twitter users reached an estimated
75 million by the end of 2009, up from approx-
imately 5 million in the previous year. Through
the twitter platform, users share either information
or opinions about personalities, politicians, prod-
ucts, companies, events (Prentice and Huffman,
2008) etc. This has been attracting the attention
of different communities interested in analyzing
its content.
Sentiment detection of tweets is one of the basic
analysis utility functions needed by various appli-
cations over twitter data. Many systems and ap-
proaches have been implemented to automatically
detect sentiment on texts (e.g., news articles, Web
reviews and Web blogs) (Pang et al, 2002; Pang
and Lee, 2004; Wiebe and Riloff, 2005; Glance
et al, 2005; Wilson et al, 2005). Most of these
approaches use the raw word representation (n-
grams) as features to build a model for sentiment
detection and perform this task over large pieces
of texts. However, the main limitation of using
these techniques for the Twitter context is mes-
sages posted on Twitter, so-called tweets, are very
short. The maximum size of a tweet is 140 char-
acters.
In this paper, we propose a 2-step sentiment
analysis classification method for Twitter, which
first classifies messages as subjective and ob-
jective, and further distinguishes the subjective
tweets as positive or negative. To reduce the la-
beling effort in creating these classifiers, instead
of using manually annotated data to compose the
training data, as regular supervised learning ap-
proaches, we leverage sources of noisy labels as
our training data. These noisy labels were pro-
vided by a few sentiment detection websites over
twitter data. To better utilize these sources, we
verify the potential value of using and combining
them, providing an analysis of the provided labels,
examine different strategies of combining these
sources in order to obtain the best outcome; and,
propose a more robust feature set that captures a
more abstract representation of tweets, composed
by meta-information associated to words and spe-
cific characteristics of how tweets are written. By
using it, we aim to handle better: the problem
of lack of information on tweets, helping on the
generalization process of the classification algo-
rithms; and the noisy and biased labels provided
by those websites.
The remainder of this paper is organized as fol-
lows. In Section 2, we provide some context about
messages on Twitter and about the websites used
as label sources. We introduce the features used
in the sentiment detection and also provide a deep
analysis of the labels generated by those sources
in Section 3. We examine different strategies of
36
combining these sources and present an extensive
experimental evaluation in Section 4. Finally, we
discuss previous works related to ours in Section 5
and conclude in Section 6, where we outline direc-
tions and future work.
2 Preliminaries
In this section, we give some context about Twitter
messages and the sources used for our data-driven
approach.
Tweets. The Twitter messages are called tweets.
There are some particular features that can be used
to compose a tweet (Figure 1 illustrates an ex-
ample): ?RT? is an acronym for retweet, which
means the tweet was forwarded from a previous
post; ?@twUser? represents that this message is a
reply to the user ?twUser?; ?#obama? is a tag pro-
vided by the user for this message, so-called hash-
tag; and ?http://bit.ly/9K4n9p? is a link to some
external source. Tweets are limited to 140 charac-
ters. Due to this lack of information in terms of
words present in a tweet, we explore some of the
tweet features listed above to boost the sentiment
detection, as we will show in detail in Section 3.
Data Sources. We collected data from 3 differ-
ent websites that provide almost real-time senti-
ment detection for tweets: Twendz, Twitter Sen-
timent and TweetFeel. To collect data, we issued
a query containing a common stopword ?of?, as
we are interested in collecting generic data, and
retrieved tweets from these sites for three weeks,
archiving the returned tweets along with their sen-
timent labels. Table 1 shows more details about
these sources. Two of the websites provide 3-
class detection: positive, negative and neutral and
one of them just 2-class detection. One thing to
note is our crawling process obtained a very dif-
ferent number of tweets from each website. This
might be a result of differences among their sam-
pling processes of Twitter stream or some kind of
filtering process to output. For instance, a site
may only present the tweets it has more confi-
dence about their sentiment. In Section 3, we
present a deep analysis of the data provided by
these sources, showing if they are useful to build
a sentiment classification.
RT @twUser: Obama is the first U.S. president not to
have seen a new state added in his lifetime.
http://bit.ly/9K4n9p #obama
Figure 1: Example of a tweet.
3 Twitter Sentiment Detection
Our goal is to categorize a tweet into one of the
three sentiment categories: positive, neutral or
negative. Similar to (Pang and Lee, 2004; Wil-
son et al, 2005), we implement a 2-step sentiment
detection framework. The first step targets on dis-
tinguishing subjective tweets from non-subjective
tweets (subjectivity detection). The second one
further classifies the subjective tweets into posi-
tive and negative, namely, the polarity detection.
Both classifiers perform prediction using an ab-
stract representation of the sentences as features,
as we show later in this section.
3.1 Features
A variety of features have been exploited on the
problem of sentiment detection (Pang and Lee,
2004; Pang et al, 2002; Wiebe et al, 1999; Wiebe
and Riloff, 2005; Riloff et al, 2006) including un-
igrams, bigrams, part-of-speech tags etc. A natu-
ral choice would be to use the raw word represen-
tation (n-grams) as features, since they obtained
good results in previous works (Pang and Lee,
2004; Pang et al, 2002) that deal with large texts.
However, as we want to perform sentiment detec-
tion on very short messages (tweets), this strat-
egy might not be effective, as shown in our ex-
periments. In this context, we are motivated to
develop an abstract representation of tweets. We
propose the use of two sets of features: meta-
information about the words on tweets and char-
acteristics of how tweets are written.
Meta-features. Given a word in a tweet, we map
it to its part-of-speech using a part-of-speech dic-
tionary1. Previous approaches (Wiebe and Riloff,
2005; Riloff et al, 2003) have shown that the ef-
fectiveness of using POS tags for this task. The
intuition is certain POS tags are good indica-
tors for sentiment tagging. For example, opin-
ion messages are more likely containing adjec-
1The pos dictionary we used in this paper is available at:
http://wordlist.sourceforge.net/pos-readme.
37
Data sources URL # Tweets Sentiments
Twendz http://twendz.waggeneredstrom.com/ 254081 pos/neg/neutral
Twitter Sentiment http://twittersentiment.appspot.com/ 79696 pos/neg/neutral
TweetFeel http://www.tweetfeel.com/ 13122 pos/neg
Table 1: Information about the 3 data sources.
tives or interjections. In addition to POS tags,
we map the word to its prior subjectivity (weak
and strong subjectivity), also used by (Wiebe and
Riloff, 2005), and polarity (positive, negative and
neutral). The prior polarity is switched from pos-
itive to negative or vice-versa when a negative
expression (as, e.g., ?don?t?, ?never?) precedes
the word. We obtained the prior subjectivity and
polarity information from subjectivity lexicon of
about 8,000 words used in (Riloff and Wiebe,
2003)2. Although this is a very comprehensive
list, slang and specific Web vocabulary are not
present on it, e.g., words as ?yummy? or ?ftw?.
For this reason, we collected popular words used
on online discussions from many online sources
and added them to this list.
Tweet Syntax Features. We exploited the syn-
tax of the tweets to compose our features. They
are: retweet; hashtag; reply; link, if the tweet con-
tains a link; punctuation (exclamation and ques-
tions marks); emoticons (textual expression rep-
resenting facial expressions); and upper cases (the
number of words that starts with upper case in the
tweet).
The frequency of each feature in a tweet is di-
vided by the number of the words in the tweet.
3.2 Subjectivity Classifier
As we mentioned before, the first step in our tweet
sentiment detection is to predict the subjectivity of
a given tweet. We decided to create a single clas-
sifier by combining the objectivity sentences from
Twendz and Twitter Sentiment (objectivity class)
and the subjectivity sentences from all 3 sources.
As we do not know the quality of the labels pro-
vided by these sources, we perform a cleaning
process over this data to assure some reasonable
quality. These are the steps:
1. Disagreement removal: we remove the
2The subjectivity lexicon is available at
http://www.cs.pitt.edu/mpqa/
tweets that are disagreed between the data
sources in terms of subjectivity;
2. Same user?s messages: we observed that the
users with the highest number of messages
in our dataset are usually those ones that post
some objective messages, for example, ad-
vertising some product or posting some job
recruiting information. For this reason, we
allowed in the training data only one message
from the same user. As we show later, this
boosts the classification performance, mainly
because it removes tweets labeled as subjec-
tive by the data sources but are in fact objec-
tive;
3. Top opinion words: to clean the objective
training set, we remove from this set tweets
that contain the top-n opinion words in the
subjectivity training set, e.g., words as cool,
suck, awesome etc.
As we show in Section 4, this process is in fact
able to remove certain noisy in the training data,
leading to a better performing subjectivity classi-
fier.
To illustrate which of the proposed features are
more effective for this task, the top-5 features in
terms of information gain, based on our training
data, are: positive polarity, link, strong subjec-
tive, upper case and verbs. Three of them are
meta-information (positive polarity, strong sub-
jective and verbs) and the other two are tweet
syntax features (link and upper case). Here is
a typical example of a objective tweet in which
the user pointed an external link and used many
upper case words: ?Starbucks Expands Pay-By-
IPhone Pilot to 1,000 Stores?Starbucks cus-
tomers with Apple iPhones or iPod touches can
.. http://oohja.com/x9UbC?.
38
3.3 Polarity Classifier
The second step of our sentiment detection ap-
proach is polarity classification, i.e., predict-
ing positive or negative sentiment on subjective
tweets. In this section, first we analyze the qual-
ity of the polarity labels provided by the three
sources, and whether their combination has the
potential to bring improvement. Second, we
present some modifications in the proposed fea-
tures that are more suitable for this task.
3.3.1 Analysis of the Data Sources
The 3 data sources used in this work provide
some kind of polarity labels (see Table 1). Two
questions we investigate regarding these sources
are: (1) how useful are these polarity labels? and
(2) does combining them bring improvement in
accuracy?
We take the following aspects into considera-
tion:
? Labeler quality: if the labelers have low qual-
ity, combine them might not bring much im-
provement (Sheng et al, 2008). In our case,
each source is treated as a labeler;
? Number of labels provided by the labelers:
if the labels are informative, i.e., the prob-
ability of them being correct is higher than
0.5, the more the number of labels, the higher
is the performance of a classifier built from
them (Sheng et al, 2008);
? Labeler bias: the labeled data provided by
the labelers might be only a subset of the
real data distribution. For instance, labelers
might be interested in only providing labels
that they are more confident about;
? Different labeler bias: if labelers make simi-
lar mistakes, the combination of them might
not bring much improvement.
We provide an empirical analysis of these
datasets to address these points. First, we measure
the polarity detection quality of a source by calcu-
lating the probability p of a label from this source
being correct. We use the data manually labeled
for assessing the classifiers? performance (testing
data, see Section 4) to obtain the correct labels of
Data sources Quality Entropy
Twendz 0.77 8.3
TwitterSentiment 0.82 7.9
TweetFeel 0.89 7.5
Table 2: Quality of the labels and entropy of the
tweets provided by each data source for the polar-
ity detection.
a data sample. Table 2 shows their values. We can
conclude from these numbers that the 3 sources
provide a reasonable quality data. This means that
combining them might bring some improvement
to the polarity detection instead of, for instance,
using one of them in isolation. An aspect that is
overlooked by quality is the bias of the data. For
instance, by examining the data from TwitterFeel,
we found out that only 4 positive words (?awe-
some?,?rock?,?love? and ?beat?) cover 95% of
their positive examples and only 6 negative words
(?hate?,?suck?,?wtf?,?piss?,?stupid? and ?fail?)
cover 96% of their negative set. Clearly, the data
provided by this source is biased towards these
words. This is probably the reason why this web-
site outputs such fewer number of tweets com-
pared to the other websites (see Table 1) as well
as why its data has the smallest entropy among
the sources (see Table 2).
The quality of the data and its individual bias
have certainly impact in the combination of labels.
However, there is other important aspect that one
needs to consider: different bias between the la-
belers. For instance, if labelers a and b make sim-
ilar decisions, we expect that combining their la-
bels would not bring much improvement. There-
fore, the diversity of labelers is a key element in
combining them (Polikar, 2006). One way to mea-
sure this is by calculating the agreement between
the labels produced by the labelers. We use the
kappa coefficient (Cohen, 1960) to measure the
degree of agreement between two sources. Ta-
ble 3 presents the coefficients for each par of data
source. All the coefficients are between 0.4 and
0.6, which represents a moderate agreement be-
tween the labelers (Landis and Koch, 1977). This
means that in fact the sources provide different
bias regarding polarity detection.
39
Data sources Kappa
Twendz/TwitterSentiment 0.58
TwitterSentiment/TweetFeel 0.58
Twendz/TweetFeel 0.44
Table 3: Kappa coefficient between pairs of
sources.
From this analysis we can conclude that com-
bining the labels provided by the 3 sources can
improve the performance of the polarity detec-
tion instead of using one of them in isolation be-
cause they provide diverse labels (moderate kappa
agreement) of reasonable quality, although there
is some issues related to bias of the labels pro-
vided by them. In our experimental evaluation in
Section 4, we present results obtained by different
strategies of combining these sources that confirm
these findings.
3.3.2 Polarity Features
The features used in the polarity detection are
the same ones used in the subjectivity detection.
However, as one would expect the set of the most
discriminative features is different between the
two tasks. For subjectivity detection, the top-5
features in terms of information gain, based on
the training data, are: negative polarity, positive
polarity, verbs, good emoticons and upper case.
For this task, the meta-information of the words
(negative polarity, positive polarity and verbs) is
more important than specific features from Twitter
(good emoticons and upper case), whereas for the
subjectivity detection, tweet syntax features have
a higher relevance.
This analysis show that prior polarity is very
important for this task. However, one limitation
of using it from a generic list is its values might
not hold for some specific scenario. For instance,
the polarity of the word ?spot? is positive accord-
ing to this list. However, looking at our training
data almost half of the occurrences of this word
appears in the positive set and the other half in
the negative set. Thus, it is not correct to as-
sume that prior polarity of ?spot? is 1 for this
particular data. This example illustrates our strat-
egy to weight the prior polarities: for each word
w with prior polarity defined by the list, we cal-
culate the prior polarity of w, pol(w), based on
the distribution of w in the positive and negative
sets. Thus, polpos(w) = count(w, pos)/count(w)
and polneg(w) = 1? polpos(w). We assume the
polarity of a word is associated with the polar-
ity of the sentence, which seems to be reasonable
since we are dealing with very short messages.
Although simple, this strategy is able to improve
the polarity detection, as we show in Section 4.
4 Experiments
We have performed an extensive performance
evaluation of our solution for twitter sentiment
detection. Besides analyzing its overall perfor-
mance, our goals included: examining different
strategies to combine the labels provided by the
sources; comparing our approach to previous ones
in this area; and evaluating how robust our solu-
tion is to the noisy and biased data described in
Section 3.
4.1 Experimental Setup
Data Sets. For the subjectivity detection, after
the cleansing processing (see Section 3), the train-
ing data contains about 200,000 tweets (roughly
100,000 tweets were labeled by the sources as
subjective ones and 100,000 objective ones), and
for polarity detection, 71046 positive and 79628
negative tweets. For test data, we manually la-
beled 1,000 tweets as positive, negative and neu-
tral. We also built a development set (1,000
tweets) to tune the parameters of the classification
algorithms.
Approaches. For both tasks, subjectivity and po-
larity detection, we compared our approach with
previous ones reported in the literature. Detailed
explanation about them are as follows:
? ReviewSA: this is the approach proposed
by Pang and Lee (Pang and Lee, 2004)
for sentiment analysis in regular online re-
views. It performs the subjectivity detec-
tion on a sentence-level relying on the prox-
imity between sentences to detect subjectiv-
ity. The set of sentences predicted as subjec-
tive is then classified as negative or positive
in terms of polarity using the unigrams that
40
compose the sentences. We used the imple-
mentation provided by LingPipe (LingPipe,
2008);
? Unigrams: Pang et al (Pang et al, 2002)
showed unigrams are effective for sentiment
detection in regular reviews. Based on that,
we built unigram-based classifiers for the
subjectivity and polarity detections over the
training data. Another approach that uses un-
igrams is the one used by TwitterSentiment
website. For polarity detection, they select
the positive examples for the training data
from the tweets containing good emoticons
and negative examples from tweets contain-
ing bad emoticons. (Go et al, 2009). We
built a polarity classifier using this approach
(Unigrams-TS).
? TwitterSA: TwitterSA exploits the features
described in Section 3 in this paper. For
the subjectivity detection, we trained a clas-
sifier from the two available sources, us-
ing the cleaning process described in Sec-
tion 3 to remove noise in the training data,
TwitterSA(cleaning), and other classifier
trained from the original data, TwitterSA(no-
cleaning). For the polarity detection task,
we built a few classifiers to compare their
performances: TwitterSA(single) and Twit-
terSA(weights) are two classifiers we trained
using combined data from the 3 sources.
The only difference is TwitterSA(weights)
uses the modification of weighting the prior
polarity of the words based on the train-
ing data. TwitterSA(voting) and Twit-
terSA(maxconf) combine classification out-
puts from 3 classifiers respectively trained
from each source. TwitterSA(voting) uses
majority voting to combine them and Twit-
terSA(maxconf) picks the one with maxi-
mum confidence score.
We use Weka (Witten and Frank, 2005) to cre-
ate the classifiers. We tried different learning al-
gorithms available on Weka and SVM obtained
the best results for Unigrams and TwitterSA. Ex-
perimental results reported in this section are ob-
tained using SVM.
4.2 Subjectivity Detection Evaluation
Table 4 shows the error rates obtained by the dif-
ferent subjectivity detection approaches. Twit-
terSA achieved lower error rate than both Uni-
grams and ReviewSA. As a result, these num-
bers confirm that features inferred from meta-
information of words and specific syntax features
from tweets are better indicators of the subjectiv-
ity than unigrams. Another advantage of our ap-
proach is since it uses only 20 features, the train-
ing and test times are much faster than using thou-
sands of features like Unigrams. One of the rea-
sons why TwitterSA obtained such a good perfor-
mance was the process of data cleansing (see Sec-
tion 3). The label quality provided by the sources
for this task was very poor: 0.66 for Twendz and
0.68 for TwitterSentiment. By cleaning the data,
the error decreased from 19.9, TwitterSA(no-
cleaning), to 18.1, TwitterSA(cleaning). Regard-
ing ReviewSA, its lower performance is expected
since tweets are composed by single sentences
and ReviewSA relies on the proximity between
sentences to perform subjectivity detection.
We also investigated the influence of the size of
training data on classification performance. Fig-
ure 2 plots the error rates obtained by TwitterSA
and Unigrams versus the number of training ex-
amples. The curve corresponding to TwitterSA
showed that it achieved good performances even
with a small training data set, and kept almost con-
stant as more examples were added to the train-
ing data, whereas for Unigrams the error rate de-
creased. For instance, with only 2,000 tweets as
training data, TwitterSA obtained 20% of error
rate whereas Unigrams 34.5%. These numbers
show that our generic representation of tweets
produces models that are able to generalize even
with a few examples.
4.3 Polarity Detection Evaluation
We provide the results for polarity detection
in Table 5. The best performance was ob-
tained by TwitterSA(maxconf), which combines
results of the 3 classifiers, respectively trained
from each source, by taking the output by the
most confident classifier, as the final predic-
tion. TwitterSA(maxconf) was followed by Twit-
terSA(weights) and TwitterSA(single), both cre-
41
ated from a single training data. This result shows
that computing the prior polarity of the words
based on the training data TwitterSA(weights)
brings some improvement for this task. Twit-
terSA(voting) obtained the highest error rate
among the TwitterSA approaches. This implies
that, in our scenario, the best way of combining
the merits of the individual classifiers is by using
a confidence score approach.
Unigrams also achieved comparable perfor-
mances. However, when reducing the size of the
training data, the performance gap between Twit-
terSA and Unigrams is much wider. Figure 3
shows the error rate of both approaches3 in func-
tion of the training size. Similar to subjectivity de-
tection, the training size does not have much influ-
ence in the error rate for TwitterSA. However for
Unigrams, it decreased significantly as the train-
ing size increased. For instance, for a training
size with 2,000 tweets, the error rate for Unigrams
was 46% versus 23.8% for our approach. As for
subjectivity detection, this occurs because our fea-
tures are in fact able to capture a more general rep-
resentation of the tweets.
Another advantage of TwitterSA over Uni-
grams is that it produces more robust models. To
illustrate this, we present the error rates of Uni-
grams and TwitterSA where the training data is
composed by data from each source in isolation.
For the TweetFeel website, where data is very bi-
ased (see Section 3), Unigrams obtained an error
rate of 44.5% whereas over a sample of the same
size of the combined training data (Figure 3), it
obtained an error rate of around 30%. Our ap-
proach also performed worse over this data than
the general one, but still had a reasonable er-
ror rate, 25.1%. Regarding the Twendz website,
which is the noisiest one (Section 3), Unigrams
also obtained a poor performance comparing it
against its performance over a sample of the gen-
eral data with a same size (see Table 5 and Fig-
ure 3). Our approach, on the other hand, was
not much influenced by the noise (22.9% on noisy
data and around 20% on the sample of same size
of the general data). Finally, since the data qual-
ity provided by TwitterSentiment is better than the
3For this experiment, we used the TwitterSA(single) con-
figuration.
Approach Error rate
TwitterSA(cleaning) 18.1
TwitterSA(no-cleaning) 19.9
Unigrams 27.6
ReviewSA 32
Table 4: Results for subjectivity detection.
Approach Error rate
TwitterSA(maxconf) 18.7
TwitterSA(weights) 19.4
TwitterSA(single) 20
TwitterSA(voting) 22.6
Unigrams 20.9
ReviewSA 21.7
Unigrams-TS 24.3
Table 5: Results for polarity detection.
Site Training Size TwitterSA Unigrams
TweetFeel 13120 25.1 44.5
Twendz 78025 22.9 32.3
TwitterSentiment 59578 22 23.4
Table 6: Training data size for each source and
error rates obtained by classifiers built from them.
 0 5 10 15 20 25 30
 35 40
 0  20000  40000  60000  80000  100000  120000  140000  160000  180000  200000Error Rate Training Size UnigramsTwitterSA
Figure 2: Influence of the training data size in the
error rate of subjectivity detection using Unigrams
and TwitterSA.
previous sources (Table 2), there was not much
impact over both classifiers created from it.
From this analysis over real data, we can con-
clude that our approach produces (1) an effective
polarity classifier even when only a small number
of training data is available; (2) a robust model to
bias and noise in the training data; and (3) com-
bining data sources with such distinct characteris-
tics, as our data analysis in Section 3 pointed out,
is effective.
42
 0 10 20 30
 40 50
 0  20000  40000  60000  80000  100000  120000  140000  160000Error Rate Training Size UnigramsTwitterSA
Figure 3: Influence of the training data size in
the error rate of polarity detection using Unigrams
and TwitterSA.
5 Related Work
There is a rich literature in the area of sentiment
detection (see e.g., (Pang et al, 2002; Pang and
Lee, 2004; Wiebe and Riloff, 2005; Go et al,
2009; Glance et al, 2005). Most of these ap-
proaches try to perform this task on large texts, as
e.g., newspaper articles and movie reviews. An-
other common characteristic of some of them is
the use of n-grams as features to create their mod-
els. For instance, Pang and Lee (Pang and Lee,
2004) explores the fact that sentences close in a
text might share the same subjectivity to create a
better subjectivity detector and, similar to (Pang et
al., 2002), uses unigrams as features for the polar-
ity detection. However, these approaches do not
obtain a good performance on detecting sentiment
on tweets, as we showed in Section 4, mainly be-
cause tweets are very short messages. In addition
to that, since they use a raw word representation,
they are more sensible to bias and noise, and need
a much higher number of examples in the train-
ing data than our approach to obtain a reasonable
performance.
The Web sources used in this paper and some
other websites provide sentiment detection for
tweets. A great limitation to evaluate them is they
do not make available how their classification was
built. One exception is TwitterSentiment (Go et
al., 2009), for instance, which considers tweets
with good emoticons as positive examples and
tweets with bad emoticons as negative examples
for the training data, and builds a classifier using
unigrams and bigrams as features. We showed
in Section 4 that our approach works better than
theirs for this problem, obtaining lower error rates.
6 Conclusions and Future Work
We have presented an effective and robust sen-
timent detection approach for Twitter messages,
which uses biased and noisy labels as input to
build its models. This performance is due to the
fact that: (1) our approach creates a more abstract
representation of these messages, instead of using
a raw word representation of them as some pre-
vious approaches; and (2) although noisy and bi-
ased, the data sources provide labels of reasonable
quality and, since they have different bias, com-
bining them also brought some benefits.
The main limitation of our approach is the cases
of sentences that contain antagonistic sentiments.
As future work, we want to perform a more fine
grained analysis of sentences in order to identify
its main focus and then based the sentiment clas-
sification on it.
References
Cohen, J. 1960. A coefficient of agreement for nomi-
nal scales. Educational and psychological measure-
ment, 20(1):37.
Glance, N., M. Hurst, K. Nigam, M. Siegler, R. Stock-
ton, and T. Tomokiyo. 2005. Deriving marketing
intelligence from online discussion. In Proceed-
ings of the eleventh ACM SIGKDD, pages 419?428.
ACM.
Go, A., R. Bhayani, and L. Huang. 2009. Twit-
ter sentiment classification using distant supervi-
sion. Technical report, Stanford Digital Library
Technologies Project.
Landis, J.R. and G.G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, pages 159?174.
LingPipe. 2008. LingPipe 3.9.1.
http://alias-i.com/lingpipe.
Pang, B. and L. Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the ACL, volume 2004.
43
Pang, B., L. Lee, and S. Vaithyanathan. 2002. Thumbs
up?: sentiment classification using machine learn-
ing techniques. In Proceedings of the ACL, pages
79?86. Association for Computational Linguistics.
Polikar, R. 2006. Ensemble based systems in deci-
sion making. IEEE Circuits and systems magazine,
6(3):21?45.
Prentice, S. and E. Huffman. 2008. Social Medias
New Role In Emergency Management. Idaho Na-
tional Laboratory, pages 1?5.
Riloff, E. and J. Wiebe. 2003. Learning extraction pat-
terns for subjective expressions. In Proceedings of
the 2003 conference on Empirical methods in natu-
ral language processing, pages 105?112.
Riloff, E., J. Wiebe, and T. Wilson. 2003. Learning
subjective nouns using extraction pattern bootstrap-
ping. In Proceedings of the 7th Conference on Nat-
ural Language Learning, pages 25?32.
Riloff, E., S. Patwardhan, and J. Wiebe. 2006. Feature
subsumption for opinion analysis. In Proceedings
of the 2006 Conference on Empirical Methods in
Natural Language Processing, pages 440?448. As-
sociation for Computational Linguistics.
Sheng, V.S., F. Provost, and P.G. Ipeirotis. 2008. Get
another label? Improving data quality and data min-
ing using multiple, noisy labelers. In Proceeding of
the 14th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 614?
622. ACM.
Wiebe, J. and E. Riloff. 2005. Creating subjective
and objective sentence classifiers from unannotated
texts. Computational Linguistics and Intelligent
Text Processing, pages 486?497.
Wiebe, J.M., RF Brace, and T.P. O?Hara. 1999. Devel-
opment and use of a gold-standard data set for sub-
jectivity classifications. In Proceedings of the ACL,
pages 246?253. Association for Computational Lin-
guistics.
Wilson, T., J. Wiebe, and P. Hoffmann. 2005. Rec-
ognizing contextual polarity in phrase-level senti-
ment analysis. In EMNLP, page 354. Association
for Computational Linguistics.
Witten, Ian H. and Eibe Frank. 2005. Data Mining:
Practical machine learning tools and techniques.
Morgan Kaufmann.
44
