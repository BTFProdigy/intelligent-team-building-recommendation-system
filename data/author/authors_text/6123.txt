Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 971?978, Vancouver, October 2005. c?2005 Association for Computational Linguistics
An Orthonormal Basis for Topic Segmentation in Tutorial Dialogue 
 
 
Andrew Olney Zhiqiang Cai 
Department of Computer Science Institute for Intelligent Systems 
University of Memphis University of Memphis 
Memphis, TN 38152 Memphis, TN 38152 
aolney@memphis.edu zcai@memphis.edu 
 
 
 
 
Abstract 
This paper explores the segmentation of 
tutorial dialogue into cohesive topics. A 
latent semantic space was created using 
conversations from human to human tu-
toring transcripts, allowing cohesion be-
tween utterances to be measured using 
vector similarity.  Previous cohesion-
based segmentation methods that focus on 
expository monologue are reapplied to 
these dialogues to create benchmarks for 
performance.  A novel moving window 
technique using orthonormal bases of se-
mantic vectors significantly outperforms 
these benchmarks on this dialogue seg-
mentation task. 
1 Introduction 
Ever since Morris and Hirst (1991)?s ground-
breaking paper, topic segmentation has been a 
steadily growing research area in computational 
linguistics, with applications in summarization 
(Barzilay and Elhadad, 1997), information retrieval 
(Salton and Allan, 1994), and text understanding 
(Kozima, 1993).  Topic segmentation likewise has 
multiple educational applications, such as question 
answering, detecting student initiative, and assess-
ing student answers. 
There have been essentially two approaches to 
topic segmentation in the past.  The first of these, 
lexical cohesion, may be used for either linear 
segmentation (Morris and Hirst, 1991; Hearst, 
1997) or hierarchical segmentation (Yarri, 1997; 
Choi, 2000).  The essential idea behind the lexical 
cohesion approaches is that different topics will 
have different vocabularies.  Therefore the lexical 
cohesion within topics will be higher than the lexi-
cal cohesion between topics, and gaps in cohesion 
may mark topic boundaries. The second major ap-
proach to topic segmentation looks for distinctive 
textual or acoustic markers of topic boundaries, 
e.g. referential noun phrases or pauses (Passonneau 
and Litman, 1993; Passonneau and Litman, 1997).  
By using multiple markers and machine learning 
methods, topic segmentation algorithms may be 
developed using this second approach that have a 
higher accuracy than methods using a single 
marker alone (Passonneau and Litman, 1997). 
The primary technique used in previous studies, 
lexical cohesion, is no stranger to the educational 
NLP community.  Lexical cohesion measured by 
latent semantic analysis (LSA) (Landauer and Du-
mais, 1997; Dumais, 1993; Manning and Sch?tze, 
1999) has been used in automated essay grading 
(Landauer, Foltz, and Laham, 1998) and in under-
standing student input during tutorial dialogue 
(Graesser et al, 2001). The present paper investi-
gates an orthonormal basis of LSA vectors, cur-
rently used by the AutoTutor ITS to assess student 
answers (Hu et al, 2003), and how it may be used 
to segment tutorial dialogue. 
The focus on dialogue distinguishes our work 
from virtually all previous work on topic segmen-
tation: prior studies have focused on monologue 
rather than dialogue. Without dialogue, previous 
approaches have only limited relevance to interac-
tive educational applications such as intelligent 
tutoring systems (ITS).  The only existing work on 
topic segmentation in dialogue, Galley et al 
(2003), segments recorded speech between multi-
ple persons using both lexical cohesion and dis-
971
tinctive textual and acoustic markers.  The present 
work differs from Galley et al (2003) in two re-
spects, viz. we focus solely on textual information 
and we directly address the problem of tutorial dia-
logue.   
In this study we apply the methods of Foltz et al 
(1998), Hearst (1994, 1997), and a new technique 
utilizing an orthonormal basis to topic segmenta-
tion of tutorial dialogue.  All three are vector space 
methods that measure lexical cohesion to deter-
mine topic shifts.  Our results show that the new 
using an orthonormal basis significantly outper-
forms the other methods. 
Section 2 reviews previous work, and Section 3 
reviews the vector space model.  Section 4 intro-
duces an extension of the vector space model 
which uses an orthonormal basis.  Section 5 out-
lines the task domain of tutorial dialogue, and Sec-
tion 6 presents the results of previous and the 
current method on this task domain. A discussion 
and comparison of these results takes place in Sec-
tion 7.  Section 8 concludes. 
2 Previous work 
Though the idea of using lexical cohesion to seg-
ment text has the advantages of simplicity and in-
tuitive appeal, it lacks a unique implementation.  
An implementation must define how to represent 
units of text, compare the cohesion between units, 
and determine whether the results of comparison 
indicate a new text segment.  Both Hearst (1994, 
1997) and Foltz et al (1998) use vector space 
methods discussed below to represent and compare 
units of text. The comparisons can be characterized 
by a moving window, where successive overlap-
ping comparisons are advanced by one unit of text.  
However, Hearst (1994, 1997) and Foltz et al 
(1998) differ on how text units are defined and on 
how to interpret the results of a comparison. 
The text unit's definition in Hearst (1994, 1997) 
and Foltz et al (1998) is generally task dependent, 
depending on what size gives the best results. For 
example, when measuring comprehension, Foltz et 
al. (1998) use the unit of the sentence, as opposed 
to the more standard unit of the proposition, be-
cause LSA is most correlated with comprehension 
at that level.  However, when using LSA to seg-
ment text, Foltz et al (1998) use the paragraph as 
the unit, to "smooth out" the local changes in cohe-
sion and become more sensitive to more global 
changes of cohesion.  Hearst likewise chooses a 
large unit, 6 token-sequences of 20 tokens (Hearst, 
1994), but varies these parameters dependent on 
the characteristics of the text to be segmented, e.g. 
paragraph size.  
Under a vector space model, comparisons are 
performed by calculating the cosine of vectors rep-
resenting text.  As stated previously, these com-
parisons reflect the cohesion between units of text. 
In order to use these comparisons to segment text, 
however, one must have a criterion in place.  Foltz 
et al (1998), noting mean cosines of .16 for 
boundaries and .43 for non-boundaries, choose a 
threshold criterion of .15, which is two standard 
deviations below the boundary mean of .43.  Using 
LSA and this criterion, Foltz et al (1998) detected 
chapter boundaries with an F-measure of .33 (see 
Manning and Sch?tze (1999) for a definition of F-
measure).  Hearst (1994, 1997) in contrast uses a 
relative comparison of cohesion, by recasting vec-
tor comparisons as depth scores.  A depth score is 
computed as the difference between a given vector 
comparison and its surrounding peaks, i.e. the local 
maxima of vector comparisons on either side of the 
given vector comparison.  The greater the differ-
ence between a given comparison and its surround-
ing peaks, the higher the depth score.  Once all the 
depth scores are calculated for a text, those that are 
higher than one standard deviation below the mean 
are taken as topic boundaries.  Using a vector 
space method without singular value decomposi-
tion, Hearst (1997) reports an F-measure of .70 
when detecting topic shifts between paragraphs.  
Thus previous work suggests that the Hearst 
(1997) method is superior to that of Foltz et al 
(1998), having roughly twice the accuracy indi-
cated by F-measure.  Although these two results 
used different data sets and are therefore not di-
rectly comparable, one would predict based on this 
limited evidence that the Hearst algorithm would 
outperform the Foltz algorithm on other topic seg-
mentation tasks. 
972
3 The vector space model 
The vector space model is a statistical technique 
that represents the similarity between collections of 
words as a cosine between vectors (Manning and 
Sch?tze, 1999).  The process begins by collecting 
text into a corpus.  A matrix is created from the 
corpus, having one row for each unique word in 
the corpus and one column for each document or 
paragraph.  The cells of the matrix consist of a 
simple count of the number of times word i ap-
peared in document j.  Since many words do not 
appear in any given document, the matrix is often 
sparse.  Weightings are applied to the cells that 
take into account the frequency of word i in docu-
ment j and the frequency of word i across all 
documents, such that distinctive words that appear 
infrequently are given the most weight. Two col-
lections of words of arbitrary size are compared by 
creating two vectors.  Each word is associated with 
a row vector in the matrix, and the vector of a col-
lection is simply the sum of all the row vectors of 
words in that collection.  Vectors are compared 
geometrically by the cosine of the angle between 
them. 
LSA (Landauer and Dumais, 1997; Dumais 
1993) is an extension of the vector space model 
that uses singular value decomposition (SVD).  
SVD is a technique that creates an approximation 
of the original word by document matrix.  After 
SVD, the original matrix is equal to the product of 
three matrices, word by singular value, singular 
value by singular value, and singular value by 
document.  The size of each singular value corre-
sponds to the amount of variance captured by a 
particular dimension of the matrix.  Because the 
singular values are ordered in decreasing size, it is 
possible to remove the smaller dimensions and still 
account for most of the variance.  The approxima-
tion to the original matrix is optimal, in the least 
squares sense, for any number of dimensions one 
would choose.  In addition, the removal of smaller 
dimensions introduces linear dependencies be-
tween words that are distinct only in dimensions 
that account for the least variance.  Consequently, 
two words that were distant in the original space 
can be near in the compressed space, causing the 
inductive machine learning and knowledge acqui-
sition effects reported in the literature (Landauer 
and Dumais, 1997).  
4 An orthonormal basis 
Cohesion can be measured by comparing the co-
sines of two successive sentences or paragraphs 
(Foltz, Kintsch, and Landauer, 1998).  However, 
cohesion is a crude measure: repetitions of a single 
sentence will be highly cohesive (cosine of 1) even 
though no new information is introduced.  A varia-
tion of the LSA algorithm using orthonormalized 
vectors provides two new measures, ?informativ-
ity? and ?relevance?, which can detect how much 
new information is added and how relevant it is in 
a context (Hu et al, 2003).  The essential idea is to 
represent context by an orthonormalized basis of 
vectors, one vector for each utterance.  The basis is 
a subspace of the higher dimensional LSA space, 
in the same way as a plane or line is a subspace of 
3D space.  The basis is created by projecting each 
utterance vector onto the basis of previous utter-
ance vectors using a method known as the Gram-
Schmidt process (Anton, 2000).  Each projected 
utterance vector has two components, a component 
parallel to the basis and a component perpendicular 
to the basis.  These two components represent ?in-
formativity? and ?relevance?, respectively.  Let us 
first consider ?relevance?. Since each vector in the 
basis is orthogonal, the basis represents all linear 
combinations of what has been previously said.  
Therefore the component of a new utterance vector 
that is parallel to the basis is already represented 
by a linear combination of the existing vectors.  
?Informativity? follows similarly: it is the perpen-
dicular component of a new utterance vector that 
can not be represented by the existing basis vec-
tors. For example, in Figure 1, a new utterance cre-
ates a new vector that can be projected to the basis, 
forming a triangle.  The leg of the triangle that lies 
VS 1 
VS 2
Informativity
Relevance 
Figure 1.  Projecting a new utterance to the basis
973
along the basis indicates the ?relevance? of the 
recent utterance to the basis; the perpendicular leg 
indicates new information.  Accordingly, a re-
peated utterance would have complete ?relevance? 
but zero new information. 
5 Procedure 
The task domain is a subset of conversations from 
human-human computer mediated tutoring ses-
sions on Newton?s Three Laws of Motion, in 
which tutor and tutee engaged in a chat room-style 
conversation.  The benefits of this task domain are 
twofold. Firstly, the conversations are already tran-
scribed.  Additionally, tutors were instructed to 
introduce problems using a fixed set of scripted 
problem statements.  Therefore each topic shift 
corresponds to a distinct problem introduced by the 
tutor.  Clearly this problem would be trivial for a 
cue phrase based approach, which could learn the 
finite set of problem introductions. However, the 
current lexical approach does not have this luxury: 
words in the problem statements recur throughout 
the following dialogue. 
Human to human computer mediated physics tu-
toring transcripts first were removed of all markup, 
translated to lower case, and each utterance was 
broken into a separate paragraph.  An LSA space 
was made with these paragraphs alone, approxi-
mately one megabyte of text.  The conversations 
were then randomly assigned to training (21 con-
versations) and testing (22 conversations).  The 
average number of utterances per topic, 16 utter-
ances, and the average number of words per utter-
ance, 32 words, were calculated to determine the 
parameters of the segmentation methods.  For ex-
ample, a moving window size greater than 16 ut-
terances implies that, in the majority of 
occurrences, the moving window straddles three 
topics as opposed to the desired two.  
To replicate Foltz et al (1998), software was 
written in Java that created a moving window of 
varying sizes on the input text, and the software 
retrieved the LSA vector and calculated the cosine 
of each window.  Hearst (1994, 1997) was repli-
cated using the JTextTile (Choi, 1999) Java soft-
ware. A variant of Hearst (1994, 1997) was created 
by using LSA instead of the standard vector space 
method.  The orthonormal basis method also used 
a moving window; however, in contrast to the pre-
vious methods, the window is not treated just as a 
large block of text.  Instead, the window consists 
of two orthonormal bases, one on either side of an 
utterance.  That is, a region of utterances above the 
test utterance is projected, utterance by utterance, 
into an orthonormal basis, and likewise a region of 
utterances below the test utterance is projected into 
another orthonormal basis.  Then the test utterance 
is projected into each orthonormal basis, yielding 
measures of ?relevance? and ?informativity? with 
respect to each.  Next the elements that make up 
each orthonormal basis are aggregated into a block, 
and a cosine is calculated between the test utter-
ance and the blocks on either side, producing a 
total of six measures. 
Each tutoring session consists of the same 10 
problems, discussed between one of a set of 4 tu-
tors and one of 18 subjects. The redundancy pro-
vides a variety of speaking and interaction styles 
on the same topic. 
Tutor: A clown is riding a 
unicycle in a straight line.  
She accidentally drops an egg 
beside her as she continues 
to move with constant veloc-
ity. Where will the egg land 
relative to the point where 
the unicycle touches the 
ground?  Explain. 
Student: The egg should land 
right next to the unicycle.  
The egg has a constant hori-
zontal velocity.  The verti-
cal velocity changes and 
decreases as gravity pulls 
the egg downward at a rate of 
9.8m/s^2.  The egg should 
therefore land right next to 
the unicycle. 
Tutor: Good! There is only 
one thing I would like to 
know. What can you say about 
the horizontal velocity of 
the egg compared to the hori-
zontal velocity of the clown? 
Student: Aren't they the 
same? 
All of the 10 problems are designed to require ap-
plication of Newton?s Laws to be solved, and 
974
therefore conversations share many terms such as 
force, velocity, acceleration, gravity, etc.  
6 Results 
For each method, the development set was first 
used to establish the parameters such as text unit 
size and classification criterion.  The methods, 
tuned to these parameters, were then applied to the 
testing data. 
 
6.1 Foltz et al (1998) 
In order to replicate Foltz et al?s results, a text unit 
size and window size needed to be chosen.  The 
utterance was chosen as the text unit size, which 
included single word utterances, full sentences, and 
multi-sentence utterances.  To determine the most 
appropriate window size, results from all sizes be-
tween 1 and 16 (the average number of utterances 
between topic shifts) were gathered.  The greatest 
difference between the means for utterances that 
introduce a topic shift versus non-shift utterances 
occurs when the window contains four utterances.  
The standard deviation is uniformly low for win-
dows containing more than two utterances and 
therefore can be disregarded in choosing a window 
size.   
The optimal cosine threshold for classification 
was found using logistic regression (Garson, 2003) 
which establishes a relationship between the cosine 
threshold and the log odds of classification. The 
optimal cutoff was found to be shift odds = .17 
with associated F-measure of .49.  The logistic 
equation of best fit is: 
 
cosine)  (-13.345  1.887  odds)ln(shift ?+=  
 
F-measure of .49 is 48% higher than the F-
measure reported by Foltz et al (1998) for seg-
menting monologue.  On the testing corpus the F-
measure is .52, which demonstrates good generali-
zation for the logistic equation given.  Compared 
the F-measure of .33 reported by Foltz et al 
(1998), the current result is 58% higher. 
6.2 Hearst (1994, 1997) 
The JTextTile software was used to implement 
Hearst (1994) on dialogue.  As with Foltz et al 
(1998), a text unit and window size had to be de-
termined for dialogue. Hearst (1994) recommends 
using the average paragraph size as the window 
size.  Using the development corpus's average 
topic length of 16 utterances as a reference point, 
F-measures were calculated for the combinations 
of window size and text unit size in Table 1. 
The optimal combination of parameters (F-
measure = .17) is a unit size of 16 words and a 
window size of 16 units.  This combination 
matches Hearst (1994)'s heuristic of choosing the 
window size to be the average paragraph length.  
  
Table 1. Unit vs. window size for Hearst method 
 
On the test set, this combination of parameters 
yielded an F-measure of .14 as opposed to the F-
measure for monologue reported by Hearst (1997), 
.70.  For dialogue, the algorithm is 20% as effec-
tive as it is for monologue.  It is unclear, however, 
exactly what part of the algorithm contributes to 
this poor performance. The two most obvious pos-
sibilities are the segmentation criterion, i.e. depth 
scores, or the standard vector space method. 
To further explore these possibilities, the Hearst 
method was augmented with LSA.  Again, the unit 
size and window size had to be calculated.  As 
with Foltz, the unit size was taken to be the utter-
ance.  The window size was determined by com-
puting F-measures on the development corpus for 
all sizes between 1 and 16.  The optimal window 
size is 9, F-measure = .22.  Given the smaller 
number of test cases, 22, this F-measure of .22 is 
not significantly different from .17.  However, the 
Foltz method is significantly higher than both of 
these, p < .10. 
6.3 Orthonormal basis 
The text unit used in the orthonormal basis is the 
single utterance.  The optimal window size, i.e. the 
orthonormal basis size, was determined by creating 
a logistic regression to calculate the maximum F-
measure for several orthonormal basis sizes.  The 
findings of this procedure are listed in Table 2. 
    Window 
size 
   
  2 4 8 16 32 
Unit 
size 8 .134 .129 .130 .146 .144 
 16 .142 .133 .130 .171 .140 
 32 .138 .132 .130 .151 .143 
975
 Table 2. F-measure for orthonormal basis sizes 
 
F-measure monotonically increases until the or-
thonormal basis holds six elements and holds rela-
tively steady for larger orthonormal basis sizes.  
Since F-measure does not increase much over .72 
for greater orthonormal basis sizes, 6 was chosen 
as the most computationally efficient size for the 
strength of the effect.  The logistic equation of best 
fit is: 
 
)ityinformativ(2.771
)relevance(-2.698
)ityinformativ(-23.567
)relevance(-30.843
)cosine (16.703 
 20.027 
odds)ln(shift  
2
2
1
1
2
?+
?+
?+
?+
?+
=   
 
Where the index of 1 indicates a measure on the 
window preceding the utterance, and an index of 2 
indicates a measure on the window following the 
utterance.  In the regression, the cosine between 
the utterance and the preceding window was not 
significant, p = .86.  This finding reflects the intui-
tion that the cosine to the following window varies 
according to whether the following window is on a 
new topic, whereas the cosine to the preceding 
window is always high. Additionally, measures of 
?relevance? and ?informativity? correspond to vec-
tor length; all other measures did not contribute 
significantly to the model and so were not in-
cluded.   
The sign of the metrics illuminates their role in 
the model.  The negative sign on the coefficients 
for relevance1, informativity1, and relevance2 indi-
cates that they are inversely correlated with an ut-
terance signaling the start of a new topic.  The only 
surprising feature is that informativity1 is nega-
tively correlated instead of positively correlated: 
one would expect a topic shift to introduce new 
information.  There is possibly some edge effect 
here, since the last move of a topic is often a sum-
marizing move that shares many of the physics 
terms present in the introduction of a new topic.  
On the other hand, the positive sign on cosine2 and 
informativity2 indicates that the start of a new topic 
should have elements in common with the follow-
ing material and add new information to that mate-
rial, as an overview would.  Beyond the sign, the 
exponentials of these values indicate how the two 
basis metrics are weighted. For example, when 
informativity2 is raised by one unit, a topic shift is 
16 times more likely.   
On the testing corpus the F-measure of the or-
thonormal basis method is .67, which is signifi-
cantly different from the performance of all three 
methods mentioned above, p < .05.   Table 3 com-
pares this result with the previous results in the 
current study for segmenting dialogue. 
 
Method Hearst Hearst + LSA Foltz 
Orth. 
basis 
F .14 .22 .52 .67 
Table 3. Comparison of dialogue segmentation methods 
7 Discussion 
The relative ranking of these results is not alto-
gether surprising given the relationships between 
inferencing and LSA and between inferencing and 
dialogue.  Foltz et al (1998) found that LSA 
makes simple bridging inferences in addition to 
detecting lexical cohesion.  These bridging infer-
ences are a kind of collocational cohesion (Halli-
day and Hassan, 1976) whereby words that co-
occur in similar contexts become highly related in 
the LSA space.  Therefore in applications where 
this kind of inferencing is required, one might ex-
pect an LSA based method to excel. 
Similarly to van Dijk and Kintsch's model of 
comprehension (van Dijk and Kintsch, 1983), dia-
logue can require inferences to maintain coher-
ence. According to Grice's Co-operative Principle, 
utterances lacking semantic coherence flout the 
Maxim of Relevance and license an inference 
(Grice, 1975):  
 
S1: Let?s go dancing. 
S2: I have an exam tomorrow. 
 
The "inference" in the sense of Foltz, Kintsch, 
and Landauer (1998) would be represented by a 
high cosine between these utterances, even though 
they don't share any of the same words.  Dialogue 
generally tends to be less lexically cohesive and 
require more inferencing than expository mono-
Size 3 4 5 6 8 10 15 
F .59 .63 .65 .72 .73 .72 .73
976
logue, so one might predict that LSA would excel 
in dialogue applications. 
However, LSA has a weakness: the cosine 
measure between two vectors does not change 
monotonically as new word vectors are added to 
either of the two vectors.  Accordingly, the addi-
tion of a word vector can cause the cosine between 
two text units to dramatically increase or decrease. 
Therefore the distinctive properties of individual 
words can be lost with the addition of more words 
to a text unit.  This problem can be addressed by 
using an orthonormal basis (Hu et al, 2003).   By 
using a basis, each utterance is kept independent, 
so ?inferencing? can extend over both the entire set 
of utterances and the linear combination of any of 
its subsets.  Accordingly, when ?inferencing? over 
the entire text unit is required, one would expect a 
basis method using LSA vectors to outperform a 
standard LSA method.  This expectation has been 
put to the test recently by Olney & Cai (2005), 
who find that an orthonormal basis can signifi-
cantly predict entailment on test data supplied by 
the PASCAL Textual Entailment Challenge 
(PASCAL, 2004). 
Beyond relative performance rankings, more 
support for the above reasoning can be found in the 
difference between Hearst and Hearst + LSA. Re-
call that in monologue, Hearst (1997) reports a 
much larger F-measure than Foltz et al (1998), .70 
vs. .33, albeit on different data sets.  In the present 
dialogue corpus, these roles are reversed, .14 vs. 
.52.  Possible reasons for this reversal are the seg-
mentation criterion, the vector space method, or 
the fact that Foltz has been trained on similar data 
via regression and Hearst has not.  However, com-
paring the Hearst algorithm with the Hearst + LSA 
algorithm indicates that a 57% improvement stems 
from the addition of LSA, keeping all other factors 
constant. While this result is not statistically sig-
nificant, the direction of the result supports the use 
of an ?inferencing? vector space method for seg-
menting dialogue.  
Unfortunately, the large difference in F-measure 
between the Foltz algorithm and the Hearst + LSA 
algorithm is more difficult to explain.  These two 
methods differ by their segmentation criterion and 
by their training (Foltz is a regression model and 
Hearst is not). It may be that Hearst (1994, 1997)?s 
segmentation criterion, i.e. depth scores, do not 
translate well to dialogue.  Perhaps the assignment 
of segment boundaries based on the relative differ-
ence between a candidate score and its surrounding 
peaks is highly sensitive to cohesion gaps created 
by conversational implicatures.  On the other hand 
the differences between these two methods may be 
entirely attributable to the amount of training they 
received.  One way to separate the contributions of 
the segmentation criterion and training would be to 
create a logistic model using the Hearst + LSA 
method and to compare this to Foltz.  
The increased effectiveness of the orthonormal 
basis method over the Foltz algorithm can also be 
explained in terms of ?inferencing?.  Since ?infer-
encing? is overwhelmed by lexical cohesion (Foltz 
et al, 1998), the increase in window size for the 
Foltz algorithm deteriorates performance for a 
window size greater than 4.  In contrast, the or-
thonormal basis method becomes most effective as 
the orthonormal basis size increases past 4.  This 
dichotomy illustrates that the Foltz algorithm is not 
complementary to an ?inferencing? approach in 
general.  Use of an orthonormal basis, on the other 
hand, increases sensitivity to collocational cohe-
sion without sacrificing lexical cohesion. 
8 Conclusion 
This study explored the segmentation of tutorial 
dialogue using techniques that have previously 
been applied to expository monologue and using a 
new orthonormal basis technique.  The techniques 
previously applied to monologue reversed their 
roles of effectiveness when applied to dialogue.  
This role reversal suggests the predominance of 
collocational cohesion, requiring ?inferencing?, 
present in this tutorial dialogue.  The orthonormal 
basis method, which we suggest has an increased 
capacity for ?inferencing?, outperformed both of 
the techniques previously applied to monologue, 
and demonstrates that segmentation of these tuto-
rial dialogues most benefits from a method sensi-
tive to lexical and collocational cohesion over 
large text units. 
Acknowledgements 
This research was supported by the National Sci-
ence Foundation (SBR 9720314, REC 0089271, 
REC 0106965, REC 0126265) and the DoD Mul-
tidisciplinary University Research Initiative 
(MURI) administered by ONR under grant 
N00014-00-1-0600.  Any opinions, findings, and 
977
conclusions or recommendations expressed in this 
material are those of the authors and do not neces-
sarily reflect the views of DoD, ONR, or NSF. 
References 
Anton, H.  (2000). Elementary linear algebra. 8th edi-
tion.  New York: John Wiley. 
Barzilay, R. & Elhadad, M. (1997). Using Lexical 
Chains for Text Summarization. Proceedings of the 
Intelligent Scalable Text Summarization Workshop. 
Choi, F. (1999). JTextTile: A free platform independent 
text segmentation algorithm.  
http://www.cs.man.ac.uk/~choif 
Choi, F. (2000). Advances in domain independent linear 
text segmentation. In Proceedings of the NAACL?00, 
May. 
van Dijk, T. A., & Kintsch, W. (1983). Strategies of 
Discourse Comprehension. New York: Academic 
Press.  
Dumais, S. (1993). LSI meets TREC: a status report. In 
Proceedings of the First Text Retrieval Conference 
(TREC1), 137-152. NIST Special Publication 500-
207. 
Foltz, P.W., Kintsch, W. & Landauer, T.K. (1998). The 
measurement of textual cohesion with latent semantic 
analysis. Discourse Processes, 25, 285-307. 
Galley, M., McKeown, K., Fosler-Lussier, E., & Jing, 
H. (2003). Discourse Segmentation of Multi-Party 
Conversation. Proceedings of the ACL. 
Garson, D. Logistic Regression. Accessed on April 
18th, 2003.  http://www2.chass.ncsu.edu/garson/ 
pa765/logistic 
Graesser, A. C., Person, N. K., Harter, D., & the Tutor-
ing Research Group. (2001). Teaching tactics and 
dialogue in AutoTutor.  International Journal of Arti-
ficial Intelligence in Education, 12, 257-279. 
Grice, H.P. (1975). Logic and conversation. In P. Cole 
& J. Morgan (Eds) Syntax and Semantics Vol 3. 41-
58. New York: Academic. 
Grosz, B.J. & Sidner, C.L. (1986). Attention, Intentions, 
and the structure of discourse. Computational Lin-
guistics, 12 (3), 175-204. 
Halliday, M. A. & Hassan, R. A.  (1976). Cohesion in 
English.  London: Longman. 
Hearst, M. (1994). Multi-paragraph segmentation of 
expository text. In Proceedings of the 32nd meeting 
of the Association for Computational Linguistics.  9-
16. 
Hearst, M. (1997). Text-Tiling: segmenting text into 
multi-paragraph subtopic passages.  Computational 
Linguistics, 23(1), 33-64. 
Hu, X., Cai, Z., Louwerse, M., Olney, A.,  Penumatsa, 
P., and Graesser, A. (2003). An improved LSA algo-
rithm to evaluate contributions in student dialogue.  
In Proceedings of the Eighteenth International Joint 
Conference on Artificial Intelligence (IJCAI-03), 
1489-1491. 
Kozima, H. (1993). Text segmentation based on similar-
ity between words. In Proceedings of ACL '93, 286-
288. 
Landauer, T. & Dumais, S. (1997). A solution to Plato 's 
problem: the latent semantic analysis theory of acqui-
sition, induction, and representation of knowledge. 
Psychological Review, 104, 211-240. 
Landauer, T. K., Foltz, P. W., & Laham, D. (1998). In-
troduction to Latent Semantic Analysis. Discourse 
Processes, 25, 259-284. 
Manning, C. & Sch?tze, H. (1999). Foundations of Sta-
tistical Natural Language Processing. Cambridge: 
MIT Press.  
Morris, J. & Hirst, G. (1991). Lexical Cohesion Com-
puted by Thesaural Relations as an Indicator of the 
Structure of Text. Computational Linguistics, 17(1), 
21-48. 
Olney, A., & Cai, Z. (2005). An Orthonormal Basis for 
Entailment. In Proceedings of the Eighteenth Interna-
tional Florida Artificial Intelligence Research Society 
Conference, 554-559. Menlo Park, Calif.: AAAI 
Press. 
PASCAL. 2004. Recognising Textual Entailment Chal-
lenge. Accessed on October 4th, 2004. 
http://www.pascal-network.org/Challenges/RTE/ 
Passonneau, R. J. & Litman, D. J. (1993). Intention-
based Segmentation: Human Reliability and correla-
tion with linguistic cues. Proceedings of the ACL, 
148-155. 
Passonneau, R. J. & Litman, D. J. (1997). Discourse 
segmentation by human and automated means. Com-
putational Linguistics, 23(1), 103?139. 
Salton, G. & Allan, J. (1994). Automatic text decompo-
sition and structuring. In Proceedings of RIAO, 6?29, 
New York, NY. 
Yaari, Y. (1997). Segmentation of expository texts by 
hierarchical agglomerative clustering. Proceedings of 
the RANLP'97. 
978
Utterance Classification in AutoTutor 
Andrew 
Olney 
Max 
Louwerse 
 Eric 
Matthews 
Johanna 
Marineau 
Heather 
Hite-Mitchell 
Arthur 
Graesser 
Institute for Intelligent Systems 
University of Memphis 
Memphis, TN 38152 
aolney@memphis.edu 
 
Abstract 
This paper describes classification of typed 
student utterances within AutoTutor, an intel-
ligent tutoring system.  Utterances are classi-
fied to one of 18 categories, including 16 
question categories.  The classifier presented 
uses part of speech tagging, cascaded finite 
state transducers, and simple disambiguation 
rules. Shallow NLP is well suited to the task: 
session log file analysis reveals significant 
classification of eleven question categories, 
frozen expressions, and assertions.  
1 Introduction 
AutoTutor is a domain-portable intelligent tutoring 
system (ITS) with current versions in the domains of 
physics and computer literacy (Graesser et al 1999; 
Olney et al 2002).   AutoTutor, like many other ITSs, is 
an intersection of applications, including tutoring, 
mixed-initiative dialogue, and question answering.  In 
each of these, utterance classification, particularly ques-
tion classification, plays a critical role.  
In tutoring, utterance classification can be used to 
track the student's level of understanding.  Contribution 
and question classifications can both play a role: contri-
butions may be compared to an expected answer 
(Graesser et al 2001) and questions may be scored by 
how "deep" they are.  For example, The PREG model 
(Otero and Graesser 2001) predicts under what circum-
stances students will ask "deep" questions, i.e. those that 
reveal a greater level of cognitive processing than who, 
what, when, or where questions.  A student who is only 
asking shallow questions, or no questions at all, is pre-
dicted by PREG to not have a situation-level under-
standing (van Dijk and Kintsch 1983) and thus to learn 
less and forget faster.  The key point is that different 
metrics for tracking student understanding are applica-
ble to questions and contributions.  Distinguishing them 
via classification is a first step to applying a metric. 
In mixed-initiative dialog systems, utterance classifi-
cation can be used to detect shifts in initiative.  For ex-
ample, a mixed-initiative system that asks, "Where 
would you like to travel", could respond to the question, 
"Where can I travel for $200?" (Allen 1999) by giving a 
list of cities.  In this example, the user is taking the ini-
tiative by requesting more information.  In order to re-
spond properly, the system must detect that the user has 
taken initiative before it can respond appropriately; oth-
erwise it might try to interpret the user's utterance as a 
travel destination.  In this sense, questions mark redirec-
tion of the dialogue, whereas contributions are continua-
tions of the dialogue.  In order for a user to redirect the 
dialogue and thus exercise initiative, a mixed-initiative 
system must be able to distinguish questions and contri-
butions. 
Question classification as early as Lehnert (1978) 
has been used as a basis for answering questions, a trend 
that continues today (Voorhees 2001).  A common fea-
ture of these question-answering systems is that they 
first determine the expected answer type implicit in the 
question.  For example, "How much does a pretzel cost" 
might be classified according to the answer type of 
MONEY or QUANTITY.  Knowledge of the expected an-
swer type can be used to narrow the search space for the 
answer, either online (Brill et al 2001) or in a database 
(Harabagiu et al 2000).  Accordingly, question answer-
ing calls for a finer discrimination of question types as 
opposed to only distinguishing questions from contribu-
tions. 
AutoTutor uses utterance classification to track stu-
dent progress, to determine initiative, and to answer 
questions. By virtue of being embedded in AutoTutor, 
the utterance classifier presented here has an unusual set 
of constraints, both practical and theoretical.  On the 
practical side, AutoTutor is a web-based application that 
performs in real time; thus utterance classification must 
also proceed in real time.  For that reason, the classifier 
uses a minimum of resources, including part of speech 
tagging (Brill 1995; Sekine and Grishman 1995) and 
cascaded finite state transducers defining the categories.  
Theoretically speaking, AutoTutor must also recognize 
questions in a meaningful way to both question answer-
ing and tutoring.  The question taxonomy utilized, that 
of Graesser et al(1992), is an extension of Lehnert's 
(1978) taxonomy for question answering and has been 
applied to human tutoring (Graesser et al 1992; 
Graesser and Person 1994).   
This paper outlines the utterance classifier and quan-
tifies its performance.  In particular, Section 2 presents 
AutoTutor.  Section 3 presents the utterance taxonomy.  
Section 4 describes the classifier algorithm.  Section 5 
delineates the training process and results.  Section 6 
presents evaluation of the classifier on real AutoTutor 
sessions.  Section 7 concludes the paper. 
2 AutoTutor 
AutoTutor is an ITS applicable to any content domain.  
Two distinct domain applications of AutoTutor are 
available on the Internet, for computer literacy and con-
ceptual physics.  The computer literacy AutoTutor, 
which has now been used in experimental evaluations 
by over 200 students, tutors students on core computer 
literacy topics covered in an introductory course, such 
as operating systems, the Internet, and hardware.  The 
topics covered by the physics AutoTutor are grounded 
in basic Newtonian mechanics and are of a similar in-
troductory nature.  It has been well documented that 
AutoTutor promotes learning gains in both versions 
(Person et al 2001). 
AutoTutor simulates the dialog patterns and peda-
gogical strategies of human tutors in a conversational 
interface that supports mixed-initiative dialog. AutoTu-
tor?s architecture is comprised of seven highly modular 
components: (1) an animated agent, (2) a curriculum 
script, (3) a speech act classifier,  (4) latent semantic 
analysis (LSA), (5) a dialog move generator, (6) a Dia-
log Advancer Network, and (7) a question-answering 
tool (Graesser et al 1998; Graesser et al 2001; 
Graesser et al 2001; Person et al 2000; Person et al 
2001; Wiemer-Hastings et al 1998).  
A tutoring session begins with a brief introduction 
from AutoTutor?s three-dimensional animated agent.  
AutoTutor then asks the student a question from one of 
topics in the curriculum script. The curriculum script 
contains lesson-specific tutor-initiated dialog, including 
important concepts, questions, cases, and problems 
(Graesser and Person 1994; Graesser et al 1995; 
McArthur et al 1990; Putnam 1987). The student sub-
mits a response to the question by typing and pressing 
the ?Submit? button. The student?s contribution is then 
segmented, parsed (Sekine and Grishman 1995) and 
sent through a rule-based utterance classifier.  The clas-
sification process makes use of only the contribution 
text and part-of-speech tag provided by the parser. 
Mixed-initiative dialog starts with utterance classifi-
cation and ends with dialog move generation, which can 
include question answering, repeating the question for 
the student, or just encouraging the student.  Concur-
rently, the LSA module evaluates the quality of the stu-
dent contributions, and in the tutor-initiative mode, the 
dialog move generator selects one or a combination of 
specific dialog moves that is both conversationally and 
pedagogically appropriate (Person et al2000; Person et 
al. 2001). The Dialog Advancer Network (DAN) is the 
intermediary of dialog move generation in all instances, 
using information from the speech act classifier and 
LSA to select the next dialog move type and appropriate 
discourse markers.  The dialog move generator selects 
the actual move.  There are twelve types of dialog 
move: Pump, Hint, Splice, Prompt, Prompt Response, 
Elaboration, Summary, and five forms of immediate 
short-feedback (Graesser and Person 1994; Graesser et 
al. 1995; Person and Graesser 1999).  
3 An utterance taxonomy 
The framework for utterance classification in Table 1 is 
familiar to taxonomies in the cognitive sciences 
(Graesser et al 1992; Graesser and Person 1994).  The 
most notable system within this framework is QUALM 
(Lehnert 1978), which utilizes twelve of the question 
categories.  The taxonomy can be divided into 3 distinct 
groups, questions, frozen expressions, and contribu-
tions.  Each of these will be discussed in turn. 
The conceptual basis of the question categories 
arises from the observation that the same question may 
be asked in different ways, e.g. "What happened?" and 
"How did this happen?"  Correspondingly, a single lexi-
cal stem for a question, like "What" can be polysemous, 
e.g. both in a definition category, "What is the definition 
of gravity?" and metacommunicative, "What did you 
say?"  Furthermore, implicit questions can arise in tutor-
ing via directives and some assertions, e.g. "Tell me 
about gravity" and "I don't know what gravity is."  In 
AutoTutor these information seeking utterances are 
classified to one of the 16 question categories. 
The emphases on queried concepts rather than ortho-
graphic forms make the categories listed in Table 1 bear 
a strong resemblance to speech acts.  Indeed, Graesser 
et al (1992) propose that the categories be distinguished 
in precisely the same way as speech acts, using seman-
tic, conceptual, and pragmatic criteria as opposed to 
syntactic and lexical criteria. Speech acts presumably 
transcend these surface criteria: it is not what is being 
said as what is done by the saying (Austin, 1962; Searle, 
1975). 
The close relation to speech acts underscores what a 
difficult task classifying conceptual questions can be. 
Jurafsky and Martin (2000) describe the problem of 
interpreting speech acts using pragmatic and semantic 
inference as AI-complete, i.e. impossible without creat-
ing a full artificial intelligence.  The alternative ex-
plored in this paper is cue or surface-based 
classification, using no context.  
It is particularly pertinent to the present discussion 
that the sixteen qualitative categories are employed in a 
quantitative classification process.  That is to say that 
for the present purposes of classification, a question 
must belong to one and only one category.  On the one 
hand this idealization is necessary to obtain easily ana-
lyzed performance data and to create a well-balanced 
training corpus.  On the other hand, it is not entirely 
accurate because some questions may be assigned to 
multiple categories, suggesting a polythetic coding 
scheme (Graesser et al 1992).  Inter-rater reliability is 
used in the current study as a benchmark to gauge this 
potential effect. 
Frozen expressions consist of metacognitive and 
metacommunicative utterances.  Metacognitive utter-
ances describe the cognitive state of the student, and 
they therefore require a different response than ques-
tions or assertions.  AutoTutor responds to metacogni-
tive utterances with canned expressions such as, "Why 
don't you give me what you know, and we'll take it from 
there."  Metacommunicative acts likewise refer to the 
dialogue between tutor and student, often calling for a 
repetition of the tutor's last utterance.  Two key points 
are worth noting: frozen expressions have a much 
smaller variability than questions or contributions, and 
frozen expressions may be followed by some content, 
making them more properly treated as questions.  For 
example, "I don't understand" is frozen, but "I don't un-
derstand gravity" is a more appropriately a question. 
Contributions in the taxonomy can be viewed as 
anything that is not frozen or a question; in fact, that is 
essentially how the classifier works.  Contributions in 
AutoTutor, either as responses to questions or un-
prompted, are tracked to evaluate student performance 
via LSA, forming the basis for feedback. 
4 Classifier Algorithm 
The present approach ignores the semantic and prag-
matic context of the questions, and utilizes surface fea-
tures to classify questions.  This shallow approach 
parallels work in question answering (Srihari and Li 
2000; Soubbotin and Soubbotin 2002; Moldovan et al
1999). Specifically, the classifier uses tagging provided 
by ApplePie (Sekine and Grishman 1995) followed by 
cascaded finite state transducers defining the categories.  
The finite state transducers are roughly described in 
Table 2. Every transducer is given a chance to match, 
and a disambiguation routine is applied at the end to 
select a single category.  
Category Example 
 
Questions 
Verification  
Disjunctive 
Concept Completion  
Feature Specification  
Quantification 
Definition  
Example 
Comparison  
Interpretation  
Causal Antecedent  
Causal Consequence  
Goal Orientation  
Instrumental/Procedural  
Enablement  
Expectational  
Judgmental 
 
 
 
Does the pumpkin land in his hands? 
Is the pumpkin accelerating or decelerating? 
Where will the pumpkin land? 
What are the components of the forces acting on the pumpkin? 
How far will the pumpkin travel? 
What is acceleration? 
What is an example of Newton's Third Law? 
What is the difference between speed and velocity? 
What is happening in this situation with the runner and pumpkin? 
What caused the pumpkin to fall? 
What happens when the runner speeds up? 
Why did you ignore air resistance? 
How do you calculate force? 
What principle allows you to ignore the vertical component of the force? 
Why doesn't the pumpkin land behind the runner?  
What do you think of my explanation? 
Frozen Expressions  
Metacognitive 
Metacommunicative 
I don't understand. 
Could you repeat that? 
Contribution The pumpkin will land in the runner's hands 
Table 1. AutoTutor?s utterance taxonomy. 
Immediately after tagging, transducers are applied to 
check for frozen expressions.  A frozen expression must 
match, and the utterance must be free of any nouns, i.e. 
not frozen+content, for the utterance to be classified as 
frozen.  Next the utterance is checked for question 
stems, e.g. WHAT, HOW, WHY, etc. and question 
mark punctuation.  If question stems are buried in the 
utterance, e.g. "I don't know what gravity is", a move-
ment rule transforms the utterance, placing the stem at 
the beginning.  Likewise if a question ends with a ques-
tion mark but has no stem, an AUX stem is placed at the 
beginning of the utterance.  In this way the same trans-
ducers can be applied to both direct and indirect ques-
tions.  At this stage, if the utterance does not possess a 
question stem and is not followed by a question mark, 
the utterance is classified as a contribution. 
Two sets of finite state transducers are applied to po-
tential questions, keyword transducers and syntactic 
pattern transducers.  Keyword transducers replace a set 
of keywords specific to a category with a symbol for 
that category.  This extra step simplifies the syntactic 
pattern transducers that look for the category symbol in 
their pattern.  The definition keyword transducer, for 
example, replaces "definition", "define", "meaning", 
"means", and "understanding" with "KEYDEF".  For 
most categories, the keyword list is quite extensive and 
exceeds the space limitations of Table 2.  Keyword 
transducers also add the category symbol to a list when 
they match; this list is used for disambiguation.  Syntac-
tic pattern transducers likewise match, putting a cate-
gory symbol on a separate disambiguation list. 
In the disambiguation routine, both lists are con-
sulted, and the first category symbol found on both lists 
determines the classification of the utterance.  Clearly 
Utterance Category Finite state transducer pattern 
Verification ^AUX 
Disjunctive ^AUX ... or 
Concept Completion ^(Who|What|When|Where) 
Feature Specification ^What ... keyword 
keyword 
Quantification ^What AUX ... keyword 
^How (ADJ|ADV) 
^MODAL you ... keyword 
Definition ^What AUX ... (keyword|a? (ADJ|ADV)* N 
^MODAL you ... keyword 
what a? (ADJ|ADV)* N BE 
Example ^AUX ... keyword 
^What AUX ... keyword 
Comparison ^What AUX ... keyword 
^How ... keyword 
^MODAL you ... keyword 
Interpretation keyword 
Causal Antecedent ^(Why|How) AUX ... (VBpast|keyword) 
^(WH|How) ... keyword 
Causal Consequence  
Goal Orientation ^(What|Why) AUX ART? (NP|SUBJPRO|keyword) 
^What ... keyword 
Instrumental/Procedural ^(WH|How) AUX ART? (N|PRO) 
^(WH|How) ... keyword 
^MODAL you ... keyword 
Enablement ^(WH|How) ... keyword 
Expectational ^Why AUX ... NEG 
Judgmental 
(you|your) ... keyword 
(should|keyword) (N|PRO) 
Frozen (no nouns) ^SUBJPRO ... keyword 
^VB ... keyword ... OBJPRO 
^AUX ... SUBJPRO ... keyword 
Contribution Everything else 
Table 2.  Finite state transducer patterns 
ordering of transducers affects which symbols are clos-
est to the beginning of the list.  Ordering is particularly 
relevant when considering categories like concept com-
pletion, which match more freely than other categories.  
Ordering gives rarer and stricter categories a chance to 
match first; this strategy is common in stemming (Paice 
1990).  
5 Training 
The classifier was built by hand in a cyclical process of 
inspecting questions, inducing rules, and testing the 
results.  The training data was derived from brainstorm-
ing sessions whose goal was to generate questions as 
lexically and syntactically distinct as possible.  Of the 
brainstormed questions, only when all five raters agreed 
on the category was a question used for training; this 
approach filtered out polythetic questions and left only 
archetypes. 
Intuitive analysis suggested that the majority of 
questions have at most a two-part pattern consisting of a 
syntactic template and/or a keyword identifiable for that 
category.  A trivial example is disjunction, whose syn-
tactic template is auxiliary-initial and corresponding 
keyword is ?or?.  Other categories were similarly de-
fined either by one or more patterns of initial constitu-
ents, or a keyword, or both.  To promote 
generalizability, extra care was given not to overfit the 
training data.  Specifically, keywords or syntactic pat-
terns were only used to define categories when they 
occurred more than once or were judged highly diagnos-
tic. 
 
 Expert 
Classifier present  ?present 
present tp fp 
?present fn tn 
Table 3.  Contingency Table. 
 
The results of the training process are shown in Ta-
ble 4.  Results from each category were compiled in 2 x 
2 contingency tables like Table 3, where tp stands for 
"true positive" and fn for "false negative". 
Recall, fallout, precision, and f-measure were calcu-
lated in the following way for each category: 
 
Recall  =  tp / ( tp + fn ) 
Fallout  =  fp / ( fp + tn ) 
Precision  =  tp / ( tp + fp ) 
 
F-measure = 2 * Recall * Precision  
 Recall + Precision 
 
Recall and fallout are often used in signal detection 
analysis to calculate a measure called d? (Green and 
Swets 1966).  Under this analysis, the performance of 
the classifier is significantly more favorable than under 
the F-measure, principally because the fallout, or false 
alarm rate, is so low.  Both in training and evaluation, 
however, the data violate assumptions of normality that 
d? requires.    
As explained in Section 3, a contribution classifica-
tion is the default when no other classification can be 
given.  As such, no training data was created for contri-
butions.  Likewise frozen expressions were judged to be 
essentially a closed class of phrases and do not require 
training.  Absence of training results for these categories 
is represented by double stars in Table 4. 
During the training process, the classifier was never 
tested on unseen data.  A number of factors it difficult to 
obtain questions suitable for testing purposes.  Brain-
stormed questions are an unreliable source of testing 
data because they are not randomly sampled.  In gen-
eral, corpora proved to be an unsatisfactory source of 
questions due to low inter-rater reliability and skewed 
distribution of categories.   
Low inter-rater reliability often could be traced to 
anaphora and pragmatic context.  For example, the 
question "Do you know what the concept of group cell 
is?" might license a definition or verification, depending 
on the common ground.  "Do you know what it is?" 
could equally license a number of categories, depending 
on the referent of "it".  Such questions are clearly be-
yond the scope of a classifier that does not use context.  
The skewed distribution of the question categories 
and their infrequency necessitates use of an extraction 
algorithm to locate them.  Simply looking for question 
marks is not enough: our estimates predict that raters 
would need to classify more than 5,000 questions ex-
tracted from the Wall Street Journal this way to get a 
mere 20 instances of the rarest types.  A bootstrapping 
approach using machine learning is a possible alterna-
tive that will be explored in the future (Abney 2002). 
Regardless of these difficulties, the strongest evalua-
tion results from using the classifier in a real world task, 
with real world data. 
6 Evaluation 
The classifier was used in AutoTutor sessions through-
out the year of 2002.  The log files from these sessions 
contained 9094 student utterances, each of which was 
classified by an expert.  The expert ratings were com-
pared to the classifier's ratings, forming a 2 x 2 contin-
gency table for each category as in Table 4.   
To expedite ratings, utterances extracted from the 
log files were split into two groups, contributions and 
non-contributions, according to their logged classifica-
tion.  Expert judges were assigned to a group and in-
structed to classify a set of utterances to one of the 18 
categories.  Though inter-rater reliability using the 
kappa statistic (Carletta 1996) may be calculated for 
each group, the distribution of categories in the contri-
bution group was highly skewed and warrants further 
discussion.   
Skewed categories bias the kappa statistic to low 
values even when the proportion of rater agreement is 
very high (Feinstein and Cicchetti 1990a; Feinstein and 
Cicchetti 1990b). In the contribution group, judges can 
expect to see mostly one category, contribution, 
whereas judges in the non-contribution group can ex-
pect to see the other 17 categories.  Expected agreement 
by chance for the contribution group was 98%.  Corre-
spondingly, inter-rater reliability using the kappa statis-
tic was low for the contribution group, .5 despite 99% 
proportion agreement, and high for non-contribution 
group, .93.   
However, the .93 inter-rater agreement can be ex-
tended to all of the utterance categories.  Due to classi-
fier error, the non-contribution group consisted of 38% 
contributions.  Thus the .93 agreement applies to contri-
butions in this group.  Equal proportion of agreement 
for contribution classifications in both groups, 99%, 
suggests that the differences in kappa solely reflect dif-
ferences in category skew across groups.  Under this 
analysis, dividing the utterances into two groups im-
proved the distribution of categories for the calculation 
of kappa (Feinstein and Cicchetti  1990b). 
Expert judges classified questions with a .93 kappa, 
which supports a monothetic classification scheme for 
this application.  In Section 3 the possibility was raised 
of a polythetic scheme for question classification, i.e. 
one in which two categories could be assigned to a 
given question.  If a polythetic scheme were truly neces-
sary, one would expect inter-rater reliability to suffer in 
a monothetic classification task.  High inter-rater reli-
ability on the monothetic classification task renders 
polythetic schemes superfluous for this application. 
The recall column for evaluation in Table 4 is gener-
ally much higher than corresponding cells in the preci-
sion column.  The disparity implies a high rate of false 
positives for each of the categories.  One possible ex-
planation is the reconstruction algorithm applied during 
classification.  It was observed that, particularly in the 
language of physics, student used question stems in ut-
terances that were not questions, e.g. ?The ball will land 
when ??  Such falsely reconstructed questions account 
for 40% of the questions detected by the classifier.  
Whether modifying the reconstruction algorithm would 
improve F-measure, i.e. improve precision without sac-
rificing recall, is a question for future research. 
The distribution of categories is highly skewed: 97% 
of the utterances were contributions, and example ques-
tions never occurred at all.  In addition to recall, fallout, 
precision, and F-measure, significance tests were calcu-
 Training Data AutoTutor Performance 
CATEGORY Recall Fallout Precision F-measure Recall Fallout Precision F-measure Likelihood Ratio 
Contribution ** ** ** ** 0.983 0.054 0.999 0.991 1508.260 
Frozen ** ** ** ** 0.899 0.002 0.849 0.873 978.810 
Concept  
Completion 0.844 0.035 0.761 0.800 0.857 0.003 0.444 0.585 235.800 
Interpretation 0.545 0.009 0.545 0.545 0.550 0.000 0.917 0.688 135.360 
Definition 0.667 0.002 0.941 0.780 0.424 0.001 0.583 0.491 131.770 
Verification 0.969 0.004 0.969 0.969 0.520 0.004 0.255 0.342 103.880 
Comparison 0.955 0.011 0.778 0.857 1.000 0.004 0.132 0.233 55.460 
Quantification 0.949 0.002 0.982 0.966 0.556 0.003 0.139 0.222 43.710 
Expecational 0.833 0.010 0.833 0.833 1.000 0.000 0.667 0.800 33.870 
Procedural 0.545 0.009 0.545 0.545 1.000 0.000 1.000 1.000 20.230 
Goal  
Orientation 0.926 0.006 0.893 0.909 1.000 0.001 0.143 0.250 14.490 
Judgmental 0.842 0.010 0.865 0.853 0.500 0.001 0.167 0.250 12.050 
Disjunction 0.926 0.000 1.000 0.962 0.333 0.000 0.250 0.286 11.910 
Causal  
Antecedent 0.667 0.017 0.667 0.667 0.200 0.001 0.083 0.118 8.350* 
Feature  
Specification 0.824 0.006 0.824 0.824 0.000 0.000 0.000 0.000 0.000* 
Enablement 0.875 0.006 0.903 0.889 0.000 0.000 0.000 0.000 0.000* 
Causal  
Consequent 0.811 0.008 0.882 0.845 0.000 0.000 0.000 0.000 0.000* 
Example 0.950 0.008 0.826 0.884 ** ** ** ** ** 
Table 4. Training data and AutoTutor results. 
lated for each category's contingency table to insure that 
the cells were statistically significant.  Since most of the 
categories had at least one cell with an expected value 
of less than 1, Fisher's exact test is more appropriate for 
significance testing than likelihood ratios or chi-square 
(Pedersen 1996).  Those categories that are not signifi-
cant are starred; all other categories are significant, p < 
.001. 
Though not appropriate for hypothesis testing in this 
instance, likelihood ratios provide a comparison of clas-
sifier performance across categories.  Likelihood ratios 
are particularly useful when comparing common and 
rare events (Dunning 1993; Plaunt and Norgard 1998), 
making them natural here given the rareness of most 
question categories and the frequency of contributions.  
The likelihood ratios in the rightmost column of Table 4 
are on a natural logarithmic scale, -2ln?, so procedural 
at e . 5 x 20.23 = 24711 is more likely than goal orientation, 
at e . 5 x 14.49 = 1401, with respect to the base rate, or null 
hypothesis. 
To judge overall performance on the AutoTutor ses-
sions, an average weighted F-measure may be calcu-
lated by summing the products of all category F-
measures with their frequencies: 
 
? +??= N fntpmeasureFFavg  
 
The average weighted F-measure reflects real world 
performance since accuracy on frequently occurring 
classes is weighted more.  The average weighted F-
measure for the evaluation data is .98, mostly due to the 
great frequency of contributions (.97 of all utterances) 
and the high associated F-measure.  Without weighting, 
the average F-measure for the significant cells is .54. 
With respect to the three applications mentioned, i) 
tracking student understanding, ii) mixed-initiative dia-
logue, and iii) questions answering, the classifier is do-
ing extremely well on the first two and adequately on 
the last.  The first two applications for the most part 
require distinguishing questions from contributions, 
which the classifier does extremely well, F-measure = 
.99.  Question answering, on the other hand, can benefit 
from more precise identification of the question type, 
and the average unweighted F-measure for the signifi-
cant questions is .48. 
7 Conclusion 
One of the objectives of this work was to see how well a 
classifier could perform with a minimum of resources.  
Using no context and only surface features, the classi-
fier performed with an average weighted F-measure of 
.98 on real world data. 
However, the question remains how performance 
will fare as rare questions become more frequent.  Scaf-
folding student questions has become a hot topic re-
cently (Graesser et al 2003).  In a system that greatly 
promotes question-asking, the weighted average of .97 
will tend to drift closer to the unweighted average of 
.54.  Thus there is clearly more work to be done. 
Future directions include using bootstrapping meth-
ods and statistical techniques on tutoring corpora and 
using context to disambiguate question classification. 
8 Acknowledgements 
This research was supported by the Office of Naval Re-
search (N00014-00-1-0600) and the National Science 
Foundation (SBR 9720314 and REC 0106965).  Any 
opinions, findings, and conclusions or recommendations 
expressed in this material are those of the authors and 
do not necessarily reflect the views of ONR or NSF. 
References 
 
Abney, Steven. 2002. Bootstrapping. In Proceedings of 
the 40th Annual Meeting of the Association for Com-
putational Linguistics, 360-367. 
Allen, J.F. 1999. Mixed Initiative Interaction.  Proc. 
IEEE Intelligent Systems 14(6).  
Austin, John. 1962. How to do things with words. Har-
vard University Press, Cambridge, MA. 
Brill, Eric. 1995.  Transformation-based error-driven 
learning and natural language processing: a case study 
in part-of-speech tagging.  Computational Linguistics, 
21(4), 543-566. 
Brill, Eric, J. Lin, M. Banko, S. Dumais, and  A. Ng. 
2001.  Data-intensive question answering.  Proceed-
ings of the 10th Annual Text Retrieval Conference 
(TREC-10).  
Carletta, J.  1996.  Assessing agreement on classification 
tasks: the kappa statistic.  Computational Linguistics,  
22(2), 249-254. 
Dunning, Ted.  1993.  Accurate methods for the statistics 
of surprise and coincidence.  Computational Linguis-
tics 19, 61-74.  
Feinstein, Alvan R. and Domenic V. Cicchetti.  1990a.  
High agreement but low kappa: the problems of two 
paradoxes.  Journal of Clinical Epidemiology, 43(6), 
543-549. 
Feinstein, Alvan R. and Domenic V. Cicchetti.  1990b.  
High agreement but low kappa: II. resolving the para-
doxes.  Journal of Clinical Epidemiology, 43(6), 551-
558. 
Graesser, Arthur, John Burger, Jack Carroll, Albert Cor-
bett, Lisa Ferro, Douglas Gordon, Warren Greiff, 
Sanda Harabagiu, Kay Howell, Henry Kelly, Diane 
Litman, Max Louwerse, Allison Moore, Adrian Pell, 
John Prange, Ellen Voorhees, and Wayne Ward.  
2003.  Question generation and answering systems, 
R&D for technology-enabled learning systems: re-
search roadmap.  Unpublished manuscript.   
Graesser, Arthur, Natalie Person, and John Huber. 1992. 
Mechanisms that generate questions. In T. Lauer, E. 
Peacock, and A. Graesser (Eds), Questions and infor-
mation systems. Earlbaum, Hillsdale, NJ. 
Graesser, Arthur and Natalie Person. 1994. Question ask-
ing during tutoring.  American Educational Research 
Journal, 31(1), 104-137. 
Graesser, Arthur, Natalie Person, and J.P. Magliano.  
1995.  Collaborative dialog patterns in naturalistic 
one-on-one tutoring.  Applied Cognitive Psychology, 
9, 359-387.   
Graesser, Arthur, Kurt van Lehn, Carolyn Rose, Pamela 
Jordan, and Derek Harter.  2001.  Intelligent tutoring 
systems with conversational dialogue.  AI Magazine 
22(4), 39-52. 
Graesser, Arthur, Peter Wiemer-Hastings, K. Wiemer-
Hastings, Roger Kreuz, and the TRG.  1999. AutoTu-
tor: A simulation of a human tutor.  Journal of Cogni-
tive Systems Research 1, 35-51. 
Green, David and John Swets.  1966.  Signal detection 
theory and psychophysics.  John Wiley, New York. 
Harabagiu, Sanda, D. Moldovan, M. Pasca, R. Mihalcea, 
M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and P. 
Morarescu. 2000. FALCON: Boosting knowledge for 
answer engines.  In Proceedings of the 9th Text Re-
trieval Conference (TREC-9). 
Jurafsky, Daniel and James Martin. 2000. Speech and 
language processing. Prentice Hall, NJ. 
Lehnert, Wendy. 1978. The Process of Question Answer-
ing. Lawrence Erlbaum Associates, Hillsdale, NJ. 
McArthur, D., C. Stasz, and M. Zmuidzinas. 1990. Tu-
toring techniques in algebra. Cognition and Instruc-
tion, 7, 197-244. 
Moldovan, Dan, Sanda Harabagiu, Marius Pasca, Rada 
Mihalcea, Richard Goodrum, Roxana Girju, and 
Vaslie Rus.  1999.  Lasso: a tool for surfing the an-
swer net  Proceedings of the 8th Annual Text Retrieval 
Conference (TREC-8), 65-73. 
Olney, Andrew, Natalie Person, Max Louwerse, and Ar-
thur Graesser.  2002. AutoTutor: a conversational tu-
toring environment.  In Proceedings of the 40th 
Annual Meeting of the Association for Computational 
Linguistics, Demonstration Abstracts, 108-109. 
Otero, J. and Arthur  Graesser.  2001.  PREG: Elements 
of a model of question asking.  Cognition & Instruc-
tion 19, 143-175. 
Paice, C.D. 1990. Another stemmer. SIGIR Forum 24 (3), 
56-61. 
Pedersen, Ted.  1996.  Fishing for exactness.  In Proceed-
ings of the South-Central SAS Users Group Confer-
ence, Austin, TX. 
Person, Natalie and Arthur Graesser.  1999.  Evolution 
of discourse in cross-age tutoring.  In A.M. 
O?Donnell and A. King (Eds.), Cognitive perspec-
tives on peer learning (pp. 69-86). Erlbaum, Mah-
wah, NJ. 
Person, Natalie, Arthur Graesser, L. Bautista, E.C. 
Mathews, and the Tutoring Research Group 2001.  
Evaluating student learning gains in two versions of 
AutoTutor. In J. D. Moore, C. L. Redfield, and W. L. 
Johnson (Eds.) Artificial intelligence in education: 
AI-ED in the wired and wireless future (pp. 286-
293). IOS Press, Amsterdam. 
Person, Natalie, Arthur Graesser, Derek Harter, E. C. 
Mathews, and the Tutoring Research Group (2000). 
Dialog move generation and conversation manage-
ment in AutoTutor.  Proceedings for the AAAI Fall 
Symposium  Series: Building Dialogue Systems for 
Tutorial Applications. Falmouth, Massachusetts. 
Plaunt, Christian and Barbara Norgard.  1998. An asso-
ciation-based method for automatic indexing with a 
controlled vocabulary.  Journal of the American Soci-
ety of Information Science, 49(10), 888-902. 
Putnam, R. T. 1987. Structuring and adjusting content 
for students: A study of  live and simulated tutoring 
of addition. American Educational Research Jour-
nal, 24, 13-48. 
Searle, John. 1975. A taxonomy of illocutionary acts. In 
K. Gunderson, (Ed.), Language, mind, and knowl-
edge. University of Minnesota Press, Minneapolis, 
MN. 
Sekine, S. and R. Grishman.  1995.  A corpus-based 
probabilistic grammar with only two nonterminals.  
Fourth International Workshop on Parsing Technol-
ogy. 
Soubbotin, M. M., and S. M. Soubbotin. 2002.  Patterns 
of potential answer expressions as clues to the right 
answers.  Proceedings of the 10th Annual Text Re-
trieval Conference (TREC-10).  
Srihari, Rohini and Wei  Li. 2000. A question answering 
system supported by information extraction.  Pro-
ceedings of the 6th Applied Natural Language Proc-
essing Conference (ANLP-2000), 166-172. 
Van Dijk, T. A., and W. Kintsch. 1983. Strategies of dis-
course comprehension. New York: Academic. 
Voorhees, Ellen.  2001.  Overview of the TREC 2001 
question answering track.  Proceedings of the 10th 
Annual Text Retrieval Conference (TREC-10), 400-
410. 
TextGraphs-2: Graph-Based Algorithms for Natural Language Processing, pages 45?52,
Rochester, April 2007 c?2007 Association for Computational Linguistics
Latent Semantic Grammar Induction:
Context, Projectivity, and Prior Distributions
Andrew M. Olney
Institute for Intelligent Systems
University of Memphis
Memphis, TN 38152
aolney@memphis.edu
Abstract
This paper presents latent semantic gram-
mars for the unsupervised induction of
English grammar. Latent semantic gram-
mars were induced by applying singu-
lar value decomposition to n-gram by
context-feature matrices. Parsing was
used to evaluate performance. Exper-
iments with context, projectivity, and
prior distributions show the relative per-
formance effects of these kinds of prior
knowledge. Results show that prior dis-
tributions, projectivity, and part of speech
information are not necessary to beat the
right branching baseline.
1 Introduction
Unsupervised grammar induction (UGI) generates a
grammar from raw text. It is an interesting problem
both theoretically and practically. Theoretically, it
connects to the linguistics debate on innate knowl-
edge (Chomsky, 1957). Practically, it has the po-
tential to supersede techniques requiring structured
text, like treebanks. Finding structure in text with
little or no prior knowledge is therefore a fundamen-
tal issue in the study of language.
However, UGI is still a largely unsolved problem.
Recent work (Klein and Manning, 2002; Klein and
Manning, 2004) has renewed interest by using a UGI
model to parse sentences from the Wall Street Jour-
nal section of the Penn Treebank (WSJ). These pars-
ing results are exciting because they demonstrate
real-world applicability to English UGI. While other
contemporary research in this area is promising, the
case for real-world English UGI has not been as
convincingly made (van Zaanen, 2000; Solan et al,
2005).
This paper weaves together two threads of in-
quiry. The first thread is latent semantics, which
have not been previously used in UGI. The second
thread is dependency-based UGI, used by Klein and
Manning (2004), which nicely dovetails with our se-
mantic approach. The combination of these threads
allows some exploration of what characteristics are
sufficient for UGI and what characteristics are nec-
essary.
2 Latent semantics
Previous work has focused on syntax to the exclu-
sion of semantics (Brill and Marcus, 1992; van Zaa-
nen, 2000; Klein and Manning, 2002; Paskin, 2001;
Klein and Manning, 2004; Solan et al, 2005). How-
ever, results from the speech recognition commu-
nity show that the inclusion of latent semantic infor-
mation can enhance the performance of their mod-
els (Coccaro and Jurafsky, 1998; Bellegarda, 2000;
Deng and Khudanpur, 2003). Using latent semantic
information to improve UGI is therefore both novel
and relevant.
The latent semantic information used by the
speech recognition community above is produced
by latent semantic analysis (LSA), also known as
latent semantic indexing (Deerwester et al, 1990;
Landauer et al, 1998). LSA creates a semantic rep-
resentation of both words and collections of words
in a vector space, using a two part process. First,
45
a term by document matrix is created in which the
frequency of word wi in document dj is the value
of cell cij . Filters may be applied during this pro-
cess which eliminate undesired terms, e.g. common
words. Weighting may also be applied to decrease
the contributions of frequent words (Dumais, 1991).
Secondly, singular value decomposition (SVD) is
applied to the term by document matrix. The re-
sulting matrix decomposition has the property that
the removal of higher-order dimensions creates an
optimal reduced representation of the original ma-
trix in the least squares sense (Berry et al, 1995).
Therefore, SVD performs a kind of dimensionality
reduction such that words appearing in different doc-
uments can acquire similar row vector representa-
tions (Landauer and Dumais, 1997). Words can be
compared by taking the cosine of their correspond-
ing row vectors. Collections of words can likewise
be compared by first adding the corresponding row
vectors in each collection, then taking the cosine be-
tween the two collection vectors.
A stumbling block to incorporating LSA into UGI
is that grammars are inherently ordered but LSA is
not. LSA is unordered because the sum of vectors is
the same regardless of the order in which they were
added. The incorporation of word order into LSA
has never been successfully carried out before, al-
though there have been attempts to apply word or-
der post-hoc to LSA (Wiemer-Hastings and Zipitria,
2001). A straightforward notion of incorporating
word order into LSA is to use n-grams instead of in-
dividual words. In this way a unigram, bigram, and
trigram would each have an atomic vector represen-
tation and be directly comparable.
It may seem counterintuitive that such an n-gram
scheme has never been used in conjunction with
LSA. Simple as this scheme may be, it quickly falls
prey to memory limitations of modern day comput-
ers for computing the SVD. The standard for com-
puting the SVD in the NLP sphere is Berry (1992)?s
SVDPACK, whose single vector Lanczos recursion
method with re-orthogonalization was incorporated
into the BellCore LSI tools. Subsequently, either
SVDPACK or the LSI tools were used by the ma-
jority of researchers in this area (Schu?tze, 1995;
Landauer and Dumais, 1997; Landauer et al, 1998;
Coccaro and Jurafsky, 1998; Foltz et al, 1998; Bel-
legarda, 2000; Deng and Khudanpur, 2003). Using
John likes string cheese.
Figure 1: A Dependency Graph
the equation reported in Larsen (1998), a standard
orthogonal SVD of a unigram/bigram by sentence
matrix of the LSA Touchstone Applied Science As-
sociates Corpus (Landauer et al, 1998) requires over
60 gigabytes of random access memory. This esti-
mate is prohibitive for all but current supercomput-
ers.
However, it is possible to use a non-orthogonal
SVD approach with significant memory savings
(Cullum and Willoughby, 2002). A non-orthogonal
approach creates the same matrix decomposition as
traditional approaches, but the resulting memory
savings allow dramatically larger matrix decompo-
sitions. Thus a non-orthongonal SVD approach is
key to the inclusion of ordered latent semantics into
our UGI model.
3 Dependency grammars
Dependency structures are an ideal grammar repre-
sentation for evaluating UGI. Because dependency
structures have no higher order nodes, e.g. NP, their
evaluation is simple: one may compare with a ref-
erence parse and count the proportion of correct de-
pendencies. For example, Figure 1 has three depen-
dencies {( John, likes ), ( cheese, likes ), ( string,
cheese ) }, so the trial parse {( John, likes ), ( string,
likes ), ( cheese, string )} has 1/3 directed dependen-
cies correct and 2/3 undirected dependencies cor-
rect. This metric avoids the biases created by brack-
eting, where over-generation or undergeneration of
brackets may cloud actual performance (Carroll et
al., 2003). Dependencies are equivalent with lexical-
ized trees (see Figures 1 and 2) so long as the depen-
dencies are projective. Dependencies are projective
when all heads and their dependents are a contigu-
ous sequence.
Dependencies have been used for UGI before with
mixed success (Paskin, 2001; Klein and Manning,
2004). Paskin (2001) created a projective model us-
ing words, and he evaluated on WSJ. Although he
reported beating the random baseline for that task,
both Klein and Manning (2004) and we have repli-
46
Slikes
NPJohn VPlikes
John likes NPcheese
string cheese
Figure 2: A Lexicalized Tree
cated the random baseline above Paskin?s results.
Klein and Manning (2004), on the other hand, have
handily beaten a random baseline using a projective
model over part of speech tags and evaluating on a
subset of WSJ, WSJ10.
4 Unanswered questions
There are several unanswered questions in
dependency-based English UGI. Some of these
may be motivated from the Klein and Manning
(2004) model, while others may be motivated
from research efforts outside the UGI community.
Altogether, these questions address what kinds
of prior knowledge are, or are not necessary for
successful UGI.
4.1 Parts of speech
Klein and Manning (2004) used part of speech tags
as basic elements instead of words. Although this
move can be motivated on data sparsity grounds, it
is somewhat at odds with the lexicalized nature of
dependency grammars. Since Paskin (2001)?s previ-
ous attempt using words as basic elements was un-
successful, it is not clear whether parts of speech are
necessary prior knowledge in this context.
4.2 Projectivity
Projectivity is an additional constraint that may not
be necessary for successful UGI. English is a projec-
tive language, but other languages, such as Bulgar-
ian, are not (Pericliev and Ilarionov, 1986). Nonpro-
jective UGI has not previously been studied, and it
is not clear how important projectivity assumptions
are to English UGI. Figure 3 gives an example of a
nonprojective construction: not all heads and their
dependents are a contiguous sequence.
John string likes cheese.
Figure 3: A Nonprojective Dependency Graph
1 2 3 4 5 6 7 8 9 10
0
0.5
1
1.5
2
2.5
3
x 104
Words Distant
N
um
be
r o
f D
ep
en
de
nc
ie
s
Figure 4: Distance Between Dependents in WSJ10
4.3 Context
The core of several UGI approaches is distributional
analysis (Brill and Marcus, 1992; van Zaanen, 2000;
Klein and Manning, 2002; Paskin, 2001; Klein and
Manning, 2004; Solan et al, 2005). The key idea in
such distributional analysis is that the function of a
word may be known if it can be substituted for an-
other word (Harris, 1954). If so, both words have the
same function. Substitutability must be defined over
a context. In UGI, this context has typically been the
preceding and following words of the target word.
However, this notion of context has an implicit as-
sumption of word order. This assumption is true for
English, but is not true for other languages such as
Latin. Therefore, it is not clear how dependent En-
glish UGI is on local linear context, e.g. preceding
and following words, or whether an unordered no-
tion of context would also be effective.
4.4 Prior distributions
Klein and Manning (2004) point their model in the
right direction by initializing the probability of de-
pendencies inversely proportional to the distance be-
tween the head and the dependent. This is a very
good initialization: Figure 4 shows the actual dis-
tances for the dataset used, WSJ10.
47
Klein (2005) states that, ?It should be emphasized
that this initialization was important in getting rea-
sonable patterns out of this model.? (p. 89). How-
ever, it is not clear that this is necessarily true for all
UGI models.
4.5 Semantics
Semantics have not been included in previous UGI
models, despite successful application in the speech
recognition community (see Section 2). However,
there have been some related efforts in unsupervised
part of speech induction (Schu?tze, 1995). These ef-
forts have used SVD as a dimensionality reduction
step between distributional analysis and clustering.
Although not labelled as ?semantic? this work has
produced the best unsupervised part of speech in-
duction results. Thus our last question is whether
SVD can be applied to a UGI model to improve re-
sults.
5 Method
5.1 Materials
The WSJ10 dataset was used for evaluation to be
comparable to previous results (Klein and Manning,
2004). WSJ10 is a subset of the Wall Street Jour-
nal section of the Penn Treebank, containing only
those sentences of 10 words or less after punctuation
has been removed. WSJ10 contains 7422 sentences.
To counteract the data sparsity encountered by using
ngrams instead of parts of speech, we used the en-
tire WSJ and year 1994 of the North American News
Text Corpus. These corpora were formatted accord-
ing to the same rules as the WSJ10, split into sen-
tences (as documents) and concatenated. The com-
bined corpus contained roughly 10 million words
and 460,000 sentences.
Dependencies, rather than the original bracketing,
were used as the gold standard for parsing perfor-
mance. Since the Penn Treebank does not label de-
pendencies, it was necessary to apply rules to extract
dependencies from WSJ10 (Collins, 1999).
5.2 Procedure
The first step is unsupervised latent semantic gram-
mar induction. This was accomplished by first cre-
ating n-gram by context feature matrices, where the
feature varies as per Section 4.3. The Contextglobal
approach uses a bigram by document matrix such
that word order is eliminated. Therefore the value
of cellij is the number of times ngrami occurred
in documentj . The matrix had approximate dimen-
sions 2.2 million by 460,000.
The Contextlocal approach uses a bigram by local
window matrix. If there are n distinct unigrams in
the corpus, the first n columns contain the counts
of the words preceding a target word, and the last n
columns contain the counts of the words following
a target word. For example, the value of at cellij
is the number of times unigramj occurred before
the target ngrami. The value of celli(j+n) is the
number of times unigramj occurred after the target
ngrami. The matrix had approximate dimensions
2.2 million by 280,000.
After the matrices were constructed, each
was transformed using SVD. Because the non-
orthogonal SVD procedure requires a number of
Lanczos steps approximately proportional to the
square of the number of dimensions desired, the
number of dimensions was limited to 100. This kept
running time and storage requirements within rea-
sonable limits, approximately 4 days and 120 giga-
bytes of disk storage to create each.
Next, a parsing table was constructed. For each
bigram, the closest unigram neighbor, in terms of
cosine, was found, cf. Brill and Marcus (1992). The
neighbor, cosine to that neighbor, and cosines of the
bigram?s constituents to that neighbor were stored.
The constituent with the highest cosine to the neigh-
bor was considered the likely head, based on clas-
sic head test arguments (Hudson, 1987). This data
was stored in a lookup table so that for each bigram
the associated information may be found in constant
time.
Next, the WSJ10 was parsed using the parsing
table described above and a minimum spanning
tree algorithm for dependency parsing (McDonald
et al, 2005). Each input sentence was tokenized
on whitespace and lowercased. Moving from left
to right, each word was paired with all remaining
words on its right. If a pair existed in the pars-
ing table, the associated information was retrieved.
This information was used to populate the fully con-
nected graph that served as input to the minimum
spanning tree algorithm. Specifically, when a pair
was retrieved from the parsing table, the arc from
48
the stored head to the dependent was given a weight
equal to the cosine between the head and the near-
est unigram neighbor for that bigram pair. Likewise
the arc from the dependent to the head was given a
weight equal to the cosine between the dependent
and the nearest unigram neighbor for that bigram
pair. Thus the weight on each arc was based on the
degree of substitutability between that word and the
nearest unigram neighbor for the bigram pair.
If a bigram was not in the parsing table, it was
given maximum weight, making that dependency
maximally unlikely. After all the words in the sen-
tence had been processed, the average of all current
weights was found, and this average was used as the
weight from a dummy root node to all other nodes
(the dummy ROOT is further motivated in Section
5.3). Therefore all words were given equal likeli-
hood of being the root of the sentence. The end
result of this graph construction process is an n by
n + 1 matrix, where n is the number of words and
there is one dummy root node. Then this graph was
input to the minimum spanning tree algorithm. The
output of this algorithm is a non-projective depen-
dency tree, which was directly compared to the gold
standard dependency tree, as well as the respective
baselines discussed in Section 5.3.
To gauge the differential effects of projectivity
and prior knowledge, the above procedure was mod-
ified in additional evaluation trials. Projectivity was
incorporated by using a bottom-up algorithm (Cov-
ington, 2001). The algorithm was applied in two
stages. First, it was applied using the nonprojective
parse as input. By comparing the output parse to the
original nonprojective parse, it is possible to identify
independent words that could not be incorporated
into the projective parse. In the second stage, the
projective algorithm was run again on the nonpro-
jective input, except this time the independent words
were allowed to link to any other words defined by
the parsing table. In other words, the first stage iden-
tifies unattached words, and the second stage ?re-
pairs? the words by finding a projective attachment
for them. This method of enforcing projectivity was
chosen because it makes use of the same informa-
tion as the nonprojective method, but it goes a step
further to enforce projectivity.
Prior distributions of dependencies, as depicted in
Figure 4, were incorporated by inversely weighting
ROOT John likes string cheese
Figure 5: Right Branching Baseline
John likes string cheese ROOT
Figure 6: Left Branching Baseline
graph edges by the distance between words. This
modification transparently applies to both the non-
projective case and the projective case.
5.3 Scoring
Two performance baselines for dependency parsing
were used in this experiment, the so-called right and
left branching baselines. A right branching baseline
predicts that the head of each word is the word to the
left, forming a chain from left to right. An example
is given in Figure 5. Conversely, a left branching
baseline predicts that the head of each word is the
word to the right, forming a chain from right to left.
An example is given in Figure 6. Although perhaps
not intuitively very powerful baselines, the right and
left branching baselines can be very effective for the
WSJ10. For WSJ10, most heads are close to their
dependents, as shown in Figure 4. For example, the
percentage of dependencies with a head either im-
mediately to the right or left is 53%. Of these neigh-
boring heads, 17% are right branching, and 36% are
left branching.
By using the sign test, the statistical significance
of parsing results can be determined. The sign test is
perhaps the most basic non-parametric tests and so is
useful for this task because it makes no assumptions
regarding the underlying distribution of data.
Consider each sentence. Every word must have
exactly one head. That means that for n words, there
is a 1/n chance of selecting the correct head (exclud-
ing self-heads and including a dummy root head). If
all dependencies in a sentence are independent, then
a sentence?s dependencies follow a binomial distri-
bution, with n equal to the number of words, p equal
to 1/n, and k equal to the number of correct depen-
dencies. From this it follows that the expected num-
ber of correct dependencies per sentence is np, or 1.
Thus the random baseline for nonprojective depen-
49
dency parsing performance is one dependency per
sentence.
Using the gold standard of the WSJ10, the number
of correct dependencies found by the latent seman-
tic model can be established. The null hypothesis
is that one randomly generated dependency should
be correct per sentence. Suppose that r+ sentences
have more correct dependencies and r? sentences
have fewer correct dependencies (i.e. 0). Under the
null hypothesis, half of the values should be above
1 and half below, so p = 1/2. Since signed dif-
ference is being considered, sentences with depen-
dencies equal to 1 are excluded. The correspond-
ing binomial distribution of the signs to calculate
whether the model is better than chance is b(n, p) =
b(r+ +r?, 1/2). The corresponding p-value may be
calculated using Equation 1.
1 ?
r+?1
?
k=0
n!
k!(n ? k)!1/2(1/2)
n?k (1)
This same method can be used for determining
statistically significant improvement over right and
left branching baselines. For each sentence, the dif-
ference between the number of correct dependen-
cies in the candidate parse and the number of cor-
rect dependencies in the baseline may be calculated.
The number of positive and negative signed differ-
ences are counted as r+ and r?, respectively, and
the procedure for calculating statistically significant
improvement is the same.
6 Results
Each model in Table 6 has significantly better per-
formance than item above using statistical proce-
dure described in Section 5.2. A number of ob-
servations can be drawn from this table. First, all
the models outperform random and right branching
baselines. This is the first time we are aware of
that this has been shown with lexical items in de-
pendency UGI. Secondly, local context outperforms
global context. This is to be expected given the rel-
atively fixed word order in English, but it is some-
what surprising that the differences between local
and global are not greater. Thirdly, it is clear that the
addition of prior knowledge, whether projectivity or
prior distributions, improves performance. Fourthly,
Method
Context/Projectivity/Prior Dependencies Correct
Random/no/no 14.2%
Right branching 17.6%
Global/no/no 17.9%
Global/no/yes 21.0%
Global/yes/no 21.4%
Global/yes/yes 21.7%
Local/no/no 22.5%
Local/no/yes 25.7%
Local/yes/yes 26.3%
Local/yes/no 26.7%
Left branching 35.8%
Table 1: Parsing results on WSJ10
projectivity and prior distributions have little addi-
tive effect. Thus it appears that they bring to bear
similar kinds of constraints.
7 Discussion
The results in Section 6 address the unanswered
questions identified in Section 4, i.e. parts of speech,
semantics, context, projectivity, and prior distribu-
tions.
The most salient result in Section 6 is successful
UGI without part of speech tags. As far as we know,
this is the first time dependency UGI has been suc-
cessful without the hidden syntactic structure pro-
vided by part of speech tags. It is interesting to note
that latent semantic grammars improve upon Paskin
(2001), even though that model is projective. It ap-
pears that lexical semantics are the reason. Thus
these results address two of the unanswered ques-
tions from Section 6 regarding parts of speech and
semantics. Semantics improve dependency UGI. In
fact, they improve dependency UGI so much so that
parts of speech are not necessary to beat a right
branching baseline.
Context has traditionally been defined locally, e.g.
the preceding and following word(s). The results
above indicate that a global definition of context is
also effective, though not quite as highly perform-
ing as a local definition on the WSJ10. This sug-
gests that English UGI is not dependent on local lin-
ear context, and it motivates future exploration of
word-order free languages using global context. It is
50
also interesting to note that the differences between
global and local contexts begin to disappear as pro-
jectivity and prior distributions are added. This sug-
gests that there is a certain level of equivalence be-
tween a global context model that favors local at-
tachments and a local context model that has no at-
tachment bias.
Projectivity has been assumed in previous cases
of English UGI (Klein and Manning, 2004; Paskin,
2001). As far as we know, this is the first time a
nonprojective model has outperformed a random or
right branching baseline. It is interesting that a non-
projective model can do so well when it assumes so
little about the structure of a language. Even more
interesting is that the addition of projectivity to the
models above increases performance only slightly.
It is tempting to speculate that projectivity may be
something of a red herring for English dependency
parsing, cf. McDonald et al (2005).
Prior distributions have been previously assumed
as well (Klein and Manning, 2004). The differential
effect of prior distributions in previous work has not
been clear. Our results indicate that a prior distribu-
tion will increase performance. However, as with
projectivity, it is interesting how well the models
perform without this prior knowledge and how slight
an increase this prior knowledge gives. Overall, the
prior distribution used in the evaluation is not neces-
sary to beat the right branching baseline.
Projectivity and prior distributions have signifi-
cant overlap when the prior distribution favors closer
attachments. Projectivity, by forcing a head to gov-
ern a contiguous subsequence, also favors closer at-
tachments. The results reported in Section 6 suggest
that there is a great deal of overlap in the benefit pro-
vided by projectivity and the prior distribution used
in the evaluation. Either one or the other produces
significant benefits, but the combination is much less
impressive.
It is worthwhile to reiterate the sparseness of prior
knowledge contained in the basic model used in
these evaluations. There are essentially four compo-
nents of prior knowledge. First, the ability to create
an ngram by context feature matrix. Secondly, the
application of SVD to that matrix. Thirdly, the cre-
ation of a fully connected dependency graph from
the post-SVD matrix. And finally, the extraction
of a minimum spanning tree from this graph. Al-
though we have not presented evaluation on word-
order free languages, the basic model just described
has no obvious bias against them. We expect that
latent semantic grammars capture some of the uni-
versals of grammar induction. A fuller exploration
and demonstration is the subject of future research.
8 Conclusion
This paper presented latent semantic grammars for
the unsupervised induction of English grammar. The
creation of latent semantic grammars and their appli-
cation to parsing were described. Experiments with
context, projectivity, and prior distributions showed
the relative performance effects of these kinds of
prior knowledge. Results show that assumptions of
prior distributions, projectivity, and part of speech
information are not necessary for this task.
References
Jerome R. Bellegarda. 2000. Large vocabulary speech
recognition with multispan statistical language mod-
els. IEEE Transactions on Speech and Audio Process-
ing, 8(1):76?84.
Michael W. Berry, Susan T. Dumais, and Gavin W.
O?Brien. 1995. Using linear algebra for intelligent in-
formation retrieval. Society for Industrial and Applied
Mathematics Review, 37(4):573?595.
Michael W. Berry. 1992. Large scale singular value com-
putations. International Journal of Supercomputer
Applications, 6(1):13?49.
Eric Brill and Mitchell Marcus. 1992. Automatically
acquiring phrase structure using distributional analy-
sis. In Speech and Natural Language: Proceedings
of a Workshop Held at Harriman, New York, pages
155?160, Philadelphia, February 23-26. Association
for Computational Linguistics.
John Carroll, Guido Minnen, and Ted Briscoe. 2003.
Parser evaluation using a grammatical relation anno-
tation scheme. In A. Abeill, editor, Treebanks: Build-
ing and Using Syntactically Annotated Corpora, chap-
ter 17, pages 299?316. Kluwer, Dordrecht.
Noam Chomsky. 1957. Syntactic Structures. Mouton,
The Hague.
Noah Coccaro and Daniel Jurafsky. 1998. Towards bet-
ter integration of semantic predictors in statistical lan-
guage modeling. In Proceedings of the International
Conference on Spoken Language Processing, pages
2403?2406, Piscataway, NJ, 30th November-4th De-
cember. IEEE.
51
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Michael A. Covington. 2001. A fundamental algorithm
for dependency parsing. In John A. Miller and Jef-
fery W. Smith, editors, Proceedings of the 39th Annual
Association for Computing Machinery Southeast Con-
ference, pages 95?102, Athens, Georgia.
Jane K. Cullum and Ralph A. Willoughby. 2002. Lanc-
zos Algorithms for Large Symmetric Eigenvalue Com-
putations, Volume 1: Theory. Society for Industrial
and Applied Mathematics, Philadelphia.
Scott C. Deerwester, Susan T. Dumais, George W. Fur-
nas, Thomas K. Landauer, and Richard A. Harshman.
1990. Indexing by latent semantic analysis. Jour-
nal of the American Society of Information Science,
41(6):391?407.
Yonggang Deng and Sanjeev Khudanpur. 2003. La-
tent semantic information in maximum entropy lan-
guage models for conversational speech recognition.
In Proceedings of Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 56?63,
Philadelphia, May 27-June 1. Association for Compu-
tational Linguistics.
Susan Dumais. 1991. Improving the retrieval of informa-
tion from external sources. Behavior Research Meth-
ods, Instruments and Computers, 23(2):229?236.
Peter W. Foltz, Walter Kintsch, and Thomas K. Lan-
dauer. 1998. The measurement of textual coherence
with latent semantic analysis. Discourse Processes,
25(2&3):285?308.
Zellig Harris. 1954. Distributional structure. Word,
10:140?162.
Richard A. Hudson. 1987. Zwicky on heads. Journal of
Linguistics, 23:109?132.
Dan Klein and Christopher D. Manning. 2002. A genera-
tive constituent-context model for improved grammar
induction. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 128?135, Philadelphia, July 7-12. Association
for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Annual Meeting of the Association for Computa-
tional Linguistics, pages 478?485, Philadelphia, July
21-26. Association for Computational Linguistics.
Dan Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis, Stanford Univer-
sity.
Thomas K. Landauer and Susan T. Dumais. 1997. A so-
lution to plato?s problem: The latent semantic analysis
theory of the acquisition, induction, and representation
of knowledge. Psychological Review, 104:211?240.
Thomas. K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. Introduction to latent semantic analysis.
Discourse Processes, 25(2&3):259?284.
Rasmus M. Larsen. 1998. Lanczos bidiagonalization
with partial reorthogonalization. Technical Report
DAIMI PB-357, Department of Computer Science,
Aarhus University.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic. 2005. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
of Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 523?530, Philadelphia, October 6-
8. Association for Computational Linguistics.
Mark A. Paskin. 2001. Grammatical bigrams. In T. G.
Dietterich, S. Becker, and Z. Ghahramani, editors, Ad-
vances in Neural Information Processing Systems 14,
pages 91?97. MIT Press, Cambridge, MA.
Vladimir Pericliev and Ilarion Ilarionov. 1986. Testing
the projectivity hypothesis. In Proceedings of the 11th
International Conference on Computational Linguis-
tics, pages 56?58, Morristown, NJ, USA. Association
for Computational Linguistics.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of the 7th European As-
sociation for Computational Linguistics Conference
(EACL-95), pages 141?149, Philadelphia, March 27-
31. Association for Computational Linguistics.
Zach Solan, David Horn, Eytan Ruppin, and Shimon
Edelman. 2005. Unsupervised learning of natural lan-
guages. Proceedings of the National Academy of Sci-
ences, 102:11629?11634.
Menno M. van Zaanen. 2000. ABL: Alignment-based
learning. In Proceedings of the 18th International
Conference on Computational Linguistics, pages 961?
967, Philadelphia, July 31-August 4. Association for
Computational Linguistics.
Peter Wiemer-Hastings and Iraide Zipitria. 2001. Rules
for syntax, vectors for semantics. In Proceedings of
the 23rd Annual Conference of the Cognitive Science
Society, pages 1112?1117, Mahwah, NJ, August 1-4.
Erlbaum.
52
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 669?672,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
An Exploration of Off Topic Conversation
Whitney L. Cade
University of Memphis
365 Innovation Drive
Memphis, TN  38152-3115
wlcade@memphis.edu
Blair A. Lehman
University of Memphis
365 Innovation Drive
Memphis, TN  38152-3115
baleh-
man@memphis.edu
Andrew Olney
University of Memphis
365 Innovation Drive
Memphis, TN  38152-3115
aolney@memphis.edu
Abstract
In a corpus of expert tutoring dialogue, con-
versation that is considered to be ?off topic? 
(non-pedagogical) according to a previous 
coding scheme is explored for its value in tu-
toring dynamics. Using the Linguistic Inquiry 
and Word Count (LIWC) tool, phases of tutor-
ing categorized as ?off topic? were compared 
with interactive problem solving phases to ex-
plore how the two differ on the emotional, 
psychological, and topical dimensions ana-
lyzed by LIWC. The results suggest that con-
versation classified as ?off topic? serves as 
motivation and broad pedagogy in tutoring. 
These findings can be used to orient future re-
search on ?off topic? conversation, and help to 
make sense of both previous coding schemes 
and noisy data sets.
1 Introduction
Methods of investigating a large and noisy data set 
are of paramount importance in computational lin-
guistics. Quite often, qualitative coding schemes 
are used to capture snapshots of the data set, but 
these may gloss over finer details or miss the larger 
picture. Add to that the messy and unpredictable 
nature of naturalistic data, and analysis becomes 
even more complicated. Therefore, a multi-method 
approach to understanding pre-existing coding 
schemes and orienting future in-depth analyses of 
those schemes proves to be a useful means of ex-
ploring one?s data.
Dialogue, particularly tutorial dialogue, is one 
area where large, noisy data sets are common. 
Computer and human tutoring data have been 
parsed, coded, and tested by a number of research-
ers, and much effort has been put into making 
sense of the variability in the task-oriented dialo-
gue (e.g. Chi, Roy, and Hausmann, 2008; Graesser, 
Person, and Magliano, 1995; Person, Lehman, and
Ozbun, 2007). This work has all been in pursuit of 
a deep understanding of the complex interaction 
between the human tutor and student, which, if 
understood, could be used to boost the efficacy of 
artificially intelligent computer tutors. Expert hu-
man tutoring has been found to increase learning 
gains by as much as 2 sigmas (Bloom, 1984), 
which makes understanding their methods and mo-
tives the goal of any tutor research.
The corpus under examination here was col-
lected with the express purpose of understanding 
how truly expert tutors manage a tutoring session, 
with an emphasis on creating a corpus of naturalis-
tic dialogue data. The corpus has been investigated 
at two different grain sizes, a dialogue move level 
and a sustained phases level. Our study investi-
gates in detail an ?other? category that these cod-
ing schemes, which emphasize the pedagogy of the 
tutors and the students reactions, classify as ?off 
topic? conversation. Off topic conversation, by 
virtue of its name, does not address the tutoring 
task in which the tutor and student are engaged. 
However, given the prevalence of off topic conver-
sation in the corpus, it is perhaps more likely that 
the function or utility of off topic conversation in 
expert tutoring is indirect rather than non-existent, 
suggesting that the noisiest part of the tutoring di-
alogue corpus, off topic conversation, should be 
further explored.
Because any topic not pertaining to the topic at 
hand may be broached in off topic conversation 
and because the dialogue itself is full of false 
669
starts, interruptions, and fragmented sentences, it is 
reasonable to explore off topic conversation using 
a bag of words method that is applicable to a varie-
ty of formal and informal texts. One such method 
is the Linguistic Inquiry and Word Count (LIWC) 
tool developed by Pennebaker et al, (2001), which 
looks for words that fall into specific, predeter-
mined categories such as COGNITIVE MECHANISMS
and POSITIVE EMOTIONS, then reports the percent 
of words in the document that fall into that catego-
ry. LIWC provides over 70 possible categories, 
and can help sketch a rough picture of the verbal 
dynamics of a text (Mairesse and Walker, 2006; 
Mihalcea and Strapparava, 2009). Using a readily 
available tool like LIWC allows an examination of 
the variability within off topic conversation based 
on predetermined LIWC features. We can also 
compare these results to a prominent pedagogical 
category, such as scaffolding, that a current coding 
scheme particularly emphasizes, and examine the 
differences between the two.
In this analysis, the task-orientation and utility 
of ?off topic? conversation are investigated by 
comparing its outcome scores in certain dimen-
sions of LIWC to a classic pedagogical and inter-
active phase of tutoring: scaffolding (Rogoff and
Gardner, 1984). Scaffolding, previously identified 
in a tutorial dialogue coding scheme (Cade, Copel-
and, Person, and D?Mello, 2008), involves much of 
the conversational give-and-take expected in ca-
sual off topic conversation, but is considered to be 
a very focused, on task phase of tutoring. Knowing 
how off topic conversation differs from scaffolding
may help further exploration of this forgotten 
phase of tutoring. Likewise, it would give us direc-
tion in how to structure future coding schemes that 
would help bring clarity to the data set.
2 Methods
In this study, pedagogical and non-pedagogical 
phases of expert tutoring sessions were compared 
on linguistic dimensions to get at the diverse nature 
of off topic conversation within a naturalistic ex-
pert tutoring session.
The corpus under examination was collected in 
a previous study on expert human tutors. There-
fore, what follows is a brief synopsis of how this 
corpus was collected.
Ten expert math and science tutors (4 male and 
6 female) were recruited through local tutoring 
agencies and schools. Tutors were considered ?ex-
pert? when they met the following criteria: they 
had to be licensed to teach at the secondary level, 
have five or more years of tutoring experience, be 
employed by a professional tutoring agency, and 
come highly recommended by school personnel 
who specialize in providing support to students 
who are struggling academically. Student partici-
pants were in grades 7 to 12, except for one who 
was obtaining a GED. All of the students were in 
academic trouble and actively sought out tutoring.
All sessions were unobtrusively videotaped at 
the location decided upon by the tutor and student. 
The researcher turned on the camera and left the 
room when the session began. Each student parti-
cipated in a maximum of two tutorial sessions, 
while each tutor participated in between two and 
eight tutoring sessions. These 50 1-hour tutoring 
sessions were then transcribed.
Two previously identified phases of tutoring (or 
?modes?), Off Topic and Scaffolding, were com-
pared to investigate their psychological, emotional, 
and topical differences. To do this, instances of 
each mode were extracted from 30 sessions (all 
sessions that contained at least one Off Topic and 
one Scaffolding mode). If a session had multiple 
occurrences of a single mode, those modes were 
compiled into a single document. Documents were 
capped at 1000 words each to prevent differences 
in word count between the modes from affecting 
the outcomes. These documents were also sepa-
rated by speaker (tutor or student); speakers may 
be differentially motivated to broach certain topics, 
and so separating out these effects leads to more 
specific identification of conversational dynamics.
Each session?s Scaffolding and Off Topic docu-
ment was then analyzed using LIWClite 7, which 
calculates the percentage of each document?s 
words that fall into specific, predefined categories. 
Though this version of LIWC offers over 70 lin-
guistic categories, only 15 were of interest in de-
termining the nature of off topic conversation: 
SOCIAL PROCESSES (ex: mate, talk, they), FAMILY
(daughter, husband, aunt), FRIENDS (buddy, neigh-
bor), AFFECTIVE PROCESSES (happy, cried), 
POSITIVE EMOTION (nice, sweet), NEGATIVE 
EMOTIONN (hurt, ugly, nasty) ANXIETY (worried, 
nervous), TENTATIVENESS (maybe, perhaps), 
CERTAINTY (always, never), WORK (majors, class), 
ACHIEVEMENT (earn, win), LEISURE (chat, movie), 
HOME (kitchen, family), NONFLUENCIES (umm, 
670
hm), and FUTURE (will, gonna). 
These categories are the most relevant in illu-
strating the emotional, topical, and psychological 
picture of conversation in tutoring when compared 
with the more on-task behavior of problem solving.
3 Discussion of Results
LIWC 
Category
T
/S
Off
Top
M
Scaff
M
Wil-
coxon
p-val
Paired
t-test
t-val
Co-
hen?s
d
Social 
Process
T 11.15 7.75 <0.01 <0.01 1.37
S 8.25 4.87 <0.01 <0.01 0.90
Positive 
Emotion
T 5.41 4.83 0.27 0.29
S 6.54 4.54 0.09 0.05 0.47
Tentative T 3.10 1.91 <0.01 <0.01 1.08
S 2.68 1.60 0.02 0.02 0.65
Work T 2.90 1.10 <0.01 <0.01 0.86
S 2.70 2.09 0.54 0.43
Achieve T 1.02 0.95 0.67 0.76
S 0.52 1.89 <0.01 <0.01 -0.92
Leisure T 0.78 0.23 0.60 0.27
S 0.50 0.15 0.05 0.07 0.50
Home T 0.30 0.04 0.02 0.05 0.53
S 0.24 0.01 0.03 0.17 0.37
Nonfluen. T 1.51 1.11 0.04 0.08 0.44
S 3.89 4.14 0.17 0.82
Future T 1.13 1.23 0.80 0.66
S 0.74 1.35 0.01 0.04 -0.49
Table 1. LIWC Dimensions with Significant Results
Since a normal distribution of scores cannot be 
assumed in this analysis, comparisons between Off 
Topic conversation and Scaffolding dialogue were 
made by comparing the LIWC scores of the modes 
using both Wilcoxon?s signed-rank test and a 
paired t-test, with similar outcomes. Effect sizes 
were also analyzed by calculating Cohen?s d. Table 
1 illustrates the significant results that emerged. In 
total, each category investigated occurs more in 
Off Topic than in Scaffolding, with the exception 
of a student?s discussion of ACHIEVEMENT and 
FUTURE.
From this analysis, an interesting pattern of re-
sults emerges. The Off Topic mode had previously 
been characterized as a conversation that had noth-
ing to do with the lesson at hand, which connoted 
that it is fairly irrelevant. However, Off Topic does 
not seem to be so wholly ?off topic.? Tutors and 
students in the Off Topic mode talk about work
more often than they do in the Scaffolding mode, 
which is a mode where nothing but work is done. 
WORK words, according to the authors of LIWC, 
are mostly school-related. Off Topic may be a 
mode that allows the tutor to discuss test-taking 
skills, study strategies, and remind students what 
tasks need to be completed before the next tutoring 
session. For instance, one tutor divided up a study 
guide into manageable portions that needed to be 
completed every night so that the student would be 
prepared for an upcoming test. Previous to now, 
these conversations have only been qualitatively 
observed, but this supports a more in-depth analy-
sis of what type of work tutors are talking about 
when they are supposedly discussing non-
pedagogical topics.
This hypothesis is supported by the significant 
amount of conversation that takes place in Off 
Topic about the home; if FAMILY and FRIENDS
(which may crop up in casual conversation about 
HOME-related topics) are not discussed significant-
ly more in Off Topic, but HOME is, it may be that 
tutors are informing students of what sort of work 
needs to be done at home, and strategies to get 
work completed when on their own.
This may also explain why both students and 
tutors use more TENTATIVE words in Off Topic. 
Although it would seem that students should be 
more tentative and nonfluent when discussing dif-
ficult problem solving, they may be tentative in 
Off Topic when the tutor makes suggestions about 
studying and working. These suggestions of the 
tutor?s may be framed using language like ?may-
be? and ?perhaps? to make them more polite, and 
the student echoes this language in return. Thus, 
tentativeness may not come from uncertainty, but 
from suggestions couched in polite language.
It also appears that Off Topic conversation may 
not serve as a ?pep talk? time; although it does 
contain more POSITIVE EMOTION words than Scaf-
folding, it does not expound upon the student?s 
achievements. ACHIEVEMENT words are more 
common in Scaffolding, where students are receiv-
ing praise for their problem solving efforts. Off 
Topic conversation may seek to motivate the stu-
dent in more subtle ways. By using more words 
that refer to SOCIAL PROCESSES (such as the third 
person plural and words like ?talked?), the tutor 
and student may be building rapport with one 
another. This rapport may become important later 
on when the tutor gives the student blatantly nega-
tive feedback (Person et al, 2007), which can be 
motivationally damaging. Rapport may protect 
against flagging motivation in the student when the 
tutor uses ?us? language and connects with the stu-
dent in a more casual conversation.
671
4 Conclusions and Future Work
Our goal in this work was to use a simple linguistic 
analysis tool to uncover the hidden depths of an 
existing dialogue coding scheme. The use of such 
tools can paint a rich picture of the psychological, 
emotional, and topical content of a corpus, and can 
be used in two ways: first, it may help determine if 
a deeper inquiry into a hypothesis is warranted, 
and second, it can immediately orient future re-
search towards key issues in a corpus without the 
less rigorous speculation and qualitative observa-
tions. The nature of broader coding schemes can 
come to be understood in a multifaceted manner 
using linguistic analysis, which may also inform 
future work.
Here, we have observed that off topic conversa-
tion in an expert tutoring dialogue corpus operates 
in a multidimensional way that is not irrelevant 
when studying the dynamics of an expert tutoring 
session. By using the LIWC tool developed by 
Pennebaker et al (2001), themes concerning inter-
personal rapport and global pedagogy emerge. The 
purpose of ?off topic? conversation in tutoring may 
therefore be linked more to building a relationship 
between the tutor and the student, which is neces-
sary for the trials of problem solving, and for the 
dispensation of ?study strategies? that are more 
globally task-oriented, but are, nonetheless, impor-
tant in understanding the pedagogical strategies of 
expert tutors. Off topic conversation was also hy-
pothesized to function similarly in other tutorial 
work (Ros?, Kumar, Aleven, Robinson, and Wu, 
2006).
One way of adding validity to these claims 
would be to investigate the topics broached in Off 
Topic through a topics model. In this way, recur-
ring themes in off topic conversation can be re-
vealed, and these themes can be aligned with the 
LIWC findings to see if a pattern emerges. From 
there, a new coding scheme may be devised to cap-
ture the multiple types of off topic conversation, 
which, for now, seem to be divided between inter-
personal, rapport building and global pedagogy. 
This method of exploring a corpus has proven to 
be a useful approach when investigating possible 
avenues of improvement to coding schemes.
Acknowledgements
The research reported here was supported by the 
Institute of Education Sciences, U.S. Department 
of Education, through Grant R305A080594 to the 
University of Memphis. The opinions expressed 
are those of the authors and do not represent views 
of the Institute or the U.S. Department of Educa-
tion.
References
Benjamin Bloom. 1984. The 2 sigma problem: The 
search for methods of group instruction as effective 
as one-to-one tutoring. Educational Researcher, 
13:4-16.
Whitney Cade, Jessica Copeland, Natalie Person, and 
Sidney D?Mello. 2008. Dialogue modes in expert tu-
toring. Proceedings of the 9th International Confe-
rence on Intelligent Tutoring Systems, 470-479. 
Springer-Verlag, Berlin, Germany.
Michelene Chi, Marguerite Roy, and Robert Hausmann.
2008. Observing tutorial dialogues collaboratively: 
Insights about human tutoring effectiveness from vi-
carious learning. Cognitive Science, 32(2):301-341.
Art Graesser, Natalie Person, and Joseph Magliano. 
1995. Collaborative dialogue patterns in naturalistic 
one-on-one tutoring. Applied Cognitive Psychology, 
9:359-387.
Fran?ois Mairesse and Marilyn Walker. 2006. Automat-
ic Recognition of Personality in Conversation. In 
Proceedings of the Human Language Technology 
Conference of the North American Chapter of the 
ACL, 85?88. Association for Computational Linguis-
tics, New York.
Rada Mihalcea and Carlo Strapparava. 2009. The Lie 
Detector: Explorations in the Automatic Recognition 
of Deceptive Language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, 309-312. 
Association for Computational Linguistics, Suntec, 
Singapore.
James Pennebaker, Martha Francis, and Roger Booth. 
2001. Linguistic Inquiry and Word Count (LIWC): 
LIWC2001. Lawrence Erlbaum Associates, Mahwah, 
NJ.
Natalie Person, Blair Lehman, and Rachel Ozbun. 2007. 
Pedagogical and motivational dialogue moves used 
by expert tutors. Presented at the 17th Annual Meet-
ing of the Society for Text and Discourse. Glasgow, 
Scotland.
Barbara Rogoff and William Gardner. 1984. Adult 
guidance of cognitive development. Everyday cogni-
tion: Its development in social context, 95-116. Har-
vard University Press, Cambridge, MA.
Carolyn Ros?, Rohit Kumar, Vincent Aleven, Allen 
Robinson, & Chih Wu. 2006. CycleTalk: Data dri-
ven design of support for simulation based learning. 
International Journal of Artificial Intelligence in 
Education, 16:195-223.
672
Proceedings of the Sixth Workshop on Innovative Use of NLP for Building Educational Applications, pages 111?119,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Generating Concept Map Exercises from Textbooks
Andrew M. Olney, Whitney L. Cade, and Claire Williams
Institute for Intelligent Systems
University of Memphis
365 Innovation Drive, Memphis, TN 38152
aolney@memphis.edu
Abstract
In this paper we present a methodology for
creating concept map exercises for students.
Concept mapping is a common pedagogical
exercise in which students generate a graph-
ical model of some domain. Our method auto-
matically extracts knowledge representations
from a textbook and uses them to generate
concept maps. The purpose of the study is to
generate and evaluate these concept maps ac-
cording to their accuracy, completeness, and
pedagogy.
1 Introduction
Concept mapping is an increasingly common educa-
tional activity, particularly in K-12 settings. Concept
maps are graphical knowledge representations that
represent a concept, question or process (Novak and
Canas, 2006). A recent meta-analysis of 55 studies
involving over five thousand participants found the
students creating concept maps had increased learn-
ing gains (d = .82) and students studying concept
maps had increased learning gains ( d = .37 ) (Nesbit
and Adesope, 2006). In comparison, novice tutoring
across many studies have had more modest learning
gains ( d = .40 ) (Cohen et al, 1982) ? comparable
to studying concept maps but not to creating them.
For difficult topics, or for students new to con-
cept mapping, some researchers propose so-called
expert skeleton concept maps (Novak and Canas,
2006). These are partially specified concept maps
that may have some existing structure and then a
?word bank? of concepts, properties, and relations
that can be used to fill in the rest of the map. This
approach is consistent with concept maps as instruc-
tional scaffolds for student learning (O?Donnell et
al., 2002). As students increase in ability, they can
move from expert skeleton concept maps to self-
generated maps.
Because concept maps are essentially knowledge
representations based in words, analysis and syn-
thesis of concept maps are theoretically amenable
to knowledge-rich computational linguistic tech-
niques. This paper presents an approach to extract-
ing concept maps from textbooks to create educa-
tional materials for students. The concept maps can
be used as expert skeleton concept maps. The rest of
the paper is organized as follows. Section 2 presents
a brief overview of concept maps from the AI, psy-
chological, and education literatures and motivates a
particular representation used in later sections. Sec-
tion 3 presents a general technique for extracting
concept maps from textbooks and generating graph-
ical depictions of these as student exercises. Sec-
tion 4 describes a comparative evaluation of maps
extracted by the model to gold-standard human gen-
erated concept maps. Section 5 discusses these re-
sults and their significance for generating concept
map exercises for students.
2 Perspectives on Concept Maps
There are many different kinds of concept maps, and
each variation imposes different computational de-
mands. One prominent perspective comes from the
AI literature in formal reasoning, as an extension of
work done a century ago by Pierce on existential
graphs (Sowa, 2007; Sowa, 2009). In this formula-
tion (which is now an ISO standard), so-called con-
111
ceptual graphs are interchangeable with predicate
calculus. Of particular importance to the current dis-
cussion is grain size, that is the level of granularity
given to nodes and relationships. In these conceptual
graphs, grain size is very small, such that each argu-
ment, e.g. John, is connected to other arguments,
e.g. Mary, through an arbitrary predicate, e.g. John
loves Mary. Aside from the tight correspondence to
logic, grain size turns out to be a relevant differentia-
tor amongst conceptualizations of conceptual graphs
amongst different fields, and one that leads to impor-
tant design decisions when extracting graphs from a
text.
Another prominent perspective comes from the
psychology literature (Graesser and Clark, 1985),
with some emphasis on modeling question ask-
ing and answering (Graesser and Franklin, 1990;
Gordon et al, 1993). In this formulation
of conceptual graphs, nodes themselves can be
propositions, e.g. ?a girl wants to play with
a doll,? and relations are (as much as pos-
sible) limited to a generic set of propositions
for a given domain. For example, one such
categorization consists of 21 relations including
is-a, has-property, has-consequence,
reason, implies, outcome, and means (Gor-
don et al, 1993). A particular advantage of limiting
relations to these categories is that the categories can
then be set into correspondence with certain ques-
tion types, e.g. definitional, causal consequent, pro-
cedural, for both the purposes of answering ques-
tions (Graesser and Franklin, 1990) as well as gen-
erating them (Gordon et al, 1993).
Finally, concept maps are widely used in science
education (Fisher et al, 2000; Mintzes et al, 2005)
for both enhancing student learning and assessment.
Even in this community, there are several formu-
lations of concept maps. One such widely known
map is a hierarchical map (Novak and Canas, 2006;
Novak, 1990), in which a core concept/question at
the root of the map drives the elaboration of the
map to more and more specific details. In hierarchi-
cal maps, nodes are not propositions, and the edges
linking nodes are not restricted (Novak and Canas,
2006). Alternative formulations to hierarchical
maps include cluster maps, MindMaps, computer-
generated associative networks, and concept-circle
diagrams, amongst others (Fisher et al, 2000).
part
abdomenarthropod posterior
has-part
is-a
has-property
Figure 1: A concept map fragment. Key terms have black
nodes.
Of particular interest is the SemNet formulation,
which is characterized by a central concept (which
has been determined as highly relevant in the do-
main) linked to other concepts using a relatively pre-
scribed set of relations (Fisher, 2010). End nodes
can be arbitrary, and cannot themselves be linked to
unless they are another core concept in the domain.
Interestingly, in the field of biology, 50% of all links
are is-a, part-of, or has-property (Fisher et al, 2000),
which suggests that generic relations may be able
to account for a large percentage of links in any
domain, with only some customization to be per-
formed for specific domains. An example SemNet
triple (start node/relation/end node) is ?prophase in-
cludes process chromosomes become visible.? Sev-
eral thousand of such triples are available online for
biology, illustrating the viability of this representa-
tional scheme for biology (Fisher, 2010).
3 Computational Model
Our approach for extracting concept maps from a bi-
ology textbook follows the general SemNet formu-
lation with some elements of the conceptual graphs
of Graesser and Clark (1985). There are two pri-
mary reasons for adopting this formulation, rather
than the others described in Section 2. By using a
highly comparable formulation to the original Sem-
Nets, one can compare generated graphs with sev-
eral thousand, expert-generated triples that are freely
available. Second, by making just a few modifica-
tions to the SemNet formalism, we can create a for-
malism that is more closely aligned with question
answering/question generation, which we believe is
a fruitful avenue for future research.
112
Our concept map representation has two signif-
icant structural elements. The first is key terms,
shown as black nodes in Figure 1. These are terms in
our domain that are pedagogically significant. Only
key terms can be the start of a triple, e.g. abdomen
is-a part. End nodes can contain key terms, other
words, or complete propositions. This structural el-
ement is aligned with SemNets. The second cen-
tral aspect of our representation is labeled edges,
shown as boxes in Figure 1. As noted by (Fisher
et al, 2000), a small set of edges can account for a
large percentage of relationships in a domain. Thus
this second structural element aligns better with psy-
chological conceptual graphs (Gordon et al, 1993;
Graesser and Clark, 1985), but remains consistent
with the spirit of the SemNet representation. The
next sections outline the techniques and models used
for defining key terms and edges, followed by our
method of graph extraction.
3.1 Key Terms
General purpose key term extraction procedures are
the subject of current research (Medelyan et al,
2009), but they are less relevant in a pedagogical
context where key terms are often already provided
in learning materials. For example, both glossaries
(Navigli and Velardi, 2008), and textbook indices
(Larran?aga et al, 2004) have previously been used
as resources in constructing domain models and on-
tologies. To develop our key terms, we used the
glossary and index from a textbook in the domain of
biology (Miller and Levine, 2002) as well as the key-
words given in a test-prep study guide (Cypress Cur-
riculum Services, 2008). Thus we can skip the key-
word extraction step of previous work on concept
map extraction (Valerio and Leake, 2008; Zouaq and
Nkambou, 2009) and the various errors associated
with that process.
3.2 Edge Relations
Since edge relations used in conceptual graphs often
depict abstract, domain-independent relationships
(Graesser and Clark, 1985; Gordon et al, 1993), it
might be inferred that these types of relationships,
e.g. is-a, has-part, has-property, are
exhaustive. While such abstract relationships may
be able to cover a sizable percentage of all relation-
ships previous work suggests new content can drive
new additions to that set (Fisher et al, 2000). In or-
der to verify the completeness of our edge relations,
we undertook an analysis of concept maps from bi-
ology.
Over a few hours, we manually clustered 4371 bi-
ology triples available on the Internet1 that span the
two topics of molecules & cells and population bi-
ology. Although these two topics represent a small
subset of biology topics, we hypothesize that as the
extremes of levels of description in biology, their re-
lations will be representative of the levels between
them.
Consistent with previous reported concept map
research in biology (Fisher et al, 2000), our cluster
analysis revealed that 50% of all relations were
either is-a, has-part, or has-property.
Overall, 252 relation types clustered into 20 rela-
tions shown in Table 1. The reduction from 252
relation types to 20 clusters generally lost little
information because the original set of relations
included many specific subclass relationships, e.g.
part-of had the subclasses composed of, has
organelle, organelle of, component
in, subcellular structure of, has
subcellular structure. In most cases
subclassing of this kind is recoverable from infor-
mation distributed across nodes. For example, if we
know that golgi body is-a organelle and we know
that eukaryotic cell has-part golgi body, then
the original relation golgi body organelle of
eukaryotic cell is implied.
Additional edge relations were added based on
the psychology literature (Graesser and Clark, 1985;
Gordon et al, 1993) as well as adjunct information
gleaned from the parser described in the next sec-
tion, raising the total number of edge relations to
30. As indicated by Table 1 a great deal of over-
lap exists between the clustered edge relations and
those in the psychological literature. However, nei-
ther goal-oriented relationships nor logical relation-
ships (and/or) were included as these did not seem
appropriate for the domain (a cell divides because it
must, not because it ?wants to?). We also removed
general relations that overlapped with more specific
ones, e.g. temporal is replaced by before, during,
after. We hypothesize that the edge relation scheme
1http://www.biologylessons.sdsu.edu
113
Relation Clustered Gordon Adjunct Relation Clustered Gordon Adjunct
after * has-consequence * * *
before * has-part * *
combine * has-property * *
connect * * implies *
contain * * isa * *
contrast * lack *
convert * location * *
definition * manner * *
direction * not *
during * * possibility *
enable * produce *
example * purpose *
extent * reciprocal *
follow * require *
function * same-as * *
Table 1: Edge relations from cluster analysis, Gordon et al (1993), and parser adjunct labels
in Table 1 would be portable to other domains, but
some additional tuning would be necessary to cap-
ture fine-grained, domain specific relationships.
3.3 Automatic Extraction
According to the representational scheme defined
above, triples always begin with a key term that is
connected by a relation to either another key term
or a propositional phrase. In other words, each key
term is the center of a radial graph. Triples begin-
ning and ending with key terms bridge these radial
graphs. The automatic extraction process follows
this representational scheme. Additionally, the fol-
lowing process was developed using a biology glos-
sary and biology study guide as a development data
set, so training and testing data were kept separate in
this study.
We processed a high school biology text (Miller
and Levine, 2002), using its index and glossary as
sources of key terms as described above, using the
LTH SRL2 parser. The LTH SRL parser is a seman-
tic role labeling parser that outputs a dependency
parse annotated with PropBank and NomBank pred-
icate/argument structures (Johansson and Nugues,
2008; Meyers et al, 2004; Palmer et al, 2005). For
each word token in a parse, the parser returns in-
2The Swedish ?Lunds Tekniska Ho?gskola? translates as
?Faculty of Engineering?
formation about the word token?s part of speech,
lemma, head, and relation to the head. Moreover,
it uses PropBank and NomBank to identify pred-
icates in the parse, either verbal predicates (Prop-
Bank) or nominal predicates (NomBank), and their
associated arguments. A slightly abbreviated exam-
ple parse corresponding to the concept map in Fig-
ure 1 is shown in Table 2.
In Table 2 the root of the sentence is ?is,? whose
head is token 0 (the implied root token) and whose
dependents are ?abdomen? and ?part,? the subject
and predicate, respectively. Predicate ?part.01,? be-
ing a noun, refers to the Nombank predicate ?part?
roleset 1. This predicate has a single argument of
type A1, i.e. theme, which is the phrase domi-
nated by ?of,? i.e. ?of an arthopod?s body.? Predi-
cate ?body.03? refers to Nombank predicate ?body?
roleset 3 and also has a single argument of type A1,
?arthopod,? dominating the phrase ?an arthopod?s.?
Potentially each of these semantic predicates repre-
sents a relation, e.g. has-part, and the syntactic in-
formation in the parse also suggests relations, e.g.
ABDOMEN is-a.
The LTH parser also marks adjunct arguments.
For example, consider the sentence ?During electron
transport, H+ ions build up in the intermembrane
space, making it positively charged.? There are four
adjuncts in this sentence: ?During electron trans-
114
port? is a temporal adjunct, ?in the intermembrane
space? is a locative adjunct, ?making it positively
charged? is an adverbial adjunct, and ?positively? is
a manner adjunct. The abundance of these adjuncts
led to the pragmatic decision to include them as edge
relation indicators in Table 1.
After parsing, four triple extractor algorithms are
applied to each sentence, targeting specific syntac-
tic/semantic features of the parse, is-a, adjectives,
prepositions, and predicates. Each extractor first at-
tempts to identify a key term as a possible start node.
The search for key terms is greedy, attempting to
match an entire phrase if possible, e.g. ?abiotic fac-
tor? rather than ?factor,? by searching the depen-
dents of an argument and applying morphological
rules for pluralization. If no key term can be found,
the prospective triple is discarded. Potentially, some
unwanted loss can occur at this stage because of
unresolved anaphora. However, it appears that the
writing style of the particular textbook used, Miller
and Levine (2002), generally minimizes anaphoric
reference.
As exemplified by Figure 1 and Table 2, several
edge relations are handled purely syntactically. The
is-a extractor considers when the root verb of the
sentence is ?be,? but not a helping verb. Is-a rela-
tions can create a special context for processing ad-
ditional relations. For example, in the sentence, ?An
abdomen is a posterior part of an arthropod?s body,?
?posterior? modifies ?part,? but the desired triple is
abdomen has-property posterior. This is an ex-
ample of the adjective extraction algorithm running
in the context of an is-a relation: rather than al-
ways using the head of the adjective as the start of
the triple, the adjective extractor considers whether
the head is a predicate nominative. Prepositions can
create a variety of edge relations. For example, if
the preposition has part of speech IN and has a LOC
dependency relation to its head (a locative relation),
then the appropriate relation is location, e.g. ?by
migrating whales in the Pacific Ocean.? becomes
whales location in the Pacific Ocean.
The predicates from PropBank and NomBank use
specialized extractors that consider both their argu-
ment structure as well as the specific sense of the
predicate used. As illustrated in some of the preced-
ing examples, not all predicates have an A0. Like-
wise not all predicates have patient/instrument roles
like A1 and A2. Ideally, every predicate would
start with A0 and end with A1, but the variability
in predicate arguments makes simple mapping unre-
alistic. To assist the predicate extractors, we created
a manual mapping between predicates, arguments,
and edge relations, for every predicate that occurred
more that 40 times in the textbook. Table 3 lists the
four most common predicates and their mappings.
Predicate Edge Relation Start End
have.03 HAS PROPERTY A0 Span
use.01 USE A0 Span
produce.01 PRODUCE A0 Span
call.01 HAS DEFINITION A1 A2
Table 3: Predicate map examples
The label ?Span? in the last column indicates that
the end node of the triple should be the text domi-
nated by the predicate. Consider the example, ?The
menstrual cycle has four phases? has AO cycle and
A1 phases. Using just A0 and A1, the extracted
triple would be menstrual cycle has-property
phases. Using the span dominated by the predi-
cate yields menstrual cycle has-property four
phases, which is more correct in this situation. As
can be seen in this example, end nodes based on
predicate spans tend to contain more words and
therefore have closer fidelity to the original sen-
tence.
After triples are extracted from the parse, they
are filtered to remove triples that are not particularly
useful for generating concept map exercises. Filters
are applied on the back end rather than during the
extraction process because the triples discarded at
this stage might be usefully used for other applica-
tions such as student modeling or question genera-
tion. The first three filters used are straightforward
and require little explanation: the repetition filter,
the adjective filter, and the nominal filter. The repeti-
tion filter considers the number of words in common
between the start and end nodes. If the number of
shared words is more than half the words in the end
node, the triple is filtered. This helps alleviate redun-
dant triples such as cell has-property cell. The
adjective filter removes any triple whose key term is
an adjective. These triples violate the assumption by
the question generator that all key terms are nouns.
115
Id Form Lemma POS Head Dependency Relation Predicate Arg 1 Arg 2
1 abdomen abdomen NN 2 SBJ
2 is be VBZ 0 ROOT
3 a DT 5 NMOD
4 posterior posterior JJ 5 NMOD
5 part part NN 2 PRD part.01
6 of IN 5 NMOD A1
7 an DT 8 NMOD
8 arthropod arthropod NN 10 NMOD A1
9 s POS 8 SUFFIX
10 body body NN 6 PMOD body.03
11 . . 2 P
Table 2: A slightly simplified semantic parse
Has-property edge relations based on adjectives
were also filtered because they tend to overgener-
ate. Finally the nominal filter removes all NomBank
predicates except has-part predicates, since these of-
ten have Span end nodes and so contain themselves,
e.g. light has-property the energy of sunlight.
The final filter uses likelihood ratios to establish
whether the relation between start and end nodes
is meaningful, i.e. something not likely to occur
by chance. This filter measures the association be-
tween the start and end node using likelihood ratios
(Dunning, 1993) and a ?2 significance criterion to
remove triples with insignificant association. As a
first step in the filter, words from the end node that
have low log entropy are removed prior to calcula-
tion. This penalizes non-distinctive words that occur
in many contexts. Next, the remaining words from
start and end nodes are pooled into bags of words,
and the likelihood ratio calculated. By transforming
the likelihood ratio to be ?2 distributed (Manning
and Schu?tze, 1999), and applying a statistical signif-
icance threshold of .0001, triples with a weak associ-
ation between start and end nodes were filtered out.
The likelihood ratio filter helps prevent sentences re-
lated to specific examples from being integrated into
concept maps for a general concept. For example,
the sentence ?In most houses, heat is supplied by
a furnace that burns oil or natural gas.? from the
textbook is part of a larger discussion about home-
ostatis. An invalid triple implied by the sentence is
heat has-property supplied by a furnace. Since
heat and furnace do not have a strong association in
the textbook overall, the likelihood ratio filter would
discard this triple.
After filtering, triples belonging to a graph are
rendered to image files using the NodeXL3 graphing
library. In each image file, a key term defines the
center of a radial graph. To prevent visual clutter,
triples that have the same edge type can be merged
into a single node as is depicted in Figure 2.
4 Evaluation
A comparison study using gold-standard, human
generated maps was performed to test the quality
of the concept maps generated by the method de-
scribed in Section 3. The gold-standard maps were
taken from Fisher (2010). Since these maps cover
only a small section of biology, only the correspond-
ing chapters from Miller and Levine (2002), chap-
ters two and seven, were used to generate concept
maps. All possible concept maps were generated
from these two chapters, and then 60 of these con-
cept maps that had a corresponding map in the gold-
standard set were selected for evaluation.
Two judges having background in biology and
pedagogy were recruited to rate both the gold stan-
dard and generated maps. Each map was rated
on the following three dimensions: the cover-
age/completeness of the map with respect to the key
term (Coverage), the accuracy of the map (Accu-
racy), and the pedagogical value of the map (Ped-
agogy). A consistent four item scale was used for
3http://nodexl.codeplex.com/
116
Figure 2: Comparison of computer and human generated concept maps for ?cohesion.? The computer generated
concept map is on the left, and the human generated map is on the right.
all ratings dimensions. An example of the four item
scale is shown in Table 4.
Score Criteria
1 The map covers the concept.
2 The map mostly covers the concept.
3 The map only slightly covers the concept.
4 The map is unrelated to the concept.
Table 4: Rating scale for coverage
Judges rated half the items, compared their scores,
and then rated the second half of the items. Inter-
rater reliability was calculated on each of the three
measures using Cronbach?s ?. Cronbach?s ? is more
appropriate than Cohen?s ? because the ratings are
ordinal rather than categorical. A Cronbach?s ? for
each measure is presented in Table 5. Most of the
reliability scores in Table 5 are close to .70, which
is typically considered satisfactory reliability. How-
ever, reliability for accuracy was poor at ? = .41.
Scale Cronbach?s ?
Coverage .75
Accuracy .41
Pedagogy .71
Table 5: Inter-rater reliability
Computer Human
Scale Mean SD Mean SD
Coverage 2.47 .55 1.67 .82
Accuracy 1.87 .67 1.47 .55
Pedagogy 2.53 .74 1.83 .90
Table 6: Inter-rater reliability and mean ratings for com-
puter and human generated maps
Means and standard deviations were computed for
each measure per condition as shown in Table 6. In
general, the means for the computer generated maps
were in between 2 and 3 on the respective scales,
while the human generated maps were between 1
and 2. The outlier is accuracy for the computer gen-
erated maps, which was significantly higher than for
the other scales. However, since the inter-rater reli-
ability for this scale was relatively low, the mean for
accuracy requires closer analysis. Inspection of the
individual means for each judge revealed that judge
A had the same mean accuracy for both human and
computer generated maps, (M = 1.73), while judge
B rated the human maps higher (M = 1.2) and the
computer generated maps lower (M = 2). Thus
it is reasonable to use this more conservative lower
mean, (M = 2), as the estimate of accuracy for the
computer-generated concept maps.
117
Wilcoxon signed ranks tests pairing computer and
human generated maps based on their key terms
were computed for each of the three scales. There
was a significant effect for coverage, Z = 2.95,
p < .003, a significant effect for accuracy, Z =
2.13, p < .03, and a significant effect for pedagogy
Z = 2.46, p < .01.
Since the purpose of the computer generated maps
is to help students learn, pedagogy is clearly the
most important of the three scales. In order to assess
how the other scales were related to pedagogy, cor-
relations were calculated. Accuracy and pedagogy
were strongly correlated, r(28) = .57, p < .001.
Coverage and pedagogy were even more strongly
correlated, r(28) = .86, p < .001.
The strong relationship between coverage and
pedagogy suggests that the number of the triples in
the map might be strongly contributing to the judges
ratings. An inspection of the number of triples in the
human maps compared to the computer generated
maps reveals that there are approximately 3.5 times
as many triples in the human maps as the computer
generated maps. To further explore this relationship,
a linear regression was conducted using the log of
number of triples in each graph to predict the mean
pedagogy score for that graph. The log number of
triples in a graph significantly predicted pedagogy
ratings, b = ?.96, t(28) = ?3.47, p < .002. The
log number of triples in the graph explained a sig-
nificant proportion of variance in pedagogy ratings,
r2 = .30, F (1, 28) = 12.02, p < .002.
These results are encouraging on two fronts. First,
the computer generated maps are on average ?mostly
accurate.? Secondly, the computer generated maps
fare less well for coverage and pedagogy, but these
two scale are highly correlated, suggesting that
judges are using a criterion largely based on com-
pleteness when scoring maps. The strength of the
log number of triples in a graph as a predictor of ped-
agogy likewise indicates that increasing the number
of triples in each graph, which would require access
to a larger sample of texts on these topics, would
increase the pedagogical ratings for the computer
generated maps. However, while gaps in the maps
would be problematic if the students were using
the maps as an authoritative source for study, gaps
are perfectly acceptable for expert skeleton concept
maps.
5 Conclusion
In this paper we have presented a methodology for
creating expert skeleton concept maps from text-
books. Our comparative analysis using human gen-
erated concept maps as a gold standard suggests that
our maps are mostly accurate and are appropriate for
use as expert skeleton concept maps.
Ideally student concept maps that extend these
skeleton maps would be automatically scored and
feedback given as is already done in intelligent tu-
toring systems like Betty?s Brain and CIRCSIM Tu-
tor(Biswas et al, 2005; Evens et al, 2001). Both
of these systems use expert-generated maps as gold
standards by which to evaluate student maps. There-
fore automatic scoring of our expert skeleton con-
cept maps would require a more complete map in
the background.
In future work we will examine increasing the
number of knowledge sources to see if this will in-
crease the pedagogical value of the concept maps
and allow for automatic scoring. However, increas-
ing the knowledge sources will also likely lead to
an increase not only in total information but also in
redundant information. Thus extending this work
to include more knowledge sources will likely re-
quire incorporating techniques from the summariza-
tion and entailment literatures to remove redundant
information.
Acknowledgments
The research reported here was supported by the In-
stitute of Education Sciences, U.S. Department of
Education, through Grant R305A080594 and by the
National Science Foundation, through Grant BCS-
0826825, to the University of Memphis. The opin-
ions expressed are those of the authors and do not
represent views of the Institute or the U.S. Depart-
ment of Education or the National Science Founda-
tion.
References
Gautam Biswas, Daniel Schwartz, Krittaya Leelawong,
and Nancy Vye. 2005. Learning by teaching: A new
agent paradigm for educational software. Applied Ar-
tificial Intelligence, 19:363?392, March.
Peter A. Cohen, James A. Kulik, and Chen-Lin C. Ku-
lik. 1982. Educational outcomes of tutoring: a meta
118
analysis of findings. American Educational Research
Journal, 19:237?248.
LLC Cypress Curriculum Services. 2008. Tennessee
Gateway Coach, Biology. Triumph Learning, New
York, NY.
Ted Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Linguis-
tics, 19:61?74, March.
Martha W. Evens, Stefan Brandle, Ru-Charn Chang,
Reva Freedman, Micheal Glass, Yoon Hee Lee,
Leem Seop Shim, Chong Woo Woo, Yuemei Zhang,
Yujian Zhou, Joel A. Michael, and Allen A. Rovick.
2001. CIRCSIM-Tutor: An intelligent tutoring sys-
tem using natural language dialogue. In Proceedings
of the 12th Midwest AI and Cognitive Science Confer-
ence (MAICS 2001), pages 16?23, Oxford, OH.
Kathleen M. Fisher, James H. Wandersee, and David E.
Moody. 2000. Mapping biology knowledge. Kluwer
Academic Pub.
Kathleen Fisher. 2010. Biology Lessons at SDSU.
http://www.biologylessons.sdsu.edu, January.
Sallie E. Gordon, Kimberly A. Schmierer, and Richard T.
Gill. 1993. Conceptual graph analysis: Knowledge
acquisition for instructional system design. Human
Factors: The Journal of the Human Factors and Er-
gonomics Society, 35(3):459?481.
Arthur C. Graesser and Leslie C. Clark. 1985. Struc-
tures and procedures of implicit knowledge. Ablex,
Norwood, NJ.
Arthur C. Graesser and Stanley P. Franklin. 1990. Quest:
A cognitive model of question answering. Discourse
Processes, 13:279?303.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic-semantic analysis
with PropBank and NomBank. In CoNLL ?08:
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 183?187,
Morristown, NJ, USA. Association for Computational
Linguistics.
Mikel Larran?aga, Urko Rueda, Jon A. Elorriaga, and
Ana Arruarte Lasa. 2004. Acquisition of the domain
structure from document indexes using heuristic rea-
soning. In Intelligent Tutoring Systems, pages 175?
186.
Christopher D. Manning and Hinrich Schu?tze. 1999.
Foundations of Statistical Natural Language Process-
ing. MIT Press, Cambridge, MA.
Olena Medelyan, Eibe Frank, and Ian H. Witten.
2009. Human-competitive tagging using automatic
keyphrase extraction. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1318?1327, Singapore, August. As-
sociation for Computational Linguistics.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In A. Meyers, editor, HLT-NAACL 2004 Work-
shop: Frontiers in Corpus Annotation, pages 24?31,
Boston, Massachusetts, USA, May 2 - May 7. Associ-
ation for Computational Linguistics.
Kenneth R. Miller and Joseph S. Levine. 2002. Prentice
Hall Biology. Pearson Education, New Jersey.
Joel J. Mintzes, James H. Wandersee, and Joseph D. No-
vak. 2005. Assessing science understanding: A hu-
man constructivist view. Academic Press.
Roberto Navigli and Paola Velardi. 2008. From glos-
saries to ontologies: Extracting semantic structure
from textual definitions. In Proceeding of the 2008
conference on Ontology Learning and Population:
Bridging the Gap between Text and Knowledge, pages
71?87, Amsterdam, The Netherlands, The Nether-
lands. IOS Press.
John C. Nesbit and Olusola O. Adesope. 2006. Learning
with concept and knowledge maps: A meta-analysis.
Review of Educational Research, 76(3):413?448.
Joeseph D. Novak and Alberto J. Canas. 2006. The
theory underlying concept maps and how to construct
them. Technical report, Institute for Human and Ma-
chine Cognition, January.
Joeseph D. Novak. 1990. Concept mapping: A useful
tool for science education. Journal of Research in Sci-
ence Teaching, 27(10):937?49.
Angela O?Donnell, Donald Dansereau, and Richard Hall.
2002. Knowledge maps as scaffolds for cognitive pro-
cessing. Educational Psychology Review, 14:71?86.
10.1023/A:1013132527007.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An annotated corpus of
semantic roles. Comput. Linguist., 31(1):71?106.
John F. Sowa. 2007. Conceptual graphs. In
F. Van Harmelen, V. Lifschitz, and B. Porter, editors,
Handbook of knowledge representation, pages 213?
237. Elsevier Science, San Diego, USA.
John F. Sowa. 2009. Conceptual graphs for representing
conceptual structures. In P. Hitzler and H. Scharfe,
editors, Conceptual Structures in Practice, pages 101?
136. Chapman & Hall/CRC.
Alejandro Valerio and David B. Leake. 2008. Associ-
ating documents to concept maps in context. In A. J.
Canas, P. Reiska, M. Ahlberg, and J. D. Novak, editors,
Proceedings of the Third International Conference on
Concept Mapping.
Amal Zouaq and Roger Nkambou. 2009. Evaluating
the generation of domain ontologies in the knowledge
puzzle project. IEEE Trans. on Knowl. and Data Eng.,
21(11):1559?1572.
119
