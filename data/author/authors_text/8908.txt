Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 49?52,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Simulating the Behaviour of Older versus Younger Users
when Interacting with Spoken Dialogue Systems
Kallirroi Georgila, Maria Wolters and Johanna D. Moore
Human Communication Research Centre
University of Edinburgh
kgeorgil|mwolters|jmoore@inf.ed.ac.uk
Abstract
In this paper we build user simulations of
older and younger adults using a corpus of
interactions with a Wizard-of-Oz appointment
scheduling system. We measure the quality of
these models with standard metrics proposed
in the literature. Our results agree with predic-
tions based on statistical analysis of the cor-
pus and previous findings about the diversity
of older people?s behaviour. Furthermore, our
results show that these metrics can be a good
predictor of the behaviour of different types of
users, which provides evidence for the validity
of current user simulation evaluation metrics.
1 Introduction
Using machine learning to induce dialogue man-
agement policies requires large amounts of training
data, and thus it is typically not feasible to build
such models solely with data from real users. In-
stead, data from real users is used to build simulated
users (SUs), who then interact with the system as
often as needed. In order to learn good policies, the
behaviour of the SUs needs to cover the range of
variation seen in real users (Schatzmann et al, 2005;
Georgila et al, 2006). Furthermore, SUs are critical
for evaluating candidate dialogue policies.
To date, several techniques for building SUs have
been investigated and metrics for evaluating their
quality have been proposed (Schatzmann et al,
2005; Georgila et al, 2006). However, to our knowl-
edge, no one has tried to build user simulations
for different populations of real users and measure
whether results from evaluating the quality of those
simulations agree with what is known about those
particular types of real users, extracted from other
studies of those populations. This is presumably due
to the lack of corpora for different types of users.
In this paper we focus on the behaviour of older
vs. younger adults. Most of the work to date on di-
alogue systems focuses on young users. However,
as average life expectancy increases, it becomes in-
creasingly important to design dialogue systems in
such a way that they can accommodate older peo-
ple?s behaviour. Older people are a user group with
distinct needs and abilities (Czaja and Lee, 2007)
that present challenges for user modelling. To our
knowledge no one so far has built statistical user
simulation models for older people. The only sta-
tistical spoken dialogue system for older people we
are aware of is Nursebot, an early application of sta-
tistical methods (POMDPs) within the context of a
medication reminder system (Roy et al, 2000).
In this study, we build SUs for both younger and
older adults using n-grams. Our data comes from a
fully annotated corpus of 447 interactions of older
and younger users with a Wizard-of-Oz (WoZ) ap-
pointment scheduling system (Georgila et al, 2008).
We then evaluate these models using standard met-
rics (Schatzmann et al, 2005; Georgila et al, 2006)
and compare our findings with the results of statisti-
cal corpus analysis.
The novelty of our work lies in two areas. First,
to the best of our knowledge this is the first time that
statistical SUs have been built for the increasingly
important population of older users.
Secondly, a general (but as yet untested) assump-
tion in this field is that current SUs are ?enough like?
real users for training good policies, and that testing
system performance in simulated dialogues is an ac-
curate indication of how a system will perform with
human users. The validity of these assumptions is
49
a critically important open research question. Cur-
rently one of the standard methods for evaluating
the quality of a SU is to run a user simulation on
a real corpus and measure how often the action gen-
erated by the SU agrees with the action observed in
the corpus (Schatzmann et al, 2005; Georgila et al,
2006). This method can certainly give us some in-
sight into how strongly a SU resembles a real user,
but the validity of the metrics used remains an open
research problem. In this paper, we take this a step
further. We measure the quality of user simulation
models for both older and younger users, and show
that these metrics are a good predictor of the be-
haviour of those two user types.
The structure of the paper is as follows: In sec-
tion 2 we describe our data set. In section 3 we
discuss the differences between older and younger
users as measured in our corpus using standard sta-
tistical techniques. Then in section 4 we present our
user simulations. Finally in section 5 we present our
conclusions and propose future work.
2 The Corpus
The dialogue corpus which our simulations are
based on was collected during a controlled experi-
ment where we systematically varied: (1) the num-
ber of options that users were presented with (one
option, two options, four options); (2) the confirma-
tion strategy employed (explicit confirmation, im-
plicit confirmation, no confirmation). The combina-
tion of these 3? 3 design choices yielded 9 different
dialogue systems.
Participants were asked to schedule a health care
appointment with each of the 9 systems, yielding a
total of 9 dialogues per participant. System utter-
ances were generated using a simple template-based
algorithm and synthesised using the speech synthe-
sis system Cerevoice (Aylett et al, 2006), which has
been shown to be intelligible to older users (Wolters
et al, 2007). The human wizard took over the func-
tion of the speech recognition, language understand-
ing, and dialogue management components.
Each dialogue corresponded to a fixed schema:
First, users arranged to see a specific health care pro-
fessional, then they arranged a specific half-day, and
finally, a specific half-hour time slot on that half-day
was agreed. In a final step, the wizard confirmed the
appointment.
The full corpus consists of 447 dialogues; 3 di-
alogues were not recorded. A total of 50 partici-
pants were recruited, of which 26 were older (50?
85) and 24 were younger (20?30). The older users
contributed 232 dialogues, the younger ones 215.
Older and younger users were matched for level of
education and gender.
All dialogues were transcribed orthographically
and annotated with dialogue acts and dialogue con-
text information. Using a unique mapping, we as-
sociate each dialogue act with a ?speech act, task?
pair, where the speech act is task independent and
the task corresponds to the slot in focus (health pro-
fessional, half-day or time slot). For each dialogue,
five measures of dialogue quality were recorded: ob-
jective task completion, perceived task completion,
appointment recall, length (in turns), and detailed
user satisfaction ratings. A detailed description of
the corpus design, statistics, and annotation scheme
is provided in (Georgila et al, 2008).
Our analysis of the corpus shows that there are
clear differences in the way users interact with the
systems. Since it is these differences that good user
simulations need to capture, the most relevant find-
ings for the present study are summarised in the next
section.
3 Older vs. Younger Users
Since the user simulations (see section 4) are based
mainly on dialogue act annotations, we will use
speech act statistics to illustrate some key differ-
ences in behaviour between older and younger users.
User speech acts were grouped into four categories
that are relevant to dialogue management: speech
acts that result in grounding (ground), speech acts
that result in confirmations (confirm) (note, this
category overlaps with ground and occurs after the
system has explicitly or implicitly attempted to con-
firm the user?s response), speech acts that indicate
user initiative (init), and speech acts that indi-
cate social interaction with the system (social).
We also computed the average number of different
speech act types used, the average number of speech
act tokens, and the average token/type ratio per user.
Results are given in Table 1.
There are 28 distinct user speech acts (Georgila et
al., 2008). Older users not only produce more indi-
vidual speech acts, they also use a far richer variety
of speech acts, on average 14 out of 28 as opposed to
9 out of 28. The token/type ratio remains the same,
however. Although the absolute frequency of confir-
mation and grounding speech acts is approximately
50
Variable Older Younger Sig.
# speech act types 14 9 ***
# speech act tokens 126 73 ***
Sp. act tokens/types 8.7 8.5 n.s.
# Confirm 31 30 n.s.
% Confirm 28.3 41.5 ***
# Ground 33 30 n.s.
% Ground 29.4 41.7 ***
# Social 26 5 ***
% Social 17.9 5.3 ***
# Init 15 3 ***
% Init 9.0 3.4 **
Table 1: Behaviour of older vs. younger users. Numbers
are summed over all dialogues and divided by the num-
ber of users. *: p<0.01, **: p<0.005, ***: p<0.001 or
better.
the same for younger and older users, the relative
frequency of these types of speech acts is far lower
for older than for younger users, because older users
are far more likely to take initiative by providing ad-
ditional information to the system and speech acts
indicating social interaction. Based on this analysis
alone, we would predict that user simulations trained
on younger users only will not fare well when tested
on older users, because the behaviour of older users
is richer and more complex.
But do older and younger users constitute two
separate groups, or are there older users that be-
have like younger ones? In the first case, we can-
not use data from older people to create simulations
of younger users? behaviour. In the second case,
data from older users might be sufficient to approx-
imately cover the full range of behaviour we see in
the data. The boxplots given in Fig. 1 indicate that
the latter is in fact true. Even though the means
differ considerably between the two groups, older
users? behaviour shows much greater variation than
that of younger users. For example, for user initia-
tive, the main range of values seen for older users
includes the majority of values observed for younger
users.
4 User Simulations
We performed 5-fold cross validation ensuring that
there was no overlap in speakers between different
folds. Each user utterance corresponds to a user ac-
tion annotated as a list of ?speech act, task? pairs.
For example, the utterance ?I?d like to see the di-
abetes nurse on Thursday morning? could be an-
notated as [(accept info, hp), (provide info, half-
Figure 1: Relative frequency of (a) grounding and (b)
user initiative.
day)] or similarly, depending on the previous sys-
tem prompt. There are 389 distinct actions for older
people and 125 for younger people. The actions of
the younger people are a subset of the actions of the
older people.
We built n-grams of system and user actions with
n varying from 2 to 5. Given a history of system and
user actions (n-1 actions) the SU generates an action
based on a probability distribution learned from the
training data (Georgila et al, 2006). We tested four
values of n, 2, 3, 4, and 5. For reasons of space, we
only report results from 3-grams because they suffer
less from data sparsity than 4- and 5-grams and take
into account larger contexts than 2-grams. However,
results are similar for all values of n.
The actions generated by our SUs were compared
to the actions observed in the corpus using five met-
rics proposed in the literature (Schatzmann et al,
2005; Georgila et al, 2006): perplexity (PP), preci-
sion, recall, expected precision and expected recall.
While precision and recall are calculated based on
the most likely action at a given state, expected pre-
cision and expected recall take into account all pos-
sible user actions at a given state. Details are given
in (Georgila et al, 2006). In our cross-validation
experiments, we used three different sources for the
training and test sets: data from older users (O), data
51
PP Prec Rec ExpPrec ExpRec
O-O 18.1 42.8 39.8 56.0 49.4
Y-O 19.6 34.2 25.1 53.4 40.7
A-O 18.7 41.1 35.9 58.9 49.0
O-Y 5.7 44.8 60.6 66.3 73.4
Y-Y 3.7 50.5 54.1 73.1 70.4
A-Y 3.8 45.8 58.5 70.5 73.0
O-A 10.3 43.7 47.2 60.3 58.0
Y-A 9.3 40.3 33.3 62.0 51.5
A-A 9.3 43.2 43.4 63.9 57.9
Table 2: Results for 3-grams and different combinations
of training and test data. O: older users, Y: younger users,
A: all users.
from younger users (Y), and data from all users (A).
Our results are summarised in Table 2.
We find that models trained on younger users, but
tested on older users (Y-O) perform worse than mod-
els trained on older users / all users and tested on
older users (O-O, A-O). Thus, models of the be-
haviour of younger users cannot be used to simulate
older users. In addition, models which are trained
on older users tend to generalise better to the whole
data set (O-A) than models trained only on younger
users (Y-A). These results are in line with our sta-
tistical analysis, which showed that the behaviour
of younger users appears to be a subset of the be-
haviour of older users. All results are statistically
significant at p<0.05 or better.
5 Conclusions
In this paper we built user simulations for older
and younger adults and evaluated them using stan-
dard metrics. Our results suggest that SUs trained
on older people may also cover the behaviour of
younger users, but not vice versa. This finding
supports the principle of ?inclusive design? (Keates
and Clarkson, 2004): designers should consider a
wide range of users when developing a product for
general use. Furthermore, our results agree with
predictions based on statistical analysis of our cor-
pus. They are also in line with findings of tests of
deployed Interactive Voice Response systems with
younger and older users (Dulude, 2002), which
show the diversity of older people?s behaviour.
Therefore, we have shown that standard metrics for
evaluating SUs are a good predictor of the behaviour
of our two user types. Overall, the metrics we used
yielded a clear and consistent picture. Although our
result needs to be verified on similar corpora, it has
an important implication for corpus design. In order
to yield realistic models of user behaviour, we need
to gather less data from students, and more data from
older and middle-aged users.
In our future work, we will perform more detailed
statistical analyses of user behaviour. In particular,
we will analyse the effect of dialogue strategies on
behaviour, experiment with different Bayesian net-
work structures, and use the resulting user simula-
tions to learn dialogue strategies for both older and
younger users as another way for testing the accu-
racy of our user models and validating our results.
Acknowledgements
This research was supported by the Wellcome Trust VIP
grant and the Scottish Funding Council grant MATCH
(HR04016). We would like to thank Robert Logie and
Sarah MacPherson for contributing to the design of the
original experiment, Neil Mayo and Joe Eddy for coding
the Wizard-of-Oz interface, Vasilis Karaiskos and Matt
Watson for collecting the data, and Melissa Kronenthal
for transcribing the dialogues.
References
M. Aylett, C. Pidcock, and M.E. Fraser. 2006. The
Cerevoice Blizzard Entry 2006: A prototype database
unit selection engine. In Proc. BLIZZARD Challenge.
S. Czaja and C. Lee. 2007. The impact of aging on ac-
cess to technology. Universal Access in the Informa-
tion Society (UAIS), 5:341?349.
L. Dulude. 2002. Automated telephone answering sys-
tems and aging. Behaviour Information Technology,
21:171?184.
K. Georgila, J. Henderson, and O. Lemon. 2006. User
simulation for spoken dialogue systems: Learning and
evaluation. In Proc. Interspeech/ICSLP.
K. Georgila, M. Wolters, V. Karaiskos, M. Kronenthal,
R. Logie, N. Mayo, J. Moore, and M. Watson. 2008.
A fully annotated corpus for studying the effect of cog-
nitive ageing on users? interactions with spoken dia-
logue systems. In Proc. LREC.
S. Keates and J. Clarkson. 2004. Inclusive Design.
Springer, London.
N. Roy, J. Pineau, and S. Thrun. 2000. Spoken dialog
management for robots. In Proc. ACL.
J. Schatzmann, K. Georgila, and S. Young. 2005. Quan-
titative evaluation of user simulation techniques for
spoken dialogue systems. In Proc. SIGdial.
M. Wolters, P. Campbell, C. DePlacido, A. Liddell, and
D. Owens. 2007. Making synthetic speech accessi-
ble to older people. In Proc. Sixth ISCA Workshop on
Speech Synthesis, Bonn, Germany.
52
A Probabilistic Genre-Independent Model of Pronominalization 
Michael Strube 
European Media Laboratory GmbH 
Villa Bosch 
Schlol3-Wolfsbrunnenweg 33 
69118 Heidelberg, Germany 
Michael. St rube@eml .vil la-bosch. de 
Maria Wolters 
Inst. f. Kommunikationsforschung u. Phonetik 
Universitiit Bonn 
Poppelsdorfer Allee 47 
53115 Bonn, Germany 
wolters@ikp.uni -bonn.de 
Abstract 
Our aim in this paper is to identify genre- 
independent factors that influence the decision to 
pronominalize. Results based on the annotation of 
twelve texts from four genres show that only a few 
factors have a strong influence on pronominaliza- 
tion across genres, i.e. distance from last mention, 
agreement, and form of the antecedent. Finally, we 
describe aprobabilistic model of pronominalization 
derived from our data. 
1 Introduction 
Generating adequate referring expressions i an ac- 
tive research topic in Natural Language Generation. 
Adequate referring expressions are those that en- 
able the user to quickly and unambiguously identify 
the discourse ntity that the expression co-specifies 
with. In this paper, we concentrate on an important 
aspect of that question, which has received less at- 
tention than the question of anaphora resolution in 
discourse interpretation, i.e., when is it feasible to 
pronominalize? 
Our aim is to identify the central factors that in- 
fluence pronominalization across genres. Section 2 
motivates and presents the factors that were investi- 
gated in this study: distance from last mention, par- 
allelism, ambiguity, syntactic function, agreement, 
sortal class, syntactic function of the antecedent and 
form of the antecedent. Our analyses are based on 
a corpus of twelve texts from four different genres 
with a total of more than 24,000 words and 7126 
referring expressions (Section 3). The results of 
the statistical analyses are summarized in Section 
4. There are strong statistical associations between 
each of the factors and pronominalization. Only 
when we combine them into a probabilistic model 
we can identify those factors whose contribution 
is really important, i.e. distance from last mention, 
agreement, and to a certain degree form of the an- 
tecedent. Since these factors can be annotated tel- 
atively cheaply, we conclude that it is possible to 
develop reasonable statistical pronominalization al- 
gorithms. 
2 Factors in Pronoun Generation 
2.1 Previous Work 
Lately, a number of researchers have done corpus- 
based work on NP generation and pronoun resolu- 
tion, and a number of studies have found differences 
in the frequency of both personal and demonstrative 
pronouns across genres. However, none of these 
studies compares the influence of different factors 
on pronoun generation across genres. 
Recently, Poesio et al (1999) have described a
corpus-based approach to statistical NP generation. 
While they ask the same question as previous re- 
searchers (e.g. Dale (1992)), their methods differ 
from traditional work on NP generation. Poesio 
et al (1999) use two kinds of factors: (1) factors 
related to the NP under consideration such as agree- 
ment information, semantic factors, and discourse 
factors, and (2) factors related to the antecedent, 
such as animacy, clause type, thematic role, proxim- 
ity, etc. Poesio et al (1999) report hat they were not 
able to annotate many of these factors reliably. On 
the basis of these annotations, they constructed de- 
cision trees for predicting surface forms of referring 
expressions based on these factors - with good re- 
sults: all 28 personal pronouns in their corpus were 
generated correctly. Unfortunately, they do not eval- 
uate the contribution of each of these factors, so we 
do not know which ones are important. 
Work on corpus-based approaches to anaphora 
resolution is more numerous. Ge et al (1998) 
describe a supervised probabilistic pronoun resolu- 
tion algorithm which is based on complete syntac- 
tic information. The factors they use include dis- 
tance from last mention, syntactic function and con- 
text, agreement information, animacy of the refer- 
ent, a simplified notion of selectional restrictions, 
18 
Agree 
Syn 
Class 
SynAnte 
FormAnte 
Dist 
Dist4 
Par 
Ambig 
Agreement in person, gender, and number 
Syntactic function 
Sortal Class (cf. Tab. 2) 
Syntactic function of antecedent. 
"F" for first mention, "N" for deadend 
Form of antecedent (pers. pron., poss. 
pron., def. NP, indef. NP, proper name) 
Distance to last mention in units 
Dist reduced to 4 values (deadend, 
Dist=0, Dist= 1, Dist>=2) 
Parallelism (Syn=SynAnte) 
Number of competing discourse ntities 
Table 1: Overview of factors 
and the length of the coreference chain. Cardie & 
Wagstaff (1999) describe an unsupervised algorithm 
for noun phrase coreference resolution. Their fac- 
tors are taken from Ge et al (1998), with two excep- 
tions. First, they replace complete syntactic infor- 
mation with information about NP bracketing. Sec- 
ond, they use the sortal class of the referent which 
they determine on the basis of WordNet (Fellbaum, 
1998). 
There has been no comparison between corpus- 
based approaches for anaphora resolution and more 
traditional algorithms based on focusing (Sidner, 
1983) or centering (Grosz et al, 1995) except for 
Azzam et al (1998). However, their comparison 
is flawed by evaluating a syntax-based focus algo- 
rithm on the basis of insufficient syntactic informa- 
tion. For pronoun generation, the original centering 
model (Grosz et al, 1995) provides a rule which is 
supposed to decide whether a referring expression 
has to be realized as a pronoun. However, this rule 
applies only to the referring expression which is the 
backward-looking center (Cb) of the current utter- 
ance. With respect o all other referring expression 
in this utterance centering is underspecified. 
Yeh & Mellish (1997) propose a set of hand- 
crafted rules for the generation of anaphora (zero 
and personal pronouns, full NPs) in Chinese. How- 
ever, the factors which appear to be important in 
their evaluation are similar to factors described 
by authors mentioned above: distance, syntactic 
constraints on zero pronouns, discourse structure, 
salience and animacy of discourse ntities. 
2.2 Our Factors 
The factors we investigate in this paper only rely on 
annotations of NPs and their co-specification rela- 
tions. We did not add any discourse structural anno- 
tation, because (1) the texts are extracts from larger 
texts which are not available to us, and (2) we have 
not yet found a labelling scheme for discourse struc- 
ture that has an inter-coder reliability comparable to 
the MUC coreference annotation scheme. 
Based on our review of the literature and relevant 
work in linguistics (for sortal class, mainly Fraurud 
(1996) and Fellbaum (1998)), we have chosen the 
nine factors listed in Table 1. Methodologically, we 
distinguish two kinds of factors: 
NP-level factors are independent from co- 
specification relations. They depend on the 
semantics of the discourse entity or on discourse 
information supplied for the NP generation algo- 
rithm by the NLG system. Typical examples are 
NP agreement by gender, number, person and case, 
the syntactic function of the NP (subject, object, 
PP adjunct, other), the sortal class of the discourse 
entity to which an NP refers, discourse structure, or 
topicality of the discourse ntities. In this paper, we 
focus on the first three factors, agreement (Agree), 
syntactic function (Syn), and sortal class (Class). 
Since we are using syntactically annotated ata 
in the Penn Treebank-II format, the syntactic func- 
tion of an NP was derived from these annotations. 
Agreement for gender, number, and person was la- 
belled by hand. Since English has almost no nomi- 
nal case morphemes, case was not annotated. 
Sortal classes provide information about the dis- 
course entity that a referring expression evokes or 
accesses. The classes, summarized in Table 2, were 
derived from EuroWordNet BaseTypes (Vossen, 
1998) and are defined extensionally on the basis 
of WordNet synsets. Their selection was motivated 
by two main considerations: all classes hould oc- 
cur in all genres, and the number of classes hould 
be as small as possible in order to avoid problems 
with sparse data. Four classes, State, Event, Action, 
and Property, cover different types of situations, 
two cover spatiotemporal characteristics of situa- 
tions (Loc/Time). The four remaining classes cover 
the two dimensions "concrete vs. abstract (Con- 
cept)" and "human (Pers) vs. non-human (PhysObj) 
vs. institutionalised groups of humans (Group)". 
Since we are only interested in the decision 
whether to employ pronouns rather than full NPs 
and less in the form of the NP itself, and since our 
methodology is based on corpus annotation, we did 
not take into account more formal semantic ate- 
gories such as kinds vs. individuals. 
Co-specification-level factors depend on infor- 
mation about sequences of referring expressions 
19 
Person 
Group 
PhysObj 
Concept 
Loc 
Time 
Event 
Action 
State 
Property 
one or more human beings 
institutionalized group of human beings 
physical object 
abstract oncept 
geographical location 
date, time span 
sth. which takes place in space and time 
sth. which is done 
state of affairs, feeling . . . .  
characteristic orattribute of sth. 
Table 2: Overview of Sortal Classes with rough 
characterizations of relevant synsets 
which co-specify with each other. Such a sequence 
consists of all referring expressions that evoke or ac- 
cess the same discourse ntity. In this paper, we use 
the following factors from the literature: distance 
to last mention (Dist and Dist4), ambiguity (Am- 
big), parallelism (Par), form of the antecedent (For- 
mAnte), and syntactic function of the antecedent 
(SynAnte). We also distinguish between discourse 
entities that are only evoked once, deadend entities, 
and entities that are accessed repeatedly. 
Parallelism is defined on the basis of syntactic 
function: a referring expression and its antecedent 
are parallel if they have the same syntactic function. 
For calculating distance and ambiguity, we seg- 
mented the texts into major clause units (MCUs). 
Each MCU consists of a major clause C plus 
any subordinate clauses and any coordinated major 
clauses whose subject is the same as that of C and 
where that subject has been elided. 
Dist provides the number of MCUs between the 
current and the last previous mention of a discourse 
entity. When an entity is evoked for the first time, 
Dist is set to "D". Dist4 is derived from Dist by as- 
signing the fixed distance 2 to all referring expres- 
sions whose antecedent is more than 1 MCU away. 
Ambiguity is defined as the number of all discourse 
entities with the same agreement features that occur 
in the previous unit or in the same unit before the 
current referring expression. 
3 Data 
Our data consisted of twelve (plus two) texts from 
the Brown corpus and the corresponding part-of- 
speech and syntactic annotations from the Penn 
Treebank (LDC, 1995). The texts were selected 
because they contained relatively little or no direct 
speech; segments of direct speech pose problems for 
both pronoun resolution and generation because of 
the change in point of view. Morpho-syntactic in- 
formation such as markables, part-of-speech labels, 
grammatical role labels, and form of referring ex- 
pression were automatically extracted from the ex- 
isting Treebank annotations. 
The texts come from four different genres: Popu- 
lar Lore (CF), Belles Lettres (CG), Fiction/General 
(CK), and Fiction/Mystery (CL). The choice of 
genres was dictated by the availability of detailed 
Treebank-II parses. Table 3 shows that the distri- 
bution of referring expressions differs considerably 
between genres. 
The texts from the two non-narrative types, CF 
and CG, contain far more discourse ntities and 
far less pronouns than the narrative genres CK and 
CL. The high number of pronouns in CK and CL 
is partly due to the fact that in one text from each 
genre, we have a first person singular narrator. CK 
patterns with CF and CG in the average number 
of MCUs; the sentences in the sample from mys- 
tery fiction are shorter and arguably less complex. 
CL also has disproportionally few deadend refer- 
ents. The high percentage of deadend referents in 
CK is due to the fact that two of the texts deal with 
relationship between two people. These four dis- 
course referents account for the 4 longest corefer- 
ence chains in CK (85, 96, 109, and 127 mentions). 
Two annotators (the authors, both trained lin- 
guists), hand-labeled the texts with co-specification 
information based on the specifications for the Mes- 
sage Understanding Coreference task (Hirschman 
& Chinchor (1997); for theoretical reasons, we did 
not mark reflexive pronouns and appositives as co- 
specifying). The MCUs were labelled by the sec- 
ond author. All referring expressions were anno- 
tated with agreement and sortal class information. 
Labels were placed using the GUI-based annotation 
tool REFEREE (DeCristofaro et al, 1999). 
The annotators developed the Sortal Class anno- 
tation guidelines on the basis of two training texts. 
Then, both labellers annotated two texts from each 
genre independently (eight in total). These eight 
texts were used to determine the reliability of the 
sortal class coding scheme. Since sortal class an- 
notation is intrinsically hard, the annotators looked 
up the senses of the head noun of each referring NP 
that was not a pronoun or a proper name in Word- 
Net. Each sense was mapped irectly to one or more 
of the ten classes given in Table 2. The annotators 
then chose the adequate sense. 
The reliability of the annotations were measured 
9111 20
Genre 
CF 
CG 
CK 
CL 
words ref. expr. entities sequ.. MCUs % pron. %deadend med. len. 
6097 1725 1223 125 304 19.59% (1.8%, 0.3%, 58.3%) 89.78% 3 
6103 1707 1290 120 269 16.17% (9.8%, 1.1%, 4%) 90.70% 2 
6020 1848 1071 113 386 36.15% (19.5%, 1.2%, 56.1%) 89.45% 2 
6018 1846 954 170 477 35.64% (14.0%, 1.5%, 53.6%) 80.09% 4 
Table 3: Relevant quantitative characteristics of the texts. Average length: 2020 words, 120 MCUs. sequ.: 
number of sequences of co-specifying referring expressions. % deadend: percentage of discourse ntities 
mentioned only once. % pronouns: percentage ofall referring expressions realized as pronouns, in brackets: 
perc. of first person singular pronouns, perc. of second person singular pronouns, perc. of third person 
singular masculine and feminine pronouns, reed. len.: median length of sequences ofco-specifying referring 
expressions 
with Cohen's n (Cohen, 1960; Carletta, 1996). Co- 
hen (1960) shows that a n between 0.68 and 0.80 al- 
lows tentative conclusions, while e; > 0.80 indicates 
reliable annotations. For genres CF (n = 0.83), CK 
(n = 0.84) and CL (n = 0.83), the sortal class an- 
notations were indeed reliable, but not for genre CG 
(n = 0.63). Nevertheless, overall, the sortal class 
annotations were reliable (n ---- 0.8). Problems are 
mainly due to the abstract classes Concept, Action, 
Event, State, and Property. Abstract head nouns 
sometimes have several senses that fit the context 
almost equally well, but that lead to different sor- 
tal classes. Another problem is metaphorical usage. 
This explains the bad results for CG, which features 
many abstract discourse ntities. 
4 Towards a Probabilistic 
Genre- Independent  Model  
In this section, we investigate owhat extent the fac- 
tors proposed in section 2.2 influence the decision to 
prominalize. For the purpose of the statistical naly- 
sis, pronominalization is modelled by a feature Pro. 
For a given referring expression, that feature has the 
value "P" if the referring expression is a personal 
or a possessive pronoun, else "N". We model this 
variable with a binomial distribution. I 
4.1 How do the Factors Affect 
Pronominalization? 
First, we examine for all nine factors if there is a 
statistical ssociation between these factors and Pro. 
Standard non-parametric tests how a strong associ- 
ation between all nine factors and Pro. 2 This holds 
~For all statistical calculations and for the logistic regres- 
sion analyses reported below, we used R (Ihaka & Gentleman, 
1996). 
2We used the KruskaI-Wallis test for the ordinal Ambig 
variable and the X2-test for the other, nominal, variables. Since 
first mentions and deadends are coded by the character "D" in 
both for all referring expressions and for those that 
occur in sequences of co-specifying referring ex- 
pressions. All of the tests were significant at the 
p < 0.001-level, with the exception of Par: for ex- 
pressions that are part of co-specification sequences 
the effect of that factor is not significant. 
In the next analysis tep, we determine which of 
the feature values are associated isproportionally 
often with pronouns, and which values tend to be 
associated with full NPs. More specifically, we test 
for each feature-value pair if the pronominalization 
probability is significantly higher or lower than that 
computed over (a) the complete data set, (b) all re- 
ferring expressions in sequences of co-specifying 
referring expressions, (c) all third person referring 
expressions in sequences. Almost all feature values 
show highly significant effects for (a) and (b), but 
some of these effects vanish in condition (c). Be- 
low, we report on associations which are significant 
at p < 0.001 under all three conditions. 
Unsurprisingly, there is a strong effect of agree- 
ment values: NPs referring to the first and second 
person are always pronominalized, and third person 
masculine or feminine NPs, which can refer to per- 
sons, are pronominalized more frequently than third 
person neuter and third person plural. Pronouns are 
strongly preferred if the distance to the antecedent is 
0 or 1 MCUs. Referring expressions are more likely 
to be pronominalized in subject position than as a 
PP adjunct, and referring expressions with adjuncts 
as antecedents are also pronominalized less often 
than those with antecedents in subject or object po- 
sition. There is a clear preference for pronouns as 
possessive determiners, and referring expressions 
that co-specify with an antecedent possessive pro- 
noun are highly likely to be pronominalised. We 
both Dist and Dist4, both are treated as a categorical variable 
by R. For more on these tests, see (Agresti, 1990). 
"1,1 21
also notice strong genre-independent ffects of par- 
allelism. Although at first glance, Ambig appears to 
have a significant effect as well, (median ambiguity 
for nouns is 3, median ambiguity for pronouns 0), 
closer inspection reveals that this is mainly due to 
first and second person and third person masculine 
and feminine pronouns. 
The sortal classes show a number of interest- 
ing patterns (cf. Table 4). Not only do the classes 
differ in the percentage of deadend entities, there 
are also marked differences in pronominalizabil- 
ity. There appear to be three groups of sortal 
classes: Person/Group, with the lowest rate of dead- 
end entities and the highest percentage of pro- 
nouns - not only due to the first and second per- 
son personal pronouns- ,  Location/PhysObj, with 
roughly two thirds of all entities not in sequences 
and a significantly lower pronominalization rate, 
and Concept/Action/Event/Property/State/Concept, 
with over 80% deadend entities. Within this group, 
Action, Event, and Concept are pronominalized 
more frequently than State and Property. Time is the 
least frequently pronominalized class. An impor- 
tant reason for the difference between Loc and Time 
might be that Times are almost always referred back 
to by temporal adverbs, while locations, especially 
towns and countries, can also be accessed via third 
person neuter personal pronouns. 
Interactions between the factors and genre were 
examined by an analysis of deviance run on a fit- 
ted logistic regression model; significance was cal- 
culated using the F-test. All factors except for Par 
show strong (p < 0.001) interactions with Genre. 
In other words, the influence of all factors but paral- 
lelism on pronominalization is mediated by Genre. 
There are two main reasons for this effect: first, 
some genres contain far more first and second per- 
son personal pronouns, which adds to the weight of 
Agree, and second, texts which are about persons 
and the actions of persons, such as the texts in CK 
and CL, tend to use more pronouns than texts which 
are mainly argumentative or expository. 
4.2 Which Factors are Important? 
To separate the important from the unimportant fac- 
tors, many researchers use decision and regression 
trees, mostly the binary CART variant (Breiman 
et al, 1984). We use a different kind of model here, 
logistic regression, which is especially well suited 
for categorical data analysis (cf. eg. Agresti (1990) 
or Kessler et al (1997)). In this model, the value 
of the binary target variable is predicted by a lin- 
ear combination of the predictor variables. Vari- 
able weights indicate the importance of a variable 
for classification: the higher the absolute value of 
the weight, the more important i is. 
Logistic regression models are not only evaluated 
by their performance on training and test data. We 
could easily construct a perfect model of any train- 
ing data set with n variables, where n is the size of 
the data set. But we need models that are small, yet 
predict the target values well. A suitable criterion 
is the Akaike Information Criterion (AIC, Akaike 
(1974)), which punishes both models that do not fit 
the data well and models that have too many pa- 
rameters. The quality of a factor is judged by the 
amount of variation in the target variable that it ex- 
plains. Note that increased prediction accuracy does 
not necessarily mean an increase in the amount of 
variation explained. As the model itself is a contin- 
uous approximation of the categorical distinctions 
to be modelled, it may occur that the numerical vari- 
ation in the predictions decreases, but that this de- 
crease is lost when re-translating numerical predic- 
tions into categorical ones. 
The factors for our model were selected based on 
the following procedure: We start with a model that 
always predicts the most frequent class. We then de- 
termine which factor provides the greatest reduction 
in the AIC, add that factor to the model and retrain. 
This step is repeated until all factors have been used 
or adding another factor does not yield any signifi- 
cant improvements anymore. 3 
This procedure invariably yields the sequence 
Dist4, Agree, Class, FormAnte, Syn, SynAnte, Am- 
big, Par, both when training models on the complete 
data set and when training on a single genre. Inspec- 
tion of the AIC values suggests that parallelism is 
the least important factor, and does not improve the 
AIC significantly. Therefore, we will discard it from 
the outset. All other factors are maintained in the 
initial full model. This model is purely additive; it 
does not include interactions between factors. This 
approach allows us to filter out factors which only 
mediate the influence of other factors, but do not ex- 
ert any significant influence of their own. Note that 
this probabilistic model only provides a numerical 
description of how its factors affect pronominaliza- 
tion in our corpus. As such, it is not equivalent to 
a theoretical model, but rather provides data for fur- 
3We excluded Dist from this stepwise procedure, since the 
relevant information is covered already by Dist4, which fur- 
thermore has much fewer values. 
22
Class 
%deadend 
% pronouns 
% pron.(sequences) 
Act Concept Event Group Loc Pers PhysObj Prop State Time 
84.1 80.0 88.0 46.1 63.3 17.3 65.5 88.5 87.8 92.9 
6.2 8,5 6.0 28.4 5.7 63.4 10.2 2.5 3.2 0.3 
32.5 29.6 33.3 51.6 15.4 73.8 27.2 21.4 23,7 4.5 
Table 4: Results for Sortal Classes. % deadend: percentage of deadend entities; % pronouns: percent 
pronominalised, % pron. (sequences: percent pronominalised relative to all occurrences in co-specification 
sequences 
% correct 
AIC 
% variation 
CF CG CK CL all 
97.1 93.5 93.6 91.5 
324.7 654.8 786.1 904.0 
83.0 65.4 70.1 65.4 
93.1 
2685.8 
68.7 
Table 5: Quality of models fitted to each of the 
genre-specific corpora (CF, CG, CK, CL) and the 
complete data set (all). % correct: correctly pre- 
dicted pronominalization decition, AIC: Akaike In- 
formation Criterion, % variation: percentage of 
original variation in the data (as measured by de- 
viance) accounted for by the model 
ther theoretical interpretation. 
Results of a first evaluation of the full model 
are summarized in Table 5. The model can ex- 
plain more than two thirds of the variation in the 
complete data set and can predict pronominalization 
quite well on the data it was fitted on. The mat- 
ter becomes more interesting when we examine the 
genre-specific results. Although overall prediction 
performance r mains table, the model is obviously 
suited better to some genres than to others. The best 
results are obtained on CF, the worst on CL (mys- 
tery fiction). In the CL texts, MCUs are short, a 
third of all referring expressions are pronouns, there 
is no first person singular narrator, and most para- 
graphs which mention persons are about the inter- 
action between two persons. 
The Relative Importance of Factors. All val- 
ues of Dist4 have very strong weights in all mod- 
els; this is clearly the most important factor. The 
same goes for Agree, where the first and second per- 
son are strong signs of pronominalization, and, to a 
lesser degree, masculine and feminine third person 
singular. The most important distinction provided 
by Class appears to be that between Persons, non- 
Persons, and Times. This holds as well when the 
model is only trained on third person referring ex- 
pressions. For singular referring expressions, Per- 
sonhood information is reflected in gender, but not 
for plural referring expressions. Another important 
influence is the form of the antecedent. The syn- 
tactic function of the referring expression and of its 
antecedent are less important, as is ambiguity. 
In order to examine the importance of the fac- 
tors in more detail, we refitted the models on the 
complete data set while omitting one or more of the 
three central features Dist4, Agree, and Class. The 
results are summarized in Table 6. The most inter- 
esting finding is that even if we exclude all three 
factors, prediction accuracy only drops by 3.2%. 
This means that the remaining 4 factors also con- 
tain most of the relevant information, but that this 
information is coded more "efficiently", so to speak, 
in the first three. Speaking of these factors, ques- 
tions concerning the effect of sortal class remains. 
Remarkably enough, when sortal class is omitted, 
accuracy increases by 0.7%. The increase in A1C 
can be explained by a decrease in the amount of 
explained variation. A third result is that informa- 
tion about the form of the antecedent can substitute 
for distance information, if that information is miss- 
ing. Both variables code the crucial distinctions be- 
tween expressions that evoke entities and those that 
access evoked entities. Furthermore, a pronominal 
antecedent tends to occur at a distance of less than 2 
MCUs. The contribution of syntactic function re- 
mains stable and significant, albeit comparatively 
unimportant. 
Predictive Power: To evaluate the predictive 
power of the models computed so far, we determine 
the percentage of correctly predicted pronouns and 
NPs. The performance of the trained models was 
compared to two very simple algorithms: 
Algorithm A: Always choose the most frequent 
option (i.e. noun). 
Algorithm B: If the antecedent is in the same 
MCU, or if it is in the previous MCU and there 
is no ambiguity, choose a pronoun; else choose 
a noun. 
Table 7 summarises the results of the compari- 
son. To determine the overall predictive power of 
23 
excluded 
none 
Class 
Agree 
Dist4 
Dist4 + Class 
Dist4 + Agree 
Agree + Class 
Dist4 + Agree + Class 
AIC 
fit 
%correct 
2686 92.6 
2785 93.3 
2984 92.6 
3346 90.2 
3443 90.2 
3597 89.6 
3098 92.6 
3739 89.4 
~ explained variation 
Dist4 Agree Class PForm Syn PSyn Ambig 
54.4 21.1 5.7 3.8 2.3 0.5 l.l 
54.4 21.1 n.a. 4.7 2.8 0.5 1.1 
54.4 n.a. 14.3 6.2 2.7 0.6 1.1 
n.a. 35.8 6.1 32 3 0.8 0.I 
n.a. 35.8 n.a. 33.7 3.4 0.8 0.1 
n.a. n.a. 31.4 35.4 3.1 0.8 0.2 
54.4 n.a. n.a. 13.11 3.5 0.5 3.6 
n.a. n.a. n.a. 52.62 4 0.7 1.7 
Table 6: Effect of leaving out any one of the three most important factors on model fit. italics: significance 
is p < 0.0.5, for all other factors, p < 0.005 or better. 
test data set 
CF CG CK CL all 
Alg. A 80.4 83.8 63.8 65.4 72.8 
Alg. B 91.1 93.0 88.6 84.7 89.4 
Model 96.5 92.2 91.8 90.9 92.6 + 0.02 
w/oClass 96.8 92.4 91.7 90.7 93.0+ 0.01 
pothesize that the decrease in performance is mainly 
due to the model itself, not to the training data. The 
results presented in both Table 5 and 7 show that 
although the model we have found is not quite as 
genre-independent as we would want it to be, it pro- 
vides a reasonable fit to all the genres we examined. 
Table 7: Results of algorithms vs. models on test 
data in % correct prediction if referring expression 
is to be pronominalised or not. Setup for genres: 
model is trained on three genres, tested on the re- 
maining one 
the model, we used 10-fold cross-validation. Al- 
gorithm A always fares worst, while algorithm B, 
which is based mainly on distance, the strongest fac- 
tor in the model, performs quite well. Its overall 
performance is 3.2% below that of  the full model, 
and 3.6% below that of the full model without sor- 
tal class information. It even outperforms the mod- 
els on CG, which has the lowest percentage of Per- 
sons (12.9% vs. 35% for CF and 43.4% and 43.5% 
for CL and CK). For all other genres, the statistical 
models outperform the simple heuristics. Excluding 
sortal class information can boost prediction perfor- 
mance on unseen data by as much as 0.4% for the 
complete corpus. The apparent contradiction be- 
tween this finding and the results reported in the 
previous section can be explained if we consider 
that not only were some sortal classes comparatively 
rare in the data (Property, Event), but that our sortal 
class definition may still be too fine-grained. 
We evaluated the genre-independence of the 
model by training on three genres and testing on the 
fourth. The results show that the model fares quite 
well for genre CF, which is also the genre where the 
overall fit was best (see Table 5). We therefore hy- 
5 Future  Work  
We have described a probabilistic model of pronom- 
inalization that is able to correctly predict 93% of 
all pronouns in a corpus that consists of twelve texts 
from four different genres. Since the model was de- 
rived from a limited corpus and a limited number of 
genres, we cannot guarantee that our results are ap- 
plicable to all texts without modifications. But since 
its performance on our sample is consistently above 
90% correct, we are reasonably confident hat our 
main findings will hold for a wide variety of texts 
and text types. In particular, we isolated several fac- 
tors which are robust predictors of pronominaliza- 
tion across genres: distance from last mention and 
agreement, and to a certain extent he form of the 
antecedent, which appears to be a good substitute if
the other two factors are not available. All three fea- 
tures can be computed on the basis of a chunk parse, 
a rough morphosyntactic analysis of the resulting 
NPs, and co-specification sequences. In computa- 
tional terms, they are comparatively cheap. Large 
corpora can be annotated relatively quickly with this 
information, which can then be used for statistical 
pronoun generation. 
The comparatively expensive sortai class anno- 
tation, on the other hand, was not very important 
in the final model; in fact, prediction accuracy de- 
creased when sortal class was included. There 
are two main reasons for this: first, the proposed 
sortal class annotation scheme needs further work, 
24 
second, the relationship between sortal class and 
pronominalization may well be too intricate to be 
modelled by the factor Class alone. 
We set out to find a genre-independent model 
of pronominalization. The model we found per- 
forms quite well, but genre still considerably affects 
its performance. Where does the remaining, unex- 
plained variation come from? The variation might 
be just that - stylistic variation. It might stem from 
one of the traditional factors that we did not take 
into account here, such as thematic role. However, 
we suspect that the crucial factor at play here is dis- 
course structure (McCoy & Strube, 1999). 
Acknowledgements Work on this paper was be- 
gun while Michael Strube was a postdoctoral fellow 
at the Institute for Research in Cognitive Science, 
University of Pennsylvania, nd Maria Wolters vis- 
ited the Institute for a week in summer 1999. We 
would like to thank Kathleen McCoy, Jonathan De- 
Cristofaro, and the three anonymous reviewers for 
their comments on earlier stages of this work. 
References 
Agresti, Alan (1990). Categorical Data Analysis. New 
York, N.Y.: Wiley. 
Akaike, H. (1974). A new look at statistical model 
identification. 1EEE Transactions Automatic Control, 
19:716-722. 
Azzam, Saliha, Kevin Humphreys & Robert Gaizauskas 
(1998). Evaluating a focus-based approach to 
anaphora resolution. In Proceedings of the 17 th In- 
ternational Conference on Computational Linguistics 
and 36 th Annual Meeting of the Association for Com- 
putational Linguistics, Montr6al, Qu6bec, Canada, 
10--14 August 1998, pp. 74-78. 
Breiman, Leo, Jerome H. Friedman, Charles J. Stone & 
R.A. Olshen (1984). Classification and Regression 
Trees. Belmont, Cal.: Wadsworth and Brooks/Cole. 
Cardie, Claire & Kiri Wagstaff (1999). Noun phrase 
coreference as clustering. In Proceedings of the 1999 
SIGDAT Conference on Empirical Methods in Natural 
Language Processing and Very Large Corpora, Col- 
lege Park, Md., 21-22 June 1999, pp. 82-89. 
Carletta, Jean (1996). Assessing agreement on classifi- 
cation tasks: The kappa statistic. Computational Lin- 
guistics, 22(2):249-254. 
Cohen, Jacob (1960). A coefficient of agreement for 
nominal scales. Educational and Psychological Mea- 
surement, 20:37--46. 
Dale, Robert (1992). Generating Referring Expressions: 
Constructing Descriptions in a Domain of Objects 
and Processes. Cambridge, Mass.: MIT Press. 
DeCristofaro, Jonathan, Michael Strube & Kathleen E 
McCoy (1999). Building a tool for annotating ref- 
erence in discourse. In ACL '99 Workshop on the 
Relationship between Discourse~Dialogue Structure 
and Reference, University of Maryland, Maryland, 21 
June, 1999, pp. 54-62. 
Fellbaum, Christiane (Ed.) (1998). WordNet: An Elec- 
tronic Lexical Database. Cambridge, Mass.: MIT 
Press. 
Fraurud, Kari (1996). Cognitive ontology and NP form. 
In T. Fretheim & J. Gundel (Eds.), Reference and 
Referent Accessibility, pp. 65-87. Amsterdam, The 
Netherlands: Benjamins. 
Ge, Niyu, John Hale & Eugene Charniak (1998). A sta- 
tistical approach to anaphora resolution. In Proceed- 
ings of the Sixth Workshop on Very Large Corpora, 
Montr6al, Canada, pp. 161-170. 
Grosz, Barbara J., Aravind K. Joshi & Scott Weinstein 
(1995). Centering: A framework for modeling the lo- 
cal coherence of discourse. Computational Linguis- 
tics, 21 (2):203-225. 
Hirschman, Lynette & Nancy Chinchor (1997). MUC- 
7 Coreference Task Definition, http://www. 
muc. sais.  com/proceed ings / .  
Ihaka, Ross & Ross Gentleman (1996). R: A language 
for data analysis and graphics. Journal of Computa- 
tional and Graphical Statistics, 5:299-314. 
Kessler, Brett, Geoffrey Nunberg & Hinrich Schiitze 
(1997). Automatic detection of text genre. In Proceed- 
ings of the 35 th Annual Meeting of the Association 
for Computational Linguistics and of the 8 th Confer- 
ence of the European Chapter of the Association for 
Computational Linguistics, Madrid, Spain, 7-12 July 
1997, pp. 32-38. 
LDC (1995). Penn Treebank-H. Linguistic Data Consor- 
tium. University of Pennsylvania, Philadelphia, Penn. 
McCoy, Kathleen F. & Michael Strube (1999). Gener- 
ating anaphoric expressions: Pronoun or definite de- 
scription? In ACL '99 Workshop on the Relationship 
between Discourse/Dialogue Structure and Reference, 
University of Maryland, Maryland, 21 June, 1999, pp. 
63-71. 
Poesio, Massimo, Renate Henschel, Janet Hitzeman & 
Rodger Kibble (1999). Statistical NP generation: A
first report. In R. Kibble & K. van Deemter (Eds.), 
Proceedings of the Workshop on The Generation of 
Nominal Expressions, 11th European Summer School 
on Logic, Language, and Information, Utrecht, 9-13 
August 1999. 
Sidner, Candace L. (1983). Focusing in the compre- 
hension of definite anaphora. In M. Brady & R.C. 
Berwick (Eds.), Computational Models of Discourse, 
pp. 267-330. Cambridge, Mass.: MIT Press. 
Vossen, Piek (Ed.) (1998). EuroWordNet: A Multilingual 
Database with Lexical Semantic Networks. Dordrecht, 
The Netherlands: Kluwer. 
Yeh, Ching-Long & Chris Mellish (1997). An empiri- 
cal study on the generation of anaphora in Chinese. 
Computational Linguistics, 23( ! ): 169-190. 
25
Prosody and the Resolution of Pronominal Anaphora 
Maria Welters 
lnstitut l'i.ir Komlnunikationsforschung und 
Phonetik, Universitfit Bonn 
Poppelsdorfer Alice 47, D-53115 Bonn 
wolters@ikp.uni-bonn, de 
Donna K. Byron 
Department of Computer Science 
University of Rochester 
Re. Box 270226, Rochester, NY 14627 
dbyron@cs, rochester, edu 
Abstract 
In this paper, we investigate the acoustic prosodic mark- 
ing o\[" demonstrative and personal pronouns in task- 
oriented dialog. Although it has been hypothesized that 
acouslie marking affects pronoun resolution, we find 
flint {l~e prosodic information extracted from tile data is 
not sufficienl to predict antcceden! lype reliably. Inter- 
speaker variation accottnts for mt, ch of lhe prosodic vari- 
ation that we find in our data. We conclude that prosodic 
cues shot, ld be handled with care in robust, speaker- 
independenl dialog systems. 
1 I n t roduct ion  
Previous work on anaphora resolution has yieMed a rich 
basis of theories and heuristics for finding antecedenls. 
However, most research to date has neglecled an impor- 
tant potential cue that is only available in spoken data: 
prosody. Prosodic marking can be used to change the 
antecedent of a pronoun, as demonsh'ated by lifts clas- 
sic example from l.akoff ( 1971 ) (capitals indicalc a pitch 
accent): 
( I ) Johll i called J imj a Relmblican, then hei insulted 
himj. 
(2) Johlll called Jim./ a Republican, then lll{j ill- 
sulled lllMi. 
But exactly how the antecedent changes due to the 
prosodic marking on tile pronoun, and whefller this effect 
happens consistently, is an open question. If consislcnl 
elfecls do exisl, they would be useful for online pronoun 
inlerpretation i spoken dialog systems. 
Prosodic prominence directs tile attention of the lis- 
tener to what is important for understanding and inteF 
pretation. But how should this principle be applied when 
words that are normally not very prominent, such as 
prollouns, are accented? More generally, does acous- 
tic marking provide syslemalic ues to characteristics of
amecedents? IVlore specitically, does it imply that tile 
antecedent is "untmtml" in some wily'? These arc tile 
two hypofl/eses we investigate in this paper. ()ur data 
consists of 322 pronouns from a large corpus of spoma- 
neous lask-orientcd dialog, the TRAINS93 corpus (Hee~ 
man and Allen, 1995). This corpus allows us to study 
pronotms as they occur in spontaneous unscripted dis- 
COLlrse, al)d is erie of tile very few speech corporit o have 
been annotated with pronoun interpretation i fommtion. 
The remainder oF this paper is structured as follows: 
In Section 2, we SUllllllill'iZe relevant work on pl'OllOUn 
resolution and report on tile few proposals for integrat- 
ing prosody into pronoun resolution algorithms. Next, 
in Section 3, we present ile dialogs used for our study 
and the attributes awfilable in tile annotation data, while 
Section 4 describes file acoustic measures that were corn- 
puled automatically from the data. Section 5 explores 
whelher there are syslematic orrelations between these 
properties and tile acoustic measures fundamental fre- 
quency, duralion, and inlensity. For lhese measures, we 
find Ihat nlost correlations are in fact due to speaker vari- 
alien, and fl/a/ speakers differ greatly in their overall 
prosodic characleristics. Finally, we investigate whether 
it is possible to use Ihesc acoustic features to predict 
prope,'ties of tile antecedent using logistic regression. 
Again, we do not find acoustic features to be reliable 
prediclors for lhe l'catures of inleresl. Therefore, we con- 
chide in Section 6 lhat acoustic measures cannot be used 
in sl)eaker-independenl o ine anaphora resolution algo- 
rithms to predict lhe features under investigation here. 
2 Background and Related Work 
There is a rich literature ?)11 resolving personal pronouns. 
Many approaches arc based on a notion of attentional 
foctls. Entities in attentional focus are highly salient, 
and pronouns are assumed to refer to tile most salient 
entity in lhc discourse (el. (Brennan el al., 1987; Az- 
zam et ill., 1998; Strube, 1998)). Centering (Grosz et 
al., 1995) is a i}amework for predicting local attentional 
focus. It assumes that tile most salient entity from sen- 
tence ,3,,_\] that is realized in sentence ,5',, is most likely 
to be pronominalized in ,3,z. That entity is termed the Cb 
(backward-looking center) of sentence ,5',,. Finding ille 
preferred ranking criteria is an active area of research. 
Byron and Stem (1998) adapted this approach, which had 
previously been applied to text, for spoken dialogs, but 
wilh linfited st,ccess. 
\]n contrast to personal pronouns, demonstratives do 
not rely on calculalions of salience. In fact, Linde (1979) 
found lhat while it was preferred for entities within the 
919 
current local t'ocus, that was used for items outside the 
current focus of attention. Passonneau (1989) showed 
that personal and demonstrative pronouns are used in 
contrasting situations: personal pronouns are preferred 
when both the pronoun and its antecedent are in sub- 
ject position, while demonstrative pronouns are preferred 
when either the pronoun or its antecedent is not ill sub- 
ject position. She also found that personal pronouns tend 
to co-specify with pronouns or base noun phrases; the 
more clause- or seutence-likc the antecedent, he more 
likely the speaker is to choose a demonstrative pronoun. 
Pronoun resolution algoritlnns tend not to cover 
demonstratives. Notable exceptions are Webber's model 
for discotn'se deixis (Webbcr, 1991) and the model de- 
veloped for spoken dialog by Eekert and Strube (1999). 
This algorithm encompasses both personal and delnon- 
strative pronouns and exploits their contrastive usage pat- 
terns, relying on syntactic lues and verb subcategoriza- 
tions as input. Neither study investigated the intluence of 
prosodic prominence on resolution. 
Most previous work on prosody and pronotm resolu- 
tion has focussed on pitch accents and third person sin- 
gular pronouns that co-specify with persons. Nakatani 
(1997) examined the antecedents of personal pronouns 
in a 20-minute narrative monologue. She found that pro- 
nouns tend to be accented il' they occur in subject po- 
sition, and if the backward-looking center (Grosz et al, 
1995) was shifted to the referent of that pronoun. She 
then extended this result to a general theory of the in- 
teraction between l)rominencc and discourse structure. 
Cahu (1995)discusses accented prorJouns on the ba- 
sis of a theory about accentual correlates of salience. 
Kamcyama (1998) interprets a pitch accent on pronouns 
in the fl'amework of Ihe alternative semantics (Rooth, 
1992) theory o1' focus. She assumes that all potential an- 
tecedents are stored in a list. Pronouns arc then resolved 
to the most preferred antecedent on that list which is syn- 
tactically and semantically compatible with the pronoun. 
Preference is modeled by an ordering on the set ol' an- 
tecedents. An accent on lhe pronoun signals that pro- 
noun resolution should not be based on the default order- 
ing, where the default is computed by a nmnber of in- 
teracting syntactic, semantic, pragmatic, and attentional 
constraints. 
Compared to he and she, it and that lmve been some- 
what neglected. There are two reasons for this: First, it 
is not considered to be as accentable as he and she by 
native speakers of both British and American English, 
whereas that is more likely than it to beat" a pitch ac- 
cent. An informal study of the London-Lund corptts of 
spoken British English (Svartvik, 1990) confirmed that 
observation. Second, that fi'cquently does not lmve a 
co-specifying NP antecedent, and most research on co- 
speciticatiou has focussed on pronouns and NPs. Work 
on accented emonstratives and pronoun resolution is ex- 
tremely scarce. Pioneering studies were conducted by 
Ft'ethcim and his collaborators. They tested the effect of 
accented sentence-initial demonstratives that co-specify 
with the preceding sentence on the resolution of ambigu- 
ous personal pronouns, and found that the pronoun an- 
tecedents switched when the demonstrative was accented 
(Fretheim ct al., 1997). However, to otu" knowledge, 
there are no studies that compare the co-specification 
preferences of accented vs. unaccented demonstratives. 
3 The Corpus: TRAINS93 
Our data is taken from the TRAINS93 corpus of hunlun- 
human problem solving dialogs in the logistics phnuting 
domain. In these dialogs, one participant plays the role 
of the planning assistant and the other attempts to con- 
struct a plan for delivering specified cargo to its destina- 
tion. We used a subset of 18 TRAINS93 dialogs in which 
the referent and antecedent of third-person on-gendcrcd 
pronouns I had been attnotated in a previous study (By- 
ron and Allen, 1998). In the dialogs used for the present 
study, 322 pronouns (158 personal and 164 demonstra- 
live) have been annotated. Personal pronouns ill the di- 
alogs are it, its, itselJ; them, the3,, their and themselves. 
Demonstrative pronouns in the annotation data are that, 
this, these, those. There are live nmle and 11 fenmle 
speakers. One female speaker contributed 89 pronouns, 
two others produced more than 30 each (one female, one 
male), the rest is divided unevenly among tile remain- 
ing 13 speakers. The set of dialogs chosen for annota- 
tion intentionally included a variety of speakers o that 
no speaker's idiosyncratic discourse strategies would be 
prevalent ill the resulting data. 
Table 1 describes the attributes caplurcd for each 
pronoun. These features were chosen for tile annota- 
tion because many previous studies have shown them 
to be imporlant for pronoun resolution. Features ill- 
clude attributes of the pronoun, its antecedent ( he dis- 
cotu'se constituent Ihat previously triggered lhe refer- 
ent), and its referent (the entity that should be substi- 
tuted for the pronoun in a semantic representation of 
the sentence). Cb was annotated using Model3 from 
(Byron and Stent, 1998) with a linear model of dis- 
course  st ructure .  Note that anno la led  prononns  were  
not limited to those with NP antecedents, as is tile case 
with most other studies. In addition to NP antecedents, 
pronouns in this data set could have an antecedent of 
some other phrase or clause type, or no annomtablc an- 
tecedent at all. There are two categories of pronouns 
with no annotalable antecedent. Ill the simplest case, 
tim pronominal reference is the first mention of the ref- 
erent ill tile dialog. That happens when the referent is in- 
ferred liom the problem solving state. For example, af- 
ter" tile utterance send the engine to Coming  
and p ick  up the  boxcars ,  a new discourse n- 
I No gendcred entities exist in this co,'pus, so gendered pronouns 
wc,-c not inchtdcd. All dcmonst,'ativc pronouns were annolated; how- 
evcf, lhcre were only 5 occurrences of "this" in the selected ialogs, 
so eonstrasts between proxinml and distal dcmonslratives could not be 
studied. 
920 
Feature 11) l)escriplion 
I'RONTYPE Pronoutl Type 
I'RONSUB,I Pronoun is suljccl 
ANTI,\]I~()I{M Antecedenl form 
I)IST I)islance to antecedent 
ANTESUILI Antecedent is subjcc! 
CB Backward-looking center 
|trOllOU 11 
category 
Possible Values 
def= tile pronoun is one of {it, its, itself, them, dmy, thcin themselves} 
dcm = the inonoun is one of {that, this, these, fllose} 
Y = prOllOtltl subject of lllaill clause of its ullerance 
N : pronotm not subject of main clause 
I'I~,()NOUN = antecedent is pronoun 
NI' = antecedent is mse noun phrase 
N()N-NP = antecedent is other constituent, at most one utterance long 
NONE = pronotm is lit'st mention or antecedent length > one tttterance 
SAME = antecedent and pronoun in same utterance 
AI)J = antecedent and pronoun in adjacent utterances 
RI{MOTE = antecedent more than one utterance before pronoun 
Y = alSteccdel l l  subject o1' the lllain chmse of its tttterance 
N = antecedent not subject of a main clause 
Y = pronoun is Cb of its utterance 
N = pronoun is not Cb 
DIST  
cldj. I'CqllO\[? 
Table 1: The features avaihtble ill the annotation data set. 
ANTE ANTESUBJ  
NP/pmn.  non-NP  none yes no same 
75.9% 6.3% 17.8 % 37.3% 62.7% 29.1% 
28.0% 36.1t0% 36.0% 14.0% 86.11% 18.9% 
51.60{, 21.4% 27.0% 25.5% 74.5% 23.9% 
personal 33.5% 20.2% 
demonslrafive 29.9% 15.2% 
lolal 31.7% 17.7% 
3hble 2: Typical properties of antccedcnts lbr personal and demonst,'ative pronouns ill file corpus. All percentages 
are given relative to tile lolal ntnnber of pronouns in that category and rounded. Boldface: most frequent antecedent 
property. 
tity, tile train composed of tile engine and Ix)xcars, is 
awfilable for anaphoric reference. In the more subtle 
case, Ihe entity was built from a stretch (51" discourse 
longer than one utterance. In an effort to achieve an ac- 
ceptable level of inier-annotalor agreelnenl for the aw 
nohltion, the maxinmm size \[or a consfiluenl to serve as 
~tll ~ltllecedelll W\[lS de\[illed l(1 be OllC ullCl'~,lllCC, l)iscourse 
entities that are built fi'om longer she/chcs of lexl include 
objects uch as tile entire 131an or tile discourse itself, and 
such items are less reliable lo annotate. 
qaking the annotated dialogs as a whole, 21.4% of all 
prollouns have ;.l non-NP antecedent, and 27% do not 
have an mmolatal~le antecedent a  a11. qhble 2 shows thal 
tile default antecedenls o1' personal and denlonsh'alive 
pronouns follow the predictions of Schiffman (1985). 
The antecedent of personal pronouns i  most likely itself 
lo be a pronoun or a full NP, while demonstratives m'e 
most likely to have no antecedent, or if there is one, it is 
ntost likely to be a non-NR The main role of prosodic ill- 
lksr,nation is to help pronoun resolution algorithms iden- 
tify cases where flmse default predictions are false. 
4 Acoustic Prosodic Cues 
Our selection (51' acottstic measures covers three classic 
components of prosody: fundamental frequency (IV()), 
duration, and intensity (Lehiste, 1970). The relation- 
ship between those cues and prosodic pronlinencc has 
been demonstrated bye.g. (Fant and Kruckenberg, 1989; 
Heufl, 1999). Tile main correlate of English stress is F0, 
the second rues! imporlant is duration, and the least im- 
porlanl is inlensity (1,chisle, 1970). Therefore, we will 
pay more allelllioll lo F0 illeflsUl'eS. Although cxperi- 
menial results indicate flint 1;0 cues of pronlinencc can 
depend on the shape of file 1:0 conlour of the uucranec 
(c.f. (Gussenhoven cl al., 1997)), we do nol control for 
such illleraclions. \]llstead, we reslricl ourselves to cues 
that are easy to COnlpute fr(ml limiled dala, so that a run- 
ning spoken dialogue system might be able to compute 
them in real time. 
4.1 Acoustic Measures 
Duration: For duration, we found lhat 1he logarith- 
mic duration wllues a,'c nornmlly distributed, bolh pooled 
over all speakers and for lhoso speakers willl more than 
20 pronouns. Logariflmtic duration is also tile target vari- 
able of many duration models such as that of (van San- 
ten, 1992). We assume that speaker-related variation is 
covered by the w,'iance of lhis normal distribution; we 
can control for speaker effects by including a SPEAKER 
factor in our models. 
F0 variables: F0 was computed using the \]2ntropic 
ESPS Waves tool get_f0 with standard settings and a 
frame rate (51' 10 ms. All F0 wdues were transt'onned into 
lhe log-domain and then pooled imo mean, minimum, 
and maximum F0 values for each word and each utter- 
ance. This log donmin is well motiw~led psychoacousti- 
cally (Zwicker and lhtstl, 1990). F0 range was computed 
oil the values in tile log-domain. We assume lhat the Iog- 
m'ithm of F0 has a nomml distribution. Therefore, we 
921 
can nommlize for speaker-dependent differences in pitch 
range by using z-scores, and we can use standard statis- 
tical analysis methods uch as ANOVA. 
Intensity: Intensity is measured as the root-mean- 
square (RMS) of signal amplitudes. We measure 
RMS relative to a baseline as given by the formula 
log(l{MS/RMSb~olino). The baseline RMS was com- 
puted on the basis of a simple pause detection algorithm, 
which takes the first nmximum in the amplitude his- 
togram to be the average amplitude of background noise. 
The baseline RMS was slightly above that value. 
4.2 Inter-Speaker Differences 
Since we need to pool data from many different speak- 
ers, we qeed to control for inter-speaker differences. 
Tim number of pronouns we have fl'om each speaker 
varies between 1 for speaker GD and 86 for speaker 
CK. Speakers PH, male, and CK, female, are the 
only ones to lmve produced more than 15 personal 
pronouns and 15 demonstratives. In order to test 
whether the SPEAKER factor affects the choice be- 
tween personal pronouns and demonstratives, we tit- 
ted a logistic regression model with the target variable 
PRONTYPE (personal or demonstrative) and the predic- 
torsANTE, ANTESUBJ ,  DIST, REFCAT,  CBand 
SPEAKER (in this sequence). REFCAT is an additional 
variable that describes the senmntic category of a pro- 
noun's referent (eg. donmin objects vs. abstract enti- 
ties). Even though SPEAKER is the last factor in the 
model, an analysis of deviance shows a signilicant intlu- 
euce (p<0.005,F=2.51,df13).  A possible explanation 
for this is that some speakers prefer to use demonstra- 
tives in contexts where others would choose a personal 
pronotm, and vice versa, or perhaps the SPEAKER vari- 
able mediates the intluence of a far ,nore complex factor 
such as problem solving strategy. Resolving this ques- 
lion is beyond the scope of this paper. 
On the basis of F0, we can establish four groups of 
speakers: The first group consists of male speakers with 
a low mean F0 and a low F0 range. In the next group, 
we find both male and female speakers with a low mean 
F0, but a far higher range. Speaker PH belongs to this 
second group. Interestingly, for these speakers, the mean 
F0 on pronouns is lower titan for those of the first group. 
Groups 3 and 4 consist entirely of female speakers, with 
group 3 using a lower range than group 4. Speaker CK 
belongs to group 4. 
5 Exploring Prominent Pronouns 
If data about prosodic prominence is to be useful for pro- 
noun resolution, then there must be prosodic cues that 
carry information about properties of the antecedent. In 
this section, we investigate if there are such cues for the 
properties that we have available in the annotation data, 
defined in ~lable 1. More specitieally, we hypothesize 
that prosodic ues will be used if the antecedent is some- 
what unusual. For example, the results of Linde and 
Property 
ANTEFORFI  
D IST  
ANTESUBJ  
df 
all 
3 range 
3 none 
2 dur 
Dam Set 
peJw. dem. CK 
110110 none  none  
l lono none  none  
dur,  no l le  \])ors.: 
mean energy 
range 
Table 3: Significant Inlluences of Antecedent Proper- 
ties (p <0.05) on Prosodic Cues. inean=z-score mean 
F0, range=range of z-score F0, dur=logarithmic dura- 
tion, dem=demonstratives, pets=personal pronotms 
Passonneau would lead us to expect that personal pro- 
nouns with non-NP antecedents and demonstratives with 
NP and pronoun antecedents will be marked. Since the 
antecedents of pronouns tend to occur no more than 1-2 
clauses ago, we would also expect pronouns with more 
remote antecedents obe marked. A first qualitative look 
at the data suggets that even il' such these tendencies are 
present in the data, they might not turn out to be signifi- 
cant. For example, in Figure 1, the means of l zmeanf0  
behave roughly as predicted, but the variation is so large 
that these differences might well be due to chance. 
5.1 Correlations between Measures and Properties 
Next, we examine whether the measures delined in Sec- 
lion 4 correlate with any particular properties o1' the 
antecedent. More precisely, if a property is cued by 
some aspect ot' prosody (either duration, F0, or inten- 
sity), then the prosody of a pronoun depends to a cer- 
lain degree on its antecedent. In a statistical analysis, 
we should lind a significant effect of the relevant an- 
tecedent property on the prosodic measure. We selected 
ANOVA as our analysis method, because our prosodic 
target variables appear to have a normal distribution. For 
each of the antecedent features delined above, we ex- 
amined its inlluence on mean F0 (imeanf0), the z- 
score of mean F0 ( l zmeanf0) ,  the z-score of F0 range 
( l z rg f0 ) ,  logarithmic duration (dur) ,  and normalized 
energy (energy) .  In addition, we added the tactors, 
PRONTYPE and SPEAKER. 
Results: The results are summarized in Table 3. For 
i zmeanf0  and energy, the influence of SPEAKER 
is always considerable. There are also consistent ef- 
fects of the syntactic position of a pronoun: In general, 
demonstratives are shorter in subject position, and for 
CK, mean F0 on personal pronouns in subject position 
is higher than on non-subject ones (228 Hz vs. 190 Hz). 
But when we turn to the factors that interest us lnOSt, 
properties of the antecedent, we cannot lind any consis- 
tent correlates, although in ahnost every data set, there 
are some prosodic ues to ANTESUBJ for personal pro- 
nouns. But what these cues are may well depend on the 
speaker, as the results for CK show. Her pitch range on 
pronouns with a stdjcct antecedent is double the range 
on pronouns with an antecedent in non-su/lject position. 
922 
E 
P?l'SOll l l l  l i ro l lOUl lS 
T - - : -  
- -7  : : 
" L ~ 
o ~ 
o o o? 
o 
I I I ~ - -  
NI I lie alllt~ non-NP  I/lO 
"\['ype of AtllCci~dt~lll 
F~ 
7 
t ) I_qll O II,"i \[ fa t  i'? (~ P fo I IO I ln~ 
: 8 
NP IIO1|11\[~ no I I -NP  pfO 
Type of Alltcccdcnl 
y~ 
K 
8 
o 
- -  f 
t~on slkhi 
Perso l | l l l  P fo l lOt l l lS  
- \ ] - -  
1 ; 
o 
o 
o 
l - -  I 
II(II1L~ stilt 
Ai/tecctletll is Stlbjc'ct 
E 
o 
i 
Demonst ra t ive  P ronouns  
m 
Cl 
fi, 
8 
t - - \ ] - - -  i 
I|OII-SlIIj Ill)lie sulj 
AiItecctlct~l is Subject 
Figure 1: Distribution el: z-score of mean F0 for dilferellt values of ANTEFORM and ANTESUI3J 
Pronouns with subject antecedents are also considerably 
louder. All ill all, antecedent prol)ertics can only ac- 
COUllt for a very small percelltage of tile wtriatioll in 
these prosodic ues. Therefore, we should i~ot expect he 
prosodic ues to be slablc, robust indicators for predict- 
ins antecedent properlies ill spoken dialog systems. 
5.2  In ter -Speaker  Var ia t ion  
we have sccn that inter-speaker di ffcrcl~ces cxpl;~i n much 
of the variation in the prosodic measures. Table 4 gives 
an idea of the size and direction of these differences. 
On the complete data set, wc lilKl that personal pro- 
nouns are shorlor lhan demonslratives, they have a lower 
intensity and show a higher average 1;0 (3~tble 4). A 
closer examination reveals considerable inter-speaker 
variation in the data, illustrated in Table 4. CK is fairly 
ptototypical. PH barely shows the difference il~ F0, al~d 
for MF, the difference in intensity is actually reversed. 
MF also has rather shor! demonstratives. Such speaker- 
specilic wlriation callnot be eliminated by nomtalization. 
It has to be controlled for in the statistical lcsls. Dis- 
covering types of speakers is diflicult - two of the 15 
speakers, CK, and PH, con/ribute 48% of all pronouns. 
5.3  P red ic t ing  Proper t ies  o f  t i le  Antecedent  
Finally, we examine how much information prosodic 
cues yield about the ~tntecedent. For this purpose, we 
set till a prediction lask not unlike one that all actual 
NLU syslenl ~lces. The input variables arc the prosodic 
properties of the pronoun, whether the protloun is per- 
sonal or demonstrative (P\]R.ONTYPE), whether it is the 
subject (PRONSUBJ), and whether it is sentence-initial 
(PRONZNIT). From this, we now have to deduce l~roper - 
lies of thc antecedent: syntactic i'olc (ANTESrdBJ), fern1 
(ANTEFORM), and distance (DZST). For prediction, wc 
used logistic regression (Agresti, 1990). This has two ad- 
vantages: not only can wc compare how well the differ- 
cnt regression models lit the data, wc call also re-analyze 
the titled model to determine which factors have a signif- 
icant inlluence oll classiIication accuracy. 
Firsl, we conslrucl a model on the basis of 
PRONTYPE,  PRONSUBJ ,  and PRONIN IT .  Then, 
we conslruct a model with these three faclors plus 
SPEAKER.. finally, we train a model with PRONTYPE, 
923 
Speaker 
dis'c. 
all 156 Hz 
CK 188Hz 
PH 126 Hz 
MF 166 Hz 
mean F0 
pelw. dem. 
157 Hz 142 Hz 
208 Hz 187 Hz 
109 Hz 110 Hz 
184 Hz 182 Hz 
z-score mean 
pets. dent 
-0.04 -0.24 
0.31 0.00 
-0.43 -0.47 
0.32 0.26 
duration 
pelw. dem 
161 ms 206 ms 
151 ms 193 ms 
179ms 252 ms 
166 ms 164 ms 
intensity 
petw. dem 
2.36 2.38 
2.51 2.54 
2.57 2.84 
2.69 2.40 
Table 4: Inter-speaker variation in prosody, disc.: complete discourse. All speakers: 322 pronouns, CK: 41 personal, 
45 demonstrative, PH: 18 personal, 24 demonstrative, MF: 7 personal, 8 demonstrative 
PRONSUBJ, PRONINIT, SPEAKER and one of the 
three measures l zmeanf0 ,  dur ,  energy .  The mod- 
els are trained to predict whether there is an antecedent 
(task noAnte) ,  whether the antecedent is a non-NP 
(task nonNP), whether the antecedent is remote (task 
remote) ,  whether the antecedent is in subject position 
(task u j ante ) ,  and whether the antecedent is the current 
Cb (task cb). All models are computed over the full data 
set, because the data set for speaker CK is not suflicient 
? for estimating the regression coefficients. The models 
are then compared to see which step yielded a significant 
improvement: adding SPEAKER or adding the prosodic 
variable after we have accounted for SPEAKER variation. 
Results: The results arc summarized in Table 5. On 
all tasks except remote ,  PRONTYPE and PRONSUBJ 
performed well. Both features have ah'oady been shown 
to be reliable cnes for prononn resoluti(m (c.f. Sec- 
tion 2). On task cb, only PRONTYPE can explain a 
signilicant amount of wuiation. Models which include 
a speaker factor ahnost always fare better. In models 
without speaker information, F0-relaled measures yield 
a larger reduction in deviance than the duration measure. 
The reason for this is that the F0 measures preserve some 
information about the ditl'ercnt speaker strategies. Once 
SPEAKER has been included as well, only dur  leads 
to significant improvements on task nonNP (p<0.05). 
Both demonstratives and personal pronouns are shorter 
when the antecedent is a non-NR 
6 Conclusion and Outlook 
In this paper, we cxamincd patterns of acoustic prosodic 
highlighting of personal and demonstrative pronouns in 
a corpus of task-oriented spontaneous dialog. To our 
knowledge, this is the lirst comparative study of this 
kind. Wc used a straightforward, theory-neutral opera- 
tionalization of "prosodic highlighting" that does not de- 
pend on complex algorithms for F0 stylization or (focal) 
accent detection and is thus very easy to incorporate into 
any real-time spoken dialog system. We chose a spo- 
ken dialog corpus that includes demonstrativc pronouns 
because demonstratives are both a prominent feature of 
problem-solving dialogs and a sorely neglected lield of 
study. In particular, we asked two questions: 
Do Speakers Signal Antecedent Properties 
Acoustically? Based on our data, the answer to this 
question is: If they do,/hey do it in a highly idiosyncratic 
way. We cannot posit any safe generalizations over sev- 
eral speakers, and li"om the perspective of an NLP appli- 
cation, such generalizations might even be dangerous. In 
order to evaluate the impact of speaker strategies on the 
resolution of pronouns, we need more data - 150 to 200 
pronouns from 4-5 speakers each. Collecting this amount 
of data in a dedicated corpus is inefficient. Therefore, 
further acoustic investigations do not make much sense at 
this point; rather, the data should be examined carefully 
for tendencies which can form the basis for dedicated 
production and perception experiments which arc explic- 
itly designed for uncovering inter-speaker variation. 
Are Acoustic Features Useful for Pronoun 
Resolution? The answer is: probably not. At least for 
this corpus, we were not able to determine any numeri- 
cal heuristics that could be utilized to aid pronoun reso- 
lution. The logistic regression experiments show that on 
a speaker-independent basis, logarithmic duration might 
well be a reliable cue to certain aspects of a pronoun's 
antecedent. In order to incorporate prosodic cues into 
an actual algorithm, we will need more training material 
and a principled evaluation procedure. We will also need 
to take into account other influences, such as dialog acts 
and dialog structure. 
Acknowledgements. Wc would like to thank the three 
anonymous reviewers, Rebecca Passonneau, Lncicn 
Galescu, James Alhm, Michael Strube, Dictmar Lancd 
and Wolf gang Hess for their comments on earlier vet'- 
sions of this work. Donna K. Byron was funded by ONR 
research grant N00014-95-1-1088 and Columbia Univer- 
sity/NSF research grant OPG:1307. For all statistical 
analyses, wc used R (Ihaka and Gentleman, 1996). 
References 
A. Agresti. 1990. Categorical Data Analysis. John Wi- 
ley. 
S. Azzam, K. Humphreys, and R. Gaizauskas 1998. 
E?tending a Simple Co,'eference Algorithm with a 
Focusing Mechanism. In New Approaches to Dis- 
course Anaphora: Proceedings of the Second Collo- 
quium on Discoulwe Anaphora and Anaphor Resolu- 
tion (DAARC2), pages 15-27. 
S. Brennan, M. Friedman, and C. Pollard. 1987. A cen- 
tering approach to pronouns. In Proceedings of the 
25 th Ammal Meeting of the Association.fi~r Compu- 
tational Linguistics (ACL '87), pages 155-162. 
924 
\]hsk 
nonNP 
noAnte  
remote  
s jante  
cb 
significant illfluence 
PRONTYPE, PRONSUBJ, PRONINIT, dur 
PRONTYPE, PRONSUBJ, PRONINIT, SPEAKER. 
110110 
PRONTYPE, PRONSUBJ  
PRONTYPE,  SPEAKER 
Table 5: Perlbrmance of Reg,'ession Models on Tasks. Listed are factors which improve perfornmnce signilicantly 
(p < 0.05) 
D. Byron and J. Allen. 1998. Resolving demonstra- 
tive pronouns in the TRAtNS93 corpus. In New Ap- 
proaches to Discoume AnaFhora: Proceedings of 
the Second Colloquium on Discou/we AnaFhora attd 
Anaphor Resolution (DAARC2), pages 68 - 81. 
I). Byron and A. Stem. 1998. A preliminary model of 
centering in dialog. In Proceedings of tire 36 th An- 
total Meeting of the Association .for Computational 
Linguistics (A CL '98). 
3. Cahn. 1995. The effect of pitch accenting on pro- 
norm referent resolulion. In Proceedings of the 33 tj~ 
Ammal Meeting of" the Association./or Computatiomd 
?ingtdstics (ACL '95), pages 290-292. 
M. F, ckert and M. Strubc. 1999. Resolving discourse de- 
ictic anaphora in dialogs. 111 I~roceedings oJ"the 9 th 
Coq/'erence of the European Chapter of the Associa- 
tion Jbr Conq)utational Linguistics ( I';ACL '99). 
G. Fant and A. Kruckenberg. 1989. Preliminaries 
to tim study of Swedish prose reading and reading 
style. KT'II Speech 7)ansmission Laborato O, Quar- 
terly Progress and Status Report, 2: 1-83. 
T Frelheinl, W. wm 1)onmlelen, and K. 13orthen. 1997. 
lJnguislic constraints on relevance in reference reso- 
lution. In K. Singer, R. Eggert, and G. Anderson, edi- 
tors, CLS, volume 33, pages 99-113. 
B. Grosz, A. Joshi, and S. Weinstein. 1995. Cenlering: 
A framework for modeling the local coherence of dis- 
course. ComputationalLinguistics, 21(2):203-226. 
C. Gussenhoven, B.H. Repp, A. Rietveld, II. P, ump 
and J. Terken. 1997. The perceptual prominence of 
t'undanmntal frequency peaks. J. Acoust. Soc. Ame,:, 
102:3009-3022. 
P. Heeman and J. Allen. 1995. The Trains Spoken l)ia- 
log Corpus. CD-ROM, lJngt, istic Data Consortium. 
B. Heuft 1999. F, ine prominenzbasierte M thode zttt 
Prosodieanalyse trod -synthese. Peter Lang, Frank- 
furt. 
R. Ihaka and R. Gentlenmn (1996). R: A language for 
data analysis and graphics. Journal q/Co/nputational 
and Graphical Statistics, 5:299-314. 
M. Kameymna. 1998. Stressed Pronouns. 111 R Bosch, 
R. van Sandt, editors, The Focus Book, pages 89-112. 
()xford University Press, Oxford. 
G. lmkoff. 1971. P,'esuppositions and relative well- 
formedness. Iii Semantics: An InteMisciplinao, 
Reader in Philosophy, Linguistics, and l'sydtology, 
pages 329-340. Cambridge University Press. 
I. Lchiste. 1970. Suprasegmentals. Mrl" Press, Cam- 
bridge, Mass. 
C Limle. 1979. Focus o1' attention and the choice of 
pronouns in discourse. In qhhny Given, editor, &,max 
and Semantics 12: Discomwe arid ,S),ntax, New York. 
Academic Press. 
C. Nakatani. 1997. The Computational Processing of 
lntonatiolml Prominence: A Functional Prosody Per- 
spective. Ph.l). thesis, Harvard University. 
R. Passonneau. 1989. Gelling al discourse referents. In 
Proceedings of the 27 u' Ammal Meeting of the Associ- 
ation for Computational Linguistics (ACL '89), pages 
51-59. 
M. Rooth. 1992. A theory of focus interpretation. Natu- 
ral Language Semantics, 1:75-112. 
P,. Schiffnlan (Passo,mcau). 1985. DA'course con- 
strair2ts on 'it' and 'that': A study of la/tguage use in 
career-courtseling interviews. Ph.\]). thesis, Universily 
of Chicago. 
\]. Swulvik, editor. 1990. 7he London Coq)us o.fSl~oken 
English: Descril)tion and Reseamh. l.und Universily 
Press, Lund. 
M. Strube 1998. Never look back: An alternative to 
centering. In Proceedings (!/" the 36 th Amlual Meet- 
ing o.f the Association for Comtmtational Linguistics 
(ACL '98), pages 1251-1257. 
J. van Sanlen 1992 Contexlt, al effects on vowel dura- 
tion. Speech Co/mmmication, 11:513-546. 
B. Webber. 1991. Structure and ostension in the inter- 
pretation of discourse deixis. Language and Cognitive 
Processes, 6:107-135. 
E. Zwicker and H. Fastl 1990. Psychoacoustics. 
Springer, Be,'lin. 
925 
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 103?106,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Learning Dialogue Strategies from Older and Younger Simulated Users
Kallirroi Georgila
Institute for Creative Technologies
University of Southern California
Playa Vista, USA
kgeorgila@ict.usc.edu
Maria K. Wolters
School of Informatics
University of Edinburgh
Edinburgh, UK
maria.wolters@ed.ac.uk
Johanna D. Moore
School of Informatics
University of Edinburgh
Edinburgh, UK
J.Moore@ed.ac.uk
Abstract
Older adults are a challenging user group
because their behaviour can be highly vari-
able. To the best of our knowledge, this
is the first study where dialogue strategies
are learned and evaluated with both sim-
ulated younger users and simulated older
users. The simulated users were derived
from a corpus of interactions with a strict
system-initiative spoken dialogue system
(SDS). Learning from simulated younger
users leads to a policy which is close to
one of the dialogue strategies of the under-
lying SDS, while the simulated older users
allow us to learn more flexible dialogue
strategies that accommodate mixed initia-
tive. We conclude that simulated users are
a useful technique for modelling the be-
haviour of new user groups.
1 Introduction
State-of-the-art statistical approaches to dia-
logue management (Frampton and Lemon, 2006;
Williams and Young, 2007) rely on having ade-
quate training data. Dialogue strategies are typ-
ically inferred from data using Reinforcement
Learning (RL), which requires on the order of
thousands of dialogues to achieve good perfor-
mance. Therefore, it is no longer feasible to rely
on data collected with real users. Instead, training
data is generated through interactions of the sys-
tem with simulated users (SUs) (Georgila et al,
2006). In order to learn good policies, the be-
haviour of the SUs needs to cover the range of
variation seen in real users (Georgila et al, 2006;
Schatzmann et al, 2006). Furthermore, SUs are
critical for evaluating candidate dialogue policies.
To date, SUs have been used to learn dialogue
strategies for specific domains such as flight reser-
vation, restaurant recommendation, etc., and to
learn both how to collect information from the
user (Frampton and Lemon, 2006) as well as how
to present information to the user (Rieser and
Lemon, 2009; Janarthanam and Lemon, 2009).
In addition to covering different domains, SUs
should also be able to model relevant user at-
tributes (Schatzmann et al, 2006), such as coop-
erativeness vs. non-cooperativeness (Lo?pez-Co?zar
et al, 2006; Jung et al, 2009), or age (Georgila et
al., 2008). In this paper, we focus on user age.
As the proportion of older people in the popu-
lation increases, it becomes essential to make spo-
ken dialogue systems (SDS) easy to use for this
group of people. Only very few spoken dialogue
systems have been developed for older people (e.g.
Nursebot (Roy et al, 2000)), and we are aware of
no work on learning specific dialogue policies for
older people using SUs and RL.
Older people present special challenges for di-
alogue systems. While cognitive and perceptual
abilities generally decline with age, the spread of
ability in older people is far larger than in any
other segment of the population (Rabbitt and An-
derson, 2005). Older users may also use differ-
ent strategies for interacting with SDS. In our pre-
vious work on studying the interactions between
older and younger users and a simulated appoint-
ment scheduling SDS (Wolters et al, 2009b), we
found that some older users were very ?social?,
treating the system like a human, and failing to
adapt to the SDS?s system-initiative dialogue strat-
egy. A third of the older users, however, tended
to be more ?factual?, using short commands and
conforming to the system?s dialogue strategy. In
that, they were very similar to the younger users
(Wolters et al, 2009b).
In previous work (Georgila et al, 2008), we
successfully built SUs for both older and younger
103
adults from the corpus used by (Wolters et al,
2009b) and documented in (Georgila et al, 2010).
When we evaluated the SUs using metrics such as
precision and recall (Georgila et al, 2006; Schatz-
mann et al, 2006), we found that SUs trained on
older users? data can cover behaviour patterns typ-
ical of younger users, but not the opposite. The
behaviour of older people is too diverse to be cap-
tured by a SU trained on younger users? data. This
result agrees with the findings of (Wolters et al,
2009b; Georgila et al, 2010).
In this study, we take our work one step
further?we use the SUs developed in (Georgila
et al, 2008) to learn dialogue policies and evalu-
ate the resulting policies with data from both older
and younger users. Our work is important for two
reasons. First, to the best of our knowledge this
is the first time that people have used SUs and
RL to learn dialogue strategies for the increas-
ingly important population of older users. Sec-
ond, despite the fact that SUs are used for learn-
ing dialogue strategies it is not clear whether they
can learn policies that are appropriate for different
user populations. We show that SUs can be suc-
cessfully used to learn policies for older users that
are adapted to their specific patterns of behaviour,
even though these patterns are far more varied than
the behaviour patterns of younger users. This pro-
vides evidence for the validity of the user simula-
tion methodology for learning and evaluating dia-
logue strategies for different user populations.
The structure of the paper is as follows: In sec-
tion 2 we describe our data set, discuss the dif-
ferences between older and younger users as seen
in our corpus, and describe our user simulations.
In section 3, we present the results of our experi-
ments. Finally, in section 4 we present our conclu-
sions and propose future work.
2 The Corpus
In the original dialogue corpus, people were asked
to schedule health care appointments with 9 dif-
ferent simulated SDS in a Wizard-of-Oz setting.
The systems varied in the number of options pre-
sented at each stage of the dialogue (1, 2, 4),
and in the confirmation strategies used (explicit
confirmation, implicit confirmation, no confirma-
tion). System utterances were generated using
a simple template-based algorithm and synthe-
sised using a female Scottish English unit selec-
tion voice. The human Wizard took over the func-
tion of speech recognition (ASR), language under-
standing (NLU), and dialogue management com-
ponents. No ASR or NLU errors were simulated,
because having to deal with ASR and/or NLU er-
rors in addition to task completion would have in-
creased cognitive load (Wolters et al, 2009a).
The system (Wizard) followed a strict policy
which resulted in dialogues with a fixed schema:
First, users arranged to see a specific health care
professional, then they arranged a specific half-
day, and finally, a specific half-hour time slot on
that half-day was agreed. Users were not allowed
to skip any stage of the dialogue. This design en-
sured that all users were presented with the rele-
vant number of options and the relevant confirma-
tion strategy at least three times per dialogue. In a
final step, the Wizard confirmed the appointment.
The full corpus consists of 447 dialogues; 3 di-
alogues were not recorded. A total of 50 partici-
pants were recruited, of which 26 were older, aged
between 50 and 85 years, and 24 were younger,
aged between 18 and 30 years. The older users
contributed 232 dialogues, the younger ones 215.
Older and younger users were matched for level
of education and gender. All dialogues were tran-
scribed orthographically and annotated with dia-
logue acts and dialogue context information. Us-
ing a unique mapping, we associate each dialogue
act with a ?speech act, task? pair, where the speech
act is task independent and the task corresponds to
the slot in focus (health professional, half-day or
time slot). For example, ?confirm pos, hp? cor-
responds to positive explicit confirmation of the
health professional slot. For each dialogue, de-
tailed measures of dialogue quality were recorded:
objective task completion, perceived task comple-
tion, appointment recall, length (in turns), and ex-
tensive user satisfaction ratings. For a detailed dis-
cussion of the corpus, see (Georgila et al, 2010).
The choice of dialogue strategy did not affect
task completion and appointment recall, but had
significant effects on efficiency (Wolters et al,
2009a). Task completion and appointment recall
were the same for older and younger users, but
older users took more turns to complete the task
(Wolters et al, 2009a). Clear differences between
the two user groups emerge when we look at in-
teraction patterns in more detail (Wolters et al,
2009b; Georgila et al, 2010). Older people tend
to ?ground? information (using repetitions) and
take the initiative more than younger people. In
our corpus it was very common that the older per-
son would provide information about the half-day
and the time slot of the appointment before hav-
ing been asked by the system. However, due to the
104
Experiment 1 Experiment 2
slot filled +50 +50
appointment confirmed +200 +200
dialogue length -5 per turn -5 per turn
slot confirmed +100 not used
wrong order -500 not used
Table 1: Reward functions for the experiments.
strict policy of the Wizard, this information would
be ignored and the system would later ask for the
information that had already been provided.
In our SUs, each user utterance corresponds to a
user action described by a list of ?speech act, task?
pairs. There are 31 distinct system actions and 389
distinct actions for older users. Younger people
used a subset of 125 of the older users? actions.
Our SUs do not simulate ASR or NLU errors since
such errors were not simulated in the collection of
the corpus.
We built n-grams of system and user actions
with n varying from 2 to 5. Given a history of n-1
actions from system and user, the SU generates an
action based on a probability distribution learned
from the training data (Georgila et al, 2006). In
the present study, n was set to 3, which means that
each user action is predicted based on the previous
user action and the previous system action.
3 Learning Dialogue Strategies
We performed two experiments. In Experiment 1,
our goal was to learn the policy of the Wizard, i.e.
the strict system-initiative policy of requesting and
confirming information for each slot before mov-
ing to the next slot, in the following order: health
professional, half-day, time slot. In Experiment
2, our goal was to learn a more flexible policy that
could accommodate some degree of user initiative.
The reward functions for both experiments are
specified in Table 1; they are similar to the reward
functions used in the literature, e.g. (Frampton and
Lemon, 2006). Slots that have been filled success-
fully and confirmed appointments are rewarded,
while long dialogues are penalised. For Experi-
ment 1, policies were rewarded that filled slots in
the correct order and that confirmed each slot af-
ter it had been filled. A large penalty was imposed
when the policy deviated from the strict slot order
(health professional, half-day, time slot). For Ex-
periment 2, these constraints were removed. Slots
could be filled in any order. Confirmations were
not required because there was no speech act in
the corpus for confirming more than one slot at a
time.
In both experiments we used the SARSA-? al-
gorithm (Sutton and Barto, 1998) for RL. 30,000
iterations were used for learning the final pol-
icy for each condition. For each experiment,
we learned two policies, Policy-Old, which was
based on simulated older users, and Policy-Young,
which was based on simulated younger users.
The resulting policies were then tested on simu-
lated older users (Test-Old) and simulated younger
users (Test-Young). To have comparable results
between Experiment 1 and Experiment 2, dur-
ing testing we score our policies using the reward
function of Experiment 2. The best possible score
is 190, i.e. the user fills all the slots in one turn
and then confirms the appointment. (Note that +50
points are given when a slot is only filled, not con-
firmed too.) For each test condition, we gener-
ated 10,000 simulated dialogues. Overall scores
for each combination of policy and SU were es-
tablished using 5-fold cross-validation.
Our results are summarised in Figure 1. While
average rewards were not affected by policy
type (ANOVA, F (1, 68)=1, p=0.3) or training
data set (F (1, 185)=3, p=0.09), we found a very
strong interaction between policy type and data
set (F (1, 3098)=51, p=0.000). Learning with
simulated younger users yields better strict poli-
cies than learning with older users (Tukey?s Hon-
est Significant Difference Test, ?=20, 95% CI
= [11, 30], p=0.000), while learning with simu-
lated older users yields better flexible policies than
learning with younger users (?=15, 95% CI =
[6, 24], p=0.001). This is what we would expect
from our corpus analysis, since the interaction be-
haviour of older users is far more variable than that
of younger users (Wolters et al, 2009b; Georgila
et al, 2010).
The strict policy that was learned from sim-
ulated younger users was as follows, with only
slight variations: first request the type of health
professional, then implicitly confirm the health
professional and request the half-day slot, then im-
plicitly confirm the half-day slot and request the
time slot, and then confirm the appointment. The
strict policy learned from simulated older users
was similar, but less successful, because most
older users do not readily conform to the fixed
structure.
The flexible policy learned from simulated older
users takes into account initiative from the user
and does not always confirm. The score for the
flexible policy learned from simulated younger
users was relatively low, even though the resulting
105
Score
140150
160170
180190
Test?Old Test?Young
Policy?OldReward?Flex
Test?Old Test?Young
Policy?YoungReward?Flex
Policy?OldReward?Strict
140150
160170
180190
Policy?YoungReward?Strict
Figure 1: Mean scores for each combination of
reward function, training set, and test set (5-fold
cross-validation).
policy was very similar to the strict policy learned
from younger users (i.e. a sequence of informa-
tion requests and implicit confirmations), and even
though the behaviour of younger users is far more
predictable than the behaviour of older users. It
appears that the explicit penalty for violating the
order of slots is crucial for fully exploiting the pat-
terns in younger users? behaviour.
4 Conclusions
We have shown that SUs can be used to learn ap-
propriate policies for older adults, even though
their interaction behaviour is more complex and
diverse than that of younger adults. Crucially, sim-
ulated older users allowed us to learn a more flex-
ible version of the strict system-initiative dialogue
strategies that were used for creating the original
corpus of interactions. These results are consis-
tent with previous analyses of the original corpus
(Wolters et al, 2009b; Georgila et al, 2010) and
support the validity of the user simulation method-
ology for learning and evaluating dialogue strate-
gies.
In our future work, we will experiment with
more complex SUs, e.g. linear feature combina-
tion models (Georgila et al, 2006), and see if they
can be used to learn similar policies. We also plan
to study the effect of training and testing with dif-
ferent user simulation techniques, such as n-grams
versus linear feature combination models.
Acknowledgements
This research was partially supported by the MATCH project
(SHEFC-HR04016, http://www.match-project.
org.uk). Georgila is supported by the U.S. Army Research,
Development, and Engineering Command (RDECOM). The
content does not necessarily reflect the position or the policy
of the U.S. Government, and no official endorsement should
be inferred.
References
M. Frampton and O. Lemon. 2006. Learning more effective
dialogue strategies using limited dialogue move features.
In Proc. ACL.
K. Georgila, J. Henderson, and O. Lemon. 2006. User simu-
lation for spoken dialogue systems: Learning and evalua-
tion. In Proc. Interspeech.
K. Georgila, M. Wolters, and J. Moore. 2008. Simulating the
behaviour of older versus younger users. In Proc. ACL.
K. Georgila, Maria Wolters, J.D. Moore, and R.H. Logie.
2010. The MATCH corpus: A corpus of older and
younger users? interactions with spoken dialogue systems.
Language Resources and Evaluation, 44(3):221?261.
S. Janarthanam and O. Lemon. 2009. A two-tier user simula-
tion model for reinforcement learning of adaptive referring
expression generation policies. In Proc. SIGdial.
S. Jung, C. Lee, K. Kim, and G.G. Lee. 2009. Hybrid ap-
proach to user intention modeling for dialog simulation.
In Proc. ACL.
R. Lo?pez-Co?zar, Z. Callejas, and M. McTear. 2006. Testing
the performance of spoken dialogue systems by means of
an artificially simulated user. Artificial Intelligence Re-
view, 26(4):291?323.
P. Rabbitt and M.M. Anderson. 2005. The lacunae of
loss? Aging and the differentiation of human abilities.
In F.I. Craik and E. Bialystok, editors, Lifespan Cogni-
tion: Mechanisms of Change, chapter 23. Oxford Univer-
sity Press, New York, NY.
V. Rieser and O. Lemon. 2009. Natural language gener-
ation as planning under uncertainty for spoken dialogue
systems. In Proc. EACL.
N. Roy, J. Pineau, and S. Thrun. 2000. Spoken dialog man-
agement for robots. In Proc. ACL.
J. Schatzmann, K. Weilhammer, M. Stuttle, and S. Young.
2006. A survey of statistical user simulation tech-
niques for reinforcement-learning of dialogue manage-
ment strategies. Knowlege Engineering Review, 21(2):97?
126.
R.S. Sutton and A.G. Barto. 1998. Reinforcement Learning:
An Introduction. MIT Press.
J. Williams and S. Young. 2007. Partially observable Markov
decision processes for spoken dialog systems. Computer
Speech and Language, 21(2):393?422.
M. Wolters, K. Georgila, J.D. Moore, R.H. Logie, S.E.
MacPherson, and M. Watson. 2009a. Reducing work-
ing memory load in spoken dialogue systems. Interacting
with Computers, 21(4):276?287.
M. Wolters, K. Georgila, J.D. Moore, and S.E. MacPherson.
2009b. Being old doesn?t mean acting old: How older
users interact with spoken dialog systems. ACM Trans.
Accessible Computing, 2(1).
106
