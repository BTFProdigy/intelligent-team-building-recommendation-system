BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 46?53,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Recognizing Speculative Language in Biomedical Research Articles:
A Linguistically Motivated Perspective
Halil Kilicoglu and Sabine Bergler
Department of Computer Science and Software Engineering
Concordia University
Montreal, Quebec, Canada
{h_kilico, bergler}@cse.concordia.ca
Abstract
We explore a linguistically motivated ap-
proach to the problem of recognizing
speculative language (?hedging?) in bio-
medical research articles. We describe a
method, which draws on prior linguistic
work as well as existing lexical resources
and extends them by introducing syntactic
patterns and a simple weighting scheme to
estimate the speculation level of the sen-
tences. We show that speculative language
can be recognized successfully with such
an approach, discuss some shortcomings of
the method and point out future research
possibilities.
1 Introduction
Science involves making hypotheses, experiment-
ing, and reasoning to reach conclusions, which are
often tentative and provisional. Scientific writing,
particularly in biomedical research articles, reflects
this, as it is rich in speculative statements, also
known as hedges. Most text processing systems
ignore hedging and focus on factual language (as-
sertions). Although assertions, sometimes mere co-
occurrence of terms, are the focus of most infor-
mation extraction and text mining applications,
identifying hedged text is crucial, because hedging
alters, in some cases even reverses, factual state-
ments. For instance, the italicized fragment in ex-
ample (1) below implies a factual statement while
example (2) contains two hedging cues (indicate
and might), which render the factual proposition
speculative:
(1) Each empty cell indicates that the corre-
sponding TPase query was not used at the par-
ticular stage of PSI-BLAST analysis.
(2) These experiments indicated that the roX
genes might function as nuclear entry sites for
the assembly of the MSL proteins on the X
chromosome.
These examples not only illustrate the phe-
nomenon of hedging in the biomedical literature,
they also highlight some of the difficulties in rec-
ognizing hedges. The word indicate plays a differ-
ent role in each example, acting as a hedging cue
only in the second.
In recent years, there has been increasing inter-
est in the speculative aspect of biomedical lan-
guage (Light et al, 2004, Wilbur et al, 2006,
Medlock and Briscoe, 2007). In general, these
studies focus on issues regarding annotating
speculation and approach the problem of recog-
nizing speculation as a text classification problem,
using the well-known ?bag of words? method
(Light et al 2004, Medlock and Briscoe, 2007) or
simple substring matching (Light et al, 2004).
While both approaches perform reasonably well,
they do not take into account the more complex
and strategic ways hedging can occur in biomedi-
cal research articles. In example (3), hedging is
achieved with a combination of referring to ex-
perimental results (We ... show that ? indicating)
and the prepositional phrase to our knowledge:
(3) We further show that D-mib is specifically
required for Ser endocytosis and signaling
during wing development indicating for the
first time to our knowledge that endocytosis
regulates Ser signaling.
In this paper, we extend previous work through
linguistically motivated techniques. In particular,
we pay special attention to syntactic structures. We
46
address lexical hedges by drawing on a set of lexi-
cal hedging cues and expanding and refining it in a
semi-automatic manner to acquire a hedging dic-
tionary. To capture more complex strategic hedges,
we determine syntactic patterns that commonly act
as hedging indicators by analyzing a publicly
available hedge classification dataset.  Further-
more, recognizing that ?not all hedges are created
equal?, we use a weighting scheme, which also
takes into consideration the strengthening or weak-
ening effect of certain syntactic structures on lexi-
cal hedging cues. Our results demonstrate that
linguistic knowledge can be used effectively to
enhance the understanding of speculative language.
2 Related Work
The term hedging was first used in linguistic con-
text by Lakoff (1972). He proposed that natural
language sentences can be true or false to some
extent, contrary to the dominant truth-conditional
semantics paradigm of the era. He was mainly
concerned with how words and phrases, such as
mainly and rather, make sentences fuzzier or less
fuzzy.
Hyland (1998) provides one of the most com-
prehensive accounts of hedging in scientific arti-
cles in the linguistics literature. He views hedges
as polypragmatic devices with an array of purposes
such as weakening the force of statement, ex-
pressing deference to the reader and signaling un-
certainty. He proposes a fuzzy model, in which he
categorizes scientific hedges by their pragmatic
purpose, such as reliability hedges and reader-
oriented hedges. He also identifies the principal
syntactic realization devices for different types of
hedges, including epistemic verbs (verbs indicating
the speaker?s mode of knowing), adverbs and mo-
dal auxiliaries and presents the most frequently
used members of these types based on analysis of a
molecular biology article corpus.
Palmer (1986) identifies epistemic modality,
which expresses the speaker?s degree of commit-
ment to the truth of proposition and is closely
linked to hedging. He identifies three types of
epistemic modality: ?speculatives? express uncer-
tainty, ?deductives? indicate an inference from ob-
servable evidence, and ?assumptives? indicate
inference from what is generally known. He fo-
cuses mainly on the use of modal verbs in ex-
pressing various types of epistemic modality.
In their investigation of event recognition in
news text, Saur? et al (2006) address event modal-
ity at the lexical and syntactic level by means of
SLINKs (subordination links), some of which
(?modal?, ?evidential?) indicate hedges. They use
corpus-induced lexical knowledge from TimeBank
(Pustejovsky et al (2003)), standard linguistic
predicate classifications, and rely on a finite-state
syntactic module to identify subordinated events
based on the subcategorization properties of the
subordinating event.
DiMarco and Mercer (2004) study the intended
communicative purpose (dispute, confirmation, use
of materials, tools, etc.) of citations in scientific
text and show that hedging is used more frequently
in citation contexts.
In the medical field, Friedman et al (1994) dis-
cuss uncertainty in radiology reports and their
natural language processing system assigns one of
five levels of certainty to extracted findings.
Light et al (2004) explore issues with annotat-
ing speculative language in biomedicine and out-
line potential applications. They manually annotate
a corpus of approximately 2,000 sentences from
MEDLINE abstracts. Each sentence is annotated as
being definite, low speculative and highly specula-
tive. They experiment with simple substring
matching and a SVM classifier, which uses single
words as features. They obtain slightly better accu-
racy with simple substring matching suggesting
that more sophisticated linguistic knowledge may
play a significant role in identification of specula-
tive language. It is also worth noting that both
techniques yield better accuracy over full abstracts
than on the last two sentences of abstracts, in
which speculative language is found to be more
prevalent.
Medlock and Briscoe (2007) extend Light et
al.?s (2004) work, taking full-text articles into con-
sideration and applying a weakly supervised
learning model, which also uses single words as
features, to classify sentences as simply specula-
tive or non-speculative. They manually annotate a
test set and employ a probabilistic model for
training set acquisition using suggest and likely as
seed words. They use Light et al?s substring
matching as the baseline and improve to a re-
call/precision break-even point (BEP) of 0.76, us-
ing a SVM committee-based model from 0.60
recall/precision BEP of the baseline. They note that
their learning models are unsuccessful in identify-
47
ing assertive statements of knowledge paucity,
generally marked syntactically rather than lexi-
cally.
Wilbur et al (2006) suggest that factual infor-
mation mining is not sufficient and present an an-
notation scheme, in which they identify five
qualitative dimensions that characterize scientific
sentences: focus (generic, scientific, methodology),
evidence (E0-E3), certainty (0-3), polarity (posi-
tive, negative) and trend (+,-).  Certainty and evi-
dence dimensions, in particular, are interesting in
terms of hedging. They present this annotation
scheme as the basis for a corpus that will be used
to automatically classify biomedical text.
Discussion of hedging in Hyland (1998) pro-
vides the basic linguistic underpinnings of the
study presented here. Our goals are similar to those
outlined in the work of Light et al (2004) and
Medlock and Briscoe (2007); however, we propose
that a more linguistically oriented approach not
only could enhance recognizing speculation, but
would also bring us closer to characterizing the
semantics of speculative language. Some of the
work discussed above (in particular, Saur? et al
(2006) and Wilbur et al (2006)) will be relevant in
that regard.
3 Methods
To develop an automatic method to identify
speculative sentences, we first compiled a set of
core lexical surface realizations of hedging drawn
from Hyland (1998). Next, we augmented this set
by analyzing a corpus of 521 sentences, 213 of
which are speculative, and also noted certain syn-
tactic structures used for hedging. Furthermore, we
identified lexical cues and syntactic patterns that
strongly suggest non-speculative contexts (?un-
hedgers?). We then expanded and manually refined
the set of lexical hedging and ?unhedging? cues
using WordNet (Fellbaum, 1998) and the UMLS
SPECIALIST Lexicon (McCray et al, 1994).
Next, we quantified the strength of the hedging
cues and patterns through corpus analysis. Finally,
to recognize the syntactic patterns, we used the
Stanford Lexicalized Parser (Klein and Manning,
2003) and its dependency parse representation
(deMarneffe et al, 2006). We use weights assigned
to hedging cues to compute an overall hedging
score for each sentence.
To evaluate the effectiveness of our method, we
used basic information retrieval evaluation metrics:
precision, recall, accuracy and F
1
 score. In addi-
tion, we measure the recall/precision break-even
point (BEP), which indicates the point at which
precision and recall are equal, to provide a com-
parison to results previously reported. As baseline,
we use the substring matching method, described
in Light et al (2004) in addition to another sub-
string matching method, which uses terms ranked
in top 15 in Medlock and Briscoe (2007). To
measure the statistical significance of differences
between the performances of baseline and our
system, we used the binomial sign test.
4 Data Set
In our experiments, we use the publicly available
hedge classification dataset
1
, reported in Medlock
and Briscoe (2007). This dataset consists of a
manually annotated test set of 1537 sentences (380
speculative) extracted from six full-text articles on
Drosophila melanogaster (fruit-fly) and a training
set of 13,964 sentences (6423 speculative) auto-
matically induced using a probabilistic acquisition
model. A pool of 300,000 sentences randomly se-
lected from an archive of 5579 full-text articles
forms the basis for training data acquisition and
drives their weakly supervised hedge classification
approach.
While this probabilistic model for training data
acquisition is suitable for the type of weakly su-
pervised learning approach they describe, we find
that it may not be suitable as a fair data sample,
since the speculative instances overemphasize
certain hedging cues used as seed terms (suggest,
likely). On the other hand, the manually annotated
test set is valuable for our purposes. To train our
system, we (the first author) manually annotated a
separate training set of 521 sentences (213 specu-
lative) from the pool, using the annotation guide-
lines provided. Despite being admittedly small, the
training set seems to provide a good sample, as the
distribution of surface realization features (epis-
temic verbs (32%), adverbs (26%), adjectives
(19%), modal verbs (%21)) correspond roughly to
that presented in Hyland (1998).
5 Core Surface Realizations of Hedging
                                                           
1
 http://www.benmedlock.co.uk/hedgeclassif.html
48
Hyland (1998) provides the most comprehensive
account of surface realizations of hedging in sci-
entific articles, categorizing them into two classes:
lexical and non-lexical features. Lexical features
include modal auxiliaries (may and might being the
strongest indicators), epistemic verbs, adjectives,
adverbs and nouns. Some common examples of
these feature types are given in Table 1.
Feature Type Examples
Modal auxiliaries may, might, could, would,
should
Epistemic judgment
verbs
suggest, indicate, specu-
late, believe, assume
Epistemic evidential
verbs
appear, seem
Epistemic deductive
verbs
conclude, infer, deduce
Epistemic adjectives likely, probable, possible
Epistemic adverbs probably, possibly, per-
haps, generally
Epistemic nouns possibility, suggestion
Table 1. Lexical surface features of hedging
Non-lexical hedges usually include reference
to limiting experimental conditions, reference to a
model or theory or admission to a lack of knowl-
edge. Their surface realizations typically go be-
yond words and even phrases. An example is given
in sentence (4), with hedging cues italicized.
(4) Whereas much attention has focused on eluci-
dating basic mechanisms governing axon de-
velopment, relatively little is known about the
genetic programs required for the establish-
ment of dendrite arborization patterns that are
hallmarks of distinct neuronal types.
While lexical features can arguably be exploited
effectively by machine learning approaches, auto-
matic identification of non-lexical hedges auto-
matically seems to require syntactic and, in some
cases, semantic analysis of the text.
Our first step was to expand on the core lexical
surface realizations identified by Hyland (1998).
6 Expansion of Lexical Hedging Cues
Epistemic verbs, adjectives, adverbs and nouns
provide the bulk of the hedging cues. Although
epistemic features are commonly referred to and
analyzed in the linguistics literature and various
widely used lexicons exist that classify different
part-of-speech (e.g., VerbNet (Kipper Schuler,
2005) for verb classes), we are unaware of any
such comprehensive classification based on epis-
temological status of the words. We explore in-
ducing such a lexicon from the core lexical
examples identified in Hyland (1998) (a total of 63
hedging cues) and expanding it semi-automatically
using two lexicons: WordNet (Fellbaum, 1998)
and UMLS SPECIALIST Lexicon (McCray,
1994).
We first extracted synonyms for each epistemic
term in our list using WordNet synsets. We then
removed those synonyms that did not occur in our
pool of sentences, since they are likely to be very
uncommon words in scientific articles. Expanding
epistemic verbs is somewhat more involved than
expanding other epistemic types, as they tend to
have more synsets, indicating a greater degree of
word sense ambiguity (assume has 9 synsets).
Based on the observation that an epistemic verb
taking a clausal complement marked with that is a
very strong indication of hedging, we only consid-
ered verb senses which subcategorize for a that
complement. Expansion via WordNet resulted in
66 additional lexical features.
Next, we considered the case of nominaliza-
tions. Again, based on corpus analysis, we noted
that nominalizations of epistemic verbs and adjec-
tives are a common and effective means of hedging
in molecular biology articles. The UMLS
SPECIALIST Lexicon provides syntactic informa-
tion, including nominalizations, for biomedical as
well as general English terms. We extracted the
nominalizations of words in our expanded diction-
ary of epistemic verbs and adjectives from UMLS
SPECIALIST Lexicon and discarded those that do
not occur in our pool of sentences, resulting in an
additional 48 terms. Additional 5 lexical hedging
cues (e.g., tend, support) were identified via man-
ual corpus analysis and further expanded using the
methodology described above.
An interesting class of cues are terms expressing
strong certainty (?unhedgers?). Used within the
scope of negation, these terms suggest hedging,
while in the absence of negation they strongly sug-
gest a non-speculative context. Examples of these
include verbs indicating certainty, such as know,
demonstrate, prove and show, and adjectives, such
as clear. These features were also added to the
dictionary and used together with other surface
49
cues to recognize speculative sentences. The
hedging dictionary contains a total of 190 features.
7 Quantifying Hedging Strength
It is clear that not all hedging devices are equally
strong and that the choice of hedging device affects
the strength of the speculation. However, deter-
mining the strength of a hedging device is not
trivial. The fuzzy pragmatic model proposed by
Hyland (1998) employs general descriptive terms
such as ?strong? and ?weak? when discussing par-
ticular cases of hedging and avoids the need for
precise quantification. Light et al (2004) report
low inter-annotator agreement in distinguishing
low speculative sentences from highly speculative
ones. From a computational perspective, it would
be useful to quantify hedging strength to determine
the confidence of the author in his or her proposi-
tion.
As a first step in accommodating noticeable dif-
ferences in strengths of hedging features, we as-
signed weights (1 to 5, 1 representing the lowest
hedging strength and 5 the highest) to all hedging
features in our dictionary. Core features were as-
signed weights based on the discussion in Hyland
(1998). For instance, he identifies modal auxilia-
ries, may and might, as the prototypical hedging
devices, and they were given weights of 5. On the
other hand, modal auxiliaries commonly used in
non-epistemic contexts (would, could) were as-
signed a lower weight of 3. Though not as strong
as may and might, core epistemic verbs and ad-
verbs are generally good hedging cues and there-
fore were assigned weights of 4. Core epistemic
adjectives and nouns often co-occur with other
syntactic features to act as strong hedging cues and
were assigned weights of 3. Terms added to the
dictionary via expansion were assigned a weight
one less than their seed terms. For instance, the
nominalization supposition has weight 2, since it is
expanded from the verb suppose (weight 3), which
is further expanded from its synonym speculate
(weight 4), a core epistemic verb. The reduction in
weights of certain hedging cues reflects their pe-
ripheral nature in hedging.
Hyland (1998) notes that writers tend to com-
bine hedges (?harmonic combinations?) and sug-
gests the possibility of constructing scales of
certainty and tentativeness from these combina-
tions. In a similar vein, we accumulate the weights
of the hedging features found in a sentence and
assign an overall hedging score to each sentence.
8 The Role of Syntax
Corpus analysis shows that various syntactic de-
vices play a prominent role in hedging, both as
hedging cues and for strengthening or weakening
effects. For instance, while some epistemic verbs
do not act as hedging cues (or may be weak hedg-
ing cues) when used alone, together with a that
complement or an infinitival clause, they are good
indicators of hedging. A good example is appear,
which often occurs in molecular biology articles
with its ?come into sight? meaning (5) and be-
comes a good hedging cue when it takes an infini-
tival complement (6):
(5) The linearity of the ommatidial arrangement
was disrupted and numerous gaps appeared
between ommatidia arrow.
(6) In these data a substantial fraction of both si-
lent and replacement DNA mutations appear to
affect fitness.
On the other hand, as discussed above, words
expressing strong certainty (?unhedgers?) are good
indicators of hedging when negated, and strongly
non-speculative otherwise.
We examined the training set and identified the
most salient syntactic patterns that play a role in
hedging. A syntactic pattern, or lack thereof, af-
fects the overall score assigned to a hedging cue; a
strengthening syntactic pattern will increase the
overall score contributed by the cue, while a weak-
ening pattern will decrease it. For instance, in sen-
tence (5) above, the absence of the infinitival
complement will reduce the score contribution of
appear by 1, resulting in a score of 3 instead of 4.
On the other hand, that appear takes an infinitival
clause in example (6) will increase the score con-
tribution of appear by 1. All score contributions of
a sentence add up to its hedging score.
A purely syntactic case is that of whether (if).
Despite being a conjunction, it seems to act as a
hedging cue when it introduces a clausal comple-
ment regardless of existence of any other hedging
cue from the hedging dictionary. The basic syntac-
tic patterns we identified and implemented and
their effect on the overall hedging score are given
in Table 2.
50
To obtain the syntactic structures of sentences,
we used the statistical Stanford Lexicalized Parser
(Klein and Manning, 2003), which provides a full
parse tree, in addition to part-of-speech tagging
based on the Penn Treebank tagset. A particularly
useful feature of the Stanford Lexicalized Parser is
typed dependency parses extracted from phrase
structure parses (deMarneffe, et al (2006)). We
use these typed dependency parses to identify
clausal complements, infinitival clauses and nega-
tion. For instance, the following two dependency
relations indicate a clausal complement marked
with that and identify the second syntactic pattern
in Table 2.
ccomp(<EPISTEMIC VERB>,<VB>)
complm(<VB>,that)
In these relations, ccomp stands for clausal
complement with internal subject and complm
stands for complementizer. VB indicates any verb.
Syntactic Pattern Effect
on Score
+1
+2
<EPISTEMIC VERB> to(inf) VB
<EPISTEMIC VERB> that(comp) VB
Otherwise
-1
+2<EPISTEMIC NOUN> followed by
that(comp)
Otherwise
-1
not <UNHEDGING VERB> +1
no| not <UNHEDGING NOUN> +2
no| not immediately followed by
<UNHEDGING ADVERB>
+1
no| not immediately followed by
<UNHEDGING ADJECTIVE>
+1
whether| if in a clausal complement
context
3
Table 2. Syntactic patterns and their effect on the over-
all hedging score.
9 Baseline
For our experiments, we used two baselines. First,
we used the substring matching method reported in
Light et al (2004), which labels sentences con-
taining one of more of the following as specula-
tive: suggest, potential, likely, may, at least, in
part, possibl, further investigation, unlikely, puta-
tive, insights, point toward, promise and propose
(Baseline1). Secondly, we used the top 15 ranked
term features determined using P(spec|x
j
) in train-
ing and classification models (at smoothing pa-
rameter 
? 
?=5) reported in Medlock and Briscoe
(2007): suggest, likely, may, might, seems, Taken,
suggests, probably, Together, suggesting, possibly,
suggested, findings, observations, Given. Our sec-
ond baseline uses the substring matching method
with these features (Baseline2).
10 Results
The evaluation results obtained using the baseline
methods are given in Table 3.
Method Precision Recall Accuracy F
1
score
Baseline1 0.79 0.40 0.82 0.53
Baseline2 0.95 0.43 0.85 0.60
Table 3. Baseline evaluation results.
The evaluation results obtained from our system
by varying the overall hedging score and using it
as threshold are given in Table 4. It is worth noting
that the highest overall hedging score we obtained
was 16; however, we do not show the results for
every possible threshold here for brevity.
Hedging
Score
Threshold
Precision Recall Accuracy F
1
score
1 0.68 0.95 0.88 0.79
2 0.75 0.94 0.91 0.83
3 0.85 0.86 0.93 0.85
4 0.91 0.71 0.91 0.80
5 0.92 0.63 0.89 0.75
6 0.97 0.40 0.85 0.57
7 1 0.19 0.79 0.33
Table 4. Evaluation results from our system.
As seen from Table 3 and Table 4, our results
show improvement over both baseline methods in
terms of accuracy and F
1
 score. Increasing the
threshold (thereby requiring more or stronger
hedging devices to qualify a sentence as specula-
tive) improves the precision while lowering the
recall. The best accuracy and F
1
 score are achieved
at threshold t=3. At this threshold, the differences
between the results obtained with our method and
baseline methods are statistically significant at
0.01 level (p < 0.01).
51
Method Recall/Precision BEP
Baseline1 0.60
Baseline2 0.76
Our system 0.85
Table 5. Recall / precision break-even point (BEP) re-
sults
With the threshold providing the best accuracy
and F
1
 score, precision and recall are roughly the
same (0.85), indicating a recall/precision BEP of
approximately 0.85, also an improvement over
0.76 achieved with a weakly supervised classifier
(Medlock and Briscoe, 2007). Recall/precision
BEP scores are given in Table 5.
11 Discussion
Our results confirm that writers of scientific arti-
cles employ basic, predictable hedging strategies to
soften their claims or to indicate uncertainty and
demonstrate that these strategies can be captured
using a combination of lexical and syntactic
means. Furthermore, the results indicate that
hedging cues can be gainfully weighted to provide
a rough measure of tentativeness or uncertainty.
For instance, a sentence with the highest overall
hedging score is given below:
(7) In one study, Liquid facets was proposed to
target Dl to an endocytic recycling compart-
ment suggesting that recycling of Dl may be
required for signaling.
On the other hand, hedging is not strong in the
following sentence, which is assigned an overall
hedging score of 2:
(8) There is no apparent need for cytochrome c
release in C. elegans since CED-4 does not re-
quire it to activate CED-3.
Below, we discuss some of the common error
types we encountered. Our discussion is based on
evaluation at hedging score threshold of 0, where
existence of a hedging cue is sufficient to label a
sentence speculative.
Most of the false negatives produced by the
system are due to syntactic patterns not addressed
by our method. For instance, negation of ?unhedg-
ers? was used as a syntactic pattern; the pattern
was able to recognize know as an ?unhedger? in
the following sentence, but not the negative quanti-
fier (l i t t le), labeling the sentence as non-
speculative.
(9) Little was known however about the specific
role of the roX RNAs during the formation of
the DCC.
In fact, Hyland (1998) notes ?negation in scien-
tific research articles shows a preference for nega-
tive quantifiers (few, little) and lexical negation
(rarely, overlook).? However, we have not en-
countered this pattern while analyzing the training
set and have not addressed it. Nevertheless, our
approach lends itself to incremental development
and adding such a pattern to our rulebase is rela-
tively simple.
Another type of false negative is caused by cer-
tain derivational forms of epistemic words. In the
following example, the adjective suggestive is not
recognized as a hedging trigger, even though its
base form suggest is an epistemic verb.
(10) Phenotypic differences are suggestive of
distinct functions for some of these genes in
regulating dendrite arborization.
It seems that more sophisticated lexicon expan-
sion rules can be employed to handle such cases.
For example, WordNet?s ?derivationally related
form? feature may be used as the basis of these
expansion rules.
Regarding false positives, most of them are due
to word sense ambiguity concerning hedging cues.
For instance, the modal auxiliary could  is fre-
quently used as a past tense form of can in scien-
tific articles to express the role of enabling
conditions and external constraints on the occur-
rence of the proposition rather than uncertainty or
tentativeness regarding the proposition.  Currently,
our system is unable to recognize such cases. An
example is given below:
(10) Also we could not find any RAG-like se-
quences in the recently sequenced sea urchin
lancelet hydra and sea anemone genomes,
which encode RAG-like sequences.
The context around the hedging cue seems to
play a role in these cases. First person plural pro-
noun (we) and/or reference to objective enabling
conditions seem to be a common characteristic
among false positive cases of could.
In other cases, such as appear, in the absence of
strengthening syntactic cues (to, that), we lower
the hedging score; however, depending on the
threshold, this may not be sufficient to render the
sentence non-speculative.  Rather than lowering
the score equally for all epistemic verbs, a more
52
appropriate approach would be to consider verb
senses separately (e.g., appear should be effec-
tively unhedged without a strengthening cue, while
suggest should only be weakened).
Another type of false positives concern ?weak?
hedging cues, such as epistemic deductive verbs
(conclude, estimate) as well as adverbs (essen-
tially, usually) and nominalizations (implication,
assumption).
We have also seen a few instances, which seem
speculative on the surface, but were labeled non-
speculative. An example is given below:
(11) Caspases can also be activated with the aid
of Apaf-1, which in turn appears to be regu-
lated by cytochrome c and dATP.
12 Conclusion and Future Work
In this paper, we present preliminary experiments
we conducted in recognizing speculative sentences.
We draw on previous linguistic work and extend it
via semi-automatic methods of lexical acquisition.
Using a corpus specifically annotated for specula-
tion, we demonstrate that our linguistically ori-
ented approach improves on the previously
reported results.
Our next goal is to extend our work using a
larger, more comprehensive corpus. This will al-
low us to identify other commonly used hedging
strategies and refine and expand the hedging dic-
tionary.  We also aim to refine the weighting
scheme in a more principled way.
While recognizing that a sentence is speculative
is useful in and of itself, it seems more interesting
and clearly much more challenging to identify
speculative sentence fragments and the proposi-
tions that are being hedged. In the future, we will
move in this direction with the goal of character-
izing the semantics of speculative language.
Acknowledgements
We would like to thank Thomas C. Rindflesch for
his suggestions and comments on the first draft of
this paper.
References
deMarneffe, M. C., MacCartney B., Manning C.D.
2006. Generating Typed Dependency Parses from
Phrase Structure Parses. In Proc of 5th International
Conference on Language Resources and Evaluation,
pp. 449-54.
DiMarco C. and Mercer R.E. 2004. Hedging in Scien-
tific Articles as a Means of Classifying Citations. In
Exploring Attitude and Affect in Text: Theories and
Applications AAAI-EAAT 2004. pp.50-4.
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, MA.
Friedman C., Alderson P., Austin J., Cimino J.J., John-
son S.B. 1994. A general natural-language text proc-
essor for clinical radiology. Journal of the American
Medical Informatics Association, 1(2): 161-74.
Hyland K. 1998. Hedging in Scientific Research Arti-
cles. John Benjamins B.V., Amsterdam, Netherlands.
Kipper Schuler, K. 2005. VerbNet: A broad-coverage,
comprehensive verb lexicon. PhD thesis, University
of Pennsylvania.
Klein D. and Manning C. D. 2003. Accurate unlexical-
ized parsing. In Proc of 41st Meeting of the Associa-
tion for Computational Linguistics. pp. 423-30.
Lakoff  G. 1972. Hedges: A Study in Meaning Criteria
and the Logic of Fuzzy Concepts. Chicago Linguis-
tics Society Papers, 8, pp.183-228.
Light M., Qiu X.Y., Srinivasan P. 2004. The Language
of Bioscience: Facts, Speculations, and Statements in
between. In BioLINK 2004: Linking Biological Lit-
erature, Ontologies and Databases, pp. 17-24.
McCray A. T., Srinivasan S., Browne A. C. 1994. Lexi-
cal methods for managing variation in biomedical
terminologies.  In Proc of 18th Annual Symposium on
Computer Applications  in  Medical Care, pp. 235-9.
Medlock B. and Briscoe T. 2007. Weakly Supervised
Learning for Hedge Classification in Scientific Lit-
erature. In Proc of 45
th
 Meeting of the Association for
Computational Linguistics. pp.992-9.
Palmer F.R. 1986. Mood and Modality. Cambridge
University Press, Cambridge, UK.
Pustejovsky J., Hanks P., Saur? R., See A., Gaizauskas
R., Setzer A., Radev D., Sundheim B., Day D. Ferro
L., Lazo M. 2003. The TimeBank Corpus. In Proc of
Corpus Linguistics. pp. 647-56.
Saur? R., Verhagen M., Pustejovsky J. 2006. SlinkET: a
partial modal parser for events. In Proc of 5
th
 Inter-
national Conference on Language Resources and
Evaluation.
Wilbur W.J., Rzhetsky A., Shatkay H. 2006. New Di-
rections in Biomedical Text Annotations: Defini-
tions, Guidelines and Corpus Construction. BMC
Bioinformatics, 7:356.
53
Proceedings of the Workshop on BioNLP: Shared Task, pages 119?127,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Syntactic Dependency Based Heuristics for Biological Event Extraction
Halil Kilicoglu and Sabine Bergler
Department of Computer Science and Software Engineering
Concordia University
1455 de Maisonneuve Blvd. West
Montre?al, Canada
{h kilico,bergler}@cse.concordia.ca
Abstract
We explore a rule-based methodology for the
BioNLP?09 Shared Task on Event Extrac-
tion, using dependency parsing as the under-
lying principle for extracting and characteriz-
ing events. We approach the speculation and
negation detection task with the same princi-
ple. Evaluation results demonstrate the util-
ity of this syntax-based approach and point out
some shortcomings that need to be addressed
in future work.
1 Introduction
Exponential increase in the amount of genomic data
necessitates sophisticated approaches to accessing
knowledge in molecular biology literature, which re-
mains the primary medium for disseminating new
knowledge in molecular biology. Extracting rela-
tions and events directly from free text facilitates
such access. Advances made in foundational areas,
such as parsing and named entity recognition, boosts
the interest in biological event extraction (Zweigen-
baum et al, 2007). The BioNLP?09 Shared Task on
Event Extraction illustrates this shift and is likely to
inform future endeavors in the field.
The difficulty of extracting biological events from
scientific literature is due to several factors. First,
sentences are long and often have long-range depen-
dencies. In addition, the biological processes de-
scribed are generally complex, involving multiple
genes or proteins as well as other biological pro-
cesses. Furthermore, biological text is rich in higher
level phenomena, such as speculation and negation,
which need to be dealt with for correct interpreta-
tion of the text. Despite all this complexity, how-
ever, a closer look at various biological corpora also
suggests that beneath the complexity lie regularities,
which may potentially be exploited using relatively
simple heuristics.
We participated in Task 1 and Task 3 of the
Shared Task on Event Extraction. Our approach
draws primarily from dependency parse representa-
tion (Mel?c?uk, 1988; deMarneffe et al, 2006). This
representation, with its ability to reveal long-range
dependencies, is suitable for building event extrac-
tion systems. Dependencies typed with grammatical
relations, in particular, benefit such applications. To
detect and characterize biological events (Task 1),
we constructed a dictionary of event triggers based
on training corpus annotations. Syntactic depen-
dency paths between event triggers and event partic-
ipants in the training corpus served in developing a
grammar for participant identification. For specula-
tion and negation recognition (Task 3), we extended
and refined our prior work in speculative language
identification, which involved dependency relations
as well. Our results show that dependency relations,
despite their imperfections, provide a good founda-
tion, on which accurate and reliable event extraction
systems can be built and that the regularities of bio-
logical text can be adequately exploited with a lim-
ited set of syntactic patterns.
2 Related Work
Co-occurrence based approaches (Jenssen et al,
2001; Ding et al, 2002) to biological relation ex-
traction provide high recall at the expense of low
119
precision. Shallow parsing and syntactic templates
(Blaschke et al, 1999; Rindflesch et al, 2000; Fried-
man et al, 2001; Blaschke and Valencia, 2001;
Leroy et al, 2003; Ahlers et al, 2007), as well as
full parsing (Daraselia et al, 2004; Yakushiji et al,
2005), have also been explored as the basis for re-
lation extraction. In contrast to co-occurrence based
methods, these more sophisticated approaches pro-
vide higher precision at the expense of lower recall.
Approaches combining the strengths of complemen-
tary models have also been proposed (Bunescu et al,
2006) for high recall and precision.
More recently, dependency parse representation
has found considerable use in relation extraction,
particularly in extraction of protein-protein interac-
tions (PPI). Fundel et al (2007) use Stanford depen-
dency parses of Medline abstracts as the basis for
rules that extract gene/protein interactions. Rinaldi
et al (2007) extract relations combining a hand-
written grammar based on dependency parsing with
a statistical language model. Airola et al (2008) ex-
tract protein-protein interactions from scientific lit-
erature using supervised machine learning based on
an all-dependency-paths kernel.
The speculative aspect of the biomedical literature
(also referred to as hedging) has been the focus of
several recent studies. These studies primarily dealt
with distinguishing speculative sentences from non-
speculative ones. Supervised machine learning tech-
niques mostly dominate this area of research (Light
et al, 2004; Medlock and Briscoe, 2007; Szarvas,
2008). A more linguistically-based approach, rely-
ing on lexical and syntactic patterns, has been ex-
plored as well (Kilicoglu and Bergler, 2008). The
scope of speculative statements is annotated in the
BioScope corpus (Vincze et al, 2008); however, ex-
periments in detecting speculation scope have yet to
be reported.
Recognizing whether extracted events are negated
is crucial, as negation reverses the meaning of a
proposition. Most of the work on negation in
the biomedical domain focused on finding negated
terms or concepts. Some of these systems are
rule-based and rely on lexical or syntactic informa-
tion (Mutalik et al, 2001; Chapman et al, 2001;
Sanchez-Graillet and Poesio, 2007); while others
(Averbuch et al, 2004; Goldin and Chapman, 2003)
experiment with machine learning techniques. A re-
cent study (Morante et al, 2008) focuses on learn-
ing negation scope using memory-based classifiers
trained on the BioScope corpus.
Our approach to Task 1 is most similar to work
of Fundel et al (2007) as it builds on dependency-
based heuristics. However, we address a larger num-
ber of event classes, including regulatory events al-
lowing participation of other events. In addition,
event triggers are central to our approach, contrast-
ing with their system and most other PPI systems
that rely on finding dependency paths between enti-
ties. We extended prior work for Task 3 and obtained
state of the art results.
3 Event Detection and Characterization
As preparation for biological event extraction, we
combined the provided annotations, tokenized in-
put and dependency parses in an XML representa-
tion. Next, we determined good trigger words for
event classes and scored them. Finally, we devel-
oped a dependency-based grammar for event partici-
pant identification, which drives our event extraction
system.
3.1 Data Preprocessing
Our event detection and characterization pipeline re-
quires XML representation of a document as in-
put. Here, the XML representation of a document
contains sentences, their offset positions and de-
pendency parses as well as entities (Proteins) and
their offset positions in addition to word information
(tokens, part-of-speech tags, indexes and lemmas).
We used the Stanford Lexicalized Parser (Klein and
Manning, 2003) to extract word-related information,
as well as for dependency parsing.
3.2 Event Triggers
After parsing the training corpus and creating an en-
riched document representation, we proceeded with
constructing a dictionary of event triggers, draw-
ing from training corpus annotations of triggers and
making further refinements, as described below.
We view event triggers essentially as predicates
and thus restricted event triggers to words carrying
verb, noun or adjective part-of-speech tags. Our
analysis suggests that, in general, trigger words with
other POS tags are tenuously annotated event trig-
gers and in fact require more context to qualify as
120
event triggers. In Example (1), by is annotated
as trigger for a Positive regulation event;
however, it seems that the meaning of the entire
prepositional phrase introduced with by contributes
to trigger such an event:
(1) These data suggest a general role for Tax in-
duction of IL-1alpha gene transcription by the
NF-kappaB pathway.
We refined the event trigger list further through
limited term expansion and filtering, based on sev-
eral observations:
1. The event triggers with prefixes, such as
co, down and up, (e.g., coexpression, down-
regulate) were expanded to include both hy-
phenated and non-hyphenated forms.
2. For a trigger that has inflectional/derivational
forms acting as triggers in the development cor-
pus but not in the training corpus, we added
these forms as event triggers. Examples include
adding dimerization after dimerize and dimin-
ished(adj) after diminish, among others.
3. We removed several event triggers, which, we
considered, required more context to qualify
as event triggers for the corresponding event
classes. (e.g., absence, absent, follow, lack)
Finally, we did not consider multi-word event
triggers. We observed that core trigger meaning
generally came from a single word token (gener-
ally head of a noun phrase) in the fragment an-
notated as event trigger. For instance, for trigger
transcriptional activation, the annotated event class
is Positive regulation, which suggests that
the head activation carries the meaning in this in-
stance (since transcriptional is an event trigger for
the distinct Transcription event class). In an-
other instance, the trigger binding activity is anno-
tated as triggering a Binding event, indicating that
the head word activity is semantically empty. We
noted some exceptions to this constraint (e.g., neg-
atively regulate, positive regulation) and dealt with
them in the postprocessing step.
For the remaining event triggers, we computed a
?goodness score? via maximum likelihood estima-
tion. For a given event class C and event trigger t,
the ?goodness score? G(t,C) then is:
G(t,C) = w(C:t)/w(t)
where w(C:t) is the number of times t occurs as a
trigger for event class C and w(t) is the frequency
of trigger t in the training corpus. The newly added
event triggers were assigned the same scores as the
trigger they are derived from.
In the event extraction step, we do not consider
event triggers with a score below an empirically de-
termined threshold.
3.3 Dependency relations for event participant
identification
To identify the event participants Theme and Cause,
we developed a grammar based on the ?collapsed?
version of Stanford Parser dependency parses of
sentences. Grammar development was driven by
extraction and ranking of typed dependency rela-
tion paths connecting event triggers to correspond-
ing event participants in the training data. We then
analyzed these paths and implemented as rules those
deemed to be both correct and sufficiently general.
More than 2,000 dependency paths were ex-
tracted; however, their distribution was Zipfian, with
approximately 70% of them occurring only once.
We concentrated on the most frequent, therefore
general, dependency paths. Unsurprisingly, the most
frequent dependency path involved the dobj (direct
object) dependency between verbal event triggers
and Theme participants, occurring 826 times. Next
was the nn (nominal modifier) dependency between
nominal event triggers and their Theme participants.
The most frequent dependency for Cause partici-
pants was, again unsurprisingly, nsubj (nominal sub-
ject). The ranking of dependency paths indicated
that path length is inversely proportional to reliabil-
ity. We implemented a total of 27 dependency path
patterns.
Some of these patterns specifically address de-
ficiencies of the Stanford Parser. Prepositional
phrases are often attached incorrectly, causing prob-
lems in participant identification. Consider, for ex-
ample, one of the more frequent dependency paths,
dobj-prep on (direct object dependency followed by
prepositional modifier headed in on), occurring be-
tween the event trigger (effect) and participant (ex-
pression, itself a sub-event trigger):
(2) We have examined the effect of leukotriene B4
121
(LTB4), a potent lipid proinflammatory medi-
ator, on the expression of the proto-oncogenes
c-jun and c-fos.
dobj(examined,effect)
prep on(examined,expression)
This dependency path occurs almost exclusively
with PP attachment errors involving on, leading us
to stipulate a ?corrective? dependency path, imple-
mented for certain trigger words (e.g., effect, influ-
ence, impact in this case). Postnominal preposi-
tional attachment heuristics detailed in Schuman and
Bergler (2006) helped determine 6 such patterns.
Two common verbs (require and involve) deserve
special attention, as the semantic roles of their sub-
ject/object constituents differ from typical verbs.
The prototypical Cause dependency, nsubj, indicates
a Theme in the following sentence:
(3) Regulation of interleukin-1beta transcription
by Epstein-Barr virus involves a number of la-
tent proteins via their interaction with RBP.
nsubj(involves,Regulation)
For these two verbs, participant identification rules
are reversed.
An interesting phenomenon is NPs with hyphen-
ated adjectival modifiers, occurring frequently in
molecular biology texts (e.g., ?... LPS-mediated
TF expression...?). The majority of these cases in-
volve regulatory events. Such cases do not in-
volve a dependency path, as the participant (in
this case, LPS) and the event trigger (mediated)
form a single word. An additional rule ad-
dresses these cases, stipulating that the substring
preceding the hyphen is the Cause of the regu-
latory event triggered by the substring following
the hyphen. (Positive regulation (Trig-
ger=mediated,Theme=TF expression,Cause=LPS)).
Events allowing event participants (regulatory
events) are treated essentially the same way as
events taking entity participants. The main differ-
ence is that, when sub-events are considered, a de-
pendency path is found between the trigger of the
main event and the trigger of its sub-event, rather
than an annotated entity, as was shown above in Ex-
ample (2).
3.4 Extracting Events
The event detection and characterization pipeline
(Task 1) consists of three steps:
1. Determining whether a word is an event trigger.
2. If the word is an event trigger, identifying its
potential participant(s).
3. If the event trigger corresponds to a regula-
tory event and it has a potential sub-event
participant, determining in a recursive fashion
whether the sub-event is a valid event.
The first step is a simple dictionary lookup. Pro-
vided that a word is tagged as noun, verb or adjec-
tive, we check whether it is in our dictionary, and if
so, determine the event class for which it has a score
above the given threshold. This word is considered
the clue for an event.
We then apply our dependency-based rules to de-
termine whether any entity or event trigger (in the
case of regulatory events) in the sentence qualifies
as an argument of the event clue. Grammar rules are
applied in the order of simplicity; rules that involve
a direct dependency between the clue and any word
of the entity are considered first.
Once a list of potential participants is obtained
by consecutive application of the rules, one of
two things may happen: Provided that sub-events
are not involved and appropriate participants have
been identified (e.g., a Theme is found for a
Localization event), the event is simply added
to the extracted event list. Otherwise, we proceed
recursively to determine whether the sub-event par-
ticipant can be resolved to a simple event. If this
yields no such simple event in the end, the event in
question is rejected. In the following example, the
event triggered by inhibit is invalid even though its
Cause JunB is recognized, because its Theme, sub-
event triggered by activation, cannot be assigned a
Theme and therefore is considered invalid.
(4) ..., JunB, is shown to inhibit activation medi-
ated by JunD.
After events are extracted in this manner, two
postprocessing rules ensure increased accuracy.
One rule deals with a limited set of multi-
word event triggers. If a Regulation event
122
has been identified and the event trigger is
modified by positive or negative (or inflec-
tional forms positively, negatively), the event
class is updated to Positive regulation or
Negative regulation, respectively. The sec-
ond rule deals with the limitation of not allowing
multiple events on the same trigger and adds to
the extracted event list a Positive regulation
event, if a Gene expression event was recog-
nized for certain triggers, including overexpression
and several others related to transfection (e.g., trans-
fect, transfection, cotransfect).
Two grammatical constructions are crucial to de-
termining the event participants: coordination and
apposition. We summarize how they affect event ex-
traction below.
3.4.1 Coordination
Coordination plays two important roles in event
extraction:
1. When the event trigger is conjoined with an-
other word token, dependency relations con-
cerning the other conjunct are also considered
for participant identification.
2. When an event is detected and its participant
is found to be coordinated with other entities,
new events are created with the event trigger
and each of these entities. An exception are
Binding events, which may have multiple
Themes. In this case, we add conjunct entities
as the Themes of the base event.
Coordination between words is largely determined
by dependency relations. The participants of a de-
pendency with a type descending from conj (con-
junct) are considered coordinated (e.g., conj and,
conj or).
Recognizing that Stanford dependency parsing
misses some expressions of coordinated entities typ-
ical of biological text (in particular, those involving
parentheses), we implemented a few additional rules
to better resolve coordinated entities. These rules
stipulate that entities that have between them:
1. Only a comma (,) or a semi-colon (;)
2. A word with CC (coordinating conjunction)
part-of-speech tag
3. A complete parenthetical expression
4. Any combination of the above
are coordinated. For instance, in Example (5), we
recognize the coordination between interleukin-2
and IL-4, even though the parser does not:
(5) The activation of NFAT by TCR signals has
been well described for interleukin-2 (IL-2)
and IL-4 gene transcription in T cells.
conj and(interleukin-2,transcription)
3.4.2 Apposition
Words in an apposition construction are con-
sidered equivalent for event extraction purposes.
Therefore, if an appropriate dependency exists be-
tween a word and the trigger and the word is in ap-
position with an entity, that entity is marked as the
event participant. In Example 6, the appos (apposi-
tive) dependency shown serves to extract the event
Positive regulation (Trigger=upregulation,
Theme=intercellular adhesion molecule-1):
(6) ... upregulation of the lung vascular adhesion
molecule, intercellular adhesion molecule-1,
was greatly reduced by...
appos(molecule,molecule-1)
prep of(upregulation,molecule)
The dependencies that we consider to encode ap-
position constructions are: appos (appositive), ab-
brev (abbreviation), prep {including, such as, com-
pared to, compared with, versus} (prepositional
modifier marked with including, such as, compared
to, compared with or versus).
3.5 Speculation and Negation Detection
Once an event list is obtained for a sentence,
our speculation and negation module determines
whether these events are speculated and/or negated,
using additional dependency-based heuristics that
consider the dependencies between the event trigger
and speculation/negation cues.
3.5.1 Speculation Recognition
We refined an existing speculation detection mod-
ule in two ways for Task 3. First, we noted that
modal verbs (e.g., may) and epistemic adverbs (e.g.,
probably) rarely mark speculative contexts in the
123
training corpus, demonstrating the lack of a stan-
dardized notion of speculation among various cor-
pora. For Task 3, we ignored lexical cues in these
classes completely for increased accuracy. Sec-
ondly, corpus analysis revealed a new syntactic pat-
tern for speculation recognition. This pattern in-
volves the class of verbs that we called active cogni-
tion verbs (e.g., examine, evaluate, analyze, study,
investigate). We search for a Theme dependency
pattern between one of these verbs and an event trig-
ger and mark the event as speculated, if such a pat-
tern exists. Nominalizations of these verbs are also
considered. In Example (7), the event triggered by
effects is speculated, since effects is the direct object
(therefore, Theme) of studied:
(7) We have studied the effects of prednisone
(PDN), ... on the production of cytokines (IL-2,
IL-6, TNF-alpha, IL-10) by peripheral T lym-
phocytes...
3.5.2 Negation Detection
Negation detection is similar to speculation detec-
tion. Several classes of negation cues have been de-
termined based on corpus analysis and the negation
module negates events if there is an appropriate de-
pendency between one of these cues and the event
triggers. The lexical cues and the dependencies that
are sought are given in Table 1.
Negation Cue Dependency
lack, absence prep of(Cue,Trigger)
unable, <not> able, fail xcomp(Cue,Trigger)
inability, failure infmod(Cue, Trigger)
no, not, cannot det(Trigger, Cue)
Table 1: Negation cues and the corresponding depen-
dencies (xcomp: clausal complement, infmod: infinitival
modifier, det: determiner)
Additionally, participation of event triggers
in dependencies of certain types is sufficient
for negating the event it triggers. Such depen-
dency types are neg (negation) and conj negcc
(negated coordination). A neg dependency ap-
plies to event triggers only, while conj negcc is
sought between event participants, as well as
event triggers. Therefore, in Example (8), an event
(Positive regulation(Trigger=transactivate,
Theme: GM-CSF, Cause=ELF1)) is negated, based
on the dependencies below:
(8) Exogenous ETS1, but not ELF1, can transacti-
vate GM-CSF, ..., in a PMA/ionomycin depen-
dent manner.
conj negcc(ETS1, ELF1)
nsubj(transactivate, ETS1)
dobj(transactivate, GM-CSF)
Finally, if none of the above applies and the word
preceding the event trigger or one of the event partic-
ipants is a negation cue (no, not, cannot), the event
is negated.
4 Results and Discussion
Our event extraction system had one of the best per-
formances in the shared task. With the approxi-
mate span matching/approximate recursive match-
ing evaluation criteria, in Task 1, we were ranked
third, while our speculation and negation detection
module performed best among the six participating
systems in Task 3. Not surprisingly, our system fa-
vors precision, typical of rule-based systems. Full
results are given in Table 2.
The results reported are at goodness score thresh-
old of .08. Increasing the threshold increases preci-
sion, while lowering recall. The threshold was de-
termined empirically.
Our results confirm the usefulness of dependency
relations as foundation for event extraction systems.
There is much room for improvement, particularly in
terms of recall, and we believe that incremental na-
ture of our system development accommodates such
improvements fairly easily.
Our view of event triggers (?once a trigger, always
a trigger?), while simplistic, provides a good start-
ing point by greatly reducing the number of trigger
candidates in a sentence and typed dependencies to
consider. However, it also leads to errors. One such
example is given in Example (9):
(9) We show that ..., and that LPS treatment en-
hances the oligomerization of TLR2.
where we identify the event Binding (Trig-
ger=oligomerization,Theme=TLR2). We consider
oligomerization a reliable trigger, since it occurs
twice in the training corpus, both times as event trig-
gers. However, in this instance, it does not trigger
124
Event Class Recall Precis. F-score
Localization 35.63 92.54 51.45
Binding 20.46 40.57 27.20
Gene expression 55.68 79.45 65.47
Transcription 15.33 60.00 24.42
Protein catabolism 64.29 56.25 60.00
Phosphorylation 69.63 95.92 80.69
EVT-TOTAL 43.10 73.47 54.33
Regulation 24.05 45.75 31.53
Positive regulation 28.79 50.45 36.66
Negative regulation 26.65 51.53 35.13
REG-TOTAL 27.47 49.89 35.43
Negation 14.98 50.75 23.13
Speculation 16.83 50.72 25.27
MOD-TOTAL 15.86 50.74 24.17
ALL-TOTAL 32.68 60.83 42.52
Table 2: Evaluation results
an event. This narrow view also leads to recall er-
rors, in which we do not recognize an event trigger
as such, simply because we have not encountered it
in the training corpus, or it does not have an appro-
priate part-of-speech tag. A more sophisticated trig-
ger learning approach could aid in better detecting
event triggers.
We dealt with some deficiencies of Stanford de-
pendency parsing through additional rules, as de-
scribed in Section 3.3. However, many depen-
dency errors are still generated, due to the com-
plexity of biological text. For instance, in Exam-
ple (10), there is a coordination construction be-
tween NF-kappaB nuclear translocation and tran-
scription of E-selectin and IL-8. However, this con-
struction is missed and an erroneous prep of de-
pendency is found, leading to two false positive
errors: Localization (Trigger=translocation,
Theme=E-selectin) and Localization (Trig-
ger=translocation, Theme=IL-8).
(10) ... leading to NF-kappaB nuclear translocation
and transcription of E-selectin and IL-8, which
results in ...
conj and(transcription, translocation)
prep of(translocation, E-selectin)
conj and(E-selectin, IL-8)
These errors can be corrected via other ?corrective?
dependency paths; however, first, a closer examina-
tion of such error patterns is necessary.
In other instances, the required dependency is
completely missed by the parser, leading to recall
errors. For instance, in Example (11), we are un-
able to recognize two events (Regulation
(Trigger=regulation, Theme=4E-BP1) and
Regulation (Trigger=regulation, Theme=4E-
BP2)), due to lack of apposition dependencies
between repressors and 4E-BP1 or 4E-BP2:
(11) ... specific regulation of two repressors of
translation initiation, 4E-BP1 and 4E-BP2.
prep of(regulation,repressors)
prep of(repressors, initiation)
conj and(intiation, 4E-BP1)
conj and(initiation, 4E-BP2)
Typical of rule-base systems, we miss events ex-
pressed using less frequent patterns. Event partic-
ipants expressed as prepositional modifiers marked
with from is one such case. An example is given
below:
(12) Calcineurin activates transcription from the
GM-CSF promoter ...
In this case, the event Transcription (Trig-
ger=transcription, Theme=GM-CSF) is missed. It is
fairly easy to add a rule to address such occurrences.
We have not attempted to resolve anaphoric ex-
pressions for the shared task, which led to a fair
number of recall errors. In a similar vein, we ig-
nored events spanning multiple sentences. We ex-
pect that several studies addressing anaphora res-
olution in biomedical text (Castan?o et al, 2002;
Gasperin and Briscoe, 2008) will inform our near
future efforts in this area.
Evaluation results regarding Task 3 may seem
poor at first; however, most of the errors concern
misidentified or missed base events. Thus, in this
section, we focus on errors specifically triggered by
speculation and negation module. In the develop-
ment corpus, we identified 39 speculation instances,
4 of which were errors due to speculation process-
ing. Of 95 annotated speculation instances, 7 were
missed due to deficiencies in speculation processing.
125
Similarly, negation processing led to 5 false posi-
tives in 31 negation instances we identified and to 5
false negatives in 107 annotated negation instances.
We found that speculation false positive er-
rors are exclusively cases for which speculation
could be argued. For instance, in Example (13),
we recognize that appears to scopes over event
Negative regulation (Trigger=negatively
regulate, Theme=IL-2R), rendering it speculative.
However, it is not annotated as such. This is
further evidence for the difficulty of annotating such
phenomena correctly and consistently, since the
exact meaning is somewhat elusive.
(13) An unidentified Ets family protein binds to the
EBS overlapping the consensus GAS motif and
appears to negatively regulate the human IL-
2R alpha promoter.
Negation pattern that involves negation cues
(no,not,cannot) in the token preceding an event trig-
ger or participant, a pattern initially considered to
increase recall, caused most of negation false posi-
tive errors. An example is given in (14):
(14) The finding that HL-60/vinc/R cells respond to
TPA with induction of a monocytic phenotype,
but not c-jun expression, suggests that ...
Complex and less frequent patterns of expressing
speculation and negation were responsible for more
recall errors. Two such examples are given below:
(15) (a) These results ... and suggest a molecular
mechanism for the inhibition of TLR2 by
DN variants.
(b) Galectin-3 is ... and is expressed in many
leukocytes, with the notable exception of
B and T lymphocytes.
In (15a), speculation is detected; however, we are
unable to recognize that it scopes over the event
triggered by inhibition. In (15b), the prepositional
phrase, with the notable exception, is not considered
to indicate negation.
5 Conclusions and Future Work
We explored a rule-based approach to biological
event detection driven by typed dependency rela-
tions. This study marks our first foray into bio-event
extraction in a general way and, thus, we consider
the results very encouraging. In one area we investi-
gated before, speculation detection, our system per-
formed best and this confirms the portability and ex-
tensibility of our approach.
Modest recall figures point to areas of improve-
ment. We plan to address anaphora resolution and
multiple sentence spanning events in the near fu-
ture. Our na??ve approach to event triggers needs
refinement and we believe that sophisticated super-
vised machine learning techniques may be helpful.
In addition, biomedical lexical resources, includ-
ing UMLS SPECIALIST Lexicon (McCray et al,
1994), may be useful in improving event trigger
detection. Finally, dependency relations based on
the Stanford Parser provided better performance in
our case, in contrast to general consensus that those
based on Charniak Parser (Charniak and Johnson,
2005) are superior, and this, too, deserves further in-
vestigation.
References
C B Ahlers, M Fiszman, D Demner-Fushman, F M Lang,
and T C Rindflesch. 2007. Extracting semantic predi-
cations from Medline citations for pharmacogenomics.
Pac Symp Biocomput, pages 209?220.
A Airola, S Pyysalo, J Bjo?rne, T Pahikkala, F Ginter, and
T Salakoski. 2008. All-paths graph kernel for protein-
protein interaction extraction with evaluation of cross-
corpus learning. BMC Bioinformatics, 9 Suppl 11:s2.
M Averbuch, T Karson, B Ben-Ami, O Maimon, and
L Rokach. 2004. Context-sensitive medical informa-
tion retrieval. In Proc MEDINFO-2004, pages 1?8.
C Blaschke and A Valencia. 2001. The potential use
of SUISEKI as a protein interaction discovery tool.
Genome Inform, 12:123?134.
C Blaschke, M A Andrade, C Ouzounis, and A Valencia.
1999. Automatic extraction of biological information
from scientific text: protein-protein interactions. In
Proc Int Conf Intell Syst Mol Biol, pages 60?67.
R Bunescu, R Mooney, A Ramani, and E Marcotte. 2006.
Integrating co-occurrence statistics with information
extraction for robust retrieval of protein interactions
from Medline. In Proc BioNLP Workshop on Link-
ing Natural Language Processing and Biology, pages
49?56.
J Castan?o, J Zhang, and J Pustejovsky. 2002. Anaphora
resolution in biomedical literature. In Proc Interna-
tional Symposium on Reference Resolution for NLP.
126
W W Chapman, W Bridewell, P Hanbury, G F Cooper,
and B G Buchanan. 2001. A simple algorithm for
identifying negated findings and diseases in discharge
summaries. J Biomed Inform, 34(5):301?310.
E Charniak and M Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proc 43rd Meeting of the Association for Computa-
tional Linguistics, pages 173?180.
N Daraselia, A. Yuryev, S Egorov, S Novichkova,
A Nikitin, and I Mazo. 2004. Extracting human pro-
tein interactions from MEDLINE using a full-sentence
parser. Bioinformatics, 20(5):604?611.
M C deMarneffe, B MacCartney, and C D Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proc 5th International Con-
ference on Language Resources and Evaluation, pages
449?454.
J Ding, D Berleant, D Nettleton, and E Wurtele. 2002.
Mining MEDLINE: abstracts, sentences, or phrases?
Pac Symp Biocomput, 7:326?337.
C Friedman, P Kra, M Krauthammer, H Yu, and A Rzhet-
sky. 2001. GENIES: a natural-langauge processing
system for the extraction of molecular pathways from
journal articles. Bioinformatics, 17(1):74?82.
K Fundel, R Ku?ffner, and R Zimmer. 2007. RelEx re-
lation extraction using dependency parse trees. Bioin-
formatics, 23(3):365?371.
C Gasperin and T Briscoe. 2008. Statistical anaphora
resolution in biomedical texts. In Proc COLING 2008.
I M Goldin and W W Chapman. 2003. Learning to detect
negation with not in medical texts. In Proc Workshop
on Text Analysis and Search for Bioinformatics at the
26th ACM SIGIR Conference.
T K Jenssen, A Laegreid, J Komorowski, and E Hovig.
2001. A literature network of human genes for high-
throughput analysis of gene expression. Nat Genet,
28:21?28.
H Kilicoglu and S Bergler. 2008. Recognizing specu-
lative language in biomedical research articles: a lin-
guistically motivated perspective. BMC Bioinformat-
ics, 9 Suppl 11:s10.
D Klein and C D Manning. 2003. Accurate unlexicalized
parsing. In Proc 41th Meeting of the Association for
Computational Linguistics, pages 423?430.
G Leroy, H Chen, and J D Martinez. 2003. A shallow
parser based on closed-class words to capture relations
in biomedical text. Journal of Biomedical Informatics,
36:145?158.
M Light, X Y Qiu, and P Srinivasan. 2004. The language
of bioscience: facts, speculations, and statements in
between. In BioLINK 2004: Linking Biological Liter-
ature, Ontologies and Databases, pages 17?24.
A T McCray, S Srinivasan, and A C Browne. 1994. Lex-
ical methods for managing variation in biomedical ter-
minologies. In Proc 18th Annual Symposium on Com-
puter Applications in Medical Care, pages 235?239.
B Medlock and T Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature.
In Proc 45th Meeting of the Association for Computa-
tional Linguistics, pages 992?999.
I A Mel?c?uk. 1988. Dependency syntax: Theory and
Practice. State University Press of New York, NY.
R Morante, A Liekens, and W Daelemans. 2008. Learn-
ing the scope of negation in biomedical text. In Proc
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 715?724.
P G Mutalik, A Deshpande, and P M Nadkarni. 2001.
Use of general-purpose negation detection to augment
concept indexing of medical documents: A quantita-
tive study using the UMLS. J Am Med Inform Assoc,
8(6):598?609.
F Rinaldi, G Schneider, K Kaljurand, M Hess, C Andro-
nis, O Konstandi, and A Persidis. 2007. Mining of
relations between proteins over biomedical scientific
literature using a deep-linguistic approach. Artif. In-
tell. Med., 39(2):127?136.
T C Rindflesch, L Tanabe, J N Weinstein, and L Hunter.
2000. EDGAR: Extraction of drugs, genes, and rela-
tions from the biomedical literature. In Proc Pacific
Symposium on Biocomputing, pages 514?525.
O Sanchez-Graillet and M Poesio. 2007. Negation of
protein protein interactions: analysis and extraction.
Bioinformatics, 23(13):424?432.
J Schuman and S Bergler. 2006. Postnominal prepo-
sitional phrase attachment in proteomics. In Proc
BioNLP Workshop on Linking Natural Language Pro-
cessing and Biology, pages 82?89.
G Szarvas. 2008. Hedge classification in biomedical
texts with a weakly supervised selection of keywords.
In Proc 46th Meeting of the Association for Computa-
tional Linguistics, pages 281?289.
V Vincze, G Szarvas, R Farkas, G Mora, and J Csirik.
2008. The BioScope corpus: biomedical texts anno-
tated for uncertainty, negation and their scopes. BMC
Bioinformatics, 9 Suppl 11:S9.
A Yakushiji, Y Miyao, Y Tateisi, and J Tsujii. 2005.
Biomedical event extraction with predicate-argument
structure patterns. In Proc First International Sympo-
sium on Semantic Mining in Biomedicine, pages 60?
69.
P Zweigenbaum, D Demner-Fushman, H Yu, and K B
Cohen. 2007. Frontiers of biological text mining: cur-
rent progress. Briefings in Bioinformatics, 8(5):358?
375.
127
Abstraction Summarization for Managing the Biomedical  
Research Literature 
 
 
Marcelo Fiszman Thomas C. Rindflesch Halil Kilicoglu 
Lister Hill National Center for Biomedical Communications 
National Library of Medicine 
Bethesda, MD 20894 
{fiszman|tcr|halil}@nlm.nih.gov 
 
 
Abstract 
We explore a semantic abstraction approach 
to automatic summarization in the biomedical 
domain. The approach relies on a semantic 
processor that functions as the source inter-
preter and produces a list of predications. A 
transformation stage then generalizes and 
condenses this list, ultimately generating a 
conceptual condensate for a disorder input 
topic. The final condensate is displayed in 
graphical form. We provide a set of principles 
for the transformation stage and describe the 
application of this approach to multidocument 
input. Finally, we examine the characteristics 
and quality of the condensates produced. 
1 Introduction 
Several approaches to text-based information manage-
ment applications are being pursued, including word-
based statistical processing and those depending on 
string matching, syntax, or semantics. Statistical sys-
tems have enjoyed considerable success for information 
retrieval, especially using the vector space model (Sal-
ton et al, 1975). Since the SIR system (Raphael, 1968), 
some have felt that automatic information management 
could best be addressed using semantic information. 
Subsequent research (Schank, 1975; Wilks, 1976) ex-
panded this paradigm. More recently, a number of ex-
amples of knowledge-based applications show 
considerable promise. These include systems for ma-
chine translation (Viegas et al, 1998), question answer-
ing, (Harabagiu et al, 2001; Clark et al, 2003), and 
information retrieval (Mihalcea and Moldovan, 2000).  
In the biomedical domain, the MEDLINE? biblio-
graphic database provides opportunities for keeping 
abreast of the research literature. However, the large 
size of this online resource presents potential challenges 
to the user. Query results often include hundreds or 
thousands of citations (including title and abstract). 
Automatic summarization offers potential help in man-
aging such results; however, the most popular approach, 
extraction, faces challenges when applied to multi-
document summarization (McKeown et al, 2001). 
Abstraction summarization offers an attractive alter-
native for managing citations resulting from MEDLINE 
searches. We present a knowledge-rich abstraction ap-
proach that depends on underspecified semantic inter-
pretation of biomedical text. As an example, a graphical 
representation (Batagelj, 2003) of the semantic predica-
tions serving as a summary (or conceptual condensate) 
from our system is shown in Figure 1. The input text 
was a MEDLINE citation with title ?Gastrointestinal 
tolerability and effectiveness of rofecoxib versus 
naproxen in the treatment of osteoarthritis: a random-
ized, controlled trial.? 
 
Figure 1. Semantic abstraction summarization 
Our semantic interpreter and the abstraction sum-
marizer based on it both draw on semantic information 
from the Unified Medical Language System? (UMLS),? 
a resource for structured knowledge in the biomedical 
domain. After introducing the semantic interpreter, we 
describe the transformation phase of our paradigm, dis-
cussing principles that depend on semantic notions in 
order to condense the semantic predications represent-
ing the content of text. Initially, this process was applied 
to summarizing single documents. We discuss its adap-
tation to multidocument input, specifically to the set of 
citations resulting from a query to the MEDLINE data-
base. Although we have not yet formally evaluated the 
effectiveness of the resulting condensate, we discuss its 
characteristics and possibilities as both an indicative and 
informative summary. 
2 
2.1 
2.2 
Background 
Lexical Semantics  
Research in lexical semantics (Cruse, 1986) provides 
insight into the interaction of reference and linguistic 
structure. In addition to paradigmatic lexical phenomena 
such as synonymy, hypernymy, and meronymy, diathe-
sis alternation (Levin and Rappaport Hovav, 1996), 
deep case (Fillmore, 1968), and the interaction of predi-
cational structure and events (Tenny and Pustejovsky, 
2000) are being investigated. Some of the consequences 
of research in lexical semantics, with particular attention 
to natural language processing, are discussed by Puste-
jovsky et al (1993) and Nirenburg and Raskin (1996). 
Implemented systems often draw on the information 
contained in WordNet (Fellbaum, 1998). 
In the biomedical domain, UMLS knowledge pro-
vides considerable support for text-based systems. 
(Burgun and Bodenreider (2001) compare the UMLS to 
WordNet.) The UMLS (Humphreys et al, 1998) con-
sists of three components: the Metathesaurus,? Seman-
tic Network (McCray, 1993), and SPECIALIST 
Lexicon (McCray et al, 1994). The Metathesaurus is at 
the core and contains more than 900,000 concepts com-
piled from more than sixty controlled vocabularies. 
Many of these have hierarchical structure, and some 
contain meronymic information in addition to hy-
pernymy. Editors combine terms in the constituent vo-
cabularies into a set of synonyms (cf. WordNet?s 
synsets), which constitutes a concept. One term in this 
set is called the ?preferred name? and is used as the 
concept name, as shown in (1). 
(1) Concept: Dyspnea  Synonyms: Breath-
lessness, Shortness of breath, Breathless, Diffi-
culty breathing, Respiration difficulty, etc. 
In addition, each concept in the Metathesaurus is as-
signed at least one semantic type (such as ?Sign or 
Symptom? for (1)), which categorizes the concept in the 
biomedical domain. The semantic types available are 
drawn from the Semantic Network, in which they are 
organized hierarchically in two single-inheritance trees, 
one under the root ?Entity? and another under ?Event?.  
The Semantic Network also contains semantic 
predications with semantic types as arguments. The 
predicates are semantic relations relevant to the bio-
medical domain and are organized as subtypes of five 
classes, such as TEMPORALLY_RELATED_TO and  
FUNCTIONALLY_RELATED_TO.  Examples are shown in 
(2). 
(2)  ?Pharmacologic Substance? TREATS ?Disease 
or Syndrome?, ?Virus? CAUSES ?Disease or 
Syndrome? 
Lexical semantic information in the UMLS is dis-
tributed between the Metathesaurus and the Semantic 
Network. The Semantic Network stipulates permissible 
argument categories for classes of semantic predica-
tions, although it does not refer to deep case relations. 
The Metathesaurus encodes synonymy, hypernymy, and 
meronymy (especially for human anatomy). Synonymy 
is represented by including synonymous terms under a 
single concept. Word sense ambiguity is represented to 
some extent in the Metathesaurus. For example dis-
charge is represented by the two concepts in (3), with 
different semantic types. 
(3) Discharge, Body Substance: ?Body Substance? 
Patient Discharge: ?Health Care Activity? 
The SPECIALIST Lexicon contains orthographic in-
formation (such as spelling variants) and syntactic in-
formation, including inflections for nouns and verbs and 
sub-categorization for verbs. A suite of lexical access 
tools accommodate other phenomena, including deriva-
tional variation.  
 SemRep 
Our summarization system relies on semantic predica-
tions provided by SemRep (Rindflesch and Fiszman, 
2003), a program that draws on UMLS information to 
provide underspecified semantic interpretation in the 
biomedical domain (Srinivasan and Rindflesch, 2002; 
Rindflesch et al, 2000). Semantic interpretation is based 
on a categorical analysis that is underspecified in that it 
is a partial parse (cf. McDonald, 1992). This analysis 
depends on the SPECIALIST Lexicon and the Xerox 
part-of-speech tagger  (Cutting et al, 1992) and pro-
vides simple noun phrases that are mapped to concepts 
in the UMLS Metathesaurus using MetaMap (Aronson, 
2001).  
The categorial analysis enhanced with Metathesau-
rus concepts and associated semantic types provides the 
basis for semantic interpretation, which relies on two 
components: a set of ?indicator? rules and an (under-
specified) dependency grammar. Indicator rules map 
between syntactic phenomena (such as verbs, nominali-
zations, and prepositions) and predicates in the Seman-
tic Network. For example, such rules stipulate that the 
preposition for indicates the semantic predicate TREATS 
in sumatriptan for migraine. The application of an indi-
cator rule satisfies the first of several necessary condi-
tions for the interpretation of a semantic predication.  
Argument identification is controlled by a partial 
dependency grammar. As is common in such grammars, 
a general principle disallows intercalated dependencies 
(crossing lines). Further, a noun phrase may not be used 
as an argument in the interpretation of more than one 
semantic predication, without license. (Coordination 
and relativization license noun phrase reuse.) A final 
principle states that if a rule can apply it must apply. 
Semantic interpretation in SemRep is not based on 
the ?real? syntactic structure of the sentence; however  
linear order of the components of the partial parse is 
crucial. Argument identification rules are articulated for 
each indicator in terms of surface subject and object.  
For example, subjects of verbs are to the left and objects 
are to the right. (Passivization is accommodated before 
final interpretation.) There are also rules for preposi-
tions and several rules for arguments of nominaliza-
tions.  
The final condition on the interpretation of an asso-
ciative semantic predication is that it must conform to 
the appropriate relationship in the Semantic Network. 
For example, if a predication is being constructed on the 
basis of an indicator rule for TREATS, the syntactic ar-
guments identified by the dependency grammar must 
have been mapped to Metathesaurus concepts with se-
mantic types that conform to the semantic arguments of 
TREATS in the Semantic Network, such as ?Pharma-
cologic Substance? and ?Disease or Syndrome?. Hy-
pernymic propositions are further controlled by 
hierarchical information in the Metathesaurus (Rind-
flesch and Fiszman, 2003). 
In processing the sentence in (4), SemRep first con-
structs the partial categorical representation given 
schematically in (5).  This is enhanced with semantic 
information from the Metathesaurus as shown in (6), 
where the corresponding concept for each relevant noun 
phrase is shown, along with its semantic type. The final 
semantic interpretation for  (4) is given in (7). 
 
(4) Mycoplasma pneumonia is an infection of the 
lung caused by Mycoplasma pneumoniae 
(5) [[Mycoplasma pneumonia] [is] [an infection] 
[of the lung] [caused] [by Mycoplasma pneumo-
niae]] 
(6) ?Mycoplasma pneumonia???Disease or Syn-
drome? 
?Infection???Disease or Syndrome? 
?Lung???Body Part, Organ, or Organ Compo-
nent? 
?Mycoplasma pneumoniae???Bacterium? 
(7) Mycoplasma Pneumonia ISA Infection  
Lung LOCATION_OF Infection 
Lung LOCATION_OF Mycoplasma Pneumonia 
Mycoplasma pneumoniae CAUSES Infection  
Mycoplasma pneumoniae CAUSES My-
coplasma Pneumonia 
3 
3.1 
Automatic Summarization 
Automatic summarization is ?a reductive transformation 
of source text to summary text through content reduc-
tion, selection, and/or generalization on what is impor-
tant in the source? (Sparck Jones, 1999). Two 
paradigms are being pursued: extraction and abstraction 
(Hahn and Mani, 2000). Extraction concentrates on cre-
ating a summary from the actual text occurring in the 
source document, relying on notions such as frequency 
of occurrence and cue phrases to identify important in-
formation. 
Abstraction, on the other hand, relies either on lin-
guistic processing followed by structural compaction 
(Mani et al, 1999) or on interpretation of the source text 
into a semantic representation, which is then condensed 
to retain only the most important information asserted in 
the source. The semantic abstraction paradigm is attrac-
tive due to its ability to manipulate information that may 
not have been explicitly articulated in the source docu-
ment. However, due to the challenges in providing se-
mantic representation, semantic abstraction has not been 
widely pursued, although the TOPIC system (Hahn and 
Reimer, 1999) is a notable exception.  
Semantic Abstraction Summarization 
We are devising an approach to automatic summariza-
tion in the semantic abstraction paradigm, relying on 
SemRep for semantic interpretation of source text. The 
transformation stage that condenses these predications is 
guided by principles articulated in terms of frequency of 
occurrence as well as lexical semantic phenomena.  
We do not produce a textual summary; instead, we 
present the disorder condensates in graphical format. 
We first discuss the application of this approach to 
summarizing single documents (full text research arti-
cles on treatment of disease) and then consider its ex-
tension to multidocument input in the form of biomedi-
cal scientific abstracts directed at clinical researchers.  
The transformation stage takes as input a list of 
Sem
3.2 Transformation 
In the semantic abstraction paradigm the transformation 
b. Connectivity: Also include ?useful? additional 
c. Novelty: Do not include predications that the 
d. Saliency: Only include the most frequently oc-
Alth urrence (saliency) plays a 
rol
(relevance), a condensation process, identi-
fies
ders} 
 {Disorders} 
isorders} 
ders} 
s of 
sem
a generalization process 
and
ovelty) provides further condensation by 
elim
tion phase 
and
n these principles are applied to the semantic 
pre
 
Rep predications and a seed disorder concept. The 
output is a conceptual condensate for the input concept. 
Before transformation begins, predications are subjected 
to a focused word sense disambiguation filter. Branded 
drug names such as Advantage (Advantage brand of 
Imidacloprid) and Direct (Direct type of resin cement), 
which are ambiguous with the more common meaning 
of their names, are resolved to their non-pharmaceutical 
sense.  
stage condenses and generalizes, and in our approach 
these processes are based on four general principles: 
a. Relevance: Include predications on the topic of 
the summary 
predications 
user already knows 
curring predications 
ough frequency of occ
e in determining predications to be included in the 
summary, the other three principles depend crucially on 
lexical semantic information from the UMLS. These 
four principles guide the phases involved in creating a 
summary. 
Phase 1 
 predications on a given topic (in this study, disor-
ders) and is controlled by a semantic schema 
(Jacquelinet et al, 2003) for that topic. The schema is 
represented as a set of predications in which the predi-
cate is drawn from a relation in the UMLS Semantic 
Network and the arguments are represented as a ?do-
main? covering a class of concepts in the Metathesaurus 
(Disorders, for example).  
{Disorders} ISA {Disor
{Etiological process} CAUSES
{Treatment} TREATS  {Disorders} 
{Body location} LOCATION_OF {D
{Disorders} OCCURS_IN {Disorders} 
{Disorders} CO-OCCURS_WITH {Disor
Each domain for the schema is defined in term
antic categorization in the Semantic Network. For 
example {Disorders} is a subset of the semantic group 
Disorders (McCray et al, 2001) and contains the fol-
lowing semantic types: ?Disease or Syndrome?, ?Neo-
plastic Process?, ?Mental or Behavioral Dysfunction?, 
and ?Sign or Symptom?. Although the schema is not 
complete, it represents a substantial amount of what can 
be said about disorders. Predications produced by Sem-
Rep must conform to this schema in order to be in-
cluded in the conceptual condensate; such predications 
are called ?core predications.? 
Phase 2 (connectivity) is 
 identifies predications occurring in neighboring 
semantic space of the core. This is accomplished by 
retrieving all the predications that share an argument 
with one of the core predications. For example, from 
Naproxen TREATS Osteoarthritis, non-core predica-
tions such as Naproxen ISA NSAID are included in the 
condensate. 
Phase 3 (n
inating predications that have a generic argument, 
as determined by hierarchical depth in the Metathesau-
rus. Arguments occurring less than an empirically de-
termined distance from the root are considered too 
general to be useful, and predications containing them 
are eliminated. For example Pharmaceutical Prepara-
tions TREATS Migraine is not included in the conden-
sate for migraine because ?Pharmaceutical 
Preparations? was determined to be generic.  
Phase 4 (saliency) is the final transforma
 its operations are adapted from TOPIC?s (Hahn and 
Reimer, 1999) saliency operators. Frequency of occur-
rence for arguments, predicates, and predications are 
calculated, and those occurring more frequently than the 
average are kept in the condensate; others are elimi-
nated.  
Whe
dications produced by SemRep for a full-text article 
with 214 sentences (Lisse et al, 2003) concerned with 
comparing naproxen and rofecoxib for treating os-
teoarthritis, with respect to effectiveness and gastroin-
testinal tolerability, the resulting condensate is given in 
Figure 2. (The abstract for this article was summarized 
in Figure 1.) 
 
Figure 2. Semantic abstraction summarization 
of a journal article on osteoarthritis 
4 Multidocument Summarization  
The MEDLINE database, developed and maintained by 
the N  than 12 
million citations (dating from the 1960?s to the present) 
d at the same time retaining differences 
that
ramework for de-
term
Th
results 
for  
Eval ation, especially for 
mult ev et al, 2003). 
It is usually classified as intrinsic (measures the quality 
nd marked the predica-
tion
h 
set
ational Library of Medicine, contains more
drawn from nearly 4,600 journals in the biomedical do-
main. Access is provided by a statistical information 
retrieval system. Due to the size of the database, 
searches often retrieve large numbers of items. For ex-
ample, the query ?diabetes? returns 207,997 citations. 
Although users can restrict searches by language, date 
and publication type (as well as specific journals), re-
sults can still be large. For example, a query for treat-
ment (only) for diabetes, limited to articles published in 
2003 and having an abstract in English finds 3,621 
items; limiting this further to articles describing clinical 
trials still returns 390 citations. We describe the adapta-
tion of our abstraction summarization process to multi-
document input for managing the results of searches in 
MEDLINE.  
Extending summarization to multidocument input 
presents challenges in removing redundancies across 
documents an
 might be important. One issue is devising a frame-
work on which to compute similarities and differences 
across documents. Radev (2000) defines twenty-four 
relationships (such as equivalence, subsumption, and 
contradiction) that might apply at various structural lev-
els across documents. Sub-events (Daniel et al, 2003) 
and sub-topics (Saggion and Lapalme, 2002) also con-
tribute to the framework used for comparing documents 
in multidocument summarization.  
A particular challenge to multidocument summariza-
tion in the extraction paradigm is determining what 
parts of documents conform to the f
ining similarities and differences. A recent study 
(Kan et al, 2001) uses topic composition from text 
headers, but other studies in the extraction paradigm 
(Goldstein et al, 1999), extraction coupled with rhetori-
cal structural identification (Teufel and Moens, 2002), 
and syntactic abstraction paradigms use  different meth-
odologies (Barzilay et al, 1999; McKeown et al, 1999). 
Our semantic abstraction summarization system 
naturally extends to multidocument input with no modi-
fication from the system designed for single documents. 
e disorder schema serves as the framework for identi-
fying sub-topics, and predications retrieved across sev-
eral documents must conform to its structure. 
Informational equivalence (and redundancy) is com-
puted on this basis. For example, all predications that 
conform to the schema line {Treatment} TREATS  
{Disorders} constitute a representation of a subtopic in 
the disorder domain. Exact matches in this set constitute 
redundant information, and other types of relationships 
can be computed on the basis of partial matches. Al-
though we concentrate on similarities across documents, 
differences could be computed by examining predica-
tions that are not shared among citations.  
We have begun testing our system applied to the re-
sults of MEDLINE searches on disorders, concentrating 
on the most recent 300 citations retrieved. The 
 migraine are represented graphically in Figure 3. 
Traversing the predicates (arcs) in this condensate pro-
vides an informative summary of these citations. 
5 Evaluation and Results 
uation in automatic summariz
idocument input, is  daunting (Rad
of the summary as related to the source documents) or 
extrinsic (how the summary affects some other task). 
Since we do not have a gold standard to compare the 
final condensates against, we performed a linguistic 
evaluation on the quality of the condensates generated 
for four diseases: migraine, angina pectoris, Crohn?s 
disease, and pneumonia. The input for each summary 
was 300 MEDLINE citations. 
Table 1 presents evaluation results. The first author 
(MF) examined the source sentence that SemRep used 
to generate each predication a
s as either correct or incorrect. Precision was calcu-
lated as the total number of correct predications divided 
by the total number of predications in the condensate. 
We also measured the reduction (compression) for 
each of the four disorder concepts. In Table 1, ?Base? is 
the number of predications SemRep produced from eac
 of 300 citations. ?Final? is the number of predica-
tions left after the final transformation. Therefore, this is 
a compression ratio on the semantic space of predica-
tions, and is different from text compression in the tradi-
tional sense. 
 
Concept Base Final C I Precision 
Migraine 2485 102 72 30 71% 
A 3 8
ia 
   
ngina 2989 41 3  80% 
Crohn?s 3077 135 71 64 53% 
Pneumon 2694 28 27 1 96% 
Total 11245 306 203 103 66% 
Table 1. ts the f di se 
r  = r
In Crohn?s disease (with lowest precision) a single 
Sem
for 52% of th ocessing the 
sen
 Resul  for our sea concepts  
C = Co rect, I Incor ect 
Rep error type in  argument identification accounts 
e mistakes. For example in pr
tence 36 patients with inflammatory bowel disease 
(11 with ulcerative colitis and 25 with Crohn?s disease), 
the parenthesized material caused SemRep to incor-
rectly  returned ?Inflammatory Bowel Diseases CO- 
OCCURS_WITH Ulcerative Colitis? and ?Ulcerative 
Colitis predicate CO-OCCURS_WITH Crohn?s Dis-
ease.? Word sense ambiguity also contributed to a large 
number of errors.  
6 Content Characterization 
We sformation stage 
has and predications 
during the summarization process. SemRep produced 
S_IN; 
and
pes in the final 
con
he 
fin
nitially parsed, only 63 are represented in 
the
o far do not accommodate. Some of 
the
lusion and Future Directions 
We raction 
summarization that produces conceptual condensates for 
condensate to the text that produced them. We 
als
ional Library of Medicine 
Re
01. Effective mapping of biomedical 
LS Metathesaurus: The MetaMap pro-
Ba
edi-
Ba
e context of multi-document summa-
Bu
and semantic classes in WordNet and the 
Cl
 to text meaning processing. Pro-
Cr
ge.  
examined the effect that  the tran
on the distribution of predicates 
2,485 predications from 300 citations retrieved for mi-
graine. Of these, 1,638 are distributed over four predi-
cates in the disorder schema (327?TREATS; 148?ISA; 
180?LOCATION_OF; 54?CAUSES; 720?
OCCURS_IN; and 209?CO-OCCURS_WITH).  
After phases 1, 2, and 3 of the transformation proc-
ess, 311 predications remain (134?TREATS; 41?ISA; 
12?LOCATION_OF; 5?CAUSES; 68?OCCUR
 51?CO-OCCURS_WITH). This reduction is largely 
due to hierarchical pruning in phase 3. 
Phase 4 operations, based on frequency of occur-
rence pruning (saliency), further condensed the list, and 
the top three TREATS predication ty
densate are (13?Sumatriptan TREATS Migraine; 6?
Botulinum Toxins TREATS Migraine; and 6?feverfew 
extract TREATS Migraine). This list represents the fact 
that Sumatriptan is a popular treatment for migraine.  
Besides frequency, another way of looking at the 
predications is typicality (Kan et al, 2001), or distribu-
tion of predications across citations.  Looking at t
al condensate for migraine and focusing on TREATS, 
the most widely distributed predications are  ?Sumatrip-
tan TREATS Migraine,? which occurs in  ten citations; 
?Botulinum Toxins TREATS Migraine? (three cita-
tions); and  ?feverfew extract TREATS Migraine? (two 
citations). 
One can also view the final condensate from the per-
spective of citations, rather than predications. Of the 
300 citations i
 final condensate, one with six predications, one with 
five predications, three with four predications, and so 
on. It is tempting to hypothesize that more highly rele-
vant citations will have produced more predications, but 
this must be formally tested in the context of the user?s 
retrieval objective.  
An informal examination of the citations that con-
tributed to the final condensate for migraine revealed 
differences that we s
se, such as publication and study type, could be ad-
dressed outside of natural language processing with 
MEDLINE metadata. Others, including medication de-
livery system and target population of the disorder 
topic, are amenable to current processing either through 
extension of the disease schema or enhancements to 
SemRep.  
7 Conc
propose a framework based on semantic abst
disorder topics that are both indicative and informative. 
The approach uses a biomedical semantic processor as 
the source interpreter. After semantic interpretation, a 
series of transformations condense the predications pro-
duced, and a final condensate is displayed in graphical 
form. 
In the future, we would like to link the predications 
in the 
o plan to evaluate the effectiveness of this approach 
in retrieving useful articles for clinical researchers. Fi-
nally, we would like to investigate additional ways of 
visualizing the condensates.  
Acknowledgements The first author was supported 
by an appointment to the Nat
search Participation Program administered by the 
Oak Ridge Institute for Science and Education through 
an inter-agency agreement between the U.S. Department 
of Energy and the National Library of Medicine. 
References 
Aronson AR. 20
text to the UM
gram. Proceedings of the AMIA Symp, pp 17-21. 
tagelj AM. 2003. Pajek - Analysis and Visualization 
of Large Networks. In M. J?nger and P. Mutzel, 
tors, Graph Drawing Software. Springer Verlag, Ber-
lin, pp 77-103. 
rzilay R, McKeown KR, Elhadad M. 1999. Informa-
tion fusion in th
rization. Proceedings of the 37th Annual Meeting of 
the Association of Computational Linguistics, pp 
550-557. 
rgun A, Bodenreider O. 2001. Comparing terms, 
concepts, 
Unified Medical Language System. Proceedings of 
the NAACL Workshop on WordNet and Other Lexical 
Resources: Applications, Extensions and Customiza-
tions, pp 77-82. 
ark P, Harrison P, Thompson J. 2003. A knowledge-
driven approach
ceedings of the HLT-NAACL Workshop on Text 
Meaning, pp 1-6.  
use DA. 1986. Lexical semantics. Cambridge Univer-
sity Press, Cambrid
Cutting D, Kupiec J, Pedersen J, Sibun P. 1992. A prac-
tical part-of-speech tagger. Proceedings of the Third 
Da
marization. Proceedings of HLT-
Fi
. 
Fe
trac-
Ha
6.  
erators for 
Ha
 
Hu
 System: 
Ja
eux P. 2003. Developing the 
Ka
on for 
Le
ment realization. unpublished ms.  
B, 
M
M
d Intelligent Systems. Law-
M
ts. 
M
hniruk A, Pate V, 
M
tion Man-
M
. Proceedings of the Annual Symp Comput 
M
t 1):216-20. 
w Mexico State Univer-
Pu
 Linguistics, 19:331-58. 
e, pp 33-145. 
st ACL 
Ra
Celebi A, Liu D, Drabek E. 2003. Evaluation chal-
Conference on Applied Natural Language Process-
ing, pp 133-40. 
niel N, Radev D, Allison T. 2003. Sub-event based 
multi-document sum
NAACL Workshop on Text Summarization, pp 9-16. 
llmore CJ. 1968. The case for case. In E. Bach and 
RT. Harms, editors, Universals in Linguistic Theory
Holt Rinehart and Winston, New York, pp 1-88. 
llbaum C. 1998. WordNet: An Electronic Lexical Da-
tabase. The MIT Press, Cambridge, MA 
Goldstein J, Mittal V, Carbonell J, Kantrowitz M. 2000. 
Multi-document summarization by sentence ex
tion. Proceedings of the ANLP/NAACL Workshop on 
Automatic Summarization, pp 40-48.  
hn U, Mani I. 2000. The challenges of automatic 
summarization. Computer, 33(11):29-3
Hahn U, Reimer U. 1999. Knowledge-based text sum-
marization: salience and generalization op
knowledge base abstraction. In I. Mani and MT. 
Maybury, editors, Advances in Automatic Text Sum-
marization. MIT Press, Cambridge, pp 215-32.  
rabagiu S, Moldovan D, Pasca M, Mihalcea R, 
Surdeanu M, Bunescu R; Girju R, Rus V, Morarescu
P. 2001. The role of lexico-semantic feedback in 
open-domain textual question-answering. Proceed-
ings of the 39th Annual Meeting of the Association 
for Computational Linguistics, pp 274-81.  
mphreys BL, Lindberg DA, Schoolman HM, Barnett 
GO. 1998. The Unified Medical Language
An informatics research collaboration. J Am Med In-
form Assoc, 5(1):1-11. 
cquelinet C, Burgun A, Delamarre D, Strang N, Djab-
bour S, Boutin B, Le B
ontological foundations of a terminological system 
for end-stage diseases, organ failure, dialysis and 
transplantation. Int J Med Inf, 70(2-3):317-28. 
n M, McKeown KR, Klavans JL. 2001. Domain-
specific informative and indicative summarizati
information retrieval. Workshop on Text Summariza-
tion (DUC3).  
vin B, Rappaport Hovav M. 1996. From lexical se-
mantics to argu
Lisse JR, Perlman M, Johansson G, Shoemaker JR, 
Schechtman J, Skalky CS, Dixon ME, Polis A
Mollen AJ, Geba GP. 2003. Gastrointestinal toler-
ability and effectiveness of rofecoxib versus 
naproxen in the treatment of osteoarthritis: a random-
ized, controlled trial. Ann Intern Med, 139(7):539-46. 
ani I, Gates B, Bloedorn E. 1999. Improving summa-
ries by revising them. Proceedings of the 37th An-
nual Meeting of the Association of Computational 
Linguistics, pp 558-65. 
cDonald DD. 1992. Robust partial parsing through 
incremental, multi-algorithm processing. In PS Ja-
cobs, editor, Text-base
rence Erlbaum Associates, New Jersey, pp 83-99. 
cKeown KR, Klavans JL, Hazivassiloglou V, Barzi-
lay R., Eskin E. 1999. Towards multidocument sum-
marization by reformulation: progress and prospec
Proceedings of the Sixteenth National Conference on 
Artificial Intelligence, pp 453-60. 
cKeown HR, Chang, SF, Cimino J, Feiner SK, 
Friedman C, Gravano L, Hatzivassiloglou V, Johnson 
S,  Jordan DA, Klavans JL, Kus
Teufel S. 2001. PERSIVAL, a system for personal-
ized search and summarization over multimedia 
healthcare information. JCDL, pp 331-40. 
cCray AT. 1993. Representing biomedical knowledge 
in the UMLS Semantic Network. High-Performance 
Medical Libraries: Advances in Informa
agement for the Virtual Era. Meckler Publishing, pp 
45-55. 
cCray AT, Srinivasan S, Browne AC. 1994. Lexical 
methods for managing variation in biomedical termi-
nologies
Appl Med Care, pp:235-9. 
cCray AT, Burgun A, Bodenreider O. 2001. Aggre-
gating UMLS semantic types for reducing conceptual 
complexity.  Medinfo, 10(P
Mihalcea R, Moldovan D. 2000. Semantic indexing 
using WordNet senses. Proceedings of the ACL 
Workshop on IR and NLP. 
Nirenburg S, Raskin V. 1996. Ten choices for lexical 
semantics. Memoranda in Computer and Cognitive 
Science. MCCS-96-304. Ne
sity. 
stejovsky J., Bergler S, Anick P. 1993. Lexical se-
mantic techniques for corpus analysis. Computa-
tional
Raphael B. 1968. SIR: Semantic information retrieval. 
In Minsky, M. (ed.) Semantic Information Process-
ing. The MIT Press, Cambridg
Radev D. 2000. A Common theory of information fu-
sion from multiple text sources, step one: cross-
document structure. Proceedings of 1
SIGDIAL Workshop on Discourse and Dialogue. 
dev D, Teufel S, Saggion H, Lam W, Blitzer J, Qi H, 
lenges in large-scale multi-document summarization: 
the MEAD project. Proceedings of ACL. 
Rindflesch TC, Bean CA, Sneiderman CA. 2000. Ar-
gument identification for arterial branching predica-
tions asserted in cardiac catheterization reports. 
Proceedings of the AMIA Symp, pp 704-8. 
:462-77.  
Sal
Sri ring text mining 
Sp tomatic summarizing: factors 
Sc
Publishing Co, Amster-
Te
 Experiments with relevance and rhetorical 
Te
di-
Vi
icative Forms in 
W
cs: An in-
Advances in Automatic Text Summarization. MIT 
Press, Cambridge, pp 1-13. 
hank RC. 1975. Conceptual information processing. 
Amsterdam. North-Holland 
dam, 
ufel S, Moens M. 2002. Summarizing scientific arti-
cles -Rindflesch TC, Fiszman M. 2003. The interaction of 
domain knowledge and linguistic structure in natural 
language processing: interpreting hypernymic propo-
sitions in biomedical text. J Biomed Infor, 36
status. Computational Linguistics, 28(4):409-445. 
nny C, Pustejovsk J. 2000. A history of events in lin-
guistic theory. In C. Tenny and J. Pustejovsky, e
tors, Events as Grammatical Objects, CSLI 
Publications, Stanford, pp 3-37. 
egas E, Mahesh K, Nirenburg  S. 1998. Semantics in 
action. In P. St. Dizier, editor, Pred
Saggion H, Lapalme G. Generating indicative-
informative summaries with SumUM. 2002. Compu-
tational Linguistics, 28(4):497-526. 
ton G, Wong A, Yang CS. 1975. A vector space 
Natural Language and in Lexical Knowledge Bases. 
Kluwer Academic Publishers, Dordrecht.  
ilks YA. 1976. Parsing English II. In E. Charniak and 
Y. Wilks, editors, Computational semanti
model for automatic indexing. Communications of 
the ACM, (18):613-20. 
nivasan P, Rindflesch T. 2002. Explo
from MEDLINE. Proceedings of the AMIA Symp, pp 
722-6. 
arck Jones K. 1999. Au
troduction to artificial intelligence and natural lan-
guage comprehension. North Holland Publishing 
Company, Amsterdam, pp 155-84. .and directions In I. Mani and MT. Maybury, editors, 
 
Figure 3. Semantic abstraction summarization on citations retrieved for migraine. Arrow thickness re-
flects redundant information (i.e. informational equivalence of sentences across multiple documents)  
Using Natural Language Processing, Locus Link, and the  
Gene Ontology to Compare OMIM to MEDLINE 
 
 
Bisharah 
Libbus 
Halil 
Kilicoglu 
Thomas C. 
Rindflesch 
James G. 
Mork 
Alan R. 
Aronson 
Lister Hill National Center for Biomedical Communications 
National Library of Medicine 
Bethesda, Maryland, 20894 
{libbus|halil|tcr|mork|alan}@nlm.nih.gov 
 
 
Abstract 
Researchers in the biomedical and molecular 
biology fields are faced with a wide variety of 
information sources. These are presented in 
the form of images, free text, and structured 
data files that include medical records, gene 
and protein sequence data, and whole genome 
microarray data, all gathered from a variety of 
experimental organisms and clinical subjects. 
The need to organize and relate this informa-
tion, particularly concerning genes,  has moti-
vated the development of resources, such as 
the Unified Medical Language System, Gene 
Ontology, LocusLink, and the Online Inheri-
tance In Man (OMIM) database. We describe 
a natural language processing application to 
extract information on genes from unstruc-
tured text and discuss ways to integrate this 
information with some of the available online 
resources.  
1 Introduction 
The current knowledge explosion in genetics and ge-
nomics poses a challenge to both researchers and medi-
cal practitioners. Traditionally, scientific reviews, which 
summarize and evaluate the literature, have been indis-
pensable in addressing this challenge. OMIM (Online 
Mendelian Inheritance in Man) (OMIM 2000), for ex-
ample, is a clinical and biomedical information resource 
on human genes and genetic disorders. It has close to 
15,000 entries detailing clinical phenotypes and disor-
ders as well as information on nearly 9,000 genes. The 
database can be searched by gene symbol, chromosomal 
location, or disorder. 
More recently, automated techniques for information 
and knowledge extraction from the literature are being 
developed to complement scientific reviews. These 
methods address the need to condense and efficiently 
present large amounts of data to the user.  The feasibil-
ity of applying natural language processing techniques 
to the biomedical literature (Friedman and Hripcsak 
1999; de Bruijn and Martin 2002) and to the wealth of 
genomics data now available (Jenssen et al 2001; Yan-
dell and Majoros 2002) is increasingly being recog-
nized. Efforts to develop systems that work toward this 
goal focus on the identification of such items as gene 
and protein names (Tanabe and Wilbur 2002) or groups 
of genes with similar function (Jenssen et al 2001; 
Masys et al 2001). Other groups are interested in identi-
fying protein-protein (Blaschke et al 1999; Temkin and 
Gilder 2003) or gene-gene interactions (Stephens et al 
2001; Tao et al 2002), inhibit relations (Pustejovsky et 
al. 2002), protein structure (Gaizauskas et al 2003), and 
pathways (Ng and Wong 1999; Friedman et al 2001). 
We discuss the modification of an existing natural 
language processing system, SemGen (Rindflesch et al 
2003), that has broad applicability to biomedical text 
and that takes advantage of online resources such as 
LocusLink and the Gene Ontology. We are pursuing 
research that identifies gene-gene interactions in text on 
genetic diseases. For example the system extracts (2) 
from (1).  
1) Here, we report that TSLC1 directly associates 
with MPP3, one of the human homologues of a 
Drosophila tumor suppressor gene, Discs large 
(Dlg). 
2) TSLC1|INTERACT_WITH|MPP3 
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 69-76.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
Due to the complexity of the language involved, the 
extraction of such predications is currently not accurate 
enough to support practical application. However, we 
suggest its potential in the context of an application that 
combines traditional, human-curated resources such as 
OMIM and emerging information extraction applica-
tions. 
2 
3 SemGen 
Molecular Biology Resources 
To support and supplement the information extracted by 
SemGen from biomedical text, we draw on two re-
sources, LocusLink and the Gene Ontology. LocusLink 
(Wheeler et al 2004) provides a single query interface 
to curated genomic sequences and genetic loci. It pre-
sents information on official nomenclature, aliases, se-
quence accessions, phenotypes, OMIM numbers, 
homology, map locations, and related Web sites, among 
others. Of particular interest is the Reference Sequence 
(RefSeq) collection, which provides a comprehensive, 
curated, integrated, non-redundant set of sequences, 
including genomic DNA, transcript (RNA), and protein 
products for major research organisms. Currently, 
SemGen uses LocusLink to obtain normalized gene 
names and Gene Ontology annotations.  
The Gene Ontology (GO) (The Gene Ontology Con-
sortium 2000, 2001, 2004) aims to provide a dynamic 
controlled vocabulary that can be applied to all organ-
isms, even while knowledge of gene and protein func-
tion is incomplete or unfolding. The GO consists of 
three separate ontologies: molecular function, biological 
process, and cellular component. These three branches 
are used to characterize gene function and products and 
provide a comprehensive structure that permits the an-
notation of molecular attributes of genes in various or-
ganisms. We use GO annotations to examine whether 
there are identifiable patterns, or concordance, in the 
function of gene pairs identified by SemGen.  
SemGen identifies gene interaction predications based  
on semantic interpretation adapted from SemRep (Srini-
vasan and Rindflesch 2002; Rindflesch and Fiszman 
2003), a general natural language processing system 
being developed for the biomedical domain. After the 
application of  a statistically-based labeled categorizer 
(Humphrey 1999) that limits input text to the molecular 
biology domain, SemGen processing proceeds in  three 
major phases: categorial analysis, identification of con-
cepts, and identification of relations.  
The initial phase relies on a parser that draws on the 
SPECIALIST Lexicon (McCray et al 1994) and the 
Xerox Part-of-Speech Tagger (Cutting et al 1992) to 
produce an underspecified categorial analysis.  
In the phase for identifying concepts, disorders as 
well as genes and proteins are isolated by mapping sim-
ple noun phrases from the previous phase to concepts in 
the Unified Medical Language System? (UMLS)? 
Metathesaurus? (Humphreys et al 1998), using 
MetaMap (Aronson 2001). ABGene, a program that 
identifies genes and proteins using several statistical and 
empirical methods (Tanabe and Wilbur 2002) is also 
consulted during this phase. In addition, a small list of 
signal words (such as gene, codon, and exon) helps 
identify genetic phenomena. For example, the genetic 
phenomena in (4) are identified from the sentence in (3). 
Concepts isolated in this phase serve as potential argu-
ments in the next phase.  
3) WIF1 was down-regulated in 64% of primary 
prostate cancers, while SFRP4 was up-regulated 
in 81% of the patients. 
4) genphenom|WIF1  
genphenom|SFRP4 
 
During the final phase, in which relations are identi-
fied, the predicates of semantic propositions are based 
on indicator rules. These stipulate verbs, nominaliza-
tions, and prepositions that ?indicate? semantic predi-
cates. During this phase, argument identification is 
constrained by an underspecified dependency grammar, 
which also attempts to accommodate coordinated argu-
ments as well as predicates.  
SemGen originally had twenty rules indicating one 
of three etiology relations between genetic phenomena 
and diseases, namely CAUSE, PREDISPOSE, and 
ASSOCIATED_WITH. In this project, we extended Sem-
Gen to cover gene-gene interaction relations: INHIBIT, 
STIMULATE, AND INTERACT_WITH. About 20 indicator 
rules were taken from MedMiner (Tanabe et al 1999). 
We supplemented this list by taking advantage of the 
verbs identified in syntactic predications by GeneScene 
(Leroy et al 2003). SemGen has 46 gene-gene interac-
tion indicator rules (mostly verbs), including 16 for 
INHIBIT (such as block, deplete, down-regulate); 12 for 
INTERACT_WITH (bind, implicate, influence, mediate); 
and 18 for STIMULATE (amplify, activate, induce, up-
regulate).  
An overview of the SemGen system is given in Fig-
ure 1, and an example is provided below. SemGen proc-
essing on input text (5) produces the underspecified 
syntactic structure (represented schematically) in (6). 
(7) illustrates genetic phenomena identified, and (8) 
shows the final semantic interpretation. 
 
 
 
Figure 1. SemGen system  
 
5) We show here that EGR1 binds to the AR in 
prostate carcinoma cells, and an EGR1-AR 
complex can be detected by chromatin im-
munoprecipitation at the enhancer of an en-
dogenous AR target gene. 
6) [We] [show] [here] [that] [EGR1] [binds] [to 
the AR] [in prostate carcinoma cells,] [and] [an 
EGR1-AR complex] [can] [be] [detected] [by 
chromatin immunoprecipitation] [at the enhan-
cer] [of an endogenous AR target gene] 
7) genphenom|egr1 
genphenom|ar 
genphenom|enhancer endogenous ar target gene 
8) egr1|INTERACT_WITH|ar 
During processing, SemGen normalizes gene sym-
bols using the preferred symbol from LocusLink. The 
final interpretation with LocusLink gene symbol is 
shown in (9).  
9) EGR1|INTERACT_WITH|AR 
As we retrieve the LocusLink symbol for a gene, we 
also get the GO terms associated with that gene. We are 
interested in extending the application of our textual 
analysis and knowledge extraction methodology and 
relating it to other biomedical and genomic resources. 
Gene Ontology is one such important resource, and be-
low we discuss the possibility that GO might shed addi-
tional light on the biological relationship between genes 
that are paired functionally based on textual analysis. 
The GO terms for the genes in (9) are given in (10) and 
(11). 
 
10) EGR1|[transcription factor activity; regulation 
of transcription, DNA-dependent; nucleus] 
11) AR|[androgen receptor activity; steroid binding; 
receptor activity; transcription factor activity; 
transport; sex differentiation; regulation of tran-
scription, DNA-dependent; signal transduction; 
cell-cell signaling; nucleus] 
4 SemGen Evaluation and Error Analysis 
Before suggesting an application using SemGen output, 
we discuss the results of error analysis performed on 
344 sentences from MEDLINE citations related to six 
genetic diseases: Alzheimer's disease, Crohn?s disease, 
lung cancer, ovarian cancer, prostate cancer and sickle 
cell anemia. Out of 442 predications identified by Sem-
Gen, 181 were correct, for 41% precision. This is not 
yet accurate enough to support a production system; 
however, the majority of the errors are focused in two 
syntactic areas, and we believe that with further devel-
opment it is possible to provide output effective for 
supporting practical applications. 
The majority of the errors fall into one of two major 
syntactic classes, relativization and coordination. A fur-
ther source of error is the fact that we have not yet ad-
dressed interaction relations that involve a process in 
addition to a gene.  
Reduced relative clauses, such as mediated by Tip60 
in (12), are a rich source of argument identification er-
rors.  
12) LRPICD dramatically inhibits APP-derived in-
tracellular domain/Fe65 transactivation medi-
ated by Tip60.  
SemGen wrongly interpreted this sentence as asserting 
that LRPICD inhibits Tip60. The rules of the under-
specified dependency grammar that identify arguments 
essentially look to the left and right of a verb for a noun 
phrase that has been marked as referring to a genetic 
phenomenon. Arguments are not allowed to be used in 
more than one predication (unless licensed by coordina-
tion or as the head of a relative clause).  
A number of phenomena conspire in (12) to wrongly 
allow TIP60  to be analyzed as the object of inhibits. 
The actual object, transactivation, was not recognized 
because we have not yet addressed processes as argu-
ments of gene interaction predications. Further, the 
predication on transactivation, with argument TIP60, 
was not interpreted, and hence TIP60 was available (in-
correctly) for the object of inhibits.  If we had recog-
nized the relative clause in (12), TIP60 would not have 
been reused as an argument of inhibits, since only heads 
of relative clauses can be reused. 
The underspecified analysis on which SemGen is 
based is not always effective in identifying verb phrase 
coordination, as in (13), leading to the incorrect inter-
pretation that WIF1 interacts with SFRP4. 
13) WIF1 was down-regulated in 64% of primary 
prostate cancers, while SFRP4 was up-regulated 
in 81% of the patients.  
A further source of error in this sentence is that 
down-regulated was analyzed by the tagger as a past 
tense rather than past participle, thus causing the argu-
ment identification phase to look for an object to the 
right of this verb form. A further issue here is that we 
have not yet addressed truncated passives.  
5 Using SemGen to Compare OMIM and 
MEDLINE  
SemGen errors notwithstanding, we are investigating 
possibilities for exploiting automatically extracted gene 
interaction predications. We discuss an application 
which compares MEDLINE text to OMIM documents, 
for specified diseases. LocusLink preferred gene sym-
bols and GO terms are an integral part of this process-
ing. We feel it is instructive to investigate the 
consequences of this comparison, anticipating results 
that are effective enough for practical application. 
We selected five diseases with a genetic component 
(Alzheimer?s disease, Crohn?s disease, lung cancer, 
prostate cancer, and sickle cell anemia), and retrieved 
the corresponding OMIM report for each disease, auto-
matically discarding sections such as references, head-
ings, and edit history. We also queried PubMed for each 
disease and retrieved all MEDLINE citations that were 
more recent than the corresponding OMIM report. Both 
OMIM and MEDLINE files were then submitted to 
SemGen.  
For each disease, the MEDLINE file was larger than 
the corresponding OMIM file, and the categorizer 
eliminated some parts of each file as not being in the 
molecular biology domain. Table 1 shows the number 
of sentences in the original input files and the number 
processed after the categorizer eliminated sentences not 
in the molecular biology domain.  
 
 
 OMIM 
Orig. 
OMIM 
Proc. 
MEDLINE 
Orig. 
MEDLINE 
Proc. 
Alz 408 264 1639 862 
Crohn 188 124 4871 1236 
LungCa 55 34 9058 2966 
ProstCa 121 69 6989 2964 
SCA 184 79 4383 1057 
 
    Table 1. Input sentences processed by SemGen 
A paragraph in the OMIM file for Alzheimer?s dis-
ease beginning with the sentence Alzheimer disease is 
by far the most common cause of dementia, for example, 
was eliminated, while a MEDLINE citation with the 
title Semantic decision making in early probable AD: A 
PET activation study was removed.   
An overview of predication types retrieved by Sem-
Gen is given in Table 2 for the files on Alzheimer?s 
disease. Of the gene-disease predications, the majority 
had predicate ASSOCIATED_WITH (15 from OMIM and 
25 from MEDLINE). For gene-gene relations, 
INTERACT_WITH predominated (3 from OMIM and 12 
from MEDLINE). 
 
Alzheimer disease OMIM MEDLINE 
Gene-Disease 16 31 
Gene-Gene  3 22 
Total 19 53 
 
Table 2. Gene interaction predication types 
 
We developed a program that compares semantic 
predications found in MEDLINE abstracts to those 
found in an OMIM report associated with a particular 
disease and classifies the comparison between two 
predications as either an exact match, partial match, or 
no match. The category of a comparison is determined 
by examining the argument and predicate fields of the 
predications. If all three fields match, the comparison is 
an exact match; if any two fields match it is a partial 
match. All other cases are considered as no match.  
Although fewer than half of the predications ex-
tracted by SemGen are likely to be correct, we provide 
some examples from the files on Alzheimer?s disease.  
(The system retains the document ID?s, which are sup-
pressed here for clarity.) Examples of partial matches 
between gene-disease predications extracted from 
OMIM and MEDLINE are shown in (14) and (15).  
14) OM: APP | ASSOCIATED_WITH | Alz-
heimer?s Disease 
ML:  CD14 | ASSOCIATED_WITH | Alz-
heimer?s Disease 
15) OM: amyloid beta peptide | 
ASSOCIATED_WITH | Alzheimer?s Disease 
ML: amyloid beta peptide | 
ASSOCIATED_WITH | Senile Plaques 
Some of the gene-disease predications that only oc-
curred in OMIM are given in (16), and a few of those 
occurring exclusively in MEDLINE are given in (17). 
 
16) TGFB1 | ASSOCIATED_WITH | Amyloid 
deposition  
        PRNP | ASSOCIATED_WITH | Amyloid 
deposition 
        Mutation 4 gene | CAUSE | Alzheimer?s Dis-
ease 
 
17) MOG | ASSOCIATED_WITH | Nervous Sys-
tem Diseases 
Acetylcholinesterase | PREDISPOSE | Alz-
heimer?s Disease  
 
In (18) are listed some of the gene-gene interaction 
predications found in MEDLINE but not in OMIM. 
18) LAMR1 | STIMULATE | HTATIP 
MAPT|INTERACT_WITH | HSPA8  
CD14 | STIMULATE | amyloid peptide 
 
6 Using the GO Terms  
As noted above, for each gene argument in the predica-
tions identified by SemGen, we retrieved from Locus-
Link the GO terms associated with that gene. We have 
begun to investigate ways in which these terms might be 
used to compare genes by looking at the gene-gene in-
teraction predications extracted from MEDLINE that 
did not occur in OMIM.  
To support this work, we developed a program that 
sorts gene-gene interaction predications by the GO 
terms of their arguments. For each gene function, the 
predications in which both arguments share the same 
function are listed first. These are followed by the 
predications in which only the first argument has that 
gene function, and then the predications in which only 
the second argument has the relevant gene function. A 
typical output file of this process is shown in (19): 
 
19) RECEPTOR ACTIVITY 
        ----------------- 
        Both Arguments: 
        DTR|STIMULATE|EGFR 
        First Argument: 
        AR|STIMULATE|TRXR3 
        EPHB2|STIMULATE|ENO2 
        Second Argument: 
        EGR1|INTERACT_WITH|AR 
        PSMC6|STIMULATE|AR 
 
The three branches of the Gene Ontology provide a 
uniform system for relating genes by function. The 
terms in the molecular function and biological process 
branches are perhaps most useful for this purpose; how-
ever, we have begun by considering all three branches 
(including the cellular component branch). The most 
effective method of exploiting GO annotations remains 
a matter of research.  
It is important to recognize that GO mapping is not 
precise; different annotators may make different GO 
assignments for the same gene. Nevertheless, GO anno-
tations provide considerable potential for relating the 
molecular functions and biological processes of genes. 
We consider one of the predications extracted from the 
MEDLINE file for prostate cancer that did not occur in 
OMIM: 
19) EGR1|INTERACT_WITH|AR 
Both genes EGR1 and AR in LocusLink elicit the 
same human gene set (367 Hs AR; 1026 Hs CDKN1A; 
1958 Hs EGR1; 3949 Hs LDLR; 4664 Hs NAB1; 4665 
Hs NAB2; 5734 Hs PTGER4; 114034 Hs TOE1). This 
suggests a high degree of sequence homology and func-
tional similarity. In addition, LocusLink provides the 
following GO terms for the two genes: 
20) EGR1: early growth response 1; LocusID: 1958 
Gene Ontology: transcription factor activity; 
regulation of transcription, DNA-dependent; 
nucleus  
21)  AR: androgen receptor (dihydrotestosterone re-
ceptor; testicular feminization; spinal and bulb 
ar muscular atrophy; Kennedy disease) ; Lo-
cusID: 367 Gene Ontology:  androgen receptor 
activity; steroid binding; receptor activity; tran-
scription factor activity; transport; sex differen-
tiation; regulation of transcription, DNA-
dependent; signal transduction; cell-cell signal-
ing; nucleus 
(The GO provides additional, hierarchical information 
for terms, which we have not yet exploited.) 
Thirty percent of the predications examined had 
some degree of overlap in their GO terms. For example, 
the terms for EGR1 (transcription factor activity; regu-
lation of transcription, DNA- dependent; and nucleus) 
are identical to three of the GO terms for the AR gene. 
This concordance may not be typical of the majority of 
paired genes in our sample. However, in the case of 
genes that do not exhibit such complete overlap, con-
cordance might be obtained at higher nodes in the clas-
sification scheme. 
An alternate approach for assessing distance be-
tween GO annotations has been suggested by Lord et al 
(2003a, 2003b). They propose a ?semantic similarity 
measure? using ontologies to explore the relationships 
between genes that may have associated interaction or 
function. The authors consider the information content 
of each GO term, defined as the number of times each 
term, or any child term, occurs. 
The fact that any one gene has a number of GO an-
notations indicates that a particular gene may perform 
more than one function or its function may be classified 
under a number of molecular activities. Some of these 
activities may be part of, i.e. extending to a variable 
degree down, the same GO structure. For example, for 
gene AR, ?receptor activity? (GO 4872) partially over-
laps with ?androgen receptor activity? (GO 4882), as 
does ?steroid binding? (GO 5496) with ?transcription 
factor activity? (GO 3700), and ?signal transduction 
(GO 7165) and ?cell-cell signaling (GO 7267). This 
indicates that in assessing similarity one needs to exam-
ine the ontology structure and not rely solely on the GO 
terms.  
While we have no experimental evidence, we would 
like to speculate about the functional or biological sig-
nificance indicated by similarity in GO annotation. 
There are three orthogonal aspects to GO: molecular 
function, biological process, and cellular component. If 
two genes map more closely in one of the taxonomies, 
then their function is necessarily more closely related. 
The majority of GO terms are in the molecular function 
taxonomy. It is conceivable that genes that map more 
closely could be involved in the same cascade or par-
ticipate in the same genetic regulatory network. There is 
increasing interest in genetic networks (e.g. 
www.genome.ad.jp/kegg/ kegg2.html; http://ecocyc.org; 
http://us.expasy.org/tools/pathways; www.biocarta.com) 
and combining the ability to search and extract informa-
tion from the literature with GO mapping could prove 
effective in elucidating the functional interactions of 
genes. 
 
7 
8 Conclusion 
Potential Knowledge Discovery 
To determine whether our automatic comparison of 
MEDLINE to OMIM based on SemGen predications 
might throw new light on  gene-gene interactions, we 
examined predications found in the MEDLINE file that 
had no match in the OMIM file. We searched the 
OMIM reports for information on the genes found in 
such predications to confirm that they were absent from 
the OMIM reports. For example, while the OMIM re-
port on colon cancer did not mention BARD1, the 
SemGen output for MEDLINE had   
22) BARD1|INTERACT_WITH|hmsh2 
The abstract containing this predication (PMID 
11498787) asserts that the BARD1 gene (LocusID 580) 
interacts with the breast cancer gene BRCA1 as well as 
with hMSH2, a mismatch repair gene associated with 
colon cancer.  BARD1 shares homology with the two 
conserved regions of BRCA1 and also interacts with the 
N-terminal region of BRCA1. Interaction of BARD1 
with BRCA1 could be essential for the function of 
BRCA1 in tumor suppression.  
Conversely, disruption of this interaction may possi-
bly contribute to the process of oncogenesis. It has been 
reported that the BRCA1/BARD1 complex is responsi-
ble for many of the tumor suppression activities of 
BRCA1 (Baer and Ludwig 2002). The gene hMSH2 
(LocusID 4436) is one of a number of genes that, when 
mutated, predisposes to colon cancer type 1. It is the 
human homolog of the bacterial mismatch repair gene 
mutS. We hypothesize that the interaction of BARD1 
with hMSH2, in a similar fashion to BRCA1, may be 
necessary for tumor suppression. Disruption of this in-
teraction may increase the likelihood of developing co-
lon cancer. Furthermore, this observation serves to point 
toward a possible link between BRCA1 and colon can-
cer. 
 
We have extended earlier work with SemGen (Rind-
flesch et al 2003) and are now able to extract from text, 
in addition to names of gene and disorders, gene-
disorder and gene-gene relations. Although SemGen is 
not at a stage where it can be used indiscriminately and 
without selective review and evaluation, it may never-
theless prove useful for reviewers by providing an effi-
cient means of scanning a large number of references 
and extracting relations involving genes and diseases. 
The process of curation and review is time consum-
ing. Given the rate at which new publications are added 
to the scientific literature, the availability of tools for 
accelerating the review process would meet a real need. 
As demonstrated by our pilot study on six disorders, 
SemGen could prove useful, even at this prototype 
stage, in extracting relevant information from the litera-
ture concerning genes and diseases. Additionally, the 
ability to scan and extract information from diverse sci-
entific domains could play an important role in identify-
ing new relationships between genes and diseases that 
would promote hypothesis-generation and advance sci-
entific research. Even with the present limitations, 
SemGen could assist in making the scientific literature 
more accessible and reduce the time it takes for re-
searchers to update their knowledge and expertise. 
References 
Aronson, A.R. (2001). ?Effective mapping of biomedi-
cal text to the UMLS Metathesaurus: the MetaMap 
program.? In Proceedings of the AMIA Annual Sym-
posium, 17-21. 
Baer, R., and Ludwig, T. (2002). ?The BRCA1/BARD1 
heterodimer: a tumor suppressor complex with ubiq-
uitin E3 ligase activity.? Current Opinion in Genetics 
& Development, 12, 86-91 
Blaschke, C.; Andrade, M.A.; Ouzounis, C.; and Valen-
cia, A. (1999). ?Automatic extraction of biological 
information from scientific text: protein-protein in-
teractions.? In Proceedings of the Seventh Interna-
tional Conference on Intelligent Systems for 
Molecular Biology, 60-70. 
de Bruijn, B., and Martin, J. (2002). ?Getting to the 
(c)ore of knowledge: mining biomedical literature.? 
International Journal of Medical Informatics, 67, 7-
18. 
Cutting, D.; Kupiec, J.; Pedersen, J.; and Sibun, P. 
(1992). ?A practical part-of-speech tagger.? In Pro-
ceedings of the Third Conference on Applied Natural 
Language Processing. 
Friedman, C., and Hripcsak, G. (1999). ?Natural lan-
guage processing and its future in medicine.? Aca-
demic Medicine, 74 (8),890-5. 
Friedman, C.; Kra, P.; Yu, H.; Krauthammer, M.; and 
Rzhetsky, A. (2001). ?GENIES: a natural-language 
processing system for the extraction of molecular 
pathways from journal articles.? Bioinformatics, 17 
Suppl 1, S74-82. 
Gaizauskas, R; Demetriou, G.; Artymiuk, P.J.; and  
Willett, P. (2003). ?Protein Structures and Informa-
tion Extraction from Biological Texts: The PASTA 
System.? Bioinformatics, 19, 135-43. 
The Gene Ontology Consortium. (2000). ?Gene ontol-
ogy: tool for the unificaiton of biology.? Nature, 25, 
25-29. 
The Gene Ontology Consortium. (2001). ?Creating the 
Gene Ontology Resource: Design and implementa-
tion.? Genome Research, 11,1425-1433. 
The Gene Ontology Consortium. (2004). ?The Gene 
Ontology (GO) database and informatics resource.? 
Nucleic Acids Research, 32,D258-D261. 
Humphrey, S. (1999). ?Automatic indexing of docu-
ments from journal descriptors: A preliminary inves-
tigation.? Journal of the American Society for 
Information Science, 50(8), 661-74. 
Humphreys, B.L.; Lindberg, D.A.; Schoolman, H.M.; 
and Barnett, G.O. (1998). ?The Unified Medical lan-
guage System: An informatics research collabora-
tion.? Journal of American Medical Informatics 
Association, 5(1), 1-13. 
Jenssen, T.K.; Laegreid, A.; Komoroswski, J.; and 
Hovig, E. (2001). ?A literature network of human 
genes for high-throughput analysis of gene expres-
sion.? Nature Genetics, 28,21-28.  
Leroy, G.; Chen, H.; Martinez, J.D. (2003) ?A shallow 
parser based on closed-class words to capture rela-
tions in biomedical text.? Journal of Biomedical In-
formatics, 36, 145-58 . 
Lord, P.W.; Stevens, R.D.; Brass, A.; and Goble, C.A. 
(2003a). ?Investigating semantic similarity measures 
across the Gene Ontology: the relationship between 
sequence and annotation.? Bioinformatics 19:1275-
1283. 
Lord, P.W.; Stevens, R.D.; Brass, A.; and Goble, C.A. 
(2003b). ?Semantic similarity measures as tools for 
exploring the Gene Ontology.? Pacific Symposium 
on. Biocomputing,  601-612. 
Masys, D.R.; Welsh, J.B.; Fink, J.L.; Gribskov, M.; 
Klacansky, I.; and Vorbeil, J. (2001). ?Use of key-
word hierarchies to interpret gene expression pat-
terns.? Bioinformatics, 17(4), 319-26. 
McCray, A.T.; Srinivasan, S.; and Browne, A.C. (1994). 
?Lexical methods for managing variation in biomedi-
cal terminologies.? In Proceedings of the Annual 
Symposium on Computer Applications in Medical 
Care, 235-9. 
Ng, S.K., and Wong, M. (1999). ?Toward routine auto-
matic pathway discovery from on-line scientific text 
abstracts.? Genome Informatics, 10,104-112. 
Online Mendelian Inheritance in Man, OMIM (2000). 
McKusick-Nathans Institute for Genetic Medicine, 
Johns Hopkins University (Baltimore, MD) and Na-
tional Center for Biotechnology Information, Na-
tional Library of Medicine (Bethesda, MD). WWW 
URL: http://www.ncbi.nlm.nih.gov/omim/.  
Pustejovsky, J.; Castano, J.; Zhang, J.; Kotecki, M.; and 
Cochran, B. (2002). ?Robust relational parsing over 
biomedical literature: extracting inhibit relations.? 
Pacific Symposium on Biocomputing, 362-73. 
Rindflesch, T. C., and Fiszman, M. (2003). ?The inter-
action of domain knowledge and linguistic structure 
in natural language processing: Interpreting hy-
pernymic propositions in biomedical text.? Journal of 
Biomedical Informatics, 36(6):462-77. 
Rindflesch, T. C.; Libbus, B.; Hristovski, D.; Aronson, 
A.R.; and Kilicoglu, H. (2003). ?Semantic relations 
asserting the etiology of genetic diseases.? In Pro-
ceedings of the AMIA Annual Symposium, 554-8. 
Srinivasan, P., Rindflesch, T.C. (2002). ?Exploring Text 
Mining from MEDLINE.? In Proceedings of the 
AMIA Annual Symposium, 722-6. 
Stephens, M.; Palakal, M.; Mukhopadhyay, S.; and 
Raje, R. (2001). ?Detecting gene relations form Med-
line abstracts.? In Proceedings of the Sixth Pacific 
Symposium on Biocomputing, 6, 483-96. 
Tanabe, L.; Scherf, U.; Smith, L.H.; Lee, J.K.; Hunter, 
L.; Weinstein, J.N. (1999). ?MedMiner: An Internet 
text- mining tool for biomedical information, with 
application to gene expression profiling.? BioTech-
niques, 27(6),1210-17. 
Tanabe, L., Wilbur, W.J. (2002). ?Tagging gene and 
protein names in biomedical text.? Bioinformatics, 
18(8), 1124-32. 
Tao, Y-C., and Leibel, R.L. (2002). ?Identifying rela-
tionships among human genes by systematic analysis 
of biological literature.? BMC Bioinformatics, 3,16-
25. 
Temkin, J. M., and Gilder, M. R. (2003). ?Extraction of 
protein interaction information from unstructured text 
using a context-free grammar.? Bioinformatics, 
19(16), 2046-53. 
Wheeler, D.L.; Church, D.M.; Edgar, R.; Federhen, S.; 
Helmberg, W.; Madden, T.L.; Pontius, J.U.; Schuler, 
G.D.; Schriml, L.M.; Sequeira, E.; Suzek, T.O.; 
Tatusova, T.A.; Wagner, L. (2004). ?Database re-
sources of the National Center for Biotechnology In-
formation: update.? Nucleic Acids Research, 32(1), 
D35-40.  
Yandell, M.D., and Majoros, W.H. (2002) ?Genomics 
and natural language processing.? Nature Reviews 
Genetics, 3, 601-610. 
 
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 46?54,
Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational Linguistics
Arguments of Nominals in Semantic Interpretation of Biomedical Text 
 
Halil Kilicoglu,1,2 Marcelo Fiszman,2 Graciela Rosemblat,2  
Sean Marimpietri,3 Thomas C. Rindflesch2 
1Concordia University, Montreal, QC, Canada 
2National Library of Medicine, Bethesda, MD, USA 
3University of California, Berkeley, CA, USA 
h_kilico@cse.concordia.ca, sean.marimpietri@gmail.com  
{fiszmanm,grosemblat,trindflesch}@mail.nih.gov 
 
Abstract 
Based on linguistic generalizations, we 
enhanced an existing semantic processor, 
SemRep, for effective interpretation of a 
wide range of patterns used to express 
arguments of nominalization in clinically 
oriented biomedical text. Nominaliza-
tions are pervasive in the scientific litera-
ture, yet few text mining systems ade-
quately address them, thus missing a 
wealth of information. We evaluated the 
system by assessing the algorithm inde-
pendently and by determining its contri-
bution to SemRep generally. The first 
evaluation demonstrated the strength of 
the method through an F-score of 0.646 
(P=0.743, R=0.569), which is more than 
20 points higher than the baseline. The 
second evaluation showed that overall 
SemRep results were increased to F-score 
0.689 (P=0.745, R=0.640), approximate-
ly 25 points better than processing with-
out nominalizations. 
1 Introduction 
Extracting semantic relations from text and 
representing them as predicate-argument struc-
tures is increasingly seen as foundational for 
mining the biomedical literature (Kim et al, 
2008). Most research has focused on relations 
indicated by verbs (Wattarujeekrit et al, 2004; 
Kogan et al, 2005). However nominalizations, 
gerunds, and relational nouns also take argu-
ments. For example, the following sentence has 
three nominalizations, treatment, suppression, 
and lactation (nominalized forms of the verbs 
treat, suppress, and lactate, respectively). Agon-
ist is derived from agonize, but indicates an 
agent rather than an event. 
Bromocriptine, an ergot alkaloid dopamine 
agonist, is a recent common treatment for 
suppression of lactation in postpartum wom-
en. 
In promoting economy of expression, nomina-
lizations are pervasive in scientific discourse, 
particularly the molecular biology sublanguage, 
due to the highly nested and complex biomolecu-
lar interactions described (Friedman et al, 2002). 
However, Cohen et al (2008) point out that no-
minalizations are more difficult to process than 
verbs. Although a few systems deal with them, 
the focus is often limited in both the nominaliza-
tions recognized and the patterns used to express 
their arguments.  Inability to interpret nominal 
constructions in a general way limits the effec-
tiveness of such systems, since a wealth of 
knowledge is missed.  
In this paper, we discuss our recent work on 
interpreting nominal forms and their arguments. 
We concentrate on nominalizations; however, the 
analysis also applies to other argument-taking 
nouns. Based on training data, we developed a 
set of linguistic generalizations and enhanced an 
existing semantic processor, SemRep, for effec-
tive interpretation of a wide range of patterns 
used to express arguments of nominalization in 
clinically oriented biomedical text. We evaluated 
the enhancements in two ways: by examining the 
ability to identify arguments of nominals inde-
pendently and the effect these enhancements had 
on the overall quality of SemRep output. 
2 Background 
The theoretical linguistics literature has ad-
dressed the syntax of nominalizations (e.g. 
Chomsky, 1970; Grimshaw, 1990; Grimshaw 
and Williams, 1993), however, largely as support 
for theoretical argumentation, rather than de-
tailed description of the facts. Quirk et al (1985) 
concentrate on the morphological derivation of 
46
nominalizations from verbs. Within the context 
of NomBank, a project dedicated to annotation of 
argument structure, Meyers et al (2004a) de-
scribe the linguistics of nominalizations, empha-
sizing semantic roles.  However, major syntactic 
patterns of argument realization are also noted. 
Cohen et al (2008) provide a comprehensive 
overview of nominalizations in biomedical text. 
They include a review of the relevant literature, 
and discuss a range of linguistic considerations, 
including morphological derivation, passiviza-
tion, transitivity, and semantic topics (e.g. 
agent/instrument (activator) vs. ac-
tion/process/state (activation)). Based on an 
analysis of the PennBioIE corpus (Kulick et al, 
2004), detailed distributional results are provided 
on alternation patterns for several nominaliza-
tions with high frequency of occurrence in bio-
medical text, such as activation and treatment.  
In computational linguistics, PUNDIT (Dahl 
et al, 1987) exploited similarities between nomi-
nalizations and related verbs.  Hull and Gomez 
(1996) describe semantic interpretation for a li-
mited set of nominalizations, relying on Word-
Net (Fellbaum, 1998) senses for restricting fillers 
of semantic roles. Meyers et al (1998) present a 
procedure which maps syntactic and semantic 
information for verbs into a set of patterns for 
nominalizations. They use NOMLEX (MacLeod 
et al, 1998), a nominalization lexicon, as the ba-
sis for this transformation. More recently, the 
availability of the NomBank corpus (Meyers et 
al., 2004b) has supported supervised machine 
learning for nominal semantic role labeling (e.g. 
Pradhan et al, 2004; Jiang and Ng, 2006; Liu 
and Ng, 2007). In contrast, Pad? et al (2008) use 
unsupervised machine learning for semantic role 
labeling of eventive nominalizations by exploit-
ing similarities between the argument structure 
of event nominalizations and corresponding 
verbs. Gurevich and Waterman (2009) use a 
large parsed corpus of Wikipedia to derive lexi-
cal models for determining the underlying argu-
ment structure of nominalizations.   
Nominalizations have only recently garnered 
attention in biomedical language processing. Ge-
neScene (Leroy and Chen, 2005) considers only 
arguments of nominalizations marked by prepo-
sitional cues. Similarly, Schuman and Bergler 
(2006) focus on the problem of prepositional 
phrase attachment. In the BioNLP?09 Shared 
Task on Event Extraction (Kim et al, 2009), the 
most frequent predicates were nominals. Several 
participating systems discuss techniques that ac-
commodate nominalizations (e.g. K. B. Cohen et 
al., 2009; Kilicoglu and Bergler, 2009). Nomina-
lizations have not previously been addressed in 
clinically oriented text.  
2.1 SemRep 
SemRep (Rindflesch and Fiszman, 2003) auto-
matically extracts semantic predications (logical 
subject-predicate-logical object triples) from un-
structured text (titles and abstracts) of MED-
LINE citations.  It uses domain knowledge from 
the Unified Medical Language System? (UMLS 
?) (Bodenreider, 2004), and the interaction of 
this knowledge and (underspecified) syntactic 
structure supports a robust system. SemRep ex-
tracts a range of semantic predications relating to 
clinical medicine (e.g. TREATS, DIAGNOSES, AD-
MINISTERED_TO, PROCESS_OF, LOCATION_OF), 
substance interactions (INTERACTS_WITH, INHI-
BITS, STIMULATES), and genetic etiology of dis-
ease (ASSOCIATED_WITH, PREDISPOSES, CAUS-
ES). For example, the program identifies the fol-
lowing predications from input text MRI re-
vealed a lacunar infarction in the left internal 
capsule. Arguments are concepts from the 
UMLS Metathesaurus and predicates are rela-
tions from the Semantic Network.  
Magnetic Resonance Imaging DIAGNOSES Infarc-
tion, Lacunar  
Internal Capsule LOCATION_OF Infarction, Lacu-
nar 
Processing relies on an underspecified syntac-
tic analysis based on the UMLS SPECIALIST 
Lexicon (McCray et al, 1994) and the MedPost 
part-of-speech tagger (Smith et al, 2004). Output 
includes phrase identification, and for simple 
noun phrases, labeling of heads and modifiers.  
[HEAD(MRI)] [revealed] [a MOD(lacunar), 
HEAD(infarction)] [in the MOD(left) MOD(internal), 
HEAD(capsule).] 
MetaMap (Aronson and Lang, 2010) maps sim-
ple noun phrases to UMLS Metathesaurus con-
cepts, as shown below. Associated semantic 
types are particularly important for subsequent 
processing. 
[HEAD(MRI){Magnetic Resonance Imaging (Di-
agnostic Procedure)}] [revealed] [a 
MOD(lacunar), HEAD(infarction) {Infarction, Lacu-
nar(Disease or Syndrome)}] [in the MOD(left) 
MOD(internal), HEAD(capsule) {Internal Cap-
sule(Body Part, Organ, or Organ Component)}.] 
47
This structure is the basis for extracting semantic 
predications, which relies on several mechan-
isms. Indicator rules map syntactic phenomena, 
such as verbs, nominalizations, prepositions, and 
modifier-head structure in the simple noun 
phrase to ontological predications. Examples in-
clude: 
reveal (verb) ? DIAGNOSES 
in (prep) ? LOCATION_OF 
SemRep currently has 630 indicator rules. Onto-
logical predications are based on a modified ver-
sion of the UMLS Semantic Network and have 
semantic types as arguments. For example:  
Diagnostic Procedure DIAGNOSES Disease or 
Syndrome 
Body Part, Organ, or Organ Component LOCA-
TION_OF Disease or Syndrome 
Construction of a semantic predication begins 
with the application of an indicator rule, and is 
then constrained by two things. Arguments must 
satisfy syntactic restrictions for the indicator and 
must have been mapped to Metathesaurus con-
cepts that match the arguments of the ontological 
predication indicated. As part of this processing, 
several syntactic phenomena are addressed, in-
cluding passivization, argument coordination, 
and some types of relativization. For both verb 
and preposition indicators, underspecified syn-
tactic rules simply ensure that subjects are on the 
left and objects on the right. Enhancing SemRep 
for nominalizations involved extending the syn-
tactic constraints for arguments of nominaliza-
tion indicators.  
3 Methods 
In order to gain insight into the principles under-
lying expression of nominal arguments, we first 
determined the 50 most common nominalizations 
in MEDLINE citations that also occur in the 
UMLS SPECIALIST Lexicon, and then analyzed 
a corpus of 1012 sentences extracted from 476 
citations containing those nominalizations. We 
further limited these sentences to those with no-
minalizations containing two overt arguments 
(since SemRep only extracts predications with 
two arguments), resulting in a final set of 383 
sentences. We determined 14 alternation patterns 
for nominalizations based on this analysis and 
devised an algorithm to accommodate them. We 
then conducted two evaluations, one to assess the 
effectiveness of the algorithm independently of 
other considerations and another to assess the 
contribution of enhanced nominalization 
processing to SemRep generally.  
3.1 Nominal Alternations  
Much work in identifying arguments of nomina-
lizations assigns semantic role, such as agent, 
patient, etc., but SemRep does not. In this analy-
sis, arguments are logical subject and object. Re-
lational nouns often allow only one argument 
(e.g. the weight of the evidence), and either one 
or both of the arguments of a nominalization or 
gerund may be left unexpressed. SemRep doesn?t 
interpret nominalizations with unexpressed ar-
guments. If both arguments appear, they fall into 
one of several patterns, and the challenge in no-
minalization processing is to accommodate these 
patterns. Cohen et al (2008) note several such 
patterns, including those in which both argu-
ments are to the right of the nominalization, cued 
by prepositions (treatment of fracture with sur-
gery), the nominalization separates the argu-
ments (fracture treatment with surgery, surgical 
treatment for fracture), and both arguments pre-
cede the nominalizations, as modifiers of it (sur-
gical fracture treatment and fracture surgical 
treatment).  
Cohen et al (2008) do not list several patterns 
we observed in the clinical domain, including 
those in which the subject appears to the right 
marked by a verb (the treatment of fracture is 
surgery) or as an appositive (the treatment of 
fracture, surgery), and those in which the subject 
appears to the left and the nominalization is ei-
ther in a prepositional phrase (surgery in the 
treatment of fracture, surgery in fracture treat-
ment) or is preceded by a verb or is parenthetical 
(surgery is (the best) treatment for fracture; sur-
gery is (the best) fracture treatment; surgery, the 
best fracture treatment). One pattern, in which 
both arguments are on the right and the subject 
precedes the object, is seen most commonly in 
the clinical domain when the nominalization has 
a lexically specified cue (e.g. the contribution of 
stem cells to kidney repair). The nominal alterna-
tion patterns are listed in Table 1. 
Generalizations about arguments of nominali-
zations are based on the position of the argu-
ments, both with respect to each other and to the 
nominalization, and whether they modify the 
nominalization or not. A modifying argument is 
internal to the simple noun phrase of which the 
nominalization is the head; other arguments 
(both to the left and to the right) are external. 
(Relativization is considered external to the sim-
ple noun phrase.) 
48
  [NOM] [PREP OBJ] [PREP SUBJ]  
Treatment of fracture with surgery 
[NOM] [PREP OBJ], [SUBJ] 
The treatment of fracture, surgery 
[NOM] [PREP OBJ] ([SUBJ]) 
The treatment of fracture (surgery) 
[NOM] [PREP OBJ] [BE] [SUBJ] 
The treatment of fracture is surgery 
[NOM] [PREP SUBJ] [PREP OBJ] 
Treatment with surgery of fracture 
[SUBJ NOM] [PREP OBJ] 
Surgical treatment of fracture 
[SUBJ] [PREP NOM] [PREP OBJ] 
Surgery in the treatment of fracture 
[SUBJ] [BE] [NOM] [PREP OBJ] 
Surgery is the treatment of fracture 
[OBJ NOM] [BE] [SUBJ] 
Fracture treatment is surgery 
[OBJ NOM] [PREP SUBJ] 
Fracture treatment with surgery 
[SUBJ] [PREP OBJ NOM] 
Surgery for fracture treatment 
[SUBJ] [BE] [OBJ NOM] 
Surgery is the fracture treatment 
[SUBJ OBJ NOM] 
Surgical fracture treatment 
[OBJ SUBJ NOM] 
Fracture surgical treatment 
Table 1. Patterns 
Argument cuing plays a prominent role in de-
fining these patterns. A cue is an overt syntactic 
element associated with an argument, and can be 
a preposition, a verb (most commonly a form of 
be), a comma, or parenthesis. A cued argument is 
in a dependency with the cue, which is itself in a 
dependency with the nominalization. The cue 
must occur between the nominalization and the 
argument, whether the argument is to the right 
(e.g. treatment of fracture) or to the left (e.g. 
surgery in the treatment). Prepositional cues for 
the objects of some nominalizations are stipu-
lated in the lexicon; some of these are obligatory 
(e.g. contribution ? to), while others are optional 
(treatment ? for).  
External arguments of nominalizations must 
be cued, and cues unambiguously signal the role 
of the argument, according to the following 
cuing rules (Cohen et al, 2008). Verbs, comma, 
parenthesis, and the prepositions by, with, and 
via cue subjects only. (By is used for semantic 
role agent and with for instrument, but SemRep 
does not exploit this distinction.) Of cues sub-
jects only if the nominalization has an obligatory 
(object) cue; it must cue objects otherwise. There 
is a class of nominalizations (e.g. cause) that do 
not allow a prepositionally cued subject. Consi-
derable variation is seen in the order of subject 
and object; however, if the subject intervenes 
between the nominalization and the object, both 
must have equal cuing status (the only possibili-
ties are that both be either uncued or cued with a 
preposition).  
3.2 Algorithm 
In extending SemRep for identifying arguments 
of nominalizations, existing machinery was ex-
ploited, namely shallow parsing, mapping simple 
noun phrases to Metathesaurus concepts, and the 
application of indicator rules to map nominaliza-
tions to enhanced Semantic Network ontological 
predications (which imposes restrictions on the 
semantic type of arguments). Finally, syntactic 
argument identification was enhanced specifical-
ly for nominalizations and exploits the linguistic 
generalizations noted. For example in the sen-
tence below, phrases have been identified and 
cervical cancer has been mapped to the Metathe-
saurus concept ?Cervix carcinoma? with seman-
tic type ?Neoplastic Process?, and vaccination to 
?Vaccination? (?Therapeutic or Preventive Pro-
cedure?). An indicator rule for prevention maps 
to the ontological predication ?Therapeutic or 
Preventive Procedure PREVENTS Neoplastic 
Process? (among others) in generating the predi-
cation: ?Vaccination PREVENTS Cervix carcino-
ma.? 
Therefore, prevention of cervical cancer with 
HPV vaccination may have a significant fi-
nancial impact.  
Processing to identify arguments for preven-
tion begins by determining whether the nomina-
lization has a lexically specified object cue. This 
information is needed to determine the cuing 
function of of. Since it is common for there to be 
at least one argument on the right, identification 
of arguments begins there. Arguments on the 
right are external and must be cued. If a cued 
argument is found, its role is determined by the 
argument cuing rules. Since prevention does not 
have a lexically specified cue, of marks its ob-
ject. Further, the semantic type of the concept for 
the object of of matches the object of the onto-
logical predication (?Neoplastic Process?).  
The algorithm next looks to the right of the 
first argument for the second argument. Since 
processing addresses only two arguments for 
nominalizations, subject and object, once the role 
49
of the first has been determined, the second can 
be inferred. For cued arguments, the process 
checks that the cue is compatible with the cuing 
rules. In all cases, the relevant semantic type 
must match the subject of the ontological predi-
cation. In this instance, with cues subjects and 
?Therapeutic or Preventive Process? matches the 
subject of the ontological predication indicated.  
If only one noun phrase to the right satisfies 
the argument cuing rules, the second argument 
must be on the left. A modifier immediately to 
the left of the nominalization (and thus an inter-
nal argument) is sought first, and its role inferred 
from the first argument. Since internal arguments 
are not cued, there is no need to ensure cuing 
compatibility. The predication ?Operative Sur-
gical Procedures TREATS Pregnancy, Ectopic? 
is found for resolution in  
Surgical resolution of an ectopic pregnancy 
in a captive gerenuk (Litocranius walleri wal-
leri).  
Resolution is an indicator for the ontological 
predication ?Therapeutic or Preventive Proce-
dure TREATS Disease or Syndrome.? Surgical 
maps to ?Operative Surgical Procedures? (?The-
rapeutic or Preventive Procedure?), which 
matches the subject of this predication, and ec-
topic pregnancy maps to ?Pregnancy, Ectopic? 
(?Disease or Syndrome?), which matches its ob-
ject.  Of marks the object of resolution. 
An argument to the left of a nominalization 
may be external, in which case a cue is neces-
sary. For preceding treatment satisfies this re-
quirement in the following sentence. 
Preclinical data have supported the use of 
fludarabine and cyclophosphamide (FC) in 
combination for the treatment of indolent 
lymphoid malignancies. 
The two drugs in this sentence map to concepts 
with semantic type ?Pharmacologic Substance? 
and the malignancy has ?Neoplastic Process?, as 
above. There is an ontological predication for 
TREATS with subject ?Pharmacologic Substance?. 
After coordination processing in SemRep, two 
predications are generated for treatment:  
Cyclophosphamide TREATS Malignant lymphoid 
neoplasm 
Fludarabine TREATS Malignant lymphoid neop-
lasm 
If there is no argument to the right, both ar-
guments must be on the left. A modifier imme-
diately to the left of the nominalization is sought 
first. Given the properties of cuing (the cue in-
tervenes between the argument and the nominali-
zation), if both arguments occur to the left, at 
least one of them must be internal, since it is not 
possible to have more than one external argu-
ment on the left (e.g. *Surgery is fracture for 
treatment). The role of the first argument is 
found based on semantic type. The first modifier 
to the left of treatment in the following sentence 
is epilepsy, which has semantic type ?Disease or 
Syndrome?, matching the object of the ontologi-
cal predication for TREATS.  
Patients with most chances of benefiting from 
surgical epilepsy treatment 
The second modifier to the left, surgical maps to 
the concept ?Operative Surgical Procedures,? 
whose semantic type matches the subject of the 
ontological predication. These conditions allow 
construction of the predication ?Operative Sur-
gical Procedures TREATS Epilepsy.?   
In the next sentence, the indicator rule for pre-
diction maps to the ontological predication 
?Amino Acid, Peptide, or Protein PREDISPOSES 
Disease or Syndrome.?  
The potential clinical role of measuring these 
apolipoproteins for ischemic stroke predic-
tion warrants further study. 
Ischemic stroke satisfies the object of this predi-
cation and apolipoproteins the subject. Since the 
external subject is cued by for, all constraints are 
satisfied and the predication ?Apolipoproteins 
PREDISPOSES Ischemic stroke? is generated.   
3.3 Evaluation 
Three-hundred sentences from 239 MEDLINE 
citations (titles and abstracts) were selected for 
annotating a test set. Some had previously been 
selected for various aspects of SemRep evalua-
tion; others were chosen randomly. A small 
number (30) were sentences in the GENIA event 
corpus (Kim et al, 2008) with bio-event-
triggering nominalizations. Annotation was con-
ducted by three of the authors. One, a linguist 
(A), judged all sentences, while the other two, a 
computer scientist (B) and a medical informatics 
researcher (C), annotated a subset. Annotation 
was not limited to nominalizations. The statistics 
regarding the individual annotations are given 
below. The numbers in parentheses show the 
number of annotated predications indicated by 
nominalizations. 
 
50
Annotator # of Sentences # of Predications 
A  300 533 (286) 
B 200 387 (190) 
C 132 244 (134) 
Table 2. Annotation statistics 
As guidance, annotators were provided UMLS 
Metathesaurus concepts for the sentences. How-
ever, they consulted the Metathesaurus directly 
to check questionable mappings. Annotation fo-
cused on the 25 predicate types SemRep ad-
dresses.  
We measured inter-annotator agreement, de-
fined as the F-score of one set of annotations, 
when the second is taken as the gold standard. 
After individual annotations were complete, two 
annotators (A and C) assessed all three sets of 
annotations and created the final reference stan-
dard. The reference standard has 569 predica-
tions, 300 of which (52.7%) are indicated by 
nominalizations. We further measured the 
agreement between individual sets of annotations 
and the reference standard. Results are given be-
low: 
 
Annotator pair # of Sentences IAA 
A-B  200 0.794 
A-C 132 0.974 
B-C 103 0.722 
A-Gold 300 0.925 
B-Gold 200 0.889 
C-Gold 132 0.906 
Table 3. Inter-annotator agreement 
We performed two evaluations. The first (ev-
al1) evaluated nominalizations in isolation, while 
the second (eval2) assessed the effect of the en-
hancements on overall semantic interpretation in 
SemRep. For eval1, we restricted SemRep to 
extract predications indicated by nominalizations 
only. The baseline was a nominalization argu-
ment identification rule which simply stipulates 
that the subject of a predicate is a concept to the 
left (starting from the modifier of the nominali-
zation, if any), and the object is a concept to the 
right. This baseline implements the underspecifi-
cation principle of SemRep, without any addi-
tional logic. We compared the results from this 
baseline to those from the algorithm described 
above to identify arguments of nominalizations. 
The gold standard for eval1 was limited to predi-
cations indicated by nominalizations.  
We investigated the effect of nominalization 
processing on SemRep generally in eval2, for 
which the baseline implementation was SemRep 
with no nominalization processing. The results 
for this baseline were evaluated against those 
obtained using SemRep with no restrictions. 
Typical evaluation metrics, precision, recall, and 
F-score, were calculated. 
4 Results and Discussion 
The results for the two evaluations are presented 
below.  
 Precision Recall F-Score 
eval1 
Baseline 0.484 0.359 0.412 
With NOM 0.743 0.569 0.645 
 
eval2 
Baseline 0.640 0.333 0.438 
With NOM 0.745 0.640 0.689 
Table 4. Evaluation results 
Results illustrate the importance of nominali-
zation processing for effectiveness of semantic 
interpretation and show that the SemRep metho-
dology naturally extends to this phenomenon. 
With a single, simple, rule (eval1 baseline), Se-
mRep achieves an F-score of 0.412. With addi-
tional processing based on linguistic generaliza-
tions, F-score improves more than 20 points. 
Further, the addition of nominalization 
processing not only enhances the coverage of 
SemRep (more than 30 points), but also increases 
precision (more than 10 points). While nominali-
zations are generally considered more difficult to 
process than verbs (Cohen et al, 2008), we were 
able to accommodate them with greater precision 
than other types of indicators, including verbs 
(0.743 vs. 0.64 in eval1 with NOM vs. eval2 
baseline) with our patterns.  
 Precision Recall F-Score 
eval1 
Baseline 0.233 0.140 0.175 
With NOM 0.690 0.400 0.506 
 
eval2 
Baseline (No 
NOM) 
0.667 0.278 0.392 
With NOM 0.698 0.514 0.592 
Table 5. Results for molecular biology sentences 
 
Limiting the evaluation to sentences focusing on 
biomolecular interactions (from GENIA), while 
not conclusive due to the small number of sen-
tences (30), also shows similar patterns, as 
shown in Table 5. As expected, while overall 
51
quality of predications is lower, since molecular 
biology text is significantly more complex than 
that in the clinical domain, improvements with 
nominalization processing are clearly seen. 
Errors were mostly due to aspects of SemRep 
orthogonal to but interacting with nominalization 
processing. Complex coordination structure was 
the main source of recall errors, as in the follow-
ing example.  
RESULTS: The best predictors of incident 
metabolic syndrome were waist circumfe-
rence (odds ratio [OR] 1.7 [1.3-2.0] per 11 
cm), HDL cholesterol (0.6 [0.4-0.7] per 15 
mg/dl), and proinsulin (1.7 [1.4-2.0] per 3.3 
pmol/l). [PMID 14988303] 
While the system was able to identify the predi-
cation ?Waist circumference PREDISPOSES 
Metabolic syndrome,? it was unable to find the 
predications below, due to its inability to identify 
the coordination of waist circumference, HDL 
cholesterol, and proinsulin.  
(FN) Proinsulin PREDISPOSES Metabolic syn-
drome 
(FN) High Density Lipoprotein Cholesterol PRE-
DISPOSES Metabolic syndrome 
 
Mapping of noun phrases to the correct UMLS 
concepts (MetaMap) is a source of both false 
positives and false negatives, particularly in the 
context of the molecular biology sentences, 
where acronyms and abbreviations are common 
and their disambiguation is nontrivial (Okazaki 
et al, 2010). For example, in the following sen-
tence  
PTK inhibition with Gen attenuated both 
LPS-induced NF-kappaB DNA binding and 
TNF-alpha production in human monocytes. 
[PMID 10210645] 
PTK was mapped to ?Ephrin receptor EphA8? 
rather than to ?Protein Tyrosine Kinase?, causing 
both a false positive and a false negative.  
(FP) Genistein INHIBITS Ephrin receptor EphA8 
(FN) Genistein INHIBITS Protein Tyrosine Kinase 
Some errors were due to failure to recognize a 
relative clause by SemRep. Only the head of 
such a structure is allowed to be an argument 
outside the structure. In the sentence below, the 
subject of treatment is hyperthermic intraperito-
neal intraoperative chemotherapy, which is the 
head of the reduced relative clause, after cytore-
ductive surgery.  
Hyperthermic intraperitoneal intraoperative 
chemotherapy after cytoreductive surgery for 
the treatment of abdominal sarcomatosis: 
clinical outcome and prognostic factors in 60 
consecutive patients. [PMID 15112276] 
SemRep failed to recognize the relative clause, 
and therefore the nominalization algorithm took 
the noun phrase inside it as the subject of treat-
ment, since it satisfies both semantic type and 
argument constraints.  
(FP) Cytoreductive surgery  TREATS Sarcamato-
sis NOS 
(FN) intraperitoneal therapy TREATS Sarcamato-
sis NOS 
A small number of errors were due solely to 
nominalization processing. In the following sen-
tence, the object of contribution is cued with in, 
rather than lexically specified to, which causes a 
recall error.  
Using SOCS-1 knockout mice, we investi-
gated the contribution of SOCS-1 in the 
development of insulin resistance induced 
by a high-fat diet (HFD). [PMID 
18929539] 
(FN) Cytokine Inducible SH-2 Containing Pro-
tein PREDISPOSES Insulin Resistance 
Accurate identification of the arguments of 
nominalizations in the molecular biology sub-
domain is more challenging than in clinically-
oriented text. Some of the syntactic structure re-
sponsible for this complexity is discussed by K. 
B. Cohen et al (2009). In particular, they note 
the problem of an argument being separated from 
the nominalization, and point out the problem of 
specifying the intervening structure. Although 
we have not focused on molecular biology, the 
analysis developed for clinical medicine shows 
promise in that domain as well. One relevant ex-
tension could address the syntactic configuration 
in which intervening structure involves an argu-
ment of a nominalization shared with a verb oc-
curring to the left of the nominalization, as in-
duced and activation interact in the following 
sentence: 
IL-2 induced less STAT1 alpha activation 
and IFN-alpha induced greater STAT5 acti-
vation in NK3.3 cells compared with preacti-
vated primary NK cells. [PMID 8683106] 
This could be addressed with an extension of our 
rule that subjects of nominalizations can be cued 
with verbs. With respect to argument identifica-
tion, induce can function like a form of be.  
52
5 Conclusion 
We discuss a linguistically principled implemen-
tation for identifying arguments of nominaliza-
tions in clinically focused biomedical text. The 
full range of such structures is rarely addressed 
by existing text mining systems, thus missing 
valuable information. The algorithm is imple-
mented inside SemRep, a general semantic inter-
preter for biomedical text. We evaluated the sys-
tem both by assessing the algorithm independent-
ly and by determining the contribution it makes 
to SemRep generally. The first evaluation re-
sulted in an F-score of 0.646 (P=0.743, 
R=0.569), which is 20 points higher than the 
baseline, while the second showed that overall 
SemRep results were increased to F-score 0.689 
(P=0.745, R=0.640), approximately 25 points 
better than processing without nominalizations. 
Since our nominalization processing is by ex-
tending SemRep, rather than by creating a dedi-
cated system, we provide the interpretation of 
these structures in a broader context. An array of 
semantic predications generated by mapping to 
an ontology (UMLS) normalizes the interpreta-
tion of verbs and nominalizations.  Processing is 
linguistically based, and several syntactic phe-
nomena are addressed, including passivization, 
argument coordination, and relativization. The 
benefits of such processing include effective ap-
plications for extracting information on genetic 
diseases from text (Masseroli et al, 2006), as 
well as research in medical knowledge summari-
zation (Fiszman et al, 2004; Fiszman et al, 
2009), literature-based discovery (Ahlers et al, 
2007; Hristovski et al, 2010), and enhanced in-
formation retrieval (Kilicoglu et al, 2008; T. 
Cohen et al, 2009). 
 
Acknowledgments 
This study was supported in part by the Intra-
mural Research Program of the National Insti-
tutes of Health, National Library of Medicine. 
References 
C. B. Ahlers, D. Hristovski, H. Kilicoglu, T. C. 
Rindflesch. 2007. Using the literature-based dis-
covery paradigm to investigate drug mechanisms. 
In Proceedings of AMIA Annual Symposium, pages 
6-10. 
A. R. Aronson and F.-M. Lang. 2010. An overview of 
MetaMap: historical perspective and recent ad-
vances. Journal of the American Medical Informat-
ics Association, 17:229-236. 
O. Bodenreider. 2004. The Unified Medical Language 
System (UMLS): integrating biomedical terminol-
ogy. Nucleic Acids Research, 32(Database is-
sue):D267-70.  
N. Chomsky. 1970. Remarks on nominalization. In 
Jacobs, Roderick, and Peter S. Rosenbaum (eds.) 
Readings in English transformational grammar. 
Boston: Ginn and Company, pages 184-221.  
K. B. Cohen, M. Palmer, L. Hunter. 2008. Nominali-
zation and alternations in biomedical language. 
PLoS ONE, 3(9): e3158. 
K. B. Cohen, K. H. Verspoor, H. L. Johnson, C. 
Roeder, P. V. Ogren, W. A. Baumgartner, E. 
White, H. Tipney, L. Hunter. 2009. High-precision 
biological event extraction with a concept recog-
nizer. In Proceedings of the BioNLP 2009 Work-
shop Companion Volume for Shared Task, pages 
50-58. 
T. Cohen, R. Schvaneveldt, T. C. Rindflesch. 2009. 
Predication-based semantic indexing: Permutations 
as a means to encode predications in semantic 
space. In Proceedings of AMIA Annual Symposium, 
pages 114-118. 
D. A. Dahl, M. S. Palmer, R. J. Passonneau. 1987. 
Nominalizations in PUNDIT. In Proceedings of 
ACL, pages 131-139. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Database. The MIT Press, Cambridge, MA. 
M. Fiszman, D. Demner-Fushman, H. Kilicoglu, T. C. 
Rindflesch. 2009. Automatic summarization of 
MEDLINE citations for evidence-based medical 
treatment: A topic-oriented evaluation. Journal of 
Biomedical Informatics, 42(5):801-813.  
M. Fiszman, T. C. Rindflesch, H. Kilicoglu. 2004. 
Abstraction summarization for managing the bio-
medical research literature. In Proceedings of 
HLT/NAACL Workshop on Computational Lexical 
Semantics, pages 76-83. 
C. Friedman, P. Kra, A. Rzhetsky. 2002. Two bio-
medical sublanguages: a description based on the 
theories of Zellig Harris. Journal of Biomedical In-
formatics, 35:222?235. 
J. Grimshaw. 1990.  Argument Structure. MIT Press, 
Cambridge, MA. 
J. Grimshaw and E. Williams. 1993. Nominalizations 
and predicative prepositional phrases. In J. Puste-
jovsky (ed.) Semantics and the Lexicon. Dordrecht: 
Kluwer Academic Publishers, pages  97-106.  
O. Gurevich and S. A. Waterman. 2009. Mining of 
parsed data to derive deverbal argument structure. 
In Proceedings of the 2009 Workshop on Grammar 
Engineering Across Frameworks. pages 19-27. 
D. Hristovski, A. Kastrin, B. Peterlin, T. C. 
Rindflesch. 2010. Combining semantic relations 
53
and DNA microarray data for novel hypothesis 
generation. In C. Blaschke, H. Shatkay (Eds.) 
ISMB/ECCB2009, Lecture Notes in Bioinformatics, 
Heidelberg: Springer-Verlag, pages 53-61.  
R. D. Hull and F. Gomez. 1996. Semantic interpreta-
tion of nominalizations. In Proceedings of AAAI, 
pages 1062-1068. 
Z. P. Jiang and H. T. Ng. 2006. Semantic role labeling 
of NomBank: A maximum entropy approach. In 
Proceedings of EMNLP?06, pages 138?145. 
H. Kilicoglu and S. Bergler. 2009. Syntactic depen-
dency based heuristics for biological event extrac-
tion. In Proceedings of the BioNLP 2009 Workshop 
Companion Volume for Shared Task, pages 119-
127. 
H. Kilicoglu, M. Fiszman, A. Rodriguez, D. Shin, A. 
M. Ripple, T. C. Rindflesch. 2008. Semantic 
MEDLINE: A Web application to manage the re-
sults of PubMed searches. In Proceedings of 
SMBM?08, pages 69-76. 
J-D. Kim, T. Ohta, S. Pyysalo, Y. Kano, J. Tsujii. 
2009. Overview of BioNLP?09 Shared Task on 
Event Extraction. In Proceedings of the BioNLP 
2009 Workshop Companion Volume for Shared 
Task, pages 1-9. 
J-D. Kim, T. Ohta, J. Tsujii. 2008. Corpus annotation 
for mining biomedical events from literature. BMC 
Bioinformatics, 9(1):10. 
Y. Kogan, N. Collier, S. Pakhomov, M. Krautham-
mer. 2005. Towards semantic role labeling & IE in 
the medical literature. In Proceedings of AMIA An-
nual Symposium, pages 410?414. 
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. 
McDonald, M. Palmer. A. Schein, L. Ungar. 2004. 
Integrated annotation for biomedical information 
extraction. In Proceedings of BioLINK: Linking 
Biological Literature, Ontologies and Databases, 
pages 61?68. 
G. Leroy and H. Chen. 2005.  Genescene: An ontolo-
gy-enhanced integration of linguistic and co-
occurrence based relations in biomedical texts. 
Journal of the American Society for Information 
Science and Technology, 56(5): 457?468. 
C. Liu and H. Ng. 2007. Learning predictive struc-
tures for semantic role labeling of NomBank. In 
Proceedings of ACL, pages 208?215. 
C. Macleod, R. Grishman, A. Meyers, L. Barrett, R. 
Reeves. 1998. NOMLEX: A lexicon of nominali-
zations. In Proceedings of EURALEX?98. 
M. Masseroli, H. Kilicoglu, F-M. Lang, T. C. 
Rindflesch. 2006. Argument-predicate distance as a 
filter for enhancing precision in extracting predica-
tions on the genetic etiology of disease. BMC Bio-
informatics, 7:291. 
A. T. McCray, S. Srinivasan, A. C. Browne. 1994. 
Lexical methods for managing variation in biomed-
ical terminologies. In Proceedings of 18th Annual 
Symposium on Computer Applications in Medical 
Care, pages 235?239. 
A. Meyers, C. Macleod, R. Yanbarger, R. Grishman, 
L. Barrett, R. Reeves. 1998. Using NOMLEX to 
produce nominalization patterns for information 
extraction. In Proceedings of the Workshop on 
Computational Treatment of Nominals (COL-
ING/ACL), pages 25-32. 
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Young, R. Grishman. 2004a. Anno-
tating noun argument structure for NomBank. In 
Proceedings of LREC. 
A. Meyers, R. Reeves, C. Macleod, R. Szekely, V. 
Zielinska, B. Young, R. Grishman. 2004b. The 
NomBank project: An interim report. In Proceed-
ings of HLT-NAACL 2004 Workshop: Frontiers in 
Corpus Annotation, pages 24?31. 
N. Okazaki, S. Ananiadou, J. Tsujii. 2010. Building a 
high quality sense inventory for improved abbrevi-
ation disambiguation. Bioinformatics: btq129+. 
S. Pad?, M. Pennacchiotti, C. Sporleder. 2008. Se-
mantic role assignment for event nominalisations 
by leveraging verbal data. In Proceedings of CoL-
ing?08, pages 665-672. 
S. Pradhan, H. Sun, W. Ward, J. Martin, D. Jurafsky. 
2004. Parsing arguments of nominalizations in 
English and Chinese. In Proceedings of 
HLT/NAACL, pages 141?144. 
R. Quirk, S. Greenbaum, G. Leech, J. Svartvik. 1985. 
A Comprehensive Grammar of the English Lan-
guage. Longman, London. 
T. C. Rindflesch and M. Fiszman. 2003. The interac-
tion of domain knowledge and linguistic structure 
in natural language processing: Interpreting hyper-
nymic propositions in biomedical text. Journal of 
Biomedical Informatics, 36(6):462-77. 
J. Schuman and S. Bergler. 2006. Postnominal prepo-
sitional phrase attachment in proteomics. In Pro-
ceedings of BioNLP Workshop on Linking Natural 
Language Processing and Biology, pages 82?89. 
L. Smith, T. C. Rindflesch, W. J. Wilbur. 2004. Med-
Post: a part-of-speech tagger for biomedical text. 
Bioinformatics, 20(14):2320-2321. 
T. Wattarujeekrit, P. K. Shah, N. Collier. 2004. PAS-
Bio: Predicate-argument structures for event ex-
traction in molecular biology. BMC Bioinformat-
ics, 5:155.  
 
54
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 70?77,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A High-Precision Approach to Detecting Hedges and Their Scopes
Halil Kilicoglu and Sabine Bergler
Department of Computer Science and Software Engineering
Concordia University
1455 de Maisonneuve Blvd. West
Montre?al, Canada
{h kilico,bergler}@cse.concordia.ca
Abstract
We extend our prior work on specula-
tive sentence recognition and speculation
scope detection in biomedical text to the
CoNLL-2010 Shared Task on Hedge De-
tection. In our participation, we sought
to assess the extensibility and portability
of our prior work, which relies on linguis-
tic categorization and weighting of hedg-
ing cues and on syntactic patterns in which
these cues play a role. For Task 1B,
we tuned our categorization and weight-
ing scheme to recognize hedging in bio-
logical text. By accommodating a small
number of vagueness quantifiers, we were
able to extend our methodology to de-
tecting vague sentences in Wikipedia arti-
cles. We exploited constituent parse trees
in addition to syntactic dependency rela-
tions in resolving hedging scope. Our re-
sults are competitive with those of closed-
domain trained systems and demonstrate
that our high-precision oriented methodol-
ogy is extensible and portable.
1 Introduction
Natural language is imbued with uncertainty,
vagueness, and subjectivity. However, informa-
tion extraction systems generally focus on ex-
tracting factual information, ignoring the wealth
of information expressed through such phenom-
ena. In recent years, the need for information ex-
traction and text mining systems to identify and
model such extra-factual information has increas-
ingly become clear. For example, online product
and movie reviews have provided a rich context
for analyzing sentiments and opinions in text (see
Pang and Lee (2008) for a recent survey), while
tentative, speculative nature of scientific writing,
particularly in biomedical literature, has provided
impetus for recent research in speculation detec-
tion (Light et al, 2004). The term hedging is often
used as an umbrella term to refer to an array of
extra-factual phenomena in natural language and
is the focus of the CoNLL-2010 Shared Task on
Hedge Detection.
The CoNLL-2010 Shared Task on Hedge De-
tection (Farkas et al, 2010) follows in the steps
of the recent BioNLP?09 Shared Task on Event
Extraction (Kim et al, 2009), in which one task
(speculation and negation detection) was con-
cerned with notions related to hedging in biomed-
ical abstracts. However, the CoNLL-2010 Shared
Task differs in several aspects. It sheds light on
the pervasiveness of hedging across genres and do-
mains: in addition to biomedical abstracts, it is
concerned with biomedical full text articles as well
as with Wikipedia articles. Both shared tasks have
been concerned with scope resolution; however,
their definitions of scope are fundamentally differ-
ent: the BioNLP?09 Shared Task takes the scope
of a speculation instance to be an abstract seman-
tic object (an event), thus a normalized logical
form. The CoNLL-2010 Shared Task, on the other
hand, defines it as a textual unit based on syntac-
tic considerations. It is also important to note that
hedging in scientific writing is a core aspect of the
genre (Hyland, 1998), while it is judged to be a
flaw which has to be eradicated in Wikipedia ar-
ticles. Therefore, hedge detection in these genres
serves different purposes: explicitly encoding the
factuality of a scientific claim (doubtful, probable,
etc.) versus flagging unreliable text.
We participated in both tasks of the CoNLL-
2010 Shared Task: namely, detection of sentences
with uncertainty (Task 1) and resolution of uncer-
tainty scope (Task 2). Since we pursued both of
these directions in prior work, one of our goals in
participating in the shared task was to assess how
our approach generalized to previously unseen
texts, even genres. Towards this goal, we adopted
70
an open-domain approach, where we aimed to use
previously developed techniques to the extent pos-
sible. Among all participating groups, we distin-
guished ourselves as the one that fully worked in
an open-domain setting. This approach worked
reasonably well for uncertainty detection (Task 1);
however, for the scope resolution task, we needed
to extend our work more substantially, since the
notion of scope was fundamentally different than
what we adopted previously. The performance
of our system was competitive; in terms of F-
measure, we were ranked near the middle in Task
1, while a more significant focus on scope reso-
lution resulted in fourth place ranking among fif-
teen systems. We obtained the highest precision
in tasks focusing on biological text. Considering
that we chose not to exploit the training data pro-
vided to the full extent, we believe that our system
is viable in terms of extensibility and portability.
2 Related Work
Several notions related to hedging have been pre-
viously explored in natural language processing.
In the news article genre, these have included
certainty, modality, and subjectivity. For ex-
ample, Rubin et al (2005) proposed a four di-
mensional model to categorize certainty in news
text: certainty level, focus, perspective and time.
In the context of TimeML (Pustejovsky et al,
2005), which focuses on temporal expressions
in news articles, event modality is encoded us-
ing subordination links (SLINKs), some of which
(MODAL,EVIDENTIAL) indicate hedging (Saur?? et
al., 2006). Saur?? (2008) exploits modality and
polarity to assess the factuality degree of events
(whether they correspond to facts, counter-facts or
possibilities), and reports on FactBank, a corpus
annotated for event factuality (Saur?? and Puste-
jovsky, 2009). Wiebe et al (2005) consider
subjectivity in news articles, and focus on the
notion of private states, encompassing specula-
tions, opinions, and evaluations in their subjectiv-
ity frames.
The importance of speculative language in
biomedical articles was first acknowledged by
Light et al (2004). Following work in this area
focused on detecting speculative sentences (Med-
lock and Briscoe, 2007; Szarvas, 2008; Kilicoglu
and Bergler, 2008). Similar to Rubin et al?s
(2005) work, Thompson et al (2008) proposed
a categorization scheme for epistemic modality in
biomedical text according to the type of infor-
mation expressed (e.g., certainty level, point of
view, knowledge type). With the availability of the
BioScope corpus (Vincze et al, 2008), in which
negation, hedging and their scopes are annotated,
studies in detecting speculation scope have also
been reported (Morante and Daelemans, 2009;
O?zgu?r and Radev, 2009). Negation and uncer-
tainty of bio-events are also annotated to some ex-
tent in the GENIA event corpus (Kim et al, 2008).
The BioNLP?09 Shared Task on Event Extraction
(Kim et al, 2009) dedicated a task to detecting
negation and speculation in biomedical abstracts,
based on the GENIA event corpus annotations.
Ganter and Strube (2009) elaborated on the link
between vagueness in Wikipedia articles indicated
by weasel words and hedging. They exploited
word frequency measures and shallow syntactic
patterns to detect weasel words in Wikipedia ar-
ticles.
3 Methods
Our methodology for hedge detection is essen-
tially rule-based and relies on a combination of
lexical and syntactic information. Lexical infor-
mation is encoded in a simple dictionary, and rel-
evant syntactic information is identified using the
Stanford Lexicalized Parser (Klein and Manning,
2003). We exploit constituent parse trees as well
as corresponding collapsed dependency represen-
tations (deMarneffe et al, 2006), provided by the
parser.
3.1 Detecting Uncertainty in Biological Text
For detecting uncertain sentences in biological text
(Task 1B), we built on the linguistically-inspired
system previously described in detail in Kilicoglu
and Bergler (2008). In summary, this system relies
on a dictionary of lexical speculation cues, derived
from a set of core surface realizations of hedging
identified by Hyland (1998) and expanded through
WordNet (Fellbaum, 1998) synsets and UMLS
SPECIALIST Lexicon (McCray et al, 1994) nom-
inalizations. A set of lexical certainty markers (un-
hedgers) are also included, as they indicate hedg-
ing when they are negated (e.g., know). These
hedging cues are categorized by their type (modal
auxiliaries, epistemic verbs, approximative adjec-
tives, etc.) and are weighted to reflect their cen-
tral/peripheral contribution to hedging, inspired by
the fuzzy model of Hyland (1998). We use a scale
71
of 1-5, where 5 is assigned to cues most central
to hedging and 1 to those that are most periph-
eral. For example, the modal auxiliary may has
a weight of 5, while a relatively weak hedging
cue, the epistemic adverb apparently, has a weight
of 2. The weight sum of cues in a sentence in
combination with a predetermined threshold de-
termines whether the sentence in question is un-
certain. Syntax, generally ignored in other stud-
ies on hedging, plays a prominent role in our ap-
proach. Certain syntactic constructions act as cues
(e.g., whether- and if -complements), while others
strengthen or weaken the effect of the cue associ-
ated with them. For example, a that-complement
taken by an epistemic verb increases the hedging
score contributed by the verb by 2, while lack of
any complement decreases the score by 1.
For the shared task, we tuned this categoriza-
tion and weighting scheme, based on an analy-
sis of the biomedical full text articles in training
data. We also adjusted the threshold. We elim-
inated some hedging cue categories completely
and adjusted the weights of a small number of
the remaining cues. The eliminated cue categories
included approximative adverbs (e.g., generally,
largely, partially) and approximative adjectives
(e.g., partial), often used to ?manipulate preci-
sion in quantification? (Hyland, 1998). The other
eliminated category included verbs of effort (e.g.,
try, attempt, seek), also referred to as rationalising
narrators (Hyland, 1998). The motivation behind
eliminating these categories was that cues belong-
ing to these categories were never annotated as
hedging cues in the training data. The elimination
process resulted in a total of 147 remaining hedg-
ing cues. Additionally, we adjusted the weights of
several other cues that were not consistently anno-
tated as cues in the training data, despite our view
that they were strong hedging cues. One example
is the epistemic verb predict, previously assigned a
weight of 4 based on Hyland?s analysis. We found
its annotation in the training data somewhat incon-
sistent, and lowered its weight to 3, thus requiring
a syntactic strengthening effect (an infinitival com-
plement, for example) for it to qualify as a hedging
cue in the current setting (threshold of 4).
3.2 Detecting Uncertainty in Wikipedia
Articles
Task 1W was concerned with detecting uncer-
tainty in Wikipedia articles. Uncertainty in this
context refers more or less to vagueness indicated
by weasel words, an undesirable feature accord-
ing to Wikipedia policy. Analysis of Wikipedia
training data provided by the organizers revealed
that there is overlap between weasel words and
hedging cues described in previous section. We,
therefore, sought to adapt our dictionary of hedg-
ing cues to the task of detecting vagueness in
Wikipedia articles. Similar to Task 1B, changes
involved eliminating cue categories and adjusting
cue weights. In addition, however, we also added
a previously unconsidered category of cues, due
to their prominence in Wikipedia data as weasel
words. This category (vagueness quantifiers (Lap-
pin, 2000)) includes words, such as some, several,
many and various, which introduce imprecision
when in modifier position. For instance, in the ex-
ample below, both some and certain contribute to
vagueness of the sentence.
(1) Even today, some cultures have certain in-
stances of their music intending to imitate
natural sounds.
For Wikipedia uncertainty detection, eliminated
categories included verbs and nouns concerning
tendencies (e.g., tend, inclination) in addition to
verbs of effort. The only modal auxiliary consis-
tently considered a weasel word was might; there-
fore, we only kept might in this category and elim-
inated the rest (e.g., may, would). Approxima-
tive adverbs, eliminated in detecting uncertainty
in biological text, not only were revived for this
task, but also their weights were increased as they
were more central to vagueness expressions. Be-
sides these changes in weighting and categoriza-
tion, the methodology for uncertainty detection in
Wikipedia articles was essentially the same as that
for biological text. The threshold we used in our
submission was, similarly, 4.
3.3 Scope Resolution for Uncertainty in
Biological Text
Task 2 of the shared task involved hedging scope
resolution in biological text. We previously tack-
led this problem within the context of biological
text in the BioNLP?09 Shared Task (Kilicoglu and
Bergler, 2009). That task defined the scope of
speculation instances as abstract, previously ex-
tracted bio-events. Our approach relied on find-
ing an appropriate syntactic dependency relation
between the bio-event trigger word identified in
72
earlier steps and the speculation cue. The cate-
gory of the hedging cue constrained the depen-
dency relations that are deemed appropriate. For
example, consider the sentence in (2a), where in-
volves is a bio-event trigger for a Regulation
event and suggest is a speculation cue of epis-
temic verb type. The first dependency relation
in (2b) indicates that the epistemic verb takes a
clausal complement headed by the bio-event trig-
ger. The second indicates that that is the comple-
mentizer. This cue category/dependency combi-
nation licenses the generation of a speculation in-
stance where the event indicated by the event trig-
ger represents the scope.
(2) (a) The results suggest that M-CSF induc-
tion of M-CSF involves G proteins, PKC
and NF kappa B.
(b) ccomp(suggest,involves)
complm(involves,that)
Several other cue category/dependency combi-
nations sought for speculation scope resolution are
given in Table 1. X represents a token that is nei-
ther a cue nor a trigger (aux: auxiliary, dobj: direct
object, neg: negation modifier).
Cue Category Dependency
Modal auxiliary (may) aux(Trigger,Cue)
Conditional (if ) complm(Trigger,Cue)
Unhedging noun dobj(X,Cue)
(evidence) ccomp(X,Trigger)
neg(Cue,no)
Table 1: Cue categories with examples and the de-
pendency relations to search
In contrast to this notion of scope being an ab-
stract semantic object, Task 2 (BioScope corpus,
in general) conceptualizes hedge scope as a con-
tinuous textual unit, including the hedging cue it-
self and the biggest syntactic unit the cue is in-
volved in (Vincze et al, 2008). This fundamen-
tal difference in conceptualization limits the di-
rect applicability of our prior approach to this
task. Nevertheless, we were able to use our work
as a building block in extending scope resolution
heuristics. We further augmented it by exploiting
constituent parse trees provided by Stanford Lex-
icalized Parser. These extensions are summarized
below.
3.3.1 Exploiting parse trees
The constituent parse trees contribute to scope
resolution uniformly across all hedging cue cate-
gories. We simply determine the phrasal node that
dominates the hedging cue and consider the tokens
within that phrase as being in the scope of the cue,
unless they meet one of the following exclusion
criteria:
1. Exclude tokens within post-cue sentential
complements (indicated by S and SBAR
nodes) introduced by a small number of
discourse markers (thus, whereas, because,
since, if, and despite).
2. Exclude punctuation marks at the right
boundary of the phrase
3. Exclude pre-cue determiners and adverbs at
the left boundary of the phrase
For example, in the sentence below, the verb
phrase that included the modal auxiliary may also
included the complement introduced by thereby.
Using the exclusion criteria 1 and 2, we excluded
the tokens following SPACER from the scope:
(3) (a) . . .motifs may be easily compared with
the results from BEAM, PRISM and
SPACER, thereby extending the SCOPE
ensemble to include a fourth class of
motifs.
(b) CUE: may
SCOPE: motifs may be easily compared
with the results from BEAM, PRISM
and SPACER
3.3.2 Extending dependency-based heuristics
The new scope definition was also accommodated
by extending the basic dependency-based heuris-
tics summarized earlier in this section. In addition
to finding the trigger word that satisfies the ap-
propriate dependency constraint with the hedging
cue (we refer to this trigger word as scope head,
henceforth), we also considered the other depen-
dency relations that the scope head was involved
in. These relations, then, were used in right ex-
pansion and left expansion of the scope. Right ex-
pansion involves finding the rightmost token that
is in a dependency relation with the scope head.
Consider the sentence below:
(4) The surprisingly low correlations between
Sig and accuracy may indicate that the ob-
jective functions employed by motif finding
73
programs are only a first approximation to bi-
ological significance.
The epistemic verb indicate has as its scope
head the token approximation, due to the existence
of a clausal complement dependency (ccomp) be-
tween them. On the other hand, the rightmost to-
ken of the sentence, significance, has a preposi-
tional modifier dependency (prep to) with approx-
imation. It is, therefore, included in the scope of
indicate. Two dependency types, adverbial clause
modifier (advcl) and conjunct (conj), were ex-
cluded from consideration when the rightmost to-
ken is sought, since they are likely to signal new
discourse units outside the scope.
In contrast to right expansion, which applies
to all hedging cue categories, left expansion ap-
plies only to a subset. Left expansion involves
searching for a subject dependency governed by
the scope head. The dependency types descend-
ing from the subject (subj) type in the Stanford de-
pendency hierarchy are considered: nsubj (nom-
inal subject), nsubjpass (passive nominal sub-
ject), csubj (clausal subject) and csubjpass (pas-
sive clausal subject). In the following example,
the first token, This, is added to the scope of likely
through left expansion (cop: copula).
(5) (a) This is most likely a conservative esti-
mate since a certain proportion of inter-
actions remain unknown . . .
(b) nsubj(likely,This)
cop(likely,is)
Left expansion was limited to the following cue
categories, with the additional constraints given:
1. Modal auxiliaries, only when their scope
head takes a passive subject (e.g., they is
added to the scope of may in they may be an-
notated as pseudogenes).
2. Cues in adjectival categories, when they are
in copular constructions (e.g., Example (5)).
3. Cues in several adjectival ad verbal cate-
gories, when they take infinitival comple-
ments (e.g., this is added to the scope of ap-
pears in However, this appears to add more
noise to the prediction without increasing the
accuracy).
After scope tokens are identified using the parse
tree as well as via left and right expansion, the al-
gorithm simply sets as scope the continuous tex-
tual unit that includes all the scope tokens and the
hedging cue. Since, likely is the hedging cue and
This and estimate are identified as scope tokens in
Example (5), the scope associated with likely be-
comes This is most likely a conservative estimate.
We found that citations, numbers and punc-
tuation marks occurring at the end of sentences
caused problems in scope resolution, specifically
in biomedical full text articles. Since they are
rarely within any scope, we implemented a simple
stripping algorithm to eliminate them from scopes
in such documents.
4 Results and Discussion
The official evaluation results regarding our sub-
mission are given in Table 2. These results were
achieved with the threshold 4, which was the opti-
mal threshold on the training data.
Prec. Recall F-score Rank
Task 1B 92.07 74.94 82.62 12/24
Task 1W 67.90 46.02 54.86 10/17
Task 2 62.47 49.47 55.21 4/15
Table 2: Evaluation results
In Task 1B, we achieved the highest precision.
However, our relatively low recall led to the place-
ment of our system in the middle. Our system al-
lows adjusting precision versus recall by setting
the threshold. In fact, setting the threshold to 3 af-
ter the shared task, we were able to obtain overall
better results (Precision=83.43, Recall=84.81, F-
score=84.12, Rank=8/24). However, we explicitly
targeted precision, and in that respect, our submis-
sion results were not surprising. In fact, we iden-
tified a new type of hedging signalled by coordi-
nation (either . . . or . . . as well as just or) in the
training data. An example is given below:
(6) (a) It will be either a sequencing error or a
pseudogene.
(b) CUE: either-or
SCOPE: either a seqeuncing error or a
pseudogene
By handling this class to some extent, we could
have increased our recall, and therefore, F-score
(65 out of 1,044 cues in the evaluation data for
biological text involved this class). However, we
decided against treating this class, as we believe
it requires a slightly different treatment due to its
special semantics.
74
In participating in Task 1W, our goal was to
test the ease of extensibility of our system. In
that regard, our results show that we were able
to exploit the overlap between our hedging cues
and the weasel words. The major difference we
noted between hedging in two genres was the
class of vagueness quantifiers, and, with little ef-
fort, we extended our system to consider them.
We also note that setting the threshold to 3 after
the shared task, our recall and F-score improved
significantly (Precision=63.21, Recall=53.67, F-
score=58.05, Rank=3/17).
Our more substantial effort for Task 2 resulted
in a better overall ranking, as well as the highest
precision in this task. In contrast to Task 1, chang-
ing the threshold in this task did not have a pos-
itive effect on the outcome. We also measured
the relative contribution of the enhancements to
scope resolution. The results are presented in Ta-
ble 3. Baseline is taken as the scope resolution al-
gorithm we developed in prior work. These results
show that: a) scope definition we adopted earlier
is essentially incompatible with the BioScope def-
inition b) simply taking the phrase that the hedg-
ing cue belongs to as the scope provides relatively
good results c) left and right expansion heuristics
are needed for increased precision and recall.
Prec. Recall F-score
Baseline 3.29 2.61 2.91
Baseline+ Left/
right expansion
25.18 20.03 22.31
Parse tree 49.20 39.10 43.58
Baseline+
Parse tree
50.66 40.27 44.87
All 62.47 49.47 55.21
Table 3: Effect of scope resolution enhancements
4.1 Error Analysis
In this section, we provide a short analysis of the
errors our system generated, focusing on biologi-
cal text.
Since our dictionary of hedging cues is incom-
plete and we did not attempt to expand it for Task
1B, we had a fair number of recall errors. As
we mentioned above, either-or constructions oc-
cur frequently in the training and evaluation data,
and we did not attempt to handle them. Addition-
ally, some lexical cues, such as feasible and im-
plicate, do not appear in our dictionary, causing
further recall errors. The weighting scheme also
affects recall. For example, the adjective appar-
ent has a weight of 2, which is not itself sufficient
to qualify a sentence as uncertain (with a thresh-
old of 4) (7a). On the other hand, when it takes
a clausal complement, the sentence is considered
uncertain (7b). The first sentence (7a) causes a re-
call error.
(7) (a) An apparent contradiction between the
previously reported number of cycling
genes . . .
(b) . . . it is apparent that the axonal termini
contain a significantly reduced number
of varicosities . . .
In some cases, syntactic constructions that play
a role in determining the certainty status of a sen-
tence cannot be correctly identified by the parser,
often leading to recall errors. For example, in the
sentence below, the clausal complement construc-
tion is missed by the parser. Since the verb indi-
cate has weight 3, this leads to a recall error in the
current setting.
(8) . . . indicating that dMyc overexpression can
substitute for PI3K activation . . .
Adjusting the weights of cues worked well gen-
erally, but also caused unexpected problems, due
to what seem like inconsistencies in annotation.
The examples below highlight the effect of low-
ering the weight of predict from 4 to 3. Exam-
ples (9a) and (9b) are almost identical on surface
and our system predicted both to be uncertain, due
to the fact that predicted took infinitival comple-
ments in both cases. However, only (9a) was an-
notated as uncertain, leading to a precision error in
(9b).
(9) (a) . . . include all protein pairs predicted to
have posterior odds ratio . . .
(b) Protein pairs predicted to have a poste-
rior odds ratio . . .
The error cases in scope resolution are more
varied. Syntax has a larger role in this task, and
therefore, parsing errors tend to affect the results
more directly. In the following example, dur-
ing left-expanding the scope of the modal auxil-
iary could, RNAi screens, rather than the full noun
phrase fruit fly RNAi screens, is identified as the
passive subject of the scope head (associated), be-
cause an appropriate modifier dependency cannot
75
be found between the noun phrase head screens
and either of the modifiers, fruit and fly.
(10) . . . was to investigate whether fruit fly RNAi
screens of conserved genes could be asso-
ciated with similar tick phenotypes and tick
gene function.
In general, the simple mechanism to exploit
constituent parse trees was useful in resolving
scope. However, it appears that a nuanced ap-
proach based on cue categories could enhance the
results further. In particular, the current mecha-
nism does not contribute much to resolving scopes
of adverbial cues. In the following example, parse
tree mechanism does not have any effect, leading
to both a precision and a recall error in scope res-
olution.
(11) (a) . . . we will consider tightening the defi-
nitions and possibly splitting them into
different roles.
(b) FP: possibly
FN: possibly splitting them into differ-
ent roles
Left/right expansion strategies were based on
the analysis of training data. However, we en-
countered errors caused by these strategies where
we found the annotations contradictory. In Exam-
ple (12a), the entire fragment is in the scope of
thought, while in (12b), the scope of suggested
does not include it was, even though on surface
both fragments are very similar.
(12) (a) . . . the kinesin-5 motor is thought to play
a key role.
(b) . . . it was suggested to enhance the nu-
clear translocation of NF-?B.
Post-processing in the form of citation stripping
was simplistic, and, therefore, was unable to han-
dle complex cases, as the one shown in the exam-
ple below. The algorithm is only able to remove
one reference at the end.
(13) (a) . . . it is possible that some other sig-
nalling system may operate with Semas
to confine dorsally projecting neurons to
dorsal neuropile [3],[40],[41].
(b) FP: may operate with Semas to con-
fine dorsally projecting neurons to dor-
sal neuropile [3],[40],
FN: may operate with Semas to con-
fine dorsally projecting neurons to dor-
sal neuropile
5 Conclusions
Rather than developing a dedicated methodology
that exclusively relies on the data provided by or-
ganizers, we chose to extend and refine our prior
work in hedge detection and used the training
data only in a limited manner: to tune our sys-
tem in a principled way. With little tuning, we
achieved the highest precision in Task 1B. We
were able to capitalize on the overlap between
hedging cues and weasel words for Task 1W and
achieved competitive results. Adapting our pre-
vious work in scope resolution to Task 2, how-
ever, was less straightforward, due to the incom-
patible definitions of scope. Nevertheless, by re-
fining the prior dependency-based heuristics with
left and right expansion strategies and utilizing a
simple mechanism for parse tree information, we
were able to accommodate the new definition of
scope to a large extent. With these results, we con-
clude that our methodology is portable and easily
extensible.
While the results show that using the parse tree
information for scope resolution benefited our per-
formance greatly, error analysis presented in the
previous sections also suggests that a finer-grained
approach based on cue categories could further
improve results, and we aim to explore this exten-
sion further.
References
Marie-Catherine deMarneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449?
454.
Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nos
Csirik, and Gyo?rgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1?12, Uppsala, Sweden, July. Associa-
tion for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: an electronic
lexical database. MIT Press, Cambridge, MA.
Viola Ganter and Michael Strube. 2009. Finding
Hedges by Chasing Weasels: Hedge Detection Us-
ingWikipedia Tags and Shallow Linguistic Features.
In Proceedings of the ACL-IJCNLP 2009 Confer-
ence Short Papers, pages 173?176.
76
Ken Hyland. 1998. Hedging in scientific research ar-
ticles. John Benjamins B.V., Amsterdam, Nether-
lands.
Halil Kilicoglu and Sabine Bergler. 2008. Recogniz-
ing speculative language in biomedical research ar-
ticles: a linguistically motivated perspective. BMC
Bioinformatics, 9 Suppl 11:s10.
Halil Kilicoglu and Sabine Bergler. 2009. Syntac-
tic dependency based heuristics for biological event
extraction. In Proceedings of Natural Language
Processing in Biomedicine (BioNLP) NAACL 2009
Workshop, pages 119?127.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008. Corpus annotation for mining biomedical
events from literature. BMC Bioinformatics, 9:10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing
in Biomedicine (BioNLP) NAACL 2009 Workshop,
pages 1?9.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41th Meeting of the Association for Computational
Linguistics, pages 423?430.
Shalom Lappin. 2000. An intensional parametric se-
mantics for vague quantifiers. Linguistics and Phi-
losophy, 23(6):599?620.
Marc Light, Xin Y. Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: facts, speculations, and
statements in between. In BioLINK 2004: Linking
Biological Literature, Ontologies and Databases,
pages 17?24.
Alexa T. McCray, Suresh Srinivasan, and Allen C.
Browne. 1994. Lexical methods for managing vari-
ation in biomedical terminologies. In Proceedings
of the 18th Annual Symposium on Computer Appli-
cations in Medical Care, pages 235?239.
Ben Medlock and Ted Briscoe. 2007. Weakly su-
pervised learning for hedge classification in scien-
tific literature. In Proceedings of the 45th Meet-
ing of the Association for Computational Linguis-
tics, pages 992?999.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts. In
Proceedings of the BioNLP 2009 Workshop, pages
28?36.
Arzucan O?zgu?r and Dragomir R. Radev. 2009. Detect-
ing speculations and their scopes in scientific text.
In Proceedings of the 2009 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1398?1407.
Bo Pang and Lillian Lee. 2008. Sentiment Analysis
and Opinion Mining. Now Publishers Inc, Boston,
MA.
James Pustejovsky, Robert Knippen, Jessica Littman,
and Roser Saur??. 2005. Temporal and event in-
formation in natural language text. Language Re-
sources and Evaluation, 39(2):123?164.
Victoria L. Rubin, Elizabeth D. Liddy, and Noriko
Kando. 2005. Certainty identification in texts: Cat-
egorization model and manual tagging results. In
James G. Shanahan, Yan Qu, and Janyce Wiebe, ed-
itors, Computing Attitude and Affect in Text: The-
ories and Applications, volume 20, pages 61?76.
Springer Netherlands, Dordrecht.
Roser Saur?? and James Pustejovsky. 2009. FactBank:
a corpus annotated with event factuality. Language
Resources and Evaluation, 43(3):227?268.
Roser Saur??, Marc Verhagen, and James Pustejovsky.
2006. Annotating and recognizing event modality in
text. In Proceedings of 19th International FLAIRS
Conference.
Roser Saur??. 2008. A Factuality Profiler for Eventual-
ities in Text. Ph.D. thesis, Brandeis University.
Gyo?rgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of the 46th Meeting
of the Association for Computational Linguistics,
pages 281?289.
Paul Thompson, Giulia Venturi, John McNaught,
Simonetta Montemagni, and Sophia Ananiadou.
2008. Categorising modality in biomedical texts. In
Proceedings of LREC 2008 Workshop on Building
and Evaluating Resources for Biomedical Text Min-
ing.
Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,
Gyo?rgy Mo?ra, and Ja?nos Csirik. 2008. The Bio-
Scope corpus: biomedical texts annotated for uncer-
tainty, negation and their scopes. BMC Bioinformat-
ics, 9 Suppl 11:S9.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2):165?210.
77
Proceedings of BioNLP Shared Task 2011 Workshop, pages 173?182,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Adapting a General Semantic Interpretation Approach to Biological Event
Extraction
Halil Kilicoglu and Sabine Bergler
Department of Computer Science and Software Engineering
Concordia University
1455 de Maisonneuve Blvd. West
Montre?al, Canada
{h kilico,bergler}@cse.concordia.ca
Abstract
The second BioNLP Shared Task on Event
Extraction (BioNLP-ST?11) follows up the
previous shared task competition with a focus
on generalization with respect to text types,
event types and subject domains. In this spirit,
we re-engineered and extended our event ex-
traction system, emphasizing linguistic gener-
alizations and avoiding domain-, event type-
or text type-specific optimizations. Similar
to our earlier system, syntactic dependencies
form the basis of our approach. However, di-
verging from that system?s more pragmatic na-
ture, we more clearly distinguish the shared
task concerns from a general semantic com-
position scheme, that is based on the no-
tion of embedding. We apply our methodol-
ogy to core bio-event extraction and specu-
lation/negation detection tasks in three main
tracks. Our results demonstrate that such a
general approach is viable and pinpoint some
of its shortcomings.
1 Introduction
In the past two years, largely due to the availabil-
ity of GENIA event corpus (Kim et al, 2008) and
the resulting shared task competition (BioNLP?09
Shared Task on Event Extraction (Kim et al,
2009)), event extraction in biological domain has
been attracting greater attention. One of the crit-
icisms towards this paradigm of corpus annota-
tion/competition has been that they are concerned
with narrow domains and specific representations,
and that they may not generalize well. For in-
stance, GENIA event corpus contains only Medline
abstracts on transcription factors in human blood
cells. Whether models trained on this corpus would
perform well on full-text articles or on text focusing
on other aspects of biomedicine (e.g., treatment or
etiology of disease) remains largely unclear. Since
annotated corpora are not available for every con-
ceivable domain, it is desirable for automatic event
extraction systems to be generally applicable to dif-
ferent types of text and domains without requiring
much training data or customization.
GENIA EPI ID BB BI
# core events 9 15 10 2 10
Triggers? Y Y Y N N
Full-text? Y N Y N N
Spec/Neg? Y Y Y N N
Table 1: An overview of BioNLP-ST?11 tracks
In the follow-up event to BioNLP?09 Shared
Task on Event Extraction, organizers of the second
BioNLP Shared Task on Event Extraction (BioNLP-
ST?11) (Kim et al, 2011a) address this challenge to
some extent. The theme of BioNLP-ST?11 is gen-
eralization and the net is cast much wider. There
are 4 event extraction tracks: in addition to the GE-
NIA track that again focuses on transcription fac-
tors (Kim et al, 2011b), the epigenetics and post-
translational modification track (EPI) focuses on
events relating to epigenetic change, such as DNA
methylation and histone modification, as well as
other common post-translational protein modifica-
tions (Ohta et al, 2011), whereas the infectious dis-
eases track (ID) focuses on bio-molecular mecha-
nisms of infectious diseases (Pyysalo et al, 2011a).
Both GENIA and ID tracks include data pertaining
to full-text articles, as well. The fourth track, Bacte-
ria, consists of two sub-tracks: Biotopes (BB) and
Interactions (BI) (Bossy et al (2011) and Jourde
173
et al (2011), respectively). A summary of the
BioNLP-ST?11 tracks is given in Table (1).
We participated in three tracks: GENIA, EPI, and
ID. In the spirit of the competition, our aim was to
demonstrate a methodology that was general and re-
quired little, if any, customization or training for in-
dividual tracks. For this purpose, we used a two-
phase approach: a syntax-driven composition phase
that exploits linguistic generalizations to create a
general semantic representation in a bottom-up man-
ner and a mapping phase, which relies on the shared
task event definitions and constraints to map rele-
vant parts of this semantic representation to event
instances. The composition phase takes as its input
simple entities and syntactic dependency relations
and is intended to be fully general. On the other
hand, the second phase is more task-specific even
though the kind of task-specific knowledge it re-
quires is largely limited to event definitions and trig-
ger expressions. In addition to extracting core bio-
logical events, our system also addresses speculation
and negation detection within the same framework.
Our results demonstrate the feasibility of a method-
ology that uses little training data or customization.
2 Methodology
In our general research, we are working towards
a linguistically-grounded, bottom-up discourse in-
terpretation scheme. In particular, we focus on
lower level discourse phenomena, such as causation,
modality, and negation, and investigate how they in-
teract with each other, as well as their effect on ba-
sic propositional semantic content (who did what to
who?) and higher discourse/pragmatics structure. In
our model, we distinguish three layers of proposi-
tions: atomic, embedding, and discourse. An atomic
proposition corresponds to the basic unit and low-
est level of meaning: in other words, a semantic re-
lation whose arguments correspond to ontologically
simple entities. Atomic propositions form the ba-
sis for embedding propositions, that is, propositions
taking as arguments other propositions (embedding
them). In turn, embedding and atomic propositions
act as arguments for discourse relations1. Our main
1Discourse relations, also referred to as coherence or rhetor-
ical relations (Mann and Thompson, 1988), are not relevant to
the shared task and, thus, we will not discuss them further in
motivation in casting the problem of discourse in-
terpretation in this structural manner is two-fold: a)
to explore the semantics of the embedding layer in
a systematic way b) to allow a bottom-up semantic
composition approach, which works its way from
atomic propositions towards discourse relations in
creating general semantic representations.
The first phase of our event extraction system
(composition) is essentially an implementation of
this semantic composition approach. Before delving
into further details regarding our implementation for
the shared task, however, it is necessary to briefly ex-
plain the embedding proposition categorization that
our interpretation scheme is based on. With this cat-
egorization, our goal is to make explicit the kind
of semantic information expressed at the embedding
layer. We distinguish three basic classes of embed-
ding propositions: MODAL, ATTRIBUTIVE, and RE-
LATIONAL. We provide a brief summary below.
2.1 MODAL type
The embedding propositions of MODAL type mod-
ify the status of the embedded proposition with re-
spect to its factuality, possibility, or necessity, and
so on. They typically involve a) judgement about
the status of the proposition, b) evidence for the
proposition, c) ability or willingness, and d) obli-
gations and permissions, corresponding roughly to
EPISTEMIC, EVIDENTIAL, DYNAMIC and DEONTIC
types (cf. Palmer (1986)), respectively. Further sub-
divisions are given in Figure (1). In the shared task
context, the MODAL class is mostly relevant to the
speculation and negation detection tasks.
2.2 ATTRIBUTIVE type
The ATTRIBUTIVE type of embedding serves to
specify an attribute of an embedded proposition (se-
mantic role of an argument). They typically involve
a verbal predicate (undergo in Example (1) below),
which takes a nominalized predicate (degradation)
as one of its syntactic arguments. The other syntac-
tic argument of the verbal predicate corresponds to
a semantic argument of the embedded predicate. In
Example (1), p105 is a semantic argument of PA-
TIENT type for the proposition indicated by degra-
dation.
this paper.
174
(1) . . . p105 undergoes degradation . . .
Verbs functioning in this way are plenty (e.g., per-
form for the AGENT role, experience for experiencer
role). With respect to the shared task, we found that
the usefulness of the ATTRIBUTIVE type of embed-
ding was largely limited to verbal predicates involve
and require and their nominal forms.
2.3 RELATIONAL type
The RELATIONAL type of embedding serves to se-
mantically link two propositions, providing a dis-
course/pragmatic function. It is characterized by
permeation of a limited set of discourse relations to
the clausal level, often signalled lexically by ?dis-
course verbs? (Danlos, 2006) (e.g., cause, mediate,
lead, correlate), their nominal forms or other ab-
stract nouns, such as role. We categorize the RELA-
TIONAL class into CAUSAL, TEMPORAL, CORREL-
ATIVE, COMPARATIVE, and SALIENCY types. In the
example below, the verbal predicate leads to indi-
cates a CAUSAL relation between the propositions
whose predicates are highlighted.
(2) Stimulation of cells leads to a rapid phospho-
rylation of I?B? . . .
While not all the subtypes of this class were relevant
to the shared task, we found that CAUSAL, CORREL-
ATIVE, and SALIENCY subtypes play a role, partic-
ularly in complex regulatory events. The portions of
the classification that pertain to the shared task are
given in Figure (1).
3 Implementation
In the shared task setting, embedding propositions
correspond to complex regulatory events (e.g., Reg-
ulation, Catalysis) as well as event modifications
(Negation and Speculation), whereas atomic propo-
sitions correspond to simple event types (e.g., Phos-
phorylation). While the treatment of these two types
differ in significant ways, they both require that sim-
ple entities are recognized, syntactic dependencies
are identified and a dictionary of trigger expressions
is available. We first briefly explain the construction
of the trigger dictionary.
3.1 Dictionary of Trigger Expressions
In the previous shared task, we relied on training
data and simple statistical measures to identify good
Figure 1: Embedding proposition categorization relevant
to the shared task
trigger expressions for events and used a list of trig-
gers that we manually compiled for speculation and
negation detection (see Kilicoglu and Bergler (2009)
for details). With respect to atomic propositions,
our method of constructing a dictionary of trigger
expressions remains essentially the same, including
the use of statistical measures to distinguish good
triggers. The only change we made was to consider
affixal negation and set polarity of several atomic
proposition triggers to negative (e.g., nonexpression,
unglycosylated). On the other hand, we have been
extending our manually compiled list of specula-
tion/negation triggers to include other types of em-
bedding triggers and to encode finer grained distinc-
tions in terms of their categorization and trigger be-
haviors. The training data provided for the shared
task also helped us expand this trigger dictionary,
particularly with respect to RELATIONAL trigger ex-
pressions. It is worth noting that we used the same
embedding trigger dictionary for all three tracks that
we participated in. Several entries from the embed-
ding trigger dictionary are summarized in Table (2).
Lexical polarity and strength values play a role
in the composition phase in associating a context-
dependent scalar value with propositions. Lexical
polarity values are largely derived from a polarity
lexicon (Wilson et al, 2005) and extended by us-
175
Trigger POS Semantic Type Lexical Polarity Strength
show VB DEMONSTRATIVE positive 1.0
unknown JJ EPISTEMIC negative 0.7
induce VB CAUSAL positive 1.0
fail VB SUCCESS negative 0.0
effect NN CAUSAL neutral 0.5
weakly RB HEDGE neutral -
absence NN REVERSE negative -
Table 2: Several entries from the embedding dictionary
ing heuristics involving the event types associated
with the trigger2. Some polarity values were as-
signed manually. Some strength values were based
on prior work (Kilicoglu and Bergler, 2008), oth-
ers were manually assigned. As Table (2) shows, in
some cases, the semantic type (e.g., DEMONSTRA-
TIVE, CAUSAL) is simply a mapping to the embed-
ding categorization. In other cases, such as weakly
or absence, the semantic type identifies the role that
the trigger plays in the composition phase. The em-
bedding trigger dictionary incorporates ambiguity;
however, for the shared task, we limit ourselves to
one semantic type per trigger to avoid the issue of
disambiguation. For ambiguous triggers extracted
from the training data, the semantic type with the
maximum likelihood is used. On the other hand, we
determined the semantic type to use manually for
triggers that we compiled independent of the train-
ing data. In this way, we use 466 triggers for atomic
propositions and 908 for embedding ones3.
3.2 Composition
As mentioned above, the composition phase as-
sumes simple entities, syntactic dependency rela-
tions and trigger expressions. Using these elements,
we construct a semantic embedding graph of the
document. To obtain syntactic dependency relations,
we segment documents into sentences, parse them
using the re-ranking parser of Charniak and John-
son (2005) adapted to the biomedical domain (Mc-
Closky and Charniak, 2008) and extract syntactic
2For example, if the most likely event type associated with
the trigger is Negative regulation, its polarity is considered neg-
ative.
3Note, however, that not all embedding propositions (or their
triggers) were directly relevant to the shared task.
dependencies from parse trees using the Stanford
dependency scheme (de Marneffe et al, 2006). In
addition to syntactic dependencies, we also require
information regarding individual tokens, including
lemma, part-of-speech, and positional information,
for which we also rely on Stanford parser tools. We
present a high level description of the composition
phase below.
3.2.1 From syntactic dependencies to
embedding graphs
As the first step in composition, we convert syn-
tactic dependencies into embedding relations. An
embedding relation, in our definition, is very simi-
lar to a syntactic dependency; it is typed and holds
between two textual elements. It diverges from a
syntactic dependency in two ways: its elements can
be multi-word expressions and it is aimed at better
reflecting the direction of the semantic dependency
between its elements. Take, for example, the sen-
tence fragment in Example (3a). Syntactic depen-
dencies are given in (3b) and the corresponding em-
bedding relations in (3c). The fact that the adjecti-
val predicate in modifier position (possible) semanti-
cally embeds its head (involvement) is captured with
the first embedding relation. The second syntactic
dependency already reflects the direction of the se-
mantic dependency between its elements accurately
and, thus, is unchanged as an embedding relation.
(3) (a) . . . possible involvement of HCMV . . .
(b) amod(involvement,possible)
prep of (involvement,HCMV)
(c) amod(possible,involvement)
prep of (involvement,HCMV)
To obtain the embedding relations in a sentence,
we apply a series of transformations to its syntactic
176
Figure 2: The embedding graph for the sentence Our previous results show that recombinant gp41 (aa565-647), the
extracellular domain of HIV-1 transmembrane glycoprotein, stimulates interleukin-10 (IL-10) production in human
monocytes. in the context of the document embedding graph for the Medline abstract with PMID 10089566.
dependencies. A transformation may not be neces-
sary, as with the prep of dependency in the exam-
ple above. It may result in collapsing several syn-
tactic dependencies into one, as well, or in splitting
one into several embedding relations. In addition
to capturing semantic dependency behavior explic-
itly, these transformations serve to incorporate se-
mantic information (entities and triggers) into the
embedding structure and to correct syntactic depen-
dencies that are systemically misidentified, such as
those that involve modifier coordination.
After these transformations, the resulting directed
acyclic embedding graph is, in the simplest case, a
tree, but more often a forest. An example graph is
given in Figure (2). The edges are associated with
the embedding relation types, and the nodes with
textual elements.
3.2.2 Composing Propositions
After constructing the embedding graph, we tra-
verse it in a bottom-up manner and compose se-
mantic propositions. Before this procedure can take
place, though, the embedding graph pertaining to
each sentence is further linked to the document em-
bedding graph in a way to reflect the proximity of
sentences, as illustrated in Figure (2). This is done
to enable discourse interpretation across sentences,
including coreference resolution.
Traversal of the embedding structure is guided by
argument identification rules, which apply to non-
leaf nodes in the embedding graph. An argument
identification rule is essentially a mapping from the
type of the embedding relation holding between a
parent node and its child node and part-of-speech of
the parent node to a logical argument type (logical
subject, logical object or adjunct). Constraints on
and exclusions from a rule can be defined, as shown
in Table (3). We currently use about 80 such rules,
mostly adapted from our previous shared task sys-
tem (Kilicoglu and Bergler, 2009).
After all the descendants of a non-leaf node are
recursively processed for arguments, a semantic
proposition can be composed. We define a seman-
tic proposition as consisting of a trigger, a collection
177
Relation Applies to Argument Constrained to Exclusions
prep on NN Object influence,impact,effect -
agent VB Subject - -
nsubjpass VB Object - -
whether comp VB Object INTERROGATIVE -
prep in NN Adjunct - effect, role, influence, importance
Table 3: Several argument identification rules. Note that constraints and exclusions may apply to trigger categories, as
well as to lemmas.
of core and adjunct arguments as well as a polarity
value and a scalar value. The polarity value can be
positive, negative or neutral. The scalar value is in
the (0,1) range. Atomic propositions are simply as-
signed polarity value of neutral4 and the scalar value
of 1.0. On the other hand, in the context of embed-
ding propositions, the computation of these values,
through which we attempt to capture some of the in-
teractions occurring at the embedding layer, is more
involved. For the sentence depicted in Figure (2),
the relevant resulting embedding and atomic propo-
sitions are given below.
(4) DEMONSTRATIVE(em1,Trigger=show,
Object=em2, Subject=Our previous results,
Polarity=positive, Value=1.0)
(5) CAUSAL(em2, Trigger=stimulates, Object=ap1,
Subject=recombinant gp41, Polarity=positive,
Value=1.0)
(6) Gene expression(ap1, Trigger= production,
Object= interleukin-10, Adjunct= human
monocytes, Polarity=neutral, Value=1.0)
The composition phase also deals with coordina-
tion of entities and propositions as well as with prop-
agation of arguments at the lower levels.
3.3 Mapping Propositions to Events
The goal of the mapping phase is to impose the
shared task constraints on the partial interpretation
achieved in the previous phase. We achieve this in
three steps.
The first step is to map embedding proposition
types to event (or event modification) types. We de-
fined constraints that guide this mapping. Some of
4Unless affixal negation is involved, in which case the as-
signed polarity value is negative.
these mappings are presented in Table (4). In this
way, Example (4) is pruned, since embedding propo-
sitions of DEMONSTRATIVE type satisfy the con-
straints only if they have negative polarity, as shown
in Table (4).
We then apply constraints concerned with the se-
mantic roles of the participants. For this step, we
define a small number of logical argument/semantic
role mappings. These are similar to argument identi-
fication rules, in that the mapping can be constrained
to certain event types or event types can be excluded
from it. We provide some of these mappings in Ta-
ble (5). With these mappings, the Object and Sub-
ject arguments of the proposition in Example (5) are
converted to Theme and Cause semantic roles, re-
spectively.
As the final step, we prune event participants that
do not conform to the event definition as well as the
propositions whose types could not be mapped to a
shared task event type. For example, a Cause par-
ticipant for a Gene expression event is pruned, since
only Theme participants are relevant for the shared
task. Further, a proposition with DEONTIC seman-
tic type is pruned, because it cannot be mapped to
a shared task type. The infectious diseases track
(ID) event type Process is interesting, because it may
take no participants at all, and we deal with this id-
iosyncrasy at this step, as well. This concludes the
progressive transformation of the graph to event and
event modification annotations.
4 Results and Discussion
With the two-phase methodology presented above,
we participated in three tracks: GENIA (Tasks 1 and
3), ID, and EPI. The official evaluation results we
obtained for the GENIA track are presented in Ta-
ble (6) and the results for the EPI and ID tracks in
178
Track Prop. Type Polarity Value Correspond. Event (Modification) Type
GENIA,ID CAUSAL neutral - Regulation
GENIA,ID,EPI SUCCESS negative - Negation
EPI CAUSAL positive - Catalysis
GENIA,ID,EPI SPECULATIVE - > 0.0 Speculation
GENIA,ID,EPI DEMONSTRATIVE negative - Speculation
Table 4: Several event (and event modification) mappings
Logical
Arg.
Semantic
Role
Constraint Exclusion
Object Theme - Process
Subject Cause - -
Subject Theme Binding -
Object Participant Process -
Object Scope Speculation,
Negation
-
Table 5: Logical argument to semantic role mappings
Table (7). With the official evaluation criteria, we
were ranked 5th in the GENIA track (5/15), 7th in
the EPI track (7/7) and 4th in the ID track (4/7).
There were only two submissions for the GENIA
speculation/negation task (Task 3) and our results
in this task were comparable to those of the other
participating group: our system performed slightly
better with speculation, and theirs with negation.
Our core module extracts adjunct arguments, us-
ing ABNER (Settles, 2005) as its source for addi-
tional named entities. We experimented with map-
ping these arguments to non-core event participants
(Site, Contextgene, etc.); however, we did not in-
clude them in our official submission, because they
seemed to require more work with respect to map-
ping to shared task specifications. Due to this short-
coming, the performance of our system suffered sig-
nificantly in the EPI track.
A particularly encouraging outcome for our sys-
tem is that our results on the GENIA development
set versus on the test set were very close (an F-
score of 51.03 vs. 50.32), indicating that our gen-
eral approach avoided overfitting, while capturing
the linguistic generalizations, as we intended. We
observe similar trends with the other tracks, as well.
In the EPI track, development/test F-score results
were 29.10 vs. 27.88; while, in the ID track, inter-
Event Class Recall Precis. F-score
Localization 39.27 90.36 54.74
Binding 29.33 49.66 36.88
Gene expression 65.87 86.84 74.91
Transcription 32.18 58.95 41.64
Protein catabolism 66.67 71.43 68.97
Phosphorylation 75.14 94.56 83.73
EVT-TOTAL 52.67 78.04 62.90
Regulation 33.77 42.48 37.63
Positive regulation 35.97 47.66 41.00
Negative regulation 36.43 43.88 39.81
REG-TOTAL 35.72 45.85 40.16
Negation 18.77 44.26 26.36
Speculation 21.10 38.46 27.25
MOD-TOTAL 19.97 40.89 26.83
ALL-TOTAL 43.55 59.58 50.32
Table 6: Official GENIA track results, with approximate
span matching/approximate recursive matching evalua-
tion criteria
estingly, our test set performance was better (39.64
vs. 44.21). We also obtained the highest recall in
the ID track, despite the fact that our system typi-
cally favors precision. We attribute this somewhat
idiosyncratic performance in the ID track partly to
the fact that we did not use a track-specific trigger
dictionary. Most of the ID track event types are
the same as those of GENIA track, which probably
led to identification of some ID events with GENIA-
only triggers5.
One of the interesting aspects of the shared task
was its inclusion of full-text articles in training and
evaluation. Cohen et al (2010) show that structure
and content of biomedical abstracts and article bod-
ies differ markedly and suggest that some of these
5This clearly also led to low precision particularly in com-
plex regulatory events.
179
Track-Eval. Type Recall Precis. F-score
EPI-FULL 20.83 42.14 27.88
EPI-CORE 40.28 76.71 52.83
ID-FULL 49.00 40.27 44.21
ID-CORE 50.77 43.25 46.71
Table 7: Official evaluation results for EPI and ID tracks.
Primary evaluation criteria underlined.
differences may pose problems in processing full-
text articles. Since one of our goals was to determine
the generality of our system across text types, we
did not perform any full text-specific optimization.
Our results on article bodies are notable: our system
had stable performance across text types (in fact, we
had a very slight F-score improvement on full-text
articles: 50.40 vs. 50.28). This contrasts with the
drop of a few points that seems to occur with other
well-performing systems. Taking only full-text arti-
cles into consideration, we would be ranked 4th in
the GENIA track. Furthermore, a preliminary error
analysis with full-text articles seems to indicate that
parsing-related errors are more prevalent in the full-
text article set than in the abstract set, consistent with
Cohen et al?s (2010) findings. At the same time, our
results confirm that we were able to abstract away
from this complexity to some degree with our ap-
proach.
We have a particular interest in speculation and
negation detection. Therefore, we examined our re-
sults on the GENIA development set with respect to
Task 3 more closely. Consistent with our previous
shared task results, we determined that the majority
of errors were due to misidentified or missed base
events (70% of the precision errors and 83% of the
recall errors)6. Task 3-specific precision errors in-
cluded cases in which speculation or negation was
debatable, as the examples below show. In Exam-
ple (7a), our system detected a Speculation instance,
due to the verbal predicate suggesting, which scopes
over the event indicated by role. In Example (7b),
our system detected a Negation instance, due to the
nominal predicate lack, which scopes over the events
indicated by expression. Neither were annotated as
6Even a bigger percentage of speculation/negation-related
errors in the EPI and ID tracks were due to the same problem,
as the overall accuracy in those tracks is lower.
such in the shared task corpus.
(7) (a) . . . suggesting a role of these 3? elements
in beta-globin gene expression.
(b) . . . DT40 B cell lines that lack expression
of either PKD1 or PKD3 . . .
Another class of precision errors was due to argu-
ment propagation up the embedding graph. It seems
the current algorithm may be too permissive in some
cases and a more refined approach to argument prop-
agation may be necessary. In the following example,
while suggest, an epistemic trigger, does not embed
induction directly (as shown in (8b)), the intermedi-
ate nodes simply propagate the proposition associ-
ated with the induction node up the graph, leading
us to conclude that the proposition triggered by in-
duction is speculated, leading to a precision error.
(8) (a) . . . these findings suggest that PWM is able
to initiate an intracytoplasmic signaling
cascade and EGR-1 induction . . .
(b) suggest ? able ? initiate ? induction
Among the recall errors, some of them were due
to shortcomings of the composition algorithm, as it
is currently implemented. One recall problem in-
volved the embedding status of and rules concern-
ing copular constructions, which we had not yet ad-
dressed. Therefore, we miss the relatively straight-
forward Speculation instances in the following ex-
amples.
(9) (a) . . . the A3G promoter appears constitu-
tively active.
(b) . . . the precise factors that mediate this in-
duction mechanism remain unknown.
Similarly, the lack of a trigger expression in our dic-
tionary may cause recall errors. The example below
shows an instance where this occurs, in addition to
lack of an appropriate argument identification rule:
(10) mRNA was quantified by real-time PCR for
FOXP3 and GATA3 expression.
Our system also missed an interesting, domain-
specific type of negation, in which the minus sign
indicates negation of the event that the entity partic-
ipates in.
(11) . . . CD14- surface Ag expression . . .
180
5 Conclusions and Future Work
We explored a two-phase approach to event ex-
traction, distinguishing general linguistic principles
from task-specific aspects, in accordance with the
generalization theme of the shared task. Our results
demonstrate the viability of this approach on both
abstracts and article bodies, while also pinpointing
some of its shortcomings. For example, our error
analysis shows that some aspects of semantic com-
position algorithm (argument propagation, in partic-
ular) requires more refinement. Furthermore, using
the same trigger expression dictionary for all tracks
seems to have negative effect on the overall perfor-
mance. The incremental nature of our system de-
velopment ensures that some of these shortcomings
will be addressed in future work.
We participated in three supporting tasks, two
of which (Co-reference (CO) and Entity Relations
(REL) tasks (Nguyen et al (2011) and Pyysalo et
al. (2011b), respectively) were relevant to the main
portion of the shared task; however, due to time con-
straints, we were not able to fully incorporate these
modules into our general framework, with the ex-
ception of the co-reference resolution of relative pro-
nouns. Since our goal is to move towards discourse
interpretation, we plan to incorporate these modules
(inter-sentential co-reference resolution, in particu-
lar) into our framework. After applying the lessons
we learned in the shared task and fully incorporating
these modules, we plan to make our system available
to the scientific community.
References
Robert Bossy, Julien Jourde, Philippe Bessie`res, Marteen
van de Guchte, and Claire Ne?dellec. 2011. BioNLP
Shared Task 2011 - Bacteria Biotope. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Meeting of the Associ-
ation for Computational Linguistics, pages 173?180.
K Bretonnel Cohen, Helen L Johnson, Karin Verspoor,
Christophe Roeder, and Lawrence E Hunter. 2010.
The structural and content aspects of abstracts versus
bodies of full text journal articles are different. BMC
Bioinformatics, 11:492.
Laurence Danlos. 2006. ?Discourse verbs? and dis-
course periphrastic links. In C Sidner, J Harpur,
A Benz, and P Ku?hnlein, editors, Second Workshop
on Constraints in Discourse (CID06), pages 59?65.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation, pages 449?454.
Julien Jourde, Alain-Pierre Manine, Philippe Veber,
Kare?n Fort, Robert Bossy, Erick Alphonse, and
Philippe Bessie`res. 2011. BioNLP Shared Task 2011
- Bacteria Gene Interactions and Renaming. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Halil Kilicoglu and Sabine Bergler. 2008. Recognizing
speculative language in biomedical research articles:
a linguistically motivated perspective. BMC Bioinfor-
matics, 9 Suppl 11:s10.
Halil Kilicoglu and Sabine Bergler. 2009. Syntactic de-
pendency based heuristics for biological event extrac-
tion. In Proceedings of Natural Language Process-
ing in Biomedicine (BioNLP) NAACL 2009 Workshop,
pages 119?127.
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9:10.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun?ichi Tsujii. 2009. Overview
of BioNLP?09 Shared Task on Event Extraction.
In Proceedings of Natural Language Processing in
Biomedicine (BioNLP) NAACL 2009 Workshop, pages
1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011a. Overview
of BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-
nori Yonezawa. 2011b. Overview of the Genia Event
task in BioNLP Shared Task 2011. In Proceedings
of the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
David McClosky and Eugene Charniak. 2008. Self-
training for biomedical parsing. In Proceedings of
the 46th Meeting of the Association for Computational
Linguistics, pages 101?104.
181
Ngan Nguyen, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
Overview of the Protein Coreference task in BioNLP
Shared Task 2011. In Proceedings of the BioNLP 2011
Workshop Companion Volume for Shared Task, Port-
land, Oregon, June. Association for Computational
Linguistics.
Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011.
Overview of the Epigenetics and Post-translational
Modifications (EPI) task of BioNLP Shared Task
2011. In Proceedings of the BioNLP 2011 Workshop
Companion Volume for Shared Task, Portland, Oregon,
June. Association for Computational Linguistics.
Frank R Palmer. 1986. Mood and modality. Cambridge
University Press, Cambridge, UK.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011a.
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, and Jun?ichi Tsujii.
2011b. Overview of the Entity Relations (REL) sup-
porting task of BioNLP Shared Task 2011. In Pro-
ceedings of the BioNLP 2011 Workshop Companion
Volume for Shared Task, Portland, Oregon, June. As-
sociation for Computational Linguistics.
Burr Settles. 2005. ABNER: An open source tool for
automatically tagging genes, proteins, and other entity
names in text. Bioinformatics, 21(14):3191?3192.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the Hu-
man Language Technologies Conference/Conference
on Empirical Methods in Natural Language Process-
ing (HLT/EMNLP-2005), pages 347?354.
182
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 54?62,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Interpreting Consumer Health Questions: The Role of Anaphora and 
Ellipsis 
 
 
Halil Kilicoglu, Marcelo Fiszman, Dina Demner-Fushman 
Lister Hill National Center for Biomedical Communications  
National Library of Medicine  
Bethesda, MD, USA 
{kilicogluh, fiszmanm, ddemner}@mail.nih.gov 
 
  
 
Abstract 
While interest in biomedical question answer-
ing has been growing, research in consumer 
health question answering remains relatively 
sparse. In this paper, we focus on the task of 
consumer health question understanding. We 
present a rule-based methodology that relies 
on lexical and syntactic information as well as 
anaphora/ellipsis resolution to construct struc-
tured representations of questions (frames). 
Our results indicate the viability of our ap-
proach and demonstrate the important role 
played by anaphora and ellipsis in interpreting 
consumer health questions. 
1 Introduction 
Question understanding is a major challenge in 
automatic question answering. An array of ap-
proaches has been developed for this task in the 
course of TREC Question Answering evaluations 
(see Prager (2006) for an overview). These col-
lectively developed approaches to question un-
derstanding were successfully applied and ex-
panded upon in IBM?s Watson system (Lally et 
al., 2012). Currently, Watson is being retargeted 
towards biomedical question answering, joining 
the ongoing research in domain-specific question 
answering (for a review, see Simpson and 
Demner-Fushman, 2012). 
Much research in automatic question answer-
ing has focused on answering well-formed fac-
toid questions. However, real-life questions that 
need to be handled by such systems are often 
posed by lay people and are not necessarily well-
formed or explicit. This is particularly evident in 
questions involving health issues. Zhang (2010), 
focusing on health-related questions submitted to 
Yahoo Answers, found that these questions pri-
marily described diseases and symptoms (ac-
companied by some demographic information), 
were fairly long, dense (incorporating more than 
one question), and contained many abbreviations 
and misspellings. For example, consider the fol-
lowing question posed by a consumer: 
(1) my question is this: I was born w/a esopha-
gus atresia w/dextrocardia. While the heart 
hasn't caused problems,the other has. I get 
food caught all the time. My question is...is 
there anything that can fix it cause I can't eat 
anything lately without getting it caught. I 
need help or will starve! 
It is clear that the person asking this question 
is mainly interested in learning about treatment 
options for his/her disease, in particular with re-
spect to his/her esophagus. Most of the textual 
content is not particularly relevant in understand-
ing the question (I need help or will starve! or I 
get food caught all the time). In addition, note 
the presence of anaphora (it referring to esopha-
gus atresia) and ellipsis (the other has [caused prob-
lems]), which should be resolved in order to auto-
matically interpret the question. Finally, note the 
informal fix instead of the more formal treat, and 
cause instead of because.  
The National Library of Medicine? (NLM?) 
receives questions from consumers on a variety 
of health-related topics. These questions are cur-
rently manually answered by customer support 
services. The overall goal of our work is to assist 
the customer support services by automatically 
interpreting these questions, using information 
retrieval techniques to find relevant documents 
and passages, and presenting the information in 
concise form for their assessment. 
In this paper, we specifically focus on ques-
tion understanding, rather than information re-
54
trieval aspects of our ongoing work. Our goal in 
question understanding is to capture the core as-
pects of the question in a structured representa-
tion (question frame), which can then be used to 
form a query for the search engine. In the current 
work, we primarily investigate and evaluate the 
role of anaphora and ellipsis resolution in under-
standing the questions. Our results confirm the 
viability of rule-based question understanding 
based on exploiting lexico-syntactic patterns and 
clearly demonstrate that anaphora and ellipsis 
resolution are beneficial for this task.  
2 Background 
Despite the growing interest to biomedical ques-
tion answering (Cairns et al, 2012; Ni et al, 
2012; Bauer and Berleant, 2012), consumer 
health question answering remains a fairly un-
derstudied area of research. The initial research 
has focused on the analysis of consumer lan-
guage (McCray et al, 1999) and the types of 
questions they asked. Spink et al (2004) found 
that health-related queries submitted to three web 
search engines in 2001 were often advice seeking 
and personalized, and fell into five major catego-
ries: general health, weight issues, reproductive 
health and puberty, pregnancy/obstetrics, and 
human relationships. Observing that health que-
ries constituted no more than 9.5% of all queries 
and declined over time, they concluded that the 
users turn more to the specialized resources for 
the answers to health-related questions. Similar 
to the findings of Zhang (2010), Beloborodov et 
al. (2013) found that diseases and symptoms 
were the most popular topics in a resource simi-
lar to Yahoo Answers, Otvety@Mail.Ru. They 
analyzed Otvety@Mail.Ru questions by mapping 
questions to body parts and organs, applying La-
tent Dirichlet Allocation method with Gibbs 
sampling to discover topics, and using a 
knowledge-based method to classify questions as 
evidence-directed or hypothesis-directed. 
First efforts in automated consumer health 
question processing were to classify the ques-
tions using machine learning techniques. In one 
study, frequently asked questions about diabetes 
were classified according to two somewhat or-
thogonal taxonomies: according to the ?medical 
type of the question? (Causes, Diagnostic, Pre-
vention, Symptoms, Treatment, etc.) and accord-
ing to the ?expected answer type? (Boolean, 
Causal, Definition, Factoid, Person, Place, etc.) 
(Cruchet et al, 2008). Support Vector Machine 
(SVM) classification achieved an F-score in low 
80s in classifying English questions to the ex-
pected answer type. The results for French and 
medical type classification in both languages 
were much lower. Liu et al (2011) found that 
SVM trained to distinguish questions asked by 
consumers from those posed by healthcare pro-
fessionals achieve F-scores in the high 80s - low 
90s. One of distinguishing characteristics of the 
consumer questions in Liu et al?s study was the 
significantly higher use of personal pronouns 
(compared to professional questions). This fea-
ture was found to be useful for machine learning; 
however, the abundance of pronouns in the long 
dense questions is also a potential source of fail-
ure in understanding the question.  
Vicedo and Ferr?ndez (2000) have shown that 
pronominal anaphora resolution improves several 
aspects of the QA systems? performance. This 
observation was supported by Harabagiu et al 
(2005) who have manually resolved coreference 
and ellipsis for 14 of the 25 scenarios in the 
TREC 2005 evaluation. Hickl et al (2006) have 
incorporated into their question answering sys-
tem a heuristic based question coreference mod-
ule that resolved referring expressions in the 
question series to antecedents mentioned in pre-
vious questions or in the target description. To 
our knowledge, coreference and ellipsis resolu-
tion has not been previously attempted in con-
sumer health question understanding. 
Another essential aspect in processing con-
sumer questions is defining a formal representa-
tion capable of capturing all important points 
needed for further processing in automatic query 
generation (in the systems that use document 
passage retrieval to find a set of potential an-
swers) and answer extraction and unification. 
Ontologies provide effective representation 
mechanisms for concepts, whereas relations are 
better captured in frame-like or event-related 
structures (Hunter and Cohen, 2006). Frame-
based representation of extracted knowledge has 
a long-standing tradition in the biomedical do-
main, for example, in MedLEE (Friedman et al, 
1994). Demner-Fushman et al (2011) showed 
that frame-based representation of clinical ques-
tions improve identification of patients eligible 
for cohort inclusion. Demner-Fushman and Ab-
hyankar (2012) extracted frames in four steps: 1) 
identification of domain concepts, 2) extraction 
of patient demographics (e.g., age, gender) and 
social history, 3) establishing dependencies be-
tween the concepts using the Stanford dependen-
cy parser (de Marneffe et al, 2006), and 4) add-
ing concepts not involved in the relations to the 
55
frame as a list of keywords.  Event-based repre-
sentations have also seen increasing use in recent 
years in biomedical text mining, with the availa-
bility of biological event corpora, including 
GENIA event (Kim et al, 2008) and GREC 
(Thompson et al, 2009), and shared task chal-
lenges (Kim et al, 2012). Most state-of-the-art 
systems address the event extraction task by 
adopting machine learning techniques, such as 
dual composition-based models (Riedel and 
McCallum, 2011), stacking-based model integra-
tion (McClosky et al, 2012), and domain adapta-
tion (Miwa et al, 2012). Good performance has 
also been reported with some rule-based systems 
(Kilicoglu and Bergler, 2012). Syntactic depend-
ency parsing has been a key component in all 
state-of-the-art event extraction systems, as well. 
The role of coreference resolution in event ex-
traction has recently been acknowledged (Kim et 
al., 2012), even though efforts in integrating co-
reference resolution into event extraction pipe-
lines have generally resulted in only modest im-
provements (Yoshikawa et al, 2011; Miwa et 
al., 2012; Kilicoglu and Bergler, 2012). 
Coreference resolution has also been tackled 
in open domain natural language processing. 
State-of-the-art systems often employ a combina-
tion of lexical, syntactic, shallow semantic and 
discourse information (e.g., speaker identifica-
tion) with deterministic rules (Lee et al, 2011). 
Interestingly, coreference resolution is one re-
search area, in which deterministic frameworks 
generally outperform machine learning models 
(Haghighi and Klein, 2009; Lee et al, 2011).  
In contrast to coreference resolution, ellipsis 
resolution remains an understudied NLP prob-
lem. One type of ellipsis that received some at-
tention is null instantiation (Fillmore and Baker, 
2001), whereby the goal is to recover the refer-
ents for an uninstantiated semantic role of a tar-
get predicate from the wider discourse context. A 
semantic evaluation challenge that focused on 
null instantiation was proposed, although partici-
pation was limited (Ruppenhofer et al, 2010). 
Gerber and Chai (2012) focused on implicit ar-
gumentation (i.e., null instantiation) for nominal 
predicates. They annotated a corpus of implicit 
arguments for a small number of nominal predi-
cates and trained a discriminative model based 
on syntactic, semantic and discourse features 
collected from various linguistic resources. Fo-
cusing on a different type of ellipsis, Bos and 
Spenader (2011) annotated a corpus of verb 
phrase ellipsis; however, so far there have been 
little work in verb phrase ellipsis resolution. We 
are also not aware of any work in ellipsis resolu-
tion in biomedical NLP.  
3 Methods   
We use a pipeline model for question analysis, 
which results in frame annotations that capture 
the content of the question. Our rule-based meth-
od begins with identifying terms (named enti-
ties/triggers) in question text. Next, we recognize 
anaphoric mentions and, if any, perform anapho-
ra resolution. The next step is to link frame trig-
gers with their theme and question cue by ex-
ploiting syntactic dependency relations. Finally, 
if frames with implicit arguments exist (that is, 
frames in which theme or question cue was not 
instantiated), we attempt to recover these argu-
ments by ellipsis resolution. In this section, we 
first describe our data selection. Then, we ex-
plain the steps in our pipeline, with particular 
emphasis on anaphora and ellipsis. The pipeline 
diagram is illustrated in Figure 1.  
 
 
Figure 1. The system pipeline diagram 
3.1 Data Selection and Annotation 
In this study, we focused on questions about ge-
netic diseases, due to their increasing prevalence. 
Since the majority of the consumers? questions 
submitted to NLM are about treatment and prog-
nosis, we selected mainly these types of ques-
tions for our training set. Note that while these 
questions mostly focused on treatment and prog-
nosis, some of them also include other types of 
questions, asking for general information or 
about diagnosis, etiology, and susceptibility 
(thus, confirming the finding of Zhang (2010)). 
The majority of selected questions were asked by 
real consumers in 2012. Due to our interest in 
genetics questions, we augmented this set with 
56
some frequently asked questions from the Genet-
ic and Rare Disease Information Center 
(GARD) 1 . Our selection yielded 32 treatment 
and 22 prognosis questions. An example treat-
ment question was provided earlier (1). The fol-
lowing is a training question on prognosis: 
(2) They have diagnosed my niece with Salla 
disease. I understand that this is a very rare 
disease and that its main origin is Finland. 
Can you please let me know what to expect? 
My niece is 7 years old. It has taken them 6 
years to finally come up with this diagnosis. 
We used training questions to gain linguistic 
insights into the problem, to develop and refine 
our methodology, and as the basis of a trig-
ger/question cue dictionary. 
After the system was developed, we selected 
29 previously unseen treatment-focused ques-
tions posed to GARD for testing. We annotated 
them with target frames (41 instances) using brat 
annotation tool (Stenetorp et al, 2012) and eval-
uated our system results against these frames. 29 
of the target frames were treatment frames. Addi-
tionally, there were 1 etiology, 6 general infor-
mation, 2 diagnosis, and 3 prognosis frames. 
3.2 Syntactic Dependency Parsing 
Our question analysis module uses typed de-
pendency relations as the basis of syntactic in-
formation. We extract syntactic dependencies 
using Stanford Parser (de Marneffe et al, 2006) 
and use its collapsed dependency format. We 
rely on Stanford Parser for tokenization, lemma-
tization, and part-of-speech tagging, as well. 
3.3 Named Entity/Trigger Detection 
We use simple dictionary lookup to map entity 
mentions in text to UMLS Metathesaurus con-
cepts (Lindberg, 1993). So far, we have focused 
on recognizing three mention categories: prob-
lems, interventions, and patients. Based on 
UMLS 2007AC release, we constructed a dic-
tionary of string/concept pairs. We limited the 
dictionary to concepts with predefined semantic 
types. For example, all problems in the diction-
ary have a semantic type that belongs to the Dis-
orders semantic group (McCray et al, 2001), 
such as Neoplastic Process and Congenital Ab-
normality. Currently our dictionary contains ap-
proximately 260K string/concept pairs. 
Dictionary lookup is also used to detect trig-
gers and question cues. We constructed a trigger 
                                                 
1 https://rarediseases.info.nih.gov/GARD/ 
and question cue dictionary based on training 
data and limited expansion. The dictionary cur-
rently contains 117 triggers and 14 question cues.  
3.4 Recognizing Anaphoric Mentions 
We focus on identifying two types of anaphoric 
phenomena: pronominal anaphora (including 
anaphora of personal and demonstrative pro-
nouns) and sortal anaphora. The following ex-
amples from the training questions illustrate 
these types. Anaphoric mentions are underlined 
and their antecedents are in bold. 
? Personal pronominal anaphora: My daughter 
has just been diagnosed with Meier-Gorlin 
syndrome. I would like to learn more about 
it ? 
? Demonstrative pronominal anaphora: We just 
found out that our grandson has 48,XXYY 
syndrome. ?  I was wondering if you could 
give us some information on what to expect 
and the prognosis for this and ..  
? Sortal anaphora: I have a 24-month-old niece 
who has the following symptoms of Cohen 
syndrome: ? I would like seek your help in 
learning more about this condition. 
To recognize mentions of personal pronominal 
and sortal anaphora, we mainly adapted the rule-
based techniques outlined in Kilicoglu and Ber-
gler (2012), itself based on the deterministic co-
reference resolution approach described in 
Haghighi and Klein (2009). While Kilicoglu and 
Bergler (2012) focused on anaphora involving 
gene/protein terms, our adaptation focuses on 
those involving problems and patients. In addi-
tion, we expanded their work by developing rules 
to recognize demonstrative pronominal anapho-
ra.  
3.4.1 Personal Pronouns 
Kilicoglu and Bergler (2012) focused on only 
resolving it and they, since, in scientific article 
genre, resolving other third person pronouns (he, 
she) was less relevant. We currently recognize 
these two pronouns, as well. For personal pro-
nouns, we merely tag the word as a pronominal 
anaphor if it is tagged as a pronoun and is in 
third person (i.e., she, he, it, they).  
3.4.2 Demonstrative Pronouns 
We rely on typed syntactic dependencies as well 
as part-of-speech tags to recognize demonstrative 
pronominal anaphora. A word is tagged as 
demonstrative pronominal anaphor if it is one of 
this, that, those, or these and if it is not the de-
57
pendent in a det (determiner) dependency (in 
other words, it is not a pronominal modifier). 
Furthermore, we ensure that the pronoun that 
does not act as a complementizer, requiring that 
it not be the dependent in a complm (complemen-
tizer) dependency. 
3.4.3 Sortal Anaphora 
In the current work, we limited sortal anaphora 
to problem terms. As in Kilicoglu and Bergler 
(2012), we require that the anaphoric noun 
phrases not include any named entity terms. 
Thus, we allow the syndrome as an anaphoric 
mention, while blocking the Stickler syndrome.  
To recognize sortal anaphora, we look for the 
presence of det dependency, where the depend-
ent is one of this, that, these, those, or the.  
Once the named entities, question cues, trig-
gers, and anaphoric mentions are identified in a 
sentence, we collapse the syntactic dependencies 
from the sentence to simplify further processing. 
This is illustrated in Table 1 for the sentence in 
(3). 
(3) My partner is a carrier for Simpson-Golabi-
Behmel syndrome and her son was diag-
nosed with this rare condition.  
 
Dependencies before Dependencies after 
amod (syndrome, simpson-golabi-behmel) 
prep_for(carrier, simpson-golabi-behmel syndrome) 
prep_for(carrier,syndrome) 
det(condition,this) 
prep_with (diagnosed, this rare condition) amod(condition, rare) 
prep_with(diagnosed, condition) 
Table 1: Syntactic dependency transformations 
3.5 Anaphora  Resolution 
Anaphora resolution is the task of finding the 
antecedent for an anaphoric mention in prior dis-
course. Our anaphora resolution method is again 
based on the work of Kilicoglu and Bergler 
(2012). However, we made simplifying assump-
tions based on our examination of the training 
questions. First observation is that each question 
is mainly about one salient topic (problem) and 
anaphoric mentions are highly likely to refer to 
this topic. Secondly, the salient topic often ap-
pears as the first named entity in the question.  
Based on these observations, we did not attempt 
to use the relatively complex, semantic graph-
based resolution strategies (e.g., graph distance) 
outlined in that work. Furthermore, we have not 
attempted to address set-instance anaphora or 
event anaphora in this work, since we did not see 
examples of these in the training data. 
Anaphora resolution begins with identifying 
the candidate antecedents (problems, patients) in 
prior discourse, which are then evaluated for syn-
tactic and semantic compatibility. For pronomi-
nal anaphora, compatibility involves person and 
number agreement between the anaphoric men-
tion and the antecedent. For sortal anaphora, 
number agreement as well as satisfying one of 
the following constraints is required: 
? Head word constraint: The head of the ana-
phoric NP and the antecedent NP match. 
This constraint allows Wolf-Hirschhorn Syn-
drome as an antecedent for this syndrome, 
matching on the word syndrome. 
? Hypernymy constraint: The head of the ana-
phoric NP is a problem hypernym and the 
antecedent is a problem term. Similar to 
gene/protein hypernym list in Kilicoglu and 
Bergler (2012), we used a small list of prob-
lem hypernym words, including disease, dis-
order, illness, syndrome, condition, and 
problem. This constraint allows Simpson-
Golabi-Behmel syndrome as an antecedent 
for this rare condition in example (3). 
We expanded number agreement test to in-
clude singular mass nouns, so that plural anapho-
ra (e.g., they) can refer to mass nouns such as 
family, group, population. In addition, we de-
fined lists of gendered nouns (e.g., son, father, 
nephew, etc. for male and wife, daughter, niece, 
etc. for female) and required gender agreement 
for pronominal anaphora. 
After the candidate antecedents are identified, 
we assign them salience scores based on the or-
der in which they appear in the question and their 
frequency in the question. The terms that appear 
earlier in the question and occur more frequently 
receive higher scores. The most salient anteced-
ent is then taken to be the coreferent. 
3.6 Frame Construction 
We adapted the frame extraction process based 
on lexico-syntactic information outlined in 
Demner-Fushman et al (2012) and somewhat 
58
modified the frames to accommodate consumer 
health questions. For each question posed, we 
aim to construct a frame which consists of the 
following elements: type, theme, and question 
cue: theme refers to the topic of the question 
(problem name, etc.), while type refers to the 
aspect of the theme that the question is about 
(treatment, prognosis, etc.) and question cue to 
the question words (what, how, are there, etc.). 
Theme element is semantically typed and is re-
stricted to the UMLS semantic group Disorders. 
From the question in (1), the following frame 
should be extracted: 
 
Treatment fix 
      Theme Esophageal atresia  
(Disease or Syndrome) 
      QCue Is there 
Table 2: Frame example 
 
We rely on syntactic dependencies to link frame 
indicators to their themes and question cues. We 
currently search for the following types of syn-
tactic dependencies between the indicator men-
tion and the argument mentions: dobj (direct ob-
ject), nsubjpass (passive nominal subject), nn 
(noun compound modifier), rcmod (relative 
clause modifier), xcomp (open clausal comple-
ment), acomp (adjectival complement), prep_of, 
prep_to, prep_for, prep_on, prep_from, 
prep_with, prep_regarding, prep_about (prepo-
sitional modifier cued by of, to, for, on, from, 
with, regarding, about, respectively). Two spe-
cial rules address the following cases: 
? If the dependency exists between a trigger of 
type T and another of type General Infor-
mation, the General Information trigger be-
comes a question cue for the frame type T. 
This handles cases such as ?Is there infor-
mation regarding prognosis..? where there is 
a prep_regarding dependency between the 
General Information trigger ?information? 
and the Prognosis trigger ?prognosis?. This 
results in ?information? becoming the ques-
tion cue for the Prognosis frame. 
? If a dependency exists between a trigger T 
and a patient term P and another between the 
patient term P and a potential theme argu-
ment A, the potential theme argument A is 
assigned as the theme of the frame indicated 
by T. This handles cases such as ?What is the 
life expectancy for a child with Dravet syn-
drome?? whereby Dravet syndrome is as-
signed the Theme role for the Prognosis 
frame indicated by life expectancy. 
3.6.1 Ellipsis Resolution 
The frame construction step may result in frames 
with uninstantiated themes or question cues. If a 
constructed frame includes a question cue but no 
theme, we attempt to recover the theme argument 
from prior discourse by ellipsis processing. Con-
sider the question in (4) and the frame in Table 3 
extracted from it in previous steps: 
(4) They have diagnosed my niece with Salla 
disease. ?Can you please let me know what 
to expect? ? 
Prognosis expect 
      Theme - 
      QCue what 
Table 3: Frame with uninstantiated Theme role 
 
In the context of consumer health questions, 
the main difficulty with resolving such cases is 
recognizing whether it is indeed a legitimate case 
of ellipsis. We use the following dependency-
based heuristics to determine the presence of el-
lipsis: 
? Check for the presence of a syntactic de-
pendency of one of the types listed in Sec-
tion 3.5, in which the frame trigger appears 
as an element. If such a dependency does not 
exist, consider it a case of ellipsis.  
? Otherwise, consider the other element of the 
dependency: 
o If the other element does not corre-
spond to a term, we cannot make a 
decision regarding ellipsis, since we 
do not know the semantics of this 
other element. 
o If it corresponds to an element that 
has already been used in creating the 
frame, the dependency is accounted 
for.  
? If all the dependencies involving the frame 
trigger are accounted for, consider it a case 
of ellipsis. 
In example (4), the trigger expect is found to 
be in an xcomp dependency with the question cue 
know, which has already been used in the frame. 
Therefore this dependency is accounted for, and 
we consider this a case of ellipsis.  On the other 
hand, consider the example:  
(5) My child has been diagnosed with pachgyria. 
What can I expect for my child?s future? 
As in the previous example, the Theme role of 
the Prognosis frame indicated by expect is unin-
stantiated. However, it is not considered an ellip-
59
tical case, since there is a prep_for dependency 
between expect and future, a word that is seman-
tically unresolved. 
Once the presence of ellipsis is ensured, we 
fill the Theme role of the frame with the most 
salient term in the question text, as in anaphora 
resolution. 
In rare cases, the frame may include a theme 
but not a question cue.  This may be due to a lack 
of explicit question expression (such as in the 
question ?treatment for Von Hippel-Lindau syn-
drome.?) or due to shortcomings in dependency-
based linking of frame triggers to question cues. 
If no fully instantiated frame was extracted from 
the question, as a last resort, we construct a 
frame without the question cue in an effort to 
increase recall.  
4 Results and Discussion 
We extracted frames from the test questions and 
compared the results with the annotated target 
frames. As evaluation metrics, we calculated 
precision, recall, and F-score. To assess the ef-
fect of various components of the system, we 
evaluated several scenarios: 
? Frame extraction without anaphora/ellipsis 
resolution (indicated as A in Table 4 below) 
? Frame extraction with anaphora/ellipsis reso-
lution (B) 
? Frame extraction without anaphora/ellipsis 
resolution but with gold triggers/named enti-
ties (C) 
? Frame extraction with anaphora/ellipsis reso-
lution and gold triggers/named entities (D) 
The evaluation results are provided in Table 4. In 
the second column, the numbers in parentheses 
correspond to the numbers of correctly identified 
frames. 
 
 # of frames Recall Precision F-score 
A 14 (13) 0.32 0.93 0.48 
B 26 (22) 0.54 0.85 0.66 
C 17 (16) 0.39 0.84 0.55 
D 35 (33) 0.80 0.94 0.86 
Table 4: Evaluation results 
 
The evaluation results show that the depend-
ency-based frame extraction method with dic-
tionary lookup is generally effective; it is precise 
in identifying frames, even though it misses 
many relevant frames, typical of most rule-based 
systems. On the other hand, anaphora/ellipsis 
resolution helps a great deal in recovering the 
relevant frames and only has a minor negative 
effect on precision of the frames, the overall ef-
fect being significantly positive. Note also that 
the increase in recall without gold triggers/named 
entities is about 40%, while that with gold trig-
gers/named entities is more than double, indicat-
ing that accurate term recognition contributes to 
better anaphora/ellipsis resolution and, in turn, to 
better question understanding. 
The dictionary-based named entity/trigger/ 
question cue detection is relatively simple, and 
while it yields good precision, the lack of terms 
in the corresponding dictionary causes recall er-
rors. An example is given in (6). The named enti-
ty Reed syndrome was not recognized due to its 
absence in the dictionary, causing two false 
negative errors. 
(6) A friend of mine was just told she has Reed 
syndrome? I was wondering if you could let 
me know where I can find more information 
on this topic. I am wondering what treat-
ments there are for this, ? 
Similarly, dependency-based frame construc-
tion is straightforward in that it mostly requires 
direct dependency relations between the trigger 
and the arguments.  While the two additional 
rules we implemented redress the shortcomings 
of this straightforward approach, there are cases 
in which dependency-based mechanism is still 
lacking. An example is given in (7). The lack of 
a direct dependency between treatments and this 
condition causes a recall error. A more sophisti-
cated mechanism based on dependency chains 
could recover such frames; however, such chains 
would also increase the likelihood of precision 
errors.  
(7) Are people with Lebers hereditary optic neu-
ropathy partially blind for a long period of 
time ?. ?Are there any surgical treatments 
available to alter this condition or is it per-
manent for life? 
Anaphora/ellipsis processing clearly benefited 
our question understanding system. However, we 
noted several errors due to shortcomings in this 
processing. For example, from the sentence in 
(8), the system constructed a General Infor-
mation frame with the trigger wonder and the 
Theme argument central core disease, which 
caused a false positive error.  
(8) After 34 years of living with central core dis-
ease, ?. My lower back doesn't seem to 
work, and I wonder if I will ever be able to 
walk up stairs or run.  
60
The system recognized that the trigger wonder 
had an uninstantiated theme argument, which it 
attempted to recover by ellipsis processing. 
However, this processing misidentified the case 
as legitimate ellipsis due to the dependency rela-
tions wonder is involved in. A more sophisticat-
ed approach would take into account specific 
selectional restrictions of predicates like wonder; 
however, the overall utility of such linguistic 
knowledge in the context of consumer health 
questions, which are often ungrammatical and 
not particularly well-written, remains uncertain. 
Our anaphora resolution method was unable to 
resolve some cases of anaphora. For example, 
consider the question in (6). The anaphoric men-
tion this topic corefers with Reed syndrome. 
However, we miss this anaphora since we did not 
consider topic as a problem hypernym in scenar-
io D, in which gold named entities are used. 
5 Conclusions and Future Work 
We presented a rule-based approach to consumer 
health question understanding which relies on 
lexico-syntactic information and anapho-
ra/ellipsis resolution. We showed that lexico-
syntactic information provides a good baseline in 
understanding such questions and that resolving 
anaphora and ellipsis has a significant impact on 
this task. 
With regard to question understanding, future 
work includes generalization of the system to 
questions on topics other than genetic disorders 
(e.g., drugs) and aspects (such as complications, 
prevention, ingredients, location information, 
etc.) and broader evaluation. We also plan to au-
tomate dictionary development to some extent 
and address misspellings and acronyms in ques-
tions. We have been extending our frames to in-
clude ancillary keywords (named entities ex-
tracted from the question) that are expected to 
assist the search engine in pinpointing the rele-
vant answer passages, similar to Demner-
Fushman and Abhyankar (2012). We will also 
continue to develop our anaphora/ellipsis pro-
cessing module, addressing the issues revealed 
by our evaluation as well as other anaphoric phe-
nomena, such as recognition of pleonastic it. 
Acknowledgments 
This research was supported by the Intramural 
Research Program of the NIH, National Library 
of Medicine. 
References  
Michael A. Bauer and Daniel Berleant. 2012. Usabil-
ity survey of biomedical question answering sys-
tems. Human Genomics, 6(1):17. 
Alexander Beloborodov, Artem Kuznetsov, Pavel 
Braslavski. 2013. Characterizing Health-Related 
Community Question Answering. In Advances in 
Information Retrieval, 680-683. 
Brian L. Cairns, Rodney D. Nielsen, James J. Masanz, 
James H. Martin, Martha S. Palmer, Wayne H. 
Ward, Guergana K. Savova. 2011. The MiPACQ 
clinical question answering system. In AMIA An-
nual Symposium Proceedings, pages 171-180. 
Sarah Cruchet, Arnaud Gaudinat, C?lia Boyer. 2008. 
Supervised approach to recognize question type in 
a QA system for health. Studies in Health Technol-
ogy and Informatics, 136:407-412. 
Marie-Catherine de Marneffe, Bill MacCartney, 
Christopher D. Manning. 2006. Generating typed 
dependency parses from phrase structure parses. In 
Proceedings of the 5th International Conference on 
Language Resources and Evaluation, pages 449-
454. 
Dina Demner-Fushman, Swapna Abhyankar, Antonio 
Jimeno-Yepes, Russell F. Loane, Bastien Rance, 
Fran?ois-Michel Lang, Nicholas C. Ide, Emilia 
Apostolova, Alan R. Aronson. 2011. A 
Knowledge-Based Approach to Medical Records 
Retrieval. In Proceedings of Text Retrieval Confer-
ence 2011.  
Dina Demner-Fushman and Swapna Abhyankar. 
2012. Syntactic-Semantic Frames for Clinical Co-
hort Identification Queries. Lecture Notes in Com-
puter Science, 7348:100-112. 
Charles J. Fillmore and Collin F. Baker. 2001. Frame 
semantics for text understanding. In Proceedings of 
the NAACL?01 Workshop on WordNet and Other 
Lexical Resources. 
Carol Friedman, Philip O. Alderson, John HM Austin, 
James J. Cimino, and Stephen B. Johnson. 1994. A 
general natural-language text processor for clinical 
radiology. Journal of the American Medical Infor-
matics Association, 1(2): 161-174. 
Matthew S. Gerber and Joyce Y. Chai. 2012. Seman-
tic Role Labeling of Implicit Arguments for Nomi-
nal Predicates. Computational Linguistics, 38(4): 
755-798. 
Aria Haghighi and Dan Klein. 2009. Simple corefer-
ence resolution with rich syntactic and semantic 
features. In Proceedings of EMNLP 2009, pages 
1152-1161. 
Sanda Harabagiu, Dan Moldovan, Christine Clark, 
Mitchell Bowden, Andrew Hickl, Patrick Wang. 
2005. Employing two question answering systems 
in TREC-2005. In Proceedings of Text Retrieval 
Conference  2005. 
61
Andrew Hickl, John Williams, Jeremy Bensley, Kirk 
Roberts, Ying Shi, Bryan Rink. 2006. Question 
Answering with LCC?s CHAUCER at TREC 2006. 
In Proceedings of Text Retrieval Conference  2006. 
Lawrence Hunter and Kevin B. Cohen. 2006. Bio-
medical language processing: what's beyond Pub-
Med? Molecular Cell. 21(5):589-94. 
Halil Kilicoglu and Sabine Bergler 2012. Biological 
Event Composition. BMC Bioinformatics, 13 
(Supplement 11):S7.  
Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii. 
2008. Corpus annotation for mining biomedical 
events from literature. BMC Bioinformatics, 9:10. 
Jin-Dong Kim, Ngan Nguyen, Yue Wang, Jun?ichi 
Tsujii, Toshihisa Takagi, Akinori Yonezawa. 2012. 
The Genia Event and Protein Coreference tasks of 
the BioNLP Shared Task 2011.  BMC Bioinformat-
ics, 13(Supplement 11):S1.  
Adam Lally, John M. Prager, Michael C. McCord, 
Branimir Boguraev, Siddharth Patwardhan, James 
Fan, Paul Fodor, Jennifer Chu-Carroll. 2012. Ques-
tion analysis: How Watson reads a clue. IBM Jour-
nal of Research and Development, 56(3):2. 
Heeyoung Lee, Yves Peirsman, Angel Chang, Na-
thanael Chambers, Mihai Surdeanu, Dan Jurafsky. 
2011. Stanford's Multi-Pass Sieve Coreference 
Resolution System at the CoNLL-2011 Shared 
Task. In Proceedings of the CoNLL-2011 Shared 
Task, pages 28-34. 
Donald A.B. Lindberg, Betsy L. Humphreys, Alexa T. 
McCray. 1993. The Unified Medical Language 
System. Methods of information in medicine, 
32(4): 281-291. 
Feifan Liu, Lamont D. Antieau, Hong Yu. 2011. To-
ward automated consumer question answering: au-
tomatically separating consumer questions from 
professional questions in the healthcare domain. 
Journal of Biomedical Informatics, 44(6): 1032-
1038. 
David McClosky, Sebastian Riedel, Mihai Surdeanu, 
Andrew McCallum, Christopher Manning. 2012. 
Combining joint models for biomedical event ex-
traction. BMC Bioinformatics, 13 (Supplement 11): 
S9. 
Alexa McCray, Russell Loane, Allen Browne, Anan-
tha Bangalore. 1999. Terminology issues in user 
access to Web-based medical information. In 
AMIA Annual Symposium Proceedings, pages 107?
111. 
Alexa McCray, Anita Burgun, Olivier Bodenreider. 
2001. Aggregating UMLS semantic types for re-
ducing conceptual complexity. In Proceedings of 
Medinfo, 10(Pt1): 216-220. 
Makoto Miwa, Paul Thompson, Sophia Ananiadou. 
2012. Boosting automatic event extraction from the 
literature using domain adaptation and coreference 
resolution. Bioinformatics, 28(13):1759-1765. 
Yuan Ni, Huijia Zhu, Peng Cai, Lei Zhang, Zhaoming 
Qui, Feng Cao. 2012. CliniQA: highly reliable 
clinical question answering system. Studies in 
Health Technology and Information, 180:215-219. 
John M. Prager. 2006. Open-domain question answer-
ing. Foundations and Trends in Information Re-
trieval, 1(2):91-231. 
Sebastian Riedel and Andrew McCallum. 2011. Fast 
and robust joint models for biomedical event ex-
traction. In Proceedings of EMNLP 2011, pages 1-
12. 
Josef Ruppenhofer, Caroline Sporleder, Roser Mo-
rante, Collin Baker, Martha Palmer. 2010. 
SemEval-2010 Task 10: Linking Events and Their 
Participants in Discourse. In Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
pages 45-50. 
Matthew S. Simpson and Dina Demner-Fushman. 
2012. Biomedical Text Mining: a Survey of Recent 
Progress. Mining Text Data 2012:465-517. 
Amanda Spink, Yin Yang, Jim Jansen, Pirrko 
Nykanen, Daniel P. Lorence, Seda Ozmutlu, H. 
Cenk Ozmutlu. 2004. A study of medical and 
health queries to web search engines. Health In-
formation & Libraries Journal. 21(1):44-51. 
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?, 
Tomoko Ohta, Sophia Ananiadou, Jun?ichi Tsujii. 
2012. Brat: a Web-based Tool for NLP-Assisted 
Text Annotation. In Proceedings of the Demon-
strations Sessions at EACL 2012, pages 102-107. 
Paul Thompson, Syed A. Iqbal, John McNaught, So-
phia Ananiadou. 2009. Construction of an annotat-
ed corpus to support biomedical information ex-
traction. BMC Bioinformatics, 10:349. 
Jos? L. Vicedo and Antonio Ferr?ndez. 2000. Im-
portance of pronominal anaphora resolution in 
question answering systems. In Proceedings of the 
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 555-562. 
Katsumasa Yoshikawa, Sebastian Riedel, Tsutomu 
Hirao, Masayuki Asahara, Yuji Matsumoto. 2011. 
Coreference Based Event-Argument Relation Ex-
traction on Biomedical Text. Journal of Biomedi-
cal Semantics, 2 (Supplement 5):S6. 
Yan Zhang. 2010. Contextualizing Consumer Health 
Information Searching: an Analysis of Questions in 
a Social Q&A Community. In Proceedings of the 
1st ACM International Health Informatics Sympo-
sium (IHI?10), pages 210-219. 
62
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 29?37,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Decomposing Consumer Health Questions
Kirk Roberts, Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-Fushman
National Library of Medicine
National Institutes of Health
Bethesda, MD 20894
robertske@nih.gov, {kilicogluh,fiszmanm,ddemner}@mail.nih.gov
Abstract
This paper presents a method for decom-
posing long, complex consumer health
questions. Our approach largely decom-
poses questions using their syntactic struc-
ture, recognizing independent questions
embedded in clauses, as well as coordi-
nations and exemplifying phrases. Addi-
tionally, we identify elements specific to
disease-related consumer health questions,
such as the focus disease and background
information. To achieve this, our approach
combines rank-and-filter machine learning
methods with rule-based methods. Our
results demonstrate significant improve-
ments over the heuristic methods typically
employed for question decomposition that
rely only on the syntactic parse tree.
1 Introduction
Natural language questions provide an intuitive
method for consumers (non-experts) to query for
health-related content. The most intuitive way
for consumers to formulate written questions is
the same way they write to other humans: multi-
sentence, complex questions that contain back-
ground information and often more than one spe-
cific question. Consider the following:
? Will Fabry disease affect a transplanted kidney?
Previous to the transplant the disease was be-
ing managed with an enzyme supplement. Will
this need to be continued? What cautions or ad-
ditional treatments are required to manage the
disease with a transplanted kidney?
This complex question contains three question
sentences and one background sentence. The fo-
cus (Fabry disease) is stated in the first question
but is necessary for a full understanding of the
other questions as well. The background sentence
is necessary to understand the second question:
the anaphor this must be resolved to an enzyme
treatment, and the predicate continue?s implicit ar-
gument that must be re-constructed from the dis-
course (i.e., continue after a kidney transplant).
The final question sentence uses a coordination
to ask two separate questions (cautions and addi-
tional treatments). A decomposition of this com-
plex question would then result in four questions:
1. Will Fabry disease affect a transplanted kidney?
2. Will enzyme treatment for Fabry disease need to
be continued after a kidney transplant?
3. What cautions are required to manage Fabry
disease with a transplanted kidney?
4. What additional treatments are required to man-
age Fabry disease with a transplanted kidney?
Each question above could be independently an-
swered by a question answering (QA) system.
While previous work has discussed methods for
resolving co-reference and implicit arguments in
consumer health questions (Kilicoglu et al., 2013),
it does not address question decomposition.
In this work, we propose methods for auto-
matically recognizing six annotation types use-
ful for decomposing consumer health questions.
These annotations distinguish between sentences
that contain questions and background informa-
tion. They also identify when a question sentence
can be split in multiple independent questions, and
29
when they contain optional or coordinated infor-
mation embedded within a question.
For each of these decomposition annotations,
we propose a combination of machine learning
(ML) and rule based methods. The ML methods
largely take the form of a 3-step rank-and-filter
approach, where candidates are generated, ranked
by an ML classifier, then the top-ranked candidate
is passed through a separate ML filtering classi-
fier. We evaluate each of these methods on a set of
1,467 consumer health questions related to genetic
and rare diseases.
2 Background
QA in the biomedical domain has been well-
studied (Demner-Fushman and Lin, 2007; Cairns
et al., 2011; Cao et al., 2011) as a means for re-
trieving medical information. This work has typ-
ically focused, however, on questions posed by
medical professionals, and the methods proposed
for question analysis generally assume a single,
concise question. For example, Demner-Fushman
and Abhyankar (2012) propose a method for ex-
tracting frames from queries for the purpose of
cohort retrieval. Their method assumes syntactic
dependencies exist between the necessary frame
elements, and is thus not well-suited to handle
long, multi-sentence questions. Similarly, Ander-
sen et al. (2012) proposes a method for converting
a concise question into a structured query. How-
ever, many medical questions require background
information that is difficult to encode in a single
question sentence. Instead, it is often more natural
to ask multiple questions over several sentences,
providing background information to give context
to the questions. Yu and Cao (2008) use a ML
method to recognize question types in professional
health questions. Their method can identify more
than one type per complex question. Without de-
composing the full question into its sub-questions,
however, the type cannot be associated with its
specific span, or with other information specific to
the sub-question. This other information can in-
clude answer types, question focus, and other an-
swer constraints. By decomposing multi-sentence
questions, these question-specific attributes can be
extracted, and the discourse structure of the larger
question can be better understood.
Question decomposition has been utilized be-
fore in open-domain QA approaches, but rarely
evaluated on its own. Lacatusu et al. (2006)
demonstrates how question decomposition can im-
prove the performance of a multi-sentence sum-
marization system. They perform what we refer
to as syntactic question decomposition, where the
syntactic structure of the question is used to iden-
tify sub-questions that can be answered in isola-
tion. A second form of question decomposition is
semantic decomposition, which can semantically
break individual questions apart to answer them
in stages. For instance, the question ?When did
the third U.S. President die?? can be semantically
decomposed ?Who was the third U.S. President??
and ?When did X die??, where the answer to the
first question is substituted into the second. Katz
and Grau (2005) discusses this kind of decompo-
sition using the syntactic structure, though it is not
empirically validated. Hartrumpf (2008) proposes
a decomposition method using only the deep se-
mantic structure. Finally, Harabagiu et al. (2006)
proposes a different type of question decomposi-
tion based on a random walk over similar ques-
tions extracted from a corpus. In our work, we
focus on syntactic question decomposition. We
demonstrate the importance of empirical evalua-
tion of question decomposition, notably the pit-
falls of heuristic approaches that rely entirely on
the syntactic parse tree. Syntactic parsers trained
on Treebank are particularly poor at both analyz-
ing questions (Judge et al., 2006) and coordination
boundaries (Hogan, 2007). Robust question de-
composition methods, therefore, must be able to
overcome many of these difficulties.
3 Consumer Health Question
Decomposition
Our goal is to decompose multi-sentence, multi-
faceted consumer health questions into concise
questions coupled with important contextual in-
formation. To this end, we utilize a set of an-
notations that identify the decomposable elements
and important contextual elements. A more de-
tailed description of these annotations is provided
in Roberts et al. (2014). The annotations are pub-
licly available at our institution website
1
. Here, we
briefly describe each annotation:
(1) BACKGROUND - a sentence indicating useful
contextual information, but lacks a question.
(2) QUESTION - a sentence or clause that indi-
cates an independent question.
1
http://lhncbc.nlm.nih.gov/project/consumer-health-
question-answering
30
Sentence Splitting 
Request 
Question 
Sentence 
Ignore 
Sentence 
Background 
Sentence 
Candidate Generation 
UMLS 
SVM Candidate Ranking 
Boundary Fixing 
Focus 
Focus Recognition 
Sentence Classification 
Background Classification 
SVM Comorbidity Classification 
SVM Diagnosis Classification 
SVM Family History Classification 
SVM ISF Classification 
SVM Lifestyle Classification 
SVM Symptom Classification 
SVM Test Classification 
Candidate Generation 
SVM Candidate Filter 
Question Recognition 
SVM Sentence Classification 
Question 
Candidate Generation 
SVM Candidate Ranking 
Exemplification Recognition 
Candidate Filter 
Candidate Generation 
SVM Candidate Ranking 
Coordination Recognition 
SVM Candidate Filter Coordination 
Exemplification 
Stanford 
Parser 
WordNet 
SVM Treatment Classification 
Figure 1: Question Decomposition Architecture. Modules with solid green lines indicate machine learn-
ing classifiers. Modules with dotted green lines indicate rule-based classifiers.
(3) COORDINATION - a phrase that spans a set of
decomposable items.
(4) EXEMPLIFICATION - a phrase that spans an
optional item.
(5) IGNORE - a sentence indicating nothing of
value is present.
(6) FOCUS - an NP indicating the theme of the
consumer health question.
Further explanations of each annotation are pro-
vided in Sections 4-9. To convert these annota-
tions into separate, decomposed questions, a sim-
ple set of recursive rules is used. The rules enu-
merate all ways of including one conjunct from
each COORDINATION as well as whether or not
to include the phrase within an EXEMPLIFICA-
TION. These rules must be applied recursively to
handle overlapping annotations (e.g., a COORDI-
NATION within an EXEMPLIFICATION). Our im-
plementation is straight-forward and not discussed
further in this paper. The BACKGROUND and FO-
CUS annotations do not play a direct role in this
process, though they provide important contextual
elements and are useful for co-reference, and are
thus still considered part of the overall decompo-
sition process.
It should also be noted that some questions are
syntactically decomposable, but doing so alters
their original meaning. Consider the following
two question sentences:
? Can this disease be cured or can we only treat
the symptoms?
? Are males or females worse affected?
While the first example contains two ?Can...?
questions and the second example contains the co-
ordination ?males or females?, both questions are
providing a choice between two alternatives and
decomposing them would alter the semantic na-
ture of the original question. In these cases, we do
not consider the questions to be decomposable.
Data We use a set of consumer health ques-
tions collected from the Genetic and Rare Dis-
eases Information Center (GARD), which main-
tains a website
2
with publicly available consumer-
submitted questions and professionally-authored
answers about genetic and rare diseases. We col-
lected 1,467 consumer health questions, consist-
ing of 4,115 sentences, 1,713 BACKGROUND sen-
tences, 37 IGNORE sentences, 2,465 QUESTIONs,
367 COORDINATIONs, 53 EXEMPLIFICATIONs,
and 1,513 FOCUS annotations. Questions with
more than one FOCUS are generally concerned
with the relation between diseases. Further infor-
mation about the corpus and the annotation pro-
cess can be found in Roberts et al. (2014).
System Architecture The architecture of our
question decomposition method is illustrated in
2
http://rarediseases.info.nih.gov/gard
31
Figure 1. To avoid confusion, in the rest of this
paper we refer to a complex consumer health ques-
tion simply as a request. Requests are sent to
the independent FOCUS recognition module (Sec-
tion 4), and then proceed through a pipeline that
includes the classification of sentences (Section 5),
the identification of separate QUESTIONs within
a question sentence (Section 6), the recognition
of COORDINATIONs (Section 7) and EXEMPLIFI-
CATIONs (Section 8), and the sub-classification of
BACKGROUND sentences (Section 9).
Experimental Setup The remainder of this pa-
per describes the individual modules in Figure 1.
For simplicity, we show results on the GARD data
for each task in its corresponding section. In all
cases, experiments are conducted using a 5-fold
cross-validation on the GARD data. The cross-
validation folds are organized at the request level
so that no two items from the same request will be
split between the training and testing data.
4 Identifying the Focal Disease
The FOCUS is the condition that disease-centered
questions are centered around. Many other dis-
eases may be mentioned, but the FOCUS is the dis-
ease of central concern. This is similar to the as-
sumption made about a central disease in Medline
abstracts (Demner-Fushman and Lin, 2007). Of-
ten the FOCUS is stated in the first sentence (typ-
ically a BACKGROUND) of the request while the
questions are near the end. The questions can-
not generally be answered outside the context of
the FOCUS, however, so its identification is a crit-
ical part of decomposition. As shown in Figure 1,
we use a 3-step process: (1) a high-recall method
identifies potential FOCUS diseases in the data, (2)
a support vector machine (SVM) ranks the FO-
CUS candidates, and (3) the highest-ranking can-
didate?s boundary is modified with a set of rules to
better match our annotation standard.
To identify candidates for the FOCUS, we use a
lexicon constructed from UMLS (Lindberg et al.,
1993). UMLS includes very generic terms, such as
disease and cancer, that are too general to exactly
match a FOCUS in our data. We allow these terms
to be candidates so as to not miss any FOCUS that
doesn?t exactly match an entry in UMLS. When
such a general term is selected as the top-ranked
FOCUS, the rules described below are capable of
expanding the term to the full disease name.
To rank candidates, we utilize an SVM (Fan et
E/R P R F
1
1st UMLS Disorder
E 19.6 19.0 19.3
R 28.2 27.4 27.8
SVM
E 56.4 54.7 55.6
R 89.2 86.5 87.9
SVM + Rules
E 74.8 72.5 73.6
R 89.5 86.8 88.1
Table 1: FOCUS recognition results. E = exact
match; R = relaxed match.
al., 2008) with a small number of feature types:
? Unigrams. Identifies generic words such as dis-
ease and syndrome that indicate good FOCUS
candidates, while also recognizing noisy UMLS
terms that are often false positives.
? UMLS semantic group (McCray et al., 2001).
? UMLS semantic type.
? Sentence Offset. The FOCUS is typically in the
first sentence, and is far more likely to be at the
beginning of the request than the end.
? Lexicon Offset. The FOCUS is typically the first
disease mentioned.
During training, the SVM considers any candidate
that overlaps the gold FOCUS to be correct. This
enables our approach to train on FOCUS examples
that do not perfectly align with a UMLS concept.
At test time, all candidates are classified, ranked
by the classifier?s confidence, and the top-ranked
candidate is considered the FOCUS.
As mentioned above, there are differences be-
tween how a FOCUS is annotated in our data and
how it is represented in the UMLS. We therefore
use a series of heuristics to alter the boundary to a
more usable FOCUS after it is chosen by the SVM.
The rules are applied iteratively to widen the FO-
CUS boundary until it cannot be expanded any fur-
ther. If a generic disease word is the only token
in the FOCUS, we add the token to the left. Con-
versely, if the token on the right is a generic dis-
ease word, it is added as well. If the word to the
left is capitalized, it is safe to assume it is part of
the disease?s name and so it is added as well. Fi-
nally, several rules recognize the various ways in
which a disease sub-type might be specified (e.g.,
Behcet?s syndrome vascular type, type 2 diabetes,
Charcot-Marie-Tooth disease type 2C).
We evaluate FOCUS recognition with both an
exact match, where the gold and automatic FOCUS
boundaries must line up perfectly, and a relaxed
match, which only requires a partial overlap. As a
baseline, we compare our results against a fully
rule-based system where the first UMLS Disor-
der term in the request is considered the FOCUS.
32
We also evaluate the effectiveness of our bound-
ary altering rules by measuring performance with-
out these rules. The results are shown in Table 1.
The baseline method shows significant problems
in precision and recall. It is not able to ignore
noisy UMLS terms (e.g., aim is both a gene and
a treatment). The SVM improves upon the rule-
based method by over 50 points in F
1
for relaxed
matching. Adding the boundary fixing rules has
little effect on relaxed matching, but greatly im-
proves exact matching: precision and recall are
improved by 18.4 and 17.8 points, respectively.
5 Classifying Sentences
Before precise question boundaries can be rec-
ognized, we first identify sentences that con-
tain QUESTIONs, as distinguished from BACK-
GROUND and IGNORE sentences. It should be
noted that many of the question sentences in our
data are not typical wh-word questions. About
20% of the questions in our data end in a period.
For instance:
? Please tell me more about this condition.
? I was wondering if you could let me know where
I can find more information on this topic.
? I would like to get in contact with other families
that have this illness.
We consider a sentence to be a question if it con-
tains any information request, explicit or implicit.
After sentence splitting, we identify sentences
using a multi-class SVM with three feature types:
? Unigrams with parts-of-speech (POS). Reduces
unigram ambiguities, such as what-WP (a pro-
noun, indicative of a question) versus what-
WDT (a determiner, not indicative).
? Bigrams.
? Parse tree tags. All Treebank tags from the syn-
tactic parse tree. Captures syntactic question
clues such as the phrase tags SQ (question sen-
tence) and WHNP (wh-word noun phrase).
The SVM classifier performs at 97.8%. For com-
parison, an SVM with only unigram features per-
forms at 97.2%. While the unigram model does a
good job classifying sentences, suggesting this is
a very easy task, the improved feature set reduces
the number of errors by 20%.
6 Identifying Questions
QUESTION recognition is the task of identifying
when a conjunction like and joins two independent
questions into a single sentence:
? [What causes the condition]
QUESTION
[and what
treatment is available?]
QUESTION
? [What is this disease]
QUESTION
[and what steps
can I take to protect my daughter?]
QUESTION
We consider the identification of separate QUES-
TIONs within a single sentence to be a differ-
ent task from COORDINATION recognition, which
finds phrases whose conjuncts can be treated in-
dependently. Linguistically, these tasks are quite
similar, but the distinction lies in whether the
right-conjunct syntactically depends on anything
to its left. For instance:
? I would like to learn [more about this condition
and what the prognosis is for a baby born with
it]
COORDINATION
.
Here, the right-conjunct starts with a question
stem (what), but is not a complete, grammatical
question on its own. Alternatively, this could be
re-formed into two separate QUESTIONs:
? [I would like to learn more about this
condition,]
QUESTION
[and what is the prognosis
is for a baby born with it.]
QUESTION
We make this distinction because the QUESTION
recognition task requires one fewer step since the
boundaries extend to the entire sentence, prevent-
ing error propagation from an input module. Fur-
ther, the features that differentiate our QUESTION
and COORDINATION annotations are different.
The two-step process for recognizing QUES-
TIONs includes: (1) a high-recall candidate gener-
ator, and (2) an SVM to eliminate candidates that
are not separate QUESTIONs. The candidates for
QUESTION recognition are simply all the ways a
sentence can be split by the conjunctions and, or,
as well as, and the forward slash (?/?). In our data,
this candidate generation process has a recall of
98.6, as a few examples were missed where candi-
dates were not separated by one of the above con-
junctions.
To filter candidates, we use an SVM with three
features types:
? The conjunction separating the QUESTIONs.
? Unigrams in the left-conjunct. Identifies when
the left-conjunct is not a QUESTION, or when a
question is part of a COORDINATION.
? The right-conjunct?s parse tree tag. Recog-
nizes when the right-conjunct is an independent
clause that may safely be split.
33
P R F
1
QUESTION split recognition
Baseline 24.7 82.4 38.0
SVM 67.7 64.7 66.2
Overall QUESTION recognition
Baseline 87.3 92.8 90.0
SVM 97.7 97.4 97.5
Table 2: QUESTION recognition results.
For evaluation, we measure both the F
1
score
for correct candidates, and the overall F
1
for all
QUESTION annotations (i.e., all QUESTION sen-
tences). We also evaluate a baseline method that
utilizes the parse tree to recognize separate QUES-
TIONs by splitting sentences where a conjunction
separates independent clauses. The results are
shown in Table 2. The baseline method has good
recall for recognizing where a sentence should be
split into multiple QUESTIONs, but it lacks preci-
sion. This is largely because it is unable to differ-
entiate clausal COORDINATIONs such as the above
example, as well as when the left-conjunct is not
actually a separate question. For instance:
? Our grandson was diagnosed recently with this
disease and I am wondering if you could send
me information on it.
The SVM-based method can overcome this prob-
lem by looking at the words in the left-conjunct.
Both methods, however, fail to recognize when
two independent question clauses are asking the
same question but providing alternative answers:
? Will this condition be with him throughout his
life, or is it possible that it will clear up?
While there are methods for handling this issue
for COORDINATION recognition, addressed be-
low, recognizing non-splittable QUESTIONs re-
quires far deeper semantic understanding which
we leave to future work.
7 Identifying Coordinations
COORDINATION recognition is the task of identi-
fying when a conjunction joins phrases within a
QUESTION that can in be separate questions:
? How can I learn more about [treatments and
clinical trials]
COORDINATION
?
? Are [muscle twitching, muscle cramps, and
muscle pain]
COORDINATION
effects of having sil-
icosis?
Unlike QUESTION recognition, the boundaries of
a COORDINATION need to be determined as well
as whether the conjuncts can semantically be split
into separate questions. We thus use a three-step
process for recognizing COORDINATIONs: (1) a
high-recall candidate generator, (2) an SVM to
rank all the candidates for a given conjunction, and
(3) an SVM to filter out top-ranked candidates.
Candidate generation begins with the identifica-
tion of valid conjunctions within a QUESTION an-
notation. We use the same four conjunctions as in
QUESTION recognition: and, or, as well as, and
the forward slash. For each of these, all possi-
ble left and right boundaries are generated, so in
a QUESTION with 4 tokens on either side of the
conjunction, there would be 16 candidates. Addi-
tionally, two adjectives separated by a comma and
immediately followed by a noun are considered a
candidate (e.g., ?a [safe, permanent]
COORDINATION
treatment?). In our data, this candidate generation
process has a recall of 98.9, as a few instances ex-
ist in which a conjunction is not used, such as:
? I am looking for any information you have
about heavy metal toxicity, [treatment,
outcomes]
EXEMPLIFICATION+COORDINATION
.
To rank candidates, we use an SVM with the
following feature types:
? If the left-conjunct is congruent with the high-
est node in the syntactic parse tree whose right-
most leaf is also the right-most token in the left-
conjunct. Essentially, this is equivalent to say-
ing whether or not the syntactic parser agrees
with the left-conjunct?s boundary.
? The equivalent heuristic for the right-conjunct.
? If a noun is in both, just the left conjunct, just
the right conjunct, or neither conjunct.
? The Levenshtein distance between the POS tag
sequences for the left- and right-conjuncts.
The first two features encode the information a
rule-based method would use if it relied entirely
on the syntactic parse tree. The remaining features
help the classifier overcome cases where the parser
may be wrong.
At training time, all candidates for a given con-
junction are generated and only the candidate that
matches the gold COORDINATION is considered
a positive example. Additionally, we annotated
the boundaries for negative COORDINATIONs (i.e.,
syntactic coordinations that do not fit our annota-
tion standard). There were 203 such instances in
the GARD data. These are considered gold CO-
ORDINATIONs for boundary ranking only.
To filter the top-ranked candidates, we use an
SVM with several feature types:
34
E/R P R F
1
Baseline
E 28.1 36.5 31.8
R 62.9 75.8 68.7
Rank + Filter
E 38.2 34.8 36.4
R 78.5 69.0 73.5
Table 3: COORDINATION recognition results.
E = exact match; R = relaxed match.
? The conjunction.
? Unigrams in the left-conjunct.
? POS of the first word in both conjuncts. CO-
ORDINATIONs often have the same first POS in
both conjuncts.
? The word immediately before the candidate.
E.g., between is a good negative indicator.
? Unigrams in the question but not the candidate.
? If the candidate takes up almost the entire ques-
tion (all but 3 tokens). Typically, COORDINA-
TIONs are much smaller than the full question.
? If more than one conjunction is in the candidate.
? If a word in the left-conjunct has an antonym
in the right conjunct. Antonyms are recognized
via WordNet (Fellbaum, 1998).
At training time, the positive examples are drawn
from the annotated COORDINATIONs, while the
negative examples are drawn from the 203 non-
gold annotations mentioned above.
In addition to evaluating this method, we
evaluate a baseline method that relies entirely
on the syntactic parse to identify COORDINA-
TION boundaries without filtering. The results
are shown in Table 3. The rank-and-filter ap-
proach shows significant gains over the rule-based
method in precision and F
1
. As can be seen in
the difference between exact and relaxed match-
ing, most of the loss for both the baseline and ML
methods come in boundary detection. Most meth-
ods overly rely upon the syntactic parser, which
performs poorly both on questions and coordina-
tions. The ML method, though, is sometimes able
to overcome this problem.
8 Identifying Exemplifications
EXEMPLIFICATION recognition is the task of iden-
tifying when a phrase provides an optional, exem-
plifying example with a more specific type of in-
formation than that asked by the rest of the ques-
tion. For instance, the following contains both an
EXEMPLIFICATION and a COORDINATION:
? Is there anything out there that can help
him [such as [medications or alternative
therapies]
COORDINATION
]
EXEMPLIFICATION
?
We could consider this to denote 3 questions:
? Is there anything out there that can help him?
? Is there anything out there that can help him
such as medications?
? Is there anything out there that can help him
such as alternative therapies?
In the latter two questions, we consider the phrase
such as to now denote a mandatory constraint on
the answer to each question, whereas in the origi-
nal question it would be considered optional.
EXEMPLIFICATION recognition is similar to
COORDINATION recognition, and its three-step
process is thus similar as well: (1) a high-recall
candidate generator, (2) an SVM to rank all the
candidates for a given trigger phrase, and (3) a set
of rules to filter out top-ranked candidates.
Candidate generation begins with the identifica-
tion of valid trigger words and phrases. These in-
clude: especially, including, particularly, specifi-
cally, and such as. For each of these, all possible
right boundaries are generated, thus EXEMPLIFI-
CATIONs have far fewer candidates than COORDI-
NATIONs. Additionally, all phrases within paren-
theses are added as EXEMPLIFICATIONs. In our
data, this candidate generation process has a recall
of 98.1, missing instances without a trigger (see
the example also missed by COORDINATION can-
didate generation in Section 7).
To rank candidates, we use an SVM with the
following feature types:
? If the right-conjunct is the highest parse node
as defined in the COORDINATION boundary fea-
ture.
? If a dependency relation crosses from the right-
conjunct to any word outside the candidate.
? POS of the word after the candidate.
As with COORDINATIONs, we annotated bound-
aries for negative EXEMPLIFICATIONs matching
the trigger words and used them as positive exam-
ples for boundary ranking.
To filter the top-ranked candidates, we use two
simple rules. First, EXEMPLIFICATIONs within
parentheses are filtered if they are acronyms or
acronym expansions. Second, cases such as the
below example are removed by looking at the
words before the candidate:
? I am particularly interested in learning more
about genetic testing for the syndrome.
In addition to evaluating this method, we eval-
uate a baseline method that relies entirely on the
35
E/R P R F
1
Baseline
E 28.9 62.3 39.5
R 39.5 84.9 53.9
Rank + Filter
E 60.8 58.5 59.6
R 80.4 77.4 78.8
Table 4: EXEMPLIFICATION recognition results.
E = exact match; R = relaxed match.
syntactic parser to identify EXEMPLIFICATION
boundaries and performs no filtering. The re-
sults are shown in Table 4. The rank-and-filter
approach shows significant gains over the rule-
based method in precision and F
1
, more than dou-
bling precision for both exact and relaxed match-
ing. There is still a drop in performance when go-
ing from relaxed to exact matching, again largely
due to the reliance on the syntactic parser.
9 Classifying Background Information
BACKGROUND sentences contain contextual in-
formation, such as whether or not a patient has
been diagnosed with the focal disease or what
symptoms they are experiencing. This informa-
tion was annotated at the sentence level, partly be-
cause of annotation convenience, but also because
phrase boundaries are not always clear for medical
concepts (Hahn et al., 2012; Forbush et al., 2013).
A difficult factor in this task, and especially on
the GARD dataset, is that consumers are not al-
ways asking about a disease for themselves. In-
stead, often they ask on behalf of another individ-
ual, often a family member. The BACKGROUND
types are thus annotated based on the person of
interest, who we refer to as the patient (in the lin-
guistic sense). For instance, if a mother has a dis-
ease but is asking about her son (e.g., asking about
the probability of her son inheriting the disease),
that sentence would be a FAMILY HISTORY, as
opposed to a DIAGNOSIS sentence.
The GARD corpus is annotated with eight
BACKGROUND types:
? COMORBIDITY
? DIAGNOSIS
? FAMILY HISTORY
? ISF (information
search failure)
? LIFESTYLE
? SYMPTOM
? TEST
? TREATMENT
ISF sentences indicate previous attempts to find
the requested information have failed, and are a
good signal to the QA system to enable more in-
depth search strategies. LIFESTYLE sentences de-
scribe the patient?s life habits (e.g., smoking, ex-
ercise). Currently, the automatic identification of
Type P R F
1
# Anns
COMORBIDITY 0.0 0.0 0.0 23
DIAGNOSIS 80.8 80.3 80.5 690
FAMILY HISTORY 67.4 38.4 48.9 151
ISF 75.0 65.9 70.1 41
LIFESTYLE 0.0 0.0 0.0 13
SYMPTOM 76.6 48.1 59.1 320
TEST 37.5 4.9 8.7 61
TREATMENT 87.3 35.0 50.0 137
Overall: Micro-F
1
: 67.3 Macro-F
1
: 39.7
Table 5: BACKGROUND results.
BACKGROUND types has not been a major focus
of our effort as no handling exists for it within our
QA system. We report a baseline method and re-
sults here to provide some insight into the diffi-
culty of the task.
BACKGROUND types are a multi-labeling prob-
lem, so we use eight binary classifiers, one for
each type. Each classifier utilizes only unigram
and bigram features. The results for the mod-
els are shown in Table 5. COMORBIDITY and
LIFESTYLE are too rare in the data (23 and 13
instances, respectively) for the classifier to iden-
tify. DIAGNOSIS questions are identified fairly
well because this is the most common type (690
instances) and because of the constrained vocabu-
lary for expressing a diagnosis. The performance
of the rest of the types is largely proportional to
the number of instances in the data, though ISF
performs quite well given only 41 instances.
10 Conclusion
We have presented a method for decomposing
consumer health questions by recognizing six an-
notation types. Some of these types are general
enough to use in open-domain question decom-
position (BACKGROUND, IGNORE, QUESTION,
COORDINATION, EXEMPLIFICATION), while oth-
ers are targeted specifically at consumer health
questions (FOCUS and the BACKGROUND sub-
types). We demonstrate that ML methods can
improve upon heuristic methods relying on the
syntactic parse tree, though parse errors are of-
ten difficult to overcome. Since significant im-
provements in performance would likely require
major advances in open-domain syntactic parsing,
we instead envision further integration of the key
tasks in consumer health question analysis: (1) in-
tegration of co-reference and implicit argument in-
formation, (2) improved identification of BACK-
GROUND types, and (3) identification of discourse
relations within questions to further leverage ques-
tion decomposition.
36
Acknowledgements
This work was supported by the intramural re-
search program at the U.S. National Library of
Medicine, National Institutes of Health. We would
additionally like to thank Stephanie M. Morri-
son and Janine Lewis for their help accessing the
GARD data.
References
Ulrich Andersen, Anna Braasch, Lina Henriksen,
Csaba Huszka, Anders Johannsen, Lars Kayser,
Bente Maegaard, Ole Norgaard, Stefan Schulz, and
J?urgen Wedekind. 2012. Creation and use of Lan-
guage Resources in a Question-Answering eHealth
System. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
pages 2536?2542.
Brian L. Cairns, Rodney D. Nielsen, James J. Masanz,
James H. Martin, Martha S. Palmer, Wayne H. Ward,
and Guergana K. Savova. 2011. The MiPACQ Clin-
ical Question Answering System. In Proceedings of
the AMIA Annual Symposium, pages 171?180.
YongGang Cao, Feifan Liu, Pippa Simpson, Lamont
Antieau, Andrew Bennett, James J. Cimino, John
Ely, and Hong Yu. 2011. AskHERMES: An on-
line question answering system for complex clini-
cal questions. Journal of Biomedical Informatics,
44:277?288.
Dina Demner-Fushman and Swapna Abhyankar. 2012.
Syntactic-Semantic Frames for Clinical Cohort
Identification Queries. In Data Integration in the
Life Sciences, volume 7348 of Lecture Notes in
Computer Science, pages 100?112.
Dina Demner-Fushman and Jimmy Lin. 2007. An-
swering Clinical Questions with Knowledge-Based
and Statistical Techniques. Computational Linguis-
tics, 33(1).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Tyler B. Forbush, Adi V. Gundlapalli, Miland N.
Palmer, Shuying Shen, Brett R. South, Guy Divita,
Marjorie Carter, Andrew Redd, Jorie M. Butler, and
Matthew Samore. 2013. ?Sitting on Pins and Nee-
dles?: Characterization of Symptom Descriptions in
Clinical Notes. In AMIA Summit on Clinical Re-
search Informatics, pages 67?71.
Udo Hahn, Elena Beisswanger, Ekaterina Buyko, Erik
Faessler, Jenny Traum?uller, Susann Schr?oder, and
Kerstin Hornbostel. 2012. Iterative Refinement
and Quality Checking of Annotation Guidelines ?
How to Deal Effectively with Semantically Sloppy
Named Entity Types, such as Pathological Phenom-
ena. In Proceedings of the Eighth International
Conference on Language Resources and Evaluation,
pages 3881?3885.
Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl.
2006. Answer Complex Questions with Random
Walk Models. In Proceedings of the 29th Annual
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 220?227.
Sven Hartrumpf. 2008. Semantic Decomposition
for Question Answering. In Proceedings on the
18th European Conference on Artificial Intelligence,
pages 313?317.
Dierdre Hogan. 2007. Coordinate Noun Phrase Dis-
ambiguation in a Generative Parsing Model. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 680?687.
John Judge, Aoife Cahill, and Josef van Genabith.
2006. QuestionBank: Creating a Corpus of Parse-
Annotated Questions. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 497?504.
Yarden Katz and Bernardo C. Grau. 2005. Repre-
senting Qualitative Spatial Information in OWL-DL.
Proceedings of OWL: Experiences and Directions.
Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-
Fushman. 2013. Interpreting Consumer Health
Questions: The Role of Anaphora and Ellipsis. In
Proceedings of the 2013 BioNLP Workshop, pages
54?62.
Finley Lacatusu, Andrew Hickl, and Sanda Harabagiu.
2006. Impact of Question Decomposition on the
Quality of Answer Summaries. In Proceedings of
LREC, pages 1147?1152.
Donald A.B. Lindberg, Betsy L. Humphreys, and
Alexa T. McCray. 1993. The Unified Medical Lan-
guage System. Methods of Information in Medicine,
32(4):281?291.
Alexa T McCray, Anita Burgun, and Olivier Boden-
reider. 2001. Aggregating UMLS Semantic Types
for Reducing Conceptual Complexity. In Studies
in Health Technology and Informatics (MEDINFO),
volume 84(1), pages 216?220.
Kirk Roberts, Kate Masterton, Marcelo Fiszman, Halil
Kilicoglu, and Dina Demner-Fushman. 2014. An-
notating Question Decomposition on Complex Med-
ical Questions. In Proceedings of LREC.
Hong Yu and YongGang Cao. 2008. Automatically
Extracting Information Needs from Ad Hoc Clini-
cal Questions. In Proceedings of the AMIA Annual
Symposium.
37
Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 45?53,
Baltimore, Maryland USA, June 26-27 2014.
c?2014 Association for Computational Linguistics
Coreference Resolution for Structured Drug Product Labels
Halil Kilicoglu and Dina Demner-Fushman
National Library of Medicine
National Institutes of Health
Bethesda, MD, 20894
{kilicogluh,ddemner}@mail.nih.gov
Abstract
FDA drug package inserts provide com-
prehensive and authoritative information
about drugs. DailyMed database is a
repository of structured product labels ex-
tracted from these package inserts. Most
salient information about drugs remains
in free text portions of these labels. Ex-
tracting information from these portions
can improve the safety and quality of drug
prescription. In this paper, we present a
study that focuses on resolution of coref-
erential information from drug labels con-
tained in DailyMed. We generalized and
expanded an existing rule-based coref-
erence resolution module for this pur-
pose. Enhancements include resolution of
set/instance anaphora, recognition of ap-
positive constructions and wider use of
UMLS semantic knowledge. We obtained
an improvement of 40% over the baseline
with unweighted average F
1
-measure us-
ing B-CUBED, MUC, and CEAF metrics.
The results underscore the importance of
set/instance anaphora and appositive con-
structions in this type of text and point out
the shortcomings in coreference annota-
tion in the dataset.
1 Introduction
Almost half of the US population uses at least one
prescription drug and over 75% of physician of-
fice visits involve drug therapy
1
. Knowing how
these drugs will affect the patient is very impor-
tant, particularly, to over 20% of the patients that
are on three or more prescription drugs
1
. FDA
drug package inserts (drug labels or Structured
1
Centers for Disease Control and Preven-
tion: FASTSTATS - Therapeutic Drug Use:
http://www.cdc.gov/nchs/fastats/drugs.htm
Product Labels (SPLs)) provide curated informa-
tion about the prescription drugs and many over-
the-counter drugs. The drug labels for most drugs
are publicly available in XML format through Dai-
lyMed
2
. Some information in these labels, such as
the drug identifiers and ingredients, could be eas-
ily extracted from the structured fields of the XML
documents. However, the salient content about in-
dications, side effects and drug-drug interactions,
among others, is buried in the free text of the
corresponding sections of the labels. Extracting
this information with natural language process-
ing techniques can facilitate automatic timely up-
dates to databases that support Electronic Health
Records in alerting physicians to potential drug in-
teractions, recommended doses, and contraindica-
tions.
Natural language processing methods are in-
creasingly used to support various clinical and
biomedical applications (Demner-Fushman et al.,
2009). Extraction of drug information is playing a
prominent role in these applications and research.
In addition to earlier research in extraction of med-
ications and relations involving medications from
clinical text and the biomedical literature (Rind-
flesch et al., 2000; Cimino et al., 2007), in the
third i2b2 shared task (Uzuner et al., 2010), 23
organizations have explored extraction of medica-
tions, their dosages, routes of administration, fre-
quencies, durations, and reasons for administra-
tion from clinical text. The best performing sys-
tems used rule-based and machine learning tech-
niques to achieve over 0.8 F-measure for extrac-
tion of medication names; however, the remain-
ing information was harder to extract. Researchers
have also tackled extraction of drug-drug interac-
tions (Herrero-Zazo et al., 2013), side effects (Xu
and Wang, 2014), and indications (Fung et al.,
2013) from various biomedical resources.
As for many other information extraction tasks,
2
DailyMed: http://dailymed.nlm.nih.gov/dailymed/about.cfm
45
extracting drug information is often made more
difficult by coreference. Coreference is defined as
the relation between linguistic expressions that are
referring to the same entity (Zheng et al., 2011).
Coreference resolution is a fundamental task in
NLP and can benefit many downstream applica-
tions, such as relation extraction, summarization,
and question answering. Difficulty of the task is
due to the fact that various levels of linguistic in-
formation (lexical, syntactic, semantic, and dis-
course contextual features) generally play a role.
Coreference occurs frequently in all types of
biomedical text, including the drug package in-
serts. Consider the example below:
(1) Since amiodarone is a substrate for
CYP3A and CYP2C8, drugs/substances
that inhibit these isoenzymes may decrease
the metabolism . . . .
In this example, the expression these isoenzymes
refer to CYP3A and CYP2C8. Resolving this
coreference instance would allow us to capture the
following drug interactions mentioned in the sen-
tence: inhibitors of CYP3A POTENTIATE amio-
darone and inhibitors of CYP2C8 POTENTIATE
amiodarone.
In this paper, we present a study that focuses on
identification of coreference links in drug labels,
with the view that these relations will facilitate
the downstream task of drug interaction recogni-
tion. The rule-based system presented is an exten-
sion of the previous work reported in Kilicoglu et
al. (2013). The main focus of the dataset, based
on SPLs, is drug interaction information. Coref-
erence is only annotated when it is relevant to ex-
tracting such information. In addition to evaluat-
ing the system against a baseline, we also manu-
ally assessed the system output for precision. Fur-
thermore, we also evaluated the system on a sim-
ilarly drug-focused corpus annotated for anaphora
(DrugNerAR) (Segura-Bedmar et al., 2010). Our
results demonstrate that set/instance anaphora res-
olution and appositive recognition can play a sig-
nificant role in this type of text and highlight some
of the major areas of difficulty and potential en-
hancements.
2 Related Work
We discuss two areas of research related to this
study in this section: processing of drug labels
and coreference resolution focusing on biomedi-
cal text. Drug labels, despite their availability and
the wealth of information contained within them,
remain underutilized. One of the reasons might be
the complexity of the text in the labels: in a review
of publicly available text sources that could be
used to augment a repository of drug indications
and adverse effects (ADEs), Smith et al. (2011)
concluded that many indication and adverse drug
event relationships in the drug labels are too com-
plex to be captured in the existing databases of in-
teractions and ADEs. Despite the complexity, the
labels were used to extract indications for drugs in
several studies. Elkin et al. (2011) automatically
extracted indications, mapped them to SNOMED-
CT and then automatically derived rules in the
form (?Drug? HasIndication ?SNOMED CT?).
Fung et al. (2013) used MetaMap (Aronson and
Lang, 2010) to extract indications and map them
to the UMLS (Lindberg et al., 1993), and then
manually validated the quality of the mappings.
Oprea et al. (2011) used information extracted
from the adverse reactions sections of 988 drugs
for computer-aided drug repurposing. Duke et
al. (2011) have developed a rule-based system that
extracted 534,125 ADEs from 5602 SPLs. Zhu
et al. (2013) extracted disease terms from five
SPL sections (indication, contraindication, ADE,
precaution, and warning) and combined the ex-
tracted terms with the drug and disease relation-
ships in NDF-RT to disambiguate the PharmGKB
drug and disease associations. A hybrid NLP sys-
tem, AutoMCExtractor, uses conditional random
fields and post-processing rules to extract medical
conditions from SPLs and build triplets in the form
of([drug name]-[medical condition]-[LOINC sec-
tion header]) (Li et al., 2013).
Coreference resolution in the biomedical do-
main was addressed in the 2011 i2b2/VA shared
task (Uzuner et al., 2012), and the 2011 BioNLP
Shared Task (Kim et al., 2012); however these
community-wide evaluations did not change much
the observation in the 2011 review by Zheng
et al. (2011) that only a handful of systems
were developed for handling anaphora and coref-
erence in clinical text and biomedical publica-
tions. Since this comprehensive article was pub-
lished, Yoshikawa et al. (2011) proposed two
coference resolution models based on support vec-
tor machine and joint Markov logic network to
aid the task of biological event extraction. Sim-
ilarly, Miwa et al. (2012) and Kilicoglu and
Bergler (2012) extended their biological event
46
extraction pipelines using rule-based corefer-
ence systems that rely on syntactic information
and predicate argument structures. Nguyen et
al. (2012) evaluated contribution of discourse pref-
erence, number agreement, and domain-specific
semantic information in capturing pronominal and
nominal anaphora referring to proteins. An ef-
fort similar to ours is that of Segura-Bedmar et
al. (2010), who resolve anaphora to support drug-
drug interaction extraction. They created a cor-
pus of 49 interactions sections extracted from the
DrugBank database, having on average 40 sen-
tences and 716 tokens. They then manually anno-
tated pronominal and nominal anaphora, and de-
veloped a rule-based approach that achieve 0.76
F
1
-measure in anaphora resolution.
3 Methods
3.1 The dataset
We used a dataset extracted from FDA drug pack-
age labels by our collaborators at FDA interested
in extracting interactions between cardiovascu-
lar drugs. The dataset consists of 159 drug la-
bels, with an average of 105 sentences and 1787
tokens per label. It is annotated for three en-
tity types (Drug, Drug Class, and Substance) and
four drug interaction types (Caution, Decrease, In-
crease, and Specific). 377 instances of corefer-
ence were annotated. Two annotators separately
annotated the labels and one of the authors per-
formed the adjudication. The relatively low num-
ber of coreference instances is due to the fact that
coreference was annotated only when it would be
relevant to drug interaction recognition task. This
parsimonious approach to annotation presents dif-
ficulty in automatically evaluating the system, and
to mitigate this, we present an assessment of the
precision of our end-to-end coreference system, as
well. We split the dataset into training and test sets
by random sampling. Training data consists of 79
documents and the test set has 80 documents. We
used the training data for analysis and as the basis
of our enhancements.
3.2 The system
The work described in this paper extends and
refines earlier work, described in Kilicoglu et
al. (2013), which focused on disease anaphora and
ellipsis in the context of consumer health ques-
tions. We briefly recap that system here. The sys-
tem begins by mapping named entities to UMLS
Metathesaurus concepts (CUIs). Next, it identifies
anaphoric expressions in text, which include per-
sonal (e.g., it, they) and demonstrative pronouns
(e.g., this, those), as well as sortal anaphora (def-
inite (e.g., with the) and demonstrative (e.g., with
that) noun phrases). The candidate antecedents
are then recognized using syntactic (person, gen-
der and number agreement, head word matching)
and semantic (hypernym and UMLS semantic type
matching) constraints. Finally, the co-referent is
then selected as the focus of the question, which is
taken as the first disease mention in the question.
The coreference resolution pipeline used in the
current work, while enhanced significantly, fol-
lows the same basic sequence. The relatively sim-
ple approach of earlier work is generally sufficient
for consumer health questions; however, we found
it insufficient when it comes to drug labels. Aside
from the obvious point that the approach was lim-
ited to diseases, there are other stylistic differences
that have an impact on coreference resolution. In
contrast to informal and casual style of consumer
health questions, drug labels are curated and pro-
vide complex indication and ADE information in
a formal style, more akin to biomedical literature.
Our analysis of the training data highlighted sev-
eral facts regarding coreference in drug labels: (1)
the set/instance anaphora (including those involv-
ing distributive anaphora such as both, each, ei-
ther) instances are prominent, (2) demonstrative
pronominal anaphora is non-existent in contrast
to consumer health questions, (3) the focus-based
salience scoring is simplistic for longer texts. We
describe the system enhancements below.
3.2.1 Generalizing from diseases to drugs
and beyond
We generalized from resolution of disease coref-
erence only to resolution of coreference involv-
ing other entity types. For this purpose, we para-
materized semantic groups and hypernym lists as-
sociated with each semantic group. We general-
ized the system in the sense that new semantic
types and hypernyms can be easily defined and
used by the system. In addition to Disorder se-
mantic group and Disorder hypernym list defined
in earlier work, we used Drug, Intervention, Pop-
ulation, Procedure, Anatomy, and Gene/Protein
semantic groups and hypernym lists. Semantic
group classification largely mimics coarse-grained
UMLS semantic groups (McCray et al., 2001).
For example, UMLS semantic types Pharmaco-
47
logic Substance and Clinical Drug are aggregated
into both Drug and Intervention semantic groups,
while Therapeutic or Preventive Procedure is as-
signed to Procedure group only. Drug hypernyms,
such as medication, drug, agent, were derived
from the training data.
3.2.2 Set/instance anaphora
Set/instance anaphora instances are prevalent in
drug labels. In our dataset, 19% of all anno-
tated anaphoric expressions indicate set/instance
anaphora (co-referring with 29% of antecedent
terms). An example was provided earlier (Ex-
ample 1). While recognizing anaphoric expres-
sions that indicate set/instance anaphora is not
necessarily difficult (i.e., recognizing these isoen-
zymes in the example), linking them to their an-
tecedents can be difficult, since it generally in-
volves correctly identifying syntactic coordina-
tion, a challenging syntactic parsing task (Ogren,
2010). Our identification of these structures re-
lies on collapsed Stanford dependency output (de
Marneffe et al., 2006) and uses syntactic and se-
mantic constraints. We examine all the depen-
dency relations extracted from a sentence and only
consider those with the type conj * (e.g., conj and,
conj or). For increased accuracy, we then check
the tokens involved in the dependency (conjuncts)
and ensure that there is a coordinating conjunc-
tion (e.g., and, or, , (comma), & (ampersand)) be-
tween them. Once such a conjunction is identified,
we then examine the semantic compatibility of the
conjuncts. In the case of entities, the compatibil-
ity involves that at the semantic group level. In the
current work, we also began recognizing distribu-
tive anaphora, such as either, each as anaphoric
expressions. When the recognized anaphoric ex-
pression is plural (as in they, these agents or either
drug), we allow the coordinated structures previ-
ously identified in this fashion as candidate an-
tecedents. The current work does not address a
more complex kind of set/instance anaphora, in
which the instances are not syntactically coordi-
nated, such as in Example (2), where such agents
refer to thiazide diuretics, in the preceding sen-
tence, as well as Potassium-sparing diuretics and
potassium supplements.
(2) . . . can attenuate potassium loss caused
by thiazide diuretics. Potassium-sparing
diuretics . . . or potassium supplements can
increase . . . . if concomitant use of
such agents is indicated . . .
3.2.3 Appositive constructions
Coreference involving appositive constructions
3
are annotated in some corpora, including the
BioNLP shared task coreference dataset (Kim
et al., 2012) and DrugNerAR corpus (Segura-
Bedmar et al., 2010). An example is given below,
in which the indefinite noun phrase a drug and the
drug lovastatin are appositives.
(3) PLETAL does not, however, appear to cause
increased blood levels of drugs metabolized
by CYP3A4, as it had no effect on lovastatin,
a drug with metabolism very sensitive to
CYP3A4 inhibition.
In our dataset, coreference involving apposi-
tive constructions were generally left unannotated.
However, it was consistently the case that when
one of the items in the construction is annotated
as the antecedent for an anaphoric expression,
the other item in the construction was also anno-
tated as such. Therefore, we identified appositive
constructions in text to aid the antecedent selec-
tion task. We used dependency relations for this
task, as well. Identifying appositives is relatively
straightforward using syntactic dependency rela-
tions. We adapted the following rule from Kil-
icoglu and Bergler (2012):
APPOS(Antecedent,Anaphor) ?
APPOS(Anaphor,Antecedent) ?
COREF(Anaphor,Antecedent)
where APPOS ? {appos, abbrev, prep including,
prep such as}. In our case, this rule becomes
(APPOS(Antecedent1,Antecedent2) ?
APPOS(Antecedent2,Antecedent1)) ?
COREF(Anaphor,Antecedent1) ?
COREF(Anaphor,Antecedent2)
which essentially states that a candidate is taken as
an antecedent, only if its appositive has been rec-
ognized as an antecedent. Additionally, semantic
compatibility between the items is required.
This allows us to identify their and Class Ia an-
tiarrhythmic drugs as co-referents in the following
example, due to the fact that the exemplification
indicated by the appositive construction between
Class Ia antiarrythmic drugs and disopyramide is
recognized, the latter previously identified as an
antecedent for their.
3
We use the term ?appositive? to cover exemplifications,
as well.
48
(4) Class Ia antiarrhythmic drugs, such as
disopyramide, quinidine and procainamide
and other Class III drugs (e.g., amiodarone)
are not recommended . . . because of their
potential to prolong refractoriness.
3.2.4 Relative pronouns
Similar to appositive constructions, relative pro-
nouns are annotated as anaphoric expressions in
some corpora (same as those for appositives), but
not in our dataset. In the example below, the rela-
tive pronoun which refers to potassium-containing
salt substitutes.
(5) . . . the concomitant use of potassium-sparing
diuretics, potassium supplements, and/or
potassium-containing salt substitutes, which
should be used cautiously. . .
Since we aim for generality and this type of
anaphora can be important for downstream ap-
plications, we implemented a rule, again taken
from Kilicoglu and Bergler (2012), which simply
states that the antecedent of a relative pronominal
anaphora is the noun phrase head it modifies.
rel(X,Anaphor) ? rcmod(Antecedent,X) ?
COREF(Anaphor,Antecedent)
where rel indicates a relative dependency, and rc-
mod a relative clause modifier dependency. We
extended this in the current work to include the
following rules:
(6) (a) LEFT(Antecedent,Anaphor) ?
NO INT WORD(Antecedent,Anaphor)
? COREF(Anaphor,Antecedent)
(b) LEFT(Antecedent,Anaphor) ? rc-
mod(Antecedent,X)? LEFT(Anaphor,X)
? COREF(Anaphor,Antecedent)
where LEFT indicates that the first argument is
to the left of the second and NO INT WORD in-
dicates that the arguments have no intervening
words between them.
3.3 Drug ingredient/brand name synonymy
A specific, non-anaphoric type of coreference,
between drug ingredient name and drug?s brand
name, is commonly annotated in our dataset. An
example is provided below, where COREG CR is
the brand name for carvedilol.
(7) The concomitant administration of amio-
darone or other CYP2C9 inhibitors such as
fluconazole with COREG CR may enhance
the -blocking properties of carvedilol . . . .
To identify this type of coreference, we use se-
mantic information from UMLS Metathesaurus.
We stipulate that, to qualify as co-referents, both
terms under consideration should map to the same
UMLS concept (i.e., that they are considered syn-
onyms). If the terms are within the same sentence,
we further require that they are appositive.
3.3.1 Demonstrative pronouns
Anaphoric expressions of demonstrative pronoun
type generally have discourse-deictic use; in other
words, they often refer to events, propositions de-
scribed in prior discourse or even to the full sen-
tences or paragraphs, rather than concrete objects
or entities (Webber, 1988). This fact was implic-
itly exploited in consumer health questions, since
the coreference resolution focused on diseases
only, which are essentially processes. However,
in drug labels, discourse-deictic use of demonstra-
tives is much more overt. Consider the sentence
below, where the demonstrative This refers to the
event of increasing the exposure to lovastatin.
(8) Co-administration of lovastatin and SAMSCA
increases the exposure to lovastatin and . . . .
This is not a clinically relevant change.
To handle such cases, we blocked entity an-
tecedents (such as drugs) for demonstrative pro-
nouns and only allowed predicates (verbs, nomi-
nalizations) as candidate antecedents.
3.3.2 Pleonastic it
We recognized pleonastic instances of the pronoun
it to disqualify them as anaphoric expressions (for
instance, it in It may be necessary to . . . ). Gen-
erally, lexical patterns involving sequence of to-
kens are used to recognize such instances (e.g.,
(Segura-Bedmar et al., 2010). We used a simple
dependency-based rule that mimics these patterns,
given below.
nsubj*(X,it) ? DEP(X,Y) ? PLEONASTIC(it)
where nsubj* refers to nsubj or nsubjpass depen-
dencies and DEP is any dependency, where DEP
/? {infmod, ccomp, xcomp}.
3.3.3 Discourse-based constraints
Previously, we did not impose limits on how far
the co-referents could be from each other, since
the entire discourse was generally short and the
salient antecedent (often the topic of the question)
appeared early in discourse. This is often not the
49
case in drug labels, especially because often intri-
cate interactions between the drug of interest and
other medications are discussed. Therefore, we
limit the discourse window from which candidate
antecedents are identified. Generally, the search
space for the antecedents is limited to the current
sentence as well as the two preceding sentences
(Segura-Bedmar et al., 2010; Nguyen et al., 2012).
In our dataset, we found that 98% of antecedents
occurred within this discourse window and, thus,
use the same search space. We make an exception
for the cases in which the anaphoric expression ap-
pear in the first sentence of a paragraph and no
compatible antecedent is found in the same sen-
tence. In this case, the search space is expanded to
the entire preceding paragraph.
We also extended the system to include different
types of salience scoring methods. For drug labels,
we use linear distance between the co-referents (in
terms of surface elements) as the salience score;
the lower this score, the better candidate the an-
tecedent is. Additionally, we implemented syn-
tactic tree distance between the co-referents as a
potential salience measure, even though this type
of salience scoring did not have an effect on our
results on drug labels.
Finally, we block candidate antecedents that
are in a direct syntactic dependency with the
anaphoric expression, except when the anaphor is
reflexive (e.g., itself ).
3.4 Evaluation
To evaluate our approach, we used a baseline simi-
lar to that reported in Segura-Bedmar et al. (2010),
which consists of selecting the closest preceding
nominal phrase for the anaphoric expressions an-
notated in their corpus. These expressions in-
clude pronominal (personal, relative, demonstra-
tive, etc.) and nominal (definite, possessive,
etc.) anaphora. We compared our system to
this baseline using the unweighted average of F
1
-
measure over B-CUBED (Bagga and Baldwin,
1998), MUC (Vilain et al., 1995), and CEAF (Luo,
2005) metrics, the standard evaluation metrics for
coreference resolution. We used the scripts pro-
vided by i2b2 shared task organizers for this pur-
pose. Since coreference annotation was parsimo-
nious in our dataset, we also manually examined a
subset of the coreference relations extracted by the
system for precision. Additionally, we tested our
system on DrugNerAR corpus (Segura-Bedmar et
al., 2010), which similarly focuses on drug inter-
actions. We compared our results to theirs, us-
ing as evaluation metrics precision, recall, and F
1
-
measure, the metrics that were used in their evalu-
ation.
4 Results and Discussion
With the drug label dataset, we obtained the best
results without relative pronominal anaphora reso-
lution and drug ingredient/brand name synonymy
strategies (OPTIMAL) and with linear distance
as the salience measure. In this setting, using
gold entity annotations, we recognized 318 coref-
erence chains, 54 of which were annotated in the
corpus. The baseline identified 1415 coreference
chains, only 10 of which were annotated. The im-
provement provided by the system over the base-
line is clear; however, the low precision/recall/F
1
-
measure, given in Table 1, should be taken with
caution due to the sparse coreference annotation
in the dataset. To get a better sense of how well
our system performs, we also performed end-to-
end coreference resolution and manually assessed
a subset of the system output (22 randomly se-
lected drug labels with 249 coreference instances).
Of these 249, 181 were deemed correct, yielding a
precision of 0.73. The baseline method extracted
1439 instances, 56 of which were deemed cor-
rect, yielding a precision of 0.04. The precision
of our method is more in line with what has been
reported in the literature (Segura-Bedmar et al.,
2010; Nguyen et al., 2012). For i2b2-style eval-
uation using the unweighted average F
1
measure
over B-CUBED, MUC, and CEAF metrics, we
considered both exact and partial mention overlap.
These results, provided in Table 1, also indicate
that the system provides a clear improvement over
the baseline.
Metric Baseline OPTIMAL
With gold entity annotations
Unweighted F
1
Partial 0.55 0.77
Unweighted F
1
Exact 0.66 0.78
Precision 0.01 0.17
Recall 0.04 0.26
F
1
-measure 0.01 0.21
End-to-end coreference resolution
Precision 0.04 0.73
Table 1: Evaluation results on drug labels
50
We also assessed the effect of various resolution
strategies on results. These results are presented in
Table 2.
Strategy F
1
-measure
OPTIMAL 0.21
OPTIMAL - SIA 0.21
OPTIMAL - APPOS 0.15
OPTIMAL + DIBS 0.16 (0.39 recall)
Table 2: Effect of coreference strategies
Disregarding set/instance anaphora resolution
(SIA) does not appear to affect the results by
much; however, this is mostly due to the fact that
the ?instance? mentions are generally exemplifica-
tions of a particular drug class which also appear
in text. In the absence of set/instance anaphora
resolution, the system often defaults to these drug
class mentions, which were annotated more often
than not, unlike the ?instance? mentions. Take the
following example:
(9) Use of ZESTRIL with potassium-sparing
diuretics (e.g., spironolactone, eplerenone,
triamterene or amiloride) . . . may lead to sig-
nificant increases . . . if concomitant use of
these agents . . .
Without set-instance anaphora resolution, the sys-
tem links these agents to potassium-sparing di-
uretics, an annotated relation. With set-instance
anaphora resolution, the same expression is linked
to individual drug names (spironolactone, etc.) as
well as the the drug class, creating a number of
false positives, which, in effect, offsets the im-
provement provided by this strategy.
On the other hand, recognizing appositive con-
structions (APPOS) appears to have a larger im-
pact; however, it should be noted that this is mostly
because it helps us expand the antecedent mention
list in the case of set/instance anaphora. For in-
stance, in Example (9), this strategy allows us to
establish the link between the anaphora and the
drug class (diuretics), since the drug class and in-
dividual drug name (spironolactone) are identified
earlier as appositive. We can conclude that, in gen-
eral, set/instance anaphora benefits from recogni-
tion of appositive constructions.
Recognizing drug ingredient/brand name syn-
onymy (DIBS) improved the recall and hurt the
precision significantly, the overall effect being
negative. Since this non-anaphoric type of coref-
erence is strictly semantic in nature and resources
from which this type of semantic information can
be derived already exist (UMLS, among others), it
is perhaps not of utmost importance that a coref-
erence resolution system recognizes such corefer-
ence.
We additionally processed the DrugNerAR cor-
pus with our system. The optimal setting for
this corpus was disregarding the drug ingredi-
ent/brand name synonymy but using relative pro-
noun anaphora resolution, based on the discus-
sion in Segura-Bedmar et al. (2010). Somewhat to
our surprise, our system did not fare well on this
corpus. We extracted 524 chains, 327 of which
(out of 669) were annotated in the corpus, yield-
ing a precision of 0.71, recall of 0.56, and F
1
-
measure of 0.63. This is about 20% lower than
their reported results. When we used their base-
line method (explained earlier), we obtained simi-
larly lower scores (precision of 0.18, recall of 0.45,
F
1
-measure of 0.26, about 40% lower than their
reported results). In light of this apparent discrep-
ancy, which clearly warrants further investigation,
it is perhaps more sensible to focus on ?improve-
ment over baseline? (reported as 73% in their pa-
per and is 140% in our case).
We analyzed some of the annotations more
closely to get a better sense of the shortcomings
of the system. The majority of errors were due to
using linear distance as the salience score. For in-
stance, in the following example, they is linked to
ACE inhibitors due to proximity, whereas the true
antecedent is these reactions (itself an anaphor and
is presumably linked to another antecedent). It
could be possible to recover this link using prin-
ciples of Centering Theory (Grosz et al., 1995),
which suggests that subjects are more central than
objects and adjuncts in an utterance. Following
this principle, the subject (these reactions) would
be preferred to ACE inhibitors as the antecedent.
(10) In the same patients, these reactions were
avoided when ACE inhibitors were temporar-
ily withheld, but they reappeared upon inad-
vertent rechallenge.
Semantic (but not syntactic) coordination some-
times leads to number disagreement between the
anaphora and a true antecedent, as shown in Ex-
ample (11), leading to false negatives. In this ex-
ample, such diuretics refers to both ALDACTONE
51
and a second diuretic; however, we are unable to
identify the link between them and the number dis-
agreement between the anaphora and either of the
antecedents blocks a potential coreference relation
between these items.
(11) If, after five days, an adequate diuretic re-
sponse to ALDACTONE has not occurred,
a second diuretic that acts more proximally
in the renal tubule may be added to the reg-
imen. Because of the additive effect of AL-
DACTONE when administered concurrently
with such diuretics . . .
5 Conclusion
We presented a coreference resolution system en-
hanced based on insights from a dataset of FDA
drug package inserts. Sparse coreference annota-
tion in the dataset presented difficulties in evaluat-
ing the results; however, based on various eval-
uation strategies, the performance improvement
due to the enhancements seems evident. Our re-
sults show that recognizing coordination and ap-
positive constructions are particularly useful and
that non-anaphoric cases of coreference can be
identified using synonymy in semantic resources,
such as UMLS. However, whether this is a task
for a coreference resolution system or a concept
normalization system is debatable. We exper-
imented with using hierarchical domain knowl-
edge in UMLS (for example, the knowledge that
lisinopril ISA angiotensin converting enzyme in-
hibitor) to resolve some cases of sortal anaphora.
Even though we did not see an improvement due
to using this type of information on our dataset,
further work is needed to assess its usefulness.
While the enhancements were evaluated on drug
labels only, they are not specific to this type of
text. Their portability to different text types is
limited only by the accuracy of underlying tools,
such as parsers, for the text type of interest and
the availability of domain knowledge in the form
of relevant semantic types, groups, hypernyms
for the entity types under consideration. The re-
sults also indicate that a more rigorous application
of syntactic constraints in the spirit of Centering
Theory (Grosz et al., 1995) could be beneficial.
Event (or clausal) anaphora and anaphora indicat-
ing discourse deixis, while rarely annotated in our
dataset, appear to occur fairly often in biomedical
text. These types of anaphora are known to be par-
ticularly challenging, and we plan to investigate
them in future research, as well.
Acknowledgments
This work was supported by the intramural re-
search program at the U.S. National Library of
Medicine, National Institutes of Health.
References
Alan R. Aronson and Franc?ois-Michel Lang. 2010. An
overview of MetaMap: historical perspective and re-
cent advances. Journal of the American Medical In-
formatics Association (JAMIA), 17(3):229?236.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
James J. Cimino, Tiffani J. Bright, and Jianhua Li.
2007. Medication reconciliation using natural lan-
guage processing and controlled terminologies. In
Klaus A. Kuhn, James R. Warren, and Tze-Yun
Leong, editors, MedInfo, volume 129 of Studies in
Health Technology and Informatics, pages 679?683.
IOS Press.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation, pages 449?
454.
Dina Demner-Fushman, Wendy W. Chapman, and
Clem J. McDonald. 2009. What can natural lan-
guage processing do for clinical decision support?
Journal of Biomedical Informatics, 5(42):760?762.
Jon Duke, Jeff Friedlin, and Patrick Ryan. 2011. A
quantitative analysis of adverse events and ?over-
warning? in drug labeling. Archives of internal
medicine, 10(171):944?946.
Peter L. Elkin, John S. Carter, Manasi Nabar, Mark
Tuttle, Michael Lincoln, and Steven H. Brown.
2011. Drug knowledge expressed as computable se-
mantic triples. Studies in health technology and in-
formatics, (166):38?47.
Kin Wah Fung, Chiang S. Jao, and Dina Demner-
Fushman. 2013. Extracting drug indication infor-
mation from structured product labels using natural
language processing. JAMIA, 20(3):482?488.
Barbara J. Grosz, Scott Weinstein, and Aravind K.
Joshi. 1995. Centering: a framework for model-
ing the local coherence of discourse. Computational
Linguistics, 21(2):203?225.
52
Mar??a Herrero-Zazo, Isabel Segura-Bedmar, Paloma
Mart??nez, and Thierry Declerck. 2013. The DDI
corpus: An annotated corpus with pharmacological
substances and drug-drug interactions. Journal of
Biomedical Informatics, 46(5):914?920.
Halil Kilicoglu and Sabine Bergler. 2012. Biolog-
ical Event Composition. BMC Bioinformatics, 13
(Suppl 11):S7.
Halil Kilicoglu, Marcelo Fiszman, and Dina Demner-
Fushman. 2013. Interpreting consumer health ques-
tions: The role of anaphora and ellipsis. In Proceed-
ings of the 2013 Workshop on Biomedical Natural
Language Processing, pages 54?62.
Jin-Dong Kim, Ngan Nguyen, YueWang, Jun?ichi Tsu-
jii, Toshihisa Takagi, and Akinori Yonezawa. 2012.
The Genia Event and Protein Coreference tasks of
the BioNLP Shared Task 2011. BMC Bioinformat-
ics, 13(Suppl 11):S1.
Qi Li, Louise Deleger, Todd Lingren, Haijun Zhai,
Megan Kaiser, Laura Stoutenborough, Anil G.
Jegga, Kevin B. Cohen, and Imre Solti. 2013. Min-
ing FDA drug labels for medical conditions. BMC
medical informatics and decision making, 13(1):53.
Donald A. B. Lindberg, Betsy L. Humphreys, and
Alexa T. McCray. 1993. The Unified Medical Lan-
guage System. Methods of Information in Medicine,
32:281?291.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In In Proc. of HLT/EMNLP,
pages 25?32.
Alexa T. McCray, Anita Burgun, and Olivier Boden-
reider. 2001. Aggregating UMLS semantic types
for reducing conceptual complexity. Proceedings of
Medinfo, 10(pt 1):216?20.
Makoto Miwa, Paul Thompson, and Sophia Ana-
niadou. 2012. Boosting automatic event ex-
traction from the literature using domain adapta-
tion and coreference resolution. Bioinformatics,
28(13):1759?1765.
Ngan L. T. Nguyen, Jin-Dong Kim, Makoto Miwa,
Takuya Matsuzaki, and Junichi Tsujii. 2012. Im-
proving protein coreference resolution by simple se-
mantic classification. BMC Bioinformatics, 13:304.
Philip V. Ogren. 2010. Improving Syntactic Coor-
dination Resolution using Language Modeling. In
NAACL (Student Research Workshop), pages 1?6.
The Association for Computational Linguistics.
T.I. Oprea, S.K. Nielsen, O. Ursu, J.J. Yang,
O. Taboureau, S.L. Mathias, L. Kouskoumvekaki,
L.A. Sklar, and C.G. Bologa. 2011. Associat-
ing Drugs, Targets and Clinical Outcomes into an
Integrated Network Affords a New Platform for
Computer-Aided Drug Repurposing. Molecular in-
formatics, 2-3(30):100?111.
Thomas C. Rindflesch, Lorrie Tanabe, John N. We-
instein, and Lawrence Hunter. 2000. EDGAR:
Extraction of drugs, genes, and relations from the
biomedical literature. In Proceedings of Pacific
Symposium on Biocomputing, pages 514?525.
Isabel Segura-Bedmar, Mario Crespo, C?esar de Pablo-
S?anchez, and Paloma Mart??nez. 2010. Resolving
anaphoras for the extraction of drug-drug interac-
tions in pharmacological documents. BMC Bioin-
formatics, 11 (Suppl 2):S1.
J.C. Smith, J.C. Denny, Q. Chen, H. Nian, A. 3rd
Spickard, S.T. Rosenbloom, and R. A. Miller. 2011.
Lessons learned from developing a drug evidence
base to support pharmacovigilance. Applied clinical
informatics, 4(4):596?617.
?
Ozlem Uzuner, Imre Solti, and Eithon Cadag. 2010.
Extracting medication information from clinical
text. JAMIA, 17(5):514?518.
?
Ozlem Uzuner, Andrea Bodnari, Shuying Shen, Tyler
Forbush, John Pestian, and Brett R. South. 2012.
Evaluating the state of the art in coreference res-
olution for electronic medical records. JAMIA,
19(5):786?791.
Marc B. Vilain, John D. Burger, John S. Aberdeen,
Dennis Connolly, and Lynette Hirschman. 1995.
A model-theoretic coreference scoring scheme. In
MUC, pages 45?52.
Bonnie L. Webber. 1988. Discourse Deixis: Reference
to Discourse Segments. In ACL, pages 113?122.
Rong Xu and QuanQiu Wang. 2014. Large-scale com-
bining signals from both biomedical literature and
the FDA Adverse Event Reporting System (FAERS)
to improve post-marketing drug safety signal detec-
tion. BMC Bioinformatics, 15:17.
Katsumasa Yoshikawa, Sebastian Riedel, Tsutomu Hi-
rao, Masayuki Asahara, and Yuji Matsumoto. 2011.
Coreference Based Event-Argument Relation Ex-
traction on Biomedical Text. Journal of Biomedical
Semantics, 2 (Suppl 5):S6.
Jiaping Zheng, Wendy W. Chapman, Rebecca S. Crow-
ley, and Guergana K. Savova. 2011. Corefer-
ence resolution: A review of general methodologies
and applications in the clinical domain. Journal of
Biomedical Informatics, 44(6):1113?1122.
Qian Zhu, Robert R. Freimuth, Jyotishman Pathak,
Matthew J. Durski, and Christopher G. Chute. 2013.
Disambiguation of PharmGKB drug-disease rela-
tions with NDF-RT and SPL. Journal of Biomedical
Informatics, 46(4):690?696.
53
