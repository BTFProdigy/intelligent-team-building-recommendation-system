Utterance Classification in AutoTutor 
Andrew 
Olney 
Max 
Louwerse 
 Eric 
Matthews 
Johanna 
Marineau 
Heather 
Hite-Mitchell 
Arthur 
Graesser 
Institute for Intelligent Systems 
University of Memphis 
Memphis, TN 38152 
aolney@memphis.edu 
 
Abstract 
This paper describes classification of typed 
student utterances within AutoTutor, an intel-
ligent tutoring system.  Utterances are classi-
fied to one of 18 categories, including 16 
question categories.  The classifier presented 
uses part of speech tagging, cascaded finite 
state transducers, and simple disambiguation 
rules. Shallow NLP is well suited to the task: 
session log file analysis reveals significant 
classification of eleven question categories, 
frozen expressions, and assertions.  
1 Introduction 
AutoTutor is a domain-portable intelligent tutoring 
system (ITS) with current versions in the domains of 
physics and computer literacy (Graesser et al 1999; 
Olney et al 2002).   AutoTutor, like many other ITSs, is 
an intersection of applications, including tutoring, 
mixed-initiative dialogue, and question answering.  In 
each of these, utterance classification, particularly ques-
tion classification, plays a critical role.  
In tutoring, utterance classification can be used to 
track the student's level of understanding.  Contribution 
and question classifications can both play a role: contri-
butions may be compared to an expected answer 
(Graesser et al 2001) and questions may be scored by 
how "deep" they are.  For example, The PREG model 
(Otero and Graesser 2001) predicts under what circum-
stances students will ask "deep" questions, i.e. those that 
reveal a greater level of cognitive processing than who, 
what, when, or where questions.  A student who is only 
asking shallow questions, or no questions at all, is pre-
dicted by PREG to not have a situation-level under-
standing (van Dijk and Kintsch 1983) and thus to learn 
less and forget faster.  The key point is that different 
metrics for tracking student understanding are applica-
ble to questions and contributions.  Distinguishing them 
via classification is a first step to applying a metric. 
In mixed-initiative dialog systems, utterance classifi-
cation can be used to detect shifts in initiative.  For ex-
ample, a mixed-initiative system that asks, "Where 
would you like to travel", could respond to the question, 
"Where can I travel for $200?" (Allen 1999) by giving a 
list of cities.  In this example, the user is taking the ini-
tiative by requesting more information.  In order to re-
spond properly, the system must detect that the user has 
taken initiative before it can respond appropriately; oth-
erwise it might try to interpret the user's utterance as a 
travel destination.  In this sense, questions mark redirec-
tion of the dialogue, whereas contributions are continua-
tions of the dialogue.  In order for a user to redirect the 
dialogue and thus exercise initiative, a mixed-initiative 
system must be able to distinguish questions and contri-
butions. 
Question classification as early as Lehnert (1978) 
has been used as a basis for answering questions, a trend 
that continues today (Voorhees 2001).  A common fea-
ture of these question-answering systems is that they 
first determine the expected answer type implicit in the 
question.  For example, "How much does a pretzel cost" 
might be classified according to the answer type of 
MONEY or QUANTITY.  Knowledge of the expected an-
swer type can be used to narrow the search space for the 
answer, either online (Brill et al 2001) or in a database 
(Harabagiu et al 2000).  Accordingly, question answer-
ing calls for a finer discrimination of question types as 
opposed to only distinguishing questions from contribu-
tions. 
AutoTutor uses utterance classification to track stu-
dent progress, to determine initiative, and to answer 
questions. By virtue of being embedded in AutoTutor, 
the utterance classifier presented here has an unusual set 
of constraints, both practical and theoretical.  On the 
practical side, AutoTutor is a web-based application that 
performs in real time; thus utterance classification must 
also proceed in real time.  For that reason, the classifier 
uses a minimum of resources, including part of speech 
tagging (Brill 1995; Sekine and Grishman 1995) and 
cascaded finite state transducers defining the categories.  
Theoretically speaking, AutoTutor must also recognize 
questions in a meaningful way to both question answer-
ing and tutoring.  The question taxonomy utilized, that 
of Graesser et al(1992), is an extension of Lehnert's 
(1978) taxonomy for question answering and has been 
applied to human tutoring (Graesser et al 1992; 
Graesser and Person 1994).   
This paper outlines the utterance classifier and quan-
tifies its performance.  In particular, Section 2 presents 
AutoTutor.  Section 3 presents the utterance taxonomy.  
Section 4 describes the classifier algorithm.  Section 5 
delineates the training process and results.  Section 6 
presents evaluation of the classifier on real AutoTutor 
sessions.  Section 7 concludes the paper. 
2 AutoTutor 
AutoTutor is an ITS applicable to any content domain.  
Two distinct domain applications of AutoTutor are 
available on the Internet, for computer literacy and con-
ceptual physics.  The computer literacy AutoTutor, 
which has now been used in experimental evaluations 
by over 200 students, tutors students on core computer 
literacy topics covered in an introductory course, such 
as operating systems, the Internet, and hardware.  The 
topics covered by the physics AutoTutor are grounded 
in basic Newtonian mechanics and are of a similar in-
troductory nature.  It has been well documented that 
AutoTutor promotes learning gains in both versions 
(Person et al 2001). 
AutoTutor simulates the dialog patterns and peda-
gogical strategies of human tutors in a conversational 
interface that supports mixed-initiative dialog. AutoTu-
tor?s architecture is comprised of seven highly modular 
components: (1) an animated agent, (2) a curriculum 
script, (3) a speech act classifier,  (4) latent semantic 
analysis (LSA), (5) a dialog move generator, (6) a Dia-
log Advancer Network, and (7) a question-answering 
tool (Graesser et al 1998; Graesser et al 2001; 
Graesser et al 2001; Person et al 2000; Person et al 
2001; Wiemer-Hastings et al 1998).  
A tutoring session begins with a brief introduction 
from AutoTutor?s three-dimensional animated agent.  
AutoTutor then asks the student a question from one of 
topics in the curriculum script. The curriculum script 
contains lesson-specific tutor-initiated dialog, including 
important concepts, questions, cases, and problems 
(Graesser and Person 1994; Graesser et al 1995; 
McArthur et al 1990; Putnam 1987). The student sub-
mits a response to the question by typing and pressing 
the ?Submit? button. The student?s contribution is then 
segmented, parsed (Sekine and Grishman 1995) and 
sent through a rule-based utterance classifier.  The clas-
sification process makes use of only the contribution 
text and part-of-speech tag provided by the parser. 
Mixed-initiative dialog starts with utterance classifi-
cation and ends with dialog move generation, which can 
include question answering, repeating the question for 
the student, or just encouraging the student.  Concur-
rently, the LSA module evaluates the quality of the stu-
dent contributions, and in the tutor-initiative mode, the 
dialog move generator selects one or a combination of 
specific dialog moves that is both conversationally and 
pedagogically appropriate (Person et al2000; Person et 
al. 2001). The Dialog Advancer Network (DAN) is the 
intermediary of dialog move generation in all instances, 
using information from the speech act classifier and 
LSA to select the next dialog move type and appropriate 
discourse markers.  The dialog move generator selects 
the actual move.  There are twelve types of dialog 
move: Pump, Hint, Splice, Prompt, Prompt Response, 
Elaboration, Summary, and five forms of immediate 
short-feedback (Graesser and Person 1994; Graesser et 
al. 1995; Person and Graesser 1999).  
3 An utterance taxonomy 
The framework for utterance classification in Table 1 is 
familiar to taxonomies in the cognitive sciences 
(Graesser et al 1992; Graesser and Person 1994).  The 
most notable system within this framework is QUALM 
(Lehnert 1978), which utilizes twelve of the question 
categories.  The taxonomy can be divided into 3 distinct 
groups, questions, frozen expressions, and contribu-
tions.  Each of these will be discussed in turn. 
The conceptual basis of the question categories 
arises from the observation that the same question may 
be asked in different ways, e.g. "What happened?" and 
"How did this happen?"  Correspondingly, a single lexi-
cal stem for a question, like "What" can be polysemous, 
e.g. both in a definition category, "What is the definition 
of gravity?" and metacommunicative, "What did you 
say?"  Furthermore, implicit questions can arise in tutor-
ing via directives and some assertions, e.g. "Tell me 
about gravity" and "I don't know what gravity is."  In 
AutoTutor these information seeking utterances are 
classified to one of the 16 question categories. 
The emphases on queried concepts rather than ortho-
graphic forms make the categories listed in Table 1 bear 
a strong resemblance to speech acts.  Indeed, Graesser 
et al (1992) propose that the categories be distinguished 
in precisely the same way as speech acts, using seman-
tic, conceptual, and pragmatic criteria as opposed to 
syntactic and lexical criteria. Speech acts presumably 
transcend these surface criteria: it is not what is being 
said as what is done by the saying (Austin, 1962; Searle, 
1975). 
The close relation to speech acts underscores what a 
difficult task classifying conceptual questions can be. 
Jurafsky and Martin (2000) describe the problem of 
interpreting speech acts using pragmatic and semantic 
inference as AI-complete, i.e. impossible without creat-
ing a full artificial intelligence.  The alternative ex-
plored in this paper is cue or surface-based 
classification, using no context.  
It is particularly pertinent to the present discussion 
that the sixteen qualitative categories are employed in a 
quantitative classification process.  That is to say that 
for the present purposes of classification, a question 
must belong to one and only one category.  On the one 
hand this idealization is necessary to obtain easily ana-
lyzed performance data and to create a well-balanced 
training corpus.  On the other hand, it is not entirely 
accurate because some questions may be assigned to 
multiple categories, suggesting a polythetic coding 
scheme (Graesser et al 1992).  Inter-rater reliability is 
used in the current study as a benchmark to gauge this 
potential effect. 
Frozen expressions consist of metacognitive and 
metacommunicative utterances.  Metacognitive utter-
ances describe the cognitive state of the student, and 
they therefore require a different response than ques-
tions or assertions.  AutoTutor responds to metacogni-
tive utterances with canned expressions such as, "Why 
don't you give me what you know, and we'll take it from 
there."  Metacommunicative acts likewise refer to the 
dialogue between tutor and student, often calling for a 
repetition of the tutor's last utterance.  Two key points 
are worth noting: frozen expressions have a much 
smaller variability than questions or contributions, and 
frozen expressions may be followed by some content, 
making them more properly treated as questions.  For 
example, "I don't understand" is frozen, but "I don't un-
derstand gravity" is a more appropriately a question. 
Contributions in the taxonomy can be viewed as 
anything that is not frozen or a question; in fact, that is 
essentially how the classifier works.  Contributions in 
AutoTutor, either as responses to questions or un-
prompted, are tracked to evaluate student performance 
via LSA, forming the basis for feedback. 
4 Classifier Algorithm 
The present approach ignores the semantic and prag-
matic context of the questions, and utilizes surface fea-
tures to classify questions.  This shallow approach 
parallels work in question answering (Srihari and Li 
2000; Soubbotin and Soubbotin 2002; Moldovan et al
1999). Specifically, the classifier uses tagging provided 
by ApplePie (Sekine and Grishman 1995) followed by 
cascaded finite state transducers defining the categories.  
The finite state transducers are roughly described in 
Table 2. Every transducer is given a chance to match, 
and a disambiguation routine is applied at the end to 
select a single category.  
Category Example 
 
Questions 
Verification  
Disjunctive 
Concept Completion  
Feature Specification  
Quantification 
Definition  
Example 
Comparison  
Interpretation  
Causal Antecedent  
Causal Consequence  
Goal Orientation  
Instrumental/Procedural  
Enablement  
Expectational  
Judgmental 
 
 
 
Does the pumpkin land in his hands? 
Is the pumpkin accelerating or decelerating? 
Where will the pumpkin land? 
What are the components of the forces acting on the pumpkin? 
How far will the pumpkin travel? 
What is acceleration? 
What is an example of Newton's Third Law? 
What is the difference between speed and velocity? 
What is happening in this situation with the runner and pumpkin? 
What caused the pumpkin to fall? 
What happens when the runner speeds up? 
Why did you ignore air resistance? 
How do you calculate force? 
What principle allows you to ignore the vertical component of the force? 
Why doesn't the pumpkin land behind the runner?  
What do you think of my explanation? 
Frozen Expressions  
Metacognitive 
Metacommunicative 
I don't understand. 
Could you repeat that? 
Contribution The pumpkin will land in the runner's hands 
Table 1. AutoTutor?s utterance taxonomy. 
Immediately after tagging, transducers are applied to 
check for frozen expressions.  A frozen expression must 
match, and the utterance must be free of any nouns, i.e. 
not frozen+content, for the utterance to be classified as 
frozen.  Next the utterance is checked for question 
stems, e.g. WHAT, HOW, WHY, etc. and question 
mark punctuation.  If question stems are buried in the 
utterance, e.g. "I don't know what gravity is", a move-
ment rule transforms the utterance, placing the stem at 
the beginning.  Likewise if a question ends with a ques-
tion mark but has no stem, an AUX stem is placed at the 
beginning of the utterance.  In this way the same trans-
ducers can be applied to both direct and indirect ques-
tions.  At this stage, if the utterance does not possess a 
question stem and is not followed by a question mark, 
the utterance is classified as a contribution. 
Two sets of finite state transducers are applied to po-
tential questions, keyword transducers and syntactic 
pattern transducers.  Keyword transducers replace a set 
of keywords specific to a category with a symbol for 
that category.  This extra step simplifies the syntactic 
pattern transducers that look for the category symbol in 
their pattern.  The definition keyword transducer, for 
example, replaces "definition", "define", "meaning", 
"means", and "understanding" with "KEYDEF".  For 
most categories, the keyword list is quite extensive and 
exceeds the space limitations of Table 2.  Keyword 
transducers also add the category symbol to a list when 
they match; this list is used for disambiguation.  Syntac-
tic pattern transducers likewise match, putting a cate-
gory symbol on a separate disambiguation list. 
In the disambiguation routine, both lists are con-
sulted, and the first category symbol found on both lists 
determines the classification of the utterance.  Clearly 
Utterance Category Finite state transducer pattern 
Verification ^AUX 
Disjunctive ^AUX ... or 
Concept Completion ^(Who|What|When|Where) 
Feature Specification ^What ... keyword 
keyword 
Quantification ^What AUX ... keyword 
^How (ADJ|ADV) 
^MODAL you ... keyword 
Definition ^What AUX ... (keyword|a? (ADJ|ADV)* N 
^MODAL you ... keyword 
what a? (ADJ|ADV)* N BE 
Example ^AUX ... keyword 
^What AUX ... keyword 
Comparison ^What AUX ... keyword 
^How ... keyword 
^MODAL you ... keyword 
Interpretation keyword 
Causal Antecedent ^(Why|How) AUX ... (VBpast|keyword) 
^(WH|How) ... keyword 
Causal Consequence  
Goal Orientation ^(What|Why) AUX ART? (NP|SUBJPRO|keyword) 
^What ... keyword 
Instrumental/Procedural ^(WH|How) AUX ART? (N|PRO) 
^(WH|How) ... keyword 
^MODAL you ... keyword 
Enablement ^(WH|How) ... keyword 
Expectational ^Why AUX ... NEG 
Judgmental 
(you|your) ... keyword 
(should|keyword) (N|PRO) 
Frozen (no nouns) ^SUBJPRO ... keyword 
^VB ... keyword ... OBJPRO 
^AUX ... SUBJPRO ... keyword 
Contribution Everything else 
Table 2.  Finite state transducer patterns 
ordering of transducers affects which symbols are clos-
est to the beginning of the list.  Ordering is particularly 
relevant when considering categories like concept com-
pletion, which match more freely than other categories.  
Ordering gives rarer and stricter categories a chance to 
match first; this strategy is common in stemming (Paice 
1990).  
5 Training 
The classifier was built by hand in a cyclical process of 
inspecting questions, inducing rules, and testing the 
results.  The training data was derived from brainstorm-
ing sessions whose goal was to generate questions as 
lexically and syntactically distinct as possible.  Of the 
brainstormed questions, only when all five raters agreed 
on the category was a question used for training; this 
approach filtered out polythetic questions and left only 
archetypes. 
Intuitive analysis suggested that the majority of 
questions have at most a two-part pattern consisting of a 
syntactic template and/or a keyword identifiable for that 
category.  A trivial example is disjunction, whose syn-
tactic template is auxiliary-initial and corresponding 
keyword is ?or?.  Other categories were similarly de-
fined either by one or more patterns of initial constitu-
ents, or a keyword, or both.  To promote 
generalizability, extra care was given not to overfit the 
training data.  Specifically, keywords or syntactic pat-
terns were only used to define categories when they 
occurred more than once or were judged highly diagnos-
tic. 
 
 Expert 
Classifier present  ?present 
present tp fp 
?present fn tn 
Table 3.  Contingency Table. 
 
The results of the training process are shown in Ta-
ble 4.  Results from each category were compiled in 2 x 
2 contingency tables like Table 3, where tp stands for 
"true positive" and fn for "false negative". 
Recall, fallout, precision, and f-measure were calcu-
lated in the following way for each category: 
 
Recall  =  tp / ( tp + fn ) 
Fallout  =  fp / ( fp + tn ) 
Precision  =  tp / ( tp + fp ) 
 
F-measure = 2 * Recall * Precision  
 Recall + Precision 
 
Recall and fallout are often used in signal detection 
analysis to calculate a measure called d? (Green and 
Swets 1966).  Under this analysis, the performance of 
the classifier is significantly more favorable than under 
the F-measure, principally because the fallout, or false 
alarm rate, is so low.  Both in training and evaluation, 
however, the data violate assumptions of normality that 
d? requires.    
As explained in Section 3, a contribution classifica-
tion is the default when no other classification can be 
given.  As such, no training data was created for contri-
butions.  Likewise frozen expressions were judged to be 
essentially a closed class of phrases and do not require 
training.  Absence of training results for these categories 
is represented by double stars in Table 4. 
During the training process, the classifier was never 
tested on unseen data.  A number of factors it difficult to 
obtain questions suitable for testing purposes.  Brain-
stormed questions are an unreliable source of testing 
data because they are not randomly sampled.  In gen-
eral, corpora proved to be an unsatisfactory source of 
questions due to low inter-rater reliability and skewed 
distribution of categories.   
Low inter-rater reliability often could be traced to 
anaphora and pragmatic context.  For example, the 
question "Do you know what the concept of group cell 
is?" might license a definition or verification, depending 
on the common ground.  "Do you know what it is?" 
could equally license a number of categories, depending 
on the referent of "it".  Such questions are clearly be-
yond the scope of a classifier that does not use context.  
The skewed distribution of the question categories 
and their infrequency necessitates use of an extraction 
algorithm to locate them.  Simply looking for question 
marks is not enough: our estimates predict that raters 
would need to classify more than 5,000 questions ex-
tracted from the Wall Street Journal this way to get a 
mere 20 instances of the rarest types.  A bootstrapping 
approach using machine learning is a possible alterna-
tive that will be explored in the future (Abney 2002). 
Regardless of these difficulties, the strongest evalua-
tion results from using the classifier in a real world task, 
with real world data. 
6 Evaluation 
The classifier was used in AutoTutor sessions through-
out the year of 2002.  The log files from these sessions 
contained 9094 student utterances, each of which was 
classified by an expert.  The expert ratings were com-
pared to the classifier's ratings, forming a 2 x 2 contin-
gency table for each category as in Table 4.   
To expedite ratings, utterances extracted from the 
log files were split into two groups, contributions and 
non-contributions, according to their logged classifica-
tion.  Expert judges were assigned to a group and in-
structed to classify a set of utterances to one of the 18 
categories.  Though inter-rater reliability using the 
kappa statistic (Carletta 1996) may be calculated for 
each group, the distribution of categories in the contri-
bution group was highly skewed and warrants further 
discussion.   
Skewed categories bias the kappa statistic to low 
values even when the proportion of rater agreement is 
very high (Feinstein and Cicchetti 1990a; Feinstein and 
Cicchetti 1990b). In the contribution group, judges can 
expect to see mostly one category, contribution, 
whereas judges in the non-contribution group can ex-
pect to see the other 17 categories.  Expected agreement 
by chance for the contribution group was 98%.  Corre-
spondingly, inter-rater reliability using the kappa statis-
tic was low for the contribution group, .5 despite 99% 
proportion agreement, and high for non-contribution 
group, .93.   
However, the .93 inter-rater agreement can be ex-
tended to all of the utterance categories.  Due to classi-
fier error, the non-contribution group consisted of 38% 
contributions.  Thus the .93 agreement applies to contri-
butions in this group.  Equal proportion of agreement 
for contribution classifications in both groups, 99%, 
suggests that the differences in kappa solely reflect dif-
ferences in category skew across groups.  Under this 
analysis, dividing the utterances into two groups im-
proved the distribution of categories for the calculation 
of kappa (Feinstein and Cicchetti  1990b). 
Expert judges classified questions with a .93 kappa, 
which supports a monothetic classification scheme for 
this application.  In Section 3 the possibility was raised 
of a polythetic scheme for question classification, i.e. 
one in which two categories could be assigned to a 
given question.  If a polythetic scheme were truly neces-
sary, one would expect inter-rater reliability to suffer in 
a monothetic classification task.  High inter-rater reli-
ability on the monothetic classification task renders 
polythetic schemes superfluous for this application. 
The recall column for evaluation in Table 4 is gener-
ally much higher than corresponding cells in the preci-
sion column.  The disparity implies a high rate of false 
positives for each of the categories.  One possible ex-
planation is the reconstruction algorithm applied during 
classification.  It was observed that, particularly in the 
language of physics, student used question stems in ut-
terances that were not questions, e.g. ?The ball will land 
when ??  Such falsely reconstructed questions account 
for 40% of the questions detected by the classifier.  
Whether modifying the reconstruction algorithm would 
improve F-measure, i.e. improve precision without sac-
rificing recall, is a question for future research. 
The distribution of categories is highly skewed: 97% 
of the utterances were contributions, and example ques-
tions never occurred at all.  In addition to recall, fallout, 
precision, and F-measure, significance tests were calcu-
 Training Data AutoTutor Performance 
CATEGORY Recall Fallout Precision F-measure Recall Fallout Precision F-measure Likelihood Ratio 
Contribution ** ** ** ** 0.983 0.054 0.999 0.991 1508.260 
Frozen ** ** ** ** 0.899 0.002 0.849 0.873 978.810 
Concept  
Completion 0.844 0.035 0.761 0.800 0.857 0.003 0.444 0.585 235.800 
Interpretation 0.545 0.009 0.545 0.545 0.550 0.000 0.917 0.688 135.360 
Definition 0.667 0.002 0.941 0.780 0.424 0.001 0.583 0.491 131.770 
Verification 0.969 0.004 0.969 0.969 0.520 0.004 0.255 0.342 103.880 
Comparison 0.955 0.011 0.778 0.857 1.000 0.004 0.132 0.233 55.460 
Quantification 0.949 0.002 0.982 0.966 0.556 0.003 0.139 0.222 43.710 
Expecational 0.833 0.010 0.833 0.833 1.000 0.000 0.667 0.800 33.870 
Procedural 0.545 0.009 0.545 0.545 1.000 0.000 1.000 1.000 20.230 
Goal  
Orientation 0.926 0.006 0.893 0.909 1.000 0.001 0.143 0.250 14.490 
Judgmental 0.842 0.010 0.865 0.853 0.500 0.001 0.167 0.250 12.050 
Disjunction 0.926 0.000 1.000 0.962 0.333 0.000 0.250 0.286 11.910 
Causal  
Antecedent 0.667 0.017 0.667 0.667 0.200 0.001 0.083 0.118 8.350* 
Feature  
Specification 0.824 0.006 0.824 0.824 0.000 0.000 0.000 0.000 0.000* 
Enablement 0.875 0.006 0.903 0.889 0.000 0.000 0.000 0.000 0.000* 
Causal  
Consequent 0.811 0.008 0.882 0.845 0.000 0.000 0.000 0.000 0.000* 
Example 0.950 0.008 0.826 0.884 ** ** ** ** ** 
Table 4. Training data and AutoTutor results. 
lated for each category's contingency table to insure that 
the cells were statistically significant.  Since most of the 
categories had at least one cell with an expected value 
of less than 1, Fisher's exact test is more appropriate for 
significance testing than likelihood ratios or chi-square 
(Pedersen 1996).  Those categories that are not signifi-
cant are starred; all other categories are significant, p < 
.001. 
Though not appropriate for hypothesis testing in this 
instance, likelihood ratios provide a comparison of clas-
sifier performance across categories.  Likelihood ratios 
are particularly useful when comparing common and 
rare events (Dunning 1993; Plaunt and Norgard 1998), 
making them natural here given the rareness of most 
question categories and the frequency of contributions.  
The likelihood ratios in the rightmost column of Table 4 
are on a natural logarithmic scale, -2ln?, so procedural 
at e . 5 x 20.23 = 24711 is more likely than goal orientation, 
at e . 5 x 14.49 = 1401, with respect to the base rate, or null 
hypothesis. 
To judge overall performance on the AutoTutor ses-
sions, an average weighted F-measure may be calcu-
lated by summing the products of all category F-
measures with their frequencies: 
 
? +??= N fntpmeasureFFavg  
 
The average weighted F-measure reflects real world 
performance since accuracy on frequently occurring 
classes is weighted more.  The average weighted F-
measure for the evaluation data is .98, mostly due to the 
great frequency of contributions (.97 of all utterances) 
and the high associated F-measure.  Without weighting, 
the average F-measure for the significant cells is .54. 
With respect to the three applications mentioned, i) 
tracking student understanding, ii) mixed-initiative dia-
logue, and iii) questions answering, the classifier is do-
ing extremely well on the first two and adequately on 
the last.  The first two applications for the most part 
require distinguishing questions from contributions, 
which the classifier does extremely well, F-measure = 
.99.  Question answering, on the other hand, can benefit 
from more precise identification of the question type, 
and the average unweighted F-measure for the signifi-
cant questions is .48. 
7 Conclusion 
One of the objectives of this work was to see how well a 
classifier could perform with a minimum of resources.  
Using no context and only surface features, the classi-
fier performed with an average weighted F-measure of 
.98 on real world data. 
However, the question remains how performance 
will fare as rare questions become more frequent.  Scaf-
folding student questions has become a hot topic re-
cently (Graesser et al 2003).  In a system that greatly 
promotes question-asking, the weighted average of .97 
will tend to drift closer to the unweighted average of 
.54.  Thus there is clearly more work to be done. 
Future directions include using bootstrapping meth-
ods and statistical techniques on tutoring corpora and 
using context to disambiguate question classification. 
8 Acknowledgements 
This research was supported by the Office of Naval Re-
search (N00014-00-1-0600) and the National Science 
Foundation (SBR 9720314 and REC 0106965).  Any 
opinions, findings, and conclusions or recommendations 
expressed in this material are those of the authors and 
do not necessarily reflect the views of ONR or NSF. 
References 
 
Abney, Steven. 2002. Bootstrapping. In Proceedings of 
the 40th Annual Meeting of the Association for Com-
putational Linguistics, 360-367. 
Allen, J.F. 1999. Mixed Initiative Interaction.  Proc. 
IEEE Intelligent Systems 14(6).  
Austin, John. 1962. How to do things with words. Har-
vard University Press, Cambridge, MA. 
Brill, Eric. 1995.  Transformation-based error-driven 
learning and natural language processing: a case study 
in part-of-speech tagging.  Computational Linguistics, 
21(4), 543-566. 
Brill, Eric, J. Lin, M. Banko, S. Dumais, and  A. Ng. 
2001.  Data-intensive question answering.  Proceed-
ings of the 10th Annual Text Retrieval Conference 
(TREC-10).  
Carletta, J.  1996.  Assessing agreement on classification 
tasks: the kappa statistic.  Computational Linguistics,  
22(2), 249-254. 
Dunning, Ted.  1993.  Accurate methods for the statistics 
of surprise and coincidence.  Computational Linguis-
tics 19, 61-74.  
Feinstein, Alvan R. and Domenic V. Cicchetti.  1990a.  
High agreement but low kappa: the problems of two 
paradoxes.  Journal of Clinical Epidemiology, 43(6), 
543-549. 
Feinstein, Alvan R. and Domenic V. Cicchetti.  1990b.  
High agreement but low kappa: II. resolving the para-
doxes.  Journal of Clinical Epidemiology, 43(6), 551-
558. 
Graesser, Arthur, John Burger, Jack Carroll, Albert Cor-
bett, Lisa Ferro, Douglas Gordon, Warren Greiff, 
Sanda Harabagiu, Kay Howell, Henry Kelly, Diane 
Litman, Max Louwerse, Allison Moore, Adrian Pell, 
John Prange, Ellen Voorhees, and Wayne Ward.  
2003.  Question generation and answering systems, 
R&D for technology-enabled learning systems: re-
search roadmap.  Unpublished manuscript.   
Graesser, Arthur, Natalie Person, and John Huber. 1992. 
Mechanisms that generate questions. In T. Lauer, E. 
Peacock, and A. Graesser (Eds), Questions and infor-
mation systems. Earlbaum, Hillsdale, NJ. 
Graesser, Arthur and Natalie Person. 1994. Question ask-
ing during tutoring.  American Educational Research 
Journal, 31(1), 104-137. 
Graesser, Arthur, Natalie Person, and J.P. Magliano.  
1995.  Collaborative dialog patterns in naturalistic 
one-on-one tutoring.  Applied Cognitive Psychology, 
9, 359-387.   
Graesser, Arthur, Kurt van Lehn, Carolyn Rose, Pamela 
Jordan, and Derek Harter.  2001.  Intelligent tutoring 
systems with conversational dialogue.  AI Magazine 
22(4), 39-52. 
Graesser, Arthur, Peter Wiemer-Hastings, K. Wiemer-
Hastings, Roger Kreuz, and the TRG.  1999. AutoTu-
tor: A simulation of a human tutor.  Journal of Cogni-
tive Systems Research 1, 35-51. 
Green, David and John Swets.  1966.  Signal detection 
theory and psychophysics.  John Wiley, New York. 
Harabagiu, Sanda, D. Moldovan, M. Pasca, R. Mihalcea, 
M. Surdeanu, R. Bunescu, R. Girju, V. Rus, and P. 
Morarescu. 2000. FALCON: Boosting knowledge for 
answer engines.  In Proceedings of the 9th Text Re-
trieval Conference (TREC-9). 
Jurafsky, Daniel and James Martin. 2000. Speech and 
language processing. Prentice Hall, NJ. 
Lehnert, Wendy. 1978. The Process of Question Answer-
ing. Lawrence Erlbaum Associates, Hillsdale, NJ. 
McArthur, D., C. Stasz, and M. Zmuidzinas. 1990. Tu-
toring techniques in algebra. Cognition and Instruc-
tion, 7, 197-244. 
Moldovan, Dan, Sanda Harabagiu, Marius Pasca, Rada 
Mihalcea, Richard Goodrum, Roxana Girju, and 
Vaslie Rus.  1999.  Lasso: a tool for surfing the an-
swer net  Proceedings of the 8th Annual Text Retrieval 
Conference (TREC-8), 65-73. 
Olney, Andrew, Natalie Person, Max Louwerse, and Ar-
thur Graesser.  2002. AutoTutor: a conversational tu-
toring environment.  In Proceedings of the 40th 
Annual Meeting of the Association for Computational 
Linguistics, Demonstration Abstracts, 108-109. 
Otero, J. and Arthur  Graesser.  2001.  PREG: Elements 
of a model of question asking.  Cognition & Instruc-
tion 19, 143-175. 
Paice, C.D. 1990. Another stemmer. SIGIR Forum 24 (3), 
56-61. 
Pedersen, Ted.  1996.  Fishing for exactness.  In Proceed-
ings of the South-Central SAS Users Group Confer-
ence, Austin, TX. 
Person, Natalie and Arthur Graesser.  1999.  Evolution 
of discourse in cross-age tutoring.  In A.M. 
O?Donnell and A. King (Eds.), Cognitive perspec-
tives on peer learning (pp. 69-86). Erlbaum, Mah-
wah, NJ. 
Person, Natalie, Arthur Graesser, L. Bautista, E.C. 
Mathews, and the Tutoring Research Group 2001.  
Evaluating student learning gains in two versions of 
AutoTutor. In J. D. Moore, C. L. Redfield, and W. L. 
Johnson (Eds.) Artificial intelligence in education: 
AI-ED in the wired and wireless future (pp. 286-
293). IOS Press, Amsterdam. 
Person, Natalie, Arthur Graesser, Derek Harter, E. C. 
Mathews, and the Tutoring Research Group (2000). 
Dialog move generation and conversation manage-
ment in AutoTutor.  Proceedings for the AAAI Fall 
Symposium  Series: Building Dialogue Systems for 
Tutorial Applications. Falmouth, Massachusetts. 
Plaunt, Christian and Barbara Norgard.  1998. An asso-
ciation-based method for automatic indexing with a 
controlled vocabulary.  Journal of the American Soci-
ety of Information Science, 49(10), 888-902. 
Putnam, R. T. 1987. Structuring and adjusting content 
for students: A study of  live and simulated tutoring 
of addition. American Educational Research Jour-
nal, 24, 13-48. 
Searle, John. 1975. A taxonomy of illocutionary acts. In 
K. Gunderson, (Ed.), Language, mind, and knowl-
edge. University of Minnesota Press, Minneapolis, 
MN. 
Sekine, S. and R. Grishman.  1995.  A corpus-based 
probabilistic grammar with only two nonterminals.  
Fourth International Workshop on Parsing Technol-
ogy. 
Soubbotin, M. M., and S. M. Soubbotin. 2002.  Patterns 
of potential answer expressions as clues to the right 
answers.  Proceedings of the 10th Annual Text Re-
trieval Conference (TREC-10).  
Srihari, Rohini and Wei  Li. 2000. A question answering 
system supported by information extraction.  Pro-
ceedings of the 6th Applied Natural Language Proc-
essing Conference (ANLP-2000), 166-172. 
Van Dijk, T. A., and W. Kintsch. 1983. Strategies of dis-
course comprehension. New York: Academic. 
Voorhees, Ellen.  2001.  Overview of the TREC 2001 
question answering track.  Proceedings of the 10th 
Annual Text Retrieval Conference (TREC-10), 400-
410. 
Proceedings of the 2nd Workshop on Building Educational Applications Using NLP,
pages 69?76, Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Evaluating State-of-the-Art Treebank-style Parsers for
Coh-Metrix and Other Learning Technology Environments
Christian F. Hempelmann, Vasile Rus, Arthur C. Graesser, and Danielle S. McNamara
Institute for Intelligent Systems
Departments of Computer Science and Psychology
The University of Memphis
Memphis, TN 38120, USA
{chmplmnn, vrus, a-graesser, dsmcnamr}@memphis.edu
Abstract
This paper evaluates a series of freely
available, state-of-the-art parsers on a
standard benchmark as well as with
respect to a set of data relevant for
measuring text cohesion. We outline
advantages and disadvantages of exist-
ing technologies and make recommen-
dations. Our performance report uses
traditional measures based on a gold
standard as well as novel dimensions
for parsing evaluation. To our knowl-
edge this is the first attempt to eval-
uate parsers accross genres and grade
levels for the implementation in learn-
ing technology.
1 Introduction
The task of syntactic parsing is valuable to
most natural language understanding applica-
tions, e.g., anaphora resolution, machine trans-
lation, or question answering. Syntactic parsing
in its most general definition may be viewed as
discovering the underlying syntactic structure of
a sentence. The specificities include the types
of elements and relations that are retrieved by
the parsing process and the way in which they
are represented. For example, Treebank-style
parsers retrieve a bracketed form that encodes
a hierarchical organization (tree) of smaller el-
ements (called phrases), while Grammatical-
Relations(GR)-style parsers explicitly output re-
lations together with elements involved in the
relation (subj(John,walk)).
The present paper presents an evaluation of
parsers for the Coh-Metrix project (Graesser et
al., 2004) at the Institute for Intelligent Systems
of the University of Memphis. Coh-Metrix is a
text-processing tool that provides new methods
of automatically assessing text cohesion, read-
ability, and difficulty. In its present form, v1.1,
few cohesion measures are based on syntactic
information, but its next incarnation, v2.0, will
depend more heavily on hierarchical syntactic
information. We are developing these measures.
Thus, our current goal is to provide the most
reliable parser output available for them, while
still being able to process larger texts in real
time. The usual trade-off between accuracy and
speed has to be taken into account.
In the first part of the evaluation, we adopt
a constituent-based approach for evaluation, as
the output parses are all derived in one way or
another from the same data and generate simi-
lar, bracketed output. The major goal is to con-
sistently evaluate the freely available state-of-
the-art parsers on a standard data set and across
genre on corpora typical for learning technology
environments. We report parsers? competitive-
ness along an array of dimensions including per-
formance, robustness, tagging facility, stability,
and length of input they can handle.
Next, we briefly address particular types of
misparses and mistags in their relation to mea-
sures planned for Coh-Metrix 2.0 and assumed
to be typical for learning technology applica-
tions. Coh-Metrix 2.0 measures that centrally
rely on good parses include:
causal and intentional cohesion, for which the
main verb and its subject must be identified;
69
anaphora resolution, for which the syntactic re-
lations of pronoun and referent must be identi-
fied;
temporal cohesion, for which the main verb and
its tense/aspect must be identified.
These measures require complex algorithms
operating on the cleanest possible sentence
parse, as a faulty parse will lead to a cascad-
ing error effect.
1.1 Parser Types
While the purpose of this work is not to propose
a taxonomy of all available parsers, we consider
it necessary to offer a brief overview of the var-
ious parser dimensions. Parsers can be classi-
fied according to their general approach (hand-
built-grammar-based versus statistical), the way
rules in parses are built (selective vs. genera-
tive), the parsing algorithm they use (LR, chart
parser, etc.), type of grammar (unification-based
grammars, context-free grammars, lexicalized
context-free grammars, etc.), the representation
of the output (bracketed, list of relations, etc.),
and the type of output itself (phrases vs gram-
matical relations). Of particular interest to our
work are Treebank-style parsers, i.e., parsers
producing an output conforming to the Penn
Treebank (PTB) annotation guidelines. The
PTB project defined a tag set and bracketed
form to represent syntactic trees that became a
standard for parsers developed/trained on PTB.
It also produced a treebank, a collection of hand-
annotated texts with syntactic information.
Given the large number of dimensions along
which parsers can be distinguished, an evalua-
tion framework that would provide both parser-
specific (to understand the strength of differ-
ent technologies) and parser-independent (to be
able to compare different parsers) performance
figures is desirable and commonly used in the
literature.
1.2 General Parser Evaluation Methods
Evaluation methods can be broadly divided
into non-corpus- and corpus-based methods
with the latter subdivided into unannotated
and annotated corpus-based methods (Carroll
et al, 1999). The non-corpus method sim-
ply lists linguistic constructions covered by the
parser/grammar. It is well-suited for hand-
built grammars because during the construction
phase the covered cases can be recorded. How-
ever, it has problems with capturing complex-
ities occuring from the interaction of covered
cases.
The most widely used corpus-based eval-
uation methods are: (1) the constituent-
based (phrase structure) method, and (2) the
dependency/GR-based method. The former has
its roots in the Grammar Evaluation Interest
Group (GEIG) scheme (Grishman et al, 1992)
developed to compare parsers with different un-
derlying grammatical formalisms. It promoted
the use of phrase-structure bracketed informa-
tion and defined Precision, Recall, and Cross-
ing Brackets measures. The GEIG measures
were extended later to constituent information
(bracketing information plus label) and have
since become the standard for reporting auto-
mated syntactic parsing performance. Among
the advantages of constituent-based evaluation
are generality (less parser specificity) and fine
grain size of the measures. On the other hand,
the measures of the method are weaker than ex-
act sentence measures (full identity), and it is
not clear if they properly measure how well a
parser identifies the true structure of a sentence.
Many phrase boundary mismatches spawn from
differences between parsers/grammars and cor-
pus annotation schemes (Lin, 1995). Usually,
treebanks are constructed with respect to infor-
mal guidelines. Annotators often interpret them
differently leading to a large number of different
structural configurations.
There are two major approaches to evaluate
parsers using the constituent-based method. On
the one hand, there is the expert-only approach
in which an expert looks at the output of a
parser, counts errors, and reports different mea-
sures. We use a variant of this approach for
the directed parser evaluation (see next section).
Using a gold standard, on the other hand, is a
method that can be automated to a higher de-
gree. It replaces the counting part of the former
method with a software system that compares
the output of the parser to the gold standard,
70
highly accurate data, manually parsed ? or au-
tomatically parsed and manually corrected ? by
human experts. The latter approach is more
useful for scaling up evaluations to large collec-
tions of data while the expert-only approach is
more flexible, allowing for evaluation of parsers
from new perspectives and with a view to spe-
cial applications, e.g., in learning technology en-
vironments.
In the first part of this work we use the gold
standard approach for parser evaluation. The
evaluation is done from two different points of
view. First, we offer a uniform evaluation for the
parsers on section 23 from the Wall Street Jour-
nal (WSJ) section of PTB, the community norm
for reporting parser performance. The goal of
this first evaluation is to offer a good estimation
of the parsers when evaluated in identical en-
vironments (same configuration parameters for
the evaluator software). We also observe the fol-
lowing features which are extremely important
for using the parsers in large-scale text process-
ing and to embed them as components in larger
systems.
Self-tagging: whether or not the parser does tag-
ging itself. It is advantageous to take in raw text
since it eliminates the need for extra modules.
Performance: if the performance is in the mid
and upper 80th percentiles.
Long sentences: the ability of the parser to han-
dle sentences longer than 40 words.
Robustness: relates to the property of a parser
to handle any type of input sentence and return
a reasonable output for it and not an empty line
or some other useless output.
Second, we evaluate the parsers on narrative
and expository texts to study their performance
across the two genres. This second evaluation
step will provide additional important results
for learning technology projects. We use evalb
(http://nlp.cs.nyu.edu/evalb/) to evaluate the
bracketing performance of the output of a parser
against a gold standard. The software evaluator
reports numerous measures of which we only re-
port the two most important: labelled precision
(LR), labelled recall (LR) which are discussed in
more detail below.
1.3 Directed Parser Evaluation Method
For the third step of this evaluation we looked
for specific problems that will affect Coh-Metrix
2.0, and presumably learning technology appli-
cations in general, with a view to amending
them by postprocessing the parser output. The
following four classes of problems in a sentence?s
parse were distinguished:
None: The parse is generally correct, unambigu-
ous, poses no problem for Coh-Metrix 2.0.
One: There was one minor problem, e.g., a mis-
labeled terminal or a wrong scope of an adver-
bial or prepositional phrase (wrong attachment
site) that did not affect the overall parse of the
sentence, which is therefore still usable for Coh-
Metrix 2.0 measures.
Two: There were two or three problems of the
type one, or a problem with the tree structure
that affected the overall parse of the sentence,
but not in a fatal manner, e.g., a wrong phrase
boundary, or a mislabelled higher constituent.
Three: There were two or more problems of the
type two, or two or more of the type one as
well as one or more of the type two, or another
fundamental problem that made the parse of the
sentence completely useless, unintelligible, e.g.,
an omitted sentence or a sentence split into two,
because a sentence boundary was misidentified.
2 Evaluated Parsers
2.1 Apple Pie
Apple Pie (AP) (Sekine and Grishman, 1995)
extracts a grammar from PTB v.2 in which S
and NP are the only true non-terminals (the
others are included into the right-hand side of
S and NP rules). The rules extracted from the
PTB have S or NP on the left-hand side and a
flat structure on the right-hand side, for instance
S ? NP VBX JJ. Each such rule has the most
common structure in the PTB associated with
it, and if the parser uses the rule it will gener-
ate its corresponding structure. The parser is
a chart parser and factors grammar rules with
common prefixes to reduce the number of active
nodes. Although the underlying model of the
parser is simple, it can?t handle sentences over
40 words due to the large variety of linguistic
71
constructs in the PTB.
2.2 Charniak?s Parser
Charniak presents a parser (CP) based on prob-
abilities gathered from the WSJ part of the PTB
(Charniak, 1997). It extracts the grammar and
probabilities and with a standard context-free
chart-parsing mechanism generates a set of pos-
sible parses for each sentence retaining the one
with the highest probability (probabilities are
not computed for all possible parses). The prob-
abilities of an entire tree are computed bottom-
up. In (Charniak, 2000), he proposes a gen-
erative model based on a Markov-grammar. It
uses a standard bottom-up, best-first probabilis-
tic parser to first generate possible parses before
ranking them with a probabilistic model.
2.3 Collins?s (Bikel?s) Parser
Collins?s statistical parser (CBP; (Collins,
1997)), improved by Bikel (Bikel, 2004), is based
on the probabilities between head-words in parse
trees. It explicitly represents the parse proba-
bilities in terms of basic syntactic relationships
of these lexical heads. Collins defines a map-
ping from parse trees to sets of dependencies,
on which he defines his statistical model. A
set of rules defines a head-child for each node
in the tree. The lexical head of the head-
child of each node becomes the lexical head of
the parent node. Associated with each node is
a set of dependencies derived in the following
way. For each non-head child, a dependency is
added to the set where the dependency is identi-
fied by a triplet consisting of the non-head-child
non-terminal, the parent non-terminal, and the
head-child non-terminal. The parser is a CYK-
style dynamic programming chart parser.
2.4 Stanford Parser
The Stanford Parser (SP) is an unlexical-
ized parser that rivals state-of-the-art lexical-
ized ones (Klein and Manning, 2003). It
uses a context-free grammar with state splits.
The parsing algorithm is simpler, the grammar
smaller and fewer parameters are needed for the
estimation. It uses a CKY chart parser which
exhaustively generates all possible parses for a
sentence before it selects the highest probabil-
ity tree. Here we used the default lexicalized
version.
3 Experiments and Results
3.1 Text Corpus
We performed experiments on three data sets.
First, we chose the norm for large scale parser
evaluation, the 2416 sentences of WSJ section
23. Since parsers have different parameters that
can be tuned leading to (slightly) different re-
sults we first report performance values on the
standard data set and then use same parameter
settings on the second data set for more reliable
comparison.
The second experiment is on a set of three nar-
rative and four expository texts. The gold stan-
dard for this second data set was built manually
by the authors starting from CP?s as well as SP?s
output on those texts. The four texts used ini-
tially are two expository and two narrative texts
of reasonable length for detailed evaluation:
The Effects of Heat (SRA Real Science Grade 2
Elementary Science): expository; 52 sentences,
392 words: 7.53 words/sentence;
The Needs of Plants (McGraw-Hill Science):
expository; 46 sentences, 458 words: 9.96
words/sentence;
Orlando (Addison Wesley Phonics Take-Home
Reader Grade 2): narrative; 65 sentences, 446
words: 6.86 words/sentence;
Moving (McGraw-Hill Reading - TerraNova Test
Preparation and Practice - Teachers Edition
Grade 3): narrative, 33 sentences, 433 words:
13.12 words/sentence.
An additional set of three texts was cho-
sen from the Touchstone Applied Science As-
sociates, Inc., (TASA) corpus with an average
sentence length of 13.06 (overall TASA average)
or higher.
Barron17: expository; DRP=75.14 (college
grade); 13 sentences, 288 words: 22.15
words/sentence;
Betty03: narrative; DRP=56.92 (5th grade); 14
sentences, 255 words: 18.21 words/sentence;
Olga91: expository; DRP=74.22 (college grade);
12 sentences, 311 words: 25.92 words/sentence.
72
We also tested all four parsers for speed on a
corpus of four texts chosen randomly from the
Metametrix corpus of school text books, across
high and low grade levels and across narrative
and science texts (see Section 3.2.2).
G4: 4th grade narrative text, 1,500 sentences,
18,835 words: 12.56 words/sentence;
G6: 6th grade science text, 1,500 sentences,
18,237 words: 12.16 words/sentence;
G11: 11th grade narrative text, 1,558 sentences,
18,583 words: 11.93 words/sentence;
G12: 12th grade science text, 1,520 sentences,
25,098 words: 16.51 words/sentence.
3.2 General Parser Evaluation Results
3.2.1 Accuracy
The parameters file we used for evalb was
the standard one that comes with the package.
Some parsers are not robust, meaning that for
some input they do not output anything, leading
to empty lines that are not handled by the evalu-
ator. Those parses had to be ?aligned? with the
gold standard files so that empty lines are elim-
inated from the output file together with their
peers in the corresponding gold standard files.
In Table 1 we report the performance values
on Section 23 of WSJ. Table 2 shows the results
for our own corpus. The table gives the average
values of two test runs, one against the SP-based
gold standard, the other against the CP-based
gold standard, to counterbalance the bias of the
standards. Note that CP and SP possibly still
score high because of this bias. However, CBP
is clearly a contender despite the bias, while AP
is not.1 The reported metrics are Labelled Pre-
cision (LP) and Labelled Recall (LR). Let us de-
note by a the number of correct phrases in the
output from a parser for a sentence, by b the
number of incorrect phrases in the output and
by c the number of phrases in the gold standard
for the same sentence. LP is defined as a/(a+b)
and LR is defined as a/c. A summary of the
other dimensions of the evaluation is offered in
Table 3. A stability dimension is not reported
1AP?s performance is reported for sentences < 40
words in length, 2,250 out of 2,416. SP is also not ro-
bust enough and the performance reported is only on
2,094 out of 2,416 sentences in section 23 of WSJ.
because we were not able to find a bullet-proof
parser so far, but we must recognize that some
parsers are significantly more stable than oth-
ers, namely CP and CBP. In terms of resources
needed, the parsers are comparable, except for
AP which uses less memory and processing time.
The LP/LR of AP is significantly lower, partly
due to its outputting partial trees for longer sen-
tences. Overall, CP offers the best performance.
Note in Table 1 that CP?s tagging accuracy is
worst among the three top parsers but still de-
livers best overall parsing results. This means
that its parsing-only performance is slighstly
better than the numbers in the table indicate.
The numbers actually represent the tagging and
parsing accuracy of the tested parsing systems.
Nevertheless, this is what we would most likely
want to know since one would prefer to input
raw text as opposed to tagged text. If more
finely grained comparisons of only the parsing
aspects of the parsers are required, perfect tags
extracted from PTB must be provided to mea-
sure performance.
Table 4 shows average measures for each of
the parsers on the PTB and seven expository
and narrative texts in the second column and
for expository and narrative in the fourth col-
umn. The third and fifth columns contain stan-
dard deviations for the previous columns, re-
spectively. Here too, CP shows the best result.
3.2.2 Speed
All parsers ran on the same Linux Debian ma-
chine: P4 at 3.4GHz with 1.0GB of RAM.2 AP?s
and SP?s high speeds can be explained to a large
degree by their skipping longer sentences, the
very ones that lead to the longer times for the
other two candidates. Taking this into account,
SP is clearly the fastest, but the large range of
processing times need to be heeded.
3.3 Directed Parser Evaluation Results
This section reports the results of expert rating
of texts for specific problems (see Section 1.3).
The best results are produced by CP with an av-
erage of 88.69% output useable for Coh-Metrix
2.0 (Table 6). CP also produces good output
2Some of the parsers also run under Windows.
73
Table 1: Accuracy of Parsers.
Parser Performance(LP/LR/Tagging - %)
WSJ 23 Expository Narrative
Applie Pie 43.71/44.29/90.26 41.63/42.70 42.84/43.84
Charniak?s 84.35/88.28/92.58 91.91/93.94 93.74/96.18
Collins/Bikel?s 84.97/87.30/93.24 82.08/85.35 67.75/85.19
Stanford 84.41/87.00/95.05 75.38/85.12 62.65/87.56
Table 2: Performance of parsers on the narrative and expository text (average against CP-based
and SP-based gold standard).
File Performance (LR/LP - %)
AP CP CBP SP
Heat 48.25/47.59 91.96/93.77 92.47/94.14 92.44/91.85
Plants 41.85/45.89 85.34/88.02 78.24/88.45 81.00/85.62
Orlando 45.82/49.03 85.83/91.88 65.87/93.97 57.75/90.72
Moving 37.77/41.45 88.93/92.74 53.94/91.68 76.56/84.97
Barron17 43.22/42.95 89.74/91.32 80.49/89.32 87.22/86.31
Betty03 46.53/44.67 90.77/90.74 87.95/85.21 74.53/80.91
Olga91 32.29/32.69 77.65/80.04 61.61/75.43 61.65/70.60
Table 3: Evaluation of Parsers with Respect to the Criteria Listed at the Top of Each Column.
Parser Self-tagging Performance Long-sentences Robustness
AP Yes No No No
CP Yes Yes Yes Yes
CBP Yes Yes Yes Yes
SP Yes Yes No No
Table 4: Average Performance of Parsers.
Parser Ave. (LR/LP - %) S.D. (%) Ave. on S.D. on
Exp+Nar (LR/LP - %) Exp+Nar (%)
AP 42.73/43.61 1.04/0.82 42.24/43.46 5.59/5.41
CP 90.00/92.80 4.98/4.07 87.17/89.79 4.85/4.66
CBP 78.27/85.95 9.22/1.17 74.36/88.31 14.24/6.51
SP 74.14/86.56 10.93/1.28 75.88/84.42 12.66/7.11
74
Table 5: Parser Speed in Seconds.
G4 G6 G11 G12
#sent 619 3336 4976 2215
AP 144 89 144 242
CP 647 499 784 1406
CBP 485 1947 1418 1126
SP 449 391 724 651
Ave. 431 732 768 856
most consistently at a standard deviation over
the seven texts of 8.86%. The other three candi-
dates are clearly trailing behing, namely by be-
tween 5% (SP) and 11% (AP). The distribution
of severe problems is comparable for all parsers.
Table 6: Average Performance of Parsers over
all Texts (Directed Evaluation).
Ave. (%) S.D. (%)
AP 77.31 15.00
CP 88.69 8.86
CBP 79.82 18.94
SP 83.43 11.42
As expected, longer sentences are more prob-
lematic for all parsers, as can be seen in Ta-
ble 7. No significant trends in performance dif-
ferences with respect to genre difference, narra-
tive (Orlando, Moving, Betty03) vs. expository
texts (Heat, Plants, Barron17, Olga91), were de-
tected (cf. also speed results in Table 5). But
we assume that the difference in average sen-
tence length obscures any genre differences in
our small sample.
The most common non-fatal problems (type
one) involved the well-documented adjunct at-
tachment site issue, in particular for preposi-
tional phrases ((Abney et al, 1999), (Brill and
Resnik, 1994), (Collins and Brooks, 1995)) as
well as adjectival phrases (Table 8)3. Similar
misattachment issues for adjuncts are encoun-
tered with adverbial phrases, but they were rare
3PP = wrong attachment site for a prepositional
phrase; ADV = wrong attachment site for an adverbial
phrase; cNP = misparsed complex noun phrase; &X =
wrong coordination
Table 7: Correlation of Average Performance
per Text for all Parsers and Average Sentence
Length (Directed Evaluation).
Text perf. (%) length (#words)
Heat 92.31 7.54
Plants 90.76 9.96
Orlando 93.46 6.86
Moving 90.91 13.12
Barron17 76.92 22.15
Betty03 71.43 18.21
Olga91 60.42 25.92
in our corpus.
Another common problem are deverbal nouns
and denominal verbs, as well as -ing/VBG
forms. They share surface forms leading to am-
biguous part of speech assignments. For many
Coh-Metrix 2.0 measures, most obviously tem-
poral cohesion, it is necessary to be able to dis-
tinguish gerunds from gerundives and deverbal
adjectives and deverbal nouns.
Table 8: Specific Problems by Parser.
PP ADV cNP &X
AP 13 10 8 9
CP 15 1 2 7
CBP 10 0 0 13
SP 22 6 3 4
Sum 60 17 13 33
Problems with NP misidentification are par-
ticularly detrimental in view of the impor-
tant role of NPs in Coh-Metrix 2.0 mea-
sures. This pertains in particular to the mistag-
ging/misparsing of complex NPs and the coor-
dination of NPs. Parses with fatal problems
are expected to produce useless results for algo-
rithms operating with them. Wrong coordina-
tion is another notorious problem of parsers (cf.
(Cremers, 1993), (Grootveld, 1994)). In our cor-
pus we found 33 instances of miscoordination,
of which 23 involved NPs. Postprocessing ap-
proaches that address these issues are currently
under investigation.
75
4 Conclusion
The paper presented the evaluation of freely
available, Treebank-style, parsers. We offered
a uniform evaluation for four parsers: Apple
Pie, Charniak?s, Collins/Bikel?s, and the Stan-
ford parser. A novelty of this work is the evalua-
tion of the parsers along new dimensions such as
stability and robustness and across genre, in par-
ticular narrative and expository. For the latter
part we developed a gold standard for narrative
and expository texts from the TASA corpus. No
significant effect, not already captured by vari-
ation in sentence length, could be found here.
Another novelty is the evaluation of the parsers
with respect to particular error types that are
anticipated to be problematic for a given use of
the resulting parses. The reader is invited to
have a closer look at the figures our tables pro-
vide. We lack the space in the present paper to
discuss them in more detail. Overall, Charniak?s
parser emerged as the most succesful candidate
of a parser to be integrated where learning tech-
nology requires syntactic information from real
text in real time.
ACKNOWLEDGEMENTS
This research was funded by Institute for Educa-
tions Science Grant IES R3056020018-02. Any
opinions, findings, and conclusions or recom-
mendations expressed in this article are those
of the authors and do not necessarily reflect the
views of the IES. We are grateful to Philip M.
McCarthy for his assistance in preparing some
of our data.
References
S. Abney, R. E. Schapire, and Y. Singer. 1999.
Boosting applied to tagging and pp attachment.
Proceedings of the 1999 Joint SIGDAT Confer-
ence on Empirical Methods in Natural Language
Processing and Very Large Corpora, pages 38?45.
D. M. Bikel. 2004. Intricacies of collins? parsing
model. Computational Linguistics, 30-4:479?511.
E. Brill and P. Resnik. 1994. A rule-based approach
to prepositional phrase attachment disambigua-
tion. In Proceedings of the 15th International Con-
ference on Computational Linguistics.
J. Carroll, E. Briscoe, and A. Sanfilippo, 1999.
Parser evaluation: current practice, pages 140?
150. EC DG-XIII LRE EAGLES Document EAG-
II-EWG-PR.1.
E. Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. Pro-
ceedings of the Fourteenth National Conference
on Artificial Intelligence, AAAI Press/MIT Press,
Menlo Park.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the North-American
Chapter of Association for Computational Lin-
guistics, Seattle, Washington.
M. Collins and J. Brooks. 1995. Prepositional phrase
attachment through a backed-off model. In Pro-
ceedings of the Third Workshop on Very Large
Corpora, Cambridge.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Com-
putational Linguistic, Madrid, Spain.
C. Cremers. 1993. On Parsing Coordination Cate-
gorially. Ph.D. thesis, Leiden University.
A. C. Graesser, D.S. McNamara, M. M. Louwerse,
and Z. Cai. 2004. Coh-metrix: Analysis of text on
cohesion and language. Behavior Research Meth-
ods, Instruments, and Computers, 36-2:193?202.
R. Grishman, C. MacLeod, and J. . Sterling. 1992.
Evaluating parsing strategies using standardized
parse files. In Proceedings of the Third Conference
on Applied Natural Language Processing, pages
156?161.
M. Grootveld. 1994. Parsing Coordination Genera-
tively. Ph.D. thesis, Leiden University.
D. Klein and C. Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the 41st Annual
Meeting of the Association for Computational Lin-
guistic, Sapporo, Japan.
D. Lin. 1995. A dependency-based method for eval-
uating broad-coverage parsers. Proceedings of In-
ternational Joint Conference on Artificial Intelli-
gence, pages 1420?1427.
A. Ratnaparkhi, J. Renyar, and S. Roukos. 1994. A
maximum entropy model for prepositional phrase
attachment. In Proceedings of the ARPA Work-
shop on Human Language Technology.
S. Sekine and R. Grishman. 1995. A corpus-
based probabilistic grammar with only two non-
terminals. Proceedings of the International Work-
shop on Parsing Technologies, pages 216?223.
76
