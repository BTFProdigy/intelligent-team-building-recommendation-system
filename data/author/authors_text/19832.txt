Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1648?1659, Dublin, Ireland, August 23-29 2014.
Learning to Generate Coherent Summary
with Discriminative Hidden Semi-Markov Model
Hitoshi Nishikawa
1
, Kazuho Arita
1
, Katsumi Tanaka
1
,
Tsutomu Hirao
2
, Toshiro Makino
1
and Yoshihiro Matsuo
1
Nippon Telegraph and Telephone Corporation
1
1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847 Japan
2
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{
nishikawa.hitoshi, arita.kazuho, tanaka.katsumi
hirao.tsutomu, makino.toshiro, matsuo.yoshihiro
}
@lab.ntt.co.jp
Abstract
In this paper we introduce a novel single-document summarization method based on a hidden
semi-Markov model. This model can naturally model single-document summarization as the
optimization problem of selecting the best sequence from among the sentences in the input doc-
ument under the given objective function and knapsack constraint. This advantage makes it
possible for sentence selection to take the coherence of the summary into account. In addition
our model can also incorporate sentence compression into the summarization process. To demon-
strate the effectiveness of our method, we conduct an experimental evaluation with a large-scale
corpus consisting of 12,748 pairs of a document and its reference. The results show that our
method significantly outperforms the competitive baselines in terms of ROUGE evaluation, and
the linguistic quality of summaries is also improved. Our method successfully mimicked the
reference summaries, about 20 percent of the summaries generated by our method were com-
pletely identical to their references. Moreover, we show that large-scale training samples are
quite effective for training a summarizer.
1 Introduction
Single-document summarization is attracting much more attention as a key technology in providing
better information access in a commercial context. The Financial Times and CNN have been providing
summaries of articles in their websites to attract users, and Summly, which has been acquired by Yahoo!,
provided the service of automatically summarizing articles on the Internet. Given the cost of manual
summarization, we can greatly improve the information access of Internet users by creating an automatic
summarizer that can approach the summarization quality of humans.
To mimic manually-written summaries, one important aspect is coherence (Nenkova and McKeown,
2011). Although coherence has been studied widely in a field of multi-document summarization (Kara-
manis et al., 2004; Barzilay and Lapata, 2005; Nishikawa et al., 2010; Christensen et al., 2013), it has not
been studied enough in the context of single-document summarization. In this paper, we revisit the prob-
lem of coherence and employ it to produce both informative and linguistically high-quality summaries.
To obtain such summaries, we introduce a novel summarization method based on a hidden semi-
Markov model. The method has the properties of both the popular single-document summarization
model, the knapsack problem, which packs the sentences into the given length and the hidden Markov
model, which takes summary coherence into account by determining sentence context when selecting
sentences. By leveraging this, we can build a summarizer that naturally achieves coherence.
We state the novelty and contributions of this paper as follows:
? We regard single-document summarization as a combinatorial optimization problem modeled by a
hidden semi-Markov model and propose an efficient decoding algorithm for the problem.
? We introduce various features related to coherence in a combinatorial formulation. We extend a
hidden semi-Markov model to achieve discrimination, so our method can take advantage of many
features for predicting coherence.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1648
? We show that our large-scale corpus greatly improves the performance of summarization.
This paper is organized as follows. In Section 2, we describe related work. In Section 3, we detail
our proposed model. We also explain how the parameters in our model are optimized and how sentences
are compressed. In Section 4, we explain how variants of the original sentences are generated. In
Section 5, we explain the decoding algorithm for our method. In Section 6, we explain the settings of
our experiments, our corpus, and compared methods. In Section 7, we show results of the experiments
conducted to evaluate our method. In Section 8, we conclude this paper.
2 Related Work
2.1 Single-Document Summarization
Basically, single-document summarization can be done through sentence selection (Nenkova and McK-
eown, 2011) . The document to be summarized is decomposed into a set of sentences and then the
summarizer selects a subset of the sentences as a summary.
McDonald (2007) pointed out that single-document summarization can be formulated as a well-known
combinatorial optimization problem, the knapsack problem. Given a set of sentences together with their
lengths and values, the summarizer packs them into a summary so that the total value is as large as possi-
ble but the total length is less than or equal to a given maximum summary length. Interestingly, a hidden
semi-Markov model (Yu, 2010) can be regarded as a natural extension of the knapsack problem, we take
advantage of this property for single-document summarization. We elaborate the relation between the
knapsack problem and the hidden semi-Markov model in Section 3.
To generate coherent summaries in single-document summarization, there are two types of ap-
proaches
1
: tree-based approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) and
sequence-based approaches (Barzilay and Lee, 2004; Shen et al., 2007). The former rely on the tree
representation of a document based on the Rhetorical Structure Theory (RST) (Mann and Thompson,
1988). Basically, the former approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) trim
the tree representation of a document by making use of nucleus-satellite relations among sentences. The
advantage of RST-based approaches is that they can take advantage of global information about the doc-
uments. However, a drawback is that they depend heavily on the RST parser that is used. Performance
is remarkably sensitive to the accuracy of RST parsing, and hence we have to build a good RST parser.
Instead of making use of the global structure of the document, the sequence-based methods rely on and
take advantage of the local coherence of sentences. As one advantage over the tree-based approaches,
the sequence-based approaches do not require tools as RST parsers, and hence they are more robust. For
this reason, this paper focuses on sequence-based approaches.
The previous works most closely related to our method are those proposed by Barzilay and Lee (2004)
and Shen et al. (2007). Barzilay and Lee built a hidden Markov model to capture the content structure of
documents and used it to identify the important sentences. Shen et al. (2007) extended the HMM-based
approach to make it discriminative by making use of conditional random fields (Lafferty et al., 2001).
Conditional random fields can incorporate various features to identify the importance of a sentence and
they showed its effectiveness. A shortcoming of these approaches is that their model only classifies sen-
tences into two classes, it cannot take account of output length directly. This deficiency is problematic
because in practical usage the maximum length of a summary is specified by the user; hence, the sum-
marizer should be able to control output length. In contrast to their method, our approach naturally takes
the maximum summary length into account when summarizing a document.
2.2 Coherence
In the context of multi-document summarization, coherence has been studied widely. In multi-document
summarization, sentences are selected from different documents, and hence some way of ordering the
sentences is required. Sentence ordering (Barzilay et al., 2002; Althaus et al., 2004; Karamanis et al.,
1
As an interesting related work, Clarke and Lapata (2007) compresses documents by making use of Centering Theory
(Grosz et al., 1995). However, in their approach, the desired length of an output summary could not be specified and hence they
said their method was compression rather than summarization.
1649
Figure 1: An example of the hidden semi-Markov model. The system observes a sequence consisting
of 10 symbols o
1
...o
10
over time t
1
...t
10
and transitions between states s
1
...s
3
. Unlike the basic hidden
Markov model, states can persist for a non-unit length. In this figure, state s
2
and state s
3
persist for
non-unit lengths. Hence, the system traverses only 6 states despite observing 10 symbols.
2004; Okazaki et al., 2004) is a task to order extracted sentences and is closely related to coherence
(Lapata, 2003; Barzilay and Lapata, 2005; Nenkova et al., 2010; Pitler et al., 2010; Louis and Nenkova,
2012). Many effective features have been found out to capture coherence and we utilize these features.
Some work proposed a model that could jointly taking the content of the summary and its coherence
into account (Nishikawa et al., 2010; Christensen et al., 2013). Since extracted sentences in multi-
document summarization must be ordered, a task that is NP-hard, they relied on integer linear program-
ming (Nishikawa et al., 2010) or a local search strategy (Christensen et al., 2013). The former can locate
the optimal solution at a heavy computation cost, while the latter runs quickly but there is no guarantee
of locating the optimal solution. In contrast to their trade-off, our proposed algorithm, based on dynamic
programming, can locate the optimal solution quickly because the single-document summarization can
skip the ordering operation by reproducing the original order of the input sentences.
In this paper, we show that coherence also takes an important role in single-document summarization.
We model the coherence between adjacent sentences in the summary by leveraging the hidden semi-
Markov model, which can naturally capture the coherence between sentences.
3 Summarization with Hidden Semi-Markov Model
We first introduce the knapsack problem, which can naturally model single-document summarization.
Next, we explain the hidden semi-Markov model and show its relationship to the knapsack problem.
Then, we elaborate our summarization method.
3.1 Knapsack Problem
The knapsack problem is a type of combinatorial optimization problem (Korte and Vygen, 2008). Given
a set of elements, each of which has a score and size, the problem is formulated as the task of finding
the best subset in terms of maximizing the sum of their scores under the size limitation. As mentioned
above, single-document summarization can be regarded as an instance of the knapsack problem. The
best combination of input sentences can be found by calculating the value of each sentence and packing
them into a summary through the dynamic programming knapsack algorithm.
3.2 Hidden Semi-Markov Model
The hidden semi-Markov model (HSMM) is an extension of the hidden Markov model (HMM) (Yu,
2010). In the popular hidden Markov model, each state persists for only one unit length. For example,
if a system observes 10 discrete symbols, it outputs 10 hidden states. In the HSMM, each state can
persist for some unit lengths through the concept of duration. For example, if a system observes 10
discrete symbols and each state persists for two unit lengths, i.e., their duration is 2, the system outputs
5 hidden states. We show an example in Figure 1. The system observes a sequence consisting of 10
symbols o
1
...o
10
over time t
1
...t
10
and transitions between states s
1
...s
3
. Unlike the basic HMM, states
can persist for a non-unit length. In this figure, state s
2
and state s
3
persist for a non-unit length. Hence,
the system traverses 6 states even though it observes 10 symbols. This property has been utilized for
1650
sequential tagging, such as named entity recognition (Sarawagi and Cohen, 2004), scene text recognition
(Weinman et al., 2008) and phonetic recognition (Kim et al., 2011).
The hidden semi-Markov model is closely related to the knapsack problem. The length, K, of the
observed symbols can be regarded as a knapsack constraint. We can consider that the system tries to pack
the states of the model into the observed sequence of symbols by transitioning over the states under the
knapsack constraint so as to maximize the likelihood. Therefore, the hidden semi-Markov can naturally
be used for single-document summarization. Suppose that the document to be summarized consists of
10 sentences and the length of each of them is measured by the number of words. In this case, the system
transitions over 10 states corresponding to the 10 sentences until it cannot select any further sentence due
to the given length requirement. Since each state persists for the length of the corresponding sentence,
the remaining length decreases every time the system transitions to a new state.
While an HMM is basically a generative model, Collins (2002) extended it to create a discriminative
model. An HSMM can also be extended to become discriminative model (Sarawagi and Cohen, 2004).
Our discriminative HSMM learns through the application of max-margin training.
3.3 Formulation
We consider there are n input sentences s
1
, s
2
, ..., s
n
. These sentences have lengths ?
1
, ?
2
, ..., ?
n
and
weights w
1
, w
2
, ..., w
n
. We assume that a sentence that has a high weight should be present in the output
summary. We also consider each sentence, s
i
, has m
i
variants s
i,1
, s
i,2
, ..., s
i,m
, each produced by some
sort of sentence compression or paraphrase module. These variants also have lengths ?
i,1
, ?
i,2
, ..., ?
i,m
i
and weights w
i,1
, w
i,2
, ..., w
i,m
i
. For simplicity, we hereinafter note the original sentences s
1
, s
2
, ..., s
n
as s
1,0
, s
2,0
, ..., s
n,0
. Hence we have original sentence s
i,0
and variants s
i,1
, s
i,2
, ..., s
i,m
. Let s
0,0
and
s
n+1,0
be special symbols indicating the beginning of a document and the end of a document, respec-
tively. We define coherence c
g,h,i,j
as the coherence between sentence s
g,h
and sentence s
i,j
. An output
summary is described as a sequence of input sentences, g. LetG be the entire set of sequences that can be
constructed from the input sentences, i.e., g ? G. Finally, let K be the maximum length of the summary
desired. With these notations, our proposed method can be formulated as the following optimization
problem:
g
?
= argmax
g?G
?
s
i,j
?sent(g)
w
i,j
+
?
(s
g,h
,s
i,j
)?adj(g)
c
g,h,i,j
(1)
s.t.
?
s
i,j
?sent(g)
?
i,j
? K, (2)
where sent(g) and adj(g) indicate a set of sentences in g and a set of adjacent sentences in g, respec-
tively. That is, our model tries to find the best sequence of sentences under the knapsack constraint so as
to maximize the sum of weights and sentence coherence. In contrast to the common knapsack problem
which cannot take the variants and sentence coherence into account, our method, based on the hidden
semi-Markov model, does so naturally.
3.4 Parameter Optimization
Here we elaborate how parameters in the model are optimized to achieve the desired summaries. The
goal is to determine the value of w
i,j
for all i, j and c
g,h,i,j
for all g, h, i, j. We define w
i,j
and c
g,h,i,j
as
follows:
w
i,j
= w
w
? f
w
(s
i,j
) (3)
c
g,h,i,j
= w
c
? f
c
(s
g,h
, s
i,j
), (4)
where f
w
and f
c
are d
w
-dimensional and d
c
-dimensional feature vectors for sentences and sentence pairs,
respectively, andw
w
andw
c
are d
w
-dimensional and d
c
-dimensional parameter vectors for sentences and
sentence pairs, respectively. The goal of optimization is to determine the values of both vector w
w
and
1651
wc
, given feature function f
w
and f
c
. For simplicity, let s be a summary, let f = ?f
w
, f
c
? be a (d
w
+ d
c
)-
dimensional feature function for the whole summary and let w = ?w
w
,w
c
? be a (d
w
+ d
c
)-dimensional
weight vector. The value that the objective function outputs for summary s is w ? f(s).
To optimize the parameter, we employ the Passive-Aggressive algorithm (Crammer, 2006), a widely-
used structured learning method. Since the algorithm offers online learning, it can learn the parameter
quickly and is easy to implement. To learn the parameter so that the output summary is optimized to
the evaluation criteria popular in document summarization research, ROUGE (Lin, 2004), we introduce
ROUGE as the loss function. The parameter is estimated by solving the following formula iteratively
2
:
w
new
= argmin
w
1
2
||w ? w
old
||
2
(5)
s.t. w ? f(r) ? w ? f(s) ? loss(s; r),
where w
new
is the parameter vector after update, w
old
is the parameter vector before update, r is a
reference summary, and loss is the loss function. We define loss as 1 ? ROUGE(s; r). Among the
variants of ROUGE, we used ROUGE-1 for the loss function.
3.4.1 Sentence Feature
The features introduced in this section are used to calculate the weights of sentences, w
i,j
.
Term Frequency: Term frequency is a classic feature in document summarization (Luhn, 1958). We
calculate the total number of times each content word occurs in the document and then, for each sentence,
sum the totals of the content words that appear in the sentence as the value of this feature.
Word: We also use the words and parts-of-speech as features.
Named Entity: Named entities such as a name of person or organization are important. We use named
entities and classes as features.
Length: The length of a sentence may indicate the information value of its content. We use the length of
a sentence, measured by character number, as a feature.
Position: The position of a sentence is a classically important feature. We use the position of a sentence,
the relative position of a sentence, whether the sentence is the first in the document and whether the
sentence is the first in a paragraph, the position of the paragraph in which the sentence is, as features.
3.4.2 Coherence Feature
The features introduced in this section are used to calculate sentence coherence, c
g,h,i,h
.
Lexical Transition: Lapata (2003) showed that the structure of the document can be captured by word-
pairs consisting of words of two adjacent sentences. We use this feature for capturing the links between
two sentences
3
. We build a set of word pairs where one occurs in a precedent sentence and the other
occurs in a succeeding one, and use the elements of the set as a feature.
Lexical Cohesion: Pitler et al. (2010) showed that the similarity of two sentences is one of the strongest
features for predicting coherence. We reproduce this feature for generating coherent summaries. We
calculate cosine similarity between two sentences and use its value as a feature.
Entity Grid: Previous studies showed that Entity Grid (Barzilay and Lapata, 2005) is a strong feature
for predicting coherence (Pitler et al., 2010). We also employ this feature for summarization. While the
entity vector made from the entity grid was originally defined for whole documents, we build the entity
vector for each pair of two sentences because our model is based on the Markovian assumption, and
hence the coherence score is defined between two sentences.
2
As we explain later in Section 5, computation complexity of our algorithm is pseudo-polynomial, and hence the best
solution of our model can be located quickly. This is also advantageous in the learning phase because to learn parameters using
structured learning, the learner has to generate a summary to calculate the loss. Since our algorithm can quickly find the best
solution and generate a summary, it can also contribute to shortening the time required for learning.
3
It is expected that this feature will also contribute to sentence selection. Barzilay and Elhadad (1997) showed that a closely
related word-pair was a good indicator for sentence selection. This feature captures this property by learning.
1652
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 18000
 0  5  10  15  20  25  30
Th
e n
um
be
r o
f s
en
ten
ces
Levenshtein distance
Figure 2: Distribution of Levenshtein distance in the
aligned sentences. Among the 36,413 sentences in
the references, 16,643 were identical (Levenshtein
distance is 0) to the aligned sentences in the input
documents.
 0.6
 0.61
 0.62
 0.63
 0.64
 0.65
 0.66
 0.67
 0.68
 0.69
 0.7
 0  2000  4000  6000  8000  10000
RO
UG
E-
2
The number of training samples
Figure 3: Learning curve of HSMM.
4 Generating Sentence Variants
Since our model can take the variants of an original sentence in the input document as in the multi-
candidate reduction framework (Zajic et al., 2007), we incorporate sentence compression.
We generate a few variants of each original sentence by trimming the dependency tree of the sentence;
this simple operation is sufficient for reproducing reference summaries. By aligning sentences in a refer-
ence summary with those in the corresponding input document
4
, we found that human summaries were
quite conservative. Among the 36,413 sentences in the references, 16,643 were identical to the aligned
sentences in the input documents. Furthermore, most remaining sentences were virtually identical to the
original sentences; revisions were minor, and can be reproduced by simple operations. Few sentences
exhibited paraphrasing or more sophisticated operations. We plot the distribution of Levenshtein distance
in the aligned sentences in Figure 2. According to this observation, we produce the following types of
variants by sentence compression:
1. Removing information in parentheses. Some sentences contain parentheses containing additional
information for readers. The first type of variant deletes text in parentheses.
2. Shortening sentences by trimming their dependency trees. Basically this method follows the sen-
tence trimmer proposed by Nomoto (2008). While using his method, we keep the predicate and its
obligatory arguments in the sentences to keep the sentences grammatical. If a predicate is trimmed,
its obligatory arguments are also trimmed and vice versa. Since there are an exponential number
of subtrees in one tree, we use only n-best subtrees by ranking them according to n-gram language
likelihood and dependency-based language likelihood. We used the dependency parser proposed by
Imamura et al (Imamura et al., 2007) to acquire the dependency tree.
5 Decoding with Dynamic Programming
To solve Equation 1 under the constraints of Equation 2, we use dynamic programming. Algorithm
1 shows the pseudo code of the decoding algorithm. Line 1 to Line 7 initializes the variables used in
the algorithm. Vector x = ?x
0
, ..., x
n+1
? stores which sentence and which variants are included in the
output summary. If x
3
= 2, s
3,2
is included in the summary. V , P and S are two-dimensional arrays,
each of which is used as a dynamic programming table. They store the process of dynamic programming.
4
Alignment proceeds in two steps: first, we calculate the Levenshtein distance between sentences in the document and its
reference, and then we align sentences so as to minimize the distance between them.
1653
Algorithm 1 Decoding Algorithm: Filling Table
1: x = ?x
0
, ..., x
n+1
?
2: for i = 0 to n + 1 do
3: x
i
= ?1
4: V [0][i]? ?1
5: P [0][i]? ?1
6: S[0][i]? 0
7: V [0][0] = 0
8: for k = 1 to K do
9: for i = 1 to n do
10: V [k][i]? V [k ? 1][i]
11: P [k][i]? P [k ? 1][i]
12: S[k][i]? S[k ? 1][i]
13: for v = 0 to m
i
do
14: if ?
i,v
? k then
15: for h = 0 to i? 1 do
16: u = V [k ? ?
i,v
][h]
17: if u ?= ?1 ? S[k ? ?
i,v
][h] + w
i,v
+ c
h,u,i,v
? S[k][i] then
18: V [k][i]? v
19: P [k][i]? h
20: S[k][i]? S[k ? ?
i,v
][h] + w
i,v
+ c
h,u,i,v
21: V [K + 1][n + 1]? 0
22: P [K + 1][n + 1]? 0
23: S[K + 1][n + 1]? 0
24: for h = 1 to n do
25: u = V [K][h]
26: if S[K][h] + c
h,u,n+1,0
? S[K + 1][n + 1] then
27: P [K + 1][n + 1]? h
28: S[K + 1][n + 1]? S[K][h] + c
h,u,n+1,0
Document Reference
Avg. # of characters 476.2 142.0
Avg. # of words 298.6 88.3
Avg. # of sentences 9.7 2.9
Table 1: The statistics of our corpus.
V [k][i] stores which variants are used at time k, i. If V [k][i] = 0, original sentence s
i,0
is selected at
time k, i. If V [k][i] = ?1, no sentence is selected at time k, i. P [k][i] stores a pointer to the sentence
connected to the front of the current sentence. S[k][i] stores the value of the objective function at time
k, i. Line 8 to Line 36 locates the best sequence of sentences based on the following recurrence formula:
S[k][i] =
{
max
h=0...i?1,v=0...m
S[k ? ?
i,v
][h] + w
i,v
+ c
h,V [k??
i,v
][h],i,v
(A)
S[k ? 1][i] (B),
(6)
where case A is: ?
i,v
? k ? S[k ? 1][i] ? S[k ? ?
i,v
][h] + w
i,v
+ c
h,V [k??
i,v
][h],i,v
and case B is:
otherwise. This recurrence formula means that at time k, i the best variant to be selected as can be
determined at time k ? ?
i,v
, h. Hence, for all k ? 1...K and i ? 1...n, the algorithm finds the best
sequence of sentences at time k, i. After Algorithm 1 locates the best sequence of sentences by filling
the tables, the best sequence can be restored by backtracing along the pointers stored in P . Finally, the
algorithm outputs x, which stores which sentences and variants are used in the best sequence. Since
this algorithm is based on a dynamic programming knapsack algorithm (Korte and Vygen, 2008), it runs
in pseudo-polynomial time. This is a significant advantage over the methods that rely on integer linear
programming solvers due to their substantial computation cost.
6 Experiments
6.1 Data
We prepared 12,748 pairs of Japanese newspaper articles and their manually-written reference sum-
maries. This is one of the largest corpus available for single-document summarization research. The
length of all references is within 150 characters. All references in the corpus were written by a specialist
staff in a Japanese newspaper company and the company sold these summaries for commercial purposes.
1654
We list the statistics of our corpus in Table 1. As shown, the task is to summarize the document in about
a third of its original length in terms of the number of words.
6.2 Evaluation Criteria
ROUGE; ROUGE is an automatic evaluation method for automatic summarization proposed by Lin
(2004). We used ROUGE-1 and ROUGE-2 to evaluate the summaries. Since our document-reference
pairs are written in Japanese, we segmented the sentences into words using the Japanese morphological
analyzer developed by Fuchi and Takagi (1998). When calculating the ROUGE score, we used only
content words (i.e. nouns, verbs and adjectives) and so excluded function words as stop words.
Linguistic Quality: To evaluate the linguistic quality of the summaries generated by our method, we
performed a manual evaluation according to quality questions proposed by the National Institute of
Standards and Technology (NIST) (2007)
5
. We randomly sampled 100 summaries from the outputs of
each method described below and asked 7 subjects to evaluate the summaries according to the questions.
All subjects were Japanese native and none were among the authors. Since the quality questions by
NIST (2007) were designed for multi-document summarization, we used 3 of the 5 NIST questions for
single-document summarization: grammaticality, referential clarity, and structure/coherence. We also
asked the subjects to evaluate overall summary quality.
6.3 Compared Methods
We compared the following 8 methods.
Random: Random method selects sentences in the input document randomly.
Lead: Lead method is a classic baseline in single-document summarization. It only extracts the words
from the beginning of the document until the extracted words reach the given length. We simply extracted
150 characters from the beginning of each document.
Knapsack: The knapsack problem can be used as a single-document summarization model (McDonald,
2007). In this baseline, the weight of each sentence was calculated based on the average probabilities
of the words in the sentence (Nenkova and Vanderwende, 2005). Then, a summary was generated by
solving the knapsack problem.
Knapsack with Supervision: Instead of the average word probabilities used in the above baseline, we
used only sentence features f
w
to weigh a sentence.
Conditional Random Fields: Conditional random fields can be used to weigh sentences (Shen et al.,
2007). Since CRFs required binary labels in learning, we aligned sentences in an input document with
the sentences in its reference as explained in Section 4. We used the probabilities of sentences from
CRFs as the weights of the knapsack problem.
Hidden Semi-Markov Model: This is our proposed method without variants of the original sentences.
It selected sentences only from the set of original sentences.
Hidden Semi-Markov Model with Compression: This is our proposed method with variants of the
original sentences. It selected from among the variants and the original ones.
Human: In the linguistic quality evaluation, we added references to the summaries generated by the
above methods to show the upper bound.
When learning, we did 10-fold cross validation. In the experiments, statistical significance was
checked by Wilcoxon signed-rank test (Wilcoxon, 1945). To counteract the problem of multiple com-
parisons, we used the Holm-Bonferroni method (Holm, 1979) to adjust the significance level, ?.
7 Results and Discussion
We show the results of our experiment in Table 2 and Table 3. In this section, first we discuss the results
of the ROUGE evaluation, and then we discuss the results of the linguistic quality evaluation.
In the ROUGE evaluation, all the compared methods except for RANDOM showed good performance.
This is because, as shown in Section 4, many references consisted of sentences identical to the original
5
Some recent studies have tried to predict the readability of the text automatically (Pitler et al., 2010).
1655
Method R-1 R-2 Idt.
RANDOM 0.417 0.291 1.2%
LEAD 0.779
C,S,U,R
0.727
C,S,U,R
4.4%
KP 0.704
R
0.611
R
9.3%
KP(S) 0.729
U,R
0.647
U,R
10.4%
CRFs 0.741
U,R
0.675
S,U,R
11.3%
HSMM 0.769
C,S,U,R
0.703
C,S,U,R
15.2%
HSMM(C) 0.785
C,S,U,R
0.722
C,S,U,R
20.4%
Table 2: Results of the ROUGE evaluation.
?R-1? and ?R-2? correspond to ROUGE-1 and
ROUGE-2, respectively. The values in the col-
umn of ?Idt.? are the percentage of summaries
completely-identical to the corresponding refer-
ences. In the table,
C,S,U,L,R
indicate statisti-
cal significance against CRFs, KP(S), KP, LEAD,
RANDOM, respectively.
Method Gram. Ref. S./C. Overall
LEAD 1.9 3.9 2.5 2.1
KP 4.1
L
3.7 3.4 3.5
KP(S) 4.2
L
3.6 3.5 3.6
L
CRFs 4.1
L
3.9 3.7
L
3.6
L
HSMM 4.3
L
4.0 4.1
L
4.0
L
HSMM(C) 4.0
L
3.9 4.0
L
3.9
L
HUMAN 4.7
L
4.5 4.7
L
4.8
L
Table 3: Results of the linguistic quality evalua-
tion. The values ranged from 1 (very poor) to 5
(very good) (National Institute of Standards and
Technology, 2007). We show statistical signifi-
cance with the same notations as Table 2.
ones, and hence the references can be reproduced if important sentences are identified. Since the com-
pression rate in our corpus was relatively light, it made important information easy to identify. Among
the compared methods, both LEAD and our proposed method, HSMM(C), achieved the best result. There
was no significant difference between LEAD and HSMM(C). This surprising performance of LEAD was
due to the ROUGE evaluation. The words in the document leads were likely to be important, and LEAD
drew on this property. However, as we mentioned later, it sacrificed the linguistic quality to achieve the
high ROUGE score. Furthermore, it failed to yield summaries identical to the reference. In contrast to
LEAD, almost 20% of the summaries generated by HSMM(C) were identical to the references. This
shows that our method successfully mimicked human assessments. HSMM followed the best models.
There was a statistically significant difference between HSMM(C) and HSMM. Since some sentences,
especially the first sentence in the document, were long and the first sentence was particularly impor-
tant to summarize the document, sentence compression yielded a significant improvement. As shown
in Table 2, employing compression greatly improved the percentage of identical summaries. HSMM
significantly outperformed all of the baseline extractive methods except LEAD. While CRFs can take
advantage of all features used in HSMM, CRFs cannot take the evaluation measure such as ROUGE and
the knapsack constraint into account in learning. HSMM also significantly outperformed KP(S). This
difference is particularly important, and shows the usefulness of features related to coherence. While
KP(S) used only features about sentences, HSMM successfully mimicked the references as it drew on
the features related to coherence.
We show the learning curve of HSMM in Figure 3. We fixed 2,748 pairs for testing, and learned
parameters from 100, 250, 500, 1,000, 2,500, 5,000, 7,500 and 10,000 pairs. The curve in the figure
clearly shows the effectiveness of our large-scale corpus in learning. It seems that the curve does not
saturate and hence HSMM performance can be improved by more training samples. As in the results
recently shown by Filippova (2013), this result implies that large-scale data is important in the field
of document summarization as in other fields of computational linguistics. Past studies in document
summarization relied on relatively small datasets consisting of a few dozen or at most a few hundred
pairs of a document and its reference in learning. In contrast to the past studies, there are over 10,000
pairs in our dataset and the results show its effectiveness.
Second, we discuss the result of the linguistic quality evaluation. Unlike the ROUGE evaluation,
HSMM achieved the best result. As previous studies have pointed out (Nenkova and McKeown, 2011),
sentence compression commonly tends to degrade the linguistic quality of a summary while improving
its content. As shown in Table 3, the grammaticality of HSMM(C) is lower than that of HSMM, but the
1656
difference is not significant. Although we could not observe any significant difference between HSMM
and other extractive baselines, our proposals, HSMM and HSMM(C), yielded the best result in terms
of structure/coherence. By making use of the features related to coherence, we successfully improved
summary quality. In contrast to the surprising performance of LEAD in the ROUGE evaluation, in the
linguistic quality evaluation, LEAD yielded the worst performance. Since LEAD had to cut the sentences
when it reached the given length, it create ungrammatical fragments.
Finally, we touch on the balance between the quality of content and linguistic quality. Comparing
Table 2 to 3, we can see the correlation between the quality of content and linguistic quality. This re-
sult is reasonable because we can extract much more information from grammatical and well-organized
sentences. Although we optimized the parameter to maximize the ROUGE score, it also yielded im-
provements in linguistic quality. This is because the manually-generated reference summaries are ba-
sically grammatical and well-organized and the parameter is learnt to mimic them. However, there is
an inherent trade-off between the quality of content and linguistic quality. For example, under stricter
length limitations, instead of cohesive devices such as conjunctions, which can improve the coherence of
sentences, content words would be preferred for summary inclusion to augment information. Balancing
them to maximize reader satisfaction is an interesting problem.
8 Conclusions
In this paper we presented a novel single-document summarization method based on the hidden semi-
Markov model, which is a natural extension of the knapsack problem. Our model naturally takes account
of sentence context when identifying important sentences. This property is particularly important to
ensure the coherence of output summaries and to produce informative and linguistically high-quality
summaries. We also proposed an algorithm based on dynamic programming so the best solution can be
located quickly. Experiments on a very large-scale single-document summarization corpus showed that
our proposed method significantly outperforms competitive baselines.
As future work, we plan to tackle on the summarization task where higher compression is demanded.
To generate shorter summaries, we plan to employ more sophisticated approaches, such as paraphrasing.
Acknowledgement
The corpus used in this paper is owned by The Mainichi Newspapers Co., Ltd. and is leased to Nippon
Telegraph and Telephone Corporation. We sincerely appreciate their consideration. We also appreciate
the insightful comments from reviewers. Their comments greatly improved the quality of this paper.
References
Ernst Althaus, Nikiforos Karamanis, and Alexander Koller. 2004. Computing locally coherent discourses. In
Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL), pages 399?406.
Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of the
Intelligent Scalable Text Summarization Workshop (ISTS), pages 10?17.
Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: an entity-based approach. In Proceedings
of the 43rd Annual Meeting on Association for Computational Linguistics (ACL), pages 141?148.
Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to
generation and summarization. In HLT-NAACL 2004: Main Proceedings, pages 113?120.
Regina Barzilay, Noemie Elhadad, and Kathleen R. McKeown. 2002. Inferring strategies for sentence ordering in
multidocument news summarization. Journal of Artificial Intelligence Research, 17:35?55.
Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni. 2013. Towards coherent multi-document
summarization. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 1163?1173.
James Clarke and Mirella Lapata. 2007. Modelling compression with discourse constraints. In Proceedings of
the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 1?11.
1657
Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1?8.
Koby Crammer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research,
7(Mar):551?585.
Hal Daume, III and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 449?456.
Katja Filippova. 2013. Overcoming the lack of parallel data in sentence compression. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1481?1491.
Takeshi Fuchi and Shinichiro Takagi. 1998. Japanese morphological analyzer using word co-occurrence: Jtag.
In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th
International Conference on Computational Linguistics (ACL-COLING), pages 409?413.
Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi. 1995. Centering: a framework for modeling the local
coherence of discourse. Computational Linguistics, 21(2):203?225.
Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino, Norihito Yasuda, and Masaaki Nagata. 2013. Single-
document summarization as a tree knapsack problem. In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages 1515?1520.
Sture Holm. 1979. A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics,
6(2):65?70.
Kenji Imamura, Genichiro Kikui, and Norihito Yasuda. 2007. Japanese dependency parsing using sequential label-
ing for semi-spoken language. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 225?228.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish, and Jon Oberlander. 2004. Evaluating centering-based
metrics of coherence. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics
(ACL), pages 391?398.
Sungwoong Kim, Sungrack Yun, and Chang D. Yoo. 2011. Large margin discriminative semi-markov model
for phonetic recognition. IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING,
7(19):1999?2012.
Bernhard Korte and Jens Vygen. 2008. Combinatorial Optimization. Springer-Verlag, third edition.
John Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on
Machine Learning (ICML), pages 282?289.
Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the
41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 545?552.
Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of ACL Workshop
Text Summarization Branches Out, pages 74?81.
Annie Louis and Ani Nenkova. 2012. A coherence model based on syntactic patterns. In Proceedings of the 2012
Conference on Empirical Methods on Natural Language Processing and Computational Natural Language
Learning (EMNLP-CoNLL).
Hans P. Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of Research and Development,
22(2):159?165.
William C. Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu. 1997. From discourse structure to text summaries. In Proceedings of ACL/EACL 1997 Summariza-
tion Workshop, pages 82?88.
Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings
of the 29th European Conference on Information Retrieval (ECIR), pages 557?564.
1658
National Institute of Standards and Technology. 2007. The linguistic quality questions. http://www-nlpir.
nist.gov/projects/duc/duc2007/quality-questions.txt.
Ani Nenkova and Kathleen McKeown. 2011. Automatic Summarization. Now Publishers.
Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Technical report,
MSR-TR-2005-101.
Ani Nenkova, Jieun Chae, Annie Louis, and Emily Pitler. 2010. Structural features for predicting the linguistic
quality of text: Applications to machine translation, automatic summarization and human-authored text. In
Emiel Krahmer and Theunem Mariet, editors, Empirical Methods in Natural Language Generation: Data-
oriented Methods and Empirical Evaluation, pages 222?241. Springer.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo, and Genichiro Kikui. 2010. Opinion summarization
with integer linear programming formulation for sentence extraction and ordering. In Coling 2010: Posters,
pages 910?918.
Tadashi Nomoto. 2008. A generic sentence trimmer with crfs. In Proceedings of the 46th Annual Conference of
the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 299?307.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishizuka. 2004. Improving chronological sentence ordering by
precedence relation. In Proceedings of the 20th International Conference on Computational Linguistics (Col-
ing), pages 750?756.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010. Automatic evaluation of linguistic quality in multi-document
summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics
(ACL), pages 544?554.
Sunita Sarawagi and William W. Cohen. 2004. Semi-markov conditional random fields for information extraction.
In Advances in Neural Information Processing Systems 17, pages 1185?1192.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Document summarization using conditional
random fields. In Proceedings of the 20th international joint conference on Artifical intelligence (IJCAI), pages
2862?2867.
Jerod J. Weinman, Erik Learned-Miller, and Allen Hanson. 2008. A discriminative semi-markov model for robust
scene text recognition. In Proceedings of the 19th International Conference on Pattern Recognition (ICPR),
pages 1?5.
Frank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80?83.
Shun-Zheng Yu. 2010. Hidden semi-markov models. Artificial Intelligence, 174(2):215?243.
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and Schwartz Richard. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization tasks. Information Processing and Management, 43:1549?
1570.
1659
Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 87?95,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Improving a Method for Quantifying Readers? Impressions of News Articles
with a Regression Equation
Tadahiko Kumamoto
Chiba Institute of Technology
2-17-1, Tsudanuma, Narashino,
Chiba 275-0016, Japan
kumamoto@net.it-chiba.ac.jp
Yukiko Kawai
Kyoto Sangyo University
Motoyama, Kamigamo,
Kita-Ku, Kyoto 603-8555,
Japan
Katsumi Tanaka
Kyoto University
Yoshida-Honmachi,
Sakyo-Ku, Kyoto 606-8501,
Japan
Abstract
In this paper, we focus on the impressions that
people gain from reading articles in Japanese
newspapers, and we propose a method for
extracting and quantifying these impressions
in real numbers. The target impressions are
limited to those represented by three bipo-
lar scales, ?Happy ? Sad,? ?Glad ? Angry,?
and ?Peaceful ? Strained,? and the strength of
each impression is computed as a real num-
ber between 1 and 7. First, we implement a
method for computing impression values of
articles using an impression lexicon. This
lexicon represents a correlation between the
words appearing in articles and the influence
of these words on the readers? impressions,
and is created from a newspaper database us-
ing a word co-occurrence based method. We
considered that some gaps would occur be-
tween values computed by such an unsuper-
vised method and those judged by the readers,
and we conducted experiments with 900 sub-
jects to identify what gaps actually occurred.
Consequently, we propose a new approach
that uses regression equations to correct im-
pression values computed by the method. Our
investigation shows that accuracy is improved
by a range of 23.2% to 42.7% by using regres-
sion equations.
1 Introduction
In recent years, many researchers have been at-
tempting to model the role of emotion in interac-
tions between people or between people and com-
puters, and to establish how to make computers rec-
ognize and express emotions (Picard, 1997; Mas-
saro, 1998; Bartneck, 2001). However, there have
not been many studies that have extracted the im-
pressions that people form after seeing or listening
to text and multimedia content. For multimedia con-
tent such as music and images, several impression-
based retrieval methods have been proposed for lo-
cating paintings and pieces of music that convey im-
pressions similar to those registered by users (Sato
et al, 2000; Kumamoto, 2005; Takayama et al,
2005). By comparison, there are only a few studies
that have extracted the readers? impressions gained
from text such as news articles, novels, and poems
(Kiyoki et al, 1994; Kumamoto and Tanaka, 2005;
Lin et al, 2008).
In this paper, we focus on the impressions that
people gain from reading articles in Japanese news-
papers, and we propose a method for extracting and
quantifying these impressions in real numbers. The
target impressions are limited to those represented
by three bipolar scales, ?Happy ? Sad,? ?Glad ? An-
gry,? and ?Peaceful ? Strained,? and the strength
of each impression is computed as a real number
between 1 and 7 denoting a position on the corre-
sponding scale. Then, interpretation of the position
is grounded based on a seven-point scale. For exam-
ple, on the scale ?Happy ? Sad,? the score 1 equals
?Happy,? the middle score 4 denotes ?Neither happy
nor sad,? and the score 7 equals ?Sad.? If the impres-
sion value of an article is 2.5, then the average reader
will experience an intermediate impression between
?Comparatively happy (2)? and ?A little happy (3)?
from reading the article.
First, we assumed that words causing a certain im-
pression from articles co-occur often with impres-
87
sion words that express that impression, and do not
co-occur very often with impression words that ex-
press the opposite impression. Proceeding with this
assumption, we implemented a method for analyz-
ing co-occurrence relationships between words in
every article extracted from a newspaper database.
We then created an impression lexicon. This lexicon
represents a correlation between the words appear-
ing in articles and the influence of these words on
the readers? impressions. We then implemented a
method that computes impression values of articles
using the lexicon. We considered that some gaps
occur between values computed by such an unsu-
pervised method and those judged by the readers,
and we conducted experiments with 900 subjects to
identify what gaps actually occurred. In these exper-
iments, each subject read ten news articles and esti-
mated her/his impressions of each article using the
three bipolar scales. Thereafter, for each scale, we
drew a scatter diagram to identify the potential cor-
respondence relationships between the values com-
puted by the method and those judged by the sub-
jects. As a result, we found that the correspondence
relationships could be approximately represented by
cubic and quintic regression equations. We, there-
fore, propose a new approach that uses regression
equations to correct impression values computed by
the method.
The rest of this paper is organized as follows. In
Section 2, we present related work. In Section 3,
we present the design of the three bipolar scales, a
method for the automated construction of an impres-
sion lexicon, and a method for computing impres-
sion values of articles using this lexicon. In Section
4, we analyze the correspondence relationships be-
tween values computed using the lexicon and those
judged by the readers, and based on the results of
this analysis, we propose a method of using regres-
sion equations to correct impression values com-
puted using the lexicon. In Section 5, we investi-
gate how far accuracy can be improved by using the
regression equations. Finally, in Section 6, we con-
clude the paper.
2 Related Work
There are many studies that identify information
givers? emotions from some sort of information that
they have transmitted (Cowie et al, 2001; Forbes-
Riley and Litman, 2004; Kleinsmith and Bianchi-
Berthouze, 2007). On the other hand, there are only
a few studies that have extracted the impressions
which information receivers gain from the text that
they have received (Kiyoki et al, 1994; Kumamoto
and Tanaka, 2005; Lin et al, 2008).
Kiyoki et al (1994) have proposed a mathemat-
ical model of meanings, and this model allows a
semantic relation to be established between words
according to a given context. Their method uses a
mathematical model and creates a semantic space
for selecting the impression words that appropriately
express impressions of text according to a given con-
text. In other words, this method does not quantify
impressions of text, but just selects one or more im-
pression words expressing the impressions. Thus,
their aim differs from ours.
Lin et al (2008) have proposed a method for clas-
sifying news articles into emotion categories from
the reader?s perspective. They have adopted a ma-
chine learning approach to build a classifier for the
method. That is, they obtained Chinese news ar-
ticles from a specific news site on the web which
allows a user to cast a vote for one of eight emo-
tions, ?happy,? ?sad,? ?angry,? ?surprising,? ?bor-
ing,? ?heartwarming,? ?awesome,? and ?useful.?
They collected 37,416 news articles along with their
voting statistics, and developed a support vector
machine-based classifier using 25,975 of them as
training data. However, their method just classifies
articles into emotion classes and does not quantify
the reader?s emotions. Thus, their aim also differs
from ours.
Kumamoto and Tanaka (2005) have proposed a
word co-occurrence-based method for quantifying
readers? impressions of news articles in real num-
bers. However, this method is similar to Turney?s
method (Turney, 2002), and it is considered to be a
Japanese version of this method in the broad sense.
Turney?s method is one for classifying various gen-
res of written reviews into ?recommended? or ?not
recommended.? His method extracts phrases with
specific patterns from text, and calculates pointwise
mutual information PMI(i, ?excellent?) between a
phrase i and the reference word ?excellent,? and
PMI(i, ?poor?) between the same phrase i and the
reference word ?poor.? Then, PMI(i, w) is calcu-
88
lated based on a co-occurrence relationship between
i and w. Next, the semantic orientation (SO) of the
phrase i is obtained by calculating the difference be-
tween PMI(i, ?excellent?) and PMI(i, ?poor?). Fi-
nally, SO of the text is determined by averaging
the SO of all the phrases. In contrast, Kumamoto
et al?s method quantifies impressions in real num-
bers, and it can deal with impressions represented
by two bipolar scales, ?Sad ? Glad? and ?Angry ?
Pleased.? For that purpose, reference words are se-
lected for each scale. Since all the reference words
are Japanese, Kumamoto et al?s method extracts
readers? impressions from Japanese articles only.
Also, conditional probabilities are used instead of
PMI. Since these methods fit our assumption that
words causing a certain impression of articles co-
occur often with the impression words that express
that impression, and do not co-occur very often with
impression words that express the opposite impres-
sion, we decided to implement a new method based
on Kumamoto et al?s method.
3 Computing impression values of news
articles using an impression lexicon
3.1 Determining target impressions
Kumamoto (2010) has designed six bipolar scales
suitable for representing impressions of news arti-
cles: ?Happy ? Sad,? ?Glad ? Angry,? ?Interesting ?
Uninteresting,? ?Optimistic ? Pessimistic,? ?Peace-
ful ? Strained,? and ?Surprising ? Common.? First,
he conducted nine experiments, in each of which
100 subjects read ten news articles and estimated
their impressions on a scale from 1 to 5 for each of
42 impression words. These 42 impression words
were manually selected from a Japanese thesaurus
(Ohno and Hamanishi, 1986) as words that can ex-
press impressions of news articles. Next, factor anal-
ysis was applied to the data obtained in the experi-
ments, and consequently the 42 words were divided
into four groups: negative words, positive words,
two words that were ?uninteresting? and ?common,?
and two words that were ?surprising? and ?unex-
pected.? In the meantime, after cluster analysis of
the data, the 42 words were divided into ten groups.
Based on the results of both analyses, the author cre-
ated the six bipolar scales presented above. How-
ever, he showed that impressions on the ?Surpris-
ing ? Common? scale differed greatly among indi-
viduals in terms of their perspective. In addition,
he insisted that processing according to the back-
ground knowledge, interest, and character of indi-
viduals was required to deal with the impressions
represented by the two scales ?Interesting ? Unin-
teresting? and ?Optimistic ? Pessimistic.? There-
fore, we decided not to use these three scales at
the present stage, and adopted the remaining three
scales, ?Happy ? Sad,? ?Glad ? Angry,? and ?Peace-
ful ? Strained.?
3.2 Constructing an impression lexicon
An impression lexicon plays an important role in
computing impressions of news articles. In this pa-
per, we describe the implementation of a method
for automatically constructing an impression lexicon
based on Kumamoto et al?s method as described ear-
lier.
First, while two contrasting reference words are
used for each scale in their method, two contrasting
sets, each consisting of multiple reference words, are
used in this paper.
Next, let the set of reference words which ex-
presses an impression at the left of a scale be SL,
and let the set of reference words which expresses
an impression at the right of the scale be SR. Arti-
cles including one or more reference words in SL or
SR are all extracted from a newspaper database, and
the number of reference words belonging to each
set is counted in each article. For this we used the
2002 to 2006 editions of the Yomiuri Newspaper
Text Database as the newspaper database. Then, let
the articles in each of which the number of refer-
ence words belonging to SL is larger than the num-
ber of reference words belonging to SR be AL, and
let the number of articles in AL be NL. Let the arti-
cles in each of which the number of reference words
belonging to SL is smaller than the number of ref-
erence words belonging to SR be AR, and let the
number of articles in AR be NR. Next, all words are
extracted from each of AL and AR except for par-
ticles, adnominal words1, and demonstratives, and
the document frequency of each word is measured.
Then, let the document frequency in AL of a word w
1This part of speech exists only in Japanese, not in English.
For example, ?that,? ?so called,? and ?of no particular distinc-
tion? are dealt with as adnominal words in Japanese.
89
Table 1: Specifications of our impression lexicon.
Scales # of entries WL WR
Happy ? Sad 387,428 4.90 3.80
Glad ? Angry 350,388 4.76 3.82
Peaceful ? Strained 324,590 3.91 4.67
be NL(w), and let the document frequency in AR of
a word w be NR(w). The revised conditional prob-
abilities of a word w are defined as follows.
PL(w) =
NL(w)
NL
, PR(w) =
NR(w)
NR
These formula are slightly different from the condi-
tional probabilities used in their method, and only
articles that satisfy the assumptions described above
are used in order to calculate PL(w) and PR(w).
Finally, the impression value v(w) of a word w is
calculated using these PL(w) and PR(w) as follows.
v(w) = PL(w) ?WL
PL(w) ?WL + PR(w) ?WR
WL = log10 NL, WR = log10 NR
That is, a weighted interior division ratio v(w) of
PL(w) and PR(w) is calculated using these formu-
las, and stored as an impression value of w in the
scale ?SL ? SR? in an impression lexicon. Note that
WL and WR denote weights, and the larger NL and
NR are, the heavier WL and WR are.
The numbers of entries in the impression lexicon
constructed as above are shown in Table 1 together
with the values of WL and WR obtained. Further,
the two contrasting sets of reference words2, which
were used in creating the impression lexicon, are
enumerated in Table 2 for each scale. These words
were determined after a few of trial and error and
are based on two criteria, namely (i) it is a verb or
adjective that expresses either of two contrasting im-
pressions represented by a scale, and (ii) as far as
possible, it does not suggest other types of impres-
sions.
2These words were translated into English by the authors.
Table 2: Reference words prepared for each scale.
Scales Reference words
Happy tanoshii (happy), tanoshimu (en-
joy), tanosimida (look forward to),
tanoshigeda (joyous)
? Sad kanashii (sad), kanashimu (suffer
sadness), kanashimida (feel sad),
kanashigeda (look sad)
Glad ureshii (glad), yorokobashii
(blessed), yorokobu (feel delight)
? Angry ikaru/okoru (get angry), ikidooru
(become irate), gekidosuru (get en-
raged)
Peaceful nodokada (peaceful), nagoyakada
(friendly), sobokuda (simple), an-
shinda (feel easy)
? Strained kinpakusuru (strained), bukimida
(scared), fuanda (be anxious), os-
oreru (fear)
3.3 Computing impression values of articles
For each scale, the impression value of an article
is calculated as follows. First, the article is seg-
mented into words using ?Juman? (Kurohashi et al,
1994)3, one of the most powerful Japanese morpho-
logical analysis systems, and an impression value
for each word is obtained by consulting the impres-
sion lexicon constructed as described in 3.2. Sev-
enteen rules that we designed are then applied to
the Juman output. For example, there is a rule
that a phrase of a negative form like ?sakujo-shi-
nai (do not erase)? should not be divided into a verb
?shi (do),? a suffix ?nai (not),? and an action noun
?sakujo (erasion)? but should be treated as a single
verb ?sakujo-shi-nai (do-not-erase).? There is also a
rule that an assertive phrase such as ?hoomuran-da
(is a home run)? should not be divided into a cop-
ula ?da (is)? and a noun ?hoomuran (a home run)?
but should form a single copula ?hoomuran-da (is-
a-home-run).? Further, there is a rule that a phrase
with a prefix, such as ?sai-charenji (re-challenge)?
should not be divided into a prefix ?sai (re)? and an
3Since there are no boundary markers between words in
Japanese, word segmentation is needed to identify individual
words.
90
action noun ?charenji (challenge)? but should form a
single action noun ?sai-charenji (re-challenge).? All
the rules are applied to the Juman output in creating
an impression lexicon and computing the impression
values of news articles. Finally, an average of the
impression values obtained for all the words except
for particles, adnominal words, and demonstratives
is calculated and presented as an impression value
of the article.
4 Correcting computed impression values
4.1 Analyzing a correspondence relationship
between computed and manually rated
values
We considered that some gaps would occur be-
tween impression values computed by an unsuper-
vised method such as the one we used and those of
the readers. We, therefore, conducted experiments
in which a total of 900 people participated as sub-
jects, and identified what gaps actually occurred.
First, we conducted experiments with 900 sub-
jects, and obtained data that described correspon-
dence relationships between news articles and im-
pressions to be extracted from the articles. That is,
the 900 subjects were randomly divided into nine
equal groups, each group consisting of 50 males and
50 females, and 90 articles which were selected from
the 2002 edition of the Mainichi Newspaper Text
Database4 were randomly divided into nine equal
parts. Then, each subject was asked to read the ten
articles presented in a random order and rate each
of them using three seven-point bipolar scales pre-
sented in a random order. The scales we used were
?Happy ? Sad,? ?Glad ? Angry,? and ?Peaceful ?
Strained,? and the subjects were asked to assess, on
a scale of 1 to 7, the intensity of each impression,
represented by each scale, from reading a target ar-
ticle. For example, on the scale ?Happy ? Sad,? the
score 1 equaled ?Happy,? the middle score 4 denoted
?Neither happy nor sad,? and the score 7 equaled
?Sad.? After the experiments, for each scale, we cal-
culated an average of the 100 values rated for every
article. We regarded this average as the impression
value to be extracted from the article. Note that, in
these experiments, we presented only the first para-
4This database is different from the Yomiuri newspaper
database we used in creating an impression lexicon.
graphs of the original news articles to the subjects.
This procedure was derived from the fact that people
can understand the outline of a news article by just
reading the first paragraph of the article, as well as
the fact that impressions of an article may change in
every paragraph. Development of a method for fol-
lowing the change of impressions in an article will
be a future project.
Next, impression values for the first paragraphs
of the 90 articles were computed by the method we
implemented in 3.3, where the first paragraphs were
identical to those presented to the subjects in the ex-
periments. Note that, according to the definition of
our equations, these impression values are close to
1 when impressions on the left of a scale are felt
strongly, and are close to 0 when impressions on the
right of a scale are felt strongly. We therefore used
the following formula and converted the computed
value into a value between 1.0 and 7.0.
Converted = (1? Computed) ? 6 + 1
Next, for each scale, we drew a scatter diagram
to identify the potential correspondence relationship
between these converted values and the averages ob-
tained in the experiments, as illustrated in Figure 1.
We can see from any of the scatter diagrams that the
impression values manually rated by the subjects are
positively correlated with those automatically com-
puted by the method we implemented. In fact, their
coefficients of correlation are 0.76, 0.84, and 0.78
from the case at the top of the figure, which are all
high. This not only means that, as an overall trend,
the underlying assumption of this paper is satisfied,
but also indicates that the correspondence relation-
ships can be represented by regression equations.
4.2 Correcting computed impression values
with regression equations
Next, we applied regression analysis to the con-
verted values and the averages, where the converted
values were used as the explanatory variable, and the
averages were used as the objective variable. In this
regression analysis, various regression models (Kan,
2000) such as linear function, logarithmic function,
logistic curve, quadratic function, cubic function,
quartic function, and quintic function were used on
91
2.0
3.0
4.0
5.0
6.0
2.5 3.0 3.5 4.0 4.5 5.0 
Converted Values
Ma
nua
lly 
rate
d
(a) In the case of ?Happy ? Sad?
2.0
3.0
4.0
5.0
6.0
2.5 3.0 3.5 4.0 4.5 5.0 
Converted Values
Ma
nua
lly 
rate
d
(b) In the case of ?Glad ? Angry?
2.0
3.0
4.0
5.0
6.0
2.5 3.0 3.5 4.0 4.5 5.0 
Converted Values
Ma
nua
lly 
rate
d
(c) In the case of ?Peaceful ? Strained?
Figure 1: Scatter diagrams and regression equations.
a trial basis. As a result, the regression equation,
which had the highest coefficient of determination,
was determined as an optimal function denoting the
correspondence relationship between the converted
values and the averages in each scale. This means
that, for each scale, the impression value of an ar-
ticle was more accurately obtained by correcting a
value computed by the method we implemented us-
ing the corresponding regression equation.
The regression equations obtained here were
??1.636x3+18.972x2?70.686x+88.515? for the
?Happy ? Sad,? ?2.385x5?46.872x4+363.660x3?
1391.589x2+2627.063x?1955.306? for the ?Glad
? Angry,? and ??1.714x3 + 21.942x2 ? 90.792x+
124.822? for the ?Peaceful ?Strained,? and they are
Table 3: Change of the Euclidean distance by using re-
gression equations.
Scales DBefore DAfter Rate1
Happy ? Sad 0.94 0.67 29.0%
Glad ? Angry 0.83 0.47 42.7%
Peaceful 0.82 0.63 23.2%
? Strained
already illustrated on the corresponding scatter dia-
grams in Figure 1. Their coefficients of determina-
tion were 0.63, 0.81, 0.64, respectively, which were
higher than 0.5 in all scales. This means that the
results of regression analysis were good. In addi-
tion, we can see from Figure 1 that each regression
equation fits the shape of the corresponding scatter
diagram.
5 Performance Evaluation
First, we estimated the accuracy of the proposed
method for learned data. For that, we used the data
obtained in the experiments described in 4.1, and in-
vestigated how far gaps between the computed val-
ues and the averages of the manually rated values
were reduced by using the regression equations. The
results are shown in Table 3. In this table, DBefore de-
notes the Euclidean distance between the computed
values without correction and the averages for the 90
articles, and DAfter denotes the Euclidean distance
between the values corrected with the correspond-
ing regression equation and the averages for the 90
articles. Then Rate1 was calculated as an improve-
ment rate by the following formula:
Rate1 =
DBefore ?DAfter
DBefore
? 100
Table 3 shows fairly high improvement rates in all
the scales, and hence we find that accuracy is im-
proved by using the regression equations. In partic-
ular, DAfter for the scale ?Glad ? Angry? is less than
0.5 or a half of a step and is sufficiently small.
Next, we calculated the accuracy of the method
(Kumamoto and Tanaka, 2005) on which the pro-
posed method is based, and compared it with that of
the proposed method. The results are shown in Ta-
ble 4. In this table, DBaseline denotes the Euclidean
92
Table 4: Comparison with a baseline method.
Scales DBaseline DProposed Rate2
Happy ? Sad 0.99 0.67 32.3%
Glad ? Angry 0.82 0.47 42.7%
Peaceful 1.00 0.63 37.0%
? Strained
distance between the values computed by the base-
line method and the averages for the 90 articles, and
DProposed is equivalent to DAfter in Table 3. Then
Rate2 is calculated as an improvement rate by the
following formula:
Rate2 =
DBaseline ?DProposed
DBaseline
? 100
Table 4 also shows that fairly high improvement
rates were obtained in all the scales. Note that the
baseline method was implemented in the following
way. First, a pair of reference words was prepared
for each scale. Actually, the pair ?tanoshii (happy)?
and ?kanashii (sad)? was used for the scale ?Happy
? Sad?; the pair ?ureshii (glad)? and ?ikaru/okoru
(get angry)? for the scale ?Glad ? Angry?; and
?nodokada (peaceful)? and ?kinpakusuru (strained)?
for the scale ?Peaceful ? Strained.? Next, an impres-
sion lexicon for the baseline method was constructed
from the news articles which were used to construct
our impression lexicon.
The results shown in Tables 3 and 4 prove that the
proposed method has a high level of accuracy for the
articles used in obtaining the regression equations.
As the next step, we estimated the accuracy of the
proposed method for unlearned data. For that, we
performed five-fold cross-validation using the data
obtained in 4.1. First, the data were randomly di-
vided into five equal parts, each part consisting of
data for 18 articles. Next, a learned data set was cre-
ated arbitrarily from four of the five parts, or data
for 72 articles, and an unlearned data set was cre-
ated from the remaining part, or data for 18 arti-
cles. Regression analysis was then applied to the
learned data set. As a result, an optimal regres-
sion equation that expressed a correspondence rela-
tionship between the computed values and the av-
erages of the manually rated values in the learned
Table 5: Estimation of overall accuracy based on five-fold
cross-validation.
Scales DMean DMax DMin
Happy ? Sad 0.69 0.78 0.57
Glad ? Angry 0.49 0.58 0.42
Peaceful ? Strained 0.64 0.81 0.50
Table 6: Influence of size of target newspaper database to
Euclidean distance.
Editions
Scales 2002-2006 2005-2006 2006
Happy ? Sad 0.67 0.69 0.73
Glad ? Angry 0.47 0.50 0.54
Peaceful 0.63 0.65 0.69
? Strained
data set was obtained for each scale. Next, we cal-
culated the Euclidean distance between the averages
for 18 articles in the unlearned data set and the val-
ues which were computed from the 18 articles them-
selves and corrected with the corresponding optimal
regression equation. The results are shown in Ta-
ble 5. In this table, DMean, DMax, and DMin de-
note the mean, maximum, and minimum values of
the five Euclidean distances calculated from a total
of five unlearned data sets, respectively. Comparing
DProposed in Table 4 and DMean in Table 5, we find
that they are almost equivalent. This means that the
proposed method is also effective for unlearned data.
Finally, we investigated how the accuracy of the
proposed method was influenced by the size of the
newspaper database used in constructing an impres-
sion lexicon. First, using each of the 2002 to 2006
editions, the 2005 to 2006 editions, and the 2006
edition only, impression lexicons were constructed.
Three regression equations were then obtained for
each lexicon in the same way. Next, for each scale,
we calculated the Euclidean distance between the
values which were computed from all the 90 arti-
cles using each lexicon and corrected with the corre-
sponding regression equation, and the averages ob-
tained in 4.1. The results are shown in Table 6. Table
6 shows that the accuracy of the proposed method is
reduced slightly as the size of newspaper database
93
becomes smaller. Conversely, this suggests that the
accuracy of the proposed method can be improved as
the size of newspaper database increases. We would
like to verify this suggestion in the near future.
6 Conclusion
This paper has proposed a method for quantitatively
identifying the impressions that people gain from
reading Japanese news articles. The key element
of the proposed method lies in a new approach that
uses regression equations to correct impression val-
ues computed from news articles by an unsuper-
vised method. Our investigation has shown that ac-
curacy for learned data is improved by a range of
23.2% to 42.7% by using regression equations, and
that accuracy for unlearned data is almost equiva-
lent to the accuracy for learned data. Note that, in
this paper, the target impressions are limited to those
represented by three bipolar scales, ?Happy ? Sad,?
?Glad ? Angry,? and ?Peaceful ? Strained,? and the
strength of each impression is computed as a real
number between 1 and 7 denoting a position on the
corresponding scale.
Our main future work is described below. Since
the proposed method uses a word co-occurrence
based method to construct an impression lexicon, it
may not be effective for other types of scale. We
therefore need to examine and consider what kinds
of scales are suitable for the proposed method. Per-
sonal adaptation is important in methods dealing
with impressions created by such artworks as music
and paintings. In order to develop a method for more
accurately quantifying readers? impressions of news
articles, we will also tackle this personal adaptation
problem. Further, we plan to integrate the proposed
method into a search engine, a recommendation sys-
tem, and an electronic book reader, and to verify the
effectiveness of readers? impressions of news arti-
cles in creating a ranking index for information re-
trieval and recommendation, or in determining the
type of emotional speech used in reading an e-paper.
Acknowledgments
A part of this work was sponsored by National In-
stitute of Information and Communications Tech-
nology (NICT), Japan, and was achieved under the
project named ?Evaluating Credibility of Web Infor-
mation.?
References
Christoph Bartneck. 2001. How convincing is Mr. Data?s
smile: Affective expressions of machines. User Mod-
eling and User-Adapted Interaction, 11:279?295.
R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis,
S. Kollias, W. Fellenz, and J. G. Taylor. 2001. Emo-
tion recognition in human-computer interaction. IEEE
Signal Processing Magazine, 18:32?80.
Kate Forbes-Riley and Diane J. Litman. 2004. Predict-
ing emotion in spoken dialogue from multiple knowl-
edge sources. In Proc. of Human Language Technol-
ogy Conf. of the NAACL, pages 201?208.
Tamio Kan. 2000. Multivariate Statistical Analysis.
Gendai-Sugakusha, Kyoto, Japan.
Yasushi Kiyoki, Takashi Kitagawa, and Takanari
Hayama. 1994. A metadatabase system for seman-
tic image search by a mathematical model of meaning.
SIGMOD Rec., 23:34?41.
Andrea Kleinsmith and Nadia Bianchi-Berthouze. 2007.
Recognizing affective dimensions from body posture.
In Proc. of the Int. Conf. on Affective Computing and
Intelligent Interaction, volume LNCS 4738, pages 48?
58, September.
Tadahiko Kumamoto and Katsumi Tanaka. 2005. Pro-
posal of impression mining from news articles. In
Proc. of Int. Conf. on Knowledge-Based Intelligent
Information and Engineering Systems, volume LNAI
3681, pages 901?910. Springer.
Tadahiko Kumamoto. 2005. Design and evaluation of
a music retrieval scheme that adapts to the user?s im-
pressions. In Proc. of Int. Conf. on User Modeling,
volume LNAI 3538, pages 287?296. Springer.
Tadahiko Kumamoto. 2010. Design of impression scales
for assessing impressions of news articles. In Proc. of
DASFAA Workshop on Social Networks and Social
Media Mining on the Web, volume LNCS 6193, pages
285?295.
Sadao Kurohashi, Toshihisa Nakamura, Yuji Matsumoto,
and Makoto Nagao. 1994. Improvements of Japanese
morphological analyzer JUMAN. In Proc. of the
Int. Workshop on Sharable Natural Language Re-
sources, pages 22?28, Nara, Japan.
Kevin Hsin-Yih Lin, Changhua Yang, and Hsin-Hsi
Chen. 2008. Emotion classification of online news
articles from the reader?s perspective. In Proc. of the
IEEE/WIC/ACM Int. Conf. on Web Intelligence and In-
telligent Agent Technology, pages 220?226, Washing-
ton, DC, USA. IEEE Computer Society.
Dominic W. Massaro. 1998. Perceiving Talking Faces:
From Speech Perception to a Behavioral Principle.
MIT Press, USA.
94
Susumu Ohno and Masando Hamanishi, editors. 1986.
Ruigo-Kokugo-Jiten. Kadokawa Shoten Publishing
Co.,Ltd., Tokyo, Japan.
Rosalind W. Picard. 1997. Affective Computing. MIT
Press.
Akira Sato, Jun Ogawa, and Hajime Kitakami. 2000. An
impression-based retrieval system of music collection.
In Proc. of the Int. Conf. on Knowledge-Based Intelli-
gent Information Engineering Systems & Allied Tech-
nologies, volume 2, pages 856?859, Brighton, UK.
Tsuyoshi Takayama, Hirotaka Sasaki, and Shigeyuki
Kuroda. 2005. Personalization by relevance ranking
feedback in impression-based retrieval for multimedia
database. Journal of Systematics, Cybernetics and In-
formatics, 3(2):85?89.
Peter D. Turney. 2002. Thumbs up or thumbs down?
semantic orientation applied to unsupervised classifi-
cation of reviews. In Proc. of the Annual Meeting of
the Association for Computational Linguistics, pages
417?424, Philadelphia, USA.
95
