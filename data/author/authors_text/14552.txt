Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 734?742,
Beijing, August 2010
Exploiting Structured Ontology to Organize Scattered Online Opinions
Yue Lu, Huizhong Duan, Hongning Wang, ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana-Champaign
{yuelu2,duan9,wang296,czhai}@illinois.edu
Abstract
We study the problem of integrating scat-
tered online opinions. For this purpose,
we propose to exploit structured ontology
to obtain well-formed relevant aspects to
a topic and use them to organize scattered
opinions to generate a structured sum-
mary. Particularly, we focus on two main
challenges in implementing this idea, (1)
how to select the most useful aspects from
a large number of aspects in the ontology
and (2) how to order the selected aspects
to optimize the readability of the struc-
tured summary. We propose and explore
several methods for solving these chal-
lenges. Experimental results on two dif-
ferent data sets (US Presidents and Digital
Cameras) show that the proposed methods
are effective for selecting aspects that can
represent the major opinions and for gen-
erating coherent ordering of aspects.
1 Introduction
The explosive growth of online opinions raises in-
teresting challenges for opinion integration and
summarization. It is especially interesting to in-
tegrate and summarize scattered opinions in blog
articles and forums as they tend to represent the
general opinions of a large number of people and
get refreshed quickly as people dynamically gen-
erate new content, making them valuable for un-
derstanding the current views of a topic.
However, opinions in blogs and forums are
usually fragmental, scattered around, and buried
among other off-topic content, so it is quite chal-
lenging to organize them in a meaningful way.
Traditional text summarization techniques gener-
ate an unstructured list of sentences as a sum-
mary, which cannot reveal representative opinions
on different aspects of a topic or effectively facil-
itate navigation into the huge opinion space. To
address this limitation, recent work has shown the
usefulness of generating a structured summary of
opinions, in which related opinions are grouped
into topical aspects with explicit labeling of all the
aspects. A major challenge in producing such a
structured summary is how to generate these as-
pects for an arbitrary topic (e.g., products, politi-
cal figures, policies, etc.). Intuitively, the aspects
should be concise phrases that can both be easily
interpreted in the context of the topic under con-
sideration and capture the major opinions. How-
ever, where can we find such phrases and which
phrases should we select as aspects? Furthermore,
once we selected aspects, how should we order
them to improve the readability of a structured
summary? One way to generate aspects is to clus-
ter all the opinion sentences and then identify rep-
resentative phrases in each cluster. Although as-
pects selected in this way can effectively capture
the major opinions, a major limitation is that it is
generally hard to ensure that the selected phrases
are well connected with the given topic (Chen and
Dumais, 2000).
In this paper, we propose a novel approach
to generating aspects by leveraging the ontolo-
gies with structured information that are available
online, such as open domain knowledge base in
Freebase1. Such kind of ontology data is not in
small scale by any measure. For example, Free-
base alone contains more than 10 million topics,
3000 types, and 30,000 properties; moreover, it is
constantly growing as people collaboratively con-
tribute. Freebase provides different properties for
different types of topics such as personal infor-
mation for a ?US President? and product features
for a ?Digital Camera?. Since this kind of re-
sources can provide related entities/relations for a
1http://www.freebase.com
734
wide range of topics , our general idea is to lever-
age them as guidance for more informed organi-
zation of scattered online opinions, and in partic-
ular, to select the most important properties of a
topic from such structured ontology as aspects to
generate a structured opinion summary. A signif-
icant advantage of this approach to aspect genera-
tion is that the selected aspects are guaranteed to
be very well connected with the topic, but it also
raises an additional challenge in selecting the as-
pects to best capture the major opinions from a
large number of aspects provided for each topic in
the ontology. Different from some existing work
on exploiting ontologies, e.g., (Sauper and Barzi-
lay, 2009), which relies on training data, we focus
on exploring unsupervised approaches, which can
be applied to a larger scope of topics.
Specifically, given a topic with entries in an on-
tology and a collection of scattered online opin-
ions about the topic, our goal is to generate a
structured summary where representative major
opinions are organized with well aligned aspects
and in an order easy for human to follow. We
propose the following general approach: First, re-
trieval techniques are employed to align opinions
to relevant aspects. Second, a subset of most inter-
esting aspects are selected. Third, we will further
order the selected aspects to present them in a rea-
sonable order. Finally, for the opinions uncovered
by the selected aspects from the ontology, we use
a phrase ranking method to suggest new aspects to
add to the ontology for increasing its coverage.
Implementing the second and third steps in-
volves special challenges. In particular, without
any training data, it is unclear how we should
show the most interesting aspects in ontology with
major opinions aligned and which presentation
order of aspects is natural and intuitive for hu-
man. Solving these two challenges is the main
focus of this paper. We propose three meth-
ods for aspect selection, i.e., size-based, opinion
coverage-based, and conditional entropy-based
methods, and two methods for aspect ordering,
i.e., ontology-ordering and coherence ordering.
We evaluate our methods on two different types of
topics: US Presidents and Digital Cameras. Qual-
itative results demonstrate the utility of integrating
opinions based on structured ontology as well as
the generalizability of proposed methods. Quan-
titative evaluation is also conducted to show the
effectiveness of our methods.
Note that we use the term ?opinion? to broadly
refer to any discussion in opinionated sources
such as blogs and reviews. This allows us to for-
mulate and solve the problem in a general way.
Indeed, the main goal of our work is to extract
and organize the major opinions about a topic that
are buried in many scattered opinionated sources
rather than perform deeper understanding of opin-
ions (e.g., distinguishing positive from negative
opinions), which can be done by using any exist-
ing sentiment analysis technique as an orthogonal
post-processing step after applying our method.
2 Related Work
Aspect summarization, i.e., structured opinion
summarization over topical aspects, has attracted
much attention recently. Existing work iden-
tifies aspects using frequent-pattern/association-
rule mining, e.g. (Liu et al, 2005; Popescu and
Etzioni, 2005), sentence clustering, e.g. (Ga-
mon et al, 2005; Leouski and Croft, 1996), or
topic modeling, e.g. (Mei et al, 2006; Titov and
McDonald, 2008). After that, meaningful and
prominent phrases need to be selected to repre-
sent the aspects, e.g. (Zhao and He, 2006; Mei
et al, 2007). However, these methods suffer from
the problem of producing trivial aspects. Conse-
quently, some of the aspects generated are very
difficult to interpret (Chen and Dumais, 2000). In
this paper, we propose a different kind of approach
that is to use aspects provided by ontology which
are known to be relevant and easy to interpret.
Ontology is used in (Carenini et al, 2005) but
only for mapping product features. The closest
work to ours are (Lu and Zhai, 2008; Sauper and
Barzilay, 2009); both try to use well-written arti-
cles for summarization. However, (Lu and Zhai,
2008) assumes the well-written article is struc-
tured with explicit or implicit aspect information,
which does not always hold in practice, while
(Sauper and Barzilay, 2009) needs a relatively
large amount of training data in the given domain.
In comparison, our work only needs the ontology
information for the given topic which is much eas-
ier to obtain from resources such as Freebase.
735
3 Methods
Given (1) an input topic T , (2) a large number of
aspects/properties A = {A1, ..., Am} from an on-
tology that are related to T , and (3) a huge col-
lection of scattered opinion sentences about the
topic DT = {s1, . . . , sn}, our goal is to gener-
ate a structured organization of opinions that are
both aligned well with the interesting aspects and
representative of major opinions about the topic.
The envisioned structured organization consists
of a sequence of selected aspects from ontol-
ogy ordered to optimize readability and a set of
sentences matching each selected aspect. Once
we obtain a set of sentences in each aspect, we
can easily apply a standard text summarization
method to further summarize these sentences, thus
the unique challenges related to our main idea of
exploiting ontology are the following, which are
also the main focus of our study:
Aspect Selection: How can we select a subset of
aspects A? ? A to capture the major opinions in
our opinion set DT ?
Aspect Ordering: How can we order a subset of
selected aspects A? so as to present them in an or-
der pi(A?) that is most natural with respect to hu-
man perception?
New Aspects Suggestion: Can we exploit the
opinions in DT to suggest new aspects to be added
to the ontology?
3.1 Aspect Selection
In order to align the scattered opinions to the
most relevant aspects, we first use each aspect la-
bel Ai ? A as a query to retrieve a set of rel-
evant opinions in the collection Si ? DT with
a standard language modeling approach, i.e., the
KL-divergence retrieval model (Zhai and Lafferty,
2001). Up to 1000 opinion sentences are retrieved
for each aspect; each opinion sentence can be po-
tentially aligned to several aspects. In this way,
scattered online discussion are linked to the most
relevant aspects in the ontology, which enables a
user to use aspects as ?semantic bridges? to navi-
gate into the opinion space..
However, there are usually a lot of candidate
aspects in an ontology, and only some are heav-
ily commented in online discussions, so showing
all the aspects is not only unnecessary, but also
overwhelming for users. To solve this problem,
we propose to utilize the aligned opinions to fur-
ther select a subset of the most interesting aspects
A? ? A with size k. Several approaches are pos-
sible for this subset selection problem.
Size-based: Intuitively, the selected subset A?
should reflect the major opinions. So a straightfor-
ward method is to order the aspects Ai by the size
of the aligned opinion sentences Si, i.e., the num-
ber of relevant opinion sentences, and then select
the top k ones.
Opinion Coverage-based: The previous method
does not consider possible redundancy among the
aspects. A better approach is to select the subset
that covers as many distinct opinion sentences as
possible. This can be formulated as a maximum
coverage problem, for which a greedy algorithm
is known to be a good approximation: we select
one aspect at a time that is aligned with the largest
number of uncovered sentences.
Conditional Entropy-based: Aspects from a struc-
tured ontology are generally quite meaningful, but
they are not designed specifically for organizing
the opinions in our data set. Thus, they do not
necessarily correspond well to the natural clus-
ters in scattered opinions. To obtain aspects that
are aligned well with the natural clusters in scat-
tered opinions, we can first cluster DT into l
clusters C = {C1, . . . , Cl} using K-means with
TF ? IDF as features, and then choose the sub-
set of aspects that minimize Conditional Entropy
of the cluster label given the aspect:
A? = argminH(C|A?) = argmin?
??
?
Ai?A?,Ci?C
p(Ai, Ci) log
p(Ai, Ci)
p(Ai)
?
?
This Conditional Entropy measures the uncer-
tainty about the cluster label of a sentence given
the knowledge of its aspect. Intuitively, if the as-
pects are aligned well with the clusters, we would
be able to predict well the cluster label of a sen-
tence if we know its aspect, thus there would be
less uncertainty about the cluster label. In the
extreme case when the cluster label can be com-
pletely determined by the aspect, the conditional
entropy would reach its minimum (i.e., 0). Intu-
itively, the conditional entropy-based method es-
sentially selects the most appropriate aspects from
736
Algorithm 1 Greedy Algorithm for
Conditional Entropy Based Aspect Selection
Input: A = {A1, ..., Am}
Output: k-sized A? ? A
1: A? = {?mi=1Ai}2: for j=1 to k do
3: bestH = ?; bestA = A0
4: for each Ai in A do
5: tempA? = {Ai, A? \Ai}
6: if H(C|tempA?) < bestH then
7: bestH = H(C|tempA?)
8: bestA = Ai
9: A? = {bestA,A? \ bestA}
10: output A?
the ontology to label clusters of opinions.
The exact solution of this combinatorial optimiza-
tion problem is NP-complete, so we employ a
polynomial time greedy algorithm to approximate
it: in the i-th iteration, we select the aspect that
can minimize the conditional entropy given the
previous i ? 1 selected aspects. Pseudo code is
given in Algorithm 1.
3.2 Aspect Ordering
In order to present the selected aspects to users
in a most natural way, it is important to obtain a
coherent order of them, i.e., generating an order
consistent with human perception. To achieve this
goal, our idea is to use human written articles on
the topic to learn how to organize the aspects au-
tomatically. Specifically, we would order aspects
so that the relative order of the sentences in all the
aspects would be as consistent with their order in
the original online discussions as possible.
Formally, the input is a subset of selected as-
pects A?; each Ai ? A? is aligned with a set of
relevant opinion sentences Si = {Si,1, Si,2, ...}.
We define a coherence measurement function over
sentence pairs Co(Si,k, Sj,l), which is set to 1 iff
Si,k appears before Sj,l in the same article. Other-
wise, it is set to 0. Then a coherence measurement
function over an aspect pair can be calculated as
Co(Ai, Aj) =
?
Si,k?Si,Sj,l?Sj Co(Si,k, Sj,l)
|Si||Sj |
As an output, we would like to find a permutation
p?i(A?) that maximizes the coherence of all pair-
wise aspects, i.e.,
p?i(A?) = arg max
pi(A?)
?
Ai,Aj?A?,Ai?Aj
Co(Ai, Aj)
Algorithm 2 Greedy Algorithm for
Coherence Based Aspect Ordering
Input: A
Output: pi(A)
1: for each Ai, Aj in A do
2: calculate Co(Ai, Aj)
3: for p = 1 to len = A.size() do
4: Max = A[1]
5: for each aspect Ai in A do
6: Ai.coherence = 0
7: for each aspect Aj in pi(A) do
8: Ai.coherence+ = Co(Aj , Ai)
9: for each aspect Aj in A, j 6= i do
10: Ai.coherence+ = Co(Ai, Aj)
11: if Ai.coherence > Max.coherence then
12: Max = Ai
13: remove Max from A; add Max to pi(A)
14: output pi(A)
where Ai ? Aj means that Ai is before Aj . It
is easy to prove that the problem is NP-complete.
Therefore, we resort to greedy algorithms to find
approximations of the solution. Particularly we
view the problem as a ranking problem. The al-
gorithm proceeds by finding at each ranking po-
sition an aspect that can maximize the coherence
measurement, starting from the top of the rank list.
The detailed algorithm is given in Algorithm 2.
3.3 New Aspects Suggestion
Finally, if the opinions cover more aspects than in
the ontology, we also want to identify informative
phrases to label such extra aspects; such phrases
can also be used to further augment the ontology
with new aspects.
This problem is similar to existing work on gen-
erating labels for clusters (Zeng et al, 2004) or
topic models (Mei et al, 2007). Here we employ
a simple but representative technique to demon-
strate the feasibility of discovering interesting new
aspects for augmenting the ontology. We first ex-
tract named entities from scattered opinions DT
using Stanford Named Entity Recognizer (Finkel
et al, 2005). After that, we rank the phrases by
pointwise Mutual Information (MI):
MI(T, ph) = log P (T, ph)P (T )P (ph)
where T is the given topic and ph refers to a candi-
date entity phrase. P (T, ph) is proportional to the
number of opinion sentences they co-occur; P (T )
or P (ph) are proportional to the number of times
T or ph appears. A higher MI value indicates a
737
Statistics Category 1 Category 2
US president Digital Camera
Number of Topics 36 110
Number of Aspects 65?26 32?4
Number of Opinions 1001?1542 170?249
Table 1: Statistics of Data Sets
stronger association. We can then suggest the top
ranked entity phrases that are not in the selected
aspects as new aspects.
4 Experiments
4.1 Data Sets
To examine the generalizability of our methods,
we test on two very different categories of top-
ics: US Presidents and Digital Cameras.2 For the
ontology, we leverage Freebase, downloading the
structured ontology for each topic. For the opin-
ion corpus, we use blog data for US Presidents and
customer reviews for Digital Cameras. The blog
entries for US Presidents were collected by using
Google Blog Search3 with the name of a president
as the query. Customer reviews for Digital Cam-
eras were crawled from CNET4. The basic statis-
tics of our data sets is shown in Table 1. For all the
data collections, Porter stemmer (Porter, 1997) is
applied and stop words are removed.
4.2 Sample Results
We first show sample results of automatic orga-
nization of online opinions. We use the opin-
ion coverage-based algorithm to select 10 aspects
(10-20 aspects were found to be optimal in (Ka?ki,
2005)) and then apply the coherence-based aspect
ordering method. The number of clusters is set so
that there are on average 15 opinions per cluster.
Opinion Organization: Table 2 and Table 3
present sample results for President Ronald Rea-
gan and Sony Cybershot DSC-W200 camera re-
spectively5. We can see that (1) although Freebase
aspects provide objective and accurate informa-
tion about the given topics, extracted opinion sen-
tences offer additional subjective information; (2)
aligning scattered opinion sentences to most rel-
evant aspects in the ontology helps digestion and
2We have made our data sets available at http://
timan.cs.uiuc.edu/downloads.html .
3http://blogsearch.google.com
4http://www.cnet.com
5Due to space limit, we only show the first few aspects as
output by our methods.
navigation; and (3) the support number, which is
the number of opinion sentences aligned to an as-
pect, can show the popularity of the aspect in the
online discussions.
Adaptability of Aspect Selection: Being un-
supervised is a significant advantage of our meth-
ods over most existing work. It provides flexibil-
ity of applying the methods in different domains
without the requirement of training data, benefit-
ing from both the ontology based template guid-
ance as well as data-driven approaches. As a re-
sult, we can generate different results for differ-
ent topics even in the same domain. In Table 4,
we show the top three selected and ordered as-
pects for Abraham Lincoln and Richard Nixon.
Although they belong to the same category, differ-
ent aspects are picked up due to the differences in
online opinions. People talk a lot about Lincoln?s
role in American Civil War and his famous quo-
tation, but when talking about Nixon, people fo-
cus on ending the Vietnam war and the Watergate
scandal. ?Date of birth? and ?Government posi-
tion? are ranked first because people tend to start
talking from these aspects, which is more natural
than starting from aspects like ?Place of death?.
Baseline Comparison: We also show below the
aspects for Lincoln generated by a representative
approach using clustering method (e.g. (Gamon et
al., 2005)). i.e., we label the largest clusters by se-
lecting phrases with top mutual information. We
can see that although some phrases make sense,
not all are well connected with the given topic;
using aspects in ontology circumvents this prob-
lem. This example confirms the finding in pre-
vious work that the popular existing clustering-
based approach to aspects generation cannot gen-
erate meaningful labels (Chen and Dumais, 2000).
Vincent
New Salem State Historic Site
USS Abraham Lincoln
Martin Luther King Jr
Gettysburg
John F.
New Aspect Discovery: Finally, in Table 5 we
show some phrases ranked among top 10 using
the method described in Section 3.3. They reveal
additional aspects covered in online discussions
and serve as candidate new aspects to be added to
Freebase. Interestingly, John Wilkes Booth, who
assassinated President Lincoln, is not explicitly
738
FreeBase Aspects Supt Representative Opinion Sentences
Appointees: 897 Martin Feldstein, whose criticism of Reagan era deficits has not been forgotten.
- Martin Feldstein Reagan?s first National Security advisor was quoted as declaring...
- Chief Economic Advisor
Government Positions Held: 967 1981 Jan 20, Ronald Reagan was sworn in as president as 52 American hostages
- President of the United States boarded a plane in Tehran and headed toward freedom.
- Jan 20, 1981 to Jan 20, 1989 40th president of the US Ronald Reagan broke the so called ?20 year curse?...
Vice president: 847 8 years, 1981-1988 George H. W. Bush as vice president under Ronald Reagan...
- George H. W. Bush ...exception to the rule was in 1976, when George H W Bush beat Ronald.
Table 2: Opinion Organization Result for President Ronald Reagan
FreeBase Aspects Supt Representative Opinion Sentences
Format: 13 Quality pictures in a compact package.
- Compact ... amazing is that this is such a small and compact unit but packs so much power.
Supported Storage Types: 11 This camera can use Memory Stick Pro Duo up to 8 GB
- Memory Stick Duo Using a universal storage card and cable (c?mon Sony)
Sensor type: 10 I think the larger ccd makes a difference.
- CCD but remember this is a small CCD in a compact point-and-shoot.
Digital zoom: 47 once the digital :smart? zoom kicks in you get another 3x of zoom
-2? I would like a higher optical zoom, the W200 does a great digital zoom translation...
Table 3: Opinion Organization Result for Sony Cybershot DSC-W200 Camera
listed in Freebase, but we can find it in people?s
online discussion using mutual information.
4.3 Evaluation of Aspect Selection
Measures: Aspect selection is a new challenge,
so there is no standard way to evaluate it. It is also
very hard for human to read all of the aspects and
opinions and then select a gold standard subset.
Therefore, we opt to use indirect measures captur-
ing different characteristics of the aspect selection
problem (1) Aspect Coverage (AC): we first as-
sign each aspect Ai to the cluster Cj that has the
most overlapping sentences with Ai, approximat-
ing the cluster that would come into mind when
a reader sees Ai. Then AC is defined as the per-
centage of the clusters covered by at least one as-
pect. (2) Aspect Precision (AP ): for each cov-
ered cluster Ci, AP measures the Jaccard similar-
ity between Ci as a set of opinions and the union
of all aspects assigned to Ci. (3) Average Aspect
Precision (AAP ): defines averaged AP for all
clusters where an uncovered Ci has a zero AP ;
it essentially combines AC and AP . We also re-
port Sentence Coverage (SC), i.e., how many dis-
tinct opinion sentences can be covered by the se-
lected aspects and Conditional Entropy (H), i.e.,
how well the selected aspects align with the nat-
ural clusters in the opinions; a smaller H value
indicates a better alignment.
Results: We summarize the evaluation results in
Measures SC H AC AP AAP
PRESIDENTS
Random 503 1.9069 0.5140 0.0933 0.1223
Size-based 500 1.9656 0.3108 0.1508 0.0949
Opin Cover 746 1.8852 0.5463 0.0913 0.1316
Cond Ent. 479 1.7687 0.5770 0.0856 0.1552
CAMERAS
Random 55 1.6389 0.6554 0.0871 0.1271
Size-based 70 1.6463 0.6071 0.1077 0.1340
Opin Cover 82 1.5866 0.6998 0.0914 0.1564
Cond Ent. 70 1.5598 0.7497 0.0789 0.1574
Table 6: Evaluation Results for Aspect Selection
Table 6. In addition to the three methods de-
scribed in Section 3.1, we also include one base-
line of averaging 10 runs of random selection. The
best performance by each measure on each data
set is highlighted in bold font. Not surprisingly,
opinion coverage-based approach has the best
sentence coverage (SC) performance and condi-
tional entropy-based greedy algorithm achieves
the lowest H . Size-based approach is best in as-
pect precision but at the cost of lowest aspect cov-
erage. The trade-off between AP and AC is com-
parable to that between precision and recall as
in information retrieval while AAP summarizes
the combination of these two. The greedy algo-
rithm based on conditional entropy outperforms
all other approaches in AC and also in AAP , sug-
gesting that it can provide a good balance between
AP and AC.
739
Supt Richard-Nixon Supt Abraham-Lincoln
50 Date of birth: 419 Government Positions Held:
- Jan 9, 1913 - United States Representative Mar 4,1847-Mar 3,1849
108 Tracks Recorded: 558 Military Commands:
- 23-73 Broadcast: End of the Vietnam War - American Civil War - United States of America
120 Works Written About This Topic: 810 Quotations: - Nearly all men can stand adversity, but if
- Watergate you want to test a man?s character, give him power.
Table 4: Comparison of Aspect Selection for Two Presidents (aligned opinions are omitted here)
Suggested Phrases Supporting Opinion Sentences
Abraham Lincoln Presidential Library CDB projects include the Abraham Lincoln Presidential Library and Museum
Abraham Lincoln Memorial ..., eventually arriving at Abraham Lincoln Memorial.
John Wilkes Booth John Wilkes Booth shoots President Abraham Lincoln at Ford?s Theatre ...
Table 5: New Phrases for Abraham Lincoln
4.4 Evaluation of Aspect Ordering
Human Annotation: In order to quantitatively
evaluate the effectiveness of aspect ordering, we
conduct user studies to establish gold standard or-
dering. Three users were each given k selected as-
pects and asked to perform two tasks for each US
President: (1) identify clusters of aspects that are
more natural to be presented together (cluster con-
straints) and (2) identify aspect pairs where one
aspect is preferred to appear before the other from
the viewpoint of readability. (order constraints).
We did not ask them to provide a full order of
the k aspects, because we suspect that there are
usually more than one ?perfect? order. Instead,
identifying partial orders or constraints is easier
for human to perform, thus provides more robust
gold standard.
Human Agreement: After obtaining the human
annotation results, we first study human consen-
sus on the ordering task. For both types of human
identified constraints, we convert them into pair-
wise relations of aspects, e.g., ?Ai and Aj should
be presented together? or ?Ai should be displayed
before Aj?. Then we calculate the agreement per-
centage among the three users. In Table 7, we can
see that only a very small percentage of pair-wise
partial orders (15.92% of the cluster constraints
and none of the order constraints) are agreed by
all the three users, though the agreement of clus-
tering is much higher than that of ordering. This
indicates that ordering the aspects is a subjective
and difficult task.
Measures: Given the human generated gold stan-
dard of partial constraints, we use the follow-
ing measures to evaluate the automatically gen-
AgreedBy Cluster Constraint Order Constraint
1 37.14% 89.22%
2 46.95% 10.78%
3 15.92% 0.00%
Table 7: Human Agreement on Ordering
erated full ordering of aspects: (1) Cluster Pre-
cision (prc): for all the aspect pairs placed in
the same cluster by human, we calculate the per-
centage of them that are also placed together in
the system output. (2) Cluster Penalty (pc): for
each aspect pair placed in the same cluster by hu-
man, we give a linear penalty proportional to the
number of aspects in between the pair that the
system places; pc can be interpreted as the aver-
age number of aspects between aspect pairs that
should be presented together in the case of mis-
ordering. Smaller penalty corresponds to better
ordering performance. (3) Order Precision (pro):
the percentage of correctly predicted aspect pairs
compared with human specified order.
Results: In Table 8, we report the ordering
performance based on two selection algorithms:
opinion coverage-based and conditional entropy-
based. Different selection algorithms provide dif-
ferent subsets of aspects for the ordering algo-
rithms to operate on. For comparison with our
coherence-based ordering algorithm, we include a
random baseline and Freebase ontology ordering.
Note that Freebase order is a very strong baseline
because it is edited by human even though the pur-
pose was not for organizing opinions. To take into
account the variation of human annotation, we use
four versions of gold standard: three are from the
individual annotators and one from the union of
their annotation. We did not include the gold stan-
740
Selection Gold Cluster Precision (prc) Cluster Penalty (pc) Order Precision (pro)
Algo STD Random Freebase Coherence Random Freebase Coherence Random Freebase Coherence
Opin Cover 1 0.3290 0.9547 0.9505 1.8798 0.1547 0.1068 0.4804 0.7059 0.4510
Opin Cover 2 0.3266 0.9293 0.8838 1.7944 0.3283 0.1818 0.4600 0.4000 0.4000
Opin Cover 3 0.2038 0.4550 0.4417 2.5208 1.3628 1.7994 0.5202 0.4561 0.5263
Opin Cover union 0.3234 0.7859 0.7237 1.8378 0.6346 0.4609 0.4678 0.4635 0.4526
Cond Entropy 1 0.2540 0.9355 0.8978 2.0656 0.2957 0.2016 0.5106 0.7111 0.5444
Cond Entropy 2 0.2535 0.7758 0.8323 2.1790 0.7530 0.5222 0.4759 0.6759 0.5093
Cond Entropy 3 0.2523 0.4030 0.5545 2.3079 2.1328 1.1611 0.5294 0.7143 0.8175
Cond Entropy union 0.3067 0.7268 0.7488 1.9735 1.0720 0.7196 0.5006 0.6500 0.6833
Table 8: Evaluation Results on Aspect Ordering
dard that is the intersection of three annotators be-
cause that would leave us with too little overlap.
We have several observations: (1) In general, re-
sults show large variations when using different
versions of gold standard, indicating the subjec-
tive nature of the ordering task. (2) Coherence-
based ordering shows similar performance to
Freebase order-based in cluster precision (prc),
but when we take into consideration the distance-
based penalty (pc) of separating aspects pairs in
the same cluster, coherence-based ordering is al-
most always significantly better except in one
case. This shows that our method can effectively
learn the coherence of aspects based on how their
aligned opinion sentences are presented in online
discussions. (3) Order precision (pro) can hardly
distinguish different ordering algorithm. This in-
dicates that people vary a lot in their preferences
as which aspects should be presented first. How-
ever, in cases when the random baseline outper-
forms others the margin is fairly small, while
Freebase order and coherence-based order have a
much larger margin of improvement when show-
ing superior performance.
5 Conclusions and Future Work
A major challenge in automatic integration of
scattered online opinions is how to organize all
the diverse opinions in a meaningful way for any
given topic. In this paper, we propose to solve this
challenge by exploiting related aspects in struc-
tured ontology which are guaranteed to be mean-
ingful and well connected to the topic. We pro-
posed three different methods for selecting a sub-
set of aspects from the ontology that can best
capture the major opinions, including size-based,
opinion coverage-based, and conditional entropy-
based methods. We also explored two ways to
order aspects, i.e., ontology-order and coherence
optimization. In addition, we also proposed ap-
propriate measures for quantitative evaluation of
both aspect selection and ordering.
Experimental evaluation on two data sets (US
President and Digital Cameras) shows that by ex-
ploiting structured ontology, we can generate in-
teresting aspects to organize scattered opinions.
The conditional entropy method is shown to be
most effective for aspect selection, and the coher-
ence optimization method is more effective than
ontology-order in optimizing the coherence of the
aspect ordering, though ontology-order also ap-
pears to perform reasonably well. In addition, by
extracting salient phrases from the major opinions
that cannot be covered well by any aspect in an
existing ontology, we can also discover interest-
ing new aspects to extend the existing ontology.
Complementary with most existing summariza-
tion work, this work proposes a new direction of
using structured information to organize and sum-
marize unstructured opinions, opening up many
interesting future research directions. For in-
stance, in order to focus on studying aspect selec-
tion and ordering, we have not tried to optimize
sentences matching with aspects in the ontology;
it would be very interesting to further study how
to accurately retrieve sentences matching each as-
pect. Another promising future work is to orga-
nize opinions using both structured ontology in-
formation and well-written overview articles.
Acknowledgment
We thank the anonymous reviewers for their use-
ful comments. This paper is based upon work sup-
ported in part by an IBM Faculty Award, an Alfred
P. Sloan Research Fellowship, an AFOSR MURI
Grant FA9550-08-1-0265, and by the National
Science Foundation under grants IIS-0347933,
IIS-0713581, IIS-0713571, and CNS-0834709.
741
References
Carenini, Giuseppe, Raymond T. Ng, and Ed Zwart.
2005. Extracting knowledge from evaluative text.
In K-CAP ?05: Proceedings of the 3rd international
conference on Knowledge capture, pages 11?18,
New York, NY, USA. ACM.
Chen, Hao and Susan Dumais. 2000. Bringing or-
der to the web: automatically categorizing search
results. In CHI ?00: Proceedings of the SIGCHI
conference on Human factors in computing systems,
pages 145?152, New York, NY, USA. ACM.
Finkel, Jenny Rose, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by gibbs
sampling. In ACL ?05: Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, pages 363?370, Morristown, NJ, USA.
Association for Computational Linguistics.
Gamon, Michael, Anthony Aue, Simon Corston-
Oliver, and Eric K. Ringger. 2005. Pulse: Min-
ing customer opinions from free text. In Famili,
A. Fazel, Joost N. Kok, Jose? Mar??a Pen?a, Arno
Siebes, and A. J. Feelders, editors, IDA, volume
3646 of Lecture Notes in Computer Science, pages
121?132. Springer.
Ka?ki, Mika. 2005. Optimizing the number of search
result categories. In CHI ?05: CHI ?05 extended
abstracts on Human factors in computing systems,
pages 1517?1520, New York, NY, USA. ACM.
Leouski, Anton V. and W. Bruce Croft. 1996. An eval-
uation of techniques for clustering search results.
Technical report.
Liu, Bing, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: analyzing and comparing opin-
ions on the web. In WWW ?05: Proceedings of the
14th international conference on World Wide Web,
pages 342?351, New York, NY, USA. ACM.
Lu, Yue and Chengxiang Zhai. 2008. Opinion in-
tegration through semi-supervised topic modeling.
In Huai, Jinpeng, Robin Chen, Hsiao-Wuen Hon,
Yunhao Liu, Wei-Ying Ma, Andrew Tomkins, and
Xiaodong Zhang, editors, WWW, pages 121?130.
ACM.
Mei, Qiaozhu, Chao Liu, Hang Su, and ChengXiang
Zhai. 2006. A probabilistic approach to spatiotem-
poral theme pattern mining on weblogs. In WWW
?06: Proceedings of the 15th international confer-
ence on World Wide Web, pages 533?542.
Mei, Qiaozhu, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic
models. In Berkhin, Pavel, Rich Caruana, and Xin-
dong Wu, editors, KDD, pages 490?499. ACM.
Pang, Bo and Lillian Lee. 2007. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1?135.
Popescu, Ana-Maria and Oren Etzioni. 2005. Ex-
tracting product features and opinions from reviews.
In HLT ?05, pages 339?346, Morristown, NJ, USA.
Association for Computational Linguistics.
Porter, M. F. 1997. An algorithm for suffix stripping.
pages 313?316.
Sauper, Christina and Regina Barzilay. 2009. Auto-
matically generating wikipedia articles: A structure-
aware approach. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the
4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP, pages 208?216,
Suntec, Singapore, August. Association for Compu-
tational Linguistics.
Titov, Ivan and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
WWW ?08: Proceeding of the 17th international
conference on World Wide Web, pages 111?120,
New York, NY, USA. ACM.
Zeng, Hua-Jun, Qi-Cai He, Zheng Chen, Wei-Ying
Ma, and Jinwen Ma. 2004. Learning to cluster
web search results. In SIGIR ?04: Proceedings
of the 27th annual international ACM SIGIR con-
ference on Research and development in informa-
tion retrieval, pages 210?217, New York, NY, USA.
ACM.
Zhai, Chengxiang and John Lafferty. 2001. Model-
based feedback in the language modeling approach
to information retrieval. In Proceedings of CIKM
2001, pages 403?410.
Zhao, Jing and Jing He. 2006. Learning to generate
labels for organizing search results from a domain-
specified corpus. In WI ?06: Proceedings of the
2006 IEEE/WIC/ACM International Conference on
Web Intelligence, pages 390?396, Washington, DC,
USA. IEEE Computer Society.
742
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1526?1535,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Structural Topic Model for Latent Topical Structure Analysis
Hongning Wang, Duo Zhang, ChengXiang Zhai
Department of Computer Science
University of Illinois at Urbana-Champaign
Urbana IL, 61801 USA
{wang296, dzhang22, czhai}@cs.uiuc.edu
Abstract
Topic models have been successfully applied
to many document analysis tasks to discover
topics embedded in text. However, existing
topic models generally cannot capture the la-
tent topical structures in documents. Since
languages are intrinsically cohesive and coher-
ent, modeling and discovering latent topical
transition structures within documents would
be beneficial for many text analysis tasks.
In this work, we propose a new topic model,
Structural Topic Model, which simultaneously
discovers topics and reveals the latent topi-
cal structures in text through explicitly model-
ing topical transitions with a latent first-order
Markov chain. Experiment results show that
the proposed Structural Topic Model can ef-
fectively discover topical structures in text,
and the identified structures significantly im-
prove the performance of tasks such as sen-
tence annotation and sentence ordering.
1 Introduction
A great amount of effort has recently been made in
applying statistical topic models (Hofmann, 1999;
Blei et al, 2003) to explore word co-occurrence pat-
terns, i.e. topics, embedded in documents. Topic
models have become important building blocks of
many interesting applications (see e.g., (Blei and
Jordan, 2003; Blei and Lafferty, 2007; Mei et al,
2007; Lu and Zhai, 2008)).
In general, topic models can discover word clus-
tering patterns in documents and project each doc-
ument to a latent topic space formed by such word
clusters. However, the topical structure in a docu-
ment, i.e., the internal dependency between the top-
ics, is generally not captured due to the exchange-
ability assumption (Blei et al, 2003), i.e., the doc-
ument generation probabilities are invariant to con-
tent permutation. In reality, natural language text
rarely consists of isolated, unrelated sentences, but
rather collocated, structured and coherent groups of
sentences (Hovy, 1993). Ignoring such latent topi-
cal structures inside the documents means wasting
valuable clues about topics and thus would lead to
non-optimal topic modeling.
Taking apartment rental advertisements as an ex-
ample, when people write advertisements for their
apartments, it?s natural to first introduce ?size? and
?address? of the apartment, and then ?rent? and
?contact?. Few people would talk about ?restric-
tion? first. If this kind of topical structures are cap-
tured by a topic model, it would not only improve
the topic mining results, but, more importantly, also
help many other document analysis tasks, such as
sentence annotation and sentence ordering.
Nevertheless, very few existing topic models at-
tempted to model such structural dependency among
topics. The Aspect HMM model introduced in
(Blei and Moreno, 2001) combines pLSA (Hof-
mann, 1999) with HMM (Rabiner, 1989) to perform
document segmentation over text streams. However,
Aspect HMM separately estimates the topics in the
training set and depends on heuristics to infer the
transitional relations between topics. The Hidden
Topic Markov Model (HTMM) proposed by (Gru-
ber et al, 2007) extends the traditional topic models
by assuming words in each sentence share the same
topic assignment, and topics transit between adja-
cent sentences. However, the transitional structures
among topics, i.e., how likely one topic would fol-
low another topic, are not captured in this model.
1526
In this paper, we propose a new topic model,
named Structural Topic Model (strTM) to model and
analyze both latent topics and topical structures in
text documents. To do so, strTM assumes: 1) words
in a document are either drawn from a content topic
or a functional (i.e., background) topic; 2) words in
the same sentence share the same content topic; and
3) content topics in the adjacent sentences follow a
topic transition that satisfies the first order Markov
property. The first assumption distinguishes the se-
mantics of the occurrence of each word in the doc-
ument, the second requirement confines the unreal-
istic ?bag-of-word? assumption into a tighter unit,
and the third assumption exploits the connection be-
tween adjacent sentences.
To evaluate the usefulness of the identified top-
ical structures by strTM, we applied strTM to the
tasks of sentence annotation and sentence ordering,
where correctly modeling the document structure
is crucial. On the corpus of 8,031 apartment ad-
vertisements from craiglist (Grenager et al, 2005)
and 1,991 movie reviews from IMDB (Zhuang et
al., 2006), strTM achieved encouraging improve-
ment in both tasks compared with the baseline meth-
ods that don?t explicitly model the topical structure.
The results confirm the necessity of modeling the
latent topical structures inside documents, and also
demonstrate the advantages of the proposed strTM
over existing topic models.
2 Related Work
Topic models have been successfully applied to
many problems, e.g., sentiment analysis (Mei et
al., 2007), document summarization (Lu and Zhai,
2008) and image annotation (Blei and Jordan, 2003).
However, in most existing work, the dependency
among the topics is loosely governed by the prior
topic distribution, e.g., Dirichlet distribution.
Some work has attempted to capture the interre-
lationship among the latent topics. Correlated Topic
Model (Blei and Lafferty, 2007) replaces Dirichlet
prior with logistic Normal prior for topic distribu-
tion in each document in order to capture the cor-
relation between the topics. HMM-LDA (Griffiths
et al, 2005) distinguishes the short-range syntactic
dependencies from long-range semantic dependen-
cies among the words in each document. But in
HMM-LDA, only the latent variables for the syn-
tactic classes are treated as a locally dependent se-
quence, while latent topics are treated the same as in
other topic models. Chen et al introduced the gen-
eralized Mallows model to constrain the latent topic
assignments (Chen et al, 2009). In their model,
they assume there exists a canonical order among
the topics in the collection of related documents and
the same topics are forced not to appear in discon-
nected portions of the topic sequence in one docu-
ment (sampling without replacement). Our method
relaxes this assumption by only postulating transi-
tional dependency between topics in the adjacent
sentences (sampling with replacement) and thus po-
tentially allows a topic to appear multiple times in
disconnected segments. As discussed in the pre-
vious section, HTMM (Gruber et al, 2007) is the
most similar model to ours. HTMM models the
document structure by assuming words in the same
sentence share the same topic assignment and suc-
cessive sentences are more likely to share the same
topic. However, HTMM only loosely models the
transition between topics as a binary relation: the
same as the previous sentence?s assignment or draw
a new one with a certain probability. This simpli-
fied coarse modeling of dependency could not fully
capture the complex structure across different docu-
ments. In contrast, our strTM model explicitly cap-
tures the regular topic transitions by postulating the
first order Markov property over the topics.
Another line of related work is discourse analysis
in natural language processing: discourse segmen-
tation (Sun et al, 2007; Galley et al, 2003) splits a
document into a linear sequence of multi-paragraph
passages, where lexical cohesion is used to link to-
gether the textual units; discourse parsing (Soricut
and Marcu, 2003; Marcu, 1998) tries to uncover a
more sophisticated hierarchical coherence structure
from text to represent the entire discourse. One work
in this line that shares a similar goal as ours is the
content models (Barzilay and Lee, 2004), where an
HMM is defined over text spans to perform infor-
mation ordering and extractive summarization. A
deficiency of the content models is that the identi-
fication of clusters of text spans is done separately
from transition modeling. Our strTM addresses this
deficiency by defining a generative process to simul-
taneously capture the topics and the transitional re-
1527
lationship among topics: allowing topic modeling
and transition modeling to reinforce each other in a
principled framework.
3 Structural Topic Model
In this section, we formally define the Structural
Topic Model (strTM) and discuss how it captures the
latent topics and topical structures within the docu-
ments simultaneously. From the theory of linguistic
analysis (Kamp, 1981), we know that document ex-
hibits internal structures, where structural segments
encapsulate semantic units that are closely related.
In strTM, we treat a sentence as the basic structure
unit, and assume all the words in a sentence share the
same topical aspect. Besides, two adjacent segments
are assumed to be highly related (capturing cohesion
in text); specifically, in strTM we pose a strong tran-
sitional dependency assumption among the topics:
the choice of topic for each sentence directly de-
pends on the previous sentence?s topic assignment,
i.e., first order Markov property. Moveover, tak-
ing the insights from HMM-LDA that not all the
words are content conveying (some of them may
just be a result of syntactic requirement), we intro-
duce a dummy functional topic zB for every sen-
tence in the document. We use this functional topic
to capture the document-independent word distribu-
tion, i.e., corpus background (Zhai et al, 2004). As
a result, in strTM, every sentence is treated as a mix-
ture of content and functional topics.
Formally, we assume a corpus consists of D doc-
uments with a vocabulary of size V, and there are
k content topics embedded in the corpus. In a given
document d, there arem sentences and each sentence
i hasNi words. We assume the topic transition prob-
ability p(z|z?) is drawn from a Multinomial distribu-
tionMul(?z?), and the word emission probability un-
der each topic p(w|z) is drawn from a Multinomial
distribution Mul(?z).
To get a unified description of the generation
process, we add another dummy topic T-START in
strTM, which is the initial topic with position ?-1?
for every document but does not emit any words.
In addition, since our functional topic is assumed to
occur in all the sentences, we don?t need to model
its transition with other content topics. We use a
Binomial variable pi to control the proportion be-
tween content and functional topics in each sen-
tence. Therefore, there are k+1 topic transitions, one
for T-START and others for k content topics; and k
emission probabilities for the content topics, with an
additional one for the functional topic zB (in total
k+1 emission probability distributions).
Conditioned on the model parameters ? =
(?, ?, pi), the generative process of a document in
strTM can be described as follows:
1. For each sentence si in document d:
(a) Draw topic zi from Multinomial distribu-
tion conditioned on the previous sentence
si?1?s topic assignment zi?1:
zi ? Mul(?zi?1)
(b) Draw each word wij in sentence si from
the mixture of content topic zi and func-
tional topic zB:
wij ? pip(wij |?, zi)+(1?pi)p(wij |?, zB)
The joint probability of sentences and topics in
one document defined by strTM is thus given by:
p(S0, S1, . . . , Sm, z|?, ?, pi) =
m
?
i=1
p(zi|?, zi?1)p(Si|zi)
(1)
where the topic to sentence emission probability is
defined as:
p(Si|zi) =
Ni
?
j=0
[
pip(wij |?, zi) + (1? pi)p(wij |?, zB)
]
(2)
This process is graphically illustrated in Figure 1.
 
zmz0 ??..
wm??..
NmD
K+1
w0
N0
K+1
 
z1
w1
N1
Tstart
Figure 1: Graphical Representation of strTM.
From the definition of strTM, we can see that the
document structure is characterized by a document-
specific topic chain, and forcing the words in one
1528
sentence to share the same content topic ensures se-
mantic cohesion of the mined topics. Although we
do not directly model the topic mixture for each doc-
ument as the traditional topic models do, the word
co-occurrence patterns within the same document
are captured by topic propagation through the transi-
tions. This can be easily understood when we write
down the posterior probability of the topic assign-
ment for a particular sentence:
p(zi|S0, S1, . . . , Sm,?)
=p(S0, S1, . . . , Sm|zi,?)p(zi)
p(S0, S1, . . . , Sm)
? p(S0, S1, . . . , Si, zi)? p(Si+1, Si+2, . . . , Sm|zi)
=
?
zi?1
p(S0, . . . , Si?1, zi?1)p(zi|zi?1)p(Si|zi)
?
?
zi+1
p(Si+1, . . . , Sm|zi+1)p(zi+1|zi) (3)
The first part of Eq(3) describes the recursive in-
fluence on the choice of topic for the ith sentence
from its preceding sentences, while the second part
captures how the succeeding sentences affect the
current topic assignment. Intuitively, when we need
to decide a sentence?s topic, we will look ?back-
ward? and ?forward? over all the sentences in the
document to determine a ?suitable? one. In addition,
because of the first order Markov property, the local
topical dependency gets more emphasis, i.e., they
are interacting directly through the transition proba-
bilities p(zi|zi?1) and p(zi+1|zi). And such interac-
tion on sentences farther away would get damped by
the multiplication of such probabilities. This result
is reasonable, especially in a long document, since
neighboring sentences are more likely to cover sim-
ilar topics than two sentences far apart.
4 Posterior Inference and Parameter
Estimation
The chain structure in strTM enables us to perform
exact inference: posterior distribution can be ef-
ficiently calculated by the forward-backward algo-
rithm, the optimal topic sequence can be inferred
using the Viterbi algorithm, and parameter estima-
tion can be solved by the Expectation Maximization
(EM) algorithm. More technical details can be found
in (Rabiner, 1989). In this section, we only discuss
strTM-specific procedures.
In the E-Step of EM algorithm, we need to col-
lect the expected count of a sequential topic pair
(z, z?) and a topic-word pair (z, w) to update the
model parameters ? and ? in the M-Step. In strTM,
E[c(z, z?)] can be easily calculated by forward-
backward algorithm. But we have to go one step
further to fetch the required sufficient statistics for
E[c(z, w)], because our emission probabilities are
defined over sentences.
Through forward-backward algorithm, we can get
the posterior probability p(si, z|d,?). In strTM,
words in one sentence are independently drawn from
either a specific content topic z or functional topic
zB according to the mixture weight pi. Therefore,
we can accumulate the expected count of (z, w) over
all the sentences by:
E[c(z, w)] =
?
d,s?d
pip(w|z)p(s, z|d,?)c(w, s)
pip(w|z) + (1? pi)p(w|zB)
(4)
where c(w, s) indicates the frequency of word w in
sentence s.
Eq(4) can be easily explained as follows. Since
we already observe topic z and sentence s co-
occur with probability p(s, z|d,?), each word w
in s should share the same probability of be-
ing observed with content topic z. Thus the ex-
pected count of c(z, w) in this sentence would be
p(s, z|d,?)c(w, s). However, since each sentence
is also associated with the functional topic zB , the
word w may also be drawn from zB . By applying
the Bayes? rule, we can properly reallocate the ex-
pected count of c(z, w) by Eq(4). The same strategy
can be applied to obtain E[c(zB, w)].
As discussed in (Johnson, 2007), to avoid the
problem that EM algorithm tends to assign a uni-
form word/state distribution to each hidden state,
which deviates from the heavily skewed word/state
distributions empirically observed, we can apply a
Bayesian estimation approach for strTM. Thus we
introduce prior distributions over the topic transi-
tion Mul(?z?) and emission probabilities Mul(?z),
and use the Variational Bayesian (VB) (Jordan et al,
1999) estimator to obtain a model with more skewed
word/state distributions.
Since both the topic transition and emission prob-
abilities are Multinomial distributions in strTM,
the conjugate Dirichlet distribution is the natural
1529
choice for imposing a prior on them (Diaconis and
Ylvisaker, 1979). Thus, we further assume:
?z ? Dir(?) (5)
?z ? Dir(?) (6)
where we use exchangeable Dirichlet distributions
to control the sparsity of ?z and ?z . As ? and ? ap-
proach zero, the prior strongly favors the models in
which each hidden state emits as few words/states as
possible. In our experiments, we empirically tuned
? and ? on different training corpus to optimize log-
likelihood.
The resulting VB estimation only requires a mi-
nor modification to the M-Step in the original EM
algorithm:
??z =
?(E[c(z?, z)] + ?)
?(E[c(z)] + k?)
(7)
??z =
?(E[c(w, z)] + ?)
?(E[c(z)] + V ?)
(8)
where ?(x) is the exponential of the first derivative
of the log-gamma function.
The optimal setting of pi for the proportion of con-
tent topics in the documents is empirically tuned by
cross-validation over the training corpus to maxi-
mize the log-likelihood.
5 Experimental Results
In this section, we demonstrate the effectiveness
of strTM in identifying latent topical structures
from documents, and quantitatively evaluate how the
mined topic transitions can help the tasks of sen-
tence annotation and sentence ordering.
5.1 Data Set
We used two different data sets for evaluation: apart-
ment advertisements (Ads) from (Grenager et al,
2005) and movie reviews (Review) from (Zhuang et
al., 2006).
The Ads data consists of 8,767 advertisements for
apartment rentals crawled from Craigslist website.
302 of them have been labeled with 11 fields, in-
cluding size, feature, address, etc., on the sentence
level. The review data contains 2,000 movie reviews
discussing 11 different movies from IMDB. These
reviews are manually labeled with 12 movie feature
labels (We didn?t use the additional opinion anno-
tations in this data set.) , e.g., VP (vision effects),
MS (music and sound effects), etc., also on the sen-
tences, but the annotations in the review data set is
much sparser than that in the Ads data set (see in Ta-
ble 1). The sentence-level annotations make it pos-
sible to quantitatively evaluate the discovered topic
structures.
We performed simple preprocessing on these
two data sets: 1) removed a standard list of stop
words, terms occurring in less than 2 documents;
2) discarded the documents with less than 2 sen-
tences; 3) aggregated sentence-level annotations
into document-level labels (binary vector) for each
document. Table 1 gives a brief summary on these
two data sets after the processing.
Ads Review
Document Size 8,031 1,991
Vocabulary Size 21,993 14,507
Avg Stn/Doc 8.0 13.9
Avg Labeled Stn/Doc 7.1* 5.1
Avg Token/Stn 14.1 20.0
*Only in 302 labeled ads
Table 1: Summary of evaluation data set
5.2 Topic Transition Modeling
First, we qualitatively demonstrate the topical struc-
ture identified by strTM from Ads data1. We trained
strTM with 11 content topics in Ads data set, used
word distribution under each class (estimated by
maximum likelihood estimator on document-level
labels) as priors to initialize the emission probabil-
ity Mul(?z) in Eq(6), and treated document-level la-
bels as the prior for transition from T-START in each
document, so that the mined topics can be aligned
with the predefined class labels. Figure 2 shows the
identified topics and the transitions among them. To
get a clearer view, we discarded the transitions be-
low a threshold of 0.1 and removed all the isolated
nodes.
From Figure 2, we can find some interesting top-
ical structures. For example, people usually start
with ?size?, ?features? and ?address?, and end
with ?contact? information when they post an apart-
1Due to the page limit, we only show the result in Ads data
set.
1530
TELEPHONE
appointment
information
contact
email
parking
kitchen
room
laundry
storage
close
shopping
transportation
bart
location
http
photos
click
pictures
view
deposit
month
lease
rent
year
pets
kitchen
cat
negotiate
smoking
water
garbage
included
paid
utilities
NUM
bedroom
bath
room
large
Figure 2: Estimated topics and topical transitions in Ads data set
ment ads. Also, we can discover a strong transition
from ?size? to ?features?. This intuitively makes
sense because people usually write ?it?s a two bed-
rooms apartment? first, and then describe other ?fea-
tures? about the apartment. The mined topics are
also quite meaningful. For example, ?restrictions?
are usually put over pets and smoking, and parking
and laundry are always the major ?features? of an
apartment.
To further quantitatively evaluate the estimated
topic transitions, we used Kullback-Leibler (KL) di-
vergency between the estimated transition matrix
and the ?ground-truth? transition matrix as the met-
ric. Each element of the ?ground-truth? transition
matrix was calculated by Eq(9), where c(z, z?) de-
notes how many sentences annotated by z? immedi-
ately precede one annotated by z. ? is a smoothing
factor, and we fixed it to 0.01 in the experiment.
p?(z|z?) = c(z, z
?) + ?
c(z) + k?
(9)
The KL divergency between two transition matri-
ces is defined in Eq(10). Because we have a k ? k
transition matrix (Tstart is not included), we calcu-
lated the average KL divergency against the ground-
truth over all the topics:
avgKL=
?k
i=1 KL(p(z|z
?
i)||p?(z|z?i))+KL(p?(z|z?i)||p(z|z?i))
2k
(10)
where p?(z|z?) is the ground-truth transition proba-
bility estimated by Eq(9), and p(z|z?) is the transi-
tion probability given by the model.
We used pLSA (Hofmann, 1999), latent permuta-
tion model (lPerm) (Chen et al, 2009) and HTMM
(Gruber et al, 2007) as the baseline methods for the
comparison. Because none of these three methods
can generate a topic transition matrix directly, we
extended them a little bit to achieve this goal. For
pLSA, we used the document-level labels as priors
for the topic distribution in each document, so that
the estimated topics can be aligned with the prede-
fined class labels. After the topics were estimated,
for each sentence we selected the topic that had
the highest posterior probability to generate the sen-
tence as its class label. For lPerm and HTMM, we
used Kuhn-Munkres algorithm (Lova?sz and Plum-
mer, 1986) to find the optimal topic-to-class align-
ment based on the sentence-level annotations. Af-
ter the sentences were annotated with class labels,
we estimated the topic transition matrices for all of
these three methods by Eq(9).
1531
Since only a small portion of sentences are an-
notated in the Review data set, very few neighbor-
ing sentences are annotated at the same time, which
introduces many noisy transitions. As a result, we
only performed the comparison on the Ads data set.
The ?ground-truth? transition matrix was estimated
based on all the 302 annotated ads.
pLSA+prior lPerm HTMM strTM
avgKL 0.743 1.101 0.572 0.372
p-value 0.023 1e-4 0.007 ?
Table 2: Comparison of estimated topic transitions on
Ads data set
In Table 2, the p-value was calculated based on t-
test of the KL divergency between each topic?s tran-
sition probability against strTM. From the results,
we can see that avgKL of strTM is smaller than the
other three baseline methods, which means the esti-
mated transitional relation by strTM is much closer
to the ground-truth transition. This demonstrates
that strTM captures the topical structure well, com-
pared with other baseline methods.
5.3 Sentence Annotation
In this section, we demonstrate how the identified
topical structure can benefit the task of sentence an-
notation. Sentence annotation is one step beyond the
traditional document classification task: in sentence
annotation, we want to predict the class label for
each sentence in the document, and this will be help-
ful for other problems, including extractive summa-
rization and passage retrieval. However, the lack of
detailed annotations on sentences greatly limits the
effectiveness of the supervised classification meth-
ods, which have been proved successful on docu-
ment classifications.
In this experiment, we propose to use strTM to ad-
dress this annotation task. One advantage of strTM
is that it captures the topic transitions on the sen-
tence level within documents, which provides a reg-
ularization over the adjacent predictions.
To examine the effectiveness of such structural
regularization, we compared strTM with four base-
line methods: pLSA, lPerm, HTMM and Naive
Bayes model. The sentence labeling approaches for
strTM, pLSA, lPerm and HTMM have been dis-
cussed in the previous section. As for Naive Bayes
model, we used EM algorithm 2 with both labeled
and unlabeled data for the training purpose (we used
the same unigram features as in topics models). We
set weights for the unlabeled data to be 10?3 in
Naive Bayes with EM.
The comparison was performed on both data sets.
We set the size of topics in each topic model equal
to the number of classes in each data set accord-
ingly. To tackle the situation where some sentences
in the document are not strictly associated with any
classes, we introduced an additional NULL content
topic in all the topic models. During the training
phase, none of the methods used the sentence-level
annotations in the documents, so that we treated the
whole corpus as the training and testing set.
To evaluate the prediction performance, we cal-
culated accuracy, recall and precision based on the
correct predictions over the sentences, and averaged
over all the classes as the criterion.
Model Accuracy Recall Precison
pLSA+prior 0.432 0.649 0.457
lPerm 0.610 0.514 0.471
HTMM 0.606 0.588 0.443
NB+EM 0.528 0.337 0.612
strTM 0.747 0.674 0.620
Table 3: Sentence annotation performance on Ads data
set
Model Accuracy Recall Precison
pLSA+prior 0.342 0.278 0.250
lPerm 0.286 0.205 0.184
HTMM 0.369 0.131 0.149
NB+EM 0.341 0.354 0.431
strTM 0.541 0.398 0.323
Table 4: Sentence annotation performance on Review
data set
Annotation performance on the two data sets is
shown in Table 3 and Table 4. We can see that strTM
outperformed all the other baseline methods on most
of the metrics: strTM has the best accuracy and re-
call on both of the two data sets. The improvement
confirms our hypothesis that besides solely depend-
ing on the local word patterns to perform predic-
2Mallet package: http://mallet.cs.umass.edu/
1532
tions, adjacent sentences provide a structural reg-
ularization in strTM (see Eq(3)). Compared with
lPerm, which postulates a strong constrain over the
topic assignment (sampling without replacement),
strTM performed much better on both of these two
data sets. This validates the benefit of modeling lo-
cal transitional relation compared with the global or-
dering. Besides, strTM achieved over 46% accu-
racy improvement compared with the second best
HTMM in the review data set. This result shows
the advantage of explicitly modeling the topic tran-
sitions between neighbor sentences instead of using
a binary relation to do so as in HTMM.
To further testify how the identified topical struc-
ture can help the sentence annotation task, we first
randomly removed 100 annotated ads from the train-
ing corpus and used them as the testing set. Then,
we used the ground-truth topic transition matrix es-
timated from the training data to order those 100 ads
according to their fitness scores under the ground-
truth topic transition matrix, which is defined in
Eq(11). We tested the prediction accuracy of differ-
ent models over two different partitions, top 50 and
bottom 50, according to this order.
fitness(d) = 1
|d|
|d|
?
i=0
log p?(ti|ti?1) (11)
where ti is the class label for ith sentence in doc-
ument d, |d| is the number of sentences in docu-
ment d, and p?(ti|ti?1) is the transition probability
estimated by Eq(9).
Top 50 p-value Bot 50 p-value
pLSA+prior 0.496 4e-12 0.542 0.004
lPerm 0.669 0.003 0.505 8e-4
HTMM 0.683 0.004 0.579 0.003
NB + EM 0.492 1e-12 0.539 0.002
strTM 0.752 ? 0.644 ?
Table 5: Sentence annotation performance according to
structural fitness
The results are shown in Table 5. From this table,
we can find that when the testing documents follow
the regular patterns as in the training data, i.e., top
50 group, strTM performs significantly better than
the other methods; when the testing documents don?t
share such structure, i.e., bottom 50 group, strTM?s
performance drops. This comparison confirms that
when a testing document shares similar topic struc-
ture as the training data, the topical transitions cap-
tured by strTM can help the sentence annotation task
a lot. In contrast, because pLSA and Naive Bayes
don?t depend on the document?s structure, their per-
formance does not change much over these two par-
titions.
5.4 Sentence Ordering
In this experiment, we illustrate how the learned top-
ical structure can help us better arrange sentences in
a document. Sentence ordering, or text planning, is
essential to many text synthesis applications, includ-
ing multi-document summarization (Goldstein et al,
2000) and concept-to-text generation (Barzilay and
Lapata, 2005).
In strTM, we evaluate all the possible orderings
of the sentences in a given document and selected
the optimal one which gives the highest generation
probability:
??(m) = argmax
?(m)
?
z
p(S?[0], S?[1], . . . , S?[m], z|?)
(12)
where ?(m) is a permutation of 1 to m, and ?[i] is
the ith element in this permutation.
To quantitatively evaluate the ordering result, we
treated the original sentence order (OSO) as the per-
fect order and used Kendall?s ?(?) (Lapata, 2006) as
the evaluation metric to compute the divergency be-
tween the optimum ordering given by the model and
OSO. Kendall?s ?(?) is widely used in information
retrieval domain to measure the correlation between
two ranked lists and it indicates how much an order-
ing differs from OSO, which ranges from 1 (perfect
matching) to -1 (totally mismatching).
Since only the HTMM and lPerm take the order
of sentences in the document into consideration, we
used them as the baselines in this experiment. We
ranked OSO together with candidate permutations
according to the corresponding model?s generation
probability. However, when the size of documents
becomes larger, it?s infeasible to permutate all the
orderings, therefore we randomly permutated 200
possible orderings of sentences as candidates when
there were more than 200 possible candidates. The
1533
2bedroom 1bath in very nice complex! Pool,
carport, laundry facilities!! Call Don (650)207-
5769 to see! Great location!! Also available,
2bed.2bath for $1275 in same complex.
=?
2bedroom 1bath in very nice complex! Pool, car-
port, laundry facilities!! Great location!! Also
available, 2bed.2bath for $1275 in same complex.
Call Don (650)207-5769 to see!
2 bedrooms 1 bath + a famyly room in a cul-de-
sac location. Please drive by and call Marilyn for
appointment 650-652-5806. Address: 517 Price
Way, Vallejo. No Pets Please!
=?
2 bedrooms 1 bath + a famyly room in a cul-de-
sac location. Address: 517 Price Way, Vallejo. No
Pets Please! Please drive by and call Marilyn for
appointment 650-652-5806.
Table 6: Sample results for document ordering by strTM
experiment was performed on both data sets with
80% data for training and the other 20% for testing.
We calculated the ?(?) of all these models for
each document in the two data sets and visualized
the distribution of ?(?) in each data set with his-
togram in Figure 3. From the results, we could ob-
serve that strTM?s ?(?) is more skewed towards the
positive range (with mean 0.619 in Ads data set and
0.398 in review data set) than lPerm?s results (with
mean 0.566 in Ads data set and 0.08 in review data
set) and HTMM?s results (with mean 0.332 in Ads
data set and 0.286 in review data set). This indi-
cates that strTM better captures the internal structure
within the documents.
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 10
100200
300400
500600
700800
900
?(?)
# of Doc
uments
AdslPermHTMMstrTM
?1 ?0.8 ?0.6 ?0.4 ?0.2 0 0.2 0.4 0.6 0.8 10
20
40
60
80
100
120
140
160
?(?)
# of Doc
uments
ReviewlPermHTMMstrTM
(a) Ads (b) Review
Figure 3: Document Ordering Performance in ?(?).
We see that all methods performed better on the
Ads data set than the review data set, suggesting
that the topical structures are more coherent in the
Ads data set than the review data. Indeed, in the
Ads data, strTM perfectly recovered 52.9% of the
original sentence order. When examining some mis-
matched results, we found that some of them were
due to an ?outlier? order given by the original docu-
ment (in comparison to the ?regular? patterns in the
set). In Table 6, we show two such examples where
we see the learned structure ?suggested? to move
the contact information to the end, which intuitively
gives us a more regular organization of the ads. It?s
hard to say that in this case, the system?s ordering is
inferior to that of the original; indeed, the system or-
der is arguably more natural than the original order.
6 Conclusions
In this paper, we proposed a new structural topic
model (strTM) to identify the latent topical struc-
ture in documents. Different from the traditional
topic models, in which exchangeability assumption
precludes them to capture the structure of a docu-
ment, strTM captures the topical structure explicitly
by introducing transitions among the topics. Experi-
ment results show that both the identified topics and
topical structure are intuitive and meaningful, and
they are helpful for improving the performance of
tasks such as sentence annotation and sentence or-
dering, where correctly recognizing the document
structure is crucial. Besides, strTM is shown to out-
perform not only the baseline topic models that fail
to model the dependency between the topics, but
also the semi-supervised Naive Bayes model for the
sentence annotation task.
Our work can be extended by incorporating richer
features, such as named entity and co-reference, to
enhance the model?s capability of structure finding.
Besides, advanced NLP techniques for document
analysis, e.g., shallow parsing, may also be used to
further improve structure finding.
7 Acknowledgments
We thank the anonymous reviewers for their use-
ful comments. This material is based upon work
supported by the National Science Foundation un-
der Grant Numbers IIS-0713581 and CNS-0834709,
and NASA grant NNX08AC35A.
1534
References
R. Barzilay and M. Lapata. 2005. Collective content se-
lection for concept-to-text generation. In Proceedings
of the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
pages 331?338.
R. Barzilay and L. Lee. 2004. Catching the drift: Proba-
bilistic content models, with applications to generation
and summarization. In Proceedings of HLT-NAACL,
pages 113?120.
D.M. Blei and M.I. Jordan. 2003. Modeling annotated
data. In Proceedings of the 26th annual international
ACM SIGIR conference, pages 127?134.
D.M. Blei and J.D. Lafferty. 2007. A correlated topic
model of science. The Annals of Applied Statistics,
1(1):17?35.
D.M. Blei and P.J. Moreno. 2001. Topic segmentation
with an aspect hidden Markov model. In Proceedings
of the 24th annual international ACM SIGIR confer-
ence, page 348. ACM.
D.M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003.
Latent dirichlet alocation. The Journal of Machine
Learning Research, 3(2-3):993 ? 1022.
H. Chen, SRK Branavan, R. Barzilay, and D.R. Karger.
2009. Global models of document structure using la-
tent permutations. In Proceedings of HLT-NAACL,
pages 371?379.
P. Diaconis and D. Ylvisaker. 1979. Conjugate pri-
ors for exponential families. The Annals of statistics,
7(2):269?281.
M. Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003. Discourse segmentation of multi-party conver-
sation. In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics-Volume 1,
pages 562?569.
J. Goldstein, V. Mittal, J. Carbonell, and M. Kantrowitz.
2000. Multi-document summarization by sentence ex-
traction. In NAACL-ANLP 2000 Workshop on Auto-
matic summarization, pages 40?48.
T. Grenager, D. Klein, and C.D. Manning. 2005. Un-
supervised learning of field segmentation models for
information extraction. In Proceedings of the 43rd an-
nual meeting on association for computational linguis-
tics, pages 371?378.
T.L. Griffiths, M. Steyvers, D.M. Blei, and J.B. Tenen-
baum. 2005. Integrating topics and syntax. Advances
in neural information processing systems, 17:537?
544.
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi. 2007.
Hidden topic markov models. volume 2, pages 163?
170.
T. Hofmann. 1999. Probabilistic latent semantic index-
ing. In Proceedings of the 22nd annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 50?57.
E.H. Hovy. 1993. Automated discourse generation using
discourse structure relations. Artificial intelligence,
63(1-2):341?385.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
M.I. Jordan, Z. Ghahramani, T.S. Jaakkola, and L.K.
Saul. 1999. An introduction to variational methods
for graphical models. Machine learning, 37(2):183?
233.
H. Kamp. 1981. A theory of truth and semantic repre-
sentation. Formal methods in the study of language,
1:277?322.
M. Lapata. 2006. Automatic evaluation of information
ordering: Kendall?s tau. Computational Linguistics,
32(4):471?484.
L. Lova?sz and M.D. Plummer. 1986. Matching theory.
Elsevier Science Ltd.
Y. Lu and C. Zhai. 2008. Opinion integration through
semi-supervised topic modeling. In Proceeding of
the 17th international conference on World Wide Web,
pages 121?130.
Daniel Marcu. 1998. The rhetorical parsing of natural
language texts. In ACL ?98, pages 96?103.
Q.Mei, X. Ling, M.Wondra, H. Su, and C.X. Zhai. 2007.
Topic sentiment mixture: modeling facets and opin-
ions in weblogs. In Proceedings of the 16th interna-
tional conference on World Wide Web, pages 171?180.
L.R. Rabiner. 1989. A tutorial on hidden Markov models
and selected applications in speech recognition. Pro-
ceedings of the IEEE, 77(2):257?286.
R. Soricut and D. Marcu. 2003. Sentence level dis-
course parsing using syntactic and lexical information.
In Proceedings of the 2003 Conference of the NAACL-
HTC, pages 149?156.
B. Sun, P. Mitra, C.L. Giles, J. Yen, and H. Zha. 2007.
Topic segmentation with shared topic detection and
alignment of multiple documents. In Proceedings of
the 30th ACM SIGIR, pages 199?206.
ChengXiang Zhai, Atulya Velivelli, and Bei Yu. 2004.
A cross-collection mixture model for comparative text
minning. In Proceeding of the 10th ACM SIGKDD
international conference on Knowledge discovery in
data mining, pages 743?748.
L. Zhuang, F. Jing, and X.Y. Zhu. 2006. Movie re-
view mining and summarization. In Proceedings of
the 15th ACM international conference on Information
and knowledge management, pages 43?50.
1535
