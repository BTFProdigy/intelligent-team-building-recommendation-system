Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 54?59,
Prague, June 2007. c?2007 Association for Computational Linguistics
On the Role of Lexical and World Knowledge in RTE3 
Peter Clark1, William R. Murray1, John Thompson1, Phil Harrison1,  
Jerry Hobbs2, Christiane Fellbaum3  
1Boeing Phantom Works, The Boeing Company, Seattle, WA 98124 
2USC/ISI, 4676 Admiralty Way, Marina del Rey, CA 90292 
3Princeton University, NJ 08544 
{peter.e.clark,william.r.murray,john.a.thompson,philip.harrison}@boeing.com, 
hobbs@isi.edu, fellbaum@clarity.princeton.edu 
 
 
Abstract 
To score well in RTE3, and even more so 
to create good justifications for entailments, 
substantial lexical and world knowledge is 
needed. With this in mind, we present an 
analysis of a sample of the RTE3 positive 
entailment pairs, to identify where and 
what kinds of world knowledge are needed 
to fully identify and justify the entailment, 
and discuss several existing resources and 
their capacity for supplying that knowledge. 
We also briefly sketch the path we are fol-
lowing to build an RTE system (Our im-
plementation is very preliminary, scoring 
50.9% at the time of RTE). The contribu-
tion of this paper is thus a framework for 
discussing the knowledge requirements 
posed by RTE and some exploration of 
how these requirements can be met. 
1 Introduction 
The Pascal RTE site defines entailment between 
two texts T and H as holding "if, typically, a hu-
man reading T would infer that H is most likely 
true" assuming "common human understanding of 
language as well as common background knowl-
edge." While a few RTE3 entailments can be rec-
ognized using simple syntactic matching, the ma-
jority rely on significant amounts of this "common 
human understanding" of lexical and world knowl-
edge. Our goal in this paper is to analyze what that 
knowledge is, create a preliminary framework for 
it, and explore a few available sources for it. In the 
short term, such knowledge can be (and has been) 
used to drive semantic matching of the T and H 
dependency/parse trees and their semantic repre-
sentations, as many prior RTE systems perform, 
e.g., (Hickl et al, 2006). In the long term, com-
puters should be able to perform deep language 
understanding to build a computational model of 
the scenario being described in T, to reason about 
the entailment, answer further questions, and create 
meaningful justifications. With this longer term 
goal in mind, it is useful to explore the types of 
knowledge required. It also gives a snapshot of the 
kinds of challenges that RTE3 poses. 
 
The scope of this paper is to examine the underly-
ing lexical/world knowledge requirements of RTE, 
rather than the more syntactic/grammatical issues 
of parsing, coreference resolution, named entity 
recognition, punctuation, coordination, typographi-
cal errors, etc. Although there is a somewhat blurry 
line between the two, this separation is useful for 
bounding the analysis. It should be noted that the 
more syntactic issues are themselves vast in RTE, 
but here we will not delve into them. Instead, we 
will perform a thought experiment in which they 
have been handled correctly. 
2 Analysis 
Based on an analysis of 100 (25%) of the positive 
entailments in the RTE3 test set, we have divided 
the knowledge requirements into several rough 
categories, which we now present. We then sum-
marize the frequency with which examples in this 
sample fell into these categories. The examples 
below are fragments of the original test questions, 
abbreviated and occasionally simplified. 
2.1 Syntactic Matching 
In a few cases, entailment can be identified by syn-
tactic matching of T and H, for example: 
54
489.T "The Gurkhas come from Nepal and??  
489.H "The Gurkhas come from Nepal." 
Other examples include 299, 489, and 456. In 
some cases, the syntactic matching can be very 
complex, e.g., examples 152, 724. 
2.2 Synonyms 
Synonymy is often needed to recognize entailment, 
648.T "?go through ? licencing procedures..." 
648.H "?go through the licencing processes." 
Other examples include 286 ("dismiss"/"throw 
out"), 37 ("begin?/"start"), 236 ("wildfire"/"bush 
fire"), and, arguably, 462 ("revenue"/"proceeds"). 
2.3 Generalizations (Hypernyms) 
Similarly, subsumption (generalization) relation-
ships between word senses need to be recognized 
(whether or not a fixed set of senses are used), eg. 
148.T "Beverly served...at WEDCOR" 
148.H "Beverly worked for WEDCOR." 
Others include 178 ("succumbed" as a kind of 
"killed"), and 453 ("take over" as a kind of "buy"). 
2.4 Noun Redundancy 
Sometimes a noun in a compound can be dropped: 
607.T "single-run production process..." 
607.H "Single-run production..." 
Other examples include 269 ("increasing preva-
lence of" ? "increasing"), 604 ("mini-mill proc-
ess" ? "mini-mill"), and (at the phrase level) 668 
("all segments of the public" ? "the public"). 
2.5 Noun-Verb Relations  
Often derivationally related nouns and verbs occur 
in the pairs. To identify and justify the entailment, 
the relationship and its nature is needed, as in: 
 
480 "Marquez is a winner..." ?"Marquez won..." 
 
Other examples include 286 ("pirated", "piracy"), 
and 75 ("invent", "invention"). In some cases, the 
deverbal noun denotes the verb's event, in other 
cases it denotes one of the verb?s arguments (e.g., 
"winner" as the subject/agent of a "win" event).  
2.6 Compound Nouns 
Some examples require inferring the semantic rela-
tion between nouns in a compound, e.g., 
168 "Sirius CEO Karmazin" ? "Karmazin is an 
executive of Sirius" 
583 "physicist Hawking" ? "Hawking is a physi-
cist" 
In some cases this is straightforward, others require 
more detailed knowledge of the entities involved. 
2.7 Definitions 
Although there is somewhat of a fuzzy boundary 
between word and world knowledge, we draw this 
distinction here. Some examples of RTE pairs 
which require knowing word meanings are: 
667 "? found guilty..." ? "?convicted..." 
328 "sufferers of coeliac disease..." ? "coeliacs..." 
 
The second example is particularly interesting as 
many readers (and computers) will not have en-
countered the word "coeliacs" before, yet a person 
can reasonably infer its meaning on the fly from 
context and morphology - something challenging 
for a machine to do. Definitions of compound 
nouns are also sometimes needed, e.g., ?family 
planning? (612) and ?cough syrup? (80).  
2.8 World Knowledge: General 
A large number of RTE pairs require non-
definitional knowledge about the way the world 
(usually) is, e.g.,: 
273 "bears kill people"  ? "bears attack people" 
 
People recognize this entailment as they know 
(have heard about) how people might be killed by 
a bear, and hence can justify why the entailment is 
valid. (They know that the first step in the bear 
killing a person is for the bear to attack that person.) 
Some other examples are: 
499 "shot dead" ? "murder" 
705 "under a contract with" ? "cooperates with" 
721 "worked on the law" ? "discussed the law" 
731 "cut tracts of forest" ? "cut trees in the forest" 
732 "establishing tree farms"  
? "trees were planted" 
639 "X's plans for reorganizing"  
? "X intends to reorganize" 
328 "the diets must be followed by <person>"  
? "the diets are for <person>" 
722 X and Y vote for Z ? X and Y agree to Z. 
 
All these cases appeal to a person's world knowl-
edge concerning the situation being described. 
55
2.9 World Knowledge: Core Theories 
In addition to this more specific knowledge of the 
world, some RTE examples appeal to more general, 
fundamental knowledge (e.g., space, time, plans, 
goals). For example 
 
6.T "Yunupingu is one of the clan of..." 
6.H "Yunupingu is a member of..." 
 
appeals to a basic rule of set inclusion, and 10 (a 
negative entailment: "unsuccessfully sought elec-
tion" ? not elected) appeals to core notions of 
goals and achievement. Several examples appeal to 
core spatial knowledge, e.g.: 
 
491.T "...come from the high mountains of Nepal." 
491.H "...come from Nepal." 
 
178.T "...3 people in Saskatchewan succumbed to 
the storm." 
178.H "...a storm in Saskatchewan." 
 
491 appeals to regional inclusion (if X location Y, 
and Y is in Z, then X location Z), and 178 appeals 
to colocation (if X is at Y, and X physically inter-
acts with Z, then Z is at Y). Other spatial examples 
include 236 ("around Sydney" ? "near Sydney"), 
and 129 ("invasion of" ? "arrived in"). 
2.10 World Knowledge: Frames and Scripts 
Although loosely delineated, another category of 
world knowledge concerns stereotypical places, 
situations and the events which occur in them, with 
various representational schemes proposed in the 
AI literature, e.g., Frames (Minsky 1985), Scripts 
(Schank 1983). Some RTE examples require rec-
ognizing the implicit scenario ("frame", "script", 
etc.) which T describes to confirm the new facts or 
relationships introduced in H are valid. A first ex-
ample is: 
 
358.T "Kiesbauer was target of a letter bomb..."  
358.H "A letter bomb was sent to Kiesbauer." 
 
A person recognizes H as entailed by T because 
he/she knows the "script" for letter bombing, 
which includes sending the bomb in the mail. Thus 
a person could also recognize alternative verbs in 
358.H as valid (e.g., "mailed", "delivered") or in-
valid (e.g., "thrown at", "dropped on"), even 
though these verbs are all strongly associated with 
words in T. For a computer to fully explain the 
entailment, it would need similar knowledge.  
 
As a second example, consider: 
 
538.T "...the O. J. Simpson murder trial..." 
538.H "O. J. Simpson was accused of murder." 
 
Again, this requires knowing about trials: That 
they involve charges, a defendant, an accusation, 
etc., in order to validate H as an entailed expansion 
of T. In this example, there is also a second twist to 
it as the noun phrase in 538.T equally supports the 
hypothesis "O. J. Simpson was murdered." (e.g., 
consider replacing "O. J." with "Nicole"). It is only 
a reference elsewhere in the T sentence to "Simp-
son's attorneys" that suggests Simpson is still alive, 
and hence couldn't have been the victim, and hence 
must be the accused, that clarifies 538.H as being 
correct, a highly complex chain of reasoning.  
 
As a third example, consider: 
736.T "In a security fraud case, Milken was sen-
tenced to 10 years in prison." 
736.H "Milken was imprisoned for security fraud." 
 
This example is particularly interesting, as one 
needs to recognize security fraud as Milken's crime, 
a connection which not stated in T. A human 
reader will recognize the notion of sentencing, and 
thus expect to see a convict and his/her crime, and 
hence can align these expectations with T, validat-
ing H. Thus again, deep knowledge of sentencing 
is needed to understand and justify the entailment. 
 
Some other examples requiring world knowledge 
to validate their expansions, include 623 ("narcot-
ics-sniffing dogs" ? "dogs are used to sniff out 
narcotics"), and 11 ("the Nintendo release of the 
game" ? "the game is produced by Nintendo"). 
2.11 Implicative Verbs 
Some RTE3 examples contain complement-taking 
verbs that make an implication (either positive or 
negative) about the complement. For example: 
 
668 "A survey shows that X..." ? "X..." 
657 "...X was seen..." ? "...X..." 
725 ?...decided to X..." ? "...X..." 
716 "...have been unable to X..." ? "...do not X" 
56
 Table 1: Frequency of different entailment 
phenomena from a sample of 100 RTE3 pairs. 
In the first 3 the implication is positive, but in the 
last the implication is negative. (Nairn et al 2006) 
provide a detailed analysis of this type of behavior. 
In fact, this notion of implicature (one part of a 
sentence making an implication about another part) 
extends beyond single verbs, and there are some 
more complex examples in RTE3, e.g.: 
 
453 "...won the battle to X..." ? "...X..." 
 
784.T "X  reassures Russia it has nothing to fear..." 
784.H "Russia fears..." 
 
In this last example the implication behavior is 
quite complex: (loosely) If X reassures Y of Z, 
then Y is concerned about not-Z. 
2.12 Metonymy/Transfer 
In some cases, language allows us to replace a 
word (sense) with a closely related word (sense):  
708.T "Revenue from stores funded..." 
708.H "stores fund..." 
 
Rules for metonymic transfer, e.g., (Fass 2000), 
can be used to define which transfers are allowed. 
Another example is 723 ??pursue its drive to-
wards X? ? ??pursue X?. 
2.13 Idioms/Protocol/Slang 
Finally, some RTE pairs rely on understanding 
idioms, slang, or special protocols used in lan-
guage, for example: 
 
12 "Drew served as Justice. Kennon returned to 
claim Drew's seat"  ? "Kennon served as Justice." 
486 "name, 1890-1970" ? "name died in 1970" 
408 "takes the title of" ? "is" 
688 "art finds its way back" ? "art gets returned" 
 
The phrases in these examples all have special 
meaning which cannot be easily derived composi-
tionally from their words, and thus require special 
handling within an entailment system. 
2.14 Frequency Statistics 
Table 1 shows the number of positive entailments 
in our sample of 100 that fell into the different 
categories (some fell into several). While there is a 
certain subjectivity in the boundaries of the catego-
ries, the most significant observation is that very 
few entailments depend purely on syntactic ma-
nipulation and simple lexical knowledge (syno-
nyms, hypernyms), and that the vast majority of 
entailments require significant world knowledge, 
highlighting one of the biggest challenges of RTE. 
In addition, the category of general world knowl-
edge -- small, non-definitional facts about the way 
the world (usually) is -- is the largest, suggesting 
that harvesting and using this kind of knowledge 
should continue to be a priority for improving per-
formance on RTE-style tasks. 
3 Sources of World Knowledge 
While there are many potential sources of the 
knowledge that we have identified, we describe 
three in a bit more detail and how they relate to the 
earlier analysis. 
3.1 WordNet 
WordNet (Fellbaum, 1998) is one of the most ex-
tensively used resources in RTE already and in 
computational linguistics in general. Despite some 
well-known problems, it provides broad coverage 
of several key relationships between word senses 
(and words), in particular for synonyms, hy-
pernyms (generalizations), meronyms (parts), and 
semantically (?morphosemantically?) related 
forms. From the preceding analysis, WordNet does 
contain the synonyms {"procedure","process"}, 
{"dismiss","throw out"}, {"begin","start"}, but 
does not contain the compound "wild fire" and 
(strictly correctly) does not contain {"reve-
nue","proceeds"} as synonyms. In addition, the 
57
three hypernyms mentioned in the earlier analysis 
are in WordNet. WordNet alo links (via the ?mor-
phosemantic? link) the 3 noun-verb pairs men-
tioned earlier (win/winner, pirated/piracy, in-
vent/invention) ? however it does not currently 
distinguish the nature of that link (e.g., agent, re-
sult, event). WordNet is currently being expanded 
to include this information, as part of the 
AQUAINT program.  
 
Two independently developed versions of the 
WordNet glosses expressed in logic are also avail-
able: Extended WordNet (Moldovan and Rus, 
2001) and a version about to be released with 
WordNet3.0 (again developed under AQUAINT). 
These in principle can help with definitional 
knowledge. From our earlier analysis, it turns out 
"convicted" is conveniently defined in WordNet as 
"pronounced or proved guilty" potentially bridging 
the gap for pair 667, although there are problems 
with the logical interpretation of this particular 
gloss in both resources mentioned. WordNet does 
have "coeliac", but not in the sense of a person 
with coeliac disease1. 
 
In addition, several existing ?core theories? (e.g., 
TimeML, IKRIS) that can supply some of the fun-
damental knowledge mentioned earlier (e.g., space, 
time, goals) are being connected to WordNet under 
the AQUAINT program. 
3.2 The DIRT Paraphrase Database 
Paraphrases have been used successfully by several 
RTE systems (e.g., Hickl et al, 2005). With re-
spect to our earlier analysis, we examined Lin and 
Pantel's (2001) paraphrase database built with their 
system DIRT, containing 12 million entries. While 
there is of course noise in the database, it contains 
a remarkable amount of sensible world knowledge 
as well as syntactic rewrites, albeit encoded as 
shallow rules lacking word senses. 
 
Looking at the examples from our earlier analysis 
of general world knowledge, we find that three are 
supported by paraphrase rules in the database: 
273: X kills Y ? X attacks Y 
499: X shoots Y ? X murders Y 
                                                 
1  This seems to be an accidental gap; WordNet contains 
many interlinked disease-patient noun pairs, incl. "dia-
betes-diabetic," "epilepsy-eplileptic," etc. 
721: X works on Y ? X discusses Y 
 
And one that could be is not, namely: 
705: X is under a contract with Y ? X coop-
erates with Y (not in the database) 
 
Other examples are outside the scope of DIRT's 
approach (i.e., ?X pattern1 Y? ? ?X pattern2 Y?), 
but nonetheless the coverage is encouraging. 
3.3 FrameNet 
In our earlier analysis, we identified knowledge 
about stereotypical situations and their events as 
important for RTE. FrameNet (Baker et al 1998) 
attempts to encode this knowledge. FrameNet was 
used with some success in RTE2 by Burchardt and 
Frank (2005). FrameNet's basic unit - a Frame - is 
a script-like conceptual schema that refers to a 
situation, object, or event along with its partici-
pants (Frame Elements), identified independent of 
their syntactic configuration. 
 
We earlier discussed how 538.T "...the O. J. Simp-
son murder trial..." might entail 538.H "O. J. Simp-
son was accused of murder." This case applies to 
FrameNet?s Trial frame, which includes the Frame 
Elements Defendant and Charges, with Charges 
being defined as "The legal label for the crime that 
the Defendant is accused of.", thus stating the rela-
tionship between the defendant and the charges, 
unstated in T but made explicit in H, validating the 
entailment. However,  the lexical units instantiat-
ing the Frame Elements are not yet disambiguated 
against a lexical database, limiting full semantic 
understanding. Moreover, FrameNet's  world 
knowledge is stated informally in English descrip-
tions, though it may be possible to convert these to 
a more machine-processable form either manually 
or automatically. 
3.4 Other Resources 
We have drawn attention to these three resources 
as they provide some answers to the requirements 
identified earlier. Several other publicly available 
resources could also be of use, including VerbNet 
(Univ Colorado at Boulder), the Component Li-
brary (Univ Texas at Austin), OpenCyc (Cycorp), 
SUMO, Stanford's additions to WordNet, and the 
Tuple Database (Boeing, following Schubert's 
2002 approach), to name but a few. 
58
4 Sketch of our RTE System 
Although not the primary purpose of this paper, we 
briefly sketch the path we are following to build an 
RTE system able to exploit the above resources. 
Our implementation is very preliminary, scoring 
50.9% at the time of RTE and 52.6% now (55.0% 
on the 525 examples it is able to analyze, guessing 
"no entailment" for the remainder). Nevertheless, 
the following shows the direction we are moving in 
 
Like several other groups, our basic approach is to 
generate logic for the T and H sentences, and then 
explore the application of inference rules to elabo-
rate T, or transform H, until H matches T. Parsing 
is done using a broad coverage chart parser. Sub-
sequently, an abstracted form of the parse tree is 
converted into a logical form, for example: 
 299.H "Tropical storms cause severe damage." 
subject(_Cause1, _Storm1) 
sobject(_Cause1, _Damage1) 
mod(_Storm1, _Tropical1) 
mod(_Damage1, _Severe1) 
input-word(_Storm1, "storm", noun) 
[same for other words] 
 
Entailment is determined if every clause in the se-
mantic representation of H semantically matches 
(subsumes) some clause in T. Two variables in a 
clause match if their input words are the same, or 
some WordNet sense of one is the same as or a 
hypernym of the other. In addition, the system 
searches for DIRT paraphrase rules that can trans-
form the sentences into a form which can then 
match. The explicit use of WordNet and DIRT re-
sults in comprehensible, machine-generated justifi-
cations when entailments are found, , e.g.,: 
 
T: "The Salvation Army operates the shelter under 
a contract with the county." 
H: "The Salvation Army collaborates with the 
county." 
Yes! Justification: I have general knowledge that: 
  IF X works with Y THEN X collaborates with Y 
Here: X = the Salvation Army, Y = the county 
Thus, here: 
        I can see from T: the Salvation Army works 
with the county (because "operate" and "work" 
mean roughly the same thing) 
Thus it follows that:  
  The Salvation Army collaborates with the county.  
We are continuing to develop our system and ex-
pand the number of knowledge sources it uses.  
5 Summary 
To recognize and justify textual entailments, and 
ultimately understand language, considerable lexi-
cal and world knowledge is needed. We have pre-
sented an analysis of some of the knowledge re-
quirements of RTE3, and commented on some 
available sources of that knowledge. The analysis 
serves to highlight the depth and variety of knowl-
edge demanded by RTE3, and contributes a rough 
framework for organizing these requirements. Ul-
timately, to fully understand language, extensive 
knowledge of the world (either manually or auto-
matically acquired) is needed. From this analysis, 
RTE3 is clearly providing a powerful driving force 
for research in this direction. 
Acknowledgements 
This work was performed under the DTO 
AQUAINT program, contract N61339-06-C-0160.  
References 
Collin Baker, Charles Fillmore, and John Lowe. 1998. 
"The Berkeley FrameNet Project". Proc 36th ACL 
Aljoscha Burchardt and Anette Frank. 2005. Approach-
ing Textual Entailment with LFG and FrameNet 
Frames. in 2nd PASCAL RTE Workshop, pp 92-97. 
Dan Fass. 1991. "Met*: A Method for Discriminating 
Metonymy and Metaphor by Computer". In Compu-
tational Linguistics 17 (1), pp 49-90. 
Christiane Fellbaum. 1998. ?WordNet: An Electronic 
Lexical Database.? The MIT Press. 
Andrew Hickl, Jeremy Bensley, John Williams, Kirk 
Roberts, Bryan Rink, and Ying Shi. ?Recognizing 
Textual Entailment with LCC?s Groundhog System?, 
in Proc 2nd PASCAL RTE Workshop, pp 80-85. 
Dekang Lin and Patrick Pantel. 2001. "Discovery of 
Inference Rules for Question Answering". Natural 
Language Engineering 7 (4) pp 343-360.  
Marvin Minsky. 1985. "A Framework for Representing 
Knowledge". In Readings in Knowledge Repn. 
Dan Moldovan and Vasile Rus, 2001. Explaining An-
swers with Extended WordNet, ACL 2001.  
Rowan Nairn, Cleo Condoravdi and Lauri Karttunen. 
2006. Computing Relative Polarity for Textual Infer-
ence. In the Proceedings of ICoS-5. 
Len Schubert. 2002. "Can we Derive General World 
Knowledge from Texts?", Proc. HLT'02, pp84-87. 
59
Augmenting WordNet for Deep
Understanding of Text
Peter Clark1
Christiane Fellbaum2
Jerry R. Hobbs3
Phil Harrison1
William R. Murray1
John Thompson1
1The Boeing Company (USA)
2Princeton University (USA)
3University of Southern California (USA)
email: peter.e.clark@boeing.com
Abstract
One of the big challenges in understanding text, i.e., constructing an over-
all coherent representation of the text, is that much information needed
in that representation is unstated (implicit). Thus, in order to ?fill in
the gaps? and create an overall representation, language processing sys-
tems need a large amount of world knowledge, and creating those knowl-
edge resources remains a fundamental challenge. In our current work,
we are seeking to augment WordNet as a knowledge resource for lan-
guage understanding in several ways: adding in formal versions of its
word sense definitions (glosses); classifying the morphosemantic links
between nouns and verbs; encoding a small number of ?core theories?
about WordNet?s most commonly used terms; and adding in simple rep-
resentations of scripts. Although this is still work in progress, we describe
our experiences so far with what we hope will be a significantly improved
resource for the deep understanding of language.
45
46 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
1 Introduction
Much information that text is intended to convey is not explicitly stated. Rather, the
reader constructs a mental model of the scene described by the text, including many
?obvious? features that were not explicitly mentioned. By one estimate, the ratio of
explicit to implicit facts is 1:8 (Graesser, 1981), making the task of understanding
text, i.e., constructing a coherent representation of the scene that the author intended
to convey, very difficult, even given the generally reasonable quality of syntactic in-
terpretation that today?s systems produce. For example, given the sentence:
A soldier was killed in a gun battle.
a reader will infer that (probably):
The soldier was shot; The soldier died; There was a fight; etc.
even though none of these facts are explicitly stated. A person is able to draw these
plausible conclusions because of the large amounts of world knowledge he/she has,
and his/her ability to use them to construct an overall mental model of the scene being
described.
A key requirement for this task is access to a large body of world knowledge. How-
ever, machines are currently poorly equipped in this regard. Although a few knowl-
edge encoding projects are underway, e.g., Cyc (Lenat and Guha, 1989), developing
such resources continues to be a major challenge, and any contribution to this task
has significant potential benefit. WordNet (Miller, 1995; Fellbaum, 1998) presents an
unique avenue for making inroads into this problem: It already has broad coverage,
multiple lexicosemantic connections, and significant knowledge encoded (albeit infor-
mally) in its glosses. It can thus be viewed as on the way to becoming an extensively
leveragable, ?lightweight? knowledge base for reasoning. In fact, WordNet aleady
plays a central role in many question-answering systems e.g., 21 of the 26 teams in
the recent PASCAL RTE3 challenge used WordNet (Giampiccolo et al, 2007), and
most other large-scale resources already include mappings to it and thus can leverage
it easily. In our work we are developing several augmentations to WordNet to improve
its utility further, and we report here on our experiences to date.
Althoughwe are performing experimentswith recognizing textual entailment (RTE)
(determining whether a hypothesis sentence H follows from some text T), it is impor-
tant to note that RTE is not our end-goal. Many existing RTE systems, e.g., (Adams
et al, 2007; Chambers et al, 2007) largely work by statistically scoring the match be-
tween T and H, but this to an extent sidesteps ?deep? language understanding, namely
building a coherent, internal representation of the overall scenario the input text was
intended to convey. RTE is one way of measuring success in this endeavor, but it is
also possible to do moderately well in RTE without the system even attempting to
?understand? the scenario the text is describing. It is yet to be seen whether very high
performance in RTE can be obtained without some kind of deep language understand-
ing of the entire scene that a text conveys.
We are testing our work with BLUE, Boeing?s Language Understanding Engine,
which we first describe. We then present the WordNet augmentations that we are de-
veloping, and our experience with these as well as with the DIRT paraphrase database.
Augmenting WordNet for Deep Understanding of Text 47
The contribution of this paper is some preliminary insight into avenues and challenges
for creating and leveraging more world knowledge, in the context of WordNet, for
deeper language understanding.
2 Text Interpretation and Subsumption
2.1 Text Interpretation
For text interpretation we are using BLUE, Boeing?s Language Understanding Engine
(Clark and Harrison, 2008), comprising a parser, logical form (LF) generator, and fi-
nal logic generator. Parsing is performed using SAPIR, a mature, bottom-up, broad
coverage chart parser (Harrison and Maxwell, 1986). The parser?s cost function is
biased by a database of manually and corpus-derived ?tuples? (good parse fragments),
as well as hand-coded preference rules. During parsing, the system also generates
a logical form (LF), a semi-formal structure between a parse and full logic, loosely
based on Schubert and Hwang (1993). The LF is a simplified and normalized tree
structure with logic-type elements, generated by rules parallel to the grammar rules,
that contains variables for noun phrases and additional expressions for other sentence
constituents. Some disambiguation decisions are performed at this stage (e.g., struc-
tural, part of speech), while others are deferred (e.g., word senses, semantic roles),
and there is no explicit quantifier scoping. A simple example of an LF is shown below
(items starting with underscores _?A?I? denote variables):
;;; LF for "A soldier was killed in a gun battle."
(DECL ((VAR _X1 "a" "soldier")
(VAR _X2 "a" "battle" (NN "gun" "battle")))
(S (PAST) NIL "kill" _X1 (PP "in" _X2)))
The LF is then used to generate ground logical assertions of the form r(x,y), con-
taining Skolem instances, by applying a set of syntactic rewrite rules recursively to it.
Verbs are reified as individuals, Davidsonian-style. An example of the output is:
;;; logic for "A soldier was killed in a gun battle."
object(kill01,soldier01)
in(kill01,battle01)
modifier(battle01,gun01)
plus predicates associating each Skolem with its corresponding input word. At this
stage of processing, the predicates are syntactic relations (subject(x,y), object(x,y),
modifier(x,y), and all the prepositions, e.g., in(x,y)). Definite coreference is computed
by a special module which uses the (logic for the) referring noun phrase as a query
on the database of assertions. Another module performs special structural transforma-
tions, e.g., when a noun or verb should map to a predicate rather than an individual.
Two additional modules perform (currently naive) word sense disambiguation (WSD)
and semantic role labelling (SRL), described further in Clark and Harrison (2008).
However, for our RTE experiments we have found it more effective to leave senses
and roles underspecified, effectively considering all valid senses and roles (for the
given lexical features) during reasoning until instantiated by the rules that apply.
48 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
2.2 Subsumption
A basic operation for reasoning is determining if one set of clauses subsumes (is more
general than, is thus implied by) another, e.g., (the logic for) ?A person likes a person?
subsumes ?A man loves a woman?. This basic operation is used both to determine if
an axiom applies, and in RTE to determine if a text H subsumes (is implied by) a text
T or its axiom-expanded elaboration. A set S1 of clauses subsumes another S2 if each
clause in S1 subsumes some (different) member of S2. A clause C1 subsumes another
C2 if both (for binary predicates) of C1?s arguments subsume the corresponding argu-
ments in C2, and C1 and C2?s predicates ?match?. An argument A1 subsumes another
A2 if some word sense for A1?s associated word is equal or more general (a hypernym
of) some word sense of A2?s associated word (thus effectively considering all possible
word senses for A1 and A2)1. We also consider adjectives related by WordNet?s ?sim-
ilar? link, e.g., ?clean? and ?pristine?, to be equal. Two syntactic predicates ?match?
(i.e., are considered to denote the same semantic relation) according to the following
rules:
1. both are the same;
2. either is the predicate ?of? or ?modifier?;
3. the predicates ?subject? and ?by? match (for passives);
4. the two predicates are in a small list of special cases that should match e.g., ?on?
and ?onto?.
These rules for matching syntactic roles are clearly an approximation to match-
ing semantic roles, but have performed better in our experiments than attempting to
explicitly assign (with error) semantic roles early on and then matching on those.
In addition, in language, ideas can be expressed using different parts of speech
(POS) for the same basic notion, e.g., verb or noun as in ?The bomb destroyed the
shrine? or ?The destruction of the shrine by the bomb? (Gurevich et al, 2006). To
handle these cross-POS variants, when finding the word senses of a word (above) our
system considers all POS, independent of its POS in the original text. Combined with
the above predicate-matching rules, this is a simple and powerful way of aligning
expressions using different POSs, e.g.:
? ?The bomb destroyed the shrine? and ?The destruction of the shrine by the
bomb? (but not ?The destruction of the bomb by the shrine?) are recognized as
equivalent.
? ?A person attacks with a bomb? and ?There is a bomb attack by a person? are
recognized as equivalent.
? ?There is a wrecked car?, ?The car was wrecked?, and ?The car is a wreck?
(adjective, verb, and noun forms) are recognized as equivalent.
Although clearly these heuristics can go wrong, they provide a basic mechanism
for assessing simple equivalence and subsumption between texts.
1Clearly this can go wrong, e.g., if the contexts of T and H are different so repeated/matching words
have incompatible intended senses, although such discontinuities are unusual in natural text.
Augmenting WordNet for Deep Understanding of Text 49
2.3 Experimental Test Bed
As an experimental test bed we have developed a publically available RTE-style test
suite2 of 250 pairs (125 entailed, 125 not entailed). As our goal is deeper semantic
processing, the texts are syntactically simpler than the PASCAL RTE sets (at www.
pascal-network.org) but semantically challenging to process. We use examples
from this test suite (and others) in this paper.
3 Exploiting Lexical & World Knowledge
3.1 Use of WordNet?s Glosses
Translation to Logic WordNet?s word sense definitions (glosses) appear to contain
substantial amounts of world knowledge that could help with semantic interpretation
of text, and we have been exploring leveraging these by translating them into first-
order logic. We have also experimented with Extended WordNet (XWN), a similar
database constructed several years ago by Moldovan and Rus (2001).
To do the translation, a different language interpreter, developed by ISI, was used
(for historic reasons? BLUE was not available at the time the translations were done,
and has not been exercised or extended for definition processing). ISI?s system works
as follows: First each gloss is converted into a sentence of the form ?word is gloss?
and parsed using the Charniak parser. Then the parse tree is then converted into a log-
ical syntax by a system called LFToolkit, developed by Nishit Rathod. In LFToolkit,
lexical items are translated into logical fragments involving variables. Finally, as syn-
tactic relations are recognized, variables in the constituents are identified as equal. For
example, ?John works? is translated into John(x1) & work(e,x2)& present(e), where e
is a working event, and then a rule which recognizes ?John? as the subject of ?works?
sets x1 and x2 equal to each other. Rules of this sort were developed for a large
majority of English syntactic constructions. ISI?s system was then used to translate
the modified WordNet glosses into axioms. For example (rewritten from the original
eventuality notation):
;;; "ambition#n2: A strong drive for success"
ambition(x1) -> a(x1) & strong(x1) & drive(x1) & for(x1,x6) & success(x6)
Predicates are assigned word senses using the new-ly released WordNet sense-
tagged gloss corpus3. This process was applied to all ? 110,000 glosses, but with
particular focus on glosses for the 5,000 ?core? (most frequently used) synsets. It
resulted in good translations for 59.4% of the 5,000 core glosses, with lower quality
for the entire gloss corpus. Where there was a failure, it was generally the result of a
bad parse, with constructions for which no LFToolkit rules had been written. In these
cases, the constituents are translated into logic, so that no information is lost; what
is lost is the equalities between variables that provides the connections between the
constituents. For instance, in the ?John works? example, we would know that there
was someone named John and that somebody works, but we would not know that they
were the same person. Altogether 98.1% of the 5,000 core glosses were translated
into correct axioms (59.4%) or axioms that had all the propositional content but were
2http://www.cs.utexas.edu/~pclark/bpi-test-suite/
3http://wordnet.princeton.edu/glosstag
50 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
disconnected in this way (38.7%). The remaining 1.9% of these glosses had bizarrely
wrong parses due to noun-adjective ambiguities or to complex conjunction ambigui-
ties.
Using the Glosses We have used a combination of these logicalized glosses and those
from XWN to infer implicit information from text. Although the quality of the logic
is generally poor (for a variety of reasons, in particular that the glosses were never
intended for machine processing in the first place), our software was able to infer
conclusions that help answer a few entailment problems, for example:
T: Britain puts curbs on immigrant labor from Bulgaria and Romania.
H: Britain restricted workers from Bulgaria.
using the logic for the definition:
restrict#v1: "restrict", "restrain": place limits on.
plus WordNet?s knowledge that: ?put? and ?place? are synonyms; ?curb? and ?limit?
are synonyms; and a laborer is a worker. In our experiments, the glosses were used to
answer 5 of the 250 entailment questions (4 correctly). More commonly, the glosses
came ?tantalizingly close? to providing the needed knowledge. For example, for:
T: A Union Pacific freight train hit five people.
H: A locomotive was pulling the train.
it seems that the definition:
train#n1: "train", "railroad train": public transport provided
by a line of railway cars coupled together and drawn by a locomotive.
is very close to providing the needed knowledge. However, unfortunately it defines
a train as ?public transport provided by cars pulled by a locomotive? rather than just
?cars pulled by a locomotive? (the locomotive pulls the cars, not the train/public-
transport), hence the hypothesis H is not concluded. Similarly:
T: The Philharmonic orchestra draws large crowds.
H: Large crowds were drawn to listen to the orchestra.
essentially requires knowledge that crowds (typically) listen to orchestras. WordNet?s
glosses come very close to providing this, with knowledge that:
orchestra = collection of musicians
musician = someone who plays musical instrument
music = sound produced by musical instruments
listen = hear = perceive sound
However, the connection that the playing results in sound production is missing,
and hence again H cannot be inferred. These experiences with the WordNet glosses
were very common. In summary, our experience is the WordNet glosses provided
some value, being used 5 times (4 correctly) on the 250 examples in our test suite,
Augmenting WordNet for Deep Understanding of Text 51
with the short, simple definitions (e.g., bleed = lose blood) being the most reliable.
The low quality of the logic was a problem (definitional text is notoriously difficult to
interpret automatically (Ide and Veronis, 1993)), although often the knowledge came
close. Finally, 110,000 rules (approx. one per gloss) is actually quite a small number;
typically only 10?s of rules fired per sentence, rarely containing the implications we
were looking for.
3.2 Typed Morphosemantic Links
WordNet contains approximately 21,000 links connecting derivationally related verb
and noun senses, e.g., employ#v2-employee#n1; employ#v2-employment#n3. These
links turn out to be essential for mapping between verbal and nominalized expressions
(e.g. using ?destroy?-?destruction?, as mentioned earlier). However, the current links
do not state the semantic type of the relation (e.g., that employee#n1 is the UNDER-
GOER of an employ#v2 event; employment#n3 is the employ#v2 EVENT itself),
which limits WordNet?s ability to help perform semantic role labeling. In addition,
not being able to distinguish the semantics of the relationships can cause errors in
reasoning, for example distinguishing between H1 and H2 in:
T: Detroit produces fast cars.
H1: Detroit?s product is fast.
H2?: Detroit?s production is fast. [NOT entailed]
T: The Zoopraxiscope was invented by Mulbridge.
H1: Mulbridge was the inventor of the Zoopraxiscope.
H2?: Mulbridge was the invention of the Zoopraxiscope. [NOT entailed]
To type these links, we have used a semi-automatic process: First, the computer
makes a ?guess? at the appropriate semantic relation based on the morphological re-
lationship between the noun and the verb (e.g., ?A?IJ-er?A?I? nouns usually refer to the
agent), and the location of the two synsets in WordNet?s taxonomy. Second, a human
validates and corrects these, a considerably faster progress than entering them from
scratch. 9 primary semantic relations (as well as 5 rarer ones) were used, namely:
agent (e.g., employ#v2-employer#n1)
undergoer/patient (e.g., employ#v2-employee#n1)
instrument (e.g., shred#v1-shredder#n1)
recipient (e.g., grant#v2-grantee#n1)
result (e.g., produce#v2-product#n2)
body-part (e.g., adduct#v1-adductor#n1)
vehicle (e.g., cruise#v4-cruiser#n3)
location (e.g., bank#v3-bank#n4)
identity/equality (eg employ#v2-employment#n1)
The resulting database of 21,000 typed links was recently completed, constituting
a major new addition to WordNet in support of deep language processing. One of the
surprising side results of this effort was discovering how often the normal morpholog-
ical defaults (e.g., ?-er? nouns refer to agents) are violated, described in more detail in
Fellbaum et al (2007). We are now in the process of incorporating the database into
our software.
52 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
3.3 Core Theories
While WordNet?s glosses and links contain world knowledge about specific entities
and relations, there is also more fundamental knowledge about language and the world
? e.g., about space, time, and causality ? which is essential for understanding many
types of text, yet is unlikely to be expressed in dictionary definitions or automatically
learnable. To address this need, we are also encoding by hand a number of theories to
support deeper reasoning (in the style of lexical decomposition). We have axiomatized
a number of abstract core theories that underlie the way we talk about events and event
structure (Hobbs, 2008). Among these are theories of composite entities (things made
of other things), scalar notions (of which space, time, and number are specializations),
change of state, and causality. For example, in the theory of change of state, the
predication change(e1,e2) says there is a change of state from state e1 to state e2. The
predication changeFrom(e1) says there is a change out of state e1. The predication
changeTo(e2) says there is a change into state e2. An inference from changeFrom(e1)
is that e1 no longer holds. An inference from changeTo(e2) is that e2 now does hold.
In the theory of causality (Hobbs, 2005), the predication cause(e1,e2), for e1 causes
e2, is explicated. One associated inference is that if the causing happens, then the
effect e2 happens. A defeasible inference is that not-cause-not often is the same as
cause:
not(cause(x,not(e)))? cause(x,e)
In the rightward direction this is of course sometimes wrong, but if we go to the
trouble of saying that the negation of something was not caused, then very often it is
a legitimate conclusion that the causing did happen.
We are connecting these theories with WordNet by mapping the core (5,000 most
common) WordNet synsets to the theory predicates. For example, the core part of
WordNet contains 450 word senses having to do with events and event structure, and
we are in the process of encoding their meanings in terms of core theory predicates.
For example, if x lets e happen (WordNet sense let#v1), then x does not cause e not to
happen:
let#v1(x,e)? not(cause(x,not(e)))
One sense of ?go? is ?changeTo?, as in ?I go crazy?
go#v4(x,e)? changeTo(e)
(The entity x is the subject of the eventuality e.) If x frees y (the verb sense of ?free?),
then x causes a change to y being free (in the adjective sense of ?free?):
free#v1(x,y)? cause(x,changeTo(free#a1(y)))
Given these mappings and the core theories themselves, this is enough to answer
the entailment pair:
T: The captors freed the hostage.
H: The captors let the hostage go free.
Augmenting WordNet for Deep Understanding of Text 53
via successive application of the above axioms:
(part of) H interpretation? let(x,go(y,free(y)))
? not(cause(x,not(changeTo(free#a1(y)))))
? cause(x,changeTo(free#a1(y)))
? free#v1(x,y)
We are still in the early stages of developing this resource and have not yet evaluated
it, but we have already seen a number of examples of its potential utility in the text
inference problem such as above.
3.4 Scripts
Simple inference rules, such as in the above resources, provide a direct means of
drawing conclusions from a few words in the input text. However, they are largely
context-independent, i.e., not sensitive to the bigger picture which the surrounding
text provides. Consider the following example:
T: A dawn bomb attack devastated a major shrine.
H: The bomb exploded.
In this case, it is hard to express the required knowledge (to conclude H follows
from T) as simple rules (e.g., the rules ?bomb ?E?? bomb explode? or ?bomb attack
? bomb explode? are not adequate, as we do not want H to follow from ?The police
destroyed the bomb.? or ?The bomb attack was thwarted?). Rather, when a person
reads T, he/she recognizes a complete scenario from multiple bits of evidence (possi-
bly in multiple sentences), and integrates what is read with that scenario. This kind of
top-down, expectation-driven process seems essential for creating an overall, coherent
representation of text.
Although scripts are an old idea (e.g., (Schank and Abelson, 1977)) there are rea-
sons their use may be more feasible today. First, rapid advances in paraphrasing sug-
gests that the matching problem ? deciding if some text is expressing part of of a
script ? may be substantially eased. (Script work in the ?70s required stories to
be worded in exactly the right way to fire a script). Second, two new approches for
amassing knowledge are available today that were not available previously, namely au-
tomated learning from corpora, and use of Web volunteers (e.g., (Chklovski, 2005)),
and may be applicable to script acquisition (Script work in the ?70s typically worked
with tiny databases of scripts). Finally, techniques for language processing have sub-
stantially improved, making core tasks (e.g., parsing) less problematic, and opening
the possibility to easy authoring of scripts in English, followed by machine interpre-
tation. FrameNet (Baker et al, 1998) already provides a few small scripts, but does
not currently encode the complex scenarios that we would like; a vastly expanded
resource would be highly useful.
We are in the early stages of exploring this avenue, encoding scripts as a list of
simple English sentences, which are then automatically translated to WordNet-sense
tagged logic using our software. For example, a ?bombing? script looks:
A building is bombed by an attacker.
The attacker plants the bomb in the building.
54 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
The bomb explodes.
The explosion damages or destroys the building.
The explosion injures or kills people in the building.
In addition, some of these sentences are flagged as ?salient?. If any salient sentence
matches (subsumes) part of the text, then the script is triggered. When triggered,
a standard graph-matching algorithm searches for the maximal overlap between the
clauses in the (interpreted) script and the clauses in the text, and then the script is
unified with the text according to that maximal overlap, thus asserting the additional
facts contained in the script to the text under consideration. In the example earlier,
the script is triggered by, and matched with, the text, thus aligning ?building? with
?shrine?, and asserting additional facts including (the logic representation of) ?The
bomb explodes? and ?The bomb was planted in the building.?.
3.5 Using DIRT Paraphrases
Like others, we have also explored the use of the DIRT paraphrase database for rea-
soning, and we report our experiences here for comparison. The database contains
12 million rules, discovered automatically from text, of form (X relation1 Y) ? (X
relation2 Y), where relation is a path in the dependency tree/parse between constitu-
tents X and Y. Although they are noisy (informally, about 50% seem reliable), they
provided some leverage for us also, for example correctly answering:
T: William Doyle works for an auction house in Manhattan.
H?: William Doyle never goes to Manhattan. [NOT entailed]
using the DIRT rule ?IF Y works in X THEN Y goes to X? combined with negation,
and
T: The president visited Iraq in September.
H: The president traveled to Iraq.
using the (slightly strange but plausible) DIRT rule ?IF Y is visited by X THEN X
flocks to Y? and that ?A?IJflock?A?I? is a type (hyponym) of ?A?IJtravel?A?I?. In our
experiments, DIRT rules were used 47 times (27 correctly) on our 250 example test
suite. The main cause of incorrect answers was questionable/incorrect rules in the
database, e.g.:
T: The US troops stayed in Iraq.
H?: The US troops left Iraq. [NOT entailed]
was found to be entailed using the DIRT rule ?IF Y stays in X THEN Y leaves X?.
In addition, DIRT does not distinguish word senses (e.g., according to DIRT, shooting
a person/basket implies killing the person/basket and scoring a person/basket), also
contributing errors.
Despite this, the DIRT rules were useful because they go beyond just the defini-
tional knowledge in WordNet. For example, according to DIRT ?X marries Y? im-
plies, among other things: Y marries X; X lives with Y; X kisses Y; X has a child with
Y; X loves Y ? all examples of plausible world knowledge. The main limitations we
Augmenting WordNet for Deep Understanding of Text 55
found were they were noisy, did not account for word senses, and only cover one rule
pattern (X r1 Y? X r2 Y). So, for example, a rule like ?X buys Y? X pays Money?
is outside the expressive scope of DIRT.
4 Preliminary Evaluation
Although this is work in progress, we have evaluated some of these augmentations
using our test suite. As our ultimate goal is deeper understanding of text, we have de-
liberately eschewed using statistical similarity measures between T and H, and instead
used abductive reasoning to create an axiom-elaborated representation of T, and then
seen if it is subsumed by H. Although not using statistical similarity clearly hurts our
score, in particular assuming ?no entailment? when the elaborated representation of T
is not subsumed by H, we believe this keeps us appropriately focused on our longer-
term goal of deeper understanding of text. The results on our 250 pairs currently are:
H or ?H predicted by: Corrrect Incorrect
Simple syntax manipulation 11 3
WordNet taxonomy + morphosemantics 14 1
WordNet logicalized glosses 4 1
DIRT paraphrase rules 27 20
H or ?H not predicted: Corrrect Incorrect
(assumed not entailed) 97 72
Thus our overall score on this test suite is 61.2%. We have also run our software on
the PASCALRTE3 dataset (Giampiccolo et al, 2007), scoring 55.7% (excluding cases
where no initial logical representation could be constructed due to parse/LF generation
failures). In some cases, other known limitations of WordNet (eg. hypernym errors,
fine-grained senses) also caused errors in our tests (outside the scope of this paper).
However, the most significant problem, at least for these tests, was lack of world
knowledge.
5 Conclusion
A big challenge for deep understanding of text ? constructing a coherent represen-
tation of the scene it is intended to convey ? is the need for large amounts of world
knowledge. We have described our work-in-progress to augment WordNet in vari-
ous ways so it can better provide some of this knowledge, and described some initial
experiences with those augmentations, as well as with the DIRT database. Existing
WordNet aleady provides extensive leverage for language processing, as evidenced
by the large number of groups using it. The contribution of this paper is some pre-
liminary insight into avenues and challenges for further developing this resource. Al-
though somewhat anecdotal at this stage, our experience suggests the augmentations
have promise for further improving deep language processing, and we hope will result
in a significantly improved resource.
Acknowledgements This work was performed under the DTO AQUAINT program,
contract N61339-06-C-0160.
56 Clark, Fellbaum, Hobbs, Harrison, Murray, and Thompson
References
Adams, R., G. Nicolae, C. Nicolae, and S. Harabagiu (2007). Textual entailment
through extended lexical overlap and lexico-semantic matching. In Proc. ACL-
PASCAL Workshop on Textual and Entailment and Paraphrasing, pp. 119?124.
Baker, C. F., C. J. Fillmore, and J. B. Lowe (1998). The Berkeley FrameNet project.
In C. Boitet and P. Whitelock (Eds.), Proc 36th ACL Conf., CA, pp. 86?90. Kauf-
mann.
Chambers, N., D. Cer, T. Grenager, D. Hall, C. K. MacCartney, M.-C. de Marneffe,
D. R. Yeh, and C. D. Manning (2007). Learning alignments and leveraging natural
logic. In Proc. ACL-PASCAL Workshop on Textual and Entailment and Paraphras-
ing, pp. 165?170.
Chklovski, T. (2005). Collecting paraphrase corpora from volunteer contributors. In
Proc 3rd Int Conf on Knowledge Capture (KCap?05), NY, pp. 115?120. ACM.
Clark, P. and P. Harrison (2008, September). Boeing?s NLP System and the Chal-
lenges of Semantic Representation. In J. Bos and R. Delmonte (Eds.), Semantics
in Text Processing. STEP 2008 Conference Proceedings, Venice, Italy.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. Cambridge, MA:
MIT Press.
Fellbaum, C., A. Osherson, and P. Clark (2007). Putting semantics into wordnet?s
morphosemantic links. In Proc. 3rd Language and Technology Conference, Poz-
nan, Poland.
Giampiccolo, D., B. Magnini, I. Dagan, and B. Dolan (2007). Textual entailment
through extended lexical overlap and lexico-semantic matching. In Proc. ACL-
PASCAL Workshop on Textual and Entailment and Paraphrasing, pp. 1?9.
Graesser, A. C. (1981). Prose Comprehension Beyond the Word. NY: Springer.
Gurevich, O., R. Crouch, T. King, and V. de Paiva (2006). Deverbal nouns in knowl-
edge representation. In Proc. FLAIRS?06.
Harrison, P. and M. Maxwell (1986). A new implementation of GPSG. In Proc. 6th
Canadian Conf on AI (CSCSI-86), pp. 78?83.
Hobbs, J. (2005). Toward a useful notion of causality for lexical semantics. Journal
of Semantics 22, 181?209.
Hobbs, J. (2008). Encoding commonsense knowledge. Technical report, USC/ISI.
http://www.isi.edu/?hobbs/csk.html.
Ide, N. and J. Veronis (1993). Extracting knowledge-bases from machine-readable
dictionaries: Have we wasted our time? In Proc KB&KB?93 Workshop, pp. 257?
266.
Augmenting WordNet for Deep Understanding of Text 57
Lenat, D. and R. Guha (1989). Building Large Knowledge-Based Systems. MA:
Addison-Wesley.
Miller, G. (1995). WordNet: a lexical database for english. Comm. of the
ACM 38(11), 39?41.
Moldovan, D. and V. Rus (2001). Explaining answers with extended wordnet. In
Proc. ACL?01.
Schank, R. and R. Abelson (1977). Scripts, Plans, Goals and Understanding. Hills-
dale, NJ: Erlbaum.
Schubert, L. and C. Hwang (1993). Episodic logic: A situational logic for NLP. In
Situation Theory and Its Applications, pp. 303?337.
