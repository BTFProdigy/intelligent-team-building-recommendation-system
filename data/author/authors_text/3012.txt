Combining Multiple Models for Speech Information Retrieval 
Muath Alzghool and Diana Inkpen  
School of Information Technology and Engineering 
University of Ottawa 
{alzghool,diana}@ site.uottawa.ca  
Abstract 
In this article we present a method for combining different information retrieval models in order to increase the retrieval performance 
in a Speech Information Retrieval task. The formulas for combining the models are tuned on training data. Then the system is evaluated 
on test data. The task is particularly difficult because the text collection is automatically transcribed spontaneous speech, with many 
recognition errors. Also, the topics are real information needs, difficult to satisfy. Information Retrieval systems are not able to obtain 
good results on this data set, except for the case when manual summaries are included. 
 
1. Introduction  
Conversational speech such as recordings of interviews or 
teleconferences is difficult to search through. The 
transcripts produced with Automatic Speech Recognition 
(ASR) systems tend to contain many recognition errors, 
leading to low Information Retrieval (IR) performance 
(Oard et al, 2007). 
Previous research has explored the idea of combining 
the results of different retrieval strategies; the motivation is 
that each technique will retrieve different sets of relevant 
documents; therefore combining the results could produce 
a better result than any of the individual techniques. We 
propose new data fusion techniques for combining the 
results of different IR models. We applied our data fusion 
techniques to the Mallach collection (Oard et al, 2007) 
used in the Cross-Language Speech Retrieval (CLSR) task 
at Cross-Language Evaluation Forum (CLEF) 2007. The 
Mallach collection comprises 8104 ?documents? which are 
manually-determined topically-coherent segments taken 
from 272 interviews with Holocaust survivors, witnesses 
and rescuers, totalling 589 hours of speech. Figure 1 shows 
the document structure in CLSR test collection, two ASR 
transcripts are available for this data, in this work we use 
the ASRTEXT2004A field provided by IBM research with 
a word error rate of 38%. Additionally, metadata fields for 
each document include: two sets of 20 automatically 
assigned keywords determined using two different kNN 
classifiers (AK1 and AK2), a set of a varying number of 
manually-assigned keywords (MK), and a manual 
3-sentence summary written by an expert in the field.  A set 
of 63 training topics and 33 test topics were generated for 
this task. The topics provided with the collection were 
created in English from actual user requests. Topics were 
structured using the standard TREC format of Title, 
Description and Narrative fields. To enable CL-SR 
experiments the topics were translated into Czech, German, 
French, and Spanish by native speakers; Figure 2 and 3 
show two examples for English and its translation in 
French respectively. Relevance judgments were generated 
using a search-guided procedure and standard pooling 
methods. See (Oard et al, 2004) for full details of the 
collection design.  
We present results on the automatic transcripts for 
English queries and translated queries (cross-language) 
for two combination methods; we also present results 
when manual summaries and manual keywords are 
indexed. 
 
<DOC> 
<DOCNO>VHF[IntCode]-[SegId].[SequenceNum]</DOCNO\> 
<INTERVIEWDATA>Interviewee name(s) and 
birthdate</INTERVIEWDATA> 
<NAME>Full name of every person mentioned</NAME> 
<MANUALKEYWORD>Thesaurus keywords assigned to the 
segment</MANUALKEYWORD> 
<SUMMARY>3-sentence segment summary</SUMMARY> 
<ASRTEXT2004A>ASR transcript produced in 
2004</ASRTEXT2004A> 
<ASRTEXT2006A>ASR transcript produced in 
2006</ASRTEXT2006A> 
<AUTOKEYWORD2004A1>Thesaurus keywords from a kNN 
classifier</AUTOKEYWORD2004A1> 
<AUTOKEYWORD2004A2>Thesaurus keywords from a second 
kNN classifier</AUTOKEYWORD2004A2> 
</DOC> 
Figure 1. Document structure in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Child survivors in Sweden  
<desc>Describe survival mechanisms of children born 
in 1930-1933 who spend the war in concentration 
camps or in hiding and who presently live in Sweden. 
 <narr>The relevant material should describe the 
circumstances and inner resources of the surviving 
children. The relevant material also describes how 
the wartime experience affected their post-war 
adult life. </top> 
Figure 2. Example for English topic in CL-SR test collection. 
 
<top>  
<num>1159  
<title>Les enfants survivants en Su?de  
<desc>Descriptions des m?canismes de survie des 
enfants n?s entre 1930 et 1933 qui ont pass? la 
guerre en camps de concentration ou cach?s et qui 
vivent actuellement en Su?de.  
<narr>? 
</top>  
Figure 3. Example for French topic in CL-SR test collection. 
2. System Description  
Our Cross-Language Information Retrieval systems 
were built with off-the-shelf components. For the retrieval 
part, the SMART (Buckley, Salton, &Allan, 1992; Salton 
&Buckley, 1988) IR system and the Terrier (Amati &Van 
Rijsbergen, 2002; Ounis et al, 2005) IR system were 
tested with many different weighting schemes for 
indexing the collection and the queries.  
SMART was originally developed at Cornell 
University in the 1960s. SMART is based on the vector 
space model of information retrieval. We use the standard 
notation: weighting scheme for the documents, followed 
by dot, followed by the weighting scheme for the queries, 
each term-weighting scheme is described as a 
combination of term frequency, collection frequency, and 
length normalization components where the schemes are 
abbreviated according to its components variations (n no 
normalization, c cosine, t idf, l log, etc.) We used nnn.ntn, 
ntn.ntn, lnn.ntn, ann.ntn, ltn.ntn, atn.ntn, ntn.nnn , 
nnc.ntc, ntc.ntc, ntc.nnc, lnc.ntc, anc.ntc, ltc.ntc, atc.ntc 
weighting schemes (Buckley, Salton, &Allan, 1992; 
Salton &Buckley, 1988);  lnn.ntn performs very well in 
CLEF-CLSR 2005 and 2006 (Alzghool &Inkpen, 2007; 
Inkpen, Alzghool, &Islam, 2006); lnn.ntn means that lnn 
was used for documents and ntn for queries according to 
the following formulas:  
0.1)ln(nln += tfweight        (1) 
tn
Ntfweight logntn ?=     (2)      
where tf denotes the term frequency of a term t in the 
document or query, N denotes the number of documents 
in the collection, and nt denotes the number of documents 
in which the term t occurs.  
Terrier was originally developed at the University of 
Glasgow. It is based on Divergence from Randomness 
models (DFR) where IR is seen as a probabilistic process 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005). We 
experimented with the In_expC2 (Inverse Expected 
Document Frequency model with Bernoulli after-effect 
and normalization) weighting model, one of Terrier?s 
DFR-based document weighting models.  
Using the In_expC2 model, the relevance score of a 
document d for a query q is given by the formula: 
                  (3) ?
?
=
qt
dtwqtfqdsim ),(.),(
where qtf is the frequency of term t in the query q, and w(t,d) 
is the relevance score of a document d for the query term t, 
given by: 
)
5.0
1log()
)1(
1(),( 2 +
+??+?
+=
e
e
et n
Ntfn
tfnn
Fdtw   (4) 
where 
-F is the term frequency of t in the whole collection. 
-N is the number of document in the whole collection.  
-nt is the document frequency of t. 
-ne is given by ))
1
(1( Fte N
n
Nn
???=  (5) 
- tfne is the normalized within-document frequency of the 
term t in the document d. It is given by the normalization 2 
(Amati &Van Rijsbergen, 2002; Ounis et al, 2005): 
)_1(log
l
lavgctftfn ee ?+?=     (6) 
where c is a parameter, tf is the within-document 
frequency of the term t in the document d, l is the 
document length, and avg_l is the average document 
length in the whole collection. 
We estimated the parameter c of the Terrier's 
normalization 2 formula by running some experiments on 
the training data, to get the best values for c depending on 
the topic fields used. We obtained the following values: 
c=0.75 for queries using the Title only, c=1 for queries 
using the Title and Description fields, and c=1 for queries 
using the Title, Description, and Narrative fields. We select 
the c value that has a best MAP score according to the 
training data. 
For translating the queries from French and Spanish 
into English, several free online machine translation tools 
were used. The idea behind using multiple translations is 
that they might provide more variety of words and 
phrases, therefore improving the retrieval performance. 
Seven online MT systems (Inkpen, Alzghool, &Islam, 
2006) were used for translating from Spanish and from 
French into English. We combined the outputs of the MT 
systems by simply concatenating all the translations. All 
seven translations of a title made the title of the translated 
query; the same was done for the description and narrative 
fields.  
We propose two methods for combining IR models. We 
use the sum of normalized weighted similarity scores of 15 
different IR schemes as shown in the following formulas: 
 
 ?
?
?+=
schemsIRi
iMAPr NormSimiWiWFusion )]()([1
34      (7) 
?
?
?=
schemsIRi
iMAPr NormSimiWiWFusion )(*)(2
34      (8)                         
where Wr(i) and WMAP(i) are experimentally determined 
weights based on the recall (the number of relevant 
documents retrieved) and precision (MAP score) values for 
each IR scheme computed on the training data. For 
example, suppose that two retrieval runs r1 and r2 give 0.3 
and 0.2 (respectively) as  MAP scores on training data; we 
normalize these scores by dividing them by the maximum 
MAP value: then WMAP(r1) is 1 and WMAP(r2) is 0.66 (then 
we compute the power 3 of these weights, so that one 
weight stays 1 and the other one decreases; we chose power 
3 for MAP score and power 4 for recall, because the MAP 
is more important than the recall). We hope that when we 
multiply the similarity values with the weights and take the 
summation over all the runs, the performance of the 
combined run will improve. NormSimi is the normalized 
similarity for each IR scheme. We did the normalization by 
dividing the similarity by the maximum similarity in the 
run. The normalization is necessary because different 
weighting schemes will generate different range of 
similarity values, so a normalization method should 
applied to each run.  Our method is differed than the work 
done by Fox and Shaw in (1994), and Lee in ( 1995); they 
combined the results by taking the summation of the 
similarity scores without giving any weight to each run. In 
our work we weight each run according to the precision 
and recall on the training data.  
3. Experimental Results 
We applied the data fusion methods described in section 2 
to 14 runs produced by SMART and one run produced by 
Terrier.  Performance results for each single run and fused 
runs are presented in Table 1, in which % change is given 
with respect to the run providing better effectiveness in 
each combination on the training data. The Manual 
English column represents the results when only the 
manual keywords and the manual summaries were used 
for indexing the documents using English topics, the 
Auto-English column represents the results when 
automatic transcripts are indexed from the documents, for 
English topics. For cross-languages experiments the 
results are represented in the columns Auto-French, and 
Auto-Spanish, when using the combined translations 
produced by the seven online MT tools, from French and 
Spanish into English. Since the result of combined 
translation for each language was better than when using 
individual translations from each MT tool on the training 
data (Inkpen, Alzghool, &Islam, 2006), we used only the 
combined translations in our experiments. 
Data fusion helps to improve the performance (MAP 
score) on the test data. The best improvement using data 
fusion (Fusion1) was on the French cross-language 
experiments with 21.7%, which is statistically significant 
while on monolingual the improvement was only 6.5% 
which is not significant. We computed these 
improvements relative to the results of the best 
single-model run, as measured on the training data. This 
supports our claim that data fusion improves the recall by 
bringing some new documents that were not retrieved by 
all the runs. On the training data, the Fusion2 method 
gives better results than Fusion1 for all cases except on 
Manual English, but on the test data Fusion1 is better than 
Fusion2. In general, the data fusion seems to help, 
because the performance on the test data in not always 
good for weighting schemes that obtain good results on 
the training data, but combining models allows the 
best-performing weighting schemes to be taken into 
consideration. 
The retrieval results for the translations from French 
were very close to the monolingual English results, 
especially on the training data, but on the test data the 
difference was significantly worse. For Spanish, the 
difference was significantly worse on the training data, 
but not on the test data.  
Experiments on manual keywords and manual 
summaries available in the test collection showed high 
improvements, the MAP score jumped from 0.0855 to 
0.2761 on the test data. 
4. Conclusion 
We experimented with two different systems: Terrier 
and SMART, with combining the various weighting 
schemes for indexing the document and query terms. We 
proposed two methods to combine different weighting 
scheme from different systems, based on weighted 
summation of normalized similarity measures; the weight 
for each scheme was based on the relative precision and 
recall on the training data. Data fusion helps to improve 
the retrieval significantly for some experiments 
(Auto-French) and for other not significantly (Manual 
English). Our result on automatic transcripts for English 
queries (the required run for the CLSR task at CLEF 
2007), obtained a MAP score of 0.0855. This result was 
significantly better than the other 4 systems that 
participated in the CLSR task at CLEF 2007(Pecina et al, 
2007). 
In future work we plan to investigate more methods of 
data fusion (to apply a normalization scheme scalable to 
unseen data), removing or correcting some of the speech 
recognition errors in the ASR content words, and to use 
speech lattices for indexing.  
5. References 
 
Alzghool, M. & Inkpen, D. (2007). Experiments for the 
cross language speech retrieval task at CLEF 2006. In 
C. Peters, (Ed.), Evaluation of multilingual and 
multi-modal information retrieval (Vol. 4730/2007, 
pp. 778-785). Springer. 
Amati, G. & Van Rijsbergen, C. J. (2002). Probabilistic 
models of information retrieval based on measuring 
the divergence from randomness (Vol. 20). ACM,  
New York. 
Buckley, C., Salton, G., & Allan, J. (1992). Automatic 
retrieval with locality information using smart. In 
Text retrieval conferenc (TREC-1) (pp. 59-72). 
Inkpen, D., Alzghool, M., & Islam, A. (2006). Using 
various indexing schemes and multiple translations in 
the CL-SR task at CLEF 2005. In C. Peters, (Ed.), 
Accessing multilingual information repositories 
(Vol. 4022/2006, pp. 760-768). Springer,  London. 
Lee, J. H. (1995). Combining multiple evidence from 
different properties of weighting schemes, 
Proceedings of the 18th annual international ACM 
SIGIR conference on Research and development in 
information retrieval. ACM, Seattle, Washington, 
United States. 
Oard, D. W., Soergel, D., Doermann, D., Huang, X., 
Murray, G. C., Wang, J., Ramabhadran, B., Franz, 
M., & Gustman, S. (2004). Building an information 
retrieval test collection for spontaneous 
conversational speech, Proceedings of the 27th 
annual international ACM SIGIR conference on 
Research and development in information retrieval. 
ACM, Sheffield, United Kingdom. 
Oard, D. W., Wang, J., Jones, G. J. F., White, R. W., 
Pecina, P., Soergel, D., Huang, X., & Shafran, I. 
(2007). Overview of the CLEF-2006 cross-language 
speech retrieval track. In C. Peters, (Ed.), Evaluation 
of multilingual and multi-modal information 
retrieval (Vol. 4730/2007, pp. 744-758). Springer,  
Heidelberg. 
Ounis, I., Amati, G., Plachouras, V., He, B., Macdonald, 
C., & Johnson, D. (2005). Terrier information 
retrieval platform In Advances in information 
retrieval (Vol. 3408/2005, pp. 517-519). Springer,  
Heidelberg. 
Pecina, P., Hoffmannov?a, P., Jones, G. J. F., Zhang, Y., 
& Oard, D. W. (2007). Overview of the CLEF-2007 
cross language speech retrieval track, Working Notes 
of the CLEF- 2007 Evaluation, . CLEF2007, 
Budapest-Hungary. 
Salton, G. & Buckley, C. (1988). Term weighting 
approaches in automatic text retrieval. Information 
Processing and Management, 24(5): 513-523. 
Shaw, J. A. & Fox, E. A. (1994). Combination of multiple 
searches. In Third text retrieval conference (trec-3) 
(pp. 105-108). National Institute of Standards and 
Technology Special Publication. 
 
 
Manual English Auto-English Auto-French Auto-Spanish Weighting 
scheme Training Test Training Test Training Test Training Test 
nnc.ntc 0.2546 0.2293 0.0888 0.0819 0.0792 0.055 0.0593 0.0614 
ntc.ntc 0.2592 0.2332 0.0892 0.0794 0.0841 0.0519 0.0663 0.0545 
lnc.ntc 0.2710 0.2363 0.0898 0.0791 0.0858 0.0576 0.0652 0.0604 
ntc.nnc 0.2344 0.2172 0.0858 0.0769 0.0745 0.0466 0.0585 0.062 
anc.ntc 0.2759 0.2343 0.0723 0.0623 0.0664 0.0376 0.0518 0.0398 
ltc.ntc 0.2639 0.2273 0.0794 0.0623 0.0754 0.0449 0.0596 0.0428 
atc.ntc 0.2606 0.2184 0.0592 0.0477 0.0525 0.0287 0.0437 0.0304 
nnn.ntn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ntn.ntn 0.2738 0.2369 0.0933 0.0795 0.0843 0.0507 0.0691 0.0578 
lnn.ntn 0.2858 0.245 0.0969 0.0799 0.0905 0.0566 0.0701 0.0589 
ntn.nnn 0.2476 0.2228 0.0900 0.0852 0.0799 0.0503 0.0599 0.061 
ann.ntn 0.2903 0.2441 0.0750 0.0670 0.0743 0.038 0.057 0.0383 
ltn.ntn 0.2870 0.2435 0.0799 0.0655 0.0871 0.0522 0.0701 0.0501 
atn.ntn 0.2843 0.2364 0.0620 0.0546 0.0722 0.0347 0.0586 0.0355 
In_expC2 0.3177 0.2737 0.0885 0.0744 0.0908 0.0487 0.0747 0.0614 
Fusion 1 0.3208 0.2761 0.0969 0.0855 0.0912 0.0622 0.0731 0.0682 
% change 1.0% 0.9% 0.0% 6.5% 0.4% 21.7% -2.2% 10.0% 
Fusion 2 0.3182 0.2741 0.0975 0.0842 0.0942 0.0602 0.0752 0.0619 
% change 0.2% 0.1% 0.6% 5.1% 3.6% 19.1% 0.7% 0.8% 
Table 1. Results (MAP scores) for 15 weighting schemes using Smart and Terrier (the In_expC2 model), and the results 
for the two Fusions Methods. In bold are the best scores for the 15 single runs on the training data and the corresponding 
results on the test data.  
 
Weighting 
scheme 
Manual English Auto-English Auto- French Auto- Spanish 
 Train. Test Train. Test Train. Test Train. Test 
nnc. ntc 2371 1827 1726 1306 1687 1122 1562 1178 
ntc.ntc 2402 1857 1675 1278 1589 1074 1466 1155 
lnc.ntc 2402 1840 1649 1301 1628 1111 1532 1196 
ntc.nnc 2354 1810 1709 1287 1662 1121 1564 1182 
anc.ntc 2405 1858 1567 1192 1482 1036 1360 1074 
ltc.ntc 2401 1864 1571 1211 1455 1046 1384 1097 
atc.ntc 2387 1858 1435 1081 1361 945 1255 1011 
nnn.ntn 2370 1823 1740 1321 1748 1158 1643 1190 
ntn.ntn 2432 1863 1709 1314 1627 1093 1502 1174 
lnn.ntn 2414 1846 1681 1325 1652 1130 1546 1194 
ntn.nnn 2370 1823 1740 1321 1748 1158 1643 1190 
ann.ntn 2427 1859 1577 1198 1473 1027 1365 1060 
ltn.ntn 2433 1876 1582 1215 1478 1070 1408 1134 
atn.ntn 2442 1859 1455 1101 1390 975 1297 1037 
In_expC2 2638 1823 1624 1286 1676 1061 1631 1172 
Fusion 1 2645 1832 1745 1334 1759 1147 1645 1219 
% change 0.3% 0.5 % 0.3% 1.0% 0.6% -1.0% 0.1% 2.4% 
Fusion 2 2647 1823 1727 1337 1736 1098 1631 1172 
% change 0.3% 0.0% 0.8% 1.2% -0.7% -5.5% -0.7% -1.5% 
Table 2. Results (number of relevant documents retrieved) for 15 weighting schemes using Terrier and SMART, and the 
results for the Fusions Methods. In bold are the best scores for the 15 single runs on training data and the corresponding 
test data. 
BioNLP 2007: Biological, translational, and clinical language processing, pages 65?72,
Prague, June 2007. c?2007 Association for Computational Linguistics
Recognising Nested Named Entities in Biomedical Text
Beatrice Alex, Barry Haddow and Claire Grover
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, EH8 9LW, UK
{balex,bhaddow,grover}@inf.ed.ac.uk
Abstract
Although recent named entity (NE) annotation ef-
forts involve the markup of nested entities, there has
been limited focus on recognising such nested struc-
tures. This paper introduces and compares three
techniques for modelling and recognising nested
entities by means of a conventional sequence tag-
ger. The methods are tested and evaluated on two
biomedical data sets that contain entity nesting. All
methods yield an improvement over the baseline tag-
ger that is only trained on flat annotation.
1 Introduction
Traditionally, named entity recognition (NER) has
focussed on entities which are continuous, non-
nested and non-overlapping. In other words, each
token in the text belongs to at most one entity, and
NEs consist of a continuous sequence of tokens.
However, in some situations, it may make sense to
relax these restrictions, for example by allowing en-
tities to be nested inside other entities, or allowing
discontinuous entities. GENIA (Ohta et al, 2002)
and BioInfer (Pyysalo et al, 2007) are examples of
recently produced NE-annotated biomedical corpora
where entities nest. Corpora in other domains, for
example the ACE1 data, also contain nested entities.
This paper compares techniques for recognising
nested entities in biomedical text. The difficulty of
this task is that the standard method for convert-
ing NER to a sequence tagging problem with BIO-
encoding (Ramshaw and Marcus, 1995), where each
1http://www.nist.gov/speech/tests/ace/
index.htm
token is assigned a tag to indicate whether it is at the
beginning (B), inside (I), or outside (O) of an en-
tity, is not directly applicable when tokens belong to
more than one entity. Here we explore methods of
reducing the nested NER problem to one or more BIO
problems so that existing NER tools can be used.
This paper is organised as follows. In Section 2,
the problem of nested entities is introduced and mo-
tivated with examples from GENIA and our EPPI
(enriched protein-protein interaction) data. Related
work is reviewed in Section 3. The proposed tech-
niques enabling NER for nested NEs are explained in
Section 4. Section 5 details the experimental setup,
including descriptive statistics of the corpora and
specifics of the classifier. The results of comparing
different tagging methods are analysed in Section 6,
with a discussion and conclusion in Section 7.
2 Nested Entities
The majority of previous work on NER is conducted
using data sets annotated either with continuous,
non-nested and non-overlapping NEs or an annota-
tion scheme reduced to a flat annotation of a similar
kind in order to simplify the recognition task. How-
ever, annotated corpora often contain entities that are
nested or discontinuous. For example, the GENIA
corpus contains nested entities such as:
<RNA><DNA>CIITA</DNA>mRNA</RNA>
where the string ?CIITA? denotes a DNA and the en-
tire string ?CIITA mRNA? refers to an RNA. Such
nesting complicates the task of traditional NER sys-
tems, which generally rely on data represented with
the BIO encoding or other flat annotation variations
thereof. The majority of NER studies on corpora
65
GENIA EPPI
Count Nesting Count Nesting
3,614 ( other name ( protein t ) t ) 1,698 ( fusion ( protein t ) t ( protein t ) )
907 ( DNA ( protein t ) t ) 1,269 ( drug/compound ( protein t ) )
856 ( protein ( protein t ) t ) 455 ( fusion ( fragment t ) t ( protein t ) )
661 ( protein t ( protein t ) ) 412 ( protein ( protein t ) t )
546 ( other name ( DNA t ) t ) 361 ( complex ( protein t ) t ( protein t ) )
541 ( other name t ( other name t ) ) 298 ( fusion ( protein t ) t ( fragment t ) )
470 ( cell type t ( cell type t ) ) 246 ( fragment t ( fragment t ) )
351 ( DNA t ( DNA t ) ) 241 ( cell line t ( cell line t ) )
326 ( other name ( virus t ) t ) 207 ( fragment ( protein t ) )
262 ( other name ( lipid t ) t ) 201 ( fusion ( protein t ) t ( mutant t ) )
Table 1: 10 most frequent types of nesting in the GENIA corpus and the combined TRAIN and DE-
VTEST sections of the EPPI data (see Section 5.1), where t represents the text.
containing nested structures focus on recognising
the outermost (non-embedded) entities (e.g. Kim et
al. 2004) , as they contain the most information,
including that of embedded entities (Zhang et al,
2004). This enables a simplification of the recog-
nition task to a sequential analysis problem.
Our aim is to recognise all levels of NE nesting
occurring in two biomedical corpora: the GENIA
corpus (Version 3.02) and the EPPI corpus (see Sec-
tion 5.1). The latter data set was collected and an-
notated as part of the TXM project. Its annotation
contains 9 different biomedical entities. While the
GENIA corpus contains nested entities up to a level
of four layers of embedding, the nested entities in
the EPPI corpus only have three layers. Table 1 lists
the ten most frequent types of entity nesting occur-
ring in both corpora. In the remainder of the paper,
we differentiate between:
embedded NEs: contained in other NEs
non-embedded NEs: not contained in other NEs
containing NEs: containing other NEs
non-containing NEs: not containing other NEs
The GENIA corpus is made up of a larger per-
centage of both embedded entity (18.61%) and con-
taining entity (16.95%) mentions than the EPPI data
(12.02% and 8.27%, respectively). In both corpora,
nesting can occur in three different ways:
1. Entities containing one or more shorter embed-
ded entities. Such nesting is very frequent in both
data sets. For example, the DNA ?IL-2 promoter? in
the GENIA corpus contains the protein ?IL-2?. In
the EPPI corpus, fusions and complexes often con-
tain nested proteins, e.g. the complex ?CBP/p300?,
where ?CBP? and ?p300? are marked as proteins.
2. Entities with more than one entity type. Al-
though they occur in both data sets, they are very
rare in the GENIA corpus. For example, the string
?p21ras? is annotated both as DNA and protein. In
the EPPI data, proteins can also be annotated as
drug/compound, where it can be clearly established
that the protein is used as a drug to affect the func-
tion of an organism, cell, or biological process.
3. Coordinated entities. Coordinated NEs account
for approximately 2% of all NEs in the GENIA and
EPPI data. In the original corpora they are anno-
tated differently, but for this work they are all con-
verted to a common format.2 The outermost anno-
tation of coordinated structures and any continuous
entity mark-up within them is retained. For exam-
ple, in ?human interleukin-2 and -4? both the con-
tinuous embedded entity ?human interleukin-2? and
the entire string are marked as proteins. The markup
for discontinuous embedded entities, like ?human
interleukin-4? in the previous example, is not re-
tained, as they could be derived in a post-processing
step once nested entities are recognised.
3 Related Work
In previous work addressing nested entities, Shen et
al. (2003), Zhang et al (2004), Zhou et al (2004),
Zhou (2006), and Gu (2006) considered the GENIA
2Both corpora are represented in XML with standoff anno-
tation, potentionally allowing overlapping NEs.
66
corpus, where nested entities are relatively frequent.
All these studies ignore embedded entities occur-
ring in coordinated structures and only retain their
outermost annotation. Shen et al (2003), Zhang et
al. (2004), and Zhou et al (2004) all report on a rule-
based approach to dealing with nested NEs in the
GENIA corpus (Version 3.0) in combination with a
Hidden Markov Model (HMM) that first recognises
innermost NEs. They use four basic hand-crafted
patterns and a combination thereof to generate nest-
ing rules from the training data and thereby derive
NEs containing the innermost NEs. The experimen-
tal setup of these studies differs slightly. While Shen
et al (2003) and Zhang et al (2004) report results
testing on 4% of the abstracts in the GENIA corpus,
Zhou et al (2004) report 10-fold cross-validation
scores. Zhou (2006) applies the same rule-based
method for dealing with nested entities to the out-
put of a mutual information independence model
(MIIM) combined with a support vector machine
(SVM) plus sigmoid. His results are based on 5-fold
cross-validation on the GENIA corpus (Version 3.0).
In each of the studies, the rule-based approach to
nested entities results in an improvement of between
3.0 and 3.5 points in F1 over the baseline model.
However, as explicitly stated by Shen et al (2003)
and Zhang et al (2004), this evaluation is limited to
non-embedded (i.e. top-level and non-nested) enti-
ties. The highest overall F1-score reported for all
entities in the GENIA corpus is 71.2 (Zhou, 2006),
which again only appears to reflect the performance
on non-embedded entities.
Zhang et al (2004) also compare the rule-based
method with HMM-based cascaded recognition that
extends iteratively from the shortest to the longest
entities. Their basic HMM model is combined with
HMM models trained on transformed cascaded an-
notations. During training, embedded entity terms
are replaced by their entity type as a way of unnest-
ing the data. During testing, subsequent iterations
rely on the tagging of the first recognition pass and
are repeated until no more entities are recognised.
However, this method only results in an improve-
ment of 1.2 points in F1 over their basic classifier.
Gu (2006) reports results on recognising nested
entities in the GENIA corpus (Version 3.02) when
training an SVM-light binary classifier to recognise
either proteins or DNA. Training with the outermost
labelling yields better performance on recognising
outermost entities and, conversely, using the inner
labelling results in highest scores for recognising in-
ner entities. The best exact match F1-scores of 73.0
and 47.5 for proteins and DNA, respectively, are ob-
tained when training on data with inner labelling and
evaluating on the inner entities.
McDonald et al (2005) propose structured multil-
abel classification as opposed to sequential labelling
for dealing with nested, discontinuous, and overlap-
ping NEs. This approach uses a novel text segment
representation in preference to the BIO-encoding.
Their corpus contains MEDLINE abstracts on the
inhibition of the enzyme CYP450 (Kulick et al,
2004), specifically those abstracts that contain at
least one overlapping and one discontinuous anno-
tation. While this data does not contain nested NEs,
discontinuous and overlapping NEs make up 6% of
all NEs. The classifier performs competitively with
sequential tagging models on continuous and non-
overlapping entities for NER and noun phrase chunk-
ing. On discontinuous and overlapping NEs in the
biomedical data alone, its best performance is 56.25
F1. As the corpus does not contain nested NEs, it
would be of interest to investigate the algorithm?s
performance on the GENIA corpus.
4 Modelling Techniques
As large amounts of time and effort have been de-
voted to work on non-nested NER using the BIO-
encoding approach, it would be useful if this work
could be easily applied to nested NER. In this paper,
three different ways of addressing nested NER will
be compared: layering, cascading, and joined la-
bel tagging. All techniques aim to reduce the nested
NER problem to one or more BIO problems, so that
existing NER tools can be used. Table 2 shows an ex-
ample representation for each modelling technique
of the following two non-nested and nested entity
annotations found in a GENIA abstract:
<multi cell>mice</multi cell> . . .
<other name><RNA><protein>tumor
necrosis factor-alpha</protein>
(<protein>TNF- alpha</protein>)
messenger RNA</RNA> levels</other name>
In layering, each level of nesting is modelled as a
separate BIO problem. The output of models trained
on individual layers is combined subsequent to tag-
ging by taking the union. Layers can be created
67
Token Inside-out layering Outside-in layering
Model Layer 1 Layer 2 Layer 3 Layer 3 Layer 2 Layer 1
mice B-multi cell O O B-multi cell O O
. . . . . . . . . . . . . . . . . . . . .
tumor B-protein B-RNA B-other name B-other name B-RNA B-protein
necrosis I-protein I-RNA I-other name I-other name I-RNA I-protein
factor-alpha I-protein I-RNA I-other name I-other name I-RNA I-protein
( O I-RNA I-other name I-other name I-RNA O
TNF-alpha B-protein I-RNA I-other name I-other name I-RNA B-protein
) O I-RNA I-other name I-other name I-RNA O
messenger O I-RNA I-other name I-other name I-RNA O
RNA O I-RNA I-other name I-other name I-RNA O
levels O O I-other name I-other name O O
Cascading Joined label tagging
Model All entity types other RNA Joined labels
mice B-multi cell O O B-multi cell+O+O
. . . . . . . . . . . . . . .
tumor B-protein B-other name B-RNA B-protein+B-RNA+B-other name
necrosis I-protein I-other name I-RNA I-protein+I-RNA+I-other name
factor-alpha I-protein I-other name I-RNA I-protein+I-RNA+I-other name
( O I-other name I-RNA O+I-RNA+I-other name
TNF-alpha B-protein I-other name I-RNA B-protein+I-RNA+I-other name
) O I-other name I-RNA O+I-RNA+I-other name
messenger O I-other name I-RNA O+I-RNA+I-other name
RNA O I-other name I-RNA O+I-RNA+I-other name
levels O I-other name O O+O+I-other name
Table 2: Example representation of nested entities for various modelling techniques.
inside-out or outside-in. For inside-out layering, the
first layer is made up of all non-containing entities,
the second layer is composed of all those entities
which only contain one layer of nesting, etc. Con-
versely, outside-in layering means that the first layer
contains all non-embedded entities, the second layer
contains all entities which are only contained within
one outer entity, etc. Both directions of layering can
be modelled using a conventional NE tagger.
Cascading reduces the nested NER task to sev-
eral BIO problems by grouping one or more entity
types and training a separate model for each group.
Again, the output from individual models is com-
bined during tagging. Subsequent models in the cas-
cade may have access to the guesses of previous
ones by means of a GUESS feature. The cascaded
method is unable to recognise entities containing en-
tities of the same type, which may be a drawback for
some data sets. Cascading also raises the issue of
how to group entity types. This is dependent on the
types of entities that nest within a given data set and
would potentially require large amounts of experi-
mentation to determine the best combination. More-
over, training a model for each entity type lengthens
training time considerably, and may degrade perfor-
mance due to the dominance of the O tags for infre-
quent categories. It is possible, however, to create a
cascaded tagger combining one model trained on all
entity types with models trained on entity types that
frequently contain other entities.
Finally, joined label tagging entails creating one
tagging problem for all entities by concatenating the
BIO tags of all levels of nesting. A conventional
named entity recogniser is then trained on the data
containing the joined labels. Once the classifier has
assigned the joined labels during tagging, they are
decoded into their original BIO format for each in-
dividual entity type. Compared to the other tech-
niques, joined label tagging involves a much larger
tag set, which can increase dramatically with the
number of entity types occurring in a data set. This
can result in data sparsity which may have a detri-
mental effect on performance.
5 Experimental Setup
5.1 Corpora
GENIA (V3.02), a large publicly available biomedi-
cal corpus annotated with biomedical NEs, is widely
used in the text mining community (Cohen et al,
2005). This data set consists of 2,000 MEDLINE ab-
stracts in the domain of molecular biology (?0.5m
tokens). The annotations used for the experiments
68
reported here are based on the GENIA ontology,
published in Ohta et al (2002). It contains the fol-
lowing classes: amino acid monomer, atom, body
part, carbohydrate, cell component, cell line, cell
type, DNA, inorganic, lipid, mono-cell, multi-cell,
nucleotide, other name, other artificial source, other
organic compound, peptide, polynucleotide, protein,
RNA, tissue, and virus. In this work, protein, DNA
and RNA sub-types are collapsed to their super-type,
as done in previous studies (e.g. Zhou 2006). To the
best of our knowledge, no inter-annotator agreement
(IAA) figures on the NE-annotation in the GENIA
corpus are reported in the literature.
The EPPI corpus consists of 217 full-text papers
selected from PubMed and PubMedCentral as con-
taining protein-protein interactions (PPIs). The pa-
pers were either retrieved in XML or HTML, depend-
ing on availability, and converted to an internal XML
format. Domain experts annotated all documents
for NEs and PPIs, as well as extra (enriched) in-
formation associated with PPIs and normalisations
of entities to publicly available ontologies. The en-
tity annotations are the focus of the current work.
The types of entities annotated in this data set are:
complex, cell line, drug/compound, experimental
method, fusion, fragment, modification, mutant, and
protein. Out of the 217 papers, 125 were singly
annotated, 65 were doubly annotated, and 27 were
triply annotated. The IAA, measured by taking the
F1 score of one annotator with respect to another
when the same paper is annotated by two different
annotators, ranges from 60.40 for the entity type
mutant to 91.59 for protein, with an overall micro-
averaged F1-score of 84.87. The EPPI corpus (?2m
tokens) is divided into three sections, TRAIN (66%),
DEVTEST (17%), and TEST (17%), with TEST only
to be used for final evaluation, and not to be con-
sulted by the researchers in the development and fea-
ture optimisation phrase. The experiments described
here involve the EPPI TRAIN and DEVTEST sets.
5.2 Pre-processing
All documents are passed through a sequence of pre-
processing steps implemented using the LT-XML2
and LT-TTT2 tools (Grover et al, 2006) with the out-
put of each step encoded in XML mark-up. Tokeni-
sation and sentence splitting is followed by part-of-
speech tagging with the Maximum Entropy Markov
Model (MEMM) tagger developed by Curran and
Clark (2003) (hereafter referred to as C&C ) for
the CoNLL-2003 shared task (Tjong Kim Sang and
De Meulder, 2003), trained on the MedPost data
(Smith et al, 2004). Information on lemmatisa-
tion, as well as abbreviations and their long forms, is
added using the morpha lemmatiser (Minnen et al,
2000) and the ExtractAbbrev script of Schwartz and
Hearst (2003), respectively. A lookup step uses on-
tological information to identify scientific and com-
mon English names of species. Finally, a rule-based
chunker marks up noun and verb groups and their
heads (Grover and Tobin, 2006).
5.3 Named Entity Tagging
The C&C tagger, referred to earlier, forms the basis
of the NER component of the TXM natural language
processing (NLP) pipeline designed to detect entity
relations and normalisations (Grover et al, 2007).
The tagger, in common with many ML approaches
to NER, reduces the entity recognition problem to
a sequence tagging problem by using the BIO en-
coding of entities. As well as performing well on
the CoNLL-2003 task, Maximum Entropy Markov
Models have also been successful on biomedical
NER tasks (Finkel et al, 2005). As the vanilla C&C
tagger (Curran and Clark, 2003) is optimised for
performance on newswire text, various modifica-
tions were applied to improve its performance for
biomedical NER. Table 3 lists the extra features
specifically designed for biomedical text. The C&C
tagger was also extended using several gazetteers,
including a protein, complex, experimental method
and modification gazetteer, targeted at recognising
entities occurring in the EPPI data. Further post-
processing specific to the EPPI data involves correct-
ing boundaries of some hyphenated proteins and fil-
tering out entities ending in punctuation.
All experiments with the C&C tagger involve 5-
fold cross-validation on all 2,000 GENIA abstracts
and the combined EPPI TRAIN and DEVTEST sets.
Cross-validation is carried out at the document level.
For simple tagging, the C&C tagger is trained on
the non-containing entities (innermost) or on the
non-embedded entities (outermost). For inside-out
and outside-in layering, a separate C&C model is
trained for each layer of entities in the data, i.e. four
models for the GENIA data and three models for
the EPPI data. Cascading is performed on individual
entities with different orderings, either ordering en-
69
Feature Description
CHARACTER Regular expressions match-
ing typical protein names
WORDSHAPE Extended version of the C&C
WORDTYPE feature
HEADWORD Head word of the current
noun phrase
ABBREVIATION Term identified as an abbre-
viation of a gazetteer term
within a document
TITLE Term seen in a noun phrase in
the document title
WORDCOUNTER Non-stop word that is among
the 10 most frequent ones in
a document
VERB Verb lemma information
added to each noun phrase
token in the sentence
FONT Text in italic and subscript
contained in the original doc-
ument format
Table 3: Extra features added to C&C .
tity models according to performance or entity fre-
quency in the training data, ranging from highest to
lowest. Cascading is also carried out on groups of
entities (e.g. one model for all entities, one for a
specific entity type, and combinations). Subsequent
models in the cascade have access to the guesses of
previous ones via a GUESS feature. Finally, joined
label tagging is done by concatenating individual
BIO tags from the innermost to the outermost layer.
As in the GENIA corpus, the most frequently an-
notated entity type in the EPPI data is protein with al-
most 55% of all annotations in the combined TRAIN
and DEVTEST data (see Table 5). Given that the
scores reported in this paper are calculated as F1
micro-averages over all categories, they are strongly
influenced by the classifier?s performance on pro-
teins. However, scoring is not limited to a particular
layer of entities (e.g. only outermost layer), but in-
cludes all levels of nesting. During scoring, a correct
match is achieved when exactly the same sequence
of text (encoded in start/end offsets) is marked with
the same entity type in the gold standard and the sys-
tem output. Precision, recall and F1 are calculated
in standard fashion from the number of true positive,
false positive and false negative NEs recognised.
6 Results
Table 4 lists overall cross-validation F1-scores cal-
culated for all NEs at all levels of nesting when ap-
plying the various modelling techniques. For GE-
NIA, cascading on individual entities when order-
ing entity models by performance yields the high-
est F1-score of 67.88. Using this method yields
an increase of 3.26 F1 over the best simple tag-
ging method, which scores 64.62 F1. Joined label
tagging results in the second best overall F1-score
of 67.82. Both layering (inside-out) and cascading
(combining a model trained on all NEs with 4 mod-
els trained on other name, DNA, protein, or RNA)
also perform competitively, reaching F1-scores of
67.62 and 67.56, respectively. In the experiments
with the EPPI corpus, cascading is also the winner
with an F1-score of 70.50 when combining a model
trained on all NEswith one trained on fusions. This
method only results in a small, yet statistically sig-
nificant (?2, p ? 0.05), increase in F1 of 0.43 over
the best simple tagging algorithm. This could be due
to the smaller number of nested NEs in the EPPI data
and the fact that this data contains many NEs with
more than one category. Layering (inside-out) per-
forms almost as well as cascading (F1=70.44).
The difference in the overall performance be-
tween the GENIA and the EPPI corpus is partially
due to the difference in the number of NEs which
C&C is required to recognise, but also due to the
fact that all features used are optimised for the EPPI
data and simply applied to the GENIA corpus. The
only feature not used for the experiments with the
GENIA corpus is FONT, as this information is not
preserved in the original XML of that corpus.
7 Discussion and Conclusion
According to the results for the modelling tech-
niques, each proposed method outperforms simple
tagging. Cascading yields the best result on the GE-
NIA (F1=67.88) and EPPI data (F1=70.50), see Ta-
ble 5 for individual entity scores. However, it in-
volves extensive amounts of experimentation to de-
termine the best model combination. The best setup
for cascading is clearly data set dependent. With
larger numbers of entity types annotated in a given
corpus, it becomes increasingly impractical to ex-
haustively test all possible orders and combinations.
Moreover, training and tagging times are lengthened
as more models are combined in the cascade.
70
GENIA V3.02 EPPI
Technique F1 Technique F1
Simple Tagging
Training on innermost entities 64.62 Training on innermost entities 70.07
Training on outermost entities 62.72 Training on outermost entities 69.18
Layering
Inside-out 67.62 Inside-out 70.44
Outside-in 67.02 Outside-in 70.21
Cascading
Individual NE models (by performance) 67.88 Individual NE models (by performance) 70.42
Individual NE models (by frequency) 67.72 Individual NE models (by frequency) 70.43
All-cell type 64.55 All-complex 70.03
All-DNA 65.02 All-drug/compound 70.08
All-other name 66.99 All-fusion 70.50
All-protein 64.77 All-protein 70.02
All-RNA 64.80 All-complex-fusion 70.46
All-other name-DNA-protein-RNA 67.56 All-drug/compound-fusion 70.50
Joined label tagging
Inside-out 67.82 Inside-out 70.37
Table 4: Cross-validation F1-scores for different modelling techniques on the GENIA and EPPI data. Scores
in italics mark statistically significant improvements (?2, p ? 0.05) over the best simple tagging score.
Despite the large number of tags involved in us-
ing joined label tagging, this method outperforms
simple tagging for both data sets and even results in
the second-best overall F1-score of 67.72 obtained
for the GENIA corpus. The fact that joined label
tagging only requires training and tagging with one
model makes this approach a viable alternative to
cascading which is far more time-consuming to run.
Inside-out layering performs competitively both
for the GENIA corpus (F1=67.62) and the EPPI cor-
pus (F1=70.37), considering how little time is in-
volved in setting up such experiments. As with
joined label tagging, minimal optimisation is re-
quired when using this method. One disadvantage
(as compared to simple, and to some extent joined
label tagging) is that training and tagging times in-
crease with the number of layers that are modelled.
In conclusion, this paper introduced and tested
three different modelling techniques for recognising
nested NEs, namely layering, cascading, and joined
label tagging. As each of them reduces nested NER
to one or more BIO-encoding problems, a conven-
tional sequence tagger can be used. It was shown
that each modelling technique outperfoms the sim-
ple tagging method for both biomedical data sets.
Future work will involve testing the proposed
techniques on other data sets containing entity nest-
ing, including the ACE data. We will also determine
their merit when applying a different learning algo-
rithm. Furthermore, possible solutions for recognis-
ing discontinuous entities will be investigated.
8 Acknowledgements
The authors are very grateful to the annotation
team, and to Cognia (http://www.cognia.
com) for their collaboration on the TXM project.
This work is supported by the Text Mining Pro-
gramme of ITI Life Sciences Scotland (http://
www.itilifesciences.com).
References
K. Bretonnel Cohen, Lynne Fox, Philip V. Ogren, and Lawrence
Hunter. 2005. Corpus design for biomedical natural
language processing. In Proceedings of the ACL-ISMB
workshop on linking biological literature, ontologies and
databases: mining biological semantics, pages 38?45.
James R. Curran and Stephen Clark. 2003. Language indepen-
dent NER using a maximum entropy tagger. In Proceedings
of CoNLL-2003, pages 164?167.
71
GENIA V3.02 EPPI
Entity type Count P R F1 Entity type Count P R F1
All 94,014 69.3 66.5 67.9 All 134,059 73.1 68.1 70.5
protein 34,813 75.1 74.9 75.0 protein 73,117 76.2 82.1 79.0
other name 20,914 60.0 67.2 63.4 expt. method 12,550 74.3 72.4 73.3
DNA 10,589 64.2 57.5 60.6 fragment 11,571 54.5 41.7 47.3
cell type 7,408 71.2 69.2 70.2 drug/compound 10,236 64.9 37.7 47.7
other org. compound 4,109 76.6 57.8 65.9 cell line 6,505 68.3 53.4 59.9
cell line 4,081 66.3 53.8 59.4 complex 6,454 62.5 32.2 42.5
lipid 2,359 76.9 65.6 70.8 modification 5,727 95.4 94.2 94.8
virus 2,133 76.0 73.4 74.7 mutant 4,025 40.7 23.2 29.6
multi-cell 1,784 72.5 60.1 65.7 fusion 3,874 56.6 36.0 44.0
Table 5: Individual counts and scores of the most frequent GENIA and all EPPI entity types for the best-
performing method: cascading.
Jenny Rose Finkel, Shipra Dingare, Christopher D. Manning,
Malvina Nissim, Beatrice Alex, and Claire Grover. 2005.
Exploring the boundaries: Gene and protein identification in
biomedical text. BMC Bioinformatics, 6(Suppl1):S5.
Claire Grover and Richard Tobin. 2006. Rule-based chunking
and reusability. In Proceedings of LREC 2006, pages 873?
878.
Claire Grover, Michael Matthews, and Richard Tobin. 2006.
Tools to address the interdependence between tokenisation
and standoff annotation. In Proceedings of NLPXML 2006,
pages 19?26.
Claire Grover, Barry Haddow, Ewan Klein, Michael Matthews,
Leif Arda Nielsen, Richard Tobin, and Xinglong Wang.
2007. Adapting a relation extraction pipeline for the BioCre-
AtIvE II task. In Proceedings of the BioCreAtIvE Workshop
2007, Madrid, Spain.
Baohua Gu. 2006. Recognizing nested named entities in GE-
NIA corpus. In Proceedings of the BioNLP Worshop, HLT-
NAACL 2006, pages 112?113.
Jin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka, Yuka
Tateisi, and Nigel Collier. 2004. Introduction to the bio-
entity recognition task at JNLPBA. In Proceedings of
JNLPBA 2004, pages 70?75.
Seth Kulick, Ann Bies, Mark Liberman, Mark Mandel, Ryan
McDonald, Martha Palmer, Andrew Schein, and Lyle Un-
gar. 2004. Integrated annotation for biomedical information
extraction. In Proceedings of BioLINK 2004, pages 61?68.
Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005.
Flexible text segmentation with structured multilabel classi-
fication. In Proceedings of HLT/EMNLP 2005, pages 987?
994.
Guido Minnen, John Carroll, and Darren Pearce. 2000. Robust,
applied morphological generation. In Proceedings of INLG
2000, pages 201?208.
Tomoko Ohta, Yuka Tateisi, Hideki Mima, and Jun?ichi Tsujii.
2002. GENIA corpus: an annotated research abstract corpus
in molecular biology domain. In Proceedings of HLT 2002,
pages 73?77.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari Bjo?rne,
Jorma Boberg, Jouni Ja?rvinen, and Tapio Salakoski. 2007.
BioInfer: a corpus for information extraction in the biomed-
ical domain. BMC Bioinformatics, 8(50).
Lance Ramshaw and Mitch Marcus. 1995. Text chunking us-
ing transformation-based learning. In Proceedings of the 3rd
Workshop on Very Large Corpora (ACL 1995), pages 82?94.
Ariel S. Schwartz and Marti A. Hearst. 2003. A simple algo-
rithm for identifying abbreviation definitions in biomedical
text. In Pacific Symposium on Biocomputing, pages 451?
462.
Dan Shen, Jie Zhang, Guodong Zhou, Jian Su, and Chew-Lim
Tan. 2003. Effective adaptation of a hidden markov model-
based named entity recognizer for biomedical domain. In
Proceedings of the BioNLP Workshop, ACL 2003, pages 49?
56.
Larry Smith, Tom Rindflesch, and W. John Wilbur. 2004. Med-
Post: a part-of-speech tagger for biomedical text. Bioinfor-
matics, 20(14):2320?2321.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduc-
tion to the CoNLL-2003 shared task: Language-independent
named entity recognition. In Proceedings of CoNLL-2003,
pages 142?147.
Jie Zhang, Dan Shen, Guodong Zhou, Jian Su, and Chew-Lim
Tan. 2004. Enhancing HMM-based biomedical named en-
tity recognition by studying special phenomena. Journal of
Biomedical Informatics, 37(6):411?422.
Guodong Zhou, Jie Zhang, Jian Su, Dan Shen, and ChewLim
Tan. 2004. Recognizing names in biomedical texts: a ma-
chine learning approach. Bioinformatics, 20(7):1178?1190.
Guodong Zhou. 2006. Recognizing names in biomedical
texts using mutual information independence model and svm
plus sigmoid. International Journal of Medical Informatics,
75:456?467.
72
BioNLP 2007: Biological, translational, and clinical language processing, pages 145?152,
Prague, June 2007. c?2007 Association for Computational Linguistics
The Extraction of Enriched Protein-Protein Interactions from Biomedical
Text
Barry Haddow and Michael Matthews
School of Informatics
University of Edinburgh
2 Buccleuch Place
Edinburgh, Scotland, EH8 9LW
{bhaddow,mmatsews}@inf.ed.ac.uk
Abstract
There has been much recent interest in the
extraction of PPIs (protein-protein interac-
tions) from biomedical texts, but in order
to assist with curation efforts, the PPIs must
be enriched with further information of bi-
ological interest. This paper describes the
implementation of a system to extract and
enrich PPIs, developed and tested using an
annotated corpus of biomedical texts, and
employing both machine-learning and rule-
based techniques.
1 Introduction
The huge volume of literature generated in the
biomedical field is such that researchers are unable
to read all the papers that interest them. Instead they
must rely on curated databases, containing informa-
tion extracted from the literature about, for example,
which proteins interact.
These curated databases are expensive to produce
as they rely on qualified biologists to select the pa-
pers, read them to extract the relevant information,
enter this information into the database, and cross-
check the information for quality control, a proce-
dure which can be very time-consuming. If NLP
techniques could be used to aid curators in their task
then the costs of producing curated databases could
be substantially reduced.
In the context of biomedical information extrac-
tion, there has been much recent interest in the
automated extraction of PPIs (protein-protein in-
teractions) from biomedical literature. The recent
BioCreAtIvE Challenge highlights the desire to uti-
lize these extraction techniques to automatically or
semi-automatically populate curated PPI databases.
However, just identifying the interactions is not nec-
essarily sufficient, as curators typically require ad-
ditional information about the interactions, such as
the experimental method used to detect the interac-
tion, and the names of any drugs used to influence
the behaviour of the proteins. Furthermore, curators
may only be interested in interactions which are ex-
perimentally proven within the paper, or where the
proteins physically touch during the interaction.
This paper describes the implementation of a
system designed to extract mentions of PPIs from
biomedical text, and to enrich those PPIs with ad-
ditional information of biological interest. The en-
riched information consists of properties (name-
value pairs associated with a PPI, for example a di-
rectness property could indicate whether the inter-
action is direct or not direct) and attributes (rela-
tions between the PPI relation or its participating
entities and other entities, such as the experimental
method used to detect the PPI). This system for ex-
tracting and enriching PPIs was developed as part of
the TXM programme, which aims to develop tools to
help with the curation of biomedical papers.
After reviewing related work in the following sec-
tion, a detailed description of how the annotated cor-
pus was created and its descriptive statistics is pro-
vided in section 3. The methods used to extract the
properties and attributes are explained in section 4,
and then evaluated and discussed in section 5. Some
conclusions and suggestions for further work are of-
fered in section 6.
145
2 Related Work
There has been much recent interest in extracting
PPIs from abstracts and full text papers (Bunescu
and Mooney, 2006; Giuliano et al, 2006; Plake et
al., 2005; Blaschke and Valencia, 2002; Donaldson
et al, 2003). In these systems however, the focus has
been on extracting just the PPIs without attempts to
enrich the PPIs with further information. Enriched
PPIs can be seen as a type of biological event ex-
traction (Alphonse et al, 2004; Wattarujeekrit et al,
2004), a technique for mapping entities found in text
to roles in predefined templates which was made
popular in the MUC tasks (Marsh and Perzanowski,
1998). There has also been work to enrich sentences
with semantic categories (Shah and Bork, 2006) and
qualitative dimensions such as polarity (Wilbur et
al., 2006).
Using NLP to aid in curation was addressed in
the KDD 2002 Cup (Yeh et al, 2002), where par-
ticipants attempted to extract records curatable with
respect to the FlyBase database, and has been further
studied by many groups (Xu et al, 2006; Karamanis
et al, 2007; Ursing et al, 2001).
The Protein-Protein Interaction task of the recent
BioCreAtIvE challenge (Krallinger et al, 2007) was
concerned with selecting papers and extracting in-
formation suitable for curation. The PPI detection
subtask (IPS) required participants not simply to de-
tect PPI mentions, but to detect curatable PPI men-
tions, in other words to enrich the PPI mentions with
extra information. Furthermore, another of the sub-
tasks (IMS) required participants to add information
about experimental methods to the curatable PPIs.
3 Data Collection and Corpus
3.1 Annotation of the Corpus
A total of 217 papers were selected for annotation
from PubMed and PubMedCentral as having exper-
imentally proven protein-protein interactions (PPIs).
The papers were annotated by a team of nine anno-
tators, all qualified in biology to at least PhD level,
over a period of approximately five months.
The XML versions of the papers were used wher-
ever possible, otherwise the HTML versions were
used and converted to XML using an in-house tool.
The full-text of each paper, including figure cap-
tions, was annotated, although the materials and
methods sections were not included in the annota-
tion.
From the 217 annotated papers, a total of 65
were selected randomly for double annotation and
27 for triple annotation. These multiply-annotated
papers were used to measure inter-annotator agree-
ment (IAA), by taking each pair of annotations on
the same paper, and scoring one annotation against
the other using the same algorithm as for scoring the
system against the annotated data (see Section 5).
Each doubly annotated paper contributed one pair of
annotations, whilst the triply annotated papers con-
tributed three pairs of annotations. The overall IAA
score is the micro-average of the F1 scores on each
pair of corresponding annotations, where it should
be emphasised that the F1 does not depend on the
order in which the annotated papers were combined.
The multiply annotated papers were not reconciled
to produce a single gold version, rather the multiple
versions were left in the corpus.
The papers were annotated for entities and rela-
tions, and the relations were enriched with proper-
ties and attributes. The entities chosen for anno-
tation were those involved in PPIs (Protein, Com-
plex, Fusion, Mutant and Fragment) and those
which could be attributes of PPIs (CellLine, Drug-
Compound, ExperimentalMethod and Modification-
Type). A description of the properties and attributes,
as well as counts and IAA scores are shown in Ta-
bles 1 and 2.
Once annotated, the corpus was split randomly
into three sections, TRAIN (66%), DEVTEST (17%)
and TEST (17%). TRAIN and DEVTEST were to be
used during the development of the system, for fea-
ture exploration, parameter tuning etc., whilst TEST
was reserved for scoring the final system. The splits
were organised so that multiply annotated versions
of the same paper were placed into the same section.
3.2 Descriptive Statistics of Corpus
The total number of distinct PPIs annotated in the
336 papers was 11523, and the PPI IAA, measured
using F1, was 64.77. The following are examples of
enriched PPIs, with the entities in bold face:
(1) Tat may also increase initiation of HIV-
1 transcription by enhancing phosphoryla-
tion of SP1, a transcription factor involved
in the basal HIV-1 transcription [14].
146
Name Explanation Values Counts Pct IAA
IsPositive The polarity of the statement about the PPI. Positive 10718 93.01 99.57Negative 836 7.26 90.12
IsDirect Whether the PPI is direct or not. Direct 7599 65.95 86.59NotDirect 3977 34.51 61.38
IsProven Whether the PPI is proven in the paper or not.
Proven 7562 65.63 87.75
Referenced 2894 25.11 88.61
Unspecified 1096 9.51 34.38
Table 1: The properties that were attached to PPIs, their possible values, counts and IAA
Name Entity type Explanation Count IAA
InteractionDetectionMethod ExperimentalMethod Method used to detect the
PPI.
2085 59.96
ParticipantIdentificationMethod ExperimentalMethod Method used to detect the
participant.
1250 36.83
ModificationBefore Modification Modification of partici-
pant before interaction.
240 68.13
ModificationAfter Modification Modification of partici-
pant after interaction.
1198 86.47
DrugTreatment DrugCompound Treatment applied to par-
ticipant.
844 49.00
CellLine CellLine Cell-line from which par-
ticipant was drawn.
2000 64.38
Table 2: The attributes that could be attached to the PPIs, with their entity type, counts and IAA
(2) To confirm that LIS1 and Tat interact in
vivo, we used yeast two-hybrid system, in
which Tat was expressed as a bait and LIS1
as a prey. Again, we found that LIS1 and
Tat interacted in this system.
In Example 1, the properties attached to the PPI be-
tween ?Tat? and ?SP1? are Referenced, Direct and
Positive, and ?phosphorylated? is attached as a Mod-
ificationAfter attribute. Example 2 shows a PPI be-
tween ?Tat? and ?LIS1? (in the second sentence)
which is given the properties Proven, Direct and
Positive, and has the InteractionDetectionMethod at-
tribute ?yeast two-hybrid system?. This second ex-
ample indicates that attributes do not have to occur
in the same sentence.
Statistics on the occurrence of properties are
shown in Table 1. For most of the property val-
ues, there are significant numbers of PPIs, except
for Unspecified and Negative, which are used in less
than 10% of cases. Note that annotators were per-
mitted to mark more than one PPI between a given
pair of entities if, for example, they wished to mark
both Positive and Negative PPIs because the author
is making a statement that proteins interact under
one condition and not under another condition. For
the purposes of data analysis and to make modelling
easier, such PPIs have been collapsed to give a single
PPI which may have multiple values for each prop-
erty and attribute.
Table 2 shows occurrence statistics for attributes,
where, as for properties, there can be multiple val-
ues for the same attribute. A notable feature of the
attribute attachment counts is that certain attributes
(ModificationBefore and DrugTreatment especially)
are quite rarely attached, making it difficult to use
statistical techniques.
Also shown in Tables 1 and 2 are the IAA figures
for all properties and attributes. The IAA for proper-
ties is generally high, excepted for the Unspecified
value of the IsProven property. This being some-
thing of a ?none of the above? category means that
the annotators probably have different standards re-
147
garding the uncertainty required before the PPI is
placed in this class. The IAA for attributes is, on
the whole, lower, with some attributes showing par-
ticularly low IAA (ParticipantIdentificationMethod).
A closer investigation shows that the bulk of the dis-
agreement is about when to attach, in other words if
both annotators decide to attach an attribute to a par-
ticular PPI, they generally agree about which one,
scoring a micro-averaged overall F1 of 95.10 in this
case.
4 Methods
4.1 Pipeline Processing
The property and attribute assignment modules were
implemented as part of an NLP pipeline based on
the LT-XML2 architecture1 . The pipeline consists of
tokenisation, lemmatisation, part-of-speech tagging,
species word identification, abbreviation detection
and chunking, named entiry recognition (NER) and
relation extraction. The part-of-speech tagging uses
the Curran and Clark POS tagger (Curran and Clark,
2003) trained on MedPost data (Smith et al, 2004),
whilst the other preprocessing stages are all rule
based. Tokenisation, species word identification and
chunking were implemented in-house using the LT-
XML2 tools (Grover and Tobin, 2006), whilst ab-
breviation extraction used the Schwartz and Hearst
abbreviation extractor (Schwartz and Hearst, 2003)
and lemmatisation used morpha (Minnen et al,
2000).
The NER module uses the Curran and Clark NER
tagger (Curran and Clark, 2003), augmented with
extra features tailored to the biomedical domain. Fi-
nally, a relation extractor based on a maximum en-
tropy model and a set of shallow linguistic features
is employed, as described in (Nielsen, 2006).
4.2 Properties
To assign properties to each PPI extracted by the
relation extraction component, a machine learning
based property tagger was trained on a set of features
extracted from the context of the PPI. The property
tagger used a separate classifier for each property,
but with the same feature set, and both Maximum
Entropy (implemented using Zhang Le?s maxent2)
and Support Vector Machines (implemented using
1http://www.ltg.ed.ac.uk/software/xml/
2http://homepages.inf.ed.ac.uk/s0450736/
maxent_toolkit.html
svmlight3) were tested. To choose an optimal fea-
ture set, an iterative greedy optimisation procedure
was employed. A set of potential features were im-
plemented, with options to turn parts of the feature
set on or off. The full feature set was then tested on
the DEVTEST data with each of the feature options
knocked out in turn. After examining the scores on
all possible feature knockouts, the one which offered
the largest gain in performance was selected and re-
moved permanently. The whole procedure was then
repeated until knockouts produced no further gains
in performance. The resulting optimised feature set
contains the following features:
ngram Both unigrams and bigrams were imple-
mented, although, after optimisation, unigrams
were switched off. The ngram feature uses vlw
backoff, which means that words are replaced
by their verb stems, backed off to lemmas and
then to the word itself if not available. Further-
more, all digits in the words are replaced with
?0?. Ngrams are extracted from the sentences
containing the participants in the PPI, and all
sentences in between. Ngrams occurring be-
fore, between and after the participants of the
PPI are treated as separate features.
entity The entity feature includes the text and type
of the entities in the PPI.
headword This feature is essentially constructed in
the same way as the ngram feature, except that
only head verbs of chunks in the context are
included, and the vlw backoff is not used.
entity-context In the entity context feature, the vlw
backoffs of the two words on either side of each
of the entities in the PPI are included, with their
positions marked.
4.3 Attributes
For attribute assignment, experiments were per-
formed with both rule-based and machine-learning
approaches. The following sections summarise the
methods used for each approach.
4.3.1 Rule-based
In the rule-based approach, hand-written rules
were written for each attribute, using part-of-speech
tags, lemmas, chunk tags, head words and the NER
tags. In all, 20 rules were written. Each rule is
3http://svmlight.joachims.org/
148
Rule Protein Prec Count
P1 ATT P2 P2 100 13
P1 is ATT by P2 P1 100 1
ATT of P2 P2 86.1 112
ATT of P1 P1 74.5 80
P1 * ATT site P1 72.2 13
P1 * ATT by * P2 P2 70.0 100
P1 * (ATT pass) * P1 P2 64.0 16
P1 * ATT * P2 P2 67.5 187
P2 ATT P2 75.0 100
P2 - any-word ATT P1 73.7 14
Table 3: The rules used to assign ModificationAfter
attributes. The protein column indicates whether the
attribute attaches to the 1st or 2nd protein, the prec
field indicates the precision of the rule on the train-
ing set and the count indicates the number of times
the rule applied correctly in training. In the rules,
P1 refers to the first protein, P2 refers to the sec-
ond protein, ATT refers to the attribute, * refers to
any number of words, any-word refers to any single
word, and pass refers to the passive voice. For exam-
ple, the rule ?P2 - any-word ATT? applied to the sen-
tence ?protein 1 is regulated by protein 2-dependent
phosphorylation? would result in the attribute phos-
phorylation being assigned as the ModificationAfter
attribute to protein 1.
ranked according to its precision as determined on
the TRAIN set, and the rules are applied in order
of their precision. This is particularly important
with modification attributes which are constrained
so that a given modification entity can only be at-
tached once per interaction. Table 3 lists the rules
used to assign the ModificationAfter attribute.
4.3.2 Machine Learning
For this approach, attributes are modelled as rela-
tions between PPIs and other entities. For each PPI
in a document, a set of candidate relations is cre-
ated between each of the entities in the PPI and each
of the attribute entities contained in the same sen-
tence(s) as the PPI4. If there are no entities of the
appropriate type for a given attribute in the same
sentence as the PPI, the sentences before and af-
ter the PPI are also scanned for candidate entities.
Each of the candidate relations that correspond to
4PPIs spanning more than 2 sentences were ignored
attributes annotated in the gold standard are consid-
ered positive examples, whilst those that were not
annotated are considered negative examples. For ex-
ample, given the following sentence:
Protein A phosphorylates protein B
[Protein] [Modification] [Protein]
If the gold standard indicates a PPI between Pro-
tein A and Protein B with phosphorylates assigned
as a ModificationAfter attribute to Protein B, four
candidate relations will be created as shown in Ta-
ble 4
Type Entity 1 Entity 2 Label
Mod Before Prot A phosphorylates neg
Mod Before Prot B phosphorylates neg
Mod After Prot A phosphorylates neg
Mod After Prot B phosphorylates pos
Table 4: Candidate Attribute Relations for Protein A
phosphorylates Protein B
A set of features is extracted for each of the exam-
ples and a maximum entropy (ME) model is trained
using Zhang Le?s maxent toolkit. The features used
are listed below:
entity The text and part-of-speech of the attribute,
as used for properties.
entity-context The entity context feature used for
properties, except that the context size was in-
creased to 4, and parts-of-speech of the context
words were also included.
ngram This is the same as the ngram feature
used for properties, except that unigrams were
switched on.
entities-between The entities that appear between
the two entities involved in the candidate rela-
tion.
parent-relation-feature Indicates the position of
the attribute entity with respect to parent PPI
(i.e. before, after, or in between). For attributes
that are in between the two entities involved in
the PPI, also indicates if the sentence is active
or passive.
5 Evaluation
5.1 Properties
To score the property tagger, precision, recall and
F1 are calculated for each of the seven possible
149
Name Value Baseline Maximum Entropy SVM
Gold Predicted Gold Predicted Gold Predicted
IsPositive Positive 96.87 97.33 97.10 98.22 97.08 98.27
Negative 0.00 0.00 38.46 48.39 45.45 57.53
IsDirect Direct 78.66 81.90 82.05 85.54 81.94 86.87
NotDirect 0.00 0.00 58.92 54.33 60.80 63.44
IsProven Proven 78.21 78.85 87.86 82.73 88.08 88.51
Referenced 0.00 0.00 81.46 69.65 82.83 81.97
Unspecified 0.00 0.00 25.74 29.41 22.77 28.00
Overall 74.20 76.24 83.87 83.33 84.09 86.79
Table 5: The performance of the property tagger, measured by training on TRAIN and DEVTEST combined,
then testing on TEST. The two scores given for each system are for testing on gold PPIs, and testing on
predicted PPIs. An F1 score is shown for each property value, as well as a microaveraged overall score.
property values and then the F1 scores are micro-
averaged to give an overall score. As mentioned in
Section 3.1, all versions of the annotation for each
multiply-annotated document were included in the
training and test sets, taking care that all versions of
the same document were included in the same set.
This has the disadvantage that the system can never
achieve 100% in cases where the annotators differ,
but the advantage of giving partial credit where there
is genuine ambiguity and the system agrees with one
of the options chosen by the annotators.
The scores for all property values, tested on TEST,
are shown in Table 5, both using the model (with
Maximum Entropy and SVM) and using a base-
line where the most popular value is assigned. Two
scores are shown, the performance as measured
when the test set has the gold PPIs, and the per-
formance when the test set has the predicted PPIs,
scored only on those PPIs where both system and
gold agree. The relation extractor used to predict
the PPIs is trained on the same documents as were
used to train the property tagger.
To see which features were most effective, a
knockout (lesion) test was conducted in which fea-
tures were knocked out one by one and performance
was measured on the DEVTEST set. In each feature
knockout, one of the features from the list in Sec-
tion 4.2 was removed. Table 6 shows how the overall
performance is affected by the different knockouts.
From the knockout experiment it is clear that the
ngram (actually bigram) feature is by far the most
effective, with the other features only contributing
marginally to the results.
Feature Knockout score Difference
vanilla 86.08 0.00
ngram 81.86 -4.22
entity 85.30 -0.77
headword 84.38 -0.50
entity-context 85.54 -0.54
Table 6: The effect of knocking out features on the
property score. Tests are conducted by training on
TRAIN and testing on DEVTEST, on predicted PPIs.
?vanilla? refers to the case where the optimal fea-
tures set is employed.
5.2 Attributes
The attributes are scored in the same manner as the
properties. Table 7 summarises the results for both
the rule-based and machine learning attribute sys-
tems. These are compared to a baseline system that
simply attaches the nearest entity of the appropriate
type for each attribute.
5.3 Discussion
The results for the more common property values are
generally close to human performance (as measured
by IAA), however performance on both IsNegative
and Unspecified is fairly low. In the case of Un-
specified, the IAA is also low, making it likely that
the training and test data is inconsistent, compound-
ing the problem of the low occurrence rate of this
value. The Negative value also suffers from a low
occurrence rate, leading to an imbalance between
Negative and Positive which makes life hard for the
150
Attribute Baseline Rule-based Machine Learning
Gold Predicted Gold Predicted Gold Predicted
InteractionDetectionMethod 36.02 39.71 39.22 41.38 37.02 46.81
ParticipantIdentificationMethod 08.68 09.27 12.32 12.87 03.37 05.97
ModificationBefore 13.10 16.00 42.22 43.84 04.88 08.33
ModificationAfter 43.37 46.00 64.93 73.04 62.32 69.64
DrugTreatment 49.57 51.11 51.29 53.33 13.90 24.52
CellLine 50.19 45.90 54.47 50.47 45.13 42.28
Overall 29.68 30.32 45.26 48.32 32.08 43.11
Table 7: The performance of the attribute tagger, on TEST. The two scores given for each system are for
testing on gold PPIs, and testing on predicted PPIs. Performance on each attribute value is measured using
F1, and then microaveraged to give an overall figure.
machine learners. However it is also possible that
the shallow linguistic features used in these experi-
ments are not sufficient to make the sometimes sub-
tle distinction between a negative statement about
an interaction and a positive one, and that models
based on a deeper linguistic analysis (e.g. parse trees
as in (Moschitti, 2004)) would be more successful.
Note also that the feature set was optimised for max-
imum performance across all property values, with
all given equal weight, but if some values are more
important than others then this could be taken into
account in the optimisation, with possibly different
feature sets used for different property names.
The results for the attributes using the rule-based
system are approximately 75% of human perfor-
mance and are higher than results for the machine
learning system. However, for the Modification-
After, CellLine, and InteractionDetectionMethod at-
tributes, which occur more frequently than the other
attributes and have higher IAA, the machine learning
system is competitive and even slightly outperforms
in the case of the InteractionDetectionMethod. The
scores are directly correlated with the IAA and both
the scores and the IAA are higher for the attributes
that tend to occur in the same sentence as the PPI. On
a practical level, this suggests that those who hope to
create similar systems would be advised to start with
local attributes and pay particular attention to IAA on
non-local attributes.
5.4 Further work
As regards properties, good results were obtained
using shallow linguistic features, but it would be
interesting to learn whether machine learning tech-
niques based on a deeper linguistic analysis would
be more effective. Also, properties were treated as
additional information added on to the PPIs after the
relation extractor had run, but perhaps it would be
more effective to combine relation extraction and
property tagging to, for example, consider positive
and negative PPIs as different types of relations.
For attributes, it would be interesting to combine
the rule-based and machine learning systems. This
has the advantage of having a system that can both
learn from annotated data when it exists, but can
be potentially improved by rules when necessary or
when annotated data is not available. Another issue
may be that some attributes might not be represented
explicitly by a single entity in a document. For ex-
ample, an experimental method may be described
rather than explicitly stated. Attributes that are not
local to the PPI caused difficulty for both the anno-
tators and the system. It would be interesting to see
if it is easier to attach attributes to a single PPI that
has been derived from the text, rather than attempt-
ing to assign attributes to each specific mention of a
PPI within the text. This could be accomplished by
attempting to merge the information gathered from
each relation along the lines described in (Hobbs,
2002)
Since the main motivation for developing the sys-
tem to extract enriched PPIs was to develop a tool to
aid curators, it would be useful to know how effec-
tive the system is in this task. Aside from (Karama-
nis et al, 2007), there has been little work published
to date on the effect that NLP could have on the cu-
ration process. In the most recent BioCreAtIvE eval-
uation, the PPI subtasks were concerned with au-
151
tomating information extraction tasks typically per-
formed by curators such as distinguishing between
curatable and non-curatable PPI mentions and spec-
ifying the details of how the PPI was detected.
6 Conclusions
A system was implemented for enriching protein-
protein interactions (PPIs) with properties and at-
tributes providing additional information useful to
biologists. It was found that a machine learning
approach to property tagging, using simple contex-
tual features, was very effective in most cases, but
less effective for values that occurred rarely, or for
which annotators found difficulty in assigning val-
ues. For the attributes, sparsity of data meant that
rule-based approaches worked best, using fairly sim-
ple rules that could be quickly developed, although
machine learning approaches could be competitive
when there was sufficient data.
7 Acknowledgements
The authors are very grateful to the annotation
team, and to Cognia (http://www.cognia.
com) for their collaboration on the TXM project.
This work is supported by the Text Mining Pro-
gramme of ITI Life Sciences Scotland (http://
www.itilifesciences.com).
References
Erick Alphonse, Sophie Aubin, Philippe Bessieres, Gilles
Bisson, Thierry Hamon, Sandrine Lagarrigue, Adeline
Nazarenko, Alain-Pierre Manine, Claire Nedellec, Mo-
hamed Ould Abdel Vetah, Thierry Poibeau, and Davy Weis-
senbacher. 2004. Event-based information extraction for the
biomedical domain: the Caderige project.
C. Blaschke and A. Valencia. 2002. The frame-based module
of the suiseki information extraction system. IEEE Intelli-
gent Systems, (17):14?20.
Razvan Bunescu and Raymond Mooney. 2006. Subsequence
kernels for relation extraction. In Y. Weiss, B. Schlkopf, and
J. Platt, editors, Advances in Neural Information Processing
Systems 18. Cambridge, MA.
James R. Curran and Stephen Clark. 2003. Language indepen-
dent NER using a maximum entropy tagger. In Proceedings
of CoNLL-2003.
Ian Donaldson, Joel Martin, Berry de Bruijn, Cheryl Wolt-
ing, Vicki Lay, Brigitte Tuekam, Shudong Zhang, Berivan
Baskin, Gary D. Bader, Katerina Michalickova, Tony Paw-
son, and Christopher W. V. Hogue. 2003. PreBIND and Tex-
tomy - mining the biomedical literature for protein-protein
interactions using a support vector machine. BMC Bioinfor-
matics, 4:11.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano. 2006.
Exploiting shallow linguistic information for relation extrac-
tion from biomedical literature. In Proceedings of the EACL.
Claire Grover and Richard Tobin. 2006. Rule-Based Chunking
and Reusability. In Proceedings of LREC 2006.
Jerry R. Hobbs. 2002. Information extraction from biomedical
text. Journal of Biomedical Informatics, 35(4):260?264.
N. Karamanis, I. Lewin, R. Seal, R. Drysdale, and E. J. Briscoe.
2007. Integrating natural language processing with flybase
curation. In Proceedings of PSB 2007.
Martin Krallinger, Florian Leitner, and Alfonso Valencia. 2007.
Assessment of the Second BioCreative PPI Task: Automatic
Extraction of Protein-Protein Interactions. In Proceedings of
the Second BioCreative Challenge Evaluation Workshop.
E. Marsh and D. Perzanowski. 1998. MUC-7 evaluation of IE
technology: Overview of results. In Proceedings of MUC-7.
Guido Minnen, John Carroll, and Darren Pearce. 2000. Robust,
applied morphological generation. In Proceedings of INLG
2000.
Alessandro Moschitti. 2004. A study on convolution kernels
for shallow semantic parsing. In Proceedings of the ACL.
Leif Arda Nielsen. 2006. Extracting protein-protein interac-
tions using simple contextual features. In Proceedings of the
BioNLP 2006 at HLT/NAACL 2006.
Conrad Plake, Jo?rg Hakenberg, and Ulf Leser. 2005. Op-
timizing syntax-patterns for discovering protein-protein-
interactions. In Proc ACM Symposium on Applied Comput-
ing, SAC, Bioinformatics Track, volume 1, March.
A.S. Schwartz and M.A. Hearst. 2003. Identifying abbreviation
definitions in biomedical text. In Proceedings of PSB 2003.
Parantu K. Shah and Peer Bork. 2006. Lsat: learning about al-
ternative transcripts in medline. Bioinformatics, 22(7):857?
865.
L. Smith, T. Rindflesch, and W. J. Wilbur. 2004. MedPost: a
part-of-speech tagger for biomedical text. Bioinformatics,
20(14):2320?2321.
Bjo?rn M. Ursing, Frank H. J. van Enckevort, Jack A. M. Leu-
nissen, and Roland J. Siezen. 2001. Exprot - a database for
experimentally verified protein functions. In Silico Biology,
2:1.
Tuangthong Wattarujeekrit, Parantu K. Shah, and Nigel Collier.
2004. PASBio: predicate-argument structures for event ex-
traction in molecular biology. BMC Bioinformatics, 5:155.
John W. Wilbur, Andrey Rzhetsky, and Hagit Shatkay. 2006.
New directions in biomedical text annotation: definitions,
guidelines and corpus construction. BMC Bioinformatics,
7:356+, July.
H. Xu, D. Krupke, J. Blake, and C. Friedman. 2006. A natural
language processing (nlp) tool to assist in the curation of the
laboratory mouse tumor biology database. AMIA Annu Symp
Proc.
Alexander Yeh, Lynette Hirschman, and Alexander Morgan.
2002. Background and overview for KDD cup 2002 task 1:
information extraction from biomedical articles. SIGKDD
Explor. Newsl., 4(2):87?89.
152
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 19?27,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Using Automated Feature Optimisation to Create an Adaptable
Relation Extraction System
Barry Haddow
School of Informatics, University of Edinburgh,
2 Buccleuch Place, Edinburgh, Scotland, EH8 9LW
bhaddow@inf.ed.ac.uk
Abstract
An adaptable relation extraction system for the
biomedical domain is presented. The system
makes use of a large set of contextual and shal-
low syntactic features, which can be automati-
cally optimised for each relation type. The sys-
tem is tested on three different relation types;
protein-protein interactions, tissue expression
relations and fragment to parent protein rela-
tions.
1 Introduction
In biomedical information extraction, research in
named entity recognition (ner) and relation extrac-
tion (re) has tended to focus on the extracting pro-
teins and their interactions, with less thought given
to how to adapt such systems to other entities and
relations of biomedical interest. This is especially
true for re, where there is very little work on rela-
tions other than protein-protein interactions. Nev-
ertheless, in order to create applications of use to
biologists such as curation assistants and improved
information extraction and retrieval systems it will
be necessary to treat a broader range of semantic re-
lations. The recent release of the Genia event corpus
(Kim et al, 2008) will help to drive this research.
The aim of this paper is to address the problem of
how to create an re system, which can be adapted to
different biomedical re problems with a minimum of
manual intervention. Since this paper focuses on re-
lation extraction, it will be assumed that the named
entities are given, in other words the human anno-
tated entities are used in all experiments. The ap-
proach taken to re is to treat it as a supervised
classification problem on relation candidates, using
a large collection of shallow syntactic and contextual
features. Relation candidates are pairs of entities,
picked out using an appropriate candidate generation
strategy. The use of shallow (as opposed to deep)
syntactic features means that the system can rely
on relatively robust linguistic tools such as part-of-
speech taggers and chunkers, rather than more brit-
tle and less widely available tools such as parsers.
The difficulty with feature-based methods is, how-
ever, how to select the best performing feature set,
as simply adding all possible features does not nec-
essarily give the best results (Guyon and Elisseeff,
2003). The approach taken here is to implement a
large feature set and then use a greedy search to
explore the feature set and select the best subset
of features. This method of feature set optimisa-
tion is not new (for example, it was applied by one
team (Ganchev et al, 2007) on the BioCreative II
Gene Mention task ), but in this work a comparison
of search starting points and feature groupings will
be presented.
All re systems require a human-annotated corpus
for testing, and since a supervised machine learning
approach is employed, a corpus is also required for
training the system. The experiments described in
this paper make use of the ITI TXM corpora (Alex
et al, 2008), which include the ppi corpus address-
ing protein-protein interactions, and the te corpus
addressing tissue expression. Both corpora consist of
approximately 200 full-text biomedical research pa-
pers annotated with entities, normalisations of enti-
ties to standard databases, relations, and with en-
riched information added to the relations. Only the
entities and relations will be considered here.
This paper is organised as follows: after reviewing
related work in the following section, the re system
is described in Section 3, including a description of
the corpora, the relation candidate extraction strate-
gies, the features employed, the feature optimisation
methods and the evaluation method. In Section 4
the results of the optimisation experiments are pre-
sented and discussed, with some concluding remarks
in Section 5.
19
2 Related Work
Recent interest in the extraction of protein-protein
interactions has been given added impetus by shared
tasks such as the Language Learning in Logic
(Cussens and Ne?dellec, 2005), and the BioCreative
II Interaction Pairs Subtask (Krallinger et al, 2008).
It should be noted that the latter task, rather than
being concerned with the extraction of specific inter-
action relation mentions, required systems to list the
(curatable) interactions at a document level. Many
teams, however, extracted the interaction mentions
as a first step and then processed these to give the
document level list of curatable interactions.
The extraction of protein-protein interactions has
also been helped by the availability of annotated cor-
pora, such as AIMed (Bunescu et al, 2005), which
consists of around 1000 Medline abstracts annotated
with proteins and their interactions. In common with
the LLL corpus, the AIMed corpus only contains
intra-sentential relations, and is somewhat smaller
than the corpus used in the current work. In addi-
tion to the work by the corpus creators (Bunescu and
Mooney, 2007), other authors have achieved good re-
sults on AIMed by making use of dependency parses
in different ways (Erkan et al, 2007; Katrenko and
Adriaans, 2006). It is not clear, however, how well
these techniques would transfer to other, similar, re
problems, and how much work would be involved in
tuning the systems for a new problem.
Supervised learning based on shallow syntactic fea-
tures has also been applied to the biomedical do-
main, again focusing on protein-protein interactions
(Nielsen, 2006; Giuliano et al, 2006). A system-
atic exploration of a set of such features for protein-
protein interaction extraction was recently provided
by Jiang and Zhai (2007), who also used features de-
rived from the Collins parser. They did not, however,
experiment with the automated optimisation of the
feature sets. In the news domain, the best reported
results on the ACE dataset1 have been achieved by
a composite kernel which depends partially on a full
parse, and partially on a collection of shallow syn-
tactic features (Zhou et al, 2007).
Aside from protein-protein interactions, there has
been little work directed at other types of relations
in the biomedical domain. Recent corpus annota-
tion projects such as Genia (Kim et al, 2008) and
BioInfer (Pyysalo et al, 2007) include multiple types
of relations, however many of the relation types are
represented in fairly small quantities. In earlier work
(Skounakis et al, 2003), the extraction of cell local-
isation relations was studied using an automatically
created corpus.
1http://www.nist.gov/speech/tests/ace/
3 Methods
3.1 Corpora
The ITI TXM corpora contain annotations related
to protein-protein interactions (in the ppi corpus),
and annotations related to tissue expression exper-
iments (in the te corpus). Each corpus consists of
biomedical research articles, selected from PubMed
and PubMedCentral either because they contain ex-
perimentally proven protein-protein interactions (for
the ppi corpus), or because they contain tissue ex-
pression experiments (for the te corpus).
The articles were annotated by a team of quali-
fied biologists. The annotations consisted of entities
(Table 1), normalisations of selected entities to stan-
dard databases, relations (Table 2) and enrichment
of relations with additional information of interest
to curators. For each corpus, the entities marked
were those involved in the relation which formed the
principal focus of that corpus (either ppi or te), and
those which could affect this relation. In the te cor-
pus, te relations were marked when the text stated
that a particular gene or gene product was present or
absent in a particular tissue, whilst ppi relations were
marked whenever a statement (positive or negative)
was made about the interaction of a pair of Proteins,
Mutants, Fragments, Complexes or Fusions. In ad-
dition, both corpora were annotated with frag re-
lations which connect Fragments and Mutants with
their parent Proteins.
Corpus Entities
ppi CellLine, Complex, DrugCompound,
ExperimentalMethod, Fragment, Fusion,
Modification, Mutant, Protein
te Complex, DevelopmentalStage, Disease,
DrugCompound, ExperimentalMethod,
Fragment, Fusion, GOMOP, Gene,
Mutant, Protein, Tissue, mRNAcDNA
Table 1: The entity types in the te and ppi corpora.
Note that GOMOP stands for ?Gene or mRNAcDNA
or Protein? and was used when the annotators felt the
author was using the term in an ambiguous way.
In order to monitor annotation quality, and to
measure of the difficulty of the task, some documents
were multiply annotated. The counts of the numbers
of unique documents in each section, together with
the numbers of annotated documents are shown in
Table 3. Note that the multiply annotated docu-
ments were not reconciled, but the multiple copies
were included in the corpus. Each corpus was split
into three sections ? train, devtest and test ?
with the first two sections being used for system de-
velopment, and the last reserved for final testing.
20
Corpus Relation
type
Entity 1 Types Entity 2 Types Count
ppi ppi Protein, Fusion, Mutant, Fragment or
Complex
Protein, Fusion, Mutant, Fragment or
Complex
11,523
frag Protein Mutant or Fragment 16,002
te te Gene, Protein, mRNAcDNA, GOMOP,
Fusion, Mutant, Complex or Fragment
Tissue 12,426
frag Protein Mutant or Fragment 4,735
Table 2: Relation types in each corpus.
Corpus Segment Unique Doc-
uments
Annotated
Documents
ppi train 133 221
devtest 39 58
train 45 57
te train 151 221
devtest 41 48
test 46 59
Table 3: Counts of documents and annotations in each
corpus.
Corpus Relation Intra Inter
ppi ppi 10,607(92.1%) 916(7.9%)
frag 10,176(63.6%) 5,826(36.4%)
te te 10,356(83.3%) 2,070(16.7%)
frag 3,335(70.4%) 1,400(29.6%)
Table 4: Counts of inter and intra-sentential relations.
Annotators were permitted to mark relations
between entities in the same sentence (intra-
sentential), or between entities in different sentences
(inter-sentential). The majority of relations were
intra-sentential, with frag relations showing the
highest proportion of inter-sententials. Table 4 shows
the counts of inter/intra-sentential relations of each
type.
Some examples of each type of relation will now
be presented. The first example is from PubMed
16436664, and is a te relation:
Our recent observations that ??v?5?1 is up-
regulated in ?scleroderma fibroblasts?1 and
that the transient overexpression of ?v?5
increases the human ??2(I) collagen?2 gene
expression in normal ?fibroblasts?2 . . .
There are two different te relations in this sentence
fragment, indicated by the numerical subscripts; the
first connects a Tissue and a Complex, and the sec-
ond connects a Tissue with a Gene. Another example
from the same paper shows a frag relation.
Because ??5?1 has a ?cytoplasmic domain?1
highly homologous to that of ?6-subunit, 42
we made a hypothesis that ?v?5 activates
SLC by the nonproteolytic pathway.
The annotators could also mark negative te and ppi
relations, as shown in the following example of a ppi
relation taken from PubMedCentral 1075921.
It was also previously reported that two
truncated versions of ?p53?1,2, consisting
of residues ?2-45?1,3 and ?46-71?2,4, do not
bind ?hRPA70?3,4 (47)
Here the ppi relations connect the two Fragments
(?2-45? and ?46-71?) to the Protein ?hRPA70?,
whilst frag relations connect the Fragments with
their parent Protein ?p53?.
In contrast with the straightforward intra-
sentential relations shown above, the following (from
PubMed 16399077) is an example of an inter-
sentential te relation (only the related entities are
shown).
To test whether SPE can activate Toll sig-
naling, we expressed activated SPE in ?S2
cells?1 and in flies, and we then assayed
the expression of the gene for Drosomycin
(Drs), an antifungal peptide known to be
induced by Toll signaling in response to mi-
crobial infection (Lemaitre et al, 1996). In
both cases, ?Drs?2 expression was signifi-
cantly induced in the absence of infection,
In this example, the annotator has connected a Tis-
sue on the first sentence, with an mRNAcDNA in
the second.
The multiply annotated documents in the corpus
were used to calculate the inter-annotator agreement
(iaa), by scoring different versions of the annota-
tion of the same document against each other. For
each corresponding pair of annotations, one anno-
tator was selected as the ?gold?, and the other an-
notator scored against the first using precision, re-
call and F1 on relations. Only relations where both
annotators agreed on the participating entities were
considered. The scores for each annotated document
pair were then micro-averaged (where each example
21
Corpus Type Intra Inter All
ppi ppi 69.7 41.1 67.0
frag 90.5 73.9 84.6
All 78.7 67.3 76.1
te te 72.8 59.4 70.1
frag 89.7 69.0 84.0
All 77.4 62.7 74.1
Table 5: iaa for relation annotation, split by inter- and
intra-sentential
is given equal weight) to produce overall iaa scores
for the corpus, shown in Table 5.
The main observations from Table 5 are that te
and ppi relations are harder to annotate than frag
relations, and that inter-sentential are harder than
intra-sentential. In particular, the iaa for intra-
sentential frag relations is very high, probably be-
cause many of these are very straightforward con-
structions such as ?Fragment of Protein?. Inter-
sentential relations are often less clear as they in-
volve linking information between several sentences,
for example using coreferences.
Both corpora were pre-processed before re was
applied. The pre-processing involved tokenisation,
sentence boundary detection, lemmatising. part-of-
speech tagging, head word detection and chunking.
The part-of-speech tagging uses the Curran & Clark
maximum entropyMarkovmodel tagger (Curran and
Clark, 2003) trained on MedPost data (Smith et
al., 2004), whilst the other preprocessing stages are
all rule-based. The tokenisation, sentence bound-
ary detection, head word identification and chunk-
ing components were implemented with the lt-xml2
tools (Grover and Tobin, 2006), and the lemmatisa-
tion used morpha (Minnen et al, 2000).
3.2 The Relation Extraction System
Relation extraction is treated a classification prob-
lem, by generating candidate relations, and classify-
ing them as either true or false. In the optimisa-
tion experiments described in this paper, Zhang Le?s
maximum entropy (maxent) classifier2 was used,
since its performance was very competitive and its
fast training time permitted extensive feature exper-
imentation. The Gaussian prior was set to 0.1, and
the maximum training iterations to 100. In order to
assess the performance of the final system, maxent
was compared with support vector machines (svm)
using the SVM light toolkit (Joachims, 1999). Since
both the classifiers assign a confidence to each pre-
diction, a varying threshold can be applied to the
output of the classifier to provide a precision-recall
2http://homepages.inf.ed.ac.uk/s0450736/maxent_
toolkit.html
tradeoff.
Candidate relations were generated by consider-
ing entity pairs of the appropriate type, taking into
account the distance between the entities. It was
thought that inter-sentential and intra-sentential re-
lations would require different feature sets and differ-
ent models, so inter- and intra-sentential candidates
were generated separately. For intra-sentential rela-
tions, all entity pairs of the appropriate type (as in
Table 2) in the same sentence were permitted as can-
didates, with the sole exclusion being that any enti-
ties contained in a Fusion entity were not allowed to
participate in candidate te relations. This restric-
tion was in place in the annotation guidelines, so no
such relations were annotated. For intra-sentential
relations in the training data, around 25-30% of the
candidate relations are actual relations.
Generating inter-sentential candidates is more
problematic, as measures must be taken to limit the
number of candidates. Inter-sentential frag candi-
dates are restricted to a distance of no more than 5
sentences, whilst inter-sentential ppi and te candi-
dates are restricted to participants in adjacent sen-
tences. Inter-sentential re is performed after intra-
sentential re, so the candidate generation strategy
has access to the annotated intra-sentential relations
(in training) and the predicted intra-sentential rela-
tions (in testing). For te and ppi, candidates are
only created for those entities not already in a rela-
tion, and for frag candidates are only created if the
Mutant or Fragment is not already in a relation. Fur-
thermore, for frag relations, if there is more than
one Protein instance with the same lexical form in
the 5 sentence window, then a candidate relation is
only created between a given Fragment/Mutant and
the nearest occurrence of this Protein. For inter-
sentential frag relations, around 20% of the candi-
dates are actual relations, however for te and ppi,
only about 1% of the candidate relations are actual
relations.
3.3 Features
Each candidate relation is mapped to a feature rep-
resentation, where the features are binary or real-
valued functions of relations. The majority of the
features are binary, although these are actually spe-
cial cases of real-valued functions, taking values 0 or
1. A feature representation of a relation is normally
written as a sequence of strings, each corresponding
to a different feature, and the presence or absence of
a binary feature indicating whether it is on or off. In
order that the relation extractor could be applied to
different problems and optimised, a large number of
features were implemented, with the intention that
the feature space could be automatically searched to
find the best subset.
22
Features are normally grouped into feature tem-
plates and, as is common in the literature, the feature
templates may also be referred to as features. For in-
stance, a feature template may be ?the token to the
right of the second entity in the relation?, which then
gives rise to a set of boolean features with the pre-
fix ctxt-w-rf1-. One such feature in this set is the
feature which indicates that the token to the right
of the second entity is ?the?, i.e. ctxt-w-rf1-the.
The feature templates are then collected into fea-
ture groups, such as ?context features?, which are
really just a convenient way of conceptualising, im-
plementing and managing the features, and do not
necessarily reflect any common behaviour amongst
the features in a group.
The following is a comprehensive list of the feature
implemented in the re system with features listed
by group, and the possible options for each feature
group given. The options are used to turn on or off
feature templates in the group, or change templates,
and may be boolean or numerical. The nature of
the options will be important in the feature explo-
ration experiments since they influence the type of
search operations which may be used to explore the
feature space. The features are virtually all domain
independent, except for perhaps the SignSlashSign
feature which is specific to te. The RelationKey-
wordFeature can easily be ported to a new domain
by generating a list of keywords appropriate for the
given relation.
In the feature group descriptions which follow, the
term ?participants? refers to the entities within the
candidate relation. Some of the features make use of
the ?vlw backoff?, which for a given token is defined
as the verb stem, backing off to the lemma if that is
not available, backing off to the token itself.
Chunk This group has three optional templates; one
which adds the concatenated sequence of chunk types
between the participants, and two templates which
add the count of chunks between the participants as
binary and numeric features, respectively. So if the
chunk count is, for example 4, the binary feature
would be chunk-bwcount-4 and the numeric feature
would have name chunk-bwcount and value 4.
EntitiesBetween This has templates to indicate the
type and relative position of the entities between the
two participants. For te relations, only Tissue en-
tities are considered, whilst for other relations only
Proteins are considered.
Entity Features derived from the participating enti-
ties are added by the templates in this group, which
has options to turn on the entity?s text, class and
bigrams of these. There is also a feature template
which adds all words in the entities as separate fea-
tures, and one that adds all words in the second en-
tity only, plus options to add features which indicate
when the two entities have the same textual form, or
when one is a substring of the other.
EntityContext The entity context can include to-
kens, part-of-speech tags, chunk tags and vlw back-
offs, each within window sizes determined by numer-
ical options. A further option can switch on a tem-
plate which adds the concatenation of all vlw back-
offs in the context, on either side of each entity, and
there is also an option to convert all tokens and vlw
backoffs to lower case before creating the features.
EntityDistance Options on this group allow the ad-
dition of the token distance and sentence distance
between the entities, as numeric or binary features.
There is also an option to add a coarse three-way
classification of the token distance.
EntityFrequency Counts are made of the number of
occurrences of each entity surface form in the docu-
ment, limited to Tissue entities for te relations, and
Proteins for frag and ppi relations. The only option
for this feature group adds a template which gener-
ates a binary feature indicating the frequency rank
of the participants? surface forms in the document.
EntityPattern The entity pattern for a given intra-
sentential candidate relation shows how its partic-
ipants lie with respect to the other entities in the
sentence. The pattern is a concatenated sequence
of the entity types in the sentence, with the par-
ticipants in upper case and other entities in lower
case. Only entity types which are valid participants
in the relation in question are included. For example
protein-PROTEIN-TISSUE would indicate a relation
between a Protein and a Tissue, with another Pro-
tein occurring first in the sentence. Options in this
group add the patterm, the total number of entities
in the sentence, and the numbers of entities for each
type.
Frame The frame is the concatenation of the tokens
between the two participants. Two boolean options
on this group specify whether or not to include the
token concatenation, and whether or not to include
the part-of-speech concatenation. A further numeric
option is used to limit the maximum frame length;
when this is set to a non-zero value longer frames are
discarded.
HeadWord All the headwords of the chunks in the
sentences containing and between the participants
are listed and used to construct the features in this
group. Options specify whether to include head
nouns and/or head verbs, and whether to convert
the headwords to lower case or replace them by their
vlw backoffs. A further option allows an additional
marker to be added to each headword feature to in-
dicate whether it is before, between or after the par-
ticipants.
NestedEntity This feature indicates whether the
participants are contained in other entities, or in each
23
other. The first option adds a feature template which
indicates which type of entity containing the two par-
ticipants, if they are both contained. The second op-
tion adds a feature to indicate whether one of the
entities is contained within the other, and the third
adds a feature to indicate whether or not there is any
whitespace between the two entities.
Ngram Three options specify what type of ngrams
to add; whether to add unigrams of the tokens in
the sentences containing the participants, whether to
add bigrams of the same tokens, and whether to add
cross-bigrams, which are bigrams of tokens before
and between the participants, and of tokens between
and after the participants. Additional options spec-
ify whether to convert tokens to vlw backoff or lower
case and whether to replace all sequences of digits
by ?0?. Further options can be used to indicate that
only ngrams in between the participants should be
added, that each ngram feature should be marked as
before, between or after, or that all entities should
replaced in the text by their type.
RelationKeyword Relation keywords are terms an-
notated as relation indicators for ppi and te, and
linked to relations. For ppi they are interaction
words, and for te they are expression level words.
Keywords are matched from a list generated during
training and there are feature options to match these
keywords before, between and after the participants,
and to add templates for the existence of a keyword,
the text of the keyword, and whether or not it is a
head word.
RelativeEntityPosition The only option on this
group specifies whether or not to sort the partici-
pant entities, alphabetically by entity type. Binary
features are added indicating whether the first entity
in the candidate relation is the first in the document,
whether it is the second, whether the participants
overlap or whether they coincide.
SignSlashSign This group is only used for te re-
lations and is designed to detected the presence of
indicators like +/+ and ?/+ in the sentence(s) con-
taining the relation. Options allow the existence and
type of the one of these expressions to be indicated,
and also its position relative to the participants, and
whether it is adjacent to one of the participants.
3.4 Optimisation
Feature selection methods include wrapper methods
where feature sets are assessed according to their ef-
fectiveness for a given learner, and filter methods
where features are removed using some criterion be-
fore being passed to the learner (Guyon and Elisseeff,
2003). In building the re system, it was found that
filter methods did not work well, probably due to the
large number of interactions between the features, so
a wrapper optimisation method was employed, con-
sisting of greedy search through the space of possible
feature sets.
In the greedy search method, an initial feature set
is selected and a model trained on the train set
and tested on the devtest set. A series of search
operators (see below) are applied to the feature set
to produce a list of proposed new feature sets, one
corresponding to each operator, and the new feature
sets are tested in the same way. If any of these new
feature sets produces better results than the origi-
nal initial set, then the best set replaces the initial
feature set and the process is iterated. The greedy
search terminates when none of the search operators
leads to an improvement. Three types of search oper-
ators are used in the greedy search, defined in terms
of the feature set structure described in Section 3.3:
1. The deletion of a feature group.
2. The increase or decrease of a numerical option
on a feature group (e.g. context size), where the
size of the change is not greater than 2.
3. The flipping of a boolean option on a feature
group.
In theory search operators which add or remove in-
dividual features could be used, but due to the large
number of features the use of such operators is not
practical. In addition, it may have been possible
to achieve more robust results using cross-validation
rather than heldout testing, but that would also re-
sult in a large increase in search time.
3.5 Evaluation
In all re experiments, the annotated entities were
assumed as given so that only re performance was
being assessed. The performance was measured us-
ing precision-recall break-even point (bep), which is
found by adjusting the decision boundary (thresh-
old) of the classifier until the precision and recall are
equal then taking the value of the F1 at this thresh-
old. The bep has the advantage over F1 that its
definition is independent of the choice of threshold,
but it can still be compared easily to the iaa and is
based on the familiar concepts of precision and recall.
4 Results
Performance of the re system on each of the four re-
lation types was optimised using the greedy feature
exploration method described in Section 3.4. Inter
and intra-sentential relations were treated separately,
with intra-sentential relation performance optimised
first. The inter-sentential performance was then as-
sessed using a ?pipeline? consisting of the best intra-
sentential relation extractor, and the inter-sentential
system being optimised.
The greedy search experiments for intra-sentential
24
relations used two different starting feature sets, an
all set in which all features groups and options were
switched on, and the context sizes in EntityContext
were set to 3, and a base set which used just Ngram
and RelativeEntityPosition features. The models
were trained on train and scored on devtest us-
ing bep. In the calculation of bep, all relations
of the appropriate type were considered, including
inter-sententials. The results of the greedy search on
intra-sentential relations are shown in Table 6.
Corpus Relation
Type
Initial
Features
Initial
bep
Final
bep
ppi ppi base 36.8 52.2
all 51.6 53.4
frag base 49.2 56.0
all 55.9 57.4
te te base 45.9 51.9
all 50.6 53.8
frag base 53.7 62.7
all 60.1 61.2
Table 6: Greedy search feature exploration for intra-
sentential relations. Performance is measured on all re-
lations, testing on devtest.
For all relation types, the greedy search improves
the performance over the base and all feature sets,
usually reaching the highest performance when start-
ing from all. Comparing the results in Table 6
with the iaa figures provided in Table 5 shows that
the system performance is around 75-80% of iaa,
with the lowest relative performances observed for
frag relations. These relations include a higher pro-
portion of inter-sententials, so systems which ignore
inter-sententials suffer a larger loss in performance.
After choosing the best system for intra-sentential
relations, the same greedy optimisation was per-
formed on the inter-sentential relations using virtu-
ally the same initial feature sets. The only differ-
ence in the feature sets is that additional options are
added to the EntityDistance feature to indicate the
sentential distance between the entities. The result
of the greedy search on the inter-sentential relations
is shown in Table 7.
The inter-sentential relation optimisation is only
really successful for the frag relations in the ppi
corpus. For te and ppi inter-sentential relations, the
number of negative examples dwarfs the few posi-
tive examples making it very difficult for the ma-
chine learner. For frag relations in both corpora,
some progress is made on the performance on inter-
sentential relations (detailed breakdown not shown)
but in the te corpus this does not translate to an
overall improvement in bep. This is because the
inter- and intra-sentential probabilities have quite
Corpus Relation
Type
Initial
Features
Initial
bep
Final
bep
ppi ppi base 53.4 53.4
all 53.4 53.4
frag base 59.6 62.2
all 61.7 62.5
te te base 53.9 54.0
all 53.9 54.0
frag base 60.4 62.8
all 62.6 62.7
Table 7: Greedy search feature exploration for inter-
sentential relations. Performance is measured on all re-
lations, testing on devtest.
different ranges for frag relations meaning that the
threshold probabilities would have to be chosen sep-
arately to give the best F1 score.
The greedy search results just presented were
based on a partitioning of the feature sets into groups
which correspond to the way in which the features
were implemented. Since the search operators apply
at group granularity, and are not able to select fea-
tures from within a group, the way in which the fea-
tures are grouped is likely to have a bearing on the
performance of the best system found by the algo-
rithm. The next set of experiments investigates the
effective the feature grouping by conducting greedy
search with groups chosen randomly.
Corpus Relation
Type
Initial
bep
Final bep Ensemble
bep
ppi ppi 51.1 52.9, 52.4, 52.7,
52.8, 52.6
52.5
frag 55.7 56.3, 56.1, 56.1,
56.3, 56.4
56.3
te te 51.4 52.0 , 51.8, 52.5,
51.9, 52.9
52.1
frag 60.1 60.8, 60.5, 60.4,
60.7, 60.5
60.4
Table 8: Greedy search feature exploration with random
feature groupings for intra-sentential relations. The ini-
tial feature set is a slightly modified all in each case, and
the search was run 5 times, testing on devtest. The
ensemble system combines the 5 optimised feature sets
using the geometric mean probability.
Using a variant of the all feature set where the con-
text sizes in EntityContext were set to 5, a greedy
search for the best performing system was imple-
mented by first dividing the feature set randomly
into 50 groups, and at each iteration testing the
performance with each group added and removed
in turn. The search was iterated until no further
improvement in performance was obtained, where
25
performance was measured using bep. As for the
previous greedy feature optimisations, the relation
extractor was trained on train and tested on de-
vtest. The results for intra-sentential relations are
shown in Table 8, where the experiment was repeated
several times with different (randomly chosen) par-
titions. After performing the five random knockout
searches of the feature space, an ensemble system
was created for each relation type by training a sys-
tem with each feature set and combining the five by
taking the geometric mean of the probabilities. The
performance of the ensemble system is shown in the
final column of Table 8.
Comparing the results in Table 8 with the corre-
sponding results for intra-sentential relations in Ta-
ble 6, it can be seen that splitting the features into re-
lated groups works better than random groups. The
ensemble does not improve on the individual scores,
probably because the systems in the ensemble are
not diverse enough (Dietterich, 2000)
To see how well the best feature sets generalise to
unseen data, re systems were trained on train and
devtest combined, and tested on test using dif-
ferent feature sets; the baseline sets (base and all),
and the fully optimised set (best). In addition, to
ensure that the greedy feature optimisation was not
biasing the feature set towards the particular learner
employed (i.e. maxent), systems were also trained
and tested using svm. The maxent system had its
Gaussian prior optimised on the devtest set, whilst
svm was found to work best with a linear kernel, and
its cost factor was optimised on devtest. The value
of the decision function was used for thresholding the
svm model in order to calculate the bep. The com-
parison of all systems on test is shown in Table 9.
Corpus RelationType Learner
Feature Set
base all best
ppi ppi maxent 39.7 48.3 49.1
svm 39.6 49.2 49.9
ppi frag maxent 56.9 68.0 69.4
svm 54.9 68.2 69.5
te te maxent 39.0 47.9 46.8
svm 39.6 49.8 50.1
te frag maxent 60.1 63.4 68.9
svm 59.7 67.7 70.4
Table 9: The performance of the system trained on train
and devtest, and tested on test. Performance is com-
pared across the baseline feature sets (base and all) and
the optimised feature set (best) using each classifier.
The results in Table 9 show that, in general, both
classifiers perform better with the all feature set than
with the base feature set, and best of all with the
best feature set. The svm classifier preserves this
ordering throughout, and actually performs better
than the maxent classifier overall, even though the
features were optimised for maxent. For maxent,
the best model outperforms all in three out of four
cases, with the exception being te.
5 Conclusions
It has been shown that a relation extraction system
based on a supervised classifier and a large collection
of shallow linguistic features can be applied to three
different types of relations in two different biomedical
corpora. Automated feature optimisation produced
small gains in performance which were still apparent
on a blind test set. Even though a wrapper method
was used using a specific classifier (maxent), the
feature set optimisations were still valid for an svm
classifier.
Since the greedy search through feature space is
essentially a beam search with a beam size of one, it
could be extended by using a larger beam-size, run-
ning the feature set comparisons in parallel to reduce
total running time to a manageable size. Ad-hoc ex-
periments have suggested that better results could
be obtained by restarting the feature optimisation
in different positions, indicating that local optima
could be a problem, but a thorough investigation
of the search space nature has been left for future
work. Furthermore, the hyperparameter optimisa-
tion of the classifiers (for example the Gaussian prior
in maxent) could be incorporated into the search.
Whilst the relation extractor was successful on
intra-sentential relations, it is less successful on inter-
sentential relations, perhaps becuase of the lingusitic
complexity of these, and the sparsity of positive ex-
amples. The split into inter- and inter-sentential
examples in the current system seems justified as
they have quite different characteristic, but there
may also be a case for splitting the intra-sententials
further, into intra- and inter-clausals, as suggested
by Maslennikov and Chua (2007), and then treating
inter-clausals and inter-sententials together. Whilst
intra-clausals are more likely to use simple construc-
tions and be amenable to modelling with shallow lin-
guistic features, inter-sententials and inter-clausals
are more likely to use complex linguistic phenomena
such as corefereces.
Acknowledgements
This work was supported by the Text Mining Pro-
gramme of ITI Life Sciences Scotland (http://www.
itilifesciences.com).
26
References
Bea Alex, Claire Grover, Barry Haddow, Mijail Kabad-
jov, Ewan Klein, Michael Matthews, Stuart Roebuck,
Richard Tobin, and Xinglong Wang. 2008. The
ITI TXM Corpora: Tissue Expressions and Protein-
Protein Interactions. In Proceedings of LREC.
Razvan C. Bunescu and Raymond J. Mooney. 2007. Ex-
tracting relations from text: From word sequences to
dependency paths. In Anne Kao and Steve Poteet, ed-
itors, Text Mining and Natural Language Processing,
pages 29?44. Springer.
Razvan Bunescu, Ruifang Ge, Rohit J. Kate, Edward M.
Marcotte, Raymond J. Mooney, Arun K. Ramani,
and Yuk W. Wong. 2005. Comparative experiments
on learning information extractors for proteins and
their interactions. Artificial Intelligence in Medicine,
33(2):139?155.
James Curran and Stephen Clark. 2003. Language in-
dependent NER using a maximum entropy tagger. In
Proceedings of CoNLL.
James Cussens and Claire Ne?dellec, editors. 2005. Pro-
ceedings of Language Learning in Logic.
Thomas G. Dietterich. 2000. Ensemble methods in ma-
chine learning. Lecture Notes in Computer Science,
1857:1?15.
Gunes Erkan, Arzucan Ozgur, and Dragomir R. Radev.
2007. Semi-supervised classification for extracting pro-
tein interaction sentences using dependency parsing.
In Proceedings of EMNLP-CoNLL.
Kuzman Ganchev, Koby Crammer, Fernando Pereira,
Gideon Mann, Kedar Bellare, Andrew McCallum,
Steven Carroll, Yang Jin, and Peter White. 2007.
Penn/UMass/CHOP Biocreative II systems. In Pro-
ceedings of the Second BioCreative Challenge Evalua-
tion Workshop.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006. Exploiting shallow linguistic information for re-
lation extraction from biomedical literature. In Pro-
ceedings of EACL.
Claire Grover and Richard Tobin. 2006. Rule-based
chunking and reusability. In Proceedings of LREC.
Isabelle Guyon and Andre? Elisseeff. 2003. An intro-
duction to variable and feature selection. Journal of
Machine Learning Research, 3(Mar):1157?1182.
Jing Jiang and Chengxiang Zhai. 2007. A systematic
exploration of the feature space for relation extraction.
In Proceedings of NAACL.
Thorsten Joachims. 1999. Making large-scale support
vector machine learning practical. In Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
S. Katrenko and P. W. Adriaans. 2006. Learning rela-
tions from biomedical corpora using dependency tree
levels. In Proceedings of Benelearn.
Jin D. Kim, Tomoko Ohta, and Jun?ichi Tsujii. 2008.
Corpus annotation for mining biomedical events from
literature. BMC Bioinformatics, 9(1).
Martin Krallinger, Florian Leitner, Carlos Rodriguez-
Penagos, and Alfonso Valencia. 2008. Overview of
the protein-protein interaction annotation extraction
task of BioCreative II. Genome Biology (in press).
Mstislav Maslennikov and Tat S. Chua. 2007. A multi-
resolution framework for information extraction from
free text. In Proceedings of ACL.
Guido Minnen, John Carroll, and Darren Pearce. 2000.
Robust, applied morphological generation. In Proceed-
ings of INLG.
Leif Arda Nielsen. 2006. Extracting protein-protein in-
teractions using simple contextual features. In Pro-
ceedings of BioNLP.
Sampo Pyysalo, Filip Ginter, Juho Heimonen, Jari
Bjorne, Jorma Boberg, Jouni Jarvinen, and Tapio
Salakoski. 2007. Bioinfer: A corpus for information
extraction in the biomedical domain. BMC Bioinfor-
matics, 8(1).
Marios Skounakis, Mark Craven, and Soumya Ray. 2003.
Hierarchical hidden markov models for information ex-
traction. In Georg Gottlob, Toby Walsh, Georg Gott-
lob, and Toby Walsh, editors, Proceedings of IJCAI.
L. Smith, T. Rindflesch, and W. J. Wilbur. 2004. Med-
Post: a part-of-speech tagger for biomedical text.
Bioinformatics, 20(14):2320?2321.
Guodong Zhou, Min Zhang, Donghong Ji, and Qiaoming
Zhu. 2007. Tree kernel-based relation extraction with
context-sensitive structured parse tree information. In
Proceedings of EMNLP-CoNLL.
27
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 160?164,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
Edinburgh?s Submission to all Tracks of the WMT2009 Shared Task
with Reordering and Speed Improvements to Moses
Philipp Koehn and Barry Haddow
School of Informatics
University of Edinburgh
pkoehn@inf.ed.ac.uk bhaddow@inf.ed.ac.uk
Abstract
Edinburgh University participated in the
WMT 2009 shared task using the Moses
phrase-based statistical machine transla-
tion decoder, building systems for all lan-
guage pairs. The system configuration was
identical for all language pairs (with a few
additional components for the German-
English language pairs). This paper de-
scribes the configuration of the systems,
plus novel contributions to Moses includ-
ing truecasing, more efficient decoding
methods, and a framework to specify re-
ordering constraints.
1 Introduction
The commitment of the University of Edinburgh to
the WMT shared tasks is to provide a strong sta-
tistical machine translation baseline with our open
source tools for all language pairs. We are again
the only institution that participated in all tracks.
The shared task is also an opportunity to incor-
porate novel contributions and test them against
the best machine translation systems for these lan-
guage pairs. In this paper we describe the speed
improvements to the Moses decoder (Koehn et al,
2007), as well as a novel framework to specify re-
ordering constraints with XML markup, which we
tested with punctuation-based constraints.
2 System Configuration
We trained a default Moses system with the fol-
lowing non-default settings:
? maximum sentence length 80
? grow-diag-final-and symmetrization of
GIZA++ alignments
? interpolated Kneser-Ney discounted 5-gram
language model
? msd-bidrectional-fe lexicalized reordering
Language ep nc news intpl.
English 449 486 216 192
French 264 311 147 131
German 785 821 449 402
Spanish 341 392 219 190
Czech *:1475 1615 752 690
Hungarian hung:2148 815 786
Table 1: Perplexity (ppl) of the domain-trained (ep
= Europarl (CzEng for Czech), nc = News Com-
mentary, news = News) and interpolated language
models.
2.1 Domain Adaptation
In contrast to last year?s task, where news transla-
tion was presented as a true out-of-domain prob-
lem, this year large monolingual news corpora
and a tuning set (last year?s test set) were pro-
vided. While still no in-domain news parallel cor-
pora were made available, the monolingual cor-
pora could be exploited for domain adaption.
For all language pairs, we built a 5-gram lan-
guage model, by first training separate language
models for the different training corpora (the par-
allel Europarl and News Commentary and new
monolingual news), and then interpolated them by
optimizing perplexity on the provided tuning set.
Perplexity numbers are shown in Table 1.
2.2 Truecasing
Our traditional method to handle case is to low-
ercase all training data, and then have a separate
recasing (or recapitalization) step. Last year, we
used truecasing: all words are normalized to their
natural case, e.g. the, John, eBay, meaning that
only sentence-leading words may be changed to
their most frequent form.
To refine last year?s approach, we record the
seen truecased instances and truecase words in test
sentences (even in the middle of sentences) to seen
forms, if possible.
Truecasing leads to small degradation in case-
160
language pair baseline w/ news mbr/mp truecased big beam ued?08 best?08
French-English uncased 21.2 23.1 23.3 22.7 22.9 19.2 21.9
cased 21.7 21.6 21.8
English-French uncased 17.8 19.4 19.6 19.6 19.7 18.2 21.4
cased 18.1 18.7 18.8
Spanish-English uncased 22.5 24.4 24.7 24.5 24.7 20.1 22.9
cased 23.0 23.3 23.4
English-Spanish uncased 22.4 23.9 24.2 23.8 24.4 20.7 22.7
cased 22.1 22.8 23.1
Czech-English uncased 16.9 18.9 18.9 18.6 18.6 14.5 14.7
cased 17.3 17.4 17.4
English-Czech uncased 11.4 13.5 13.6 13.6 13.8 9.6 11.9
cased 12.2 13.0 13.2
Hungarian-English uncased - 11.3 11.4 10.9 11.0 8.8
cased 8.3 10.1 10.2
English-Hungarian uncased - 9.0 9.3 9.2 9.5 6.5
cased 8.1 8.4 8.7
Table 2: Results overview for news-dev2009b sets: We see significant BLEU score increases with the
addition of news data to the language model and using truecasing. As a comparison our results and the
best systems from last year on the full news-dev2009 set are shown.
insensitive BLEU, but to a significant gain in case-
sensitive BLEU. Note that we still do not properly
address all-caps portions or headlines with our ap-
proach.
2.3 Results
Results on the development sets are summarized
in Table 2. We see significant gains with the addi-
tion of news data to the language model (about 2
BLEU points) and using truecasing (about 0.5?1.0
BLEU points), and minor if any gains using min-
imum Bayes risk decoding (mbr), the monotone-
at-punctuation reordering constraint (mp, see Sec-
tion 3.2), and bigger beam sizes.
2.4 German?English
For German?English, we additionally incorpo-
rated
rule-based reordering ? We parse the input us-
ing the Collins parser (Collins, 1997) and ap-
ply a set of reordering rules to re-arrange the
German sentence so that it corresponds more
closely English word order (Collins et al,
2005).
compound splitting ? We split German com-
pound words (mostly nouns), based on the
frequency of the words in the potential de-
compositions (Koehn and Knight, 2003a).
part-of-speech language model ? We use fac-
tored translation models (Koehn and Hoang,
2007) to also output part-of-speech tags with
each word in a single phrase mapping and run
a second n-gram model over them. The En-
German?English BLEU
(ued?08: 17.1, best?08: 19.7) (uncased)
baseline 16.6
+ interpolated news LM 20.6
+ minimum Bayes risk decoding 20.6
+ monotone at punctuation 20.9
+ truecasing 20.9
+ rule-based reordering 21.7
+ compound splitting 22.0
+ part-of-speech LM 22.1
+ big beam 22.3
Table 3: Results for German?English with the in-
cremental addition of methods beyond a baseline
trained on the parallel corpus
English?German BLEU
(ued?08: 12.1, best?08: 14.2) (uncased)
baseline 13.5
+ interpolated news LM 15.2
+ minimum Bayes risk decoding 15.2
+ monotone at punctuation 15.2
+ truecasing 15.2
+ morphological LM 15.2
+ big beam 15.7
Table 4: Results for English?German with the in-
cremental addition of methods beyiond a baseline
trained on the parallel corpus
glish part-of-speech tags are obtained using
MXPOST (Ratnaparkhi, 1996).
2.5 English-German
For English?German, we additionally incorpo-
rated a morphological language model the same
way we incorporated a part-of-speech language
model in the other translation direction. The
morphological tags were obtained using LoPar
(Schmidt and Schulte im Walde, 2000).
161
&
German-English & French-English
Figure 1: Early discarding results in speedier but still accurate search, compared to reducing stack size.
3 Recent Improvements
In this section, we describe recent improvements
to the Moses decoder for the WMT 2009 shared
task.
3.1 Early Discarding
We implemented in Moses a more efficient beam
search, following suggestions by Moore and Quirk
(2007). In short, the guiding principle of this work
is not to build a hypothesis and not to compute its
language model scores, if it is likely to be too bad
anyway.
Before a hypothesis is generated, the following
checks are employed:
1. the minimum allowed score for a hypothesis
is the worst score on the stack (if full) or the
threshold for the stack (if higher or stack not
full) plus an early discarding threshold cush-
ion
2. if (a) new hypothesis future score, (b) the cur-
rent hypothesis actual score, and (c) the fu-
ture cost of the translation option are worse
than the allowed score, do not generate the
hypothesis
3. if adding all real costs except for the language
model costs (i.e., reordering costs) makes the
score worse than the allowed score, do not
generate the hypothesis.
4. complete generation of the hypothesis and
add it to the stack
Note that check 1 and 2 mostly consists of
adding and comparing already computed values.
In our implementation, step 3 implies the some-
what costly construction of the hypothesis data
structure, while step 4 performs the expensive
language model calculation. Without these opti-
mizations, the decoder spends about 60-70% of
the search time computing language model scores.
With these optimization, the vast majority of po-
tential hypotheses are not built.
See Figure 1 for the time/search-accuracy trade-
offs using this early discarding strategy. Given
a stack size, we can vary the threshold cushion
mentioned in step 1 above. A tighter threshold
(the factor 1.0 implies no cushion at all), results
in speedier but worse search. Note, however, that
the degradation in quality for a given time point
is less severe than the alternative ? reducing the
stack size (and also tightening the beam thresh-
old, not shown in the figure). To mention just two
data points in the German-English setting: Stack
size of 500 and early discarding threshold of 1.0
results in faster search (150ms/word) and better
quality (73.5% search accuracy) than the default
search setting of a stack size 200 and no early dis-
carding (252ms/word for 62.5% seach accuracy).
Accuracy is measured against the best translations
found under any setting.
Note that this early discarding is related to ideas
behind cube pruning (Huang and Chiang, 2007),
which generates the top n most promising hy-
potheses, but in our method the decision not to
generate hypotheses is guided by the quality of hy-
potheses on the result stack.
3.2 Framework to Specify Reordering
Constraints
Commonly in statistical machine translation,
punctuation tokens are treated just like words. For
tokens such as commas, many possible transla-
tions are collected and they may be translated into
any of these choices or reordered if the language
model sees gains. In fact, since the comma is one
162
'&
$
%
Requiring the translation of quoted material as a block:
He said <zone> " yes " </zone> .
Hard reordering constraint:
Number 1 : <wall/> the beginning .
Local hard reordering constraint within zone:
A new idea <zone> ( <wall/> maybe not new <wall/> ) </zone> has come forward .
Nesting:
The <zone> " new <zone> ( old ) </zone> " </zone> proposal .
Figure 2: Framework to specify reordering constraints with zones and walls. Words within zones have
to be translated without reordering with outside material. Walls form hard reordering constraints, over
which words may not be reordered (limited to zones, if defined within them).
the most frequent tokens in a corpus and not very
consistently translated across languages, it has a
very noisy translation table, often with 10,000s if
not 100,000s of translations.
Punctuation has a meaningful role in structur-
ing a sentence, and we see some gains exploiting
this in the systems we built last year. By dis-
allowing reordering over commas and sentence-
ending punctuation, we avoid mixing words from
different clauses, and typically see gains of 0.1?
0.2 BLEU.
But also other punctuation tokens imply re-
ordering constraints. Parentheses, brackets, and
quotation marks typically define units that should
be translated as blocks, meaning that words should
not be moved in or out of sequences in quotes and
alike.
To handle such reordering constraints, we intro-
duced a framework that uses what we call zones
and walls. A zone is a sequence of words that
should be translated as block. This does not mean
that the sequence cannot be reordered as a whole,
but that once we start to translate words in a zone,
we have to finish all its words before moving out-
side again. To put it another way: words may not
reordered into or out of zones.
A wall is a hard reordering constraint that re-
quires that all words preceeding it have to be trans-
lated before words after may be translated. If we
specify walls within zones, then we consider them
local walls where the before-mentioned constraint
only applies within the zone.
Walls and zones may be specified with XML
markup to the Moses decoder. See Figure 2 for a
few examples. We use the extended XML frame-
work to
1. limit reordering of clause-ending punctuation
(walls)
2. define zones for quoted and parenthetical
word sequences
3. limit reordering of quotes and parentheses
(local walls within zones)
4. specify translations for punctuation (not
comma).
Only (1) leads to any noticable change in BLEU in
the WMT 2009 shared task, a slight gain 0.1?0.2.
Note that this framework may be used in other
ways. For instance, we may want to revisit
our work on noun phrase translation (Koehn and
Knight, 2003b), and check if enforcing the trans-
lation of noun phrases as blocks is beneficial
or harmful to overall machine translation perfor-
mance.
Acknowledgements
This work was supported by the EuroMatrix
project funded by the European Commission (6th
Framework Programme) and made use of the re-
sources provided by the Edinburgh Compute and
Data Facility (http://www.ecdf.ed.ac.uk/).
The ECDF is partially supported by the eDIKT
initiative (http://www.edikt.org.uk/).
References
Collins, M. (1997). Three generative, lexicalized
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association of
Computational Linguistics (ACL).
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
163
Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Ar-
bor, Michigan. Association for Computational
Linguistics.
Huang, L. and Chiang, D. (2007). Forest rescor-
ing: Faster decoding with integrated language
models. In Proceedings of the 45th Annual
Meeting of the Association of Computational
Linguistics, pages 144?151, Prague, Czech Re-
public. Association for Computational Linguis-
tics.
Koehn, P. and Hoang, H. (2007). Factored trans-
lation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 868?876.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B., Shen,
W., Moran, C., Zens, R., Dyer, C. J., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th
Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
Koehn, P. and Knight, K. (2003a). Empirical
methods for compound splitting. In Proceed-
ings of Meeting of the European Chapter of
the Association of Computational Linguistics
(EACL).
Koehn, P. and Knight, K. (2003b). Feature-rich
translation of noun phrases. In 41st Annual
Meeting of the Association of Computational
Linguistics (ACL).
Moore, R. C. and Quirk, C. (2007). Faster beam-
search decoding for phrasal statistical machine
translation. In Proceedings of the MT Summit
XI.
Ratnaparkhi, A. (1996). A maximum entropy part-
of-speech tagger. In Proceedings of the Empir-
ical Methods in Natural Language Processing
Conference.
Schmidt, H. and Schulte im Walde, S. (2000). Ro-
bust German noun chunking with a probabilistic
context-free grammar. In Proceedings of the In-
ternational Conference on Computational Lin-
guistics (COLING).
164
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 102?110,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Monte Carlo inference and maximization for phrase-based translation
Abhishek Arun?
a.arun@sms.ed.ac.uk
Phil Blunsom?
pblunsom@inf.ed.ac.uk
Chris Dyer?
redpony@umd.edu
Adam Lopez?
alopez@inf.ed.ac.uk
Barry Haddow?
bhaddow@inf.ed.ac.uk
Philipp Koehn?
pkoehn@inf.ed.ac.uk
?Department of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
?Department of Linguistics
University of Maryland
College Park, MD 20742, USA
Abstract
Recent advances in statistical machine
translation have used beam search for
approximate NP-complete inference within
probabilistic translation models. We present
an alternative approach of sampling from the
posterior distribution defined by a translation
model. We define a novel Gibbs sampler
for sampling translations given a source
sentence and show that it effectively explores
this posterior distribution. In doing so
we overcome the limitations of heuristic
beam search and obtain theoretically sound
solutions to inference problems such as
finding the maximum probability translation
and minimum expected risk training and
decoding.
1 Introduction
Statistical machine translation (SMT) poses the
problem: given a foreign sentence f , find the
translation e? that maximises the conditional
posterior probability p(e|f). This probabilistic
formulation of translation has driven development
of state-of-the-art systems which are able to learn
from parallel corpora which were generated for
other purposes ? a direct result of employing a
mathematical framework that we can reason about
independently of any particular model.
For example, we can train SMT models using
maximum likelihood estimation (Brown et al, 1993;
Och and Ney, 2000; Marcu and Wong, 2002). Alter-
natively, we can train to minimise probabilistic con-
ceptions of risk (expected loss) with respect to trans-
lation metrics, thereby obtaining better results for
those metrics (Kumar and Byrne, 2004; Smith and
Eisner, 2006; Zens and Ney, 2007). We can also use
Bayesian inference techniques to avoid resorting to
heuristics that damage the probabilistic interpreta-
tion of the models (Zhang et al, 2008; DeNero et
al., 2008; Blunsom et al, 2009).
Most models define multiple derivations for each
translation; the probability of a translation is thus
the sum over all of its derivations. Unfortunately,
finding the maximum probability translation is NP-
hard for all but the most trivial of models in this
setting (Sima?an, 1996). It is thus necessary to resort
to approximations for this sum and the search for its
maximum e?.
The most common of these approximations is
the max-derivation approximation, which for many
models can be computed in polynomial time via
dynamic programming (DP). Though effective for
some problems, it has many serious drawbacks for
probabilistic inference:
1. It typically differs from the true model maxi-
mum.
2. It often requires additional approximations in
search, leading to further error.
3. It introduces restrictions on models, such as
use of only local features.
4. It provides no good solution to compute the
normalization factor Z(f) required by many prob-
abilistic algorithms.
In this work, we solve these problems using a
Monte Carlo technique with none of the above draw-
backs. Our technique is based on a novel Gibbs
sampler that draws samples from the posterior dis-
tribution of a phrase-based translation model (Koehn
et al, 2003) but operates in linear time with respect
to the number of input words (Section 2). We show
102
that it is effective for both decoding (Section 3) and
minimum risk training (Section 4).
2 A Gibbs sampler for phrase-based
translation models
We begin by assuming a phrase-based translation
model in which the input sentence, f , is segmented
into phrases, which are sequences of adjacent
words.1 Each foreign phrase is translated into the
target language, to produce an output sentence e
and an alignment a representing the mapping from
source to target phrases. Phrases are allowed to be
reordered during translation.
The model is defined with a log-linear form,
with feature function vector h and parametrised by
weight vector ?, as described in Koehn et al (2003).
P (e, a|f ;?) = exp [? ? h(e, a, f)]?
?e?,a?? exp [? ? h(e?, a?, f)]
(1)
The features h of the model are usually few and
are themselves typically probabilistic models
indicating e.g, the relative frequency of a target
phrase translation given a source phrase (translation
model), the fluency of the target phrase (language
model) and how phrases reorder with respect
to adjacent phrases (reordering model). There
is a further parameter ? that limits how many
source language words may intervene between
two adjacent target language phrases. For the
experiments in this paper, we use ? = 6.
2.1 Gibbs sampling
We use Markov chain Monte Carlo (MCMC) as an
alternative to DP search (Geman and Geman, 1984;
Metropolis and Ulam, 1949). MCMC probabilis-
tically generates sample derivations from the com-
plete search space. The probability of generating
each sample is conditioned on the previous sam-
ple, forming a Markov chain. After a long enough
interval (referred to as the burn-in) this chain returns
samples from the desired distribution.
Our MCMC sampler uses Gibbs sampling, which
obtains samples from the joint distribution of a set
of random variables X = {X1, . . . , Xn}. It starts
with some initial state (X1 = x10, . . . , Xn = xn0),
and generates a Markov chain of samples, where
1These phrases are not necessarily linguistically motivated.
each sample is the result of applying a set of Gibbs
operators to the previous sample. Each operator is
defined by specifying a subset of the random vari-
ables Y ? X , which the operator updates by sam-
pling from the conditional distribution P (Y |X \Y ).
The set X \ Y is referred to as the Markov blanket
and is unchanged by the operator.
In the case of translation, we require a Gibbs sam-
pler that produces a sequence of samples, SN1 =
(e1, a1) . . . (eN , aN ), that are drawn from the dis-
tribution P (e, a|f). These samples can thus be used
to estimate the expectation of a function h(e, a, f)
under the distribution as follows:
EP (a,e|f)[h] = limN??
1
N
N?
i=1
h(ai, ei, f) (2)
Taking h to be an indicator function
h = ?(a, a?)?(e, e?) provides an estimate of
P (a?, e?|f), and using h = ?(e, e?) marginalises over
all derivations a?, yielding an estimate of P (e?|f).
2.2 Gibbs operators
Our sampler consists of three operators. Examples
of these are depicted in Figure 1.
The RETRANS operator varies the translation of a
single source phrase. Segmentation, alignment, and
all other translations are held constant.
The MERGE-SPLIT operator varies the source
segmentation at a single word boundary. If the
boundary is a split point in the current hypothesis,
the adjoining phrases can be merged, provided
that the corresponding target phrases are adjacent
and the phrase table contains a translation of the
merged phrase. If the boundary is not a split point,
the covering phrase may be split, provided that
the phrase table contains a translation of both new
phrases. Remaining segmentation points, phrase
alignment and phrase translations are held constant.
The REORDER operator varies the target phrase
order for a pair of source phrases, provided that
the new alignment does not violate reordering limit
?. Segmentation, phrase translations, and all other
alignments are held constant.
To illustrate the RETRANS operator, we will
assume a simplified model with two features: a
bigram language model Plm and a translation model
Ptm. Both features are assigned a weight of 1.
103
c?est un re?sultat remarquable
it is some result remarkable
(a)
Initial
c?est un re?sultat remarquable
but some result remarkable
(b)
Retrans
c?est un re?sultat remarquable
it is a result remarkable
(c)
Merge
c?est un re?sultat remarquable
it is a remarkable result
(d)
Reorder
1
Figure 1: Example evolution of an initial hypothesis via
application of several operators, with Markov blanket
indicated by shading.
We denote the start of the sentence with S and the
language model context with C. Assuming the
French phrase c?est can be translated either as it is or
but, the RETRANS operator at step (b) stochastically
chooses an English phrase, e? in proportion to the
phrases? conditional probabilities.
P (but|c?est,C) = Ptm(but|c?est) ? Plm(S but some)Z
and
P (it is|c?est,C) = Ptm(it is|c?est) ? Plm(S it is some)Z
where
Z = Ptm(but|c?est) ? Plm(S but some) +
Ptm(it is|c?est) ? Plm(S it is some)
Conditional distributions for the MERGE-SPLIT and
REORDER operators can be derived in an analogous
fashion.
A complete iteration of the sampler consists of
applying each operator at each possible point in the
sentence, and a sample is collected after each opera-
tor has performed a complete pass.
2.3 Algorithmic complexity
Since both the RETRANS and MERGE-SPLIT oper-
ators are applied by iterating over source side word
positions, their complexity is linear in the size of the
input.
The REORDER operator iterates over the positions
in the input and for the source phrase found at that
position considers swapping its target phrase with
that of every other source phrase, provided that the
reordering limit is not violated. This means that it
can only consider swaps within a fixed-length win-
dow, so complexity is linear in sentence length.
2.4 Experimental verification
To verify that our sampler was behaving as expected,
we computed the KL divergence between its
inferred distribution q?(e|f) and the true distribution
over a single sentence (Figure 2). We computed
the true posterior distribution p(e|f) under an
Arabic-English phrase-based translation model
with parameters trained to maximise expected
BLEU (Section 4), summing out the derivations for
identical translations and computing the partition
term Z(f). As the number of iterations increases,
the KL divergence between the distributions
approaches zero.
3 Decoding
The task of decoding amounts to finding the single
translation e? that maximises or minimises some cri-
terion given a source sentence f . In this section
we consider three common approaches to decod-
ing, maximum translation (MaxTrans), maximum
derivation (MaxDeriv), and minimum risk decoding
(MinRisk):
e? =
?
?
?
argmax(e,a) p(e, a|f) (MaxDeriv)
argmaxe p(e|f) (MaxTrans)
argmine?e? `e?(e)p(e?|f) (MinRisk)
In the minimum risk decoder, `e?(e) is any real-
valued loss (error) function that computes the error
of one hypothesis e with respect to some reference
e?. Our loss is a sentence-level approximation of
(1 ? BLEU).
As noted in section 2, the Gibbs sampler can
be used to provide an estimate of the probability
distribution P (a, e|f) and therefore to determine
the maximum of this distribution, in other words
the most likely derivation. Furthermore, we can
marginalise over the alignments to estimate P (e|f)
104
Iterations
KL d
iverg
ence
l l l
l l
l
l l
l
l
l
l
l l l
l l
10 100 1000 10000 100000 1000000
0.00
1
0.01
0.1
KL Divergence
Figure 2: The KL divergence of the true posterior distri-
bution and the distribution estimated by the Gibbs sam-
pler at different numbers of iterations for the Arabic
source sentence r}ys wzrA? mAlyzyA yzwr Alflbyn (in
English, The prime minister of Malaysia visits the Philip-
pines).
and so obtain the most likely translation. The Gibbs
sampler can therefore be used as a decoder, either
running in max-derivation and max-translation
mode. Using the Gibbs sampler in this way makes
max-translation decoding tractable, and so will
help determine whether max-translation offers any
benefit over the usual max-derivation. Using the
Gibbs sampler as a decoder also allows us to verify
that it is producing valid samples from the desired
distribution.
3.1 Training data and preparation.
The experiments in this section were performed
using the French-English and German-English
parallel corpora from the WMT09 shared translation
task (Callison-Burch et al, 2009), as well as 300k
parallel Arabic-English sentences from the NIST
MT evaluation training data.2 For all language
pairs, we constructed a phrase-based translation
model as described in Koehn et al (2003), limiting
the phrase length to 5. The target side of the parallel
corpus was used to train a 3-gram language model.
2The Arabic-English training data consists of the
eTIRR corpus (LDC2004E72), the Arabic news corpus
(LDC2004T17), the Ummah corpus (LDC2004T18), and the
sentences with confidence c > 0.995 in the ISI automatically
extracted web parallel corpus (LDC2006T02).
For the German and French systems, the DEV2006
set was used for model tuning and the TEST2007
(in-domain) and NEWS-DEV2009B (out-of-domain)
sets for testing. For the Arabic system, the MT02
set (10 reference translations) was used for tuning
and MT03 (4 reference translations) was used for
evaluation. To reduce the size of the phrase table,
we used the association-score technique suggested
by Johnson et al (2007a). Translation quality is
reported using case-insensitive BLEU (Papineni et
al., 2002).
3.2 Translation performance
For the experiments reported in this section, we
used feature weights trained with minimum error
rate training (MERT; Och, 2003) . Because MERT
ignores the denominator in Equation 1, it is invari-
ant with respect to the scale of the weight vector
? ? the Moses implementation simply normalises
the weight vector it finds by its `1-norm. However,
when we use these weights in a true probabilistic
model, the scaling factor affects the behaviour of
the model since it determines how peaked or flat the
distribution is. If the scaling factor is too small, then
the distribution is too flat and the sampler spends
too much time exploring unimportant probability
regions. If it is too large, then the distribution is too
peaked and the sampler may concentrate on a very
narrow probability region. We optimised the scaling
factor on a 200-sentence portion of the tuning set,
finding that a multiplicative factor of 10 worked best
for fr-en and a multiplicative factor of 6 for de-en. 3
The first experiment shows the effect of different
initialisations and numbers of sampler iterations on
max-derivation decoding performance of the sam-
pler. The Moses decoder (Koehn et al, 2007) was
used to generate the starting hypothesis, either in
full DP max-derivation mode, or alternatively with
restrictions on the features and reordering, or with
zero weights to simulate a random initialisation, and
the number of iterations varied from 100 to 200,000,
with a 100 iteration burn-in in each case. Figure 3
shows the variation of model score with sampler iter-
ation, for the different starting points, and for both
language pairs.
3We experimented with annealing, where the scale factor is
gradually increased to sharpen the distribution while sampling.
However, we found no improvements with annealing.
105
?
20.1
?
20.0
?
19.9
?
19.8
?
19.7
?
19.6
Iterations
Mod
el sc
ore
100 1000 10000
French?English
Initialisationfull
mono
nolm
zero
?
40.6
?
40.4
?
40.2
?
40.0
?
39.8
Iterations
Mod
el sc
ore
100 1000 10000 100000
German?English
Initialisationfull
mono
nolm
zero
Figure 3: Mean maximum model score, as a function of iteration number and starting point. The starting point can
either be the full max-derivation translation (full), the monotone translation (mono), the monotone translation with no
language model (nolm) or the monotone translation with all weights set to zero (zero).
Comparing the best model scores found by the
sampler, with those found by the Moses decoder
with its default settings, we found that around
50,000 sampling iterations were required for
fr-en and 100,000 for de-en, for the sampler to
give equivalent model scores to Moses. From
Figure 3 we can see that the starting point did not
have an appreciable effect on the model score of
the best derivation, except with low numbers of
iterations. This indicates that the sampler is able
to move fairly quickly towards the maximum of
the distribution from any starting point, in other
words it has good mobility. Running the sampler
for 100,000 iterations took on average 1670 seconds
per sentence on the French-English data set and
1552 seconds per sentence on German-English.
A further indication of the dependence of sampler
accuracy on the iteration count is provided by Fig-
ure 4. In this graph, we show the mean Spearman?s
rank correlation between the nbest lists of deriva-
tions when ranked by (i) model score and (ii) the
posterior probability estimated by the sampler. This
measure of sampler accuracy also shows a logarith-
mic dependence on the sample size.
3.3 Minimum risk decoding
The sampler also allows us to perform minimum
Bayes risk (MBR) decoding, a technique introduced
by Kumar and Byrne (2004). In their work, as an
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Iterations
Corr
elati
on
100 1000 10000 100000
Language Pairsfr?ende?en
Figure 4: Mean Spearman?s rank correlation of 1000-best
list of derivations ranked according to (i) model score and
(ii) posterior probability estimated by sampler. This was
measured on a 200 sentence subset of DEV2006.
approximation of the model probability distribution,
the expected loss of the decoder is calculated by
summing over an n-best list. With the Gibbs sam-
pler, however, we should be able to obtain a much
more accurate view of the model probability distri-
bution. In order to compare max-translation, max-
derivation and MBR decoding with the Gibbs sam-
pler, and the Moses baseline, we ran experiments
106
fr-en de-en
in out in out
Moses 32.7 19.1 27.4 15.9
MaxD 32.6 19.1 27.0 15.5
MaxT 32.6 19.1 27.4 16.0
MBR 32.6 19.2 27.3 16.0
Table 1: Comparison of the BLEU score of the Moses
decoder with the sampler running in max-derivation
(MaxD), max-translation (MaxT) and minumum Bayes
risk (MBR) modes. The test sets are TEST2007 (in) and
NEWS-DEV2009B (out)
on both European language pairs, using both the in-
domain and out-of-domain test sets. The sampler
was initialised with the output of Moses with the
feature weights set to zero and restricted to mono-
tone, and run for 100,000 iterations with a 100 iter-
ation burn-in. The scale factors were set to the same
values as in the previous experiment. The relative
translation quality (measured according to BLEU) is
shown in Table 1.
3.4 Discussion
These results show very little difference between the
decoding methods, indicating that the Gibbs sam-
pling decoder can perform as well as a standard DP
based max-derivation decoder with these models,
and that there is no gain from doing max-translation
or MBR decoding. However it should be noted that
the model used for these experiments was optimised
by MERT, for max-derivation decoding, and so the
experiments do not rule out the possibility that max-
translation and MBR decoding will offer an advan-
tage on an appropriately optimised model.
4 Minimum risk training
In the previous section, we described how our sam-
pler can be used to search for the best translation
under a variety of decoding criteria (max deriva-
tion, translation, and minimum risk). However, there
appeared to be little benefit to marginalizing over
the latent derivations. This is almost certainly a side
effect of the MERT training approach that was used
to construct the models so as to maximise the per-
formance of the model on its single best derivation,
without regard to the shape of the rest of the dis-
tribution (Blunsom et al, 2008). In this section we
describe a further application of the Gibbs sampler:
to do unbiased minimum risk training.
While there have been at least two previous
attempts to do minimum risk training for MT, both
approaches relied on biased k-best approximations
(Smith and Eisner, 2006; Zens and Ney, 2007).
Since we sample from the whole distribution, we
will have a more accurate risk assessment.
The risk, or expected loss, of a probabilistic trans-
lation model on a corpus D, defined with respect to
a particular loss function `e?(e), where e? is the refer-
ence translation and e is a hypothesis translation
L = ?
?e?,f??D
?
e
p(e|f)`e?(e) (3)
This value can be trivially computed using equa-
tion (2). In this section, we are concerned with find-
ing the parameters ? that minimise (3). Fortunately,
with the log-linear parameterization of p(e|f), L is
differentiable with respect to ?:
?L
??k
= ?
?e?,f??D
?
e
p(e|f)`e?(e)
(
hk ? Ep(e|f)[hk]
)
(4)
Equation (4) is slightly more complicated to com-
pute using the sampler since it requires the feature
expectation in order to evaluate the final term. How-
ever, this can be done simply by making two passes
over the samples, computing the feature expecta-
tions on the first pass and the gradient on the second.
We have now shown how to compute our
objective (3), the expected loss, and a gradient
with respect to the model parameters we want
to optimise, (4), so we can use any standard
first-order optimization technique. Since the
sampler introduces stochasticity into the gradient
and objective, we use stochastic gradient descent
methods which are more robust to noise than
more sophisticated quasi-Newtonian methods
like L-BFGS (Schraudolph et al, 2007). For the
experiments below, we updated the learning rate
after each step proportionally to difference in
successive gradients (Schraudolph, 1999).
For the experiments reported in this section, we
used sample sizes of 8000 and estimated the gradi-
ent on sets of 100 sentences drawn randomly (with
replacement) from the development corpus. For a
107
Training Decoder MT03
Moses Max Derivation 44.6
MERT Moses MBR 44.8
Gibbs MBR 44.9
Moses Max Derivation 40.6
MinRisk MaxTrans 41.8
Gibbs MBR 42.9
Table 2: Decoding with minimum risk trained systems,
compared with decoding with MERT-trained systems on
Arabic to English MT03 data
loss function we use 4-gram (1 ? BLEU) computed
individually for each sentence4. By examining per-
formance on held-out data, we find the model con-
verges typically in fewer than 20 iterations.
4.1 Training experiments
During preliminary experiments with training, we
observed on a held-out data set (portions of MT04)
that the magnitude of the weights vector increased
steadily (effectively sharpening the distribution), but
without any obvious change in the objective. Since
this resulted in poor generalization we added a reg-
ularization term of ||~? ? ~?||2/2?2 to L. We initially
set the means to zero, but after further observing that
the translations under all decoding criteria tended to
be shorter than the reference (causing a significant
drop in performance when evaluated using BLEU),
we found that performance could be improved by
setting ?WP = ?0.5, indicating a preference for a
lower weight on this parameter.
Table 2 compares the performance on Arabic to
English translation of systems tuned with MERT
(maximizing corpus BLEU) with systems tuned to
maximise expected sentence-level BLEU. Although
the performance of the minimum risk model under
all decoding criteria is lower than that of the orig-
inal MERT model, we note that the positive effect
of marginalizing over derivations as well as using
minimum risk decoding for obtaining good results
on this model. A full exploration of minimum risk
training is beyond the scope of this paper, but these
initial experiments should help emphasise the versa-
tility of the sampler and its utility in solving a variety
of problems. In the conclusion, we will, however,
4The ngram precision counts are smoothed by adding 0.01
for n > 1
discuss some possible future directions that can be
taken to make this style of training more competitive
with standard baseline systems.
5 Discussion and future work
We have described an algorithmic technique that
solves certain problems, but also verifies the utility
of standard approximation techniques. For exam-
ple, we found that on standard test sets the sampler
performs similarly to the DP max-derivation solu-
tion and equally well regardless of how it is ini-
tialised. From this we conclude that at least for
MERT-trained models, the max-derivation approx-
imation is adequate for finding the best translation.
Although the training approach presented in
Section 4 has a number of theoretical advantages,
its performance in a one-best evaluation falls short
when compared with a system tuned for optimal
one-best performance using MERT. This contradicts
the results of Zens and Ney (2007), who optimise
the same objective and report improvements over a
MERT baseline. We conjecture that the difference
is due to the biased k-best approximation they used.
By considering only the most probable derivations,
they optimise a smoothed error surface (as one
does in minimum risk training), but not one that
is indicative of the true risk. If our hypothesis
is accurate, then the advantage is accidental and
ultimately a liability. Our results are in line with
those reported by Smith and Eisner (2006) who
find degradation in performance when minimizing
risk, but compensate by ?sharpening? the model
distribution for the final training iterations,
effectively maximising one-best performance
rather minimising risk over the full distribution
defined by their model. In future work, we will
explore possibilities for artificially sharpening the
distribution during training so as to better anticipate
the one-best evaluation conditions typical of MT.
However, for applications which truly do require a
distribution over translations, such as re-ranking,
our method for minimising expected risk would be
the objective of choice.
Using sampling for model induction has two fur-
ther advantages that we intend to explore. First,
although MERT performs quite well on models with
108
small numbers of features (such as those we consid-
ered in this paper), in general the algorithm severely
limits the number of features that can be used since
it does not use gradient-based updates during opti-
mization, instead updating one feature at a time. Our
training method (Section 4) does not have this limi-
tation, so it can use many more features.
Finally, for the DP-based max-derivation approx-
imation to be computationally efficient, the features
characterizing the steps in the derivation must be
either computable independently of each other or
with only limited local context (as in the case of the
language model or distortion costs). This has led to
a situation where entire classes of potentially use-
ful features are not considered because they would
be impractical to integrate into a DP based trans-
lation system. With the sampler this restriction is
mitigated: any function of h(e, f, a) may partici-
pate in the translation model subject only to its own
computability. Freed from the rusty manacles of
dynamic programming, we anticipate development
of many useful features.
6 Related work
Our sampler is similar to the decoder of Germann
et al (2001), which starts with an approximate solu-
tion and then incrementally improves it via operators
such as RETRANS and MERGE-SPLIT. It is also
similar to the estimator of Marcu and Wong (2002),
who employ the same operators to search the align-
ment space from a heuristic initialisation. Although
the operators are similar, the use is different. These
previous efforts employed their operators in a greedy
hill-climbing search. In contrast, our operators are
applied probabilistically, making them theoretically
well-founded for a variety of inference problems.
Our use of Gibbs sampling follows from its
increasing use in Bayesian inference problems in
NLP (Finkel et al, 2006; Johnson et al, 2007b).
Most closely related is the work of DeNero
et al (2008), who derive a Gibbs sampler for
phrase-based alignment, using it to infer phrase
translation probabilities. The use of Monte Carlo
techniques to calculate posteriors is similar to that
of Chappelier and Rajman (2000) who use those
techniques to find the best parse under models where
the derivation and the parse are not isomorphic.
To our knowledge, we are the first to apply Monte
Carlo methods to maximum translation and mini-
mum risk translation. Approaches to the former
(Blunsom et al, 2008; May and Knight, 2006) rely
on dynamic programming techniques which do not
scale well without heuristic approximations, while
approaches to the latter (Smith and Eisner, 2006;
Zens et al, 2007) use biased k-best approximations.
7 Conclusion
We have described a Gibbs sampler for approxi-
mating two intractable problems in SMT: maximum
translation decoding (and its variant, minimum risk
decoding) and minimum risk training. By using
Monte Carlo techniques we avoid the biases associ-
ated with the more commonly used DP based max-
derivation (or k-best derivation) approximation. In
doing so we provide a further tool to the translation
community that we envision will allow the devel-
opment and analysis of increasing theoretically well
motivated techniques.
Acknowledgments
This research was supported in part by the GALE
program of the Defense Advanced Research Projects
Agency, Contract No. HR0011-06-2-001; and by the
EuroMatrix project funded by the European Commission
(6th Framework Programme). The project made use of
the resources provided by the Edinburgh Compute and
Data Facility (http://www.ecdf.ed.ac.uk/).
The ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
References
P. Blunsom, T. Cohn, and M. Osborne. 2008. A discrim-
inative latent variable model for statistical machine
translation. In Proc. of ACL-HLT.
P. Blunsom, T. Cohn, and M. Osborne. 2009. Bayesian
synchronous grammar induction. In Advances in Neu-
ral Information Processing Systems 21, pages 161?
168.
P. F. Brown, V. J. Della Pietra, S. A. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder,
editors. 2009. Proc. of Workshop on Machine Trans-
lations, Athens.
J.-C. Chappelier and M. Rajman. 2000. Monte-Carlo
sampling for NP-hard maximization problems in the
109
framework of weighted parsing. In Natural Language
Processing ? NLP 2000, number 1835 in Lecture Notes
in Artificial Intelligence, pages 106?117. Springer.
J. DeNero, A. Bouchard, and D. Klein. 2008. Sam-
pling alignment structure under a Bayesian translation
model. In Proc. of EMNLP.
J. R. Finkel, C. D. Manning, and A. Y. Ng. 2006. Solv-
ing the problem of cascading errors: Approximate
bayesian inference for linguistic annotation pipelines.
In Proc. of EMNLP.
S. Geman and D. Geman. 1984. Stochastic relaxation,
Gibbs distributions and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
U. Germann, M. Jahr, K. Knight, D. Marcu, and
K. Yamada. 2001. Fast decoding and optimal decod-
ing for machine translation. In Proceedings of ACL.
Association for Computational Linguistics, July.
J. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007a.
Improving translation quality by discarding most of
the phrasetable. In Proc. of EMNLP-CoNLL, Prague.
M. Johnson, T. Griffiths, and S. Goldwater. 2007b.
Bayesian inference for PCFGs via Markov chain
Monte Carlo. In Proc. of NAACL-HLT, pages 139?
146, Rochester, New York, April.
P. Koehn, F. Och, and D.Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL, pages 48?
54, Morristown, NJ, USA.
P. Koehn, H. Hoang, A. B. Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL
Demonstration Session, pages 177?180, June.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk
decoding for statistical machine translation. In Pro-
cessings of HLT-NAACL.
D. Marcu and W. Wong. 2002. A phrase-based, joint
probability model for statistical machine translation.
In Proc. of EMNLP, pages 133?139.
J. May and K. Knight. 2006. A better n-best list: Prac-
tical determinization of weighted finite tree automata.
In Proc. of NAACL-HLT.
N. Metropolis and S. Ulam. 1949. The Monte Carlo
method. Journal of the American Statistical Associa-
tion, 44(247):335?341.
F. Och and H. Ney. 2000. A comparison of alignment
models for statistical machine translation. In Proc. of
COLING, Saarbrucken, Germany, July.
F. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL, pages 160?167,
Sapporo, Japan, July.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proc. of ACL, pages 311?318.
N. N. Schraudolph, J. Yu, and S. Gu?nter. 2007. A
stochastic quasi-Newton method for online convex
optimization. In Proc. of Artificial Intelligence and
Statistics.
N. N. Schraudolph. 1999. Local gain adaptation in
stochastic gradient descent. Technical Report IDSIA-
09-99, IDSIA.
K. Sima?an. 1996. Computational complexity of proba-
bilistic disambiguation by means of tree grammars. In
Proc. of COLING, Copenhagen.
D. A. Smith and J. Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proc. of
COLING-ACL, pages 787?794.
R. Zens and H. Ney. 2007. Efficient phrase-table repre-
sentation for machine translation with applications to
online MT and speech translation. In Proc. of NAACL-
HLT, Rochester, New York.
R. Zens, S. Hasan, and H. Ney. 2007. A systematic com-
parison of training criteria for statistical machine trans-
lation. In Proc. of EMNLP, pages 524?532, Prague,
Czech Republic.
H. Zhang, C. Quirk, R. C. Moore, and D. Gildea. 2008.
Bayesian learning of non-compositional phrases with
synchronous parsing. In Proc. of ACL: HLT, pages
97?105, Columbus, Ohio.
110
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 328?337,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Dynamic Topic Adaptation for Phrase-based MT
Eva Hasler
1
, Phil Blunsom
2
, Philipp Koehn
1
, Barry Haddow
1
1
School of Informatics, University of Edinburgh
2
Dept. of Computer Science, University of Oxford
Abstract
Translating text from diverse sources
poses a challenge to current machine
translation systems which are rarely
adapted to structure beyond corpus level.
We explore topic adaptation on a diverse
data set and present a new bilingual vari-
ant of Latent Dirichlet Allocation to com-
pute topic-adapted, probabilistic phrase
translation features. We dynamically in-
fer document-specific translation proba-
bilities for test sets of unknown origin,
thereby capturing the effects of document
context on phrase translations. We show
gains of up to 1.26 BLEU over the base-
line and 1.04 over a domain adaptation
benchmark. We further provide an anal-
ysis of the domain-specific data and show
additive gains of our model in combination
with other types of topic-adapted features.
1 Introduction
In statistical machine translation (SMT), there has
been a lot of interest in trying to incorporate in-
formation about the provenance of training exam-
ples in order to improve translations for specific
target domains. A popular approach are mixture
models (Foster and Kuhn, 2007) where each com-
ponent contains data from a specific genre or do-
main. Mixture models can be trained for cross-
domain adaption when the target domain is known
or for dynamic adaptation when the target domain
is inferred from the source text under translation.
More recent domain adaptation methods employ
corpus or instance weights to promote relevant
training examples (Matsoukas et al., 2009; Fos-
ter et al., 2010) or do more radical data selection
based on language model perplexity (Axelrod et
al., 2011). In this work, we are interested in the
dynamic adaptation case, which is challenging be-
cause we cannot tune our model towards any spe-
cific domain.
In previous literature, domains have often been
loosely defined in terms of corpora, for exam-
ple, news texts would be defined as belonging to
the news domain, ignoring the specific content of
news documents. It is often assumed that the data
within a domain is homogeneous in terms of style
and vocabulary, though that is not always true in
practice. The term topic on the other hand can
describe the thematic content of a document (e.g.
politics, economy, medicine) or a latent cluster in a
topic model. Topic modelling for machine transla-
tion aims to find a match between thematic context
and topic clusters. We view topic adaptation as
fine-grained domain adaptation with the implicit
assumption that there can be multiple distributions
over translations within the same data set. If these
distributions overlap, then we expect topic adapta-
tion to help separate them and yield better trans-
lations than an unadapted system. Topics can be
of varying granularity and are therefore a flexi-
ble means to structure data that is not uniform
enough to be modelled in its entirety. In recent
years there have been several attempts to integrat-
ing topical information into SMT either by learn-
ing better word alignments (Zhao and Xing, 2006),
by adapting translation features cross-domain (Su
et al., 2012), or by dynamically adapting lexical
weights (Eidelman et al., 2012) or adding sparse
topic features (Hasler et al., 2012).
We take a new approach to topic adaptation by
estimating probabilistic phrase translation features
in a completely Bayesian fashion. The motivation
is that automatically identifying topics in the train-
ing data can help to select the appropriate transla-
tion of a source phrase in the context of a docu-
ment. By adapting a system to automatically in-
duced topics we do not have to trust data from a
given domain to be uniform. We also overcome
the problem of defining the level of granularity for
domain adaptation. With more and more training
data automatically extracted from the web and lit-
tle knowledge about its content, we believe this
is an important area to focus on. Translation of
web sites is already a popular application for MT
systems and could be helped by dynamic model
adaptation. We present results on a mixed data
set of the TED corpus, parts of the Commoncrawl
corpus which contains crawled web data and parts
of the News Commentary corpus which contains
328
Figure 1: Phrasal LDA model for inference on
training data.
documents about politics and economics. We be-
lieve that the broad range of this data set makes it a
suitable testbed for topic adaptation. We focus on
translation model adaptation to learn how words
and phrases translate in a given document-context
without knowing the origin of the document. By
learning translations over latent topics and com-
bining several topic-adapted features we achieve
improvements of more than 1 BLEU point.
2 Bilingual topic models over phrase
pairs
Our model is based on LDA and infers topics
as distributions over phrase pairs instead of over
words. It is specific to machine translation in that
the conditional dependencies between source and
target phrases are modelled explicitly, and there-
fore we refer to it as phrasal LDA. Topic distribu-
tions learned on a training corpus are carried over
to tuning and test sets by running a modified in-
ference algorithm on the source side text of those
sets. Translation probabilities are adapted sepa-
rately to each source text under translation which
makes this a dynamic topic adaptation approach.
In the following we explain our approach to topic
modelling with the objective of estimating better
phrase translation probabilities for data sets that
exhibit a heterogeneous structure in terms of vo-
cabulary and style. The advantage from a mod-
elling point of view is that unlike with mixture
models, we avoid sparsity problems that would
arise if we treated documents or sets of documents
as domains and learned separate models for them.
2.1 Latent Dirichlet Allocation (LDA)
LDA is a generative model that learns latent top-
ics in a document collection. In the original
formulation, topics are multinomial distributions
over words of the vocabulary and each docu-
ment is assigned a multinomial distribution over
topics (Blei et al., 2003). Our goal is to learn
topic-dependent phrase translation probabilities
and hence we modify this formulation by replac-
ing words with phrase pairs. This is straightfor-
ward when both source and target phrases are ob-
served but requires a modified inference approach
when only source phrases are observed in an un-
known test set. Different from standard LDA and
previous uses of LDA for MT, we define a bilin-
gual topic model that learns topic distributions
over phrase pairs. This allows us to model the
units of interest in a more principled way, without
the need to map per-word or per-sentence topics to
phrase pairs. Figure 1 shows a graphical represen-
tation of the following generative process.
For each of N documents in the collection
1. Choose topic distribution ?
d
? Dirichlet(?).
2. Choose the number of phrases pairs P
d
in the
document, P
d
? Poisson(?).
3. For every position d
i
in the document corre-
sponding to a phrase pair p
d,i
of source and
target phrase s
i
and t
i
1
:
(a) Choose a topic z
d,i
?Multinomial(?
d
).
(b) Conditioned on topic z
d,i
, choose a
source phrase s
d,i
?Multinomial(?
z
d,i
).
(c) Conditioned on z
d,i
and s
d,i
, choose tar-
get phrase t
d,i
?Multinomial(?
s
d,i
,z
d,i
).
?, ? and ? are parameters of the Dirichlet dis-
tributions, which are asymmetric for k = 0. Our
inference algorithm is an implementation of col-
lapsed variational Bayes (CVB), with a first-order
Gaussian approximation (Teh et al., 2006). It has
been shown to be more accurate than standard VB
and to converge faster than collapsed Gibbs sam-
pling (Teh et al., 2006; Wang and Blunsom, 2013),
with little loss in accuracy. Because we have to
do inference over a large number of phrase pairs,
CVB is more practical than Gibbs sampling.
2.2 Overview of training strategy
Ultimately, we want to learn translation probabil-
ities for all possible phrase pairs that apply to a
given test document during decoding. Therefore,
topic modelling operates on phrase pairs as they
will be seen during decoding. Given word-aligned
parallel corpora from several domains, we extract
lists of per-document phrase pairs produced by the
extraction algorithm in the Moses toolkit (Koehn
et al., 2007) which contain all phrase pairs consis-
tent with the word alignment. We run CVB on the
set of all training documents to learn latent topics
without providing information about the domains.
1
Parallel documents are modelled as bags of phrase pairs.
329
Using the trained model, CVB with modified in-
ference is run on all test documents with the set of
possible phrase translations that a decoder would
load from a phrase table before decoding. When
test inference has finished, we compute adapted
translation probabilities at the document-level by
marginalising over topics for each phrase pair.
3 Bilingual topic inference
3.1 Inference on training documents
The aim of inference on the training data is to
find latent topics in the distributions over phrase
pairs in each document.This is done by repeatedly
visiting all phrase pair positions in all documents,
computing conditional topic probabilities and up-
dating counts. To bias the model to cluster stop
word phrases in one topic, we place an asymmet-
ric prior over the hyperparameters
2
as described in
(Wallach et al., 2009) to make one of the topics a
priori more probable in every document. We use
a fixed-point update (Minka, 2012) to update the
hyperparameters after every iteration. For CVB
the conditional probability of topic z
d,i
given the
current state of all variables except z
d,i
is
P(z
d,i
= k|z
?(d,i)
,s, t,d,?,?,?) ?
(E
q?
[n
?(d,i)
.,k,s,t
]+?)
(E
q?
[n
?(d,i)
.,k,s,.
]+T
s
??)
(E
q?
[n
?(d,i)
.,k,s,.
]+ ?)
(E
q?
[n
?(d,i)
.,k,.
]+S ? ?)
?(E
q?
[n
?(d,i)
d,k,.
]+?) (1)
where s and t are all source and target phrases in
the collection. n
?(d,i)
.,k,s,t
, n
?(d,i)
.,k,s,.
and n
?(d,i)
d,k,.
are cooc-
currence counts of topics with phrase pairs, source
phrases and documents respectively. E
q?
is the
expectation under the variational posterior and in
comparison to Gibbs sampling where the posterior
would otherwise look very similar, counts are re-
placed by their means. n
?(d,i)
.,k,.
is a topic occurrence
count, T
s
is the number of possible target phrases
for a given source phrase and S is the total num-
ber of source phrases. By modelling phrase trans-
lation probabilities separately as P(t
i
|s
i
,z
i
= k, ..)
and P(s
i
|z
i
= k, ..), we can put different priors on
these distributions. For example, we want a sparse
distribution over target phrases for a given source
phrase and topic to express our translation prefer-
ence under each topic. The algorithm stops when
the variational posterior has converged for all doc-
uments or after a maximum of 100 iterations.
3.2 Inference on tuning and test documents
To compute translation probabilities for tuning
and test documents where target phrases are not
2
Omitted from the following equations for simplicity.
observed, the variational posterior is adapted as
shown in Equation 2
P(z
d,i
= k, t
i, j
|z
?(d,i)
,s, t
?(d,i)
,d,?,?,?) ?
(E
q?
[n
?(d,i)
.,k,s,t
j
]+?)
(E
q?
[n
?(d,i)
.,k,s,.
]+T
s
??)
(E
q?
[n
?(d,i)
.,k,s,.
]+ ?)
(E
q?
[n
?(d,i)
.,k,.
]+S ? ?)
?(E
q?
[n
?(d,i)
d,k,.
]+?) (2)
which now computes the joint conditional prob-
ability of a topic k and a target phrase t
i, j
, given the
source phrase s
i
and the test document d. There-
fore, the size of the support changes from K to
K ?T
s
. While during training inference we compute
a distribution over topics for each source-target
pair, in test inference we can use the posterior to
marginalise out the topics and get a distribution
over target phrases for each source phrase.
We use the Moses decoder to produce lists of
translation options for each document in the tun-
ing and test sets. These lists comprise all phrase
pairs that will enter the search space at decod-
ing time. By default, only 20 target phrases per
source phrase are loaded from the phrase table,
so in order to allow for new phrase pairs to en-
ter the search space and for translation probabil-
ities to be computed more accurately, we allow
for up to 200 target phrases per source. For each
source sentence, we consider all possible phrase
segmentations and applicable target phrases. Un-
like in training, we do not iterate over all phrase
pairs in the list but over blocks of up to 200 target
phrases for a given source phrase. The algorithm
stops when all marginal translation probabilities
have converged though in practice we stopped ear-
lier to avoid overfitting.
3.3 Phrase translation probabilities
After topic inference on the tuning and test data,
the forward translation probabilities P(t|s,d) are
computed. This is done separately for every doc-
ument d because we are interested in the trans-
lation probabilities that depend on the inferred
topic proportions for a given document. For ev-
ery document, we iterate over source positions p
d,i
and use the current variational posterior to com-
pute P(t
i, j
|s
i
,d) for all possible target phrases by
marginalizing over topics:
P(t
i, j
|s
i
,d) =
?
k
P(z
i
= k, t
i, j
|z
?(d,i)
,s, t
?(d,i)
,d)
This is straightforward because during test in-
ference the variational posterior is normalised to
a distribution over topics and target phrases for
a given source phrase. If a source phrase oc-
curs multiple times in the same document, the
probabilities are averaged over all occurrences.
The inverse translation probabilities can be com-
puted analogously except that in cases where we
330
do not have variational posteriors for a given pair
of source and target phrases, an approximation is
needed. We omit the results here since our exper-
iments so far did not indicate improvements with
the inverse features included.
4 More topic-adapted features
Inspired by previous work on topic adaptation for
SMT, we add three additional topic-adapted fea-
tures to our model. All of these features make
use of the topic mixtures learned by our bilingual
topic model. The first feature is an adapted lexi-
cal weight, similar to the features in the work of
Eidelman et al. (2012). Our feature is different in
that we marginalise over topics to produce a single
adapted feature where v[k] is the k
th
element of a
document topic vector for document d and w(t|s,k)
is a topic-dependent word translation probability:
lex(
?
t|s?,d) =
|t|
?
i
1
{ j|(i, j) ? a}
?
?(i, j)?a
?
k
w(t|s,k) ? v[k]
? ?? ?
w(t|s)
(3)
The second feature is a target unigram feature
similar to the lazy MDI adaptation of Ruiz and
Federico (2012). It includes an additional term
that measures the relevance of a target word w
i
by
comparing its document-specific probability P
doc
to its probability under the asymmetric topic 0:
trgUnigrams
t
=
|t|
?
i=1
f (
P
doc
(w
i
)
P
baseline
(w
i
)
)
? ?? ?
lazy MDI
? f (
P
doc
(w
i
)
P
topic0
(w
i
)
)
? ?? ?
relevance
(4)
f (x) =
2
1+
1
x
, x > 0 (5)
The third feature is a document similarity fea-
ture, similar to the semantic feature described by
Banchs and Costa-juss? (2011):
docSim
t
= max
i
(1? JSD(v
train doc
i
,v
test doc
)) (6)
where v
train_doc
i
and v
test_doc
are document topic
vector of training and test documents. Because
topic 0 captures phrase pairs that are common to
many documents, we exclude it from the topic
vectors before computing similarities.
4.1 Feature combination
We tried integrating the four topic-adapted fea-
tures separately and in all possible combinations.
As we will see in the results section, while all fea-
tures improve over the baseline in isolation, the
adapted translation feature P(t|s,d) is the strongest
feature. For the features that have a counterpart in
the baseline model (p(t|s,d) and lex(t|s,d)), we ex-
perimented with either adding or replacing them in
Data Mixed CC NC TED
Train 354K (6450) 110K 103K 140K
Dev 2453 (39) 818 817 818
Test 5664 (112) 1892 1878 1894
Table 1: Number of sentence pairs and documents
(in brackets) in the French-English data sets. The
training data has 2.7M English words per domain.
the log-linear model. We found that while adding
the features worked well and yielded close to zero
weights for their baseline counterparts after tun-
ing, replacing them yielded better results in com-
bination with the other adapted features. We be-
lieve the reason could be that fewer phrase table
features in total are easier to optimise.
5 Experimental setup
5.1 Data and baselines
Our experiments were carried out on a mixed
data set, containing the TED corpus (Cettolo et
al., 2012), parts of the News Commentary cor-
pus (NC) and parts of the Commoncrawl corpus
(CC) from the WMT13 shared task (Bojar et al.,
2013) as described in Table 1. We were guided
by two constraints in chosing our data set. 1) the
data has document boundaries and the content of
each document is assumed to be topically related,
2) there is some degree of topical variation within
each data set. In order to compare to domain adap-
tation approaches, we chose a setup with data from
different corpora. We want to abstract away from
adaptation effects that concern tuning of length
penalties and language models, so we use a mixed
tuning set containing data from all three domains
and train one language model on the concatenation
of (equally sized) target sides of the training data.
Word alignments are trained on the concatenation
of all training data and fixed for all models.
Our baseline (ALL) is a phrase-based French-
English system trained on the concatenation of
all parallel data. It was built with the Moses
toolkit (Koehn et al., 2007) using the 14 standard
core features including a 5gram language model.
Translation quality is evaluated on a large test set,
using the average feature weights of three optimi-
sation runs with PRO (Hopkins and May, 2011).
We use the mteval-v13a.pl script to compute case-
insensitive BLEU. As domain-aware benchmark
systems, we use the phrase table fill-up method
(FILLUP) of Bisazza et al. (2011) which pre-
serves the translation scores of phrases from the
IN model and the linear mixture models (LIN-
TM) of Sennrich (2012b) (both available in the
Moses toolkit). For both systems, we build sepa-
rate phrase tables for each domain and use a wrap-
per to decode tuning and test sets with domain-
specific tables. Both benchmarks have an advan-
331
Model Mixed CC NC TED
IN 26.77 18.76 29.56 32.47
ALL 26.86 19.61 29.42 31.88
Table 2: BLEU of in-domain and baseline models.
Model Avg JSD Rank1-diff
Ted-IN vs ALL 0.15 10.8%
CC-IN vs ALL 0.17 18.4%
NC-IN vs ALL 0.13 13.3%
Table 3: Average JSD of IN vs. ALL models.
Rank1-diff: % PT entries where preferred transla-
tion changes.
tage over our model because they are aware of do-
main boundaries in the test set. Further, LIN-TM
adapts phrase table features in both translation di-
rections while we only adapt the forward features.
Table 2 shows BLEU scores of the baseline sys-
tem as well as the performance of three in-domain
models (IN) tuned under the same conditions. For
the IN models, every portion of the test set is de-
coded with a domain-specific model. Results on
the test set are broken down by domain but also
reported for the entire test set (mixed). For Ted
and NC, the in-domain models perform better than
ALL, while for CC the all-domain model improves
quite significantly over IN.
5.2 General properties of the data sets
In this section we analyse some internal properties
of our three data sets that are relevant for adapta-
tion. All of the scores were computed on the sets
of source side tokens of the test set which were
limited to contain content words (nouns, verbs, ad-
jectives and adverbs). The test set was tagged with
the French TreeTagger (Schmid, 1994). The top of
Table 3 shows the average Jensen-Shannon diver-
gence (using log
2
, JSD ? [0,1]) of each in-domain
model in comparison to the all-domain model,
which is an indicator of how much the distribu-
tions in the IN model change when adding out-of-
domain data. Likewise, Rank1-diff gives the per-
centage of word tokens in the test set where the
preferred translation according to p(e| f ) changes
between IN and ALL. These are the words that
are most affected by adding data to the IN model.
Both numbers show that for Commoncrawl the IN
and ALL models differ more than in the other two
data sets. According to the JS divergence between
NC-IN and ALL, translation distibutions in the NC
phrase table are most similar to the ALL phrase
table. Table 4 shows the average JSD for each IN
model compared to a model trained on half of its
in-domain data. This score gives an idea of how
diverse a data set is, measured by comparing dis-
tributions over translations for source words in the
test set. According to this score, Commoncrawl
is the most diverse data set and Ted the most uni-
Model Avg JSD
Ted-half vs Ted-full 0.07
CC-half vs CC-full 0.17
NC-half vs NC-full 0.09
Table 4: Average JSD of in-domain models
trained on half vs. all of the data.
form. Note however, that these divergence scores
do not provide information about the relative qual-
ity of the systems under comparison. For CC,
the ALL model yields a much higher BLEU score
than the IN model and it is likely that this is due to
noisy data in the CC corpus. In this case, the high
divergence is likely to mean that distributions are
corrected by out-of-domain data rather than being
shifted away from in-domain distributions.
5.3 Topic-dependent decoding
The phrase translation probabilities and additional
features described in the last two sections are used
as features in the log-linear translation model in
addition to the baseline translation features. When
combining all four adapted features, we replace
P(t|s) and lex(t|s) by their adapted counterparts.
We construct separate phrase tables for each doc-
ument in the development and test sets and use a
wrapper around the decoder to ensure that each in-
put document is paired with a configuration file
pointing to its document-specific translation table.
Documents are decoded in sequence so that only
one phrase table needs to be loaded at a time. Us-
ing the wrapped decoder we can run parameter op-
timisation (PRO) in the usual way to get one set of
tuned weights for all test documents.
6 Results
In this section we present experimental results
with phrasal LDA. We show BLEU scores in com-
parison to a baseline system and two domain-
aware benchmark systems. We also evaluate
the adapted translation distributions by looking at
translation probabilities under specific topics and
inspect translations of ambiguous source words.
6.1 Analyis of bilingual topic models
We experimented with different numbers of top-
ics for phrasal LDA. The diagrams in Figure 2
shows blocks of training and test documents in
each of the three domains for a model with 20 top-
ics. Darker shading means that documents have
a higher proportion of a particular topic in their
document-topic distribution. The first topic is the
one that was affected by the asymmetric prior and
inspecting its most probable phrase pairs showed
that it had ?collected? a large number of stop word
phrases. This explains why it is the topic that
is most shared across documents and domains.
332
Figure 2: Document-topic distributions for train-
ing (top) and test (bottom) documents, grouped by
domain and averaged into blocks for visualisation.
Topic 8 Topic 11
europ?enne? european crise? crisis
politiques? political taux? rate
politique? policy financi?re?financial
int?r?ts? interests mon?taire? monetary
Topic 14 Topic 19
h?tel? hotel web? web
plage? beach utiliser? use
situ?? located logiciel? software
chambres? bedrooms donn?es? data
Figure 3: Frequent phrase pairs in learned topics.
There is quite a clear horizontal separation be-
tween documents of different domains, for exam-
ple, topics 6, 8, 19 occur mostly in Ted, NC and
CC documents respectively. The overall structure
is very similar between training (top) and test (bot-
tom) documents, which shows that test inference
was successful in carrying over the information
learned on training documents. There is also some
degree of topic sharing across domains, for exam-
ple topics 4 and 15 occur in documents of all three
domains. Figure 3 shows examples of latent topics
found during inference on the training data. Topic
8 and 11 seem to be about politics and economy
and occur frequently in documents from the NC
corpus. Topic 14 contains phrases related to ho-
tels and topic 19 is about web and software, both
frequent themes in the CC corpus.
6.2 Comparison according to BLEU
In Table 5 we compare our topic-adapted features
when added separately to the baseline phrase ta-
ble. The inclusion of each feature improves over
the concatenation baseline but the combination
of all four features gives the best overall results.
Though the relative performance differs slightly
for each domain portion in the test set, overall the
adapted lexical weight is the weakest feature and
the adapted translation probability is the strongest
feature. We also performed feature ablation tests
and found that no combination of features was su-
perior to combining all four features. This con-
firms that the gains of each feature lead to additive
improvements in the combined model.
In Table 6 we compare topic-adapted models
Model Mixed CC NC TED
lex(e|f,d) 26.99 19.93 29.34 32.19
trgUnigrams 27.15 19.90 29.54 32.50
docSim 27.22 20.11 29.63 32.40
p(e|f,d) 27.31 20.23 29.52 32.58
All features 27.67 20.40 30.04 33.08
Table 5: BLEU scores of pLDA features (50 top-
ics), separately and combined.
Model Mixed CC NC TED
ALL -26.86 19.61 29.42 31.88
3 topics -26.95 19.83 29.46 32.02
5 topics *27.48 19.98 29.94 33.04
10 topics *27.65 20.34 29.99 33.14
20 topics *27.63 20.39 29.93 33.09
50 topics *27.67 20.40 30.04 33.08
100 topics *27.65 20.54 30.00 32.90
>ALL +0.81 +0.93 +0.62 +1.26
Table 6: BLEU scores of baseline and topic-
adapted systems (pLDA) with all 4 features and
largest improvements over baseline.
with varying numbers of topics to the concatena-
tion baseline. We see a consistent gain on all do-
mains when increasing the number of topics from
three to five and ten topics. This is evidence that
the number of domain labels is in fact smaller
than the number of underlying topics. The opti-
mal number of latent topics varies for each domain
and reflects our insights from section 5.2. The CC
domain was shown to be the most diverse and the
best performance on the CC portion of the test set
is achieved with 100 topics. Likewise, the TED
domain was shown to be least diverse and here
the best performance is achieved with only 10 top-
ics. The best performance on the entire test set is
achieved with 50 topics, which is also the optimal
number of topics for the NC domain. The bot-
ton row of the table indicates the relative improve-
ment of the best topic-adapted model per domain
over the ALL model. Using all four topic-adapted
features yields an improvement of 0.81 BLEU on
the mixed test set. The highest improvement on a
given domain is achieved for TED with an increase
of 1.26 BLEU. The smallest improvement is mea-
sured on the NC domain. This is in line with the
observation that distributions in the NC in-domain
table are most similar to the ALL table, therefore
we would expect the smallest improvement for do-
main or topic adaptation. We used bootstrap re-
sampling (Koehn, 2004) to measure significance
on the mixed test set and marked all statistically
significant results compared to the respective base-
lines with asterisk (*: p? 0.01).
To demonstrate the benefit of topic adaptation
over more standard domain adaptation approaches
for a diverse data set, we show the performance
333
Model Mixed CC NC TED
FILLUP -27.12 19.36 29.78 32.71
LIN-TM -27.24 19.61 29.87 32.73
pLDA *27.67 20.40 30.04 33.08
>FILLUP +0.55 +1.04 +0.26 +0.37
>LIN-TM +0.43 +0.79 +0.17 +0.35
Table 7: Comparison of best pLDA system with
two domain-aware benchmark systems.
Model Mixed CC NC TED
LIN-LM
+ ALL -27.16 19.71 29.77 32.46
+ FILLUP -27.20 19.37 29.84 32.90
+ LIN-TM -27.34 19.59 29.92 33.02
+ pLDA *27.84 20.48 30.03 33.57
>ALL +0.68 +0.77 +0.26 +1.11
Table 8: Combination of all models with addi-
tional LM adaptation (pLDA: 50 topics).
of two state-of-the-art domain-adapted systems in
Table 7. Both FILLUP and LIN-TM improve over
the ALL model on the mixed test set, by 0.26 and
0.38 BLEU respectively. The largest improvement
is on TED while on the CC domain, FILLUP de-
creases in performance and LIN-TM yields no im-
provement either. This shows that relying on in-
domain distributions for adaptation to a noisy and
diverse domain like CC is problematic. The pLDA
model yields the largest improvement over the
domain-adapted systems on the CC test set, with
in increase of 1.04 BLEU over FILLUP and 0.79
over LIN-TM. The improvements on the other two
domains are smaller but consistent.
We also compare the best model from Table 6
to all other models in combination with linearly
interpolated language models (LIN-LM), interpo-
lated separately for each domain. Though the
improvements are slightly smaller than without
adapted language models, there is still a gain over
the concatenation baseline of 0.68 BLEU on the
mixed test set and similar improvements to before
over the benchmarks (on TED the improvements
are actually even larger). Thus, we have shown
that topic-adaptation is effective for test sets of
diverse documents and that we can achieve sub-
stantial improvements even in comparison with
domain-adapted translation and language models.
6.3 Properties of adapted distributions and
topic-specific translations
The first column of Table 9 shows the average en-
tropy of phrase table entries in the adapted models
according to p(t|s,d) versus the all-domain model,
computed over source tokens in the test set that
are content words. The entropy decreases in the
adapted tables in all cases which is an indicator
that the distributions over translations of content
Set Model Avg entropy Avg perplexity
CC
pLDA 3.74 9.21
ALL 3.99 10.13
NC
pLDA 3.42 6.96
ALL 3.82 7.51
TED
pLDA 3.33 9.17
ALL 4.00 9.71
Table 9: Average entropy of translation distribu-
tions and test set perplexity of the adapted model.
r?gime
topic 6 diet = 0.79 diet aids = 0.04
topic 8 regime* = 0.82 rule = 0.05
topic 19 restrictions = 0.53 diplomats = 0.10
noyau
topic 9 nucleus* = 0.89 core = 0.01
topic 11 core* = 0.93 inner = 0.03
topic 19 kernel = 0.58 core = 0.11
d?mon
topic 6 devil = 0.89 demon = 0.07
topic 8 demon* = 0.98 devil = 0.01
topic 19 daemon = 0.95 demon = 0.04
Table 10: The two most probable translations of
r?gime, noyau and d?mon and probabilities under
different latent topics (*: preferred by ALL).
words have become more peaked. The second col-
umn shows the average perplexity of target tokens
in the test set which is a measure of how likely a
model is to produce words in the reference trans-
lation. We use the alignment information between
source and reference and therefore limit our anal-
ysis to pairs of aligned words, but nevertheless
this shows that the adapted translation distribu-
tions model the test set distributions better than the
baseline model. Therefore, the adapted distribu-
tions are not just more peaked but also more often
peaked towards the correct translation.
Table 10 shows examples of ambiguous French
words that have different preferred translations de-
pending on the latent topic. The word r?gime can
be translated as diet, regime and restrictions and
the model has learned that the probability over
translations changes when moving from one topic
to another (preferred translations under the ALL
model are marked with *). For example, the trans-
lation to diet is most probable under topic 6 and
the translation to regime which would occur in
a political context is most probable under topic
8. Topic 6 is most prominent among Ted docu-
ments while topic 8 is found most frequently in
News Commentary documents which have a high
percentage of politically related text. The French
word noyau can be translated to nucleus (physics),
core (generic) and kernel (IT) among other trans-
lations and the topics that exhibit these preferred
translations can be attributed to Ted (which con-
tains many talks about physics), NC and CC (with
334
Src: ?il suffit d??jecter le noyau et d?en ins?rer un autre, comme ce qu?on fait pour le cl?nage.?
BL: ?it is the nucleus eject and insert another, like what we do to the cl?nage.?
pLDA: ?he just eject the nucleus and insert another, like what we do to the cl?nage.? (nucleus = 0.77)
Ref: ?you can just pop out the nucleus and pop in another one, and that?s what you?ve all heard about with cloning.?
Src: ?pourtant ceci obligerait les contribuables des pays de ce noyau ? fournir du capital au sud?
BL: ?but this would force western taxpayers to provide the nucleus of capital in the south?
pLDA: ?but this would force western taxpayers to provide the core of capital in the south? (core = 0.78)
Ref: ?but this would unfairly force taxpayers in the core countries to provide capital to the south?
Src: ?le noyau contient de nombreux pilotes, afin de fonctionner chez la plupart des utilisateurs.?
BL: ?the nucleus contains many drivers, in order to work for most users.?
pLDA: ?the kernel contains many drivers, to work for most users.? (kernel = 0.53)
Ref: ?the precompiled kernel includes a lot of drivers, in order to work for most users.?
Figure 4: pLDA correctly translates noyau in test docs from Ted, NC and CC (adapted probabilities in
brackets). The baseline (nucleus = 0.27, core = 0.27, kernel = 0.23) translates all instances to nucleus.
many IT-related documents). The last example,
d?mon, has three frequent translations in English:
devil, demon and daemon. The last translation
refers to a computer process and would occur in an
IT context. The topic-phrase probabilities reveal
that its mostly likely translation as daemon occurs
under topic 19 which clusters IT-related phrase
pairs and is frequent in the CC corpus. These
examples show that our model can disambiguate
phrase translations using latent topics.
As another motivating example, in Figure 4 we
compare the output of our adapted models to the
output produced by the all-domain baseline for the
word noyau from Table 10. While the ALL base-
line translates each instance of noyau to nucleus,
the adapted model translates each instance differ-
ently depending on the inferred topic mixtures for
each document and always matches the reference
translation. The probabilities in brackets show
that the chosen translations were indeed the most
likely under the respective adapted model. While
the ALL model has a flat distribution over pos-
sible translations, the adapted models are peaked
towards the correct translation. This shows that
topic-specific translation probabilities are neces-
sary when the translation of a word shifts between
topics or domains and that peaked, adapted distri-
butions can lead to more correct translations.
7 Related work
There has been a lot of previous work using topic
information for SMT, most of it using monolin-
gual topic models. For example, Gong and Zhou
(2011) use the topical relevance of a target phrase,
computed using a mapping between source and
target side topics, as an additional feature in de-
coding. Axelrod et al. (2012) build topic-specific
translation models from the TED corpus and se-
lect topic-relevant data from the UN corpus to im-
prove coverage. Su et al. (2012) perform phrase
table adaptation in a setting where only monolin-
gual in-domain data and parallel out-of-domain
data are available. Eidelman et al. (2012) use
topic-dependent lexical weights as features in the
translation model, which is similar to our work
in that topic features are tuned towards useful-
ness of topic information and not towards a tar-
get domain. Hewavitharana et al. (2013) per-
form dynamic adaptation with monolingual top-
ics, encoding topic similarity between a conversa-
tion and training documents in an additional fea-
ture. This is similar to the work of Banchs and
Costa-juss? (2011), both of which inspired our
document similarity feature. Also related is the
work of Sennrich (2012a) who explore mixture-
modelling on unsupervised clusters for domain
adaptation and Chen et al. (2013) who compute
phrase pair features from vector space representa-
tions that capture domain similarity to a develop-
ment set. Both are cross-domain adaptation ap-
proaches, though. Instances of multilingual topic
models outside the field of MT include Boyd-
Graber and Blei (2009; Boyd-Graber and Resnik
(2010) who learn cross-lingual topic correspon-
dences (but do not learn conditional distributions
like our model does). In terms of model structure,
our model is similar to BiTAM (Zhao and Xing,
2006) which is an LDA-style model to learn topic-
based word alignments. The work of Carpuat and
Wu (2007) is similar to ours in spirit, but they pre-
dict the most probable translation in a context at
the token level while our adaptation operates at the
type level of a document.
8 Conclusion
We have presented a novel bilingual topic model
based on LDA and applied it to the task of transla-
tion model adaptation on a diverse French-English
data set. Our model infers topic distributions over
phrase pairs to compute document-specific trans-
lation probabilities and performs dynamic adap-
tation on test documents of unknown origin. We
have shown that our model outperforms a concate-
nation baseline and two domain-adapted bench-
mark systems with BLEU gains of up to 1.26 on
domain-specific test set portions and 0.81 overall.
We have also shown that a combination of topic-
adapted features performs better than each feature
in isolation and that these gains are additive. An
analysis of the data revealed that topic adaptation
compares most favourably to domain adaptation
when the domain in question is rather diverse.
335
Acknowledgements
This work was supported by funding from the
Scottish Informatics and Computer Science Al-
liance (Eva Hasler) and funding from the Eu-
ropean Union Seventh Framework Programme
(FP7/2007-2013) under grant agreement 287658
(EU BRIDGE) and grant agreement 288769 (AC-
CEPT). Thanks to Chris Dyer for an initial discus-
sion about the phrasal LDA model.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP. Association
for Computational Linguistics.
Amittai Axelrod, Xiaodong He, Li Deng, Alex Acero,
and Mei-Yuh Hwang. 2012. New methods and
evaluation experiments on translating TED talks in
the IWSLT benchmark. In Proceedings of ICASSP.
IEEE.
Rafael E. Banchs and Marta R. Costa-juss?. 2011. A
semantic feature for statistical machine translation.
In Proceedings of the Fifth Workshop on Syntax,
Semantics and Structure in Statistical Translation,
SSST-5. Association for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus Interpolation Methods for
Phrase-based SMT Adaptation. In Proceedings of
IWSLT.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet allocation.
JMLR.
Ond
?
rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of WMT 2013. Asso-
ciation for Computational Linguistics.
Jordan Boyd-Graber and David Blei. 2009. Multilin-
gual Topic Models for Unaligned Text. In Proceed-
ings of the Twenty-Fifth Conference on Uncertainty
in Artificial Intelligence. AUAI Press.
Jordan Boyd-Graber and Philip Resnik. 2010. Holistic
Sentiment Analysis Across Languages: Multilingual
Supervised Latent Dirichlet Allocation. In Proceed-
ings of EMNLP. Association for Computational Lin-
guistics.
Marine Carpuat and Dekai Wu. 2007. How phrase
sense disambiguation outperforms word sense dis-
ambiguation for SMT. In International Conference
on Theoretical and Methodological Issues in MT.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. Wit3: Web inventory of transcribed
and translated talks. In Proceedings of EAMT.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in SMT. In
Proceedings of ACL. Association for Computational
Linguistics.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic models for dynamic translation
model adaptation. In Proceedings of ACL. Associa-
tion for Computational Linguistics.
G. Foster and R. Kuhn. 2007. Mixture-model adapta-
tion for SMT. In Proceedings of WMT. Association
for Computational Linguistics.
G. Foster, C. Goutte, and R. Kuhn. 2010. Discrimi-
native instance weighting for domain adaptation in
SMT. In Proceedings of EMNLP. Association for
Computational Linguistics.
Zhengxian Gong and Guodong Zhou. 2011. Employ-
ing topic modeling for SMT. In Proceedings of
IEEE (CSAE), volume 4.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse lexicalised features and topic adaptation for
SMT. In Proceedings of IWSLT.
S. Hewavitharana, D. Mehay, S. Ananthakrishnan, and
P. Natarajan. 2013. Incremental topic-based TM
adaptation for conversational SLT. In Proceedings
of ACL. Association for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP. Association for
Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for SMT. In ACL 2007: Demo and
poster sessions. Association for Computational Lin-
guistics.
Philipp Koehn. 2004. Statistical significance tests
for machine translation evaluation. In Proceedings
of EMNLP. Association for Computational Linguis-
tics.
S. Matsoukas, A. Rosti, and B. Zhang. 2009. Discrim-
inative corpus weight estimation for MT. In Pro-
ceedings of EMNLP. Association for Computational
Linguistics.
Thomas P Minka. 2012. Estimating a Dirichlet distri-
bution. Technical report.
Nick Ruiz and Marcello Federico. 2012. MDI Adap-
tation for the Lazy: Avoiding Normalization in LM
Adaptation for Lecture Translation. In Proceedings
of IWSLT.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing.
336
Rico Sennrich. 2012a. Mixture-modeling with unsu-
pervised clusters for domain adaptation in SMT. In
Proceedings of EAMT.
Rico Sennrich. 2012b. Perplexity Minimization for
Translation Model Domain Adaptation in SMT. In
Proceedings of EACL. Association for Computa-
tional Linguistics.
J. Su, H. Wu, H. Wang, Y. Chen, X. Shi, H. Dong,
and Q. Liu. 2012. Translation model adaptation
for SMT with monolingual topic information. In
Proceedings of ACL. Association for Computational
Linguistics.
Yee Whye Teh, David Newman, and Max Welling.
2006. A collapsed variational Bayesian inference
algorithm for LDA. In Proceedings of NIPS.
Hanna M. Wallach, David M. Mimno, and Andrew Mc-
Callum. 2009. Rethinking LDA: Why priors matter.
In Proceedings of NIPS.
Pengyu Wang and Phil Blunsom. 2013. Collapsed
variational Bayesian inference for Hidden Markov
Models. In AISTATS, volume 31 of JMLR Proceed-
ings, pages 599?607.
Bing Zhao and Eric P. Xing. 2006. Bilingual topic ad-
mixture models for word alignment. In Proceedings
of ACL. Association for Computational Linguistics.
337
Proceedings of NAACL-HLT 2013, pages 342?347,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Applying Pairwise Ranked Optimisation to Improve the Interpolation of
Translation Models
Barry Haddow
University of Edinburgh
Scotland
bhaddow@inf.ed.ac.uk
Abstract
In Statistical Machine Translation we often
have to combine different sources of parallel
training data to build a good system. One way
of doing this is to build separate translation
models from each data set and linearly inter-
polate them, and to date the main method for
optimising the interpolation weights is to min-
imise the model perplexity on a heldout set. In
this work, rather than optimising for this indi-
rect measure, we directly optimise for BLEU
on the tuning set and show improvements in
average performance over two data sets and 8
language pairs.
1 Introduction
Statistical Machine Translation (SMT) requires
large quantities of parallel training data in order to
produce high quality translation systems. This train-
ing data, however, is often scarce and must be drawn
from whatever sources are available. If these data
sources differ systematically from each other, and/or
from the test data, then the problem of combining
these disparate data sets to create the best possible
translation system is known as domain adaptation.
One approach to domain adaptation is to build
separate models for each training domain, then
weight them to create a system tuned to the test do-
main. In SMT, a successful approach to building do-
main specific language models is to build one from
each corpus, then linearly interpolate them, choos-
ing weights that minimise the perplexity on a suit-
able heldout set of in-domain data. This method
has been applied by many authors (e.g. (Koehn and
Schroeder, 2007)), and is implemented in popular
language modelling tools like IRSTLM (Federico et
al., 2008) and SRILM (Stolcke, 2002).
Similar interpolation techniques have been devel-
oped for translation model interpolation (Foster et
al., 2010; Sennrich, 2012) for phrase-based systems
but have not been as widely adopted, perhaps be-
cause the efficacy of the methods is not as clear-
cut. In this previous work, the authors used stan-
dard phrase extraction heuristics to extract phrases
from a heldout set of parallel sentences, then tuned
the translation model (i.e. the phrase table) inter-
polation weights to minimise the perplexity of the
interpolated model on this set of extracted phrases.
In this paper, we try to improve on this perplexity
optimisation of phrase table interpolation weights by
addressing two of its shortcomings. The first prob-
lem is that the perplexity is not well defined because
of the differing coverage of the phrase tables, and
their partial coverage of the phrases extracted from
the heldout set. Secondly, perplexity may not corre-
late with the performance of the final SMT system.
So, instead of optimising the interpolation
weights for the indirect goal of translation model
perplexity, we optimise them directly for transla-
tion performance. We do this by incorporating these
weights into SMT tuning using a modified version of
Pairwise Ranked Optimisation (PRO) (Hopkins and
May, 2011).
In experiments on two different domain adapta-
tion problems and 8 language pairs, we show that
our method achieves comparable or improved per-
formance, when compared to the perplexity minimi-
sation method. This is an encouraging result as it
342
shows that PRO can be adapted to optimise transla-
tion parameters other than those in the standard lin-
ear model.
2 Optimising Phrase Table Interpolation
Weights
2.1 Previous Approaches
In the work of Foster and Kuhn (2007), linear inter-
polation weights were derived from different mea-
sures of distance between the training corpora, but
this was not found to be successful. Optimising the
weights to minimise perplexity, as described in the
introduction, was found by later authors to be more
useful (Foster et al, 2010; Sennrich, 2012), gener-
ally showing small improvements over the default
approach of concatenating all training data.
An alternative approach is to use log-linear inter-
polation, so that the interpolation weights can be
easily optimised in tuning (Koehn and Schroeder,
2007; Bertoldi and Federico, 2009; Banerjee et al,
2011). However, this effectively multiplies the prob-
abilities across phrase tables, which does not seem
appropriate, especially for phrases absent from 1 ta-
ble.
2.2 Tuning SMT Systems
The standard SMT model scores translation hy-
potheses as a linear combination of features. The
model score of a hypothesis e is then defined to
be w ? h(e, f, a) where w is a weight vector, and
h(e, f, a) a vector of feature functions defined over
source sentences (f ), hypotheses, and their align-
ments (a). The weights are normally optimised
(tuned) to maximise BLEU on a heldout set (the tun-
ing set).
The most popular algorithm for this weight op-
timisation is the line-search based MERT (Och,
2003), but recently other algorithms that support
more features, such as PRO (Hopkins and May,
2011) or MIRA-based algorithms (Watanabe et al,
2007; Chiang et al, 2008; Cherry and Foster, 2012),
have been introduced. All these algorithms assume
that the model score is a linear function of the pa-
rameters w. However since the phrase table prob-
abilities enter the score function in log form, if
these probabilities are a linear interpolation, then the
model score is not a linear function of the interpola-
tion weights. We will show that PRO can be used
to simultaneously optimise such non-linear parame-
ters.
2.3 Pairwise Ranked Optimisation
PRO is a batch tuning algorithm in the sense that
there is an outer loop which repeatedly decodes a
small (1000-2000 sentence) tuning set and passes
the n-best lists from this tuning set to the core al-
gorithm (also known as the inner loop). The core
algorithm samples pairs of hypotheses from the n-
best lists (according to a specific procedure), and
uses these samples to optimise the weight vector w.
The core algorithm in PRO will now be explained
in more detail. Suppose that the N sampled hypoth-
esis pairs (x?i , x
?
i ) are indexed by i and have corre-
sponding feature vectors pairs (h?i ,h
?
i ). If the gain
of a given hypothesis (we use smoothed sentence
BLEU) is given by the function g(x), then we define
yi by
yi ? sgn(g(x
?
i )? g(x
?
i )) (1)
For weights w, and hypothesis pair (x?i , x
?
i ), the
(model) score difference ?swi is given by:
?swi ? s
w(x?i )? s
w(x?i ) ? w ?
(
h?i ? h
?
i
)
(2)
Then the core PRO algorithm updates the weight
vector to w? by solving the following optimisation
problem:
w? = arg max
w
N?
i=1
log (? (yi?s
w
i )) (3)
where ?(x) is the standard sigmoid function. The
derivative of the function can be computed easily,
and the optimisation problem can be solved with
standard numerical optimisation algorithms such as
L-BFGS (Byrd et al, 1995). PRO is normally im-
plemented by converting each sample to a training
example for a 2 class maximum entropy classifier,
with the feature values set to ?hi and the responses
set to the yi, whereupon the log-likelihood is the ob-
jective given in Equation (3). As in maximum en-
tropy modeling, it is usual to add a Gaussian prior to
the objective (3) in PRO training.
2.4 Extending PRO for Mixture Models
We now show how to apply the PRO tuning algo-
rithm of the previous subsection to simultaneously
343
optimise the weights of the translation system, and
the interpolation weights.
In the standard phrase-based model, some of the
features are derived from logs of phrase translation
probabilities. If the phrase table is actually a linear
interpolation of two (or more) phrase tables, then
we can consider these features as also being func-
tions of the interpolation weights. The interpola-
tion weights then enter the score differences {?swi }
via the phrase features, and we can jointly optimise
the objective in Equation (3) for translation model
weights and interpolation weights.
To make this more concrete, suppose that the fea-
ture vector consists of m phrase table features and
n?m other features1
h ? (log(p1), . . . , log(pm), hm+1, . . . hn) (4)
where each pj is an interpolation of two probability
distributions pjA and p
j
B . So, p
j ? ?jpjA+(1??
j)pjB
with 0 ? ?j ? 1. Defining ? ? (?1 . . . ?m), the
optimisation problem is then:
(w?,??) = arg max(w,?)
?N
i=1 log
(
?
(
yi?s
(w,?)
i
))
(5)
where the sum is over the sampled hypothesis pairs
and the ? indicates the difference between the
model scores of the two hypotheses in the pair, as
before. The model score s(w,?)i is given by
m?
j=1
(
wj ? log
(
?jpjAi + (1? ?
j)pjBi)
))
+
n?
j=m+1
wjhji (6)
where w ? (wi . . . wn). A Gaussian regularisa-
tion term is added to the objective, as it was for
PRO. By replacing the core algorithm of PRO with
the optimisation above, the interpolation weights
can be trained simultaneously with the other model
weights.
Actually, the above explanation contains a simpli-
fication, in that it shows the phrase features interpo-
lated at sentence level. In reality the phrase features
1Since the phrase penalty feature is a constant across phrase
pairs it is not interpolated, and so is classed with the the ?other?
features. The lexical scores, although not actually probabilities,
are interpolated.
are interpolated at the phrase level, then combined to
give the sentence level feature value. This makes the
definition of the objective more complex than that
shown above, but still optimisable using bounded L-
BFGS.
3 Experiments
3.1 Corpus and Baselines
We ran experiments with data from the WMT shared
tasks (Callison-Burch et al, 2007; Callison-Burch et
al., 2012), as well as OpenSubtitles data2 released by
the OPUS project (Tiedemann, 2009).
The experiments targeted both the news-
commentary (nc) and OpenSubtitles (st) domains,
with nc-devtest2007 and nc-test2007
for tuning and testing in the nc domain, respec-
tively, and corresponding 2000 sentence tuning
and test sets selected from the st data. The news-
commentary v7 corpus and a 200k sentence corpus
selected from the remaining st data were used as
in-domain training data for the respective domains,
with europarl v7 (ep) used as out-of-domain train-
ing data in both cases. The language pairs we tested
were the WMT language pairs for nc (English (en)
to and from Spanish (es), German (de), French (fr)
and Czech (cs)), with Dutch (nl) substituted for de
in the st experiments.
To build phrase-based translation systems, we
used the standard Moses (Koehn et al, 2007) train-
ing pipeline, in particular employing the usual 5
phrase features ? forward and backward phrase
probabilities, forward and backward lexical scores
and a phrase penalty. The 5-gram Kneser-Ney
smoothed language models were trained by SRILM
(Stolcke, 2002), with KenLM (Heafield, 2011) used
at runtime. The language model is always a linear
interpolation of models estimated on the in- and out-
of-domain corpora, with weights tuned by SRILM?s
perplexity minimisation3. All experiments were run
three times with BLEU scores averaged, as recom-
mended by Clark et al (2011). Performance was
evaluated using case-insensitive BLEU (Papineni et
al., 2002), as implemented in Moses.
The baseline systems were tuned using the Moses
version of PRO, a reimplementation of the original
2www.opensubtitles.org
3Our method could also be applied to language model inter-
polation but we chose to focus on phrase tables in this paper.
344
algorithm using the sampling scheme recommended
by Hopkins and May. We ran 15 iterations of PRO,
choosing the weights that maximised BLEU on the
tuning set. For the PRO training of the interpo-
lated models, we used the same sampling scheme,
with optimisation of the model weights and interpo-
lation weights implemented in Python using scipy4.
The implementation is available in Moses, in the
contrib/promix directory.
The phrase table interpolation and perplexity-
based minimisation of interpolation weights used
the code accompanying Sennrich (2012), also avail-
able in Moses.
3.2 Results
For each of the two test sets (nc and st), we com-
pare four different translation systems (three base-
line systems, and our new interpolation method):
in Phrase and reordering tables were built from just
the in-domain data.
joint Phrase and reordering tables were built from
the in- and out-of-domain data, concatenated.
perp Separate phrase tables built on in- and out-of-
domain data, interpolated using perplexity min-
imisation. The reordering table is as for joint.
pro-mix As perp, but interpolation weights opti-
mised using our modified PRO algorithm.
So the two interpolated models (perp and pro-mix)
are the same as joint except that their 4 non-constant
phrase features are interpolated across the two sep-
arate phrase tables. Note that the language models
are the same across all four systems.
The results of this comparison over the 8 language
pairs are shown in Figure 1, and summarised in Ta-
ble 1, which shows the mean BLEU change relative
to the in system. It can be seen that the pro-mix
method presented here is out-performing the per-
plexity optimisation on the nc data set, and perform-
ing similarly on the st data set.
joint perp pro-mix
nc +0.18 +0.44 +0.91
st -0.04 +0.55 +0.48
Table 1: Mean BLEU relative to in system for each
data set. System names as in Figure 1
.
4www.scipy.org
4 Discussion and Conclusions
The results show that the pro-mix method is a vi-
able way of tuning systems built with interpolated
phrase tables, and performs better than the current
perplexity minimisation method on one of two data
sets used in experiments. On the other data set (st),
the out-of-domain data makes much less difference
to the system performance in general, most proba-
bly because the difference between the in and out-
of-domain data sets in much larger (Haddow and
Koehn, 2012). Whilst the differences between pro-
mix and perplexity minimisation are not large on the
nc test set (about +0.5 BLEU) the results have been
demonstrated to apply across many language pairs.
The advantage of the pro-mix method over other
approaches is that it directly optimises the mea-
sure that we are interested in, rather than optimising
an intermediate measure and hoping that translation
performance improves. In this work we optimise for
BLEU, but the same method could easily be used to
optimise for any sentence-level translation metric.
Acknowledgments
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement 288769 (ACCEPT).
References
Pratyush Banerjee, Sudip K. Naskar, Johann Roturier,
Andy Way, and Josef van Genabith. 2011. Domain
Adaptation in Statistical Machine Translation of User-
Forum Data using Component Level Mixture Mod-
elling. In Proceedings of MT Summit.
Nicola Bertoldi and Marcello Federico. 2009. Domain
Adaptation for Statistical Machine Translation from
Monolingual Resources. In Proceedings of WMT.
R. H. Byrd, P. Lu, and J. Nocedal. 1995. A limited
memory algorithm for bound constrained optimiza-
tion. SIAM Journal on Scientific and Statistical Com-
puting, 16(5):1190?1208.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
345
Findings of the 2012 Workshop on Statistical Machine
Translation. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, pages 10?
51, Montre?al, Canada, June. Association for Compu-
tational Linguistics.
Colin Cherry and George Foster. 2012. Batch Tuning
Strategies for Statistical Machine Translation. In Pro-
ceedings of NAACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online Large-Margin Training of Syntactic and Struc-
tural Translation Features. In Proceedings of EMNLP.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.
2011. Better hypothesis testing for statistical machine
translation: Controlling for optimizer instability. In
Proceedings of ACL.
Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008. IRSTLM: an Open Source Toolkit for Handling
Large Scale Language Models. In Proceedings of In-
terspeech, Brisbane, Australie.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
128?135, Prague, Czech Republic, June. Association
for Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative Instance Weighting for Domain Adap-
tation in Statistical Machine Translation. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 451?459, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Barry Haddow and Philipp Koehn. 2012. Analysing
the Effect of Out-of-Domain Data on SMT Systems.
In Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 422?432, Montre?al,
Canada, June. Association for Computational Linguis-
tics.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
Ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 1352?1362, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in Domain Adaptation for Statistical Machine Transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the ACL Demo Sessions, pages 177?180,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Franz J. Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of
ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Rico Sennrich. 2012. Perplexity Minimization for Trans-
lation Model Domain Adaptation in Statistical Ma-
chine Translation. In Proceedings of EACL.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Intl. Conf. on Spo-
ken Language Processing, vol. 2, pages 901?904.
Jo?rg Tiedemann. 2009. News from OPUS - A Collection
of Multilingual Parallel Corpora with Tools and Inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova, and
R. Mitkov, editors, Recent Advances in Natural Lan-
guage Processing (vol V), pages 237?248. John Ben-
jamins, Amsterdam/Philadelphia.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online Large-Margin Training for Sta-
tistical Machine Translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic, June. Association for
Computational Linguistics.
346
cs?en en?cs de?en en?de fr?en en?fr es?en en?es
injoint perppro?mix News Commentary
Ble
u
0
10
20
30
cs?en en?cs nl?en en?nl fr?en en?fr es?en en?es
injoint perppro?mix Open Subtitles
Ble
u
0
5
10
15
20
25
Figure 1: Comparison of the performance (BLEU) on in-domain data, of our pro-mix interpolation weight
tuning method with three baselines: in using just in-domain parallel training data training; joint also using
europarl data; and perp using perplexity minimisation to interpolate in-domain and europarl data.
347
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 115?120,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
More Linguistic Annotation for Statistical Machine Translation
Philipp Koehn, Barry Haddow, Philip Williams, and Hieu Hoang
University of Edinburgh
Edinburgh, United Kingdom
{pkoehn,bhaddow,p.j.williams-2,h.hoang}@inf.ed.ac.uk
Abstract
We report on efforts to build large-scale
translation systems for eight European
language pairs. We achieve most gains
from the use of larger training corpora and
basic modeling, but also show promising
results from integrating more linguistic an-
notation.
1 Introduction
We participated in the shared translation task of
the ACL Workshop for Statistical Machine Trans-
lation 2010 in all language pairs. We continued
our efforts to integrate linguistic annotation into
the translation process, using factored and tree-
based translation models. On average we out-
performed our submission from last year by 2.16
BLEU points on the same newstest2009 test set.
While the submitted system follows the factored
phrase-based approach, we also built hierarchical
and syntax-based models for the English?German
language pair and report on its performance on the
development test sets. All our systems are based
on the Moses toolkit (Koehn et al, 2007).
We achieved gains over the systems from last
year by consistently exploiting all available train-
ing data, using large-scale domain-interpolated,
and consistent use of the factored translation
model to integrate n-gram models over speech
tags. We also experimented with novel domain
adaptation methods, with mixed results.
2 Baseline System
The baseline system uses all available training
data, except for the large UN and 109 corpora, as
well as the optional LDC Gigaword corpus. It uses
a straight-forward setup of the Moses decoder.
Some relevant parameter settings are:
? maximum sentence length 80 words
? tokenization with hyphen splitting
? truecasing
? grow-diag-final-and alignment heuristic
? msd-bidirectional-fe lexicalized reordering
? interpolated 5-gram language model
? tuning on newsdev2009
? testing during development on newstest2009
? MBR decoding
? no reordering over punctuation
? cube pruning
We used most of these setting in our submission
last year (Koehn and Haddow, 2009).
The main difference to our baseline system
from the submission from last year is the use of ad-
ditional training data: larger releases of the News
Commentary, Europarl, Czeng, and monolingual
news corpora. The first two parallel corpora in-
creased roughly 10-20% in size, while the Czeng
parallel corpus and the monolingual news corpora
are five times and twice as big, respectively.
We also handled some of the corpus preparation
steps with more care to avoid some data incon-
sistency problems from last year (affecting mostly
the French language pairs).
An overview of the results is given in Table 1.
The baseline outperforms our submission from
last year by an average of +1.25 points. The gains
for the individual language pairs track the increase
in training data (most significantly for the Czech?
English pairs), and the French?English data pro-
cessing issue.
Note that last year?s submission used special
handling of the German?English language pair,
which we did not replicate in the baseline system,
but report on below.
The table also contains results on the extensions
discussed in the next section.
115
Language Pair ?09 Baseline GT Smooth. UN Data Factored Beam
Spanish-English 24.41 25.25 (+0.76) 25.48 (+0.23) 26.03 (+0.55) 26.20 (+0.17) 26.22 (+0.02)
French-English 23.88 25.23 (+1.35) 25.37 (+0.14) 25.92 (+0.55) 26.13 (+0.21) 26.07 (?0.08)
German-English 18.51 19.47 (+0.96) 19.51 (+0.04) - 21.09 (+0.24) 21.10 (+0.01)
Czech-English 18.49 20.74 (+2.25) 21.19 (+0.45) - 21.33 (+0.14) 21.32 (?0.01)
English-Spanish 23.27 24.20 (+0.93) 24.65 (+0.45) 24.65 (+0.30) 24.37 (?0.28) 24.42 (+0.05)
English-French 22.50 23.83 (+1.33) 23.72 (?0.11) 24.70 (+0.98) 24.74 (+0.04) 24.92 (+0.18)
English-German 14.22 14.68 (+0.46) 14.81 (+0.13) - 15.28 (+0.47) 15.34 (+0.06)
English-Czech 12.64 14.63 (+1.99) 14.68 (+0.05) - - -
avg +1.25 +0.17 +0.60 +0.14 +0.03
Table 1: Overview of results: baseline system and extensions. On average we outperformed our sub-
mission from last year by 1.87 BLEU points on the same newstest2009 test set. For additional gains for
French?English and German?English, please see Tables 7 and 8.
Czech?English
Corpus Num. Tokens Pplx. Weight
EU 29,238,799 582 0.054
Fiction 15,441,105 429 0.028
Navajo 561,144 671 0.002
News (czeng) 2,909,322 288 0.127
News (mono) 1,148,480,525 175 0.599
Subtitles 23,914,244 526 0.019
Techdoc 8,322,958 851 0.099
Web 4,469,177 441 0.073
French?English
Corpus Num. Tokens Pplx. Weight
Europarl 50,132,615 352 0.105
News Com. 2,101,921 311 0.204
UN 216,052,412 383 0.089
News 1,148,480,525 175 0.601
Table 2: English LM interpolation: number of to-
kens, perplexity, and interpolation weight for the
different corpora
2.1 Interpolated Language Model
The WMT training data exhibits an increasing di-
versity of corpora: Europarl, News Commentary,
UN, 109, News ? and seven different sources
within the Czeng corpus.
It is well known that domain adaptation is an
important step in optimizing machine translation
systems. A relatively simple and straight-forward
method is the linear interpolation of the language
model, as we explored previously (Koehn and
Schroeder, 2007; Schwenk and Koehn, 2008).
We trained domain-specific language models
separately and then linearly interpolated them us-
ing SRILM toolkit (Stolke, 2002) with weights op-
Language Pair Cased Uncased
Spanish-English 25.25 26.36 (+1.11)
French-English 25.23 26.29 (+1.06)
German-English 19.47 20.63 (+1.16)
Czech-English 20.74 21.76 (+1.02)
English-Spanish 24.20 25.47 (+1.27)
English-French 23.83 25.02 (+1.19)
English-German 14.68 15.18 (+0.50)
English-Czech 14.63 15.13 (+0.50)
avg +0.98
Table 3: Effect of truecasing: cased and uncased
BLEU scores
timized on the development set newsdev2009.
See Table 2 for numbers on perplexity, corpus
sizes, and interpolation weights. Note, for in-
stance, the relatively high weight for the News
Commentary corpus (0.204) compared to the Eu-
roparl corpus (0.105) in the English language
model for the French-English system, despite the
latter being about 25 times bigger.
2.2 Truecasing
As last year, we deal with uppercase and lowercase
forms of the same words by truecasing the corpus.
This means that we change each surface word oc-
currence of a word to its natural case, e.g., the, Eu-
rope. During truecasing, we change the first word
of a sentence to its most frequent casing. During
de-truecasing, we uppercase the first letter of the
first word of a sentence.
See Table 3 for the performance of this method.
In this table, we compare the cased and uncased
BLEU scores, and observe that we lose on average
roughly one BLEU point due to wrong casing.
116
Count Count of Count Discount Count*
1 357,929,182 0.140 0.140
2 24,966,751 0.487 0.975
3 8,112,930 0.671 2.014
4 4,084,365 0.714 2.858
5 2,334,274 0.817 4.088
Table 4: Good Turing smoothing, as in the
French?English model: counts, counts of counts,
discounting factor and discounted count
3 Extensions
In this section, we describe extensions over the
baseline system. On average, these give us im-
provements of about 1 BLEU point over the base-
line.
3.1 Good Turing Smoothing
Traditionally, we use raw counts to estimate con-
ditional probabilities for phrase translation. How-
ever, this method gives dubious results for rare
counts. The most blatant case is the single oc-
currence of a foreign phrase, whose sole English
translation will receive the translation probability
1
1 = 1.
Foster et al (2006) applied ideas from language
model smoothing to the translation model. Good
Turing smoothing (Good, 1953) uses counts of
counts statistics to assess how likely we will see
a word (or, in our case, a phrase) again, if we have
seen it n times in the training corpus. Instead of
using the raw counts, adapted (lower) counts are
used in the estimation of the conditional probabil-
ity distribution.
The count of counts are collected for the phrase
pairs. See Table 4 for details on how this ef-
fects the French?English model. For instance,
we find singleton 357,929,182 phrase pairs and
24,966,751 phrase pairs that occur twice. The
Good Turing formula tells us to adapt singleton
counts to 24,966,751357,929,182 = 0.14. This means for our
degenerate example of a single occurrence of a
single French phrase that its single English transla-
tion has probability 0.141 = 0.14 (we do not adjust
the denominator).
Good Turing smoothing of the translation table
gives us a gain of +0.17 BLEU points on average,
and improvements for 7 out of 8 language pairs.
For details refer back to Table 1.
Model BLEU
Baseline 14.81
Part-of-Speech 15.03 (+0.22)
Morphogical 15.28 (+0.47)
Table 5: English?German: use of morphological
and part-of-speech n-gram models
3.2 UN Data
While we already used the UN data in the lan-
guage model for the Spanish?English and French?
English language pairs, we now also add it to the
translation model.
The corpus is very large, four times bigger than
the already used training data, but relatively out
of domain, as indicated by the high perplexity and
low interpolation weight during language model
interpolation (recall Table 2).
Adding the corpus to the four systems gives im-
provements of +0.60 BLEU points on average.
For details refer back to Table 1.
3.3 POS n-gram Model
The factored model approach (Koehn and Hoang,
2007) allows us to integrate 7-gram models over
part-of-speech tags. The part-of-speech tags are
produced during decoding by the phrase mapping
of surface words on the source side to a factored
representation of surface words and their part-of-
speech tags on the target side in one translation
step.
We previously used this additional scoring com-
ponent for the German?English language pairs
with success. Thus we now applied to it all other
language pairs (except for English?Czech due to
the lack of a Czech part-of-speech tagger).
We used the following part-of-speech taggers:
? English: mxpost1
? German: LoPar2
? French: TreeTagger3
? Spanish: TreeTagger
For English?German, we also used morpholog-
ical tags, which give better performance than just
basic part-of-speech tags (+0.46 vs. +0.22, see Ta-
ble 5). We observe gains for all language pairs
except for English?Spanish, possibly due to the
1www.inf.ed.ac.uk/resources/nlp/local doc/MXPOST.html
2www.ims.uni-stuttgart.de/projekte/gramotron/SOFTWARE/
LoPar.html
3www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
117
Model BLEU
Baseline 14.81
Part-of-Speech 15.03 (+0.22)
Morphogical 15.28 (+0.47)
Table 6: English?German: use of morphological
and part-of-speech n-gram models
Language Pair Baseline with 109
French?English 25.92 27.15 (+1.23)
English?French 24.70 24.80 (+0.10)
Table 7: Use of large French?English corpus
faulty use of the Spanish part-of-speech tagger.
We gain +0.14 BLEU points on average (includ-
ing the ?0.28 drop for Spanish). For details refer
back to Table 1.
3.4 Bigger Beam Sizes
As a final general improvement, we adjusted the
beam settings during decoding. We increased the
pop-limit from 5,000 to 20,000 and the translation
table limit from the default 20 to 50.
The decoder is quite fast, partly due to multi-
threaded decoding using 4 cores machines (Had-
dow, 2010). Increasing the beam sizes slowed
down decoding speed from about 2 seconds per
sentence to about 8 sec/sentence.
However, this resulted only in minimal gains,
on average +0.03 BLEU. For details refer back to
Table 1.
3.5 109 Corpus
Last year, due to time constraints, we were not
able to use the billion word 109 corpus for the
French?English language pairs. This is largest
publicly available parallel corpus, and it does
strain computing resources, for instance forcing
us to use multi-threaded GIZA++ (Gao and Vogel,
2008).
Table 7 shows the gains obtained from us-
ing this corpus in both the translation model and
the language model opposed to a baseline sys-
tem trained with otherwise the same settings. For
French?English we see large gains (+1.23), but not
for English?French (+0.10).
Our official submission for the French?English
language pairs used these models. They did not in-
clude a part-of-speech language model and bigger
beam sizes.
Model BLEU
Baseline 19.51
+ compound splitting 20.09 (+0.58)
+ pre-reordering 20.03 (+0.52)
+ both 20.85 (+1.34)
Table 8: Special handling of German?English
Language Pair Baseline Weighted TM
Spanish-English 26.20 26.15 (?0.05)
French-English 26.11 26.30 (+0.19)
German-English 21.09 20.81 (?0.28)
Czech-English 21.33 21.21 (?0.12)
English-German 15.28 15.01 (?0.27)
avg. ?0.11
Table 9: Interpolating the translation model with
language model weights
3.6 German?English
For the German?English language direction, we
used two additional processing steps that have
shown to be successful in the past, and again re-
sulted in significant gains.
We split large words based on word frequen-
cies to tackle the problem of word compounds in
German (Koehn and Knight, 2003). Secondly, we
re-order the German input to the decoder (and the
German side of the training data) to align more
closely to the English target language (Collins
et al, 2005).
The two methods improve +0.58 and +0.52 over
the baseline individually, and +1.34 when com-
bined. See also Table 8.
3.7 Translation Model Interpolation
Finally, we explored a novel domain adaption
method for the translation model. Since the in-
terpolation of language models is very success-
ful, we want to interpolate translation models sim-
ilarly. Given interpolation weights, the resulting
translation table is a weighted linear interpolation
of the individual translation models trained sepa-
rately for each domain.
However, while for language models we have a
effective method to find the interpolation weights
(optimizing perplexity on a development set), we
do not have such a method for the translation
model. Thus, we simply recycle the weights we
obtained from language model interpolation (ex-
cluding the weighting for monolingual corpora).
118
Model BLEU
phrase-based 14.81
factored phrase-based 15.28
hierarchical 14.86
target syntax 14.66
Table 10: Tree-based models for English?German
Over the Spanish?English baseline system, we
obtained gains of +0.39 BLEU points. Unfortu-
nately, we did not see comparable gains on the sys-
tems optimized by the preceding steps. In fact, in
4 out of 5 language pairs, we observed lower BLEU
scores. See Table 9 for details.
We did not use this method in our submission.
4 Tree-Based Models
A major extension of the capabilities of the Moses
system is the accommodation of tree-based mod-
els (Hoang et al, 2009). While we have not yet
carried out sufficient experimentation and opti-
mization of the implementation, we took the occa-
sion of the shared translation task as a opportunity
to build large-scale systems using such models.
We build two translation systems: One using
tree-based models without additional linguistic an-
notation, which are known as hierarchical phrase-
based models (Chiang, 2005), and another sys-
tem that uses linguistic annotation on the target
side, which are known under many names such as
string-to-tree models or syntactified target models
(Marcu et al, 2006).
Both models are trained using a very similar
pipeline as for the phrase model. The main dif-
ference is that the translation rules do not have to
be contiguous phrases, but may contain gaps with
are labeled and co-ordinated by non-terminal sym-
bols. Decoding with such models requires a very
different algorithm, which is related to syntactic
chart parsing.
In the target syntax model, the target gaps and
the entire target phrase must map to constituents
in the parse tree. This restriction may be relaxed
by adding constituent labels such as DET+ADJ or
NP\DET to group neighboring constituents or indi-
cate constituents that lack an initial child, respec-
tively (Zollmann and Venugopal, 2006).
We applied these models to the English?
German language direction, which is of particu-
lar interest to us due to the rich target side mor-
phology and large degree of reordering, resulting
in relatively poor performance. See Table 10 for
experimental results with the two traditional mod-
els (phrase-based model and a factored model that
includes a 7-gram morphological tag model) and
the two newer models (hierarchical and target syn-
tax). The performance of the phrase-based, hierar-
chical, and target syntax model are close in terms
of BLEU.
5 Conclusions
We obtained substantial gains over our systems
from last year for all language pairs. To a large
part, these gains are due to additional training data
and our ability to exploit them.
We also saw gains from adding linguistic an-
notation (in form of 7-gram models over part-of-
speech tags) and promising results for tree-based
models. At this point, we are quite satisfied be-
ing able to build competitive systems with these
new models, which opens up major new research
directions.
Everything we described here is part of the open
source Moses toolkit. Thus, all our experiments
should be replicable with publicly available re-
sources.
Acknowledgement
This work was supported by the EuroMatrixPlus
project funded by the European Commission (7th
Framework Programme).
References
Chiang, D. (2005). A hierarchical phrase-based
model for statistical machine translation. In
Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics
(ACL?05), pages 263?270, Ann Arbor, Michi-
gan. Association for Computational Linguistics.
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Ar-
bor, Michigan. Association for Computational
Linguistics.
Foster, G., Kuhn, R., and Johnson, H. (2006).
Phrasetable smoothing for statistical machine
translation. In Proceedings of the 2006 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 53?61, Sydney, Aus-
119
tralia. Association for Computational Linguis-
tics.
Gao, Q. and Vogel, S. (2008). Parallel implemen-
tations of word alignment tool. In ACL Work-
shop on Software Engineering, Testing, and
Quality Assurance for Natural Language Pro-
cessing, pages 49?57.
Good, I. J. (1953). The population frequency of
species and the estimation of population param-
eters. Biometrika, 40:237?264.
Haddow, B. (2010). Adding multi-threaded de-
coding to moses. The Prague Bulletin of Math-
ematical Linguistics, (93):57?66.
Hoang, H., Koehn, P., and Lopez, A. (2009). A
unified framework for phrase-based, hierarchi-
cal, and syntax-based statistical machine trans-
lation. In Proceedings of IWSLT.
Koehn, P. and Haddow, B. (2009). Edinburgh?s
submission to all tracks of the WMT2009
shared task with reordering and speed improve-
ments to Moses. In Proceedings of the Fourth
Workshop on Statistical Machine Translation,
pages 160?164, Athens, Greece. Association
for Computational Linguistics.
Koehn, P. and Hoang, H. (2007). Factored trans-
lation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 868?876.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B., Shen,
W., Moran, C., Zens, R., Dyer, C. J., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open source toolkit for statistical ma-
chine translation. In Proceedings of the 45th
Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions,
pages 177?180, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
Koehn, P. and Knight, K. (2003). Empirical meth-
ods for compound splitting. In Proceedings of
Meeting of the European Chapter of the Associ-
ation of Computational Linguistics (EACL).
Koehn, P. and Schroeder, J. (2007). Experiments
in domain adaptation for statistical machine
translation. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages
224?227, Prague, Czech Republic. Association
for Computational Linguistics.
Marcu, D., Wang, W., Echihabi, A., and Knight,
K. (2006). Spmt: Statistical machine transla-
tion with syntactified target language phrases.
In Proceedings of the 2006 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 44?52, Sydney, Australia. Associa-
tion for Computational Linguistics.
Schwenk, H. and Koehn, P. (2008). Large and
diverse language models for statistical machine
translation. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Language
Processing (IJCNLP).
Stolke, A. (2002). SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the
International Conference on Spoken Language
Processing.
Zollmann, A. and Venugopal, A. (2006). Syntax
augmented machine translation via chart pars-
ing. In Proceedings on the Workshop on Statis-
tical Machine Translation, pages 138?141, New
York City. Association for Computational Lin-
guistics.
120
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 365?374,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
A Unified Approach to Minimum Risk Training and Decoding
Abhishek Arun, Barry Haddow and Philipp Koehn
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, UK
a.arun@sms.ed.ac.uk, {bhaddow,pkoehn}@inf.ed.ac.uk
Abstract
We present a unified approach to perform-
ing minimum risk training and minimum
Bayes risk (MBR) decoding with BLEU
in a phrase-based model. Key to our ap-
proach is the use of a Gibbs sampler that
allows us to explore the entire probabil-
ity distribution and maintain a strict prob-
abilistic formulation across the pipeline.
We also describe a new sampling algo-
rithm called corpus sampling which al-
lows us at training time to use BLEU in-
stead of an approximation thereof. Our
approach is theoretically sound and gives
better (up to +0.6%BLEU) and more sta-
ble results than the standard MERT opti-
mization algorithm. By comparing our ap-
proach to lattice MBR, we are also able to
gain crucial insights about both methods.
1 Introduction
According to statistical decision theory, the opti-
mal decision rule for any statistical model is the
solution that minimizes its risk (expected loss).
This solution is often referred to as the Minimum
Bayes Risk (MBR) solution (Kumar and Byrne,
2004). Since machine translation (MT) mod-
els are typically evaluated by BLEU (Papineni et
al., 2002), a loss function which rewards partial
matches, the MBR solution is to be preferred to
the Maximum A Posteriori (MAP) solution.
In most statistical MT (SMT) systems, MBR
is implemented as a reranker of a list1 of trans-
lations generated by a first-pass decoder. This de-
coder typically assigns unnormalised log probabil-
ities (known as scores) to each translation hypoth-
1We use the term list to denote any enumerable represen-
tation of translation hypotheses e.g n-best list, translation lat-
tice or forest.
esis, so these scores must be converted to proba-
bilities in order to apply MBR. In order to perform
this conversion, it is first necessary to compute the
normalization function Z. Since Z is defined as
an intractable sum over all possible translations, it
is approximated by summing over the translations
in the list. The second step is to find the correct
scale factor for the scores using a hyper-parameter
search over held-out data. This is needed because
the model parameters for the first-pass decoder are
normally learnt using MERT (Och, 2003), which
is invariant under scaling of the scores.
Both these steps are theoretically unsatisfactory
methods of estimating the posterior probability
distribution since the approximation to Z is an un-
bounded term and the scaling factor is an artificial
way of inducing a probability distribution.
Recently, (Tromble et al, 2008; Kumar et al,
2009) have shown that using a search lattice to im-
prove the estimation of the true probability distri-
bution can lead to improved MBR performance.
However, these approaches still rely on MERT for
training the base model, and in fact introduce sev-
eral extra parameters which must also be estimated
using either grid search or a second MERT run.
The lattice pruning required to make these tech-
niques tractable is quite drastic, and is in addi-
tion to the pruning already performed during the
search. Such extensive pruning is liable to render
any probability estimates heavily biased (Blunsom
and Osborne, 2008; Bouchard-Co?te? et al, 2009).
Here, we present a unified approach to training
and decoding in a phrase-based translation model
(Koehn et al, 2003) which keeps the objective
constant across the translation pipeline and so ob-
viates the need for any extra hyper-parameter fit-
ting. We use the phrase-based Gibbs sampler of
Arun et al (2009) at training time to compute the
gradient of our minimum risk training objective in
order to apply first-order optimization techniques,
365
and at test time we use it to estimate the posterior
distribution required by MBR (Section 3).
We experimented with two different objective
functions for training (Section 4). First, follow-
ing (Arun et al, 2009), we define our objective
at the sentence-level using a sentence-level variant
of BLEU. Then, in order to reduce the mismatch
between training and test loss functions, we also
tried directly optimising the expected corpus level
BLEU, where we introduce a novel sampling tech-
nique, which we call corpus sampling to calculate
the required expectations.
The methods presented in this paper are theo-
retically sound. Moreover, experimental evidence
on three language pairs shows that our training
regime is more stable than MERT, able to gener-
alize better and generally leads to improvement in
translation when used with sampling based MBR
(Section 5). An added benefit is that the trained
weights also lead to better performance when used
with a beam-search based decoder.
2 Inference methods for MT
We assume a phrase-based machine translation
model, defined with a log-linear form, with feature
function vector h and parametrized by weight vec-
tor ?, as described in Koehn et al (2003). The in-
put sentence, f , is segmented into phrases, which
are sequences of adjacent words. Each source
phrase is translated into the target language, to
produce an output sentence e and an alignment
a representing the mapping from source to target
phrases. Phrases are allowed to be reordered.
p(e, a|f ;?) =
exp [? ? h(e, a, f)]
?
?e?,a?? exp [? ? h(e
?, a?, f)]
(1)
MAP decoding under this model consists of
finding the most likely output string, e?:
e? = argmaxe
?
a?4(e,f)
p(e, a|f) (2)
where4(e, f) is the set of all derivations of output
string e given source string f .
Summing over all the derivations is intractable,
making approximations necessary. The most com-
mon of these approximations is the Viterbi approx-
imation, which simply chooses the most likely
derivation ?e?, a??. This approximation can be
computed in polynomial time via dynamic pro-
gramming (DP). Though fast and effective for
many problems, it has two serious drawbacks for
probabilistic inference. First, the error incurred
by the Viterbi maximum with respect to the true
model maximum is unbounded. Second, the DP
solution requires substantial pruning and restricts
the use of non-local features. The latter problem
persists even in the variational approximations of
Li et al (2009), which attempt to solve the former.
2.1 Gibbs sampling for phrase-based MT
An alternate approximate inference method for
phrase-based MT without any of the previously
mentioned drawbacks is the Gibbs sampler (Ge-
man and Geman, 1984) of Arun et al (2009)
which draws samples from the posterior distribu-
tion of the translation model. For the work pre-
sented in this paper, we use this sampler.
The sampler produces a sequence of samples,
SN1 = (e1, a1) . . . (eN , aN ), that are drawn from
the distribution p(e, a|f). These samples can be
used to estimate the expectation of a function
h(e, a, f) as follows:
Ep(a,e|f)[h] = lim
N??
1
N
N?
i=1
h(ai, ei, f) (3)
3 Decoding
In this work, we are interested in performing MBR
decoding with BLEU. We define the MBR decision
rule following Tromble et al (2008):
e? = argmax
e?H
?
e??E
BLEUe(e
?)p(e?|f) (4)
where H refers to the hypothesis space from
which translations are chosen, E refers to the
evidence space used for calculating risk and
BLEUe(e?) is a gain function that indicates the re-
ward of hypothesising e? when the reference solu-
tion is e.
To perform MBR decoding using the sampler,
let the function h in Equation 3 be the indica-
tor function h = ?(a, a?)?(e, e?). Then, Equa-
tion 3 provides an estimate of p(a?, e?|f), and using
h = ?(e, e?) marginalizes over all derivations a?,
yielding an estimate of p(e?|f). MBR is computed
at the sentence-level while BLEU is a corpus-level
metric, so instead we use a sentence-level approx-
imation of BLEU.2
The sampler can be used to perform two other
decoding tasks: the mode of the estimated dis-
tribution p(a?, e?|f) is the maximum derivation
(MaxDeriv) solution while the mode of p(e?|f) is
the maximum translation (MaxTrans) solution.
2The ngram precision counts are smoothed by adding 0.01
for n > 1
366
4 Minimum Risk Training
In order to train models suitable for use with Max-
Trans or MBR decoding, we need to employ a
training method which takes account of the whole
distribution. To this end, we employ minimum risk
training to find weights ? for Equation 1 that mini-
mize the expected loss on the training set. We con-
sider two variants of minimum risk training: sen-
tence sampling optimizes an objective defined at
the sentence level and corpus sampling a corpus-
based objective.
4.1 Sentence sampling
Since BLEU, the metric we care about, is a gain
function, our objective function maximizes the ex-
pected gain of our model. The expected gain, G
of a probabilistic translation model on a corpus D,
defined with respect to the gain function BLEUe?(e)
is given by
G =
?
?e?,f??D
?
e,a
p(e, a|f)BLEUe?(e) (5)
where e? is the reference translation, e is a hypoth-
esis translation and BLEU refers to the sentence-
level approximation of the metric.
Using the probabilistic formulation of Equation
1, the optimization of the objective in (5) is facil-
itated by the fact that it is continuous and differ-
entiable with respect to the model parameters ? to
give
?G
??k
=
?
?e?,f?
?D
?
e,a
BLEUe?(e)
?p
??k
where
?p
??k
=
(
hk ? Ep(e,a|f)[hk]
)
p(e, a|f)
(6)
Since the gradient is expressed in terms of ex-
pectations of feature values, it can easily be calcu-
lated using the sampler and then first-order opti-
mization techniques can be applied to find optimal
values of ?. Because of the noise introduced by
the sampler, we used stochastic gradient descent
(SGD), with a learning rate that gets updated after
each step proportionally to difference in succes-
sive gradients (Schraudolph, 1999).
While our initial formulation of minimum risk
training is similar to that of Arun et al (2009), in
preliminary experiments we observed a tendency
for translation performance on held-out data to
quickly increase to a maximum and then plateau.
Hypothesizing that we were being trapped in lo-
cal maxima as G is non-convex, we decided to
employ deterministic annealing (Rose, 1998) to
smooth the objective function to ensure that the
optimizer explored as large a region as possible of
the space before it settled on an optimal weight set.
Our instantiation of deterministic annealing (DA)
is based on the work of Smith and Eisner (2006),
and involves the addition of an entropic prior to
the objective in Equation 5 to give
G? =
?
?e?,f??D
[(
?
e,a
p(e, a|f)BLEUe?(e)
)
+ T.H(p)
]
where H(p) is the entropy of the probability dis-
tribution p(e, a|f), and T > 0 is a temperature
paramater which is gradually lowered as the opti-
mization progresses according to some annealing
schedule.
Differentiating with respect to ?k then shows
that the annealed gradient is given by the follow-
ing expression:
?
?e?,f?
?D
?
e,a
(BLEUe?(e)? T (1 + log p))
?p
??k
where
?p
??k
=
(
hk ? Ep(e,a|f)[hk]
)
p(e, a|f)
A high value of T leads the optimizer to find
weights which describe a fairly flat distribution,
whereas a lower value of T pushes the optimizer
towards a more peaked distribution. We perform
10 to 20 iterations of SGD at each temperature.
In their deterministic annealing formulation,
(Smith and Eisner, 2006; Li and Eisner, 2009), ex-
press the parameterization of the distribution ? as
??? (where ? is the scaling factor) and perform op-
timization in two steps, the first optimizing ?? and
the second optimizing ?. We experimented with
this two stage optimization process, but found that
simply performing an unconstrained optimization
on ? gave better results.
4.2 Corpus sampling
While the objective functions in Equations 5 and
4.1 use a sentence-level variant of BLEU, the
model?s test-time performance is evaluated with
corpus level BLEU. The lack of correlation be-
tween sentence-level BLEU and corpus BLEU is
well-known (Chiang et al, 2008a). Therefore, in
an effort to address this issue, we tried maximizing
expected corpus BLEU directly.
In other words, given a training corpus of the
form ?CF , CE?? where CF is a set of source sen-
tences and CE? its corresponding reference transla-
tions, we consider a gain function defined on the
367
hypothesized translation CE of the input CF with
respect to CE? .
The objective in equation 5 therefore becomes:
G =
?
CE
P (CE |CF )BLEUCE? (CE) (7)
The pair (CE , CF ) is denoted as a cor-
pus sample corresponding to a sequence
(e1, a1), . . . , (eN , aN ) of derivations of the
corresponding source strings f1, . . . , fN of
source corpus CF .
Although the sampler described in Section 2
generates samples at the sentence level, we can use
it to generate corpus samples by applying the fol-
lowing procedure (see Figure 1). For each source
sentence f i in the corpus, we generate a sequence
of samples (ei1, a
i
1), . . . , (e
i
n, a
i
n) using the sam-
pler. From each of these sequences of samples, we
then resample new sequences of derivation sam-
ples, one for each source sentence in the corpus.
The first corpus sample is then obtained by iter-
ating through the source sentences and taking the
first resampled derivation for each sentence, then
the second corpus sample by taking the second re-
sampled derivation, and so on. The resampling
step is necessary to eliminate any biases due to the
order of the generated samples.
The corpus sampling procedure invariably gen-
erates a set of samples which are all distinct and so
would give us a uniform estimate of the probabil-
ity distribution P (CE |CF ). However this is not a
problem since we are not interested in evaluating
the actual distribution; we just need to calculate
expectations of feature values and BLEU scores
over the distribution. The feature values of a cor-
pus sample are the average of the feature values of
its constituting derivations and its BLEU score is
computed based on the yield of its derivations.
When training using corpus sampling we pro-
cess the training corpus in batches ?CF , CE??, treat-
ing each batch as a corpus in its own right, and
updating the weights after each batch.
The gradient for the objective function in (7) is:
?G
??k
=
?
CE
BLEUCE? (CE)
?P
??k
where
?P
??k
=
(
hCk ? EP (CE |CF )[h
C
k ]
)
P (CE |CF )
where hCk is the k-th component of a corpus
sample feature vector.
During deterministic annealing for sentence
sampling, the entropy term is computed over the
f1 f2 f3
A D K
B E L
A F L
C G L
B H M
f1 f2 f3
A F L
B E L
S
A
M
P
L
E
 
F
R
O
M
 
E
M
P
I
R
I
C
A
L
 
D
I
S
T
R
I
B
U
T
I
O
N
Extract Corpus 
Samples
f1 f2 f3
{A, F, L }
Corpus Sample 1
{B, E, L }
Corpus Sample 2
SAMPLE FROM 
P(e,a | f)
Figure 1: Example illustrating the extraction of 2
corpus samples for a corpus of source sentences
f1, f2, f3. In the first step, we sample 5 deriva-
tions for each source sentence. We then resample
2 derivations from the empirical distributions of
each source sentence.
distribution p(e, a|f) of each individual sentence.
While corpus sampling, we are considering the
distribution P (CE |CF ) but the estimated distribu-
tion is always uniform. So we define the entropic
prior term over the distribution p(e, a|f) of the
sentences making up the corpus sample.
The annealed corpus sampling objective is
therefore:
?
CE
P (CE |CF )BLEUCE? (CE)+
T
|CF |
?
f?CF
H(p(e, a|f))
The gradient of this objective is of similar form
to the sentence sampling gradient in Equation (6).
5 Experiments
5.1 Training Data and Preparation
The experiments in this section were performed
using the Europarl section of the French-English
and German-English parallel corpora from the
WMT09 shared translation task (Callison-Burch et
al., 2009), as well as 300k parallel Arabic-English
sentences from the NIST MT evaluation train-
ing data.3 For all language pairs, we constructed
3The Arabic-English training data consists of the
eTIRR corpus (LDC2004E72), the Arabic news corpus
(LDC2004T17), the Ummah corpus (LDC2004T18), and the
368
a phrase-based translation model as described in
Koehn et al (2003), limiting the phrase length to
5. The target side of the parallel corpus was used
to train 3-gram language models. For the German
and French systems, the DEV2006 set was used
for model tuning and the first half of TEST2007
(in-domain) for heldout testing. Final testing was
performed on NEWS-DEV2009B (out-of-domain)
and the first half of TEST2008 (in-domain). For
the Arabic system, the MT02 set (10 reference
translations) was used for tuning and MT03 and
MT05 (4 reference translations, each) were used
for held-out testing and final testing respectively.
To reduce the size of the phrase table, we used the
association-score technique suggested by Johnson
et al (2007). Translation quality is reported using
case-insensitive BLEU.
5.2 Baseline
Our baseline system is phrase-based
Moses (Koehn et al, 2007) with feature weights
trained using MERT. Moses and the Gibbs
sampler use identical feature sets.4
The MERT optimization algorithm uses multi-
ple random restarts to avoid getting stuck in a poor
local optima. Therefore, every time MERT is run,
it produces a slightly different final weight vector
leading to varying test set results. While this char-
acteristic of MERT is typically ignored, we ac-
count for it by performing MERT training 10 times
for each of the 3 language pairs, decoding the test
sets with each of the 10 optimized weight sets. We
present the best and the worst test set results along
with the mean and the standard deviation (?) of
these results in Table 1. We report results using
the Moses implementation of Viterbi, nbest MBR
and lattice MBR decoding (Kumar et al, 2009). 5
For both nbest and lattice MBR decoding, the hy-
pothesis set was composed of the top 1000 unique
translations produced by the Viterbi decoder, and
the same 1000 translations were used as evidence
set for nbest MBR.
As Table 1 shows, translation results using
MERT optimized weights vary markedly from one
sentences with confidence c > 0.995 in the ISI automatically
extracted web parallel corpus (LDC2006T02).
4We use 5 translation model scores, distance-based distor-
tion, language model and word penalty. The reordering limit
is set to 6 for all experiments.
5For nbest and lattice MBR decoding, we optimized for
the scaling factor using a grid-search on held-out data. For
lattice MBR decoding, we optimized the lattice density and
set the p and r parameters as per Tromble et al (2008).
tuning run to the other, with results varying from
a range of 0.3% BLEU to 1.3% BLEU when using
Viterbi decoding. We also see that, bar in-domain
German to English, MBR decoding gives a small
improvement on all other datasets.
Surprisingly, lattice MBR only gives improve-
ments on two datasets and actually leads to a drop
in performance on the other 3 datasets. We discuss
possible reasons for this in Section 6.
5.3 Sentence sampling
At training time, the optimization algorithm is ini-
tialized with zero weights and the sampler is ini-
tialized with a random derivation from Moses. To
get rid of any initialization biases, the first 100
samples are discarded.6 We then run the sampler
for 1000 iterations after which we perform reheat-
ing whereby the distribution is progressively flat-
tened. Samples are not collected during this pe-
riod. Reheating allows the sampler more mobil-
ity around the search space thus possibly escaping
any local optima it might be trapped in. We subse-
quently run the sampler for 1000 more iterations.
We denote this procedure as running 2 chains of
the sampler. We use batch sizes of 96 randomly
selected sentences for SGD optimization.
During DA, our cooling schedule is an exponen-
tially decaying one with decay rate set to 0.9, per-
forming 20 iterations of SGD optimization at each
temperature setting. Five training runs were per-
formed and the BLEU scores averaged. The fea-
ture weights were output every 50 iterations and
performance measured on the heldout set by run-
ning the sampler as a decoder. At decode time,
we use the same sampler configurations as during
training but run 2 chains each for 5000 iterations.
For MBR decoding, we use the entirety of this
sample set as our evidence set and use the top 1000
most probable translations as the hypothesis set.
5.4 Corpus sampling
For our corpus sampling experiments, we sample
using the same procedure as in sentence sampling
but using 2 chains of 2000 iterations. We then
resample 2000 corpus samples from the empiri-
cal distribution estimated from the first 4000 sam-
ples. For Arabic-English training, we used batch
sizes of 100 randomly selected sentences for ex-
periments without DA and batches of 400 random
6This procedure is referred to as burn-in in the MCMC
literature.
369
Viterbi nMBR lMBR
min max mean ? min max mean ? min max mean ?
AR-EN MT05 43.7 44.3 44.0 0.17 44.2 44.5 44.4 0.13 44.2 44.6 44.5 0.12
FR-EN In 33.1 33.4 33.3 0.10 33.2 33.6 33.4 0.12 32.3 32.7 32.6 0.13
FR-EN Out 19.1 19.6 19.4 0.18 19.3 19.7 19.5 0.12 19.1 19.4 19.3 0.12
DE-EN In 27.6 27.9 27.8 0.10 27.6 27.9 27.7 0.10 27.2 27.5 27.4 0.10
DE-EN Out 14.9 16.2 15.7 0.33 15.0 16.3 15.7 0.33 15.3 16.4 16.0 0.30
Table 1: Baseline results - MERT trained models decoded using Viterbi, nbest MBR (nMBR) and lattice
MBR (lMBR). MERT was run 10 times for each language pair. We report minimum, maximum, mean
and standard deviation of test set BLEU scores across the 10 runs.
200 400 600 80015
20
25
30
Training iterations
Bleu
Sentence Sampling, Without DA
l l l l l l l l l l l l l l l l l
27.4
l MaxDerivMaxTransMBR 100 300 500 70015
20
25
30
Training iterations
Bleu
Sentence Sampling, With DA
l
l
l
l
l l
l l l l l l l l l 28.2
l MaxDerivMaxTransMBR 50 100 150 200 25015
20
25
30
Training iterations
Bleu
Corpus Sampling, Without DA
l l l l l l l l l l l l l 28.1
l MaxDerivMaxTransMBR 50 150 250 35015
20
25
30
Training iterations
Bleu
Corpus Sampling, With DA
l l
l l
l l l
l l l l l l l l l l 28.5
l MaxDerivMaxTransMBR
Figure 2: Heldout performance for German-English training averaged across 5 minimum risk training
runs. Best scores achieved are indicated by dotted line.
sentences with DA. The size of the batches cor-
responds to the number of sentences that form a
corpus sample. For German/French to English ex-
periments, we used batches of 100 random sen-
tences for training with and without DA. We per-
form 10 optimizations at each temperature setting
during deterministic annealing. Test time condi-
tions are identical to the sentence sampling ones
and we measure performance on a held-out set af-
ter every 20 iterations of the learner.
5.5 Results
Figures 2 and 3 show the scores on the German-
English and Arabic-English held-out sets respec-
tively comparing all four training regimes: corpus
vs sentence sampling, DA vs without DA. Results
for French-English training are similar.
We focus our analysis on the Arabic-English ex-
perimental setup. Without deterministic anneal-
ing, the learner converges quickly, usually after
just 20 iterations, after which performance de-
grades steadily. The magnitudes of the weights
are large, sharpening the distribution. There is
not much diversity amongst the sampled deriva-
tions, i.e. the entropy of the sample set is low.
Therefore, all 3 decoding regimes give very simi-
lar results. With the addition of the entropic prior,
the model is slow to converge before the so-called
phase transition occurs (usually after around 50
iterations), after which performance goes up to
reach a peak (45.2 BLEU) higher than that without
the prior (44.2 BLEU), before steadily declining.
The entropic prior encourages diversity among the
sample set, especially at high temperature settings.
In the presence of diversity, the benefits of
marginalization over derivations is clear: Max-
Trans does better than MaxDeriv and MBR does
best, confirm recent findings of (Blunsom et al,
2008; Arun et al, 2009) that MaxTrans improves
over MaxDeriv decoding for models trained to ac-
count for multiple derivations. As the temperature
decreases to zero, the model sharpens, effectively
intent on maximizing one-best performance and
thus voiding the benefits of MaxTrans and MBR.
Figures 2 and 3 also show that corpus sampling
improves over sentence sampling, although not by
much (+ 0.3 BLEU).
5.6 Comparison with MERT baseline
Having established the superiority of the pipeline
of expected corpus BLEU training with DA fol-
lowed by MBR decoding over other alternatives
considered, we compare it to the best results ob-
tained with MERT optimized Moses (bold scores
from Table 1). To account for sampler variance
during both training and decoding, we average
scores across 50 runs; 10 decoding runs each using
the best weight set from 5 training runs. Results
370
0 200 600 1000 140030
35
40
45
50
Training iterations
Bleu
Sentence Sampling, Without DA
l l l l l l l l l l l l l l l l l l l l l l l l l l l l l
44.2
l MaxDerivMaxTransMBR 0 500 1000 150030
35
40
45
50
Training iterations
Bleu
Sentence Sampling With DA
l
l l l
l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l l 45.2
l MaxDerivMaxTransMBR 20 40 60 80 100 14030
35
40
45
50
Training iterations
Bleu
Corpus Sampling, Without DA
l l l l l l l 44.5
l MaxDerivMaxTransMBR 100 200 300 40030
35
40
45
50
Training iterations
Bleu
Corpus Sampling, With DA
l
l l
l l l l l l l l l l l l l l l l l l 45.5
l MaxDerivMaxTransMBR
Figure 3: Heldout performance for Arabic-English training averaged across 5 minimum risk training
runs. Best scores achieved are indicated by dotted line.
are shown in Table 2.7
We observe that on 3 out of 5 datasets, the sam-
pler results are much more stable than MERT and
as stable on the other 2 datasets. We attribute the
improved stability to the more powerful optimiza-
tion algorithm used by the sampler which uses gra-
dient information to steer the model towards better
weights. MERT, alternatively, optimizes one fea-
ture at a time using line search and therefore does
not explore the full feature space as thoroughly.
Translation results with the sampler are better
than with MERT on 2 datasets, are equal on an-
other 2 and worse in one case. The improvements
withe the sampler are obtained in the case of out-
of-domain data suggesting that the minimum risk
training objective generalizes better than the 1-
best objective of MERT.
MERT/Moses Sampler
Test set Best ? MBR ?
AR-EN MT05 44.5 (lMBR) 0.12 44.5 0.14
FR-EN In 33.4 (nMBR) 0.12 33.2 0.06
FR-EN Out 19.5 (nMBR) 0.12 19.8 0.05
DE-EN In 27.8 (Viterbi) 0.10 27.8 0.11
DE-EN Out 16.0 (lMBR) 0.30 16.6 0.12
Table 2: Final results comparing MERT/Moses
pipeline with unified sampler pipeline. Sampler
uses corpus sampling during training and MBR
decoding at test time. Moses results are aver-
aged across decoding runs using weights from
10 MERT runs and sampler results are averaged
across 10 decoding runs for each of 5 different
training runs. We report BLEU scores and standard
deviation (?).
7The MBR decoding times, averaged over 10 decoding
runs of 50 sentences each, are 10 secs/sent for Moses nbest
MBR, 40 secs/sent for Moses lattice MBR and 180 secs/sent
for the sampler.
Viterbi nMBR lMBR Sampler
MBR
AR-EN MT05 44.2 44.4 44.8 44.8
FR-EN In 33.1 33.2 33.3 33.3
FR-EN Out 19.6 19.8 19.9 19.9
DE-EN In 27.7 27.9 28.0 28.0
DE-EN Out 16.0 16.3 16.6 16.6
Table 3: Comparison of decoding methods using
expected BLEU trained weights. We report Viterbi,
nbest MBR (nMBR) and lattice MBR (lMBR) de-
coding scores vs best sampler MBR decoding per-
formance. We selected the best weight set based
on performance on heldout data.
5.7 Moses with expected BLEU weights
In a final set of experiments, we reran the Moses
decoder this time using weights obtained through
expected BLEU optimization. Here, for each lan-
guage pair, we picked the weight set that gave the
best results on held-out data. Note that the results
which we show in Table 3 are over one run only,
so are not strictly comparable to those in Table 2
which are averaged over several training and de-
coding runs. We also report the best results ob-
tained with the sampler MBR decoder using these
weights.
In contrast to Table 1, here we see a consistent
improvement across all test-sets when going from
Viterbi decoding to n-best then to lattice MBR.
Except for in-domain French-English, the transla-
tion results are superior to the best scores shown
(in bold) in Table 1, confirming that the minimum
risk training objective is able to find good weight
sets. Interestingly, we also observe that sampler
MBR gets the same exact results for all test sets as
lattice MBR.
371
6 Discussion
We have shown that the sampler of Arun et al
(2009) can be used to perform minimum risk train-
ing over an unpruned search space. Our pro-
posed corpus sampling technique, like MERT, is
able to optimize corpus BLEU directly whereas
alternate parameter estimation techniques usually
employed in SMT optimize approximations of
BLEU. Chiang et al (2008b) accounts for the on-
line nature of the MIRA optimization algorithm
by smoothing the sentence-level BLEU precision
counts of a translation with a weighted average of
the precision counts of previously decoded sen-
tences, thus approximating corpus BLEU. As
for minimum risk training, prior implementations
have either used sentence-level BLEU (Zens et al,
2007) or a linear approximation to BLEU (Smith
and Eisner, 2006; Li and Eisner, 2009).
At test time, the sampler works best as an MBR
decoder, but also allows us to verify past claims
about the benefits of marginalizing over align-
ments during decoding. We compare the sam-
pler MBR decoder?s performance against MERT-
optimized Moses run under three different decod-
ing regimes, finding that the sampler does as well
or better on 4 out of 5 datasets.
Our training and testing pipeline has the advan-
tage of being able to handle a large number of both
local and global features so we expect in the future
to outperform the standard MERT and dynamic
programming-based search pipeline further.
As shown in Section 5.2, lattice MBR in some
cases leads to a marked drop in performance. (Ku-
mar et al, 2009) mention that the linear approx-
imation to BLEU used in their lattice MBR algo-
rithm is not guaranteed to match corpus BLEU, es-
pecially on unseen test sets. To account for these
cases, they allow their algorithm to back-off to the
MAP solution. One possible reason for the drop
in performance in our lattice MBR experiments is
that the implementation we use does not employ
this back-off strategy.
Table 3 provides valuable insights as to the mer-
its of the lattice MBR approach versus our own
sampling based pipeline. Firstly, whereas with
MERT optimized weights, the benefits of lattice
MBR are debatable (Table 1), running Moses with
minimum risk trained weights gives results that
are in line with what we would expect - lattice
MBR does systematically better than competing
decoding algorithms. This suggests that the unbi-
ased minimum risk training criterion used by the
sampler is a better fit for lattice MBR than the
MERT criterion, and also that the mismatch be-
tween linear and corpus BLEU mentioned before
might not be the reason for the results in Table 1.
Secondly, we find that sampling MBR matches
lattice MBR on the minimum risk trained weights.
The MBR sampler uses samples drawn from the
distribution as hypothesis and evidence sets, typi-
cally 1000 samples for the former and 10000 sam-
ples for the latter. In the lattice MBR experiments
of Tromble et al (2008), it is shown that this size
of hypothesis set is sufficient. Their evidence set,
however, is significantly larger than ours.8Table 3
suggests that, since it is not biased by heuris-
tic pruning, the sampler?s limited evidence set is
enough to give a good estimate of the probabil-
ity distribution whereas beam-search based MBR
needs to scale from using n-best lists to lattices to
get equivalent results.
Sampling the phrase-based model is expensive,
meaning that lattice MBR is still faster (around
4x) to run than sampler MBR. However, due to
the unified nature of the training and decoding cri-
terion in our approach, the minimum risk trained
weights can be plugged directly into the sam-
pler MBR decoder, whereas lattice MBR requires
an additional expensive step of tuning the model
hyper-parameters (Kumar et al, 2009).
In future work, we also intend to look at more
efficient ways of generating samples. One pos-
sibility is to interleave Gibbs sampling steps us-
ing low order ngram language model distributions
with Metropolis-Hasting steps that use higher or-
der language model distributions.
7 Related Work
Expected BLEU training for phrase-based models
has been successfully attempted by (Smith and
Eisner, 2006; Zens et al, 2007), however they both
used biased n-best lists to approximate the pos-
terior distribution. Li and Eisner (2009) present
work on performing expected BLEU training with
deterministic annealing on translation forests gen-
erated by Hiero (Chiang, 2007). Since BLEU does
not factorize over the search graph, they use the
linear approximation of Tromble et al (2008) in-
stead.
Pauls et al (2009) present an alternate training
criterion over translation forests called CoBLEU,
8up to 1081 as per Tromble et al (2008)
372
similar in spirit to expected BLEU training, but
aimed to maximize the expected counts of n-grams
appearing in reference translations. This training
criterion is used in conjunction with consensus de-
coding (DeNero et al, 2009), a linear-time ap-
proximation of MBR.
In contrast to the approaches above, the algo-
rithms presented in this paper are able to explore
an unpruned search space. By using corpus sam-
pling, we can perform minimum risk training with
corpus BLEU rather than any approximations of
this metric. Also, since we maintain a probabilis-
tic formulation across training and decoding, our
approach does not require a grid-search for a scal-
ing factor as in Tromble et al (2008).
8 Conclusions
We have presented a unified approach to the task
of parameter estimation and decoding for a phrase-
based system using the standard translation eval-
uation metric, BLEU. Using a Gibbs sampler to
explore the entire probability distribution allows
us to implement two probabilistic sound algo-
rithms, minimum risk training and its equivalent,
MBR decoding, in an unbiased way. The proba-
bilistic formulation also allows us to use gradient
based optimization techniques which produce sta-
ble model parameters. At decoding time, we show
the benefits of marginalizing over derivations and
that MBR gives better results than other decoding
criteria.
Since our optimization algorithm can cope with
a large number of features, in future work, we
plan to incorporate more expressive features in
the model. We use a Gibbs sampler for inference
so there is scope for exploring non-local features
which might not easily be added to dynamic pro-
gramming based models.
Acknowledgments
This research was supported in part by the GALE pro-
gram of the Defense Advanced Research Projects Agency,
Contract No. HR0011-06-2-001; and by the EuroMa-
trix project funded by the European Commission (6th
Framework Programme). The project made use of
the resources provided by the Edinburgh Compute and
Data Facility (http://www.ecdf.ed.ac.uk/). The
ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
References
Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blunsom,
Adam Lopez, and Philipp Koehn. 2009. Monte carlo in-
ference and maximization for phrase-based translation. In
Proceedings of CoNLL, pages 102?110.
Phil Blunsom and Miles Osborne. 2008. Probabilistic infer-
ence for machine translation. In Proc. of EMNLP 2008.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008. A
discriminative latent variable model for statistical machine
translation. In Proc. of ACL-HLT.
Alexandre Bouchard-Co?te?, Slav Petrov, and Dan Klein.
2009. Randomized pruning: Efficiently calculating ex-
pectations in large dynamic programs. In Advances in
Neural Information Processing Systems 22, pages 144?
152.
Chris Callison-Burch, Philipp Koehn, Christoph Monz, and
Josh Schroeder, editors. 2009. Proc. of Workshop on Ma-
chine Translations.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008a. Decomposability of transla-
tion metrics for improved evaluation and efficient algo-
rithms. In Proceedings of the 2008 Conference on Empir-
ical Methods in Natural Language Processing, pages 610?
619, Honolulu, Hawaii, October. Association for Compu-
tational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008b. On-
line large-margin training of syntactic and structural trans-
lation features. In Proceedings of the 2008 Conference
on Empirical Methods in Natural Language Processing,
pages 224?233, Honolulu, Hawaii, October. Association
for Computational Linguistics.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2):201?228.
John DeNero, David Chiang, and Kevin Knight. 2009. Fast
consensus decoding over translation forests. In Proceed-
ings of ACL/AFNLP, pages 567?575.
Stuart Geman and Donald Geman. 1984. Stochastic relax-
ation, Gibbs distributions and the Bayesian restoration of
images. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence, 6:721?741.
J.H. Johnson, J. Martin, G. Foster, and R. Kuhn. 2007.
Improving translation quality by discarding most of the
phrasetable. In Proc. of EMNLP-CoNLL, Prague.
P. Koehn, F.J. Och, and D. Marcu. 2003. Statistical phrase-
based translation. In Proc. of HLT-NAACL, pages 48?54,
Morristown, NJ, USA.
P. Koehn, H. Hoang, A. Birch Mayne, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst.
2007. Moses: Open source toolkit for statistical machine
translation. In Proceedings of ACL Demos, pages 177?
180.
S. Kumar and W. Byrne. 2004. Minimum Bayes-risk decod-
ing for statistical machine translation. In Processings of
HLT-NAACL.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and Franz
Och. 2009. Efficient minimum error rate training and
minimum bayes-risk decoding for translation hypergraphs
and lattices. In Proceedings of ACL/AFNLP, pages 163?
171.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-risk
training on translation forests. In Proceedings of EMNLP,
pages 40?51.
373
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009. Vari-
ational decoding for statistical machine translation. In
Proceedings of ACL/AFNLP, pages 593?601.
F. Och. 2003. Minimum error rate training in statistical ma-
chine translation. In Proceedings of ACL, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a method for automatic evaluation of machine
translation. In Proceedings of ACL, pages 311?318.
Adam Pauls, John Denero, and Dan Klein. 2009. Consensus
training for consensus decoding in machine translation. In
Proceedings of EMNLP, pages 1418?1427.
Kenneth Rose. 1998. Deterministic annealing for clustering,
compression, classification, regression, and related opti-
mization problems. In Proceedings of the IEEE, pages
2210?2239.
Nicol N. Schraudolph. 1999. Local gain adaptation in
stochastic gradient descent. Technical Report IDSIA-09-
99, IDSIA.
David A. Smith and Jason Eisner. 2006. Minimum risk an-
nealing for training log-linear models. In Proceedings of
COLING-ACL, pages 787?794.
Roy Tromble, Shankar Kumar, Franz Och, and Wolfgang
Macherey. 2008. Lattice Minimum Bayes-Risk decod-
ing for statistical machine translation. In Proceedings of
EMNLP, pages 620?629.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007. A sys-
tematic comparison of training criteria for statistical ma-
chine translation. In Proceedings of EMNLP, pages 524?
532.
374
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 261?271,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
SampleRank Training for Phrase-Based Machine Translation
Barry Haddow
School of Informatics
University of Edinburgh
bhaddow@inf.ed.ac.uk
Abhishek Arun
Microsoft UK
abarun@microsoft.com
Philipp Koehn
School of Informatics
University of Edinburgh
pkoehn@inf.ed.ac.uk
Abstract
Statistical machine translation systems are
normally optimised for a chosen gain func-
tion (metric) by using MERT to find the best
model weights. This algorithm suffers from
stability problems and cannot scale beyond
20-30 features. We present an alternative al-
gorithm for discriminative training of phrase-
basedMT systems, SampleRank, which scales
to hundreds of features, equals or beats MERT
on both small and medium sized systems, and
permits the use of sentence or document level
features. SampleRank proceeds by repeatedly
updating the model weights to ensure that the
ranking of output sentences induced by the
model is the same as that induced by the gain
function.
1 Introduction
In phrase-based machine translation (PBMT), the
standard approach is to express the probability dis-
tribution p(a, e|f) (where f is the source sentence
and (a, e) is the aligned target sentence) in terms of
a linear model based on a small set of feature func-
tions
p(a, e|f) ? exp
(
n?
i=1
wihi(a, e, f)
)
(1)
The feature functions {hi} typically include log
probabilities of generative models such as trans-
lation, language and reordering, as well as non-
probabilistic features such as word, phrase and dis-
tortion penalties. The feature weights w = {wi}
are normally trained using MERT (minimum error
rate training) (Och, 2003), to maximise performance
as measured by an automated metric such as BLEU
(Papineni et al, 2002). MERT training uses a par-
allel data set (known as the tuning set) consisting of
about 1000-2000 sentences, distinct from the data
set used to build the generative models. Optimis-
ing the weights in Equation (1) is often referred to
as tuning the MT system, to differentiate it from the
process of training the generative models.
MERT?s inability to scale beyond 20-30 features,
as well as its instability (Foster and Kuhn, 2009)
have led to investigation into alternative ways of
tuning MT systems. The development of tuning
methods is complicated, however by, the use of
BLEU as an objective function. This objective in
its usual form is not differentiable, and has a highly
non-convex error surface (Och, 2003). Furthermore
BLEU is evaluated at the corpus level rather than at
the sentence level, so tuning methods either have to
consider the entire corpus, or resort to a sentence-
level approximation of BLEU. It is unlikely, how-
ever, that the difficulties in discriminative MT tun-
ing are due solely to the use of BLEU as a metric ?
because evaluation of translation is so difficult, any
reasonable gain function is likely to have a complex
relationship with the model parameters.
Gradient-based tuning methods, such as mini-
mum risk training, have been investigated as pos-
sible alternatives to MERT. Expected BLEU is nor-
mally adopted as the objective since it is differen-
tiable and so can be optimised by a form of stochas-
tic gradient ascent. The feature expectations re-
quired for the gradient calculation can be obtained
from n-best lists or lattices (Smith and Eisner, 2006;
Li and Eisner, 2009), or using sampling (Arun et al,
2010), both of which can be computationally expen-
sive.
261
Margin-based techniques such as perceptron
training (Liang et al, 2006) and MIRA (Chiang et
al., 2008; Watanabe et al, 2007) have also been
shown to be able to tune MT systems and scale to
large numbers of features, but these generally in-
volve repeatedly decoding the tuning set (and so
are expensive) and require sentence-level approxi-
mations to the BLEU objective.
In this paper we present an alternative method of
tuning MT systems known as SampleRank, which
has certain advantages over other methods in use to-
day. SampleRank operates by repeatedly sampling
pairs of translation hypotheses (for a given source
sentence) and updating the feature weights if the
ranking induced by the MT model (1) is different
from the ranking induced by the gain function (i.e.
BLEU). By considering the translation hypotheses
in batches, it is possible to directly optimise corpus
level metrics like BLEU without resorting to sentence
level approximations.
Tuning using SampleRank does not limit the size
of the feature set in the same way as MERT does,
and indeed it will be shown that SampleRank can
successfully train a model with several hundred fea-
tures. Using just the core PBMT features and train-
ing using SampleRank will be shown to achieve
BLEU scores which equal or exceed those produced
by MERT trained models.
Since SampleRank does not require repeated de-
coding of the tuning set, and is easily parallelisable,
it can run at an acceptable speed, and since it always
maintains a complete translation hypothesis, it opens
up the possibility of sentence or document level fea-
tures1.
2 Method
2.1 SampleRank Training
SampleRank (Culotta, 2008; Wick et al, 2009) is
an online training algorithm that was introduced for
parameter learning in weighted logics, and has been
applied to complex graphical models (Wick et al,
2011). Assume a probabilistic model p(y|x) admit-
ting a log-linear parametrisation
p(y|x) ? exp
?
i
(wi?i(x, y)) (2)
1As long as the batches described in Section 2.2.1 respect
document boundaries.
where {?i} are a set of feature functions and {wi}
are corresponding feature weights. SampleRank can
be used to optimise the feature weights to maximise
a given gain function.
SampleRank is a supervised training algorithm,
requiring a set of labelled training data D =
{(x1, y1}, . . . , (xn, yn)}, where the xi are the inputs
and the yi the outputs. The algorithm works by con-
sidering each training example (xi, yi) in turn, and
repeatedly sampling pairs of outputs from a neigh-
bourhood defined in the space of all possible out-
puts, updating the weights when the ranking of the
pair due to the model scores is different from the
ranking due to the gain function. So if the sampled
pair of outputs for xi is (y, y?), where p(y?|xi) >
p(y|xi), the weights are updated iff gain(y?, yi) <
gain(y, yi).
The sampled pairs are drawn from a chain which
can be constructed in a similar way to an MCMC
(Markov Chain Monte Carlo) chain.
In (Culotta, 2008) different strategies are explored
for building the chain, choosing the neighbourhood
and updating the weights.
2.2 SampleRank Training for Machine
Translation
We adapted SampleRank for the tuning of PBMT
systems, as summarised in Algorithm 1. The defi-
nitions of the functions in the algorithm (described
in the following subsections) draw inspiration from
work on MIRA training for MT (Watanabe et al,
2007; Chiang et al, 2008). SampleRank is used to
optimise the parameter weights in (1) using the tun-
ing set.
2.2.1 Gain Function
The first thing that needs to be defined in Algo-
rithm 1 is the gain function. For this we use BLEU,
the most popular gain function for automated MT
evaluation, although the procedure described here
will work with any gain function that can be evalu-
ated quickly. Using BLEU, however, creates a prob-
lem, as BLEU is defined at the corpus level rather
than the sentence level, and in previous work on
SampleRank, the training data is processed one ex-
ample at a time. In other work on online train-
ing for SMT, (Liang et al, 2006; Chiang et al,
2008), sentence-level approximations to BLEU were
262
Algorithm 1 The SampleRank algorithm for tuning
phrase-based MT systems.
Require: Tuning data:
D = {(f1, e1), . . . , (fn, en)}
Require: gain(y, y?): A function which scores a
set of hypotheses (y?) against a set of references
(y).
Require: score(x, y): A function which computes
a model score for a set of hypotheses y and
source sentences x.
1: for epoch = 1 to number of epochs do
2: A? D
3: while A is non-empty do
4: Pick (x, y), a batch of sentence pairs, ran-
domly from A, and remove.
5: Initialise y0, a set of translation hypotheses
for x.
6: for s = 1 to number of samples do
7: N ? ChooseNeighbourhood(ys?1)
8: y? ? ChooseSample(N)
9: y+ ? ChooseOracle(N)
10: if gain(y,y
?)?gain(y,y+)
score(x,y?)?score(x,y+) < 0 then
11: UpdateWeights()
12: end if
13: ys ? y?
14: end for
15: end while
16: end for
employed, however in this work we directly opti-
mise corpus BLEU by processing the data in small
batches. Using batches was found to work better
than processing the data sentence by sentence.
So the while loop in Algorithm 1 iterates through
the tuning data in batches of parallel sentences,
rather than single sentences. One complete pass
through the tuning data is known as an epoch, and
normally SampleRank training is run for several
epochs. The gain on a particular batch is calcu-
lated by scoring the current set of hypotheses for
the whole batch against the references for that batch.
When calculating BLEU, a smoothing constant of
0.01 is added to all counts in order to avoid zero
counts.
2.2.2 Sample Generation
For each iteration of the while loop in Algo-
rithm 1, a new batch of parallel sentences is cho-
sen from the tuning set, and a corresponding new
set of translation hypotheses must be generated (the
y0 in line 5 of Algorithm 1). These initial hypothe-
ses are generated by glossing. For each word in the
source, the most likely translation option (according
to the weighted phrase-internal score) is selected,
and these translations are joined together monoton-
ically. This method of initialisation was chosen be-
cause it was simple and fast, and experiments with
an alternative method of initialisation (where the de-
coder was run with random scores assigned to hy-
potheses) showed very little difference in perfor-
mance.
Once the initial set of hypotheses for the new
batch is created, the SampleRank innermost loop
(lines 6-14 in Algorithm 1) proceeds by repeatedly
choosing a sample hypothesis set (y?) and an oracle
hypothesis set (y+), corresponding to the source side
of the batch (x).
Given the current hypothesis set ys?1 =
(e1, . . . , ek), the sample and oracle are chosen as
follows. Firstly, a hypothesis ej is selected randomly
from ys?1 , and a neighbourhood of alternate hy-
potheses N 3 ej generated using operators from
Arun et al (2009) (explained shortly). Model scores
are calculated for all the hypotheses in N , converted
to probabilities using Equation (1), and a sample e?j
taken from N using these probabilities. The sam-
ple hypothesis set (y?) is then the current hypothesis
set (ys?1) with ej replaced by e?j . The oracle is cre-
ated, analogously Chiang et al (2008), by choosing
e+j ? N to maximise the sum of gain (calculated on
the batch) and model score. The oracle hypothesis
set (y+) is then ys?1 with ej replaced by e
+
j .
We now describe how the neighbourhood is cho-
sen. Given a single hypothesis ej , a neighbourhood
is generated by first randomly choosing one of the
two operators MERGE-SPLIT or REORDER, then ran-
domly choosing a point of application for the op-
erator, then applying it to generate the neighbour-
hood. The MERGE-SPLIT operator can be applied
at any inter-word position, and generates its neigh-
bourhood by listing all hypotheses obtained by op-
tionally merging or splitting the phrases(s) touching
263
that position, and retranslating them. The REORDER
operator applies at a pair of target phrases (subject
to distortion limits) and generates a neighbourhood
containing two hypotheses, one with the original or-
der and one with the chosen phrases swapped. The
distortion limits and translation option pruning used
by the operators matches those used in decoding, so
together they are able to explore the same hypothe-
sis space as the decoder. A fuller explanation of the
two operators is give in Arun et al (2009).
2.2.3 Weight Updates
After choosing the sample and oracle hypothe-
sis set (y? and y+), the weight update may be per-
formed. The weights of the model are updated if the
relative ranking of the sample hypothesis set and the
oracle hypothesis set provided by the model score is
different from that provided by the gain. The model
score function score(x, y) is defined for a hypothe-
sis set y = e1, . . . ek as follows:
score(x, y) =
k?
j=1
(
n?
i=1
wihi(aj , ej , fj)
)
(3)
where x = f1, . . . fk are the corresponding source
sentences. The weight update is performed iff
score(x, y?) 6= score(x, y+) and the following con-
dition is satisfied:
gain(y, y?)? gain(y, y+)
score(x, y?)? score(x, y+)
< 0 (4)
where the gain() function is just the BLEU score.
The weight update used in this work is a MIRA-
like update from ws?1 to ws defined as follows:
ws = argmin
w
(?w ?ws?1?+ C?) (5)
subject to
scorew(x, y
+)? scorew(x, y
?) + ?
?M ? (gain(y, y+)? gain(y, y?))
(6)
The margin scaling M is set to be gain(y, y+), so
that ranking violations of low BLEU solutions are as-
signed a lower importance than ranking violations of
high BLEU solutions. The ? in (5) is a slack variable,
whose influence is controlled by C (set to 0.01), and
which has the effect of ?clipping? the magnitude of
the weight updates. Since there is only one con-
straint, there is no need to use an iterative method
such as Hildreth?s, because it is straightforward to
solve the optimisation in (5) and (6) exactly using its
Lagrangian dual, following (Crammer et al, 2006).
The weight update is then given by
ws = ws?1 + min
(
b
?a?2
, C
)
a
where a = h(a+j , e
+
j , fj)? h(a
?
j , e
?
j , fj)
and b = M
(
gain(y, y+)? gain(y, y?)
)
?
(
score(x, y+)? gain(y, y?)
)
After updating the weights, the current hypothesis
set (ys) is updated to be the sample hypothesis set
(y?), as in line 13 of Algorithm 1, and then the next
sample is generated.
2.2.4 Implementation Considerations
After each iteration of the inner loop of Algorithm
1, the weights are collected, and the overall weights
output by the tuning algorithm are the average of all
these collected weights. When each new batch is
loaded at the start of the inner loop, a period of burn-
in is run, analogous to the burn-in used in MCMC
sampling, where no weight updates are performed
and weights are not collected.
In order to help the stability of the tuning algo-
rithm, and to enable it to process the tuning data
more quickly, several chains are run in parallel, each
with their own set of current weights, and each pro-
cessing a distinct subset of the tuning data. The
weights are mixed (averaged) after each epoch. The
same technique is frequently adopted for the aver-
aged perceptron (McDonald et al, 2010).
3 Experiments
3.1 Corpora and Baselines
The experiments in this section were conducted with
French-English and German-English sections of the
WMT20112 shared task data. In particular, we used
News-Commentary data (nc11), and Europarl data
(ep11) for training the generative models. Phrase
tables were built from lowercased versions of the
2http://www.statmt.org/wmt11/
264
parallel texts using the standard Moses3 training
pipeline, with the target side of the texts used to
build Kneser-Ney smoothed language models using
the SRILM toolkit4. These data sets were used to
build two phrase-based translation systems: WMT-
SMALL and WMT-LARGE.
The WMT-SMALL translation system uses a trans-
lation model built from just the nc11 data (about
115,000 sentences), and a 3-gram language model
built from the target side of this data set. The fea-
tures used in the WMT-SMALL translation system
were the five Moses translation features, a language
model feature, a word penalty feature and a distor-
tion distance feature.
To build the WMT-LARGE translation system, both
the ep11 data set and the nc11 data set were con-
catenated together before building the translation
model out of the resulting corpus of about 2 mil-
lion sentences. Separate 5-gram language models
were built from the target side of the two data sets
and then they were interpolated using weights cho-
sen to minimise the perplexity on the tuning set
(Koehn and Schroeder, 2007). In the WMT-LARGE
system, the eight core features were supplemented
with the six features of the lexicalised reordering
model, which was trained on the same data as was
used to build the translation model. Whilst a train-
ing set size of 2 million sentences would not nor-
mally be sufficient to build a competitive system for
an MT shared task, it is sufficient to show that how
SampleRank training performs on a realistic sized
system, whilst still allowing for plenty of experime-
nation with the algorithm?s parameters.
For tuning, the nc-devtest2007 was used,
with the first half of nc-test2007 corpus
used for heldout testing and nc-test2008 and
newstest2010 reserved for final testing. The
tuning and heldout sets are about 1000 sentences in
size, whereas the final test sets are approximately
2000 sentences each.
In Table 1, the performance (in BLEU5) of
untrained and MERT-tuned models on the
heldout set is shown6. The untuned models
3http://www.statmt.org/moses/
4http://www-speech.sri.com/projects/
srilm/
5Calculated with multi-bleu.perl
6All BLEU scores and standard deviations are rounded to one
use the default weights output by the Moses
train-model.perl script, whereas the perfor-
mance of the tuned models is the mean across five
different MERT runs.
All decoding in this paper is with Moses, using
default settings.
Pair System untuned MERT-tuned
fr-en WMT-SMALL 28.0 29.2 (0.2)
WMT-LARGE 29.4 32.5 (0.1)
de-en WMT-SMALL 25.0 25.3 (0.1)
WMT-LARGE 26.6 26.8 (0.2)
Table 1: Untrained and MERT-trained performance
on heldout. MERT training is repeated five times,
with the table showing the mean BLEU, and standard
deviation in brackets.
3.2 SampleRank Training For Small Models
First we look at how SampleRank training compares
to MERT training using the WMT-SMALL models.
Using the smaller models allows reasonably quick
experimentation with a large range of different pa-
rameter settings.
For these experiments, the epoch size is set at
1024, and we vary both the number of cores and the
number of samples used in training. The number of
cores n is set to either 1,2,4,8 or 16, meaning that
each epoch we split the tuning data into n different,
non-overlapping shards, passing a different shard to
each process, so the shard size k is set to 1024/n. In
each process, a burn of 100 ?k samples is run (with-
out updating the weights), followed by either 100?k
or 500?k samples with weight updates, using the al-
gorithm described in Section 2.2. After an epoch is
completed, the current weights are averaged across
all processes to give the new current weights in each
process. At intervals of 50000 samples in each core,
weights are averaged across all samples so far, and
across all cores, and used to decode the heldout set
to measure performance.
In Figure 1, learning curves are shown for the
100 sample-per-sentence case, for 1, 4 and 16 cores,
for French-English. The training is repeated five
times and the error bars in the graph indicate the
decimal place.
265
l
l l
l l l l
l l l l l
l l l
l l l l l l l l l
l l
l
l
l
l
l
l
l
l l
l
l l l
27
28
29
30
31
Samples per core (thousands)
Bleu
0 500 1000 1500 2000
(a) 1 core
l
l
l
l l l l l
l
l
l
l
l
l l l
l
l
l
l
l
l
l l
l
27
28
29
30
31
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(b) 4 cores
l
l
ll l
l l l l l l l
l
l ll l
27
28
29
30
31
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(c) 16 cores
Figure 1: SampleRank learning curves for the WMT-SMALL French-English system, for 1, 4 and 16 cores.
The dashed line shows the mean MERT performance, which has a standard deviation of 0.2.
spread across the different training runs. Increasing
the number of cores makes a clear difference to the
training, with the single core training run failing to
reach the the level of MERT, and the 16 core train-
ing run exceeding the mean MERT performance by
more than 0.5 BLEU. Using a single core also results
in a much bigger training variance, which makes
sense as using more cores and averaging weights
reduces the adverse effect of a single chain going
astray. The higher BLEU score achieved when us-
ing the larger number of cores is probably because
a larger portion of the parameter space is being ex-
plored.
In one sense, the x axes of the graphs in Figure 1
are not comparable, since increasing the number of
cores and keeping the number of samples per core
increases the total computing time. However even if
the single core training was run for much longer, it
did not reach the level of performance obtained by
multi-core training. Limited experimentation with
increasing the core count to 32 did not show any ap-
preciable gain, despite greatly increasing the com-
puting resources required.
The training runs shown in Figure 1 take between
21 hours (for 16 cores) and 35 hours (for a single
core)7. In the 16 core runs each core is doing the
same amount of work as in the single core runs, so
the difference in time is due to the extra effort in-
volved in dealing with larger batches. These times
are for the 100 samples-per-sentence condition, and
7The processors are Intel Xeon 5450 (3GHz)
increasing to 500 samples-per-sentence provides a
speed-up of about 25%, since proportionally less
time is spent on burn-in. Most of the time is spent
in BLEU evaluation, so improved memoisation and
incremental evaluation would reduce training time.
In Table 2 the mean maximum BLEU achieved on
the heldout set at each parameter setting is shown.
By this it is meant that for each of the five training
runs at each (samples,cores) setting, the maximum
BLEU on heldout data is observed, and these max-
ima are averaged across the five runs. It can be seen
that changing the samples-per-sentence makes little
difference, but there is a definite effect of increasing
the core count.
Cores 100 Samples 500 Samples
1 29.1 (0.2) 29.2 (0.1)
2 29.3 (0.1) 29.3 (0.1)
4 29.6 (0.1) 29.5 (0.1)
8 30.0 (0.0) 29.9 (0.1)
16 30.0 (0.1) 29.8 (0.1)
Table 2: Mean maximum heldout performance for
SampleRank training of the French-English WMT-
SMALL model. Standard deviations are shown in
brackets.
The learning curves for the equivalent German-
English model are shown in Figure 2 and show a
fairly different behaviour to their French-English
counterparts. Again, using more cores helps to im-
266
prove and stabilise the performance, but there is lit-
tle if any improvement throughout training. As with
MERT training, SampleRank training of the model
weights makes little difference to the BLEU score,
suggesting a fairly flat error surface.
Table 3 shows the mean maximum BLEU score
on heldout data, the equivalent of Table 2, but for
German-English. The results show very little varia-
tion as the samples-per-sentence and core counts are
changed.
Cores 100 Samples 500 Samples
1 25.2 (0.0) 25.3 (0.1)
2 25.4 (0.1) 25.4 (0.1)
4 25.4 (0.1) 25.4 (0.1)
8 25.4 (0.1) 25.4 (0.1)
16 25.3 (0.1) 25.4 (0.1)
Table 3: Mean maximum heldout performance for
SampleRank training of the German-English WMT-
SMALL model. Standard deviations are shown in
brackets
3.3 SampleRank Training for Larger Models
For the training of the WMT-LARGE systems with
SampleRank, similar experiments to those in Sec-
tion 3.2 were run, although only for 8 and 16 cores.
The learning curves for the two language pairs (Fig-
ure 3) show roughly similar patterns to those in
the previous section, in that the French-English sys-
tem gradually increases performance through train-
ing to reach a maximum, as opposed to the German-
English system with its fairly flat learning curve.
Training times are around 27 hours for the 500 sam-
ple curve shown in Figure 3, increasing to 64 hours
for 100 samples-per-sentence.
In Table 4, the mean maximum BLEU scores are
shown for each configuration. of each language pair,
calculated in the manner described in the previous
section. For the larger system, SampleRank shows
a smaller advantage over MERT for French-English,
and little if any gain for German-English. For both
large and small German-English models, neither of
the parameter tuning algorithms are able to lift BLEU
scores very far above the scores obtained from the
untuned weights set by the Moses training script.
Pair Cores 100 Samples 500 Samples
fr-en 8 32.6 (0.1) 32.7 (0.1)
16 32.8 (0.1) 32.9 (0.1)
de-en 8 26.9 (0.0) 27.0 (0.1)
16 26.8 (0.1) 26.9 (0.1)
Table 4: Mean (and standard deviation) of maximum
heldout performance for SampleRank training of the
WMT-LARGE model.
3.4 SampleRank Training for Larger Feature
Sets
The final set of experiments are concerned with us-
ing SampleRank training for larger feature sets than
the 10-20 typically used in MERT-trained models.
The models considered in this section are based on
the WMT-SMALL systems, but also include a fam-
ily of part-of-speech tag based phrase boundary fea-
tures.
The phrase boundary features are defined by con-
sidering the target-side part-of-speech tag bigrams
spanning each phrase boundary in the hypothesis,
and allowing a separate feature to fire for each bi-
gram. Dummy phrases with parts-of-speech <S>
and </S> are inserted at the start and end of the
sentence, and also used to construct phrase bound-
ary features. The example in Figure 4 shows the
phrase-boundary features from a typical hypothe-
sis. The idea is similar to a part-of-speech language
model, but discriminatively trained, and targeted at
how phrases are joined together in the hypothesis.
The target-side part-of-speech tags are added us-
ing the Brill tagger, and incorporated into the phrase
table using the factored translation modelling capa-
bilities of Moses (Koehn and Hoang, 2007).
Adding the phrase boundary features to the WMT-
SMALL system increased the feature count from 8
to around 800. Training experiments were run for
both the French-English and German-English mod-
els, using the same configuration as in Section 3.2,
varying the number of cores (8 or 16) and the num-
ber of samples per sentence (100 or 500). Train-
ing times were similar to those for the WMT-SMALL
system. The mean maximum scores on heldout are
shown in Table 5. We suspect that these features are
fixing some short range reordering problems which
267
l l l
l
l l l
l
l l l l l l l l l l l l l l l l l l
l l l l l
l
l
23
24
25
26
27
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(a) 1 core
l
l l
l l l l l l l l l l l l l l l l l
l
l l l
23
24
25
26
27
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(b) 4 cores
l l l l
l
l l l l l l l l l l l l l l l ll l l
l
23
24
25
26
27
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(c) 16 cores
Figure 2: SampleRank learning curves for the WMT-SMALL German-English system, for 1, 4 and 16 cores.
The dashed line shows the mean MERT performance, which has a standard deviation of 0.1.
l
l l
l
l
l l
l l
l l l l l l l
l l l ll
l
30
31
32
33
34
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(a) French-English
l
l
l l
l
l l l l l l l l l l l l l
l l l l
l
l ll
l
l
l
l
l l
24
25
26
27
28
Samples per core (thousands)
Bleu
0 500 1000 1500 2000 2500
(b) German-English
Figure 3: SampleRank learning curves for the WMT-LARGE French-English and German-English systems,
using 8 cores and 500 samples per sentence. The dashed line shows the mean MERT performance, which
has a standard deviation of 0.07 (fr-en) and 0.2 (de-en).
occur in the former language pair, but since the re-
ordering problems in the latter language pair tend to
be longer range, adding these extra features just tend
to add extra noise to the model.
3.5 Comparison of MERT and SampleRank on
Test Data
Final testing was performed on the nc-test2008
and newstest2010 data sets. The former is quite
similar to the tuning and heldout data, whilst the lat-
ter can be considered to be ?out-of-domain?, so pro-
vides a check to see whether the model weights are
being tuned too heavily towards the domain.
For the SampleRank experiments on the test set,
the best training configurations were chosen from
the results in Tables 2, 3, 4 and 5, and the best per-
forming weight sets for each of the five runs for this
configuration. For the MERT trained models, the
same five models from Table 1 were used. The test
set results are shown in Table 6.
The patterns observed on the heldout data carry
over, to a large extent, to the test data. This is
especially true for the WMT-SMALL system, where
similar improvements (for French-English) over the
MERT trained system are observed on the SampleR-
ank trained system. For the WMT-LARGE system,
the slightly improved performance that SampleRank
offered on the in-domain data is no longer there, al-
268
Hypothesis [europe ?s] [after] [racial] [house divided against itself]
Tags <S> NNP POS IN JJ NN VBN IN PRP </S>
This produces five phrase boundary features: <S>:NNP, POS:IN, IN:JJ, JJ:NN and PRP:</S>.
Figure 4: The definition of the phrase boundary feature from part-of-speech tags
fr-en de-en
Training System nc-test2008 newstest2010 nc-test2008 newstest2010
MERT WMT-SMALL 28.1 (0.1) 19.6 (0.1) 25.9 (0.1) 16.4 (0.2)
SampleRank WMT-SMALL 28.7 (0.0) 20.1 (0.1) 25.9 (0.1) 16.6 (0.1)
SampleRank WMT-SMALL+pb 28.8 (0.1) 19.8 (0.1) 25.9 (0.1) 16.7 (0.1)
MERT WMT-LARGE 30.1 (0.1) 22.9 (0.1) 28.0 (0.2) 19.1 (0.2)
SampleRank WMT-LARGE 30.0 (0.1) 23.6 (0.3) 28.1 (0.1) 19.5 (0.2)
Table 6: Comparison of MERT trained and SampleRank trained models on the test sets. The WMT-
SMALL+pb model is the model with phrase boundary features, as described in Section 3.4
Pair Cores 100 Samples 500 Samples
fr-en 8 30.2 (0.0) 30.2 (0.0)
16 30.3 (0.0) 30.3 (0.00)
de-en 8 25.1 (0.1) 25.1 (0.0)
16 25.0 (0.1) 25.0 (0.0)
Table 5: Mean (and standard deviation) of maximum
heldout performance for SampleRank training of the
WMT-SMALL model, with the phrase boundary fea-
ture.
though interestingly there is a reasonable improve-
ment on out-of-domain, over the MERT trained
model, similar to the effect observed in (Arun et
al., 2010). Finally, the improvements offered by the
phrase boundary feature are reduced, perhaps an in-
dication of some over-fitting.
4 Related Work
Whilst MERT (Och, 2003) is still the dominant al-
gorithm used for discriminative training (tuning) of
SMT systems, research into improving on MERT?s
line search has tended to focus either on gradient-
based or margin-based techniques.
Gradient-based techniques require a differentiable
objective, and expected sentence BLEU is the most
popular choice, beginning with Smith and Eisner
(2006). They used n-best lists to calculate the fea-
ture expectations required for the gradient, optimis-
ing a second order Taylor approximation of expected
sentence BLEU. They also introduced the idea of de-
terministic annealing to the SMT community, where
an entropy term is added to the objective in train-
ing, and has its temperature progressively lowered
in order to sharpen the model probability distribu-
tion. The work of Smith and Eisner was extended
by Li and Eisner (2009) who were able to obtain
much better estimates of feature expectations by us-
ing a packed chart instead of an n-best list. They
also demonstrated that their method could extend to
large feature sets, although their experiments were
only run on small data sets.
An alternative method of calculating the feature
expectations for expected BLEU training is Monte-
Carlo Markov Chain (MCMC) approximation, and
this was explored in (Arun et al, 2009) and (Arun et
al., 2010). The sampling methods introduced in this
earlier work form the basis of the current work, al-
though in using the sampler for expected BLEU train-
ing, many samples must be collected before making
a parameter weight update, as opposed to the cur-
rent work where weights may be updated after ev-
ery sample. One novel feature of Arun et al (2010)
is that they were able to train to directly maximise
corpus BLEU, instead of its sentence-based approx-
imation, although this only made a small difference
to the results. The training methods in (Arun et al,
269
2010) are very resource intensive, with the experi-
ments running for 48 hours on around 40 cores, on
a pruned phrase table derived from Europarl, and a
3-gram language model.
Instead of using expected BLEU as a training ob-
jective, Blunsom et al (2008) trained their model to
directly maximise the log-likelihood of the discrim-
inative model, estimating feature expectations from
a packed chart. Their model treats derivations as
a latent variable, directly modelling the translation
probability.
Margin-based techniques have the advantage that
they do not have to employ expensive and com-
plex algorithms to calculate the feature expectations.
Typically, either perceptron ((Liang et al, 2006),
(Arun and Koehn, 2007)) or MIRA ((Watanabe et
al., 2007), (Chiang et al, 2008)) is employed, but
in both cases the idea is to repeatedly decode sen-
tences from the tuning set, and update the parame-
ter weights if the best hypothesis according to the
model differs from some ?oracle? sentence. The ap-
proaches differ in the way they compute the oracle
sentence, as well as the way the weights are updated.
Normally sentences are processed one-by-one, with
a weight update after considering each sentence, and
sentence BLEU is used as the objective. However
Chiang et al (2008) introduced an approximation to
corpus BLEU by using a rolling history. Both papers
on MIRA demonstrated its ability to extend to large
numbers of features.
In the only known application of SampleRank to
SMT, Roth et al (2010) deploys quite a different
translation model to the usual phrase-based model,
allowing overlapping phrases and implemented as a
factor graph. Decoding is with a rather slow stochas-
tic search and performance is quite poor, but this
model, in common with the training algorithm pre-
sented in the current work, permits features which
depend on the whole sentence.
5 Discussion and Conclusions
The results presented in Table 6 show that Sam-
pleRank is a viable method of parameter tuning for
phrase-based MT systems, beating MERT in many
cases, and equalling it in others. It is also able to
do what MERT cannot do, and scale to a large num-
ber of features, with the phrase boundary feature of
Section 3.4 providing a ?proof-of-concept?.
A further potential advantage of SampleRank is
that it allows training with features which depend
on the whole sentence, or even the whole document,
since a full set of hypotheses is retained through-
out training. Of course adding these features pre-
cludes decoding with the usual dynamic program-
ming based decoders, and would require an alterna-
tive method, such as MCMC (Arun et al, 2009).
As with the other alternatives to MERT men-
tioned in this paper, SampleRank training presents
the problem of determining convergence. With
MERT this is straightforward, since training (nor-
mally) comes to a halt when the estimated tuning
BLEU stops increasing and the weights stop chang-
ing. With methods such as minimum risk training,
MIRA and SampleRank, some kind of early stop-
ping criterion is usually employed, which lengthens
training unnecessarily, and adds costly decodes to
the training process. Building up sufficient practical
experience with each of these methods will offset
these problems somewhat.
Another important item for future work is to com-
pare SampleRank training with MIRA training, in
terms of performance, speed and ability to handle
large feature sets.
The code used for the experiments in this paper is
available under an open source license8.
Acknowledgements
This research was supported by the EuroMatrixPlus
project funded by the European Commission (7th Frame-
work Programme) and by the GALE program of the
Defense Advanced Research Projects Agency, Contract
No. HR0011-06-2-001. The project made use of the re-
sources provided by the Edinburgh Compute and Data
Facility (http://www.ecdf.ed.ac.uk/). The
ECDF is partially supported by the eDIKT initiative
(http://www.edikt.org.uk/).
The authors would like to thank Sebastian Riedel for
helpful discussions related to this work.
References
Abhishek Arun and Philipp Koehn. 2007. Online Learn-
ing Methods For Discriminative Training of Phrase
8https://mosesdecoder.svn.sourceforge.
net/svnroot/mosesdecoder/branches/
samplerank
270
Based Statistical Machine Translation. In Proceedings
of MT Summit.
Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blun-
som, Adam Lopez, and Philipp Koehn. 2009. Monte
Carlo inference and maximization for phrase-based
translation. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 102?110, Boulder, Colorado,
June. Association for Computational Linguistics.
Abhishek Arun, Barry Haddow, and Philipp Koehn.
2010. A Unified Approach to Minimum Risk Training
and Decoding. In Proceedings of the Joint Fifth Work-
shop on Statistical Machine Translation and Metrics-
MATR, pages 365?374, Uppsala, Sweden, July. Asso-
ciation for Computational Linguistics.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A Discriminative Latent Variable Model for Statistical
Machine Translation. In Proceedings of ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online Large-Margin Training of Syntactic and Struc-
tural Translation Features. In Proceedings of EMNLP.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online Passive-
Aggressive Algorithms. Journal of Machine Learning
Research, 7:551?585, March.
Aron Culotta. 2008. Learning and inference in weighted
logic with application to natural language processing.
Ph.D. thesis, University of Massachusetts, May.
George Foster and Roland Kuhn. 2009. Stabilizing
Minimum Error Rate Training. In Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 242?249, Athens, Greece, March. Association
for Computational Linguistics.
Philipp Koehn and Hieu Hoang. 2007. Factored Transla-
tion Models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868?876.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in Domain Adaptation for Statistical Machine Transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Zhifei Li and Jason Eisner. 2009. First- and Second-
order Expectation Semirings with Applications to
Minimum-Risk Training on Translation Forests. In
Proceedings of EMNLP.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An End-to-End Discriminative Ap-
proach to Machine Translation. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, pages 761?768, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Ryan McDonald, Keith Hall, and Gideon Mann. 2010.
Distributed Training Strategies for the Structured Per-
ceptron. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
456?464, Los Angeles, California, June. Association
for Computational Linguistics.
Franz J. Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of
ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Benjamin Roth, Andrew McCallum, Marc Dymetman,
and Nicola Cancedda. 2010. Machine Translation
Using Overlapping Alignments and SampleRank. In
Proceedings of AMTA.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proceed-
ings of COLING/ACL, pages 787?794, Morristown,
NJ, USA. Association for Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online Large-Margin Training for Sta-
tistical Machine Translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic, June. Association for
Computational Linguistics.
Michael Wick, Khashayar Rohanimanesh, Aron Culotta,
and Andrew McCallum. 2009. SampleRank: Learn-
ing Preferences from Atomic Gradients. In Proceed-
ings of NIPS Workshop on Advances in Ranking.
Michael Wick, Khashayar Rohanimanesh, Kedare Bel-
lare, Aron Culotta, and Andrew McCallum. 2011.
SampleRank: training factor graphs with atomic gra-
dients. In Proceedings of ICML.
271
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 317?321,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Towards Effective Use of Training Data in Statistical Machine Translation
Philipp Koehn and Barry Haddow
University of Edinburgh
Edinburgh, United Kingdom
{pkoehn,bhaddow}@inf.ed.ac.uk
Abstract
We report on findings of exploiting large data
sets for translation modeling, language mod-
eling and tuning for the development of com-
petitive machine translation systems for eight
language pairs.
1 Introduction
We report on experiments carried out for the devel-
opment of competitive systems on the datasets of the
2012 Workshop on Statistical Machine Translation.
Our main focus was directed on the effective use
of all the available training data during training of
translation and language models and tuning.
We use the open source machine translation sys-
tem Moses (Koehn et al, 2007) and other standard
open source tools, hence all our experiments are
straightforwardly replicable1.
Compared to all single system submissions by
participants of the workshop we achieved the best
BLEU scores for four language pairs (es-en, en-es,
cs-en, en-cs), the 2nd best results for two language
pairs (fr-en, de-en), as well as a 3rd place (en-de)
and a 5th place (en-fr) for the remaining pairs. We
improved upon this in the post-evaluation period for
some of the language pairs by more systematically
applying our methods.
During the development of our system, we saw
most gains from using large corpora for translation
model training, especially when using subsampling
techniques for out-of-domain sets, using large cor-
pora for language model training, and larger tuning
sets. We also observed mixed results with alternative
tuning methods. We also experimented with hierar-
chical models and semi-supervised training, but did
not achieve any improvements.
1Configuration files and instructions are available at http:
//www.statmt.org/wmt12/uedin/.
LP Baseline +UN
fr-en 28.2 28.4 (+.2)
es-en 29.1 28.9 (?.2)
en-fr 28.8 28.7 (?.1)
en-es 31.0 30.9 (?.1)
LP Baseline +GigaFrEn
fr-en 28.7 29.1 (+.4)
en-fr 29.3 30.3 (+1.0)
Table 1: Gains from larger translation models: UN (about
300 million English words), GigaFrEn (about 550 million
English words).
We report all results in case-sensitive BLEU (mt-
eval13a) on the newstest2011 test set (Callison-
Burch et al, 2011). Please also note that base-
line scores vary throughout the paper, since different
methods were investigated at different time points.
2 Better Translation Models
2.1 Using Large Training Sets
The WMT evaluation campaign works with the
largest training sets in the field. Our French-English
systems are trained on a parallel corpus with 1,072
million French and 934 million English words.
Training a system on this amount of data takes about
two weeks.
The basic data sets for the language pairs are the
Europarl and NewsCommentary corpora consist of
about 50 million words and 3 million words, respec-
tively. These corpora are quite close to the target
domain of news reports, and give quite good results.
Table 1 shows the gains from using the much larger
UN (about 300 million words) and GigaFrEn cor-
pora (about 550 million words).
From these results, it is not clear if the UN is help-
ful, but the GigaFrEn corpus gives large gains (+0.4
BLEU and +1.0 BLEU).
317
Model 1 Moore-Lewis
LP Base- Before After Before After
line 10% 50% 10% 50% 10% 50% 10% 50%
fr-en 29.3 28.5(?.8) 29.1(?.2) 28.6(?.7) 28.9(?.4) 29.1(?.2) 29.6(+.3) 29.1(?.2) 29.4(+.1)
en-fr 30.1 29.1(?1.0) 30.1(?.0) 29.3(?.8) 29.8(?.3) 29.9(?.2) 30.2(+.1) 29.9(?.2) 30.1(?.0)
es-en 29.0 28.9(?.1) 29.0(?.0) 29.0(?.0) 29.0(?.0) 29.0(?.0) 29.1(+.1) 29.4(+.4) 29.2(+.2)
en-es 30.9 30.9(?.0) 31.0(+.1) 30.8(?.1) 30.7(?.2) 31.4(+.5) 31.5(+.6) 31.5(+.6) 31.3(+.4)
Table 2: Subsampling UN and GigaFrEn corpora using Model 1 and Moore-Lewis filtering, before and after word
alignment
2.2 Subsampling
We experimented with two different types of sub-
sampling techniques ? Model 1, similar to that used
by Schwenk et al (2011), and modified Moore-
Lewis (Axelrod et al, 2011) ? for the language pairs
es-en, en-es, fr-en and en-fr. In each case the idea
was to include the NewsCommentary and Europarl
corpora in their entirety, and to score the sentences
in the remaining corpora (the selection corpus) using
one of the two measures, adding either the top 10%
or top 50% of the selection corpus to the training
data.
For Model 1 filtering, we trained IBM Model 1
on Europarl and NewsCommentary concatenated, in
both directions, and scored the sentences in the se-
lection corpus using the length-normalised sum of
the IBM Model scores. For the modified Moore-
Lewis filtering, we trained two 5-gram language
models for source and target, the first on 5M sen-
tences from the news2011 monolingual data, and
the second on 5M words from the selection corpus,
using the same vocabulary. The modified Moore-
Lewis score for a sentence is the sum of the source
and target?s perplexity difference for the two lan-
guage models.
For the Spanish experiments, the selection corpus
was the UN data, whilst for the French experiments
it was the UN data and the GigaFrEn data, concate-
nated and with duplicates removed.
The results of the subsampling are shown in Ta-
ble 2, where the BLEU scores are averaged over
2 tuning runs. The conclusion was that modified
Moore-Lewis subsampling was effective (and was
used in our final submissions), but Model 1 sam-
pling made no difference for the Spanish systems,
and was harmful for the French systems.
3 Better Language Models
In previous years, we were not able to make use
of the monolingual LDC Gigaword corpora due to
lack of sufficiently powerful computing resources.
These corpora exist for English (4.3 billion words),
Spanish (1.1 billion words), and French (0.8 billion
words). With the acquisition of large memory ma-
chines2, we were now able to train language models
on this data. Use of these large language models dur-
ing decoding is aided by more efficient storage and
inference (Heafield, 2011).
Still, even with that much RAM it is not possi-
ble to train a language model with SRILM (Stolke,
2002) in one pass. Hence, we broke up the train-
ing corpus by source (New York Times, Washington
Post, ...) and trained separate language model for
each. The largest individual corpus was the English
New York Times portion which consists of 1.5 billion
words and took close to 100GB of RAM. We also
trained individual language models for each year of
WMT12?s monolingual corpus.
We interpolated the language models using the
SRILM toolkit. The toolkit has a limit of 10 lan-
guage models to be merged at once, so we had to in-
terpolate sub-groups of some of the language models
(the WMT12 monolingual news models) first. It is
not clear if this is harmful, but building separate lan-
guage model for each source and year and interpo-
late those many more models did hurt significantly.
Table 3 shows that we gain around half a BLEU
point into Spanish and French, as well as German?
English, and around one and a half BLEU points for
the other language pairs into English.
2Dell Poweredge R710, equipped with two 6-core Intel
Xeon X5660 CPUs running at 2.8GHz, with each core able to
run two threads (24 threads total), six 3TB disks and 144GB
RAM, and cost ?6000.
318
LP Baseline +LDC Giga
de-en 21.9 22.4 (+.5)
cs-en 24.2 25.6 (+1.4)
fr-en 29.1 31.0 (+1.9)
es-en 29.1 30.7 (+1.6)
en-es 31.5 31.8 (+.3)
en-fr 30.3 30.8 (+.5)
Table 3: Using the LDC Gigaword corpora to train larger
language models.
LP Baseline Big-Tune
de-en 21.4 21.6 (+.2)
fr-en 28.4 28.7 (+.3)
es-en 28.9 29.0 (+.1)
cs-en 23.9 24.1 (+.2)
en-de 15.8 15.9 (+.1)
en-fr 28.7 29.2 (+.5)
en-es 30.9 31.2 (+.2)
en-cs 17.2 17.4 (+.2)
Table 4: Using a larger tuning set (7567 sentences) by
combining newstest 2008 to 2010.
4 Better Tuning
4.1 Bigger Tuning Sets
In recent experiments, mainly geared towards using
much larger feature sets, we learned that larger tun-
ing sets may give better and more stable results. We
tested this hypothesis here as well.
By concatenating the sets from three years (2008-
2010), we constructed a tuning set of 7567 sentences
per language. Table 4 shows that we gain on average
about +0.2 BLEU points.
4.2 Pairwise Ranked Optimization
We recently added an implementation of the pair-
wise ranked optimization (PRO) tuning method
(Hopkins and May, 2011) to Moses as an alterna-
tive to Och?s (2003) minimum error rate training
(MERT). We checked if this method gives us better
results. Table 5 shows a mixed picture. PRO gives
slightly shorter translations, probably because it op-
timises sentence rather than corpus BLEU, which has
a noticeable effect on the BLEU score. For 2 lan-
guage pairs we see better results, for 4 worse, and
for 1 there is no difference. On other data and lan-
LP MERT PRO PRO-MERT
de-en 21.7 (1.01) 21.9 (1.00) +.2 21.7 (1.01) ?.0
es-en 29.1 (1.02) 29.1 (1.01) ?.0 29.1 (1.02) ?.0
cs-en 24.2 (1.03) 24.5 (1.00) +.3 24.2 (1.03) ?.0
en-de 16.0 (1.00) 15.7 (0.96) ?.3 16.0 (1.00) ?.0
en-fr 29.3 (0.98) 28.9 (0.96) ?.4 29.3 (0.98) ?.0
en-es 31.5 (0.98) 31.3 (0.97) ?.2 31.4 (0.98) ?.1
en-cs 17.4 (0.97) 16.9 (0.92) ?.5 17.3 (0.97) ?.1
Table 5: Replacing the line search method of MERT with
pairwise ranked optimization (PRO).
guage conditions we have observed better and more
stable results with PRO.
We tried to use PRO to generate starting points for
MERT optimization. Theoretically this will lead to
better optimization on the tuning set, since MERT
optimization steps on PRO weights will never lead
to worse results on the sampled n-best lists. This
method (PRO-MERT in the table) applied here,
however, did not lead to significantly different re-
sults than plain MERT.
5 What did not Work
Not everything we tried worked out. Notably, two
promising directions ? hierarchical models and
semi-supervised learning ? did not yield any im-
provements. It is not clear if we failed or if the
methods failed, but we will investigate this further
in future work.
5.1 Hierarchical Models
Hierarchical models (Chiang, 2007) have been sup-
ported already for a few years by Moses, and they
give significantly better performance for Chinese?
English over phrase-based models. While we have
not yet seen benefits for many other language pairs,
the eight language pairs of WMT12 allowed us to
compare these two models more extensively, also
in view of recent enhancements resulting in better
search accuracy.
Since hierarchical models are much larger
(roughly 10 times bigger), we trained hierarchical
models on downsized training data for most lan-
guage pairs. For Spanish and French, this ex-
cludes UN and GigaFrEn; for Czech some parts
of the CzEng corpus were excluded based on their
lower language model interpolation weights relative
319
LP Phrase Downsized Hierarchical
de-en 21.6 same 21.4 (?.2)
fr-en 28.7 27.9 27.6 (?.3)
es-en 29.0 28.9 28.4 (?.5)
cs-en 24.1 22.4 22.0 (?.4)
en-de 15.9 same 15.5 (?.4)
en-fr 29.2 28.8 28.0 (?.8)
en-es 31.2 30.8 30.4 (?.4)
en-cs 17.4 16.2 15.6 (?.6)
Table 6: Hierarchical phrase models vs. baseline phrase-
based models.
to their size.
Table 6 shows inferior performance for all lan-
guage pairs (by about half a BLEU point), although
results for German?English are close (?0.2 BLEU).
5.2 Semi-Supervised Learning
Other research groups have reported improvements
using semi-supervised learning methods to cre-
ate synthetic parallel data from monolingual data
(Schwenk et al, 2008; Abdul-Rauf and Schwenk,
2009; Bertoldi and Federico, 2009; Lambert et al,
2011). The idea is to translate in-domain monolin-
gual data with a baseline system and filter the result
for use as an additional parallel corpus.
Table 7 shows out results when trying to emulate
the approach of Lambert et al (2011). We translate
the some of the 2011 monolingual news data (139
million words for French and 100 million words for
English) from the target language into the source
language with a baseline system trained on Europarl
and News Commentary. Adding all the obtained
data hurts (except for minimal improvements over
a small French-English system). When we filtered
out half of the sentences based on translation scores,
results were even worse.
Acknowledgments
This work was supported by the EuroMatrixPlus
project funded by the European Commission (7th
Framework Programme).
References
Abdul-Rauf, S. and Schwenk, H. (2009). On the
use of comparable corpora to improve SMT per-
Setup Baseline +synthetic +syn-half
fr-en ep+nc 28.0 28.1 (+.1) 28.0 (?.0)
+un 28.7 28.6 (?.1) 28.5 (?.2)
en-fr ep+nc 28.8 28.2 (?.6) 28.1 (?.7)
+un 29.3 28.9 (?.4) 28.9 (?.4)
Table 7: Using semi-supervised methods to add synthetic
parallel data to a baseline system trained on Europarl
(ep)m News Commentary (nc) and United Nations (un).
We added all generated data (synthetic) or filtered out half
based on model scores (syn-half).
formance. In Proceedings of the 12th Confer-
ence of the European Chapter of the ACL (EACL
2009), pages 16?23, Athens, Greece. Association
for Computational Linguistics.
Axelrod, A., He, X., and Gao, J. (2011). Domain
adaptation via pseudo in-domain data selection.
In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing,
pages 355?362, Edinburgh, Scotland, UK. Asso-
ciation for Computational Linguistics.
Bertoldi, N. and Federico, M. (2009). Domain
adaptation for statistical machine translation with
monolingual resources. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 182?189, Athens, Greece. Association
for Computational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22?64, Edinburgh, Scot-
land. Association for Computational Linguistics.
Chiang, D. (2007). Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2).
Heafield, K. (2011). Kenlm: Faster and smaller
language model queries. In Proceedings of the
Sixth Workshop on Statistical Machine Transla-
tion, pages 187?197, Edinburgh, Scotland. Asso-
ciation for Computational Linguistics.
Hopkins, M. and May, J. (2011). Tuning as ranking.
In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1352?1362, Edinburgh, Scotland, UK. As-
sociation for Computational Linguistics.
320
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B., Shen,
W., Moran, C., Zens, R., Dyer, C. J., Bojar, O.,
Constantin, A., and Herbst, E. (2007). Moses:
Open source toolkit for statistical machine trans-
lation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguis-
tics Companion Volume Proceedings of the Demo
and Poster Sessions, pages 177?180, Prague,
Czech Republic. Association for Computational
Linguistics.
Lambert, P., Schwenk, H., Servan, C., and Abdul-
Rauf, S. (2011). Investigations on translation
model adaptation using monolingual data. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 284?293, Edinburgh,
Scotland. Association for Computational Linguis-
tics.
Och, F. J. (2003). Minimum error rate training for
statistical machine translation. In Proceedings
of the 41st Annual Meeting of the Association of
Computational Linguistics (ACL).
Schwenk, H., Este`ve, Y., and Rauf, S. A. (2008).
The LIUM Arabic/English statistical machine
translation system for IWSLT 2008. In Proceed-
ings of the International Workshop on Spoken
Language Translation (IWSLT), pages 63?68.
Schwenk, H., Lambert, P., Barrault, L., Servan, C.,
Abdul-Rauf, S., Afli, H., and Shah, K. (2011).
Lium?s smt machine translation systems for wmt
2011. In Proceedings of the Sixth Workshop on
Statistical Machine Translation, pages 464?469,
Edinburgh, Scotland. Association for Computa-
tional Linguistics.
Stolke, A. (2002). SRILM - an extensible language
modeling toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Process-
ing.
321
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 422?432,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Analysing the Effect of Out-of-Domain Data on SMT Systems
Barry Haddow and Philipp Koehn
School of Informatics
University of Edinburgh
Edinburgh, EH8 9AB, Scotland
{bhaddow,pkoehn}@inf.ed.ac.uk
Abstract
In statistical machine translation (SMT), it is
known that performance declines when the
training data is in a different domain from the
test data. Nevertheless, it is frequently nec-
essary to supplement scarce in-domain train-
ing data with out-of-domain data. In this pa-
per, we first try to relate the effect of the out-
of-domain data on translation performance to
measures of corpus similarity, then we sep-
arately analyse the effect of adding the out-
of-domain data at different parts of the train-
ing pipeline (alignment, phrase extraction, and
phrase scoring). Through experiments in 2 do-
mains and 8 language pairs it is shown that
the out-of-domain data improves coverage and
translation of rare words, but may degrade the
translation quality for more common words.
1 Introduction
In statistical machine translation (SMT), domain
adaptation can be thought of as the problem of train-
ing a system on data mainly drawn from one do-
main (e.g. parliamentary proceedings) and trying
to maximise its performance on a different domain
(e.g. news). There is likely to be some parallel data
similar to the test data, but as such data is expen-
sive to create, it tends to be scarce. The concept of
?domain? is rarely given a precise definition, but it is
normally understood that data from the same domain
is in some sense similar (for example in the words
and grammatical constructions used) and data from
different domains shows less similarities. Data from
the same domain as the test set is usually referred
to as in-domain and data from a different domain is
referred to as out-of-domain.
The aim of this paper is to shed some light on
what domain actually is, and why it matters. The
fact that a mismatch between training and test data
domains reduces translation performance has been
observed in previous studies, and will be confirmed
here for multiple data sets and languages, but the
question of why domain matters for performance has
not been fully addressed in the literature. Exper-
iments in this paper will be conducted on phrase-
based machine translation (PBMT) systems, but
similar conclusions are likely to apply to other types
of SMT systems. Furthermore, we will mainly be
concerned with the effect of domain on the transla-
tion model, since it depends on parallel data which
is more likely to be in short supply than monolingual
data, and domain adaptation for language modelling
has been more thoroughly studied.
The effect of a shift of domain in the parallel data
is complicated by the fact that training a translation
model is a multi-stage process. First the parallel
data is word-aligned, normally using the IBM mod-
els (Brown et al, 1994), then phrases are extracted
using some heuristics (Och et al, 1999) and scored
using a maximum likelihood estimate. Since the ef-
fect of domain may be felt at the alignment stage, the
extraction stage, or the scoring stage, we have de-
signed experiments to try to tease these apart. Exper-
iments comparing the effect of domain at the align-
ment stage with the extraction and scoring stages
have already been presented by (Duh et al, 2010), so
we focus more on the differences between extraction
and scoring. In other words, we examine whether
adding more data (in or out-of domain) helps im-
prove coverage of the phrase table, or helps improve
the scoring of phrases.
A further question that we wish to address is
422
whether adding out-of-domain parallel data affects
words with different frequencies to different de-
grees. For example, a large out-of-domain data set
may improve the translation of rare words by pro-
viding better coverage, but degrade translation of
more common words by providing erroneous out-of-
domain translations. In fact, the evidence presented
in Section 3.5 will show a much clearer effect on low
frequency words than on medium or high frequency
words, but the total token count of these low fre-
quency words is still small, so they don?t necessarily
have much effect on overall measures of translation
quality.
In summary, the main contributions of this paper
are:
? It presents experiments on 8 language pairs
and 2 domains showing the effect on BLEU of
adding out-of-domain data.
? It provides evidence that the difference be-
tween in and out-of domain translation perfor-
mance is correlated with differences in word
distribution and out-of-vocabulary rates.
? It develops a method for separating the effects
of phrase extraction and scoring, showing that
good coverage is nearly always more important
than good scoring, and that out-of-domain data
can adversely affect phrase scores.
? It shows that adding out-of-domain data clearly
improves translation of rare words, but may
have a small negative effect on more common
words.
2 Related Work
The most closely related work to the current one is
that of (Duh et al, 2010). In this paper they consider
the domain adaptation problem for PBMT, and in-
vestigate whether the out-of-domain data helps more
at the word alignment stage, or at the phrase extrac-
tion and scoring stages. Extensive experiments on
4 different data sets, and 10 different language pairs
show mixed results, with the overall conclusion be-
ing that it is difficult to predict how best to include
out-of-domain data in the PBMT training pipeline.
Unlike in the current work, Duh et al do not sepa-
rate phrase extraction and scoring in order to anal-
yse the effect of domain on them separately. They
make the point that adding extra out-of-domain data
may degrade translation by introducing unwanted
lexical ambiguity, showing anecdotal evidence for
this. Similar arguments were presented in (Sennrich,
2012).
A recent paper which does attempt to tease apart
phrase extraction and scoring is (Bisazza et al,
2011). In this work, the authors try to improve a
system trained on in-domain data by including extra
entries (termed ?fill-up?) from out-of-domain data ?
this is similar to the nc+epE and st+epE systems
in Section 3.4. It is shown by Bisazza et al that this
fill-up technique has a similar effect to using MERT
to weight the in and out-of domain phrase tables. In
the experiments in Section 3.4 we confirm that fill-
up techniques mostly provide better results than us-
ing a concatenation of in and out-of domain data.
There has been quite a lot of work on finding ways
of weighting in and out-of domain data for SMT
(as opposed to simply concatenating the data sets),
both for language and translation modelling. Inter-
polating language models using perplexity is fairly
well-established (e.g. Koehn and Schroeder (2007)),
but for phrase-tables it is unclear whether perplexity
minimisation (Foster et al, 2010; Sennrich, 2012) or
linear or log-linear interpolation (Foster and Kuhn,
2007; Civera and Juan, 2007; Koehn and Schroeder,
2007) is the best approach. Also, other authors (Fos-
ter et al, 2010; Niehues and Waibel, 2010; Shah et
al., 2010) have tried to weight the input sentences or
extracted phrases before the phrase tables are built.
In this type of approach, the main problem is how
to train the weights of the sentences or phrases, and
each of the papers has followed a different approach.
Instead of weighting the out-of-domain data,
some authors have investigated data selection meth-
ods for domain adaptation (Yasuda et al, 2008;
Mansour et al, 2011; Schwenk et al, 2011; Axelrod
et al, 2011). This is effectively the same as using
a 1-0 weighting for input sentences, but has the ad-
vantage that it is usually easier to tune a threshold
than it is to train weights for all input sentences or
phrases. The other advantage of doing data selection
is that it can potentially remove noisy (e.g. incor-
rectly aligned) data. However it will be seen later in
this paper that out-of-domain data can usually con-
tribute something useful to the translation system,
so the 1-0 weighting of data-selection may be some-
what heavy-handed.
423
3 Experiments
3.1 Corpora and Baselines
The experiments in this paper used data from
the WMT09 and WMT11 shared tasks (Callison-
Burch et al, 2009; Callison-Burch et al, 2011), as
well as OpenSubtitles data1 released by the OPUS
project (Tiedemann, 2009).
From the WMT data, both news-commentary-
v6 (nc) and europarl-v6 (ep) were used for
training translation models and language mod-
els, with nc-devtest2007 used for tuning and
nc-test2007 for testing. The experiments were
run on all language pairs used in the WMT shared
tasks, i.e. English (en) into and out of Spanish (es),
German (de), French (fr) and Czech (cs).
From the OpenSubtitles (st) data, we chose
8 language pairs ? English to and from Span-
ish, French, Czech and Dutch (nl) ? selected be-
cause they have at least 200k sentences of parallel
data available. 2000 sentence tuning and test sets
(st-dev and st-devtest) were selected from
the parallel data by extracting every nth sentence,
and a 200k sentence training corpus was selected
from the remaining data.
Using test sets from both news-commentary and
OpenSubtitles gives two domain adaptation tasks,
where in both cases the out-of-domain data is eu-
roparl, a significantly larger training set than the in-
domain data. The three data sets in use in this paper
are summarised in Table 1.
The translation systems consisted of phrase ta-
bles and lexicalised reordering tables estimated us-
ing the standard Moses (Koehn et al, 2007) train-
ing pipeline, and 5-gram Kneser-Ney smoothed lan-
guage models estimated using the SRILM toolkit
(Stolcke, 2002), with KenLM (Heafield, 2011) used
at runtime. Separate language models were built
on the target side of the in-domain and out-of-
domain training data, then linearly interpolated us-
ing SRILM to minimise perplexity on the tuning
set (e.g. Koehn and Schroeder (2007)). Tuning
of models used minimum error rate training (Och,
2003), repeated 3 times and averaged (Clark et
al., 2011). Performance is evaluated using case-
insensitive BLEU (Papineni et al, 2002), as imple-
1www.opensubtitles.org
mented using the Moses multi-bleu.pl script.
Name Language
pairs
train tune test
Europarl en?fr 1.8M n/a n/a
(ep) en?es 1.8M n/a n/a
en?de 1.7M n/a n/a
en?cs 460k n/a n/a
en?nl 1.8M n/a n/a
News en?fr 114k 1000 2000
Commentary en?es 130k 1000 2000
(nc) en?de 135k 1000 2000
en?cs 122k 1000 2000
Subtitles en?fr 200k 2000 2000
(st) en?es 200k 2000 2000
en?nl 200k 2000 2000
en?cs 200k 2000 2000
Table 1: Summary of the data sets used, with ap-
proximate sentence counts
3.2 Comparing In-domain and Out-of-domain
Data
The aim of this section is to provide both a quali-
tative and quantitative comparison of the three data
sets used in this paper.
Firstly, consider the extracts from the English sec-
tions of the three training sets shown in Figure 1.
The first extract, from the Europarl corpus, shows a
formal style with long sentences. However this is
still spoken text so contains a preponderance of first
and second person forms. In terms of subject mat-
ter, the corpus covers a broad range of topics, but all
from the angle of European legislation, institutions
and policies. Where languages (e.g. English, French
and Spanish) have new world and old world variants,
Europarl sticks to the old world variants.
The extract from the News Commentary corpus
again shows a formal tone, but because this is news
analysis, it tends to favour the third person. It is writ-
ten text, and covers a wider range of subjects than
Europarl, and also encompasses both new and old
world versions of the European languages.
The Subtitles text shown in the last example ap-
pears qualitatively more different from the other
two. It is spoken text, like Europarl, but consists of
short, informal sentences with many colloquialisms,
as well as possible optical character recognition er-
424
Although, as you will have seen, the dreaded ?millennium bug? failed to materialise, still the people in a
number of countries suffered a series of natural disasters that truly were dreadful.
You have requested a debate on this subject in the course of the next few days, during this part-session.
In the meantime, I should like to observe a minute? s silence, as a number of Members have requested, on
behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the
European Union.
(a) Europarl
Desperate to hold onto power, Pervez Musharraf has discarded Pakistan?s constitutional framework and
declared a state of emergency.
His goal?
To stifle the independent judiciary and free media.
Artfully, though shamelessly, he has tried to sell this action as an effort to bring about stability and help
fight the war on terror more effectively.
(b) News commentary
I?il call in 30 minutes to check
Is your mother here, too?
Why are you outside?
It?s no fun listening to women?s talk
Well, why don?t we go in together
(c) OpenSubtitles
Figure 1: Extracts from the English portion of the training corpora
rors. It is likely to contain a mixture of regional vari-
ations of the languages, reflecting the diversity of the
film sources.
In order to obtain a quantitative measure of do-
main differences, we used both language model
(LM) perplexity, and out-of-vocabulary (OOV) rate,
in the two test domains. For the nc domain, per-
plexity was compared by training trigram LMs (with
SRILM and Kneser-Ney smoothing) on each of the
ep, nc and ep+nc training sets, taking the inter-
section of the ep and nc vocabularies as the LM
vocabulary. The perplexities of the nc test set wer
calculated using each of the LMs. A correspond-
ing set of LMs was trained to compare perplexities
on the st test set, and all perplexity comparisons
were performed on all five languages. The SRILM
toolkit was also used to calculate OOV rates on the
test set, by training language models with an open
vocabulary, and using no unknown word probability
estimation.
The perplexities and OOV rates on each test cor-
pora are shown in Figure 2. The pattern of perplexi-
ties is quite distinct across the two test domains, with
the perplexity from out-of-domain data relatively
much higher for the st test set. The in-domain data
LM also shows the lowest perplexity consistently on
this test set, whilst for nc, the in-domain LM has
a similar perplexity to the ep+nc LM. In fact for
3/5 languages (fr,cs and de) the ep+nc LM has the
lowest perplexity.
With regard to the OOV rates, it is notable that
for nc the rate is actually higher for the in-domain
LM than the out-of-domain LM in three of the lan-
guages: French, German and Spanish. The most
likely reason for this is that these languages have
a relatively rich morphology, so the larger out-
of-domain corpus (Table 1) gives greater cover-
age of the different grammatical suffixes. Czech
shows a different pattern because in this case the
out-of-domain corpus is not much bigger than the
in-domain corpus, and English is morphologically
much simpler so the increase in corpus size does not
help the OOV rate so much.
425
cs de en es fr
3?gram Perplexity
0
100
200
300
400
500
600
700
cs de en es fr
epncep+nc
OOV
0
500
100
0
150
0
(a) Test on news commentary.
cs en es fr nl
3?gram Perplexity
0
100
200
300
400
cs en es fr nl
epstep+st
OOV
0
500
100
0
150
0
(b) Test on subtitles.
Figure 2: Comparison of perplexities and OOV rates on in-domain test data
3.3 Comparing Translation Performance of In
and Out-of-domain Systems
Translation performance was measured on each of
the test sets (nc and st) using systems built from
just the in-domain parallel data, from just the out-of-
domain parallel data, and on a concatenation of the
in and out-of domain data. In other words, systems
built from the ep, nc and ep+nc parallel texts were
evaluated on the nc test data, and systems built from
ep, st and ep+st were evaluated on the st test
data. In all cases, the parallel training set was used
to build both the phrase table and the lexicalised re-
ordering models, the language model was the inter-
polated one described in Section 3.1, and the system
was tuned on data from the same domain as the test
set.
From Figure 3 it is clear that the difference be-
tween the in and out-of domain training sets is much
bigger for st than for nc. The BLEU scores on nc
for the nc trained systems are on average 1.3 BLEU
points higher than those for the ep trained systems,
whilst the scores on st gain an average of 6.0 BLEU
points when the training data is switched from ep
to st. The patterns are quite consistent across lan-
guages for the st tested systems, with the gains
varying just from 5.2 to 7.2. However for the nc
tested systems there are some language pairs which
show a gain of more than 2 BLEU points when mov-
ing from out-of to in-domain training data (cs-en,
en-es and es-en), whereas en-fr shows no change.
The main link between the perplexity and OOV re-
sults in Figure 2 and the BLEU score variations in
Figure 3 is that the larger in/out differences between
the two domains is reflected in larger BLEU differ-
ences. However it is also notable that the two lan-
guages which display a rise in perplexity between
nc and ep+nc are es and en, and for both es-en and
en-es the ep+nc translation system performs worse
than the nc trained system.
The BLEU gain from concatenating the in and out-
of domain data, over just using the in-domain data
can be quite small. For the nc domain this averages
at 0.5 BLEU (with 3/8 language pairs showing a de-
crease), whilst for the st domain the average gain is
only 0.2 BLEU (with again 3/8 language pairs show-
ing a decrease). So even though adding the out-of-
domain data increases the training set size by a fac-
tor of 10 in most cases, its effect on BLEU score is
small.
426
cs?en en?cs de?en en?de fr?en en?fr es?en en?es
ep
ncep+nc
Bleu
0
5
10
15
20
25
30
35
(a) Test on news commentary
cs?en en?cs nl?en en?nl fr?en en?fr es?en en?es
epstep+st
Bleu
0
5
10
15
20
25
(b) Test on subtitles
Figure 3: Comparison of translation performance using models from in-domain, out-of-domain and joint
data.
3.4 Why Does Adding Parallel Data Help?
In the previous section it was found that, across all
language pairs and both data sets, adding in-domain
data to an out-of-domain training set nearly always
has a positive impact on performance, whilst adding
out-of-domain data to an in-domain training set can
sometimes have a small positive effect. In this sec-
tion several experiments are performed with ?inter-
mediate? phrase tables (built from a single parallel
corpus, augmented with some elements of the other
parallel corpus) in order to determine how different
aspects of the extra data affect performance. In par-
ticular, the experiments are designed to show the ef-
fect of the extra data on the alignments, the phrase
scoring and the phrase coverage, whether adding in-
domain data to an existing out-of-domain trained
system, or vice-versa.
For each of the language pairs used in this paper,
and each of the two domains, two series of experi-
ments were run comparing systems built from a sin-
gle parallel training set, intermediate systems, and
systems built from a concatenation of the in and out-
of-domain parallel data sets. Only the parallel data
was varied, the language models were as described
in Section 3.1, and the lexicalised reordering mod-
els were built from both training sets in all cases,
except for the systems built from a single parallel
data set2. This gives a total of four series of experi-
ments, where the ordered pair of data sets (x,y) was
set to one of (ep,nc), (nc,ep), (ep,st), (st,ep).
In each of these series, the following translation sys-
tems were trained:
x The translation table and lexicalised reordering
model were estimated from the x corpus alone.
x+y The translation system built from the x and y
parallel corpora concatenated.
x+yA As x but using the additional y corpus to cre-
ate the alignments. This means that GIZA++
was run across the entire x+y corpus, but only
the x section of it was used to extract and score
phrases.
x+yW As x+yA but using the phrase scores from
the x+y phrase table. This is effectively the
x+y system, with any entries in the phrase table
that are just found in the y corpus removed.
2Further experiments were run using the parallel data from a
single data set to build the translation model, and both data sets
to build the lexicalised reordering model, but the difference in
score compared to the x system was small (< 0.1 BLEU)
427
x+yE As x+yA but adding the extra entries from
the x+y phrase table. This is effectively the
x+y system, but with the scores on all phrases
that are found in x phrase table set to their val-
ues from that table.
All systems were tuned and tested on the appro-
priate in-domain data set (either nc or st). Note
that in the intermediate systems, the phrase table
scores may no longer correspond to valid probability
distributions, but this is not important as the proba-
bilistic interpretation is never used in decoding any-
way.
The graphs in Figure 4 show the performance
comparison between the single corpus systems, the
intermediate systems, and the concatenated corpus
systems, averaged across all 8 language pairs. Table
2 shows the full results broken down by language
pair, for completeness, but the patterns are reason-
ably consistent across language pair.
Firstly, compare the x+yW and x+yE systems, i.e.
the systems where we add just the weights from the
second parallel data set versus those where we add
just the entries. When x is the out-of-domain (ep)
data, then it is clearly more profitable to update the
phrase-table entries than the weights from the in-
domain data. In fact for the systems tested on st,
the difference is quite striking with a +5.7 BLEU
gain for the ep+stE system over the baseline ep
system, but only a +1.5 gain for the ep+stW sys-
tem. For the systems tested on the nc, adding the
entries from nc gives a larger gain in BLEU than
adding the weights (+1.3 versus +0.8), but both
improve the BLEU scores over the ep+ncA system.
The conclusion is that the extra entries from the in-
domain data (the ?fill-up? of Bisazza et al (2011))
are more important than the improvements in phrase
scoring that in-domain data may provide.
Looking at the other two sets of x+yW and x+yE
systems, i.e. those where x is the in-domain data,
tells another story. In this case, the results on both
the nc and st test sets (Figure 4(b)) suggest that it is
generally more useful to use the out-of-domain data
as only a source of extra phrase-table entries. This is
because the x+epE systems are the highest scoring
in both cases, scoring higher than systems built from
all the data concatenated by margins of 0.5 (for nc)
and 0.4 (for st). This pattern is consistent across
all the language pairs for nc, and across 5 of the 8
language pairs for st. Using the out-of-domain data
set to update only the weights (the x+epW systems)
generally degrades performance when compared to
the systems that only use the ep data at alignment
time (the x+epA systems).
The size of the effect of adding extra data to the
alignment stage only is mixed (as observed by (Duh
et al, 2010)), but in general all the x+yA systems
show an improvement over the x systems. In fact,
for the st domain, adding ep at the alignment stage
is the only consistent way to improve BLEU. Adding
the weights, entries, or the complete out-of-domain
data set does not always help.
3.5 Word Precision Versus Frequency
The final set of experiments addresses the question
of whether the change of translation quality when
adding out-of-domain has a different effect depend-
ing on word frequency. To do this, the systems
trained on in-domain only are compared with the
systems trained on all data concatenated, using a
technique for measuring the precision of the trans-
lation for each word type.
To calculate the precision of a word type, it is
necessary to examine each translated sentence to
see which source words were translated correctly.
This is done by recording the word alignment in the
phrase mappings and tracking it through the transla-
tion process. If a word is produced multiple times in
the translation, but occurs a fewer number of times
in the reference, then it is assigned partial credit.
Many-to-many word alignments are treated simi-
larly. Precision for each word type is then calculated
in the usual way, as the number of times that word
appears correctly in the output, divided by the to-
tal number of appearances. The word types are then
binned according to the log2 of their frequency in
the in-domain corpus and the average precision for
each bin calculated, then these are in turn averaged
across language pairs.
The graphs in Figure 5 compare the in-domain
source frequency versus precision relationship for
systems built using just the in-domain data, and sys-
tems built using both in and out-of domain data.
There is a consistent increase in precision for lower
frequency words (occurring less than 30 times in
training), but the total number of occurrences of
these words is low, so they contribute less to over-
428
ep
ep+
ncA
ep+
ncW
ep+
ncE ep+
nc
Bleu
0
5
10
15
20
25
+0.4 +0.8 +1.3 +1.7
(a) Start with ep, test on nc.
nc
nc+
epA
nc+
epW
nc+
epE ep+
nc
Bleu
0
5
10
15
20
25
30
+0.5 +0.3 +1.0 +0.5
(b) Start with nc, test on nc.
ep
ep+
stA
ep+
stW
ep+
stE ep+
st
Bleu
0
5
10
15
20
+0.6 +1.5
+5.7 +6.2
(c) Start with ep, test on st.
st
st+e
pA
st+e
pW
st+e
pE
ep+
st
Bleu
0
5
10
15
20
+0.3
?0.2 +0.6 +0.2
(d) Start with st, test on st
Figure 4: Showing the performance change when starting with either in or out-of domain data, and adding
elements of the other data set. The ?A? indicates that the second data set is only used for alignments, the ?W?
indicates that it contributes alignments and phrase scores, and the ?E? indicates that it contributes alignments
and phrase entries. The figures above each bar shows the performance change relative to the single corpus
system.
System cs-en en-cs de-en en-de fr-en en-fr es-en en-es
ep 23.3 13.4 25.5 17.5 28.9 29.2 35.4 34.5
ep+ncA 23.5 (+0.2) 13.8 (+0.4) 25.9 (+0.4) 17.9 (+0.4) 29.3 (+0.4) 29.6 (+0.4) 35.7 (+0.3) 34.9 (+0.5)
ep+ncW 24.0 (+0.7) 14.2 (+0.8) 26.3 (+0.8) 18.2 (+0.7) 29.4 (+0.5) 29.8 (+0.6) 36.3 (+0.9) 35.6 (+1.1)
ep+ncE 26.2 (+2.9) 14.0 (+0.6) 27.0 (+1.5) 18.5 (+1.0) 29.7 (+0.9) 30.0 (+0.8) 37.0 (+1.7) 35.7 (+1.3)
nc 26.1 (+2.9) 14.3 (+0.9) 26.7 (+1.2) 18.0 (+0.6) 29.3 (+0.4) 29.1 (-0.1) 37.6 (+2.2) 36.5 (+2.1)
nc+epA 26.8 (+3.5) 14.6 (+1.2) 27.5 (+2.0) 18.5 (+1.0) 30.4 (+1.5) 29.9 (+0.7) 37.7 (+2.3) 36.4 (+2.0)
nc+epW 26.6 (+3.3) 14.4 (+1.0) 27.4 (+1.9) 18.4 (+1.0) 29.5 (+0.6) 29.8 (+0.6) 37.2 (+1.8) 36.5 (+2.0)
nc+epE 27.4 (+4.1) 14.7 (+1.3) 28.1 (+2.6) 19.0 (+1.5) 30.9 (+2.0) 30.2 (+1.0) 38.4 (+3.0) 36.9 (+2.4)
ep+nc 26.9 (+3.6) 14.2 (+0.8) 27.4 (+1.9) 18.8 (+1.3) 30.4 (+1.5) 30.0 (+0.8) 37.4 (+2.0) 36.4 (+2.0)
System cs-en en-cs nl-en en-nl fr-en en-fr es-en en-es
ep 10.9 6.9 18.2 15.7 14.5 13.8 19.1 17.1
ep+stA 11.9 (+1.0) 7.5 (+0.6) 19.0 (+0.8) 16.3 (+0.5) 15.0 (+0.5) 14.1 (+0.3) 19.8 (+0.7) 17.8 (+0.7)
ep+stW 12.2 (+1.3) 8.1 (+1.2) 20.0 (+1.7) 17.4 (+1.7) 15.8 (+1.3) 14.9 (+1.1) 20.8 (+1.7) 18.8 (+1.8)
ep+stE 18.0 (+7.1) 12.4 (+5.5) 22.5 (+4.2) 20.6 (+4.9) 19.6 (+5.1) 19.9 (+6.1) 25.6 (+6.5) 23.3 (+6.3)
st 18.0 (+7.2) 12.2 (+5.3) 23.4 (+5.1) 21.3 (+5.6) 19.7 (+5.2) 19.8 (+6.0) 26.3 (+7.2) 23.2 (+6.1)
st+epA 18.4 (+7.6) 12.4 (+5.5) 23.6 (+5.4) 21.3 (+5.6) 20.2 (+5.7) 20.1 (+6.3) 26.4 (+7.3) 23.5 (+6.5)
st+epW 18.2 (+7.3) 12.2 (+5.3) 22.4 (+4.2) 21.0 (+5.3) 19.9 (+5.4) 19.8 (+6.0) 25.8 (+6.7) 23.2 (+6.1)
st+epE 19.1 (+8.3) 12.5 (+5.6) 24.0 (+5.8) 21.7 (+6.0) 20.6 (+6.1) 20.9 (+7.1) 26.0 (+6.9) 23.7 (+6.6)
ep+st 18.5 (+7.6) 12.5 (+5.6) 23.0 (+4.8) 21.2 (+5.5) 20.4 (+5.9) 20.2 (+6.5) 26.0 (+6.9) 23.8 (+6.8)
Table 2: Complete scores for the experiments described in Section 3.4 and summarised in Figure 4. Naming
of the systems is explained in the text, and in the caption for Figure 4
429
unk  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17log2(count)0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Precis
ion
ncep+nc
(a) News commentary
unk  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17log2(count)0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Precis
ion
step+st
(b) Subtitles
Figure 5: Performance comparison of in-domain systems versus systems built from in and out-of domain
data concatenated. Precision is plotted against log2 of in-domain training frequency, and averaged across all
8 language pairs. The width of the bars indicates the average total number of occurrences in the test set.
all measures of translation quality. For the words
with moderate training set frequencies, the precision
is actually slightly higher for the systems built with
just in-domain data, an effect that is more marked
for the st domain.
4 Conclusions
In this paper we have attempted to give an in-
depth analysis of the domain adaptation problem for
two different domain adaptation problems in phrase-
based MT. The differences between the two prob-
lems are clearly illustrated by the results in Fig-
ures 2 and 3, where we see that the difference be-
tween the in-domain and out-of-domain data are
larger for the OpenSubtitles domain than for the
News-Commentary domain. This can be detected
by the differences in word distribution and out-of-
vocabulary rates observed in Figure 2, and is re-
flected by the differing translation results in Figure
3.
However, the experiments of Sections 3.4 and 3.5
show some common themes emerging in the two do-
mains. In both cases, the out-of-domain data helps
most when it is just allowed to add entries (i.e. ?fill
in?) the phrase-table, and using the scores provided
by out-of-domain data has a tendency to be harmful
to translation quality. The precision results of Sec-
tion 3.5 show out-of-domain data (when it is sim-
ply added to the training set) mainly helping with
the low frequency words, and having a neutral or
harmful effect for higher frequency words. This ex-
plains why approaches which try to weight the out-
of-domain data in some way (e.g. corpus weighting
or instance weighting) can be more successful than
430
simply concatenating data sets. It also suggests that
the way forward is to look for methods that use the
out-of-domain data mainly for rarer words, and not
to change translations which have a lot of evidence
in the in-domain data.
5 Acknowledgments
This work was supported by the EuroMatrixPlus3
and Accept4 projects funded by the European Com-
mission (7th Framework Programme).
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011.
Domain Adaptation via Pseudo In-Domain Data Se-
lection. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing,
pages 355?362, Edinburgh, Scotland, UK., July. As-
sociation for Computational Linguistics.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus Interpolation Methods for Phrase-
based SMT Adaptation. In Proceedings of IWSLT.
Peter F. Brown, Stephen Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1994. The Mathe-
matics of Statistical Machine Translation: Parameter
Estimation. Computational Linguistics, 19(2):263?
311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 Work-
shop on Statistical Machine Translation. In Proceed-
ings of the Sixth Workshop on Statistical Machine
Translation, pages 22?64, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Jorge Civera and Alfons Juan. 2007. Domain Adaptation
in Statistical Machine Translation with Mixture Mod-
elling. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 177?180, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Jonathan Clark, Chris Dyer, Alon Lavie, and Noah Smith.
2011. Better hypothesis testing for statistical machine
translation: Controlling for optimizer instability. In
Proceedings of ACL.
3www.euromatrixplus.net
4www.accept.unige.ch
Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.
2010. Analysis of translation model adaptation in sta-
tistical machine translation. In Proceedings of IWSLT.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
128?135, Prague, Czech Republic, June. Association
for Computational Linguistics.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative Instance Weighting for Domain Adap-
tation in Statistical Machine Translation. In Proceed-
ings of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 451?459, Cam-
bridge, MA, October. Association for Computational
Linguistics.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Philipp Koehn and Josh Schroeder. 2007. Experiments
in Domain Adaptation for Statistical Machine Transla-
tion. In Proceedings of the Second Workshop on Sta-
tistical Machine Translation, pages 224?227, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the ACL Demo Sessions, pages 177?180,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Saab Mansour, Joern Wuebker, and Hermann Ney. 2011.
Combining Translation and Language Model Scoring
for Domain-Specific Data Filtering. In Proceedings of
IWSLT.
Jan Niehues and Alex Waibel. 2010. Domain Adapta-
tion in Statistical Machine Translation using Factored
Translation Models. In Proceedings of EAMT.
Franz J. Och, Christoph Tillman, and Hermann Ney.
1999. Improved Alignment Models for Statistical Ma-
chine Translation. In Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing
and Very Large Corpora.
Franz J. Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings of
ACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of 40th
431
Annual Meeting of the Association for Computational
Linguistics, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July. Association for Computational Lin-
guistics.
Holger Schwenk, Patrik Lambert, Lo??c Barrault,
Christophe Servan, Sadaf Abdul-Rauf, Haithem Afli,
and Kashif Shah. 2011. LIUM?s SMT Machine Trans-
lation Systems for WMT 2011. In Proceedings of
the Sixth Workshop on Statistical Machine Translation,
pages 464?469, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Rico Sennrich. 2012. Perplexity Minimization for Trans-
lation Model Domain Adaptation in Statistical Ma-
chine Translation. In Proceedings of EACL.
Kashif Shah, Lo??c Barrault, and Holger Schwenk. 2010.
Translation Model Adaptation by Resampling. In Pro-
ceedings of the Joint Fifth Workshop on Statistical Ma-
chine Translation and MetricsMATR, pages 392?399,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Andreas Stolcke. 2002. SRILM ? An Extensible Lan-
guage Modeling Toolkit. In Proc. Intl. Conf. on Spo-
ken Language Processing, vol. 2, pages 901?904.
Jo?rg Tiedemann. 2009. News from OPUS - A Collection
of Multilingual Parallel Corpora with Tools and Inter-
faces. In N. Nicolov, K. Bontcheva, G. Angelova, and
R. Mitkov, editors, Recent Advances in Natural Lan-
guage Processing (vol V), pages 237?248. John Ben-
jamins, Amsterdam/Philadelphia.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, and
Eiichiro Sumita. 2008. Method of Selecting Train-
ing Data to Build a Compact and Efficient Translation
Model. In Proceedings of IJCNLP.
432
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1?44,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Findings of the 2013 Workshop on Statistical Machine Translation
Ondr?ej Bojar
Charles University in Prague
Christian Buck
University of Edinburgh
Chris Callison-Burch
University of Pennsylvania
Christian Federmann
Saarland University
Barry Haddow
University of Edinburgh
Philipp Koehn
University of Edinburgh
Christof Monz
University of Amsterdam
Matt Post
Johns Hopkins University
Radu Soricut
Google
Lucia Specia
University of Sheffield
Abstract
We present the results of the WMT13
shared tasks, which included a translation
task, a task for run-time estimation of ma-
chine translation quality, and an unoffi-
cial metrics task. This year, 143 machine
translation systems were submitted to the
ten translation tasks from 23 institutions.
An additional 6 anonymized systems were
included, and were then evaluated both au-
tomatically and manually, in our largest
manual evaluation to date. The quality es-
timation task had four subtasks, with a to-
tal of 14 teams, submitting 55 entries.
1 Introduction
We present the results of the shared tasks of
the Workshop on Statistical Machine Translation
(WMT) held at ACL 2013. This workshop builds
on seven previous WMT workshops (Koehn and
Monz, 2006; Callison-Burch et al, 2007, 2008,
2009, 2010, 2011, 2012).
This year we conducted three official tasks: a
translation task, a human evaluation of transla-
tion results, and a quality estimation task.1 In
the translation task (?2), participants were asked
to translate a shared test set, optionally restrict-
ing themselves to the provided training data. We
held ten translation tasks this year, between En-
glish and each of Czech, French, German, Span-
ish, and Russian. The Russian translation tasks
were new this year, and were also the most popu-
lar. The system outputs for each task were evalu-
ated both automatically and manually.
The human evaluation task (?3) involves ask-
ing human judges to rank sentences output by
anonymized systems. We obtained large numbers
of rankings from two groups: researchers (who
1The traditional metrics task is evaluated in a separate pa-
per (Macha?c?ek and Bojar, 2013).
contributed evaluations proportional to the number
of tasks they entered) and workers on Amazon?s
Mechanical Turk (who were paid). This year?s ef-
fort was our largest yet by a wide margin; we man-
aged to collect an order of magnitude more judg-
ments than in the past, allowing us to achieve sta-
tistical significance on the majority of the pairwise
system rankings. This year, we are also clustering
the systems according to these significance results,
instead of presenting a total ordering over systems.
The focus of the quality estimation task (?6)
is to produce real-time estimates of sentence- or
word-level machine translation quality. This task
has potential usefulness in a range of settings, such
as prioritizing output for human post-editing, or
selecting the best translations from a number of
systems. This year the following subtasks were
proposed: prediction of percentage of word edits
necessary to fix a sentence, ranking of up to five al-
ternative translations for a given source sentence,
prediction of post-editing time for a sentence, and
prediction of word-level scores for a given trans-
lation (correct/incorrect and types of edits). The
datasets included English-Spanish and German-
English news translations produced by a number
of machine translation systems. This marks the
second year we have conducted this task.
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dis-
seminate common test sets and public training data
with published performance numbers, and to re-
fine evaluation methodologies for machine trans-
lation. As before, all of the data, translations,
and collected human judgments are publicly avail-
able.2 We hope these datasets serve as a valu-
able resource for research into statistical machine
translation, system combination, and automatic
evaluation or prediction of translation quality.
2http://statmt.org/wmt13/results.html
1
2 Overview of the Translation Task
The recurring task of the workshop examines
translation between English and five other lan-
guages: German, Spanish, French, Czech, and ?
new this year ? Russian. We created a test set for
each language pair by translating newspaper arti-
cles and provided training data.
2.1 Test data
The test data for this year?s task was selected from
news stories from online sources. A total of 52
articles were selected, in roughly equal amounts
from a variety of Czech, English, French, German,
Spanish, and Russian news sites:3
Czech: aktua?lne?.cz (1), CTK (1), den??k (1),
iDNES.cz (3), lidovky.cz (1), Novinky.cz (2)
French: Cyber Presse (3), Le Devoir (1), Le
Monde (3), Liberation (2)
Spanish: ABC.es (2), BBC Spanish (1), El Peri-
odico (1), Milenio (3), Noroeste (1), Primera
Hora (3)
English: BBC (2), CNN (2), Economist (1),
Guardian (1), New York Times (2), The Tele-
graph (1)
German: Der Standard (1), Deutsche Welle (1),
FAZ (1), Frankfurter Rundschau (2), Welt (2)
Russian: AIF (2), BBC Russian (2), Izvestiya (1),
Rosbalt (1), Vesti (1)
The stories were translated by the professional
translation agency Capita, funded by the EU
Framework Programme 7 project MosesCore, and
by Yandex, a Russian search engine.4 All of the
translations were done directly, and not via an in-
termediate language.
2.2 Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
train language models, and development sets to
tune system parameters. Some training corpora
were identical from last year (Europarl5, United
Nations, French-English 109 corpus, CzEng),
some were updated (News Commentary, mono-
lingual data), and new corpora were added (Com-
mon Crawl (Smith et al, 2013), Russian-English
3For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
4http://www.yandex.com/
5As of Fall 2011, the proceedings of the European Parlia-
ment are no longer translated into all official languages.
parallel data provided by Yandex, Russian-English
Wikipedia Headlines provided by CMU).
Some statistics about the training materials are
given in Figure 1.
2.3 Submitted systems
We received 143 submissions from 23 institu-
tions. The participating institutions and their en-
try names are listed in Table 1; each system did
not necessarily appear in all translation tasks. We
also included three commercial off-the-shelf MT
systems and three online statistical MT systems,6
which we anonymized.
For presentation of the results, systems are
treated as either constrained or unconstrained, de-
pending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial sys-
tems are treated as unconstrained during the auto-
matic and human evaluations.
3 Human Evaluation
As with past workshops, we contend that auto-
matic measures of machine translation quality are
an imperfect substitute for human assessments.
We therefore conduct a manual evaluation of the
system outputs and define its results to be the prin-
cipal ranking of the workshop. In this section, we
describe how we collected this data and compute
the results, and then present the official results of
the ranking.
We run the evaluation campaign using an up-
dated version of Appraise (Federmann, 2012); the
tool has been extended to support collecting judg-
ments using Amazon?s Mechanical Turk, replac-
ing the annotation system used in previous WMTs.
The software, including all changes made for this
year?s workshop, is available from GitHub.7
This year differs from prior years in a few im-
portant ways:
? We collected about ten times more judgments
that we have in the past, using judgments
from both participants in the shared task and
non-experts hired on Amazon?s Mechanical
Turk.
? Instead of presenting a total ordering of sys-
tems for each pair, we cluster them and report
a ranking over the clusters.
6Thanks to Herve? Saint-Amand and Martin Popel for har-
vesting these entries.
7https://github.com/cfedermann/Appraise
2
Europarl Parallel Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,965,734 2,007,723 1,920,209 646,605
Words 56,895,229 54,420,026 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 176,258 117,481 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Parallel Corpus
Spanish? English French? English German? English Czech? English Russian? English
Sentences 174,441 157,168 178,221 140,324 150,217
Words 5,116,388 4,520,796 4,928,135 4,066,721 4,597,904 4,541,058 3,206,423 3,507,249 3,841,950 4,008,949
Distinct words 84,273 61,693 69,028 58,295 142,461 61,761 138,991 54,270 145,997 57,991
Common Crawl Parallel Corpus
Spanish? English French? English German? English Czech? English Russian? English
Sentences 1,845,286 3,244,152 2,399,123 161,838 878,386
Words 49,561,060 46,861,758 91,328,790 81,096,306 54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words 710,755 640,778 889,291 859,017 1,640,835 823,480 210,170 128,212 764,203 432,062
United Nations Parallel Corpus
Spanish? English French? English
Sentences 11,196,913 12,886,831
Words 318,788,686 365,127,098 411,916,781 360,341,450
Distinct words 593,567 581,339 565,553 666,077
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Parallel Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Yandex 1M Parallel Corpus
Russian? English
Sentences 1,000,000
Words 24,121,459 26,107,293
Distinct words 701,809 387,646
Wiki Headlines Parallel Corpus
Russian? English
Sentences 514,859
Words 1,191,474 1,230,644
Distinct words 282,989 251,328
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,218,201 2,123,835 2,190,579 2,176,537 668,595
Words 59,848,044 60,476,282 63,439,791 53,534,167 14,946,399
Distinct words 123,059 181,837 145,496 394,781 172,461
News Language Model Data
English Spanish French German Czech Russian
Sentence 68,521,621 13,384,314 21,195,476 54,619,789 27,540,749 19,912,911
Words 1,613,778,461 386,014,234 524,541,570 983,818,841 456,271,247 351,595,790
Distinct words 3,392,137 1,163,825 1,590,187 6,814,953 2,655,813 2,195,112
News Test Set
English Spanish French German Czech Russian
Sentences 3000
Words 64,810 73,659 73,659 63,412 57,050 58,327
Distinct words 8,935 10,601 11,441 12,189 15,324 15,736
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
3
ID Institution
BALAGUR Yandex School of Data Analysis (Borisov et al, 2013)
CMU
CMU-TREE-TO-TREE
Carnegie Mellon University (Ammar et al, 2013)
CU-BOJAR,
CU-DEPFIX,
CU-TAMCHYNA
Charles University in Prague (Bojar et al, 2013)
CU-KAREL, CU-ZEMAN Charles University in Prague (B??lek and Zeman, 2013)
CU-PHRASEFIX,
CU-TECTOMT
Charles University in Prague (Galus?c?a?kova? et al, 2013)
DCU Dublin City University (Rubino et al, 2013a)
DCU-FDA Dublin City University (Bicici, 2013a)
DCU-OKITA Dublin City University (Okita et al, 2013)
DESRT Universita` di Pisa (Miceli Barone and Attardi, 2013)
ITS-LATL University of Geneva
JHU Johns Hopkins University (Post et al, 2013)
KIT Karlsruhe Institute of Technology (Cho et al, 2013)
LIA Universite? d?Avignon (Huet et al, 2013)
LIMSI LIMSI (Allauzen et al, 2013)
MES-* Munich / Edinburgh / Stuttgart (Durrani et al, 2013a; Weller et al, 2013)
OMNIFLUENT SAIC (Matusov and Leusch, 2013)
PROMT PROMT Automated Translations Solutions
QCRI-MES Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al, 2013)
QUAERO QUAERO (Peitz et al, 2013a)
RWTH RWTH Aachen (Peitz et al, 2013b)
SHEF University of Sheffield
STANFORD Stanford University (Green et al, 2013)
TALP-UPC TALP Research Centre (Formiga et al, 2013a)
TUBITAK TU?BI?TAK-BI?LGEM (Durgar El-Kahlout and Mermer, 2013)
UCAM University of Cambridge (Pino et al, 2013)
UEDIN,
UEDIN-HEAFIELD
University of Edinburgh (Durrani et al, 2013b)
UEDIN-SYNTAX University of Edinburgh (Nadejde et al, 2013)
UMD University of Maryland (Eidelman et al, 2013)
UU Uppsala University (Stymne et al, 2013)
COMMERCIAL-1,2,3 Anonymized commercial systems
ONLINE-A,B,G Anonymized online systems
Table 1: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
4
3.1 Ranking translations of sentences
The ranking among systems is produced by col-
lecting a large number of rankings between the
systems? translations. Every language task had
many participating systems (the largest was 19,
for the Russian-English task). Rather than asking
judges to provide a complete ordering over all the
translations of a source segment, we instead ran-
domly select five systems and ask the judge to rank
just those. We call each of these a ranking task.
A screenshot of the ranking interface is shown in
Figure 2.
For each ranking task, the judge is presented
with a source segment, a reference translation,
and the outputs of five systems (anonymized and
randomly-ordered). The following simple instruc-
tions are provided:
You are shown a source sentence fol-
lowed by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
The rankings of the systems are numbered from 1
to 5, with 1 being the best translation and 5 be-
ing the worst. Each ranking task has the potential
to provide 10 pairwise rankings, and fewer if the
judge chooses any ties. For example, the ranking
{A:1, B:2, C:4, D:3, E:5}
provides 10 pairwise rankings, while the ranking
{A:3, B:3, C:4, D:3, E:1}
provides just 7. The absolute value of the ranking
or the degree of difference is not considered.
We use the collected pairwise rankings to assign
each system a score that reflects how highly that
system was usually ranked by the annotators. The
score for some system A reflects how frequently it
was judged to be better than other systems when
compared on the same segment; its score is the
number of pairwise rankings where it was judged
to be better, divided by the total number of non-
tying pairwise comparisons. These scores were
used to compute clusters of systems and rankings
between them (?3.4).
3.2 Collecting the data
A goal this year was to collect enough data to
achieve statistical significance in the rankings. We
distributed the workload among two groups of
judges: researchers and Turkers. The researcher
group comprised partipants in the shared task, who
were asked to contribute judgments on 300 sen-
tences for each system they contributed. The re-
searcher evaluation was held over three weeks
from May 17?June 7, and yielded about 280k pair-
wise rankings.
The Turker group was composed of non-expert
annotators hired on Amazon?s Mechanical Turk
(MTurk). A basic unit of work on MTurk is called
a Human Intelligence Task (HIT) and included
three ranking tasks, for which we paid $0.25. To
ensure that the Turkers provided high quality an-
notations, this portion of the evaluation was be-
gun after the researcher portion had completed,
enabling us to embed controls in the form of high-
consensus pairwise rankings in the Turker HITs.
To build these controls, we collected ranking tasks
containing pairwise rankings with a high degree of
researcher consensus. An example task is here:
SENTENCE 504
SOURCE Vor den heiligen Sta?tten verbeugen
REFERENCE Let?s worship the holy places
SYSTEM A Before the holy sites curtain
SYSTEM B Before we bow to the Holy Places
SYSTEM C To the holy sites bow
SYSTEM D Bow down to the holy sites
SYSTEM E Before the holy sites pay
MATRIX
A B C D E
A - 0 0 0 3
B 5 - 0 1 5
C 6 6 - 0 6
D 6 8 5 - 6
E 0 0 0 0 -
Matrix entry Mi,j records the number of re-
searchers who judged System i to be better than
System j. We use as controls pairwise judgments
for which |Mi,j?Mj,i| > 5, i.e., judgments where
the researcher consensus ran strongly in one direc-
tion. We rejected HITs from Turkers who encoun-
tered at least 10 of these controls and failed more
than 50% of them.
There were 463 people who participated in the
Turker portion of the manual evaluation, contribut-
ing 664k pairwise rankings from Turkers who
passed the controls. Together with the researcher
judgments, we collected close to a million pair-
wise rankings, compared to 101k collected last
year: a ten-fold increase. Table 2 contains more
detail.
5
Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a
source segment, a reference translation, and the outputs of five systems (anonymized and randomly-ordered) and has to rank
these according to their translation quality, ties are allowed. For technical reasons, annotators on Amazon?s Mechanical Turk
received all three ranking tasks for a single HIT on a single page, one upon the other.
3.3 Annotator agreement
Each year we calculate annotator agreement
scores for the human evaluation as a measure of
the reliability of the rankings. We measured pair-
wise agreement among annotators using Cohen?s
kappa coefficient (?) (Cohen, 1960), which is de-
fined as
? = P (A)? P (E)1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance. Note that ? is ba-
sically a normalized version of P (A), one which
takes into account how meaningful it is for anno-
tators to agree with each other, by incorporating
P (E). The values for ? range from 0 to 1, with
zero indicating no agreement and 1 perfect agree-
ment.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate
at which annotators agree, in the context of pair-
wise comparisons.
As for P (E), it should capture the probability
that two annotators would agree randomly. There-
fore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is
computed empirically, by observing how often an-
notators actually rank two systems as being tied.
Table 3 gives ? values for inter-annotator agree-
ment for WMT11?WMT13 while Table 4 de-
tails intra-annotator agreement scores. Due to the
change of annotation software, we used a slightly
different way of computing annotator agreement
scores. Therefore, we chose to re-compute values
for previous WMTs to allow for a fair comparison.
The exact interpretation of the kappa coefficient is
difficult, but according to Landis and Koch (1977),
0?0.2 is slight, 0.2?0.4 is fair, 0.4?0.6 is moderate,
6
LANGUAGE PAIR Systems Rankings Average
Czech-English 11 85,469 7,769.91
English-Czech 12 102,842 8,570.17
German-English 17 128,668 7,568.71
English-German 15 77,286 5,152.40
Spanish-English 12 67,832 5,652.67
English-Spanish 13 60,464 4,651.08
French-English 13 80,741 6,210.85
English-French 17 100,783 5,928.41
Russian-English 19 151,422 7,969.58
English-Russian 14 87,323 6,237.36
Total 148 942,840 6,370.54
WMT12 103 101,969 999.69
WMT11 133 63,045 474.02
Table 2: Amount of data collected in the WMT13 manual evaluation. The final two rows report summary information from the
previous two workshops.
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m
Czech-English 0.400 0.311 0.244 0.342 0.279
English-Czech 0.460 0.359 0.168 0.408 0.075
German-English 0.324 0.385 0.299 0.443 0.324
English-German 0.378 0.356 0.267 0.457 0.239
Spanish-English 0.494 0.298 0.277 0.415 0.295
English-Spanish 0.367 0.254 0.206 0.333 0.249
French-English 0.402 0.272 0.275 0.405 0.321
English-French 0.406 0.296 0.231 0.434 0.237
Russian-English ? ? 0.278 0.315 0.324
English-Russian ? ? 0.243 0.416 0.207
Table 3: ? scores measuring inter-annotator agreement. The WMT13r and WMT13m columns provide breakdowns for re-
searcher annotations and MTurk annotations, respectively. See Table 4 for corresponding intra-annotator agreement scores.
0.6?0.8 is substantial, and 0.8?1.0 is almost per-
fect. We find that the agreement rates are more or
less the same as in prior years.
The WMT13 column contains both researcher
and Turker annotations at a roughly 1:2 ratio. The
final two columns break out agreement numbers
between these two groups. The researcher agree-
ment rates are similar to agreement rates from past
years, while the Turker agreement are well below
researcher agreement rates, varying widely, but of-
ten comparable to WMT11 and WMT12. Clearly,
researchers are providing us with more consistent
opinions, but whether these differences are ex-
plained by Turkers racing through jobs, the partic-
ularities that inform researchers judging systems
they know well, or something else, is hard to tell.
Intra-annotator agreement scores are also on par
from last year?s level, and are often much better.
We observe better intra-annotator agreement for
researchers compared to Turkers.
As a small test, we varied the threshold of ac-
ceptance against the controls for the Turker data
alone and computed inter-annotator agreement
scores on the datasets for the Russian?English task
(the only language pair where we had enough data
at high thresholds). Table 5 shows that higher
thresholds do indeed give us better agreements,
but not monotonically. The increasing ?s sug-
gests that we can find a segment of Turkers who
do a better job and that perhaps a slightly higher
threshold of 0.6 would serve us better, while the
remaining difference against the researchers sug-
gests there may be different mindsets informing
the decisions. In any case, getting the best perfor-
mance out of the Turkers remains difficult.
3.4 System Score
Given the multitude of pairwise comparisons, we
would like to rank the systems according to a
single score computed for each system. In re-
7
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m
Czech-English 0.597 0.454 0.479 0.483 0.478
English-Czech 0.601 0.390 0.290 0.547 0.242
German-English 0.576 0.392 0.535 0.643 0.515
English-German 0.528 0.433 0.498 0.649 0.452
Spanish-English 0.574 1.000 0.575 0.605 0.537
English-Spanish 0.426 0.329 0.492 0.468 0.492
French-English 0.673 0.360 0.578 0.585 0.565
English-French 0.524 0.414 0.495 0.630 0.486
Russian-English ? ? 0.450 0.363 0.477
English-Russian ? ? 0.513 0.582 0.500
Table 4: ? scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the
human evaluation. The WMT13r and WMT13m columns provide breakdowns for researcher annotations and MTurk annota-
tions, respectively. The perfect inter-annotator agreement for Spanish-English is a result of there being very little data for that
language pair.
thresh. rankings ?
0.5 16,605 0.234
0.6 9,999 0.337
0.7 3,219 0.360
0.8 1,851 0.395
0.9 849 0.336
Table 5: Agreement as a function of threshold for Turkers on
the Russian?English task. The threshold is the percentage of
controls a Turker must pass for her rankings to be accepted.
cent evaluation campaigns, we tweaked the metric
and now arrived at a intuitive score that has been
demonstrated to be accurate in ranking systems ac-
cording to their true quality (Koehn, 2012).
The score, which we call EXPECTED WINS, has
an intuitive explanation. If the system is compared
against a randomly picked opposing system, on a
randomly picked sentence, by a randomly picked
judge, what is the probability that its translation is
ranked higher?
Formally, the score for a system Si among a set
of systems {Sj} given a pool of pairwise rankings
summarized as win(A,B) ? the number of times
system A is ranked higher than system B ? is
defined as follows:
score(Si) = 1|{Sj}|
?
j,j 6=i
win(Si, Sj)
win(Si, Sj) + win(Sj , Si)
Note that this score ignores ties.
3.5 Rank Ranges and Clusters
Given the scores, we would like to rank the sys-
tems, which is straightforward. But we would also
like to know, if the obtained system ranking is
statistically significant. Typically, given the large
number of systems that participate, and the simi-
larity of the systems given a common training data
condition and often common toolsets, there will be
some systems that will be very close in quality.
To establish the reliability of the obtained sys-
tem ranking, we use bootstrap resampling. We
sample from the set of pairwise rankings an equal
sized set of pairwise rankings (allowing for multi-
ple drawings of the same pairwise ranking), com-
pute the expected wins score for each system
based on this sample, and rank each system. By
repeating this procedure a 1,000 times, we can de-
termine a range of ranks, into which system falls
at least 95% of the time (i.e., at least 950 times) ?
corresponding to a p-level of p ? 0.05.
Furthermore, given the rank ranges for each sys-
tem, we can cluster systems with overlapping rank
ranges.8
For all language pairs and all systems, Table 6
reports all system scores, rank ranges, and clus-
ters. The official interpretation of these results
is that systems in the same cluster are considered
tied. Given the large number of judgements that
we collected, it was possible to group on average
about two systems in a cluster, even though the
systems in the middle are typically in larger clus-
ters.
8Formally, given ranges defined by start(Si) and end(Si),
we seek the largest set of clusters {Cc} that satisfies:
?S ?C : S ? C
S ? Ca, S ? Cb ? Ca = Cb
Ca 6= Cb ? ?Si ? Ca, Sj ? Cb :
start(Si) > end(Sj) or start(Sj) > end(Si)
8
Czech-English
# score range system
1 0.607 1 UEDIN-HEAFIELD
2 0.582 2-3 ONLINE-B
0.573 2-4 MES
0.562 3-5 UEDIN
0.547 4-7 ONLINE-A
0.542 5-7 UEDIN-SYNTAX
0.534 6-7 CU-ZEMAN
8 0.482 8 CU-TAMCHYNA
9 0.458 9 DCU-FDA
10 0.321 10 JHU
11 0.297 11 SHEF-WPROA
English-Czech
# score range system
1 0.580 1-2 CU-BOJAR
0.578 1-2 CU-DEPFIX
3 0.562 3 ONLINE-B
4 0.525 4 UEDIN
5 0.505 5-7 CU-ZEMAN
0.502 5-7 MES
0.499 5-8 ONLINE-A
0.484 7-9 CU-PHRASEFIX
0.476 8-9 CU-TECTOMT
10 0.457 10-11 COMMERCIAL-1
0.450 10-11 COMMERCIAL-2
12 0.389 12 SHEF-WPROA
Spanish-English
# score range system
1 0.624 1 UEDIN-HEAFIELD
2 0.595 2 ONLINE-B
3 0.570 3-5 UEDIN
0.570 3-5 ONLINE-A
0.567 3-5 MES
6 0.537 6 LIMSI-SOUL
7 0.514 7 DCU
8 0.488 8-9 DCU-OKITA
0.484 8-9 DCU-FDA
10 0.462 10 CU-ZEMAN
11 0.425 11 JHU
12 0.169 12 SHEF-WPROA
English-Spanish
# rank range system
1 0.637 1 ONLINE-B
2 0.582 2-4 ONLINE-A
0.578 2-4 UEDIN
0.567 3-4 PROMT
5 0.535 5-6 MES
0.528 5-6 TALP-UPC
7 0.491 7-8 LIMSI
0.474 7-9 DCU
0.472 8-10 DCU-FDA
0.455 9-11 DCU-OKITA
0.446 10-11 CU-ZEMAN
12 0.417 12 JHU
13 0.324 13 SHEF-WPROA
German-English
# rank range system
1 0.660 1 ONLINE-B
2 0.620 2-3 ONLINE-A
0.608 2-3 UEDIN-SYNTAX
4 0.586 4-5 UEDIN
0.584 4-5 QUAERO
0.571 5-7 KIT
0.562 6-7 MES
8 0.543 8-9 RWTH-JANE
0.533 8-10 MES-REORDER
0.526 9-10 LIMSI-SOUL
11 0.480 11 TUBITAK
12 0.462 12-13 UMD
0.462 12-13 DCU
14 0.396 14 CU-ZEMAN
15 0.367 15 JHU
16 0.311 16 SHEF-WPROA
17 0.238 17 DESRT
English-German
# rank range system
1 0.637 1-2 ONLINE-B
0.636 1-2 PROMT
3 0.614 3 UEDIN-SYNTAX
0.587 3-5 ONLINE-A
0.571 4-6 UEDIN
0.554 5-6 KIT
7 0.523 7 STANFORD
8 0.507 8 LIMSI-SOUL
9 0.477 9-11 MES-REORDER
0.476 9-11 JHU
0.460 10-12 CU-ZEMAN
0.453 11-12 TUBITAK
13 0.361 13 UU
14 0.329 14-15 SHEF-WPROA
0.323 14-15 RWTH-JANE
English-Russian
# rank range system
1 0.641 1 PROMT
2 0.623 2 ONLINE-B
3 0.556 3-4 CMU
0.542 3-6 ONLINE-G
0.538 3-7 ONLINE-A
0.531 4-7 UEDIN
0.520 5-7 QCRI-MES
8 0.498 8 CU-KAREL
9 0.478 9-10 MES-QCRI
0.469 9-10 JHU
11 0.434 11-12 COMMERCIAL-3
0.426 11-13 LIA
0.419 12-13 BALAGUR
14 0.331 14 CU-ZEMAN
French-English
# rank range system
1 0.638 1 UEDIN-HEAFIELD
2 0.604 2-3 UEDIN
0.591 2-3 ONLINE-B
4 0.573 4-5 LIMSI-SOUL
0.562 4-5 KIT
0.541 5-6 ONLINE-A
7 0.512 7 MES-SIMPLIFIED
8 0.486 8 DCU
9 0.439 9-10 RWTH
0.429 9-11 CMU-T2T
0.420 10-11 CU-ZEMAN
12 0.389 12 JHU
13 0.322 13 SHEF-WPROA
English-French
# rank range system
1 0.607 1-2 UEDIN
0.600 1-3 ONLINE-B
0.588 2-4 LIMSI-SOUL
0.584 3-4 KIT
5 0.553 5-7 PROMT
0.551 5-8 STANFORD
0.547 5-8 MES
0.537 6-9 MES-INFLECTION
0.533 7-10 RWTH-PB
0.516 9-11 ONLINE-A
0.499 10-11 DCU
12 0.427 12 CU-ZEMAN
13 0.408 13 JHU
14 0.382 14 OMNIFLUENT
15 0.350 15 ITS-LATL
16 0.326 16 ITS-LATL-PE
Russian-English
# rank range system
1 0.657 1 ONLINE-B
2 0.604 2-3 CMU
0.588 2-3 ONLINE-A
4 0.562 4-6 ONLINE-G
0.561 4-6 PROMT
0.550 5-7 QCRI-MES
0.546 5-7 UCAM
8 0.527 8-9 BALAGUR
0.519 8-10 MES-QCRI
0.507 9-11 UEDIN
0.497 10-12 OMNIFLUENT
0.492 11-14 LIA
0.483 12-15 OMNIFLUENT-C
0.481 12-15 UMD
0.476 13-15 CU-KAREL
16 0.432 16 COMMERCIAL-3
17 0.417 17 UEDIN-SYNTAX
18 0.396 18 JHU
19 0.215 19 CU-ZEMAN
Table 6: Official results for the WMT13 translation task. Systems are ordered by the expected win score. Lines between
systems indicate clusters according to bootstrap resampling at p-level p ? .05. This method is also used to determine the
range of ranks into which system falls. Systems with grey background indicate use of resources that fall outside the constraints
provided for the shared task.
9
4 Understandability of English?Czech
For the English-to-Czech translation, we con-
ducted a variation of the ?understandability? test
as introduced in WMT09 (Callison-Burch et al,
2009) and used in WMT10. In order to obtain
additional reference translations, we conflated this
test with post-editing. The procedure was as fol-
lows:
1. Monolingual editing (also called blind edit-
ing). The first annotator is given just the MT
output and requested to correct it. Given er-
rors in MT outputs, some guessing of the
original meaning is often inevitable and the
annotators are welcome to try. If unable, they
can mark the sentences as incomprehensible.
2. Review. A second annotator is asked to
validate the monolingual edit given both the
source and reference translations. Our in-
structions specify three options:
(a) If the monolingual edit is an adequate
translation and acceptably fluent Czech,
confirm it without changes.
(b) If the monolingual edit is adequate but
needs polishing, modify the sentence
and prefix it with the label ?OK:?.
(c) If the monolingual edit is wrong, cor-
rect it. You may start from the origi-
nal unedited MT output, if that is eas-
ier. Avoid using the reference directly,
prefer words from MT output whenever
possible.
The motivation behind this procedure is that we
want to save the time necessary for reading the
sentence. If the reviewer has already considered
whether the sentence is an acceptable translation,
they do not need to read the MT output again in
order to post-edit it. Our approach is thus some-
what the converse of Aziz et al (2013) who ana-
lyze post-editing effort to obtain rankings of MT
systems. We want to measure the understandabil-
ity of MT outputs and obtain post-edits at the same
time.
Both annotation steps were carried out in
the CASMACAT/Matecat post-editing user inter-
face.9, modified to provide the relevant variants of
the sentence next to the main edit box. Screen-
shots of the two annotation phases are given in
Figure 3 and Figure 4.
9http://www.casmacat.eu/index.php?n=Workbench
Occurrence GOOD ALMOST BAD EMPTY Total
First 34.7 0.1 42.3 11.0 4082
Repeated 41.1 0.1 41.0 6.1 805
Overall 35.8 0.1 42.1 10.2 4887
Table 7: Distribution of review statuses.
Similarly to the traditional ranking task, we pro-
vided three consecutive sentences from the origi-
nal text, each translated with a different MT sys-
tem. The annotators are free to use this contex-
tual information when guessing the meaning or re-
viewing the monolingual edits. Each ?annotation
HIT? consists of 24 sentences, i.e. 8 snippets of 3
consecutive sentences.
4.1 Basic Statistics on Editing
In total, 21 annotators took part in the exercise, 20
of them contributed to monolingual editing and 19
contributed to the reviews.
Connecting each review with the monolingual
edit (some edits received multiple reviews), we ob-
tain one data row. We collected 4887 data rows
(i.e. sentence revisions) for 3538 monolingual ed-
its, covering 1468 source sentences as translated
by 12 MT systems (including the reference).
Not all MT systems were considered for each
sentence, we preferred to obtain judgments for
more source sentences.
Based on the annotation instructions, each data
row has one of the four possible statuses: GOOD,
ALMOST, BAD, and EMPTY. GOOD rows are
those where the reviewer accepted the monolin-
gual edit without changes, ALMOST edits were
modified by the reviewer but they were marked as
?OK?. BAD edits were changed by the reviewer
and no ?OK? mark was given. Finally, the sta-
tus EMPTY is assigned to rows where the mono-
lingual editor refused to edit the sentence. The
EMPTY rows nevertheless contain the (?regular?)
post-edit of the reviewer, so they still provide a
new reference translation for the sentence.
Table 7 summarizes the distribution of row sta-
tuses depending on one more significant distinc-
tion: whether the monolingual editor has seen the
sentence before or not. We see that EMPTY and
BAD monolingual edits together drop by about
6% absolute when the sentence is not new to the
monolingual editor. The occurrence is counted as
?repeated? regardless whether the annotator has
previously seen the sentence in an editing or re-
viewing task. Unless stated otherwise, we exclude
repeated edits from our calculations.
10
Figure 3: In this screen, the annotator is expected to correct the MT output given only the context of at most two neighbouring
machine-translated sentences.
ALMOST Pairwise
treated Comparisons Agreement ?
inter
separate 2690 56.0 0.270
as BAD 2690 67.9 0.351
as GOOD 2690 65.2 0.289
intra
separate 170 65.3 0.410
as BAD 170 69.4 0.386
as GOOD 170 71.8 0.422
Table 8: Annotator agreement when reviewing monolingual
edits.
4.2 Agreement on Understandability
Before looking at individual system results, we
consider annotator agreement in the review step.
Details are given in Table 8. Given a (non-
EMPTY) string from a monolingual edit, we
would like to know how often two acceptability
judgments by two different reviewers (inter-) or
the same reviewer (intra-) agree. The repeated ed-
its remain in this analysis because we are not in-
terested in the origin of the string.
Our annotation setup leads to three possible la-
bels: GOOD, ALMOST, and BAD. The agree-
ment on one of three classes is bound to be lower
than the agreement on two classes, so we also re-
interpret ALMOST as either GOOD or BAD. Gen-
erally speaking, ALMOST is a positive judgment,
so it would be natural to treat it as GOOD. How-
ever, in our particular setup, when the reviewer
modified the sentence and forgot to add the label
?OK:?, the item ended up in the BAD class. We
conclude that this is indeed the case: the inter-
annotator agreement appears higher if ALMOST
is treated as BAD. Future versions of the review-
ing interface should perhaps first ask for the yes/no
judgment and only then allow to post-edit.
The ? values in Table 8 are the Fleiss?
kappa (Fleiss, 1971), accounting for agreement by
chance given the observed label distributions.
In WMT09, the agreements for this task were
higher: 77.4 for inter-AA and 86.6 for intra-AA.
(In 2010, the agreements for this task were not re-
ported.) It is difficult to say whether the differ-
ence lies in the particular language pair, the dif-
ferent set of annotators, or the different user in-
terface for our reviewing task. In 2009 and 2010,
the reviewers were shown 5 monolingual edits at
once and they were asked to judge each as accept-
able or not acceptable. We show just one segment
and they have probably set their minds on the post-
editing rather than acceptability judgment. We be-
lieve that higher agreements can be reached if the
reviewers first validate one or more of the edits and
only then are allowed to post-edit it.
4.3 Understandability of English?Czech
Table 9 brings about the first main result of our
post-editing effort. For each system (including
the reference translation), we check how often a
monolingual edit was marked OK or ALMOST
by the subsequent reviewer. The average under-
standability across all MT systems into Czech is
44.2?1.6%. This is a considerable improvement
compared to 2009 where the best systems pro-
duced about 32% understandable sentences. In
11
Figure 4: In this screen, the annotator is expected to validate the monolingual edit, correcting it if necessary. The annotator is
expected to add the prefix ?OK:? if the correction was more or less cosmetic.
Rank System Total Observations % Understandable
Overall incl. ref. 4082 46.7?1.6
Overall without ref. 3808 44.2?1.6
1 Reference 274?31 80.3?4.8
2-6 CU-ZEMAN 348?34 51.7?5.1
2-6 UEDIN 332?33 51.5?5.4
2-6 ONLINE-B 337?34 50.7?5.3
2-6 CU-BOJAR 341?35 50.7?5.2
2-7 CU-DEPFIX 350?34 48.0?5.3
6-10 COMMERCIAL-2 358?36 43.6?5.2
6-11 COMMERCIAL-1 316?34 41.5?5.5
7-12 CU-TECTOMT 338?34 39.4?5.2
8-12 MES 346?36 38.4?5.2
8-12 CU-PHRASEFIX 394?40 38.1?4.8
10-12 SHEF-WPROA 348?32 34.2?5.1
2009 Reference 91
2009 Best System 32
2010 Reference 97
2010 Best System 58
Table 9: Understandability of English?Czech systems. The
? values indicate empirical confidence bounds at 95%. Rank
ranges were also obtained in the same resampling: in 95% of
observations, the system was ranked in the given range.
2010, the best systems or system combinations
reached 55%?58%. The test set across years and
the quality of references and judgments also play a
role. In our annotation setup, the references appear
to be correctly understandable only to 80.3?4.8%.
To estimate the variance of these results due
to the particular sentences chosen, we draw 1000
random samples from the dataset, preserving the
dataset size and repeating some. The exact num-
ber of judgments per system can thus vary. We
report the 95% empirical confidence interval after
the ??? signs in Table 9 (the systems range from
?4.8 to?5.5). When we drop individual blind ed-
itors or reviewers, the understandability judgments
differ by about ?2 to ?4. In other words, the de-
pendence on the test set appears higher than the
dependence on the annotators.
The limited size of our dataset alows us only
to separate two main groups of systems: those
ranking 2?6 and those ranking worse. This rough
grouping vaguely matches with WMT13 ranking
results as given in Table 6. A somewhat surpris-
ing observation is that two automatic corrections
ranked better in WMT13 ranking but score worse
in understandability: CU-DEPFIX fixes some lost
negation and some agreement errors of CU-BOJAR
and CU-PHRASEFIX is a standard statistical post-
editing of a transfer-based system CU-TECTOMT.
A detailed inspection of the data is necessary to
explain this.
5 More Reference Translations for Czech
Our annotation procedure described in Section 4
allowed us to obtain a considerable number of ad-
ditional reference translations on top of official
single reference.
12
Refs 1 2 3 4 5 6 7 8 9 10-16
Sents 233 709 174 123 60 48 40 27 25 29
Table 10: Number of source sentences with the given number
of distinct reference translations.
In total, our edits cover 1468 source sentences,
i.e. about a half of the official test set size, and pro-
vide 4311 unique references. On average, one sen-
tence in our set has 2.94?2.17 unique reference
translations. Table 10 provides a histogram.
It is well known that automatic MT evalua-
tion methods perform better with more references,
because a single one may not confirm a correct
part of MT output. This issue is more severe
for morphologically rich languages like Czech
where about 1/3 of MT output was correct but not
confirmed by the reference (Bojar et al, 2010).
Advanced evaluation methods apply paraphras-
ing to smooth out some of the lexical divergence
(Kauchak and Barzilay, 2006; Snover et al, 2009;
Denkowski and Lavie, 2010). Simpler techniques
such as lemmatizing are effective for morphologi-
cally rich languages (Tantug et al, 2008; Kos and
Bojar, 2009) but they will lose resolution once the
systems start performing generally well.
WMTs have taken the stance that a big enough
test set with just a single reference should compen-
sate for the lack of other references. We use our
post-edited reference translations to check this as-
sumption for BLEU and NIST as implemented in
mteval-13a (international tokenization switched
on, which is not the default setting).
We run many probes, randomly picking the test
set size (number of distinct sentences) and the
number of distinct references per sentence. Note
that such test sets are somewhat artificially more
diverse; in narrow domains, source sentences can
repeat and even appear verbatim in the training
data, and in natural test sets with multiple refer-
ences, short sentences can receive several identical
translations.
For each probe, we measure the Spearman?s
rank correlation coefficient ? of the ranks pro-
posed by BLEU or NIST and the manual ranks.
We use the same implementation as applied in the
WMT13 Shared Metrics Task (Macha?c?ek and Bo-
jar, 2013). Note that the WMT13 metrics task still
uses the WMT12 evaluation method ignoring ties,
not the expected wins. As Koehn (2012) shows,
the two methods do not differ much.
Overall, the correlation is strongly impacted by
Figure 5: Correlation of BLEU and WMT13 manual ranks
for English?Czech translation
Figure 6: Correlation of NIST and WMT13 manual ranks
for English?Czech translation
the particular choice of test sentences and refer-
ence translations. By picking sentences randomly,
similarly or equally sized test sets can reach dif-
ferent correlations. Indeed, e.g. for a test set of
about 1500 distinct sentences selected from the
3000-sentence official test set (1 reference trans-
lation), we obtain correlations for BLEU between
0.86 and 0.94.
Figure 5 plots the correlations of BLEU and the
system rankings, Figure 6 provides the same pic-
ture for NIST. The upper triangular part of the plot
contains samples from our post-edited reference
translations, the lower rectangular part contains
probes from the official test set of 3000 sentences
with 1 reference translation.
To interpret the observations, we also calculate
the average and standard deviation of correlations
for each cell in Figures 5 and 6. Figures 7 and
8 plot the values for 1, 6, 7 and 8 references for
13
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 10  100  1000C
o
r
r
e
l
a
t
i
o
n
 
o
f
 
B
L
E
U
 
a
n
d
 
m
a
n
u
a
l
 
r
a
n
k
i
n
g
Test set size
Refs: official 1Refs: postedited 1Refs: postedited 6Refs: postedited 7Refs: postedited 8
Figure 7: Projections from Figure 5 of BLEU and WMT13
manual ranks for English?Czech translation
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 10  100  1000C
o
r
r
e
l
a
t
i
o
n
 
o
f
 
N
I
S
T
 
a
n
d
 
m
a
n
u
a
l
 
r
a
n
k
i
n
g
Test set size
Refs: official 1Refs: postedited 1Refs: postedited 6Refs: postedited 7Refs: postedited 8
Figure 8: Projections from Figure 6 of NIST and WMT13
manual ranks for English?Czech translation
BLEU and NIST, resp. The projections confirm
that the average correlations grow with test set
size, the growth is however sub-logarithmic.
Starting from as few as a dozen of sentences, we
see that using more references is better than using
a larger test set. For BLEU, we however already
seem to reach false positives at 7 references for
one or two hundred sentences: larger sets with just
one reference may correlate slightly better.
Using one reference obtained by post-editing
seems better than using the official (independent)
reference translations. BLEU is more affected
than NIST by this difference even at relatively
large test set size. Note that our post-edits are in-
spired by all MT systems, the good as well as the
bad ones. This probably provides our set with a
certain balance.
Overall, the best balance between the test set
size and the number of references seems to lie
somewhere around 7 references and 100 or 200
sentences. Creating such a test set could be even
cheaper than the standard 3000 sentences with just
one reference. However, the wide error bars re-
mind us that even this setting can lead to correla-
tions anywhere between 0.86 and 0.96. For other
languages, data sets types or other MT evaluation
methods, the best setting can be quite different and
has to be sought for.
6 Quality Estimation Task
Machine translation quality estimation is the task
of predicting a quality score for a machine trans-
lated text without access to reference translations.
The most common approach is to treat the problem
as a supervised machine learning task, using stan-
dard regression or classification algorithms. The
second edition of the WMT shared task on qual-
ity estimation builds on the previous edition of the
task (Callison-Burch et al, 2012), with variants to
this previous task, including both sentence-level
and word-level estimation, with new training and
test datasets, along with evaluation metrics and
baseline systems.
The motivation to include both sentence- and
word-level estimation come from the different po-
tential applications of these variants. Some inter-
esting uses of sentence-level quality estimation are
the following:
? Decide whether a given translation is good
enough for publishing as is.
? Inform readers of the target language only
whether or not they can rely on a translation.
? Filter out sentences that are not good enough
for post-editing by professional translators.
? Select the best translation among options
from multiple MT and/or translation memory
systems.
Some interesting uses of word-level quality es-
timation are the following:
? Highlight words that need editing in post-
editing tasks.
? Inform readers of portions of the sentence
which are not reliable.
? Select the best segments among options from
multiple translation systems for MT system
combination.
The goals of this year?s shared task were:
14
? To explore various granularity levels for the
task (sentence-level and word-level).
? To explore the prediction of more objective
scores such as edit distance and post-editing
time.
? To explore the use of quality estimation tech-
niques to replace reference-based MT evalua-
tion metrics in the task of ranking alternative
translations generated by different MT sys-
tems.
? To identify new and effective quality indica-
tors (features) for all variants of the quality
estimation task.
? To identify effective machine learning tech-
niques for all variants of the quality estima-
tion task.
? To establish the state of the art performance
in the field.
Four subtasks were proposed, as we discuss in
Sections 6.1 and 6.2. Each subtask provides spe-
cific datasets, annotated for quality according to
the subtask (Section 6.3), and evaluates the system
submissions using specific metrics (Section 6.6).
When available, external resources (e.g. SMT
training corpus) and translation engine-related re-
sources were given to participants (Section 6.4),
who could also use any additional external re-
sources (no distinction between open and close
tracks is made). Participants were also provided
with a software package to extract quality esti-
mation features and perform model learning (Sec-
tion 6.5), with a suggested list of baseline features
and learning method (Section 6.7). Participants
could submit up to two systems for each subtask.
6.1 Sentence-level Quality Estimation
Task 1.1 Predicting Post-editing Distance This
task is similar to the quality estimation task in
WMT12, but with one important difference in the
scoring variant: instead of using the post-editing
effort scores in the [1-5] range, we use HTER
(Snover et al, 2006) as quality score. This score
is to be interpreted as the minimum edit distance
between the machine translation and its manually
post-edited version, and its range is [0, 1] (0 when
no edit needs to be made, and 1 when all words
need to be edited). Two variants of the results
could be submitted in the shared task:
? Scoring: A quality score for each sentence
translation in [0,1], to be interpreted as an
HTER score; lower scores mean better trans-
lations.
? Ranking: A ranking of sentence translations
for all source test sentences from best to
worst. For this variant, it does not matter how
the ranking is produced (from HTER predic-
tions, likert predictions, or even without ma-
chine learning). The reference ranking is de-
fined based on the true HTER scores.
Task 1.2 Selecting Best Translation This task
consists in ranking up to five alternative transla-
tions for the same source sentence produced by
multiple MT systems. We use essentially the same
data provided to participants of previous years
WMT?s evaluation metrics task ? where MT eval-
uation metrics are assessed according to how well
they correlate with human rankings. However, ref-
erence translations produced by humans are not be
used in this task.
Task 1.3 Predicting Post-editing Time For this
task systems are required to produce, for each
translation, the expected time (in seconds) it
would take a translator to post-edit such an MT
output. The main application for predictions of
this type is in computer-aided translation where
the predicted time can be used to select among dif-
ferent hypotheses or even to omit any MT output
in cases where no good suggestion is available.
6.2 Word-level Quality Estimation
Based on the data of Task 1.3, we define Task 2, a
word-level annotation task for which participants
are asked to produce a label for each token that
indicates whether the word should be changed by
a post-editor or kept in the final translation. We
consider the following two sets of labels for pre-
diction:
? Binary classification: a keep/change label,
the latter meaning that the token should be
corrected in the post-editing process.
? Multi-class classification: a label specifying
the edit action that should be performed on
the token (keep as is, delete, or substitute).
6.3 Datasets
Task 1.1 Predicting post-editing distance For
the training of models, we provided the WMT12
15
quality estimation dataset: 2,254 English-
Spanish news sentences extracted from previous
WMT translation task English-Spanish test sets
(WMT09, WMT10, and WMT12). These were
translated by a phrase-based SMT Moses system
trained on Europarl and News Commentaries cor-
pora as provided by WMT, along with their source
sentences, reference translations, post-edited
translations, and HTER scores. We used TERp
(default settings: tokenised, case insensitive,
etc., but capped to 1)10 to compute the HTER
scores. Likert scores in [1,5] were also provided,
as participants may choose to use them for the
ranking variant.
As test data, we use a subset of the WMT13
English-Spanish news test set with 500 sentences,
whose translations were produced by the same
SMT system used for the training set. To com-
pute the true HTER labels, the translations were
post-edited under the same conditions as those on
the training set. As in any blind shared task, the
HTER scores were solely used to evaluate the sub-
missions, and were only released to participants
after they submitted their systems.
A few variations of the training and test data
were provided, including a version with cases re-
stored and a version detokenized. In addition,
we provided a number of engine-internal informa-
tion from Moses for glass-box feature extraction,
such as phrase and word alignments, model scores,
word graph, n-best lists and information from the
decoder?s search graph.
Task 1.2 Selecting best translation As training
data, we provided a large set of up to five alter-
native machine translations produced by different
MT systems for each source sentence and ranked
for quality by humans. This was the outcome of
the manual evaluation of the translation task from
WMT09-WMT12. It includes two language pairs:
German-English and English-Spanish, with 7,098
and 4,592 source sentences and up to five ranked
translations, totalling 32,922 and 22,447 transla-
tions, respectively.
As test data, a set of up to five alternative ma-
chine translations per source sentence from the
WMT08 test sets was provided, with 365 (1,810)
and 264 (1,315) source sentences (translations)
for German-English and English-Spanish, respec-
tively. We note that there was some overlap be-
tween the MT systems used in the training data
10http://www.umiacs.umd.edu/?snover/terp/
and test datasets, but not all systems were the
same, as different systems participate in WMT
over the years.
Task 1.3 and Task 2 Predicting post-editing
time and word-level edits For Tasks 1.3 and 2
we provides a new dataset consisting of 22 English
news articles which were translated into Span-
ish using Moses and post-edited during a CAS-
MACAT11 field trial. Of these, 15 documents have
been processed repeatedly by at least 2 out of 5
translators, resulting in a total of 1,087 segments.
For each segment we provided:
? English source and Spanish translation.
? Spanish MT output which was used as basis
for post-editing.
? Document and translator ID.
? Position of the segment within the document.
The metadata about translator and document was
made available as we expect that translator perfor-
mance and normalisation over document complex-
ity can be helpful when predicting the time spend
on a given segment.
For the training portion of the data we also pro-
vided:
? Time to post-edit in seconds (Task 1.3).
? Binary (Keep, Change) and multiclass (Keep,
Substitute, Delete) labels on word level along
with explicit tokenization (Task 2).
The labels in Task 2 are derived by comput-
ing WER between the original machine translation
and its post-edited version.
6.4 Resources
For all tasks, we provided resources to extract
quality estimation features when these were avail-
able:
? The SMT training corpus (WMT News and
Europarl): source and target sides of the cor-
pus used to train the SMT engines for Tasks
1.1, 1.3, and 2, and truecase models gener-
ated from these. These corpora can also be
used for Task 1.2, but we note that some of
the MT systems used in the datasets of this
task were not statistical or did not use (only)
the training corpus provided by WMT.
11http://casmacat.eu/
16
? Language models: n-gram language models
of source and target languages generated us-
ing the SMT training corpora and standard
toolkits such as SRILM Stolcke (2002), and
a language model of POS tags for the target
language. We also provided unigram, bigram
and trigram counts.
? IBM Model 1 lexical tables generated by
GIZA++ using the SMT training corpora.
? Phrase tables with word alignment informa-
tion generated by scripts provided by Moses
from the parallel corpora.
? For Tasks 1.1, 1.3 and 2, the Moses config-
uration file used for decoding or the code to
re-run the entire Moses system.
? For Task 1.1, both English and Spanish re-
sources for a number of advanced features
such as pre-generated PCFG parsing models,
topic models, global lexicon models and mu-
tual information trigger models.
We refer the reader to the QUEST website12 for
a detailed list of resources provided for each task.
6.5 QUEST Framework
QUEST (Specia et al, 2013) is an open source
framework for quality estimation which provides a
wide variety of feature extractors from source and
translation texts and external resources and tools.
These range from simple, language-independent
features, to advanced, linguistically motivated fea-
tures. They include features that rely on informa-
tion from the MT system that generated the trans-
lations (glass-box features), and features that are
oblivious to the way translations were produced
(black-box features).
QUEST also integrates a well-known machine
learning toolkit, scikit-learn,13 and other algo-
rithms that are known to perform well on this task
(e.g. Gaussian Processes), providing a simple and
effective way of experimenting with techniques
for feature selection and model building, as well
as parameter optimisation through grid search.
From QUEST, a subset of 17 features and an
SVM regression implementation were used as
baseline for Tasks 1.1, 1.2 and 1.3. The software
was made available to all participants.
12http://www.quest.dcs.shef.ac.uk/
13http://scikit-learn.org/
6.6 Evaluation Metrics
Task 1.1 Predicting post-editing distance
Evaluation is performed against the HTER and/or
ranking of translations using the same metrics as
in WMT12. For the scoring variant of the task,
we use two standard metrics for regression tasks:
Mean Absolute Error (MAE) as a primary metric,
and Root of Mean Squared Error (RMSE) as a
secondary metric. To improve readability, we
report these error numbers by first mapping the
HTER values to the [0, 100] interval, to be read
as percentage-points of the HTER metric. For a
given test set S with entries si, 1 ? i ? |S|, we
denote by H(si) the proposed score for entry si
(hypothesis), and by V (si) the reference value for
entry si (gold-standard value):
MAE =
?N
i=1 |H(si)? V (si)|
|S|
RMSE =
??N
i=1(H(si)? V (si))2
|S|
Both these metrics are non-parametric, auto-
matic and deterministic (and therefore consistent),
and extrinsically interpretable. For instance, a
MAE value of 10 means that, on average, the ab-
solute difference between the hypothesized score
and the reference score value is 10 percentage
points (i.e., 0.10 difference in HTER scores). The
interpretation of RMSE is similar, with the differ-
ence that RMSE penalises larger errors more (via
the square function).
For the ranking variant of the task, we use the
DeltaAvg metric proposed in the 2012 edition of
the task (Callison-Burch et al, 2012) as our main
metric. This metric assumes that each reference
test instance has an extrinsic number associated
with it that represents its ranking with respect to
the other test instances. For completeness, we
present here again the definition of DeltaAvg.
The goal of the DeltaAvg metric is to measure
how valuable a proposed ranking (which we call a
hypothesis ranking) is, according to the true rank-
ing values associated with the test instances. We
first define a parametrised version of this metric,
called DeltaAvg[n]. The following notations are
used: for a given entry sentence s, V (s) represents
the function that associates an extrinsic value to
that entry; we extend this notation to a set S, with
V (S) representing the average of all V (s), s ? S.
17
Intuitively, V (S) is a quantitative measure of the
?quality? of the set S, as induced by the extrinsic
values associated with the entries in S. For a set
of ranked entries S and a parameter n, we denote
by S1 the first quantile of set S (the highest-ranked
entries), S2 the second quantile, and so on, for n
quantiles of equal sizes.14 We also use the nota-
tion Si,j = ?jk=i Sk. Using these notations, wedefine:
DeltaAvgV [n] =
?n?1
k=1 V (S1,k)
n? 1 ? V (S)
When the valuation function V is clear from the
context, we write DeltaAvg[n] for DeltaAvgV [n].
The parameter n represents the number of quan-
tiles we want to split the set S into. For instance,
n = 2 gives DeltaAvg[2] = V (S1)?V (S), hence it
measures the difference between the quality of the
top quantile (top half) S1 and the overall quality
(represented by V (S)). For n = 3, DeltaAvg[3] =
(V (S1)+V (S1,2)/2?V (S) = ((V (S1)?V (S))+
(V (S1,2?V (S)))/2, hence it measures an average
difference across two cases: between the quality of
the top quantile (top third) and the overall quality,
and between the quality of the top two quantiles
(S1 ? S2, top two-thirds) and the overall quality.
In general, DeltaAvg[n] measures an average dif-
ference in quality across n ? 1 cases, with each
case measuring the impact in quality of adding an
additional quantile, from top to bottom. Finally,
we define:
DeltaAvgV =
?N
n=2 DeltaAvgV [n]
N ? 1
where N = |S|/2. As before, we write DeltaAvg
for DeltaAvgV when the valuation function V is
clear from the context. The DeltaAvg metric is an
average across all DeltaAvg[n] values, for those
n values for which the resulting quantiles have at
least 2 entries (no singleton quantiles).
We present results for DeltaAvg using as valu-
ation function V the HTER scores, as defined in
Section 6.3. We also use Spearman?s rank correla-
tion coefficient ? as a secondary metric.
Task 1.2 Selecting best translation The perfor-
mance on the task of selecting the best transla-
tion from a pool of translation candidates is mea-
14If the size |S| is not divisible by n, then the last quantile
Sn is assumed to contain the rest of the entries.
sured by comparing proposed (hypothesis) rank-
ings against human-produced rankings. The met-
ric used is Kendall?s ? rank correlation coefficient,
computed as follows:
? = |concordant pairs| ? |discordant pairs||total pairs|
where a concordant pair is a pair of two transla-
tions for the same source segment in which the
ranking order proposed by a human annotator and
the ranking order of the hypothesis agree; in a dis-
cordant pair, they disagree. The possible values of
? range between 1 (where all pairs are concordant)
and ?1 (where all pairs are discordant). Thus a
system with ranking predictions having a higher
? value makes predictions that are more similar
to human judgements than a system with ranking
predictions having a lower ? . Note that, in general,
being able to predict rankings with an accuracy
of ? = ?1 is as difficult as predicting rankings
with an accuracy of ? = 1, whereas a completely
random ranking would have an expected value of
? = 0. The range is therefore said to be symmet-
ric.
However, there are two distinct ways of mea-
suring rank correlation using Kendall?s ? , related
to the way ties are treated. They greatly affect how
Kendall?s ? numbers are to be interpreted, and es-
pecially the symmetry property. We explain the
difference in detail in what follows.
Kendall?s ? with ties penalised If the goal is
to measure to what extent the difference in qual-
ity visible to a human annotator has been captured
by an automatically produced hypothesis (recall-
oriented view), then proposing a tie between t1
and t2 (t1-equal-to-t2) when the pair was judged
(in the reference) as t1-better-than-t2 is treated as
a failure-to-recall. In other words, it is as bad as
proposing t1-worse-than-t2. Henceforth, we call
this recall-oriented measure ?Kendall?s ? with ties
penalised?. This metric has the following proper-
ties:
? it is completely fair when comparing differ-
ent methods to produce ranking hypotheses,
because the denominator (number of total
pairs) is the same (it is the number of non-
tied pairs under the human judgements).
? it is non-symmetric, in the sense that a value
of ? = ?1 is not as difficult to obtain as ? =
18
1 (simply proposing only ties gets a ? = ?1);
hence, the sign of the ? value matters.
? the expected value of a completely random
ranking is not necessarily ? = 0, but rather
depends on the number of ties in the refer-
ence rankings (i.e., it is test set dependent).
Kendall?s ? with ties ignored If the goal
is to measure to what extent the difference in
quality signalled by an automatically produced
hypothesis is reflected in the human annota-
tion (precision-oriented view), then proposing t1-
equal-to-t2 when the pair was judged differently
in the reference does no harm the metric.
Henceforth, we call this precision-oriented
measure ?Kendall?s ? with ties ignored?. This
metric has the following properties:
? it is not completely fair when comparing dif-
ferent methods to produce ranking hypothe-
ses, because the denominator (number of to-
tal pairs) may not be the same (it is the num-
ber of non-tied pairs under each system?s pro-
posal).
? it is symmetric, in the sense that a value of
? = ?1 is as difficult to obtain as ? = 1;
hence, the sign of the ? value may not mat-
ter. 15
? the expected value of a completely random
ranking is ? = 0 (test-set independent).
The first property is the most worrisome from
the perspective of reporting the results of a shared
task, because a system may fare very well on this
metric simply because it choses not to commit
(proposes ties) most of the time. Therefore, to
give a better understanding of the systems? perfor-
mance, for Kendall?s ? with ties ignored we also
provide the number of non-ties proposed by each
system.
Task 1.3 Predicting post-editing time Submis-
sions are evaluated in terms of Mean Average Er-
ror (MAE) against the actual time spent by post-
editors (in seconds). By using a linear error mea-
sure we limit the influence of outliers: sentences
that took very long to edit or where the measure-
ment taken is questionable.
15In real life applications this distinction matters. Even
if, from a computational perspective, it is as hard to get ?
close to?1 as it is to get it close to 1, knowing the sign is the
difference between selecting the best or the worse translation.
To further analyse the influence of extreme val-
ues, we also compute Spearman?s rank correlation
? coefficient which does not depend on the abso-
lute values of the predictions.
We also give RMSE and Pearson?s correlation
coefficient r for reference.
Task 2 Predicting word-level scores The word-
level task is primarily evaluated by macro-
averaged F-measure. Because the class distribu-
tion is skewed ? in the test data about one third
of the tokens are marked as correct ? we compute
precision and recall and F1 for each class individ-
ually. Consider the following confusion matrix for
the two classes Keep and Change:
predicted
(K)eep (C)hange
expected (K)eep 10 20(C)hange 30 40
For the given example we derive true-positive
(tp), true-negative (tn), false-positive (fp), and
false-negative (fn) counts:
tpK = 10 fpK = 30 fnK = 20
tpC = 40 fpC = 20 fnC = 30
precisionK =
tpK
tpK + fpK
= 10/40
recallK =
tpK
tpK + fnK
= 10/30
F1,K =
2 ? precisionK ? recallK
precisionK +recallK
A single cumulative statistic can be computed
by averaging the resulting F-measures (macro av-
eraging) or by micro averaging in which case pre-
cision and recall are first computed by accumulat-
ing the relevant values for all classes (O?zgu?r et al,
2005), e.g.
precision = tpK + tpC(tpK + fpK) + (tpC + fpC)
The latter gives equal weight to each exam-
ple and is therefore dominated by performance on
the largest class while macro-averaged F-measure
gives equal weight to each class.
The same setup is used to evaluate the perfor-
mance in the multiclass setting. Please note that
here the test data only contains 4% examples for
class (D)elete.
19
ID Participating team
CMU Carnegie Mellon University, USA (Hildebrand and Vogel, 2013)
CNGL Centre for Next Generation Localization, Ireland (Bicici, 2013b)
DCU Dublin City University, Ireland (Almaghout and Specia, 2013)
DCU-SYMC Dublin City University & Symantec, Ireland (Rubino et al, 2013b)
DFKI German Research Centre for Artificial Intelligence, Germany (Avramidis and
Popovic, 2013)
FBK-UEdin Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de
Souza et al, 2013)
LIG Laboratoire d?Informatique Grenoble, France (Luong et al, 2013)
LIMSI Laboratoire d?Informatique pour la Me?canique et les Sciences de l?Inge?nieur,
France (Singh et al, 2013)
LORIA Lorraine Laboratory of Research in Computer Science and its Applications,
France (Langlois and Smaili, 2013)
SHEF University of Sheffield, UK (Beck et al, 2013)
TCD-CNGL Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013)
TCD-DCU-CNGL Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and
Rubino, 2013)
UMAC University of Macau, China (Han et al, 2013)
UPC Universitat Politecnica de Catalunya, Spain (Formiga et al, 2013b)
Table 11: Participants in the WMT13 Quality Estimation shared task.
6.7 Participants
Table 11 lists all participating teams submitting
systems to any subtask in this shared task. Each
team was allowed up to two submissions for each
subtask. In the descriptions below participation in
specific tasks is denoted by a task identifier: T1.1,
T1.2, T1.3, and T2.
Sentence-level baseline system (T1.1, T1.3):
QUEST was used to extract 17 system-
independent features from the source and
translation files and the SMT training cor-
pus that were found to be relevant in previous
work (same features as in the WMT12 shared
task):
? number of tokens in the source and tar-
get sentences.
? average source token length.
? average number of occurrences of the
target word within the target sentence.
? number of punctuation marks in source
and target sentences.
? Language model probability of source
and target sentences using language
models provided by the task.
? average number of translations per
source word in the sentence: as given
by IBM 1 model thresholded so that
P (t|s) > 0.2, and so that P (t|s) > 0.01
weighted by the inverse frequency of
each word in the source side of the SMT
training corpus.
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower
frequency words) and 4 (higher fre-
quency words) in the source side of the
SMT training corpus
? percentage of unigrams in the source
sentence seen in the source side of the
SMT training corpus.
These features are used to train a Support
Vector Machine (SVM) regression algorithm
using a radial basis function kernel within the
SCIKIT-LEARN toolkit. The ?,  and C pa-
rameters were optimized using a grid-search
and 5-fold cross validation on the training
set. We note that although the system is re-
ferred to as a ?baseline?, it is in fact a strong
system. For tasks of the same type as 1.1
and 1.3, it has proved robust across a range
of language pairs, MT systems, and text do-
mains for predicting post-editing effort, as it
has also been shown in the previous edition
of the task (Callison-Burch et al, 2012).
The same features could be useful for a base-
line system for Task 1.2. In our official re-
20
sults, however, the baseline for Task 1.2 is
simpler than that: it proposes random ranks
for each pair of alternative translations for a
given source sentence, as we will discuss in
Section 6.8.
CMU (T1.1, T1.2, T1.3): The CMU quality
estimation system was trained on features
based on language models, the MT sys-
tem?s distortion model and phrase table fea-
tures, statistical word lexica, several sentence
length statistics, source language word and
bi-gram frequency statistics, n-best list agree-
ment and diversity, source language parse,
source-target word alignment and a depen-
dency parse based cohesion penalty. These
features were extracted using GIZA++, a
forced alignment algorithm and the Stanford
parser (de Marneffe et al, 2006). The pre-
diction models were trained using four clas-
sifiers in the Weka toolkit (Hall et al, 2009):
linear regression, M5P trees, multi layer per-
ceptron and SVM regression. In addition to
main system submission, a classic n-best list
re-ranking approach was used for Task 1.2.
CNGL (T1.1, T1.2, T1.3, T2): CNGL systems
are based on referential translation machines
(RTM) (Bic?ici and van Genabith, 2013), par-
allel feature decay algorithms (FDA) (Bicici,
2013a), and machine translation performance
predictor (MTPP) (Bic?ici et al, 2013), all
of which allow to obtain language and MT
system-independent predictions. For each
task, RTM models were developed using the
parallel corpora and the language model cor-
pora distributed by the WMT13 translation
task and the language model corpora pro-
vided by LDC for English and Spanish.
The sentence-level features are described in
MTPP (Bic?ici et al, 2013); they include
monolingual or bilingual features using n-
grams defined over text or common cover
link (CCL) (Seginer, 2007) structures as the
basic units of information over which sim-
ilarity calculations are made. RTMs use
308 features about coverage and diversity,
IBM1, and sentence translation performance,
retrieval closeness and minimum Bayes re-
trieval risk, distributional similarity and en-
tropy, IBM2 alignment, character n-grams,
and sentence readability. The learning mod-
els are Support Vector Machines (SVR) and
SVR with partial least squares (SVRPLS).
The word-level features include CCL links,
word length, location, prefix, suffix, form,
context, and alignment, totalling 511K fea-
tures for binary classification, and 637K for
multiclass classification. Generalised lin-
ear models (GLM) (Collins, 2002) and GLM
with dynamic learning (GLMd) were used.
DCU (T1.2): The main German-English submis-
sion uses six Combinatory Categorial Gram-
mar (CCG) features: CCG supertag lan-
guage model perplexity and log probability,
the number of maximal CCG constituents in
the translation output which are the highest-
probability minimum number of CCG con-
stituents that span the translation output, the
percentage of CCG argument mismatches be-
tween each subsequent CCG supertags, the
percentage of CCG argument mismatches be-
tween each subsequent CCG maximal cate-
gories and the minimum number of phrases
detected in the translation output. A second
submission uses the aforementioned CCG
features combined with 80 features from
QUEST as described in (Specia, 2011). For
the CCG features, the C&C parser was used
to parse the translation output. Moses was
used to build the phrase table from the SMT
training corpus with maximum phrase length
set to 7. The language model of supertags
was built using the SRILM toolkit. As learn-
ing algorithm, Logistic Regression as pro-
vided by the SCIKIT-LEARN toolkit was used.
The training data was prepared by converting
each ranking of translation outputs to a set
of pairwise comparisons according to the ap-
proach proposed by Avramidis et al (2011).
The rankings were generated back from pair-
wise comparisons predicted by the model.
DCU-SYMC (T1.1): The DCU-Symantec team
employed a wide set of features which in-
cluded language model, n-gram counts and
word-alignment features as well as syntac-
tic features, topic model features and pseudo-
reference features. The main learning algo-
rithm was SVR, but regression tree learning
was used to perform feature selection, re-
ducing the initial set of 442 features to 96
features (DCU-Symantec alltypes) and 134
21
(DCU-Symantec combine). Two methods
for feature selection were used: a best-first
search in the feature space using regression
trees to evaluate the subsets, and reading bi-
narised features directly from the nodes of
pruned regression trees.
The following NLP tools were used in feature
extraction: the Brown English Wall-Street-
Journal-trained statistical parser (Charniak
and Johnson, 2005), a Lexical Functional
Grammar parser (XLE), together with a
hand-crafted Lexical Functional Grammar,
the English ParGram grammar (Kaplan et al,
2004), and the TreeTagger part-of-speech
tagger (Schmidt, 1994) with off-the-shelf
publicly available pre-trained tagging mod-
els for English and Spanish. For pseudo-
reference features, the Bing, Moses and Sys-
tran translation systems were used. The Mal-
let toolkit (McCallum, 2002) was used to
build the topic models and features based on
a grammar checker were extracted with Lan-
guageTool.16
DFKI (T1.2, T1.3): DFKI?s submission for Task
1.2 was based on decomposing rankings into
pairs (Avramidis, 2012), where the best sys-
tem for each pair was predicted with Lo-
gistic Regression (LogReg). For German-
English, LogReg was trained with Stepwise
Feature Selection (Hosmer, 1989) on two
feature sets: Feature Set 24 includes ba-
sic counts augmented with PCFG parsing
features (number of VPs, alternative parses,
parse probability) on both source and tar-
get sentences (Avramidis et al, 2011), and
pseudo-reference METEOR score; the most
successful set, Feature Set 33 combines those
24 features with the 17 baseline features. For
English-Spanish, LogReg was used with L2
Regularisation (Lin et al, 2007) and two fea-
ture sets were devised after scoring features
with ReliefF (Kononenko, 1994) and Infor-
mation Gain (Hunt et al, 1966). Feature Set
431 combines 30 features with highest abso-
lute Relief-F and Information Gain (15 from
each). features with the highest
Task 1.3 was modelled using feature sets
selected after Relief-F scoring of external
black-box and glass-box features extracted
16http://www.languagetool.org/
from the SMT decoding process. The most
successful submission (linear6) was trained
with Linear Regression including the 17 fea-
tures with highest positive Relief-F. Most
prominent features include the alternative
possible parses of the source and target sen-
tence, the positions of the phrases with the
lowest and highest probability and future
cost estimate in the translation, the counts of
phrases in the decoding graph whose prob-
ability or whether the future cost estimate
is higher/lower than their standard deviation,
counts of verbs and determiners, etc. The
second submission (pls8) was trained with
Partial Least Squares regression (Stone and
Brooks, 1990) including more glass-box fea-
tures.
FBK-Uedin (T1.1, T1.3):
The submissions explored features built on
MT engine resources including automatic
word alignment, n-best candidate translation
lists, back-translations and word posterior
probabilities. Information about word align-
ments is used to extract quantitative (amount
and distribution of the alignments) and qual-
itative (importance of the aligned terms) fea-
tures under the assumption that alignment
information can help tasks where sentence-
level semantic relations need to be identified
(Souza et al, 2013). Three similar English-
Spanish systems are built and used to provide
pseudo-references (Soricut et al, 2012) and
back-translations, from which automatic MT
evaluation metrics could be computed and
used as features.
All features were computed over a concatena-
tion of several publicly available parallel cor-
pora for the English-Spanish language pair
such as Europarl, News Commentary, and
MultiUN. The models were developed using
supervised learning algorithms: SVMs (with
feature selection step prior to model learning)
and extremely randomized trees.
LIG (T2): The LIG systems are designed to
deal with both binary and multiclass variants
of the word level task. They integrate sev-
eral features including: system-based (graph
topology, language model, alignment con-
text, etc.), lexical (Part-of-Speech tags), syn-
tactic (constituent label, distance to the con-
22
stituent tree root) and semantic (target and
source polysemy count). Besides the exist-
ing components of the SMT system, feature
extraction requires further external tools and
resources, such as: TreeTagger (for POS tag-
ging), Bekerley Parser trained with AnCora
treebank (for generating constituent trees in
Spanish), WordNet and BabelNet (for pol-
ysemy count), Google Translate. The fea-
ture set is then combined and trained using
a Conditional Random Fields (CRF) learn-
ing method. During the labelling phase, the
optimal threshold is tuned using a small de-
velopment set split from the original training
set. In order to retain the most informative
features and eliminate the redundant ones, a
Sequential Backward Selection algorithm is
employed over the all-feature systems. With
the binary classifier, the Boosting technique
is applied to allow a number of sub feature
sets to complement each other, resulting in
the ?stronger? combined system.
LIMSI (T1.1, T1.3): The two tasks were treated
as regression problems using a simple elas-
tic regression, a linear model trained with L1
and L2 regularisers. Regarding features, the
submissions mainly aimed at evaluating the
usefulness for quality estimation of n-gram
posterior probabilities (Gispert et al, 2013)
that quantify the probability for a given n-
gram to be part of the system output. Their
computation relies on all the hypotheses con-
sidered by a SMT system during decoding:
intuitively, the more hypotheses a n-gram ap-
pears in, the more confident the system is
that this n-gram is part of the correct trans-
lation, and the higher its posterior probabil-
ity is. The feature set contains 395 other fea-
tures that differs, in two ways, from the tra-
ditional features used in quality estimation.
First, it includes several features based on
large span continuous space language mod-
els (Le et al, 2011) that have already proved
their efficiency both for the translation task
and the quality estimation task. Second, each
feature was expanded into two ?normalized
forms? in which their value was divided ei-
ther by the source length or the target length
and, when relevant, into a ?ratio form? in
which the feature value computed on the tar-
get sentence is divided by its value computed
in the source sentence.
LORIA (T1.1): The system uses the 17 baseline
features, plus several numerical and boolean
features computed from the source and target
sentences (Langlois et al, 2012). These are
based on language model information (per-
plexity, level of back-off, intra-lingual trig-
gers), translation table (IBM1 table, inter-
lingual triggers). For language models, for-
ward and backward models are built. Each
feature gives a score to each word in the sen-
tence, and the score of the sentence is the av-
erage of word scores. For several features,
the score of a word depends on the score of its
neighbours. This leads to 66 features. Sup-
port Vector Machines are used to learn a re-
gression model. In training is done in a multi-
stage procedure aimed at increasing the size
of the training corpus. Initially, the train-
ing corpus with machine translated sentences
provided by the task is used to train an SVM
model. Then this model is applied to the post-
edited and reference sentences (also provided
as part of the task). These are added to the
quality estimation training corpus using as la-
bels the SVM predictions. An algorithm to
tune the predicted scores on a development
corpus is used.
SHEF (T1.1, T1.3): These submissions use
Gaussian Processes, a non-parametric prob-
abilistic learning framework for regression,
along with two techniques to improve predic-
tion performance and minimise the amount
of resources needed for the problem: feature
selection based on optimised hyperparame-
ters and active learning to reduce the training
set size (and therefore the annotation effort).
The initial set features contains all black box
and glass box features available within the
QUEST framework (Specia et al, 2013) for
the dataset at hand (160 in total for Task 1.1,
and 80 for Task 1.3). The query selection
strategy for active learning is based on the
informativeness of the instances using Infor-
mation Density, a measure that leverages be-
tween the variance among instances and how
dense the region (in the feature space) where
the instance is located is. To perform fea-
ture selection, following (Shah et al, 2013)
features are ranked by the Gaussian Process
23
algorithm according to their learned length
scales, which can be interpreted as the rel-
evance of such feature for the model. This
information was used for feature selection
by discarding the lowest ranked (least use-
ful) ones. based on empirical results found
in (Shah et al, 2013), the top 25 features for
both models were selected and used to retrain
the same regression algorithm.
UPC (T1.2): The methodology used a broad set
of features, mainly available through the last
version of the Asiya toolkit for MT evalua-
tion (Gonza`lez et al, 2012)17. Concretely,
86 features were derived for the German-to-
English and 97 features for the English-to-
Spanish tasks. These features cover differ-
ent approaches and include standard qual-
ity estimation features, as provided by the
above mentioned Asiya and QUEST toolk-
its, but also a variety of features based on
pseudo-references, explicit semantic analy-
sis and specialised language models trained
on the parallel and monolingual corpora pro-
vided by the WMT Translation Task.
The system selection task is approached by
means of pairwise ranking decisions. It uses
Random Forest classifiers with ties, expand-
ing the work of 402013cFormiga et al), from
which a full ranking can be derived and the
best system per sentence is identified. Once
the classes are given by the Random Forest,
one can build a graph by means of the adja-
cency matrix of the pairwise decision. The fi-
nal ranking is assigned through a dominance
scheme similar to Pighin et al (2012).
An important remark of the methodology is
the feature selection process, since it was no-
ticed that the learner was sensitive to the fea-
tures used. Selecting the appropriate set of
features was crucial to achieve a good per-
formance. The best feature combination was
composed of: i) a baseline quality estimation
feature set (Asiya or Quest) but not both of
them, ii) Length Model, iii) Pseudo-reference
aligned based features, and iv) adapted lan-
guage models. However, within the de-en
task, substituting Length Model and Aligned
Pseudo-references by the features based on
17http://asiya.lsi.upc.edu/
Semantic Roles could bring marginally bet-
ter accuracy.
TCD-CNGL (T1.1) and TCD-DCU-CNGL
(T1.3): The system is based on features
which are commonly used for style classifi-
cation (e.g. author identification). The as-
sumption is that low/high quality translations
can be characterised by some patterns which
are frequent and/or differ significantly from
the opposite category. Such features are in-
tended to focus on striking patterns rather
than to capture the global quality in a sen-
tence, but they are used in conjunction with
classical features for quality estimation (lan-
guage modelling, etc.). This requires two
steps in the training process: first the refer-
ence categories against which sentences will
be compared are built, then the standard qual-
ity estimation model training stage is per-
formed. Both datasets (Tasks 1.1 and 1.3)
were used for both tasks. Since the number
of features can be very high (up to 65,000),
a combination of various heuristics for se-
lecting features was used before the training
stage (the submitted systems were trained us-
ing SVM with RBF kernels).
UMAC (T1.1, T1.2, T2): For Task 1.1, the fea-
ture set consists in POS sequences of the
source and target languages, using 12 uni-
versal tags that are common in both lan-
guages. The algorithm is an enhanced ver-
sion of the BLEU metric (EBLEU) designed
with a modified length penalty and added re-
call factor, and having the precision and re-
call components grouped using the harmonic
mean. For Task 1.2, in addition to the uni-
versal POS sequences of the source and tar-
get languages, features include the scores of
length penalty, precision, recall and rank.
Variants of EBLEU with different strategies
for alignment are used, as well as a Na??ve
Bayes classification algorithm. For Task 2,
the features used are unigrams (from previous
4th to following 3rd tokens), bigrams (from
previous 2nd to following 2nd tokens), skip
bigrams (previous and next token), trigrams
(from previous 2nd to following 2nd tokens).
The learning algorithms are Conditional Ran-
dom Fields and Na??ve Bayes.
24
6.8 Results
In what follows we give the official results for all
tasks followed by a discussion that highlights the
main findings for each of the tasks.
Task 1.1 Predicting post-editing distance
Table 12 summarises the results for the ranking
variant of the task. They are sorted from best to
worse using the DeltaAvg metric scores as primary
key and the Spearman?s rank correlation scores as
secondary key.
The winning submissions for the ranking vari-
ant of Task 1.1 are CNGL SVRPLS, with a
DeltaAvg score of 11.09, and DCU-SYMC all-
types, with a DeltaAvg score of 10.13. While the
former holds the higher score, the difference is not
significant at the p ? 0.05 level as estimated by a
bootstrap resampling test.
Both submissions are better than the baseline
system by a very wide margin, a larger relative im-
provement than that obtained in the corresponding
WMT12 task. In addition, five submissions (out
of 12 systems) scored significantly higher than the
baseline system (systems above the middle gray
area), which is a larger proportion than that in last
year?s task (only 3 out of 16 systems), indicat-
ing that this shared task succeeded in pushing the
state-of-the-art performance to new levels.
In addition to the performance of the official
submission, we report results obtained by two or-
acle methods: the gold-label HTER metric com-
puted against the post-edited translations as ref-
erence (Oracle HTER), and the BLEU metric (1-
BLEU to obtain the same range as HTER) com-
puted against the same post-edited translations as
reference (Oracle HBLEU). The ?Oracle HTER?
DeltaAvg score of 16.38 gives an upperbound in
terms of DeltaAvg for the test set used in this eval-
uation. It indicates that, for this set, the differ-
ence in post-editing effort between the top quality
quantiles and the overall quality is 16.38 on aver-
age. The oracle based on HBLEU gives a lower
DeltaAvg score, which is expected since HTER
was our actual gold label. However, it is still
significantly higher than the score of the winning
submission, which shows that there is significant
room for improvement even by the highest scor-
ing submissions.
The results for the scoring variant of the task
are presented in Table 13, sorted from best to
worse by using the MAE metric scores as primary
key and the RMSE metric scores as secondary key.
According to MAE scores, the winning submis-
sion is SHEF FS (MAE = 12.42), which uses fea-
ture selection and a novel learning algorithm for
the task, Gaussian Processes. The baseline sys-
tem is measured to have an MAE of 14.81, with
six other submissions having performances that
are not different from the baseline at a statisti-
cally significant level, as shown by the gray area
in the middle of Table 13). Nine submissions (out
of 16) scored significantly higher than the base-
line system (systems above the middle gray area),
a considerably higher proportion of submissions
as compared to last year (5 out of 19), which indi-
cates that this shared task also succeeded in push-
ing the state-of-the-art performance to new levels
in terms of absolute scoring. Only one (6%) sys-
tem scored significantly lower than the baseline,
as opposed to 8 (42%) in last year?s task.
For the sake of completeness, we also show or-
acles figures using the same methods as for the
ranking variant of the task. Here the lowerbound
in error (Oracle HTER) will clearly be zero, as
both MAE and RMSE are measured against the
same gold label used for the oracle computation.
?Oracle HBLEU? is also not indicative in this
case, as the although the values for the two metrics
(HTER and HBLEU) are within the same ranges,
they are not directly comparable. This explains the
larger MAE/RMSE figures for ?Oracle HBLEU?
than those for most submissions.
Task 1.2 Selecting the best translation
Below we present the results for this task for each
of the two Kendall?s ? flavours presented in Sec-
tion 6.6, for the German-English test set (Tables 14
and 16) and the English-Spanish test set (Tables 15
and 17). The results are sorted from best to worse
using each of the Kendall?s ? metric flavours.
For German-English, the winning submission is
DFKI?s logRegFss33 entry, for both Kendall?s ?
with ties penalised and ties ignored, with ? = 0.31
(since this submission has no ties, the two met-
rics give the same ? value). A trivial baseline that
proposes random ranks (with ties allowed) has a
Kendall?s ? with ties penalised of -0.12 (as this
metric penalises the system?s ties that were non-
ties in the reference), and a Kendall?s ? with ties
ignored of 0.08. Most of the submissions per-
formed better than this simple baseline. More in-
terestingly perhaps is the comparison between the
best submission and the performance by an ora-
25
System ID DeltaAvg Spearman ?
? CNGL SVRPLS 11.09 0.55
? DCU-SYMC alltypes 10.13 0.59
SHEF FS 9.76 0.57
CNGL SVR 9.88 0.51
DCU-SYMC combine 9.84 0.59
CMU noB 8.98 0.57
SHEF FS-AL 8.85 0.50
Baseline bb17 SVR 8.52 0.46
CMU full 8.23 0.54
LIMSI 8.15 0.44
TCD-CNGL open 6.03 0.33
TCD-CNGL restricted 5.85 0.31
UMAC 2.74 0.11
Oracle HTER 16.38 1.00
Oracle HBLEU 15.74 0.93
Table 12: Official results for the ranking variant of the WMT13 Quality Estimation Task 1.1. The winning submissions are
indicated by a ? (they are significantly better than all other submissions according to bootstrap resampling (10k times) with
95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant
level according to the same test. Oracle results that use human-references are also shown for comparison purposes.
System ID MAE RMSE
? SHEF FS 12.42 15.74
SHEF FS-AL 13.02 17.03
CNGL SVRPLS 13.26 16.82
LIMSI 13.32 17.22
DCU-SYMC combine 13.45 16.64
DCU-SYMC alltypes 13.51 17.14
CMU noB 13.84 17.46
CNGL SVR 13.85 17.28
FBK-UEdin extra 14.38 17.68
FBK-UEdin rand-svr 14.50 17.73
LORIA inctrain 14.79 18.34
Baseline bb17 SVR 14.81 18.22
TCD-CNGL open 14.81 19.00
LORIA inctraincont 14.83 18.17
TCD-CNGL restricted 15.20 19.59
CMU full 15.25 18.97
UMAC 16.97 21.94
Oracle HTER 0.00 0.00
Oracle HBLEU (1-HBLEU) 16.85 19.72
Table 13: Official results for the scoring variant of the WMT13 Quality Estimation Task 1.1. The winning submission is
indicated by a ? (it is significantly better than the other submissions according to bootstrap resampling (10k times) with 95%
confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant level
according to the same test. Oracle results that use human-references are also shown for comparison purposes.
26
German-English System ID Kendall?s ? with ties penalised
? DFKI logRegFss33 0.31
DFKI logRegFss24 0.28
CNGL SVRPLSF1 0.17
CNGL SVRF1 0.17
DCU CCG 0.15
UPC AQE+SEM+LM 0.11
UPC AQE+LeM+ALGPR+LM 0.10
DCU baseline+CCG 0.00
Baseline Random-ranks-with-ties -0.12
UMAC EBLEU-I -0.39
UMAC NB-LPR -0.49
Oracle Human 1.00
Oracle BLEU (margin 0.00) 0.19
Oracle BLEU (margin 0.01) 0.05
Oracle METEOR-ex (margin 0.00) 0.23
Oracle METEOR-ex (margin 0.01) 0.06
Table 14: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric
Kendall?s ? with ties penalised. The winning submissions are indicated by a ?. Oracle results that use human-references are
also shown for comparison purposes.
English-Spanish System ID Kendall?s ? with ties penalised
? CNGL SVRPLSF1 0.15
CNGL SVRF1 0.13
DFKI logRegL2-411 0.09
DFKI logRegL2-431 0.04
UPC QQE+LeM+ALGPR+LM -0.03
UPC AQE+LeM+ALGPR+LM -0.06
CMU BLEUopt -0.11
Baseline Random-ranks-with-ties -0.23
UMAC EBLEU-A -0.27
UMAC EBLEU-I -0.35
CMU cls -0.63
Oracle Human 1.00
Oracle BLEU (margin 0.00) 0.17
Oracle BLEU (margin 0.02) -0.06
Oracle METEOR-ex (margin 0.00) 0.19
Oracle METEOR-ex (margin 0.02) 0.05
Table 15: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric
Kendall?s ? with ties penalised. The winning submissions are indicated by a ?. Oracle results that use human-references are
also shown for comparison purposes.
27
German-English System ID Kendall?s ? with ties ignored Nr. of non-ties / Nr. of decisions
? DFKI logRegFss33 0.31 882/882
DFKI logRegFss24 0.28 882/882
UPC AQE+SEM+LM 0.27 768/882
UPC AQE+LeM+ALGPR+LM 0.24 788/882
DCU CCG 0.18 862/882
CNGL SVRPLSF1 0.17 882/882
CNGL SVRF1 0.17 881/882
Baseline Random-ranks-with-ties 0.08 718/882
DCU baseline+CCG 0.01 874/882
UMAC NB-LPR 0.01 447/882
UMAC EBLEU-I -0.03 558/882
Oracle Human 1.00 882/882
Oracle BLEU (margin 0.00) 0.22 859/882
Oracle BLEU (margin 0.01) 0.27 728/882
Oracle METEOR-ex (margin 0.00) 0.20 869/882
Oracle METEOR-ex (margin 0.01) 0.24 757/882
Table 16: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric
Kendall?s ? with ties ignored. The winning submissions are indicated by a ?. Oracle results that use human-references are also
shown for comparison purposes.
English-Spanish System ID Kendall?s ? with ties ignored Nr. of non-ties / Nr. of decisions
? CMU cls 0.23 192/633
CNGL SVRPLSF1 0.16 632/633
CNGL SVRF1 0.13 631/633
DFKI logRegL2-411 0.13 610/633
UPC QQE+LeM+ALGPR+LM 0.11 554/633
UPC AQE+LeM+ALGPR+LM 0.08 554/633
UMAC EBLEU-A 0.07 430/633
DFKI logRegL2-431 0.04 633/633
Baseline Random-ranks-with-ties 0.03 507/633
UMAC EBLEU-I 0.02 407/633
CMU BLEUopt -0.11 633/633
Oracle Human 1.00 633/633
Oracle BLEU (margin 0.00) 0.19 621/633
Oracle BLEU (margin 0.02) 0.26 474/633
Oracle METEOR-ex (margin 0.00) 0.25 623/633
Oracle METEOR-ex (margin 0.02) 0.28 517/633
Table 17: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric
Kendall?s ? with ties ignored. The winning submissions are indicated by a ?. Oracle results that use human-references are also
shown for comparison purposes.
28
cle method that has access to human-created refer-
ences. This oracle uses human references to com-
pute BLEU and METEOR scores for each trans-
lation segment, and consequently computes rank-
ings for the competing translations based on these
scores. To reflect the impact of ties on the two
versions of Kendall?s ? metric we use, we allow
these ranks to be tied if the difference between the
oracle BLEU or METEOR scores is smaller than
a margin (see lower section of Tables 14 and 16,
with margins of 0 and 0.01 for the scores). For ex-
ample, under a regime of BLEU with margin 0.01,
a translation with BLEU score of 0.172 would get
the same rank as a translation with BLEU score of
0.164 (difference of 0.008), but a higher rank than
a translation with BLEU score of 0.158 (difference
of 0.014). Not surprisingly, under the Kendall?s
? with ties penalised the best Oracle BLEU or
METEOR performance happens for a 0.0 mar-
gin (which makes ties possible only for exactly-
matching scores), for a value of ? = 0.19 and
? = 0.23, respectively. Under the Kendall?s ? with
ties ignored, the Oracle BLEU performance for a
0.01 margin (i.e, translations under 1 BLEU point
should be considered as having the same rank)
achieves ? = 0.27, while Oracle METEOR for a
0.01 margin achieves ? = 0.24. These values are
lower than the ? = 0.31 of the winning submis-
sion without access to reference translations, sug-
gesting that quality estimation models are capable
of better modelling translation differences com-
pared to traditional, human reference-based MT
evaluation metrics.
For English-Spanish, under Kendall?s ? with
ties penalised the winning submission is CNGL?s
SVRPLSF1, with ? = 0.15. Under Kendall?s ?
with ties ignored, the best scoring submission is
CMU?s cls with ? = 0.23, but this is achieved
by offering non-tie judgements only for 192 of the
633 total judgements (30% of them). As we dis-
cussed in Section 6.6, the ?Kendall?s ? with ties
ignored? metric is weak with respect to compar-
ing different submissions, since it favours systems
that are do not commit to a given rank and rather
produce a large number of ties. This becomes even
clearer when we look at the performance of the or-
acle methods (Tables 15 and 17). Under Kendall?s
? with ties penalised, ?Oracle BLEU? (margin
0.00) achieves ? = 0.17, while under Kendall?s
? with ties ignored, ?Oracle BLEU? (margin 0.02)
has a ? = 0.26. This results in 474 non-tie deci-
sions (75% of them), and a better ? value com-
pared to ?Oracle BLEU? (margin 0.00), with a
? = 0.19 under the same metric. The oracle values
for both BLEU and METEOR are close to the ?
values of the winning submissions, supporting the
conclusion that quality estimation techniques can
successfully replace traditional, human reference-
based MT evaluation metrics.
Task 1.3 Predicting post-editing time
Results for this task are presented in Table 18.
A third of the submissions was able to beat the
baseline. Among these FBK-UEDIN?s submission
ranked best in terms of MAE, our main metric for
this task, and also achieved the lowest RMSE.
Only three systems were able to beat our base-
line in terms of MAE. Please note that while all
features were available to the participants, our
baseline is actually a competitive system.
The second-best entry, CNGL SVR, reached
the highest Spearman?s rank correlation, our sec-
ondary metric. Furthermore, in terms of this met-
ric all four top-ranking entries, two by CNGL and
FBK-UEDIN respectively, are significantly better
than the baseline (10k bootstrap resampling test
with 95% confidence intervals). As high ranking
submissions also yield strong rank correlation to
the observed post-editing time, we can be confi-
dent that improvements in MAE are not only due
to better handling of extreme cases.
Many participants submitted two variants of
their systems with different numbers of features
and/or machine learning approaches. In Table 18
we can see these are grouped closely together giv-
ing rise to the assumption that the general pool of
available features and thereby the used resources
and strongest features are most relevant for a sys-
tem?s performance. Another hint in that direction
is the observation the top-ranked systems rely on
additional data and resources to generate their fea-
tures.
Task 2 Predicting word-level scores
Results for this task are presented in Table 19 and
20, sorted by macro average F1. Since this is a
new task, we have yet to establish a strong base-
line. For reference we provide a trivial baseline
that predicts the dominant class ? (K)eep ? for ev-
ery token.
The first observation in Table 19 is that this triv-
ial baseline is difficult to beat in terms of accuracy.
However, considering our main metric ? macro-
29
System ID MAE RMSE Pearson?s r Spearman?s ?
? FBK-UEDIN Extra 47.5 82.6 0.65 0.75
? FBK-UEDIN Rand-SVR 47.9 86.7 0.66 0.74
CNGL SVR 49.2 90.4 0.67 0.76
CNGL SVRPLS 49.6 86.6 0.68 0.74
CMU slim 51.6 84.7 0.63 0.68
Baseline bb17 SVR 51.9 93.4 0.61 0.70
DFKI linear6 52.4 84.3 0.64 0.68
CMU full 53.6 92.2 0.58 0.60
DFKI pls8 53.6 88.3 0.59 0.67
TCD-DCU-CNGL SVM2 55.8 98.9 0.47 0.60
TCD-DCU-CNGL SVM1 55.9 99.4 0.48 0.60
SHEF FS 55.9 103.1 0.42 0.61
SHEF FS-AL 64.6 99.1 0.57 0.60
LIMSI elastic 70.6 114.4 0.58 0.64
Table 18: Official results for the Task 1.3 of the WMT13 Quality Estimation shared-task. The winning submissions are
indicated by a ? (they are significantly better than all other submissions according to bootstrap resampling (10k times) with
95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant
level according to the same test.
Keep Change
System ID Accuracy Prec. Recall F1 Prec. Recall F1 Macro F1
? LIG FS BIN 0.74 0.79 0.86 0.82 0.56 0.43 0.48 0.65
? LIG BOOST BIN 0.74 0.78 0.88 0.83 0.57 0.37 0.45 0.64
CNGL GLM 0.70 0.76 0.86 0.80 0.47 0.31 0.38 0.59
UMAC NB 0.56 0.82 0.49 0.62 0.37 0.73 0.49 0.55
CNGL GLMd 0.71 0.74 0.93 0.82 0.51 0.19 0.28 0.55
UMAC CRF 0.71 0.72 0.98 0.83 0.49 0.04 0.07 0.45
Baseline (one class) 0.71 0.71 1.00 0.83 0.00 0.00 0.00 0.42
Table 19: Official results for Task 2: binary classification on word level of the WMT13 Quality Estimation shared-task. The
winning submissions are indicated by a ?.
System ID F1 Keep F1 Substitute F1 Delete Micro-F1 Macro-F1
? LIG FS MULT 0.83 0.44 0.072 0.72 0.45
? LIG ALL MULT 0.83 0.45 0.064 0.72 0.45
UMAC NB 0.62 0.43 0.042 0.52 0.36
CNGL GLM 0.83 0.18 0.028 0.71 0.35
CNGL GLMd 0.83 0.14 0.034 0.72 0.34
UMAC CRF 0.83 0.04 0.012 0.71 0.29
Baseline (one class) 0.83 0.00 0.000 0.71 0.28
Table 20: Official results for Task 2: multiclass classification on word level of the WMT13 Quality Estimation shared-task.
The winning submissions are indicated by a ?.
30
average F1 ? it is clear that all systems outperform
the baseline. The winning systems by LIG for the
binary task are also the top ranking systems on the
multiclass task.
While promising results are found for the bi-
nary variant of the task where systems are able to
achieve an F1 of almost 0.5 for the relevant class
? Change, the multiclass prediction variant of the
task seem to suffer from its severe class imbalance.
In fact, none of the systems shows good perfor-
mance when predicting deletions.
6.9 Discussion
In what follows, we discuss the main accomplish-
ments of this shared task starting from the goals
we had previously identified for it.
Explore various granularity levels for the
quality-prediction task The decision on which
level of granularity quality estimation is applied
depends strongly on the intended application. In
Task 2 we tested binary word-level classification
in a post-editing setting. If such annotation is pre-
sented through a user interface we imagine that
words marked as incorrect would be hidden from
the editor, highlighted as possibly wrong or that a
list of alternatives would we generated.
With respect to the poor improvements over
trivial baselines, we consider that the results for
word-level prediction could be mostly connected
to limitations of the datasets provided, which are
very small for word-level prediction, as compared
to successful previous work such as (Bach et al,
2011). Despite the limited amount of training
data, several systems were able to predict dubious
words (binary variant of the task), showing that
this can be a promising task. Extending the granu-
larity even further by predicting the actual editing
action necessary for a word yielded less positive
results than the binary setting.
We cannot directly compare sentence- and
word-level results. However, since sentence-level
predictions can benefit from more information
available and therefore more signal on which the
prediction is based, the natural conclusion is that,
if there is a choice in the prediction granularity,
to opt for the coarser one possible (i.e., sentence-
level over word-level). But certain applications
may require finer granularity levels, and therefore
word-level predictions can still be very valuable.
Explore the prediction of more objective scores
Given the multitude of possible applications for
quality estimation we must decide which predicted
values are both useful and accurate. In this year?s
task we have attempted to address the useful-
ness criterion by moving from the subjective, hu-
man judgement-based scores, to the prediction of
scores that can be more easily interpreted for prac-
tical applications: post-editing distance or types of
edits (word-level), post-editing time, and ranking
of alternative translations.
The general promise of using objective scores is
that predicting a value that is related to the use case
will make quality estimation more applicable and
yield lower deviance compared to the use of proxy
metrics. The magnitude of this benefit should be
sufficient to account for the possible additional ef-
fort related to collecting such scores.
While a direct comparison between the differ-
ent types of scores used for this year?s tasks is not
possible as they are based on different datasets, if
we compare last year?s task on predicting 1-5 lik-
ert scores (and generating an overall ranking of all
translations in the test set) with this year?s Task
1.1, which is virtually the same, but using post-
editing distance as gold-label, we see that the num-
ber of systems that outperform the baseline 18 is
proportionally larger this year. We can also notice
a higher relative improvement of these submis-
sions over the baseline system. While this could
simply be a consequence of progress in the field, it
may also provide an indication that objective met-
rics are more suitable for the problem.
Particularly with respect to post-editing time,
given that this label has a long tailed distribution
and is not trivial to measure even in a controlled
environment, the results of Task 1.3 are encour-
aging. Comparison with the better results seen
on Tasks 1.1 and 1.2, however, suggests that, for
Task 1.3, additional data processing, filtering, and
modelling (including modelling translator-specific
traits such as their variance in time) is required, as
evidenced in (Cohn and Specia, 2013).
Explore the use of quality estimation tech-
niques to replace reference-based MT evalua-
tion metrics When it comes to the task of au-
tomatically ranking alternative translations gener-
ated by different MT systems, the traditional use
of reference-based MT evaluation metrics is chal-
lenged by the findings of this task.
The top ranking quality estimation submissions
18The two baselines are exactly the same, and therefore the
comparison is meaningful.
31
to Task 1.2 have performances that outperform or
are at least at the same level with the ones that
involve the use of human references. The most in-
teresting property of these techniques is that, be-
ing reference-free, they can be used for any source
sentences, and therefore are ready to be deployed
for arbitrary texts.
An immediate application for this capability is
a procedure by which MT system-selection is per-
formed, based on the output of such quality esti-
mators. Additional measurements are needed to
determine the level of improvement in translation
quality that the current performance of these tech-
niques can achieve in a system-selection scenario.
Identify new and effective quality indicators
Quality indicators, or features, are core to the
problem of quality estimation. One significant dif-
ference this year with respect to previous year was
the availability of QUEST, a framework for the ex-
traction of a large number of features. A few sub-
missions used these larger sets ? as opposed to the
17 baseline features used in the 2012 edition ? as
their starting point, to which they added other fea-
tures. Most features available in this framework,
however, had already been used in previous work.
Novel families of features used this year which
seems to have played an important role are those
proposed by CNGL. They include a number of
language and MT-system independent monolin-
gual and bilingual similarity metrics between the
sentences for prediction and corpora of the lan-
guage pair under consideration. Based on standard
regression algorithm (the same used by the base-
line system), the submissions from CNGL using
such feature families topped many of the tasks.
Another interesting family of features is that
used by TCD-CNGL and TCD-DCU-CNGL for
Tasks 1.1 and 1.3. These were borrowed from
work on style or authorship identification. The as-
sumption is that low/high quality translations can
be characterised by some patterns which are fre-
quent and/or differ significantly from patterns be-
longing to the opposite category.
Like in last year?s task, the vast majority of
the participating systems used external resources
in addition to those provided for the task, par-
ticularly for linguistically-oriented features, such
as parsers, part-of-speech taggers, named entity
recognizers, etc. A novel set of syntactic fea-
tures based on Combinatory Categorial Grammar
(CCG) performed reasonably well in Task 1.2:
with six CCG-based features and no additional
features, the system outperformed the baseline
system and also a second submission where the
17 baseline features were added. This highlights
the potential of linguistically-motivated features
for the problem.
As expected, different feature sets were used
for different tasks. This is essential for Task 2,
where word-level features are certainly necessary.
For example, LIG used a number of lexical fea-
tures such as part-of-speech tag, word-posterior
probabilities, syntactic (constituent label, distance
to the constituent tree root, and target and source
polysemy count). For submissions where a se-
quence labelling algorithm such as a Conditional
Random Fields was used for prediction, the inter-
dependencies between adjacent words and labels
was also modelled though features.
Pseudo-references, i.e., scores from standard
evaluation metrics such as BLEU based on trans-
lations generated by an alternative MT system as
?reference?, featured in more than half of the sub-
missions for sentence-level tasks. This is not sur-
prising given their performance in previous work
on quality estimation.
Identify effective machine learning techniques
for all variants of the quality estimation task
For the sentence-level tasks, standard regression
methods such as SVR performed well as in the
previous edition of the shared task, topping the
results for the ranking variant of Task 1.1, both
first and second place. In fact this algorithm was
used by most submissions that outperformed the
baseline. An alternative algorithm to SVR with
very promising results and which was introduced
for the problem this year is that of Gaussian Pro-
cesses. It was used by SHEF, the winning submis-
sion in the scoring variant of Task 1.1, which also
performed well in the ranking variant, despite its
hyperparameters having been optimised for scor-
ing only. Algorithms behave similarly for Task
1.3, with SVR performing particularly well.
For Task 1.2, logistic regression performed the
best or among the best, along with SVR. One of
the most effective approach for this task, however,
appears to be one that is better tailored for the
task, namely pair-wise decomposition for ranking.
This approach benefits from transforming a k-way
ranking problem into a series of simpler, 2-way
ranking problems, which can be more accurately
solved. Another approach that shows promise is
32
that of ensemble of regressors, in which the output
is the results combining the predictions of differ-
ent regression models.
Linear-chain Conditional Random Fields are a
popular model of choice for sequence labelling
tasks and have been successfully used by several
participants in Task 2, along with discriminatively
trained Hidden Markov Models and Na??ve Bayes.
As in the previous edition, feature engineer-
ing and feature selection prior to model learning
were important components in many submissions.
However, the role of individual features is hard
to judge separately from the role of the machine
learning techniques employed.
Establish the state of the art performance All
four tasks addressed in this shared task have
achieved a dual role that is important for the re-
search community: (i) to make publicly available
new data sets that can serve to compare different
approaches and contributions; and (ii) to estab-
lish the present state-of-the-art performance in the
field, so that progress can be easily measured and
tracked. In addition, the public availability of the
scoring scripts makes evaluation and direct com-
parison straightforward.
Many participants submitted predictions for
several tasks. Comparison of the results shows
that there is little overlap between the best sys-
tems when the predicted value is varied. While
we did not formally require the participants to use
similar systems across tasks, these results indicate
that specialised systems with features selected de-
pending on the predicted variable can in fact be
beneficial.
As we mentioned before, compared to the pre-
vious edition of the task, we noticed (for Task
1.1) a larger relative improvement of scores over
the baseline system, as well as a larger propor-
tion of systems outperforming the baseline sys-
tems, which are a good indication that the field is
progressing over the years. For example, in the
scoring variant of Task 1.1, last year only 5 out of
20 systems (i.e. 25% of the systems) were able to
significantly outperform the baseline. This year, 9
out 16 systems (i.e. 56%) outperformed the same
baseline. Last year, the relative improvement of
the winning submission with respect to the base-
line system was 13%, while this year the relative
improvement is of 19%.
Overall, the tables of results presented in Sec-
tion 6.8 give a comprehensive view of the current
state-of-the-art on the data sets used for this shared
task, as well as indications on how much room
there still is for improvement via figures from ora-
cle methods. As a result, people interested in con-
tributing to research in these machine translation
quality estimation tasks will be able to do so in a
principled way, with clearly established state-of-
the-art levels and straightforward means of com-
parison.
7 Summary
As in previous incarnations of this workshop we
carried out an extensive manual and automatic
evaluation of machine translation performance,
and we used the human judgements that we col-
lected to validate automatic metrics of translation
quality. We also refined last year?s quality estima-
tion task, asking for methods that predict sentence-
level post-editing effort and time, rank translations
from alternative systems, and pinpoint words in
the output that are more likely to be wrong.
As in previous years, all data sets generated by
this workshop, including the human judgments,
system translations and automatic scores, are pub-
licly available for other researchers to analyze.19
Acknowledgments
This work was supported in parts by the
MosesCore, Casmacat, Khresmoi, Matecat and
QTLaunchPad projects funded by the European
Commission (7th Framework Programme), and by
gifts from Google, Microsoft and Yandex.
We would also like to thank our colleagues Ma-
tous? Macha?c?ek and Martin Popel for detailed dis-
cussions.
References
Allauzen, A., Pe?cheux, N., Do, Q. K., Dinarelli,
M., Lavergne, T., Max, A., Le, H.-S., and Yvon,
F. (2013). LIMSI @ WMT13. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 60?67, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Almaghout, H. and Specia, L. (2013). A CCG-
based Quality Estimation Metric for Statistical
Machine Translation. In Proceedings of MT
Summit XIV (to appear), Nice, France.
Ammar, W., Chahuneau, V., Denkowski, M., Han-
neman, G., Ling, W., Matthews, A., Murray,
19http://statmt.org/wmt13/results.html
33
K., Segall, N., Lavie, A., and Dyer, C. (2013).
The CMU machine translation systems at WMT
2013: Syntax, synthetic translation options, and
pseudo-references. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 68?75, Sofia, Bulgaria. Association for
Computational Linguistics.
Avramidis, E. (2012). Comparative Quality Es-
timation: Automatic Sentence-Level Ranking
of Multiple Machine Translation Outputs. In
Proceedings of 24th International Conference
on Computational Linguistics, pages 115?132,
Mumbai, India.
Avramidis, E. and Popovic, M. (2013). Selecting
feature sets for comparative and time-oriented
quality estimation of machine translation out-
put. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 327?
334, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Avramidis, E., Popovic?, M., Vilar, D., and Bur-
chardt, A. (2011). Evaluate with confidence es-
timation: Machine ranking of translation out-
puts using grammatical features. In Proceed-
ings of the Sixth Workshop on Statistical Ma-
chine Translation.
Aziz, W., Mitkov, R., and Specia, L. (2013).
Ranking Machine Translation Systems via Post-
Editing. In Proc. of Text, Speech and Dialogue
(TSD), Lecture Notes in Artificial Intelligence,
Berlin / Heidelberg. Za?padoc?eska? univerzita v
Plzni, Springer Verlag.
Bach, N., Huang, F., and Al-Onaizan, Y. (2011).
Goodness: A method for measuring machine
translation confidence. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 211?219, Portland, Ore-
gon, USA.
Beck, D., Shah, K., Cohn, T., and Specia, L.
(2013). SHEF-Lite: When less is more for
translation quality estimation. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 335?340, Sofia, Bulgaria.
Association for Computational Linguistics.
Bic?ici, E., Groves, D., and van Genabith, J. (2013).
Predicting sentence translation quality using ex-
trinsic and language independent features. Ma-
chine Translation.
Bic?ici, E. and van Genabith, J. (2013). CNGL-
CORE: Referential translation machines for
measuring semantic similarity. In *SEM 2013:
The Second Joint Conference on Lexical and
Computational Semantics, Atlanta, Georgia,
USA. Association for Computational Linguis-
tics.
Bicici, E. (2013a). Feature decay algorithms
for fast deployment of accurate statistical ma-
chine translation systems. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 76?82, Sofia, Bulgaria. Associa-
tion for Computational Linguistics.
Bicici, E. (2013b). Referential translation ma-
chines for quality estimation. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 341?349, Sofia, Bulgaria.
Association for Computational Linguistics.
B??lek, K. and Zeman, D. (2013). CUni multilin-
gual matrix in the WMT 2013 shared task. In
Proceedings of the Eighth Workshop on Statis-
tical Machine Translation, pages 83?89, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Bojar, O., Kos, K., and Marec?ek, D. (2010). Tack-
ling Sparse Data Issue in Machine Translation
Evaluation. In Proceedings of the ACL 2010
Conference Short Papers, pages 86?91, Upp-
sala, Sweden. Association for Computational
Linguistics.
Bojar, O., Rosa, R., and Tamchyna, A. (2013).
Chimera ? three heads for English-to-Czech
translation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
90?96, Sofia, Bulgaria. Association for Compu-
tational Linguistics.
Borisov, A., Dlougach, J., and Galinskaya, I.
(2013). Yandex school of data analysis ma-
chine translation systems for WMT13. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation, pages 97?101, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2007). (Meta-) evaluation
of machine translation. In Proceedings of the
Second Workshop on Statistical Machine Trans-
lation (WMT07), Prague, Czech Republic.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2008). Further meta-
34
evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Ma-
chine Translation (WMT08), Colmbus, Ohio.
Callison-Burch, C., Koehn, P., Monz, C., Pe-
terson, K., Przybocki, M., and Zaidan, O. F.
(2010). Findings of the 2010 joint workshop
on statistical machine translation and metrics
for machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation (WMT10), Uppsala, Sweden.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10?
51, Montre?al, Canada. Association for Compu-
tational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Schroeder, J. (2009). Findings of the 2009
workshop on statistical machine translation. In
Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT09), Athens,
Greece.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64, Edinburgh,
Scotland.
Camargo de Souza, J. G., Buck, C., Turchi, M.,
and Negri, M. (2013). FBK-UEdin participa-
tion to the WMT13 quality estimation shared
task. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 350?
356, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Charniak, E. and Johnson, M. (2005). Coarse-to-
fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual
Meeting on Association for Computational Lin-
guistics, pages 173?180. Association for Com-
putational Linguistics.
Cho, E., Ha, T.-L., Mediani, M., Niehues, J., Her-
rmann, T., Slawik, I., and Waibel, A. (2013).
The Karlsruhe Institute of Technology transla-
tion systems for the WMT 2013. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 102?106, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Cohen, J. (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurment, 20(1):37?46.
Cohn, T. and Specia, L. (2013). Modelling Anno-
tator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality
Estimation. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (to appear).
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the ACL-02 conference on Empir-
ical methods in natural language processing -
Volume 10, EMNLP ?02, pages 1?8, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
de Marneffe, M.-C., MacCartney, B., and Man-
ning, C. D. (2006). Generating typed depen-
dency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
Denkowski, M. and Lavie, A. (2010). Meteor-next
and the meteor paraphrase tables: improved
evaluation support for five target languages. In
Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR,
WMT ?10, pages 339?342, Stroudsburg, PA,
USA. Association for Computational Linguis-
tics.
Durgar El-Kahlout, I. and Mermer, C. (2013).
TU?btak-blgem german-english machine trans-
lation systems for w13. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 107?111, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Durrani, N., Fraser, A., Schmid, H., Sajjad, H.,
and Farkas, R. (2013a). Munich-Edinburgh-
Stuttgart submissions of OSM systems at
WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
120?125, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Durrani, N., Haddow, B., Heafield, K., and Koehn,
P. (2013b). Edinburgh?s machine translation
systems for European language pairs. In Pro-
ceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 112?119, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
35
Eidelman, V., Wu, K., Ture, F., Resnik, P., and Lin,
J. (2013). Towards efficient large-scale feature-
rich statistical machine translation. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 126?131, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Federmann, C. (2012). Appraise: An Open-
Source Toolkit for Manual Evaluation of Ma-
chine Translation Output. The Prague Bulletin
of Mathematical Linguistics (PBML), 98:25?
35.
Fleiss, J. L. (1971). Measuring nominal scale
agreement among many raters. Psychological
Bulletin, 76(5):378?382.
Formiga, L., Costa-jussa`, M. R., Marin?o, J. B.,
Fonollosa, J. A. R., Barro?n-Ceden?o, A., and
Marquez, L. (2013a). The TALP-UPC phrase-
based translation systems for WMT13: System
combination with morphology generation, do-
main adaptation and corpus filtering. In Pro-
ceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 132?138, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Formiga, L., Gonza`lez, M., Barro?n-Ceden?o, A.,
Fonollosa, J. A. R., and Marquez, L. (2013b).
The TALP-UPC approach to system selection:
Asiya features and pairwise classification using
random forests. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 357?362, Sofia, Bulgaria. Association for
Computational Linguistics.
Formiga, L., Ma`rquez, L., and Pujantell, J.
(2013c). Real-life translation quality estimation
for mt system selection. In Proceedings of MT
Summit XIV (to appear), Nice, France.
Galus?c?a?kova?, P., Popel, M., and Bojar, O. (2013).
PhraseFix: Statistical post-editing of TectoMT.
In Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, pages 139?145,
Sofia, Bulgaria. Association for Computational
Linguistics.
Gispert, A., Blackwood, G., Iglesias, G., and
Byrne, W. (2013). N-gram posterior probabil-
ity confidence measures for statistical machine
translation: an empirical study. Machine Trans-
lation, 27:85?114.
Gonza`lez, M., Gime?nez, J., and Ma`rquez, L.
(2012). A graphical interface for mt evaluation
and error analysis. In Proceedings of the ACL
2012 System Demonstrations, pages 139?144,
Jeju Island, Korea.
Green, S., Cer, D., Reschke, K., Voigt, R., Bauer,
J., Wang, S., Silveira, N., Neidert, J., and Man-
ning, C. D. (2013). Feature-rich phrase-based
translation: Stanford University?s submission to
the WMT 2013 translation task. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 146?151, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Hall, M., Frank, E., Holmes, G., Pfahringer,
B., Reutemann, P., and Witten, I. H. (2009).
The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Han, A. L.-F., Wong, D. F., Chao, L. S., Lu, Y., He,
L., Wang, Y., and Zhou, J. (2013). A descrip-
tion of tunable machine translation evaluation
systems in WMT13 metrics task. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 412?419, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Hildebrand, S. and Vogel, S. (2013). MT quality
estimation: The CMU system for WMT?13. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 371?377, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Hosmer, D. (1989). Applied logistic regression.
Wiley, New York, 8th edition.
Huet, S., Manishina, E., and Lefe`vre, F.
(2013). Factored machine translation systems
for Russian-English. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 152?155, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Hunt, E., Martin, J., and Stone, P. (1966). Experi-
ments in Induction. Academic Press, New York.
Kaplan, R., Riezler, S., King, T., Maxwell, J.,
Vasserman, A., and Crouch, R. (2004). Speed
and accuracy in shallow and deep stochastic
parsing. In Proceedings of the Human Lan-
guage Technology Conference and the 4th An-
nual Meeting of the North American Chapter of
the Association for Computational Linguistics
(HLT/NAACL 04).
36
Kauchak, D. and Barzilay, R. (2006). Paraphras-
ing for automatic evaluation. In Proceedings
of the main conference on Human Language
Technology Conference of the North American
Chapter of the Association of Computational
Linguistics, HLT-NAACL ?06, pages 455?462,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Koehn, P. (2012). Simulating human judgment in
machine translation evaluation campaigns. In
International Workshop on Spoken Language
Translation (IWSLT).
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween European languages. In Proceedings of
NAACL 2006 Workshop on Statistical Machine
Translation, New York, New York.
Kononenko, I. (1994). Estimating attributes: anal-
ysis and extensions of RELIEF. In Proceedings
of the European conference on machine learn-
ing on Machine Learning, pages 171?182, Se-
caucus, NJ, USA. Springer-Verlag New York,
Inc.
Kos, K. and Bojar, O. (2009). Evaluation of Ma-
chine Translation Metrics for Czech as the Tar-
get Language. Prague Bulletin of Mathematical
Linguistics, 92:135?147.
Landis, J. R. and Koch, G. G. (1977). The mea-
surement of observer agreement for categorical
data. Biometrics, 33:159?174.
Langlois, D., Raybaud, S., and Sma??li, K. (2012).
Loria system for the wmt12 quality estimation
shared task. In Proceedings of the Seventh
Workshop on Statistical Machine Translation,
pages 114?119, Montre?al, Canada.
Langlois, D. and Smaili, K. (2013). LORIA sys-
tem for the WMT13 quality estimation shared
task. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 378?
383, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Le, H. S., Oparin, I., Allauzen, A., Gauvain, J.-L.,
and Yvon, F. (2011). Structured output layer
neural network language model. In ICASSP,
pages 5524?5527.
Lin, C.-J., Weng, R. C., and Keerthi, S. S. (2007).
Trust region Newton methods for large-scale lo-
gistic regression. In Proceedings of the 24th
international conference on Machine learning
- ICML ?07, pages 561?568, New York, New
York, USA. ACM Press.
Luong, N. Q., Lecouteux, B., and Besacier, L.
(2013). LIG system for WMT13 QE task: In-
vestigating the usefulness of features in word
confidence estimation for MT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 384?389, Sofia, Bulgaria.
Association for Computational Linguistics.
Macha?c?ek, M. and Bojar, O. (2013). Results of the
WMT13 metrics shared task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 43?49, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Matusov, E. and Leusch, G. (2013). Omnifluent
English-to-French and Russian-to-English sys-
tems for the 2013 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 156?161, Sofia, Bulgaria. Association for
Computational Linguistics.
McCallum, A. K. (2002). MALLET: a machine
learning for language toolkit.
Miceli Barone, A. V. and Attardi, G. (2013).
Pre-reordering for machine translation using
transition-based walks on dependency parse
trees. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 162?
167, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Moreau, E. and Rubino, R. (2013). An approach
using style classification features for quality es-
timation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
427?432, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Nadejde, M., Williams, P., and Koehn, P. (2013).
Edinburgh?s syntax-based machine translation
systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
168?174, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Okita, T., Liu, Q., and van Genabith, J. (2013).
Shallow semantically-informed PBSMT and
HPBSMT. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
175?182, Sofia, Bulgaria. Association for Com-
putational Linguistics.
O?zgu?r, A., O?zgu?r, L., and Gu?ngo?r, T. (2005). Text
37
categorization with class-based and corpus-
based keyword selection. In Proceedings of
the 20th International Conference on Computer
and Information Sciences, ISCIS?05, pages
606?615, Berlin, Heidelberg. Springer.
Peitz, S., Mansour, S., Huck, M., Freitag, M.,
Ney, H., Cho, E., Herrmann, T., Mediani,
M., Niehues, J., Waibel, A., Allauzen, A.,
Khanh Do, Q., Buschbeck, B., and Wand-
macher, T. (2013a). Joint WMT 2013 submis-
sion of the QUAERO project. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 183?190, Sofia, Bulgaria.
Association for Computational Linguistics.
Peitz, S., Mansour, S., Peter, J.-T., Schmidt, C.,
Wuebker, J., Huck, M., Freitag, M., and Ney,
H. (2013b). The RWTH aachen machine trans-
lation system for WMT 2013. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 191?197, Sofia, Bulgaria.
Association for Computational Linguistics.
Pighin, D., Formiga, L., and Ma`rquez, L.
(2012). A graph-based strategy to streamline
translation quality assessments. In Proceed-
ings of the Tenth Conference of the Associa-
tion for Machine Translation in the Americas
(AMTA?2012), San Diego, USA.
Pino, J., Waite, A., Xiao, T., de Gispert, A., Flego,
F., and Byrne, W. (2013). The University of
Cambridge Russian-English system at WMT13.
In Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, pages 198?203,
Sofia, Bulgaria. Association for Computational
Linguistics.
Post, M., Ganitkevitch, J., Orland, L., Weese, J.,
Cao, Y., and Callison-Burch, C. (2013). Joshua
5.0: Sparser, better, faster, server. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 204?210, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Rubino, R., Toral, A., Corte?s Va??llo, S., Xie, J.,
Wu, X., Doherty, S., and Liu, Q. (2013a). The
CNGL-DCU-Prompsit translation systems for
WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
211?216, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Rubino, R., Wagner, J., Foster, J., Roturier, J.,
Samad Zadeh Kaljahi, R., and Hollowood, F.
(2013b). DCU-Symantec at the WMT 2013
quality estimation shared task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 390?395, Sofia, Bulgaria.
Association for Computational Linguistics.
Sajjad, H., Smekalova, S., Durrani, N., Fraser, A.,
and Schmid, H. (2013). QCRI-MES submis-
sion at WMT13: Using transliteration mining
to improve statistical machine translation. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 217?222, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Schmidt, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
the International Conference on New Methods
in Natural Language Processing.
Seginer, Y. (2007). Learning Syntactic Structure.
PhD thesis, University of Amsterdam.
Shah, K., Cohn, T., and Specia, L. (2013). An In-
vestigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings
of MT Summit XIV (to appear), Nice, France.
Singh, A. K., Wisniewski, G., and Yvon, F.
(2013). LIMSI submission for the WMT?13
quality estimation task: an experiment with n-
gram posteriors. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 396?402, Sofia, Bulgaria. Association for
Computational Linguistics.
Smith, J., Saint-Amand, H., Plamada, M., Koehn,
P., Callison-Burch, C., and Lopez, A. (2013).
Dirt cheap web-scale parallel text from the
Common Crawl. In Proceedings of the 2013
Conference of the Association for Computa-
tional Linguistics (ACL 2013), Sofia, Bulgaria.
Association for Computational Linguistics.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference
of the Association for Machine Translation in
the Americas (AMTA-2006), Cambridge, Mas-
sachusetts.
Snover, M., Madnani, N., Dorr, B. J., and
Schwartz, R. (2009). Fluency, adequacy, or
hter?: exploring different human judgments
with a tunable mt metric. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
38
lation, StatMT ?09, pages 259?268, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Soricut, R., Bach, N., and Wang, Z. (2012). The
SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceed-
ings of the 7th Workshop on Statistical Machine
Translation, pages 145?151.
Souza, J. G. C. d., Espl-Gomis, M., Turchi, M.,
and Negri, M. (2013). Exploiting qualitative in-
formation from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual
Meeting of the Association for Computational
Linguistics - Short Papers (ACL Short Papers
2013).
Specia, L. (2011). Exploiting Objective Annota-
tions for Measuring Translation Post-editing Ef-
fort. In Proceedings of the 15th Conference of
the European Association for Machine Transla-
tion, pages 73?80, Leuven.
Specia, L., Shah, K., de Souza, J. G. C., and Cohn,
T. (2013). QuEst - A Translation Quality Esti-
mation Framework. In Proceedings of the 51th
Conference of the Association for Computa-
tional Linguistics (ACL), Demo Session, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Stolcke, A. (2002). SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the
7th International Conference on Spoken Lan-
guage Processing (ICSLP 2002), pages 901?
904.
Stone, M. and Brooks, R. J. (1990). Contin-
uum regression: cross-validated sequentially
constructed prediction embracing ordinary least
squares, partial least squares and principal com-
ponents regression. Journal of the Royal
Statistical Society Series B Methodological,
52(2):237?269.
Stymne, S., Hardmeier, C., Tiedemann, J., and
Nivre, J. (2013). Tunable distortion limits and
corpus cleaning for SMT. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 223?229, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Tantug, A. C., Oflazer, K., and El-Kahlout, I. D.
(2008). BLEU+: a Tool for Fine-Grained BLEU
Computation. In (ELRA), E. L. R. A., edi-
tor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08),
Marrakech, Morocco.
Weller, M., Kisselew, M., Smekalova, S., Fraser,
A., Schmid, H., Durrani, N., Sajjad, H., and
Farkas, R. (2013). Munich-Edinburgh-Stuttgart
submissions at WMT13: Morphological and
syntactic processing for SMT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 230?237, Sofia, Bulgaria.
Association for Computational Linguistics.
39
A Pairwise System Comparisons by Human Judges
Tables 21?30 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row, ignoring ties. Bolding indicates the winner of the two systems.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
Each table contains final rows showing how likely a system would win when paired against a randomly
selected system (the expected win ratio score) and the rank range according bootstrap resampling (p ?
0.05). Gray lines separate clusters based on non-overlapping rank ranges.
UE
DI
N-
HE
AF
IE
LD
ON
LI
NE
-B
M
ES
UE
DI
N
ON
LI
NE
-A
UE
DI
N-
SY
NT
AX
CU
-ZE
M
AN
CU
-TA
M
CH
YN
A
DC
U-
FD
A
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .50 .48? .43? .47? .43? .44? .38? .32? .25? .26?
ONLINE-B .50 ? .46? .48? .47? .49 .44? .40? .39? .29? .27?
MES .52? .54? ? .49 .47? .44? .45? .42? .41? .27? .25?
UEDIN .57? .52? .51 ? .51 .48? .47? .42? .39? .28? .25?
ONLINE-A .53? .53? .53? .49 ? .48 .51 .44? .42? .31? .30?
UEDIN-SYNTAX .57? .51 .56? .52? .52 ? .51 .43? .41? .29? .26?
CU-ZEMAN .56? .56? .55? .53? .49 .49 ? .45? .42? .32? .29?
CU-TAMCHYNA .62? .60? .58? .58? .56? .57? .55? ? .46? .35? .32?
DCU-FDA .68? .61? .59? .61? .58? .59? .58? .54? ? .32? .32?
JHU .75? .71? .73? .72? .69? .71? .68? .65? .68? ? .46?
SHEF-WPROA .74? .73? .75? .75? .70? .74? .71? .68? .68? .54? ?
score .60 .58 .57 .56 .54 .54 .53 .48 .45 .32 .29
rank 1 2-3 2-4 3-5 4-7 5-7 6-7 8 9 10 11
Table 21: Head to head comparison, ignoring ties, for Czech-English systems
CU
-B
OJ
AR
CU
-D
EP
FI
X
ON
LI
NE
-B
UE
DI
N
CU
-ZE
M
AN
M
ES
ON
LI
NE
-A
CU
-PH
RA
SE
FI
X
CU
-TE
CT
OM
T
CO
M
M
ER
CI
AL
-1
CO
M
M
ER
CI
AL
-2
SH
EF
-W
PR
OA
CU-BOJAR ? .51 .47? .44? .42? .43? .48 .41? .37? .39? .38? .33?
CU-DEPFIX .49 ? .48? .42? .43? .41? .47? .42? .40? .40? .39? .34?
ONLINE-B .53? .52? ? .47? .44? .44? .44? .44? .44? .41? .36? .34?
UEDIN .56? .58? .53? ? .47? .47? .48 .45? .44? .42? .43? .38?
CU-ZEMAN .58? .57? .56? .53? ? .49 .49 .48? .46? .47? .47? .35?
MES .57? .59? .56? .53? .51 ? .50 .47? .46? .43? .44? .42?
ONLINE-A .52 .53? .56? .52 .51 .50 ? .52 .47? .47? .47? .46?
CU-PHRASEFIX .59? .58? .56? .55? .52? .53? .48 ? .49 .48? .49 .42?
CU-TECTOMT .63? .60? .56? .56? .54? .54? .53? .51 ? .46? .46? .40?
COMMERCIAL-1 .61? .60? .59? .58? .53? .57? .53? .52? .54? ? .49 .42?
COMMERCIAL-2 .62? .61? .64? .57? .53? .56? .53? .51 .54? .51 ? .43?
SHEF-WPROA .67? .66? .66? .62? .65? .58? .54? .58? .60? .58? .57? ?
score .58 .57 .56 .52 .50 .50 .49 .48 .47 .45 .45 .38
rank 1-2 1-2 3 4 5-7 5-7 5-8 7-9 8-9 10-11 10-11 12
Table 22: Head to head comparison, ignoring ties, for English-Czech systems
40
ON
LI
NE
-B
ON
LI
NE
-A
UE
DI
N-
SY
NT
AX
UE
DI
N
QU
AE
RO
KI
T
M
ES
RW
TH
-JA
NE
M
ES
-SZ
EG
ED
-R
EO
RD
ER
-SP
LI
T
LI
M
SI
-N
CO
DE
-SO
UL
TU
BI
TA
K
UM
D
DC
U
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
DE
SR
T
ONLINE-B ? .48 .44? .37? .44? .41? .42? .40? .35? .37? .32? .31? .31? .27? .23? .18? .16?
ONLINE-A .52 ? .47 .45? .47 .43? .42? .41? .44? .40? .35? .36? .34? .31? .27? .25? .21?
UEDIN-SYNTAX .56? .53 ? .48 .46? .48? .46? .46? .45? .45? .35? .35? .34? .28? .25? .20? .19?
UEDIN .63? .55? .52 ? .51 .46? .47? .49 .44? .43? .39? .34? .35? .32? .28? .24? .22?
QUAERO .56? .53 .54? .49 ? .49 .52 .44? .46? .44? .39? .38? .37? .30? .31? .25? .21?
KIT .59? .57? .52? .54? .51 ? .45? .51 .43? .46? .37? .38? .41? .35? .31? .25? .21?
MES .58? .58? .54? .53? .48 .55? ? .49 .49 .46? .44? .37? .40? .34? .30? .26? .20?
RWTH-JANE .60? .59? .54? .51 .56? .49 .51 ? .46? .50 .45? .46? .47? .38? .33? .28? .20?
MES-SZEGED-REORDER-SPLIT .65? .56? .55? .56? .54? .57? .51 .54? ? .53? .44? .41? .41? .36? .34? .31? .21?
LIMSI-NCODE-SOUL .63? .60? .55? .57? .56? .54? .54? .50 .47? ? .51 .45? .43? .37? .34? .30? .22?
TUBITAK .68? .65? .65? .61? .61? .63? .56? .55? .56? .49 ? .48? .49 .39? .41? .30? .25?
UMD .69? .64? .65? .66? .62? .62? .63? .54? .59? .55? .52? ? .48? .41? .40? .33? .27?
DCU .69? .66? .66? .65? .63? .59? .60? .53? .59? .57? .51 .52? ? .41? .38? .37? .25?
CU-ZEMAN .73? .69? .72? .68? .70? .65? .66? .62? .64? .63? .61? .59? .59? ? .44? .43? .29?
JHU .77? .73? .75? .72? .69? .69? .70? .67? .66? .66? .59? .60? .62? .56? ? .43? .30?
SHEF-WPROA .82? .75? .80? .76? .75? .75? .74? .72? .69? .70? .70? .67? .63? .57? .57? ? .41?
DESRT .84? .79? .81? .78? .79? .79? .80? .80? .79? .78? .75? .73? .75? .71? .70? .59? ?
score .66 .62 .60 .58 .58 .57 .56 .54 .53 .52 .48 .46 .46 .39 .36 .31 .23
rank 1 2-3 2-3 4-5 4-5 5-7 6-7 8-9 8-10 9-10 11 12-13 12-13 14 15 16 17
Table 23: Head to head comparison, ignoring ties, for German-English systems
ON
LI
NE
-B
PR
OM
T
UE
DI
N-
SY
NT
AX
ON
LI
NE
-A
UE
DI
N
KI
T
ST
AN
FO
RD
LI
M
SI
-N
CO
DE
-SO
UL
M
ES
-R
EO
RD
ER
JH
U
CU
-ZE
M
AN
TU
BI
TA
K
UU SH
EF
-W
PR
OA
RW
TH
-JA
NE
ONLINE-B ? .55? .50 .45? .45? .34? .37? .37? .37? .32? .32? .33? .24? .21? .26?
PROMT .45? ? .48? .50 .43? .40? .39? .36? .37? .31? .31? .32? .27? .24? .27?
UEDIN-SYNTAX .50 .52? ? .57? .45? .43? .38? .41? .39? .38? .33? .33? .26? .25? .22?
ONLINE-A .55? .50 .43? ? .51 .42? .48 .41? .36? .44? .44? .38? .32? .27? .29?
UEDIN .55? .57? .55? .49 ? .52 .45? .45? .42? .43? .37? .34? .29? .27? .31?
KIT .66? .60? .57? .58? .48 ? .48 .45? .42? .36? .39? .40? .30? .29? .26?
STANFORD .63? .61? .62? .52 .55? .52 ? .50 .44? .48 .44? .43? .34? .29? .32?
LIMSI-NCODE-SOUL .63? .64? .59? .59? .55? .55? .50 ? .44? .44? .44? .47? .40? .34? .33?
MES-REORDER .63? .63? .61? .64? .58? .58? .56? .56? ? .50 .46? .49 .38? .37? .34?
JHU .68? .69? .62? .56? .57? .64? .52 .56? .50 ? .48? .45? .36? .37? .34?
CU-ZEMAN .68? .69? .67? .56? .63? .61? .56? .56? .54? .52? ? .48 .40? .33? .34?
TUBITAK .67? .68? .67? .62? .66? .60? .57? .53? .51 .55? .52 ? .38? .40? .32?
UU .76? .73? .74? .68? .71? .70? .66? .60? .62? .64? .60? .62? ? .44? .46?
SHEF-WPROA .79? .76? .75? .73? .73? .71? .71? .66? .63? .63? .67? .60? .56? ? .47?
RWTH-JANE .74? .73? .78? .71? .69? .74? .68? .67? .66? .66? .66? .68? .54? .53? ?
score .63 .63 .61 .58 .57 .55 .52 .50 .47 .47 .46 .45 .36 .32 .32
rank 1-2 1-2 3 3-5 4-6 5-6 7 8 9-11 9-11 10-12 11-12 13 14-15 14-15
Table 24: Head to head comparison, ignoring ties, for English-German systems
41
UE
DI
N-
HE
AF
IE
LD
UE
DI
N
ON
LI
NE
-B
LI
M
SI
-N
CO
DE
-SO
UL
KI
T
ON
LI
NE
-A
M
ES
-SI
M
PL
IF
IE
DF
RE
NC
H
DC
U
RW
TH
CM
U-
TR
EE
-TO
-TR
EE
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .45? .46? .46? .42? .42? .34? .34? .29? .33? .31? .28? .24?
UEDIN .55? ? .52? .43? .45? .46? .40? .38? .33? .36? .33? .32? .23?
ONLINE-B .54? .48? ? .49 .46? .44? .45? .40? .38? .34? .36? .31? .26?
LIMSI-NCODE-SOUL .54? .57? .51 ? .52? .47 .45? .42? .38? .36? .34? .31? .28?
KIT .58? .55? .54? .48? ? .47 .46? .44? .39? .38? .37? .33? .28?
ONLINE-A .58? .54? .56? .53 .53 ? .47 .45? .40? .40? .39? .34? .32?
MES-SIMPLIFIEDFRENCH .66? .60? .55? .55? .54? .53 ? .48? .44? .40? .39? .39? .32?
DCU .66? .62? .60? .58? .56? .55? .52? ? .45? .45? .42? .41? .36?
RWTH .71? .67? .62? .62? .61? .60? .56? .55? ? .48? .47? .47? .38?
CMU-TREE-TO-TREE .67? .64? .66? .64? .62? .60? .60? .55? .52? ? .50 .48 .37?
CU-ZEMAN .69? .67? .64? .66? .63? .61? .61? .58? .53? .50 ? .47? .39?
JHU .72? .68? .69? .69? .67? .66? .61? .59? .53? .52 .53? ? .45?
SHEF-WPROA .76? .77? .74? .72? .72? .68? .68? .64? .62? .63? .61? .55? ?
score .63 .60 .59 .57 .56 .54 .51 .48 .43 .42 .42 .38 .32
rank 1 2-3 2-3 4-5 4-5 5-6 7 8 9-10 9-11 10-11 12 13
Table 25: Head to head comparison, ignoring ties, for French-English systems
UE
DI
N
ON
LI
NE
-B
LI
M
SI
-N
CO
DE
-SO
UL
KI
T
PR
OM
T
ST
AN
FO
RD
M
ES
M
ES
-IN
FL
EC
TI
ON
RW
TH
-PH
RA
SE
-B
AS
ED
-JA
NE
ON
LI
NE
-A
DC
U
CU
-ZE
M
AN
JH
U
OM
NI
FL
UE
NT
IT
S-L
AT
L
IT
S-L
AT
L-P
E
UEDIN ? .49 .47? .48 .50 .44? .41? .40? .47? .39? .41? .35? .29? .30? .27? .24?
ONLINE-B .51 ? .46? .47? .47? .44? .49 .43? .43? .43? .38? .35? .36? .28? .25? .25?
LIMSI-NCODE-SOUL .53? .54? ? .45? .48 .48 .45? .43? .44? .45? .41? .32? .34? .30? .27? .27?
KIT .52 .53? .55? ? .48 .46? .45? .43? .45? .46? .38? .30? .33? .31? .29? .29?
PROMT .50 .53? .52 .52 ? .50 .48 .52? .45? .47 .48? .38? .36? .36? .34? .31?
STANFORD .56? .56? .52 .54? .50 ? .52 .48 .44? .49 .44? .39? .34? .36? .30? .29?
MES .59? .51 .55? .55? .52 .48 ? .52 .51 .45? .45? .36? .37? .34? .29? .29?
MES-INFLECTION .60? .57? .57? .57? .48? .52 .48 ? .54? .51 .46? .37? .35? .31? .33? .31?
RWTH-PHRASE-BASED-JANE .53? .57? .56? .55? .55? .56? .49 .46? ? .53 .49 .38? .36? .34? .35? .31?
ONLINE-A .61? .57? .55? .54? .53 .51 .55? .49 .47 ? .50 .45? .38? .38? .39? .35?
DCU .59? .62? .59? .62? .52? .56? .55? .54? .51 .50 ? .42? .40? .40? .36? .35?
CU-ZEMAN .65? .65? .68? .70? .62? .61? .64? .63? .62? .55? .58? ? .50 .42? .41? .37?
JHU .71? .64? .66? .67? .64? .66? .63? .65? .64? .62? .60? .50 ? .47? .42? .38?
OMNIFLUENT .70? .72? .70? .69? .64? .64? .66? .69? .66? .62? .60? .58? .53? ? .43? .42?
ITS-LATL .73? .75? .72? .71? .66? .70? .71? .67? .65? .61? .64? .59? .58? .57? ? .45?
ITS-LATL-PE .76? .75? .73? .71? .69? .71? .71? .69? .69? .65? .65? .63? .62? .58? .55? ?
score .60 .60 .58 .58 .55 .55 .54 .53 .53 .51 .49 .42 .40 .38 .35 .32
rank 1-2 1-3 2-4 3-4 5-7 5-8 5-8 6-9 7-10 9-11 10-11 12 13 14 15 16
Table 26: Head to head comparison, ignoring ties, for English-French systems
42
UE
DI
N-
HE
AF
IE
LD
ON
LI
NE
-B
UE
DI
N
ON
LI
NE
-A
M
ES
LI
M
SI
-N
CO
DE
-SO
UL
DC
U
DC
U-
OK
IT
A
DC
U-
FD
A
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .49 .42? .45? .43? .40? .34? .43? .37? .34? .31? .15?
ONLINE-B .51 ? .49 .44? .46? .47? .42? .39? .40? .37? .37? .16?
UEDIN .58? .51 ? .55? .50 .47? .43? .42? .39? .39? .35? .14?
ONLINE-A .55? .56? .45? ? .50 .44? .45? .42? .42? .41? .37? .18?
MES .57? .54? .50 .50 ? .47? .45? .41? .41? .40? .38? .15?
LIMSI-NCODE-SOUL .60? .53? .53? .56? .53? ? .46? .45? .44? .43? .38? .18?
DCU .66? .58? .57? .55? .55? .54? ? .44? .47? .42? .41? .16?
DCU-OKITA .57? .61? .58? .58? .59? .55? .56? ? .49 .46? .46? .18?
DCU-FDA .63? .60? .61? .58? .59? .56? .53? .51 ? .48? .43? .18?
CU-ZEMAN .66? .63? .61? .59? .60? .57? .58? .54? .52? ? .43? .18?
JHU .69? .63? .65? .63? .62? .62? .59? .54? .57? .57? ? .22?
SHEF-WPROA .85? .84? .86? .82? .85? .82? .84? .82? .82? .82? .78? ?
score .62 .59 .57 .57 .56 .53 .51 .48 .48 .46 .42 .16
rank 1 2 3-5 3-5 3-5 6 7 8-9 8-9 10 11 12
Table 27: Head to head comparison, ignoring ties, for Spanish-English systems
ON
LI
NE
-B
ON
LI
NE
-A
UE
DI
N
PR
OM
T
M
ES
TA
LP
-U
PC
LI
M
SI
-N
CO
DE
DC
U
DC
U-
FD
A
DC
U-
OK
IT
A
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
ONLINE-B ? .49 .45? .43? .38? .35? .34? .35? .37? .34? .33? .32? .23?
ONLINE-A .51 ? .49 .48 .38? .46? .42? .41? .43? .38? .38? .37? .31?
UEDIN .55? .51 ? .49 .46? .45? .43? .42? .36? .38? .38? .38? .26?
PROMT .57? .52 .51 ? .46? .48 .43? .43? .40? .37? .39? .34? .29?
MES .62? .62? .54? .54? ? .46? .44? .44? .41? .40? .43? .36? .32?
TALP-UPC .65? .54? .55? .52 .54? ? .50 .45? .44? .40? .40? .37? .32?
LIMSI-NCODE .66? .58? .57? .57? .56? .50 ? .46? .51 .48 .44? .45? .35?
DCU .65? .59? .58? .57? .56? .55? .54? ? .50 .48 .48 .45? .36?
DCU-FDA .63? .57? .64? .60? .59? .56? .49 .50 ? .53? .49 .42? .32?
DCU-OKITA .66? .62? .62? .63? .60? .60? .52 .52 .47? ? .50 .47? .36?
CU-ZEMAN .67? .62? .62? .61? .57? .60? .56? .52 .51 .50 ? .46? .40?
JHU .68? .63? .62? .66? .64? .63? .55? .55? .58? .53? .54? ? .37?
SHEF-WPROA .77? .69? .74? .71? .68? .68? .65? .64? .68? .64? .60? .63? ?
score .63 .58 .57 .56 .53 .52 .49 .47 .47 .45 .44 .41 .32
rank 1 2-4 2-4 3-4 5-6 5-6 7-8 7-9 8-10 9-11 10-11 12 13
Table 28: Head to head comparison, ignoring ties, for English-Spanish systems
43
ON
LI
NE
-B
CM
U
ON
LI
NE
-A
ON
LI
NE
-G
PR
OM
T
QC
RI
-M
ES
UC
AM
-M
UL
TI
FR
ON
TE
ND
BA
LA
GU
R
M
ES
-Q
CR
I
UE
DI
N
OM
NI
FL
UE
NT
-U
NC
NS
TR
LI
A
OM
NI
FL
UE
NT
-C
NS
TR
UM
D
CU
-K
AR
EL
CO
M
M
ER
CI
AL
-3
UE
DI
N-
SY
NT
AX
JH
U
CU
-ZE
M
AN
ONLINE-B ? .40? .42? .41? .37? .37? .41? .33? .33? .37? .33? .33? .35? .38? .34? .33? .29? .28? .14?
CMU .60? ? .50 .46? .43? .47? .42? .42? .39? .43? .41? .41? .40? .38? .36? .30? .30? .29? .17?
ONLINE-A .58? .50 ? .50 .51 .43? .47? .44? .40? .41? .43? .38? .40? .38? .38? .39? .34? .30? .19?
ONLINE-G .59? .54? .50 ? .55? .50 .51 .48 .42? .41? .44? .43? .46? .40? .44? .36? .34? .33? .19?
PROMT .63? .57? .49 .45? ? .43? .47? .43? .47? .47? .43? .39? .44? .43? .37? .41? .40? .38? .25?
QCRI-MES .63? .53? .57? .50 .57? ? .48 .46? .47? .45? .43? .45? .45? .38? .42? .37? .33? .40? .19?
UCAM-MULTIFRONTEND .59? .58? .53? .49 .53? .52 ? .47? .48 .46? .46? .42? .45? .46? .45? .40? .39? .33? .17?
BALAGUR .67? .58? .56? .52 .57? .54? .53? ? .47? .49 .45? .53? .40? .44? .44? .41? .36? .33? .23?
MES-QCRI .67? .61? .60? .58? .53? .53? .52 .53? ? .49 .47? .47? .43? .43? .44? .38? .42? .39? .17?
UEDIN .63? .57? .59? .59? .53? .55? .54? .51 .51 ? .48 .52 .44? .52 .49 .42? .43? .35? .21?
OMNIFLUENT-UNCNSTR .67? .59? .57? .56? .57? .57? .54? .55? .53? .52 ? .51 .46? .48 .48 .44? .40? .39? .25?
LIA .67? .59? .62? .57? .61? .55? .58? .47? .53? .48 .49 ? .51 .49 .48 .50 .41? .39? .20?
OMNIFLUENT-CNSTR .65? .60? .60? .54? .56? .55? .55? .60? .57? .56? .54? .49 ? .51 .48 .47? .40? .40? .25?
UMD .62? .62? .62? .60? .57? .62? .54? .56? .57? .48 .52 .51 .49 ? .53? .42? .46? .42? .19?
CU-KAREL .66? .64? .62? .56? .63? .58? .55? .56? .56? .51 .52 .52 .52 .47? ? .44? .40? .47? .24?
COMMERCIAL-3 .67? .70? .61? .64? .59? .63? .60? .59? .62? .58? .56? .50 .53? .58? .56? ? .51 .44? .32?
UEDIN-SYNTAX .71? .70? .66? .66? .60? .67? .61? .64? .58? .57? .60? .59? .60? .54? .60? .49 ? .45? .25?
JHU .72? .71? .70? .67? .62? .60? .67? .67? .61? .65? .61? .61? .60? .58? .53? .56? .55? ? .24?
CU-ZEMAN .86? .83? .81? .81? .75? .81? .83? .77? .83? .79? .75? .80? .75? .81? .76? .68? .75? .76? ?
score .65 .60 .58 .56 .56 .55 .54 .52 .51 .50 .49 .49 .48 .48 .47 .43 .41 .39 .21
rank 1 2-3 2-3 4-6 4-6 5-7 5-7 8-9 8-10 9-11 10-12 11-14 12-15 12-15 13-15 16 17 18 19
Table 29: Head to head comparison, ignoring ties, for Russian-English systems
PR
OM
T
ON
LI
NE
-B
CM
U
ON
LI
NE
-G
ON
LI
NE
-A
UE
DI
N
QC
RI
-M
ES
CU
-K
AR
EL
M
ES
-Q
CR
I
JH
U
CO
M
M
ER
CI
AL
-3
LI
A
BA
LA
GU
R
CU
-ZE
M
AN
PROMT ? .44? .39? .47 .46? .36? .37? .37? .32? .35? .28? .30? .32? .24?
ONLINE-B .56? ? .44? .41? .44? .38? .37? .35? .33? .39? .33? .31? .35? .24?
CMU .61? .56? ? .52 .49 .47? .43? .41? .39? .44? .44? .40? .35? .28?
ONLINE-G .53 .59? .48 ? .48 .50 .48 .46 .46? .42? .38? .43? .38? .36?
ONLINE-A .54? .56? .51 .52 ? .47 .49 .49 .48 .44? .38? .40? .40? .34?
UEDIN .64? .62? .53? .50 .53 ? .49 .46? .42? .39? .44? .41? .38? .29?
QCRI-MES .63? .63? .57? .52 .51 .51 ? .48 .45? .44? .42? .39? .40? .29?
CU-KAREL .63? .65? .59? .54 .51 .54? .52 ? .50 .46? .43? .40? .42? .34?
MES-QCRI .68? .67? .61? .54? .52 .58? .55? .50 ? .48? .47? .43? .45? .34?
JHU .65? .61? .56? .58? .56? .61? .56? .54? .52? ? .51 .44? .44? .33?
COMMERCIAL-3 .72? .67? .56? .62? .62? .56? .58? .57? .53? .49 ? .52 .48 .44?
LIA .70? .69? .60? .57? .60? .59? .61? .60? .57? .56? .48 ? .47? .41?
BALAGUR .68? .65? .65? .62? .60? .62? .60? .58? .55? .56? .52 .53? ? .41?
CU-ZEMAN .76? .76? .72? .64? .66? .71? .71? .66? .66? .67? .56? .59? .59? ?
score .64 .62 .55 .54 .53 .53 .52 .49 .47 .46 .43 .42 .41 .33
rank 1 2 3-4 3-6 3-7 4-7 5-7 8 9-10 9-10 11-12 11-13 12-13 14
Table 30: Head to head comparison, ignoring ties, for English-Russian systems
44
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 52?61,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The Feasibility of HMEANT as a Human MT Evaluation Metric
Alexandra Birch Barry Haddow Ulrich Germann
a.birch@ed.ac.uk bhaddow@inf.ed.ac.uk ugermann@inf.ed.ac.uk
Maria Nadejde Christian Buck Philipp Koehn
maria.nadejde@gmail.com cbuck@lantis.de pkoehn@inf.ed.ac.uk
University of Edinburgh
10 Crichton Street
Edinburgh, EH8 9AB, UK
Abstract
There has been a recent surge of interest in
semantic machine translation, which stan-
dard automatic metrics struggle to evalu-
ate. A family of measures called MEANT
has been proposed which uses semantic
role labels (SRL) to overcome this prob-
lem. The human variant, HMEANT, has
largely been evaluated using correlation
with human contrastive evaluations, the
standard human evaluation metric for the
WMT shared tasks. In this paper we claim
that for a human metric to be useful, it
needs to be evaluated on intrinsic proper-
ties. It needs to be reliable; it needs to
work across different language pairs; and
it needs to be lightweight. Most impor-
tantly, however, a human metric must be
discerning. We conclude that HMEANT
is a step in the right direction, but has
some serious flaws. The reliance on verbs
as heads of frames, and the assumption
that annotators need minimal guidelines
are particularly problematic.
1 Introduction
Human evaluation is essential in machine transla-
tion (MT) research because it is the ultimate way
to judge system quality. Furthermore, human eval-
uation is used to evaluate automatic metrics which
are necessary for tuning system parameters. Un-
fortunately, there is no clear consensus on which
evaluation strategy is best. Humans have been
asked to judge if translations are correct, to grade
them and to rank them. But it is often very difficult
to decide how good a translation is, when there are
so many possible ways of translating a sentence.
Another problem is that different types of evalua-
tion might be useful for different purposes. If the
MT is going to be the basis of a human transla-
tor?s work-flow, then post-editing effort seems like
a natural fit. However, for people using MT for
gisting, what we really want is some measure of
how much meaning has been retained.
We clearly need a metric which tries to answer
the question, how much of the meaning does the
translation capture. In this paper, we explore the
use of human evaluation metrics which attempt
to capture the extent of this meaning retention.
In particular, we consider HMEANT (Lo and Wu,
2011a), a metric that uses semantic role labels
to measure how much of the ?who, why, when,
where? has been preserved. For HMEANT evalua-
tion, annotators are instructed to identify verbs as
heads of semantic frames. Then they attach role
fillers to the heads and finally they align heads
and role fillers in the candidate translation with
those in a reference translation. In a series of pa-
pers, Lo and Wu (2010, 2011b,a, 2012) explored a
number of questions, evaluating HMEANT by us-
ing correlation statistics to compare it to judge-
ments of human adequacy and contrastive evalu-
ations. Given the drawbacks of those evaluation
measures, which we discuss in Sec. 2, they could
just as well have been evaluating the human ade-
quacy and contrastive judgements using HMEANT.
Human evaluation metrics need to be judged on
other intrinsic qualities, which we describe below.
The aim of this paper is to evaluate the effec-
tiveness of HMEANT, with the goal of using it to
judge the relative merits of different MT systems,
for example in the shared task of the Workshop on
Machine Translation.
In order to be useful, an MT evaluation metric
must be reliable, be language independent, have
discriminatory power, and be efficient. We address
each of these criteria as follows:
52
Reliability We produce extensive IAA (Inter-
annotator agreement) for HMEANT, breaking it
down into the different stages of annotation. Our
experimental results show that whilst the IAA for
HMEANT is acceptable at the individual stages of
the annotation, the compounding effect of dis-
agreement at each stage of the pipeline greatly re-
duces the effective overall IAA ? to 0.44 on role
alignment for German, and, only slightly better,
0.59 for English. This raises doubts about the reli-
ability of HMEANT in its current form.
Discriminatory Power We consider output of
three types of MT system (Phrase-based, Syntax-
based and Rule-based) to attempt to gain insight
into the different types of semantic information
preserved by the different systems. The Syntax-
based system seems to have a slight edge overall,
but since IAA is so low, this result has to be taken
with a grain of salt.
Language Independence We apply HMEANT
to both English and German translation outputs,
showing that the guidelines can be adapted to the
new language.
Efficiency Whilst HMEANT evaluation will
never be as fast as, for example, the contrastive
judgements used for the WMT shared task,
it is still reasonably efficient considering the
fine-grained nature of the evaluation. On average,
annotators evaluated about 10 sentences per hour.
2 Related Work
Even though the idea that machine translation re-
quires a semantic representation of the translated
content is as old as the idea of computer-based
translation itself (Weaver, 1955), it has not been
until recently that people have begun to combine
statistical models with semantic representations.
Jones et al (2012), for example, represent mean-
ing as directed acyclic graphs and map these to
PropBank (Palmer et al, 2005) style dependen-
cies. To evaluate such approaches properly, we
need evaluation metrics that capture the accuracy
of the translation.
Current automatic metrics of machine trans-
lation, such as BLEU (Papineni et al, 2002),
METEOR (Lavie and Denkowski, 2009) and
TER (Snover et al, 2009b), which have greatly
accelerated progress in MT research, rely on shal-
low surface properties of the translations, and
only indirectly capture whether or not the trans-
lation preserves the meaning. This has meant that
potentially more sophisticated translation models
are pitted against the flatter phrase-based mod-
els, based on metrics which cannot reflect their
strengths. Callison-Burch et al (2011) provide ev-
idence that automatic metrics are inconsistent with
human judgements when comparing rule-based
against statistical machine translation systems.
Automatic evaluation metrics are evaluated and
calibrated based on their correlation with human
judgements. However, after more than 60 years
of research into machine translation, there is still
no consensus on how to evaluate machine transla-
tion based on human judgements. (Hutchins and
Somers, 1992; Przybocki et al, 2009).
One obvious approach is to ask annotators to
rate translation candidates on a numerical scale.
Under the DARPA TIDES program, the Linguistic
Data Consortium (2002) developed an evaluation
scheme that relies on two five-point scales repre-
senting fluency and adequacy. This was also the
human evaluation scheme used in the annual MT
competitions sponsored by NIST (2005).
In an analysis of human evaluation results for
the WMT ?07 workshop, however, Callison-Burch
et al (2007) found high correlation between flu-
ency and adequacy scores assigned by individual
annotators, suggesting that human annotators are
not able to separate these two evaluation dimen-
sions easily. Furthermore these absolute scores
show low inter-annotator agreement. Instead of
giving absolute quality assessments, annotators
appeared to be using their ratings to rank trans-
lation candidates according to their overall prefer-
ence for one over the other.
In line with these findings, Callison-Burch et al
(2007) proposed to let annotators rank translation
candidates directly, without asking them to assign
an absolute quality assessment to each candidate.
This type of human evaluation has been performed
in the last six Workshops on Statistical Machine
Translation.
Although it is useful to have a score or a rank
for a particular sentence, especially for evaluat-
ing automatic metrics, these ratings are necessar-
ily a simplification of the real differences between
translations. Translations can contain a large num-
ber of different types of errors of varying severity.
Even if we put aside difficulties with selecting one
preferred sentence, ranking judgements are diffi-
cult to generalise. Humans are shown five transla-
tions at a time, and there is a high cognitive cost to
ranking these at once. Furthermore, these repre-
53
sent a subset of the competing systems, and these
rankings must be combined with other annotators
judgements on five other system outputs to com-
pute an overall ranking. The methodology for in-
terpreting the contrastive evaluations has been the
subject of much recent debate in the community
(Bojar et al, 2011; Lopez, 2012).
There has been some effort to overcome these
problems. HTER (Snover et al, 2009a) is a met-
ric which counts the number of edits needed by a
human to convert the machine translation so as to
convey the same meaning as the reference. This
type of evaluation is of some use when one is us-
ing MT to aid human translation (although the re-
lationship between number of edits and actual ef-
fort is not straightforward (Koponen, 2012)), but
it is not so helpful when one?s task is gisting. The
number of edits need not correlate with the sever-
ity of the semantic differences between the two
sentences. The loss of a negative, for instance, is
only one edit away from the original, but the se-
mantics change completely.
Alternatively, HyTER (Dreyer and Marcu,
2012) is an annotation tool which allows a user
to create an exponential number of correct trans-
lations for a given sentence. These references are
then efficiently exploited to compare with machine
translation output. The authors argue that the cur-
rent metrics fail simply because they have access
to sets of reference translations which are simply
too small. However, the fact is that even if one
does have access to large numbers of translations,
it is very difficult to determine whether the refer-
ence correctly captures the essential semantic con-
tent of the references.
The idea of using semantic role labels to evalu-
ate machine translation is not new. Gime?nez and
Ma`rquez (2007) proposed using automatically as-
signed semantic role labels as a feature in a com-
bined MT metric. The main difference between
this application of semantic roles and MEANT is
that arguments for specific verbs are taken into ac-
count, instead of just applying the subset agent,
patient and benefactor. This idea would probably
help human annotators to handle sentences with
passives, copulas and other constructions which
do not easily match the most basic arguments. On
the other hand, verb specific arguments are lan-
guage dependent.
Bojar and Wu (2012), applying HMEANT to
English-to-Czech MT output, identified a number
of problems with HMEANT, and suggested a vari-
ety of improvements. In some respects, this work
is very similar, except that our goal is to evaluate
HMEANT along a range of intrinsic properties, to
determine how useful the metric really is to evalu-
ation campaigns such as the workshop on machine
translation.
3 Evaluation with HMEANT
3.1 Annotation Procedure
The goal of the HMEANT metric is to capture es-
sential semantic content, but still be simple and
fast. There are two stages to the annotation, the
first of which is semantic role labelling (SRL).
Here the annotator is directed to select the actions,
or frame heads, by marking all the verbs in the sen-
tence except for auxilliaries and modals. The roles
(or slot fillers) within the frame are then marked
and each is linked with a unique action. Each role
is given a type from an inventory of 11 (Table 1),
and an action with its collection of corresponding
roles is known as a frame. In the role annotation
the idea is to get the annotator to recognise who
did what to who, when, where and why in both the
references and the MT outputs.
who what whom when where
agent patient benefactive temporal locative
why how
purpose degree, manner, modal, negation, other
Table 1: Semantic roles
The second stage in the annotation is alignment,
where the annotators match elements of the SRL
annotation in the reference with that in the MT
output. The annotators link both actions and roles,
and these alignments can be matched as ?Correct?
or ?Partial? matches, depending on how well the
action or role is translated. The guidelines for the
annotators are deliberately minimalistic, with the
argument being that non-experts can get started
quickly. Lo and Wu (2011a) claim that unskilled
annotators can be trained within 15 minutes.
In all such human evaluation, there is a trade-
off between simplicity and accuracy. Clearly when
evaluating bad machine translation output, we do
not want to label too much. However, sometimes
having so little choice of semantic roles can lead
to confusion and slow down the annotator when
more complicated examples do not fit the scheme.
Therefore, common exceptions need to be handled
either in the roles provided, or in the annotator
guidelines.
54
3.2 Calculation of Score
The overall HMEANT score for MT evaluation
is computed as the f-score from the counts of
matches of frames and their role fillers between
the reference and the MT output. Unmatched
frames are excluded from the calculation together
with all their corresponding roles.
In recognition that preservation of some types
of semantic relations may be more important than
others for a human to understand a sentence, one
may want to weight them differently in the com-
putation of the HMEANT score. Lo and Wu (2012)
train weights for each role filler type to optimise
correlation with human adequacy judgements. As
an unsupervised alternative, they suggest weight-
ing roles according to their frequency as approxi-
mation to their importance.
Since the main focus of the current paper is the
annotation of the actions, roles and alignments that
HMEANT depends on, we do not explore such dif-
ferent weight-setting schemes, but set the weights
uniformly, with the exception of a partial align-
ment, which is given a weight of 0.5. HMEANT is
thus defined as follows:
Fi = # correct or partially correct fillers
for PRED i in MT
MTi = total # fillers for PRED i in MT
REFi = total # fillers for PRED i in REF
P =
?
matched i
Fi
MTi
R =
?
matched i
Fi
REFi
Ptotal =
Pcorrect + 0.5Ppartial
total # predicates in MT
Rtotal =
Pcorrect + 0.5Ppartial
total # predicates in REF
HMEANT = 2 ? Ptotal ?RtotalPtotal +Rtotal
3.3 Automating HMEANT
One of the main directions taken by the authors of
HMEANT is in creating a fully automated version
of the metric (MEANT) in (Lo et al, 2012). The
metric combines shallow semantic parsing with a
simple maximum weighted bipartite matching al-
gorithm for aligning semantic frames. They use
approximate matching schemes (Cosine and Jac-
card similarity) for matching roles, with the lat-
ter producing better alignments (Tumuluru et al,
2012). They demonstrate that MEANT corre-
lates with human adequacy judgements better than
other commonly used automatic metrics. In this
paper we focus on human evaluation, as it is es-
sential for building better automatic metrics, and
therefore a more fundamental problem.
4 Experimental Setup
4.1 Systems and Data Sets
We performed HMEANT evaluation on three
systems selected from 2013 WMT evaluation1.
The systems we selected were uedin-wmt13,
uedin-syntax and rbmt-3, which were cho-
sen to provide us with a high performing phrase-
based system, a high performing syntax-based
system and the top performing rule-based system,
respectively. The cased BLEU scores of the three
systems are shown in Table 2.
System Type de-en en-de
uedin-wmt13 Phrase 26.6 20.1
uedin-syntax Syntax 26.3 19.4
rbmt-3 Rule 18.8 16.5
Table 2: Cased BLEU on the full newstest2013
test set for the systems used in this study
We randomly selected sentences from the en-de
and de-en newstest2013 tasks, and extracted
the corresponding references and system outputs
for these sentences. For the en-de task, 75% of our
selected sentences were selected from the section
of newstest2013 that was originally in Ger-
man, with the other 25% from the section that was
originally in English. The sentence selection for
the de-en task was performed in a similar man-
ner. For presentation to the annotators, the sen-
tences were split into segments of 12. We found
that with practice, annotators could complete one
of these segments in around 100-120 minutes. In
total, with close to 70 hours of annotator effort,
we evaluated 142 sentences of German, and 72
sentences of English. The annotation for each
sentence includes 1 reference, 3 system outputs,
and their corresponding alignments. Apart from 5
singly-annotated German sentences, and 1 singly-
annotated English sentence, all sentences were an-
notated by exactly 2 annotators.
1www.statmt.org/wmt13
55
4.2 Annotation
The annotation for English was performed by 3
different annotators (E1, E2 and E3), and the Ger-
man annotation by 2 annotators (D1 and D2).
All the English annotators were machine transla-
tion researchers, with E1 and E2 both native En-
glish speakers whereas E3 is not a native speaker,
but lives and works in an English-speaking coun-
try. The two German annotators were both native
speakers of German, with no background in com-
putational linguistics, although D2 is a teacher of
German as a second language and has had linguis-
tic training.
The HMEANT evaluation task was carried out
following the framework described in Lo and Wu
(2011a) and Bojar and Wu (2012). For each sen-
tence in the evaluation set, the annotators were first
asked to mark the semantic frames and roles (i.e.,
slot fillers within the frame) in a human reference
translation of the respective sentence. They were
then presented with the output of several machine
translation systems for the same source sentence,
one system at a time, with the reference transla-
tion and its annotations visible in the left half of
the screen (cf. Fig. 1). For each system, the an-
notators were asked to annotate semantic frames
and slot fillers in the translation first, and then
align them with frame heads and slot fillers in
the human reference translation. Annotations and
alignment were performed with Edi-HMEANT2,
a web-based annotation tool for HMEANT that
we developed on the basis of Yawat (Germann,
2008). The tool allows the alignment of slots from
different semantic frames, and the alignment of
slots of different types; however, such alignments
are not considered in the computation of the final
HMEANT score.
The annotation guidelines were essentially
those used in Bojar and Wu (2012), with some ad-
ditional English examples, and a complete set of
German examples. For ease of comparison with
prior work, we used the same set of semantic role
labels as Bojar and Wu (2012), shown in Table 1.
Given the restriction that the head of a frame can
consist of only one word, a convention was made
that all other verbs attached to the main verb such
as modals, auxiliaries or separable particles for
German verbs, would be labelled as modal. This
was the only change we made to the HMEANT
2Edi-HMEANT is part of the Edinburgh
Multi-text Annotation and Alignment Tool Suite
(http://www.statmt.org/edimtaats).
scheme.
5 Results and Discussion
5.1 Inter-Annotator Agreement
We first measured IAA on role identification, as
in Lo and Wu (2011a), except that we use exact
match on word spans as opposed to the approx-
imate match employed in that reference. Whilst
exact match is a harsher measure, penalising dis-
agreements related to punctuation and articles, us-
ing any sort of approximate match would mean
having to deal with N:M matches. IAA is defined
as follows:
IAA = 2 ? P ?RP +R
Where P is defined as the number of labels (ei-
ther heads, roles, or alignments) that match be-
tween annotators, divided by the total number of
labels given by annotator 1. And R is defined the
same way for annotator 2. This is similar to an
F-measure (f1), where we consider one of the an-
notators as the gold standard. The IAA for role
identification is shown in Table 3.
Reference Hypothesis
Lang. matches f1 matches f1
de 865 0.846 2091 0.737
en 461 0.759 1199 0.749
Table 3: IAA for role identification. This is calcu-
lated by considering exact endpoint matches on all
spans (predicates and arguments).
The agreements in Table 3 are not too differ-
ent from those reported in earlier work. We note
that the IAA for the German annotators drops for
the MT system outputs, but this may be because
the English annotators (as MT researchers) are less
bothered by bad MT output than their counterparts
working on the German texts.
Next we looked at the IAA on role classifica-
tion, the other IAA figure provided by Lo and Wu
(2011a). We only considered roles where both an-
notators had marked the same span in the same
frame, with the frame being identified by its ac-
tion. The IAA for role classification is shown in
Table 4.
Again, we show similar levels of IAA to those
reported in (Lo and Wu, 2011a). Examining the
disagreements in more detail, we produced counts
of the most common role type disagreements, by
56
Figure 1: Example of a sentence pair annotated with Edi-HMEANT. The reference translation is on
the left, the machine translation output on the right. Head and slot fillers for each semantic frame are
marked by selecting spans in the text and automatically listed in tables below the respective sentences.
Frames and slot fillers are aligned by clicking on table cells. The alignments of the semantic frames are
highlighted: green (grey in black and white version) for exact match and grey (light grey) for partial
match.
Reference Hypothesis
Lang. matches f1 matches f1
de 425 0.717 1050 0.769
en 245 0.825 634 0.826
Table 4: IAA for role classification. We only con-
sider cases where annotators had marked the same
span in the same frame.
Role 1 Role 2 Count
Agent Experiencer-Patient 110
Degree-Extent Modal 92
Beneficiary Experiencer-Patient 45
Experiencer-Patient Manner 26
Manner Other 25
Table 5: Most common role type disagreements,
for German
language. We show the top 5 disagreements in Ta-
bles 5 and 6. Essentially these show that the most
common role types provide the most confusions.
In order to shed more light on the role type dis-
agreements, we examined a random sample of 10
of the English annotations where the annotators
had disagreed about ?Agent? versus ?Experiencer-
Patient?. In 7 of these cases, there was a definite
correct answer, according to the annotation guide-
lines. Of the other 3, there were 2 cases of poor
MT output making the semantic interpretation dif-
ficult, and one case of existential ?there?. Of the 7
cases where one annotator appears in error, 3 were
passive, 1 was a copula, and 1 involved the verb
Role 1 Role 2 Count
Agent Experiencer-Patient 44
Manner Other 22
Degree-Extent Temporal 12
Degree-Extent Other 12
Beneficiary Experiencer-Patient 11
Table 6: Most common role type disagreements,
for English
?receive?. For the other 2 there was no clear rea-
son for the error. From this small sample, we sug-
gest that passive constructions are still difficult to
annotate semantically.
The last of elements of the semantic frames to
be considered for IAA are the actions, i.e. the
frame heads or predicates. In this case identifying
a match was straightforward as actions are identi-
fied by a single token. The IAA for action identi-
fication is shown in Table 7.
Reference Hypothesis
Lang. matches f1 matches f1
de 238 0.937 592 0.826
en 126 0.818 362 0.868
Table 7: IAA for action identification.
We see fairly high IAA for actions, which seems
encouraging, but given the importance of actions
in HMEANT, we probably need the scores to be
higher. Most of the problems with the identifica-
tion of actions centre around multiple-verb con-
structions and participles.
We now turn our attention to the second stage
of the annotation process where the annotators
marked alignments between slots and roles. These
provide the relevant statistics for the calculation of
the HMEANT score so it is important that they are
annotated reliably.
Firstly, we consider the alignment of actions. In
this case, we use pipelined statistics, in that if one
annotator marks actions in the reference and hy-
pothesis, then aligns them, whilst the other anno-
tator does not mark the corresponding actions, we
still count this as an action alignment mismatch.
This creates a harsher measure on action align-
ment, but gives a better idea of the overall relia-
bility of the annotation task. In Table 8 we show
the IAA (as F1) on action alignments. Comparing
Tables 8 and 7 we see that, for English at least, the
57
Lang. matches f1
de 300 0.655
en 275 0.769
Table 8: IAA for action alignment, collapsing par-
tial and full alignment
agreement on action alignment is not much lower
than that on action identification, indicating that if
annotators agree on the actions then they generally
agree on how they align. For German, however,
the IAA on action alignment is a bit lower, ap-
parently because one of the annotators was much
stricter about which actions they aligned.
In order to calculate the IAA on role align-
ments, we only consider those alignments that
connect two roles in aligned frames, of the same
type, since these are the only role alignments that
count for computing the HMEANT score. This
means that if one of the annotators does not align
the frames, then all the contained role alignments
are counted as mismatches. We do not consider
the spans when calculating the agreement on role
alignments, meaning that if one annotator has an
alignment between roles of type T in frame F ,
and the other annotator also aligns the same types
of roles in the same frame, then they are consid-
ered as a match. This is done because it is only the
counts of alignments that are relevant for HMEANT
scoring. The IAA on the role alignments is quite
Lang. matches f1
de 448 0.442
en 506 0.596
Table 9: IAA for role alignment.
low, dipping below 0.5 for German. This is mainly
because of the pipelining effect, where annota-
tion disagreements at each stage are compounded.
Since the final HMEANT score is computed essen-
tially by counting role alignments, this level of
IAA causes problems for this score calculation.
We computed HMEANT and BLEU scores for the
hypotheses annotated by each annotator pair. The
HMEANT scores were calculated as described in
Section 3.2. The two metrics are calculated for
each sentence (we apply +1 smoothing for BLEU),
then averaged across all sentences. Table 10 shows
the scores organised by annotator pair and sys-
tem type. The agreement in the overall scores is
not good, but really just reflects the compounded
Annotator System BLEU HMEANT HMEANT
Pair (Annot. 1) (Annot. 2)
Phrase 0.310 0.626 (2) 0.672 (3)
E1, E2 Syntax 0.291 0.635 (1) 0.730 (1)
Rule 0.252 0.578 (3) 0.673 (2)
Phrase 0.378 0.569 (1) 0.602 (3)
E1, E3 Syntax 0.376 0.553 (2) 0.627 (2)
Rule 0.320 0.546 (3) 0.646 (1)
Phrase 0.360 0.669 (2) 0.696 (3)
E2, E3 Syntax 0.362 0.751 (1) 0.739 (1)
Rule 0.308 0.624 (3) 0.716 (2)
Phrase 0.296 0.327 (1) 0.631 (3)
D1, D2 Syntax 0.321 0.312 (2) 0.707 (1)
Rule 0.242 0.274 (3) 0.648 (2)
Table 10: Scores assigned by each annotator pair.
The numbers in brackets after the HMEANT scores
show the relative ranking assigned by each anno-
tator.
agreement problems in the role alignments (Table
9). In no case do the annotators choose a consis-
tent ranking of the 3 systems, and in 2 of the 4 an-
notator pairs, the annotators disagree about which
is the top performing system.
5.2 Overall Scores
In this section we report the overall HMEANT
scores of the three systems whose output we an-
notated. Our main focus on this paper was on the
annotation task, so we do not wish to emphasise
the scoring, but it is nevertheless an important end-
product of the HMEANT annotation process. The
overall scores (HMEANT and +1 smoothed sen-
tence BLEU, averaged across sentences and anno-
tators) are given in Table 11.
Language System BLEU HMEANT
Phrase 0.351 0.634
en Syntax 0.344 0.667
Rule 0.295 0.625
Phrase 0.294 0.482
de Syntax 0.302 0.517
Rule 0.242 0.464
Table 11: Comparison of mean HMEANT and
(smoothed sentence) BLEU for the three systems.
From the table we can observe that, whilst
BLEU shows similar scores for the phrase-based
and syntax-based systems, with lower scores for
the rule-based system, HMEANT shows the syntax-
based system as being ahead, with the other two
showing similar performance. We would caution
against reading too much into this, considering the
relatively small number of sentences annotated,
58
and the issues with IAA exposed in the previous
section, but it is an encouraging results for syntax-
based MT.
5.3 Discussion
Machine translation research needs a reliable
method for evaluating and comparing different
machine translation systems. The performance of
HMEANT as shown in the previous section is dis-
appointing. The fact that the final role IAA, in Ta-
ble 9, is 0.442 for German and 0.596 for English,
demonstrates that there are fundamental problems
with the scheme. One of the areas of greatest con-
fusion is between what seems like one of the eas-
iest role types to distinguish: agent and patient.
Here is an example of a passive where one anno-
tator has marked ?tea? wrongly as agent, and the
other annotator correctly labelled it as patient:
Reference: In the kitchen, tea is prepared for
the guests
ACTION prepared
LOCATIVE In the kitchen
AGENT / PATIENT tea
MODAL is
BENEFICIARY for the guests
We would argue that the most important change
to HMEANT must be in creating more comprehen-
sive annotation guidelines, with examples of diffi-
cult cases. Bojar and Wu (2012) listed a number of
problems and improvements to HMEANT, which
we largely agree with. We list the most important
limitations of HMEANT that we have encountered:
? Single Word Heads Verbal predicates often
consist of multiple words, which can be split.
For example: ?Take him up on his offer?.
? Heads being limited to verbs The semantics
of verbs can often be carried by an equivalent
noun and should be allowed by HMEANT. For
example ?My father broke down and cried .?,
the verb ?cried? is correctly paraphrased in
?My father collapsed in tears .?
? Copular Verbs These do not fit in to the lim-
ited list of role types. For example forcing
this sentence ?The story is plausible?, to have
and agent and patient is confusing.
? Prepositional Phrases attaching to a noun
These can greatly affect the semantics of a
sentence, but HMEANT has no way of captur-
ing this.
? Semantics not on head This frequently oc-
curs with light verbs, for example ?Bouson
did the review of the paper? is equivalent to
?Bouson reviewed the paper?.
? Hierarchy of frames There are often frames
which are embedded in other frames, for ex-
ample in reported speech. It is not clear
whether errors at the lowest level should be
marked wrong just at that point, or whether
they should be marked wrong all the way up
the semantic tree. For example: ?Arafat said
?Isreal suffocates such a hope in the germ? ?.
The frame headed by ?said? is largely cor-
rect, but the reported speech is not. The pa-
tient role of the verb ?said? could be aligned
as correct, as the error is already captured in
relation to the verb ?suffocates?.
? No discourse markers These are impor-
tant for capturing the relationships between
frames and should be labelled.
6 Conclusion
HMEANT represents an attempt to create a human
evaluation for machine translation which directly
measures the semantic content preserved by the
MT. It partly succeeds. However we have cast
doubt on the claim that HMEANT can be reliably
annotated with minimal annotator training and
guidelines. In the most extensive study of inter-
annotator agreement yet performed for HMEANT,
across two language pairs, we have shown that the
disagreements between annotators make it diffi-
cult to reliably compare different MT systems with
HMEANT scores.
Furthermore, the fact that HMEANT is restricted
to annotating purely verbal predicates results in
some important disadvantages. Ideally we need a
more general definition of a frame, not restricted
to purely verbal predicates, and we would like
to be able to link frames. We should explore
the feasibility of a semantic framework which at-
tempts to overcome reliance on syntactic proper-
ties such as Universal Conceptual Cognitive An-
notation (Abend and Rappoport, 2013).
7 Acknowledgements
The research leading to these results has received
funding from the European Union Seventh Frame-
work Programme (FP7/2007-2013) under grant
agreement 287658 (EU BRIDGE).
59
References
Abend, Omri and Ari Rappoport. 2013. ?Univer-
sal Conceptual Cognitive Annotation (UCCA).?
Proceedings of ACL.
Bojar, Ondrej, Milos? Ercegovc?evic?, Martin Popel,
and Omar Zaidan. 2011. ?A Grain of Salt for the
WMT Manual Evaluation.? Proceedings of the
Sixth Workshop on Statistical Machine Transla-
tion, 1?11. Edinburgh, Scotland.
Bojar, Ondrej and Dekai Wu. 2012. ?Towards a
Predicate-Argument Evaluation for MT.? Pro-
ceedings of SSST, 30?38.
Callison-Burch, Chris, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder.
2007. ?(Meta-) evaluation of machine trans-
lation.? Proceedings of the Second Workshop
on Statistical Machine Translation, 136?158.
Prague, Czech Republic.
Callison-Burch, Chris, Philipp Koehn, Christof
Monz, and Omar F Zaidan. 2011. ?Findings of
the 2011 workshop on statistical machine trans-
lation.? Proceedings of the Sixth Workshop on
Statistical Machine Translation, 22?64.
Dreyer, Markus and Daniel Marcu. 2012. ?Hyter:
Meaning-equivalent semantics for translation
evaluation.? Proceedings of the 2012 Con-
ference of the North American Chapter of
the Association for Computational Linguis-
tics: Human Language Technologies, 162?171.
Montre?al, Canada.
Germann, Ulrich. 2008. ?Yawat: Yet Another
Word Alignment Tool.? Proceedings of the
ACL-08: HLT Demo Session, 20?23. Colum-
bus, Ohio.
Gime?nez, Jesu?s and Llu??s Ma`rquez. 2007. ?Lin-
guistic features for automatic evaluation of het-
erogenous mt systems.? Proceedings of the Sec-
ond Workshop on Statistical Machine Transla-
tion, StatMT ?07, 256?264. Stroudsburg, PA,
USA.
Hutchins, W. J. and H. L. Somers. 1992. An intro-
duction to machine translation. Academic Press
New York.
Jones, Bevan, Jacob Andreas, Daniel Bauer,
Karl Moritz Hermann, and Kevin Knight. 2012.
?Semantics-based machine translation with hy-
peredge replacement grammars.? Proceedings
of COLING.
Koponen, Maarit. 2012. ?Comparing human per-
ceptions of post-editing effort with post-editing
operations.? Proceedings of the Seventh Work-
shop on Statistical Machine Translation, 181?
190. Montre?al, Canada.
Lavie, Alon and Michael Denkowski. 2009. ?The
METEOR metric for automatic evaluation of
machine translation.? Machine Translation.
Linguistic Data Consortium. 2002. ?Lin-
guistic data annotation specification: As-
sessment of fluency and adequacy in
Chinese-English translation.? http:
//projects.ldc.upenn.edu/TIDES/
Translation/TranAssessSpec.pdf.
Lo, Chi-kiu, Anand Karthik Tumuluru, and Dekai
Wu. 2012. ?Fully automatic semantic MT eval-
uation.? Proceedings of WMT, 243?252.
Lo, Chi-kiu and Dekai Wu. 2010. ?Evaluating
machine translation utility via semantic role la-
bels.? Proceedings of LREC, 2873?2877.
Lo, Chi-kiu and Dekai Wu. 2011a. ?MEANT : An
inexpensive , high-accuracy , semi-automatic
metric for evaluating translation utility via se-
mantic frames.? Proceedings of ACL, 220?229.
Lo, Chi-kiu and Dekai Wu. 2011b. ?Structured vs.
flat semantic role representations for machine
translation evaluation.? Proceedings of SSST,
10?20.
Lo, Chi-kiu and Dekai Wu. 2012. ?Unsupervised
vs. supervised weight estimation for semantic
MT evaluation metrics.? Proceedings of SSST,
49?56.
Lopez, Adam. 2012. ?Putting human assessments
of machine translation systems in order.? Pro-
ceedings of WMT, 1?9.
NIST. 2005. ?The 2005 NIST machine
translation evaluation plan (MT-05).?
http://www.itl.nist.gov/iad/
mig/tests/mt/2005/doc/mt05_
evalplan.v1.1.pdf.
Palmer, Martha, Daniel Gildea, and Paul Kings-
bury. 2005. ?The proposition bank: An anno-
tated corpus of semantic roles.? Computational
Linguistics, 31(1):71?106.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. ?BLEU: a method for au-
tomatic evaluation of machine translation.? Pro-
ceedings of the Association for Computational
Linguistics, 311?318. Philadelphia, USA.
Przybocki, Mark, Kay Peterson, Se?bastien Bron-
sart, and Gregory Sanders. 2009. ?The NIST
60
2008 metrics for machine translation challen-
geoverview, methodology, metrics, and results.?
Machine Translation, 23(2):71?103.
Snover, Matthew, Nitin Madnani, Bonnie Dorr,
and Richard Schwartz. 2009a. ?Fluency, ad-
equacy, or HTER? exploring different human
judgments with a tunable MT metric.? Proceed-
ings of the Workshop on Statistical Machine
Translation at the Meeting of the European
Chapter of the Association for Computational
Linguistics (EACL-2009). Athens, Greece.
Snover, Matthew, Nitin Madnani, Bonnie Dorr,
and Richard Schwartz. 2009b. ?TER-plus:
paraphrase, semantic, and alignment enhance-
ments to translation edit rate.? Machine Trans-
lation.
Tumuluru, Anand Karthik, Chi-kiu Lo, and Dekai
Wu. 2012. ?Accuracy and robustness in measur-
ing the lexical similarity of semantic role fillers
for automatic semantic MT evaluation.? Pro-
ceedings of PACLIC, 574?581.
Weaver, Warren. 1955. ?Translation.? William N.
Locke and Andrew D. Booth (eds.), Machine
Translation of Languages; Fourteen Essays,
15?23. Cambridge, MA: MIT Press. Reprint of
a memorandum written in 1949.
61
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 114?121,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Edinburgh?s Machine Translation Systems for European Language Pairs
Nadir Durrani, Barry Haddow, Kenneth Heafield, and Philipp Koehn
School of Informatics
University of Edinburgh
Scotland, United Kingdom
{dnadir,bhaddow,kheafiel,pkoehn}@inf.ed.ac.uk
Abstract
We validated various novel and recently
proposed methods for statistical machine
translation on 10 language pairs, using
large data resources. We saw gains
from optimizing parameters, training with
sparse features, the operation sequence
model, and domain adaptation techniques.
We also report on utilizing a huge lan-
guage model trained on 126 billion tokens.
The annual machine translation evaluation cam-
paign for European languages organized around
the ACL Workshop on Statistical Machine Trans-
lation offers the opportunity to test recent advance-
ments in machine translation in large data condi-
tion across several diverse language pairs.
Building on our own developments and external
contributions to the Moses open source toolkit, we
carried out extensive experiments that, by early in-
dications, led to a strong showing in the evaluation
campaign.
We would like to stress especially two contri-
butions: the use of the new operation sequence
model (Section 3) within Moses, and ? in a sepa-
rate unconstraint track submission ? the use of a
huge language model trained on 126 billion tokens
with a new training tool (Section 4).
1 Initial System Development
We start with systems (Haddow and Koehn, 2012)
that we developed for the 2012 Workshop on
Statistical Machine Translation (Callison-Burch
et al, 2012). The notable features of these systems
are:
? Moses phrase-based models with mostly de-
fault settings
? training on all available parallel data, includ-
ing the large UN parallel data, the French-
English 109 parallel data and the LDC Giga-
word data
? very large tuning set consisting of the test sets
from 2008-2010, with a total of 7,567 sen-
tences per language
? German?English with syntactic pre-
reordering (Collins et al, 2005), compound
splitting (Koehn and Knight, 2003) and use
of factored representation for a POS target
sequence model (Koehn and Hoang, 2007)
? English?German with morphological target
sequence model
Note that while our final 2012 systems in-
cluded subsampling of training data with modified
Moore-Lewis filtering (Axelrod et al, 2011), we
did not use such filtering at the starting point of
our development. We will report on such filtering
in Section 2.
Moreover, our system development initially
used the WMT 2012 data condition, since it took
place throughout 2012, and we switched to WMT
2013 training data at a later stage. In this sec-
tion, we report cased BLEU scores (Papineni et al,
2001) on newstest2011.
1.1 Factored Backoff (German?English)
We have consistently used factored models in past
WMT systems for the German?English language
pairs to include POS and morphological target se-
quence models. But we did not use the factored
decomposition of translation options into multi-
ple mapping steps, since this usually lead to much
slower systems with usually worse results.
A good place, however, for factored decompo-
sition is the handling of rare and unknown source
words which have more frequent morphological
variants (Koehn and Haddow, 2012a). Here, we
used only factored backoff for unknown words,
giving gains in BLEU of +.12 for German?English.
1.2 Tuning with k-best MIRA
In preparation for training with sparse features, we
moved away from MERT which is known to fall
114
apart with many more than a couple of dozen fea-
tures. Instead, we used k-best MIRA (Cherry and
Foster, 2012). For the different language pairs, we
saw improvements in BLEU of -.05 to +.39, with an
average of +.09. There was only a minimal change
in the length ratio (Table 1)
MERT k-best MIRA ?
de-en 22.11 (1.010) 22.10 (1.008) ?.01 (+.002)
fr-en 30.00 (1.023) 30.11 (1.026) +.11 (?.003)
es-en 30.42 (1.021) 30.63 (1.020) +.21 (?.001)
cs-en 25.54 (1.022) 25.49 (1.024) ?.05 (?.002)
en-de 16.08 (0.995) 16.04 (1.001) ?.04 (?.006)
en-fr 29.26 (0.980) 29.65 (0.982) +.39 (?.002)
en-es 31.92 (0.985) 31.95 (0.985) +.03 (?.000)
en-cs 17.38 (0.967) 17.42 (0.974) +.04 (?.007)
avg ? ? +.09
Table 1: Tuning with k-best MIRA instead of MERT
(cased BLEU scores with length ratio)
1.3 Translation Table Smoothing with
Kneser-Ney Discounting
Previously, we smoothed counts for the phrasal
conditional probability distributions in the trans-
lation model with Good Turing discounting. We
explored the use of Kneser-Ney discounting, but
results are mixed (no difference on average, see
Table 2), so we did not pursue this further.
Good Turing Kneser Ney ?
de-en 22.10 22.15 +.05
fr-en 30.11 30.13 +.02
es-en 30.63 30.64 +.01
cs-en 25.49 25.56 +.07
en-de 16.04 15.93 ?.11
en-fr 29.65 29.75 +.10
en-es 31.95 31.98 +.03
en-cs 17.42 17.26 ?.16
avg ? ? ?.00
Table 2: Translation model smoothing with Kneser-Ney
1.4 Sparse Features
A significant extension of the Moses system over
the last couple of years was the support for large
numbers of sparse features. This year, we tested
this capability on our big WMT systems. First, we
used features proposed by Chiang et al (2009):
? phrase pair count bin features (bins 1, 2, 3,
4?5, 6?9, 10+)
? target word insertion features
? source word deletion features
? word translation features
? phrase length feature (source, target, both)
The lexical features were restricted to the 50 most
frequent words. All these features together only
gave minor improvements (Table 3).
baseline sparse ?
de-en 22.10 22.02 ?.08
fr-en 30.11 30.24 +.13
es-en 30.63 30.61 ?.02
cs-en 25.49 25.49 ?.00
en-de 16.04 15.93 ?.09
en-fr 29.65 29.81 +.16
en-es 31.95 32.02 +.07
en-cs 17.42 17.28 ?.14
avg ? ? +.04
Table 3: Sparse features
We also explored domain features in the sparse
feature framework, in three different variations.
Assume that we have three domains, and a phrase
pair occurs in domain A 15 times, in domain B 5
times, and in domain C never.
We compute three types of domain features:
? binary indicator, if phrase-pairs occurs in do-
main (example: indA = 1, indB = 1, indC = 0)
? ratio how frequent the phrase pairs occurs in
domain (example: ratioA = 1515+5 = .75, ratioB =
5
15+5 = .25, ratioC = 0)
? subset of domains in which phrase pair oc-
curs (example: subsetAB = 1, other subsets 0)
We tested all three feature types, and found
the biggest gain with the domain indicator feature
(+.11, Table 4). Note that we define as domain the
different corpora (Europarl, etc.). The number of
domains ranges from 2 to 9 (see column #d).1
#d base. indicator ratio subset
de-en 2 22.10 22.14 +.04 22.07 ?.03 22.12 +.02
fr-en 4 30.11 30.34 +.23 30.29 +.18 30.15 +.04
es-en 3 30.63 30.88 +.25 30.64 +.01 30.82 +.19
cs-en 9 25.49 25.58 +.09 25.58 +.09 25.46 ?.03
en-de 2 16.122 16.14 +.02 15.96 ?.16 16.01 ?.11
en-fr 4 29.65 29.75 +.10 29.71 +.05 29.70 +.05
en-es 3 31.95 32.06 +.11 32.13 +.18 32.02 +.07
en-cs 9 17.42 17.45 +.03 17.35 ?.07 17.44 +.02
avg. - ? +.11 +.03 +.03
Table 4: Sparse domain features
When combining the domain features and the
other sparse features, we see roughly additive
gains (Table 5). We use the domain indicator fea-
ture and the other sparse features in subsequent ex-
periments.
1In the final experiments on the 2013 data condition, one
domain (commoncrawl) was added for all language pairs.
115
baseline indicator ratio subset
de-en 22.10 22.18 +.08 22.10 ?.00 22.16 +.06
fr-en 30.11 30.41 +.30 30.49 +.38 30.36 +.25
es-en 30.63 30.75 +.12 30.56 ?.07 30.85 +.22
cs-en 25.49 25.56 +.07 25.63 +.14 25.43 ?.06
en-de 16.12 15.95 ?.17 15.96 ?.16 16.05 ?.07
en-fr 29.65 29.96 +.31 29.88 +.23 29.92 +.27
en-es 31.95 32.12 +.17 32.16 +.21 32.08 +.23
en-cs 17.42 17.38 ?.04 17.35 ?.07 17.40 ?.02
avg. ? +.11 +.09 +.11
Table 5: Combining domain and other sparse features
1.5 Tuning Settings
Given the opportunity to explore the parameter
tuning of models with sparse features across many
language pairs, we investigated a number of set-
tings. We expect tuning to work better with more
iterations, longer n-best lists and bigger cube prun-
ing pop limits. Our baseline settings are 10 itera-
tions with 100-best lists (accumulating) and a pop
limit of 1000 for tuning and 5000 for testing.
base 25 it. 25it+1k-best 25it+pop5k
de-en 22.18 22.16 ?.02 22.14 ?.04 22.17 ?.01
fr-en 30.41 30.40 ?.01 30.44 +.03 30.49 +.08
es-en 30.75 30.91 +.16 30.86 +.11 30.81 +.06
cs-en 25.56 25.60 +.04 25.64 +.08 25.56 ?.00
en-de 15.96 15.99 +.03 16.05 +.09 15.96 ?.00
en-fr 29.96 29.90 ?.06 29.95 ?.01 29.92 ?.04
en-es 32.12 32.17 +.05 32.11 ?.01 32.19 +.07
en-cs 17.38 17.43 +.05 17.50 +.12 17.38 ?.00
avg ? +.03 +.05 +.02
Table 6: Tuning settings (number of iterations, size of n-best
list, and cube pruning pop limit)
Results support running tuning for 25 iterations
but we see no gains for 5000 pops. There is ev-
idence that an n-best list size of 1000 is better in
tuning but we did not adopt this since these large
lists take up a lot of disk space and slow down the
MIRA optimization step (Table 6).
1.6 Smaller Phrases
Given the very large corpus sizes (up to a billion
words of parallel data for French?English), the
size of translation model and lexicalized reorder-
ing model becomes a challenge. Hence, we want
to examine if restriction to smaller phrases is fea-
sible without loss in translation quality. Results
in Table 7 suggest that a maximum phrase length
of 5 gives almost identical results, and only with
a phrase length limit of 4 significant losses occur.
We adopted the limit of 5.
max 7 max 6 max 5 max 4
de-en 22.16 22.03 ?.13 22.05 ?.11 22.17 +.01
fr-en 30.40 30.30 ?.10 30.39 ?.01 30.23 ?.17
es-en 30.91 30.80 ?.09 30.86 ?.05 30.81 ?.10
cs-en 25.60 25.55 ?.05 25.53 ?.07 25.48 ?.12
en-de 15.99 15.94 ?.05 15.97 ?.02 16.03 +.04
en-fr 29.90 29.97 +.07 29.89 ?.01 29.77 ?.13
en-es 32.17 32.13 ?.04 32.27 +.10 31.93 ?.24
en-cs 17.43 17.46 +.03 17.41 ?.02 17.41 ?.02
avg ? ?.05 ?.03 ?.09
Table 7: Maximum phrase length, reduced from baseline
1.7 Unpruned Language Models
Previously, we trained 5-gram language models
using the default settings of the SRILM toolkit in
terms of singleton pruning. Thus, training throws
out all singletons n-grams of order 3 and higher.
We explored whether unpruned language models
could give better performance, even if we are only
able to train 4-gram models due to memory con-
straints. At the time, we were not able to build un-
pruned 4-gram language models for English, but
for the other language pairs we did see improve-
ments of -.07 to +.13 (Table 8). We adopted such
models for these language pairs.
5g pruned 4g unpruned ?
en-fr 29.89 29.83 ?.07
en-es 32.27 32.34 +.07
en-cs 17.41 17.54 +.13
Table 8: Language models without singleton pruning
1.8 Translations per Input Phrase
Finally, we explored one more parameter: the limit
on how many translation options are considered
per input phrase. The default for this setting is 20.
However, our experiments (Table 9) show that we
can get better results with a translation table limit
of 100, so we adopted this.
ttl 20 ttl 30 ttl 50 ttl 100
de-en 21.05 +.06 +.09 +.01
fr-en 30.39 ?.02 +.05 +.07
es-en 30.86 ?.00 ?.03 ?.07
cs-en 25.53 +.24 +.13 +.20
en-de 15.97 +.03 +.07 +.11
en-fr 29.83 +.14 +.19 +.13
en-es 32.34 +.08 +.10 +.07
en-cs 17.54 ?.05 ?.02 +.01
avg ? +.06 +.07 +.07
Table 9: Maximal number translations per input phrase
1.9 Other Experiments
We explored a number of other settings and fea-
tures, but did not observe any gains.
116
? Using HMM alignment instead of IBM
Model 4 leads to losses of ?.01 to ?.27.
? An earlier check of modified Moore?Lewis
filtering (see also below in Section 3) gave
very inconsistent results.
? Filtering the phrase table with significance
filtering (Johnson et al, 2007) leads to losses
of ?.19 to ?.63.
? Throwing out phrase pairs with direct transla-
tion probability ?(e?|f?) of less than 10?5 has
almost no effect.
? Double-checking the contribution of the
sparse lexical features in the final setup, we
observe an average losses of ?.07 when drop-
ping these features.
? For the German?English language pairs we
saw some benefits to using sparse lexical fea-
tures over POS tags instead of words, so we
used this in the final system.
1.10 Summary
We adopted a number of changes that improved
our baseline system by an average of +.30, see Ta-
ble 10 for a breakdown.
avg. method
+.01 factored backoff
+.09 kbest MIRA
+.11 sparse features and domain indicator
+.03 tuning with 25 iterations
?.03 maximum phrase length 5
+.02 unpruned 4-gram LM
+.07 translation table limit 100
+.30 total
Table 10: Summary of impact of changes
Minor improvements that we did not adopt was
avoiding reducing maximum phrase length to 5
(average +.03) and tuning with 1000-best lists
(+.02).
The improvements differed significantly by lan-
guage pair, as detailed in Table 11, with the
biggest gains for English?French (+.70), no gain
for English?German and no gain for English?
German.
1.11 New Data
The final experiment of the initial system devel-
opment phase was to train the systems on the new
data, adding newstest2011 to the tuning set (now
10,068 sentences). Table 12 reports the gains on
newstest2012 due to added data, indicating very
clearly that valuable new data resources became
available this year.
baseline improved ?
de-en 21.99 22.09 +.10
fr-en 30.00 30.46 +.46
es-en 30.42 30.79 +.37
cs-en 25.54 25.73 +.19
en-de 16.08 16.08 ?.00
en-fr 29.26 29.96 +.70
en-es 31.92 32.41 +.49
en-cs 17.38 17.55 +.17
Table 11: Overall improvements per language pair
WMT 2012 WMT 2013 ?
de-en 23.11 24.01 +0.90
fr-en 29.25 30.77 +1.52
es-en 32.80 33.99 +1.19
cs-en 22.53 22.86 +0.33
ru-en ? 31.67 ?
en-de 16.78 17.95 +1.17
en-fr 27.92 28.76 +0.84
en-es 33.41 34.00 +0.59
en-cs 15.51 15.78 +0.27
en-ru ? 23.78 ?
Table 12: Training with new data (newstest2012 scores)
2 Domain Adaptation Techniques
We explored two additional domain adaptation
techniques: phrase table interpolation and modi-
fied Moore-Lewis filtering.
2.1 Phrase Table Interpolation
We experimented with phrase-table interpolation
using perplexity minimisation (Foster et al, 2010;
Sennrich, 2012). In particular, we used the im-
plementation released with Sennrich (2012) and
available in Moses, comparing both the naive and
modified interpolation methods from that paper.
For each language pair, we took the alignments
created from all the data concatenated, built sepa-
rate phrase tables from each of the individual cor-
pora, and interpolated using each method. The re-
sults are shown in Table 13
baseline naive modified
fr-en 30.77 30.63 ?.14 ?
es-en? 33.98 33.83 ?.15 34.03 +.05
cs-en? 23.19 22.77 ?.42 23.03 ?.17
ru-en 31.67 31.42 ?.25 31.59 ?.08
en-fr 28.76 28.88 +.12 ?
en-es 34.00 34.07 +.07 34.31 +.31
en-cs 15.78 15.88 +.10 15.87 +.09
en-ru 23.78 23.84 +.06 23.68 ?.10
Table 13: Comparison of phrase-table interpolation (two
methods) with baseline (on newstest2012). The baselines are
as Table 12 except for the starred rows where tuning with
PRO was found to be better. The modified interpolation was
not possible in fr?en as it uses to much RAM.
The results from the phrase-table interpolation
are quite mixed, and we only used the technique
117
for the final system in en-es. An interpolation
based on PRO has recently been shown (Haddow,
2013) to improve on perplexity minimisation is
some cases, but the current implementation of this
method is limited to 2 phrase-tables, so we did not
use it in this evaluation.
2.2 Modified Moore-Lewis Filtering
In last year?s evaluation (Koehn and Haddow,
2012b) we had some success with modified
Moore-Lewis filtering (Moore and Lewis, 2010;
Axelrod et al, 2011) of the training data. This
year we conducted experiments in most of the lan-
guage pairs using MML filtering, and also exper-
imented using instance weighting (Mansour and
Ney, 2012) using the (exponential of) the MML
weights. The results are show in Table 14
base MML Inst. Wt Inst. Wt
line 20% (scale)
fr-en 30.77 ? ? ?
es-en? 33.98 34.26 +.28 33.85 ?.13 33.98 ?.00
cs-en? 23.19 22.62 ?.57 23.17 ?.02 23.13 ?.06
ru-en 31.67 31.58 ?.09 31.57 ?.10 31.62 ?.05
en-fr 28.67 28.74 +.07 28.81 +.17 28.63 ?.04
en-es 34.00 34.07 +.07 34.27 +.27 34.03 +.03
en-cs 15.78 15.37 ?.41 15.87 +.09 15.89 +.11
en-ru 23.78 22.90 ?.88 23.82 +.05 23.72 ?.06
Table 14: Comparison of MML filtering and weighting with
baseline. The MML uses monolingual news as in-domain,
and selects from all training data after alignment.The weight-
ing uses the MML weights, optionally downscaled by 10,
then exponentiated. Baselines are as Table 13.
As with phrase-table interpolation, MML filter-
ing and weighting shows a very mixed picture, and
not the consistent improvements these techniques
offer on IWSLT data. In the final systems, we used
MML filtering only for es-en.
3 Operation Sequence Model (OSM)
We enhanced the phrase segmentation and re-
ordering mechanism by integrating OSM: an op-
eration sequence N-gram-based translation and re-
ordering model (Durrani et al, 2011) into the
Moses phrase-based decoder. The model is based
on minimal translation units (MTUs) and Markov
chains over sequences of operations. An opera-
tion can be (a) to jointly generate a bi-language
MTU, composed from source and target words, or
(b) to perform reordering by inserting gaps and do-
ing jumps.
Model: Given a bilingual sentence pair <
F,E > and its alignment A, we transform it to
Figure 1: Bilingual Sentence with Alignments
sequence of operations (o1, o2, . . . , oJ ) and learn
a Markov model over this sequence as:
posm(F,E,A) = p(oJ1 ) =
J?
j=1
p(oj |oj?n+1, ..., oj?1)
By coupling reordering with lexical generation,
each (translation or reordering) decision condi-
tions on n ? 1 previous (translation and reorder-
ing) decisions spanning across phrasal boundaries
thus overcoming the problematic phrasal indepen-
dence assumption in the phrase-based model. In
the OSM model, the reordering decisions influ-
ence lexical selection and vice versa. Lexical gen-
eration is strongly coupled with reordering thus
improving the overall reordering mechanism.
We used the modified version of the OSM
model (Durrani et al, 2013b) that addition-
ally handles discontinuous and unaligned target
MTUs3. We borrow 4 count-based supportive fea-
tures, the Gap, Open Gap, Gap-width and Dele-
tion penalties from Durrani et al (2011).
Training: During training, each bilingual sen-
tence pair is deterministically converted to a
unique sequence of operations. Please refer to
Durrani et al (2011) for a list of operations and
the conversion algorithm and see Figure 1 and Ta-
ble 15 for a sample bilingual sentence pair and
its step-wise conversion into a sequence of oper-
ation. A 9-gram Kneser-Ney smoothed operation
sequence model is trained with SRILM.
Search: Although the OSM model is based on
minimal units, phrase-based search on top of OSM
model was found to be superior to the MTU-based
decoding in Durrani et al (2013a). Following this
framework allows us to use OSM model in tandem
with phrase-based models. We integrated the gen-
erative story of the OSM model into the hypothe-
sis extension of the phrase-based Moses decoder.
Please refer to (Durrani et al, 2013b) for details.
Results: Table 16 shows case-sensitive BLEU
scores on newstest2012 and newstest2013 for fi-
3In the original OSM model these are removed from the
alignments through a post-processing heuristic which hurts in
some language pairs. See Durrani et al (2013b) for detailed
experiments.
118
Operation Sequence Generation
Generate(Ich, I) Ich ?
I
Generate Target Only (do) Ich ?
I do
Insert Gap Ich nicht ?
Generate (nicht, not) I do not
Jump Back (1) Ich gehe ? nicht
Generate (gehe, go) I do not go
Generate Source Only (ja) Ich gehe ja ? nicht
I do not go
Jump Forward Ich gehe ja nicht ?
I do not go
Generate (zum, to the) . . . gehe ja nicht zum ?
. . . not go to the
Generate (haus, house) . . . ja nicht zum haus ?
. . . go to the house
Table 15: Step-wise Generation of Figure 1
LP Baseline +OSM
newstest 2012 2013 2012 2013
de-en 23.85 26.54 24.11 +.26 26.83 +.29
fr-en 30.77 31.09 30.96 +.19 31.46 +.37
es-en 34.02 30.04 34.51 +.49 30.94 +.90
cs-en 22.70 25.70 23.03 +.33 25.79 +.09
ru-en 31.87 24.00 32.33 +.46 24.33 +.33
en-de 17.95 20.06 18.02 +.07 20.26 +.20
en-fr 28.76 30.03 29.36 +.60 30.39 +.36
en-es 33.87 29.66 34.44 +.57 30.10 +.44
en-cs 15.81 18.35 16.16 +.35 18.62 +.27
en-ru 23.75 18.44 24.05 +.30 18.84 +.40
Table 16: Results using the OSM Feature
nal systems from Section 1 and these systems aug-
mented with the operation sequence model. The
model gives gains for all language pairs (BLEU
+.09 to +.90, average +.37, on newstest2013).
4 Huge Language Models
To overcome the memory limitations of SRILM,
we implemented modified Kneser-Ney (Kneser
and Ney, 1995; Chen and Goodman, 1998)
smoothing from scratch using disk-based stream-
ing algorithms. This open-source4 tool is de-
scribed fully by Heafield et al (2013). We used it
to estimate an unpruned 5?gram language model
on web pages from ClueWeb09.5 The corpus was
preprocessed by removing spam (Cormack et al,
2011), selecting English documents, splitting sen-
tences, deduplicating, tokenizing, and truecasing.
Estimation on the remaining 126 billion tokens
took 2.8 days on a single machine with 140 GB
RAM (of which 123 GB was used at peak) and six
hard drives in a RAID5 configuration. Statistics
about the resulting model are shown in Table 17.
4http://kheafield.com/code/
5http://lemurproject.org/clueweb09/
1 2 3 4 5
393m 3,775m 17,629m 39,919m 59,794m
Table 17: Counts of unique n-grams (m for millions) for the
5 orders in the unconstrained language model
The large language model was then quantized
to 10 bits and compressed to 643 GB with KenLM
(Heafield, 2011), loaded onto a machine with 1
TB RAM, and used as an additional feature in
unconstrained French?English, Spanish?English,
and Czech?English submissions. This additional
language model is the only difference between our
final constrained and unconstrained submissions;
no additional parallel data was used. Results are
shown in Table 18. Improvement from large lan-
guage models is not a new result (Brants et al,
2007); the primary contribution is estimating on a
single machine.
Constrained Unconstrained ?
fr-en 31.46 32.24 +.78
es-en 30.59 31.37 +.78
cs-en 27.38 28.16 +.78
ru-en 24.33 25.14 +.81
Table 18: Gain on newstest2013 from the unconstrained lan-
guage model. Our time on shared machines with 1 TB is
limited so Russian?English was run after the deadline and
German?English was not ready in time.
5 Summary
Table 19 breaks down the gains over the final sys-
tem from Section 1 from using the operation se-
quence models (OSM), modified Moore-Lewis fil-
tering (MML), fixing a bug with the sparse lex-
ical features (Sparse-Lex Bugfix), and instance
weighting (Instance Wt.), translation model com-
bination (TM-Combine), and use of the huge lan-
guage model (ClueWeb09 LM).
Acknowledgments
Thanks to Miles Osborne for preprocessing the ClueWeb09
corpus. The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme (FP7/2007-2013) under grant
agreement 287658 (EU BRIDGE) and grant agreement
288487(MosesCore).This work made use of the resources
provided by the Edinburgh Compute and Data Facility6.
The ECDF is partially supported by the eDIKT initia-
tive7. This work also used the Extreme Science and
Engineering Discovery Environment (XSEDE), which is
supported by National Science Foundation grant number
OCI-1053575. Specifically, Stampede was used under
allocation TG-CCR110017.
6http://www.ecdf.ed.ac.uk/
7http://www.edikt.org.uk/
119
System 2012 2013
Spanish-English
1. Baseline 34.02 30.04
2. 1+OSM 34.51 +.49 30.94 +.90
3. 1+MML (20%) 34.38 +.36 30.38 +.34
4. 1+Sparse-Lex Bugfix 34.17 +.15 30.33 +.29
5. 1+2+3: OSM+MML 34.65 +.63 30.51 +.47
6. 1+2+3+4 34.68 +.66 30.59 +.55
7. 6+ClueWeb09 LM 31.37 +1.33
English-Spanish
1. Baseline 33.87 29.66
2. 1+OSM 34.44 +.57 30.10 +.44
3. 1+TM-Combine 34.31 +44 29.76 +.10
4. 1+Instance Wt. 34.27 +.40 29.63 ?.03
5. 1+Sparse-Lex Bugfix 34.20 +.33 29.86 +.20
6. 1+2+3: OSM+TM-Cmb. 34.63 +.76 30.21 +.55
7. 1+2+4: OSM+Inst. Wt. 34.58 +.71 30.11 +.45
8. 1+2+3+5 34.78 +.91 30.43 +.77
Czech-English
1. Baseline 22.70 25.70
2. 1+OSM 23.03 +.33 25.79 +.09
3. 1+with PRO 23.19 +.49 26.08 +.38
4. 1+Sparse-Lex Bugfix 22.86 +.16 25.74 +.04
5. 1+OSM+PRO 23.42 +.72 26.23 +.53
6. 1+2+3+4 23.16 +.46 25.94 +.24
7. 5+ClueWeb09 LM 27.06 +.36
English-Czech
1. Baseline 15.85 18.35
2. 1+OSM 16.16 +.31 18.62 +.27
French-English
1. Baseline 30.77 31.09
2. 1+OSM 30.96 +.19 31.46 +.37
3. 2+ClueWeb09 LM 32.24 +1.15
English-French
1. Baseline 28.76 30.03
2. 1+OSM 29.36 +.60 30.39 +.36
3. 1+Sparse-Lex Bugfix 28.97 +.21 30.08 +.05
4. 1+2+3 29.37 +.61 30.58 +.55
German-English
1. Baseline 23.85 26.54
2. 1+OSM 24.11 +.26 26.83 +.29
English-German
1. Baseline 17.95 20.06
2. 1+OSM 18.02 +.07 20.26 +.20
Russian-English
1. Baseline 31.87 24.00
2. 1+OSM 32.33 +.46 24.33 +.33
English-Russian
1. Baseline 23.75 18.44
2. 1+OSM 24.05 +.40 18.84 +.40
Table 19: Summary of methods with BLEU scores on news-
test2012 and newstest2013. Bold systems were submitted,
with the ClueWeb09 LM systems submitted in the uncon-
straint track. The German?English and English?German
OSM systems did not complete in time for the official sub-
mission.
References
Axelrod, A., He, X., and Gao, J. (2011). Domain adaptation
via pseudo in-domain data selection. In Proceedings of the
2011 Conference on Empirical Methods in Natural Lan-
guage Processing, pages 355?362, Edinburgh, Scotland,
UK. Association for Computational Linguistics.
Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J.
(2007). Large language models in machine translation.
In Proceedings of the 2007 Joint Conference on Empir-
ical Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-CoNLL),
pages 858?867.
Callison-Burch, C., Koehn, P., Monz, C., Post, M., Soricut,
R., and Specia, L. (2012). Findings of the 2012 work-
shop on statistical machine translation. In Proceedings of
the Seventh Workshop on Statistical Machine Translation,
pages 10?48, Montreal, Canada. Association for Compu-
tational Linguistics.
Chen, S. and Goodman, J. (1998). An empirical study of
smoothing techniques for language modeling. Technical
Report TR-10-98, Harvard University.
Cherry, C. and Foster, G. (2012). Batch tuning strategies for
statistical machine translation. In Proceedings of the 2012
Conference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Language
Technologies, pages 427?436, Montre?al, Canada. Associ-
ation for Computational Linguistics.
Chiang, D., Knight, K., and Wang, W. (2009). 11,001 new
features for statistical machine translation. In Proceedings
of Human Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Association
for Computational Linguistics, pages 218?226, Boulder,
Colorado. Association for Computational Linguistics.
Collins, M., Koehn, P., and Kucerova, I. (2005). Clause
restructuring for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 531?540,
Ann Arbor, Michigan. Association for Computational Lin-
guistics.
Cormack, G. V., Smucker, M. D., and Clarke, C. L. (2011).
Efficient and effective spam filtering and re-ranking for
large web datasets. Information retrieval, 14(5):441?465.
Durrani, N., Fraser, A., and Schmid, H. (2013a). Model With
Minimal Translation Units, But Decode With Phrases. In
The 2013 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, Atlanta, Georgia, USA. Associ-
ation for Computational Linguistics.
Durrani, N., Fraser, A., Schmid, H., Hoang, H., and Koehn,
P. (2013b). Can Markov Models Over Minimal Transla-
tion Units Help Phrase-Based SMT? In Proceedings of
the 51st Annual Meeting of the Association for Computa-
tional Linguistics, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Durrani, N., Schmid, H., and Fraser, A. (2011). A Joint Se-
quence Translation Model with Integrated Reordering. In
Proceedings of the 49th Annual Meeting of the Association
for Computational Linguistics: Human Language Tech-
nologies, pages 1045?1054, Portland, Oregon, USA.
Foster, G., Goutte, C., and Kuhn, R. (2010). Discriminative
instance weighting for domain adaptation in statistical ma-
chine translation. In Proceedings of the 2010 Conference
on Empirical Methods in Natural Language Processing,
pages 451?459, Cambridge, MA. Association for Compu-
tational Linguistics.
120
Haddow, B. (2013). Applying pairwise ranked optimisation
to improve the interpolation of translation models. In Pro-
ceedings of the 2013 Conference of the North American
Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 342?347, Atlanta,
Georgia. Association for Computational Linguistics.
Haddow, B. and Koehn, P. (2012). Analysing the effect of
out-of-domain data on smt systems. In Proceedings of
the Seventh Workshop on Statistical Machine Translation,
pages 175?185, Montreal, Canada. Association for Com-
putational Linguistics.
Heafield, K. (2011). KenLM: Faster and smaller language
model queries. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 187?197, Edin-
burgh, Scotland. Association for Computational Linguis-
tics.
Heafield, K., Pouzyrevsky, I., Clark, J. H., and Koehn, P.
(2013). Scalable modified Kneser-Ney language model
estimation. In Proceedings of the 51st Annual Meeting of
the Association for Computational Linguistics, Sofia, Bul-
garia.
Johnson, H., Martin, J., Foster, G., and Kuhn, R. (2007).
Improving translation quality by discarding most of the
phrasetable. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 967?975.
Kneser, R. and Ney, H. (1995). Improved backing-off for
m-gram language modeling. In Proceedings of the IEEE
International Conference on Acoustics, Speech and Signal
Processing, pages 181?184.
Koehn, P. and Haddow, B. (2012a). Interpolated backoff for
factored translation models. In Proceedings of the Tenth
Conference of the Association for Machine Translation in
the Americas (AMTA).
Koehn, P. and Haddow, B. (2012b). Towards Effective Use of
Training Data in Statistical Machine Translation. In Pro-
ceedings of the Seventh Workshop on Statistical Machine
Translation, pages 317?321, Montre?al, Canada. Associa-
tion for Computational Linguistics.
Koehn, P. and Hoang, H. (2007). Factored Translation
Models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 868?876, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Koehn, P. and Knight, K. (2003). Empirical methods for com-
pound splitting. In Proceedings of Meeting of the Euro-
pean Chapter of the Association of Computational Lin-
guistics (EACL).
Mansour, S. and Ney, H. (2012). A Simple and Effec-
tive Weighted Phrase Extraction for Machine Translation
Adaptation. In Proceedings of IWSLT.
Moore, R. C. and Lewis, W. (2010). Intelligent selection of
language model training data. In Proceedings of the ACL
2010 Conference Short Papers, pages 220?224, Uppsala,
Sweden. Association for Computational Linguistics.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J. (2001).
BLEU: a method for automatic evaluation of machine
translation. Technical Report RC22176(W0109-022),
IBM Research Report.
Sennrich, R. (2012). Perplexity minimization for translation
model domain adaptation in statistical machine transla-
tion. In Proceedings of the 13th Conference of the Eu-
ropean Chapter of the Association for Computational Lin-
guistics, pages 539?549, Avignon, France. Association for
Computational Linguistics.
121
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 109?116,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Two Approaches to Correcting Homophone Confusions
in a Hybrid Machine Translation System
Pierrette Bouillon1, Johanna Gerlach1, Ulrich Germann2, Barry Haddow2, Manny Rayner1
(1) FTI/TIM, University of Geneva, Switzerland
{Pierrette.Bouillon,Johanna.Gerlach,Emmanuel.Rayner}@unige.ch
(2) School of Informatics, University of Edinburgh, Scotland
{ugermann,bhaddow}@inf.ed.ac.uk
Abstract
In the context of a hybrid French-to-
English SMT system for translating on-
line forum posts, we present two meth-
ods for addressing the common problem
of homophone confusions in colloquial
written language. The first is based on
hand-coded rules; the second on weighted
graphs derived from a large-scale pro-
nunciation resource, with weights trained
from a small bicorpus of domain language.
With automatic evaluation, the weighted
graph method yields an improvement of
about +0.63 BLEU points, while the rule-
based method scores about the same as the
baseline. On contrastive manual evalua-
tion, both methods give highly significant
improvements (p < 0.0001) and score
about equally when compared against each
other.
1 Introduction and motivation
The data used to train Statistical Machine Transla-
tion (SMT) systems is most often taken from the
proceedings of large multilingual organisations,
the generic example being the Europarl corpus
(Koehn, 2005); for academic evaluation exercises,
the test data may well also be taken from the same
source. Texts of this kind are carefully cleaned-up
formal language. However, real MT systems of-
ten need to handle text from very different genres,
which as usual causes problems.
This paper addresses a problem common in do-
mains containing informally written text: spelling
errors based on homophone confusions. Con-
cretely, the work reported was carried out in the
context of the ACCEPT project, which deals with
the increasingly important topic of translating on-
line forum posts; the experiments we describe
were performed using French data taken from the
Symantec forum, the concrete task being to trans-
late it into English. The language in these posts is
very far from that which appears in Hansard. Peo-
ple write quickly and carelessly, and no attempt is
made to clean up the results. In particular, spelling
is often uncertain.
One of the particular challenges in the task
considered here is that French has a high fre-
quency of homophones, which often cause confu-
sion in written language. Everyone who speaks
English is familiar with the fact that careless writ-
ers may confuse its (?of or belonging to it?) and
it?s (contraction of ?it is? or ?it has?). French has
the same problem, but to a much greater degree.
Even when someone is working in an environment
where an online spell-checker is available, it is
easy to write ou (?or?) instead of ou` (?where?),
la (?the-feminine?) instead of la` (?there?) or ce
(?this?) instead of se (?him/herself?). Even worse,
there is systematic homophony in verb-form end-
ings: for example, utiliser (?to use?) utilisez (?you
use?) and utilise? (?used?) are all homophones.
In French posts from the Symantec forum, we
find that between 10% and 15% of all sentences
contain at least one homophone error, depending
on exactly how the term is defined1. Substituting
a word with an incorrect homophone will often re-
sult in a translation error. Figure 1 shows typical
examples of homophone errors and their effect on
translation.
The core translation engine in our application
is a normal SMT system, bracketed between pre-
and post-editing phases. In what follows, we con-
trast two different approaches to handling homo-
phone errors, which involve pre-editing in dif-
ferent ways. The first approach is based on
knowledge-intensive construction of regular ex-
pression rules, which use the surrounding context
to correct the most frequent types of homophone
1Unclear cases include hyphenation, elison and some ex-
amples of missing or incorrect accents.
109
source automatic translation
original La sa ne pose pas de proble`me ... The its is not the issue ...
corrected La` c?a ne pose pas de proble`me ... Here it is not a problem
original ... (du moins on ne recoit pas l?alerte). ... (at least we do not recoit alert).
corrected ... (du moins on ne rec?oit pas l?alerte). .. (at least it does not receive the alert).
Figure 1: Examples of homophone errors in French forum data, contrasting English translations produced
by the SMT engine from plain and corrected versions.
confusions.
The second is an engineering method: we use a
commercial pronunciation-generation tool to gen-
erate a homophone dictionary, then use this dictio-
nary to turn the input into a weighted graph where
each word is replaced by a weighted disjunction of
homophones. Related, though less elaborate, work
has been reported by Bertoldi et al (2010), who
address spelling errors using a character-level con-
fusion network based on common character con-
fusions in typed English and test them on artifi-
cially created noisy data. Formiga and Fonollosa
(2012) also used character-based models to correct
spelling on informally written English data.
The two approaches in the present paper ex-
ploit fundamentally different knowledge sources
in trying to identify and correct homophone er-
rors. The rule-based method relies exclusively
on source-side information, encoding patterns in-
dicative of common French homophone confu-
sions. The weighted graph method shifts the bal-
ance to the target side; the choice between poten-
tial homophone alternatives is made primarily by
the target language model, though the source lan-
guage weights and the translation model are also
involved.
The rest of the paper is organised as follows.
Section 2 describes the basic framework in more
detail, and Section 3 the experiments. Section 4
summarises and concludes.
2 Basic framework
The goal of the ACCEPT project is to provide
easy cross-lingual access to posts in online fo-
rums. Given the large variety of possible techni-
cal topics and the limited supply of online gurus,
it frequently happens that users, searching forum
posts online, find that the answer they need is in a
language they do not know.
Currently available tools, for example Google
Translate, are of course a great deal better than
nothing, but still leave much to be desired. When
one considers that advice given in an online fo-
rum may not be easy to follow even for native lan-
guage speakers, it is unsurprising that a Google-
translated version often fails to be useful. There is
consequently strong motivation to develop an in-
frastructure explicitly designed to produce high-
quality translations. ACCEPT intends to achieve
this by a combination of three technologies: pre-
editing of the source; domain-tuned SMT; and
post-editing of the target. The pre- and post-
editing stages are performed partly using auto-
matic tools, and partly by manual intervention on
the part of the user communities which typically
grow up around online forums. We now briefly
describe the automatic parts of the system.
2.1 SMT engine and corpus data
The SMT engine used is a phrase-based system
trained with the standard Moses pipeline (Koehn
et al, 2007), using GIZA++ (Och and Ney,
2000) for word alignment and SRILM (Stolcke,
2002) for the estimation of 5-gram Kneser-Ney
smoothed (Kneser and Ney, 1995) language mod-
els.
For training the translation and lexicalised re-
ordering models we used the releases of europarl
and news-commentary provided for the WMT12
shared task (Callison-Burch et al, 2012), together
with a dataset from the ACCEPT project consist-
ing mainly of technical product manuals and mar-
keting materials.
For language modelling we used the target sides
of all the parallel data, together with approx-
imately 900 000 words of monolingual English
data extracted from web forums of the type that
we wish to translate. Separate language models
were trained on each of the data sets, then these
were linearly interpolated using SRILM to min-
imise perplexity on a heldout portion of the forum
data.
110
For tuning and testing, we extracted 1022 sen-
tences randomly from a collection of monolin-
gual French Symantec forum data (distinct from
the monolingual English forum data), translated
these using Google Translate, then post-edited
to create references. The post-editing was per-
formed by a native English speaker, who is also
fluent in French. This 1022-sentence parallel text
was then split into two equal halves (devtest a
and devtest b) for minimum error rate tuning
(MERT) and testing, respectively.
2.2 Rule-based pre-editing engine
Rule-based processing is carried out using the
Acrolinx engine (Bredenkamp et al, 2000), which
supports spelling, grammar, style and terminology
checking. These methods of pre-editing were orig-
inally designed to be applied by authors during the
technical documentation authoring process. The
author gets error markings and improvement sug-
gestions, and decides about reformulations. It is
also possible to apply the provided suggestions
automatically as direct reformulations. Rules are
written in a regular-expression-based formalism
which can access tagger-generated part-of-speech
information. The rule-writer can specify both pos-
itive evidence (patterns that will trigger applica-
tion of the rule) and negative evidence (patterns
that will block application).
3 Experiments
We compared the rule-based and weighted graph
approaches, evaluating each of them on the 511
sentence devtest b corpus. The baseline SMT
system, with no pre-editing, achieves an average
BLEU score of 42.47 on this set.
3.1 The rule-based approach
Under the ACCEPT project, a set of lightweight
pre-editing rules have been developed specifically
for the Symantec Forum translation task. Some
of the rules are automatic (direct reformulations);
others present the user with a set of suggestions.
The evaluations described in Gerlach et al (2013)
demonstrate that pre-editing with the rules has a
significant positive effect on the quality of SMT-
based translation.
The implemented rules address four main phe-
nomena: differences between informal and for-
mal language (Rayner et al, 2012), differences
between local French and English word-order, el-
lision/punctuation, and word confusions. Rules
for resolving homophone confusions belong to the
fourth group. They are shown in Table 1, together
with approximate frequencies of occurrence in the
development corpus.
Table 1: Hand-coded rules for homophone confu-
sions and per-sentence frequency of applicability
in the development corpus. Some of the rules also
cover non-homophone errors, so the frequency fig-
ures are slight overestimates as far as homophones
are concerned.
Rule Freq.
a/as/a` 4.17%
noun phrase agreement 3.20%
incorrect verb ending (er/e?/ez) 2.90%
missing hyphenation 2.08%
subject verb agreement 1.90%
missing elision 1.26%
du/du? 0.35%
la/la` 0.32%
ou/ou` 0.28%
ce/se 0.27%
Verb/noun 0.23%
tous/tout 0.22%
indicative/imperative 0.19%
future/conditional tense 0.14%
sur/su?r 0.10%
quel que/quelque 0.08%
ma/m?a 0.06%
quelle/qu?elle/quel/quels 0.05%
c?a/sa 0.04%
des/de`s 0.04%
et/est 0.02%
ci/si 0.01%
m?y/mi/mis 0.01%
other 0.17%
Total 18.09%
The set of Acrolinx pre-editing rules potentially
relevant to resolution of homophone errors was
applied to the devtest b set test corpus (Sec-
tion 2.1). In order to be able to make a fair com-
parison with the weighted-graph method, we only
used rules with a unique suggestion, which could
be run automatically. Applying these rules pro-
duced 430 changed words in the test corpus, but
did not change the average BLEU score signifi-
cantly (42.38).
Corrections made with a human in the loop,
used as ?oracle? input for the SMT system, by the
111
way, achieve an average BLEU score2 of 43.11 ?
roughly on par with the weighted-graph approach
described below.
3.2 The weighted graph approach
In our second approach, the basic idea is to trans-
form the input sentence into a confusion network
(Bertoldi et al, 2008) which presents the trans-
lation system with a weighted list of homophone
alternatives for each input word. The system is
free to choose a path through a network of words
that optimizes the internal hypothesis score; the
weighting scheme for the alternatives can be used
to guide the decoder. The conjecture is that the
combination of the confusion network weights, the
translation model and the target language model
can resolve homophone confusions.
3.2.1 Defining sets of confusable words
To compile lists of homophones, we used the com-
mercial Nuance Toolkit pronounce utility as
our source of French pronunciation information.
We began by extracting a list of all the lexical
items which occurred in the training portion of
the French Symantec forum data, giving us 30 565
words. We then ran pronounce over this list.
The Nuance utility does not simply perform table
lookups, but is capable of creating pronunciations
on the fly; it could in particular assign plausible
pronunciations to most of the misspellings that oc-
curred in the corpus. In general, a word is given
more than one possible pronunciation. This can be
for several reasons; in particular, some sounds in
French can systematically be pronounced in more
than one way, and pronunciation is often also de-
pendent on whether the word is followed by a con-
sonant or vowel. Table 2 shows examples.
Using the data taken from pronounce, we
grouped words together into clusters which have
a common pronunciation; since words typically
have more than one pronunciation, they will typi-
cally also belong to more than one cluster. We then
contructed sets of possible alternatives for words
by including, for each word W , all the words W ?
such that W and W ? occurred in the same cluster;
since careless French writing is also characterised
by mistakes in placing accents, we added all words
W ? such that W and W ? are identical up to drop-
ping accents. Table 3 shows typical results.
2With parameter sets from tuning the system on raw in-
put and input preprocessed with the fully automatic rules; cf.
Sec. 3.3.
Word Pronunciation
ans A?
A?z
pre?vu p r E v y
p r e v y
que?bec k e b E k
roule r u l
r u l *
Table 2: Examples of French pronunciations gen-
erated by pronounce. The format used is the
Nuance version of ARPABET.
Intuitively, it is in general unlikely that, on see-
ing a word which occurs frequently in the corpus,
we will want to hypothesize that it may be a mis-
spelling of one which occurs very infrequently.
We consequently filtered the sets of alternatives
to remove all words on the right whose frequency
was less than 0.05 times that of the word on the
left.
Table 3: Examples of sets of possible alternatives
for words, generated by considering both homo-
phone and accent confusions.
Word Alternatives
aux au aux haut
cre?er cre?er cre?ez cre?e? cre?e?e cre?e?es cre?e?s
co?te cote cote? co?te co?te? quot quote
ho?te haut haute ho?te ho?tes
il e elle elles il ils l le y
me`ne main mene? me`ne
nom nom noms non
ou ou ou`
saine sain saine saines sce`ne seine
traits trait traits tray tre tres tre`s
3.2.2 Setting confusion network weights
In a small series of preliminary experiments we
first tested three na??ve weighting schemes for the
confusion networks.
? using a uniform distribution that assigns
equal weight to all spelling alternatives;
? setting weights proportional to the unigram
probability of the word in question;
? computing the weights as state probabilities
in a trellis with the forward-backward algo-
rithm (Rabiner, 1989), an algorithm widely
112
Table 4: Decoder performance with different con-
fusion network weighting schemes.
weighting scheme av. BLEUa std.
none (baseline system) 42.47 ? .22
uniform 41.50 ? .37
unigram 41.58 ? .26
fwd-bwd (bigram) 41.81 ? .16
bigram context
(interpolated)
43.10 ? .32
aBased on muliple tuning runs with random parameter ini-
tializations.
used in speech recognition. Suppose that
each word w?i in the observed translation in-
put sentence is produced while the writer has
a particular ?true? word wi ? Ci in mind,
where Ci is the set of words confusable with
w?i. For the sake of simplicity, we assume that
within a confusion set, all ?true word? op-
tions are equally likely, i.e., p(w?i |wi = x) =
1
|Ci| for x ? Ci. The writer chooses the next
word wi+1 according to the conditional word
bigram probability p(wi+1 |wi).
The forward probability fwd i(x) is the prob-
ability of arriving in state wi = x at time
i, regardless of the sequence of states visited
en-route; the backward probability bwd i(x)
is the probability of arriving at the end of the
sentence coming from state wi = x, regard-
less of the path taken. These probabilities can
be computed efficiently with dynamic pro-
gramming.
The weight assigned to a particular ho-
mophone alternative x at position i in the
confusion network is the joint forward and
backward probability:
weight i(x) = fwd i(x) ? bwd i(x).
In practice, it turns out that these three na??ve
weighting schemes do more harm than good, as
the results in Table 4 show. Clearly, they rely too
much on overall language statistics (unigram and
bigram probabilities) and pay too little attention to
the actual input.
We therefore designed a fourth weighting
scheme (?bigram context interpolated?) that
gives more weight to the observed input and com-
putes the weights as the average of two score com-
ponents. The first is a binary feature function
that assigns 1 to each word actually observed in
the input, and 0 to its homophone alternatives.
The second component is the bigram-based in-
context probability of each candidate. Unlike the
forward-backward weighting scheme, which con-
siders all possible context words for each candi-
date (as specified in the respective confusion sets),
the new scheme only considers the words in the
actual input as context words.
It would have be desirable to keep the two score
components separate and tune their weights to-
gether with all the other parameters of the SMT
system. Unfortunately, the current implementa-
tion of confusion network-based decoding in the
Moses decoder allows only one single weight in
the specification of confusion networks, so that we
had to combine the two components into one score
before feeding the confusion network into the de-
coder.
With the improved weighting scheme, the con-
fusion network approach does outperform the
baseline system, giving an average BLEU of 43.10
(+0.63).
3.3 Automatic evaluation (BLEU)
Due to the relatively small size of the evalua-
tion set and instability inherent in minimum error
rate training (Foster and Kuhn, 2009; Clark et al,
2011), results of individual tuning and evaluation
runs can be unreliable. We therefore preformed
multiple tuning and evaluation runs for each sys-
tem (baseline, rule-based and weighted graph). To
illustrate the precision of the BLEU score on our
data sets, we plot in Fig. 2 for each individual tun-
ing run the BLEU score achieved on the tuning
set (x-axis) against the performance on the evalua-
tion set (y-axis). The variance along the x-axis for
each system is due to search errors in parameter
optimization. Since the search space is not con-
vex, the tuning process can get stuck in local max-
ima. The apparent poor local correlation between
performance on the tuning set and performance on
the evaluation set for each system shows the effect
of the sampling error.
With larger tuning and evaluation sets, we
would expect the correlation between the two
to improve. The scatter plot suggests that the
weighted-graph system does on average produce
significantly better translations (with respect to
BLEU) than both the baseline and the rule-based
system, whereas the difference between the base-
line and the rule-based system is within the range
113
41.6
41.8
42
42.2
42.4
42.6
42.8
43
43.2
43.4
43.6
43.8
46.4 46.6 46.8 47 47.2 47.4 47.6 47.8
B
LE
U
sc
o
re
o
n
ev
al
u
at
io
n
se
t
BLEU score on tuning set
BLEU scores on evaluation set
? ? .95 conf. int.
? baseline 42.47 .22 42.04?42.89
? rule-based 42.38 .23 41.94?42.83
+ weighted graph 43.10 .32 42.48?43.72
Figure 2: BLEU scores (in points) for the baseline, rule-based and weighted graph-based systems.
of statistical error.
To study the effect of tuning condition (tun-
ing on raw vs. input pre-processed by rules), we
also translated both the raw and the pre-processed
evaluation corpus with all parameter setting that
we had obtained during the various experiments.
Figure 3 plots (with solid markers) performance
on raw input (x-axis) against translation of pre-
processed input (y-axis). We observe that while
preprocessing harms performance for certain pa-
rameter settings, most of the time proprocessing
does lead to improvements in BLEU score. The
slight deterioration we observed when comparing
system tuned on exactly the type of input that they
were to translate later (i.e., raw or preprocessed)
seems to be a imprecision in the measurement
caused by training instability and sampling error
rather than the result of systematic input deterio-
ration due to preprocessing. Overall, the improve-
ments are small and not statistically significant,
but there appears to be a positive trend.
To gauge the benefits of more extensive pre-
processing and input error correction we produced
and translated ?oracle? input by also applying rules
from the Acrolinx engine that currently require a
human in the loop who decides whether or not the
rule in question should be applied. The boost in
performance is shown by the hollow markers in
Fig. 3. Here, translation of pre-processed input
consistently fares better than translation of the raw
input.
3.4 Human evaluation
Although BLEU suggests that the weighted-graph
method significantly outscores both the baseline
and the rule-based method (p < 0.05 over 25 tun-
ing runs), the absolute differences are small, and
we decided that it would be prudent to carry out a
human evaluation as well. Following the method-
ology of Rayner et al (2012), we performed con-
trastive judging on the Amazon Mechanical Turk
(AMT) to compare different versions of the sys-
tem. Subjects were recruited from Canada, a bilin-
gual French/English country, requesting English
native speakers with good written French; we also
limited the call to AMT workers who had already
completed at least 50 assignments, at least 80%
of which had been accepted. Judging assignments
were split into groups of 20 triplets, where each
triplet consisted of a source sentence and two dif-
ferent target sentences; the judge was asked to
say which translation was better, using a five-point
scale {better, slightly-better, about-equal, slightly-
worse, worse}. The order of the two targets was
114
 41.5
 42
 42.5
 43
 43.5
 41.6  41.8  42  42.2  42.4  42.6  42.8  43
BL
EU
 s
co
re
 o
n 
in
pu
t p
re
pr
oc
es
se
d 
by
 ru
le
s 
/ o
ra
cle
 in
pu
t
BLEU score on raw baseline input
baseline vs. oracle input; system tuned on baseline input
baseline vs. oracle input; system tuned on preprocessed input
baseline vs. rule-processed input; system tuned on baseline input
baseline vs. rule-processed input; system tuned on preprocessed input
threshold for improvement (above this line) vs. deterioration (below)
Figure 3: BLEU scores (in points) the two input conditions ?baseline? and ?rule-based? (solid markers).
The hollow markers show the BLEU score on human-corrected ?oracle? input using a more extensive set
of rules / suggestions from the Acrolinx engine that require a human in the loop.
randomised. Judges were paid $1 for each group
of 20 triplets. Each triplet was judged three times.
Using the above method, we posted AMT tasks
Table 5: Comparison between baseline, rule-based
and weighted-graph versions, evaluated on the
511-utterance devtest b corpus and judged by
three AMT-recruited judges. Figures are presented
both for majority voting and for unanimous deci-
sions only.
Majority Unanimous
baseline vs rule-based
baseline better 83 16.2% 48 9.4%
r-based better 204 40.0% 161 31.5%
Unclear 36 7.0% 93 18.1%
Equal 188 36.8% 209 40.9%
baseline vs weighted-graph
baseline better 115 22.5% 52 10.1%
w-graph better 193 37.8% 119 23.3%
Unclear 46 9.0% 99 19.4%
Equal 157 30.7% 241 47.2%
rule-based vs weighted-graph
r-based better 141 27.6% 68 13.3%
w-graph better 123 24.1% 70 13.7%
Unclear 25 4.9% 142 27.8%
Equal 222 43.4% 231 45.2%
to compare a) the baseline system against the
rule-based system, b) the baseline system against
the best weighted-graph system (interpolated-
bigram) from Section 3.2.2 and c) the rule-
based system and the weighted-graph system
against each other. The results are shown in
Table 5; in the second and third columns, dis-
agreements are resolved by majority voting, and
in the fourth and fifth we only count cases
where the judges are unanimous, the others be-
ing scored as unclear. In both cases, we re-
duce the original five-point scale to a three-point
scale {better, equal/unclear, worse}3. Irrespec-
tive of the method used to resolve disagreements,
the differences ?rule-based system/baseline? and
?weighted-graph system/baseline? are highly sig-
nificant (p < 0.0001) according to the McNe-
mar sign test, while the difference ?rule-based
system/weighted-graph system? is not significant.
We were somewhat puzzled that BLEU makes
the weighted-graph system clearly better than the
rule-based one, while manual evaluation rates
them as approximately equal. The explanation
seems to be to do with the fact that manual evalu-
ation operates at the sentence level, giving equal
importance to all sentences, while BLEU oper-
3For reasons we do not fully understand, we get better
inter-judge agreement this way than we do when we origi-
nally ask for judgements on a three-point scale.
115
ates at the word level and consequently counts
longer sentences as more important. If we calcu-
late BLEU on a per-sentence basis and then av-
erage the scores, we find that the results for the
two systems are nearly the same; per-sentence
BLEU differences also correlate reasonably well
with majority judgements (Pearson correlation co-
efficient of 0.39). It is unclear to us, however,
whether the difference between per-sentence and
per-word BLEU evaluation points to anything par-
ticularly interesting.
4 Conclusions
We have presented two methods for addressing
the common problem of homophone confusions in
colloquial written language in the context of an
SMT system. The weighted-graph method pro-
duced a small but significant increase in BLEU,
while the rule-based one was about the same as
the baseline. Both methods, however, gave clearly
significant improvements on contrastive manual
evaluation carried out through AMT, with no sig-
nificant difference in performance when the two
were compared directly.
The small but consistent improvements in
BLEU score that we observed with the human-
in-the-loop oracle input over the fully automatic
rule-based setup invite further investigation. How
many of the decisions currently left to the hu-
man can be automated? Is there a fair way of
comparing and evaluating fully automatic against
semi-automatic setups? Work on these topics is in
preparation and will be reported elsewhere.
Acknowledgements
The work described in this paper was performed
as part of the Seventh Framework Programme AC-
CEPT project, under grant agreement 288769.
References
Bertoldi, Nicola, Mauro Cettolo, and Marcello
Federico. 2010. ?Statistical machine translation
of texts with misspelled words.? NAACL. Los
Angeles, CA, USA.
Bertoldi, Nicola, Richard Zens, Marcello Fed-
erico, and Wade Shen. 2008. ?Efficient speech
translation through confusion network decod-
ing.? IEEE Transactions on Audio, Speech &
Language Processing, 16(8):1696?1705.
Bredenkamp, Andrew, Berthold Crysmann, and
Mirela Petrea. 2000. ?Looking for errors : A
declarative formalism for resource-adaptive lan-
guage checking.? LREC. Athens, Greece.
Callison-Burch, Chris, Philipp Koehn, Christof
Monz, et al (eds.). 2012. Seventh Workshop
on Statistical Machine Translation (WMT).
Montre?al, Canada.
Clark, Jonathan H., Chris Dyer, Alon Lavie, and
Noah A. Smith. 2011. ?Better hypothesis test-
ing for statistical machine translation: Control-
ling for optimizer instability.? ACL-HLT. Port-
land, OR, USA.
Formiga, Lluis and Jose? A. R. Fonollosa. 2012.
?Dealing with input noise in statistical machine
translation.? COLING. Mumbai, India.
Foster, George and Roland Kuhn. 2009. ?Sta-
bilizing minimum error rate training.? WMT.
Athens, Greece.
Gerlach, Johanna, Victoria Porro, Pierrette Bouil-
lon, and Sabine Lehmann. 2013. ?La pre?-
e?dition avec des re`gles peu cou?teuses, utile pour
la TA statistique?? TALN-RECITAL. Sables
d?Olonne, France.
Kneser, Reinhard and Hermann Ney. 1995. ?Im-
proved backing-off for m-gram language mod-
eling.? ICASSP. Detroit, MI, USA.
Koehn, Philipp. 2005. ?Europarl: A parallel cor-
pus for statistical machine translation.? MT
Summit X. Phuket, Thailand.
Koehn, Philipp, Hieu Hoang, Alexandra Birch,
et al 2007. ?Moses: Open source toolkit for
statistical machine translation.? ACL Demon-
stration Session. Prague, Czech Republic.
Och, Franz Josef and Hermann Ney. 2000. ?Im-
proved statistical alignment models.? ACL.
Hong Kong.
Rabiner, Lawrence R. 1989. ?A tutorial on hid-
den markov models and selected applications in
speech recognition.? Proceedings of the IEEE,
257?286.
Rayner, Manny, Pierrette Bouillon, and Barry
Haddow. 2012. ?Using source-language trans-
formations to address register mismatches in
SMT.? AMTA. San Diego, CA, USA.
Stolcke, Andreas. 2002. ?SRILM - an extensi-
ble language modeling toolkit.? ICSLP. Denver,
CO, USA.
116
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12?58,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Findings of the 2014 Workshop on Statistical Machine Translation
Ond
?
rej Bojar
Charles University in Prague
Christian Buck
University of Edinburgh
Christian Federmann
Microsoft Research
Barry Haddow
University of Edinburgh
Philipp Koehn
JHU / Edinburgh
Johannes Leveling
Dublin City University
Christof Monz
University of Amsterdam
Pavel Pecina
Charles University in Prague
Matt Post
Johns Hopkins University
Herve Saint-Amand
University of Edinburgh
Radu Soricut
Google
Lucia Specia
University of Sheffield
Ale
?
s Tamchyna
Charles University in Prague
Abstract
This paper presents the results of the
WMT14 shared tasks, which included a
standard news translation task, a sepa-
rate medical translation task, a task for
run-time estimation of machine translation
quality, and a metrics task. This year, 143
machine translation systems from 23 insti-
tutions were submitted to the ten transla-
tion directions in the standard translation
task. An additional 6 anonymized sys-
tems were included, and were then evalu-
ated both automatically and manually. The
quality estimation task had four subtasks,
with a total of 10 teams, submitting 57 en-
tries.
1 Introduction
We present the results of the shared tasks of
the Workshop on Statistical Machine Translation
(WMT) held at ACL 2014. This workshop builds
on eight previous WMT workshops (Koehn and
Monz, 2006; Callison-Burch et al., 2007, 2008,
2009, 2010, 2011, 2012; Bojar et al., 2013).
This year we conducted four official tasks: a
translation task, a quality estimation task, a met-
rics task
1
and a medical translation task. In the
translation task (?2), participants were asked to
translate a shared test set, optionally restricting
themselves to the provided training data. We held
ten translation tasks this year, between English and
each of Czech, French, German, Hindi, and Rus-
sian. The Hindi translation tasks were new this
year, providing a lesser resourced data condition
on a challenging language pair. The system out-
puts for each task were evaluated both automati-
cally and manually.
1
The metrics task is reported in a separate paper
(Mach?a?cek and Bojar, 2014).
The human evaluation (?3) involves asking
human judges to rank sentences output by
anonymized systems. We obtained large num-
bers of rankings from researchers who contributed
evaluations proportional to the number of tasks
they entered. Last year, we dramatically increased
the number of judgments, achieving much more
meaningful rankings. This year, we developed a
new ranking method that allows us to achieve the
same with fewer judgments.
The quality estimation task (?4) this year
included sentence- and word-level subtasks:
sentence-level prediction of 1-3 likert scores,
sentence-level prediction of percentage of word
edits necessary to fix a sentence, sentence-level
prediction of post-editing time, and word-level
prediction of scores at different levels of granular-
ity (correct/incorrect, accuracy/fluency errors, and
specific types of errors). Datasets were released
with English-Spanish, English-German, Spanish-
English and German-English news translations
produced by 2-3 machine translation systems and,
for some subtasks, a human translation.
The medical translation task (?5) was intro-
duced this year. Unlike the ?standard? translation
task, the test sets come from the very specialized
domain of medical texts. The aim of this task was
not only domain adaptation but also the utilization
of translation systems in a larger scenario, namely
cross-lingual information retrieval (IR). Extrinsic
evaluation in an IR setting was a part of this task
(on the other hand, manual evaluation of transla-
tion quality was not carried out).
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dis-
seminate common test sets and public training data
with published performance numbers, and to re-
fine evaluation and estimation methodologies for
machine translation. As before, all of the data,
12
translations, and collected human judgments are
publicly available.
2
We hope these datasets serve
as a valuable resource for research into statistical
machine translation and automatic evaluation or
prediction of translation quality.
2 Overview of the Translation Task
The recurring task of the workshop examines
translation between English and other languages.
As in the previous years, the other languages in-
clude German, French, Czech and Russian.
We dropped Spanish and added Hindi this year.
From a linguistic point of view, Spanish poses
similar problems as French, making its prior in-
clusion less valuable. Hindi is not only interest-
ing since it is a more distant language than the
European languages we include, but also because
we have much less training data, thus forcing re-
searchers to deal with low resource conditions, but
also providing them with a language pair that does
not suffer from the computational complexities of
having to deal with massive amounts of training
data.
We created a test set for each language pair by
translating newspaper articles and provided train-
ing data.
2.1 Test data
The test data for this year?s task was selected from
news stories from online sources, as before. How-
ever, we changed our method to create the test sets.
In previous years, we took equal amounts of
source sentences from all six languages involved
(around 500 sentences each), and translated them
into all other languages. While this produced a
multi-parallel test corpus that could be also used
for language pairs (such as Czech-Russian) that
we did not include in the evaluation, it did suf-
fer from artifacts from the larger distance between
source and target sentences. Most test sentences
involved the translation a source sentence that
was translated from a their language into a tar-
get sentence (which was compared against a trans-
lation from that third language as well). Ques-
tions have been raised, if the evaluation of, say,
French-English translation is best served when
testing on sentences that have been originally writ-
ten in, say, Czech. For discussions about trans-
lationese please for instance refer to Koppel and
Ordan (2011).
2
http://statmt.org/wmt14/results.html
This year, we took about 1500 English sen-
tences and translated them into the other 5 lan-
guages, and then additional 1500 sentences from
each of the other languages and translated them
into English. This gave us test sets of about 3000
sentences for our English-X language pairs, which
have been either written originally written in En-
glish and translated into X, or vice versa.
The composition of the test documents is shown
in Table 1. The stories were translated by the pro-
fessional translation agency Capita, funded by the
EU Framework Programme 7 project MosesCore,
and by Yandex, a Russian search engine com-
pany.
3
All of the translations were done directly,
and not via an intermediate language.
2.2 Training data
As in past years we provided parallel corpora
to train translation models, monolingual cor-
pora to train language models, and development
sets to tune system parameters. Some train-
ing corpora were identical from last year (Eu-
roparl
4
, United Nations, French-English 10
9
cor-
pus, CzEng, Common Crawl, Russian-English
Wikipedia Headlines provided by CMU), some
were updated (Russian-English parallel data pro-
vided by Yandex, News Commentary, monolin-
gual data), and a new corpus was added (Hindi-
English corpus, Bojar et al. (2014)), Hindi-English
Wikipedia Headline corpus).
Some statistics about the training materials are
given in Figure 1.
2.3 Submitted systems
We received 143 submissions from 23 institu-
tions. The participating institutions and their entry
names are listed in Table 2; each system did not
necessarily appear in all translation tasks. We also
included four commercial off-the-shelf MT sys-
tems and four online statistical MT systems, which
we anonymized.
For presentation of the results, systems are
treated as either constrained or unconstrained, de-
pending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial sys-
tems are treated as unconstrained during the auto-
matic and human evaluations.
3
http://www.yandex.com/
4
As of Fall 2011, the proceedings of the European Parlia-
ment are no longer translated into all official languages.
13
Europarl Parallel Corpus
French? English German? English Czech? English
Sentences 2,007,723 1,920,209 646,605
Words 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Parallel Corpus
French? English German? English Czech? English Russian? English
Sentences 183,251 201,288 146,549 165,602
Words 5,688,656 4,659,619 5,105,101 5,046,157 3,288,645 3,590,287 4,153,847 4,339,974
Distinct words 72,863 62,673 150,760 65,520 139,477 55,547 151,101 60,801
Common Crawl Parallel Corpus
French? English German? English Czech? English Russian? English
Sentences 3,244,152 2,399,123 161,838 878,386
Words 91,328,790 81,096,306 54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words 889,291 859,017 1,640,835 823,480 210,170 128,212 764,203 432,062
United Nations Parallel Corpus
French? English
Sentences 12,886,831
Words 411,916,781 360,341,450
Distinct words 565,553 666,077
10
9
Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Parallel Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Hindi-English Parallel Corpus
Hindi? English
Sentences 287,202
Words 6,002,418 3,953,851
Distinct words 121,236 105,330
Yandex 1M Parallel Corpus
Russian? English
Sentences 1,000,000
Words 24,121,459 26,107,293
Distinct words 701,809 387,646
Wiki Headlines Parallel Corpus
Russian? English Hindi? English
Sentences 514,859 32,863
Words 1,191,474 1,230,644 141,042 70,075
Distinct words 282,989 251,328 25,678 26,989
Europarl Language Model Data
English French German Czech
Sentence 2,218,201 2,190,579 2,176,537 668,595
Words 59,848,044 63,439,791 53,534,167 14,946,399
Distinct words 123,059 145,496 394,781 172,461
News Language Model Data
English French German Czech Russian Hindi
Sentence 90,209,983 30,451,749 89,634,193 36,426,900 32,245,651 1,275,921
Words 2,109,603,244 748,852,739 1,606,506,785 602,950,410 575,423,682 36,297,394
Distinct words 4,089,792 1,906,470 10,248,707 3,101,846 2,860,837 258,759
News Test Set
French? English German? English Czech? English Russian? English Hindi? English
Sentences 3003 3003 3003 3003 2507
Words 81,194 71,147 63,078 67,624 60,240 68,866 62,107 69,329 86,974 55,822
Distinct words 11,715 10,610 13,930 10,458 16,774 9,893 17,009 9,938 8,292 9,217
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
14
Language Sources (Number of Documents)
Czech aktu?aln?e.cz (2), blesk.cz (3), blisty.cz (1), den??k.cz (9), e15.cz (1), iDNES.cz (17), ihned.cz (14), lidovky.cz (8), medi-
afax.cz (2), metro.cz (1), Novinky.cz (5), pravo.novinky.cz (6), reflex.cz (2), tyden.cz (1), zdn.cz (1).
French BBC French Africa (1), Canoe (9), Croix (4), Cyber Presse (12), Dernieres Nouvelles (1), dhnet.be (5), Equipe (1),
Euronews (6), Journal Metro.com (1), La Libre.be (2), La Meuse.be (2), Le Devoir (3), Le Figaro (8), Le Monde (3),
Les Echos (15), Lexpress.fr (3), Liberation (1), L?independant (2), Metro France (1), Nice-Matin (6), Le Nouvel Ob-
servateur (3), Radio Canada (6), Reuters (7).
English ABC News (5), BBC (5), CBS News (5), CNN (5), Daily Mail (5), Financial Times (5), Fox News (2), Globe and
Mail (1), Independent (1), Los Angeles Times (1), New Yorker (1), News.com Australia (16), Reuters (3), Scotsman (2),
smh.com.au (2), stv.tv (1), Telegraph (6), UPI (2).
German Abendzeitung N?urnberg (1), all-in.de (2), Augsburger Allgemeine (1), AZ Online (1), B?orsenzeitung (1), come-
on.de (1), Der Westen (2), DZ Online (1), Reutlinger General-Anzeiger (1), Generalanzeiger Bonn (1), Giessener
Anzeiger (1), Goslarsche Zeitung (1), Hersfelder Zeitung (1), J?udische Allgemeine (1), Kreisanzeiger (2),
Kreiszeitung (2), Krone (1), Lampertheimer Zeitung (2), Lausitzer Rundschau (1), Mittelbayerische (1), Morgen-
post (1), nachrichten.at (1), Neue Presse (1), OP Online (1), Potsdamer Neueste Nachrichten (1), Passauer Neue
Presse (1), Recklingh?auser Zeitung (1), Rhein Zeitung (1), salzburg.com (1), Schwarzw?alder Bote (29), Segeberger
Zeitung (1), Soester Anzeiger (1), S?udkurier (17), svz.de (1), Tagesspiegel (1), Usinger Anzeiger (3), Volksblatt.li (1),
Westf?alischen Anzeiger (3), Wiener Zeitung (1), Wiesbadener Kurier (1), Westdeutsche Zeitung (1), Wilhelmshavener
Zeitung (1), Yahoo Deutschland (1).
Hindi Bhaskar (24), Jagran (61), Navbharat Times / India Times (4), ndtv (2).
Russian 168.ru (1), aif (3), altapress.ru (2), argumenti.ru (2), BBC Russian (3), belta.by (2), communa.ru (1), dp.ru (1), eg-
online.ru (1), Euronews (2), fakty.ua (2), gazeta.ru (1), inotv.rt.com (1), interfax (1), Izvestiya (1), Kommersant (7),
kp (2), lenta.ru (4), lgng (1), litrossia.ru (1), mirnov.ru (5), mk (8), mn.ru (2), newizv (2), nov-pravda.ru (1), no-
vayagazeta (1), nr2.ru (8), pnp.ru (1), rbc.ru (3), ria.ru (4), rosbalt.ru (1), sovsport.ru (6), Sport Express (10), trud.ru (4),
tumentoday.ru (1), vesti.ru (10), zr.ru (1).
Table 1: Composition of the test set. For more details see the XML test files. The docid tag gives the source and the date for
each document in the test set, and the origlang tag indicates the original source language.
3 Human Evaluation
As with past workshops, we contend that auto-
matic measures of machine translation quality are
an imperfect substitute for human assessments.
We therefore conduct a manual evaluation of the
system outputs and define its results to be the prin-
cipal ranking of the workshop. In this section, we
describe how we collected this data and compute
the results, and then present the official results of
the ranking.
This year?s evaluation was conducted a bit dif-
ferently. The main differences are:
? In contrast to the past two years, we collected
judgments entirely from researchers partici-
pating in the shared tasks and trusted friends
of the community. Last year, about two thirds
of the data were solicited from random volun-
teers on the Amazon Mechanical Turk. For
some language pairs, the Turkers data had
much lower inter-annotator agreement com-
pared to the researchers.
? As a result, we collected about seventy-five
percent less data, but were able to obtain
good confidence intervals on the clusters with
the use of new approaches to ranking.
? We compared three different ranking method-
ologies, selecting the one with the highest ac-
curacy on held-out data.
We also maintain many of our customs from
prior years, including the presentation of the re-
sults in terms of a partial ordering (clustering) of
the systems. Systems in the same cluster could not
be meaningfully distinguished and should be con-
sidered ties.
3.1 Data collection
The system ranking is produced from a large set of
pairwise annotations between system pairs. These
pairwise annotations are collected in an evaluation
campaign that enlists participants in the shared
task to contribute one hundred ?Human Intelli-
gence Tasks? (HITs) per system submitted. Each
HIT consists of three ranking tasks. In a rank-
ing task, an annotator is presented with a source
segment, a human reference translation, and the
outputs of five anonymized systems, randomly se-
lected from the set of participating systems, and
randomly ordered.
To run the evaluation, we use Appraise
5
(Fe-
dermann, 2012), an open-source tool built on
Python?s Django framework. At the top of each
HIT, the following instructions are provided:
You are shown a source sentence fol-
lowed by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
5
https://github.com/cfedermann/Appraise
15
ID Institution
AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014)
CIMS University of Stuttgart / University of Munich (Cap et al., 2014)
CMU Carnegie Mellon University (Matthews et al., 2014)
CU-* Charles University, Prague (Tamchyna et al., 2014)
DCU-FDA Dublin City University (Bicici et al., 2014)
DCU-ICTCAS Dublin City University (Li et al., 2014b)
DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014)
EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014)
KIT Karlsruhe Institute of Technology (Herrmann et al., 2014)
IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014)
IIIT-HYDERABAD IIIT Hyderabad
IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014)
IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014)
KAZNU Amandyk Kartbayev, FBK
LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014)
MANAWI-* Universit?at des Saarlandes (Tan and Pal, 2014)
MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014)
PROMT-RULE,
PROMT-HYBRID
PROMT
RWTH RWTH Aachen (Peitz et al., 2014)
STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014)
UA-* University of Alicante (S?anchez-Cartagena et al., 2014)
UEDIN-PHRASE,
UEDIN-UNCNSTR
University of Edinburgh (Durrani et al., 2014b)
UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014)
UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014)
YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014)
COMMERCIAL-[1,2] Two commercial machine translation systems
ONLINE-[A,B,C,G] Four online statistical machine translation systems
RBMT-[1,4] Two rule-based statistical machine translation systems
Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
16
Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a
source segment, a reference translation, and the outputs of five systems (anonymized and randomly ordered), and is asked to
rank these according to their translation quality, with ties allowed.
A screenshot of the ranking interface is shown in
Figure 2. Annotators are asked to rank the sys-
tems from 1 (best) to 5 (worst), with ties permit-
ted. Note that a lower rank is better. The rankings
provided by a ranking task are then reduced to a
set of ten pairwise rankings produced by consider-
ing all
(
5
2
)
combinations of systems in the ranking
task. For example, consider the following annota-
tion provided among systems A,B, F,H , and J :
1 2 3 4 5
F ?
A ?
B ?
J ?
H ?
This is reduced to the following set of pairwise
judgments:
A > B,A = F,A > H,A < J
B < F,B < H,B < J
F > H,F < J
H < J
Here,A > B should be read is ?A is ranked higher
than (worse than) B?. Note that by this procedure,
the absolute value of ranks and the magnitude of
their differences are discarded.
For WMT13, nearly a million pairwise anno-
tations were collected from both researchers and
paid workers on Amazon?s Mechanical Turk, in
a roughly 1:2 ratio. This year, we collected data
from researchers only, an ability that was enabled
by the use of a new technique for producing the
partial ranking for each task (?3.3.3). Table 3 con-
tains more detail.
3.2 Annotator agreement
Each year we calculate annotator agreement
scores for the human evaluation as a measure of
the reliability of the rankings. We measured pair-
wise agreement among annotators using Cohen?s
kappa coefficient (?) (Cohen, 1960). If P (A) be
the proportion of times that the annotators agree,
and P (E) is the proportion of time that they would
17
LANGUAGE PAIR Systems Rankings Average
Czech?English 5 21,130 4,226.0
English?Czech 10 55,900 5,590.0
German?English 13 25,260 1,943.0
English?German 18 54,660 3,036.6
French?English 8 26,090 3,261.2
English?French 13 33,350 2,565.3
Russian?English 13 34,460 2,650.7
English?Russian 9 28,960 3,217.7
Hindi?English 9 20,900 2,322.2
English?Hindi 12 28,120 2,343.3
TOTAL WMT 14 110 328,830 2,989.3
WMT13 148 942,840 6,370.5
WMT12 103 101,969 999.6
WMT11 133 63,045 474.0
Table 3: Amount of data collected in the WMT14 manual evaluation. The final three rows report summary information from
the previous two workshops.
agree by chance, then Cohen?s kappa is:
? =
P (A)? P (E)
1? P (E)
Note that ? is basically a normalized version of
P (A), one which takes into account how mean-
ingful it is for annotators to agree with each other
by incorporating P (E). The values for ? range
from 0 to 1, with zero indicating no agreement and
1 perfect agreement.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A < B, A = B, or A > B. In
other words, P (A) is the empirical, observed rate
at which annotators agree, in the context of pair-
wise comparisons.
As for P (E), it captures the probability that two
annotators would agree randomly. Therefore:
P (E) = P (A<B)
2
+ P (A=B)
2
+ P (A>B)
2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is
computed empirically, by observing how often an-
notators actually rank two systems as being tied.
Table 4 gives ? values for inter-annotator agree-
ment for WMT11?WMT14 while Table 5 de-
tails intra-annotator agreement scores, including
the division of researchers (WMT13
r
) and MTurk
(WMT13
m
) data. The exact interpretation of the
kappa coefficient is difficult, but according to Lan-
dis and Koch (1977), 0?0.2 is slight, 0.2?0.4 is
fair, 0.4?0.6 is moderate, 0.6?0.8 is substantial,
and 0.8?1.0 is almost perfect. The agreement rates
are more or less in line with prior years: worse for
some tasks, better for others, and on average, the
best since WMT11 (where agreement scores were
likely inflated due to inclusion of reference trans-
lations in the comparisons).
3.3 Models of System Rankings
The collected pairwise rankings are used to pro-
duce a ranking of the systems. Machine transla-
tion evaluation has always been a subject of con-
tention, and no exception to this rule exists for the
WMT manual evaluation. While the precise met-
ric has varied over the years, it has always shared
a common idea of computing the average num-
ber of times each system was judged better than
other systems, and ranking from highest to low-
est. For example, in WMT11 Callison-Burch et al.
(2011), the metric computed the percentage of the
time each system was ranked better than or equal
to other systems, and included comparisons to hu-
man references. In WMT12 Callison-Burch et al.
(2012), comparisons to references were dropped.
In WMT13, rankings were produced over 1,000
bootstrap-resampled sets of the training data. A
rank range was collected for each system across
these folds; the average value was used to order
the systems, and a 95% confidence interval across
these ranks was used to organize the systems into
equivalence classes containing systems with over-
18
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13
r
WMT13
m
WMT14
Czech?English 0.400 0.311 0.244 0.342 0.279 0.305
English?Czech 0.460 0.359 0.168 0.408 0.075 0.360
German?English 0.324 0.385 0.299 0.443 0.324 0.368
English?German 0.378 0.356 0.267 0.457 0.239 0.427
French?English 0.402 0.272 0.275 0.405 0.321 0.357
English?French 0.406 0.296 0.231 0.434 0.237 0.302
Hindi?English ? ? ? ? ? 0.400
English?Hindi ? ? ? ? ? 0.413
Russian?English ? ? 0.278 0.315 0.324 0.324
English?Russian ? ? 0.243 0.416 0.207 0.418
MEAN 0.395 0.330 0.260 0.367
Table 4: ? scores measuring inter-annotator agreement. See Table 5 for corresponding intra-annotator agreement scores.
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13
r
WMT13
m
WMT14
Czech?English 0.597 0.454 0.479 0.483 0.478 0.382
English?Czech 0.601 0.390 0.290 0.547 0.242 0.448
German?English 0.576 0.392 0.535 0.643 0.515 0.344
English?German 0.528 0.433 0.498 0.649 0.452 0.576
French?English 0.673 0.360 0.578 0.585 0.565 0.629
English?French 0.524 0.414 0.495 0.630 0.486 0.507
Hindi?English ? ? ? ? ? 0.605
English?Hindi ? ? ? ? ? 0.535
Russian?English ? ? 0.450 0.363 0.477 0.629
English?Russian ? ? 0.513 0.582 0.500 0.570
MEAN 0.583 0.407 0.479 0.522
Table 5: ? scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the
human evaluation.
lapping ranges.
This year, we introduce two new changes. First,
we pit the WMT13 method against two new ap-
proaches: that of Hopkins and May (2013, ?3.3.2),
and another based on TrueSkill (Sakaguchi et al.,
2014, ?3.3.3). Second, we compare these two
methods against WMT13?s ?Expected Wins? ap-
proach, and then select among them by determin-
ing which of them has the highest accuracy in
terms of predicting annotations on a held-out set
of pairwise judgments.
3.3.1 Method 1: Expected Wins (EW)
Introduced for WMT13, the EXPECTED WINS has
an intuitive score demonstrated to be accurate in
ranking systems according to an underlying model
of ?relative ability? (Koehn, 2012a). The idea is
to gauge the probability that a system S
i
will be
ranked better than another system randomly cho-
sen from a pool of opponents {S
j
: j 6= i}. If
we define the function win(A,B) as the number
of times system A is ranked better than system B,
then we can define this as follows:
score
EW
(S
i
) =
1
|{S
j
}|
?
j,j 6=i
win(S
i
, S
j
)
win(S
i
, S
j
) + win(S
j
, S
i
)
Note that this score ignores ties.
3.3.2 Method 2: Hopkins and May (HM)
Hopkins and May (2013) introduced a graphical
model formulation of the task, which makes the
notion of underlying system ability even more ex-
plicit. Each system S
J
in the pool {S
j
} is repre-
sented by an associated relative ability ?
j
and a
variance ?
2
a
(fixed across all systems) which serve
as the parameters of a Gaussian distribution. Sam-
ples from this distribution represent the quality
of sentence translations, with higher quality sam-
ples having higher values. Pairwise annotations
(S
1
, S
2
, pi) are generated according to the follow-
ing process:
19
1. Select two systems S
1
and S
2
from the pool
of systems {S
j
}
2. Draw two ?translations?, adding random
Gaussian noise with variance ?
2
obs
to simulate
the subjectivity of the task and the differences
among annotators:
q
1
? N (?
S
1
, ?
2
a
) +N (0, ?
2
obs
)
q
2
? N (?
S
2
, ?
2
a
) +N (0, ?
2
obs
)
3. Let d be a nonzero real number that defines
a fixed decision radius. Produce a rating pi
according to:
pi =
?
?
?
< q
1
? q
2
> d
> q
2
? q
1
> d
= otherwise
Hopkins and May use Gibbs sampling to infer
the set of system means from an annotated dataset.
Details of this inference procedure can be found in
Sakaguchi et al. (2014). The score used to produce
the rankings is simply the system mean associated
with each system:
score
HM
(S
i
) = ?
S
i
3.3.3 Method 3: TrueSkill (TS)
TrueSkill is an adaptive, online system that em-
ploys a similar model of relative ability Herbrich
et al. (2006). It was initially developed for Xbox
Live?s online player community, where it is used
to model player ability, assign levels, and select
competitive matches. Each player S
j
is modeled
by two parameters: TrueSkill?s current estimate
of each system?s relative ability, ?
S
j
, and a per-
system measure of TrueSkill?s uncertainty of those
estimates, ?
2
S
j
. When the outcome of a match is
observed, TrueSkill uses the relative status of the
two systems to update these estimates. If a trans-
lation from a system with a high mean is judged
better than a system with a greatly lower mean, the
result is not surprising, and the update size for the
corresponding system means will be small. On the
other hand, when an upset occurs in a competition,
the means will receive larger updates. Sakaguchi
et al. (2014) provide an adaptation of this approach
to the WMT manual evaluation, and showed that
it performed well on WMT13 data.
Similar to the Hopkins and May model,
TrueSkill scores systems by their inferred means:
score
TS
(S
i
) = ?
S
i
This score is then used to sort the systems and pro-
duce the ranking.
3.4 Method Selection
We have three methods which, provided with the
collected data, produce different rankings of the
systems. Which of them is correct? More imme-
diately, which one of them should we publish as
the official ranking for the WMT14 manual eval-
uation? As discussed, the method used to com-
pute the ranking has been tweaked a bit each year
over the past few years in response to criticisms
(e.g., Lopez (2012); Bojar et al. (2011)). While the
changes were reasonable (and later corroborated),
Hopkins and May (2013) pointed out that this task
of model selection should be driven by empirical
evaluation on held-out data, and suggested per-
plexity as the metric of choice.
We choose instead a more direct gold-standard
evaluation metric: the accuracy of the rankings
produced by each method in predicting pairwise
judgments. We use each method to produce a par-
tial ordering of the systems, grouping them into
equivalence classes. This partial ordering unam-
biguously assigns a prediction pi
P
between any
pair of systems (S
i
, S
j
). By comparing the pre-
dicted relationship pi
P
to the actual annotation for
each pairwise judgment in the test data (by token),
we can compute an accuracy score for each model.
We predict accuracy in this manner using 100-
fold cross-validation. For each task, we split the
data into a fixed set of 100 randomly-selected
folds. Each fold serves as a test set, with the
remaining ninety-nine folds available as training
data for each method. Note that the total order-
ing over systems provided by the score
?
functions
defined do not predict ties. In order to do enable
the models to predict ties, we produce equivalence
classes using the following procedure:
? Assign S
1
to a cluster
? For each system S
i
, assign it to the current
cluster if score(S
i?1
) ? score(S
i
) ? r; oth-
erwise, assign it to a new cluster
The value of r (the decision radius for ties)
is tuned using accuracy on the entire training
data using grid search over the values r ?
0, 0.01, 0.02, . . . , .25 (26 values in total). This
value is tuned separately for each method on each
fold. Table 6 contains an example partial ordering.
20
System Score Rank
B 0.60 1
D 0.44 2
E 0.39 2
A 0.25 2
F -0.09 3
C -0.22 3
Table 6: The partial ordering computed with the provided
scores when r = 0.15.
Task EW HM TS Oracle
Czech?English 40.4 41.1 41.1 41.2
English?Czech 45.3 45.6 45.9 46.8
French?English 49.0 49.4 49.3 50.3
English?French 44.6 44.4 44.7 46.0
German?English 43.5 43.7 43.7 45.2
English?German 47.3 47.4 47.2 48.2
Hindi?English 62.5 62.2 62.5 62.6
English?Hindi 53.3 53.7 53.5 55.7
Russian?English 47.6 47.7 47.7 50.6
English?Russian 46.5 46.1 46.4 48.2
MEAN 48.0 48.1 48.2 49.2
Table 7: Accuracies for each method across 100 folds, for
each translation task. The oracle uses the most frequent out-
come between each pair of systems, and therefore might not
constitute a feasible ranking.
After training, each model has defined a partial
ordering over systems.
6
This is then used to com-
pute accuracy on all the pairwise judgments in the
test fold. This process yields 100 accuracies for
each method; the average accuracy across all the
folds can then be used to compute the best method.
Table 7 contains accuracy results for the three
methods on the WMT14 tasks. On average, there
is a small improvement in accuracy moving from
Expected Wins to the H&M model, and then again
to the TrueSkill model; however, there is no pat-
tern to the best model for each class. The Oracle
column is computed by selecting the most prob-
able outcome (pi ? {<,=, >}) for each system
pair, and provides an upper bound on accuracy
when predicting outcomes using only system-level
information. Furthermore, this method of oracle
computation might not represent a feasible rank-
ing or clustering,
7
.
The TrueSkill approach was best overall, so we
used it to produce the official rankings for all lan-
6
It is a total ordering when r = 0, or when all the system
scores are outside the decision radius.
7
For example, if there were a cycle of ?better than? judg-
ments among a set of systems.
guage pairs.
3.5 Rank Ranges and Clusters
Above we saw how to produce system scores for
each method, which provides a total ordering of
the systems. But we would also like to know if the
obtained system ranking is statistically significant.
Given the large number of systems that participate,
and the similarity of the underlying systems result-
ing from the common training data condition and
(often) toolsets, there will be some systems that
will be very close in quality. These systems should
be grouped together in equivalence classes.
To establish the reliability of the obtained sys-
tem ranking, we use bootstrap resampling. We
sample from the set of pairwise rankings an equal
sized set of pairwise rankings (allowing for multi-
ple drawings of the same pairwise ranking), com-
pute a TrueSkill model score for each system
based on this sample, and then rank the systems
from 1..|{S
j
}|. By repeating this procedure 1,000
times, we can determine a range of ranks, into
which system falls at least 95% of the time (i.e.,
at least 950 times) ? corresponding to a p-level
of p ? 0.05. Furthermore, given the rank ranges
for each system, we can cluster systems with over-
lapping rank ranges.
8
Table 8 reports all system scores, rank ranges,
and clusters for all language pairs and all systems.
The official interpretation of these results is that
systems in the same cluster are considered tied.
Given the large number of judgments that we col-
lected, it was possible to group on average about
two systems in a cluster, even though the systems
in the middle are typically in larger clusters.
3.6 Cluster analysis
The official ranking results for English-German
produced clusters compute at the 90% confidence
level due to the presence of a very large cluster
(of nine systems). While there is always the pos-
sibility that this cluster reflects a true ambiguity, it
is more likely due to the fact that we didn?t have
enough data: English?German had the most sys-
8
Formally, given ranges defined by start(S
i
) and end(S
i
),
we seek the largest set of clusters {C
c
} that satisfies:
?S ?C : S ? C
S ? C
a
, S ? C
b
? C
a
= C
b
C
a
6= C
b
? ?S
i
? C
a
, S
j
? C
b
:
start(S
i
) > end(S
j
) or start(S
j
) > end(S
i
)
21
Czech?English
# score range system
1 0.591 1 ONLINE-B
2 0.290 2 UEDIN-PHRASE
3 -0.171 3-4 UEDIN-SYNTAX
-0.243 3-4 ONLINE-A
4 -0.468 5 CU-MOSES
English?Czech
# score range system
1 0.371 1-3 CU-DEPFIX
0.356 1-3 UEDIN-UNCNSTR
0.333 1-4 CU-BOJAR
0.287 3-4 CU-FUNKY
2 0.169 5-6 ONLINE-B
0.113 5-6 UEDIN-PHRASE
3 0.030 7 ONLINE-A
4 -0.175 8 CU-TECTO
5 -0.534 9 COMMERCIAL1
6 -0.950 10 COMMERCIAL2
Russian?English
# score range system
1 0.583 1 AFRL-PE
2 0.299 2 ONLINE-B
3 0.190 3-5 ONLINE-A
0.178 3-5 PROMT-HYBRID
0.123 4-7 PROMT-RULE
0.104 5-8 UEDIN-PHRASE
0.069 5-8 YANDEX
0.066 5-8 ONLINE-G
4 -0.017 9 AFRL
5 -0.159 10 UEDIN-SYNTAX
6 -0.306 11 KAZNU
7 -0.487 12 RBMT1
8 -0.642 13 RBMT4
English?Russian
# score range system
1 0.575 1-2 PROMT-RULE
0.547 1-2 ONLINE-B
2 0.426 3 PROMT-HYBRID
3 0.305 4-5 UEDIN-UNCNSTR
0.231 4-5 ONLINE-G
4 0.089 6-7 ONLINE-A
0.031 6-7 UEDIN-PHRASE
5 -0.920 8 RBMT4
6 -1.284 9 RBMT1
German?English
# score range system
1 0.451 1 ONLINE-B
2 0.267 2-3 UEDIN-SYNTAX
0.258 2-3 ONLINE-A
3 0.147 4-6 LIMSI-KIT
0.146 4-6 UEDIN-PHRASE
0.138 4-6 EU-BRIDGE
4 0.026 7-8 KIT
-0.049 7-8 RWTH
5 -0.125 9-11 DCU-ICTCAS
-0.157 9-11 CMU
-0.192 9-11 RBMT4
6 -0.306 12 RBMT1
7 -0.604 13 ONLINE-C
French?English
# score range system
1 0.608 1 UEDIN-PHRASE
2 0.479 2-4 KIT
0.475 2-4 ONLINE-B
0.428 2-4 STANFORD
3 0.331 5 ONLINE-A
4 -0.389 6 RBMT1
5 -0.648 7 RBMT4
6 -1.284 8 ONLINE-C
English?French
# score range system
1 0.327 1 ONLINE-B
2 0.232 2-4 UEDIN-PHRASE
0.194 2-5 KIT
0.185 2-5 MATRAN
0.142 4-6 MATRAN-RULES
0.120 4-6 ONLINE-A
3 0.003 7-9 UU-DOCENT
-0.019 7-10 PROMT-HYBRID
-0.033 7-10 UA
-0.069 8-10 PROMT-RULE
4 -0.215 11 RBMT1
5 -0.328 12 RBMT4
6 -0.540 13 ONLINE-C
English?German
# score range system
1 0.264 1-2 UEDIN-SYNTAX
0.242 1-2 ONLINE-B
2 0.167 3-6 ONLINE-A
0.156 3-6 PROMT-HYBRID
0.155 3-6 PROMT-RULE
0.155 3-6 UEDIN-STANFORD
3 0.094 7 EU-BRIDGE
4 0.033 8-10 RBMT4
0.031 8-10 UEDIN-PHRASE
0.012 8-10 RBMT1
5 -0.032 11-12 KIT
-0.069 11-13 STANFORD-UNC
-0.100 12-14 CIMS
-0.126 13-15 STANFORD
-0.158 14-16 UU
-0.191 15-16 ONLINE-C
6 -0.307 17-18 IMS-TTT
-0.325 17-18 UU-DOCENT
Hindi?English
# score range system
1 1.326 1 ONLINE-B
2 0.559 2-3 ONLINE-A
0.476 2-4 UEDIN-SYNTAX
0.434 3-4 CMU
3 0.323 5 UEDIN-PHRASE
4 -0.198 6-7 AFRL
-0.280 6-7 IIT-BOMBAY
5 -0.549 8 DCU-LINGO24
6 -2.092 9 IIIT-HYDERABAD
English?Hindi
# score range system
1 1.008 1 ONLINE-B
2 0.915 2 ONLINE-A
3 0.214 3 UEDIN-UNCNSTR
4 0.120 4-5 UEDIN-PHRASE
0.054 4-5 CU-MOSES
5 -0.111 6-7 IIT-BOMBAY
-0.142 6-7 IPN-UPV-CNTXT
6 -0.233 8-9 DCU-LINGO24
-0.261 8-9 IPN-UPV-NODEV
7 -0.449 10-11 MANAWI-H1
-0.494 10-11 MANAWI
8 -0.622 12 MANAWI-RMOOV
Table 8: Official results for the WMT14 translation task. Systems are ordered by their inferred system means. Lines between
systems indicate clusters according to bootstrap resampling at p-level p ? .05, except for English?German, where p ? 0.1.
This method is also used to determine the range of ranks into which system falls. Systems with grey background indicate use
of resources that fall outside the constraints provided for the shared task.
22
tems (18, compared to 13 for the next languages),
yet only an average amount of per-system data.
Here, we look at this language pair in more detail,
in order to justify this decision, and to shed light
on the differences between the ranking methods.
Table 9 presents the 95% confidence-level clus-
terings for English?German computed with each
of the three methods, along with lines that show
the reorderings of the systems between them. Re-
orderings of this type have been used to argue
against the reliability of the official WMT rank-
ing (Lopez, 2012; Hopkins and May, 2013). This
table shows that these reorderings are captured en-
tirely by the clustering approach we used. This rel-
ative consensus of these independently-computed
and somewhat different models suggests that the
published ranking is approaching the true ambigu-
ity underlying systems within the same cluster.
Looking across all language pairs, we find that
the total ordering predicted by EW and TS is ex-
actly the same for eight of the ten language pair
tasks, and is constrained to reorderings within
the official cluster for the other two (German?
English ? just one adjacent swap ? and English?
German, depicted in Table 9).
3.7 Conclusions
The official ranking method employed by WMT
over the past few years has changed a few times as
a result of error analysis and introspection. Until
this year, these results were largely based on the
intuitions of the community and organizers about
deficiencies in the models. In addition to their in-
tuitive appeal, many of these changes (such as the
decision to throw out comparisons against refer-
ences) have been empirically validated Hopkins
and May (2013). The actual effect of the refine-
ments in the ranking metric has been minor pertur-
bations in the permutation of systems. The cluster-
ing method of Koehn (2012b), in which the official
rankings are presented as a partial (instead of to-
tal) ordering, alleviated many of the problems ob-
served by Lopez (2012), and also capture all the
variance across the new systems introduced this
year. In addition, presenting systems as clusters
appeals to intuition. As such, we disagree with
claims that there is a problem with irreproducibil-
ity of the results of the workshop evaluation task,
and especially disagree that there is anything ap-
proaching a ?crisis of confidence? (Hopkins and
May, 2013). These claims seem to us to be over-
stated.
Conducting proper model selection by compar-
ison on held-out data, however, is a welcome sug-
gestion, and our inclusion of this process supports
improved confidence in the ranking results. That
said, it is notable that the different methods com-
pute very similar orderings. This avoids hallu-
cinating distinctions among systems that are not
really there, and captures the intuition that some
systems are basically equivalent. The chief ben-
efit of the TrueSkill model is not in outputting a
better complete ranking of the systems, but lies in
its reduced variance, which allow us to cluster the
systems with less data. There is also the unex-
plored avenue of using TrueSkill to drive the data
collection, steering the annotations of judges to-
wards evenly matched systems during the collec-
tion phase, potentially allowing confident results
to be presented while collecting even less data.
There is, of course, more work to be done.
We have produced this year statistically significant
clusters with a third of the data required last year,
which is an improvement. Models of relative abil-
ity are a natural fit for the manual evaluation, and
the introduction of an online Bayesian approach
to data collection present further opportunities to
reduce the amount of data needed. These methods
also provide a framework for extending the models
in a variety of potentially useful ways, including
modeling annotator bias, incorporating sentence
metadata (such as length, difficulty, or subtopic),
and adding features of the sentence pairs.
4 Quality Estimation Task
Machine translation quality estimation is the task
of predicting a quality score for a machine trans-
lated text without access to reference translations.
The most common approach is to treat the problem
as a supervised machine learning task, using stan-
dard regression or classification algorithms. The
third edition of the WMT shared task on qual-
ity estimation builds on the previous editions of
the task (Callison-Burch et al., 2012; Bojar et al.,
2013), with tasks including both sentence-level
and word-level estimation, with new training and
test datasets.
The goals of this year?s shared task were:
? To investigate the effectiveness of different
quality labels.
? To explore word-level quality prediction at
23
Expected Wins Hopkins & May TrueSkill
UEDIN-SYNTAX UEDIN-SYNTAX UEDIN-SYNTAX
ONLINE-B ONLINE-B ONLINE-B
ONLINE-A UEDIN-STANFORD ONLINE-A
UEDIN-STANFORD PROMT-HYBRID PROMT-HYBRID
PROMT-RULE ONLINE-A PROMT-RULE
PROMT-HYBRID PROMT-RULE UEDIN-STANFORD
EU-BRIDGE EU-BRIDGE EU-BRIDGE
RBMT4 UEDIN-PHRASE RBMT4
UEDIN-PHRASE RBMT4 UEDIN-PHRASE
RBMT1 RBMT1 RBMT1
KIT KIT KIT
STANFORD-UNC STANFORD-UNC STANFORD-UNC
CIMS CIMS CIMS
STANFORD STANFORD STANFORD
UU UU UU
ONLINE-C ONLINE-C ONLINE-C
IMS-TTT UU-DOCENT IMS-TTT
UU-DOCENT IMS-TTT UU-DOCENT
Table 9: A comparison of the rankings produced by Expected Wins, Hopkins & May, and TrueSkill for English?German (the
task with the most systems and the largest cluster). The lines extending all the way across mark the official English?German
clustering (computed from TrueSkill with 90% confidence intervals), while bold entries mark the start of new clusters within
each method or column (computed at the 95% confidence level). The TrueSkill clusterings contain all the system reorderings
across the other two ranking methods.
different levels of granularity.
? To study the effects of training and test
datasets with mixed domains, language pairs
and MT systems.
? To examine the effectiveness of quality pre-
diction methods on human translations.
Four tasks were proposed: Tasks 1.1, 1.2, 1.3
are defined at the sentence-level (Sections 4.1),
while Task 2, at the word-level (Section 4.2). Each
task provides one or more datasets with up to four
language pairs each: English-Spanish, English-
German, German-English, Spanish-English, and
up to four alternative translations generated by:
a statistical MT system (SMT), a rule-based MT
system (RBMT), a hybrid MT system, and a hu-
man. These datasets were annotated with differ-
ent labels for quality by professional translators as
part of the QTLaunchPad
9
project. External re-
sources (e.g. parallel corpora) were provided to
participants. Any additional resources, including
additional quality estimation training data, could
9
http://www.qt21.eu/launchpad/
be used by participants (no distinction between
open and close tracks is made). Participants were
also provided with a software package to extract
quality estimation features and perform model
learning, with a suggested list of baseline features
and learning method for sentence-level prediction.
Participants, described in Section 4.3, could sub-
mit up to two systems for each task.
Data used for building specific MT systems or
internal system information (such as n-best lists)
were not made available this year as multiple MT
systems were used to produced the datasets, in-
cluding rule-based systems. In addition, part of
the translations were produced by humans. Infor-
mation on the sources of translations was not pro-
vided either. Therefore, as a general rule, partici-
pants were only allowed to use black-box features.
4.1 Sentence-level Quality Estimation
For the sentence-level tasks, two variants of the
results could be submitted for each task and lan-
guage pair:
? Scoring: An absolute quality score for each
sentence translation according to the type of
24
prediction, to be interpreted as an error met-
ric: lower scores mean better translations.
? Ranking: A ranking of sentence translations
for all source test sentences from best to
worst. For this variant, it does not matter how
the ranking is produced (from HTER predic-
tions, likert predictions, or even without ma-
chine learning).
Evaluation was performed against the true label
and/or HTER ranking using the same metrics as in
previous years:
? Scoring: Mean Average Error (MAE) (pri-
mary metric), Root Mean Squared Error
(RMSE).
? Ranking: DeltaAvg (primary metric) (Bojar
et al., 2013) and Spearman?s rank correlation.
For all sentence-level these tasks, the same 17
features as in WMT12-13 were used to build base-
line systems. The SVM regression algorithm
within QUEST (Specia et al., 2013)
10
was applied
for that with RBF kernel and grid search for pa-
rameter optimisation.
Task 1.1 Predicting post-editing effort
Data in this task is labelled with discrete and
absolute scores for perceived post-editing effort,
where:
? 1 = Perfect translation, no post-editing
needed at all.
? 2 = Near miss translation: translation con-
tains maximum of 2-3 errors, and possibly
additional errors that can be easily fixed (cap-
italisation, punctuation, etc.).
? 3 = Very low quality translation, cannot be
easily fixed.
The datasets were annotated in a ?triage? phase
aimed at selecting translations of type ?2? (near
miss) that could be annotated for errors at the
word-level using the MQM metric (see Task 2, be-
low) for a more fine-grained and systematic trans-
lation quality analysis. Word-level errors in trans-
lations of type ?3? are too difficult if not impos-
sible to annotate and classify, particularly as they
often contain inter-related errors in contiguous or
overlapping word spans.
10
http://www.quest.dcs.shef.ac.uk/
For the training of prediction models, we pro-
vide a new dataset consisting of source sen-
tences and their human translations, as well as
two-three versions of machine translations (by an
SMT system, an RBMT system and, for English-
Spanish/German only, a hybrid system), all in the
news domain, extracted from tests sets of various
WMT years and MT systems that participated in
the translation shared task:
# Source sentences # Target sentences
954 English 3,816 Spanish
350 English 1,400 German
350 German 1,050 English
350 Spanish 1,050 English
As test data, for each language pair and MT sys-
tem (or human translation) we provide a new set
of translations produced by the same MT systems
(and humans) as those used for the training data:
# Source sentences # Target sentences
150 English 600 Spanish
150 English 600 German
150 German 450 English
150 Spanish 450 English
The distribution of true scores in both training
and test sets for each language pair is given in Fig-
ures 3.
0%#
10%#
20%#
30%#
40%#
50%#
60%#
{en-
de-1
}#
{en-
de-2
}#
{en-
de-3
}#
{de-
en-1
}#
{de-
en-2
}#
{de-
en-3
}#
{en-
es-1
}#
{en-
es-2
}##
{en-
es-3
}##
{es-
en-1
}#
{es-
en-2
}#
{es-
en-3
}#
#Training##### #Test####
Figure 3: Distribution of true 1-3 scores by langauge pair.
Additionally, we provide some out of domain
test data. These translations were annotated in
the same way as above, each dataset by one Lan-
guage Service Provider (LSP), i.e, one profes-
sional translator, with two LPSs producing data in-
dependently for English-Spanish. They were gen-
erated using the LSPs? own source data (a different
domain from news), and own MT system (differ-
ent from the three used for the official datasets).
The results on these datasets were not considered
25
for the official ranking of the participating sys-
tems:
# Source sentences # Target sentences
971 English 971 Spanish
297 English 297 German
388 Spanish 388 English
Task 1.2 Predicting percentage of edits
In this task we use HTER (Snover et al., 2006) as
quality score. This score is to be interpreted as
the minimum edit distance between the machine
translation and its manually post-edited version,
and its range is [0, 1] (0 when no edit needs to
be made, and 1 when all words need to be edited).
We used TERp (default settings: tokenised, case
insensitive, etc., but capped to 1)
11
to compute the
HTER scores.
For practical reasons, the data is a subset of
Task 1.1?s dataset: only translations produced
by the SMT system English-Spanish. As train-
ing data, we provide 896 English-Spanish trans-
lation suggestions and their post-editions. As
test data, we provide a new set of 208 English-
Spanish translations produced by the same SMT
system. Each of the training and test translations
was post-edited by a professional translator using
the CASMACAT
12
web-based tool, which also col-
lects post-editing time on a sentence-basis.
Task 1.3 Predicting post-editing time
For this task systems are required to produce, for
each translation, a real valued estimate of the time
(in milliseconds) it takes a translator to post-edit
the translation. The training and test sets are a sub-
set of that uses in Task 1.2 (subject to filtering of
outliers). The difference is that the labels are now
the number of milliseconds that were necessary to
post-edit each translation.
As training data, we provide 650 English-
Spanish translation suggestions and their post-
editions. As test data, we provide a new set of 208
English-Spanish translations (same test data as for
Task 1.2).
4.2 Word-level Quality Estimation
The data for this task is based on a subset of the
datasets used for Task 1.1, for all language pairs,
11
http://www.umiacs.umd.edu/
?
snover/terp/
12
http://casmacat.eu/
human and machine translations: those transla-
tions labelled ?2? (near misses), plus additional
data provided by industry (either on the news do-
main or on other domains, such as technical doc-
umentation, produced using their own MT sys-
tems, and also pre-labelled as ?2?). All seg-
ments were annotated with word-level labels by
professional translators using the core categories
in MQM (Multidimensional Quality Metrics)
13
as
error typology (see Figure 4). Each word or se-
quence of words was annotated with a single error.
For (supposedly rare) cases where a decision be-
tween multiple fine-grained error types could not
be made, annotators were requested to choose a
coarser error category in the hierarchy.
Participants are asked to produce a label for
each token that indicates quality at different lev-
els of granularity:
? Binary classification: an OK / bad label,
where bad indicates the need for editing the
token.
? Level 1 classification: an OK / accuracy /
fluency label, specifying coarser level cate-
gories of errors for each token, or ?OK? for
tokens with no error.
? Multi-class classification: one of the labels
specifying the error type for the token (termi-
nology, mistranslation, missing word, etc.) in
Figure 4, or ?OK? for tokens with no error.
As training data, we provide tokenised transla-
tion output for all language pairs, human and ma-
chine translations, with tokens annotated with all
issue types listed above, or ?OK?. The annotation
was performed manually by professional transla-
tors as part of the QTLaunchPad project. For
the coarser variants, fine-grained errors are gen-
eralised to Accuracy or Fluency, or ?bad? for the
binary variant. The amount of available training
data varies by language pair:
# Source sentences # Target sentences
1,957 English 1,957 Spanish
715 English 715 German
350 German 350 English
900 Spanish 900 English
13
http://www.qt21.eu/launchpad/content/
training
26
Figure 4: MQM metric as error typology.
As test data, we provide additional data points
for all language pairs, human and machine trans-
lations:
# Source sentences # Target sentences
382 English 382 Spanish
150 English 150 German
100 German 100 English
150 Spanish 150 English
In contrast to Tasks 1.1?1.3, no baseline feature
set is provided to the participants.
Similar to last year (Bojar et al., 2013), the
word-level task is primarily evaluated by macro-
averaged F-measure (in %). Because the class dis-
tribution is skewed ? in the test data about 78% of
the tokens are marked as ?OK? ? we compute pre-
cision, recall, and F
1
for each class individually,
weighting F
1
scores by the frequency of the class
in the test data. This avoids giving undue impor-
tance to less frequent classes. Consider the follow-
ing confusion matrix for Level 1 annotation, i.e.
the three classes (O)K, (F)luency, and (A)ccuracy:
reference
O F A
predicted
O 4172 1482 193
F 1819 1333 214
A 198 133 69
For each of the three classes we assume a binary
setting (one-vs-all) and derive true-positive (tp),
false-positive (fp), and false-negative (fn) counts
from the rows and columns of the confusion ma-
trix as follows:
tp
O
= 4172
fp
O
= 1482 + 193 = 1675
fn
O
= 1819 + 198 = 2017
tp
F
= 1333
fp
F
= 1819 + 214 = 2033
fn
F
= 1482 + 133 = 1615
tp
A
= 69
fp
A
= 198 + 133 = 331
fn
A
= 193 + 214 = 407
We continue to compute F
1
scores for each
class c ? {O,F,A}:
precision
c
= tp
c
/(tp
c
+ fp
c
)
recall
c
= tp
c
/(tp
c
+ fn
c
)
F
1,c
=
2 ? precision
c
? recall
c
precision
c
+recall
c
yielding:
precision
O
= 4172/(4172 + 1675) = 0.7135
recall
O
= 4172/(4172 + 2017) = 0.6741
F
1,O
=
2 ? 0.7135 ? 0.6741
0.7135 + 0.6741
= 0.6932
? ? ?
F
1,F
= 0.4222
F
1,A
= 0.1575
Finally, we compute the average of F
1,c
scores
weighted by the occurrence count N(c) of c:
weightedF
1,ALL
=
1
?
c
N(c)
?
c
N
c
? F
1,c
weightedF
1,ERR
=
1
?
c:c 6=O
N(c)
?
c:c 6=O
N
c
? F
1,c
27
which for the above example gives:
weightedF
1,ALL
=
1
6189 + 2948 + 476
?
(6189 ? 0.6932 + 2948 ? 0.4222
+476 ? 0.1575) = 0.5836
weightedF
1,ERR
=
1
2948 + 476
?
(2948 ? 0.4222 + 476 ? 0.1575)
= 0.3854
We choose F
1,ERR
as our primary evaluation mea-
sure because it most closely mimics the common
application of F
1
scores in binary classification:
one is interested in the performance in detecting a
positive class, which in this case would be erro-
neous words. This does, however, ignore the num-
ber of correctly classified words of the OK class,
which is why we also report F
1,ALL
. In addition,
we follow Powers (2011) and report Matthews
Correlation Coefficient (MCC), averaged in the
same way as F
1
, as our secondary metric. Finally,
for contrast we also report Accuracy (ACC).
4.3 Participants
Table 10 lists all participating teams. Each team
was allowed up to two submissions for each task
and language pair. In the descriptions below, par-
ticipation in specific tasks is denoted by a task
identifier: T1.1, T1.2, T1.3, and T2.
Sentence-level baseline system (T1.1, T1.2,
T1.3): QUEST is used to extract 17 system-
independent features from source and trans-
lation sentences and parallel corpora (same
features as in the WMT12 shared task):
? number of tokens in the source and tar-
get sentences.
? average source token length.
? average number of occurrences of the
target word within the target sentence.
? number of punctuation marks in source
and target sentences.
? language model (LM) probability of
source and target sentences based on
models for the WMT News Commen-
tary corpus.
? average number of translations per
source word in the sentence as given by
IBM Model 1 extracted from the WMT
News Commentary parallel corpus, and
thresholded so that P (t|s) > 0.2, or
so that P (t|s) > 0.01 weighted by the
inverse frequency of each word in the
source side of the parallel corpus.
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower
frequency words) and 4 (higher fre-
quency words) in the source language
extracted from the WMT News Com-
mentary corpus.
? percentage of unigrams in the source
sentence seen in the source side of the
WMT News Commentary corpus.
These features are used to train a Support
Vector Machine (SVM) regression algorithm
using a radial basis function kernel within
the SCIKIT-LEARN toolkit. The ?,  and C
parameters were optimised via grid search
with 5-fold cross validation on the training
set. We note that although the system is re-
ferred to as ?baseline?, it is in fact a strong
system. It has proved robust across a range
of language pairs, MT systems, and text do-
mains for predicting various forms of post-
editing effort (Callison-Burch et al., 2012;
Bojar et al., 2013).
DCU (T1.1): DCU-MIXED and DCU-SVR use
a selection of features available in QUEST,
such as punctuation statistics, LM perplex-
ity, n-gram frequency quartile statistics and
coarse-grained POS frequency ratios, and
four additional feature types: combined POS
and stop word LM features, source-side
pseudo-reference features, inverse glass-box
features for translating the translation and er-
ror grammar parsing features. For machine
learning, the QUEST framework is expanded
to combine logistic regression and support
vector regression and to handle cross- valida-
tion and randomisation in a way that training
items with the same source side are kept to-
gether. External resources are monolingual
corpora taken from the WMT 2014 transla-
tion task for LMs, the MT system used for the
inverse glass-box features (Li et al., 2014b)
and, for error grammar parsing, the Penn-
Treebank and an error grammar derived from
it (Foster, 2007).
28
ID Participating team
DCU Dublin City University Team 1, Ireland (Hokamp et al., 2014)
DFKI German Research Centre for Artificial Intelligence, Germany (Avramidis,
2014)
FBK-UPV-UEDIN Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia,
Spain & University of Edinburgh, UK (Camargo de Souza et al., 2014)
LIG Laboratoire d?Informatique Grenoble, France (Luong et al., 2014)
LIMSI Laboratoire d?Informatique pour la M?ecanique et les Sciences de l?Ing?enieur,
France (Wisniewski et al., 2014)
MULTILIZER Multilizer, Finland
RTM-DCU Dublin City University Team 2, Ireland (Bicici and Way, 2014)
SHEF-lite University of Sheffield Team 1, UK (Beck et al., 2014)
USHEFF University of Sheffield Team 2, UK (Scarton and Specia, 2014)
YANDEX Yandex, Russia
Table 10: Participants in the WMT14 Quality Estimation shared task.
DFKI (T1.2): DFKI/SVR builds upon the base-
line system (above) by adding non-redundant
data from the WMT13 task for predicting
the same label (HTER) and additional fea-
tures such as (a) rule-based language cor-
rections (language tool) (b), PCFG parsing
statistics and counts of tree labels, (c) po-
sition statistics of parsing labels, (d) posi-
tion statistics of trigrams with low probabil-
ity. DFKI/SVRxdata uses a similar setting,
with the addition of more training data from
non-minimally post-edited translation out-
puts (references), filtered based on a thresh-
old on the edit distance between the MT out-
put and the freely-translated reference.
FBK-UPV-UEDIN (T1.2, T1.3, T2): The sub-
missions for the word-level task (T2) use fea-
tures extracted from word posterior probabil-
ities and confusion network descriptors com-
puted over the 100k-best hypothesis transla-
tions generated by a phrase-based SMT sys-
tem. They also use features from word lexi-
cons, and POS tags of each word for source
and translation sentences. The predictions of
the Binary model are used as a feature for the
Level 1 and Multi-class settings. Both condi-
tional random fields (CRF) and bidirectional
long short-term memory recurrent neural net-
works (BLSTM-RNNs) are used for the Bi-
nary setting, and BLSTM-RNNs only for the
Level 1 and Multi-class settings.
The sentence-level QE submissions (T1.2
and T1.3) are trained on black-box features
extracted using QUEST in addition to fea-
tures based on word alignments, word poste-
rior probabilities and diversity scores (Souza
et al., 2013). These features are computed
over 100k-best hypothesis translations also
used for task 2. In addition, a set of ratios
computed from the word-level predictions of
the model trained on the binary setting of
task 2 is used. A total of 221 features and
the extremely randomised trees (Geurts et al.,
2006) learning algorithm are used to train re-
gression models.
LIG (T2): Conditional Random Fields classi-
fiers are trained with features used in LIG?s
WMT13 systems (Luong et al., 2013): tar-
get and source words, alignment informa-
tion, source and target alignment context,
LM scores, target and source POS tags,
lexical categorisations (stopword, punctua-
tion, proper name, numerical), constituent
label, depth in the constituent tree, target
polysemy count, pseudo reference. These
are combined with novel features: word
occurrence in multiple translation systems
and POS tag-based LM scores (longest tar-
get/source n-gram length and backoff score
for POS tag). These features require external
NLP tools and resources such as: TreeTag-
ger, GIZA++, Bekerley parser, Link Gram-
mar parser, WordNet and BabelNet, Google
Translate (pseudo-reference). For the binary
task, the optimal classification threshold is
tuned based on a development set split from
the original training set. Feature selection is
employed over the all features (for the binary
29
task only), with the Sequential Backward Se-
lection algorithm. The best performing fea-
ture set is then also used for the Level 1 and
Multi-class variants.
LIMSI (T2): The submission relies on a ran-
dom forest classifier and considers only 16
dense and continuous features. To prevent
sparsity issues, lexicalised information such
as the word or the previous word identities
is not included. The features considered are
mostly classic MT features and can be cat-
egorised into two classes: association fea-
tures, which describe the quality of the as-
sociation between the source sentence and
each target word, and fluency features, which
describe the ?quality? of the translation hy-
potheses. The latter rely on different lan-
guage models (either on POS or on words)
and the former on IBM Model 1 translation
probabilities and on pseudo- references, i.e.
translation produced by an independent MT
system. Random forests are known to per-
form well in tasks like this one, in which
only a few dense and continuous features are
available, possibly because of their ability to
take into account complex interactions be-
tween features and to automatically partition
the continuous feature values into a discrete
set of intervals that achieves the best classifi-
cation performance. Since they predict the
class probabilities, it is possible to directly
optimize the F
1
score during training by find-
ing, with a grid search method, the decision
threshold that achieved the best F
1
score on
the training set.
MULTILIZER (T1.2, T1.3): The 80 black-box
features from QUEST are used in addition to
new features based on using other MT en-
gines for forward and backward translations.
In forward translations, the idea is that dif-
ferent MT engines make different mistakes.
Therefore, when several forward translations
are similar to each other, these translations
are more likely to be correct. This is con-
firmed by the Pearson correlation of similar-
ities between the forward translations against
the true scores (above 0.5). A backward
translation is very error-prone and therefore
it has to be used in combination with for-
ward translations. A single back-translation
similar to original source segment does not
bring much information. Instead, when sev-
eral MT engines give back-translations simi-
lar to this source segment, one can conclude
that the translation is reliable. Those transla-
tions where similarities both in forward trans-
lation and backward translation are high are
intuitively more likely to be good. A simple
feature selection method that omits all fea-
tures with Pearson correlation against the true
scores below 0.2 is used. The systems sub-
mitted are obtained using linear regression
models.
RTM-DCU (T1.1, T1.2, T1.3, T2): RTM-DCU
systems are based on referential translation
machines (RTM) (Bic?ici, 2013) and parallel
feature decay algorithms (ParFDA5) (Bic?ici
et al., 2014), which allow language and MT
system-independent predictions. For each
task, individual RTM models are developed
using the parallel corpora and the language
model corpora distributed by the WMT14
translation task and the language model cor-
pora provided by LDC for English and Span-
ish. RTMs use 337 to 437 sentence-level fea-
tures for coverage and diversity, IBM1 and
sentence translation performance, retrieval
closeness and minimum Bayes retrieval risk,
distributional similarity and entropy, IBM2
alignment, character n-grams, sentence read-
ability, and parse output tree structures. The
features use ngrams defined over text or com-
mon cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over
which similarity calculations are performed.
Learning models include ridge regression
(RR), support vector machines (SVR), and
regression trees (TREE), which are applied
after partial least squares (PLS) or feature
selection (FS). For word-level prediction,
generalised linear models (GLM) (Collins,
2002) and GLM with dynamic learning
(GLMd) (Bic?ici, 2013) are used with word-
level features including CCL links, word
length, location, prefix, suffix, form, context,
and alignment, totalling up to a couple of mil-
lion features.
SHEF-lite (T1.1, T1.2, T1.3): These submis-
sions use the framework of Multi-task Gaus-
sian Processes, where multiple datasets are
30
combined in a multi-task setting similar to
the one used by Cohn and Specia (2013).
For T1.1, data for all language pairs is put
together, and each language is considered a
task. For T1.2 and T1.3, additional datasets
from previous shared task years are used,
each encoded as a different task. For all tasks,
the QUEST framework is used to extract a set
of 80 black-box features (a superset of the 17
baseline features). To cope with the large size
of the datasets, the SHEF-lite-sparse submis-
sion uses Sparse Gaussian Processes, which
provide sensible sparse approximations using
only a subset of instances (inducing inputs)
to speed up training and prediction. For this
?sparse? submission, feature selection is per-
formed following the approach of Shah et al.
(2013) by ranking features according to their
learned length-scales and selecting the top 40
features.
USHEFF (T1.1, T1.2, T1.3): USHEFF submis-
sions exploit the use of consensus among
MT systems by comparing the MT sys-
tem output to several alternative translations
generated by other MT systems (pseudo-
references). The comparison is done using
standard evaluation metrics (BLEU, TER,
METEOR, ROUGE for all tasks, and two
metrics based on syntactic similarities from
shallow and dependency parser information
for T1.2 and T1.3). Figures extracted from
such metrics are used as features to com-
plement prediction models trained on the 17
baseline features. Different from the standard
use of pseudo-reference features, these fea-
tures do not assume that the alternative MT
systems are better than the system of inter-
est. A more realistic scenario is considered
where the quality of the pseudo-references is
not known. For T1, no external systems in
addition to those provided for the shared task
are used: for a given translation, all alter-
native translations for the same source seg-
ment (two or three, depending on the lan-
guage pair) are used as pseudo-references.
For T1.2 and T1.3, for each source sentence,
all alternative translations produced by MT
systems on the same data (WMT12/13) are
used as pseudo-references. The hypothesis
is that by using translations from several MT
systems one can find consensual information
and this can smooth out the effect of ?coinci-
dences? in the similarities between systems?
translations. SVM regression with radial ba-
sis function kernel and hyper-parameters op-
timised via grid search is used to build the
models.
YANDEX (T1.1): Both submissions are based
on the the 80 black-box features, plus an
LM score from a larger language model,
a pseudo-reference, and several additional
features based on POS tags and syntactic
parsers. The first attempt uses an extract
of the top 5 features selected with a greedy
search from the set of all features. SVM re-
gression is used as machine learning algo-
rithm. The second attempt uses the same
features processed with Yandex? implemen-
tation of the gradient tree boosting (Ma-
trixNet).
4.4 Results
In what follows we give the official results for all
tasks followed by a discussion that highlights the
main findings for each of the tasks.
Task 1.1 Predicting post-editing effort
Table 11 summarises the results for the ranking
variant of Task 1.1. They are sorted from best to
worst using the DeltaAvg metric scores as primary
key and the Spearman?s rank correlation scores as
secondary key.
The winning submissions for the ranking vari-
ant of Task 1.1 are as follows: for English-Spanish
it is RTM-DCU/RTM-TREE, with a DeltaAvg
score of 0.26; for Spanish-English it is USH-
EFF, with a DeltaAvg score of 0.23; for English-
German it is again RTM-DCU/RTM-TREE, with a
DeltaAvg score of 0.39; and for German-English it
is RTM-DCU/RTM-RR, with a DeltaAvg score of
0.38. These winning submissions are better than
the baseline system by a large margin, which indi-
cates that current best performance in MT quality
estimation has reached levels that are clearly be-
yond what the baseline system can produce. As for
the other systems, according to DeltaAvg, com-
pared to the previous year results a smaller per-
centage of systems is able to beat the baseline.
This might be a consequence of the use of the met-
ric for the prediction of only three discrete labels.
The results for the scoring task are presented in
Table 12, sorted from best to worst using the MAE
31
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-PLS-TREE 0.26 0.38
? RTM-DCU/RTM-TREE 0.26 0.41
? YANDEX/SHAD BOOSTEDTREES2 0.23 0.35
USHEFF 0.21 0.33
SHEFF-lite 0.21 0.33
YANDEX/SHAD SVR1 0.18 0.29
SHEFF-lite-sparse 0.17 0.27
Baseline SVM 0.14 0.22
Spanish-English
? USHEFF 0.23 0.30
? RTM-DCU/RTM-PLS-RR 0.20 0.35
? RTM-DCU/RTM-FS-RR 0.19 0.36
Baseline SVM 0.12 0.21
SHEFF-lite-sparse 0.12 0.17
SHEFF-lite 0.11 0.15
English-German
? RTM-DCU/RTM-TREE 0.39 0.54
RTM-DCU/RTM-PLS-TREE 0.33 0.42
USHEFF 0.26 0.41
SHEFF-lite 0.26 0.36
Baseline SVM 0.23 0.34
SHEFF-lite-sparse 0.23 0.33
German-English
? RTM-DCU/RTM-RR 0.38 0.51
? RTM-DCU/RTM-PLS-RR 0.35 0.45
USHEFF 0.28 0.30
SHEFF-lite 0.24 0.27
Baseline SVM 0.21 0.25
SHEFF-lite-sparse 0.14 0.17
Table 11: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
32
System ID MAE RMSE
English-Spanish
? RTM-DCU/RTM-PLS-TREE 0.49 0.61
? SHEFF-lite 0.49 0.63
? USHEFF 0.49 0.63
? SHEFF-lite/sparse 0.49 0.69
? RTM-DCU/RTM-TREE 0.49 0.61
Baseline SVM 0.52 0.66
YANDEX/SHAD BOOSTEDTREES2 0.56 0.68
YANDEX/SHAD SVR1 0.64 0.81
DCU-Chris/SVR 0.66 0.88
DCU-Chris/MIXED 0.94 1.14
Spanish-English
? RTM-DCU/RTM-FS-RR 0.53 0.64
? SHEFF-lite/sparse 0.54 0.69
? RTM-DCU/RTM-PLS-RR 0.55 0.71
USHEFF 0.57 0.67
Baseline SVM 0.57 0.68
SHEFF-lite 0.62 0.77
DCU-Chris/MIXED 0.65 0.91
English-German
? RTM-DCU/RTM-TREE 0.58 0.68
RTM-DCU/RTM-PLS-TREE 0.60 0.71
SHEFF-lite 0.63 0.74
USHEFF 0.64 0.75
SHEFF-lite/sparse 0.64 0.75
Baseline SVM 0.64 0.76
DCU-Chris/MIXED 0.69 0.98
German-English
? RTM-DCU/RTM-RR 0.55 0.67
? RTM-DCU/RTM-PLS-RR 0.57 0.74
USHEFF 0.63 0.76
SHEFF-lite 0.65 0.77
Baseline SVM 0.65 0.78
Table 12: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
33
metric scores as primary key and the RMSE metric
scores as secondary key.
The winning submissions for the scoring variant
of Task 1.1 are as follows: for English-Spanish it
is RTM-DCU/RTM-TREE with a MAE of 0.49;
for Spanish-English it is RTM-DCU/RTM-FS-
RR with a MAE of 0.53; for English-German
it is again RTM-DCU/RTM-TREE, with a MAE
of 0.58; and for German-English it is RTM-
DCU/RTM-RR with a MAE of 0.55. These sub-
missions are again much better than the baseline
system, which under the scoring variant seems
to perform at a middle-of-the-pack level or lower
compared to the overall pool of submissions.
Overall, more systems are able to outperform the
baseline according to the scoring metric.
The top system for most language pairs are
essentially based on the same core techniques
(RTM-DCU) according to both the DeltaAvg and
MAE metrics. The ranking of other systems, how-
ever, can be substantially different according to the
two metrics.
Task 1.2 Predicting percentage of edits
Table 13 summarises the results for the ranking
variant of Task 1.2. For readability purposes we
have used a multiplication-factor of 100 in the
scoring script, which makes the HTER numbers
(both predicted and gold) to be in the [0, 100]
range. They are sorted from best to worst using
the DeltaAvg metric scores as primary key and the
Spearman?s rank correlation scores as secondary
key.
The winning submission for the ranking vari-
ant of Task 1.2 is RTM-DCU/RTM-SVR, with a
DeltaAvg score of 9.31. There is a large mar-
gin between this score and the baseline score of
DeltaAvg 5.08, which indicates again that current
best performance has reached levels that are much
beyond what this baseline system can produce.
The vast majority of the submissions perform bet-
ter than the baseline (the only exception is the sub-
mission from SHEFF-lite, for which the authors
report a major issue with the learning algorithm).
The results for the scoring variant are presented
in Table 14, sorted from best to worst by using the
MAE metric scores as primary key and the RMSE
metric scores as secondary key.
The winning submission for the scoring variant
of Task 1.2 is FBK-UPV-UEDIN/WP with a MAE
of 12.89, while the baseline system has a MAE
of 15.23. Most of the submissions perform better
than the baseline.
Task 1.3 Predicting post-editing time
Table 15 summarises the results for the ranking
variant of Task 1.3. For readability purposes, we
have used a multiplication-factor of 0.001 in the
scoring script, which makes the time (both pre-
dicted and gold) to be measured in seconds. They
are sorted from best to worst using the DeltaAvg
metric scores as primary key and the Spearman?s
rank correlation scores as secondary key.
The winning submission for the ranking vari-
ant of Task 1.3 is RTM-DCU/RTM-RR, with a
DeltaAvg score of 17.02 (when predicting sec-
onds). The interesting aspect of these results is
that the DeltaAvg numbers have a direct real-
world interpretation, in terms of time spent (or
saved, depending on one?s view-point) for post-
editing machine-produced translations. A more
elaborate discussion on this point can be found in
Section 4.5.
The winning submission for the scoring variant
of Task 1.3 is RTM-DCU/RTM-SVR, with a MAE
of 16.77. Note that all of the submissions perform
significantly better than the baseline, which has a
MAE of 21.49, and that the majority is not signif-
icantly worse than the top scoring submission.
Task 2 Predicting word-level edits
The results for Task 2 are summarised in Tables
17?19. The results are ordered by F
1
score for
the Error (BAD) class. For comparison, two triv-
ial baselines are included, one that marks every
word as correct and that marks every word with
the most common error class found in the training
data. Both baselines are clearly useless for any ap-
plication, but help put the results in perspective.
Most teams submitted systems for a single lan-
guage pair: English-Spanish; only a single team
produced predictions for all four pairs.
Table 17 gives the results of the binary (OK vs.
BAD) classification variant of Task 2. The win-
ning submissions for this variant are as follows:
for English-Spanish it is FBK-UPV-UEDIN/RNN
with a weighted F
1
of 48.73; for Spanish-
English it is RTM-DCU/RTM-GLMd with a
weighted F
1
of 29.14; for English-German it is
RTM-DCU/RTM-GLM with a weighted F
1
of
45.30; and for German-English it is again RTM-
DCU/RTM-GLM with a weighted F
1
of 26.13.
Remarkably, for three out of four language
pairs, the systems fail to beat our trivial baseline of
34
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-SVR 9.31 0.53
? RTM-DCU/RTM-TREE 8.57 0.48
? USHEFF 7.93 0.45
SHEFF-lite/sparse 7.69 0.43
Baseline 5.08 0.31
SHEFF-lite 0.72 0.09
Table 13: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (100k times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
System ID MAE RMSE
English-Spanish
? FBK-UPV-UEDIN/WP 12.89 16.74
? RTM-DCU/RTM-SVR 13.40 16.69
? USHEFF 13.61 17.84
RTM-DCU/RTM-TREE 14.03 17.48
DFKI/SVR 14.32 17.74
FBK-UPV-UEDIN/NOWP 14.38 18.10
SHEFF-lite/sparse 15.04 18.38
MULTILIZER 15.04 20.86
Baseline 15.23 19.48
DFKI/SVRxdata 16.01 19.52
SHEFF-lite 18.15 23.41
Table 14: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-RR 17.02 0.68
? RTM-DCU/RTM-SVR 16.60 0.67
SHEFF-lite/sparse 16.33 0.63
SHEFF-lite 16.08 0.64
USHEFF 14.98 0.59
Baseline 14.71 0.57
Table 15: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
35
System ID MAE RMSE
English-Spanish
? RTM-DCU/RTM-SVR 16.77 26.17
?MULTILIZER/MLZ2 17.07 25.83
? SHEFF-lite 17.13 27.33
?MULTILIZER/MLZ1 17.31 25.51
? SHEFF-lite/sparse 17.42 27.35
? FBK-UPV-UEDIN/WP 17.48 25.31
RTM-DCU/RTM-RR 17.50 25.97
FBK-UPV-UEDIN/NOWP 18.69 26.58
USHEFF 21.48 34.28
Baseline 21.49 34.28
Table 16: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
weighted F
1
F
1
System ID All Bad ? MCC ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 64.38
Baseline (always Bad) 18.71 52.53 0.00 35.62
? FBK-UPV-UEDIN/RNN 62.00 48.73 18.23 61.62
LIMSI/RF 60.55 47.32 15.44 60.09
LIG/FS 63.55 44.47 19.41 64.67
LIG/BL ALL 63.77 44.11 19.91 65.12
FBK-UPV-UEDIN/RNN+tandem+crf 62.17 42.63 16.32 63.26
RTM-DCU/RTM-GLM 60.68 35.08 13.45 63.74
RTM-DCU/RTM-GLMd 60.24 32.89 12.98 63.97
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 82.37
Baseline (always Bad) 5.28 29.98 0.00 17.63
? RTM-DCU/RTM-GLMd 79.54 29.14 25.47 82.98
RTM-DCU/RTM-GLM 79.42 26.91 25.93 83.43
English-German
Baseline (always OK) 59.39 0.00 0.00 71.33
Baseline (always Bad) 12.78 44.57 0.00 28.67
? RTM-DCU/RTM-GLM 71.51 45.30 28.61 72.97
RTM-DCU/RTM-GLMd 68.73 36.91 21.32 71.41
German-English
Baseline (always OK) 67.82 0.00 0.00 77.60
Baseline (always Bad) 8.20 36.60 0.00 22.40
? RTM-DCU/RTM-GLM 72.41 26.13 16.08 76.14
RTM-DCU/RTM-GLMd 71.42 22.97 12.63 75.46
Table 17: Official results for the binary part of the WMT14 Quality Evaluation Task 2. The winning submissions are indicated
by a ?. All values are given as percentages.
36
marking all the words as wrong. This may either
indicate that the predictions themselves are of low
quality or the chosen evaluation approach is mis-
leading. On the other hand F
1
scores are a com-
mon measure of binary classification performance
and no averaging is performed here.
Table 18 gives the results of the Level 1
classification (OK, Fluency, Accuracy) variant
of Task 2. Here the second baseline is to
always predict Fluency errors, as this is the
most common error category in the training
data. The winning submissions of this vari-
ant are as follows: for English-Spanish it
is FBK-UPV-UEDIN/RNN+tandem+crf with a
weighted F
1
of 23.94 and for Spanish-English,
English-German, and German-English it is RTM-
DCU/RTM-GLMd with weighted F
1
scores of
23.94, 21.94, and 8.57 respectively.
As before, all systems fail to outperform the
single-class baseline for the Spanish-English lan-
guage pair according to our primary metric. How-
ever, for Spanish-English and English-German
both submissions are able to beat the baseline by
large margin. We also observe that the absolute
numbers vary greatly between language pairs.
Table 19 gives the results of the Multi-class
classification variant of Task 2. Again, the sec-
ond baseline is to always predict the most common
error category in the training data, which varies
depending on language pair and produces and in-
creasingly weak baseline as the number of classes
rises.
The winning submissions of this variant are
as follows: for English-Spanish, Spanish-English,
and English-German it is RTM-DCU/RTM-GLM
with weighted F
1
scores of 26.84, 8.75, and 15.02
respectively and and for German-English it is
RTM-DCU/RTM-GLMd with a weighted F
1
of
3.08. Not only do these systems perform above
our baselines for all but the German-English lan-
guage pair, they also outperform all other sub-
missions for English-Spanish. Remarkably, RTM-
DCU/RTM-GLM wins English-Spanish for all of
the proposed metrics by a sizeable margin.
4.5 Discussion
In what follows, we discuss the main accomplish-
ments of this year?s shared task starting from the
goals we had previously identified for it.
Investigating the effectiveness of different
quality labels
For the sentence-level tasks, the results of this
year?s shared task allow us to investigate the ef-
fectiveness of predicting translation quality using
three very different quality labels: perceived post-
editing effort on a scale of [1-3] (Task 1.1); HTER
scores (Task 1.2); and the time that a translator
takes to post-edit the translation (Task 1.3). One of
the ways one can compare the effectiveness across
all these different labels is to look at how well
the models can produce predictions that correlate
with the gold label that we have at our disposal.
A measure of correlation that does not depend
on the value of the labels is Spearman?s ranking
correlation. From this perspective, the label that
seems the most effective appears to be post-editing
time (Task 1.3), with the best system (RTM-
DCU/RTM-RR) producing a Spearman?s ? of 0.68
(English-Spanish translations, see Table 15). In
comparison, when perceived post-editing effort la-
bels are used (Task 1.1), the best systems achieve
a Spearman?s ? of 0.38 and 0.30 for English-
Spanish and Spanish-English translations, respec-
tively, and ? of 0.54 and 0.51 for English-German
and German-English, respectively (Table 11); for
HTER scores (Task 1.2) the best systems achieve
a Spearman?s ? of 0.53 for English-Spanish trans-
lations (Table 13).
This comparison across tasks seems to indicate
that, among the three labels we have proposed,
post-editing time seems to be the most learnable,
in the sense that automatic predictions can vest
match the gold labels (in this case, with respect
to the rankings they induce). A possible reason
for this is that post-editing time correlates with the
length of the source sentence whereas HTER is a
normalised measure.
Compared to the results regarding time predic-
tion in the Quality Evaluation shared task from
2013 (Bojar et al., 2013), we note that this time
all submissions were able to beat the baseline sys-
tem (compared to only 1/3 of the submissions in
2013). In addition, better handling of the data
acquisition reduced the number of outliers in this
year?s dataset allowing for numbers that are more
reliably interpretable. As an example of its in-
terpretability, consider the following: the winning
submission for the ranking variant of Task 1.3 is
RTM-DCU/RTM-RR, with a a Spearman?s ? of
0.68 and a DeltaAvg score of 17.02 (when predict-
37
weighted F
1
weighted MCC
System ID All Errors ? All Errors ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 0.00 64.38
Baseline (always fluency) 14.39 40.41 0.00 0.00 30.67
? FBK-UPV-UEDIN/RNN+tandem+crf 58.36 38.54 16.63 13.89 57.98
FBK-UPV-UEDIN/RNN 60.32 37.25 18.22 15.51 61.75
LIG/BL ALL 58.97 31.79 14.95 11.48 61.13
LIG/FS 58.95 31.78 14.92 11.46 61.10
RTM-DCU/RTM-GLMd 58.23 26.62 12.60 12.76 62.94
RTM-DCU/RTM-GLM 56.47 29.91 8.11 7.96 58.56
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always fluency) 2.67 15.13 0.00 0.00 12.24
? RTM-DCU/RTM-GLMd 78.89 23.94 25.41 25.45 83.17
RTM-DCU/RTM-GLM 78.78 21.96 26.31 26.99 83.69
English-German
Baseline (always OK) 59.39 0.00 0.00 0.00 71.33
Baseline (always fluency) 3.83 13.35 0.00 0.00 14.82
? RTM-DCU/RTM-GLMd 64.58 21.94 17.69 15.92 69.26
RTM-DCU/RTM-GLM 64.43 21.10 16.99 14.93 69.34
German-English
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always fluency) 3.34 14.92 0.00 0.00 13.79
? RTM-DCU/RTM-GLMd 69.17 8.57 10.61 5.76 75.91
RTM-DCU/RTM-GLM 69.09 8.26 9.95 5.76 75.97
Table 18: Official results for the Level 1 classification part of the WMT14 Quality Evaluation Task 2. The winning submissions
are indicated by a ?. All values are given as percentages.
38
weighted F
1
weighted MCC
System ID All Errors ? All Errors ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 0.00 64.38
Baseline (always unintelligible) 7.93 22.26 0.00 0.00 21.99
? RTM-DCU/RTM-GLM 60.52 26.84 23.77 21.45 66.83
FBK-UPV-UEDIN/RNN+tandem+crf 52.96 23.07 15.17 10.74 52.13
LIG/BL ALL 56.66 20.50 18.56 13.39 60.39
LIG/FS 56.66 20.50 18.56 13.39 60.39
FBK-UPV-UEDIN/RNN 52.84 17.09 7.66 4.24 57.18
RTM-DCU/RTM-GLMd 51.87 3.22 10.16 4.04 64.42
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always word order) 0.34 1.96 0.00 0.00 4.24
? RTM-DCU/RTM-GLM 76.34 8.75 19.82 13.43 83.27
RTM-DCU/RTM-GLMd 76.21 8.19 19.35 15.32 83.17
English-German
Baseline (always OK) 59.39 0.00 0.00 0.00 71.33
Baseline (always mistranslation) 2.48 8.66 0.00 0.00 11.78
? RTM-DCU/RTM-GLM 63.57 15.02 17.57 15.08 70.82
RTM-DCU/RTM-GLMd 63.33 12.48 18.70 13.20 71.45
German-English
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always word order) 1.56 6.96 0.00 0.00 9.23
? RTM-DCU/RTM-GLMd 67.62 3.08 7.19 1.48 74.73
RTM-DCU/RTM-GLM 67.86 2.36 7.55 1.79 75.75
Table 19: Official results for the Multi-class classification part of the WMT14 Quality Evaluation Task 2. The winning
submissions are indicated by a ?. All values are given as percentages.
39
ing seconds). This number has a direct real-world
interpretation: using the order proposed by this
system, a human translator would spend, on av-
erage, about 17 seconds less on a sentence taken
from the top of the ranking compared to a sen-
tence picked randomly from the set.
14
To put this
number into perspective, for this dataset the av-
erage time to complete a sentence post-editing is
39 seconds. As such, one has an immediate inter-
pretation for the usefulness of using such a rank-
ing: translating around 100 sentences taken from
the top of the rankings would take around 36min
(at about 22 seconds/sentence), while translating
the same number of sentences extracted randomly
from the same dataset would take around 1h5min
(at about 39 seconds/sentence). It is in this sense
that we consider post-editing time an interpretable
label.
Another desirable property of label predictions
is usefulness; this property, however, it highly
task-dependent and therefore cannot be judged in
the absence of a specific task. For instance, an in-
terpretable label like post-editing time may not be
that useful in a task the requires one to place the
machine translations into ?ready to publish? and
?not ready to publish? bins. For such an appli-
cation, labels such as the ones used by Task 1.1
are clearly more useful, and also very much inter-
pretable within the scope of the task. Our attempt
at presenting the Quality Prediction task with a va-
riety of prediction labels illustrates a good range
of properties for the proposed labels and enables
one to draw certain conclusions depending on the
needs of the specific task at hand.
For the word-level tasks, different quality labels
equate with using different levels of granularity for
the predictions, which we discuss next.
Exploring word-level quality prediction at
different levels of granularity
Previous work on word-level predictions, e.g. (Bo-
jar et al., 2013) has focused on prediction of auto-
matically derived labels, generally due to practical
considerations as the manual annotation is labour
intensive. While easily applicable, automatic an-
notations, using for example TER alignment be-
tween the machine translation and reference (or
post-edition), face the same problems as automatic
14
Note that the 17.02 seconds figure is a difference in real-
time, not predicted time; what is considered in this variant of
Task 1.3 is only the predicted ranking of data points, not the
absolute values of the predictions.
MT evaluation metrics as they fail to account for
different word choices and lack the ability to re-
liably distinguish meaning preserving reorderings
from those that change the semantics of the out-
put. Furthermore, previous automatic annotation
for word-level quality estimation has focused on
binary labels: correct / incorrect, or at most, the
main edit operations that can be captured by align-
ment metrics like TER: correct, insertion, dele-
tion, substitution.
In this year?s task we were able to provide
manual fine-grained annotations at the word-level
produced by humans irrespective of references or
post-editions. Error categories range from fre-
quent ones, such as unintelligible, mistranslation,
and terminology, to rare ones such as additions or
omissions. For example, only 10 out of more than
3,400 errors in the English-Spanish test set fall
into the latter categories, while over 2,000 words
are marked as unintelligible. By hierarchically
grouping errors into coarser categories we aimed
to find a compromise between data sparsity and
the expressiveness of the labels. What marks a
good compromise depends on the use case, which
we do not specify here, and the quality of the finer
grained predictions: if a system is able to predict
even rare errors these may be grouped later if nec-
essary.
Overall, word-level error prediction seems to re-
main a challenging task as evidenced by the fact
that many submissions were unable to beat a triv-
ial baseline. We hypothesise that this is at least
partially due to a mismatch in loss-functions used
in training and testing. We know from the sys-
tem descriptions that some systems were tuned to
optimise squared error or accuracy, while evalua-
tion was performed using weighted F
1
scores. On
the other hand, even a comparison of just accuracy
shows that systems struggle to obtain a lower error
rates than the ?all-OK? baseline.
Such performance problems are consistent over
the three levels of granularity, contrary to the in-
tuition that binary classification would be easier.
A notable exception is the RTM-DCU/RTM-GLM
system, which is able to beat both the baseline and
all other systems on the Multi-Class variant of the
English-Spanish task ? cf. Table 19 ? with regard
to all metrics. For this and most other submis-
sions we observe that labels are not consistent for
different granularities, i.e. at token marked with a
specific error in the multi-class variant may still
40
carry an ?OK? label in binary annotation. Thus,
additional coarse grained annotations may be de-
rived by automatic means. For example, mapping
the multi-class predictions of the above system to
coarser categories improves the F
1,ERR
score in
Table 17 from 35.08 to 37.02 but does not change
the rank with respect to the other entries.
The fact that coarse grained predictions seem
not to be derived from the fine-grained ones leads
us to believe that most participants treated the
different granularities as independent classifica-
tion tasks. The FBK-UPV-UEDIN team trans-
fers information in the opposite direction by using
their binary predictions as features for Level-1 and
multi-class.
Given the current quality of word-level predic-
tion it remains unclear if these systems can already
be employed in a practical setting, e.g. to focus the
attention of post-editors.
Studying the effects of training and test
datasets with mixed domains, language pairs
and MT systems
This year?s shared task made available datasets for
more than one language pair with the same or dif-
ferent types of annotation, 2-3 multiple MT sys-
tems (plus a human translation) per language pair,
and out-of-domain test data (Tasks 1.1 and 2). In-
stances for each language pair were kept in sep-
arate datasets and thus the ?language pair? vari-
able can be analysed independently. However, for
a given language pair, datasets mix translation sys-
tems (and humans) in Task 1.1, and also text do-
mains in Task 2.
Directly comparing the performance across lan-
guage pairs is not possible, given that their
datasets have different numbers of instances (pro-
duced by 3 or 4 systems) and/or different true
score distributions (see Figure 3). For a relative
comparison (although not all systems submitted
results for all language pairs, which is especially
true in Task 2), we observe in Task 1.1 that for all
language pairs generally at least half of the sys-
tems did better than the baseline. To our surprise,
only one submission combined data for multiple
languages together for Task 1.1: SHEF-lite, treat-
ing each language pair data as a different task in
a multi-task learning setting. However, only for
the ?sparse? variant of the submission significant
gains were reported over modelling each task in-
dependently (with the tasks still sharing the same
data kernel and the same hyperparameters).
The interpretation of the results for Task 2 is
very dependent on the evaluation metric used,
but generally speaking a large variation in per-
formance was found between different languages,
with English-Spanish performing the best, possi-
bly given the much larger number of training in-
stances. Data for Task 2 also presented varied true
score distributions (as shown by the performance
of the baseline (e.g. always ?OK?) in Tables 17-
19.
One of the main goals with Task 1.1 (and Task 2
to some extent) was to test the robustness of mod-
els in a blind setting where multiple MT systems
(and human translations) are put together and their
identifiers are now known. All submissions for
these tasks were therefore translation system ag-
nostic, with no submission attempting to perform
meta-identification of the origins of the transla-
tions. For Task 1.1, data from multiple MT sys-
tems was explicitly used by USHEFF though the
idea of consensus translations. Translations from
all but the system of interest for the same source
segment were used as pseudo-references. The
submission significantly outperformed the base-
line for all language pairs and did particularly well
for Spanish-English and English-Spanish.
An in depth analysis of Task 1.1?s datasets on
the difference in prediction performance between
models built and applied for individual transla-
tion systems and models built and tested for all
translations pooled together is presented in (Shah
and Specia, 2014). Not surprisingly, the former
models perform significantly better, with MAE
scores ranging between 0.35 and 0.5 for differ-
ent language pairs and MT systems, and signifi-
cantly lower scores for models trained and tested
on human translations only (MAE scores between
0.2 and 0.35 for different language pairs), against
MAE scores ranging between 0.5 and 0.65 for
models with pooled data.
For Tasks 1.2 and 1.3, two submissions included
English-Spanish data which had been produced by
yet different MT systems (SHEF-lite and DFKI).
While using these additional instances seemed at-
tractive given the small number of instances avail-
able for these tasks, it is not clear what their contri-
bution was. For example, with a reduced set of in-
stances (only 400) from the combined sets, SHEF-
lite/sparse performed significantly better than its
variant SHEF-lite.
Finally, with respect to out-of-domain (different
41
text domain and MT system) test data, for Task
1.1, none of the papers submitted included experi-
ments. (Shah and Specia, 2014) applied the mod-
els trained on pooled datasets (as explained above)
for each language pair to the out-of-domain test
sets. The results were surprisingly positive, with
average MAE score of 0.5, compared to the 0.5-
0.65 range for in-domain data (see above). Further
analysis is necessary to understand the reasons for
that.
In Task 2, the official training and test sets al-
ready include out-of-domain data because of the
very small amount of in-domain data available,
and thus is is hard to isolate the effect of this data
on the results.
Examining the effectiveness of quality
prediction methods on human translations
Datasets for Tasks 1.1 and 2 contain human trans-
lations, in addition to the automatic translations
from various MT systems. Predicting human
translation quality is an area that has been largely
unexplored. Previous work has looked into dis-
tinguishing human from machine translations (e.g.
(Gamon et al., 2005)), but this problem setting is
somehow artificial, and moreover arguably harder
to solve nowadays given the higher general qual-
ity of current MT systems (Shah and Specia,
2014). Although human translations are obviously
of higher quality in general, many segments are
translated by MT systems with the same or similar
levels of quality as human translation. This is par-
ticularly true for Task 2, since data had been pre-
viously categorised and only ?near misses? were
selected for the word-level annotation, i.e., human
and machine translations that were both nearly
perfect in this case.
While no distinction was made between human
and machine translations in our tasks, we believe
the mix of these two types of translations has had
a negative impact in prediction performance. Intu-
itively, one can expect errors in human translation
to be more subtle, and hence more difficult to cap-
ture via standard quality estimation features. For
example, an incorrect lexical choice (due to, e.g.,
ambiguity) which still fits the context and does not
make the translation ungrammatical is unlikely to
be captured. We hoped that participants would de-
sign features for this particular type of translation,
but although linguistically motivated features have
been exploited, they did not seem appropriate for
human translations.
It is interesting to mention the indirect use of
human translations by USHEFF for Tasks 1.1-1.3:
given a translation for a source segment, all other
translations for the same segment were used as
pseudo-references. Apart from when this transla-
tion was actually the human translation, the hu-
man translation was effectively used as a refer-
ence. While this reference was mixed with 2-
3 other pseudo-references (other machine transla-
tions) for the feature computations, these features
led to significant gains in performance over the
baseline features Scarton and Specia (2014).
We believe that more investigation is needed for
human translation quality prediction. Tasks ded-
icated to this type of data at both sentence- and
word-level in the next editions of this shared task
would be a possible starting point. The acquisi-
tion of such data is however much more costly, as
it is arguably hard to find examples of low quality
human translation, unless specific settings, such as
translation learner corpora, are considered.
5 Medical Translation Task
The Medical Translation Task addresses the prob-
lem of domain-specific and genre-specific ma-
chine translation. The task is split into two sub-
tasks: summary translation, focused on transla-
tion of sentences from summaries of medical ar-
ticles, and query translation, focused on transla-
tion of queries entered by users into medical infor-
mation search engines.
In general, texts of specific domains and gen-
res are characterized by the occurrence of special
vocabulary and syntactic constructions which are
rare or even absent in traditional (general-domain)
training data and therefore difficult for MT. Spe-
cific training data (containing such vocabulary and
syntactic constructions) is usually scarce or not
available at all. Medicine, however, is an exam-
ple of a domain for which in-domain training data
(both parallel and monolingual) is publicly avail-
able in amounts which allow to train a complete
SMT system or to adapt an existing one.
5.1 Task Description
In the Medical Translation Task, we provided links
to various medical-domain training resources and
asked participants to use the data to train or adapt
their systems to translate unseen test sets for both
subtasks between English and Czech (CS), Ger-
man (DE), and French (FR), in both directions.
42
The summary translation test data is domain-
specific, but otherwise can be considered as ordi-
nary sentences. On the other hand, the query trans-
lation test data is also specific for its genre (gen-
eral style) ? it contains short sequences of (more
or less) of independent terms rather than complete
and grammatical sentences, the usual target of cur-
rent MT systems.
Similarly to the standard Translation Task, the
participants of the Medical Translation Task were
allowed to use only the provided resources in the
constrained task (in addition to data allowed in
the constrained standard Translation Task), but
could exploit any additional resources in the un-
constrained task. The submissions were expected
with true letter casing and detokenized. The trans-
lation quality was measured using automatic eval-
uation metrics, manual evaluation was not per-
formed.
5.2 Test and Development Data
The test and development data sets for this task
were provided by the EU FP7 project Khres-
moi.
15
This projects develops a multi-lingual
multi-modal search and access system for biomed-
ical information and documents and its MT com-
ponent allows users to use non-English queries to
search in English documents and see summaries
of retrieved documents in their preferred language
(Czech, German, or French). The statistics of the
data sets are presented in Tables 20 and 21.
For the summary translation subtask, 1,000
and 500 sentences were provided for test devel-
opment purposes, respectively. The sentences
were randomly sampled from automatically gen-
erated summaries (extracts) of English documents
(web pages) containing medical information rel-
evant to 50 topics provided for the CLEF 2013
eHealth Task 3.
16
Out-of-domain and ungram-
matical sentences were manually removed. The
sentences were then translated by medical experts
into Czech, German and French, and the transla-
tions were reviewed. Each sentence was provided
with the corresponding document ID and topic ID.
The set also included a description for each of the
50 topics. The data package (Khresmoi Summary
Translation Test Data 1.1) is now available from
the LINDAT/CLARIN repository
17
and more de-
15
http://khresmoi.eu/
16
https://sites.google.com/site/
shareclefehealth/
17
http://hdl.handle.net/11858/
tails can be found in Zde?nka Ure?sov?a and Pecina
(2014).
For the query translation subtask, the main
test set contains 1,000 queries for test and 508
queries for development purposes. The original
English queries were extracted at random from
real user query logs provided by the Health on the
Net foundation
18
(queries by general public) and
the Trip database
19
(queries by medical experts).
Each query was translated into Czech, German,
and French by medical experts and the transla-
tions were reviewed. The data package (Khresmoi
Query Translation Test Data 1.0) is available from
the LINDAT/CLARIN repository.
20
An additional test set for the query translation
subtask was adopted from the CLEF 2013 eHealth
Task 3 (Pecina et al., 2014). It contains 50 queries
constructed from titles of the test topics (originally
in English) translated into Czech, German, and
French by medical experts. The participants were
asked to translate the queries back to English and
the resulting translations were used in an informa-
tion retrieval (IR) experiment for extrinsic evalua-
tion.
5.3 Training Data
This section reviews the in-domain resources
which were allowed for the constrained Medical
Translation Task in addition to resources for the
constrained standard Translation Task (see Section
2). Most of the corpora are available for direct
download, others can be obtained upon registra-
tion. The corpora usually employ their own, more
or less complex data format. To lower the entry
barrier, we provided a set of easy-to-use scripts to
convert the data to a plain text format suitable for
MT training.
5.3.1 Parallel Training Data
The medical-domain parallel data includes the fol-
lowing corpora (see Table 22 for statistics): The
EMEA corpus (Tiedemann, 2009) contains doc-
uments from the European Medicines Agency,
automatically processed and aligned on sentence
level. It is available for many language pairs, in-
cluding those relevant to this task. UMLS is a
multilingual metathesaurus of health and biomed-
00-097C-0000-0023-866E-1
18
http://www.hon.ch/
19
http://www.tripdatabase.com/
20
http://hdl.handle.net/11858/
00-097C-0000-0022-D9BF-5
43
sents tokens
total Czech German French English
dev 500 9,209 9,924 12,369 10,350
test 1,000 19,191 20,831 26,183 21,423
Table 20: Statistics of summary test data.
queries tokens
total general expert Czech German French English
dev 508 249 259 1,128 1,041 1,335 1,084
test 1,000 500 500 2,121 1,951 2,490 2,067
Table 21: Statistics of query test data.
L1?L2 Czech?English DE?EN FR?EN
data set sents L1 tokens L2 tokens sents L1 tokens L2 tokens sents L1 tokens L2 tokens
EMEA 1,053 13,872 14,378 1,108 13,946 14,953 1,092 17,605 14,786
UMLS 1,441 4,248 5,579 2,001 6,613 8,153 2,171 8,505 8,524
Wiki 3 5 6 10 19 22 8 19 17
MuchMore 29 688 740
PatTr 1,848 102,418 106,727 2,201 127,098 108,665
COPPA 664 49,016 39,933
Table 22: Statistics of the in-domain parallel training data allowed for the constrained task (in thousands).
data set English Czech German French
PatTR 121,592 53,242 54,608
UMLS 7,991 63 24 37
Wiki 26,945 1,784 10,232 8,376
AACT 13,341
DrugBank 953
FMA 884
GENIA 557
GREC 62
PIL 662
Table 23: Sizes of monolingual training data allowed for the
constrained tasks (in thousands of tokens).
ical vocabularies and standards (U.S. National Li-
brary of Medicine, 2009). The UMLS dataset
was constructed by selecting the concepts which
have translations in the respective languages. The
Wiki dataset contains bilingual pairs of titles of
Wikipedia articles belonging to the categories
identified to be medical-domain within the Khres-
moi project. It is available for all three lan-
guage pairs. The MuchMore Springer Corpus
is a German?English parallel corpus of medical
journals abstracts published by Springer (Buitelaar
et al., 2003). PatTR is a parallel corpus extracted
from the MAREC patent collection (W?aschle and
Riezler, 2012). It is available for German?English
and French?English. For the medical domain,
we only consider text from patents indicated to
be from the medicine-related categories (A61,
C12N, C12P). COPPA (Corpus of Parallel Patent
Applications (Pouliquen and Mazenc, 2011) is a
French?English parallel corpus extracted from the
MAREC patent collection (W?aschle and Riezler,
2012). The medical-domain subset is identified by
the same categories as in PatTR.
5.3.2 Monolingual Training Data
The medical-domain monolingual data consists of
the following corpora (statistics are presented in
Table 23): The monolingual UMLS dataset con-
tains concept descriptions in CS, DE, and FR ex-
tracted from the UMLS Metathesaurus (see Sec-
tion 5.3.1). The monolingual Wiki dataset con-
sists of articles belonging to the categories iden-
tified to be medical-domain within the Khresmoi
project. The PatTR dataset contains non-parallel
data extracted from the medical patents included
in the PatTR corpus (see Section 5.3.1). AACT is a
collection of restructured and reformatted English
texts publicly available and downloadable from
ClinicalTrials.gov, containing clinical studies con-
ducted around the world. DrugBank is a bioin-
formatics and cheminformatics resource contain-
ing drug descriptions (Knox et al., 2011). GENIA
is a corpus of biomedical literature compiled and
annotated within the GENIA project (Kim et al.,
2003). FMA stands for the Foundational Model
of Anatomy Ontology, a knowledge source for
biomedical informatics concerned with symbolic
representation of the phenotypic structure of the
human body (Rosse and Mejino Jr., 2008). GREC
(Gene Regulation Event Corpus) is a semantically
annotated English corpus of abstracts of biomedi-
cal papers (Thompson et al., 2009). The PIL cor-
pus is a collection of documents giving instruc-
tions to patients about their medication (Bouayad-
Agha et al., 2000).
5.4 Participants
A total of eight teams participated in the Medical
Translation Task by submitting their systems to at
least one subtask for one or more translation direc-
tions. A list of the participants is given in Table 24;
we provide short descriptions of their systems in
the following.
CUNI was involved in the organization of the task,
and their primary goal was to set up a baseline for
both the subtasks and for all translation directions.
44
ID Participating team
CUNI Charles University in Prague (Du?sek et al., 2014)
DCU-Q Dublin City University (Okita et al., 2014)
DCU-S Dublin City University (Zhang et al., 2014)
LIMSI Laboratoire dInformatique pour la Mecanique et les Sciences de lIng?enieur (P?echeux et al., 2014)
POSTECH Pohang University of Science and Technology (Li et al., 2014a)
UEDIN University of Edinburgh (Durrani et al., 2014a)
UM-DA University of Macau (Wang et al., 2014)
UM-WDA University of Macau (Lu et al., 2014)
Table 24: Participants in the WMT14 Medical Translation Task.
Their systems are based on the Moses phrase-
based toolkit and linear interpolation of in-domain
and out-of-domain language models and phrase ta-
bles. The constrained/unconstrained systems dif-
fer in the training data only. The constrained
ones are built using all allowed training data; the
unconstrained ones take advantage of additional
web-crawled monolingual data used for training of
the language models, and additional parallel non-
medical data from the PatTr and COPPA patent
collections.
DCU-Q submitted a system designed specifically
for terminology translation in the query translation
task for EN?FR and FR?EN. This system supports
six terminology extraction methods and is able to
detect rare word pairs including zero-appearance
word pairs. It uses monotonic decoding with lat-
tice inputs, avoiding unnecessary hypothesis ex-
pansions by the reordering model.
DCU-S submitted a system to the FR?EN sum-
mary translation subtask only. The system is
similar to DCU?s system for patent translation
(phrased-based using Moses) but adapted to trans-
late medical summaries and reports.
LIMSI took part in the summary translation sub-
task for English to French.Their primary submis-
sion uses a combination of two translation sys-
tems: NCODE, based on bilingual n-gram trans-
lation models; and an on-the-fly estimation of
the parameters of Moses along with a vector
space model to perform domain adaptation. A
continuous-space language model is also used in
a post-processing step for each system.
POSTECH submitted a phrase-based SMT sys-
tem and query translation system for the DE?EN
language pair in both subtasks. They analysed
three types of query formation, generated query
translation candidates using term-to-term dictio-
naries and a phrase-based system, and then scored
them using a co-occurrence word frequency mea-
sure to select the best candidate.
UEDIN applied the Moses phrase-based system to
all language pairs and both subtasks. They used
the hierarchical reordering model and the OSM
feature, same as in UEDIN?s news translation sys-
tem, and applied compound splitting to German
input. They used separate language models built
on in-domain and out-of-domain data with linear
interpolation. For all language pairs except CS-
EN and DE-EN, they selected data for the transla-
tion model using modified Moore-Lewis filtering.
For DE-EN and CS-EN, they concatenated all the
supplied parallel training data.
UM-DA submitted systems for all language pairs
in the summary translation subtask based on a
combination of different adaptation steps, namely
domain-specific pre-processing, language model
adaptation, translation model adaptation, numeric
adaptation, and hyphenated word adaptation. Data
for the domain-adapted language and translation
models were selected using various data selection
techniques.
UM-WDA submitted systems for all language
pairs in the summary translation subtask. Their
systems are domain-adapted using web-crawled
in-domain resources: bilingual dictionaries and
monolingual data. The translation model and lan-
guage model trained on the crawled data were in-
terpolated with the best-performing language and
translation model employed in the UM-DA sys-
tems.
5.5 Results
MT quality in the Medical Translation Task
is evaluated using automatic evaluation metrics:
BLEU (Papineni et al., 2002), TER (Snover et al.,
2006), PER (Tillmann et al., 1997), and CDER
(Leusch et al., 2006). BLEU scores are reported as
percentage and all error rates are reported as one
minus the original value, also as percentage, so
that all metrics are in the 0-100 range, and higher
scores indicate better translations.
The main reason for not conducting human
evaluation, as it happens in the standard Trans-
45
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech?English
CUNI 29.64 29.79
?
1.07 47.45
?
1.15 61.64
?
1.06 52.18
?
0.98 31.68
?
1.14 49.84
?
1.10 64.38
?
1.06 54.10
?
0.96
CUNI 22.44 22.57
?
0.95 41.43
?
1.16 55.46
?
1.09 46.42
?
0.96 32.34
?
1.12 50.24
?
1.20 65.07
?
1.10 54.42
?
0.96
UEDIN 36.65 36.87
?
1.23 54.35
?
1.19 67.16
?
1.00 57.61
?
1.01 38.02
?
1.24 56.14
?
1.17 69.24
?
1.01 58.96
?
0.96
UM-DA 37.62 37.79
?
1.26 54.55
?
1.20 68.29
?
0.88 57.28
?
1.03 38.81
?
1.28 56.04
?
1.20 70.06
?
0.82 58.45
?
1.05
CUNI 22.92 23.06
?
0.97 42.49
?
1.10 56.10
?
1.12 47.13
?
0.95 33.18
?
1.15 51.48
?
1.15 66.00
?
1.03 55.30
?
0.96
CUNI 22.69 22.84
?
0.98 42.21
?
1.14 56.01
?
1.11 46.79
?
0.94 32.84
?
1.13 51.10
?
1.11 65.79
?
1.07 54.81
?
0.96
UM-WDA 37.35 37.53
?
1.26 54.39
?
1.19 68.21
?
0.83 57.16
?
1.07 38.61
?
1.27 55.92
?
1.17 70.02
?
0.81 58.36
?
1.07
ONLINE 39.57
?
1.21 58.24
?
1.14 70.16
?
0.78 60.04
?
1.02 40.62
?
1.23 59.72
?
1.11 71.94
?
0.74 61.26
?
1.01
German?English
CUNI 28.20 28.34
?
1.12 46.66
?
1.13 61.53
?
1.03 50.57
?
0.93 30.69
?
1.19 48.91
?
1.16 64.12
?
1.04 52.52
?
0.95
CUNI 28.85 28.99
?
1.15 47.12
?
1.15 61.98
?
1.07 50.72
?
0.98 31.37
?
1.21 49.29
?
1.13 64.53
?
1.05 52.64
?
0.98
POSTECH 25.92 25.99
?
1.06 43.66
?
1.14 59.62
?
0.92 47.13
?
0.90 26.97
?
1.06 45.13
?
1.12 61.53
?
0.89 48.37
?
0.88
UEDIN 37.31 37.53
?
1.19 55.72
?
1.14 68.82
?
0.99 58.35
?
0.95 38.60
?
1.25 57.18
?
1.12 70.46
?
0.98 59.53
?
0.94
UM-DA 35.71 35.81
?
1.23 53.08
?
1.16 66.82
?
0.98 55.91
?
0.96 36.55
?
1.27 54.01
?
1.13 68.05
?
0.97 56.78
?
0.95
CUNI 30.58 30.71
?
1.10 48.68
?
1.09 63.19
?
1.08 52.72
?
0.94 33.14
?
1.19 50.98
?
1.06 65.88
?
1.04 54.74
?
0.94
CUNI 30.22 30.32
?
1.12 47.71
?
1.18 62.20
?
1.10 52.17
?
0.91 32.75
?
1.20 50.00
?
1.14 64.87
?
1.06 54.19
?
0.92
UM-WDA 32.70 32.88
?
1.19 49.60
?
1.18 63.74
?
1.01 53.50
?
0.96 33.95
?
1.23 51.05
?
1.19 65.54
?
0.98 54.73
?
0.96
ONLINE 41.18
?
1.24 59.33
?
1.09 70.95
?
0.92 61.92
?
1.01 42.29
?
1.23 60.76
?
1.08 72.51
?
0.88 63.06
?
0.96
French?English
CUNI 34.42 34.55
?
1.20 52.24
?
1.17 64.52
?
1.03 56.48
?
0.91 36.52
?
1.23 54.35
?
1.12 67.07
?
1.00 58.34
?
0.91
CUNI 33.67 33.59
?
1.16 50.39
?
1.23 61.75
?
1.16 56.74
?
0.97 35.55
?
1.21 52.55
?
1.26 64.45
?
1.13 58.63
?
0.91
DCU-B 44.85 45.01
?
1.24 62.57
?
1.12 74.11
?
0.78 64.33
?
0.99 46.12
?
1.26 64.04
?
1.06 75.84
?
0.74 65.55
?
0.94
UEDIN 46.44 46.68
?
1.26 64.12
?
1.16 74.47
?
0.87 66.40
?
0.96 48.01
?
1.29 65.70
?
1.15 76.30
?
0.86 67.76
?
0.91
UM-DA 47.08 47.22
?
1.33 64.08
?
1.16 75.41
?
0.88 66.15
?
0.96 48.23
?
1.31 65.36
?
1.10 76.95
?
0.89 67.18
?
0.93
CUNI 34.74 34.89
?
1.12 52.39
?
1.16 63.76
?
1.09 57.29
?
0.94 36.84
?
1.17 54.56
?
1.13 66.43
?
1.07 59.14
?
0.90
CUNI 35.04 34.99
?
1.18 52.11
?
1.24 63.24
?
1.09 57.51
?
0.97 37.04
?
1.18 54.38
?
1.17 66.02
?
1.05 59.55
?
0.93
UM-WDA 43.84 44.06
?
1.32 61.14
?
1.18 73.13
?
0.87 63.09
?
1.00 45.17
?
1.36 62.63
?
1.15 74.94
?
0.84 64.37
?
0.99
ONLINE 46.99
?
1.35 64.31
?
1.12 76.07
?
0.78 66.09
?
1.00 47.99
?
1.33 65.65
?
1.07 77.65
?
0.75 67.20
?
0.96
English?Czech
CUNI 17.36 17.65
?
0.96 37.17
?
1.02 49.13
?
0.98 40.31
?
0.95 18.75
?
0.96 38.32
?
1.02 50.82
?
0.91 41.39
?
0.94
CUNI 16.64 16.89
?
0.93 36.57
?
1.05 48.79
?
0.98 39.46
?
0.90 17.94
?
0.96 37.74
?
1.03 50.50
?
0.97 40.59
?
0.91
UEDIN 23.45 23.74
?
1.00 44.20
?
1.10 55.38
?
0.88 46.23
?
0.99 24.20
?
1.00 44.92
?
1.08 56.38
?
0.90 46.78
?
1.00
UM-DA 22.61 22.72
?
0.98 42.73
?
1.16 54.12
?
0.93 44.73
?
1.01 23.12
?
1.01 43.41
?
1.14 55.11
?
0.93 45.32
?
1.02
CUNI 20.56 20.84
?
1.01 39.98
?
1.09 51.98
?
0.99 42.86
?
1.00 22.03
?
1.05 41.19
?
1.08 53.66
?
0.97 43.93
?
1.01
CUNI 19.50 19.72
?
0.97 38.09
?
1.10 50.12
?
1.06 41.50
?
0.96 20.91
?
1.02 39.26
?
1.12 51.79
?
1.04 42.59
?
0.96
UM-WDA 22.14 22.33
?
0.96 42.30
?
1.11 53.89
?
0.92 44.48
?
1.01 22.72
?
0.97 43.02
?
1.09 54.89
?
0.95 45.08
?
0.99
ONLINE 33.45
?
1.28 51.64
?
1.28 61.82
?
1.10 53.97
?
1.18 34.02
?
1.31 52.35
?
1.22 62.84
?
1.08 54.52
?
1.18
English?German
CUNI 12.52 12.64
?
0.77 29.84
?
0.99 45.38
?
1.14 34.69
?
0.81 16.63
?
0.91 33.63
?
1.07 50.03
?
1.24 38.43
?
0.87
CUNI 12.42 12.53
?
0.77 29.02
?
1.05 44.27
?
1.16 34.62
?
0.78 16.41
?
0.91 32.87
?
1.08 48.99
?
1.21 38.37
?
0.86
POSTECH 15.46 15.59
?
0.91 34.41
?
1.01 49.00
?
0.83 37.11
?
0.90 15.98
?
0.92 34.98
?
1.00 49.94
?
0.81 37.60
?
0.87
UEDIN 20.88 21.01
?
1.03 40.03
?
1.08 55.54
?
0.91 42.95
?
0.90 21.40
?
1.03 40.55
?
1.08 56.33
?
0.92 43.41
?
0.90
UM-DA 20.89 21.09
?
1.07 40.76
?
1.03 55.45
?
0.89 43.02
?
0.93 21.52
?
1.08 41.31
?
1.01 56.38
?
0.90 43.58
?
0.91
CUNI 14.29 14.42
?
0.81 31.82
?
1.03 47.01
?
1.13 36.81
?
0.79 18.87
?
0.90 35.76
?
1.11 51.76
?
1.17 40.65
?
0.87
CUNI 13.44 13.58
?
0.75 30.37
?
1.03 45.80
?
1.14 35.80
?
0.76 17.84
?
0.89 34.41
?
1.13 50.75
?
1.18 39.85
?
0.78
UM-WDA 18.77 18.91
?
1.00 37.92
?
1.02 53.59
?
0.85 40.90
?
0.86 19.30
?
1.02 38.42
?
1.01 54.40
?
0.85 41.34
?
0.86
ONLINE 23.92
?
1.06 44.33
?
0.97 57.47
?
0.80 46.35
?
0.91 24.29
?
1.07 44.83
?
0.98 58.20
?
0.80 46.71
?
0.92
English?French
CUNI 30.30 30.67
?
1.11 46.59
?
1.09 59.83
?
1.04 50.51
?
0.93 32.06
?
1.12 48.01
?
1.09 61.66
?
1.00 51.83
?
0.94
CUNI 29.35 29.71
?
1.10 45.84
?
1.07 58.81
?
1.04 50.00
?
0.96 31.02
?
1.10 47.24
?
1.09 60.57
?
1.02 51.31
?
0.94
LIMSI 40.14 43.54
?
1.22 59.70
?
1.04 69.45
?
0.86 61.35
?
0.96 44.04
?
1.22 60.32
?
1.03 70.20
?
0.85 61.90
?
0.94
LIMSI 38.83 42.21
?
1.13 58.88
?
1.01 68.70
?
0.81 60.59
?
0.93 42.69
?
1.12 59.53
?
0.98 69.50
?
0.80 61.17
?
0.91
UEDIN 40.74 44.24
?
1.16 60.66
?
1.07 70.35
?
0.82 62.28
?
0.95 44.85
?
1.17 61.43
?
1.05 71.27
?
0.81 62.94
?
0.91
UM-DA 41.24 41.68
?
1.12 58.72
?
1.06 69.37
?
0.78 60.12
?
0.95 42.16
?
1.11 59.39
?
1.05 70.21
?
0.77 60.71
?
0.92
CUNI 32.23 32.61
?
1.09 48.48
?
1.08 61.13
?
1.01 52.24
?
0.93 34.08
?
1.10 49.93
?
1.11 62.92
?
0.99 53.65
?
0.92
CUNI 32.45 32.84
?
1.06 48.68
?
1.06 61.32
?
0.98 52.35
?
0.94 34.22
?
1.07 50.09
?
1.04 63.04
?
0.96 53.67
?
0.91
UM-WDA 40.78 41.16
?
1.13 58.20
?
0.99 68.93
?
0.84 59.64
?
0.94 41.79
?
1.12 59.10
?
0.96 70.01
?
0.84 60.39
?
0.91
ONLINE 58.63
?
1.26 70.70
?
1.12 78.22
?
0.81 71.89
?
0.96 59.27
?
1.26 71.50
?
1.10 79.16
?
0.81 72.63
?
0.94
Table 25: Official results of translation quality evaluation in the medical summary translation subtask.
46
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech?English
CUNI 10.71 10.57
?
3.42 15.72
?
2.77 23.37
?
3.03 18.68
?
2.42 30.13
?
4.85 53.38
?
3.01 62.53
?
2.84 55.44
?
2.87
CUNI 9.92 9.78
?
3.04 16.84
?
2.84 23.80
?
3.08 19.85
?
2.40 28.21
?
4.56 54.15
?
3.04 62.56
?
2.99 55.91
?
2.79
UEDIN 24.66 24.68
?
4.52 39.88
?
3.05 49.97
?
3.29 41.81
?
2.80 28.25
?
4.94 45.31
?
3.14 55.66
?
3.06 46.67
?
2.77
CUNI 12.00 11.86
?
3.42 18.49
?
2.74 24.67
?
2.85 21.08
?
2.29 31.91
?
4.81 57.61
?
3.13 65.02
?
2.99 59.24
?
2.69
CUNI 10.54 10.39
?
3.48 18.86
?
2.48 26.65
?
2.05 20.53
?
2.08 32.39
?
5.45 56.79
?
3.02 65.52
?
2.26 57.96
?
2.56
ONLINE 28.88
?
4.96 47.31
?
3.35 55.19
?
3.21 49.88
?
2.89 35.33
?
5.20 55.80
?
3.20 64.05
?
2.97 57.94
?
2.85
German?English
CUNI 10.90 10.74
?
3.41 18.89
?
2.39 26.09
?
2.00 20.29
?
2.07 32.15
?
5.23 55.56
?
2.90 63.68
?
2.34 56.45
?
2.62
CUNI 10.71 10.55
?
3.47 18.40
?
2.35 25.45
?
2.04 19.84
?
2.07 32.06
?
5.19 54.85
?
2.91 62.87
?
2.39 55.52
?
2.61
POSTECH 18.06 17.97
?
4.38 28.57
?
3.30 40.38
?
2.77 31.79
?
2.80 21.99
?
4.65 35.76
?
3.35 47.84
?
2.82 38.84
?
2.92
POSTECH 17.99 17.88
?
4.72 29.79
?
3.04 41.15
?
2.48 32.49
?
2.63 24.41
?
4.83 41.72
?
3.19 53.33
?
2.55 44.06
?
2.88
UEDIN 23.33 23.39
?
4.37 38.55
?
3.65 48.21
?
3.43 40.75
?
3.05 27.17
?
4.63 43.87
?
3.52 53.76
?
3.48 45.72
?
3.03
CUNI 10.54 10.39
?
3.48 18.86
?
2.48 26.65
?
2.05 20.53
?
2.08 32.39
?
5.45 56.79
?
3.02 65.52
?
2.26 57.96
?
2.56
CUNI 8.75 8.49
?
3.60 19.10
?
2.27 24.98
?
1.95 19.95
?
2.02 30.00
?
5.59 56.07
?
2.92 62.92
?
2.32 56.27
?
2.56
ONLINE 19.97
?
4.46 37.03
?
3.26 43.91
?
3.22 40.95
?
2.93 33.86
?
4.87 53.28
?
3.28 60.86
?
3.22 56.33
?
2.98
French?English
CUNI 13.90 13.79
?
3.61 18.49
?
2.55 28.35
?
2.81 20.36
?
2.20 34.97
?
5.34 59.54
?
2.94 72.30
?
2.63 58.86
?
2.76
CUNI 12.10 11.95
?
3.41 17.23
?
2.57 27.12
?
2.88 19.15
?
2.28 33.74
?
5.01 58.95
?
2.96 71.25
?
2.76 58.20
?
2.81
DCU-Q 30.85 31.24
?
5.08 58.88
?
2.97 67.94
?
2.62 59.19
?
2.62 36.88
?
5.07 66.38
?
2.85 75.86
?
2.37 66.29
?
2.55
DCU-Q 26.51 26.16
?
4.40 48.02
?
3.72 57.34
?
3.24 53.56
?
2.79 28.61
?
4.52 53.65
?
3.73 63.51
?
3.21 59.07
?
2.79
UEDIN 27.20 27.60
?
3.98 38.54
?
3.22 48.81
?
3.26 39.77
?
2.95 32.23
?
4.27 43.66
?
3.20 54.31
?
3.17 44.53
?
2.79
CUNI 14.03 14.00
?
3.30 20.11
?
2.38 29.00
?
2.71 21.62
?
2.22 38.98
?
5.08 62.90
?
2.87 74.49
?
2.45 62.12
?
2.64
CUNI 13.38 13.16
?
3.52 17.79
?
2.56 28.84
?
2.81 19.17
?
2.23 35.00
?
5.20 59.52
?
2.98 73.08
?
2.57 58.41
?
2.68
ONLINE 32.96
?
5.04 53.68
?
3.21 64.27
?
2.80 54.40
?
2.66 38.09
?
5.52 61.44
?
3.08 72.59
?
2.61 61.60
?
2.78
English?Czech
CUNI 8.37 8.00
?
3.65 17.74
?
2.23 26.46
?
1.96 19.48
?
2.10 19.49
?
4.60 41.53
?
2.94 51.34
?
2.51 42.54
?
2.74
CUNI 9.04 8.75
?
3.64 18.25
?
2.27 26.97
?
1.92 19.69
?
2.11 21.46
?
5.05 42.36
?
3.09 51.99
?
2.40 43.18
?
2.68
UEDIN 12.57 12.40
?
3.61 21.15
?
2.96 33.56
?
2.80 22.30
?
2.67 14.06
?
3.80 24.92
?
2.90 37.85
?
2.72 25.58
?
2.70
UEDIN 6.64 6.21
?
4.73 -2.35
?
3.06 5.95
?
3.48 -0.97
?
3.12 14.35
?
3.52 14.51
?
3.19 24.96
?
3.50 15.11
?
3.10
CUNI 9.06 8.64
?
3.82 19.92
?
2.24 26.97
?
1.94 20.82
?
2.06 22.42
?
5.24 44.89
?
2.94 52.89
?
2.40 45.36
?
2.78
CUNI 8.49 8.01
?
6.05 18.13
?
2.28 25.19
?
1.86 19.19
?
2.01 21.04
?
4.80 42.66
?
2.87 50.34
?
2.47 43.30
?
2.74
ONLINE 21.09
?
4.60 48.56
?
2.82 54.72
?
2.51 48.30
?
2.83 24.37
?
4.80 51.93
?
2.74 58.10
?
2.50 51.62
?
2.80
English?German
CUNI 10.17 10.01
?
3.92 26.48
?
3.24 36.71
?
3.37 29.26
?
2.96 13.02
?
4.17 31.96
?
3.41 42.39
?
3.21 34.61
?
2.95
CUNI 9.98 9.69
?
3.94 26.16
?
3.19 35.50
?
3.23 28.86
?
2.94 12.90
?
4.28 31.75
?
3.33 41.24
?
3.21 34.38
?
3.05
POSTECH 13.43 13.01
?
5.91 26.38
?
3.09 35.75
?
3.16 27.86
?
2.82 15.05
?
5.71 30.45
?
3.10 39.89
?
3.14 31.79
?
3.00
POSTECH 13.41 13.15
?
5.21 22.18
?
3.09 30.89
?
3.31 24.17
?
3.06 14.96
?
5.15 26.13
?
3.19 34.92
?
3.40 27.98
?
3.12
UEDIN 10.45 10.14
?
3.86 23.44
?
3.43 34.55
?
3.34 25.46
?
3.17 11.91
?
4.42 27.91
?
3.45 39.08
?
3.42 29.63
?
3.31
CUNI 8.91 7.72
?
6.48 30.05
?
3.22 40.65
?
2.71 31.91
?
2.88 13.66
?
5.37 35.51
?
3.28 46.12
?
2.74 37.27
?
3.01
CUNI 9.14 8.69
?
6.44 27.66
?
3.31 37.95
?
3.45 31.00
?
2.82 14.03
?
5.92 33.53
?
3.45 44.03
?
3.53 36.73
?
3.00
ONLINE 20.07
?
6.06 41.07
?
3.23 47.41
?
2.86 41.61
?
3.02 21.67
?
6.23 43.78
?
3.23 50.18
?
2.95 44.26
?
3.06
English?French
CUNI 13.12 12.92
?
2.84 21.95
?
2.41 33.19
?
2.09 23.70
?
2.24 28.42
?
3.98 51.43
?
2.90 63.74
?
2.35 52.64
?
2.58
CUNI 12.80 12.65
?
2.81 19.16
?
2.61 31.61
?
2.21 21.91
?
2.32 27.52
?
4.05 47.47
?
3.08 61.43
?
2.37 49.82
?
2.72
DCU-Q 27.69 27.84
?
4.11 48.97
?
3.06 60.90
?
2.55 51.84
?
2.83 28.98
?
4.16 51.73
?
3.10 63.84
?
2.47 54.43
?
2.76
UEDIN 20.16 21.76
?
3.42 31.66
?
4.23 44.37
?
4.13 44.29
?
2.73 23.25
?
3.49 35.38
?
4.19 48.52
?
4.07 47.94
?
2.75
CUNI 13.78 13.57
?
3.00 21.92
?
2.51 33.47
?
2.03 24.16
?
2.32 30.07
?
4.10 51.12
?
3.08 63.61
?
2.45 52.96
?
2.67
CUNI 15.27 15.24
?
3.12 23.58
?
2.54 34.39
?
2.54 25.79
?
2.32 31.40
?
4.15 53.60
?
2.96 65.39
?
2.57 55.47
?
2.69
ONLINE 28.93
?
3.66 49.20
?
3.08 60.85
?
2.69 51.68
?
2.78 30.88
?
3.66 52.25
?
3.08 64.06
?
2.62 54.59
?
2.68
Table 26: Official results of translation quality evaluation in the medical query translation subtask.
source lang. ID P@5 P@10 NDCG@5 NDCG@10 MAP Rprec bpref rel
Czech?English CUNI 0.3280 0.3340 0.2873 0.2936 0.2217 0.2362 0.3473 1461
German?English CUNI 0.2800 0.3000 0.2467 0.2630 0.2057 0.2077 0.3310 1426
French?English CUNI 0.3280 0.3380 0.2811 0.2882 0.2206 0.2284 0.3504 1481
DCU-Q 0.3480 0.3460 0.3060 0.3072 0.2252 0.2358 0.3659 1524
UEDIN 0.4440 0.4300 0.3793 0.3826 0.2843 0.2935 0.3936 1544
English (monolingual) 0.4600 0.4700 0.4091 0.4205 0.3035 0.3198 0.3858 1638
Table 27: Official results of retrieval evaluation in the query translation subtask.
47
lation Task, was the lack of domain expertise of
prospective raters. While in the standard task, the
only requirement for the raters was to be a na-
tive speaker of the target language, in the Med-
ical Translation Task, a very good knowledge of
the domain would be necessary to provide reli-
able judgements and the raters with such an ex-
pertise (medical doctors and native speakers) were
not available.
The complete results of the task are presented
in Table 25 (for summary translation) and Ta-
bles 26 and 27 (for query translation). Partici-
pant IDs given in bold indicate primary submis-
sions, IDs in normal font refer to contrastive sub-
missions. The first section for each translation di-
rection (white background) refers to constrained
submissions and the second one (light-gray back-
ground) to unconstrained submissions. The col-
umn denoted as ?original? contains BLEU scores
as reported by the Matrix submission system ob-
tained on the original submitted translations. Due
to punctuation inconsistency in the original refer-
ence translations, we decided to perform punctu-
ation normalization before calculating the official
scores. The columns denoted as ?normalized true-
cased? contain scores obtained on the submitted
translations after punctuation normalization and
the columns denoted as ?normalized lowercased?
contain scores obtained after punctuation normal-
ization and lowercasing. The normalization script
is available in the package with summary transla-
tion test data. The confidence intervals were ob-
tained by bootstrap resampling with a confidence
level of 95%. Figures in bold denote the best con-
strained system and, if its score is higher, the best
unconstrained system for each translation direc-
tion and each metric. For comparison, we also
present results of a major on-line translation sys-
tem (denoted as ONLINE).
The results of the extrinsic evaluation of query
translation submissions are given in 27. We used
the CLEF 2013 eHealth Task 3 test collection con-
taining about 1 million web pages (in English),
50 test queries (originally in English and trans-
lated to Czech, German, and French), and their
relevance assessments. Some of the participants
of the WMT Medical Task (three teams with five
submissions in total) submitted translations of the
queries (from Czech, German, and French) into
English and these translations were used to query
the CLEF 2013 eHealth Task 3 test collection us-
ing a state-of-the-art system based on a BM25
model, described in Pecina et al. (2014). Origi-
nally, we asked for 10 best translations for each
query, but only the best one were used for the
evaluation. The results are provided in terms of
standard IR evaluation measures: precision at a
cut-off of 5 and 10 documents (P@5, P@10),
normalized discounted cumulative gain (J?arvelin
and Kek?al?ainen, 2002) at 5 and 10 documents
(NDCG@5, NDCG@10), mean average precision
(MAP) (Voorhees and Harman, 2005), precision
reached after R documents retrieved, where R in-
dicates the number of the relevant documents for
each query in the entire collection (Rprec), binary
preference (bpref) (Buckley and Voorhees, 2004),
and number or relevant documents retrieved (rel).
The cross-lingual results are also compared with
the monolingual one (obtained by using the refer-
ence (English) translations of the test topics) to see
how the system would perform if the queries were
translated perfectly.
5.6 Discussion and Conclusion
Both the subtasks turned out to be quite challeng-
ing not only because of the specific domain ? in
summary sentences, we can observe much higher
density of terminology than in ordinary sentences;
the queries, which are also rich in terminology, do
not form sentences at all.
Most submissions were based on systems par-
ticipating in the standard Translation Task and
trained on the provided data or its subsets CUNI
provided baseline systems for all language pairs in
both subtasks, which turned to be relatively strong
for the query translation task, especially in trans-
lation to English, but only in terms of scores ob-
tained on normalized and lowercased translations
since their truecasing component did not perform
well.
In the summary translation subtask, the best
overall results were achieved by the UEDIN team
which won for DE?EN, EN?CS, and EN?FR, fol-
lowed by the UM-DA team, which performed on
par with UEDIN in all other translation.
The unconstrained submissions in almost all
cases did not outperform the results of the con-
strained submissions. Some improvements were
observed in the query translations subtasks by the
CUNI?s unconstrained system with language mod-
els trained on larger in-domain data.
The ONLINE system outperforms all other sub-
48
missions with only two exceptions ? the UM-DA?s
and UEDIN?s systems for the summary translation
in the FR?EN direction, though the score differ-
ences are within the 95% confidence interval.
In the query translation subtask, DCU-Q built
a system designed specifically for terminology
translation between French and English and out-
performed all other participants in translation into
English; however, the confidence intervals in the
query translation task are much wider and most of
the differences in scores of the automatic metrics
are not statistically significant.
The extrinsic evaluation in the cross-lingual in-
formation retrieval was conducted for translations
into English only. CUNI provided the baselines
for all directions, but other submissions were done
for FR?EN only. Here, the winner is UEDIN, who
outperformed both CUNI and DCU-Q, and their
scores are very close to those obtained using the
reference English translations.
Acknowledgments
This work was supported in parts by the
MosesCore, Casmacat, Khresmoi, Matecat and
QTLaunchPad projects funded by the European
Commission (7th Framework Programme), and by
gifts from Yandex.
We would also like to thank our colleagues Ma-
tou?s Mach?a?cek and Martin Popel for detailed dis-
cussions.
References
Avramidis, E. (2014). Efforts on machine learning
over human-mediated translation edit rate. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Beck, D., Shah, K., and Specia, L. (2014). Shef-
lite 2.0: Sparse multi-task gaussian processes
for translation quality estimation. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Bic?ici, E. (2013). Referential translation machines
for quality estimation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, Sofia, Bulgaria.
Bic?ici, E., Liu, Q., and Way, A. (2014). Parallel
FDA5 for fast deployment of accurate statisti-
cal machine translation systems. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, USA. Association
for Computational Linguistics.
Bicici, E., Liu, Q., and Way, A. (2014). Parallel
fda5 for fast deployment of accurate statistical
machine translation systems. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bicici, E. and Way, A. (2014). Referential transla-
tion machines for predicting translation quality.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Bojar, O., Buck, C., Callison-Burch, C., Feder-
mann, C., Haddow, B., Koehn, P., Monz, C.,
Post, M., Soricut, R., and Specia, L. (2013).
Findings of the 2013 Workshop on Statistical
Machine Translation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 1?42, Sofia, Bulgaria. Association
for Computational Linguistics.
Bojar, O., Diatka, V., Rychl?y, P., Stra?n?ak, P.,
Tamchyna, A., and Zeman, D. (2014). Hindi-
English and Hindi-only Corpus for Machine
Translation. In Proceedings of the Ninth Inter-
national Language Resources and Evaluation
Conference, Reykjavik, Iceland. ELRA.
Bojar, O., Ercegov?cevi?c, M., Popel, M., and
Zaidan, O. (2011). A grain of salt for the WMT
manual evaluation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation,
pages 1?11, Edinburgh, Scotland. Association
for Computational Linguistics.
Borisov, A. and Galinskaya, I. (2014). Yandex
school of data analysis russian-english machine
translation system for wmt14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bouayad-Agha, N., Scott, D. R., and Power, R.
(2000). Integrating content and style in doc-
uments: A case study of patient information
leaflets. Information Design Journal, 9(2?
3):161?176.
Buckley, C. and Voorhees, E. M. (2004). Re-
trieval evaluation with incomplete information.
49
In Proceedings of the 27th Annual International
ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 25?
32, Sheffield, United Kingdom.
Buitelaar, P., Sacaleanu, B.,
?
Spela Vintar, Stef-
fen, D., Volk, M., Dejean, H., Gaussier, E.,
Widdows, D., Weiser, O., and Frederking, R.
(2003). Multilingual concept hierarchies for
medical information organization and retrieval.
Public deliverable, MuchMore project.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2007). (Meta-) evaluation
of machine translation. In Proceedings of the
Second Workshop on Statistical Machine Trans-
lation (WMT07), Prague, Czech Republic.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2008). Further meta-
evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Ma-
chine Translation (WMT08), Colmbus, Ohio.
Callison-Burch, C., Koehn, P., Monz, C., Pe-
terson, K., Przybocki, M., and Zaidan, O. F.
(2010). Findings of the 2010 joint workshop
on statistical machine translation and metrics
for machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation (WMT10), Uppsala, Sweden.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10?
51, Montr?eal, Canada. Association for Compu-
tational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Schroeder, J. (2009). Findings of the 2009
workshop on statistical machine translation. In
Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT09), Athens,
Greece.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64, Edinburgh,
Scotland.
Camargo de Souza, J. G., Gonz?alez-Rubio, J.,
Buck, C., Turchi, M., and Negri, M. (2014).
Fbk-upv-uedin participation in the wmt14 qual-
ity estimation shared-task. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Cap, F., Weller, M., Ramm, A., and Fraser, A.
(2014). Cims ? the cis and ims joint submis-
sion to wmt 2014 translating from english into
german. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Cohen, J. (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurment, 20(1):37?46.
Cohn, T. and Specia, L. (2013). Modelling an-
notator bias with multi-task gaussian processes:
An application to machine translation quality
estimation. In Proceedings of the 51st An-
nual Meeting of the Association for Compu-
tational Linguistics, ACL-2013, pages 32?42,
Sofia, Bulgaria.
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the ACL-02 conference on Empir-
ical methods in natural language processing -
Volume 10, EMNLP ?02, pages 1?8, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Costa-juss`a, M. R., Gupta, P., Rosso, P., and
Banchs, R. E. (2014). English-to-hindi sys-
tem description for wmt 2014: Deep source-
context features for moses. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Do, Q. K., Herrmann, T., Niehues, J., Allauzen,
A., Yvon, F., and Waibel, A. (2014). The
kit-limsi translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Dungarwal, P., Chatterjee, R., Mishra, A.,
Kunchukuttan, A., Shah, R., and Bhattacharyya,
P. (2014). The iit bombay hindi-english transla-
tion system at wmt 2014. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
50
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014a). Edinburgh?s phrase-based machine
translation systems for wmt-14. In Proceedings
of the ACL 2014 Ninth Workshop of Statistical
Machine Translation, Baltimore, USA.
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014b). Edinburghs phrase-based machine
translation systems for wmt-14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Du?sek, O., Haji?c, J., Hlav?a?cov?a, J., Nov?ak, M.,
Pecina, P., Rosa, R., Tamchyna, A., Ure?sov?a,
Z., and Zeman, D. (2014). Machine transla-
tion of medical texts in the khresmoi project. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
Federmann, C. (2012). Appraise: An Open-
Source Toolkit for Manual Evaluation of Ma-
chine Translation Output. The Prague Bulletin
of Mathematical Linguistics (PBML), 98:25?
35.
Foster, J. (2007). Treebanks gone bad: Parser eval-
uation and retraining using a treebank of un-
grammatical sentences. International Journal
on Document Analysis and Recognition, 10(3-
4):129?145.
Freitag, M., Peitz, S., Wuebker, J., Ney, H., Huck,
M., Sennrich, R., Durrani, N., Nadejde, M.,
Williams, P., Koehn, P., Herrmann, T., Cho,
E., and Waibel, A. (2014). Eu-bridge mt:
Combined machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Gamon, M., Aue, A., and Smets, M. (2005).
Sentence-level MT evaluation without reference
translations: beyond language modeling. In
Proceedings of the Annual Conference of the
European Association for Machine Translation,
Budapest.
Geurts, P., Ernst, D., and Wehenkel, L. (2006). Ex-
tremely randomized trees. Machine Learning,
63(1):3?42.
Green, S., Cer, D., and Manning, C. (2014).
Phrasal: A toolkit for new directions in statis-
tical machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hardmeier, C., Stymne, S., Tiedemann, J., Smith,
A., and Nivre, J. (2014). Anaphora models and
reordering for phrase-based smt. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Herbrich, R., Minka, T., and Graepel, T. (2006).
TrueSkill
TM
: A Bayesian Skill Rating Sys-
tem. In Proceedings of the Twentieth Annual
Conference on Neural Information Processing
Systems, pages 569?576, Vancouver, British
Columbia, Canada. MIT Press.
Herrmann, T., Mediani, M., Cho, E., Ha, T.-L.,
Niehues, J., Slawik, I., Zhang, Y., and Waibel,
A. (2014). The karlsruhe institute of technol-
ogy translation systems for the wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Hokamp, C., Calixto, I., Wagner, J., and Zhang,
J. (2014). Target-centric features for transla-
tion quality estimation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hopkins, M. and May, J. (2013). Models of trans-
lation competitions. In Proceedings of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1416?1424, Sofia, Bulgaria.
J?arvelin, K. and Kek?al?ainen, J. (2002). Cumu-
lated gain-based evaluation of ir techniques.
ACM Transactions on Information Systems,
20(4):422?446.
Kim, J.-D., Ohta, T., Tateisi, Y., and Tsujii, J.
(2003). GENIA corpus ? a semantically anno-
tated corpus for bio-textmining. Bioinformatics,
19(suppl 1):i180?i182.
Knox, C., Law, V., Jewison, T., Liu, P., Ly,
S., Frolkis, A., Pon, A., Banco, K., Mak, C.,
Neveu, V., Djoumbou, Y., Eisner, R., Guo,
A. C., and Wishart, D. S. (2011). DrugBank 3.0:
a comprehensive resource for Omics research
on drugs. Nucleic acids research, 39(suppl
1):D1035?D1041.
Koehn, P. (2012a). Simulating human judgment in
51
machine translation evaluation campaigns. In
International Workshop on Spoken Language
Translation (IWSLT).
Koehn, P. (2012b). Simulating Human Judgment
in Machine Translation Evaluation Campaigns.
In Proceedings of the Ninth International Work-
shop on Spoken Language Translation, pages
179?184, Hong Kong, China.
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween European languages. In Proceedings of
NAACL 2006 Workshop on Statistical Machine
Translation, New York, New York.
Koppel, M. and Ordan, N. (2011). Translationese
and its dialects. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Techolo-
gies, pages 1318?1326, Portland, Oregon.
Landis, J. R. and Koch, G. G. (1977). The mea-
surement of observer agreement for categorical
data. Biometrics, 33:159?174.
Leusch, G., Ueffing, N., and Ney, H. (2006). Cder:
Efficient mt evaluation using block movements.
In Proceedings of the 11th Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 241?248, Trento,
Italy.
Li, J., Kim, S.-J., Na, H., and Lee, J.-H. (2014a).
Postech?s system description for medical text
translation task. In Proceedings of the ACL
2014 Ninth Workshop of Statistical Machine
Translation, Baltimore, USA.
Li, L., Wu, X., Vaillo, S. C., Xie, J., Way, A., and
Liu, Q. (2014b). The dcu-ictcas mt system at
wmt 2014 on german-english translation task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Lopez, A. (2012). Putting Human Assessments of
Machine Translation Systems in Order. In Pro-
ceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 1?9, Montr?eal,
Canada. Association for Computational Lin-
guistics.
Lu, Y., Wang, L., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira, F. (2014). Domain adapta-
tion for medical text translation using web re-
sources. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
Luong, N. Q., Besacier, L., and Lecouteux, B.
(2014). Lig system for word level qe task at
wmt14. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Luong, N. Q., Lecouteux, B., and Besacier, L.
(2013). LIG system for WMT13 QE task: In-
vestigating the usefulness of features in word
confidence estimation for MT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 384?389, Sofia, Bulgaria.
Association for Computational Linguistics.
Mach?a?cek, M. and Bojar, O. (2014). Results of
the wmt14 metrics shared task. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Matthews, A., Ammar, W., Bhatia, A., Feely, W.,
Hanneman, G., Schlinger, E., Swayamdipta, S.,
Tsvetkov, Y., Lavie, A., and Dyer, C. (2014).
The cmu machine translation systems at wmt
2014. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Neidert, J., Schuster, S., Green, S., Heafield, K.,
and Manning, C. (2014). Stanford universitys
submissions to the wmt 2014 translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Okita, T., Vahid, A. H., Way, A., and Liu, Q.
(2014). Dcu terminology translation system for
medical query subtask at wmt14. In Proceed-
ings of the ACL 2014 Ninth Workshop of Statis-
tical Machine Translation, Baltimore, USA.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a method for automatic eval-
uation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 311?318,
Philadelphia, PA, USA. Association for Com-
putational Linguistics.
P?echeux, N., Gong, L., Do, Q. K., Marie, B.,
Ivanishcheva, Y., Allauzen, A., Lavergne, T.,
52
Niehues, J., Max, A., and Yvon, Y. (2014).
LIMSI @ WMT?14 Medical Translation Task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, USA.
Pecina, P., Du?sek, O., Goeuriot, L., Haji?c, J.,
Hlav?a?cov?a, J., Jones, G., Kelly, L., Leveling, J.,
Mare?cek, D., Nov?ak, M., Popel, M., Rosa, R.,
Tamchyna, A., and Ure?sov?a, Z. (2014). Adapta-
tion of machine translation for multilingual in-
formation retrieval in the medical domain. Arti-
ficial Intelligence in Medicine, (0):?.
Peitz, S., Wuebker, J., Freitag, M., and Ney, H.
(2014). The rwth aachen german-english ma-
chine translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Pouliquen, B. and Mazenc, C. (2011). COPPA,
CLIR and TAPTA: three tools to assist in over-
coming the patent barrier at WIPO. In Pro-
ceedings of the Thirteenth Machine Translation
Summit, pages 24?30, Xiamen, China. Asia-
Pacific Association for Machine Translation.
Powers, D. M. W. (2011). Evaluation: from preci-
sion, recall and f-measure to roc, informedness,
markedness & correlation. Journal of Machine
Learning Technologies.
Quernheim, D. and Cap, F. (2014). Large-scale ex-
act decoding: The ims-ttt submission to wmt14.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Rosse, C. and Mejino Jr., J. L. V. (2008). The
foundational model of anatomy ontology. In
Burger, A., Davidson, D., and Baldock, R., ed-
itors, Anatomy Ontologies for Bioinformatics,
volume 6 of Computational Biology, pages 59?
117. Springer London.
Rubino, R., Toral, A., S?anchez-Cartagena, V. M.,
Ferr?andez-Tordera, J., Ortiz Rojas, S., Ram??rez-
S?anchez, G., S?anchez-Mart??nez, F., and Way,
A. (2014). Abu-matran at wmt 2014 transla-
tion task: Two-step data selection and rbmt-
style synthetic rules. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Sakaguchi, K., Post, M., and Van Durme, B.
(2014). Efficient elicitation of annotations for
human evaluation of machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland.
S?anchez-Cartagena, V. M., P?erez-Ortiz, J. A., and
S?anchez-Mart??nez, F. (2014). The ua-prompsit
hybrid machine translation system for the 2014
workshop on statistical machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Scarton, C. and Specia, L. (2014). Exploring con-
sensus in machine translation for quality esti-
mation. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Schwartz, L., Anderson, T., Gwinnup, J., and
Young, K. (2014). Machine translation and
monolingual postediting: The afrl wmt-14 sys-
tem. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Seginer, Y. (2007). Learning Syntactic Structure.
PhD thesis, University of Amsterdam.
Shah, K., Cohn, T., and Specia, L. (2013). An
investigation on the effectiveness of features for
translation quality estimation. In Proceedings
of the Machine Translation Summit XIV, pages
167?174, Nice, France.
Shah, K. and Specia, L. (2014). Quality estimation
for translation selection. In Proceedings of the
17th Annual Conference of the European As-
sociation for Machine Translation, Dubrovnik,
Croatia.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference
of the Association for Machine Translation in
the Americas (AMTA-2006), Cambridge, Mas-
sachusetts.
Souza, J. G. C. d., Espl-Gomis, M., Turchi, M.,
and Negri, M. (2013). Exploiting qualitative in-
formation from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual
53
Meeting of the Association for Computational
Linguistics - Short Papers (ACL Short Papers
2013).
Specia, L., Shah, K., de Souza, J. G. C., and Cohn,
T. (2013). QuEst - A Translation Quality Esti-
mation Framework. In Proceedings of the 51th
Conference of the Association for Computa-
tional Linguistics (ACL), Demo Session, Sofia,
Bulgaria.
Tamchyna, A., Popel, M., Rosa, R., and Bojar, O.
(2014). Cuni in wmt14: Chimera still awaits
bellerophon. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, Balti-
more, Maryland, USA. Association for Compu-
tational Linguistics.
Tan, L. and Pal, S. (2014). Manawi: Using
multi-word expressions and named entities to
improve machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Thompson, P., Iqbal, S., McNaught, J., and Ana-
niadou, S. (2009). Construction of an annotated
corpus to support biomedical information ex-
traction. BMC bioinformatics, 10(1):349.
Tiedemann, J. (2009). News from OPUS ? a
collection of multilingual parallel corpora with
tools and interfaces. In Recent Advances in
Natural Language Processing, volume 5, pages
237?248, Borovets, Bulgaria. John Benjamins.
Tillmann, C., Vogel, S., Ney, H., Zubiaga, A.,
and Sawaf, H. (1997). Accelerated DP based
search for statistical translation. In Kokki-
nakis, G., Fakotakis, N., and Dermatas, E., edi-
tors, Proceedings of the Fifth European Confer-
ence on Speech Communication and Technol-
ogy, pages 2667?2670, Rhodes, Greece. Inter-
national Speech Communication Association.
U.S. National Library of Medicine (2009). UMLS
reference manual. Metathesaurus. Bethesda,
MD, USA.
Voorhees, E. M. and Harman, D. K., editors
(2005). TREC: Experiment and evaluation in
information retrieval, volume 63 of Digital li-
braries and electronic publishing series. MIT
press Cambridge, Cambridge, MA, USA.
Wang, L., Lu, Y., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira., F. (2014). Combining domain
adaptation approaches for medical text transla-
tion. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
W?aschle, K. and Riezler, S. (2012). Analyz-
ing parallelism and domain similarities in the
MAREC patent corpus. In Salampasis, M. and
Larsen, B., editors, Multidisciplinary Informa-
tion Retrieval, volume 7356 of Lecture Notes
in Computer Science, pages 12?27. Springer
Berlin Heidelberg.
Williams, P., Sennrich, R., Nadejde, M., Huck, M.,
Hasler, E., and Koehn, P. (2014). Edinburghs
syntax-based systems at wmt 2014. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Wisniewski, G., P?echeux, N., Allauzen, A., and
Yvon, F. (2014). Limsi submission for wmt?14
qe task. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
wu, x., Haque, R., Okita, T., Arora, P., Way, A.,
and Liu, Q. (2014). Dcu-lingo24 participation
in wmt 2014 hindi-english translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Zde?nka Ure?sov?a, Ond?rej Du?sek, J. H. and Pecina,
P. (2014). Multilingual test sets for machine
translation of search queries for cross-lingual
information retrieval in the medical domain. In
To appear in Proceedings of the Ninth Interna-
tional Conference on Language Resources and
Evaluation, Reykjavik, Iceland.
Zhang, J., Wu, X., Calixto, I., Vahid, A. H., Zhang,
X., Way, A., and Liu, Q. (2014). Experiments in
medical translation shared task at wmt 2014. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
54
A Pairwise System Comparisons by Human Judges
Tables 28?37 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row, ignoring ties. Bolding indicates the winner of the two systems.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
Each table contains final rows showing how likely a system would win when paired against a randomly
selected system (the expected win ratio score) and the rank range according the official method used in
Table 8. Gray lines separate clusters based on non-overlapping rank ranges.
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
A
C
U
-
M
O
S
E
S
ONLINE-B ? .47? .43? .42? .39?
UEDIN-PHRASE .53? ? .44? .44? .41?
UEDIN-SYNTAX .57? .56? ? .49 .48?
ONLINE-A .58? .56? .51 ? .48?
CU-MOSES .61? .59? .52? .52? ?
score .57 .54 .47 .46 .44
rank 1 2 3-4 3-4 5
Table 28: Head to head comparison, ignoring ties, for Czech-English systems
C
U
-
D
E
P
F
I
X
U
E
D
I
N
-
U
N
C
N
S
T
R
C
U
-
B
O
J
A
R
C
U
-
F
U
N
K
Y
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
O
N
L
I
N
E
-
A
C
U
-
T
E
C
T
O
C
O
M
M
E
R
C
I
A
L
1
C
O
M
M
E
R
C
I
A
L
2
CU-DEPFIX ? .50 .42? .48 .44? .43? .41? .35? .30? .24?
UEDIN-UNCNSTR .50 ? .51 .48 .42? .37? .42? .39? .31? .26?
CU-BOJAR .58? .49 ? .49 .45? .44? .40? .36? .32? .24?
CU-FUNKY .52 .52 .51 ? .48 .47? .44? .34? .33? .26?
ONLINE-B .56? .58? .55? .52 ? .48 .47? .41? .31? .26?
UEDIN-PHRASE .57? .63? .56? .53? .52 ? .48 .44? .32? .27?
ONLINE-A .59? .58? .60? .56? .53? .52 ? .45? .37? .30?
CU-TECTO .65? .61? .64? .66? .59? .56? .55? ? .42? .30?
COMMERCIAL1 .70? .69? .68? .67? .69? .68? .63? .58? ? .40?
COMMERCIAL2 .76? .74? .76? .74? .74? .73? .70? .70? .60? ?
score .60 .59 .58 .57 .54 .52 .50 .44 .36 .28
rank 1-3 1-3 1-4 3-4 5-6 5-6 7 8 9 10
Table 29: Head to head comparison, ignoring ties, for English-Czech systems
55
O
N
L
I
N
E
-
B
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
A
L
I
M
S
I
-
K
I
T
E
U
-
B
R
I
D
G
E
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
R
W
T
H
D
C
U
-
I
C
T
C
A
S
C
M
U
R
B
M
T
4
R
B
M
T
1
O
N
L
I
N
E
-
C
ONLINE-B ? .46 .40? .41? .35? .42? .38? .35? .40? .31? .33? .32? .22?
UEDIN-SYNTAX .54 ? .51 .47 .47 .45 .45? .39? .36? .38? .35? .34? .27?
ONLINE-A .60? .49 ? .42? .44? .51 .41? .38? .44? .42? .38? .31? .20?
LIMSI-KIT .59? .53 .58? ? .55 .53 .31? .45? .39? .41? .37? .35? .29?
EU-BRIDGE .65? .53 .56? .45 ? .45 .44? .48 .40? .37? .39? .37? .30?
UEDIN-PHRASE .58? .55 .49 .47 .55 ? .48 .39? .34? .45? .40? .40? .34?
KIT .62? .55? .59? .69? .56? .52 ? .45? .41? .45? .47 .40? .31?
RWTH .65? .61? .62? .55? .52 .61? .55? ? .54 .44? .44? .38? .37?
DCU-ICTCAS .60? .64? .56? .61? .60? .66? .59? .46 ? .51 .49 .46? .40?
CMU .69? .62? .58? .59? .63? .55? .55? .56? .49 ? .53 .42? .43?
RBMT4 .67? .65? .62? .63? .61? .60? .53 .56? .51 .47 ? .51 .37?
RBMT1 .68? .66? .69? .65? .63? .60? .60? .62? .54? .58? .49 ? .38?
ONLINE-C .78? .73? .80? .71? .70? .66? .69? .63? .60? .57? .63? .62? ?
score .63 .58 .58 .55 .55 .54 .49 .47 .45 .44 .44 .40 .32
rank 1 2-3 2-3 4-6 4-6 4-6 7-8 7-8 9-11 9-11 9-11 12 13
Table 30: Head to head comparison, ignoring ties, for German-English systems
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
P
R
O
M
T
-
H
Y
B
R
I
D
P
R
O
M
T
-
R
U
L
E
U
E
D
I
N
-
S
T
A
N
F
O
R
D
E
U
-
B
R
I
D
G
E
R
B
M
T
4
U
E
D
I
N
-
P
H
R
A
S
E
R
B
M
T
1
K
I
T
S
T
A
N
F
O
R
D
-
U
N
C
C
I
M
S
S
T
A
N
F
O
R
D
U
U
O
N
L
I
N
E
-
C
I
M
S
-
T
T
T
U
U
-
D
O
C
E
N
T
UEDIN-SYNTAX ? .55? .46? .45? .46? .44? .41? .45? .43? .41? .38? .38? .36? .33? .38? .30? .30? .25?
ONLINE-B .45? ? .50 .48 .50 .47 .43? .46? .41? .45? .39? .39? .37? .32? .35? .34? .30? .29?
ONLINE-A .54? .50 ? .44? .52 .50 .45? .43? .43? .42? .39? .41? .42? .42? .37? .44? .38? .33?
PROMT-HYBRID .55? .52 .56? ? .45? .47 .47 .46? .50 .44? .42? .40? .41? .38? .39? .39? .33? .34?
PROMT-RULE .54? .50 .48 .55? ? .51 .47 .47 .45? .38? .42? .40? .43? .41? .43? .38? .35? .29?
UEDIN-STANFORD .56? .53 .50 .53 .49 ? .48 .50 .47 .44? .46 .36? .36? .36? .36? .35? .30? .32?
EU-BRIDGE .59? .57? .55? .53 .53 .52 ? .46? .43? .52 .42? .42? .45? .35? .36? .41? .38? .30?
RBMT4 .55? .54? .57? .54? .53 .50 .54? ? .53 .49 .44? .49 .50 .47 .40? .42? .38? .40?
UEDIN-PHRASE .57? .59? .57? .50 .55? .53 .57? .47 ? .50 .55? .47 .45? .44? .43? .42? .37? .34?
RBMT1 .59? .55? .58? .56? .62? .56? .48 .51 .50 ? .47 .47 .45? .47 .43? .42? .38? .41?
KIT .62? .61? .61? .58? .58? .54 .58? .56? .45? .53 ? .47 .49 .46 .43? .48 .34? .37?
STANFORD-UNC .62? .61? .59? .60? .60? .64? .58? .51 .53 .53 .53 ? .48 .47 .45? .45? .39? .41?
CIMS .64? .63? .58? .59? .57? .64? .55? .50 .55? .55? .51 .52 ? .53 .42? .52 .47 .42?
STANFORD .67? .68? .58? .62? .59? .64? .65? .53 .56? .53 .54 .53 .47 ? .53 .42? .39? .48
UU .62? .65? .62? .61? .57? .64? .64? .60? .57? .57? .57? .55? .58? .47 ? .46? .45? .38?
ONLINE-C .70? .66? .56? .61? .62? .65? .59? .58? .58? .58? .52 .55? .48 .58? .54? ? .48 .47
IMS-TTT .70? .70? .62? .67? .65? .70? .62? .62? .63? .62? .66? .61? .53 .61? .55? .52 ? .49
UU-DOCENT .75? .71? .67? .66? .71? .68? .70? .60? .66? .59? .63? .59? .58? .52 .62? .53 .51 ?
score .60 .59 .56 .56 .56 .56 .54 .51 .51 .50 .48 .47 .46 .44 .43 .42 .38 .37
rank 1-2 1-2 3-6 3-6 3-6 3-6 7 8-10 8-10 8-10 11-12 11-13 12-14 13-15 14-16 15-16 17-18 17-18
Table 31: Head to head comparison, ignoring ties, for English-German systems
56
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
O
N
L
I
N
E
-
B
S
T
A
N
F
O
R
D
O
N
L
I
N
E
-
A
R
B
M
T
1
R
B
M
T
4
O
N
L
I
N
E
-
C
UEDIN-PHRASE ? .48 .48 .45? .43? .28? .28? .19?
KIT .52 ? .54? .48 .44? .31? .29? .21?
ONLINE-B .52 .46? ? .51 .47 .31? .30? .24?
STANFORD .55? .52 .49 ? .46? .34? .30? .23?
ONLINE-A .57? .56? .53 .54? ? .32? .29? .21?
RBMT1 .72? .69? .69? .66? .68? ? .42? .33?
RBMT4 .72? .71? .70? .70? .71? .58? ? .39?
ONLINE-C .81? .79? .76? .77? .79? .67? .61? ?
score .63 .60 .59 .58 .57 .40 .35 .25
rank 1 2-4 2-4 2-4 5 6 7 8
Table 32: Head to head comparison, ignoring ties, for French-English systems
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
M
A
T
R
A
N
M
A
T
R
A
N
-
R
U
L
E
S
O
N
L
I
N
E
-
A
U
U
-
D
O
C
E
N
T
P
R
O
M
T
-
H
Y
B
R
I
D
U
A
P
R
O
M
T
-
R
U
L
E
R
B
M
T
1
R
B
M
T
4
O
N
L
I
N
E
-
C
ONLINE-B ? .46? .48 .46? .50 .41? .39? .39? .37? .38? .37? .35? .27?
UEDIN-PHRASE .54? ? .50 .47 .46 .46? .42? .41? .46? .42? .35? .34? .33?
KIT .52 .50 ? .53 .51 .50 .43? .49 .41? .42? .35? .37? .29?
MATRAN .54? .53 .47 ? .49 .50 .43? .43? .38? .48 .40? .34? .32?
MATRAN-RULES .50 .54 .49 .51 ? .53 .40? .45? .46? .42? .44? .40? .34?
ONLINE-A .59? .54? .50 .50 .47 ? .44? .49 .47 .45? .42? .37? .34?
UU-DOCENT .61? .58? .57? .57? .60? .56? ? .43? .52 .46? .39? .44? .33?
PROMT-HYBRID .61? .59? .51 .57? .55? .51 .57? ? .50 .41? .46? .44? .35?
UA .63? .54? .59? .62? .54? .53 .48 .50 ? .49 .46? .43? .34?
PROMT-RULE .62? .58? .58? .52 .58? .55? .54? .59? .51 ? .47 .39? .37?
RBMT1 .63? .65? .65? .60? .56? .58? .61? .54? .54? .53 ? .46? .45?
RBMT4 .65? .66? .63? .66? .60? .63? .56? .56? .57? .61? .54? ? .45?
ONLINE-C .73? .67? .71? .67? .66? .66? .67? .65? .66? .63? .55? .55? ?
score .59 .57 .55 .55 .54 .53 .49 .49 .48 .47 .43 .40 .34
rank 1 2-4 2-5 2-5 4-6 4-6 7-9 7-10 7-10 8-10 11 12 13
Table 33: Head to head comparison, ignoring ties, for English-French systems
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
U
E
D
I
N
-
S
Y
N
T
A
X
C
M
U
U
E
D
I
N
-
P
H
R
A
S
E
A
F
R
L
I
I
T
-
B
O
M
B
A
Y
D
C
U
-
L
I
N
G
O
2
4
I
I
I
T
-
H
Y
D
E
R
A
B
A
D
ONLINE-B ? .36? .33? .37? .31? .21? .20? .14? .00
ONLINE-A .64? ? .48 .47? .44? .31? .30? .24? .12?
UEDIN-SYNTAX .67? .52 ? .47 .46? .33? .29? .24? .12?
CMU .63? .53? .53 ? .47 .37? .31? .26? .11?
UEDIN-PHRASE .69? .56? .54? .53 ? .40? .33? .25? .11?
AFRL .79? .69? .67? .63? .60? ? .53 .40? .16?
IIT-BOMBAY .80? .70? .71? .69? .67? .47 ? .44? .19?
DCU-LINGO24 .86? .76? .76? .74? .75? .60? .56? ? .19?
IIIT-HYDERABAD .94? .88? .88? .89? .89? .84? .81? .81? ?
score .75 .62 .61 .60 .57 .44 .41 .34 .13
rank 1 2-3 2-4 3-4 5 6-7 6-7 8 9
Table 34: Head to head comparison, ignoring ties, for Hindi-English systems
57
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
U
E
D
I
N
-
U
N
C
N
S
T
R
U
E
D
I
N
-
P
H
R
A
S
E
C
U
-
M
O
S
E
S
I
I
T
-
B
O
M
B
A
Y
I
P
N
-
U
P
V
-
C
N
T
X
T
D
C
U
-
L
I
N
G
O
2
4
I
P
N
-
U
P
V
-
N
O
D
E
V
M
A
N
A
W
I
-
H
1
M
A
N
A
W
I
M
A
N
A
W
I
-
R
M
O
O
V
ONLINE-B ? .49 .28? .29? .27? .23? .22? .20? .17? .12? .13? .13?
ONLINE-A .51 ? .31? .29? .27? .25? .20? .20? .21? .19? .16? .15?
UEDIN-UNCNSTR .72? .69? ? .44? .49 .39? .40? .34? .39? .29? .30? .27?
UEDIN-PHRASE .71? .71? .56? ? .48 .45? .44? .39? .37? .31? .31? .32?
CU-MOSES .73? .73? .51 .52 ? .47 .42? .40? .45? .36? .35? .33?
IIT-BOMBAY .77? .75? .61? .55? .53 ? .50 .47 .45? .41? .40? .36?
IPN-UPV-CNTXT .78? .80? .60? .56? .58? .50 ? .51 .41? .40? .40? .37?
DCU-LINGO24 .80? .80? .66? .61? .60? .53 .49 ? .52 .41? .41? .39?
IPN-UPV-NODEV .83? .79? .61? .63? .55? .55? .59? .48 ? .46? .44? .38?
MANAWI-H1 .88? .81? .71? .69? .64? .59? .60? .59? .54? ? .35? .34?
MANAWI .87? .84? .70? .69? .65? .60? .60? .59? .56? .65? ? .39?
MANAWI-RMOOV .87? .85? .73? .68? .67? .64? .63? .61? .62? .66? .61? ?
score .77 .75 .57 .54 .52 .47 .46 .43 .42 .38 .35 .31
rank 1 2 3 4-5 4-5 6-7 6-7 8-9 8-9 10-11 10-11 12
Table 35: Head to head comparison, ignoring ties, for English-Hindi systems
A
F
R
L
-
P
E
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
P
R
O
M
T
-
H
Y
B
R
I
D
P
R
O
M
T
-
R
U
L
E
U
E
D
I
N
-
P
H
R
A
S
E
Y
A
N
D
E
X
O
N
L
I
N
E
-
G
A
F
R
L
U
E
D
I
N
-
S
Y
N
T
A
X
K
A
Z
N
U
R
B
M
T
1
R
B
M
T
4
AFRL-PE ? .42? .40? .39? .39? .41? .35? .39? .28? .26? .26? .29? .21?
ONLINE-B .58? ? .42? .43? .45? .45? .42? .43? .46? .37? .33? .29? .31?
ONLINE-A .60? .58? ? .50 .45? .51 .47 .45? .42? .40? .33? .32? .30?
PROMT-HYBRID .61? .57? .50 ? .47 .45? .49 .44? .43? .44? .39? .31? .27?
PROMT-RULE .61? .55? .55? .53 ? .46? .47 .49 .48 .42? .36? .34? .30?
UEDIN-PHRASE .59? .55? .49 .55? .54? ? .49 .50 .47 .44? .32? .37? .29?
YANDEX .65? .58? .53 .51 .53 .51 ? .48 .50 .43? .34? .36? .34?
ONLINE-G .61? .57? .55? .56? .51 .50 .52 ? .48 .43? .39? .35? .30?
AFRL .72? .54? .58? .57? .52 .53 .50 .52 ? .44? .41? .41? .37?
UEDIN-SYNTAX .74? .63? .60? .56? .58? .56? .57? .57? .56? ? .51 .36? .37?
KAZNU .74? .67? .67? .61? .64? .68? .66? .61? .59? .49 ? .44? .38?
RBMT1 .71? .71? .68? .69? .66? .63? .64? .65? .59? .64? .56? ? .47
RBMT4 .79? .69? .70? .73? .70? .71? .66? .70? .63? .63? .62? .53 ?
score .66 .58 .55 .55 .53 .53 .52 .51 .49 .45 .40 .36 .32
rank 1 2 3-5 3-5 4-7 5-8 5-8 5-8 9 10 11 12 13
Table 36: Head to head comparison, ignoring ties, for Russian-English systems
P
R
O
M
T
-
R
U
L
E
O
N
L
I
N
E
-
B
P
R
O
M
T
-
H
Y
B
R
I
D
U
E
D
I
N
-
U
N
C
N
S
T
R
O
N
L
I
N
E
-
G
O
N
L
I
N
E
-
A
U
E
D
I
N
-
P
H
R
A
S
E
R
B
M
T
4
R
B
M
T
1
PROMT-RULE ? .51 .45? .43? .43? .39? .38? .15? .00
ONLINE-B .49 ? .50 .47? .38? .36? .38? .16? .13?
PROMT-HYBRID .55? .50 ? .49 .47 .39? .40? .18? .15?
UEDIN-UNCNSTR .57? .53? .51 ? .50 .44? .36? .25? .18?
ONLINE-G .57? .62? .53 .50 ? .46? .44? .23? .18?
ONLINE-A .61? .64? .61? .56? .54? ? .49 .24? .18?
UEDIN-PHRASE .62? .62? .60? .64? .56? .51 ? .30? .21?
RBMT4 .85? .84? .82? .75? .77? .76? .70? ? .42?
RBMT1 .91? .87? .85? .82? .82? .82? .79? .58? ?
score .64 .64 .61 .58 .55 .51 .49 .26 .19
rank 1-2 1-2 3 4-5 4-5 6-7 6-7 8 9
Table 37: Head to head comparison, ignoring ties, for English-Russian systems
58
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 97?104,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
Edinburgh?s Phrase-based Machine Translation Systems for WMT-14
Nadir Durrani Barry Haddow Philipp Koehn
School of Informatics
University of Edinburgh
{dnadir,bhaddow,pkoehn}@inf.ed.ac.uk
Kenneth Heafield
Computer Science Department
Stanford University
heafield@cs.stanford.edu
Abstract
This paper describes the University of Ed-
inburgh?s (UEDIN) phrase-based submis-
sions to the translation and medical trans-
lation shared tasks of the 2014 Work-
shop on Statistical Machine Translation
(WMT). We participated in all language
pairs. We have improved upon our 2013
system by i) using generalized represen-
tations, specifically automatic word clus-
ters for translations out of English, ii) us-
ing unsupervised character-based models
to translate unknown words in Russian-
English and Hindi-English pairs, iii) syn-
thesizing Hindi data from closely-related
Urdu data, and iv) building huge language
on the common crawl corpus.
1 Translation Task
Our baseline systems are based on the setup de-
scribed in (Durrani et al., 2013b) that we used
for the Eighth Workshop on Statistical Machine
Translation (Bojar et al., 2013). The notable fea-
tures of these systems are described in the follow-
ing section. The experiments that we carried out
for this year?s translation task are described in the
following sections.
1.1 Baseline
We trained our systems with the following set-
tings: a maximum sentence length of 80, grow-
diag-final-and symmetrization of GIZA++ align-
ments, an interpolated Kneser-Ney smoothed 5-
gram language model with KenLM (Heafield,
2011) used at runtime, hierarchical lexicalized re-
ordering (Galley and Manning, 2008), a lexically-
driven 5-gram operation sequence model (OSM)
(Durrani et al., 2013a) with 4 count-based sup-
portive features, sparse domain indicator, phrase
length, and count bin features (Blunsom and Os-
borne, 2008; Chiang et al., 2009), a distortion limit
of 6, maximum phrase-length of 5, 100-best trans-
lation options, Minimum Bayes Risk decoding
(Kumar and Byrne, 2004), Cube Pruning (Huang
and Chiang, 2007), with a stack-size of 1000
during tuning and 5000 during test and the no-
reordering-over-punctuation heuristic (Koehn and
Haddow, 2009). We used POS and morphologi-
cal tags as additional factors in phrase translation
models (Koehn and Hoang, 2007) for German-
English language pairs. We also trained target se-
quence models on the in-domain subset of the par-
allel corpus using Kneser-Ney smoothed 7-gram
models. We used syntactic-preordering (Collins
et al., 2005) and compound splitting (Koehn and
Knight, 2003) for German-to-English systems.
We used trivia tokenizer for tokenizing Hindi.
The systems were tuned on a very large tun-
ing set consisting of the test sets from 2008-2012,
with a total of 13,071 sentences. We used news-
test 2013 for the dev experiments. For Russian-
English pairs news-test 2012 was used for tuning
and for Hindi-English pairs, we divided the news-
dev 2014 into two halves, used the first half for
tuning and second for dev experiments.
1.2 Using Generalized Word Representations
We explored the use of automatic word clusters
in phrase-based models (Durrani et al., 2014a).
We computed the clusters with GIZA++?s mkcls
(Och, 1999) on the source and target side of the
parallel training corpus. Clusters are word classes
that are optimized to reduce n-gram perplexity.
By generating a cluster identifier for each out-
put word, we are able to add an n-gram model
97
over these identifiers as an additional scoring func-
tion. The inclusion of such an additional factor
is trivial given the factored model implementation
(Koehn and Hoang, 2007) of Moses (Koehn et al.,
2007). The n-gram model is trained in the similar
way as the regular language model. We trained
domain-specific language models separately and
then linearly interpolated them using SRILM with
weights optimized on the tuning set (Schwenk and
Koehn, 2008).
We also trained OSM models over cluster-ids
(?). The lexically driven OSM model falls back to
very small context sizes of two to three operations
due to data sparsity. Learning operation sequences
over cluster-ids enables us to learn richer trans-
lation and reordering patterns that can generalize
better in sparse data conditions. Table 1 shows
gains from adding target LM and OSM models
over cluster-ids. Using word clusters was found
more useful translating from English-to-*.
from English into English
Lang B
0
+Cid ? B
0
+Cid ?
de 20.60 20.85 +0.25 27.44 27.34 -0.10
cs 18.84 19.39 +0.55 26.42 26.42 ?0.00
fr 30.73 30.82 +0.09 31.64 31.76 +0.12
ru 18.78 19.67 +0.89 24.45 24.63 +0.18
hi 10.39 10.52 +0.13 15.48 15.26 -0.22
Table 1: Using Word Clusters in Phrase-based and
OSM models ? B
0
= System without Clusters,
+Cid = with Cluster
We also trained OSM models over POS and
morph tags. For the English-to-German sys-
tem we added an OSM model over [pos, morph]
(source:pos, target:morph) and for the German-
to-English system we added an OSM model over
[morph,pos] (source:morph, target:pos), a config-
uration that was found to work best in our previous
experiments (Birch et al., 2013). Table 2 shows
gains from additionally using OSM models over
POS/morph tags.
Lang B
0
+OSM
p,m
?
en-de 20.44 20.60 +0.16
de-en 27.24 27.44 +0.20
Table 2: Using POS and Morph Tags in
OSM models ? B
0
= Baseline, +OSM
p,m
=
POS/Morph-based OSM
1.3 Unsupervised Transliteration Model
Last year, our Russian-English systems performed
badly on the human evaluation. In comparison
other participants that used transliteration did well.
We could not train a transliteration system due
to unavailability of a transliteration training data.
This year we used an EM-based method to in-
duce unsupervised transliteration models (Durrani
et al., 2014b). We extracted transliteration pairs
automatically from the word-aligned parallel data
and used it to learn a transliteration system. We
then built transliteration phrase-tables for trans-
lating OOV words and used the post-decoding
method (Method 2 as described in the paper) to
translate these.
Pair Training OOV B
0
+T
r
?
ru-en 232K 1356 24.63 25.06 +0.41
en-ru 232K 681 19.67 19.91 +0.24
hi-en 38K 503 14.67 15.48 +0.81
en-hi 38K 394 11.76 12.83 +1.07
Table 3: Using Unsupervised Transliteration
Model ? Training = Extracted Transliteration Cor-
pus (types), OOV = Out-of-vocabulary words (to-
kens) B
0
= System without Transliteration, +T
r
= Transliterating OOVs
Table 3 shows the number (types) of translit-
eration pairs extracted using unsupervised min-
ing, number of OOV words (tokens) in each pair
and the gains achieved by transliterating unknown
words.
1.4 Synthesizing Hindi Data from Urdu
Hindi and Urdu are closely related language pairs
that share grammatical structure and have a large
overlap in vocabulary. This provides a strong
motivation to transform any Urdu-English paral-
lel data into Hindi-English by translating the Urdu
part into Hindi. We made use of the Urdu-English
segment of the Indic multi-parallel corpus (Post
et al., 2012) which contains roughly 87K sentence
pairs. The Hindi-English segment of this corpus
is a subset of parallel data made available for the
translation task but is completely disjoint from the
Urdu-English segment.
We initially trained a Urdu-to-Hindi SMT sys-
tem using a very tiny EMILLE
1
corpus (Baker
1
EMILLE corpus contains roughly 12000 sentences of
Hindi and Urdu comparable data. From these we were able
to sentence align 7000 sentences to build an Urdu-to-Hindi
system.
98
et al., 2002). But we found this system to be use-
less for translating the Urdu part of Indic data due
to domain mismatch and huge number of OOV
words (approximately 310K tokens). To reduce
sparsity we synthesized additional phrase-tables
using interpolation and transliteration.
Interpolation: We trained two phrase transla-
tion tables p(u?
i
|e?
i
) and p(e?
i
|
?
h
i
), from Urdu-
English (Indic corpus) and Hindi-English (Hin-
dEnCorp (Bojar et al., 2014)) bilingual cor-
pora. Given the phrase-table for Urdu-English
p(u?
i
|e?
i
) and the phrase-table for English-Hindi
p(e?
i
|
?
h
i
), we estimated a Urdu-Hindi phrase-table
p(u?
i
|
?
h
i
) using the well-known convolution model
(Utiyama and Isahara, 2007; Wu and Wang, 2007):
p(u?
i
|
?
h
i
) =
?
e?
i
p(u?
i
|e?
i
)p(e?
i
|
?
h
i
)
The number of entries in the baseline Urdu-to-
Hindi phrase-table were approximately 254K. Us-
ing interpolation we were able to build a phrase-
table containing roughly 10M phrases. This re-
duced the number of OOV tokens from 310K to
approximately 50K.
Transliteration: Urdu and Hindi are written in
different scripts (Arabic and Devanagri respec-
tively). We added a transliteration component
to our Urdu-to-Hindi system. An unsupervised
transliteration model is learned from the word-
alignments of Urdu-Hindi parallel data. We were
able to extract around 2800 transliteration pairs.
To learn a richer transliteration model, we addi-
tionally fed the interpolated phrase-table, as de-
scribed above, to the transliteration miner. We
were able to mine additional 21000 translitera-
tion pairs and built a Urdu-Hindi character-based
model from it. The transliteration module can
be used to translate the 50K OOV words but
previous research (Durrani et al., 2010; Nakov
and Tiedemann, 2012) has shown that translit-
eration is useful for more than just translating
OOV words when translating closely related lan-
guage pairs. To fully capitalize on the large over-
lap in Hindi?Urdu vocabulary, we transliterated
each word in the Urdu test-data into Hindi and
produced a phrase-table with 100-best transliter-
ations. The two synthesized (triangulated and
transliterated) phrase-tables are then used along
with the baseline Urdu-to-Hindi phrase-table in
a log-linear model. Detailed results on Urdu-to-
Hindi baseline and improvements obtained from
using transliteration and triangulated phrase-tables
are presented in Durrani and Koehn (2014). Using
our best Urdu-to-Hindi system, we translated the
Urdu part of the multi-indic corpus to form Hindi-
English parallel data. Table 4 shows results from
using the synthesized Hindi-English corpus in iso-
lation (Syn) and on top of the baseline system
(B
0
+ Syn).
Pair B
0
Syn ? B
0
+ Syn ?
hi-en 14.28 10.49 -3.79 14.72 +0.44
en-hi 10.59 9.01 -1.58 11.76 +1.17
Table 4: Evaluating Synthesized (Syn) Hindi-
English Parallel Data, B
0
= System without Syn-
thesized Data
1.5 Huge Language Models
Our unconstrained submissions use an additional
language model trained on web pages from the
2012, 2013, and winter 2013 CommonCrawl.
2
The additional language model is the only differ-
ence between the constrained and unconstrained
submissions; we did not use additional parallel
data. These language models were trained on text
provided by the CommonCrawl foundation, which
they converted to UTF-8 after stripping HTML.
Languages were detected using the Compact Lan-
guage Detection 2
3
and, except for Hindi where
we lack tools, sentences were split with the Eu-
roparl sentence splitter (Koehn, 2005). All text
was then deduplicated, minimizing the impact of
boilerplate, such as social media sharing buttons.
We then tokenized and truecased the text as usual.
Statistics are shown in Table 5. A full description
of the pipeline, including a public data release, ap-
pears in Buck et al. (2014).
Lang Lines (B) Tokens (B) Bytes
en 59.13 975.63 5.14 TiB
de 3.87 51.93 317.46 GiB
fr 3.04 49.31 273.96 GiB
ru 1.79 21.41 220.62 GiB
cs 0.47 5.79 34.67 GiB
hi 0.01 0.28 3.39 GiB
Table 5: Size of huge language model training data
We built unpruned modified Kneser-Ney lan-
guage models using lmplz (Heafield et al., 2013).
2
http://commoncrawl.org
3
https://code.google.com/p/cld2/
99
Pair B
0
+L
newstest 2013 2014 2013 2014
en-de 20.85 20.10 ? 20.61 +0.51
en-cs 19.39 21.00 20.03 +0.64 21.60 +0.60
en-ru 19.90 28.70 20.80 +0.90 29.90 +1.20
en-hi 11.43 11.10 12.83 +1.40 12.50 +1.40
hi-en 15.48 13.90 ? 14.80 +0.90
Table 6: Gains obtained by using huge language
models ? B
0
= Baseline, +L = Adding Huge LM
While the Hindi and Czech models are small
enough to run directly, models for other languages
are quite large.We therefore created a filter that op-
erates directly on files in KenLM trie binary for-
mat, preserving only n-grams whose words all ap-
pear in the target side vocabulary of at least one
source sentence. For example, an English lan-
guage model trained on just the 2012 and 2013
crawls takes 3.5 TB without any quantization. Af-
ter filtering to the Hindi-English tuning set, the
model fit in 908 GB, again without quantization.
We were then able to tune the system on a machine
with 1 TB RAM. Results are shown in Table 6; we
did not submit to English-French because the sys-
tem takes too long to tune.
1.6 Miscellaneous
Hindi-English: 1) A large number of Hindi sen-
tences in the Hindi-English parallel corpus were
ending with a full-stop ?.?, although the end-of-
the-sentence marker in Hindi is ?Danda? (|). Re-
placing full-stops with Danda gave improvement
of +0.20 for hi-en and +0.40 in en-hi. 2) Using
Wiki subtitles did not give any improvement in
BLEU and were in fact harmful for the en-hi di-
rection.
Russian-English: We tried to improve word-
alignments by integrating a transliteration sub-
model into GIZA++ word aligner. The probabil-
ity of a word pair is calculated as an interpola-
tion of the transliteration probability and transla-
tion probability stored in the t-table of the differ-
ent alignment models used by the GIZA++ aligner.
This interpolation is done for all iterations of all
alignment models (See Sajjad et al. (2013) for de-
tails). Due to shortage of time we could only run it
for Russian-to-English. The improved alignments
gave a gain of +0.21 on news-test 2013 and +0.40
on news-test 2014.
Pair GIZA++ Fast Align ?
de-en 24.02 23.89 ?.13
fr-en 30.78 30.66 ?.12
es-en 34.07 34.24 +.17
cs-en 22.63 22.44 ?.19
ru-en 31.68 32.03 +.35
en-de 18.04 17.88 ?.16
en-fr 28.96 28.83 ?.13
en-es 34.15 34.32 +.17
en-cs 15.70 16.02 +.32
avg +.03
Table 7: Comparison of fast word alignment
method (Dyer et al., 2013) against GIZA++
(WMT 2013 data condition, test on new-
stest2012). The method was not used in the official
submission.
Pair Baseline MSD Hier. MSD Hier. MSLR
de-en 27.04 27.10 +.06 27.17 +.13
fr-en 31.63 - 31.65 +.02
es-en 31.20 31.14 ?.06 31.25 +.05
cs-en 26.11 26.32 +.21 26.26 +.15
ru-en 24.09 24.01 ?.08 24.19 +.11
en-de 20.43 20.34 ?.09 20.32 -.11
en-fr 30.54 - 30.52 ?.02
en-es 30.36 30.44 +.08 30.51 +.15
en-cs 18.53 18.59 +.06 18.66 +.13
en-ru 18.37 18.47 +.10 18.19 ?.18
avg + .035 +.045
Table 8: Hierarchical lexicalized reordering model
(Galley and Manning, 2008).
Fast align: In preliminary experiments, we
compared the fast word alignment method by
Dyer et al. (2013) against our traditional use of
GIZA++. Results are quite mixed (Table 7), rang-
ing from a gain of +.35 for Russian-English to a
loss of ?.19 for Czech-English. We stayed with
GIZA++ for all of our other experiments.
Hierarchical lexicalized reordering model:
We explored the use of the hierarchical lexicalized
reordering model (Galley and Manning, 2008)
in two variants: using the same orientations as
our traditional model (monotone, discontinuous,
swap), and one that distinguishes the discontin-
uous orientations to the left and right. Table 8
shows slight improvements with these models, so
we used them in our baseline.
Threshold filtering of phrase table: We exper-
imented with discarding some phrase table entry
due to their low probability. We found that phrase
translations with the phrase translation probability
100
?(f |e)<10
?4
can be safely discarded with almost
no change in translations. However, discarding
phrase translations with the inverse phrase transla-
tion probability ?(e|f)<10
?4
is more risky, espe-
cially with morphologically rich target languages,
so we kept those.
1.7 Summary
Table 9 shows cumulative gains obtained from us-
ing word classes, transliteration and big language
models
4
over the baseline system. Our German-
English constrained systems were used for EU-
Bridge system combination, a collaborative effort
to improve the state-of-the-art in machine transla-
tion (See Freitag et al. (2014) for details).
from English into English
Lang B
0
B
1
? B
0
B
1
?
de 20.44 20.85 +0.41 27.24 27.44 +0.20
cs 18.84 20.03 +1.19 26.42 26.42 ?0.00
fr 30.73 30.82 +0.09 31.64 31.76 +0.12
ru 18.78 20.81 +2.03 24.45 25.21 +0.76
hi 9.27 12.83 +3.56 14.08 15.48 +1.40
Table 9: Cumulative gains obtained for each lan-
guage ? B
0
= Baseline, B
1
= Best System
2 Medical Translation Task
For the medical translation task, the organisers
supplied several medical domain corpora (detailed
on the task website), as well some out-of-domain
patent data, and also all the data available for the
constrained track of the news translation task was
permitted. In general, we attempted to use all of
this data, except for the LDC Gigaword language
model data (for reasons of time) and we divided
the data into ?in-domain? and ?out-of-domain?
corpora. The data sets are summarised in Tables
10 and 11.
In order to create systems for the medical trans-
lation tasks, we used phrase-based Moses with ex-
actly the same settings as for the news translation
task, including the OSM (Durrani et al., 2011),
and compound splitting Koehn and Knight (2003)
for German source. We did not use word clusters
(Section 1.2), as they did not give good results on
this task, but we have yet to find a reason for this.
For language model training, we decided not to
build separate models on each corpus as there was
4
Cumulative gains do not include gains obtain from big
language models for hi-en and en-de.
Data Set cs-en de-en fr-en
coppa-in n n y
PatTR-in-claims n y y
PatTR-in-abstract n y y
PatTR-in-titles n y y
UMLS y y y
MuchMore n y n
EMEA y y y
WikiTitles y y y
PatTR-out n y y
coppa-out n n y
MultiUN n n y
czeng y n n
europarl y y y
news-comm y y y
commoncrawl y y y
FrEnGiga n n y
Table 10: Parallel data sets used in the medical
translation task. The sets above the line were clas-
sified as ?in-domain? and those below as ?out-of-
domain?.
Data Set cs de en fr
PIL n n y n
DrugBank n n y n
WikiArticles y y y y
PatTR-in-description n y y y
GENIA n n y n
FMA n n y n
AACT n n y n
PatTR-out-description n y y y
Table 11: Additional monolingual data used in
the medical translation task. Those above the line
were classified as ?in-domain? and the one below
as ?out-of-domain?. We also used the target sides
of all the parallel corpora for language modelling.
a large variation in corpus sizes. Instead we con-
catenated the in-domain target sides with the in-
domain extra monolingual data to create training
data for an in-domain language model, and simi-
larly for the out-of-domain data. The two language
models were interpolated using SRILM, minimis-
ing perplexity on the Khresmoi summary develop-
ment data.
During system development, we only had 500
sentences of development data (SUMMARY-DEV)
from the Khresmoi project, so we decided to se-
lect further development and devtest data from the
EMEA corpus, reasoning that it was fairly close
in domain to SUMMARY-DEV. We selected a tun-
ing set (5000 sentence pairs, which were added to
SUMMARY-DEV) and a devtest set (3000 sentence
pairs) from EMEA after first de-duplicating it, and
ignoring sentence pairs which were too short, or
101
contained too many capital letters or numbers. The
EMEA contains many duplicated sentences, and
we removed all sentence pairs where either side
was a duplicate, reducing the size of the corpus
to about 25% of the original. We also removed
EMEA from Czeng, since otherwise it would over-
lap with our selected development sets.
We also experimented with modified Moore-
Lewis (Moore and Lewis, 2010; Axelrod et al.,
2011) data selection, using the EMEA corpus as
the in-domain corpus (for the language model re-
quired in MML) and selecting from all the out-of-
domain data.
When running on the final test set (SUMMARY-
TEST) we found that it was better to tune just on
SUMMARY-DEV, even though it was much smaller
than the EMEA dev set we had selected. All but
two (cs-en, de-en) of our submitted systems used
the MML selection, because it worked better on
our EMEA devtest set. However, as can be seen
from Table 12, systems built with all the data gen-
erally perform better. We concluded that EMEA
was not a good representative of the Khresmoi
data, perhaps because of domain differences, or
perhaps just because of the alignment noise that
appears (from informal inspection) to be present
in EMEA.
from English into English
in in+20 in+out in in+20 in+out
de 18.59 20.88 ? 36.17 ? 38.57
cs 18.78 23.45 23.77 30.12 ? 36.32
fr 35.24 40.74 41.04 45.15 46.44 46.58
Table 12: Results (cased BLEU) on the khresmoi
summary test set. The ?in? systems include all
in-domain data, the ?in+20? systems also include
20% of the out-of-domain data and the ?out? sys-
tems include all data. The submitted systems are
shown in italics, except for de-en and cs-en where
we submitted a ?in+out? systems. For de-en, this
was tuned on SUMMARY-DEV plus the EMEA dev
set and scored 37.31, whilst for cs-en we included
LDC Giga in the LM, and scored 36.65.
For translating the Khresmoi queries, we used
the same systems as for the summaries, except that
generally we did not retune on the SUMMARY-DEV
data. We added a post-processing script to strip
out extraneous stop words, which improved BLEU,
but we would not expect it to matter in a real CLIR
system as it would do its own stop-word removal.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme (FP7/2007-2013) un-
der grant agreements n
?
287658 (EU-BRIDGE),
n
?
287688 (MateCat) and n
?
288769 (ACCEPT).
Huge language model experiments made use of
the Stampede supercomputer provided by the
Texas Advanced Computing Center (TACC) at
The University of Texas at Austin under NSF
XSEDE allocation TG-CCR140009. We also ac-
knowledge the support of the Defense Advanced
Research Projects Agency (DARPA) Broad Op-
erational Language Translation (BOLT) program
through IBM. This publication only reflects the
authors? views.
References
Axelrod, A., He, X., and Gao, J. (2011). Domain
adaptation via pseudo in-domain data selection.
In Proceedings of the 2011 Conference on Em-
pirical Methods in Natural Language Process-
ing, pages 355?362, Edinburgh, Scotland, UK.
Association for Computational Linguistics.
Baker, P., Hardie, A., McEnery, T., Cunningham,
H., and Gaizauskas, R. J. (2002). EMILLE,
a 67-million word corpus of indic languages:
Data collection, mark-up and harmonisation. In
LREC.
Birch, A., Durrani, N., and Koehn, P. (2013). Ed-
inburgh SLT and MT system description for the
IWSLT 2013 evaluation. In Proceedings of the
10th International Workshop on Spoken Lan-
guage Translation, pages 40?48, Heidelberg,
Germany.
Blunsom, P. and Osborne, M. (2008). Probabilis-
tic inference for machine translation. In Pro-
ceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing,
pages 215?223, Honolulu, Hawaii. Association
for Computational Linguistics.
Bojar, O., Buck, C., Callison-Burch, C., Feder-
mann, C., Haddow, B., Koehn, P., Monz, C.,
Post, M., Soricut, R., and Specia, L. (2013).
Findings of the 2013 workshop on statistical
machine translation. In Eighth Workshop on
Statistical Machine Translation, WMT-2013,
pages 1?44, Sofia, Bulgaria.
Bojar, O., Diatka, V., Rychl?y, P., Stra?n?ak, P.,
Tamchyna, A., and Zeman, D. (2014). Hindi-
102
English and Hindi-only Corpus for Machine
Translation. In Proceedings of the Ninth In-
ternational Language Resources and Evalua-
tion Conference (LREC?14), Reykjavik, Ice-
land. ELRA, European Language Resources
Association. in prep.
Buck, C., Heafield, K., and van Ooyen, B. (2014).
N-gram counts and language models from the
common crawl. In Proceedings of the Language
Resources and Evaluation Conference, Reyk-
jav??k, Iceland.
Chiang, D., Knight, K., and Wang, W. (2009).
11,001 New Features for Statistical Machine
Translation. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Confer-
ence of the North American Chapter of the As-
sociation for Computational Linguistics, pages
218?226, Boulder, Colorado. Association for
Computational Linguistics.
Collins, M., Koehn, P., and Kucerova, I. (2005).
Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational
Linguistics (ACL?05), pages 531?540, Ann Ar-
bor, Michigan. Association for Computational
Linguistics.
Durrani, N., Fraser, A., Schmid, H., Hoang, H.,
and Koehn, P. (2013a). Can markov mod-
els over minimal translation units help phrase-
based SMT? In Proceedings of the 51st An-
nual Meeting of the Association for Computa-
tional Linguistics, Sofia, Bulgaria. Association
for Computational Linguistics.
Durrani, N., Haddow, B., Heafield, K., and Koehn,
P. (2013b). Edinburgh?s machine translation
systems for european language pairs. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria. Associa-
tion for Computational Linguistics.
Durrani, N. and Koehn, P. (2014). Improving ma-
chine translation via triangulation and transliter-
ation. In Proceedings of the 17th Annual Con-
ference of the European Association for Ma-
chine Translation (EAMT), Dubrovnik, Croatia.
Durrani, N., Koehn, P., Schmid, H., and Fraser,
A. (2014a). Investigating the usefulness of
generalized word representations in SMT. In
Proceedings of the 25th Annual Conference on
Computational Linguistics (COLING), Dublin,
Ireland. To Appear.
Durrani, N., Sajjad, H., Fraser, A., and Schmid,
H. (2010). Hindi-to-urdu machine translation
through transliteration. In Proceedings of the
48th Annual Meeting of the Association for
Computational Linguistics, pages 465?474, Up-
psala, Sweden. Association for Computational
Linguistics.
Durrani, N., Sajjad, H., Hoang, H., and Koehn, P.
(2014b). Integrating an unsupervised translit-
eration model into statistical machine transla-
tion. In Proceedings of the 15th Conference of
the European Chapter of the ACL (EACL 2014),
Gothenburg, Sweden. Association for Compu-
tational Linguistics.
Durrani, N., Schmid, H., and Fraser, A. (2011).
A joint sequence translation model with inte-
grated reordering. In Proceedings of the 49th
Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Tech-
nologies, pages 1045?1054, Portland, Oregon,
USA.
Dyer, C., Chahuneau, V., and Smith, N. A. (2013).
A simple, fast, and effective reparameterization
of ibm model 2. In Proceedings of the 2013
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, pages 644?
648, Atlanta, Georgia. Association for Compu-
tational Linguistics.
Freitag, M., Peitz, S., Wuebker, J., Ney, H., Huck,
M., Sennrich, R., Durrani, N., Nadejde, M.,
Williams, P., Koehn, P., Herrmann, T., Cho,
E., and Waibel, A. (2014). EU-BRIDGE MT:
combined machine translation. In Proceedings
of the ACL 2014 Ninth Workshop on Statistical
Machine Translation, Baltimore, MD, USA.
Galley, M. and Manning, C. D. (2008). A sim-
ple and effective hierarchical phrase reorder-
ing model. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 848?856, Honolulu,
Hawaii.
Heafield, K. (2011). Kenlm: Faster and smaller
language model queries. In Proceedings of the
Sixth Workshop on Statistical Machine Trans-
lation, pages 187?197, Edinburgh, Scotland,
United Kingdom.
Heafield, K., Pouzyrevsky, I., Clark, J. H., and
Koehn, P. (2013). Scalable modified Kneser-
Ney language model estimation. In Proceedings
103
of the 51st Annual Meeting of the Association
for Computational Linguistics, Sofia, Bulgaria.
Huang, L. and Chiang, D. (2007). Forest rescor-
ing: Faster decoding with integrated language
models. In Proceedings of the 45th Annual
Meeting of the Association of Computational
Linguistics, pages 144?151, Prague, Czech Re-
public. Association for Computational Linguis-
tics.
Koehn, P. (2005). Europarl: A parallel corpus for
statistical machine translation. In Proceedings
of MT Summit.
Koehn, P. and Haddow, B. (2009). Edinburgh?s
Submission to all Tracks of the WMT 2009
Shared Task with Reordering and Speed Im-
provements to Moses. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation, pages 160?164, Athens, Greece. Associ-
ation for Computational Linguistics.
Koehn, P. and Hoang, H. (2007). Factored trans-
lation models. In Proceedings of the 2007
Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL),
pages 868?876, Prague, Czech Republic. Asso-
ciation for Computational Linguistics.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B.,
Shen, W., Moran, C., Zens, R., Dyer, C., Bo-
jar, O., Constantin, A., and Herbst, E. (2007).
Moses: Open source toolkit for statistical ma-
chine translation. In ACL 2007 Demonstrations,
Prague, Czech Republic.
Koehn, P. and Knight, K. (2003). Empirical meth-
ods for compound splitting. In Proceedings of
Meeting of the European Chapter of the Associ-
ation of Computational Linguistics (EACL).
Kumar, S. and Byrne, W. J. (2004). Mini-
mum bayes-risk decoding for statistical ma-
chine translation. In HLT-NAACL, pages 169?
176.
Moore, R. C. and Lewis, W. (2010). Intelligent
selection of language model training data. In
Proceedings of the ACL 2010 Conference Short
Papers, pages 220?224, Uppsala, Sweden. As-
sociation for Computational Linguistics.
Nakov, P. and Tiedemann, J. (2012). Combining
word-level and character-level models for ma-
chine translation between closely-related lan-
guages. In Proceedings of the 50th Annual
Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers), pages
301?305, Jeju Island, Korea. Association for
Computational Linguistics.
Och, F. J. (1999). An efficient method for deter-
mining bilingual word classes. In Ninth Confer-
ence the European Chapter of the Association
for Computational Linguistics (EACL), pages
71?76.
Post, M., Callison-Burch, C., and Osborne, M.
(2012). Constructing parallel corpora for six in-
dian languages via crowdsourcing. In Proceed-
ings of the Seventh Workshop on Statistical Ma-
chine Translation, pages 401?409, Montr?eal,
Canada. Association for Computational Lin-
guistics.
Sajjad, H., Smekalova, S., Durrani, N., Fraser, A.,
and Schmid, H. (2013). QCRI-MES submis-
sion at wmt13: Using transliteration mining to
improve statistical machine translation. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation, Sofia, Bulgaria. Associa-
tion for Computational Linguistics.
Schwenk, H. and Koehn, P. (2008). Large and di-
verse language models for statistical machine
translation. In International Joint Conference
on Natural Language Processing, pages 661?
666.
Utiyama, M. and Isahara, H. (2007). A compar-
ison of pivot methods for phrase-based statis-
tical machine translation. In 2007 Meeting of
the North American Chapter of the Association
for Computational Linguistics (NAACL), pages
484?491.
Wu, H. and Wang, H. (2007). Pivot language
approach for phrase-based statistical machine
translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational
Linguistics, pages 856?863, Prague, Czech Re-
public. Association for Computational Linguis-
tics.
104
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 445?456,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Dynamic Topic Adaptation for SMT using Distributional Profiles
Eva Hasler
1
Barry Haddow
1
Philipp Koehn
1,2
1
School of Informatics, University of Edinburgh
2
Center for Language and Speech Processing, Johns Hopkins University
e.hasler@ed.ac.uk, {bhaddow,pkoehn}@inf.ed.ac.uk
Abstract
Despite its potential to improve lexical
selection, most state-of-the-art machine
translation systems take only minimal con-
textual information into account. We cap-
ture context with a topic model over dis-
tributional profiles built from the context
words of each translation unit. Topic dis-
tributions are inferred for each transla-
tion unit and used to adapt the translation
model dynamically to a given test context
by measuring their similarity. We show
that combining information from both lo-
cal and global test contexts helps to im-
prove lexical selection and outperforms a
baseline system by up to 1.15 BLEU. We
test our topic-adapted model on a diverse
data set containing documents from three
different domains and achieve competitive
performance in comparison with two su-
pervised domain-adapted systems.
1 Introduction
The task of lexical selection plays an important
role in statistical machine translation (SMT). It
strongly depends on context and is particularly dif-
ficult when the domain of a test document is un-
known, for example when translating web doc-
uments from diverse sources. Selecting transla-
tions of words or phrases that preserve the sense
of the source words is closely related to the field
of word sense disambiguation (WSD), which has
been studied extensively in the past.
Most approaches to WSD model context at the
sentence level and do not take the wider context
of a word into account. Some of the ideas from
the field of WSD have been adapted for machine
translation (Carpuat and Wu, 2007b; Carpuat and
Wu, 2007a; Chan et al., 2007). For example,
Carpuat and Wu (2007a) extend word sense dis-
ambiguation to phrase sense disambiguation and
show improved performance due to the better fit
with multiple possible segmentations in a phrase-
based system. Carpuat (2009) test the ?one sense
per discourse? hypothesis (Gale et al., 1992) for
MT and find that enforcing it as a constraint at the
document level could potentially improve transla-
tion quality. Our goal is to make correct lexical
choices in a given context without explicitly en-
forcing translation consistency.
More recent work in SMT uses latent repre-
sentations of the document context to dynam-
ically adapt the translation model with either
monolingual topic models (Eidelman et al., 2012;
Hewavitharana et al., 2013) or bilingual topic
models (Hasler et al., 2014), thereby allowing the
translation system to disambiguate source phrases
using document context. Eidelman et al. (2012)
also apply a topic model to each test sentence and
find that sentence context is sufficient for pick-
ing good translations, but they do not attempt to
combine sentence and document level informa-
tion. Sentence-level topic adaptation for SMT has
also been employed by Hasler et al. (2012). Other
approaches to topic adaptation for SMT include
Zhao and Xing (2007) and Tam et al. (2008), both
of which use adapted lexical weights.
In this paper, we present a topic model that
learns latent distributional representations of the
context of a phrase pair which can be applied to
both local and global contexts at test time. We
introduce similarity features that compare latent
representations of phrase pair types to test con-
texts to disambiguate senses for improved lexi-
cal selection. We also propose different strate-
gies for combining local and global topical context
and show that using clues from both levels of con-
texts is beneficial for translation model adaptation.
We evaluate our model on a dynamic adaptation
task where the domain of a test document is un-
known and hence the problem of lexical selection
is harder.
445
2 Related work
Most work in the WSD literature has modelled
disambiguation using a limited window of con-
text around the word to disambiguate. Cai et al.
(2007), Boyd-graber and Blei (2007) and Li et al.
(2010) further tried to integrate the notion of la-
tent topics to address the sparsity problem of the
lexicalised features typically used in WSD classi-
fiers. The most closely related work in the area
of sense disambiguation is by Dinu and Lapata
(2010) who propose a disambiguation method for
solving lexical similarity and substitution tasks.
They measure word similarity in context by learn-
ing distributions over senses for each target word
in the form of lower-dimensional distributional
representations. Before computing word similar-
ities, they contextualise the global sense distribu-
tion of a word using the sense distribution of words
in the test context, thereby shifting the sense distri-
bution towards the test context. We adopt a simi-
lar distributional representation, but argue that our
representation does not need this disambiguation
step because at the level of phrase pairs the ambi-
guity is already much reduced.
Our model performs adaptation using similar-
ity features which is similar to the approach of
Costa-juss`a and Banchs (2010) who learn a vec-
tor space model that captures the source context
of every training sentence. In Banchs and Costa-
juss`a (2011), the vector space model is replaced
with representations inferred by Latent Seman-
tic Indexing. However, because their latent rep-
resentations are learned over training sentences,
they have to compare the current test sentence to
the latent vector of every training instance associ-
ated with a translation unit. The highest similar-
ity value is then used as a feature value. Instead,
our model learns latent distributional representa-
tions of phrase pairs that can be directly compared
to test contexts and are likely to be more robust.
Because context words of a phrase pair are tied to-
gether in the distributional representations, we can
use sparse priors to cluster context words associ-
ated with the same phrase pair into few topics.
Recently, Chen et al. (2013) have proposed a
vector space model for domain adaptation where
phrase pairs are assigned vectors that are defined
in terms of the training corpora. A similar vector
is built for an in-domain development set and the
similarity to the development set is used as a fea-
ture during translation. While their vector repre-
sentations are similar to our latent topic represen-
tations, their model has no notion of structure be-
yond corpus boundaries and is adapted towards a
single target domain (cross-domain). Instead, our
model learns the latent topical structure automati-
cally and the translation model is adapted dynam-
ically to each test instance.
We are not aware of prior work in the field of
MT that investigates combinations of local and
global context. In their recent work on neural lan-
guage models, Huang et al. (2012) combine the
scores of two neural networks modelling the word
embeddings of previous words in a sequence as
well as those of words from the surrounding doc-
ument by averaging over all word embeddings oc-
curring in the same document. The score of the
next word in a sequence is computed as the sum of
the scores of both networks, but they do not con-
sider alternative ways of combining contextual in-
formation.
3 Phrase pair topic model (PPT)
Our proposed model aims to capture the relation-
ship between phrase pairs and source words that
frequently occur in the local context of a phrase
pair, that is, context words occurring in the same
sentence. It therefore follows the distributional
hypothesis (Harris, 1954) which states that words
that occur in the same contexts tend to have sim-
ilar meanings. For a phrase pair, the idea is that
words that occur frequently in its context are in-
dicative of the sense that is captured by the target
phrase translating the source phrase.
We assume that all phrase pairs share a global
set of topics and during topic inference the distri-
bution over topics for each phrase pair is induced
from the latent topic of its context words in the
training data. In order to learn topic distributions
for each phrase pair, we represent phrase pairs as
documents containing all context words from the
source sentence context in the training data. These
distributional profiles of phrase pairs are the in-
put to the topic modelling algorithm which learns
topic clusters over context words.
Figure 1a shows a graphical representation of
the following generative process for training. For
each of P phrase pairs pp
i
in the collection
1. Draw a topic distribution from an asymmetric
Dirichlet prior, ?
p
? Dirichlet(?
0
, ? . . . ?).
2. For each position c in the distributional pro-
file of pp
i
, draw a topic from that distribution,
z
p,c
?Multinomial(?
p
).
446
(a) Inference on phrase pair documents (training).
(b) Inference on local test contexts (test).
Figure 1: Graphical representation of the phrase
pair topic (PPT) model.
3. Conditioned on topic z
p,c
, choose a context
word w
p,c
?Multinomial(?
z
p,c
).
? and ? are parameters of the Dirichlet distribu-
tions and ?
k
denotes topic-dependent vocabularies
over context words. Test contexts are generated
similarly by drawing topic mixtures ?
l
for each test
context
1
as shown in Figure 1b, drawing topics z
for each context position and then drawing context
wordsw for each z. The asymmetric prior on topic
distributions (?
0
for topic 0 and ? for all other top-
ics) encodes the intuition that there are words oc-
curring in the context of many phrase pairs which
1
A local test context is defined as all words in the test
sentence excluding stop words, while contexts of phrase pairs
in training do not include the words belonging to the source
phrase. The naming in the figure refers to local test contexts
L, but global test contexts will be defined similarly.
can be grouped under a topic with higher a priori
probability than the other topics. Figure 1a shows
the model for training inference on the distribu-
tional representations for each phrase pair, where
C
l?all
denotes the number of context words in all
sentence contexts that the phrase pair was seen in
the training data, P denotes the number of phrase
pairs and K denotes the number of latent topics.
The model in Figure 1b has the same structure
but shows inference on test contexts, where C
l
de-
notes the number of context words in the test sen-
tence context and L denotes the number of test in-
stances. ?
p
and ?
l
denote the topic distribution for
a phrase pair and a test context, respectively.
3.1 Inference for PPT model
We use collapsed variational Bayes (Teh et al.,
2006) to infer the parameters of the PPT model.
The posterior distribution over topics is computed
as shown below
P (z
p,c
= k|z
?(p,c)
,w
c
, p, ?, ?) ?
(E
q?
[n
?(p,c)
.,k,w
c
] + ?)
(E
q?
[n
?(p,c)
.,k,.
] +W
c
? ?)
? (E
q?
[n
?(p,c)
d,k,.
] + ?)
(1)
where z
p,c
denotes the topic at position c in
the distributional profile p, w
c
denotes all con-
text word tokens in the collection, W
c
is the total
number of context words and E
q?
is the expecta-
tion under the variational posterior. n
?(p,c)
.,k,w
c
and
n
?(p,c)
p,k,.
are counts of topics occurring with context
words and distributional profiles, respectively, and
n
?(p,c)
.,k,.
is a topic occurrence count.
Before training the topic model, we remove stop
words from all documents. When inferring top-
ics for test contexts, we ignore unseen words be-
cause they do not contribute information for topic
inference. In order to speed up training inference,
we limit the documents in the collection to those
corresponding to phrase pairs that are needed to
translate the test set
2
. Inference was run for 50 it-
erations on the distributional profiles for training
and for 10 iterations on the test contexts. The out-
put of the training inference step is a model file
with all the necessary statistics to compute pos-
terior topic distributions (which are loaded before
running test inference), and the set of topic vectors
for all phrase pairs. The output of test inference is
2
Reducing the training contexts by scaling or sampling
would be expected to speed up inference considerably.
447
the set of induced topic vectors for all test con-
texts.
3.2 Modelling local and global context
At training time, our model has access to context
words only from the local contexts of each
phrase pair in their distributional profiles, that is,
other words in the same source sentence as the
phrase pair. This is useful for reducing noise and
constraining the semantic space that the model
considers for each phrase pair during training. At
test time, however, we are not limited to applying
the model only to the immediate surroundings of
a source phrase to disambiguate its meaning. We
can potentially take any size of test context into
account to disambiguate the possible senses of a
source phrase, but for simplicity we consider two
sizes of context here which we refer to as local
and global context.
Local context Words appearing in the sentence
around a test source phrase, excluding stop words.
Global context Words appearing in the document
around a test source phrase, excluding stop words.
4 Similarity features
We define similarity features that compare the
topic vector ?
p
assigned to a phrase pair
3
to the
topic vector assigned to a test context, The fea-
ture is defined for each source phrase and all its
possible translations in the phrase table, as shown
below
sim(pp
i
, test context) = cosine(?
p
i
, ?
c
),
?pp
i
? {pp
i
|s??
?
t
i
} (2)
Unlike Banchs and Costa-juss`a (2011), we do
not learn topic vectors for every training sentence
which results in a topic vector per phrase pair to-
ken, but instead we learn topic vectors for each
phrase pair type. This is more efficient but also
more appealing from a modelling point of view, as
the topic distributions associated with phrase pairs
can be thought of as expected latent contexts. The
application of the similarity feature is visualised
in Figure 2. On the left, there are two applicable
phrase pairs for the source phrase noyau, noyau
? kernel and noyau? nucleus, with their distri-
butional representations (words belonging to the
3
The mass of topic 0 is removed from the vectors and
the vectors are renormalised before computing similarity fea-
tures.
IT topic versus the scientific topic) and assigned
topic vectors ?
p
. The local and global test contexts
are similarly represented by a document contain-
ing the context words and a resulting topic vector
?
l
or ?
g
. The test context vector ?
c
can be one of
?
l
and ?
g
or a combination of both. In this ex-
ample, the distributional representation of noyau
? kernel has a larger topical overlap with the test
context and will more likely be selected during de-
coding.
Figure 2: Similarity between topic vectors of two
applicable phrase pairs ?
p
and the topic vectors ?
l
and ?
g
from the local and global test context dur-
ing test time.
While this work focuses on exploring vec-
tor space similarity for adaptation, mostly for
computational ease, it may be possible to derive
probabilistic translation features from the PPT
model. This could be a useful addition to the
model and we leave this as an avenue for future
work.
Types of similarity features
We experiment with local and global phrase simi-
larity features, phrSim-local and phrSim-global, to
perform dynamic topic adaptation. These two sim-
ilarity features can be combined by adding them
both to the log-linear SMT model, in which case
each receive separate feature weights. Whenever
we use the + symbol in our results tables, the
additional features were combined with existing
features log-linearly. However, we also experi-
mented with an alternative combination of local
and global information where we combine the lo-
cal and global topic vectors for each test context
before computing similarity features.
4
We were
4
The combined topic vectors were renormalised before
computing their similarities with each candidate phrase pair.
448
motivated by the observation that there are cases
where the local and global features have an op-
posite preference for one translation over another,
but the log-linear combination can only learn a
global preference for one of the features. Com-
bining the topic vectors allows us to potentially
encode a preference for one of the contexts that
depends on each test instance.
For similarity features derived from combined
topic vectors, ? denotes the additive combination
of topic vectors,? denotes the multiplicative com-
bination of topic vectors and~ denotes a combina-
tion that favours the local context for longer sen-
tences and backs off incrementally to the global
context for shorter sentences.
5
The intuition be-
hind this combination is that if there is already suf-
ficient evidence in the local context, the local topic
mixture may be more reliable than the global mix-
ture.
We also experiment with a combination of the
phrase pair similarity features derived from the
PPT model with a document similarity feature
from the pLDA model described in Hasler et al.
(2014). The motivation is that their model learns
topic mixtures for documents and uses phrases in-
stead of words to infer the topical context. There-
fore, it might provide additional information to our
similarity features.
5 Data and experimental setup
Our experiments were carried out on a mixed
French-English data set containing the TED cor-
pus (Cettolo et al., 2012), parts of the News Com-
mentary corpus (NC) and parts of the Common-
crawl corpus (CC) from the WMT13 shared task
(Bojar et al., 2013) as described in Table 1. To
ensure that the baseline model does not have an
implicit preference for any particular domain, we
selected subsets of the NC and CC corpora such
that the training data contains 2.7M English words
per domain. We were guided by two constraints
in chosing our data set in order to simulate an
environment where very diverse documents have
to be translated, which is a typical scenario for
web translation engines: 1) the data has docu-
ment boundaries and the content of each docu-
ment is assumed to be topically related, 2) there is
some degree of topical variation within each data
set. This setup allows us to evaluate our dynamic
5
The interpolation weights between local and global topic
vectors were set proportional to sentence lengths between 1
and 30. The length of longer sentences was clipped to 30.
topic adaptation approach because the test docu-
ments are from different domains and also differ
within each domain, which makes lexical selec-
tion a much harder problem. The topic adaptation
approach does not make use of the domain labels
in training or test, because it infers topic mixtures
in an unsupervised way. However, we compare the
performance of our dynamic approach to domain
adaptation methods by providing them the domain
labels for each document in training and test.
In order to abstract away from adaptation ef-
fects that concern tuning of length penalties and
language models, we use a mixed tuning set con-
taining data from all three domains and train one
language model on the concatenation of the tar-
get sides of the training data. Word alignments
are trained on the concatenation of all training data
and fixed for all models. Table 2 shows the aver-
age length of a document for each domain. While
a CC document contains 29.1 sentences on aver-
age, documents from NC and TED are on average
more than twice as long. The length of a document
could have an influence on how reliable global
topic information is but also on how important it
is to have information from both local and global
test contexts.
Data Mixed CC NC TED
Train 354K (6450) 110K 103K 140K
Dev 2453 (39) 818 817 818
Test 5664 (112) 1892 1878 1894
Table 1: Number of sentence pairs and documents
(in brackets) in the data sets.
Data CC NC TED
Test documents 65 31 24
Avg sentences/doc 29.1 60.6 78.9
Table 2: Average number of sentences per docu-
ment in the test set (per domain).
5.1 Unadapted baseline system
Our baseline is a phrase-based French-English
system trained on the concatenation of all parallel
data. It was built with the Moses toolkit (Koehn
et al., 2007) using the 14 standard core features
including a 5-gram language model. Translation
quality is evaluated on a large test set, using the
average feature weights of three optimisation runs
with PRO (Hopkins and May, 2011). We use the
449
noyau? kernel noyau? nucleus noyau? core
Figure 3: Topic distributions for source phrase noyau and three of its translations (20 topics without topic
0). Colored bars correspond to topics IT, politics, science, economy with topic proportions ?10%.
mteval-v13a.pl script to compute case-insensitive
BLEU scores.
5.2 Domain-adapted benchmark systems
As domain-aware benchmark systems, we use
the linear mixture model (DOMAIN1) of Sen-
nrich (2012) and the phrase table fill-up method
(DOMAIN2) of Bisazza et al. (2011) (both avail-
able in the Moses toolkit). For both systems,
the domain labels of the documents are used to
group documents of the same domain together. We
build adapted tables for each domain by treating
the remaining documents as out-of-domain data
and combining in-domain with out-of-domain ta-
bles. For development and test, the domain labels
are used to select the respective domain-adapted
model for decoding. Both systems have an advan-
tage over our model because of their knowledge
of domain boundaries in the data. This allows for
much more confident lexical choices than using an
unadapted system but is not possible without prior
knowledge about each document.
5.3 Implementation of similarity features
After all topic vectors have been computed, a fea-
ture generation step precomputes the similarity
features for all pairs of test contexts and applica-
ble phrase pairs for translating source phrases in
a test instance. The phrase table of the baseline
model is filtered for every test instance (a sentence
or document, depending on the context setting)
and each entry is augmented with features that ex-
press its semantic similarity to the test context. We
use a wrapper around the Moses decoder to reload
the phrase table for each test instance, which en-
ables us to run parameter optimisation (PRO) in
the usual way to get one set of tuned weights for
all test sentences. It would be conceivable to use
topic-specific weights instead of one set of global
weights, but this is not the focus of this work.
6 Qualitative evaluation of phrase pair
topic distributions
In order to verify that the topic model is learning
useful topic representations for phrase pairs, we
inspect the inferred topic distributions for three
phrase pairs where the translation of the same
source word differs depending on the topical
context: noyau ? kernel, noyau ? nucleus
and noyau ? core. Figure 3 shows the topic
distributions for a PPT model with 20 topics
(with topic 0 removed) and highlights the most
prominent topics with labels describing their
content (politics, IT, science, economy)
6
. The
most peaked topic distribution was learned for
the phrase pair noyau ? kernel which would be
expected to occur mostly in an IT context and
the topic with the largest probability mass is in
fact related to IT. The most prominent topic for
the phrase pair noyau ? nucleus is the science
topic, though it seems to be occurring in with the
political topic as well. The phrase pair noyau
? core was assigned the most ambiguous topic
distribution with peaks at the politics, economy
and IT topics. Note also that its topic distribution
overlaps with those of the other translations, for
example, like the phrase pair noyau ? kernel,
it can occur in IT contexts. This shows that the
model captures the fact that even within a given
topic there can still be ambiguity about the correct
translation (both target phrases kernel and core
are plausible translations in an IT context).
6
Topic labels were assigned by inspecting the most prob-
able context words for each topic according to the model.
450
Ambiguity of phrase pair topic vectors
The examples in the previous section show that
the level of ambiguity differs between phrase pairs
that constitute translations of the same source
phrase. It is worth noting that introducing bilin-
gual information into topic modelling reduces the
sense ambiguity present in monolingual text by
preserving only the intersection of the senses of
source and target phrases. For example, the distri-
butional profiles of the source phrase noyau would
contain words that belong to the senses IT, poli-
tics, science and economy, while the words in the
context of the target phrase kernel can belong to
the senses IT and food (with source context words
such as grain, prot?eines, produire). Thus, the
monolingual representations would still contain a
relatively high level of ambiguity while the distri-
butional profile of the phrase pair noyau? kernel
preserves only the IT sense.
7 Results and discussion
In this section we present experimental results
of our model with different context settings and
against different baselines. We used bootstrap re-
sampling (Koehn, 2004) to measure significance
on the mixed test set and marked all statistically
significant results compared to the respective base-
lines with asterisk (*: p ? 0.01).
7.1 Local context
In Table 3 we compare the results of the con-
catenation baseline and a model containing the
phrSim-local feature in addition to the baseline
features, for different numbers of latent topics. We
show results for the mixed test set containing doc-
uments from all three domains as well as the in-
dividual results on the documents from each do-
main. While all topic settings yield improvements
over the baseline, the largest improvement on the
mixed test set (+0.48 BLEU) is achieved with 50
topics. Topic adaptation is most effective on the
TED portion of the test set where the increase in
BLEU is 0.59.
7.2 Global context
Table 4 shows the results of the baseline plus the
phrSim-global feature that takes into account the
whole document context of a test sentence. While
the largest overall improvement on the mixed test
set is equal to the improvement of the local feature,
there are differences in performance for the indi-
vidual domains. For Commoncrawl documents,
Model Mixed CC NC TED
Baseline -26.86 19.61 29.42 31.88
10 topics *27.15 19.87 29.63 32.36
20 topics *27.19 19.92 29.76 32.31
50 topics *27.34 20.13 29.70 32.47
100 topics *27.26 20.02 29.75 32.40
>Baseline +0.48 +0.52 +0.34 +0.59
Table 3: BLEU scores of baseline system +
phrSim-local feature for different numbers of top-
ics.
the results vary slightly but the largest improve-
ment is still achieved with 50 topics and is al-
most the same for both. For News Commentary,
the scores with the local feature are consistently
higher than the scores with the global feature (0.20
and 0.22 BLEU higher for 20 and 50 topics). For
TED, the trend is opposite with the global feature
performing better than the local feature for all top-
ics (0.28 and 0.40 BLEU higher for 10 and 20 top-
ics). The best improvement over the baseline for
TED is 0.83 BLEU, which is higher than the im-
provement with the local feature.
Model Mixed CC NC TED
Baseline -26.86 19.61 29.42 31.88
10 topics *27.30 20.01 29.61 32.64
20 topics *27.34 20.07 29.56 32.71
50 topics *27.27 20.12 29.48 32.55
100 topics *27.24 19.95 29.66 32.52
>Baseline +0.48 +0.51 +0.24 +0.83
Table 4: BLEU scores of baseline system +
phrSim-global feature for different numbers of
topics.
7.3 Relation to properties of test documents
To make these results more interpretable, Ta-
ble 5 lists some of the properties of the test doc-
uments per domain. Of the three domains, CC
has the shortest documents on average and TED
the longest. To understand how this affects topic
inference, we measure topical drift as the aver-
age divergence (cosine distance) of the local topic
distributions for each test sentence to the global
topic distribution of their surrounding document.
There seems to be a correlation between docu-
ment length and topical drift, with CC documents
showing the least topical drift and TED documents
showing the most. This makes sense intuitively
451
because the longer a document is, the more likely
it is that the content of a given sentence diverges
from the overall topical structure of the document.
While this can explain why for CC documents us-
ing local or global context results in similar perfor-
mance, it does not explain the better performance
of the local feature for NC documents. The last
row of Table 5 shows that sentences in the NC
documents are on average the longest and longer
sentences would be expected to yield more reli-
able topic estimates than shorter sentences. Thus,
we assume that local context yields better perfor-
mance for NC because on average the sentences
are long enough to yield reliable topic estimates.
When local context provides reliable information,
it may be more informative than global context be-
cause it can be more specific.
For TED, we see the largest topical drift per
document, which could lead us to believe that the
document topic mixtures do not reflect the topical
content of the sentences too well. But considering
that the sentences are on average shorter than for
the other two domains, it is more likely that the
local context in TED documents can be unreliable
when the sentences are too short. TED documents
contain transcribed speech and are probably less
dense in terms of information content than News
commentary documents. Therefore, the global
context may be more informative for TED which
could explain why relying on the global topic
mixtures yields better results.
Property CC NC TED
Per document
Avg number of sentences 29.1 60.6 78.9
Avg topical divergence 0.35 0.43 0.49
Avg sentence length 26.2 31.5 21.7
Table 5: Properties of test documents per domain.
Average topical divergence is defined as the aver-
age cosine distance of local to global topic distri-
butions in a document.
7.4 Combinations of local and global context
In Table 6 we compare a system that already con-
tains the global feature from a model with 50 top-
ics to the combinations of local and global simi-
larity features described in Section 4.
Of the four combinations, the additive combi-
nation of topic vectors (?) yields the largest im-
provement over the baseline with +0.63 BLEU on
Model Mixed CC NC TED
Baseline -26.86 19.61 29.42 31.88
+ global -27.27 20.12 29.48 32.55
+ local *27.43 20.18 29.65 32.79
? local *27.49 20.30 29.66 32.76
? local -27.34 20.24 29.61 32.50
~ local *27.45 20.22 29.51 32.79
? >BL +0.63 +0.69 +0.24 +0.88
Table 6: BLEU scores of baseline and combina-
tions of phrase pair similarity features with local
and global context (significance compared to base-
line+global). All models were trained with 50 top-
ics.
the mixed test set and +0.88 BLEU on TED. The
improvements of the combined model are larger
than the improvements for each context on its own,
with the only exception being the NC portion of
the test set where the improvement is not larger
than using just the local context. A possible reason
is that when one feature is consistently better for
one of the domains (local context for NC), the log-
linear combination of both features (tuned on data
from all domains) would result in a weaker overall
model for that domain. However, if both features
encode similar information, as we assume to be the
case for CC documents, the presence of both fea-
tures would reinforce the preference of each and
result in equal or better performance. For the ad-
ditive combination, we expect a similar effect be-
cause adding together two topics vectors that have
peaks at different topics would make the resulting
topic vector less peaked than either of the original
vectors.
The additive topic vector combination is
slightly better than the log-linear feature combina-
tion, though the difference is small. Nevertheless,
it shows that combining topic vectors before com-
puting similarity features is a viable alternative
to log-linear combination, with the potential to
design more expressive combination functions.
The multiplicative combination performs slightly
worse than the additive combination, which
suggests that the information provided by the two
contexts is not always in agreement. In some
cases, the global context may be more reliable
while in other cases the local context may have
more accurate topic estimates and a voting ap-
proach does not take advantage of complementary
information. The combination of topic vectors
452
Source: Le noyau contient de nombreux pilotes, afin de fonctionner chez la plupart des utilisateurs.
Reference: The precompiled kernel includes a lot of drivers, in order to work for most users.
Source: Il est prudent de consulter les pages de manuel ou les faq sp?ecifiques `a votre os.
Reference: It?s best to consult the man pages or faqs for your os.
Source: Nous fournissons nano (un petit ?editeur), vim (vi am?elior?e), qemacs (clone de emacs), elvis, joe .
Reference: Nano (a lightweight editor), vim (vi improved), qemacs (emacs clone), elvis and joe.
Source: Elle a introduit des politiques [..] `a cot?e des relations de gouvernement `a gouvernement traditionnelles.
Reference: She has introduced policies [..] alongside traditional government-to-government relations.
Figure 4: Examples of test sentences and reference translations with the ambiguous source words and
their translations in bold.
depending on sentence length (~) performs well
for CC and TED but less well for NC where we
would expect that it helps to prefer the local
information. This indicates that the rather ad-
hoc way in which we encoded dependency on
the sentence length may need further refinement
to make better use of the local context information.
Model noyau? os?
Baseline nucleus bones
global kernel* os*
local nucleus bones
global?local kernel* os*
Table 7: Translations of ambiguous source words
where global context yields the correct translation
(* denotes the correct translation).
Model elvis? relations?
Baseline elvis* relations*
global the king relationship
local elvis* relations*
global?local the king relations*
Table 8: Translations of ambiguous source words
where local context yields the correct translation
(* denotes the correct translation).
7.5 Effect of contexts on translation
To give an intuition of how lexical selection is af-
fected by contextual information, Figure 4 shows
four test sentences with an ambiguous source word
and its translation in bold. The corresponding
translations with the baseline, the global and lo-
cal similarity features and the additive combina-
tion are shown in Table 7 for the first two examples
where the global context yields the correct transla-
tion (as indicated by *) and in Table 8 for the last
two examples where the local context yields the
correct translation.
7
In Table 7, the additive com-
bination preserves the choice of the global model
and yields the correct translations, while in Table 8
only the second example is translated correctly by
the combined model. A possible explanation is
that the topical signal from the global context is
stronger and results in more discriminative simi-
larity values. In that case, the preference of the
global context would be likely to have a larger in-
fluence on the similarity values in the combined
model. A useful extension could be to try to de-
tect for a given test instance which context pro-
vides more reliable information (beyond encoding
sentence length) and boost the topic distribution
from that context in the combination.
7.6 Comparison with domain adaptation
Table 9 compares the additive model (?) to the
two domain-adapted systems that know the do-
main label of each document during training and
test. Our topic-adapted model yields overall com-
petitive performance with improvements of +0.37
and +0.25 BLEU on the mixed test set, respec-
tively. While it yields slightly lower performance
on the NC documents, it achieves equal perfor-
mance on TED documents and improves by up to
+0.94 BLEU on Commoncrawl documents. This
can be explained by the fact that Commoncrawl is
the most diverse of the three domains with docu-
ments crawled from all over web, thus we expect
topic adaptation to be most effective in compari-
son to domain adaptation in this scenario. Our dy-
namic approach allows us to adapt the similarity
features to each test sentence and test document
individually and is therefore more flexible than
7
For these examples, the local model happens to yield the
same translations as the baseline model.
453
Type of adaptation Model Mixed CC NC TED
Domain-adapted
DOMAIN1 -27.24 19.61 29.87 32.73
DOMAIN2 -27.12 19.36 29.78 32.71
Topic-adapted global ? local *27.49 20.30 29.66 32.76
>DOMAIN1 +0.25 +0.69 -0.21 +0.03
>DOMAIN2 +0.37 +0.94 -0.12 +0.05
Table 9: BLEU scores of translation model using similarity features derived from PPT model (50 topics)
in comparison with two (supervised) domain-adapted systems.
Model Mixed CC NC TED
Baseline -26.86 19.61 29.42 31.88
+ docSim -27.22 20.11 29.63 32.40
+ phrSim-global ? phrSim-local *27.58 20.34 29.71 32.96
+ phrSim-global ~ phrSim-local *27.60 20.35 29.70 33.03
global~local>BL +0.74 +0.74 +0.38 +1.15
Table 10: BLEU scores of baseline, baseline + document similarity feature and additional phrase pair
similarity features (significance compared to baseline+docSim). All models were trained with 50 topics.
cross-domain adaptation approaches while requir-
ing no information about the domain of a test in-
stance.
7.7 Combination with an additional
document similarity feature
To find out whether similarity features derived
from different types of topic models can provide
complementary information, we add the phrSim
features to a system that already includes a docu-
ment similarity feature (docSim) derived from the
pLDA model (Hasler et al., 2014) which learns
topic distributions at the document level and uses
phrases instead of words as the minimal units. The
results are shown in Table 10. Adding the two
best combinations of local and global context from
Table 6 yields the best results on TED documents
with an increase of 0.63 BLEU over the baseline +
docSim model and 1.15 BLEU over the baseline.
On the mixed test set, the improvement is 0.38
BLEU over the baseline + docSim model and 0.74
BLEU over the baseline. Thus, we show that com-
bining different scopes and granularities of sim-
ilarity features consistently improves translation
results and yields larger gains than using each of
the similarity features alone.
8 Conclusion
We have presented a new topic model for dynamic
adaptation of machine translation systems that
learns topic distributions for phrase pairs. These
latent topic representations can be compared to la-
tent representations of local or global test contexts
and integrated into the translation model via simi-
larity features.
Our experimental results show that it is ben-
eficial for adaptation to use contextual informa-
tion from both local and global contexts, with
BLEU improvements of up to 1.15 over the base-
line system on TED documents and 0.74 on a
large mixed test set with documents from three do-
mains. Among four different combinations of lo-
cal and global information, we found that the ad-
ditive combination of topic vectors performs best.
We conclude that information from both contexts
should be combined to correct potential topic de-
tection errors in either of the two contexts. We
also show that our dynamic adaptation approach
performs competitively in comparison with two
supervised domain-adapted systems and that the
largest improvement is achieved for the most di-
verse portion of the test set.
In future work, we would like to experiment
with more compact distributional profiles to speed
up inference and explore the possibilities of de-
riving probabilistic translation features from the
PPT model as an extension to the current model.
Another avenue for future work could be to com-
bine contextual information that captures different
types of information, for example, to distinguish
between semantic and syntactic aspects in the lo-
cal context.
454
Acknowledgements
This work was supported by funding from the
Scottish Informatics and Computer Science Al-
liance (Eva Hasler) and funding from the Eu-
ropean Union Seventh Framework Programme
(FP7/2007-2013) under grant agreement 287658
(EU BRIDGE) and grant agreement 288769 (AC-
CEPT). Thanks to Annie Louis for helpful com-
ments on a draft of this paper and thanks to the
anonymous reviewers for their useful feedback.
References
Rafael E Banchs and Marta R Costa-juss`a. 2011. A Se-
mantic Feature for Statistical Machine Translation.
In SSST-5 Proceedings of the Fifth Workshop on Syn-
tax, Semantics and Structure in Statistical Transla-
tion, pages 126?134.
Arianna Bisazza, Nick Ruiz, and Marcello Federico.
2011. Fill-up versus Interpolation Methods for
Phrase-based SMT Adaptation. In Proceedings of
IWSLT.
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 workshop
on statistical machine translation. In Proceedings of
WMT 2013.
Jordan Boyd-graber and David Blei. 2007. A Topic
Model for Word Sense Disambiguation. In Proceed-
ings of EMNLP-CoNLL, pages 1024?1033.
Jun Fu Cai, Wee Sun Lee, and Yee Whye Teh. 2007.
Improving Word Sense Disambiguation Using Topic
Features. In Proceedings of EMNLP, pages 1015?
1023.
Marine Carpuat and Dekai Wu. 2007a. How Phrase
Sense Disambiguation outperforms Word Sense Dis-
ambiguation for Statistical Machine Translation.
In International Conference on Theoretical and
Methodological Issues in MT.
Marine Carpuat and Dekai Wu. 2007b. Improving Sta-
tistical Machine Translation using Word Sense Dis-
ambiguation. In Proceedings of EMNLP, pages 61?
72.
Marine Carpuat. 2009. One Translation per Discourse.
In Proceedings of the NAACL HLT Workshop on Se-
mantic Evaluations: Recent Achievements and Fu-
ture Directions, pages 19?27.
Mauro Cettolo, Christian Girardi, and Marcello Fed-
erico. 2012. WIT3: Web Inventory of Transcribed
and Translated Talks. In Proceedings of EAMT.
Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007. Word Sense Disambiguation Improves Statis-
tical Machine Translation. In Proceedings of ACL.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector Space Model for Adaptation in Statistical
Machine Translation. In Proceedings of ACL, pages
1285?1293.
Marta R. Costa-juss`a and Rafael E. Banchs. 2010. A
vector-space dynamic feature for phrase-based sta-
tistical machine translation. Journal of Intelligent
Information Systems, 37(2):139?154, August.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
Distributional Similarity in Context. In Proceedings
of EMNLP, pages 1162?1172.
Vladimir Eidelman, Jordan Boyd-Graber, and Philip
Resnik. 2012. Topic Models for Dynamic Trans-
lation Model Adaptation. In Proceedings of ACL.
William A Gale, Kenneth W Church, and David
Yarowsky. 1992. One Sense Per Discourse. In
Proceedings of the workshop on Speech and Natu-
ral Language.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146?162.
Eva Hasler, Barry Haddow, and Philipp Koehn. 2012.
Sparse Lexicalised Features and Topic Adaptation
for SMT. In Proceedings of IWSLT.
Eva Hasler, Phil Blunsom, Philipp Koehn, and Barry
Haddow. 2014. Dynamic Topic Adaptation for
Phrase-based MT. In Proceedings of the 14th Con-
ference of the European Chapter of the Association
for Computational Linguistics, Gothenburg, Swe-
den.
Sanjika Hewavitharana, Dennis N Mehay, and
Sankaranarayanan Ananthakrishnan. 2013. Incre-
mental Topic-Based Translation Model Adaptation
for Conversational Spoken Language Translation.
In Proceedings of ACL, pages 697?701.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
Edinburgh, United Kingdom.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving Word
Representations via Global Context and Multiple
Word Prototypes. In Proceedings of ACL, pages
873?882.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for SMT. In Proceedings of ACL:
Demo and poster sessions.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proc. of
EMNLP.
455
Linlin Li, Benjamin Roth, and Caroline Sporleder.
2010. Topic Models for Word Sense Disambigua-
tion and Token-based Idiom Detection. In Proceed-
ings of ACL, pages 1138?1147.
Rico Sennrich. 2012. Perplexity Minimization for
Translation Model Domain Adaptation in Statistical
Machine Translation. In Proceedings of EACL.
Yik-Cheung Tam, Ian Lane, and Tanja Schultz. 2008.
Bilingual LSA-based adaptation for statistical ma-
chine translation. Machine Translation, 21(4):187?
207, November.
Yee Whye Teh, David Newman, and Max Welling.
2006. A collapsed variational Bayesian inference
algorithm for LDA. In Proceedings of NIPS.
B Zhao and E P Xing. 2007. HM-BiTAM: Bilingual
topic exploration, word alignment, and translation.
Neural Information Processing.
456
