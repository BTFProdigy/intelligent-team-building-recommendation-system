Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 285?288,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Iterative Scaling and Coordinate Descent Methods for Maximum Entropy
Fang-Lan Huang, Cho-Jui Hsieh, Kai-Wei Chang, and Chih-Jen Lin
Department of Computer Science
National Taiwan University
Taipei 106, Taiwan
{d93011,b92085,b92084,cjlin}@csie.ntu.edu.tw
Abstract
Maximum entropy (Maxent) is useful in
many areas. Iterative scaling (IS) methods
are one of the most popular approaches to
solve Maxent. With many variants of IS
methods, it is difficult to understand them
and see the differences. In this paper, we
create a general and unified framework for
IS methods. This framework also connects
IS and coordinate descent (CD) methods.
Besides, we develop a CD method for
Maxent. Results show that it is faster than
existing iterative scaling methods1.
1 Introduction
Maximum entropy (Maxent) is widely used in
many areas such as natural language processing
(NLP) and document classification. Maxent mod-
els the conditional probability as:
P
w
(y|x)?S
w
(x, y)/T
w
(x), (1)
S
w
(x, y)?e
P
t
w
t
f
t
(x,y)
, T
w
(x)?
?
y
S
w
(x, y),
where x indicates a context, y is the label of the
context, and w ? Rn is the weight vector. A
function f
t
(x, y) denotes the t-th feature extracted
from the context x and the label y.
Given an empirical probability distribution
?
P (x, y) obtained from training samples, Maxent
minimizes the following negative log-likelihood:
min
w
?
?
x,y
?
P (x, y) logP
w
(y|x)
=
?
x
?
P (x) log T
w
(x)?
?
t
w
t
?
P (f
t
),
(2)
where ?P (x) =
?
y
?
P (x, y) is the marginal prob-
ability of x, and ?P (f
t
) =
?
x,y
?
P (x, y)f
t
(x, y) is
the expected value of f
t
(x, y). To avoid overfit-
ting the training samples, some add a regulariza-
tion term and solve:
min
w
L(w)?
?
x
?
P (x)logT
w
(x)?
?
t
w
t
?
P(f
t
)+
P
t
w
2
t
2?
2
,
(3)
1A complete version of this work is at http:
//www.csie.ntu.edu.tw/
?
cjlin/papers/
maxent_journal.pdf.
where ? is a regularization parameter. We focus
on (3) instead of (2) because (3) is strictly convex.
Iterative scaling (IS) methods are popular in
training Maxent models. They all share the same
property of solving a one-variable sub-problem
at a time. Existing IS methods include general-
ized iterative scaling (GIS) by Darroch and Rat-
cliff (1972), improved iterative scaling (IIS) by
Della Pietra et al (1997), and sequential condi-
tional generalized iterative scaling (SCGIS) by
Goodman (2002). In optimization, coordinate de-
scent (CD) is a popular method which also solves
a one-variable sub-problem at a time. With these
many IS and CD methods, it is uneasy to see their
differences. In Section 2, we propose a unified
framework to describe IS and CD methods from
an optimization viewpoint. Using this framework,
we design a fast CD approach for Maxent in Sec-
tion 3. In Section 4, we compare the proposed
CD method with IS and LBFGS methods. Results
show that the CD method is more efficient.
Notation n is the number of features. The total
number of nonzeros in samples and the average
number of nonzeros per feature are respectively
#nz ?
?
x,y
?
t:f
t
(x,y) 6=0
1 and ?l ? #nz/n.
2 A Framework for IS Methods
2.1 The Framework
The one-variable sub-problem of IS methods is re-
lated to the function reduction L(w+ze
t
)?L(w),
where e
t
= [0, . . . , 0, 1, 0, . . . , 0]
T
. IS methods
differ in how they approximate the function reduc-
tion. They can also be categorized according to
whether w?s components are sequentially or par-
allely updated. In this section, we create a frame-
work in Figure 1 for these methods.
Sequential update For a sequential-update
algorithm, once a one-variable sub-problem is
solved, the corresponding element in w is up-
dated. The new w is then used to construct the
next sub-problem. The procedure is sketched in
285
Iterative scaling
Sequential update
Find A
t
(z) to approximate
L(w + ze
t
)? L(w)
SCGIS
Let A
t
(z) =
L(w+ze
t
)?L(w)
CD
Parallel update
Find a separable function A(z) to
approximate L(w + z)? L(w)
GIS, IIS
Figure 1: An illustration of various iterative scaling methods.
Algorithm 1 A sequential-update IS method
While w is not optimal
For t = 1, . . . , n
1. Find an approximate function A
t
(z) sat-
isfying (4).
2. Approximately min
z
A
t
(z) to get z?
t
.
3. w
t
? w
t
+ z?
t
.
Algorithm 1. If the t-th component is selected for
update, a sequential IS method solves the follow-
ing one-variable sub-problem:
min
z
A
t
(z),
where A
t
(z) bounds the function difference:
A
t
(z) ? L(w + ze
t
)? L(w)
=
?
x
?
P (x) log
T
w+ze
t
(x)
T
w
(x)
+ Q
t
(z)
(4)
and Q
t
(z)?
2w
t
z+z
2
2?
2
? z
?
P (f
t
). (5)
An approximate function A
t
(z) satisfying (4) does
not ensure that the function value is strictly de-
creasing. That is, the new function value L(w +
ze
t
) may be only the same as L(w). Therefore,
we can impose an additional condition
A
t
(0) = 0 (6)
on the approximate function A
t
(z). If A?
t
(0) 6= 0
and assume z?
t
? argmin
z
A
t
(z) exists, with the
condition A
t
(0)=0, we have A
t
(z?
t
)<0. This in-
equality and (4) then imply L(w + z?
t
e
t
)<L(w).
If A?
t
(0) = ?
t
L(w) = 0, the convexity of L(w)
implies that we cannot decrease the function value
by modifying w
t
. Then we should move on to
modify other components of w.
A CD method can be viewed as a sequential IS
method. It solves the following sub-problem:
min
z
A
CD
t
(z) = L(w + ze
t
)? L(w)
without any approximation. Existing IS methods
consider approximations as A
t
(z) may be simpler
for minimization.
Parallel update A parallel IS method simul-
taneously constructs n independent one-variable
sub-problems. After (approximately) solving all
of them, the whole vector w is updated. Algo-
rithm 2 gives the procedure. The differentiable
function A(z), z ? Rn, is an approximation of
L(w + z)? L(w) satisfying
A(z) ? L(w + z)? L(w), A(0) = 0, and
A(z) =
?
t
A
t
(z
t
).
(7)
Similar to (4) and (6), the first two conditions en-
Algorithm 2 A parallel-update IS method
While w is not optimal
1. Find approximate functions A
t
(z
t
) ?t satis-
fying (7).
2. For t = 1, . . . , n
Approximately min
z
t
A
t
(z
t
) to get z?
t
.
3. For t = 1, . . . , n
w
t
? w
t
+ z?
t
.
sure that the function value is strictly decreasing.
The last condition shows thatA(z)is separable, so
min
z
A(z) =
?
t
min
z
t
A
t
(z
t
).
That is,we can minimizeA
t
(z
t
),?t simultaneously,
and then update w
t
?t together. A parallel-update
method possesses nice implementation properties.
However, since it less aggressively updates w, it
usually converges slower. If A(z) satisfies (7),
taking z = z
t
e
t
implies that (4) and (6) hold for
any A
t
(z
t
). A parallel method could thus be trans-
formed to a sequential method using the same ap-
proximate function, but not vice versa.
2.2 Existing Iterative Scaling Methods
We introduce GIS, IIS and SCGIS via the pro-
posed framework. GIS and IIS use a parallel up-
date, but SCGIS is sequential. Their approximate
functions aim to bound the function reduction
L(w+z)?L(w)=
?
x
?
P (x) log
T
w+z
(x)
T
w
(x)
+
?
t
Q
t
(z
t
),
(8)
where T
w
(x) and Q
t
(z
t
) are defined in (1) and (5),
respectively. Then GIS, IIS and SCGIS use simi-
lar inequalities to get approximate functions. They
apply log? ? ?? 1 ?? > 0 to get
(8)??
x,y
?
P (x)P
w
(y|x)(e
P
t
z
t
f
t
(x,y)
?1)+
?
t
Q
t
(z
t
).
(9)
GIS defines
f
#
? max
x,y
f
#
(x, y), f
#
(x, y) ?
?
t
f
t
(x, y),
and adds a feature f
n+1
(x, y)?f
#
?f
#
(x, y) with
z
n+1
=0. Assuming f
t
(x, y) ? 0, ?t, x, y, and
using Jensen?s inequality
e
P
n+1
t=1
f
t
(x,y)
f
#
(z
t
f
#
)
?
?
n+1
t=1
f
t
(x,y)
f
#
e
z
t
f
#
and
e
P
t
z
t
f
t
(x,y)
?
?
t
f
t
(x,y)
f
#
e
z
t
f
#
+
f
n+1
(x,y)
f
#
, (10)
we obtain n independent one-variable functions:
A
GIS
t
(z
t
) =
e
z
t
f
#
?1
f
#
?
x,y
?
P (x)P
w
(y|x)f
t
(x, y)
+ Q
t
(z
t
).
286
IIS applies Jensen?s inequality
e
P
t
f
t
(x,y)
f
#
(x,y)
(z
t
f
#
(x,y))
?
?
t
f
t
(x,y)
f
#
(x,y)
e
z
t
f
#
(x,y)
on (9) to get the approximate function
A
IIS
t
(z
t
) =
?
x,y
?
P (x)P
w
(y|x)f
t
(x, y)
e
z
t
f
#
(x,y)
?1
f
#
(x,y)
+ Q
t
(z
t
).
SCGIS is a sequential-update method. It replaces
f
# in GIS with f#
t
? max
x,y
f
t
(x, y). Using z
t
e
t
as z in (8), a derivation similar to (10) gives
e
z
t
f
t
(x,y)
?
f
t
(x,y)
f
#
t
e
z
t
f
#
t
+
f
#
t
?f
t
(x,y)
f
#
t
.
The approximate function of SCGIS is
A
SCGIS
t
(z
t
) =
e
z
t
f
#
t
?1
f
#
t
?
x,y
?
P (x)P
w
(y|x)f
t
(x, y)
+ Q
t
(z
t
).
We prove the linear convergence of existing IS
methods (proof omitted):
Theorem 1 Assume each sub-problem As
t
(z
t
) is
exactly minimized, where s is IIS, GIS, SCGIS, or
CD. The sequence {wk} generated by any of these
four methods linearly converges. That is, there is
a constant ? ? (0, 1) such that
L(w
k+1
)?L(w
?
) ? (1??)(L(w
k
)?L(w
?
)),?k,
where w? is the global optimum of (3).
2.3 Solving one-variable sub-problems
Without the regularization term, by A?
t
(z
t
) = 0,
GIS and SCGIS both have a simple closed-form
solution of the sub-problem. With the regular-
ization term, the sub-problems no longer have a
closed-form solution. We discuss the cost of solv-
ing sub-problems by the Newton method, which
iteratively updates z
t
by
z
t
? z
t
?A
s
t
?
(z
t
)/A
s
t
??
(z
t
). (11)
Here s indicates an IS or a CD method.
Below we check the calculation of As
t
?
(z
t
) as
the cost of As
t
??
(z
t
) is similar. We have
A
s
t
?
(z
t
)=
?
x,y
?
P (x)P
w
(y|x)f
t
(x, y)e
z
t
f
s
(x,y)
+ Q
?
t
(z
t
)
(12)
where
f
s
(x, y) ?
?
?
?
?
?
f
# if s is GIS,
f
#
t
if s is SCGIS,
f
#
(x, y) if s is IIS.
For CD,
A
CD
t
?
(z
t
)=Q
?
t
(z
t
)+
?
x,y
?
P (x)P
w+z
t
e
t
(y|x)f
t
(x, y).
(13)
The main cost is on calculating P
w
(y|x) ?x, y,
whenever w is updated. Parallel-update ap-
proaches calculate P
w
(y|x) once every n sub-
problems, but sequential-update methods evalu-
ates P
w
(y|x) after every sub-problem. Consider
the situation of updating w to w+z
t
e
t
. By (1),
Table 1: Time for minimizing A
t
(z
t
) by the New-
ton method CD GIS SCGIS IIS
1st Newton direction O(?l) O(?l) O(?l) O(?l)
Each subsequent
Newton direction O(
?
l) O(1) O(1) O(
?
l)
obtaining P
w+z
t
e
t
(y|x) ?x, y requires expensive
O(#nz) operations to evaluate S
w+z
t
e
t
(x, y) and
T
w+z
t
e
t
(x) ?x, y. A trick to trade memory for
time is to store all S
w
(x, y) and T
w
(x),
S
w+z
t
e
t
(x, y)=S
w
(x, y)e
z
t
f
t
(x,y)
,
T
w+z
t
e
t
(x)=T
w
(x)+
?
y
S
w
(x, y)(e
z
t
f
t
(x,y)
?1).
Since S
w+z
t
e
t
(x, y) = S
w
(x, y) if f
t
(x, y) =
0, this procedure reduces the the O(#nz) opera-
tions to O(#nz/n) = O(?l). However, it needs
extra spaces to store all S
w
(x, y) and T
w
(x).
This trick for updating P
w
(y|x) has been used
in SCGIS (Goodman, 2002). Thus, the first
Newton iteration of all methods discussed here
takes O(?l) operations. For each subsequent
Newton iteration, CD needs O(?l) as it calcu-
lates P
w+z
t
e
t
(y|x) whenever z
t
is changed. For
GIS and SCGIS, if ?
x,y
?
P (x)P
w
(y|x)f
t
(x, y)
is stored at the first Newton iteration, then (12)
can be done in O(1) time. For IIS, because
f
#
(x, y) of (12) depends on x and y, we cannot
store
?
x,y
?
P (x)P
w
(y|x)f
t
(x, y) as in GIS and
SCGIS. Hence each Newton direction needs O(?l).
We summarize the cost for solving sub-problems
in Table 1.
3 Comparison and a New CD Method
3.1 Comparison of IS/CD methods
From the above discussion, an IS or a CD method
falls into a place between two extreme designs:
A
t
(z
t
) a loose bound
?
A
t
(z
t
) a tight bound
Easy to minimize A
t
(z
t
) Hard to minimizeA
t
(z
t
)
There is a tradeoff between the tightness to bound
the function difference and the hardness to solve
the sub-problem. To check how IS and CD meth-
ods fit into this explanation, we obtain relation-
ships of their approximate functions:
A
CD
t
(z
t
) ? A
SCGIS
t
(z
t
) ? A
GIS
t
(z
t
),
A
CD
t
(z
t
) ? A
IIS
t
(z
t
) ? A
GIS
t
(z
t
) ? z
t
.
(14)
The derivation is omitted. From (14), CD con-
siders more accurate sub-problems than SCGIS
and GIS. However, to solve each sub-problem,
from Table 1, CD?s each Newton step takes more
time. The same situation occurs in comparing
IIS and GIS. Therefore, while a tight A
t
(z
t
) can
287
give faster convergence by handling fewer sub-
problems, the total time may not be less due to
the higher cost of each sub-problem.
3.2 A Fast CD Method
We develop a CD method which is cheaper in
solving each sub-problem but still enjoys fast fi-
nal convergence. This method is modified from
Chang et al (2008), a CD approach for linear
SVM. We approximately minimize ACD
t
(z) by ap-
plying only one Newton iteration. The Newton di-
rection at z = 0 is now
d = ?A
CD
t
?
(0)/A
CD
t
??
(0). (15)
As taking the full Newton direction may not de-
crease the function value, we need a line search
procedure to find ? ? 0 such that z = ?d satisfies
the following sufficient decrease condition:
A
CD
t
(z)?A
CD
t
(0) = A
CD
t
(z) ? ?zA
CD
t
?
(0), (16)
where ? is a constant in (0, 1/2). A simple
way to find ? is by sequentially checking ? =
1, ?, ?
2
, . . . , where ? ? (0, 1). The line search
procedure is guaranteed to stop (proof omitted).
We can further prove that near the optimum two
results hold: First, the Newton direction (15) sat-
isfies the sufficient decrease condition (16) with
?=1. Then the cost for each sub-problem is O(?l),
similar to that for exactly solving sub-problems of
GIS or SCGIS. This result is important as other-
wise each trial of z = ?d expensively costs O(?l)
for calculating ACD
t
(z). Second, taking one New-
ton direction of the tighter ACD
t
(z
t
) reduces the
function L(w) more rapidly than exactly minimiz-
ing a loose A
t
(z
t
) of GIS, IIS or SCGIS. These
two results show that the new CD method im-
proves upon the traditional CD by approximately
solving sub-problems, while still maintains fast
convergence.
4 Experiments
We apply Maxent models to part of
speech (POS) tagging for BROWN corpus
(http://www.nltk.org) and chunk-
ing tasks for CoNLL2000 (http://www.
cnts.ua.ac.be/conll2000/chunking).
We randomly split the BROWN corpus
to 4/5 training and 1/5 testing. Our im-
plementation is built upon OpenNLP
(http://maxent.sourceforge.net).
We implement CD (the new one in Section 3.2),
GIS, SCGIS, and LBFGS for comparisons. We
include LBFGS as Malouf (2002) reported that
it is better than other approaches including GIS
0 500 1000 1500 2000
10?2
10?1
100
101
Training Time (s)
Re
lat
ive
 fu
nc
tio
n 
va
lue
 di
ffe
re
nc
e
 
 
CD
SCGIS
GIS
LBFGS
(a) BROWN
0 50 100 150 200
10?2
10?1
100
101
102
Training Time (s)
Re
lat
ive
 fu
nc
tio
n 
va
lue
 di
ffe
re
nc
e
 
 
CD
SCGIS
GIS
LBFGS
(b) CoNLL2000
0 500 1000 1500 2000
94
94.5
95
95.5
96
96.5
97
Training Time (s)
Te
sti
ng
 A
cc
ur
ac
y
 
 
CD
SCGIS
GIS
LBFGS
(c) BROWN
0 50 100 150 200
90
90.5
91
91.5
92
92.5
93
93.5
Training Time (s)
F1
 m
ea
su
re
 
 
CD
SCGIS
GIS
LBFGS
(d) CoNLL2000
Figure 2: First row: time versus the relative func-
tion value difference (17). Second row: time ver-
sus testing accuracy/F1. Time is in seconds.
and IIS. We use ?2 = 10, and set ? = 0.5 and
? = 0.001 in (16).
We begin at checking time versus the relative
difference of the function value to the optimum:
L(w)? L(w
?
)/L(w
?
). (17)
Results are in the first row of Figure 2. We check
in the second row of Figure 2 about testing ac-
curacy/F1 versus training time. Among the three
IS/CD methods compared, the new CD approach
is the fastest. SCGIS comes the second, while
GIS is the last. This result is consistent with
the tightness of their approximation functions; see
(14). LBFGS has fast final convergence, but it
does not perform well in the beginning.
5 Conclusions
In summary, we create a general framework for
explaining IS methods. Based on this framework,
we develop a new CD method for Maxent. It is
more efficient than existing IS methods.
References
K.-W. Chang, C.-J. Hsieh, and C.-J. Lin. 2008. Coor-
dinate descent method for large-scale L2-loss linear
SVM. JMLR, 9:1369?1398.
John N. Darroch and Douglas Ratcliff. 1972. Gener-
alized iterative scaling for log-linear models. Ann.
Math. Statist., 43(5):1470?1480.
Stephen Della Pietra, Vincent Della Pietra, and John
Lafferty. 1997. Inducing features of random fields.
IEEE PAMI, 19(4):380?393.
Joshua Goodman. 2002. Sequential conditional gener-
alized iterative scaling. In ACL, pages 9?16.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In
CONLL.
288
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 601?612,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Constrained Latent Variable Model for Coreference Resolution
Kai-Wei Chang Rajhans Samdani Dan Roth
University of Illinois at Urbana-Champaign
{kchang10|rsamdan2|danr}@illinois.edu
Abstract
Coreference resolution is a well known clus-
tering task in Natural Language Processing. In
this paper, we describe the Latent Left Linking
model (L3M), a novel, principled, and linguis-
tically motivated latent structured prediction
approach to coreference resolution. We show
that L3M admits efficient inference and can be
augmented with knowledge-based constraints;
we also present a fast stochastic gradient based
learning. Experiments on ACE and Ontonotes
data show that L3M and its constrained ver-
sion, CL3M, are more accurate than several
state-of-the-art approaches as well as some
structured prediction models proposed in the
literature.
1 Introduction
Coreference resolution is a challenging task, that in-
volves identification and clustering of noun phrases
mentions that refer to the same real-world entity.
Most machine learning approaches to coreference
resolution learn a scoring function to estimate the
compatibility between two mentions or two sets of
previously clustered mentions. Then, a decoding al-
gorithm is designed to aggregate these scores and
find an optimal clustering assignment.
The most popular of these frameworks is the pair-
wise mention model (Soon et al, 2001; Ng and
Cardie, 2002; Bengtson and Roth, 2008), which
learns a compatibility score of mention-pairs and
uses these pairwise scores to obtain a global cluster-
ing. Recently, efforts have been made (Haghighi and
Klein, 2010; Rahman and Ng, 2011b; Rahman and
Ng, 2011c) to consider models that capture higher
order interactions, in particular, between mentions
and previously identified entities (that is, between
mentions and clusters). While such models are po-
tentially more expressive, they are largely based on
heuristics to achieve computational tractability.
This paper focuses on a novel and principled ma-
chine learning framework that pushes the state-of-
the-art while operating at a mention-pair granularity.
We present two models ? the Latent Left-Linking
Model (L3M), and a version of that is augmented
with domain knowledge-based constraints, the Con-
strained Latent Left-Linking Model (CL3M). L3M
admits efficient inference, linking each mention to a
previously occurring mention to its left, much like
the existing best-left-link inference models (Ng and
Cardie, 2002; Bengtson and Roth, 2008). How-
ever, unlike previous best-link techniques, learning
in our case is performed jointly with decoding ? we
present a novel latent structural SVM approach, op-
timized using a fast stochastic gradient-based tech-
nique. Furthermore, we present a probabilistic gen-
eralization of L3M that is more expressive in that
it is capable of considering mention-entity interac-
tions using scores at the mention-pair granularity.
We augment this model with a temperature-like pa-
rameter (Samdani et al, 2012) to provide additional
flexibility.
CL3M augments L3M with knowledge-based
constraints following (Roth and Yih, 2004; Denis
and Baldridge, 2007). This capability is very de-
sirable as shown by the success of the rule-based de-
terministic approach of Raghunathan et al (2010)
in the CoNLL shared task 2011 (Pradhan et al,
2011). In L3M, domain-specific constraints are in-
corporated into learning and inference in a straight-
forward way. CL3M scores a mention?s contribution
to its cluster by combining the corresponding score
601
of the underlying L3M model with that from a set of
constraints.
Most importantly, in our experiments on bench-
mark coreference datasets, we show that CL3M,
with just five constraints, compares favorably with
other, more complicated, state-of-the-art algorithms
on a variety of evaluation metrics. Over-
all, the main contribution of this paper is a
principled machine learning model operating at
mention-pair granularity, using easy to implement
constraint-augmented inference and learning, that
yields competitive results on coreference resolution
on Ontonotes-5.0 (Pradhan et al, 2012) and ACE
2004 (NIST, 2004).
2 Related Work
The idea of Latent Left-linking Model (L3M) is in-
spired by a popular inference approach to corefer-
ence which we call the Best-Left-Link approach (Ng
and Cardie, 2002; Bengtson and Roth, 2008). In the
best-left-link strategy, each mention i is connected
to the best antecedent mention j with j < i (i.e. a
mention occurring to the left of i, assuming a left-
to-right reading order), thereby creating a left-link.
The ?best? antecedent mention is the one with the
highest pairwise score, wij ; furthermore, if wij is
below some threshold, say 0, then i is not connected
to any antecedent mention. The final clustering is
a transitive closure of these ?best? links. The intu-
ition behind best-left-link strategy is based on how
humans read and decipher coreference links ? they
mostly rely on information to the left of the men-
tion when deciding whether to add it to a previously
constructed cluster or not. This strategy has been
successful and commonly used in coreference res-
olution (Ng and Cardie, 2002; Bengtson and Roth,
2008; Stoyanov et al, 2009). However, most works
have developed ad-hoc approaches to implement this
idea. For instance, Bengtson and Roth (2008) train
a model w on binary training data generated by tak-
ing for each mention, the closest antecedent corefer-
ent mention as a positive example, and all the other
mentions as negative examples. Similar approaches
to training and, additionally, decoupling the training
stage from the clustering stage were used by other
systems. In this paper, we formalize the learning
problem of the best-left-link model as a structured
prediction problem and analyze our system with de-
tailed experiments. Furthermore, we generalize this
approach by considering multiple pairwise left-links
instead of just the best link, efficiently capturing the
notion of a mention-to-cluster link.
Many techniques in the coreference literature
break away from the mention pair-based, best-left-
link paradigm. Denis and Baldridge (2008) and Ng
(2005) learn a local ranker to rank the mention
pairs based on their compatibility. While these ap-
proaches achieve decent empirical performance, it
is unclear why these are the right ways to train the
model. Some techniques consider a more expres-
sive model by using features defined over mention-
cluster or cluster-cluster (Rahman and Ng, 2011c;
Stoyanov and Eisner, 2012; Haghighi and Klein,
2010). For these models, the inference and learn-
ing algorithms are usually complicated. Very re-
cently, Durrett et al (2013) propose a probabilis-
tic model which enforces structural agreement con-
straints between specified properties of mention
cluster when using a mention-pair model. This ap-
proach is very related to the probabilistic extension
of our method as both models attempt to leverage
entity-level information from mention-pair features.
However, our approach is simpler because it directly
considers the probabilities of multiple links. Fur-
thermore, while their model performs only slightly
better than the Stanford rule-based system (Lee et
al., 2011), we significantly outperform this system.
Most importantly, our model obtains state-of-the-art
performance on OntoNotes-5.0 while still operating
at the mention-pair granularity. We believe that this
is due to our novel and principled structured predic-
tion framework which results in accurate (and effi-
cient) training.
Several structured prediction techniques have
been applied to coreference resolution in the ma-
chine learning literature. For example, McCallum
and Wellner (2003) and Finley and Joachims (2005)
model coreference as a correlational clustering prob-
lem (Bansal et al, 2002) on a complete graph over
the mentions with edge weights given by the pair-
wise classifier. However, correlational clustering is
known to be NP Hard (Bansal et al, 2002); nonethe-
less, an ILP solver or an approximate inference algo-
rithm can be used to solve this problem. Another ap-
proach proposed by Yu and Joachims (2009) formu-
602
lates coreference with latent spanning trees. How-
ever, their approach has no directionality between
mentions, whereas our latent structure captures the
natural left-to-right ordering of mentions. In our
experiments (Sec. 5), we show that our technique
vastly outperforms both the spanning tree and the
correlational clustering techniques. We also com-
pare with (Fernandes et al, 2012) and the pub-
licly available Stanford coreference system (Raghu-
nathan et al, 2010; Lee et al, 2011), a state-of-the-
art rule-based system.
Finally, some research (Ratinov and Roth, 2012;
Bansal and Klein, 2012; Rahman and Ng, 2011a)
has tried to integrate world knowledge from web-
based statistics or knowledge bases into a corefer-
ence system. World knowledge is potentially use-
ful for resolving coreference and can be injected
into our system in a straightforward way via the
constraints framework. We will show an example
of incorporating our system with name-entity and
WordNet-based similarity metric (Q. Do, 2009) in
Sec. 5. Including massive amount of information
from knowledge resources is not the focus of this
paper and may distort the comparison with other
relevant models but our results indicate that this is
doable in our model, and may provide significant
improvements.
3 Latent Left-Linking Model with
Constraints
In this section, we describe our Constrained Latent
Left-Linking Model (CL3M). CL3M is inspired by
a few ideas from the literature: (a) the popular Best-
Left-Link inference approach to coreference (Ng and
Cardie, 2002; Bengtson and Roth, 2008), and (b) the
injection of domain knowledge-based constraints for
structured prediction (Roth and Yih, 2004; Clarke
and Lapata, 2006; Chang et al, 2012b; Ganchev et
al., 2010; Koo et al, 2010; Pascal and Baldridge,
2009).
We first introduce the notion of a pairwise
mention-scorer, then introduce our Left-Linking
Model (L3M), and finally describe how to inject con-
straints into our model.
Let d be a document with md mentions. Mentions
are denoted solely using their indices, ranging from
1 to md. A coreference clustering C for document
d is a collection of disjoint sets partitioning the set
{1, . . . , md}. We represent C as a binary function
with C(i, j) = 1 if mentions i and j are coreferent,
otherwise C(i, j) = 0. Let s(C;w, d) be the score
of a given clustering C for a given document and a
given pairwise weight vector w. Then, during infer-
ence, a clustering C is predicted by maximizing the
scoring function s(C;w, d), over all valid (i.e. sat-
isfying symmetry and transitivity) clustering binary
functions C : {1, . . . , md}?{1, . . . , md} ? {0, 1}.
3.1 Mention Pair Scorer
We model the task of coreference resolution using a
pairwise scorer which indicates the compatibility of
a pair of mentions. The inference routine then pre-
dicts the final clustering ? a structured prediction
problem ? using these pairwise scores.
Specifically, for any two mentions i and j (w.l.o.g.
j < i), we produce a pairwise compatibility score
wji using extracted features ?(j, i) as
wji = w ? ?(j, i) , (1)
where w is a weight parameter that is learned.
3.2 Latent Left-Linking Model
Our inference algorithm is inspired by the best-left-
link approach. In particular, the score s(C; d,w) is
defined so that each mention links to the antecedent
mention (to its left) with the highest score (as long
as the score is above some threshold, say, 0). Specif-
ically:
s(C; d,w) =
md
?
i=1
max
0?j<i,C(i,j)=1
w ? ?(j, i) . (2)
In order to simplify the notation, we introduce a
dummy mention with index 0, which is to the left
(i.e. appears before) of all other mentions and has
w0i = 0 for all actual mentions i > 0. For a given
clustering C, if a mention i is not co-clustered with
any previous actual mention j, 0 < j < i, then we
assume that i links to 0 and C(i, 0) = 1. In other
words, C(i, 0) = 1 iff i is the first actual item of a
cluster in C. However, such an item i is not consid-
ered to be co-clustered with 0 and for any valid clus-
tering, item 0 is always in a singleton dummy clus-
ter, which is eventually discarded. The important
property of the score s is that it is exactly maximized
603
by the best-left-link inference, as it maximizes indi-
vidual left link scores and the creation of one left-
link does not affect the creation of other left-links.
3.3 Learning
We use a max-margin approach to learn w. We are
given a training set D of documents where for each
document d ? D, Cd refers to the annotated ground
truth clustering. Then we learn w by minimizing
L(w) =
?
2
?w?2 + 1|D|
?
d?D
1
md
(
max
C
(
s(C; d,w)
+ ?(C, Cd)
)
? s(Cd; d,w)
)
,
where ?(C, Cd) is a loss function used in corefer-
ence. In order to achieve tractable loss-augmented
minimization ? something not possible with stan-
dard loss functions used in coreference (e.g.
B3 (Bagga and Baldwin, 1998)) ? we use a de-
composable loss function that just counts the num-
ber of mention pairs on which C and Cd disagree:
?(C, Cd) =
?md
i,j=0,j<i IC(i,j)=Cd(i,j), where I is
a binary indicator function. This loss function
is equivalent to the numerator of the Rand index
loss (Rand, 1971). With this form of loss function
and using the scoring function in Eq. (2), we can
write L(w) as
?
2
?w?2 + 1|D|
?
d?D
1
md
md
?
i=1
(
max
0?j<i
(
w ? ?(j, i)
+ ?(Cd, i, j)
)
? max
0?j<i,C(i,j)=1
(w ? ?(j, i))
)
,
(3)
where ?(Cd, i, j) = 1 ? Cd(i, j) is the loss-based
margin that is 1 if i and j are not coreferent in Cd,
and is 0 otherwise. In the above objective function,
the left-links remain latent while we get to observe
the clustering. This objective function is related to
latent structural SVMs (Yu and Joachims, 2009).
However Yu and Joachims (2009) use a spanning
tree based latent structure which does not have the
left-to-right directionality we exploit. We can mini-
mize the above function using Concave Convex Pro-
cedure (Yuille and Rangarajan, 2003), which is guar-
anteed to reach the local minima. However, such a
procedure is costly as it requires doing inference on
all the documents to compute a single gradient up-
date. Consequently, we choose a faster stochastic
sub-gradient descent (SGD) approach. Since L(w)
in Eq. (3) decomposes not only over training doc-
uments, but also over individual mentions in each
document, we can perform SGD on a per-mention
basis. The stochastic sub-gradient w.r.t. mention i
in document d is given by
?L(w)id ? ?(j?, i) ? ?(j??, i) + ?w, where (4)
j? = arg max
0?j<i
(w ? ?(j, i) + 1 ? Cd(i, j))
j?? = arg max
0?j<i,C(i,j)=1
w ? ?(j, i)
While SGD has no theoretically convergence guar-
antee, it works excellently in our experiments.
Specifically, we observe that SGD achieves similar
training performance to CCCP with a speed-up of
around 10,000.
3.4 Incorporating Constraints
Next, we show how to incorporate domain
knowledge-based constraints into L3M and gener-
alize it to CL3M. In CL3M, we obtain a cluster-
ing by maximizing a constraint-augmented scoring
function f given by
s(C; d,w) +
nc
?
p=1
?p?p(d, C),
where the second term on the R.H.S. is the
score contributed by domain specific constraints
?1, . . . , ?nc with their respective scores ?1, . . . , ?nc .
In particular, ?p(d, C) measures the extent to which
a given clustering C satisfies the pth constraint. Note
that this framework is general and can be applied to
inject mention-to-cluster or cluster-to-cluster level
constraints too. However, for simplicity, we con-
sider here only constraints between mention pairs.
This allows us derive fast greedy algorithm to solve
the inference problem. The details of our constraints
are presented in Sec. 5.
All of our constraints can be categorized into two
groups: ?must-link? and ?cannot-link?.?Must-link?
constraints encourage a pair of mentions to connect,
while ?cannot-link? constraints discourage mention
pairs from being linked. Consequently, the coeffi-
cients ?p associated with ?must-link? constraints are
positive while ?p for ?cannot-link? constraints are
negative. In the following, we briefly discuss how to
604
solve the inference problem with these two types of
constraints.
We slightly abuse notations and use ?p(j, i) to in-
dicate the pth constraint on a pair of mentions (i, j).
?p(j, i) is a binary function that is 1 iff two mentions
i and j satisfy the conditions specified in constraint
p. Chang et al (2011) shows that best-left-link in-
ference can be formulated as an ILP problem. When
we add constraints, the ILP becomes:
arg max
B,C?{0,1}
?
i,j:j<i
wjiBji +
?
i,j
?p?p(j, i)Cij
s.t Ckj ? Cij + Cki ? 1, ?i, j, k,
?i?1
j=0
Bji = 1, ?i
Bji < Cji, Cji = Cji,?i, j,
(5)
where Cij ? C(i, j) is a binary variable indicating
whether i and j are in the same cluster or not and
Bji is an auxiliary variable indicating the best-left-
link for mention i. The first set of inequality con-
straints in (5) enforces the transitive closure of the
clustering. The constraints Bji < Cji,?i, j enforce
the consistency between these two sets of variables.
One can use an off-the-shelf solver to solve Eq.
(5). However, when the absolute values of the con-
straint scores (|?p|) are high (the hard constraint
case), then the following greedy algorithm approxi-
mately solves the inference efficiently. We scan the
document from left-to-right (or in any other arbitrary
order). When processing mention i, we find
j? = arg max
j<i
wji +
?
k:C?(k,j)=1
?
p
?p?p(k, i),
(6)
where C? is the current clustering obtained from the
previous inference steps. Then, we add a link be-
tween mention i and j?. The rest of the infer-
ence process is the same as in the original best-left-
link inference. Specifically, this inference procedure
combines the classifier score for mention pair i, j,
with the constraints score of all mentions currently
co-clustered with j. We discuss this further in Sec-
tion 5.
4 Probabilistic Latent Left-Linking Model
In this section, we extend and generalize our left-
linking model approach to a probabilistic model,
Probabilistic Latent Left-Linking Model (PL3M),
that allows us to naturally consider mention-to-
entity (or mention-to-cluster) links. While in L3M,
we assumed that each mention links determinis-
tically to the max-scoring mention on its left, in
PL3M, we assume that mention i links to mention
j, j ? i, with probability given by
Pr[j ? i; d,w] = e
1
? (w??(i,j))
Zi(w, ?)
. (7)
Here Zi(w, ?) =
?
0?k<i e
1
? (w??(i,k)) is a normal-
izing constant and ? ? (0, 1] is a constant tem-
perature parameter that is tuned on a development
set (Samdani et al, 2012). We assume that the event
that mention i links to a mention j is independent of
the event that mention i? links to j? for i 6= i?.
Inference with PL3M: Given the probability of a
link as in Eq. (7), the probability that mention i joins
an existing cluster c, Pr[c ? i; d,w], is simply the
sum of the probabilities of i linking to the mentions
inside c:
Pr[c ? i; d,w] =
?
j?c,0?j<i
Pr[j ? i; d,w]
=
?
j?c,0?j<i
e
1
? (w??(i,j))
Zi(d,w, ?)
. (8)
Based on Eq. (8) and making use of the indepen-
dence assumption of left-links, we follow a simple
greedy clustering (or inference) algorithm: sequen-
tially add each mention i to a previously formed
cluster c?, where c? = arg maxc Pr[c ? i; d,w].
If the arg max cluster is the singleton cluster with
the dummy mention 0 (i.e. the score of all other
clusters is below the threshold of 0), then i starts a
new cluster and is not included in the dummy clus-
ter. Note that we link a mention to a cluster tak-
ing into account all the mentions inside that cluster,
mimicking the notion of a mention-to-cluster link.
This provides more expressiveness than the Best-
Left-Link inference, where a mention connects to
a cluster solely based on a single pairwise link to
some antecedent mention (the best-link mention) in
that cluster.
The case of ? = 0: As ? approaches zero, it is
easy to show that the probability P [j ? i; d, w]
605
in Eq. (7) approaches a Kronecker delta function
that puts probability 1 on the max-scoring mention
j = arg max0?k<i w??(i, j) (assuming no ties), and
0 everywhere else (Pletscher et al, 2010; Samdani et
al., 2012). Consequently, as ? ? 0, Pr[c ? i; d,w]
in Eq. 8 approaches a Kronecker delta function cen-
tered on the cluster containing the max-scoring men-
tion, thus reducing to the best-link case of L3M.
Thus, PL3M, when tuning the value of ?, is a strictly
more general model than L3M.
Learning with PL3M We use a likelihood-based
approach to learning with PL3M, and first compute
the probability Pr[C; d,w] of generating a cluster-
ing C, given w. We then learn w by minimizing
the regularized negative log-likelihood of the data,
augmenting the partition function with a loss-based
margin (Gimpel and Smith, 2010). We omit the de-
tails of likelihood computation due to lack of space.
With PL3M, we again follow a stochastic gradi-
ent descent technique instead of CCCP for the same
reasons mentioned in Sec. 3.3. The stochastic gra-
dient (subgradient when ? = 0) w.r.t. mention i in
document d is given by
?LL(w)id ?
?
0?j<i
pj?(i, j) ?
?
0?j<i
p?j?(i, j) + ?w,
where pj and p?j , j = 0, . . . , i ? 1, are non-negative
weights that sum to one and are given by
pj =
e
1
? (w??(i,j)+?(Cd,i,j))
?
0?k<i e
1
? (w??(i,k)+?(Cd,i,k))
and
p?j =
Cd(i, j)Zi(d,w, ?)
Zi(Cd; d,w, ?)
Pr[j ? i; d,w] .
Interestingly, the above update rule generalizes the
one for L3M, as we are incorporating a weighted
sum of all previous mentions in the update rule.
With ? ? 0, the SGD in Eq. (4) converges to the
SGD update in L3M (Eq. (4)). Finally, in the pres-
ence of constraints, we can fold them inside the pair-
wise link probabilities as in Eq. (6).
5 Experiments and Results
In this section, we present our experiments on the
two commonly used benchmarks for coreference
? Ontonotes-5.0 (Pradhan et al, 2012) and ACE
2004 (NIST, 2004). Table 1 exhibits our bottom line
results: CL3M achieves the best result reported on
Ontonotes-5.0 development set and essentially ties
with (Fernandes et al, 2012) on the test set. As
shown in Table 3, CL3M is also the best algorithm
on ACE and when evaluated on the gold mentions
of Ontonotes. We show that CL3M performs partic-
ularly well on clusters containing named entity men-
tions, which are more important for many informa-
tion extraction applications. In the rest of this sec-
tion, after describing our experimental setting, we
provide careful analysis of our algorithms and com-
pare them to competitive coreference approaches in
the literature.
5.1 Experimental Setup
Datasets: ACE 2004 contains 443 documents ?
we used a standard split of these documents into
268 training, 68 development, and 106 testing doc-
uments used by Culotta et al (2007) and Bengt-
son and Roth (2008). OntoNotes-5.0 dataset, re-
leased for the CoNLL 2012 Shared Task (Pradhan et
al., 2012), is by far the largest annotated corpus on
coreference. It contains 3,145 annotated documents
drawn from a wide variety of sources ? newswire,
bible, broadcast transcripts, magazine articles, and
web blogs. We report results on both development
set and test set. To test on the development set, we
further split the training data into training and devel-
opment sets.
Classifier details: For each of the pairwise ap-
proaches, we assume the pairwise score is given by
w??(?, ?)+t where ? are the features, w is the weight
vector learned by the approach, and t is a threshold
which we set to 0 during learning (as in Eq. (1)), but
use a tuned value (tuned on a development set) dur-
ing testing. For learning with L3M, we do stochastic
gradient descent with 5 passes over the data. Empir-
ically, we observe that this is enough to generate a
stable model. For PL3M (Sec. 4), we tune the value
of ? using the development set picking the best ?
from {0.0, 0.2, . . . , 1.0}. Recall that when ? = 0,
PL3M is the same as L3M. We refer to L3M and
PL3M with incorporating constraints during infer-
ence as CL3M and CPL3M (Sec. 3.4), respectively.
Metrics: We compare the systems using three
popular metrics for coreference ? MUC (Vilain et
al., 1995), BCUB (Bagga and Baldwin, 1998), and
606
Entity-based CEAF (CEAFe) (Luo, 2005). Follow-
ing, the CoNLL shared tasks (Pradhan et al, 2012),
we use the average F1 scores of these three metrics
as the main metric of comparison.
Features: We build our system on the publicly
available Illinois-Coref system1 primarily because it
contains a rich set of features presented in Bengtson
and Roth (2008) and Chang et al (2012a) (the latter
adds features for pronominal anaphora resolution).
We also compare with the Best-Left-Link approach
described by Bengtson and Roth (2008).
Constraints: We consider the following con-
straints in CL3M and CPL3M.
? SameSpan: two mentions must be linked to
each other if they share the same surface text
span and the number of words in the text span
is larger than a threshold (set as 5 in our imple-
mentation).
? SameDetNom: two mentions must be linked
to each other if both mentions start with a de-
terminer and the [0,1] wordnet-based similarity
score between the mention head words is above
a threshold (set to 0.8).
? SameProperName: two mentions must be
linked if they are both proper names and the
similarity score measured by a named entity-
based similarity metric, Illinois NESim2, are
higher than a threshold (set to 0.8). For a per-
son entity we add additional rules to extract the
first name, last name and professional title as
properties.
? ModifierMismatch: the constraint prevents two
mentions to be linked if the head modifiers
conflict. For example, the constraint prevents
?northern Taiwan? from linking to ?southern
Taiwan?. We gather a list of mutual exclusive
modifiers from the training data.
? PropertyMismatch: the constraint prevents two
mentions to be linked if their properties con-
flict. For example, it prevents male pronouns
to link to female pronouns and ?Mr. Clinton?
to link to ?Mrs. Clinton? by checking the gen-
der property. The properties we consider are
gender, number, professional title and the na-
1The system is available at http://cogcomp.cs.
illinois.edu/page/software_view/Coref/
2http://cogcomp.cs.illinois.edu/page/
software_view/NESim
MUC BCUB CEAFe AVG
Dev Set
Stanford 64.30 70.46 46.35 60.37
(Chang et al, 2012a) 65.75 70.25 45.30 60.43
(Martschat et al, 2012) 66.76 71.91 47.52 62.06
(Bjo?rkelund and Farkas, ) 67.12 71.18 46.84 61.71
(Chen and Ng, 2012) 66.4 71.8 48.8 62.3
(Fernandes et al, 2012) 69.46 71.93 48.66 63.35
L3M 67.88 71.88 47.16 62.30
CL3M 69.20 72.89 48.67 63.59
Test Set
Stanford 63.83 68.52 45.36 59.23
(Chang et al, 2012a) 66.38 69.34 44.81 60.18
(Martschat et al, 2012) 66.97 70.36 46.60 61.31
(Bjo?rkelund and Farkas, ) 67.58 70.26 45.87 61.24
(Chen and Ng, 2012) 63.7 69.0 46.4 59.7
(Fernandes et al, 2012) 70.51 71.24 48.37 63.37
L3M 68.31 70.81 46.73 61.95
CL3M 69.64 71.93 48.32 63.30
Table 1: Performance on OntoNotes-5.0 with predicted
mentions. We report the F1 scores (%) on various coref-
erence metrics (MUC, BCUB, CEAF). The column AVG
shows the average scores of the three. We observe that
PL3M and CPL3M (see Sec. 4) yields the same perfor-
mance as L3M and CL3M, respectively as the tuned ? for
all the datasets turned out to be 0.
tionality.
While the ?must-link? constraints described in the
paper can be treated as features, due to their high
precision, treating them as hard constraints (set ? to
a high value) is a safe and direct way to inject hu-
man knowledge into the learning model. Moreover,
our framework allows a constraint to use informa-
tion from previous decisions (such as ?cannot-link?
constraints). Treating such constraints as features
will complicate the learning model.
5.2 Performance of the End-to-End System
We compare our system with the top systems re-
ported in the CoNLL shared task 2012 as well as
with the Stanford?s publicly released rule-based sys-
tem (Lee et al, 2013; Lee et al, 2011), which won
the CoNLL 2011 Shared Task (Pradhan et al, 2011).
Note that all the systems use the same annotations
(e.g., gender prediction, part-of-speech tags, name
entity tags) provided by the shared task organizers.
607
However, each system implements its own mention
detector and pipelines the identified mentions into
the coreference clustering component. Moreover,
different systems use a different set of features. In
order to partially control for errors on mention de-
tection and better evaluate the clustering component
in our coreference system, we will also present re-
sults on correct (gold) mentions in the next section.
Table 1 shows the end-to-end results. On the
development set, only the best performing system
of Fernandes et al (2012) is better than L3M, but this
difference disappears when we use our system with
constraints, CL3M. Although our system is much
simple, it achieves the best B3 score on the test set
and is competitive with the best system participated
in the CoNLL shared task 2012.
Performance on named entities: The corefer-
ence annotation in Ontonotes 5.0 includes various
types of mentions. However, not all mention types
are equally interesting. In particular, clusters which
contain at least one proper name or a named entity
mention are more important for information extrac-
tion tasks like Wikification (Mihalcea and Csomai,
2007; Ratinov et al, 2011), cross-document coref-
erence resolution (Bagga and Baldwin, 1998), and
entity linking and knowledge based population (Ji
and Grishman, 2011).
Inspired by this, we compare our system to the
best systems in the CoNLL shared task of 2011
(Stanford (Lee et al, 2011)) and 2012 (Fernan-
des (Fernandes et al, 2012)) on the following spe-
cific tasks on Ontonotes-5.0.
? ENT-C: Evaluate the system on clusters that
contain at least one proper name mention. We
generate the gold annotation and system out-
puts by using the gold and predicted name en-
tity tag annotations provided by the CoNLL
shard task 2012. That is, if a cluster does not
include any name entity mention, then it will
be removed from the final clustering.
? PER-C: As in the construction of ENT-C, but
here we only consider clusters which contain at
least one ?Person (PER)? entity.
? ORG-C: As in the construction of Entity-C, but
here we only consider clusters which contain at
least one ?Organization (ORG)? entity.
Typically, the clusters that get ignored in the above
definitions contain only first and second person
Task Stanford Fernandes L3M CL3M
ENT-C 44.06 47.05 46.63 48.02
PER-C 34.04 36.43 37.01 37.57
ORG-C 25.02 26.23 26.22 27.01
Table 2: Performance on named entities for OntoNotes-
5.0 data. We compare our system to Fernandes (Fernan-
des et al, 2012) and Stanford (Lee et al, 2013) systems.
pronouns (which often happens in transcribed dis-
course.) Also note that all the systems are trained
with the same name entity tags, provided by the
shared task organizers, and we use the same name
entity tags to construct the specific clustering. Also,
in order to further ensure fairness, we do not tune
our system to favor the evaluation of these specific
types of clusters. We chose to do so because we only
have access to the system output of Fernandes et al
(2012).
Table 2 shows the results. The performance of
all systems degrades when considering only clusters
that contain name entities, indicating that ENT-C is
actually a harder task than the original coreference
resolution problem. In particular, resolving ORG
coreferent clusters is hard, because names of organi-
zations are sometimes confused with person names,
and they can be referred to using a range of pronouns
(including ?we? and ?it?). Overall, CL3M outper-
forms all the competing systems on the clusters that
contain at least one specific type of entity by a mar-
gin larger than that for the overall coreference.
5.3 Analysis on Gold Mentions
To better understand the contribution of our joint
learning and clustering model, we present experi-
ments assuming that gold mentions are given. The
definitions of gold mentions in ACE and Ontonotes
are different because Ontonotes-5.0 excludes single-
ton clusters in the annotation. In addition, Ontonotes
includes longer mentions; for example, it includes
NP and appositives in the same mention. We com-
pare with the publicly available Stanford (Lee et al,
2011) and IllinoisCoref (Chang et al, 2012a) sys-
tems; the system of Fernandes et al (2012) is not
publicly available. In addition, we also compare
with the following two structured prediction base-
lines that use the same set of features as L3M and
PL3M.
608
MUC BCUB CEAFe AVG
ACE 2004 Gold Ment.
All-Link-Red. 77.45 81.10 77.57 78.71
Spanning 73.31 79.25 74.66 75.74
IllinoisCoref 76.02 81.04 77.6 78.22
Stanford 75.04 80.45 76.75 77.41
(Stoyanov and Eisner, 2012) 80.1 81.8 - -
L3M 77.57 81.77 78.15 79.16
PL3M 78.18 82.09 79.21 79.83
CL3M 78.17 81.64 78.45 79.42
CPL3M 78.29 82.20 79.26 79.91
Ontonotes 5.0 Gold Ment.
All-Link-Red. 83.72 75.59 64.00 74.44
Spanning 83.64 74.83 61.07 73.18
IllinoisCoref 80.84 74.29 65.96 73.70
Stanford 82.26 76.82 61.69 73.59
L3M 83.44 78.12 64.56 75.37
PL3M 83.97 78.25 65.69 75.97
CL3M 84.10 78.30 68.74 77.05
CPL3M 84.80 78.74 68.75 77.43
Table 3: Performance on ACE 2004 and OntoNotes-5.0.
All-Link-Red. is based on correlational clustering; Span-
ning is based on latent spanning forest based clustering
(see Sec. 2). Our proposed approach is L3M (Sec. 3) and
PL3M (sec. 4). CL3M and CPL3M are the version with
incorporating constraints.
1. All-Link-Red: a reduced and faster alterna-
tive to the correlational clustering based ap-
proach (Finley and Joachims, 2005). We im-
plemented this algorithm as an ILP and droped
one of the three transitivity constraints for each
triplet of mention variables. Following Pascal
and Baldridge (2009) and Chang et al (2011)
we observe that this slightly improves the ac-
curacy over a pure correlation clustering ap-
proach, in addition to speeding up inference.
2. Spanning: the latent spanning forest based ap-
proach presented by Yu and Joachims (2009).
We use the publicly available implementation
provided by the authors3 for the ACE data;
since their CCCP implementation is slow, we
implemented our own stochastic gradient de-
scent version to scale it to the much larger
Ontonotes data.
3Available at http://www.cs.cornell.edu/ cnyu/latentssvm/
Table 3 lists the results. Although L3M is simple
and use only the features defined on pairwise men-
tions, it compares favorably with all recently pub-
lished results. Moreover, the probabilistic general-
ization of L3M, PL3M, achieves even better perfor-
mance. For example, L3M with ? = 0.2 improves
L3M with ? = 0 by 0.7 points in ACE 2004. In par-
ticular, This shows that considering more than a one
left-links is helpful. This is in contrast with the pre-
dicted mentions where ? = 0 performed best. We
suspect that this is because noisy mentions can hurt
the performance of PL3M that takes into account
not just the best scoring links, but also weaker links
which are likely to be less reliable (more false pos-
itives). Also, as opposed to what is reported by Yu
and Joachims (2009), the correlation clustering ap-
proach performs better than the spanning forest ap-
proach. We think that this is because we compare
the systems on different metrics than they did and
also because we use exact ILP inference for corre-
lational clustering whereas Yu and Joachims (2009)
used approximate greedy inference.
Both L3M and PL3M can be benefit from using
constraints. However, The constraints improve only
marginally on the ACE 2004 data because ACE uses
shorter phrases as mentions. Consequently, con-
straints designed for leveraging information from
long mention spans are less effective. Overall, the
experiments show that L3M and PL3M perform well
on modeling coreference clustering.
5.4 Ablation Study of Constrains
Finally, we study the value of individual constraints
by adding one constraint at a time to the corefer-
ence system starting with the simple L3M model.
The system with all the constraints added is the
CL3M model introduced in Table 1. We then re-
move individual constraints from CL3M to assess
its contribution. Table 4 shows the results on the
Ontonotes dataset with predicted mentions. Overall,
it is shown that each one of the constraints has a con-
tribution, and that using all the constraints improves
the performance of the system by 1.29% in the AVG
F1 score. In particular, most of this improvement
(1.19%) is due to the must-link constraints (the first
four constraints in the table). The must-link con-
straints are more useful for L3M as L3M achieves
higher precision than recall (e.g., the precision and
609
MUC BCUB CEAFe AVG
L3M 67.88 71.88 47.16 62.30
+SameSpan 68.27 72.27 47.73 62.75
+SameDetNom 68.79 72.57 48.30 63.22
+SameProperName 69.11 72.81 48.56 63.49
+ModifierMismatch 69.11 72.81 48.58 63.50
+PropertyMismatch 69.20 72.89 48.67 63.59(i.e. CL3M)
-SameSpan 68.91 72.66 48.36 63.31
-SameDetNom 68.62 72.51 48.06 63.06
-SameProperName 68.97 72.69 48.50 63.39
-ModifierMismatch 69.12 72.80 48.63 63.52
-PropertyMismatch 69.11 72.81 48.58 63.50
Table 4: Ablation study on constraints. We first show
cumulative performance on OntoNotes-5.0 data with pre-
dicted mentions as constraints are added one at a time into
the coreference system. Then we demonstrate the value
of individual constraints by leaving out one constraint at
each time.
recall of L3M are 78.38% and 67.96%, respectively
in B3). As a result, the must-link constraints, which
aim at improving the recall, do better when optimiz-
ing F1.
6 Conclusions
We presented a principled yet simple framework for
coreference resolution. Furthermore, we showed
that our model can be augmented in a straightfor-
ward way with knowledge based constraints, to im-
prove performance. We also presented a probabilis-
tic generalization of this model that can take into
account entity-mention links by considering mul-
tiple possible coreference links. We proposed a
fast stochastic gradient-based learning technique for
our model. Our model, while operating at men-
tion pair granularity, obtains state-of-the-art results
on OntoNotes-5.0, and performs especially well on
mention clusters containing named entities. We pro-
vided a detailed analysis of our experimental results.
Acknowledgments Supported by the Intelligence Advanced
Research Projects Activity (IARPA) via Department of Interior National
Business Center contract number D11PC20155. The U.S. Government
is authorized to reproduce and distribute reprints for Governmental pur-
poses notwithstanding any copyright annotation thereon. Disclaimer:
The views and conclusions contained herein are those of the authors and
should not be interpreted as necessarily representing the official policies
or endorsements, either expressed or implied, of IARPA, DoI/NBC, or
the U.S. Government.
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In In The First International Con-
ference on Language Resources and Evaluation Work-
shop on Linguistics Coreference.
M. Bansal and D. Klein. 2012. Coreference semantics
from web features. In Proceedings of ACL, Jeju Island,
South Korea, July.
N. Bansal, A. Blum, and S. Chawla. 2002. Correlation
clustering. In Proceedings of the 43rd Symposium on
Foundations of Computer Science.
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP, 10.
A. Bjo?rkelund and R. Farkas.
K.-W. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference protocols
for coreference resolution. In CoNLL Shared Task.
K.-W. Chang, R. Samdani, A. Rozovskaya, M. Sammons,
and D. Roth. 2012a. Illinois-coref: The UI system
in the CoNLL-2012 Shared Task. In CoNLL Shared
Task.
M. Chang, L. Ratinov, and D. Roth. 2012b. Structured
learning with constrained conditional models. Ma-
chine Learning, 88(3):399?431, 6.
C. Chen and V. Ng. 2012. Combining the best of two
worlds: A hybrid approach to multilingual corefer-
ence resolution. In Joint Conference on EMNLP and
CoNLL - Shared Task.
J. Clarke and M. Lapata. 2006. Constraint-based
sentence compression: An integer programming ap-
proach. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics (ACL),
pages 144?151, Sydney, Australia, July. ACL.
A. Culotta, M. Wick, R. Hall, and A. McCallum. 2007.
First-order probabilistic models for coreference reso-
lution. In HLT/NAACL.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of the Annual Meeting
of the North American Association of Computational
Linguistics (NAACL).
P. Denis and J. Baldridge. 2008. Specialized models and
ranking for coreference resolution. In EMNLP, pages
660?669.
G. Durrett, D. Hall, and D. Klein. 2013. Decentral-
ized entity-level modeling for coreference resolution.
In Proceedings of ACL, August.
610
E. R. Fernandes, C. N. dos Santos, and R. L. Milidiu?.
2012. Latent structure perceptron with feature induc-
tion for unrestricted coreference resolution. In Joint
Conference on EMNLP and CoNLL - Shared Task.
T. Finley and T. Joachims. 2005. Supervised cluster-
ing with support vector machines. In Proceedings
of the International Conference on Machine Learning
(ICML).
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior regularization for structured latent
variable models. Journal of Machine Learning Re-
search.
K. Gimpel and N. A. Smith. 2010. Softmax-margin
CRFs: Training log-linear models with cost functions.
In NAACL.
A. Haghighi and D. Klein. 2010. Coreference resolution
in a modular, entity-centered model. In NAACL.
H. Ji and R. Grishman. 2011. Knowledge base popula-
tion: successful approaches and challenges. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies - Volume 1.
T. Koo, A. M. Rush, M. Collins, T. Jaakkola, and D. Son-
tag. 2010. Dual decomposition for parsing with non-
projective head automata. In EMNLP.
H. Lee, Y. Peirsman, A. Chang, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2011. Stanford?s multi-
pass sieve coreference resolution system at the conll-
2011 shared task. In Proceedings of the CoNLL-2011
Shared Task.
H. Lee, A. Chang, Y. Peirsman, N. Chambers, M. Sur-
deanu, and D. Jurafsky. 2013. Deterministic coref-
erence resolution based on entity-centric, precision-
ranked rules. Computational Linguistics, 39(4).
X. Luo. 2005. On coreference resolution performance
metrics. In EMNLP.
S. Martschat, J. Cai, S. Broscheit, ?E. Mu?jdricza-Maydt,
and M. Strube. 2012. A multigraph model for corefer-
ence resolution. In Joint Conference on EMNLP and
CoNLL - Shared Task, July.
A. McCallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS).
R. Mihalcea and A. Csomai. 2007. Wikify!: linking doc-
uments to encyclopedic knowledge. In CIKM.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In ACL.
Vincent Ng. 2005. Supervised ranking for pronoun res-
olution: Some recent improvements. In AAAI, pages
1081?1086.
NIST. 2004. The ACE evaluation plan.
D. Pascal and J. Baldridge. 2009. Global joint models for
coreference resolution and named entity classification.
In Procesamiento del Lenguaje Natural.
P. Pletscher, C. S. Ong, and J. M. Buhmann. 2010. En-
tropy and margin maximization for structured output
learning. In ECML PKDD.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. Conll-2011 shared
task: Modeling unrestricted coreference in ontonotes.
In CoNLL.
S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and
Y. Zhang. 2012. CoNLL-2012 shared task: Modeling
multilingual unrestricted coreference in OntoNotes. In
CoNLL 2012.
M. Sammons Y. Tu V. Vydiswaran Q. Do, D. Roth. 2009.
Robust, light-weight approaches to compute lexical
similarity. Technical report.
K. Raghunathan, H. Lee, S. Rangarajan, N. Chambers,
M. Surdeanu, D. Jurafsky, and C. Manning. 2010.
A multi-pass sieve for coreference resolution. In
EMNLP.
A. Rahman and V. Ng. 2011a. Coreference resolution
with world knowledge. In ACL, pages 814?824.
A. Rahman and V. Ng. 2011b. Ensemble-based corefer-
ence resolution. In IJCAI.
A. Rahman and V. Ng. 2011c. Narrowing the modeling
gap: a cluster-ranking approach to coreference resolu-
tion. JAIR.
W.M. Rand. 1971. Objective criteria for the evaluation
of clustering methods. Journal of the American Statis-
tical Association, 66(336):846?850.
L. Ratinov and D. Roth. 2012. Learning-based multi-
sieve co-reference resolution with knowledge. In
EMNLP.
L. Ratinov, D. Roth, D. Downey, and M. Anderson.
2011. Local and global algorithms for disambiguation
to wikipedia. In Proc. of the Annual Meeting of the
Association for Computational Linguistics (ACL).
Dan Roth and Wen Tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. In Proceedings of CoNLL-04, pages 1?8.
R. Samdani, M. Chang, and D. Roth. 2012. Unified ex-
pectation maximization. In NAACL.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Comput. Linguist.
V. Stoyanov and J. Eisner. 2012. Easy-first coreference
resolution. In COLING, pages 2519?2534.
V. Stoyanov, N. Gilbert, C. Cardie, and E. Riloff. 2009.
Conundrums in noun phrase coreference resolution:
making sense of the state-of-the-art. In ACL.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
611
scoring scheme. In Proceedings of the 6th conference
on Message understanding.
C. Yu and T. Joachims. 2009. Learning structural svms
with latent variables. In Proceedings of the Interna-
tional Conference on Machine Learning (ICML).
A. L. Yuille and A. Rangarajan. 2003. The concave-
convex procedure. Neural Computation, 15(4).
612
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1602?1612,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Multi-Relational Latent Semantic Analysis
Kai-Wei Chang?
University of Illinois
Urbana, IL 61801, USA
kchang10@illinois.edu
Wen-tau Yih Christopher Meek
Microsoft Research
Redmond, WA 98052, USA
{scottyih,meek}@microsoft.com
Abstract
We present Multi-Relational Latent Seman-
tic Analysis (MRLSA) which generalizes La-
tent Semantic Analysis (LSA). MRLSA pro-
vides an elegant approach to combining mul-
tiple relations between words by construct-
ing a 3-way tensor. Similar to LSA, a low-
rank approximation of the tensor is derived
using a tensor decomposition. Each word in
the vocabulary is thus represented by a vec-
tor in the latent semantic space and each re-
lation is captured by a latent square matrix.
The degree of two words having a specific
relation can then be measured through sim-
ple linear algebraic operations. We demon-
strate that by integrating multiple relations
from both homogeneous and heterogeneous
information sources, MRLSA achieves state-
of-the-art performance on existing benchmark
datasets for two relations, antonymy and is-a.
1 Introduction
Continuous semantic space representations have
proven successful in a wide variety of NLP and IR
applications, such as document clustering (Xu et al,
2003) and cross-lingual document retrieval (Dumais
et al, 1997; Platt et al, 2010) at the document level
and sentential semantics (Guo and Diab, 2012; Guo
and Diab, 2013) and syntactic parsing (Socher et
al., 2013) at the sentence level. Such representa-
tions also play an important role in applications for
lexical semantics, such as word sense disambigua-
tion (Boyd-Graber et al, 2007), measuring word
?Work conducted while interning at Microsoft Research.
similarity (Deerwester et al, 1990) and relational
similarity (Turney, 2006; Zhila et al, 2013; Mikolov
et al, 2013). In many of these applications, La-
tent Semantic Analysis (LSA) (Deerwester et al,
1990) has been widely used, serving as a fundamen-
tal component or as a strong baseline.
LSA operates by mapping text objects, typically
documents and words, to a latent semantic space.
The proximity of the vectors in this space implies
that the original text objects are semantically re-
lated. However, one well-known limitation of LSA
is that it is unable to differentiate fine-grained re-
lations. For instance, when applied to lexical se-
mantics, synonyms and antonyms may both be as-
signed high similarity scores (Landauer and Laham,
1998; Landauer, 2002). Asymmetric relations like
hyponyms and hypernyms also cannot be differenti-
ated. Although there exists some recent work, such
as PILSA which tries to overcome this weakness
of LSA by introducing the notion of polarity (Yih
et al, 2012). This extension, however, can only
handle two opposing relations (e.g., synonyms and
antonyms), leaving open the challenge of encoding
multiple relations.
In this paper, we propose Multi-Relational Latent
Semantic Analysis (MRLSA), which strictly gener-
alizes LSA to incorporate information of multiple
relations concurrently. Similar to LSA or PILSA
when applied to lexical semantics, each word is still
mapped to a vector in the latent space. However,
when measuring whether two words have a specific
relation (e.g., antonymy or is-a), the word vectors
will be mapped to a new space according to the rela-
tion where the degree of having this relation will be
1602
judged by cosine similarity. The raw data construc-
tion in MRLSA is straightforward and similar to the
document-term matrix in LSA. However, instead of
using one matrix to capture all relations, we extend
the representation to a 3-way tensor. Each slice cor-
responds to the document-term matrix in the original
LSA design but for a specific relation. Analogous to
LSA, the whole linear transformation mapping is de-
rived through tensor decomposition, which provides
a low-rank approximation of the original tensor. As
a result, previously unseen relations between two
words can be discovered, and the information en-
coded in other relations can influence the construc-
tion of the latent representations, and thus poten-
tially improves the overall quality. In addition, the
information in different slices can come from het-
erogeneous sources (conceptually similar to (Riedel
et al, 2013)), which not only improves the model,
but also extends the word coverage in a reliable way.
We provide empirical evidence that MRLSA is ef-
fective using two different word relations: antonymy
and is-a. We use the benchmark GRE test of closest-
opposites (Mohammad et al, 2008) to show that
MRLSA performs comparably to PILSA, which was
the pervious state-of-the-art approach on this prob-
lem, when given the same amount of information. In
addition, when other words and relations are avail-
able, potentially from additional resources, MRLSA
is able to outperform previous methods significantly.
We use the is-a relation to demonstrate that MRLSA
is capable of handling asymmetric relations. We
take the list of word pairs from the Class-Inclusion
(i.e., is-a) relations in SemEval-2012 Task 2 (Jur-
gens et al, 2012), and use our model to measure the
degree of two words have this relation. The mea-
sures derived from our model correlate with human
judgement better than the best system that partici-
pated in the task.
The rest of this paper is organized as follows. We
first survey some related work in Section 2, followed
by a more detailed description of LSA and PILSA
in Section 3. Our proposed model, MRLSA, is pre-
sented in Section 4. Section 5 presents our experi-
mental results. Finally, Section 6 concludes the pa-
per.
2 Related Work
MRLSA can be viewed as a model that derives gen-
eral continuous space representations for capturing
lexical semantics, with the help of tensor decompo-
sition techniques. We highlight some recent work
related to our approach.
The most commonly used continuous space rep-
resentation of text is arguably the vector space
model (VSM) (Turney and Pantel, 2010). In this
representation, each text object can be represented
by a high-dimensional sparse vector, such as a
term-vector or a document-vector that denotes the
statistics of term occurrences (Salton et al, 1975)
in a large corpus. The text can also be repre-
sented by a low-dimensional dense vector derived
by linear projection models like latent semantic
analysis (LSA) (Deerwester et al, 1990), by dis-
criminative learning methods like Siamese neural
networks (Yih et al, 2011), recurrent neural net-
works (Mikolov et al, 2013) and recursive neu-
ral networks (Socher et al, 2011), or by graphical
models such as probabilistic latent semantic anal-
ysis (PLSA) (Hofmann, 1999) and latent Dirichlet
allocation (LDA) (Blei et al, 2003). As a general-
ization of LSA, MRLSA is also a linear projection
model. However, while the words are represented
by vectors as well, multiple relations between words
are captured separately by matrices.
In the context of lexical semantics, VSMs provide
a natural way of measuring semantic word related-
ness by computing the distance between the cor-
responding vectors, which has been a standard ap-
proach (Agirre et al, 2009; Reisinger and Mooney,
2010; Yih and Qazvinian, 2012). These approaches
do not apply directly to the problem of modeling
other types of relations. Existing methods that do
handle multiple relations often use a model com-
bination scheme to integrate signals from various
types of information sources. For instance, mor-
phological variations discovered from the Google
n-gram corpus have been combined with informa-
tion from thesauri and vector-based word related-
ness models for detecting antonyms (Mohammad et
al., 2008). An alternative approach proposed by Tur-
ney (2008) that handles synonyms, antonyms and
associations is to use a uniform approach by first
reducing the problem to determining whether two
1603
pairs of words can be analogous, and then predicting
it using a supervised model with features based on
the frequencies of patterns in the corpus. Similarly,
to measure whether two word pairs have the same
relation, Zhila et al (2013) proposed to combine het-
erogeneous models, which achieved state-of-the-art
performance. In comparison, MRLSA models mul-
tiple lexical relations holistically. The degree that
two words having a particular relation is estimated
using the same linear function of the corresponding
vectors and matrix.
Tensor decomposition generalizes matrix factor-
ization and has been applied to several NLP applica-
tions recently. For example, Cohen et al (2013) pro-
posed an approximation algorithm for PCFG pars-
ing that relies on Kruskal decomposition. Van de
Cruys et al (2013) modeled the composition of
subject-verb-object triples using Tucker decompo-
sition, which results in a better similarity measure
for transitive phrases. Similar to this construction
but used in the community-based question answer-
ing (CQA) scenario, Qiu et al (2013) represented
triples of question title, question content and answer
as a tensor and applied 3-mode SVD to derive latent
semantic representations for question matching. The
construction of MRLSA bears some resemblance to
the work that use tensors to capture triples. How-
ever, our goal of modeling different relations for lex-
ical semantics is very different from the intended us-
age of tensor decomposition in the existing work.
3 Latent Semantic Analysis
Latent Semantic Analysis (LSA) (Deerwester et al,
1990) is a widely used continuous vector space
model that maps words and documents into a low
dimensional space. LSA consists of two main steps.
First, taking a collection of d documents that con-
tains words from a vocabulary list of size n, it first
constructs a d ? n document-term matrix W to en-
code the occurrence information of a word in a docu-
ment. For instance, in its simplest form, the element
Wi,j can be the term frequency of the j-th word in
the i-th document. In practice, a weighting scheme
that better captures the importance of a word in the
document, such as TF?IDF (Salton et al, 1975),
is often used instead. Notice that ?document? here
simply means a group of words and has been applied
W V X = U T
Figure 1: SVD applied to a d?n document-term ma-
trix W. The rank-k approximation, X, is the mul-
tiplication of U, ? and VT , where U and V are
d ? k and n ? k orthonormal matrices and ? is a
k ? k diagonal matrix. The column vectors of VT
multiplied by the singular values ? represent words
in the latent semantic space.
to various texts including news articles, sentences
and bags of words. Once the matrix is constructed,
the second step is to apply singular value decom-
position (SVD) to W in order to derive a low-rank
approximation. To have a rank-k approximation, X
is the reconstruction matrix of W, defined as
W ? X = U?VT (1)
where the dimensions of U and V are d? k and
n? k, respectively, and ? is a k ? k diagonal ma-
trix. In addition, the columns in U and V are or-
thonormal and the elements in ? are the singular
values and are conventionally reverse-ordered. Fig-
ure 1 illustrates this decomposition.
LSA can be used to compute the similarity be-
tween two documents or two words in the latent
space. For instance, to compare the u-th and v-th
words in the vocabulary, one can compute the co-
sine similarity of the u-th and v-th column vectors
of X, the reconstruction matrix of W. In contrast to
a direct lexical matching via the columns of W, the
similarity measure computed as a result of the SVD
may have a nonzero similarity score even if these
two words do not co-occur in any documents. This
is due to the fact that those words can share some
latent components.
An alternative view of using LSA is to treat the
column vectors of ?VT as a representation of the
words in a new k-dimensional latent space. This
comes from the observation that the inner product
of every two column vectors in X is the inner prod-
uct of the corresponding column vectors of ?VT ,
1604
joyfulness
gladden
sad
1
anger
1
-1
0
1
1
0
0
-1
0
1
0
0
-1
1
0
0
0
0
1
0
0
0
0
0
0
0
0
Figure 2: The matrix construction of PILSA. The
vocabulary is {joy, gladden, sorrow, sadden, anger,
emotion, feeling} and target words are {joyfulness,
gladden, sad, anger}. For ease of presentation,
we show the numbers with 0-1 values instead of
TF?IDF scores. The polarity (i.e., sign) indicates
whether the term in the vocabulary is a synonym or
antonym of the target word.
which can be derived from the equations below.
XTX = (U?VT )T (U?VT )
= V?UTU?VT (? is diagonal)
= V?2VT (Columns of U are orthonormal)
= (?VT )T (?VT ) (2)
Thus, the semantic relatedness between the i-th and
j-th words can be computed by cosine similarity1:
cos(X:,i,X:,j) (3)
When used to compare words, one well-known
limitation of LSA is that the score captures the gen-
eral notion of semantic similarity, and is unable
to distinguish fine-grained word relations, such as
antonyms (Landauer and Laham, 1998; Landauer,
2002). This is due to the fact that the raw matrix rep-
resentation only records the occurrences of words in
documents without knowing the specific relation be-
tween the word and document. To address this issue,
Yih et al (2012) proposed a polarity inducing latent
semantic analysis model recently, which we intro-
duce next.
1Cosine similarity is equivalent to the inner product of the
normalized vectors.
3.1 Polarity Inducing Latent Semantic
Analysis
In order to distinguish antonyms from synonyms,
the polarity inducing LSA (PILSA) model (Yih et
al., 2012) takes a thesaurus as input. Synonyms and
antonyms of the same target word are grouped to-
gether as a ?document? and a document-term matrix
is constructed accordingly as done in LSA. Because
each word in a group belongs to either one of the two
opposite relations, synonymy and antonymy, the po-
larity information is induced by flipping the signs of
antonyms. While the absolute value of each element
in the matrix is still the same TF?IDF score, the
elements that correspond to the antonyms become
negative.
This design has an intriguing effect. When com-
paring two words using the cosine similarity (or sim-
ply inner product) of their corresponding column
vectors in the matrix, the score of a synonym pair
remains positive, but the score of an antonym pair
becomes negative. Figure 2 illustrates this design
using a simplified matrix as example.
Once the matrix is constructed, PILSA applies
SVD as done in LSA, which generalizes the model
to go beyond lexical matching. The sign of the co-
sine score of the column vectors of any two words
indicates whether they are close to synonyms or to
antonyms and the absolute value reflects the degree
of the relation. When all the column vectors are nor-
malized to unit vectors, it can also be viewed as syn-
onyms are clustered together and antonyms lie on
the opposite sides of a unit sphere. Although PILSA
successfully extends LSA to handle not just one sin-
gle occurrence relation, the extension is limited to
encoding two opposing relations
4 Multi-Relational Latent Semantic
Analysis
The fundamental reason why it is difficult to handle
multiple relations is due to the 2-dimensional ma-
trix representation. In order to overcome this, we
encode the raw data in a 3-way tensor. Each slice
captures a particular relation and is in the format of
the document-term matrix in LSA. Just as in LSA,
where the low-rank approximation by SVD helps
generalize the representation and discover unseen
relations, we apply a tensor decomposition method,
1605
joyfulness
gladden
sad
1
anger
1
0
0
1
1
0
0
0
0
1
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
(a) Synonym layer
joyfulness
gladden
sad
0
anger
0
1
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
(b) Antonym layer
joyfulness
gladden
sad
0
anger
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
1
1
1
0
1
1
(c) Hypernym layer
Figure 3: The three slices of MRLSA raw tensorW for an example with vocabulary {joy, gladden, sorrow,
sadden, anger, emotion, feeling} and target words {joyfulness, gladden, sad, anger}. Figures 3(a), 3(b), 3(c)
show the matrices W:,:,syn, W:,:,ant, W:,:,hyper, respectively. Rows represent documents (see definition in
text), and columns represent words. For ease of presentation, we show numbers with 0-1 values instead of
TF?IDF scores.
the Tucker decomposition, to the tensor.
4.1 Representing Multi-Relational Data in
Tensors
A tensor is simply a multi-dimensional array. In this
work, we use a 3-way tensor W to encode multi-
ple word relations. An element of W is denoted
by Wi,j,k using its indices, and W:,:,k represents
the k-th slice of W (a slice of a 3-way tensor is
a matrix, obtained by fixing the third index). Fol-
lowing (Kolda and Bader, 2009), a fiber of a ten-
sor W:,j,k is a vector, which is a high order analog
of a matrix row or column.
When constructing the raw tensorW in MRLSA,
each slice is analogous to the document-term ma-
trix in LSA, but created based on the data of a par-
ticular relation, such as synonyms. With a slight
abuse of notation, we sometimes use the value rather
than index when there is no confusion. For in-
stance, W:,?word?,k represents the fiber correspond-
ing to the ?word? in slice k, and W:,:,syn refers to
the slice that encodes the synonymy relation. Below
we use an example to compare this construction to
the raw matrix in PILSA, and discuss how it extends
LSA.
Suppose we are interested in representing two re-
lations, synonymy and antonymy. The raw tensor in
MRLSA would then consist of two slices, W:,:,syn
and W:,:,ant, to encode synonyms and antonyms of
target words from a knowledge source (e.g., a the-
saurus). Each row in W:,:,syn represents the syn-
onyms of a target word, and the corresponding
row in W:,:,ant encodes its antonyms. Figures 3(a)
and 3(b) illustrate an example, where ?joy?, ?glad-
den? are synonyms of the target word ?joyfulness?
and ?sorrow? is its antonym. Therefore, the values
of the corresponding entries are 1. Notice that the
matrix W? = W:,:,syn ? W:,:,ant is identical to the
PILSA raw matrix. We can extend the construction
above to enable MRLSA to utilize other semantic
relations (e.g., hypernymy) by adding a slice cor-
responding to each relation of interest. Fig. 3(c)
demonstrates how to add another slice W:,:,hyper to
the tensor for encoding hypernyms.
4.2 Tensor Decomposition
The MRLSA raw tensor encodes relations in one or
more data resources, such as thesauri. However, the
knowledge from a thesaurus is usually noisy and in-
complete. In this section, we derive a low-rank ap-
proximation of the tensor to generalize the knowl-
edge. This step is analogous to the rank-k approxi-
mation in LSA.
Various tensor decomposition methods have been
proposed in literature. Among them, Tucker decom-
position (Tucker, 1966) is recognized as a multi-
dimensional extension of SVD and has been widely
used in many applications. An illustration of this
method is in Fig. 4(a). In Tucker decomposition,
a d? n?m tensor W is decomposed into four
components G,U,V,T. A low-rank approximation
1606
X U VG
T
T=W  
(a) Tucker Tensor Decomposition
X U VS: , : , 1 T=
S
(b) Our Reformulation
Figure 4: Fig. 4(a) illustrates the Tucker tensor decomposition method which factors a 3-way tensorW to
three orthogonal matrices, U,V,T, and a core tensor G. We further apply a n-mode matrix product on the
core tensor G with T. Consequently, each slice of the resulted core tensor S (a square matrix) captures a
semantic relation type, and each column of VT is a vector representing a word.
X ofW is defined by
Wi,j,k ? Xi,j,k
=
R1?
r1=1
R2?
r2=1
R3?
r3=1
Gr1,r2,r3Ui,r1Vj,r2Tk,r3 ,
where G is a core tensor with dimensionsR1?R2?
R3 and U,V,T are orthogonal matrices with di-
mensions d ? R1, n ? R2,m ? R3, respectively.
The rank parameters R1 ? d,R2 ? n,R3 ? m are
given as input to the algorithm. In MRLSA, m (the
number of relations) is usually small, while d and n
are typically large (often in the scale of hundreds
of thousands). Therefore, we choose R1 = R2 = ? ,
?  d, n andR3 = m, where ? is typically less than
1000.
To make the analogy to SVD clear, we rewrite the
results of Tucker decomposition by performing a n-
mode matrix product over the core tensor G with the
matrix T. This produces a tensor S where each slice
is a linear combination of the slices of G with coeffi-
cients given by T (see (Kolda and Bader, 2009) for
detail). That is, we have
S:,:,k =
m?
t=1
Tt,kG:,:,t, ?k.
An illustration is shown in Fig. 4(b), Then, a
straightforward calculation shows that k-th slice of
tensorW is approximated by
W:,:,k ? X:,:,k = US:,:,kV
T . (4)
Comparing Eq. (4) to Eq. (1), one can observe
that matrices U and V play similar roles here, and
each slice of the core tensor S is analogous to ?.
However, the square matrix G:,:,k is not necessary
to be diagonal. As in SVD, the column vectors
of G:,:,kVT (capture both word and relation infor-
mation) behave similarly to the column vectors of
the original tensor sliceW:,:,k.
4.3 Measuring the Degrees of Word Relations
In principle, the raw information in the input ten-
sor W can be used for computing lexical similarity
using the cosine score between the column vectors
for two words from the same slice of the tensor. To
measure the degree of other relations, however, our
approach requires one to specify a pivot slice. The
key role of the pivot slice is to expand the lexical
coverage of the relation of interest to additional lexi-
cal entries and, for this reason, the pivot slice should
be chosen to capture the equivalence of the lexical
entries. In this paper, we use the synonymy relation
as our pivot slice. First we consider measuring the
degree of a relation rel holding between the i-th and
j-th words using the raw tensor W , which can be
computed as
cos
(
W:,i,syn,W:,j,rel
)
. (5)
This measurement can be motivated from the logical
rule: syn(wordi, target) ? rel(target,wordj) ?
rel(wordi,wordj), where the pivot relation syn ex-
pands the coverage of the relation of interest rel.
Turning to the use of the tensor decomposition,
we use a similar derivation to Eq. (3), and measure
the degree of relation rel between two words by
cos
(
S:,:,synVTi,:,S:,:,relV
T
j,:
)
. (6)
1607
For instance, the degree of antonymy between
?joy? and ?sorrow? is measured by the co-
sine similarity between the respective fibers
cos(X:,?joy?,syn,X:,?sorrow?,ant). We can encode both
symmetric relations (e.g., antonymy and synonymy)
and asymmetric relations (e.g., hypernymy and
hyponymy) in the same tensor representation. For a
symmetric relation, we use both cos(X:,i,syn,X:,j,rel)
and cos(X:,j,syn,X:,i,rel) and measure the degree of
a symmetric relation by the average of these two
cosine similarity scores. However, for asymmetric
relations, we use only cos(X:,i,syn,X:,j,rel).
5 Experiments
We evaluate MRLSA on two tasks: answering the
closest-opposite GRE questions and measuring de-
grees of various class-inclusion (i.e., is-a) relations.
In both tasks, we design the experiments to empir-
ically validate the following claims. When encod-
ing two opposite relations from the same source,
MRLSA performs comparably to PILSA. However,
MRLSA generalizes LSA to model multiple rela-
tions, which could be obtained from both homoge-
neous and heterogeneous data sources. As a result,
the performance of a target task can be further im-
proved.
5.1 Experimental Setup
We construct the raw tensors to encode a particular
relation in each slice based on two data sources.
Encarta The Encarta thesaurus is developed by
Bloomsbury Publishing Plc2. For each target word,
it provides a list of synonyms and antonyms. We
use the same version of the thesaurus as in (Yih et
al., 2012), which contains about 47k words and a
vocabulary list of approximately 50k words.
WordNet We use four types of relations from
WordNet: synonymy, antonymy, hypernymy and
hyponymy. The number of target words and the
size of the vocabulary in our version are 117,791
and 149,400, respectively. WordNet has better vo-
cabulary coverage, but fewer antonym pairs. For
instance, the WordNet antonym slice contains only
46,945 nonzero entries, while the Encarta antonym
slice has 129,733.
2http://www.bloomsbury.com
We apply a memory-efficient Tucker decomposi-
tion algorithm (Kolda and Sun, 2008) implemented
in tensor toolbox v2.5 (Bader et al, 2012)3 to factor
the tensor. The largest tensor considered in this pa-
per can be decomposed in about 3 hours using less
than 4GB of memory with a commodity PC.
5.2 Answering GRE Antonym Questions
The first task is to answer the closest-opposite ques-
tions from the GRE test provided by Mohammad et
al. (2008)4. Each question in this test consists of
a target word and five candidate words, where the
goal is to pick the candidate word that has the most
opposite meaning to the target word. In order to
have a fair comparison, we use the same data split
as in (Mohammad et al, 2008), with 162 questions
used for the development set and 950 for test. Fol-
lowing (Mohammad et al, 2008; Yih et al, 2012),
we report the results in precision (accuracy of the
questions that the system attempts to answer), re-
call (percentage of the questions answered correctly
over all questions) and F1 (the harmonic mean of
precision and recall).
We tune two sets of parameters using the devel-
opment set: (1) the rank parameter ? in the tensor
decomposition and (2) the scaling factors of differ-
ent slices of the tensor. The rank parameter spec-
ifies the number of dimensions of the latent space.
In the experiments, We pick the best value of ? from
{100, 200, 300, 500, 750, 1000}. The scaling factors
adjust the values of each slice of the tensor. The el-
ements of each slice are multiplied by the scaling
factor before factorization. This is important be-
cause Tucker decomposition minimizes the recon-
struction error (the Frobenius norm of the residual
tensor). As a result, the slice with a larger range of
values becomes more influential to U and V. In this
work, we fixW:,:,ant, and search for the scaling fac-
tor of W:,:,syn in {0.25, 0.5, 1, 2, 4} and the factors
ofW:,:,hyper andW:,:,hypo in {0.0625, 0.125, 0.25}.
Table 1 summarizes the results of training
3http://www.sandia.gov/?tgkolda/
TensorToolbox. The Tucker decomposition involves
performing SVD on a large matrix. We modify the MATLAB
code of tensor toolbox to use the built-in svd function instead
of svds. This modification reduces both the running time and
memory usage.
4http://www.saifmohammad.com
1608
Dev. Set Test Set
Prec. Rec. F1 Prec. Rec. F1
WordNet Lookup 0.40 0.40 0.40 0.42 0.41 0.42
WordNet RawTensor 0.42 0.41 0.42 0.42 0.41 0.42
WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60
WordNet MRLSA:Syn+Ant 0.63 0.62 0.62 0.59 0.58 0.59
WordNet MRLSA:4-layers 0.66 0.65 0.65 0.61 0.59 0.60
Encarta Lookup 0.65 0.61 0.63 0.61 0.56 0.59
Encarta RawTensor 0.67 0.64 0.65 0.62 0.57 0.59
Encarta PILSA 0.86 0.81 0.84 0.81 0.74 0.77
Encarta MRLSA:Syn+Ant 0.87 0.82 0.84 0.82 0.74 0.78
MRLSA:WordNet+Encarta 0.88 0.85 0.87 0.81 0.77 0.79
Table 1: GRE antonym test results of models based on Encarta and WordNet data in precision, recall and F1.
RawTensor evaluates the performance of the tensor with 2 slices encoding synonyms and antonyms be-
fore decomposition (see Eq. (5)), which is comparable to checking the original data directly (Lookup).
MRLSA:Syn+Ant applies Tucker decomposition to the raw tensor and measures the degree of antonymy
using Eq. (6). The result is similar to that of PILSA (see Sec. 3.1). MRLSA:4-layers adds hypernyms and
hyponyms from WordNet; MRLSA:WordNet+Encarta consists of synonyms/antonyms from Encarta and hy-
pernyms/hyponyms from WordNet, where the target words are aligned using the synonymy relations. Both
models demonstrate the advantage of encoding more relations, from either the same or different resources.
MRLSA using two different corpora, Encarta and
WordNet. The performance of the MRLSA raw
tensor is close to that of looking up the thesaurus.
This indicates the tensor representation is able to
capture the word relations explicitly described in
the thesaurus. After conducting tensor decomposi-
tion, MRLSA:Syn+Ant achieves similar results to
PILSA. This confirms our claim that when giv-
ing the same among of information, MRLSA per-
forms at least comparably to PILSA. However, the
true power of MRLSA is its ability to incorpo-
rate other semantic relations to boost the perfor-
mance of the target task. For example, when
we add the hypernymy and hyponymy relations to
the tensor, these class-inclusion relations provide a
weak signal to help resolve antonymy. We sus-
pect that this is due to the fact that antonyms typ-
ically share the same properties but only have the
opposite meaning on one particular semantic di-
mension. For instance, the antonyms ?sadness?
and ?happiness? are different forms of emotion.
When two words are hyponyms of a target word,
the likelihood that they are antonyms should thus
be increased. We show that the target relations
and these auxiliary semantic relations can be col-
lected from the same data source (e.g., WordNet
MRLSA:4-layers) or from multiple, heterogeneous
sources (e.g., MRLSA:WordNET+Encarta). In both
cases, the performance of the model improves as
more relations are incorporated. Moreover, our ex-
periments show that adding the hypernym and hy-
ponym layers from WordNet improves modeling
antonym relations based on the Encarta thesaurus.
This suggests that the weak signal from a resource
with a large vocabulary (e.g., WordNet) can help
predict relations between out-of-vocabulary words
and thus improve the recall.
To better understand the model, we examine the
top antonyms for three question words from the
GRE test. The lists below show antonyms and their
MRLSA scores for each of the GRE question words
as determined by the MRLSA:WordNET+Encarta
model. Antonyms that can be found directly in the
Encarta thesaurus are in italics.
inanimate alive (0.91), living (0.90), bodily (0.90), in-
the-flesh (0.89), incarnate (0.89)
alleviate exacerbate (0.68), make-worse (0.67), in-
flame (0.66), amplify (0.65), stir-up (0.64)
relish detest (0.33), abhor (0.33), abominate (0.33), de-
spise (0.33), loathe (0.31)
We can see that from these examples, MRLSA not
1609
Dev. Test
1a (Taxonomic) 1b (Functional) 1c (Singular) 1d (Plural) Avg.
WordNet Lookup 52.9 34.5 41.4 34.3 36.7
WordNet RawTensor 51.0 38.3 50.0 42.1 43.5
WordNet MRLSA:Syn+Hypony 55.8 41.7 (43.2) 51.0 (51.4) 37.5 (44.4) 43.4 (46.3)
WordNet MRLSA:4-layers 52.9 51.5 (53.9) 51.9 (60.0) 43.5 (50.5) 49.0 (54.8)
MRLSA:WordNet+Encarta 62.1 55.3 (58.7) 57.1 (65.7) 48.6 (53.7) 55.8 (60.1)
UTDNB (Rink and Harabagiu, 2012) - 38.3 36.7 28.2 34.4
Table 2: Results of measuring the class-inclusion (is-a) relations in MaxDiff accuracy (see text for de-
tail). RawTensor has synonym and hyponym slices and measures the degree of is-a relation using Eq. (5).
MRLSA:Syn+Hypo factors the raw tensor and judges the relation by Eq. (6). The constructions of
MRLSA:4-layers and MRLSA:WordNet+Encarta are the same as in Sec. 5.2 (see the caption of Table 1
for detail). For MRLSA models, numbers shown in the parentheses are the results when parameters are
tuned using the test sets. UTDNB is the results of the best performing system in SemEval-2012 Task 2.
only preserves the antonyms in the thesaurus, but
also discovers additional ones, such as exacerbate
and inflame for ?alleviate?. Another interesting find-
ing is that while the scores are useful in ranking
the candidate words, they might not be comparable
across different question words. This could be an
issue for some applications, which need to make a
binary decision on whether two words are antonyms.
5.3 Measuring degrees of Is-A relations
We evaluate MRLSA using the class-inclusion por-
tion of SemEval-2012 Task 2 data (Jurgens et al,
2012). Here the goal is to measure the degree
of two words having the is-a relation. Five an-
notated datasets are provided for different subcate-
gories of this relation: 1a-taxonomic, 1b-functional,
1c-singular, 1d-plural, 1e-class individual. We omit
1e because it focuses on real world entities (e.g.,
queen:Elizabeth, river:Nile), which are not included
in WordNet.
Each dataset contains about 100 questions based
on approximately 40 word pairs. The question con-
sists of 4 randomly chosen word pairs and asks the
best and worst pairs that exemplify the specific is-a
relation. The performance is measured by the av-
erage prediction accuracy, also called the MaxDiff
accuracy (Louviere and Woodworth, 1991).
Because the questions are generated from the
same set of word pairs, these questions are not mutu-
ally independent. Therefore, it is not proper to split
the data of each subcategory into the development
and test sets. Alternatively, we follow the setting
of SemEval-2012 Task 2 and use the first subcat-
egory (1a-taxonomy) to tune the model and eval-
uate its performance based on the results on other
datasets. Since the models are tuned and tested on
different types of subcategories, they might not be
the optimal ones when evaluated on the test sets.
Therefore, we show results using the best parame-
ters tuned on the development set and those tuned on
the test set, where the latter suggests a performance
upper-bound. Besides the rank parameter, we tune
the scaling factors of the synonym, hypernym and
hyponym slices from {4, 16, 64}. The scaling factor
of the antonym slice is fixed to 1.
Table 2 shows the performance in MaxDiff accu-
racy. Results show that even the raw tensor repre-
sentation (RawTensor) performs better than Word-
Net lookup. We suspect that this is because the
tensor representation can capture the fact that the
hyponyms of a word are usually synonymous to
each other. By performing Tucker decomposition
on the raw Tensor, MRLSA achieves better per-
formance. MRLSA:4-layers further leverages the
information from antonyms and hypernyms and
thus improves the model. As we notice in the
GRE antonym test, models based on the Encarta
thesaurus perform better in predicting antonyms.
Therefore, it is interesting to check if combining
synonyms and antonyms from Encarta helps. As
a result, MRLSA:WordNet+Encarta improves over
MRLSA:4-layers significantly. This demonstrates
again that MRLSA can leverage knowledge stored in
heterogeneous resources. Notably, MRLSA outper-
1610
forms the best system participated in the SemEval-
2012 task with a large margin, with a difference of
21.4 in MaxDiff accuracy.
Next we examine the top words that have the is-
a relation relative to three question words from the
task. The lists below show the hyponyms and their
respective MRLSA scores for each of the question
words as determined by MRLSA:4-layers.
bird ostrich (0.75), gamecock (0.75), nighthawk (0.75),
amazon (0.74), parrot (0.74)
automobile minivan (0.48), wagon (0.48), taxi (0.46),
minicab (0.45), gypsy cab (0.45)
vegetable buttercrunch (0.61), yellow turnip (0.61), ro-
maine (0.61), chipotle (0.61), chilli (0.61)
Although the model in general does a good job
finding hyponyms, we observe that some suggested
words, such as buttercrunch (a mild lettuce) vs.
?vegetable?, do not seem intuitive (e.g., compared to
carrot). Having one additional slice to capture the
general term co-occurrence relation may help im-
prove the model in this respect.
6 Conclusions
In this paper, we propose Multi-Relational Latent
Semantic Analysis (MRLSA) which generalizes La-
tent Semantic Analysis (LSA) for lexical seman-
tics. MRLSA models multiple word relations by
leveraging a 3-way tensor, where each slice cap-
tures one particular relation. A low-rank approx-
imation of the tensor is then derived using a ten-
sor decomposition. Consequently, words in the vo-
cabulary are represented by vectors in the latent se-
mantic space, and each relation is captured by a
latent square matrix. Given two words, MRLSA
not only can measure their degree of having a spe-
cific relation, but also can discover unknown rela-
tions between them. These advantages have been
demonstrated in our experiments. By encoding re-
lations from both homogeneous or heterogeneous
data sources, MRLSA achieves state-of-the-art per-
formance on existing benchmark datasets for two re-
lations, antonymy and is-a.
For future work, we plan to explore directions that
aim for improving both the quality and word cover-
age of the model. For instance, the knowledge en-
coded by MRLSA can be enriched by adding more
relations from a variety of linguistic resources, in-
cluding the co-occurrence relations from large cor-
pora. On model refinement, we notice that MRLSA
can be viewed as a 3-layer neural network without
applying the sigmoid function. Following the strat-
egy of using Siamese neural networks to enhance
PILSA (Yih et al, 2012), training MRLSA with a
multi-task discriminative learning setting can be a
promising approach as well.
Acknowledgments
We thank Geoff Zweig for valuable discussions and
the anonymous reviewers for their comments.
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova, M. Pas?ca
and A. Soroa. 2009. A study on similarity and re-
latedness using distributional and WordNet-based ap-
proaches. In NAACL ?09, pages 19?27.
Brett W. Bader, Tamara G. Kolda, et al 2012. Matlab
tensor toolbox version 2.5. Available online, January.
David M. Blei, Andrew Y. Ng, Michael I. Jordan, and
John Lafferty. 2003. Latent dirichlet alocation. Jour-
nal of Machine Learning Research, 3:993?1022.
Jordan L Boyd-Graber, David M Blei, and Xiaojin Zhu.
2007. A topic model for word sense disambiguation.
In EMNLP-CoNLL, pages 1024?1033.
Shay B. Cohen, Giorgio Satta, and Michael Collins.
2013. Approximate PCFG parsing using tensor de-
composition. In NAACL-HLT 2013, pages 487?496.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society for Information
Science, 41(96).
S. Dumais, T. Letsche, M. Littman, and T. Landauer.
1997. Automatic cross-language retrieval using latent
semantic indexing. In AAAI-97 Spring Symposium Se-
ries: Cross-Language Text and Speech Retrieval.
Weiwei Guo and Mona Diab. 2012. Modeling sentences
in the latent space. In ACL 2012, pages 864?872.
Weiwei Guo and Mona Diab. 2013. Improving lexical
semantics for sentential semantics: Modeling selec-
tional preference and similar words in a latent variable
model. In NAACL-HLT 2013, pages 739?745.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proceedings of Uncertainty in Artificial
Intelligence, pages 289?296.
D. Jurgens, S. Mohammad, P. Turney, and K. Holyoak.
2012. SemEval-2012 Task 2: Measuring degrees of
relational similarity. In Proceedings of the Sixth Inter-
national Workshop on Semantic Evaluation (SemEval
2012), pages 356?364.
1611
Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455?500, September.
Tamara G. Kolda and Jimeng Sun. 2008. Scalable ten-
sor decompositions for multi-aspect data mining. In
ICDM 2008, pages 363?372.
T. Landauer and D. Laham. 1998. Learning human-
like knowledge by singular value decomposition: A
progress report. In NIPS 1998.
T. Landauer. 2002. On the computational basis of learn-
ing and cognition: Arguments from lsa. Psychology of
Learning and Motivation, 41:43?84.
Jordan J. Louviere and G. G. Woodworth. 1991. Best-
worst scaling: A model for the largest difference judg-
ments. Technical report, University of Alberta.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013. Linguistic regularities in continuous space word
representations. In NAACL-HLT 2013.
Saif Mohammad, Bonnie Dorr, and Graeme Hirst. 2008.
Computing word pair antonymy. In Empirical Meth-
ods in Natural Language Processing (EMNLP).
John Platt, Kristina Toutanova, and Wen-tau Yih. 2010.
Translingual document representations from discrimi-
native projections. In Proceedings of EMNLP, pages
251?261.
Xipeng Qiu, Le Tian, and Xuanjing Huang. 2013. Latent
semantic tensor indexing for community-based ques-
tion answering. In Proceedings of the 51st Annual
Meeting of the Association for Computational Linguis-
tics (Volume 2: Short Papers), pages 434?439, Sofia,
Bulgaria, August. Association for Computational Lin-
guistics.
Joseph Reisinger and Raymond J. Mooney. 2010. Multi-
prototype vector-space models of word meaning. In
Proceedings of HLT-NAACL, pages 109?117.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL-HLT 2013, pages 74?84.
Bryan Rink and Sanda Harabagiu. 2012. UTD: Deter-
mining relational similarity using lexical patterns. In
Proceedings of the Sixth International Workshop on
Semantic Evaluation (SemEval 2012), pages 413?418,
Montre?al, Canada, 7-8 June. Association for Compu-
tational Linguistics.
G. Salton, A. Wong, and C. S. Yang. 1975. A Vector
Space Model for Automatic Indexing. Communica-
tions of the ACM, 18(11).
Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,
and Christopher D. Manning. 2011. Parsing natural
scenes and natural language with recursive neural net-
works. In ICML ?11.
Richard Socher, John Bauer, Christopher D. Manning,
and Andrew Y. Ng. 2013. Parsing with compositional
vector grammars. In Annual Meeting of the Associa-
tion for Computational Linguistics (ACL).
Ledyard R Tucker. 1966. Some mathematical
notes on three-mode factor analysis. Psychometrika,
31(3):279?311.
Peter D. Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of semantics.
Journal of Artificial Intelligence Research, 37(1):141?
188.
P. D. Turney. 2006. Similarity of semantic relations.
Computational Linguistics, 32(3):379?416.
Peter Turney. 2008. A uniform approach to analo-
gies, synonyms, antonyms, and associations. In In-
ternational Conference on Computational Linguistics
(COLING).
Tim Van de Cruys, Thierry Poibeau, and Anna Korho-
nen. 2013. A tensor-based factorization model of se-
mantic compositionality. In Proceedings of the 2013
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 1142?1151, Atlanta, Geor-
gia, June. Association for Computational Linguistics.
Wei Xu, Xin Liu, and Yihong Gong. 2003. Document
clustering based on non-negative matrix factorization.
In Proceedings of the 26th annual international ACM
SIGIR conference on Research and development in in-
formaion retrieval, pages 267?273, New York, NY,
USA. ACM.
Wen-tau Yih and Vahed Qazvinian. 2012. Measur-
ing word relatedness using heterogeneous vector space
models. In Proceedings of NAACL-HLT, pages 616?
620, Montre?al, Canada, June.
Wen-tau Yih, Kristina Toutanova, John C. Platt, and
Christopher Meek. 2011. Learning discriminative
projections for text similarity measures. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning, pages 247?256, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
Wen-tau Yih, Geoffrey Zweig, and John Platt. 2012. Po-
larity inducing latent semantic analysis. In Proceed-
ings of NAACL-HLT, pages 1212?1222, Jeju Island,
Korea, July.
Alisa Zhila, Wen-tau Yih, Christopher Meek, Geoffrey
Zweig, and Tomas Mikolov. 2013. Combining het-
erogeneous models for measuring relational similar-
ity. In Proceedings of the 2013 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1000?1009, Atlanta, Georgia, June. Asso-
ciation for Computational Linguistics.
1612
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1568?1579,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Typed Tensor Decomposition of Knowledge Bases for Relation Extraction
Kai-Wei Chang
??
Wen-tau Yih
\
Bishan Yang
??
Christopher Meek
\
?
University of Illinois, Urbana, IL 61801, USA
?
Cornell University, Ithaca, NY 14850, USA
\
Microsoft Research, Redmond, WA 98052, USA
Abstract
While relation extraction has traditionally
been viewed as a task relying solely on
textual data, recent work has shown that
by taking as input existing facts in the form
of entity-relation triples from both knowl-
edge bases and textual data, the perfor-
mance of relation extraction can be im-
proved significantly. Following this new
paradigm, we propose a tensor decompo-
sition approach for knowledge base em-
bedding that is highly scalable, and is es-
pecially suitable for relation extraction.
By leveraging relational domain knowl-
edge about entity type information, our
learning algorithm is significantly faster
than previous approaches and is better
able to discover new relations missing
from the database. In addition, when ap-
plied to a relation extraction task, our ap-
proach alone is comparable to several ex-
isting systems, and improves the weighted
mean average precision of a state-of-the-
art method by 10 points when used as a
subcomponent.
1 Introduction
Identifying the relationship between entities from
free text, relation extraction is a key task for ac-
quiring new facts to increase the coverage of a
structured knowledge base. Given a pre-defined
database schema, traditional relation extraction
approaches focus on learning a classifier using tex-
tual data alone, such as patterns between the oc-
currences of two entities in documents, to deter-
mine whether the entities have a particular rela-
tion. Other than using the existing known facts
to label the text corpora in a distant supervision
setting (Bunescu and Mooney, 2007; Mintz et al.,
?
Work conducted while interning at Microsoft Research.
2009; Riedel et al., 2010; Ritter et al., 2013), an
existing knowledge base is typically not involved
in the process of relation extraction.
However, this paradigm has started to shift re-
cently, as researchers showed that by taking exist-
ing facts of a knowledge base as an integral part of
relation extraction, the model can leverage richer
information and thus yields better performance.
For instance, Riedel et al. (2013) borrowed the
idea of collective filtering and constructed a ma-
trix where each row is a pair of entities and each
column is a particular relation. For a true entity-
relation triple (e
1
, r, e
2
), either from the text cor-
pus or from the knowledge base, the correspond-
ing entry in the matrix is 1. A previously unknown
fact (i.e., triple) can be discovered through ma-
trix decomposition. This approach can be viewed
as creating vector representations of each relation
and candidate pair of entities. Because each entity
does not have its own representation, relationships
of any unpaired entities cannot be discovered. Al-
ternatively, Weston et al. (2013) created two types
of embedding ? one based on textual similarity and
the other based on knowledge base, where the lat-
ter maps each entity and relation to the same d-
dimensional vector space using a model proposed
by Bordes et al. (2013a). They also showed that
combining these two models results in a signif-
icant improvement over the model trained using
only textual data.
To make such an integrated strategy work, it is
important to capture all existing entities and rela-
tions, as well as the known facts, from both tex-
tual data and large databases. In this paper, we
propose a new knowledge base embedding model,
TRESCAL, that is highly efficient and scalable,
with relation extraction as our target application.
Our work is built on top of RESCAL (Nickel
et al., 2011), which is a tensor decomposition
method that has proven its scalability by factoring
YAGO (Biega et al., 2013) with 3 million entities
1568
and 41 million triples (Nickel et al., 2012). We
improve the tensor decomposition model with two
technical innovations. First, we exclude the triples
that do not satisfy the relational constraints (e.g.,
both arguments of the relation spouse-of need to
be person entities) from the loss, which is done
by selecting sub-matrices of each slice of the ten-
sor during training. Second, we introduce a math-
ematical technique that significantly reduces the
computational complexity in both time and space
when the loss function contains a regularization
term. As a consequence, our method is more than
four times faster than RESCAL, and is also more
accurate in discovering unseen triples.
Our contributions are twofold. First, compared
to other knowledge base embedding methods de-
veloped more recently, it is much more efficient
to train our model. As will be seen in Sec. 5,
when applied to a large knowledge base created
using NELL (Carlson et al., 2010) that has 1.8M
entity-relation triples, our method finishes training
in 4 to 5 hours, while an alternative method (Bor-
des et al., 2013a) needs almost 3 days. Moreover,
the prediction accuracy of our model is competi-
tive to others, if not higher. Second, to validate its
value to relation extraction, we apply TRESCAL to
extracting relations from a free text corpus along
with a knowledge base, using the data provided
in (Riedel et al., 2013). We show that TRESCAL
is complementary to existing systems and signif-
icantly improves their performance when using it
as a subcomponent. For instance, this strategy im-
proves the weighted mean average precision of the
best approach in (Riedel et al., 2013) by 10 points
(47% to 57%).
The remainder of this paper is organized as fol-
lows. We survey most related work in Sec. 2 and
provide the technical background of our approach
in Sec. 3. Our approach is detailed in Sec. 4, fol-
lowed by the experimental validation in Sec. 5. Fi-
nally, Sec. 6 concludes the paper.
2 Related Work
Our approach of creating knowledge base em-
bedding is based on tensor decomposition, which
is a well-developed mathematical tool for data
analysis. Existing tensor decomposition models
can be categorized into two main families: the
CP and Tucker decompositions. The CP (CAN-
DECOMP/PARAFAC) decomposition (Kruskal,
1977; Kiers, 2000) approximates a tensor by a sum
of rank-one tensors, while the Tucker decompo-
sition (Tucker, 1966), also known as high-order
SVD (De Lathauwer et al., 2000), factorizes a ten-
sor into a core tensor multiplied by a matrix along
each dimension. A highly scalable distributional
algorithm using the Map-Reduce architecture has
been proposed recently for computing CP (Kang et
al., 2012), but not for the Tucker decomposition,
probably due to its inherently more complicated
model form.
Matrix and tensor decomposition methods have
been applied to modeling multi-relational data.
For instance, Speer et al. (2008) aimed to cre-
ate vectors of latent components for representing
concepts in a common sense knowledge base us-
ing SVD. Franz et al. (2009) proposed TripleRank
to model the subject-predicate-object
RDF triples in a tensor, and then applied the CP
decomposition to identify hidden triples. Fol-
lowing the same tensor encoding, Nickel et al.
(2011) proposed RESCAL, a restricted form of
Tucker decomposition for discovering previously
unknown triples in a knowledge base, and later
demonstrated its scalability by applying it to
YAGO, which was encoded in a 3M ? 3M ? 38
tensor with 41M triples (Nickel et al., 2012).
Methods that revise the objective function
based on additional domain information have been
proposed, such as MrWTD, a multi-relational
weighted tensor decomposition method (London
et al., 2013), coupled matrix and tensor fac-
torization (Papalexakis et al., 2014), and col-
lective matrix factorization (Singh and Gordon,
2008). Alternatively, instead of optimizing for the
least-squares reconduction loss, a non-parametric
Bayesian approach for 3-way tensor decomposi-
tion for modeling relational data has also been pro-
posed (Sutskever et al., 2009). Despite the exis-
tence of a wide variety of tensor decomposition
models, most methods do not scale well and have
only been tested on datasets that are much smaller
than the size of real-world knowledge bases.
Multi-relational data can be modeled by neural-
network methods as well. For instance, Bordes et
al. (2013b) proposed the Semantic Matching En-
ergy model (SME), which aims to have the same
d-dimensional vector representations for both en-
tities and relations. Given the vectors of entities
e
1
, e
2
and relation r. They first learn the latent
representations of (e
1
, r) and (e
2
, r). The score
of (e
1
, r, e
2
) is determined by the inner product
1569
of the vectors of (e
1
, r) and (e
2
, r). Later, they
proposed a more scalable method called translat-
ing embeddings (TransE) (Bordes et al., 2013a).
While both entities and relations are still repre-
sented by vectors, the score of (e
1
, r, e
2
) becomes
the negative dissimilarity measure of the corre-
sponding vectors ??e
i
+ r
k
? e
j
?, motivated by
the work in (Mikolov et al., 2013b; Mikolov et al.,
2013a). Alternatively, Socher et al. (2013) pro-
posed a Neural Tensor Network (NTN) that repre-
sents entities in d-dimensional vectors created sep-
arately by averaging pre-trained word vectors, and
then learns a d?d?m tensor describing the inter-
actions between these latent components in each
of the m relations. All these methods optimize
for loss functions that are more directly related to
the true objective ? the prediction accuracy of cor-
rect entity-relation triples, compared to the mean-
squared reconstruction error in our method. Nev-
ertheless, they typically require much longer train-
ing time.
3 Background
In this section, we first describe how entity-
relation triples are encoded in a tensor. We then
introduce the recently proposed tensor decompo-
sition method, RESCAL (Nickel et al., 2011) and
explain how it adopts an alternating least-squares
method, ASALSAN (Bader et al., 2007), to com-
pute the factorization.
3.1 Encoding Binary Relations in a Tensor
Suppose we are given a knowledge base with
n entities and m relation types, and the facts
in the knowledge base are denoted as a set of
entity-relation triples T = {(e
i
, r
k
, e
j
)}, where
i, j ? {1, 2, ? ? ?n} and k ? {1, 2, ? ? ?m}. A
triple (e
i
, r
k
, e
j
) simply means that the i-th en-
tity and the j-th entity have the k-th relation.
Following (Franz et al., 2009), these triples can
naturally be encoded in a 3-way tensor X ?
{0, 1}
n?n?m
, such that X
i,j,k
= 1 if and only if
the triple (e
i
, r
k
, e
j
) ? T
1
. The tensor can be
viewed as consisting of m slices, where each slice
is an n?n square matrix, denoting the interactions
of the entities of a particular relation type. In the
remainder of this paper, we will use X
k
to refer to
the k-th slice of the tensor X . Fig. 1 illustrates this
representation.
1
This representation can easily be extended for a proba-
bilistic knowledge base by allowing nonnegative real values.
e1  en
e 1  
 e n
?? k
Figure 1: A tensor encoding of m binary relation
types and n entities. A sliceX
k
denotes the entities
having the k-th relation.
3.2 RESCAL
In order to identify latent components in a ten-
sor for collective learning, Nickel et al. (2011)
proposed RESCAL, which is a tensor decomposi-
tion approach specifically designed for the multi-
relational data described in Sec. 3.1. Given a ten-
sor X
n?n?m
, RESCAL aims to have a rank-r ap-
proximation, where each slice X
k
is factorized as
X
k
? AR
k
A
T
. (1)
A is an n ? r matrix, where the i-th row denotes
the r latent components of the i-th entity. R
k
is an
asymmetric r ? r matrix that describes the inter-
actions of the latent components according to the
k-th relation. Notice that while R
k
differs in each
slice, A remains the same.
A and R
k
are derived by minimizing the loss
function below.
min
A,R
k
f(A,R
k
) + ? ? g(A,R
k
), (2)
where f(A,R
k
) =
1
2
(
?
k
?X
k
?AR
k
A
T
?
2
F
)
is the mean-squared reconstruction error and
g(A,R
k
) =
1
2
(
?A?
2
F
+
?
k
?R
k
?
2
F
)
is the regu-
larization term.
RESCAL is a special form of Tucker decom-
position (Tucker, 1966) operating on a 3-way ten-
sor. Its model form (Eq. (1)) can also be regarded
as a relaxed form of DEDICOM (Bader et al.,
2007), which derives the low-rank approximation
as: X
k
? AD
k
RD
k
A
T
. To compare RESCAL
to other tensor decomposition methods, interested
readers can refer to (Kolda and Bader, 2009).
1570
The optimization problem in Eq. (2) can be
solved using the efficient alternating least-squares
(ALS) method. This approach alternatively fixes
R
k
to solve for A and then fixes A to solve
R
k
. The whole procedure stops until
f(A,R
k
)
?X?
2
F
con-
verges to some small threshold  or the maximum
number of iterations has been reached.
By finding the solutions where the gradients are
0, we can derive the update rules of A and R
k
as
below.
A?
[
?
k
X
k
AR
T
k
+X
T
k
AR
k
][
?
k
B
k
+C
k
+?I
]
?1
,
where B
k
= R
k
A
T
AR
T
k
and C
k
= R
T
k
A
T
AR
k
.
vec(R
k
)?
(
Z
T
Z + ?I
)
?1
Z
T
vec(X
k
), (3)
where vec(R
k
) is the vectorization of R
k
, Z =
A?A and the operator ? is the Kronecker prod-
uct.
Complexity Analysis Following the analysis in
(Nickel et al., 2012), we assume that each X
k
is a
sparse matrix, and let p be the number of non-zero
entries
2
. The complexity of computing X
k
AR
T
k
and X
T
k
AR
k
is O(pr + nr
2
). Evaluating B
k
and
C
k
requires O(nr
2
) and the matrix inversion re-
quires O(r
3
). Therefore, the complexity of updat-
ing A isO(pr+nr
2
) assuming n r. The updat-
ing rule of R
k
involves inverting an r
2
? r
2
ma-
trix. Therefore, directly computing the inversion
requires time complexity O(r
6
) and space com-
plexity O(r
4
). Although Nickel et al. (2012) con-
sidered using QR decomposition to simplify the
updates, it is still time consuming with the time
complexity O(r
6
+ pr
2
). Therefore, the total time
complexity isO(r
6
+pr
2
) and the step of updating
R
k
is the bottleneck in the optimization process.
We will describe how to reduce the time complex-
ity of this step to O(nr
2
+ pr) in Section 4.2.
4 Approach
We describe how we leverage the relational do-
main knowledge in this section. By removing the
incompatible entity-relation triples from the loss
2
Notice that we use a slightly different definition of p
from the one in (Nickel et al., 2012). The time complexity
of multiplying an n ? n sparse matrix X
k
with p non-zero
entries by an n? r dense matrix is O(pr) assuming n r.
function, training can be done much more effi-
ciently and results in a model with higher pre-
diction accuracy. In addition, we also introduce
a mathematical technique to reduce the compu-
tational complexity of the tensor decomposition
methods when taking into account the regulariza-
tion term.
4.1 Applying Relational Domain Knowledge
In the domain of knowledge bases, the notion of
entity types is the side information that commonly
exists and dictates whether some entities can be
legitimate arguments of a given predicate. For
instance, suppose the relation of interest is born-
in, which denotes the birth location of a person.
When asked whether an incompatible pair of en-
tities, such as two person entities like Abraham
Lincoln and John Henry, having this rela-
tion, we can immediately reject the possibility. Al-
though the type information and the constraints
are readily available, it is overlooked in the pre-
vious work on matrix and tensor decomposition
models for knowledge bases (Riedel et al., 2013;
Nickel et al., 2012). Ignoring the type information
has two implications. Incompatible entity-relation
triples still participate in the loss function of the
optimization problem, which incurs unnecessary
computation. Moreover, by choosing values for
these incompatible entries we introduce errors in
training the model that can reduce the quality of
the model.
Based on this observation, we propose Typed-
RESCAL, or TRESCAL, which leverages the en-
tity type information to improve both the effi-
ciency of model training and the quality of the
model in term of prediction accuracy. We em-
ploy a direct and simple approach by excluding
the triples of the incompatible entity types from
the loss in Eq. (2). For each relation, let L
k
and
R
k
be the set of entities with a compatible type to
the k-th relation. That is, (e
i
, r
k
, e
j
) is a feasible
triple if and only if e
i
? L
k
and e
j
? R
k
. For no-
tational convenience, we use A
k
l
,A
k
r
to denote
the sub-matrices of A that consists of rows asso-
ciated with L
k
and R
k
, respectively. Analogously,
let X
k
lr
be the sub-matrix of X
k
that consists of
only the entity pairs compatible to the k-th rela-
tion. The rows and columns of X
k
lr
map to the en-
tities in A
k
l
and A
k
r
, respectively. In other words,
entries of X
k
but not in X
k
lr
do not satisfy the type
constraint and are ignored from the computation.
1571
~ ~ ? ? 
?k A 
A TRk
A kl A krT?klr
e ??Lk
e ??Rk
Figure 2: The construction of TRESCAL. Suppose
the k-th relation is born-in. L
k
is then a set of
person entities and R
k
is a set of location entities.
Only the sub-matrix corresponds to the compati-
ble entity pairs (i.e., X
k
lr
) and the sub-matrices of
the associated entities (i.e., A
k
l
and A
T
k
r
) will be
included in the loss.
Fig. 2 illustrates this construction.
TRESCAL solves the following optimization
problem:
min
A,R
k
f
?
(A,R
k
) + ? ? g(A,R
k
), (4)
where f
?
(A,R
k
) =
1
2
?
k
?X
k
lr
?A
k
l
R
k
A
T
k
r
?
2
F
and g(A,R
k
) =
1
2
(
?A?
2
F
+
?
k
?R
k
?
2
F
)
.
Similarly, A and R
k
can be solved using the
alternating least-squares method. The update rule
of A is
A?
[
?
k
(
X
k
lr
A
k
r
R
T
k
+ X
T
k
lr
A
k
l
R
k
)
]
?
[
?
k
B
k
r
+ C
k
l
+ ?I
]
?1
,
where B
k
r
= R
k
A
T
k
r
A
k
r
R
T
k
and C
k
l
=
R
T
k
A
T
k
l
A
k
l
R
k
.
The update ofR
k
becomes:
vec(R
k
)?
(
A
T
k
r
A
k
r
?A
T
k
l
A
k
l
+ ?I
)
?1
?
vec(A
k
l
T
X
k
lr
A
k
r
),
(5)
Complexity Analysis Let n? be the average
number of entities with a compatible type to a
relation. Follow a similar derivation in Sec. 3.2,
the time complexity of updating A isO(pr+ n?r
2
)
and the time complexity of updating R
k
remains
to be O(r
6
+ pr
2
).
4.2 Handling Regularization Efficiently
Examining the update rules of both RESCAL
and TRESCAL, we can see that the most time-
consuming part is the matrix inversions. For
RESCAL, this is the term (Z
T
Z+?I)
?1
in Eq. (3),
where Z = A?A. Nickel et al. (2011) made the
observation that if ? = 0, the matrix inversion can
be calculated by
(Z
T
Z)
?1
= (A
T
A)
?1
A? (A
T
A)
?1
A.
Then, it only involves an inversion of an r? r ma-
trix, namely A
T
A. However, if ? > 0, directly
calculating Eq. (3) requires to invert an r
2
? r
2
matrix and thus becomes a bottleneck in solving
Eq. (2).
To reduce the computational complexity of
the update rules of R
k
, we compute the inver-
sion
(
Z
T
Z + ?I
)
?1
by applying singular value
decomposition (SVD) to A, such that A =
U?V
T
, where U and V are orthogonal matrices
and ? is a diagonal matrix. Then by using proper-
ties of the Kronecker product we have:
(
Z
T
Z + ?I
)
?1
=
(
?I + V?
2
V
T
?V?
2
V
T
)
?1
=
(
?I + (V ?V)(?
2
??
2
)(V ?V)
T
)
?1
= (V ?V)
(
?I + ?
2
??
2
)
?1
(V ?V)
T
.
The last equality holds because V ? V is
also an orthogonal matrix. We leave the de-
tailed derivations in Appendix A. Notice that
(
?I + ?
2
??
2
)
?1
is a diagonal matrix. There-
fore, the inversion calculation is trivial.
This technique can be applied to TRESCAL
as well. By applying SVD to both A
k
l
and A
k
r
, we have A
k
l
= U
k
l
?
k
l
V
T
k
l
and
A
k
r
= U
k
r
?
k
r
V
T
k
r
, respectively. The computa-
tion of
(
A
T
k
r
A
k
r
?A
T
k
l
A
k
l
+ ?I
)
?1
of Eq. (5)
thus becomes:
(V
k
l
?V
k
r
)
(
?I + ?
2
k
l
??
2
k
r
)
?1
(V
k
l
?V
k
r
)
T
.
The procedure of updating R is depicted in Al-
gorithm 1.
Complexity Analysis For RESCAL, V and ?
can be computed by finding eigenvectors of A
T
A.
Therefore, computing SVD of A costs O(nr
2
+
r
3
) = O(nr
2
). Computing Step 4 in Algorithm 1
takes O(nr
2
+ pr). Step 5 and Step 6 require
1572
Algorithm 1 UpdatingR in TRESCAL
Require: X , A, and entity sets R
k
,L
k
,?k
Ensure: R
k
,?k.
1: for k = 1 . . .m do
2: [U
k
l
,?
2
k
l
,V
k
l
]? SVD(A
T
k
l
A
k
l
).
3: [U
k
r
,?
2
k
r
,V
k
r
]? SVD(A
T
k
r
A
k
r
).
4: M
1
? V
T
k
l
A
T
k
l
X
k
lr
A
k
r
V
k
r
.
5: M
2
? diag(?
2
k
l
) diag(?
2
k
r
)
T
+ ?1.
(1 is a matrix of all ones. Function diag
converts the diagonal entries of a matrix to
a vector. )
6: R
k
? V
k
l
(M
1
./M
2
)V
T
k
r
.
(The operator ?./? is element-wise divi-
sion.)
7: end for
O(r
2
) and O(r
3
), respectively. The overall time
complexity of updatingR
k
becomesO(nr
2
+pr).
Using a similar derivation, the time complex-
ity of updating R
k
in TRESCAL is O(n?r
2
+ pr).
Therefore, the total complexity of each iteration is
O(n?r
2
+ pr).
5 Experiments
We conduct two sets of experiments. The first
evaluates the proposed TRESCAL algorithm on
inferring unknown facts using existing relation?
entity triples, while the second demonstrates its
application to relation extraction when a text cor-
pus is available.
5.1 Knowledge Base Completion
We evaluate our approach on a knowledge base
generated by the CMU Never Ending Language
Learning (NELL) project (Carlson et al., 2010).
NELL collects human knowledge from the web
and has generated millions of entity-relation
triples. We use the data generated from version
165 for training
3
, and collect the new triples gen-
erated between NELL versions 166 and 533 as the
development set and those generated between ver-
sion 534 and 745 as the test set
4
. The data statistics
of the training set are summarized in Table 1. The
numbers of triples in the development and test sets
are 19,665 and 117,889, respectively. Notice that
this dataset is substantially larger than the datasets
used in recent work. For example, the Freebase
data used in (Socher et al., 2013) and (Bordes et
3
http://www.cs.cmu.edu/
?
nlao/
4
http://bit.ly/trescal
NELL
# entities 753k
# relation types 229
# entity types 300
# entity-relation triples 1.8M
Table 1: Data statistics of the training set from
NELL in our experiments.
al., 2013a) have 316k and 483k
5
triples, respec-
tively, compared to 1.8M in this dataset.
In the NELL dataset, the entity type informa-
tion is encoded in a specific relation, called Gen-
eralization. Each entity in the knowledge base is
assigned to at least one category presented by the
Generalization relationship. Based on this infor-
mation, the compatible entity type constraint of
each relation can be easily identified. Specifically,
we examined the entities and relations that occur
in the triples of the training data, and counted all
the types appearing in these instances of a given
relation legitimate.
We implement RESCAL and TRESCAL in
MATLAB with the Matlab tensor Toolbox (Bader
et al., 2012). With the efficient implementation
described in Section 4.2, all experiments can be
conducted on a commodity PC with 16 GB mem-
ory. We set the maximal number of iterations of
both RESCAL and TRESCAL to be 10, which we
found empirically to be enough to generate a sta-
ble model. Note that Eq. (4) is non-convex, and the
optimization process does not guarantee to con-
verge to a global minimum. Therefore, initial-
izing the model properly might be important for
the performance. Following the implementation of
RESCAL, we initialize A by performing singular
value decomposition over
?
X =
?
k
(X
k
+ X
T
k
),
such that
?
X = U?V
T
and set A = U. Then,
we apply the update rule ofR
k
to initialize {R
k
}.
RESCAL and TRESCAL have two types of param-
eters: (1) the rank r of the decomposed tensor and
(2) the regularization parameter ?. We tune the
rank parameter on development set in a range of
{100, 200, 300, 400} and the regularization pa-
rameter in a range of {0.01, 0.05, 0.1, 0.5, 1}.
For comparison, we also use the code released
by Bordes et al. (2013a), which is implemented
using Python and the Theano library (Bergstra
et al., 2010), to train a TransE model using the
5
In (Bordes et al., 2013a), there is a much larger dataset,
FB1M, that has 17.5M triples used for evaluation. However,
this dataset has not been released.
1573
Entity Retrieval Relation Retrieval
TransE RESCAL TRESCAL TransE RESCAL TRESCAL
w/o type checking 51.41%
?
51.59% 54.79% 75.88% 73.15%
?
76.12%
w/ type checking 67.56% 62.91%
?
69.26% 70.71%
?
73.08%
?
75.70%
Table 2: Model performance in mean average precision (MAP) on entity retrieval and relation retrieval.
? and ? indicate the comparison to TRESCAL in the same setting is statistically significant using a paired-
t test on average precision of each query, with p < 0.01 and p < 0.05, respectively. Enforcing type
constraints during test time improves entity retrieval substantially, but does not help in relation retrieval.
same NELL dataset. We reserved randomly 1%
of the training triples for the code to evaluate the
model performance in each iteration. As sug-
gested in their paper, we experiment with sev-
eral hyper-parameters, including learning rate of
{0.01, 0.001}, the latent dimension of {50, 100}
and the similarity measure of {L1, L2}. In addi-
tion, we also adjust the number of batches of {50,
100, 1000}. Of all the configurations, we keep the
models picked by the method, as well as the fi-
nal model after 500 training iterations. The final
model is chosen by the performance on our devel-
opment set.
5.1.1 Training Time Reduction
We first present experimental results demonstrat-
ing that TRESCAL indeed reduces the time re-
quired to factorize a knowledge database, com-
pared to RESCAL. The experiment is conducted
on NELL with r = 300 and ? = 0.1. When
? 6= 0, the original RESCAL algorithm described
in (Nickel et al., 2011; Nickel et al., 2012) cannot
handle a large r, because updating matrices {R
k
}
requires O(r
4
) memory. Later in this section, we
will show that in some situation a large rank r is
necessary for achieving good testing performance.
Comparing TRESCAL with RESCAL, each it-
eration of TRESCAL takes 1,608 seconds, while
that of RESCAL takes 7,415 seconds. In other
words, by inducing the entity type information
and constraints, TRESCAL enjoys around 4.6 times
speed-up, compared to an improved regularized
version of RESCAL. When updating A and {R
k
}
TRESCAL only requires operating on sub-matrices
of A, {R
k
} and {X
k
}, which reduces the compu-
tation substantially. In average, TRESCAL filters
96% of entity triples that have incompatible types.
In contrast, it takes TransE at least 2 days and 19
hours to finish training the model (the default 500
iterations)
6
, while TRESCAL finishes the training
6
It took almost 4 days to train the best TransE model that
in roughly 4 to 5 hours
7
.
5.1.2 Test Performance Improvement
We consider two different types of tasks to evalu-
ate the prediction accuracy of different models ?
entity retrieval and relation retrieval.
Entity Retrieval In the first task, we collect a
set of entity-relation pairs {(e
i
, r
k
)} and aim at
predicting e
j
such that the tuple (e
i
, r
k
, e
j
) is a
recorded triple in the NELL knowledge base. For
each pair (e
i
, r
k
), we collect triples {(e
i
, r
k
, e
?
j
)}
from the NELL test corpus as positive samples
and randomly pick 100 entries e
?
j
to form negative
samples {e
i
, r
k
, e
?
j
}. Given A and R
k
from the
factorization generated by RESCAL or TRESCAL,
the score assigned to a triple {e
i
, r
k
, e
?
j
} is com-
puted by a
T
i
R
k
a
j
where a
i
and a
j
are the i-th
and j-th rows of A. In TransE, the score is de-
termined by the negative dissimilarity measures of
the learned embeddings: ?d(e
i
, r
k
, e
?
j
) = ??e
i
+
r
k
? e
?
j
?
2
2
.
We evaluate the performance using mean aver-
age precision (MAP), which is a robust and sta-
ble metric (Manning et al., 2008). As can be
observed in Table 2 (left), TRESCAL achieves
54.79%, which outperforms 51.59% of RESCAL
and 51.41% of TransE. Adding constraints during
test time by assigning the lowest score to the en-
tity triples with incompatible types improves re-
sults of all models ? TRESCAL still performs the
best (69.26%), compared to TransE (67.56%) and
RESCAL (62.91%).
Relation Retrieval In the second task, given a
relation type r
k
, we are looking for the entity pairs
(e
i
, e
j
) that have this specific relationship. To gen-
erate test data, for each relation type, we collect
is included in Table 2.
7
We also tested the released code from (Socher et al.,
2013) for training a neural tensor network model. However,
we are not able to finish the experiments as each iteration of
this method takes almost 5 hours.
1574
gold entity pairs from the NELL knowledge base
as positive samples and randomly pick a set of en-
tity pairs as negative samples such that the number
of positive samples are the same as negative ones.
Results presented in Table 2 (right) show that
TRESCAL achieves 76.12%, while RESCAL and
TransE are 73.15% and 75.88%, respectively.
Therefore, incorporating the type information in
training seems to help in this task as well. Enforc-
ing the type constraints during test time does not
help as in entity retrieval. By removing incom-
patible entity pairs, the performance of TRESCAL,
RESCAL and TransE drop slightly to 75.70%,
73.08% and 70.71% respectively. One possible
explanation is that the task of relation retrieval is
easier than entity retrieval. The incorrect type in-
formation of some entities ends up filtering out a
small number of entity pairs that were retrieved
correctly by the model.
Notice that TRESCAL achieves different levels
of performance on various relations. For example,
it performs well on predicting AthletePlaysSport
(81%) and CoachesInLeague (88%), but achieves
suboptimal performance on predicting Works-
For (49%) and BuildingLocatedInCity (35%).
We hypothesize that it is easier to gener-
alize entity-relation triples when the relation
has several related relations. For examples,
AthletePlaysForTeam and TeamPlaysSport may
help discover entity-relation triples of Ath-
letePlaysSport.
5.1.3 Sensitivity to Parameters
We also study if TRESCAL is sensitive to the rank
parameter r and the regularization parameter ?,
where the detailed results can be found in Ap-
pendix B. In short, we found that increasing the
rank r generally leads to better models. Also,
while the model is not very sensitive to the value
of the regularization parameter ?, tuning ? is still
necessary for achieving the best performance.
5.2 Relation Extraction
Next, we apply TRESCAL to the task of extract-
ing relations between entities, jointly from a text
corpus and a structured knowledge base. We use
a corpus from (Riedel et al., 2013) that is cre-
ated by aligning the entities in NYTimes and Free-
base. The corpus consists of a training set and a
test set. In the training set, a list of entity pairs
are provided, along with surface patterns extracted
from NYTimes and known relations obtained from
Freebase. In the test set, only the surface patterns
are given. By jointly factoring a matrix consist-
ing of the surface patterns and relations, Riedel et
al. (2013) show that their model is able to capture
the mapping between the surface patterns and the
structured relations and hence is able to extract the
entity relations from free text. In the following, we
show that TRESCAL can be applied to this task.
We focus on the 19 relations listed in Table 1
of (Riedel et al., 2013) and only consider the
surface patterns that co-occur with these 19 re-
lations. We prune the surface patterns that oc-
cur less than 5 times and remove the entities that
are not involved in any relation and surface pat-
tern. Based on the training and test sets, we
build a 80,698?80,698?1,652 tensor, where each
slice captures a particular structured relation or a
surface pattern between two entities. There are
72 fine types extracted from Freebase assigned
to 53,836 entities that are recorded in Freebase.
In addition, special types, PER, LOC, ORG and
MISC, are assigned to the remaining 26,862 enti-
ties based on the predicted NER tags provided by
the corpus. A type is considered incompatible to a
relation or a surface pattern if in the training data,
none of the argument entities of the relation be-
longs to the type. We use r = 400 and ? = 0.1 in
TRESCAL to factorize the tensor.
We compare the proposed TRESCAL model to
RI13 (Riedel et al., 2013), YA11 (Yao et al., 2011),
MI09 (Mintz et al., 2009) and SU12 (Surdeanu et
al., 2012)
8
. We follow the protocol used in (Riedel
et al., 2013) to evaluate the results. Given a re-
lation as query, the top 1,000 entity pairs output
by each system are collected and the top 100 ones
are judged manually. Besides comparing individ-
ual models, we also report the results of combined
models. To combine the scores from two models,
we simply normalize the scores of entity-relation
tuples to zero mean and unit variance and take the
average. The results are summarized in Table 3.
As can been seen in the table, using TRESCAL
alone is not very effective and its performance is
only compatible to MI09 and YA11, and is sig-
nificantly inferior to RI13. This is understandable
because the problem setting favors RI13 as only
entity pairs that have occurred in the text or the
database will be considered in RI13, both during
model training and testing. In contrast, TRESCAL
8
The corpus and the system outputs are from http://
www.riedelcastro.org/uschema
1575
Relation # MI09 YA11 SU12 RI13 TR TR+SU12 TR+RI13
person/company 171 0.41 0.40 0.43 0.49 0.43 0.53 0.64
location/containedby 90 0.39 0.43 0.44 0.56 0.23 0.46 0.58
parent/child 47 0.05 0.10 0.25 0.31 0.19 0.24 0.35
person/place of birth 43 0.32 0.31 0.34 0.37 0.50 0.61 0.66
person/nationality 38 0.10 0.30 0.09 0.16 0.13 0.16 0.22
author/works written 28 0.52 0.53 0.54 0.71 0.00 0.39 0.62
person/place of death 26 0.58 0.58 0.63 0.63 0.54 0.72 0.89
neighborhood/neighborhood of 13 0.00 0.00 0.08 0.67 0.08 0.13 0.73
person/parents 8 0.21 0.24 0.51 0.34 0.01 0.16 0.38
company/founders 7 0.14 0.14 0.30 0.39 0.06 0.17 0.44
film/directed by 4 0.06 0.15 0.25 0.30 0.03 0.13 0.35
sports team/league 4 0.00 0.43 0.18 0.63 0.50 0.29 0.63
team/arena stadium 3 0.00 0.06 0.06 0.08 0.00 0.04 0.09
team owner/teams owned 2 0.00 0.50 0.70 0.75 0.00 0.00 0.75
roadcast/area served 2 1.00 0.50 1.00 1.00 0.50 0.83 1.00
structure/architect 2 0.00 0.00 1.00 1.00 0.00 0.02 1.00
composer/compositions 2 0.00 0.00 0.00 0.12 0.00 0.00 0.12
person/religion 1 0.00 1.00 1.00 1.00 0.00 1.00 1.00
film/produced by 1 1.00 1.00 1.00 0.33 0.00 1.00 0.25
Weighted MAP 0.33 0.36 0.39 0.47 0.30 0.44 0.57
Table 3: Weighted Mean Average Precisions. The # column shows the number of true facts in the pool.
Bold faced are winners per relation, italics indicate ties based on a sign test.
predicts all the possible combinations between en-
tities and relations, which makes the model less fit
to the task. However, when combining TRESCAL
with a pure text-based method, such as SU12,
we can clearly see TRESCAL is complementary
to SU12 (0.39 to 0.44 in weighted MAP score),
which makes the results competitive to RI13.
Interestingly, although both TRESCAL and RI13
leverage information from the knowledge base, we
find that by combining them, the performance is
improved quite substantially (0.47 to 0.57). We
suspect that the reason is that in our construc-
tion, each entity has its own vector representa-
tion, which is lacked in RI13. As a result, the
new triples that TRESCAL finds are very different
from those found by RI13. Nevertheless, com-
bining more methods do not always yield an im-
provement. For example, combining TR, RI13 and
SU12 together (not included in Table 3) achieves
almost the same performance as TR+RI13.
6 Conclusions
In this paper we developed TRESCAL, a tensor
decomposition method that leverages relational
domain knowledge. We use relational domain
knowledge to capture which triples are potentially
valid and found that, by excluding the triples that
are incompatible when performing tensor decom-
position, we can significantly reduce the train-
ing time and improve the prediction performance
as compared with RESCAL and TransE. More-
over, we demonstrated its effectiveness in the ap-
plication of relation extraction. Evaluated on the
dataset provided in (Riedel et al., 2013), the per-
formance of TRESCAL alone is comparable to sev-
eral existing systems that leverage the idea of dis-
tant supervision. When combined with the state-
of-the-art systems, we found that the results can
be further improved. For instance, the weighted
mean average precision of the previous best ap-
proach in (Riedel et al., 2013) has been increased
by 10 points (47% to 57%).
There are a number of interesting potential ex-
tensions of our work. First, while the experiments
in this paper are on traditional knowledge bases
and textual data, the idea of leveraging relational
domain knowledge is likely to be of value to other
linguistic databases as well. For instance, part-of-
speech tags can be viewed as the ?types? of words.
Incorporating such information in other tensor de-
composition methods (e.g., (Chang et al., 2013))
may help lexical semantic representations. Sec-
ond, relational domain knowledge goes beyond
entity types and their compatibility with specific
relations. For instance, the entity-relation triple
(e
1
, child-of, e
2
) can be valid only if e
1
.type =
person ? e
2
.type = person ? e
1
.age < e
2
.age.
It would be interesting to explore the possibility
of developing efficient methods to leverage other
types of relational domain knowledge. Finally, we
would like to create more sophisticated models of
knowledge base embedding, targeting complex in-
1576
ference tasks to better support semantic parsing
and question answering.
Acknowledgments
We thank Sebastian Riedel for providing the data
for experiments. We are also grateful to the anony-
mous reviewers for their valuable comments.
References
Brett W Bader, Richard A Harshman, and Tamara G
Kolda. 2007. Temporal analysis of semantic graphs
using ASALSAN. In ICDM, pages 33?42. IEEE.
Brett W. Bader, Tamara G. Kolda, et al. 2012. Matlab
tensor toolbox version 2.5. Available online, Jan-
uary.
James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien,
Pascal Lamblin, Razvan Pascanu, Guillaume Des-
jardins, Joseph Turian, David Warde-Farley, and
Yoshua Bengio. 2010. Theano: a CPU and
GPU math expression compiler. In Proceedings
of the Python for Scientific Computing Conference
(SciPy), June. Oral Presentation.
Joanna Biega, Erdal Kuzey, and Fabian M Suchanek.
2013. Inside YOGO2s: a transparent information
extraction architecture. In WWW, pages 325?328.
A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston,
and O. Yakhnenko. 2013a. Translating Embeddings
for Modeling Multi-relational Data. In Advances in
Neural Information Processing Systems 26.
Antoine Bordes, Xavier Glorot, Jason Weston, and
Yoshua Bengio. 2013b. A semantic matching en-
ergy function for learning with multi-relational data.
Machine Learning, pages 1?27.
Razvan Bunescu and Raymond Mooney. 2007. Learn-
ing to extract relations from the web using mini-
mal supervision. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics, pages 576?583, Prague, Czech Republic,
June. Association for Computational Linguistics.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr
Settles, Estevam R. Hruschka Jr., and Tom M.
Mitchell. 2010. Toward an architecture for never-
ending language learning. In AAAI.
Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.
2013. Multi-relational latent semantic analysis. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1602?1612, Seattle, Washington, USA, October.
Association for Computational Linguistics.
Lieven De Lathauwer, Bart De Moor, and Joos Vande-
walle. 2000. A multilinear singular value decompo-
sition. SIAM journal on Matrix Analysis and Appli-
cations, 21(4):1253?1278.
Thomas Franz, Antje Schultz, Sergej Sizov, and Steffen
Staab. 2009. Triplerank: Ranking semantic web
data by tensor decomposition. In The Semantic Web-
ISWC 2009, pages 213?228. Springer.
U Kang, Evangelos Papalexakis, Abhay Harpale, and
Christos Faloutsos. 2012. Gigatensor: scaling ten-
sor analysis up by 100 times-algorithms and discov-
eries. In KDD, pages 316?324. ACM.
Henk AL Kiers. 2000. Towards a standardized nota-
tion and terminology in multiway analysis. Journal
of chemometrics, 14(3):105?122.
Tamara G. Kolda and Brett W. Bader. 2009. Ten-
sor decompositions and applications. SIAM Review,
51(3):455?500, September.
Joseph B Kruskal. 1977. Three-way arrays: rank and
uniqueness of trilinear decompositions, with appli-
cation to arithmetic complexity and statistics. Lin-
ear algebra and its applications, 18(2):95?138.
Alan J Laub, 2005. Matrix analysis for scientists and
engineers, chapter 13, pages 139?150. SIAM.
Ben London, Theodoros Rekatsinas, Bert Huang, and
Lise Getoor. 2013. Multi-relational learning using
weighted tensor decomposition with modular loss.
Technical report, University of Maryland College
Park. http://arxiv.org/abs/1303.1733.
C. Manning, P. Raghavan, and H. Schutze. 2008.
Introduction to Information Retrieval. Cambridge
University Press.
T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and
J. Dean. 2013a. Distributed representations of
words and phrases and their compositionality. In
Advances in Neural Information Processing Systems
26.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013b. Linguistic regularities in continuous space
word representations. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 746?751, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
1003?1011, Suntec, Singapore, August. Association
for Computational Linguistics.
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2011. A three-way model for collective
learning on multi-relational data. In ICML, pages
809?816.
1577
Maximilian Nickel, Volker Tresp, and Hans-Peter
Kriegel. 2012. Factorizing YAGO: scalable ma-
chine learning for linked data. In WWW, pages 271?
280.
Evangelos E Papalexakis, Tom M Mitchell, Nicholas D
Sidiropoulos, Christos Faloutsos, Partha Pratim
Talukdar, and Brian Murphy. 2014. Turbo-smt:
Accelerating coupled sparse matrix-tensor factoriza-
tions by 200x. In SDM.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Proceedings of ECML/PKDD
2010. Springer.
Sebastian Riedel, Limin Yao, Andrew McCallum, and
Benjamin M. Marlin. 2013. Relation extraction
with matrix factorization and universal schemas. In
NAACL, pages 74?84.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction. Transactions
of the Association for Computational Linguistics,
1:367?378, October.
Ajit P Singh and Geoffrey J Gordon. 2008. Relational
learning via collective matrix factorization. In Pro-
ceedings of the 14th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 650?658. ACM.
Richard Socher, Danqi Chen, Christopher D. Manning,
and Andrew Y. Ng. 2013. Reasoning With Neural
Tensor Networks For Knowledge Base Completion.
In Advances in Neural Information Processing Sys-
tems 26.
Robert Speer, Catherine Havasi, and Henry Lieberman.
2008. Analogyspace: Reducing the dimensionality
of common sense knowledge. In AAAI, pages 548?
553.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher D. Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Ilya Sutskever, Joshua B Tenenbaum, and Ruslan
Salakhutdinov. 2009. Modelling relational data us-
ing Bayesian clustered tensor factorization. In NIPS,
pages 1821?1828.
Ledyard R Tucker. 1966. Some mathematical notes
on three-mode factor analysis. Psychometrika,
31(3):279?311.
Jason Weston, Antoine Bordes, Oksana Yakhnenko,
and Nicolas Usunier. 2013. Connecting language
and knowledge bases with embedding models for re-
lation extraction. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1366?1371, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation dis-
covery using generative models. In Proceedings of
the 2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1456?1466, Edin-
burgh, Scotland, UK., July. Association for Compu-
tational Linguistics.
Appendix A Detailed Derivation
We first introduce some lemmas that will be useful
for our derivation. Lemmas 2, 3 and 4 are the basic
properties of the Kronecker product. Their proofs
can be found at (Laub, 2005).
Lemma 1. Let V be an orthogonal matrix and
? a diagonal matrix. Then (I + V?V
T
)
?1
=
V(I + ?)
?1
V
T
.
Proof.
(I + V?V
T
)
?1
= (VIV
T
+ V?V
T
)
?1
= V(I + ?)
?1
V
T
Lemma 2. (A?B)(C?D) = AC?BD.
Lemma 3. (A?B)
T
= A
T
?B
T
.
Lemma 4. If A and B are orthogonal matrices,
then A?B will also be an orthogonal matrix.
Let Z = A ? A and apply singular value
decomposition to A = U?V
T
. The term
(
Z
T
Z + ?I
)
?1
can be rewritten as:
(
Z
T
Z + ?I
)
?1
=
(
?I + (A
T
?A
T
)(A?A)
)
?1
(6)
=
(
?I + A
T
A?A
T
A
)
?1
(7)
=
(
?I + V?
2
V
T
?V?
2
V
T
)
?1
(8)
=
(
?I + (V ?V)(?
2
??
2
)(V ?V)
T
)
?1
(9)
= (V ?V)
(
?I + ?
2
??
2
)
?1
(V ?V)
T
(10)
Eq. (6) is from replacing Z with A ? A and
Lemma 3. Eq. (7) is from Lemma 2. Eq. (8) is
from the properties of SVD, where U and V are
orthonormal matrices. Eq. (9) is from Lemma 2
and Lemma 3. Finally, Eq. (10) comes from
Lemma 1.
1578
Figure 3: Prediction performance of TRESCAL
and RESCAL with different rank (r).
Figure 4: Prediction performance of TRESCAL
with different regularization parameter (?).
Appendix B Hyper-parameter Sensitivity
We study if TRESCAL is sensitive to the rank
parameter r and the regularization parameter ?.
We use the task of relation retrieval and present
the model performance on the development set.
Fig. 3 shows the performance of TRESCAL and
RESCAL with different rank (r) values while fix-
ing ? = 0.01. Results show that both TRESCAL
and RESCAL achieve better performance when r
is reasonably large. TRESCAL obtains a bet-
ter model with smaller r than RESCAL, because
TRESCAL only needs to fit the triples of the com-
patible entity types. Therefore, it allows to use
smaller number of latent variables to fit the train-
ing data.
Fixing r = 400, Fig. 4 shows the performance
of TRESCAL at different values of the regulariza-
tion parameter ?, including no regularization at
all (? = 0). While the results suggest that the
method is not very sensitive to ?, tuning ? is still
necessary for achieving the best performance.
1579
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 40?44,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Inference Protocols for Coreference Resolution
Kai-Wei Chang Rajhans Samdani
Alla Rozovskaya Nick Rizzolo
Mark Sammons Dan Roth
University of Illinois at Urbana-Champaign
{kchang10|rsamdan2|rozovska|rizzolo|mssammon|danr}@illinois.edu
Abstract
This paper presents Illinois-Coref, a system
for coreference resolution that participated
in the CoNLL-2011 shared task. We in-
vestigate two inference methods, Best-Link
and All-Link, along with their corresponding,
pairwise and structured, learning protocols.
Within these, we provide a flexible architec-
ture for incorporating linguistically-motivated
constraints, several of which we developed
and integrated. We compare and evaluate the
inference approaches and the contribution of
constraints, analyze the mistakes of the sys-
tem, and discuss the challenges of resolving
coreference for the OntoNotes-4.0 data set.
1 Introduction
The coreference resolution task is challenging, re-
quiring a human or automated reader to identify
denotative phrases (?mentions?) and link them to
an underlying set of referents. Human readers use
syntactic and semantic cues to identify and dis-
ambiguate the referring phrases; a successful auto-
mated system must replicate this behavior by linking
mentions that refer to the same underlying entity.
This paper describes Illinois-Coref, a corefer-
ence resolution system built on Learning Based
Java (Rizzolo and Roth, 2010), that participated
in the ?closed? track of the CoNLL-2011 shared
task (Pradhan et al, 2011). Building on elements
of the coreference system described in Bengtson
and Roth (2008), we design an end-to-end system
(Sec. 2) that identifies candidate mentions and then
applies one of two inference protocols, Best-Link
and All-Link (Sec. 2.3), to disambiguate and clus-
ter them. These protocols were designed to easily
incorporate domain knowledge in the form of con-
straints. In Sec. 2.4, we describe the constraints that
we develop and incorporate into the system. The
different strategies for mention detection and infer-
ence, and the integration of constraints are evaluated
in Sections 3 and 4.
2 Architecture
Illinois-Coref follows the architecture used in
Bengtson and Roth (2008). First, candidate men-
tions are detected (Sec. 2.1). Next, a pairwise
classifier is applied to each pair of mentions, gen-
erating a score that indicates their compatibility
(Sec. 2.2). Next, at inference stage, a coreference
decoder (Sec. 2.3) aggregates these scores into men-
tion clusters. The original system uses the Best-Link
approach; we also experiment with All-Link decod-
ing. This flexible decoder architecture allows lin-
guistic or knowledge-based constraints to be easily
added to the system: constraints may force mentions
to be coreferent or non-coreferent and can be option-
ally used in either of the inference protocols. We
designed and implemented several such constraints
(Sec. 2.4). Finally, since mentions that are in single-
ton clusters are not annotated in the OntoNotes-4.0
data set, we remove those as a post-processing step.
2.1 Mention Detection
Given a document, a mention detector generates a
set of mention candidates that are used by the subse-
quent components of the system. A robust mention
detector is crucial, as detection errors will propagate
to the coreference stage. As we show in Sec. 3, the
system that uses gold mentions outperforms the sys-
tem that uses predicted mentions by a large margin,
from 15% to 18% absolute difference.
40
For the ACE 2004 coreference task, a good per-
formance in mention detection is typically achieved
by training a classifier e.g., (Bengtson and Roth,
2008). However, this model is not appropriate for
the OntoNotes-4.0 data set, in which (in contrast to
the ACE 2004 corpus) singleton mentions are not
annotated: a specific noun phrase (NP) may corre-
spond to a mention in one document but will not
be a mention in another document. Therefore, we
designed a high recall (? 90%) and low precision
(? 35%) rule-based mention detection system that
includes all phrases recognized as Named Entities
(NE?s) and all phrases tagged as NPs in the syntac-
tic parse of the text. As a post-processing step, we
remove all predicted mentions that remain in single-
ton clusters after the inference stage.
The best mention detection result on the DEV set1
is 64.93% in F1 score (after coreference resolution)
and is achieved by our best inference protocol, Best-
Link with constraints.
2.2 Pairwise Mention Scoring
The basic input to our inference algorithm is a pair-
wise mention score, which indicates the compatibil-
ity score of a pair of mentions. For any two mentions
u and v, the compatibility score wuv is produced
by a pairwise scoring component that uses extracted
features ?(u, v) and linguistic constraints c:
wuv = w ? ?(u, v) + c(u, v) + t, (1)
where w is a weight vector learned from training
data, c(u, v) is a compatibility score given by the
constraints, and t is a threshold parameter (to be
tuned). We use the same features as Bengtson and
Roth (2008), with the knowledge extracted from the
OntoNotes-4.0 annotation. The exact use of the
scores and the procedure for learning weights w are
specific to the inference algorithm and are described
next.
2.3 Inference
In this section, we present our inference techniques
for coreference resolution. These clustering tech-
niques take as input a set of pairwise mention scores
over a document and aggregate them into globally
1In the shared task, the data set is split into three sets:
TRAIN, DEV, and TEST.
consistent cliques representing entities. We investi-
gate the traditional Best-Link approach and a more
intuitively appealing All-Link algorithm.
2.3.1 Best-Link
Best-Link is a popular approach to coreference
resolution. For each mention, it considers the best
mention on its left to connect to (best according
the pairwise score wuv) and creates a link between
them if the pairwise score is above some thresh-
old. Although its strategy is simple, Bengtson and
Roth (2008) show that with a careful design, it can
achieve highly competitive performance.
Inference: We give an integer linear programming
(ILP) formulation of Best-Link inference in order to
present both of our inference algorithms within the
same framework. Given a pairwise scorer w, we
can compute the compatibility scores ? wuv from
Eq. (1) ? for all mention pairs u and v. Let yuv be
a binary variable, such that yuv = 1 only if u and v
are in the same cluster. For a document d, Best-Link
solves the following ILP formulation:
argmaxy
?
u,v
wuvyuv
s.t
?
u<v
yuv ? 1 ?v,
yuw ? {0, 1}.
(2)
Eq. (2) generates a set of connected components and
all the mentions in each connected component con-
stitute an entity.
Learning: We follow the strategy in (Bengtson
and Roth, 2008, Section 2.2) to learn the pairwise
scoring function w. The scoring function is trained
on:
? Positive examples: for each mention u, we con-
struct a positive example (u, v), where v is the
closest preceding mention in u?s equivalence
class.
? Negative examples: all mention pairs (u, v),
where v is a preceding mention of u and u, v
are not in the same class.
As a result of the singleton mentions not being anno-
tated, there is an inconsistency in the sample distri-
butions in the training and inference phases. There-
fore, we apply the mention detector to the training
set, and train the classifier using the union set of gold
and predicted mentions.
41
2.3.2 All-Link
The All-Link inference approach scores a cluster-
ing of mentions by including all possible pairwise
links in the score. It is also known as correlational
clustering (Bansal et al, 2002) and has been applied
to coreference resolution in the form of supervised
clustering (Mccallum and Wellner, 2003; Finley and
Joachims, 2005).
Inference: Similar to Best-Link, for a document d,
All-Link inference finds a clustering All-Link(d;w)
by solving the following ILP problem:
argmaxy
?
u,v
wuvyuv
s.t yuw ? yuv + yvw ? 1 ?u,w, v,
yuw ? {0, 1}.
(3)
The inequality constraints in Eq. (3) enforce the
transitive closure of the clustering. The solution of
Eq. (3) is a set of cliques, and the mentions in the
same cliques corefer.
Learning: We present a structured perceptron al-
gorithm, which is similar to supervised clustering
algorithm (Finley and Joachims, 2005) to learn w.
Note that as an approximation, it is certainly pos-
sible to use the weight parameter learned by using,
say, averaged perceptron over positive and negative
links. The pseudocode is presented in Algorithm 1.
Algorithm 1 Structured Perceptron like learning al-
gorithm for All-Link inference
Given: Annotated documents D and initial
weight winit
Initialize w ? winit
for Document d in D do
Clustering y ? All-Link(d;w)
for all pairs of mentions u and v do
I1(u, v) = [u, v coreferent in D]
I2(u, v) = [y(u) = y(v)]
w ? w +
(
I1(u, v)? I2(u, v)
)
?(u, v)
end for
end for
return w
For the All-Link clustering, we drop one of the
three transitivity constraints for each triple of men-
tion variables. Similar to Pascal and Baldridge
(2009), we observe that this improves accuracy ?
the reader is referred to Pascal and Baldridge (2009)
for more details.
2.4 Constraints
The constraints in our inference algorithm are based
on the analysis of mistakes on the DEV set2. Since
the majority of errors are mistakes in recall, where
the system fails to link mentions that refer to the
same entity, we define three high precision con-
straints that improve recall on NPs with definite de-
terminers and mentions whose heads are NE?s.
The patterns used by constraints to match mention
pairs have some overlap with those used by the pair-
wise mention scorer, but their formulation as con-
straints allow us to focus on a subset of mentions
to which a certain pattern applies with high preci-
sion. For example, the constraints use a rule-based
string similarity measure that accounts for the in-
ferred semantic type of the mentions compared. Ex-
amples of mention pairs that are correctly linked by
the constraints are: Governor Bush? Bush; a cru-
cial swing state , Florida? Florida; Sony itself ?
Sony; Farmers? Los Angeles - based Farmers.
3 Experiments and Results
In this section, we present the performance of the
system on the OntoNotes-4.0 data set. A previous
experiment using an earlier version of this data can
be found in (Pradhan et al, 2007). Table 1 shows the
performance for the two inference protocols, with
and without constraints. Best-Link outperforms All-
Link for both predicted and gold mentions. Adding
constraints improves the performance slightly for
Best-Link on predicted mentions. In the other con-
figurations, the constraints either do not affect the
performance or slightly degrade it.
Table 2 shows the results obtained on TEST, using
the best system configurations found on DEV. We
report results on predicted mentions with predicted
boundaries, predicted mentions with gold bound-
aries, and when using gold mentions3.
2We provide a more detailed analysis of the errors in Sec. 4.
3Note that the gold boundaries results are different from the
gold mention results. Specifying gold mentions requires coref-
erence resolution to exclude singleton mentions. Gold bound-
aries are provided by the task organizers and also include sin-
gleton mentions.
42
Method
Pred. Mentions w/Pred. Boundaries Gold Mentions
MD MUC BCUB CEAF AVG MUC BCUB CEAF AVG
Best-Link 64.70 55.67 69.21 43.78 56.22 80.58 75.68 64.69 73.65
Best-Link W/ Const. 64.69 55.8 69.29 43.96 56.35 80.56 75.02 64.24 73.27
All-Link 63.30 54.56 68.50 42.15 55.07 77.72 73.65 59.17 70.18
All-Link W/ Const. 63.39 54.56 68.46 42.20 55.07 77.94 73.43 59.47 70.28
Table 1: The performance of the two inference protocols on both gold and predicted mentions. The systems are
trained on the TRAIN set and evaluated on the DEV set. We report the F1 scores (%) on mention detection (MD)
and coreference metrics (MUC, BCUB, CEAF). The column AVG shows the averaged scores of the three coreference
metrics.
Task MD MUC BCUB CEAF AVG
Pred. Mentions w/ Pred. Boundaries 64.88 57.15 67.14 41.94 55.96
Pred. Mentions w/ Gold Boundaries 67.92 59.79 68.65 41.42 56.62
Gold Mentions - 82.55 73.70 65.24 73.83
Table 2: The results of our submitted system on the TEST set. The system uses Best-Link decoding with constraints
on predicted mentions and Best-Link decoding without constraints on gold mentions. The systems are trained on a
collection of TRAIN and DEV sets.
4 Discussion
Most of the mistakes made by the system are due to
not linking co-referring mentions. The constraints
improve slightly the recall on a subset of mentions,
and here we show other common errors for the sys-
tem. For instance, the system fails to link the two
mentions, the Emory University hospital in Atlanta
and the hospital behind me, since each of the men-
tions has a modifier that is not part of the other men-
tion. Another common error is related to pronoun
resolution, especially when a pronoun has several
antecedents in the immediate context, appropriate in
gender, number, and animacy, as in ? E. Robert Wal-
lach was sentenced by a U.S. judge in New York to
six years in prison and fined $ 250,000 for his rack-
eteering conviction in the Wedtech scandal .?: both
E. Robert Wallach and a U.S. judge are appropri-
ate antecedents for the pronoun his. Pronoun errors
are especially important to address since 35% of the
mentions are pronouns.
The system also incorrectly links some mentions,
such as: ?The suspect said it took months to repack-
age...? (?it? cannot refer to a human); ?They see
them.? (subject and object in the same sentence are
linked); and ?Many freeway accidents occur simply
because people stay inside the car and sort out...?
(the NP the car should not be linked to any other
mention, since it does not refer to a specific entity).
5 Conclusions
We have investigated a coreference resolution sys-
tem that uses a rich set of features and two popular
types of clustering algorithm.
While the All-Link clustering seems to be capable
of taking more information into account for making
clustering decisions, as it requires each mention in
a cluster to be compatible with all other mentions in
that cluster, the Best-Link approach still outperforms
it. This raises a natural algorithmic question regard-
ing the inherent nature of clustering style most suit-
able for coreference and regarding possible ways of
infusing more knowledge into different coreference
clustering styles. Our approach accommodates in-
fusion of knowledge via constraints, and we have
demonstrated its utility in an end-to-end coreference
system.
Acknowledgments This research is supported by the Defense
Advanced Research Projects Agency (DARPA) Machine Reading Pro-
gram under Air Force Research Laboratory (AFRL) prime contract no.
FA8750-09-C-0181 and the Army Research Laboratory (ARL) under
agreement W911NF-09-2-0053. Any opinions, findings, and conclu-
sion or recommendations expressed in this material are those of the au-
thor(s) and do not necessarily reflect the view of the DARPA, AFRL,
ARL or the US government.
43
References
N. Bansal, A. Blum, and S. Chawla. 2002. Correlation
clustering. In Proceedings of the 43rd Symposium on
Foundations of Computer Science.
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP, 10.
T. Finley and T. Joachims. 2005. Supervised cluster-
ing with support vector machines. In Proceedings
of the International Conference on Machine Learning
(ICML).
A. Mccallum and B. Wellner. 2003. Toward condi-
tional models of identity uncertainty with application
to proper noun coreference. In The Conference on
Advances in Neural Information Processing Systems
(NIPS).
D. Pascal and J. Baldridge. 2009. Global joint models for
coreference resolution and named entity classification.
In Procesamiento del Lenguaje Natural.
S. Pradhan, L. Ramshaw, R. Weischedel, J. MacBride,
and L. Micciulla. 2007. Unrestricted Coreference:
Identifying Entities and Events in OntoNotes. In in
Proceedings of the IEEE International Conference on
Semantic Computing (ICSC), September 17-19.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. Conll-2011 shared
task: Modeling unrestricted coreference in ontonotes.
In Proceedings of the Annual Conference on Compu-
tational Natural Language Learning (CoNLL).
N. Rizzolo and D. Roth. 2010. Learning Based Java
for Rapid Development of NLP Systems. In Proceed-
ings of the International Conference on Language Re-
sources and Evaluation (LREC), Valletta, Malta, 5.
44
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 113?117,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
Illinois-Coref: The UI System in the CoNLL-2012 Shared Task
Kai-Wei Chang Rajhans Samdani Alla Rozovskaya Mark Sammons Dan Roth
University of Illinois at Urbana-Champaign
{kchang10|rsamdan2|rozovska|mssammon|danr}@illinois.edu
Abstract
The CoNLL-2012 shared task is an extension
of the last year?s coreference task. We partici-
pated in the closed track of the shared tasks in
both years. In this paper, we present the im-
provements of Illinois-Coref system from last
year. We focus on improving mention detec-
tion and pronoun coreference resolution, and
present a new learning protocol. These new
strategies boost the performance of the system
by 5% MUC F1, 0.8% BCUB F1, and 1.7%
CEAF F1 on the OntoNotes-5.0 development
set.
1 Introduction
Coreference resolution has been a popular topic of
study in recent years. In the task, a system requires
to identify denotative phrases (?mentions?) and to
cluster the mentions into equivalence classes, so that
the mentions in the same class refer to the same en-
tity in the real world.
Coreference resolution is a central task in the
Natural Language Processing research. Both the
CoNLL-2011 (Pradhan et al, 2011) and CoNLL-
2012 (Pradhan et al, 2012) shared tasks focus on
resolving coreference on the OntoNotes corpus. We
also participated in the CoNLL-2011 shared task.
Our system (Chang et al, 2011) ranked first in two
out of four scoring metrics (BCUB and BLANC),
and ranked third in the average score. This year,
we further improve the system in several respects.
In Sec. 2, we describe the Illinois-Coref system
for the CoNLL-2011 shared task, which we take as
the baseline. Then, we discuss the improvements
on mention detection (Sec. 3.1), pronoun resolu-
tion (Sec. 3.2), and learning algorithm (Sec. 3.3).
Section 4 shows experimental results and Section 5
offers a brief discussion.
2 Baseline System
We use the Illinois-Coref system from CoNLL-2011
as the basis for our current system and refer to it as
the baseline. We give a brief outline here, but fo-
cus on the innovations that we developed; a detailed
description of the last year?s system can be found in
(Chang et al, 2011).
The Illinois-Coref system uses a machine learn-
ing approach to coreference, with an inference pro-
cedure that supports straightforward inclusion of do-
main knowledge via constraints.
The system first uses heuristics based on Named
Entity recognition, syntactic parsing, and shallow
parsing to identify candidate mentions. A pair-
wise scorer w generates compatibility scores wuv
for pairs of candidate mentions u and v using ex-
tracted features ?(u, v) and linguistic constraints c.
wuv = w ? ?(u, v) + c(u, v) + t, (1)
where t is a threshold parameter (to be tuned). An
inference procedure then determines the optimal set
of links to retain, incorporating constraints that may
override the classifier prediction for a given mention
pair. A post-processing step removes mentions in
singleton clusters.
Last year, we found that a Best-Link decoding
strategy outperformed an All-Link strategy. The
Best-Link approach scans candidate mentions in a
document from left to right. At each mention, if cer-
tain conditions are satisfied, the pairwise scores of
all previous mentions are considered, together with
any constraints that apply. If one or more viable
113
links is available, the highest-scoring link is selected
and added to the set of coreference links. After the
scan is complete, the transitive closure of edges is
taken to generate the coreference clusters, each clus-
ter corresponding to a single predicted entity in the
document.
The formulation of this best-link solution is as fol-
lows. For two mentions u and v, u < v indicates
that the mention u precedes v in the document. Let
yuv be a binary variable, such that yuv = 1 only if
u and v are in the same cluster. For a document d,
Best-Link solves the following formulation:
argmaxy
?
u,v:u<v
wuvyuv
s.t
?
u<v
yuv ? 1 ?v,
yuw ? {0, 1}.
(2)
Eq. (2) generates a set of connected components
and the set of mentions in each connected compo-
nent constitute an entity. Note that we solve the
above Best-Link inference using an efficient algo-
rithm (Bengtson and Roth, 2008) which runs in time
quadratic in the number of mentions.
3 Improvements over the Baseline System
Below, we describe improvements introduced to the
baseline Illinois-Coref system.
3.1 Mention Detection
Mention detection is a crucial component of an end-
to-end coreference system, as mention detection er-
rors will propagate to the final coreference chain.
Illinois-Coref implements a high recall and low
precision rule-based system that includes all noun
phrases, pronouns and named entities as candidate
mentions. The error analysis shows that there are
two main types of errors.
Non-referential Noun Phrases. Non-referential
noun phrases are candidate noun phrases, identified
through a syntactic parser, that are unlikely to re-
fer to any entity in the real world (e.g., ?the same
time?). Note that because singleton mentions are not
annotated in the OntoNotes corpus, such phrases are
not considered as mentions. Non-referential noun
phrases are a problem, since during the coreference
stage they may be incorrectly linked to a valid men-
tion, thereby decreasing the precision of the system.
To deal with this problem, we use the training data
to count the number of times that a candidate noun
phrase happens to be a gold mention. Then, we re-
move candidate mentions that frequently appear in
the training data but never appear as gold mentions.
Relaxing this approach, we also take the predicted
head word and the words before and after the men-
tion into account. This helps remove noun phrases
headed by a preposition (e.g., the noun ?fact? in the
phrase ?in fact?). This strategy will slightly degrade
the recall of mention detection, so we tune a thresh-
old learned on the training data for the mention re-
moval.
Incorrect Mention Boundary. A lot of errors in
mention detection happen when predicting mention
boundaries. There are two main reasons for bound-
ary errors: parser mistakes and annotation incon-
sistencies. A mistake made by the parser may be
due to a wrong attachment or adding extra words
to a mention. For example, if the parser attaches
the relative clause inside of the noun phrase ?Pres-
ident Bush, who traveled to China yesterday? to a
different noun, the algorithm will predict ?President
Bush? as a mention instead of ?President Bush, who
traveled to China yesterday?; thus it will make an er-
ror, since the gold mention also includes the relative
clause. In this case, we prefer to keep the candi-
date with a larger span. On the other hand, we may
predict ?President Bush at Dayton? instead of ?Pres-
ident Bush?, if the parser incorrectly attaches the
prepositional phrase. Another example is when ex-
tra words are added, as in ?Today President Bush?.
A correct detection of mention boundaries is cru-
cial to the end-to-end coreference system. The re-
sults in (Chang et al, 2011, Section 3) show that the
baseline system can be improved from 55.96 avg F1
to 56.62 in avg F1 by using gold mention boundaries
generated from a gold annotation of the parsing tree
and the name entity tagging. However, fixing men-
tion boundaries in an end-to-end system is difficult
and requires additional knowledge. In the current
implementation, we focus on a subset of mentions
to further improve the mention detection stage of the
baseline system. Specifically, we fix mentions start-
ing with a stop word and mentions ending with a
punctuation mark. We also use training data to learn
patterns of inappropriate mention boundaries. The
mention candidates that match the patterns are re-
114
moved. This strategy is similar to the method used
to remove non-referential noun phrases.
As for annotation inconsistency, we find that in a
few documents, a punctuation mark or an apostrophe
used to mark the possessive form are inconsistently
added to the end of a mention. The problem results
in an incorrect matching between the gold and pre-
dicted mentions and downgrades the performance of
the learned model. Moreover, the incorrect mention
boundary problem also affects the training phase be-
cause our system is trained on a union set of the pre-
dicted and gold mentions. To fix this problem, in
the training phase, we perform a relaxed matching
between predicted mentions and gold mentions and
ignore the punctuation marks and mentions that start
with one of the following: adverb, verb, determiner,
and cardinal number. For example, we successfully
match the predicted mention ?now the army? to the
gold mention ?the army? and match the predicted
mention ?Sony ?s? to the gold mention ?Sony.? Note
that we cannot fix the inconsistency problem in the
test data.
3.2 Pronoun Resolution
The baseline system uses an identical model for
coreference resolution on both pronouns and non-
pronominal mentions. However, in the litera-
ture (Bengtson and Roth, 2008; Rahman and Ng,
2011; Denis and Baldridge, 2007) the features
for coreference resolution on pronouns and non-
pronouns are usually different. For example, lexi-
cal features play an important role in non-pronoun
coreference resolution, but are less important for
pronoun anaphora resolution. On the other hand,
gender features are not as important in non-pronoun
coreference resolution.
We consider training two separate classifiers with
different sets of features for pronoun and non-
pronoun coreference resolution. Then, in the decod-
ing stage, pronoun and non-pronominal mentions
use different classifiers to find the best antecedent
mention to link to. We use the same features for
non-pronoun coreference resolution, as the baseline
system. For the pronoun anaphora classifier, we use
a set of features described in (Denis and Baldridge,
2007), with some additional features. The aug-
mented feature set includes features to identify if a
pronoun or an antecedent is a speaker in the sen-
Algorithm 1 Online Latent Structured Learning for
Coreference Resolution
Loop until convergence:
For each document Dt and each v ? Dt
1. Let u? = max
u?y(v)
wT?(u, v), and
2. u? = max
u?{u<v}?{?}
wT?(u, v) + ?(u, v, y(v))
3. Let w? w + ?wT (?(u?, v)? ?(u?, v)).
tence. It also includes features to reflect the docu-
ment type. In Section 4, we will demonstrate the im-
provement of using separate classifiers for pronoun
and non-pronoun coreference resolution.
3.3 Learning Protocol for Best-Link Inference
The baseline system applies the strategy in (Bengt-
son and Roth, 2008, Section 2.2) to learn the pair-
wise scoring functionw using the Averaged Percep-
tron algorithm. The algorithm is trained on mention
pairs generated on a per-mention basis. The exam-
ples are generated for a mention v as
? Positive examples: (u, v) is used as a positive
example where u < v is the closest mention to
v in v?s cluster
? Negative examples: for all w with u < w < v,
(w, v) forms a negative example.
Although this approach is simple, it suffers from
a severe label imbalance problem. Moreover, it does
not relate well to the best-link inference, as the deci-
sion of picking the closest preceding mention seems
rather ad-hoc. For example, consider three men-
tions belonging to the same cluster: {m1: ?Presi-
dent Bush?, m2: ?he?, m3:?George Bush?}. The
baseline system always chooses the pair (m2,m3)
as a positive example because m2 is the closet men-
tion of m3. However, it is more proper to learn the
model on the positive pair (m1,m3), as it provides
more information. Since the best links are not given
but are latent in our learning problem, we use an on-
line latent structured learning algorithm (Connor et
al., 2011) to address this problem.
We consider a structured problem that takes men-
tion v and its preceding mentions {u | u < v} as
inputs. The output variables y(v) is the set of an-
tecedent mentions that co-refer with v. We define
a latent structure h(v) to be the bestlink decision
of v. It takes the value ? if v is the first mention
115
Method
Without Separating Pronouns With Separating Pronouns
MD MUC BCUB CEAF AVG MD MUC BCUB CEAF AVG
Binary Classifier (baseline) 70.53 61.63 69.26 43.03 57.97 73.24 64.57 69.78 44.95 59.76
Latent-Structured Learning 73.02 64.98 70.00 44.48 59.82 73.95 65.75 70.25 45.30 60.43
Table 1: The performance of different learning strategies for best-link decoding algorithm. We show the results
with/without using separate pronoun anaphora resolver. The systems are trained on the TRAIN set and evaluated on
the CoNLL-2012 DEV set. We report the F1 scores (%) on mention detection (MD) and coreference metrics (MUC,
BCUB, CEAF). The column AVG shows the averaged scores of the three coreference metrics.
System MD MUC BCUB CEAF AVG
Baseline 64.58 55.49 69.15 43.72 56.12
New Sys. 70.03 60.65 69.95 45.39 58.66
Table 2: The improvement of Illinois-Coref. We report
the F1 scores (%) on the DEV set from CoNLL-2011
shared task. Note that the CoNLL-2011 data set does not
include corpora of bible and of telephone conversation.
in the equivalence class, otherwise it takes values
from {u | u < v}. We define a loss function
?(h(v), v, y(v)) as
?(h(v), v, y(v)) =
{
0 h(v) ? y(v),
1 h(v) /? y(v).
We further define the feature vector ?(?, v) to be a
zero vector and ? to be the learning rate in Percep-
tron algorithm. Then, the weight vectorw in (1) can
be learned from Algorithm 1. At each step, Alg. 1
picks a mention v and finds the Best-Link decision
u? that is consistent with the gold cluster. Then, it
solves a loss-augmented inference problem to find
the best link decision u? with current model (u? = ?
if the classifier decides that v does not have coref-
erent antecedent mention). Finally, the model w is
updated by the difference between the feature vec-
tors ?(u?, v) and ?(u?, v).
Alg. 1 makes learning more coherent with infer-
ence. Furthermore, it naturally solves the data im-
balance problem. Lastly, this algorithm is fast and
converges very quickly.
4 Experiments and Results
In this section, we demonstrate the performance of
Illinois-Coref on the OntoNotes-5.0 data set. A pre-
vious experiment using an earlier version of this data
can be found in (Pradhan et al, 2007). We first show
the improvement of the mention detection system.
Then, we compare different learning protocols for
coreference resolution. Finally, we show the overall
performance improvement of Illinois-Coref system.
First, we analyze the performance of mention de-
tection before the coreference stage. Note that sin-
gleton mentions are included since it is not possible
to identify singleton mentions before running coref-
erence. They are removed in the post-processing
stage. The mention detection performance of the
end-to-end system will be discussed later in this sec-
tion. With the strategy described in Section 3.1, we
improve the F1 score for mention detection from
55.92% to 57.89%. Moreover, we improve the de-
tection performance on short named entity mentions
(name entity with less than 5 words) from 61.36 to
64.00 in F1 scores. Such mentions are more impor-
tant because they are easier to resolve in the corefer-
ence layer.
Regarding the learning algorithm, Table 1 shows
the performance of the two learning protocols
with/without separating pronoun anaphora resolver.
The results show that both strategies of using a pro-
noun classifier and training a latent structured model
with a online algorithm improve the system perfor-
mance. Combining the two strategies, the avg F1
score is improved by 2.45%.
Finally, we compare the final system with the
baseline system. We evaluate both systems on the
CoNLL-11 DEV data set, as the baseline system
is tuned on it. The results show that Illinois-Coref
achieves better scores on all the metrics. The men-
tion detection performance after coreference resolu-
tion is also significantly improved.
116
Task MD MUC BCUB CEAF AVG
English (Pred. Mentions) 74.32 66.38 69.34 44.81 60.18
English (Gold Mention Boundaries) 75.72 67.80 69.75 45.12 60.89
English (Gold Mentions) 100.00 85.74 77.46 68.46 77.22
Chinese (Pred Mentions) 47.58 37.93 63.23 35.97 45.71
Table 3: The results of our submitted system on the TEST set. The systems are trained on a collection of TRAIN and
DEV sets.
4.1 Chinese Coreference Resolution
We apply the same system to Chinese coreference
resolution. However, because the pronoun proper-
ties in Chinese are different from those in English,
we do not train separate classifiers for pronoun and
non-pronoun coreference resolution. Our Chinese
coreference resolution on Dev set achieves 37.88%
MUC, 63.37% BCUB, and 35.78% CEAF in F1
score. The performance for Chinese coreference is
not as good as the performance of the coreference
system for English. One reason for that is that we
use the same feature set for both Chinese and En-
glish systems, and the feature set is developed for
the English corpus. Studying the value of strong fea-
tures for Chinese coreference resolution system is a
potential topic for future research.
4.2 Test Results
Table 3 shows the results obtained on TEST, using
the best system configurations found on DEV. We
report results on both English and Chinese coref-
erence resolution on predicted mentions with pre-
dicted boundaries. For English coreference resolu-
tion, we also report the results when using gold men-
tions and when using gold mention boundaries1.
5 Conclusion
We described strategies for improving mention de-
tection and proposed an online latent structure al-
gorithm for coreference resolution. We also pro-
posed using separate classifiers for making Best-
Link decisions on pronoun and non-pronoun men-
tions. These strategies significantly improve the
Illinois-Coref system.
1Note that, in Ontonotes annotation, specifying gold men-
tions requires coreference resolution to exclude singleton men-
tions. Gold mention boundaries are provided by the task orga-
nizers and include singleton mentions.
Acknowledgments This research is supported by the
Defense Advanced Research Projects Agency (DARPA)
Machine Reading Program under Air Force Research
Laboratory (AFRL) prime contract no. FA8750-09-C-
0181 and the Army Research Laboratory (ARL) under
agreement W911NF-09-2-0053. Any opinions, findings,
and conclusion or recommendations expressed in this ma-
terial are those of the author(s) and do not necessarily
reflect the view of the DARPA, AFRL, ARL or the US
government.
References
E. Bengtson and D. Roth. 2008. Understanding the value
of features for coreference resolution. In EMNLP.
K. Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,
M. Sammons, and D. Roth. 2011. Inference proto-
cols for coreference resolution. In CoNLL.
M. Connor, C. Fisher, and D. Roth. 2011. Online latent
structure training for language acquisition. In IJCAI.
P. Denis and J. Baldridge. 2007. A ranking approach to
pronoun resolution. In IJCAI.
S. Pradhan, L. Ramshaw, R. Weischedel, J. MacBride,
and L. Micciulla. 2007. Unrestricted Coreference:
Identifying Entities and Events in OntoNotes. In
ICSC.
S. Pradhan, L. Ramshaw, M. Marcus, M. Palmer,
R. Weischedel, and N. Xue. 2011. CoNLL-2011
shared task: Modeling unrestricted coreference in
OntoNotes. In CoNLL.
S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and
Y. Zhang. 2012. CoNLL-2012 shared task: Modeling
multilingual unrestricted coreference in OntoNotes. In
CoNLL.
A. Rahman and V. Ng. 2011. Narrowing the modeling
gap: a cluster-ranking approach to coreference resolu-
tion. Journal of AI Research, 40(1):469?521.
117
Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 13?19,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
The University of Illinois System in the CoNLL-2013 Shared Task
Alla Rozovskaya Kai-Wei Chang Mark Sammons Dan Roth
Cognitive Computation Group
University of Illinois at Urbana-Champaign
Urbana, IL 61801
{rozovska,kchang10,mssammon,danr}@illinois.edu
Abstract
The CoNLL-2013 shared task focuses on
correcting grammatical errors in essays
written by non-native learners of English.
In this paper, we describe the University
of Illinois system that participated in the
shared task. The system consists of five
components and targets five types of com-
mon grammatical mistakes made by En-
glish as Second Language writers. We de-
scribe our underlying approach, which re-
lates to our previous work, and describe
the novel aspects of the system in more de-
tail. Out of 17 participating teams, our sys-
tem is ranked first based on both the orig-
inal annotation and on the revised annota-
tion.
1 Introduction
The task of correcting grammar and usage mis-
takes made by English as a Second Language
(ESL) writers is difficult for several reasons. First,
many of these errors are context-sensitive mistakes
that confuse valid English words and thus can-
not be detected without considering the context
around the word. Second, the relative frequency
of mistakes is quite low: for a given type of mis-
take, an ESL writer will typically make mistakes
in only a small proportion of relevant structures.
For example, determiner mistakes usually occur
in 5% to 10% of noun phrases in various anno-
tated ESL corpora (Rozovskaya and Roth, 2010a).
Third, an ESL writer may make multiple mistakes
in a single sentence, which may give misleading
local cues for individual classifiers. In the exam-
ple shown in Figure 1, the agreement error on the
verb ?tend? interacts with the noun number error
on the word ?equipments?.
Therefore , the *equipments/equipment of bio-
metric identification *tend/tends to be in-
expensive .
Figure 1: Representative ESL errors in a sample
sentence from the training data.
The CoNLL-2013 shared task (Ng et al, 2013)
focuses on the following five common mistakes
made by ESL writers:
? article/determiner
? preposition
? noun number
? subject-verb agreement
? verb form
Errors outside this target group are present in the
task corpora, but are not evaluated.
In this paper, we present a system that combines
a set of statistical models, where each model spe-
cializes in correcting one of the errors described
above. Because the individual error types have
different characteristics, we use several different
approaches. The article system builds on the el-
ements of the system described in (Rozovskaya
and Roth, 2010c). The preposition classifier uses
a combined system, building on work described
in (Rozovskaya and Roth, 2011) and (Rozovskaya
and Roth, 2010b). The remaining three models are
all Na??ve Bayes classifiers trained on the Google
Web 1T 5-gram corpus (henceforth, Google cor-
pus, (Brants and Franz, 2006)).
We first briefly discuss the task (Section 2) and
give the overview of our system (Section 3). We
then describe the error-specific components (Sec-
tions 3.1, 3.2 and 3.3). The sections describ-
ing individual components quantify their perfor-
mance on splits of the training data. In Section 4,
13
we evaluate the complete system on the training
data using 5-fold cross-validation (hereafter, ?5-
fold CV?) and in Section 5 we show the results we
obtained on test.
We close with a discussion focused on error
analysis (Section 6) and our conclusions (Sec-
tion 7).
2 Task Description
The CoNLL-2013 shared task focuses on correct-
ing five types of mistakes that are commonly made
by non-native speakers of English. The train-
ing data released by the task organizers comes
from the NUCLE corpus (Dahlmeier et al, 2013),
which contains essays written by learners of En-
glish as a foreign language and is corrected by
English teachers. The test data for the task con-
sists of an additional set of 50 student essays. Ta-
ble 1 illustrates the mistakes considered in the task
and Table 2 illustrates the distribution of these er-
rors in the released training data and the test data.
We note that the test data contains a much larger
proportion of annotated mistakes. For example,
while only 2.4% of noun phrases in the training
data have determiner errors, in the test data 10%
of noun phrases have mistakes.
Error type Percentage of errors
Training Test
Articles 2.4% 10.0%
Prepositions 2.0% 10.7%
Noun number 1.6% 6.0%
Subject-verb agreement 2.0% 5.2%
Verb form 0.8% 2.5%
Table 2: Statistics on error distribution in train-
ing and test data. Percentage denotes the erro-
neous instances with respect to the total number of
relevant instances in the data. For example, 10%
of noun phrases in the test data have determiner
errors.
Since the task focuses on five error types, only
annotations marking these mistakes were kept.
Note that while the other error annotations were
removed, the errors still remain in the data.
3 System Components
Our system consists of five components that ad-
dress individually article1, preposition, noun verb
1We will use the terms ?article-? and ?determiner errors?
interchangeably: article errors constitute the majority of de-
form and subject-verb agreement errors.
Our article and preposition modules build on the
elements of the systems described in Rozovskaya
and Roth (2010b), Rozovskaya and Roth (2010c)
and Rozovskaya and Roth (2011). The article sys-
tem is trained using the Averaged Perceptron (AP)
algorithm (Freund and Schapire, 1999), imple-
mented within Learning Based Java (Rizzolo and
Roth, 2010). The AP system is trained using the
inflation method (Rozovskaya et al, 2012). Our
preposition system is a Na??ve Bayes (NB) classi-
fier trained on the Google corpus and with prior
parameters adapted to the learner data.
The other modules ? those that correct noun and
verb errors ? are all NB models trained on the
Google corpus.
All components take as input the corpus doc-
uments preprocessed with a part-of-speech tag-
ger2 and shallow parser3 (Punyakanok and Roth,
2001). Note that the shared task data already
contains comparable pre-processing information,
in addition to other information, including depen-
dency parse and constituency parse, but we chose
to run our own pre-processing tools. The article
module uses the POS and chunker output to gen-
erate some of its features and to generate candi-
dates (likely contexts for missing articles). The
other system components use the pre-processing
tools only as part of candidate generation (e.g., to
identify all nouns in the data for the noun classi-
fier) because these components are trained on the
Google corpus and thus only employ word n-gram
features.
During development, we split the released train-
ing data into five parts. The results in Sections 3.1,
3.2, and 3.3 give performance of 5-fold CV on the
training data. In Section 4 we report the develop-
ment 5-fold CV results of the complete model and
the performance on the test data. Note that the per-
formance reported for the overall task on the test
data in Section 4 reflects the system that makes use
of the entire training corpus. It is also important to
remark that only the determiner system is trained
on the ESL data. The other models are trained on
native data, and the ESL training data is only used
to optimize the decision thresholds of the models.
terminer errors, and we address only article mistakes.
2http://cogcomp.cs.illinois.edu/page/
software view/POS
3http://cogcomp.cs.illinois.edu/page/
software view/Chunker
14
Error type Examples
Article ?It is also important to create *a/? better material that can support
*the/? buildings despite any natural disaster like earthquakes.?
Preposition ?As the number of people grows, the need *of /for habitable environ-
ment is unquestionably essential.
Noun number Some countries are having difficulties in managing a place to live for
their *citizen/citizens as they tend to get overpopulated.?
Subject-verb agreement ?Therefore , the equipments of biometric identification *tend/tends
to be inexpensive.
Verb form
?...countries with a lot of deserts can terraform their desert to increase
their habitable land and *using/use irrigation..?
?it was not *surprised/surprising to observe an increasing need for a
convenient and cost effective platform.?
Table 1: Example errors. Note that only the errors exemplifying the relevant phenomena are marked
in the table; the sentences may contain other mistakes. Errors marked as verb form include multiple
grammatical phenomena that may characterize verbs.
3.1 Determiners
There are three types of determiner error: omitting
a determiner; choosing an incorrect determiner;
and adding a spurious determiner. Even though
the majority of determiner errors involve article
mistakes, some of these errors involve personal
and possessive pronouns.4 Most of the determiner
errors, however, involve omitting an article (these
make up over 60% in the training data). Similar er-
ror patterns have been observed in other ESL cor-
pora (Rozovskaya and Roth, 2010a).
Our system focuses on article errors. The sys-
tem first extracts from the data all articles, and all
spaces at the beginning of a noun phrase where an
article is likely to be omitted (Han et al, 2006; Ro-
zovskaya and Roth, 2010c). Then we train a multi-
class classifier with features described in Table 3.
These features were used successfully in previous
tasks in error correction (Rozovskaya et al, 2012;
Rozovskaya et al, 2011).
The original word choice (the source article)
used by the writer is also used as a feature. Since
the errors are sparse, this feature causes the model
to abstain from flagging a mistake, which results
in low recall. To avoid this problem, we adopt the
approach proposed in (Rozovskaya et al, 2012),
the error inflation method, and add artificial arti-
cle errors in the training data based on the error
distribution on the training set. This method pre-
vents the source feature from dominating the con-
text features, and improves the recall of the sys-
4e.g. ?Pat apologized to me for not keeping the*/my se-
crets.?
tem.
We experimented with two types of classifiers:
Averaged Perceptron (AP) and an L1-generalized
logistic regression classifier (LR). Since the arti-
cle system is trained on the ESL data, of which
we have a limited amount, we also experimented
with adding a language model (LM) feature to the
LR learner. This feature indicates if the correc-
tion is accepted by a language model trained on
the Google corpus. The performance of each clas-
sifier on 5-fold CV on the training data is shown in
Table 4. The results show that AP performs better
than LR. We observed that adding the LM feature
improves precision but results in lower F1, so we
chose the AP classifier without the LM feature for
our final system.
Model Precision Recall F1
AP (inflation) 0.17 0.31 0.22
AP (inflation+LM) 0.26 0.15 0.19
LR (inflation) 0.17 0.29 0.22
LR (inflation+LM) 0.24 0.21 0.22
Table 4: Article development results Results on 5-fold
CV. AP With Inflation achieves the best development using an
inflation constant of 0.85. AP achieves higher performance
without using the language model feature.
3.2 Prepositions
The most common preposition errors are replace-
ments, i.e., where the author correctly recognized
the need for a preposition, but chose the wrong one
to use.
15
Feature Type Description
Word n-grams wB, w2B, w3B, wA, w2A, w3A, wBwA, w2BwB, wAw2A, w3Bw2BwB, w2BwBwA, wBwAw2A, wAw2Aw3A,
w4Bw3Bw2BwB, w3w2BwBwA, w2BwBwAw2A, wBwAw2Aw3A, wAw2Aw3w4A
POS features pB, p2B, p3B , pA, p2A, p3A, pBpA, p2BpB, pAp2A, pBwB, pAwA, p2Bw2B, p2Aw2A, p2BpBpA, pBpAp2A,
pAp2Ap3A
NP1 headWord, npWords, NC, adj&headWord, adjTag&headWord, adj&NC, adjTag&NC, npTags&headWord, npTags&NC
NP2 headWord&headPOS, headNumber
wordsAfterNP headWord&wordAfterNP, npWords&wordAfterNP, headWord&2wordsAfterNP, npWords&2wordsAfterNP, headWord&3wordsAfterNP,
npWords&3wordsAfterNP
wordBeforeNP wB&fi ?i ? NP1
Verb verb, verb&fi ?i ? NP1
Preposition prep&fi ?i ? NP1
Source the word used by the original writer
LM a binary feature assigned by a language model
Table 3: Features used in the article error correction system. wB and wA denote the word immediately before and after
the target, respectively; and pB and pA denote the POS tag before and after the target. headWord denotes the head of the NP
complement. NC stands for noun compound and is active if second to last word in the NP is tagged as a noun. Verb features are
active if the NP is the direct object of a verb. Preposition features are active if the NP is immediately preceded by a preposition.
adj feature is active if the first word (or the second word preceded by an adverb) in the NP is an adjective. npWords and npTags
denote all words (POS tags) in the NP.
3.2.1 Preposition Features
All features used in the preposition module are
lexical: word n-grams in the 4-word window
around the target preposition. The NB-priors clas-
sifier, which is part of our model, can only make
use of the word n-gram features; it uses n-gram
features of lengths 3, 4, and 5. Note that since the
NB model is trained on the Google corpus, the an-
notated ESL training data is used only to replace
the prior parameters of the model (see Rozovskaya
and Roth, 2011 for more details).
3.2.2 Training the Preposition System
Correcting preposition errors requires more data
to achieve performance comparable to article er-
ror correction due to the task complexity (Gamon,
2010). We found that training an AP model on
the ESL training data with more sophisticated fea-
tures is not as effective as training on a native En-
glish dataset of larger size. The ESL training data
contains slightly over 100K preposition examples,
which is several orders of magnitude smaller than
the Google n-gram corpus. We use the shared
task training data to replace the prior parameters
of the model (see Rozovskaya and Roth, 2011 for
more details). The NB-priors model does not tar-
get preposition omissions and insertions: it cor-
rects only preposition replacements that involve
the 12 most common English prepositions. The
task includes mistakes that cover 36 prepositions
but we found that the model performance drops
once the confusion set becomes too large. Table
5 shows the performance of the system on the 5-
fold CV on the training data, where each time the
classifier was trained on 80% of the documents.
Model Precision Recall F1
NB-priors 0.14 0.14 0.14
Table 5: Preposition results: NB with priors. Results on
5-fold CV. The model is trained on the Google corpus.
3.3 Correcting Nouns and Verbs
The three remaining types of errors ? noun num-
ber errors, subject-verb agreement, and the various
verb form mistakes ? are corrected using separate
NB models also trained on the Google corpus. We
focus here on the selection of candidates for cor-
rection, as this strongly affects performance.
3.3.1 Candidate Selection
This stage selects the set of words that are pre-
sented as input to the classifier. This is a crucial
step because it limits the performance of any sys-
tem: those errors that are missed at this stage have
no chance of being detected by the later stages.
This is also a challenging step as the class of
verbs and nouns is open, with many English verbs
and nouns being compatible with multiple parts of
speech. This problem does not arise in preposi-
tion and article error correction, where candidates
are determined by surface form (i.e. can be deter-
mined using a closed list of prepositions or arti-
cles).
We use the POS tag and the shallow parser out-
put to identify the set of candidates that are input
to the classifiers. In particular, for nouns, we col-
lect all words tagged as NN or NNS. Since pre-
processing tools are known to make more mis-
takes on ESL data than on native data, this pro-
cedure does not have a perfect result on the iden-
tification of all noun mistakes. For example, we
16
miss about 10% of noun errors due to POS/shallow
parser errors. For verbs, we compared several
candidate selection methods. Method (1) ex-
tracts all verbs heading a verb phrase, as iden-
tified by the shallow parser. Method (2) ex-
pands this set to words tagged with one of the
verb POS tags {VB,VBN,VBG,VBD,VBP,VBZ}.
However, generating candidates by selecting only
those tagged as verbs is not good enough, since the
POS tagger performance on ESL data is known to
be suboptimal (Nagata et al, 2011), especially for
verbs containing errors. For example, verbs lack-
ing agreement markers are likely to be mistagged
as nouns (Lee and Seneff, 2008). Erroneous verbs
are exactly the cases that we wish to include.
Method (3) adds words that are in the lemma list of
common English verbs compiled using the Giga-
word corpus. The last method has the highest re-
call on the candidate identification; it misses only
5% of verb errors, and also has better performance
in the complete model. We thus use this method.
3.3.2 Noun-Verb Correction Performance
Table 6 shows the performance of the systems
based on 5-fold CV on the training data. Each
model is trained individually on the Google cor-
pus, and is individually processed to optimize the
respective thresholds.
Model Precision Recall F1
Noun number 0.17 0.38 0.23
Subject-verb agr. 0.19 0.24 0.21
Verb form 0.07 0.20 0.10
Table 6: Noun, subject-verb agreement and
verb form results. Results on 5-fold CV. The
models are trained on the Google corpus.
4 Combined Model
In the previous sections, we described the individ-
ual components of the system developed to target
specific error types. The combined model includes
all of these modules, which are each applied to
examples individually: there is no pipeline, and
the individual predictions of the modules are then
pooled.
The combined system also includes a post-
processing step where we remove certain correc-
tions of noun and verb forms that we found oc-
cur quite often but are never correct. This hap-
pens when both choices ? the writer?s selection
and the correction ? are valid but the latter is ob-
served more frequently in the native training data.
For example, the phrase ?developing country? is
changed to ?developed country? even though both
are legitimate English expressions. If a correction
is frequently proposed but always results in a false
alarm, we add it to a list of changes that is ignored
when we generate the system output. When we
generate the output on Test set, 8 unique pairs of
such changes are ignored (36 pairs of changes in
total).
We now show the combined results on the train-
ing data by conducting 5-fold CV, where we add
one component at a time. Table 8 shows that the
recall and the F1 scores improve when each com-
ponent is added to the system. The final system
achieves an F1 score of 0.21 on the training data
in 5-fold CV.
Model Precision Recall F1
Articles 0.16 0.12 0.14
+Prepositions 0.16 0.14 0.15
+Noun number 0.17 0.23 0.20
+Subject-verb agr. 0.18 0.25 0.21
+Verb form (All) 0.18 0.27 0.21
Table 7: Results on 5-fold CV on the training
data. The article model is trained on the ESL
data using AP. The other models are trained on the
Google corpus. The last line shows the results,
when all of the five modules are included.
5 Test Results
The previous section showed the performance of
the system on the training data. In this section,
we show the results on the test set. As previously,
the performance improves when each component
is added into the final system. However, we also
note that the precision is much higher while the
recall is only slightly lower. We attribute this in-
creased precision to the observed differences in
the percentage of annotated errors in training vs.
test (see Section 3) and hypothesize that the train-
ing data may contain additional relevant errors that
were not included in the annotation.
Besides the original official annotations an-
nounced by the organizers, another set of anno-
tations is offered based on the combination of re-
vised official annotations and accepted alternative
annotations proposed by participants. We show in
Table 8 when our system is scored based on the
17
revised annotations, both the precision and the re-
call are higher. Our system achieves the highest
scores out of 17 participating teams based on both
the original and revised annotations.
Model Precision Recall F1
Scores based on the original annotations
Articles 0.48 0.11 0.18
+Prepositions 0.45 0.12 0.19
+Noun number 0.48 0.21 0.29
+Subject-verb agr. 0.48 0.22 0.30
+Verb form (All) 0.46 0.23 0.31
Scores based on the revised annotations
All 0.62 0.32 0.42
Table 8: Results on Test. The article model is
trained on the ESL data using AP. The other mod-
els are trained on the Google corpus. All denotes
the results of the complete model that includes all
of the five modules.
6 Discussion and Error Analysis
Here, we present some interesting errors that our
system makes.
6.1 Error Analysis
Incorrect verb form correction: Safety is one of
the crucial problems that many countries and com-
panies *concerned/concerns.
Here, the phrasing requires multiple changes;
to maintain the same word order, this correction
would be needed in tandem with the insertion of
the auxiliary ?have? to create a passive construc-
tion.
Incorrect determiner insertion: In this era,
Engineering designs can help to provide more
habitable accommodation by designing a stronger
material so it?s possible to create a taller and safer
building, a better and efficient sanitation system
to prevent *?/ the disease, and also by designing
a way to change the condition of the inhabitable
environment.
This example requires a model of discourse at
the level of recognizing when a specific disease
is a focus of the text, rather than disease in gen-
eral. The use of a singular construction ?a taller
and safer building? in this context is somewhat un-
conventional and potentially makes this distinction
even harder to detect.
Incorrect verb number correction:
One current human *need/needs that should
be given priority is the search for renewable re-
sources.
This appears to be the result of the system
heuristics intended to mitigate POS tagging errors
on ESL text, where the word ?need? is considered
as a candidate verb rather thana noun; this results
in an incorrect change to make the ?verb? agree in
number with the phrase ?one human?.
Incorrect determiner deletion: This had
shown that the engineering design process is es-
sential in solving problems and it ensures that the
problem is thoroughly looked into and ensure that
the engineers are generating ideas that target the
main problem, *the/? depletion and harmful fuel.
In this example, local context may suggest a list
structure, but the wider context indicates that the
comma represents an appositive structure.
6.2 Discussion
Note that the presence of multiple errors can have
very negative effects on preprocessing. For exam-
ple, when an incorrect verb form is used that re-
sults in a word form commonly used as a noun,
the outputs of the parsers tend to be incorrect. This
limits the potential of rule-based approaches.
Machine learning approaches, on the other
hand, require sufficient examples of each error
type to allow robust statistical modeling of contex-
tual features. Given the general sparsity of ESL
errors, together with the additional noise intro-
duced into more sophisticated preprocessing com-
ponents by errors with overlapping contexts, it ap-
pears hard to leverage these more sophisticated
tools to generate features for machine learning ap-
proaches. This motivates our use of just POS and
shallow parse analysis, together with language-
modeling approaches that can use counts derived
from very large native corpora, to provide robust
inputs for machine learning algorithms.
The interaction between errors suggests that
constraints could be used to improve results by en-
suring, for example, that verb number, noun num-
ber, and noun phrase determiner are consistent.
This is more difficult than it may first appear for
two reasons. First, the noun that is the subject
of the verb under consideration may be relatively
distant in the sentence (due to the presence of in-
tervening relative clauses, for example). Second,
the constraint only limits the possible correction
options: the correct number for the noun in fo-
18
cus may depend on the form used in the preceding
sentences ? for example, to distinguish between a
general statement about some type of entity, and a
statement about a specific entity.
These observations suggest that achieving very
high performance in the task of grammar correc-
tion requires sophisticated modeling of deep struc-
ture in natural language documents.
7 Conclusion
We have described our system that participated in
the shared task on grammatical error correction
and ranked first out of 17 participating teams. We
built specialized models for the five types of mis-
takes that are the focus of the competition. We
have also presented error analysis of the system
output and discussed possible directions for future
work.
Acknowledgments
This material is based on research sponsored by DARPA under agreement num-
ber FA8750-13-2-0008. The U.S. Government is authorized to reproduce and
distribute reprints for Governmental purposes notwithstanding any copyright
notation thereon. The views and conclusions contained herein are those of the
authors and should not be interpreted as necessarily representing the official
policies or endorsements, either expressed or implied, of DARPA or the U.S.
Government. This research is also supported by a grant from the U.S. Depart-
ment of Education and by the DARPA Machine Reading Program under Air
Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-018.
References
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium, Philadelphia, PA.
D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building
a large annotated corpus of learner english: The nus
corpus of learner english. In Proc. of the NAACL
HLT 2013 Eighth Workshop on Innovative Use of
NLP for Building Educational Applications, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Machine Learning.
M. Gamon. 2010. Using mostly native data to correct
errors in learners? writing. In NAACL, pages 163?
171, Los Angeles, California, June.
N. Han, M. Chodorow, and C. Leacock. 2006. De-
tecting errors in English article usage by non-native
speakers. Journal of Natural Language Engineer-
ing, 12(2):115?129.
J. Lee and S. Seneff. 2008. Correcting misuse of verb
forms. In ACL, pages 174?182, Columbus, Ohio,
June. Association for Computational Linguistics.
R. Nagata, E. Whittaker, and V. Sheinman. 2011. Cre-
ating a manually error-tagged and shallow-parsed
learner corpus. In ACL, pages 1210?1219, Portland,
Oregon, USA, June. Association for Computational
Linguistics.
H. T. Ng, S. M. Wu, Y. Wu, Ch. Hadiwinoto, and
J. Tetreault. 2013. The conll-2013 shared task
on grammatical error correction. In Proc. of the
Seventeenth Conference on Computational Natural
Language Learning. Association for Computational
Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS.
N. Rizzolo and D. Roth. 2010. Learning Based Java
for Rapid Development of NLP Systems. In LREC.
A. Rozovskaya and D. Roth. 2010a. Annotating ESL
errors: Challenges and rewards. In Proceedings of
the NAACL Workshop on Innovative Use of NLP for
Building Educational Applications.
A. Rozovskaya and D. Roth. 2010b. Generating con-
fusion sets for context-sensitive error correction. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing (EMNLP).
A. Rozovskaya and D. Roth. 2010c. Training
paradigms for correcting errors in grammar and us-
age. In NAACL.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for esl correction tasks.
In ACL.
A. Rozovskaya, M. Sammons, J. Gioja, and D. Roth.
2011. University of Illinois system in HOO text cor-
rection shared task.
A. Rozovskaya, M. Sammons, and D. Roth. 2012. The
UI system in the hoo 2012 shared task on error cor-
rection.
19
Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 34?42,
Baltimore, Maryland, 26-27 July 2014.
c?2014 Association for Computational Linguistics
The Illinois-Columbia System in the CoNLL-2014 Shared Task
Alla Rozovskaya
1
Kai-Wei Chang
2
Mark Sammons
2
Dan Roth
2
Nizar Habash
1
1
Center for Computational Learning Systems, Columbia University
{alla,habash}@ccls.columbia.edu
2
Cognitive Computation Group, University of Illinois at Urbana-Champaign
{kchang10,mssammon,danr}@illinois.edu
Abstract
The CoNLL-2014 shared task is an ex-
tension of last year?s shared task and fo-
cuses on correcting grammatical errors in
essays written by non-native learners of
English. In this paper, we describe the
Illinois-Columbia system that participated
in the shared task. Our system ranked sec-
ond on the original annotations and first on
the revised annotations.
The core of the system is based on the
University of Illinois model that placed
first in the CoNLL-2013 shared task. This
baseline model has been improved and ex-
panded for this year?s competition in sev-
eral respects. We describe our underly-
ing approach, which relates to our previ-
ous work, and describe the novel aspects
of the system in more detail.
1 Introduction
The topic of text correction has seen a lot of inter-
est in the past several years, with a focus on cor-
recting grammatical errors made by English as a
Second Language (ESL) learners. ESL error cor-
rection is an important problem since most writers
of English are not native English speakers. The in-
creased interest in this topic can be seen not only
from the number of papers published on the topic
but also from the three competitions devoted to
grammatical error correction for non-native writ-
ers that have recently taken place: HOO-2011
(Dale and Kilgarriff, 2011), HOO-2012 (Dale et
al., 2012), and the CoNLL-2013 shared task (Ng
et al., 2013).
In all three shared tasks, the participating sys-
tems performed at a level that is considered ex-
tremely low compared to performance obtained in
other areas of NLP: even the best systems attained
F1 scores in the range of 20-30 points.
The key reason that text correction is a diffi-
cult task is that even for non-native English speak-
ers, writing accuracy is very high, as errors are
very sparse. Even for some of the most com-
mon types of errors, such as article and preposi-
tion usage, the majority of the words in these cate-
gories (over 90%) are used correctly. For instance,
in the CoNLL training data, only 2% of preposi-
tions are incorrectly used. Because errors are so
sparse, it is more difficult for a system to identify a
mistake accurately and without introducing many
false alarms.
The CoNLL-2014 shared task (Ng et al., 2014)
is an extension of the CoNLL-2013 shared task
(Ng et al., 2013). Both competitions make use
of essays written by ESL learners at the National
University of Singapore. However, while the first
one focused on five kinds of mistakes that are com-
monly made by ESL writers ? article, preposition,
noun number, verb agreement, and verb form ?
this year?s competition covers all errors occurring
in the data. Errors outside the target group were
present in the task corpora last year as well, but
were not evaluated.
Our system extends the one developed by the
University of Illinois (Rozovskaya et al., 2013)
that placed first in the CoNLL-2013 competition.
For this year?s shared task, the system has been
extended and improved in several respects: we ex-
tended the set of errors addressed by the system,
developed a general approach for improving the
error-specific models, and added a joint inference
component to address interaction among errors.
See Rozovskaya and Roth (2013) for more detail.
We briefly discuss the task (Section 2) and give
an overview of the baseline Illinois system (Sec-
tion 3). Section 4 presents the novel aspects of the
system. In Section 5, we evaluate the complete
system on the development data and show the re-
sults obtained on test. We offer error analysis and a
brief discussion in Section 6. Section 7 concludes.
34
Error type Rel. freq. Examples
Article (ArtOrDet) 14.98% *?/The government should help encourage *the/?
breakthroughs as well as *a/? complete medication
system .
Wrong collocation (Wci) 11.94% Some people started to *think/wonder if electronic
products can replace human beings for better perfor-
mances .
Local redundancy (Rloc-) 10.52% Some solutions *{as examples}/? would be to design
plants/fertilizers that give higher yield ...
Noun number (Nn) 8.49% There are many reports around the internet and on
newspaper stating that some users ? *iPhone/iPhones
exploded .
Verb tense (Vt) 7.21% Through the thousands of years , most Chinese scholars
*are/{have been} greatly affected by Confucianism .
Orthography/punctuation (Mec) 6.88% Even British Prime Minister , Gordon Brown *?/, has
urged that all cars in *britain/Britain to be green by
2020 .
Preposition (Prep) 5.43% I do not agree *on/with this argument that surveillance
technology should not be used to track people .
Word form (Wform) 4.87% On the other hand , the application of surveillance tech-
nology serves as a warning to the *murders/murderers
and they might not commit more murder .
Subject-verb agreement (SVA) 3.44% However , tracking people *are/is difficult and different
from tracking goods .
Verb form (Vform) 3.25% Travelers survive in desert thanks to GPS
*guide/guiding them .
Tone (Wtone) 1.29% Hence , as technology especially in the medical field
continues to get developed and updated , people {do
n?t}/{do not} risk their lives anymore .
Table 1: Example errors. In the parentheses, the error codes used in the shared task are shown. Note
that only the errors exemplifying the relevant phenomena are marked in the table; the sentences may
contain other mistakes. Errors marked as verb form include multiple grammatical phenomena that may
characterize verbs. Our system addresses all of the error types except ?Wrong Collocation? and ?Local
Redundancy?.
2 Task Description
Both the training and the test data of the CoNLL-
2014 shared task consist of essays written by stu-
dents at the National University of Singapore. The
training data contains 1.2 million words from the
NUCLE corpus (Dahlmeier et al., 2013) corrected
by English teachers, and an additional set of about
30,000 words that was released last year as a test
set for the CoNLL-2013 shared task. We use last
year?s test data as a development set; the results in
the subsequent sections are reported on this subset.
The CoNLL corpus error tagset includes 28 er-
ror categories. Table 1 illustrates the most com-
mon error categories in the training data; errors are
marked with an asterisk, and ? denotes a missing
word. Our system targets all of these, with the ex-
ception of collocation and local redundancy errors.
Among the less commonly occurring error types,
our system addresses tone (style) errors; these are
illustrated in the table.
It should be noted that the proportion of erro-
neous instances is several times higher in the de-
velopment data than in the training data for all of
the error categories. For example, while only 2.4%
of noun phrases in the training data have deter-
miner errors, in the development data 10% of noun
phrases have determiner errors.
35
?Hence, the environmental *factor/factors also
*contributes/contribute to various difficulties,
*included/including problems in nuclear tech-
nology.?
Error type Confusion set
Noun number {factor, factors}
Verb Agreement {contribute, contributes}
Verb Form
{included, including,
includes, include}
Table 2: Sample confusion sets for noun num-
ber, verb agreement, and verb form.
3 The Baseline System
In this section, we briefly describe the Univer-
sity of Illinois system (henceforth Illinois; in the
overview paper of the shared task the system is re-
ferred to as UI) that achieved the best result in the
CoNLL-2013 shared task and which we use as our
baseline model. For a complete description, we
refer the reader to Rozovskaya et al. (2013).
The Illinois system implements five
independently-trained machine-learning clas-
sifiers that follow the popular approach to ESL
error correction borrowed from the context-
sensitive spelling correction task (Golding and
Roth, 1999; Carlson et al., 2001). A confusion
set is defined as a list of confusable words.
Each occurrence of a confusable word in text is
represented as a vector of features derived from a
context window around the target. The problem
is cast as a multi-class classification task and a
classifier is trained on native or learner data. At
prediction time, the model selects the most likely
candidate from the confusion set.
The confusion set for prepositions includes the
top 12 most frequent English prepositions (this
year, we extend the confusion set and also target
extraneous preposition usage). The article confu-
sion set is as follows: {a, the, ?}.
1
The confu-
sion sets for noun, agreement, and form modules
depend on the target word and include its morpho-
logical variants. Table 2 shows sample confusion
sets for noun, agreement, and form errors.
Each classifier takes as input the corpus doc-
uments preprocessed with a part-of-speech tag-
1
? denotes noun-phrase-initial contexts where an article
is likely to have been omitted. The variants ?a? and ?an? are
conflated and are restored later.
ger
2
and shallow parser
3
(Punyakanok and Roth,
2001). The other system components use the pre-
processing tools only as part of candidate genera-
tion (e.g., to identify all nouns in the data for the
noun classifier).
The choice of learning algorithm for each clas-
sifier is motivated by earlier findings showing
that discriminative classifiers outperform other
machine-learning methods on error correction
tasks (Rozovskaya and Roth, 2011). Thus, the
classifiers trained on the learner data make use of
a discriminative model. Because the Google cor-
pus does not contain complete sentences but only
n-gram counts of length up to five, training a dis-
criminative model is not desirable, and we thus use
NB (details in Rozovskaya and Roth (2011)).
The article classifier is a discriminative model
that draws on the state-of-the-art approach de-
scribed in Rozovskaya et al. (2012). The model
makes use of the Averaged Perceptron (AP) algo-
rithm (Freund and Schapire, 1996) and is trained
on the training data of the shared task with rich
features. The article module uses the POS and
chunker output to generate some of its features and
candidates (likely contexts for missing articles).
The original word choice (the source article)
used by the writer is also used as a feature. Since
the errors are sparse, this feature causes the model
to abstain from flagging mistakes, resulting in low
recall. To avoid this problem, we adopt the ap-
proach proposed in Rozovskaya et al. (2012), the
error inflation method, and add artificial article er-
rors to the training data based on the error distribu-
tion on the training set. This method prevents the
source feature from dominating the context fea-
tures, and improves the recall of the system.
The other classifiers in the baseline system ?
noun number, verb agreement, verb form, and
preposition ? are trained on native English data,
the Google Web 1T 5-gram corpus (henceforth,
Google, (Brants and Franz, 2006)) with the Na??ve
Bayes (NB) algorithm. All models use word n-
gram features derived from the 4-word window
around the target word. In the preposition model,
priors for preposition preferences are learned from
the shared task training data (Rozovskaya and
Roth, 2011).
The modules targeting verb agreement and
2
http://cogcomp.cs.illinois.edu/page/
software view/POS
3
http://cogcomp.cs.illinois.edu/page/
software view/Chunker
36
verb form mistakes draw on the linguistically-
motivated approach to correcting verb errors pro-
posed in Rozovskaya et. al (2014).
4 The CoNLL-2014 System
The system in the CoNLL-2014 shared task is im-
proved in three ways: 1) Additional error-specific
classifiers: word form, orthography/punctuation,
and style; 2) Model combination; and 3) Joint in-
ference to address interacting errors. Table 3 sum-
marizes the Illinois and the Illinois-Columbia sys-
tems.
4.1 Targeting Additional Errors
The Illinois-Columbia system implements several
new classifiers to address word form, orthography
and punctuation, and style errors (Table 1).
4.1.1 Word Form Errors
Word form (Wform) errors are grammatical er-
rors that involve confusing words that share a
base form but differ in derivational morphology,
e.g. ?use? and ?usage? (see also Table 1). Con-
fusion sets for word form errors thus should in-
clude words that differ derivationally but share the
same base form. In contrast to verb form errors
where confusion sets specify all possible inflec-
tional forms for a given verb, here, the associated
parts-of-speech may vary more widely. An ex-
ample of a confusion set is {technique, technical,
technology, technological}.
Because word form errors encompass a wide
range of misuse, one approach is to consider ev-
ery word as an error candidate. We follow a more
conservative method and only attempt to correct
those words that occurred in the training data and
were tagged as word form errors (we cleaned up
that list by removing noisy annotations).
A further challenge in addressing word form er-
rors is generating confusion sets. We found that
about 45% of corrections for word form errors in
the development data are covered by the confusion
sets from the training data for the same word. We
thus derive the confusion sets using the training
data. Specifically, for every source word that is
tagged as a word form error in the training data,
the confusion set includes all labels to which that
word is mapped in the training data. In addition,
plural and singular forms are added for all words
tagged as nouns, and inflectional forms are added
for words tagged as verbs. For more detail on
correcting verb errors, we refer the reader to Ro-
zovskaya et al. (2014).
4.1.2 Orthography and Punctuation Errors
The Mec error category includes errors in
spelling, context-sensitive spelling, capitalization,
and punctuation. Our system addresses punctua-
tion errors and capitalization errors.
To correct capitalization errors, we collected
words that are always capitalized in the train-
ing and development data when not occurring
sentence-initially.
The punctuation classifier includes two mod-
ules: a learned component targets missing and
extraneous comma usage and is an AP classifier
trained on the learner data with error inflation.
A second, pattern-based component, complements
the AP model: it inserts missing commas by using
a set of patterns that overwhelmingly prefer the us-
age of a comma, e.g. when a sentence starts with
the word ?hence?. The patterns are learned auto-
matically over the training data: specifically, us-
ing a sliding window of three words on each side,
we compiled a list of word n-gram contexts that
are strongly associated with the usage of a comma.
This list is then used to insert missing commas in
the test data.
4.1.3 Style Errors
The style (Wtone) errors marked in the corpus are
diverse, and the annotations are often not consis-
tent. We constructed a pattern-based system to
deal with two types of style errors that are com-
monly annotated. The first type of style edit avoids
using contractions of negated auxiliary verbs. For
example, it changes ?do n?t? to ?do not?. We use a
pattern-based classifier to identify such errors and
replace the contractions. The second type of style
edit encourages the use of a semi-colon to join
two independent clauses when a conjunctive ad-
verb is used. For example, it edits ?[clause], how-
ever, [clause]? to ?[clause]; however, [clause]?. To
identify such errors, we use a part-of-speech tag-
ger to recognize conjunctive adverbs signifying in-
dependent clauses: if two clauses are joined by the
pattern ?, [conjunctive adverb],?, we will replace it
with ?; [conjunctive adverb],?.
4.2 Modules not Included in the Final System
In addition to the modules described above, we at-
tempted to address two other common error cate-
gories: spelling errors and collocation errors. We
37
Illinois
Classifiers Training data Algorithm
Article Learner AP with inflation
Preposition Native NB-priors
Noun number Native NB
Verb agreement Native NB
Verb form Native NB
Illinois-Columbia
Classifiers Training data Algorithm
Article Learner and native AP with infl. (learner) and NB-priors (native)
Preposition Learner and native AP with infl. (learner) and NB-priors (native)
Noun number Learner and native AP with infl. (learner) and NB (native)
Verb agreement Native AP with infl. (learner) and NB (native)
Verb form Native NB-priors
Word form Native NB-priors
Orthography/punctuation Learner AP and pattern-based
Style Learner Pattern-based
Model combination Section 4.3
Global inference Section 4.4
Table 3: The baseline (Illinois) system vs. the Illinois-Columbia system. AP stands for Averaged
Perceptron, and NB stands for the Na??ve Bayes algorithm.
describe these below even though they were not
included in the final system.
Regular spelling errors are noticeable but not
very frequent, and a number are not marked in
the corpus (for example, the word ?dictronary? in-
stead of ?dictionary? is not tagged as an error). We
used an open source package ? ?Jazzy?
4
? to at-
tempt to automatically correct these errors to im-
prove context signals for other modules. However,
there are often multiple similar words that can be
proposed as corrections, and Jazzy uses phonetic
guidelines that sometimes lead to unintuitive pro-
posals (such as ?doctrinaire? for ?dictronary?). It
would be possible to extend the system with a filter
on candidate answers that uses n-grams or some
other context model to choose better candidates,
but the relatively small number of such errors lim-
its the potential impact of such a system.
Collocation errors are the second most common
error category accounting for 11.94% of all errors
in the training data (Table 1). We tried using the
Illinois context-sensitive spelling system
5
to de-
tect these errors, but this system requires prede-
fined confusion sets to detect possible errors and
to propose valid corrections. The coverage of the
pre-existing confusion sets was poor ? the system
4
http://jazzy.sourceforge.net
5
http://cogcomp.cs.illinois.edu/cssc/
could potentially correct only 2.5% of collocation
errors ? and it is difficult to generate new con-
fusion sets that generalize well, which requires a
great deal of annotated training data. The sys-
tem performance was relatively poor because it
proposed many spurious corrections: we believe
this is due to the relatively limited context it uses,
which makes it particularly susceptible to making
mistakes when there are multiple errors in close
proximity.
4.3 Model Combination
Model combination is another key extension of the
Illinois system.
In the Illinois-Columbia system, article, prepo-
sition, noun, and verb agreement errors are each
addressed via a model that combines error predic-
tions made by a classifier trained on the learner
data with the AP algorithm and those made by
the NB model trained on the Google corpus. The
AP classifiers all make use of richer sets of fea-
tures than the native-trained classifiers: the article,
noun number, and preposition classifiers employ
features that use POS information, while the verb
agreement classifier also makes use of dependency
features extracted using a parser (de Marneffe et
al., 2008). For more detail on the features used
in the agreement module, we refer the reader to
38
Rozovskaya et al. (2014). Finally, all of the AP
models use the source word of the author as a fea-
ture and, similar to the article AP classifier (Sec-
tion 3), implement the error inflation method. The
combined model generates a union of corrections
produced by the components.
We found that for every error type, the com-
bined model is superior to each of the single classi-
fiers, as it combines the advantages of both of the
classifiers so that they complement one another.
In particular, while each of the learner and native
components have similar precision, since the pre-
dictions made differ, the recall of the combined
model improves.
4.4 Joint Inference
One of the mistakes typical for Illinois system
were inconsistent predictions. Inconsistent predic-
tions occur when the classifiers address grammat-
ical phenomena that interact at the sentence level,
e.g. noun number and verb agreement. To ad-
dress this problem, the Illinois-Columbia system
makes use of global inference via an Integer Lin-
ear Programming formulation (Rozovskaya and
Roth, 2013). Note that Rozovskaya and Roth
(2013) also describe a joint learning model that
performs better than the joint inference approach.
However, the joint learning model is based on
training a joint model on the Google corpus, and
is not as strong as the individually-trained classi-
fiers of the Illinois-Columbia system that combine
predictions from two components ? NB classifiers
trained on the native data from the Google corpus
and AP models trained on the learner data (Sec-
tion 4.3).
5 Experimental Results
In Sections 3 and 4, we described the individual
system components that address different types of
errors. In this section, we show how the system
improves when each component is added into the
system. In this year?s competition, systems are
compared using F0.5 measure instead of F1. This
is because in error correction good precision is
more important than having a high recall, and the
F0.5 reflects that by weighing precision twice as
much as recall. System output is scored with the
M2 scorer (Dahlmeier and Ng, 2012).
Table 4 reports performance results of each in-
dividual classifier. In the final system, the arti-
cle, preposition, noun number, and verb agree-
Model P R F0.5
Articles (AP) 38.97 8.85 23.19
Articles (NB-priors) 47.34 6.01 19.93
Articles (Comb.) 38.73 10.93 25.67
Prep. (AP) 34.00 0.5 2.35
Prep. (NB-priors) 33.33 0.79 3.61
Prep. (Comb.) 30.06 1.17 5.13
Noun number (NB) 44.74 5.48 18.39
Noun number (AP) 82.35 0.41 2.01
Noun number (Comb.) 45.02 5.57 18.63
Verb agr. (AP) 38.56 1.23 5.46
Verb agr. (NB) 63.41 0.76 3.64
Verb agr. (Comb.) 41.09 1.55 6.75
Verb form (NB-priors) 59.26 1.41 6.42
Word form (NB-priors) 57.54 3.02 12.48
Mec (AP; patterns) 48.48 0.47 2.26
Style (patterns) 84.62 0.64 3.13
Table 4: Performance of classifiers targeting
specific errors.
Model P R F0.5
The baseline (Illinois) system
Articles 38.97 8.85 23.19
+Prepositions 39.24 9.35 23.93
+Noun number 42.13 14.83 30.79
+Subject-verb agr. 42.25 16.06 31.86
+Verb form 43.19 17.20 33.17
Model Combination
+Model combination 42.72 20.19 34.92
Additional Classifiers
+Word form 43.39 21.54 36.07
+Mec 43.70 22.04 36.52
+Style 44.22 21.54 37.09
Joint Inference
+Joint Inference 44.28 22.57 37.13
Table 5: Results on the development data. The
top part of the table shows the performance of the
baseline (Illinois) system from last year.
P R F0.5
Scores based on the original annotations
41.78 24.88 36.79
Scores based on the revised annotations
52.44 29.89 45.57
Table 6: Results on Test.
39
ment classifiers use combined models, each con-
sisting of a classifier trained on the learner data
and a classifier trained on native data. We report
performance of each such component separately
and when they are combined. The results show
that combining models boosts the performance of
each classifier: for example, the performance of
the article classifier improves by more than 2 F0.5
points. It should be noted that results are com-
puted with respect to all errors present in the data.
For this reason, recall is low.
Next, in Table 5, we show the contribution of
the novel components over the baseline system on
the development set. As described in Section 3,
the baseline Illinois system consists of five indi-
vidual components; their performance is shown in
the top part of the table. Note that although for the
development set we make use of last year?s test
set, these results are not comparable to the perfor-
mance results reported in last year?s competition
that used the F1 measure. Overall, the baseline
system achieves an F0.5 score of 33.17 on the de-
velopment set.
Then, by applying the model combination tech-
nique introduced in Section 4.3, the performance
is improved to 34.92. By adding modules to tar-
get three additional error types, the overall perfor-
mance becomes 37.09. Finally, the joint inference
technique (see Section 4.4) slightly improves the
performance further. The final system achieves an
F0.5 score of 37.13.
Table 6 shows the results on the test set provided
by the organizers. As was done previously, the
organizers also offered another set of annotations
based on the combination of revised official anno-
tations and accepted alternative annotations pro-
posed by participants. Performance results on this
set are also shown in Table 6.
6 Discussion and Error Analysis
Here, we present some interesting errors that our
system makes on the development set and discuss
our observations on the competition. We analyze
both the false positive errors and those cases that
are missed by our system.
6.1 Error Analysis
Stylistic preference Surveillance technology
such as RFID (radio-frequency identification) is
one type of examples that has currently been im-
plemented.
Here, our system proposes a change to plural
for the noun ?technology?. The gold standard
solution instead proposes a large number of cor-
rections throughout that work with the choice of
the singular ?technology?. However, using the
plural ?technologies? as proposed by the Illinois-
Columbia system is quite acceptable, and a com-
parable number of corrections would make the rest
of the sentence compatible. Note also that the
gold standard proposes the use of commas around
the phrase ?such as RFID (radio-frequency iden-
tification)?, which could also be omitted based on
stylistic considerations alone.
Word choice The high accuracy in utiliz-
ing surveillance technology eliminates the
*amount/number of disagreements among people.
The use of ?amount? versus ?number? depends
on the noun to which the term attaches. This could
conceivably be achieved by using a rule and word
list, but many such rules would be needed and each
would have relatively low coverage. Our system
does not detect this error.
Presence of multiple errors Not only the details
of location will be provided, but also may lead to
find out the root of this kind of children trading
agency and it helps to prevent more this kind of
tragedy to happen on any family.
The writer has made numerous errors in this
sentence. To determine the correct preposition in
the marked location requires at least the preced-
ing verb phrase to be corrected to ?from happen-
ing?; the extraneous ?more? after ?prevent? in turn
makes the verb phrase correction more unlikely as
it perturbs the contextual clues that a system might
learn to make that correction. Our system pro-
poses a different preposition ? ?in? ? that is better
than the original in the local context, but which is
not correct in the wider context.
Locally coherent, globally incorrect People?s
lives become from increasingly convenient to al-
most luxury, thanks to the implementation of in-
creasingly technology available for the Man?s life.
In this example, the system proposes to delete
the preposition ?from?. This correctiom improves
the local coherency of the sentence. However, the
resulting construction is not consistent with ?to al-
most luxury?, suggesting a more complex correc-
tion (changing the word ?become? to ?are going?).
40
Cascading NLP errors In this, I mean that we
can input this device implant into an animal or
birds species, for us to track their movements and
actions relating to our human research that can
bring us to a new regime.
The word ?implant? in the example sentence
has been identified as a verb by the system and
not a noun due to the unusual use as part of the
phrase ?device implant?. As a result, the system
incorrectly proposes the verb form correction ?im-
planted?.
6.2 Discussion
The error analysis suggests that there are three sig-
nificant challenges to developing a better gram-
mar correction system for the CoNLL-2014 shared
task: identifying candidate errors; modeling the
context of possible errors widely enough to cap-
ture long-distance cues where necessary; and
modeling stylistic preferences involving word
choice, selection of plural or singular, standards
for punctuation, use of a definite or indefinite arti-
cle (or no article at all), and so on. For ESL writ-
ers, the tendency for multiple errors to be made in
close proximity means that global decisions must
be made about sets of possible mistakes, and a sys-
tem must therefore have a quite sophisticated ab-
stract model to generate the basis for consistent
sets of corrections to be proposed.
7 Conclusion
We have described our system that participated in
the shared task on grammatical error correction.
The system builds on the elements of the Illinois
system that participated in last year?s shared task.
We extended and improved the Illinois system in
three key dimensions, which we presented and
evaluated in this paper. We have also presented
error analysis of the system output and discussed
possible directions for future work.
Acknowledgments
This material is based on research sponsored by DARPA un-
der agreement number FA8750-13-2-0008. The U.S. Gov-
ernment is authorized to reproduce and distribute reprints for
Governmental purposes notwithstanding any copyright nota-
tion thereon. The views and conclusions contained herein are
those of the authors and should not be interpreted as necessar-
ily representing the official policies or endorsements, either
expressed or implied, of DARPA or the U.S. Government.
This research is also supported by a grant from the U.S. De-
partment of Education and by the DARPA Machine Reading
Program under Air Force Research Laboratory (AFRL) prime
contract no. FA8750-09-C-018. The first and last authors
were partially funded by grant NPRP-4-1058-1-168 from the
Qatar National Research Fund (a member of the Qatar Foun-
dation). The statements made herein are solely the responsi-
bility of the authors.
References
T. Brants and A. Franz. 2006. Web 1T 5-gram Version
1. Linguistic Data Consortium, Philadelphia, PA.
A. J. Carlson, J. Rosen, and D. Roth. 2001. Scaling up
context sensitive text correction. In IAAI.
D. Dahlmeier and H.T. Ng. 2012. Better evaluation
for grammatical error correction. In NAACL, pages
568?572, Montr?eal, Canada, June. Association for
Computational Linguistics.
D. Dahlmeier, H.T. Ng, and S.M. Wu. 2013. Building
a large annotated corpus of learner english: The nus
corpus of learner english. In Proc. of the NAACL
HLT 2013 Eighth Workshop on Innovative Use of
NLP for Building Educational Applications, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
R. Dale and A. Kilgarriff. 2011. Helping Our Own:
The HOO 2011 pilot shared task. In Proceedings of
the 13th European Workshop on Natural Language
Generation.
R. Dale, I. Anisimoff, and G. Narroway. 2012. A
report on the preposition and determiner error cor-
rection shared task. In Proc. of the NAACL HLT
2012 Seventh Workshop on Innovative Use of NLP
for Building Educational Applications, Montreal,
Canada, June. Association for Computational Lin-
guistics.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding contradic-
tions in text. In ACL.
Yoav Freund and Robert E. Schapire. 1996. Experi-
ments with a new boosting algorithm. In Proc. 13th
International Conference on Machine Learning.
A. R. Golding and D. Roth. 1999. A Winnow
based approach to context-sensitive spelling correc-
tion. Machine Learning.
H.T. Ng, S.M. Wu, Y. Wu, C. Hadiwinoto, and
J. Tetreault. 2013. The conll-2013 shared task
on grammatical error correction. In Proceedings of
the Seventeenth Conference on Computational Nat-
ural Language Learning: Shared Task, pages 1?12,
Sofia, Bulgaria, August. Association for Computa-
tional Linguistics.
41
H. T. Ng, S. M. Wu, T. Briscoe, C. Hadiwinoto, R. H.
Susanto, and C. Bryant. 2014. The CoNLL-2014
shared task on grammatical error correction. In Pro-
ceedings of the Eighteenth Conference on Compu-
tational Natural Language Learning: Shared Task,
Baltimore, Maryland, USA, June. Association for
Computational Linguistics.
V. Punyakanok and D. Roth. 2001. The use of classi-
fiers in sequential inference. In NIPS.
A. Rozovskaya and D. Roth. 2011. Algorithm selec-
tion and model adaptation for esl correction tasks.
In ACL.
A. Rozovskaya and D. Roth. 2013. Joint learning
and inference for grammatical error correction. In
EMNLP, 10.
A. Rozovskaya, M. Sammons, and D. Roth. 2012.
The UI system in the HOO 2012 shared task on er-
ror correction. In Proc. of the Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL) Workshop
on Innovative Use of NLP for Building Educational
Applications.
A. Rozovskaya, K.-W. Chang, M. Sammons, and
D. Roth. 2013. The University of Illinois system
in the CoNLL-2013 shared task. In CoNLL Shared
Task.
A. Rozovskaya, D. Roth, and V. Srikumar. 2014. Cor-
recting grammatical verb errors. In EACL.
42
