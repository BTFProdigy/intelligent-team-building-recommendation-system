Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1034?1043, Prague, June 2007. c?2007 Association for Computational Linguistics
Validation and Evaluation of Automatically Acquired Multiword
Expressions for Grammar Engineering
Aline Villavicencio??, Valia Kordoni?, Yi Zhang?,
Marco Idiart? and Carlos Ramisch?
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?Department of Computer Sciences, Bath University (UK)
?Department of Computational Linguistics, Saarland University, and DFKI GmbH (Germany)
?Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
avillavicencio@inf.ufrgs.br, {yzhang,kordoni}@coli.uni-sb.de
idiart@if.ufrgs.br, ceramisch@inf.ufrgs.br
Abstract
This paper focuses on the evaluation of meth-
ods for the automatic acquisition of Multiword
Expressions (MWEs) for robust grammar engi-
neering. First we investigate the hypothesis that
MWEs can be detected by the distinct statistical
properties of their component words, regardless
of their type, comparing 3 statistical measures:
mutual information (MI), ?2 and permutation
entropy (PE). Our overall conclusion is that at
least two measures, MI and PE, seem to differen-
tiate MWEs from non-MWEs. We then investi-
gate the influence of the size and quality of differ-
ent corpora, using the BNC and the Web search
engines Google and Yahoo. We conclude that, in
terms of language usage, web generated corpora
are fairly similar to more carefully built corpora,
like the BNC, indicating that the lack of con-
trol and balance of these corpora are probably
compensated by their size. Finally, we show a
qualitative evaluation of the results of automat-
ically adding extracted MWEs to existing lin-
guistic resources. We argue that such a process
improves qualitatively, if a more compositional
approach to grammar/lexicon automated exten-
sion is adopted.
1 Introduction
The task of automatically identifying Multiword
Expressions (MWEs) like phrasal verbs (break
down) and compound nouns (coffee machine)
using statistical measures has been the focus
of considerable investigative effort, (e.g. Pearce
(2002), Evert and Krenn (2005) and Zhang et
al. (2006)). Given the heterogeneousness of
the different phenomena that are considered to
be MWEs, there is no consensus about which
method is best suited for which type of MWE,
and if there is a single method that can be suc-
cessfully used for any kind of MWE.
Another difficulty for work on MWE identifi-
cation is that of the evaluation of the results ob-
tained (Pearce, 2002; Evert and Krenn, 2005),
starting from the lack of consensus about a pre-
cise definition for MWEs (Villavicencio et al,
2005).
In this paper we investigate some of the is-
sues involved in the evaluation of automatically
extracted MWEs, from their extraction to their
subsequent use in an NLP task. In order to do
that, we present a discussion of different statisti-
cal measures, and the influence that the size and
quality of different data sources have. We then
perform a comparison of these measures and dis-
cuss whether there is a single measure that has
good overall performance for MWEs in general,
regardless of their type. Finally, we perform a
qualitative evaluation of the results of adding
automatically extracted MWEs to a linguistic
resource, taking as basis for the evaluation the
approach proposed by Zhang et al (2006). We
argue that such results can improve in quality
if a more compositional approach to MWE en-
coding is adopted for the grammar extension.
Having more accurate means of deciding for an
appropriate method for identifying and incor-
porating MWEs is critical for maintaining the
quality of linguistic resources for precise NLP.
This paper starts with a discussion of MWEs
(? 2), of their coverage in linguistic resources
(? 3), and of some methods proposed for auto-
matically identifying them (? 4). This is fol-
lowed by a detailed investigation and compar-
ison of measures for MWE identification (? 5).
1034
After that we present an approach for predicting
appropriate lexico-syntactic categories for their
inclusion in a linguistic resource, and an evalu-
ation of the results in a parsing task(? 7). We
finish with some conclusions and discussion of
future work.
2 Multiword Expressions
The term Multiword Expressions has been used
to describe expressions for which the syntactic or
semantic properties of the whole expression can-
not be derived from its parts (Sag et al, 2002),
including a large number of related but distinct
phenomena, such as phrasal verbs (e.g. come
along), nominal compounds (e.g. frying pan),
institutionalised phrases (e.g. bread and butter),
and many others. Jackendoff (1997) estimates
the number of MWEs in a speaker?s lexicon to
be comparable to the number of single words.
However, due to their heterogeneous character-
istics,MWEs present a tough challenge for both
linguistic and computational work (Sag et al,
2002). For instance, some MWEs are fixed, and
do not present internal variation, such as ad hoc,
while others allow different degrees of internal
variability and modification, such as spill beans
(spill several/musical/mountains of beans).
Sag et al (2002) discuss two main ap-
proaches commonly employed in NLP for treat-
ing MWEs: the words-with-spaces approach
models an MWE as a single lexical entry and it
can adequately capture fixed MWEs like by and
large. A compositional approach treats MWEs
by general and compositional methods of lin-
guistic analysis, being able to capture more syn-
tactically flexible MWEs, like rock boat, which
cannot be satisfactorily captured by a words-
with-spaces approach, since it would require lex-
ical entries to be added for all the possible
variations of an MWE (e.g. rock/rocks/rocking
this/that/his... boat). Therefore, to provide a
unified account for the detection and encoding
of these distinct but related phenomena is a real
challenge for NLP systems.
3 Grammar and Lexicon Coverage in
Deep Processing
Many NLP tasks and applications, like Parsing
and Machine Translation, depend on large-scale
linguistic resources, such as electronic dictionar-
ies and grammars for precise results. Several
substantial resources exist: e.g., hand-crafted
large-scale grammars like the English Resource
Grammar (ERG - Flickinger (2000)) and the
Dutch Alpino Grammar (Bouma et al, 2001).
Unfortunately, the construction of these re-
sources is the manual result of human efforts and
therefore likely to contain errors of omission and
commission (Briscoe and Carroll, 1997). Fur-
thermore, due to the open-ended and dynamic
nature of languages, such linguistic resources are
likely to be incomplete, and manual encoding of
new entries and constructions is labour-intensive
and costly.
Take, for instance, the coverage test results
for the ERG (a broad-coverage precision HPSG
grammar for English) on the British National
Corpus (BNC). Baldwin et al (2004), among
many others, have investigated the main causes
of parse failure, parsing a random sample of
20,000 strings from the written component of
the BNC using the ERG. They have found that
the large majority of failures is caused by miss-
ing lexical entries, with 40% of the cases, and
missing constructions, with 39%, where missing
MWEs accounted for 8% of total errors. That is,
even by a margin, the lexical coverage is lower
than the grammar construction coverage.
This indicates the acute need for robust (semi-
)automated ways of acquiring lexical informa-
tion for MWEs, and this is the one of the goals
of this work. In the next section we discuss
some approaches that have been developed in re-
cent years to (semi-)automatically detect and/or
repair lexical and grammar errors in linguistic
grammars and/or extend their coverage.
4 Acquiring MWEs
The automatic acquisition of specific types of
MWE has attracted much interest (Pearce,
2002; Baldwin and Villavicencio, 2002; Evert
and Krenn, 2005; Villavicencio, 2005; van der
1035
Beek, 2005; Nicholson and Baldwin, 2006). For
instance, Baldwin and Villavicencio (2002) pro-
posed a combination of methods to extract Verb-
Particle Constructions (VPCs) from unanno-
tated corpora, that in an evaluation on the
Wall Street Journal achieved 85.9% precision
and 87.1% recall. Nicholson and Baldwin (2006)
investigated the prediction of the inherent se-
mantic relation of a given compound nominaliza-
tion using as statistical measure the confidence
interval.
On the other hand, Zhang et al (2006) looked
at MWEs in general investigating the semi-
automated detection of MWE candidates in
texts using error mining techniques and vali-
dating them using a combination of the World
Wide Web as a corpus and some statistical mea-
sures. 6248 sentences were then extracted from
the BNC; these contained at least one of the 311
MWE candidates verified with World Wide Web
in the way described in Zhang et al (2006). For
each occurrence of the MWE candidates in this
set of sentences, the lexical type predictor pro-
posed in Zhang and Kordoni (2006) predicted a
lexical entry candidate. This resulted in 373 ad-
ditional MWE lexical entries for the ERG gram-
mar using a words-with-spaces approach. As re-
ported in Zhang et al (2006), this addition to
the grammar resulted in a significant increase in
grammar coverage of 14.4%. However, no fur-
ther evaluation was done of the results of the
measures used on the identification of MWEs or
of the resulting grammar, as not all MWEs can
be correctly handled by the simple words-with-
spaces approach (Sag et al, 2002). And these
are the starting points of the work we are re-
porting on here.
5 Evaluation of the Identification of
MWEs
One way of viewing the MWE identification task
is, given a list of sequences of words, to distin-
guish those that are genuine MWEs (e.g. in the
red), from those that are just sequences of words
that do not form any kind of meaningful unit
(e.g. of alcohol and). In order to do that, one
commonly used approach is to employ statisti-
cal measures (e.g. Pearce (2002) for collocations
and Zhang et al (2006) for MWEs in general).
When dealing with statistical analysis there are
two important statistical questions that should
be addressed: How reliable is the corpus used?
and How precise is the chosen statistical measure
to distinguish the phenomena studied?.
In this section we look at these issues, for the
particular case of trigrams, by testing different
corpora and different statistical measures. For
that we use 1039 trigrams that are the output
of Zhang et al (2006) error mining system, and
frequencies collected from the BNC and from
the World Wide Web. The former were col-
lected from two different portions of the BNC,
namely the fragment of the BNC (BNCf ) used
in the error-mining experiments, and the com-
plete BNC (from the site http://pie.usna.edu/),
to test whether a larger sample of a more ho-
mogeneous and well balanced corpus improves
results significantly. For the latter we used two
different search engines: Google and Yahoo, and
the frequencies collected reflect the number of
pages that had exact matches of the n-grams
searched, using the API tools for each engine.
5.1 Comparing Corpora
A corpus for NLP related work should be a re-
liable sample of the linguistic output of a given
language. For this work in particular, we expect
that the relative ordering in frequency for differ-
ent n-grams is preserved across corpora, in the
same domain (e.g. a corpus of chemistry arti-
cles). For, if this is not the case, different con-
clusions are certain to be drawn from different
corpora.
The first test we performed was a direct com-
parison of the rank plots of the relative fre-
quency of trigrams for the four corpora. We
ranked 1039 MWE-candidate trigrams accord-
ing to their occurrence in each corpus and we
normalised this value by the total number of
times any one of the 1039 trigrams appeared
for each corpus. These normalisation values
were: 66,101 times in BNCf , 322,325 in BNC,
224,479,065 in Google and 6,081,786,313 in Ya-
hoo. It is possible to have an estimate of the size
of each corpus from these numbers: the trigrams
1036
account for something like 0.3% of the BNC cor-
pora, while for Google and Yahoo nothing can
be said since their sizes are not reliable numbers.
Figure 1 displays the results. The overall rank-
ing distribution is very similar for these corpora
showing the expected Zipf like behaviour in spite
of their different sizes.
10-5
10-4
10-3
10-2
10-1
1 10 100 1000
re
la
tiv
e 
fre
qu
en
cy
rank
BNCf
BNC
Google
Yahoo
Figure 1: Relative frequency rank for the 1039
trigrams analysed.
Of course, the information coming from Fig-
ure 1 is not sufficient for our purposes. The or-
der of the trigrams could be very different inside
each corpus. Therefore a second test is needed
to compare the rankings of the n-grams in each
corpus. In order to do that we measure the
Kendall?s ? scores between corpora. Kendall?s ?
is a non-parametric method for estimating cor-
relation between datasets (Press et al, 1992).
For the number of trigrams studied here the
Kendall?s scores obtained imply a significant cor-
relation between the corpora with p<0.000001.
The significance indicates that the data are cor-
related and the null hypothesis of statistical
independence is certainly disproved. Unfortu-
nately disproving the null hypothesis does not
give much information about the degree of cor-
relation; it only asserts that it exists. Thus, it
could be a very insignificant correlation. In ta-
ble 1, we display a more intuitive measure to
estimate the correlation, the probability Q that
any 2 trigrams chosen from two corpora have
the same relative ordering in frequency. This
probability is related to Kendall?s ? through the
expression Q = (1 + ?)/2 .
BNC Google Yahoo
BNCf 0.81 0.73 0.78
BNC 0.73 0.77
Google 0.86
Table 1: The probability Q of 2 trigrams hav-
ing the same frequency rank order for different
corpora.
The results show that the four corpora are
certainly correlated, and can probably be used
interchangeably to access most of the statisti-
cal properties of the trigrams. Interestingly, a
higher correlation was observed between Yahoo
and Google than between BNCf and BNC, even
though BNCf is a fragment of BNC, and there-
fore would be expected to have a very high cor-
relation. This suggests that as corpora sizes
increase, so do the correlations between them,
meaning that they are more likely to agree on
the ranking of a given MWE.
5.2 Comparing statistical measures -
are they equivalent?
Here we concentrate on a single corpus, BNCf ,
and compare the three statistical measures for
MWE identification: Mutual Information (MI),
?2 and Permutation Entropy (PE)(Zhang et al,
2006), to investigate if they order the trigrams
in the same fashion.
MI and ?2 are typical measures of associa-
tion that compare the joint probability of occur-
rence of a certain group of events p(abc) with
a prediction derived from the null hypothesis
of statistical independence between these events
p?(abc) = p(a)p(b)p(c) (Press et al, 1992). In
our case the events are the occurrences of words
in a given position in an n-gram. For a trigram
with words w1w2w3, ?2 is calculated as:
?2 =
?
a,b,c
[ n(abc)? n?(abc) ]2
n?(abc)
where a corresponds either to the word w1 or to
?w1 (all but the word w1) and so on. n(abc)
is the number of trigrams abc in the corpus,
n?(abc) = n(a)n(b)n(c)/N2 is the predicted
number from the null hypothesis, n(a) is the
1037
number of unigrams a, and N the number of
words in the corpus. Mutual Information, in
terms of these numbers, is:
MI =
?
a,b,c
n(abc)
N log2
[ n(abc)
n?(abc)
]
The third measure, permutation entropy, is a
measure of order association. Given the words
w1, w2, and w3, PE is calculated in this work as:
PE = ?
?
(i,j,k)
p(wiwjwk) ln [ p(wiwjwk) ]
where the sum runs over all the permutations
of the indexes and, therefore, over all possible
positions of the selected words in the trigram.
The probabilities are estimated from the number
of occurrences of each permutation of a trigram
(e.g. by and large, large by and, and large by,
and by large, large and by, and by large and) as:
p(w1w2w3) =
n(w1w2w3)
?
(i,j,k)
n(wiwjwk)
PE was proposed by Zhang et al (2006) as a
possible measure to detect MWEs, under the
hypothesis that MWEs are more rigid to per-
mutations and therefore present smaller PEs.
Even though it is quite different from MI and
?2, PE can also be thought as an indirect mea-
sure of statistical independence, since the more
independent the words are the closer PE is from
its maximal value (ln 6, for trigrams). One pos-
sible advantage of this measure over the others
is that it does not rely on single word counts,
which are less accurate in Web based corpora.
Given the rankings produced for each one of
these three measures we again use Kendall?s ?
test to assess correlation and its significance.
Table 2 displays the Q probability of finding
the same ordering in these three measures. The
general conclusion from the table is that even
though there is statistical significance in the cor-
relations found (the p values are not displayed,
but they are very low as before) the differ-
ent measures order the trigrams very differently.
There is a 70% chance of getting the same order
from MI and ?2, but it is safe to say that these
measures are very different from the PE, since
their Q values are very close to pure chance.
MI??2 MI?PE ?2?PE
Q 0.71 0.55 0.45
Table 2: The probability Q of having 2 trigrams
with the same rank order for different statistical
measures.
5.3 Comparing Statistical Measures -
are they useful?
The use of statistical measures is widespread in
NLP but there is no consensus about how good
these measures are for describing natural lan-
guage phenomena. It is not clear what exactly
they capture when analysing the data.
In order to evaluate if they would make good
predictors for MWEs, we compare the measures
distributions for MWEs and non-MWEs. For
that we selected as gold standard a set of around
400 MWE candidates annotated by a native
speaker1 as MWEs or not. We then calculated
the histograms for the values of MI, ?2 and
PE for the two groups. MI and ?2 were cal-
culated only for BNCf . Table 3 displays the re-
sults of the Kolmogorov-Smirnof test (Press et
al., 1992) for these histograms, where the first
value is Kolmogorov-Smirnov D value (D?[0,1]
and large D values indicate large differences be-
tween distributions) and the second is the signif-
icance probability (p) associated to D given the
sizes of the data sets, in this case 90 for MWEs
and 292 for non-MWEs.
MIBNCf ?2BNCf PEY ahoo PEGoogle
D 0.27 0.13 0.27 0.24
p< 0.0001 0.154 0.0001 0.0005
Table 3: Comparison of MI, ?2 and PE
The surprising result is that there is no statis-
tical significance, at least using the Kolmogorov-
Smirnov test, that indicates that being or not
an MWE has some effect in the value of the tri-
gram?s ?2. The same does not happen for MI
or PE. They do seem to differentiate between
MWEs and non-MWEs. As discussed before the
statistical significance implies the existence of an
1The native speaker is a linguist expert in MWEs.
1038
effect but has very little to say about the inten-
sity of the effect. As in the case of this work our
interest is to use the effect to predict MWEs,
the intensity is very important. In the figures
that follow we show the normalised histograms
for MI, ?2(for the BNCf ) and PE (for the case
of Yahoo) for MWEs and non-MWEs. The ideal
scenario would be to have non overlapping dis-
tributions for the two cases, so a simple thresh-
old operation would be enough to distinguish
MWEs. This is not the case in any of the plots.
Starting from Figure 3 it clearly illustrates the
negative result for ?2 in table 3. The other two
distributions show a visible effect in the form of
a slight displacement of the distributions to the
left for MWEs. In particular for the distribution
of PE, the large peak on the right, representing
the n-grams whose word order is irrelevant with
respect to its occurrence, has an important re-
duction for MWEs.
The statistical measures discussed here are
all different forms of measuring correlations be-
tween the component words of MWEs. There-
fore, as some types of MWEs may have stronger
constraints on word order, we believe that more
visible effects can be seen in these measures if we
look at their application for individual types of
MWEs, which is planned for future work. This
will bring an improvement to the power of MWE
prediction of these measures.
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 0.2
-5.5 -5 -4.5 -4 -3.5 -3 -2.5 -2
Pr
ob
ab
ilit
y
log(MI)
MWEs
non-MWEs
Figure 2: Normalised histograms of MI values
for MWEs and non-MWEs in BNCf .
 0
 0.02
 0.04
 0.06
 0.08
 0.1
 0.12
 0.14
 0.16
 0.18
 2  3  4  5  6  7  8
Pr
ob
ab
ilit
y
log(?2)
MWEs
non-MWEs
Figure 3: Normalised histograms of ?2 values
for MWEs and non-MWEs in BNCf .
 0
 0.05
 0.1
 0.15
 0.2
 0.25
-3.5 -3 -2.5 -2 -1.5 -1 -0.5  0  0.5
Pr
ob
ab
ilit
y
log(PE(Yahoo))
MWEs
non-MWEs
Figure 4: Normalised histograms of PE values
for MWEs and non-MWEs in Yahoo.
6 Evaluation of the Extensions to
the Grammar
Our ultimate goal is to maximally automate
the process of discovering and handling MWEs.
With good statistical measures, we are able
to distinguish genuine MWE from non-MWEs
among the n-gram candidates. However, from
the perspective of grammar engineering, even
with a good candidate list of MWEs, great ef-
fort is still required in order to incorporate such
word units into a given grammar automatically
and in a precise way.
Zhang et al (2006) tried a simple ?word with
spaces? approach. By acquiring new lexical en-
tries for the MWEs candidates validated by the
statistical measures, the grammar coverage was
shown to improve significantly. However, no fur-
ther investigation on the parser accuracy was re-
ported there.
Taking a closer look at the MWE candidates
1039
proposed, we find that only a small proportion of
them can be handled appropriately by the?word
with spaces? approach of Zhang et al (2006).
Simply adding new lexical entries for all MWEs
can be a workaround for enhancing the parser
coverage, but the quality of the parser output is
clearly linguistically less interesting.
On the other hand, we also find that a large
proportion of MWEs that cannot be correctly
handled by the grammar can be covered prop-
erly in a constructional way by adding one lex-
ical entry for the head (governing) word of the
MWE. For example, the expression foot the bill
will be correctly handled with a standard head-
complement rule, if there is a transitive verb
reading for the word foot in the lexicon. Some
other examples are: to put forward, the good of,
in combination with, . . . , where lexical exten-
sion to the words in bold will allow the gram-
mar to cover these MWEs. In this paper, we
employ a constructional approach for the acqui-
sition of new lexical entries for the head words
of the MWEs.2
It is arguable that such an approach may lead
to some potential grammar overgeneration, as
there is no selectional restriction expressed in
the new lexical entry. However, as far as the
parsing task is concerned, such overgeneration
is not likely to reduce the accuracy of the gram-
mar significantly as we show later in this paper
through a thorough evaluation.
6.1 Experimental Setup
With the complete list of 1039 MWE candidates
discussed in section 5, we rank each n-gram
according to each of the three statistical mea-
sures. The average of all the rankings is used
as the combined measure of the MWE candi-
dates. Since we are only interested in acquiring
new lexical entries for MWEs which are not cov-
ered by the grammar, we used the error mining
results (Zhang et al, 2006; van Noord, 2004)
to only keep those candidates with parsability
? 0.1. The top 30 MWE candidates are used in
2The combination of the ?word with space? approach
of Zhang et al (2006) with the constructional approach
we propose here is an interesting topic that we want to
investigate in future research.
this experiment.
We used simple heuristics in order to extract
the head words from these MWEs:
? the n-grams are POS-tagged with an auto-
matic tagger;
? finite verbs in the n-grams are extracted as
head words;
? nouns are also extracted if there is no verb
in the n-gram.
Occasionally, the tagger errors might introduce
wrong head words. However, the lexical type
predictor of Zhang and Kordoni (2006) that we
used in our experiments did not generate inter-
esting new entries for them in the subsequent
steps, and they were thus discarded, as discussed
below.
With the 30 MWE candidates, we extracted
a sub-corpus from the BNC with 674 sentences
which included at least one of these MWEs. The
lexical acquisition technique described in Zhang
and Kordoni (2006) was used with this sub-
corpus in order to acquire new lexical entries for
the head words. The lexical acquisition model
was trained with the Redwoods treebank (Oepen
et al, 2002), following Zhang et al (2006).
The lexical prediction model predicted for
each occurrence of the head words a most plau-
sible lexical type in that context. Only those
predictions that occurred 5 times or more were
taken into consideration for the generation of the
new lexical entries. As a result, we obtained 21
new lexical entries.
These new lexical entries were later merged
into the ERG lexicon. To evaluate the grammar
performance with and without these new lexical
entries, we
1. parsed the sub-corpus with/without new
lexical entries and compared the grammar
coverage;
2. inspected the parser output manually and
evaluated the grammar accuracy.
In parsing the sub-corpus, we used the PET
parser (Callmeier, 2001). For the manual eval-
1040
uation of the parser output, we used the tree-
banking tools of the [incr tsdb()] system (Oepen,
2001).
6.2 Grammar Performance
Table 4 shows that the grammar coverage im-
proved significantly (from 7.1% to 22.7%) with
the acquired lexical entries for the head words
of the MWEs. This improvement in coverage
is largely comparable to the result reported in
(Zhang et al, 2006), where the coverage was re-
ported to raise from 5% to 18% with the ?word
with spaces? approach (see also section 4).
It is also worth mentioning that Zhang et al
(2006) added 373 new lexical entries for a to-
tal of 311 MWE candidates, with an average
of 1.2 entries per MWE. In our experiment, we
achieved a similar coverage improvement with
only 21 new entries for 30 different MWE candi-
dates, with an average of 0.7 entries per MWE.
This suggests that the lexical entries acquired
in our experiment are of much higher linguistic
generality.
To evaluate the grammar accuracy, we man-
ually checked the parser outputs for the sen-
tences in the sub-corpus which received at least
one analysis from the grammar before and af-
ter the lexical extension. Before the lexical ex-
tension, 48 sentences are parsed, among which
32 (66.7%) sentences contain at least one cor-
rect reading (table 4). After adding the 21 new
lexical entries, 153 sentences are parsed, out of
which 124 (81.0%) sentences contain at least one
correct reading.
Baldwin et al (2004) reported in an earlier
study that for BNC data, about 83% of the sen-
tences covered by the ERG have a correct parse.
In our experiment, we observed a much lower
accuracy on the sub-corpus of BNC which con-
tains a lot of MWEs. However, after the lexical
extension, the accuracy of the grammar recovers
to the normal level.
It is also worth noticing that we did not re-
ceive a larger average number of analyses per
sentence (table 4), as it was largely balanced
by the significant increase of sentences covered
by the new lexical entries. We also found
that the disambiguation model as described by
Toutanova et al (2002) performed reasonably
well, and the best analysis is ranked among top-
5 for 66% of the cases, and top-10 for 75%.
All of these indicate that our approach of lexi-
cal acquisition for head words of MWEs achieves
a significant improvement in grammar coverage
without damaging the grammar accuracy. Op-
tionally, the grammar developers can check the
validity of the lexical entries before they are
added into the lexicon. Nonetheless, even a
semi-automatic procedure like this can largely
reduce the manual work of grammar writers.
7 Conclusions
In this paper we looked at some of the issues
involved in the evaluation of the identification
of MWEs. In particular we evaluated the use
of three statistical measures for automatically
identifying MWEs. The results suggest that at
least two of them (MI and PE) can distinguish
MWEs. In terms of the corpora used, a sur-
prisingly higher level of agreement was found
between different corpora (Google and Yahoo)
than between two fragments of the same one.
This tells us two lessons. First that even though
Google and Yahoo were not carefully built to be
language corpora their sizes compensate for that
making them fairly good samples of language
usage. Second, a fraction of a smaller well bal-
anced corpus may not necessarily be as balanced
as the whole.
Furthermore, we argued that for precise gram-
mar engineering it is important to perform a
careful evaluation of the effects of including au-
tomatically acquired MWEs to a grammar. We
looked at the evaluation of the effects in cover-
age, size of the grammar and accuracy of the
parses after adding the MWE-candidates. We
adopted a compositional approach to the en-
coding of MWEs, using some heuristics to de-
tect the head of an MWE, and this resulted in
a smaller grammar than that by Zhang et al
(2006), still achieving a similar increase in cov-
erage and maintaining a high level of accuracy of
parses, comparable to that reported by Baldwin
et al (2004).
The statistical measures are currently only
1041
item # parsed # avg. analysis # coverage %
ERG 674 48 335.08 7.1%
ERG + MWE 674 153 285.01 22.7%
Table 4: ERG coverage with/without lexical acquisition for the head words of MWEs
used in a preprocessing step to filter the non-
MWEs for the lexical type predictor. Alterna-
tively, the statistical outcomes can be incorpo-
rated more tightly, i.e. to combine with the lex-
ical type predictor and give confidence scores on
the resulting lexical entries. These possibilities
will be explored in future work.
References
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: A case study on verb-
particles. In Proc. of the 6th Conference on Nat-
ural Language Learning (CoNLL-2002), Taipei,
Taiwan.
Timothy Baldwin, Emily M. Bender, Dan Flickinger,
Ara Kim, and Stephan Oepen. 2004. Road-testing
the English Resource Grammar over the British
National Corpus. In Proceedings of the Fourth
International Conference on Language Resources
and Evaluation (LREC 2004), Lisbon, Portugal.
Gosse Bouma, Gertjan van Noord, and Robert Mal-
ouf. 2001. Alpino: Wide-coverage computational
analysis of dutch. In Computational Linguistics in
The Netherlands 2000.
Ted Briscoe and John Carroll. 1997. Automatic
extraction of subcategorization from corpora. In
Fifth Conference on Applied Natural Language
Processing, Washington, USA.
Ulrich Callmeier. 2001. Efficient parsing with large-
scale unification grammars. Master?s thesis, Uni-
versita?t des Saarlandes, Saarbru?cken, Germany.
Stefan Evert and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of sta-
tistical association measures. Computer Speech
and Language, 19(4):450?466.
Dan Flickinger. 2000. On building a more efficient
grammar by exploiting types. Natural Language
Engineering, 6(1):15?28.
Ray Jackendoff. 1997. Twistin? the night away. Lan-
guage, 73:534?59.
Jeremy Nicholson and Timothy Baldwin. 2006. In-
terpretation of compound nominalisations using
corpus and web statistics. In Proceedings of the
Workshop on Multiword Expressions: Identifying
and Exploiting Underlying Properties, pages 54?
61, Sydney, Australia. Association for Computa-
tional Linguistics.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and
Thorsten Brants. 2002. The LinGO Redwoods
treebank: Motivation and preliminary applica-
tions. In Proceedings of COLING 2002: The 17th
International Conference on Computational Lin-
guistics: Project Notes, Taipei.
Stephan Oepen. 2001. [incr tsdb()] ? competence
and performance laboratory. User manual. Tech-
nical report, Computational Linguistics, Saarland
University, Saarbru?cken, Germany.
Darren Pearce. 2002. A comparative evaluation of
collocation extraction techniques. In Third Inter-
national Conference on Language Resources and
Evaluation, Las Palmas, Canary Islands, Spain.
William H. Press, Saul A. Teukolsky, William T. Vet-
terling, and Brian P. Flannery. 1992. Numerical
Recipes in C: The Art of Scientific Computing.
Second edition. Cambridge University Press.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multiword
expressions: A pain in the neck for NLP. In Pro-
ceedings of the 3rd International Conference on In-
telligent Text Processing and Computational Lin-
guistics (CICLing-2002), pages 1?15, Mexico City,
Mexico.
Kristina Toutanova, Christoper D. Manning, Stu-
art M. Shieber, Dan Flickinger, and Stephan
Oepen. 2002. Parse ranking for a rich HPSG
grammar. In Proceedings of the First Workshop
on Treebanks and Linguistic Theories (TLT2002),
pages 253?263, Sozopol, Bulgaria.
Leonoor van der Beek. 2005. The extraction of
determinerless pps. In Proceedings of the ACL-
SIGSEM Workshop on The Linguistic Dimensions
of Prepositions and their Use in Computational
Linguistics Formalisms and Applications, Colch-
ester, UK.
Gertjan van Noord. 2004. Error mining for wide-
coverage grammar engineering. In Proceedings of
1042
the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL?04), Main Volume, pages
446?453, Barcelona, Spain, July.
Aline Villavicencio, Francis Bond, Anna Korhonen,
and Diana McCarthy. 2005. Introduction to the
special issue on multiword expressions: having a
crack at a hard nut. Journal of Computer Speech
and Language Processing, 19(4):365?377.
Aline Villavicencio. 2005. The availability of verb-
particle constructions in lexical resources: How
much is enough? Journal of Computer Speech
and Language Processing, 19.
Yi Zhang and Valia Kordoni. 2006. Automated deep
lexical acquisition for robust open texts process-
ing. In Proceedings of the Fifth International
Conference on Language Resources and Evalua-
tion (LREC 2006), Genoa, Italy.
Yi Zhang, Valia Kordoni, Aline Villavicencio, and
Marco Idiart. 2006. Automated multiword ex-
pression prediction for grammar engineering. In
Proceedings of the Workshop on Multiword Ex-
pressions: Identifying and Exploiting Underlying
Properties, pages 36?44, Sydney, Australia. Asso-
ciation for Computational Linguistics.
1043
Coling 2010: Poster Volume, pages 1041?1049,
Beijing, August 2010
Web-based and combined language models:
a case study on noun compound identification
Carlos Ramisch?? Aline Villavicencio? Christian Boitet?
? GETALP ? Laboratory of Informatics of Grenoble, University of Grenoble
? Institute of Informatics, Federal University of Rio Grande do Sul
{ceramisch,avillavicencio}@inf.ufrgs.br Christian.Boitet@imag.fr
Abstract
This paper looks at the web as a corpus
and at the effects of using web counts
to model language, particularly when we
consider them as a domain-specific versus
a general-purpose resource. We first com-
pare three vocabularies that were ranked
according to frequencies drawn from
general-purpose, specialised and web cor-
pora. Then, we look at methods to com-
bine heterogeneous corpora and evaluate
the individual and combined counts in the
automatic extraction of noun compounds
from English general-purpose and spe-
cialised texts. Better n-gram counts can
help improve the performance of empiri-
cal NLP systems that rely on n-gram lan-
guage models.
1 Introduction
Corpora have been extensively employed in sev-
eral NLP tasks as the basis for automatically
learning models for language analysis and gener-
ation. In theory, data-driven (empirical or statis-
tical) approaches are well suited to take intrinsic
characteristics of human language into account. In
practice, external factors also determine to what
extent they will be popular and/or effective for a
given task, so that they have shown different per-
formances according to the availability of corpora,
to the linguistic complexity of the task, etc.
An essential component of most empirical sys-
tems is the language model (LM) and, in partic-
ular, n-gram language models. It is the LM that
tells the system how likely a word or n-gram is in
that language, based on the counts obtained from
corpora. However, corpora represent a sample of
a language and will be sparse, i.e. certain words or
expressions will not occur. One alternative to min-
imise the negative effects of data sparseness and
account for the probability of out-of-vocabulary
words is to use discounting techniques, where a
constant probability mass is discounted from each
n-gram and assigned to unseen n-grams. Another
strategy is to estimate the probability of an un-
seen n-gram by backing off to the probability of
the smaller n-grams that compose it.
In recent years, there has also been some ef-
fort in using the web to overcome data sparseness,
given that the web is several orders of magnitude
larger than any available corpus. However, it is
not straightforward to decide whether (a) it is bet-
ter to use the web than a standard corpus for a
given task or not, and (b) whether corpus and web
counts should be combined and how this should
be done (e.g. using interpolation or back-off tech-
niques). As a consequence there is a strong need
for better understanding of the impacts of web fre-
quencies in NLP systems and tasks.
More reliable ways of combining word counts
could improve the quality of empirical NLP sys-
tems. Thus, in this paper we discuss web-based
word frequency distributions (? 2) and investigate
to what extent ?web-as-a-corpus? approaches can
be employed in NLP tasks compared to standard
corpora (? 3). Then, we present the results of
two experiments. First, we compare word counts
drawn from general-purpose corpora, from spe-
cialised corpora and from the web (? 4). Second,
we propose several methods to combine data from
heterogeneous corpora (? 5), and evaluate their ef-
fectiveness in the context of a specific multiword
1041
expression task: automatic noun compound iden-
tification. We close this paper with some conclu-
sions and future work (? 6).
2 The web as a corpus
Conventional and, in particular, domain-specific
corpora, are valuable resources which provide a
closed-world environment where precise n-gram
counts can be obtained. As they tend to be smaller
than general purpose corpora, data sparseness can
considerably hinder the results of statistical meth-
ods. For instance, in the biomedical Genia cor-
pus (Ohta et al, 2002), 45% of the words occur
only once (so-called hapax legomena), and this is
a very poor basis for a statistical method to decide
whether this is a significant event or just random
noise.
One possible solution is to see the web as a
very large corpus containing pages written in sev-
eral languages and being representative of a large
fraction of human knowledge. However, there are
some differences between using regular corpora
and the web as a corpus, as discussed by Kilgar-
riff (2003). One assumption, in particular, is that
page counts can approximate word counts, so that
the total number of pages is used as an estimator
of the n-gram count, regardless of how many oc-
currences of the n-gram they contain.
This simple underlying assumption has been
employed for several tasks. For example, Grefen-
stette (1999), in the context of example-based ma-
chine translation, uses web counts to decide which
of a set of possible translations is the most natural
one for a given sequence of words (e.g. groupe de
travail as work group vs labour collective). Like-
wise, Keller and Lapata (2003) use the web to esti-
mate the frequencies of unseen nominal bigrams,
while Nicholson and Baldwin (2006) look at the
interpretation of noun compounds based on the
individual counts of the nouns and on the global
count of the compound estimated from the web as
a large corpus.
Villavicencio et al (2007) show that the web
and the British National Corpus (BNC) could be
used interchangeably to identify general-purpose
and type-independent multiword expressions. La-
pata and Keller (2005) perform a careful and
systematic evaluation of the web as a corpus in
other general-purpose tasks both for analysis and
generation, comparing it with a standard corpus
(the BNC) and using two different techniques to
combine them: linear interpolation and back-off.
Their results show that, while web counts are not
as effective for some tasks as standard counts, the
combined counts can generate results, for most
tasks, that are as good as the results produced by
the best individual corpus between the BNC and
the web. Nakov (2007) further investigates these
tasks and finds that, for many of them, effective
attribute selection can produce results that are at
least comparable to those from the BNC using
counts obtained from the web.
On the one hand, the web can minimise the
problem of sparse data, helping distinguish rare
from invalid cases. Moreover, a search engine al-
lows access to ever increasing quantities of data,
even for rare constructions and words, which
counts are usually equated to the number of pages
in which they occur. On the other hand, n-
grams in the highest frequency ranges, such as
the words the, up and down, are often assigned
the estimated size of the web, uniformly. While
this still gives an idea of their massive occur-
rence, it does not provide a finer grained distinc-
tion among them (e.g. in the BNC, the, down and
up occur 6,187,267, 84,446 and 195,426 times,
respectively, while in Yahoo! they all occur in
2,147,483,647 pages).
3 Standard vs web corpora
When we compare n-gram counts estimated from
the web with counts taken from a well-formed
standard corpus, we notice that web counts are
?estimated? or ?approximated? as page counts,
whereas standard corpus counts are the exact
number of occurrences of the n-gram. In this way,
web counts are dependent on the particular search
engine?s algorithms and representations, and these
may perform approximations to handle the large
size of their indexing structures and procedures,
such as ignoring punctuation and using stopword
lists (Kilgarriff, 2007). This assumption, as well
as the following discussion, are not valid for for
controlled data sets derived from Web data, such
1042
as the Google 1 trillion n-grams1. Thus, our re-
sults cannot be compared to those using this kind
of data (Bergsma et al, 2009).
In data-driven techniques, some statistical mea-
sures are based on contingency tables, and the
counts for each of the table cells can be straight-
forwardly computed from a standard corpus.
However, this is not the case for the web, where
the occurrences of an n-gram are not precisely
calculated in relation to the occurrences of the
(n? 1)-grams composing it. For instance, the
n-gram the man may appear in 200,000 pages,
while the words the and man appear in respec-
tively 1,000,000 and 200,000 pages, implying that
the word man occurs with no other word than the2.
In addition, the distribution of words in a stan-
dard corpus follows the well known Zipfian dis-
tribution (Baayen, 2001) while, in the web, it is
very difficult to distinguish frequent words or n-
grams as they are often estimated as the size of the
web. For instance, the Yahoo! frequencies plotted
in figure 1(a) are flattened in the upper part, giv-
ing the same page counts for more than 700 of the
most frequent words. Another issue is the size of
the corpus, which is an important information, of-
ten needed to compute frequencies from counts or
to estimate probabilities in n-gram models. Un-
like the size of a standard corpus, which is easily
obtained, it is very difficult to estimate how many
pages exist on the web, especially as this number
is always increasing.
But perhaps the biggest advantage of the web is
its availability, even for resource-poor languages
and domains. It is a free, expanding and easily ac-
cessible resource that is representative of language
use, in the sense that it contains a great variability
of writing styles, text genres, language levels and
knowledge domains.
4 Analysing n-gram frequencies
In this section, we describe an experiment to com-
pare the probability distribution of the vocabulary
of two corpora, Europarl (Koehn, 2005) and Ge-
nia (Ohta et al, 2002), that represent a sample
of general-purpose and specialised English. In
1This dataset is released through LDC and is not freely
available. Therefore, we do not consider it in our evaluation.
2In practice, this procedure can lead to negative counts.
Vep Vgenia Vinter
types 104,144 20,876 6,798
hapax 41,377 9,410 ?
tokens 39,595,352 486,823 ?
Table 1: Some characteristics of general vs
domain-specific corpora.
addition to both corpora, we also considered the
counts from the web as a corpus, using Google
and Yahoo! APIs, and these four corpora act as n-
gram count sources. To do that, we preprocessed
the data (? 4.1), extracted the vocabularies from
each corpus and calculated their counts in our
four n-gram count sources (? 4.2), analysing their
rank plots to compare how each of these sources
models general-purpose and specialised language
(? 4.3). The experiments described in this sec-
tion were implemented in the mwetoolkit and
are available at http://sf.net/projects/
mwetoolkit/.
4.1 Preprocessing
The Europarl corpus v3.0 (ep) contains transcrip-
tions of the speeches held at the European Par-
liament, with more than 1.4M sentences and
39,595,352 words. The Genia corpus (genia) con-
tains abstracts of scientific articles in biomedicine,
with around 1.8K sentences and 486,823 words.
These standard corpora were preprocessed in the
following way:
1. conversion to XML, lemmatisation and POS
tagging3;
2. case homogenisation, based on the following
criteria:
? all-uppercase and mixed case words
were normalised to their predominant
form, if it accounts for at least 80% of
the occurrences;
? uppercase words at the beginning of
sentences were lowercased;
? other words were not modified.
3Genia contains manual POS tag annota-
tion. Europarl was tagged using the TreeTagger
(www.ims.uni-stuttgart.de/projekte/
corplex/TreeTagger).
1043
This lowercasing algorithm helps to deal with the
massive use of abbreviations, acronyms, named
entities, and formulae found in specialised cor-
pora, such as those containing biomedical (and
other specialised) scientific articles.
For calculating arbitrary-sized n-grams in large
textual corpora efficiently, we implemented a
structure based on suffix arrays (Yamamoto and
Church, 2001). While suffix trees are often used
in LM tools, where n-grams have a fixed size, they
are not fit for arbitrary length n-gram searches and
can consume quite large amounts of memory to
store all the node pointers. Suffix arrays, on the
other hand, allow for arbitrary length n-grams to
be counted in a time that is proportional to log(N),
where N is the number of words (which is equiva-
lent to the number of suffixes) in the corpus. Suf-
fix arrays use a constant amount of memory pro-
portional to N. In our implementation, where ev-
ery word and every word position in the corpus are
encoded as a 4-byte integer, it corresponds pre-
cisely to 4?2?N plus the size of the vocabulary,
which is generally very small if compared to N,
given a typical token/type ratio. The construction
of the suffix array takes O(N log2 N) operations,
due to a sorting step at the end of the process.
4.2 Vocabulary creation
After preprocessing, we extracted all the unigram
surface forms (i.e. all words) from ep and from ge-
nia, generating two vocabularies, Vep and Vgenia,
where the words are ranked in descending fre-
quency order with respect to the corpus itself seen
as a n-gram count source. Formally, we can model
a vocabulary as a set V of words vi ?V taken from
a corpus. A word count is the value c(vi) = n of a
function that goes from words to natural numbers,
c : V ? N. Therefore, there is always an implicit
word order relation?r in a vocabulary, that can be
generated from V and c by using the order relation
? in N4. Thus, a rank is defined as a partially-
ordered set formed by a vocabulary?word order
pair relation: ?V,?r?.
Table 1 summarises some measures of the ex-
tracted vocabularies, where Vinter denotes the in-
tersection of Vep and Vgenia. Notice that Vinter
4That is, ?v1,v2 ?V , suppose c(v1) = n1 and c(v2) = n2,
then v1 ?r v2 if and only if n1 ? n2.
n-gram genia ep google yahoo
642 1 4 8090K 220M
African 2 2028 15400K 916M
fatty 16 22 2550K 59700K
medicine 4 643 21900K 934M
Mac 15 3 34500K 1910M
SH2 27 1 113K 3270K
advances 4 646 6200K 173M
thereby 29 2370 8210K 145M
Table 2: Distribution of some words in Vinter.
contains considerably less entries than the small-
est vocabulary (Vgenia). This shows to what ex-
tent both types of text differ and how important
it is to use the correct techniques when work-
ing with domain-specific data in empirical ap-
proaches. The table also shows the number of ha-
pax legomena (i.e. words that occur only once) in
each corpus, and in this aspect both corpora are
similar5. It also shows how sparseness affects lan-
guage, since a vocabulary that is 400% bigger has
only 5% less hapax legomena.
For each entry in each vocabulary, we ob-
tained a count estimated from four different n-
gram count sources: ep, genia, Google as a cor-
pus (google) and Yahoo! as a corpus (yahoo). The
latter were configured to return only results for
pages in English. Table 2 shows an example of
entries extracted from Vinter. Notice that there are
no zeroes in columns genia and ep, since this vo-
cabulary only contains words that occur at least
once in these corpora. Also, some words like Mac
and SH2, that are probably specialised terms, oc-
cur more in genia than in ep even if the latter is
more than 80 times larger than the former.
4.3 Rank analyses
For each vocabulary, we want to estimate how
similar the ranks generated by each of the four
count sources are. Figure 1 shows the rank po-
sition (x) against the frequency (y) of words in
Vgenia, Vep and Vinter, where each plotted point rep-
resents a rank position according to corpus fre-
5The percentual difference in the proportion of hapax
legomena can be explained by the fact that genia is much
smaller than ep.
1044
10-5
10-4
10-3
10-2
10-1
1 10 100 1000 10000
No
rm
ali
ze
d f
req
ue
nc
y
Rank
Rankplot of vgenia
genia
epgoogleyahoo
(a) Rank plot of Vgenia.
10-5
10-4
10-3
10-2
10-1
1 10 100 1000 10000
No
rm
ali
ze
d f
req
ue
nc
y
Rank
Rankplot of vep
genia
epgoogleyahoo
(b) Rank plot of Vep.
10-5
10-4
10-3
10-2
10-1
1 10 100 1000 10000
No
rm
ali
ze
d f
req
ue
nc
y
Rank
Rankplot of vinter
genia
epgoogleyahoo
(c) Rank plot of Vinter.
Figure 1: Plot of normalised frequencies of vocabularies according to rank positions, log-log scale.
quencies and may correspond to several different
words.6 The four sources have similar shaped
curves for each of the three vocabularies: ep
and genia could be reasonably approximated by
a linear regression curve (in the log-log domain).
google and yahoo present Zipfian curves for low
frequency ranges but have a flat line for higher
frequencies, and the phenomenon seems consis-
tent in all vocabularies and more intense on yahoo.
This is related to the problem discussed in sec-
tion 3 which is that web-based frequencies are not
accurate to model common words because web
counts correspond to page counts and not to word
counts, and that a common word will probably ap-
pear dozens of times in a single page. Nonethe-
less, google seems more robust to this effect,
and indeed yahoo returns exactly the same value
(roughly 2 billion pages) for a large number of
common words, producing the perfectly straight
line in the rank plots. Moreover, the problem
seems less serious in Vinter, but this could be due
to its much smaller size. These results show that
google is incapable of distinguishing among the
top-100 words while yahoo is incapable of distin-
guishing among the top-1000 words, and this can
be a serious drawback for web-based counts both
in general-purpose and specialised NLP tasks.
The curves agree in a large portion of the fre-
quency range, and the only interval in which ge-
nia and ep disagree is in lower frequencies (shown
in the bottom right corner). This happens be-
6Given the Zipfian behaviour of word probability distri-
butions, a log-log scale was used to plot the curves.
cause general-purpose ep frequencies are much
less accurate to model the specialised genia vo-
cabulary, specially in low frequency ranges when
sparseness becomes more marked (figure 1(a)),
and vice-versa (figure 1(b)). This effect is min-
imised in figure 1(c), corresponding to Vinter.
Although both vocabularies present the same
word frequency distributions, it does not mean
that their ranks are similar for the four count
sources. Tables 3 and 4 show the correlation
scores for the compared count sources and for the
two vocabularies, using Kendall?s ? . The ? corre-
lation index estimates the probability that a word
pair in a given rank has the same respective po-
sition in another rank, in spite of the distance be-
tween the words7.
In the two vocabularies, correlation is low,
which indicates that the ranks tend to order words
differently even if there are some similarities in
terms of the shape of the frequency distribution.
When we compare genia with google and with
yahoo, we observe that yahoo is slightly less cor-
related with genia than google, probably because
of its uniform count estimates for frequent words.
However, both seem to be more similar to genia
than ep.
A comparison of ep with google and with yahoo
shows that web frequencies are much more similar
to a general-purpose count source like ep than to
a specialised source like genia. Additionally, both
yahoo and google seem equally correlated to ep.
7For all correlation values, p < 0.001 for the alternative
hypothesis that ? is greater than 0.
1045
Vgenia Vgenia Vgenia Vgenia
top middle bottom
genia-ep 0.26 0.24 0.13 0.06
genia-google 0.28 0.24 0.18 0.09
genia-yahoo 0.27 0.22 0.17 0.09
ep-google 0.57 0.68 0.53 0.49
ep-yahoo 0.57 0.68 0.53 0.49
google-yahoo 0.90 0.90 0.89 0.89
Table 3: Kendall?s ? for count sources in Vgenia.
Vep Vep Vep Vep
top middle bottom
genia-ep 0.26 0.36 0.07 0.04
genia-google 0.27 0.39 0.15 0.12
genia-yahoo 0.24 0.35 0.12 0.10
ep-google 0.40 0.45 0.22 0.09
ep-yahoo 0.38 0.44 0.20 0.08
google-yahoo 0.86 0.89 0.84 0.83
Table 4: Kendall?s ? for count sources in Vep.
Surprisingly, this correlation is higher for Vgenia
than for Vep, as web frequencies and ep frequen-
cies are more similar for a specialised vocabulary
than for a general-purpose vocabulary. This could
mean that the three perform similarly (poorly) at
estimating frequencies for the biomedical vocab-
ulary (Vgenia) whereas they differ considerably at
estimating general-purpose frequencies.
The correlation of the rank (first column) is also
decomposed into the correlation for top words
(more than 10 occurrences), middle words (10 to
3 occurrences) and bottom words (2 and 1 occur-
rences). Except for the pair google-yahoo, the cor-
relation is much higher in the top portion of the
vocabulary and is close to zero in the long tail.
In spite of the logarithmic scale of the graphics
in figure 1, that show the largest difference in the
top part, the bottom part is actually the most ir-
regular. The only exception is ep compared with
the web count sources in Vgenia: these two pairs do
not present the high variability of the other com-
pared pairs, and this means that using ep counts
(general-purpose) to estimate genia counts (spe-
cialised) is similar to using web counts, indepen-
dently of the position of the word in the rank.
Counts from google and from yahoo are also very
similar, specially if we also consider Spearman?s
? , that is very close to total correlation. Web ranks
are also more similar for a specialised vocabulary
than for a general-purpose one, providing further
evidence for the hypothesis that the higher corre-
lation is a consequence of both sources being poor
frequency estimators. That is, for a given vocabu-
lary, when web count sources are good estimators,
they will be more distinct (e.g. having less zero
frequencies).
5 Combining corpora frequencies
In our second experiment, the goal is to propose
and to evaluate techniques for the combination
of n-gram counts from heterogeneous sources.
Therefore, we will use the insights about the vo-
cabulary differences presented in the previous sec-
tion. In this evaluation, we measure the impact
of the suggested techniques in the identification
of noun?noun compounds in corpora. Noun com-
pounds are very frequent in general-purpose and
specialised texts (e.g. bus stop, European Union
and gene activation). We extract them automat-
ically from ep and from genia using a standard
method based on POS patterns and association
measures (Evert and Krenn, 2005; Pecina, 2008;
Ramisch et al, 2010).
5.1 Experimental setup
The evaluation task consists of, given a corpus
of N words, extract all occurrences of adjacent
pairs of nouns8 and then rank them using a stan-
dard statistical measure that estimates the asso-
ciation strength between the two nouns. Analo-
gously to the formalism adopted in section 4.2,
we assume that, for each corpus, we generate a
set NN containing n-grams v1...n ? NN9 for which
we obtain n-gram counts from four sources. The
elements in NN are generated by comparing the
POS pattern noun?noun against all the bigrams in
the corpus and keeping only those pairs of adja-
cent words that match the pattern. The calculation
of the association measure, considering a bigram
v1v2, is based on a contingency table which cells
8We ignore other types of compounds, e.g. adjective?
noun pairs.
9We abbreviate a sequence v1 . . .vn as v1...n.
1046
contain all possible outcomes a1a2,ai ? {vi,?vi}.
For web-based counts, we corrected up to 2% of
them by forcing the frequency of a unigram to be
at least equal to the frequency of the bigram in
which it occurs. Such inconsistencies are incom-
patible with statistical approaches based on con-
tingency table, as discussed in section 2.
The log-likelihood association measure (LL, al-
ternatively called expected mutual information),
estimates the difference between the observed ta-
ble and the expected table under the assumption of
independent events, where E(a1 . . .an) =
n?
i=1
c(ai)
Nn?1is calculated using maximum likelihood:
LL(v1v2) = ?
a1a2
c(a1a2)? log2
c(a1a2)
E(a1a2)
The evaluation of the NN lists is performed au-
tomatically with the help of existing noun com-
pound dictionaries. The general-purpose gold
standard, used to evaluate NNep, is composed of
bigram noun compounds extracted from several
resources: 6,212 entries from the Cambridge In-
ternational Dictionary of English, 22,981 from
Wordnet and 2,849 from the data sets of MWE
200810. Those were merged into a single general-
purpose gold standard that contains 28,622 bi-
gram noun compounds. The specialised gold stan-
dard, used to evaluate NNgenia, is composed of
7,441 bigrams extracted from constituent annota-
tion of the genia corpus with respect to concepts
in the Genia ontology (Kim et al, 2006).
True positives (TPs) are the n-grams of NN
that are contained in the respective gold standard,
while n-grams that do not appear in the gold stan-
dard are considered false positives11. While this
is a simplification that underestimates the perfor-
mance of the method, it is appropriate for the pur-
pose of this evaluation because we compare only
the mean average precision (MAP) between two
NN ranks, in order to verify whether improve-
ments obtained by the combined frequencies are
10420 entries provided by Timothy Baldwin, 2,169 en-
tries provided by Su Nam Kim and 250 entries provided by
Preslav Nakov, freely available at http://multiword.
sf.net/
11In fact, nothing can be said about an n-gram that is not
in a (limited-coverage) dictionary, further manual annotation
would be necessary to asses its relevance.
significant. Additionaly, MWEs are complex lin-
guistic phenomena, and their annotation, specially
in a domain corpus, is a difficult task that reaches
low agreement rates, sometimes even for expert
native speakers. Therefore, not only for theo-
retical reasons but also for practical reasons, we
adopted an automatic evaluation procedure rath-
ern than annotating the top candidates in the lists
by hand.
Since the log-likelihood measure is a function
that assigns a real value to each n-gram, there is
a rank relation ?r that will be used to calculate
MAP as follows:
MAP(NN,?r) =
?
v1...n?NN
P(v1...n)? p(v1...n)
|TPs in NN| ,
where p = 1 if v1...n is a TP, 0 else, and the preci-
sion P(v1...n) of a given n-gram corresponds to the
number of TPs before v1...n in ?NN,?r? over the
total number of n-grams before v1...n in ?NN,?r?.
5.2 Combination heuristics
From the initial list of 176,552 lemmatised n-
grams in NNep and 14,594 in NNgenia, we fil-
tered out all hapax legomena in order to remove
noise and avoid useless computations. Then, we
counted the occurrences of v1, v2 and v1v2 in our
four sources, and those were used to calculate the
four LL values of n-grams in both lists. We also
propose three heuristics to combine a set of m
count sources c1 through cm into a single count
source ccomb:
ccomb(v1...n) =
m?
i=1
wi(v1...n)? ci(v1...n),
where w(v1...n) is a function that assigns a weight
between 0 and 1 for each count source accord-
ing to the n-gram v1...n. Three different func-
tions were used in our experiments: uniform
linear interpolation assumes a constant and uni-
form weight w(v1...n) = 1/m for all n-grams; pro-
portional linear interpolation assumes a constant
weight wi(v1...n) = ((?mj=1 N j)?Ni)/?mj=1 N j that
is proportional to the inverse size of the corpus;
and back-off uses the uniform interpolation of
web frequencies whenever the n-gram count in the
original corpus falls below a threshold (empiri-
cally defined as log2(N/100,000)).
1047
MAP of rank NNgenia NNep
LLgenia 0.4400 0.0462
LLep 0.4351 0.0371
LLgoogle 0.4297 0.0532
LLyahoo 0.4209 0.0508
LLuni f orm 0.4254 0.0508
LLproportional 0.4262 0.0520
LLbacko f f 0.3719 0.0370
Table 5: Performance of compound extraction.
Table 5 shows that the performance of back-
off is below all other techniques for both vocab-
ularies, thus excluding it as a successful combina-
tion heuristic. The large difference between MAP
scores for NNep and for NNgenia is explained by
the relative size of the gold standards: while the
general-purpose reference accounts for 16% of the
size of the NNep set, the specialised reference has
as many entries as 50% of NNgenia. Moreover, the
former was created by joining heterogeneous re-
sources while the latter was compiled by human
annotators from the Genia corpus itself. The goal
of our evaluation, however, is not to compare the
difficulty of each task, but to compare the com-
bination heuristics presented in each row of the
table.
The best MAP for NNgenia was obtained with
genia, that significantly outperforms all other
sources except ep12. On the other hand, the use
of web-based or interpolated counts in extracting
specialised noun?noun compounds does not im-
prove the performance of results based on sparse
but reliable counts drawn from well-formed cor-
pora. Nonetheless, the performance of ep in spe-
cialised extraction is surprising and could only be
explained by some overlap between the corpora.
Moreover, the interpolated counts are not signif-
icantly different from google counts, even if this
corpus should have the weakest weight in propor-
tional interpolation.
General-purpose compound extraction, how-
ever, benefits from the counts drawn from large
corpora as google and yahoo. Indeed, the former
12Significance was assessed through a standard one-tailed
t test for equal sample sizes and variances, ? = 0.005.
significantly outperforms all other count sources,
closely followed by proportional counts. In
both vocabularies, proportional interpolation per-
forms very similar to the best count source, but,
strangely enough, it still does not outperform
google. Further data inspection would be needed
to explain these results for the interpolated combi-
nation and to try to shed some light on the reason
why the backoff method performs so poorly.
6 Future perspectives
In this work, we presented a detailed evalua-
tion of the use of web frequencies as estima-
tors of corpus frequencies in general-purpose and
specialised tasks, discussing some important as-
pects of corpus-based versus web-based n-gram
frequencies. The results indicate that they are
not only very distinct but they are so in different
ways. The importance of domain-specific data for
modelling a specialised vocabulary is discussed in
terms of using ep to get Vgenia counts. Further-
more, the web corpora were more similar to genia
than to ep, which can be explained by the fact that
?similar? is different from ?good?, i.e. they might
be equally bad in modelling genia while they are
distinctly better for ep.
We also proposed heuristics to combine count
sources inspired by standard interpolation and
back-off techniques. Results show that we can-
not use web-based or combined counts to identify
specialised noun compounds, since they do not
help minimise data sparseness. However, general-
purpose extraction is improved with the use of
web counts instead of counts drawn from standard
corpora.
Future work includes extending this research
to other languages and domains in order to es-
timate how much of these results depend on the
corpora sizes. Moreover, as current interpolation
techniques usually combine two corpora, weights
are estimated in a more or less ad hoc proce-
dure (Lapata and Keller, 2005). Interpolating sev-
eral corpora would need a more controlled learn-
ing technique to obtain optimal weights for each
frequency function. Additionally, the evaluation
shows that corpora perform differently according
to the frequency range. This insight could be used
to define weight functions for interpolation.
1048
Acknowledgements
This research was partly supported by CNPq
(Projects 479824/2009-6 and 309569/2009-5),
FINEP and SEBRAE (COMUNICA project
FINEP/SEBRAE 1194/07). Special thanks to
Fl?vio Brun for his thorough work as volunteer
proofreader.
References
Baayen, R. Harald. 2001. Word Frequency Distri-
butions, volume 18 of Text, Speech and Language
Technology. Springer.
Bergsma, Shane, Dekang Lin, and Randy Goebel.
2009. Web-scale N-gram models for lexical disam-
biguation. In Boutilier, Craig, editor, Proceedings
of the 21st International Joint Conference on Arti-
ficial Intelligence (IJCAI 2009), pages 1507?1512,
Pasadena, CA, USA, July.
Evert, Stefan and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of sta-
tistical association measures. Computer Speech &
Language Special issue on Multiword Expression,
19(4):450?466.
Grefenstette, Gregory. 1999. The World Wide Web
as a resource for example-based machine translation
tasks. In Proceedings of the Twenty-First Interna-
tional Conference on Translating and the Computer,
London, UK, November. ASLIB.
Keller, Frank and Mirella Lapata. 2003. Using
the web to obtain frequencies for unseen bigrams.
Computational Linguistics Special Issue on the Web
as Corpus, 29(3):459?484.
Kilgarriff, Adam and Gregory Grefenstette. 2003. In-
troduction to the special issue on the web as corpus.
Computational Linguistics Special Issue on the Web
as Corpus, 29(3):333?347.
Kilgarriff, Adam. 2007. Googleology is bad science.
Computational Linguistics, 33(1):147?151.
Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, and
Jun?ichi Tsujii. 2006. GENIA ontology. Techni-
cal report, Tsujii Laboratory, University of Tokyo.
Koehn, Philipp. 2005. Europarl: A parallel corpus for
statistical machine translation. In Proceedings of
the Tenth Machine Translation Summit(MT Summit
2005), Phuket, Thailand, September. Asian-Pacific
Association for Machine Translation.
Lapata, Mirella and Frank Keller. 2005. Web-
based models for natural language processing. ACM
Transactions on Speech and Language Processing
(TSLP), 2(1):1?31.
Nakov, Preslav. 2007. Using the Web as an Implicit
Training Set: Application to Noun Compound Syn-
tax and Semantics. Ph.D. thesis, EECS Department,
University of California, Berkeley, CA, USA.
Nicholson, Jeremy and Timothy Baldwin. 2006. Inter-
pretation of compound nominalisations using cor-
pus and web statistics. In Moir?n, Bego?a Villada,
Aline Villavicencio, Diana McCarthy, Stefan Ev-
ert, and Suzanne Stevenson, editors, Proceedings of
the COLING/ACL Workshop on Multiword Expres-
sions: Identifying and Exploiting Underlying Prop-
erties (MWE 2006), pages 54?61, Sidney, Australia,
July. Association for Computational Linguistics.
Ohta, Tomoko, Yuka Tateishi, and Jin-Dong Kim.
2002. The GENIA corpus: an annotated research
abstract corpus in molecular biology domain. In
Proceedings of the Second Human Language Tech-
nology Conference (HLT 2002), pages 82?86, San
Diego, CA, USA, March. Morgan Kaufmann Pub-
lishers.
Pecina, Pavel. 2008. Reference data for czech collo-
cation extraction. In Gregoire, Nicole, Stefan Ev-
ert, and Brigitte Krenn, editors, Proceedings of the
LREC Workshop Towards a Shared Task for Multi-
word Expressions (MWE 2008), pages 11?14, Mar-
rakech, Morocco, June.
Ramisch, Carlos, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a framework for mul-
tiword expression identification. In Calzolari, Nico-
letta, Khalid Choukri, Bente Maegaard, Joseph Mar-
iani, Jan Odjik, Stelios Piperidis, Mike Rosner, and
Daniel Tapias, editors, Proceedings of the Seventh
International Conference on Language Resources
and Evaluation (LREC 2010), Valetta, Malta, May.
European Language Resources Association.
Villavicencio, Aline, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation
and evaluation of automatically acquired multi-
word expressions for grammar engineering. In
Eisner, Jason, editor, Proceedings of the 2007
Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2007), pages
1034?1043, Prague, Czech Republic, June. Associ-
ation for Computational Linguistics.
Yamamoto, Mikio and Kenneth W. Church. 2001. Us-
ing suffix arrays to compute term frequency and
document frequency for all substrings in a corpus.
Computational Linguistics, 27(1):1?30.
1049
Coling 2010: Demonstration Volume, pages 57?60,
Beijing, August 2010
Multiword Expressions in the wild?
The mwetoolkit comes in handy
Carlos Ramisch?? Aline Villavicencio? Christian Boitet?
? GETALP ? Laboratory of Informatics of Grenoble, University of Grenoble
? Institute of Informatics, Federal University of Rio Grande do Sul
{ceramisch,avillavicencio}@inf.ufrgs.br Christian.Boitet@imag.fr
Abstract
The mwetoolkit is a tool for auto-
matic extraction of Multiword Expres-
sions (MWEs) from monolingual corpora.
It both generates and validates MWE can-
didates. The generation is based on sur-
face forms, while for the validation, a se-
ries of criteria for removing noise are pro-
vided, such as some (language indepen-
dent) association measures.1 In this paper,
we present the use of the mwetoolkit
in a standard configuration, for extracting
MWEs from a corpus of general-purpose
English. The functionalities of the toolkit
are discussed in terms of a set of selected
examples, comparing it with related work
on MWE extraction.
1 MWEs in a nutshell
One of the factors that makes Natural Language
Processing (NLP) a challenging area is the fact
that some linguistic phenomena are not entirely
compositional or predictable. For instance, why
do we prefer to say full moon instead of total moon
or entire moon if all these words can be consid-
ered synonyms to transmit the idea of complete-
ness? This is an example of a collocation, i.e. a
sequence of words that tend to occur together and
whose interpretation generally crosses the bound-
aries between words (Smadja, 1993). More gen-
erally, collocations are a frequent type of mul-
tiword expression (MWE), a sequence of words
that presents some lexical, syntactic, semantic,
pragmatic or statistical idiosyncrasies (Sag et al,
2002). The definition of MWE also includes a
wide range of constructions like phrasal verbs (go
1The first version of the toolkit was presented in
(Ramisch et al, 2010b), where we described a language- and
type-independent methodology.
ahead, give up), noun compounds (ground speed),
fixed expressions (a priori) and multiword termi-
nology (design pattern). Due to their heterogene-
ity, MWEs vary in terms of syntactic flexibility
(let alne vs the moon is at the full) and semantic
opaqueness (wheel chair vs pass away).
While fairly studied and analysed in general
Linguistics, MWEs are a weakness in current
computational approaches to language. This is
understandable, since the manual creation of lan-
guage resources for NLP applications is expen-
sive and demands a considerable amount of ef-
fort. However, next-generation NLP systems need
to take MWEs into account, because they corre-
spond to a large fraction of the lexicon of a na-
tive speaker (Jackendoff, 1997). Particularly in
the context of domain adaptation, where we would
like to minimise the effort of porting a given sys-
tem to a new domain, MWEs are likely to play a
capital role. Indeed, theoretical estimations show
that specialised lexica may contain between 50%
and 70% of multiword entries (Sag et al, 2002).
Empirical evidence confirms these estimations: as
an example, we found that 56.7% of the terms
annotated in the Genia corpus are composed by
two or more words, and this is an underestimation
since it does not include general-purpose MWEs
such as phrasal verbs and fixed expressions.
The goal of mwetoolkit is to aid lexicog-
raphers and terminographers in the task of creat-
ing language resources that include multiword en-
tries. Therefore, we assume that, whenever a tex-
tual corpus of the target language/domain is avail-
able, it is possible to automatically extract inter-
esting sequences of words that can be regarded as
candidate MWEs.
2 Inside the black box
MWE identification is composed of two phases:
first, we automatically generate a list of candi-
57
mle = c(w1 . . .wn)N
dice = n? c(w1 . . .wn)?ni=1 c(wi)
pmi = log2
c(w1 . . .wn)
E(w1 . . .wn)
t-score = c(w1 . . .wn)?E(w1 . . .wn)?c(w1 . . .wn)
Figure 1: A candidate is a sequence of words w1 to
wn, with word counts c(w1) . . .c(wn) and n-gram
count c(w1 . . .wn) in a corpus with N words. The
expected count if words co-occurred by chance is
E(w1 . . .wn)? c(w1)...c(wn)Nn?1 .
dates from the corpus; then we filter them, so that
we can discard as much noise as possible. Can-
didate generation uses flat linguistic information
such as surface forms, lemmas and parts of speech
(POS).2 We can then define target sequences of
POS, such as VERB NOUN sequences, or even
more fine-grained constraints which use lemmas,
like take NOUN and give NOUN, or POS patterns
that include wildcards that stand for any word or
POS.3 The optimal POS patterns for a given do-
main, language and MWE type can be defined
based on the analysis of the data.
For the candidate filtering a set of association
measures (AMs), listed in figure 1, are calculated
for each candidate. A simple threshold can sub-
sequently be applied to filter out all the candidates
for which the AMs fall below a user-defined value.
If a gold standard is available, the toolkit can build
a classifier, automatically annotating each candi-
date to indicate whether it is contained in the gold
standard (i.e. it is regarded as a true MWE) or
not (i.e. it is regarded as a non-MWE).4 This
annotation is not used to filter the lists, but only
2If tools like a POS tagger are not available for a lan-
guage/domain, it is possible to generate simple n-gram lists
(n = 1..10), but the quality will be inferior. A possible solu-
tion is to filter out candidates on a keyword basis, e.g. from
a list of stopwords).
3Although syntactic information can provide better re-
sults for some types of MWEs, like collocations (Seretan,
2008), currently no syntactic information is allowed as a cri-
terion for candidate generation, keeping the toolkit as simple
and language independent as possible.
4The gold standard can be a dictionary or a manually an-
notated list of candidates.
candidate fEP fgoogle class
status quo 137 1940K True
US navy 4 1320K False
International Cooperation 2 1150K False
Cooperation Agreement 188 115K True
Panama Canal 2 753K True
security institution 5 8190 False
lending institution 4 54800 True
human right 2 251K True
Human Rights 3067 3400K False
pro-human right 2 34 False
Table 1: Example of MWE candidates extracted
by mwetoolkit.
by the classifier to learn the relation between the
AMs and the MWE class of the candidate. This
is particularly useful because, to date, it remains
unclear which AM performs better for a partic-
ular type or language, and the classifier applies
measures according to their efficacy in filtering
the candidates.Some examples of output are pre-
sented in table 1.
3 Getting started
The toolkit is open source software that can
be freely downloaded (sf.net/projects/
mwetoolkit). As a demonstration, we present
the extraction of noun-noun compounds from the
general-purpose English Europarl (EP) corpus5.
To preprocess the corpus, we used the sen-
tence splitter and tokeniser provided with EP, fol-
lowed by a lowercasing treatment (integrated in
the toolkit), and lemmatisation and POS tagging
using the TreeTagger6. The tagset was simplified
since some distinctions among plural/singular and
proper nouns were irrelevant.
From the preprocessed corpus, we obtained all
sequences of 2 nouns, which resulted in 176,552
unique noun compound candidates. Then, we ob-
tained the corpus counts for the bigrams and their
component unigrams in the EP corpus. Adopt-
ing the web as a corpus, we also use the number
of pages retrieved by Google and by Yahoo! as
5www.statmt.org/europarl.
6http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger/.
58
raw EN 
Europar
l
sentenc
e-split, 
lowerca
sed,
tokenis
ed, POS
-tagged
,
lemmat
ised Eu
roparl  Preproce
ssing
noun-no
un 
candida
tes
filtered-
1 
candida
tes
filtered-
2 
candida
tes  Count ca
ndidate
s
  Thresh
old
 Associa
tion me
as.
 Sort an
d thresh
old
mwetoo
lkit
Figure 2: Step-by-step demonstration on the EP
corpus.
counts. The mwetoolkit implements a cache
mechanism to avoid redundant queries, but to
speed up the process7, we filtered out all candi-
dates occurring less than two times in EP, which
reduced the list of candidates to 64,551 entries
(filtered-1 candidates in figure 2).
For the second filtering step, we calculated
four AMs for each of the three frequency sources
(EP, Google and Yahoo!). Some results on ma-
chine learning applied to the candidate lists of
the mwetoolkit can be found in Ramisch et al
(2010b). Here, we will limit ourselves to a dis-
cussion on some advantages and inconvenients of
the chosen approach by analysing a list of selected
examples.
4 Pros and cons
One of the biggest advantages of our approach is
that, since it is language independent, it is straight-
forward to apply it on corpora in virtually any
language. Moreover, it is not dependent on a
specific type of construction or syntactic formal-
ism. Of course, since it only uses limited linguis-
tic information, the accuracy of the resulting lists
can always be further improved with language-
dependent tools. In sum, the toolkit allows users
to perform systematic MWE extraction with con-
sistent intermediary files and well defined scripts
and arguments (avoiding the need for a series of ad
hoc separate scripts). Even if some basic knowl-
edge about how to run Python scripts and how to
7Yahoo! limits the queries to 5,000/day.
pass arguments to the command line is necessary,
the user is not required to be a programmer.
Nested MWEs are a problem in the current
approach. Table 1 shows two bigrams Interna-
tional Cooperation and Cooperation Agreement,
both evaluated as False candidates. However, they
could be considered as parts of a larger MWE In-
ternational Cooperation Agreement, but with the
current methodology it is not possible to detect
this kind of situation. Another case where the
candidate contains a MWE is the example pro-
human right, and in this case it would be neces-
sary to separate the prefix from the MWE, i.e. to
re-tokenise the words around the MWE candidate.
Indeed, tools for consistent tokenisation, specially
concerning dashes and slashes, could improve the
quality of the results, in particular for specialised
corpora.
The toolkit provides full integration with web
search engine APIs. The latter, however, are of
limited utility because search engines are not only
slow but also return more or less arbitrary num-
bers, some times even inconsistent (Ramisch et
al., 2010c). When large corpora like EP are avail-
able, we suggest that it is better to use its counts
rather than web counts. The toolkit provides an
efficient indexing mechanism, allowing for arbi-
trary n-grams to be counted in linear time.
The automatic evaluation of the candidates will
always be limited by the coverage of the reference
list. In the examples, Panama Canal is consid-
ered as a true MWE whereas US navy is not, but
both are proper names and the latter should also
be included as a true candidate. The same happens
for the candidates Human Rights and human right.
The mwetoolkit is an early prototype whose
simple design allows fine tuning of knowledge-
poor methods for MWE extraction. However, we
believe that there is room for improvement at sev-
eral points of the extraction methodology.
5 From now on
One of our goals for future versions is to be able
to extract bilingual MWEs from parallel or com-
parable corpora automatically. This could be done
through the inclusion of automatic word align-
ment information. Some previous experiments
show, however, that this may not be enough, as
59
automatic word alignment uses almost no lin-
guistic information and its output is often quite
noisy (Ramisch et al, 2010a). Combining align-
ment and shallow linguistic information seems a
promising solution for the automatic extraction
of bilingual MWEs. The potential uses of these
lexica are multiple, but the most obvious appli-
cation is machine translation. On the one hand,
MWEs could be used to guide the word align-
ment process. For instance, this could solve the
problem of aligning a language where compounds
are separate words, like French, with a language
that joins compound words together, like Ger-
man. In statistical machine translation systems,
MWEs could help to filter phrase tables or to boost
the scores of phrases which words are likely to
be multiwords.Some types of MWE (e.g. collo-
cations) could help in the semantic disambigua-
tion of words in the source language. The sense
of a word defined by its collocate can allow to
chose the correct target word or expression (Sere-
tan, 2008).
We would also like to improve the techniques
implemented for candidate filtering. Related work
showed that association measures based on con-
tingency tables are more robust to data sparseness
(Evert and Krenn, 2005). However, they are pair-
wise comparisons and their application on arbi-
trarily long n-grams is not straightforward. An
heuristics to adapt these measures is to apply them
recursively over increasing n-gram length. Other
features that could provide better classification
are context words, linguistic information coming
from simple word lexica, syntax, semantic classes
and domain-specific keywords. While for poor-
resourced languages we can only count on shallow
linguistic information, it is unreasonable to ignore
available information for other languages. In gen-
eral, machine learning performs better when more
information is available (Pecina, 2008).
We would like to evaluate our toolkit on several
data sets, varying the languages, domains and tar-
get MWE types. This would allow us to assign
its quantitative performance and to compare it to
other tools performing similar tasks. Additionally,
we could evaluate how well the classifiers perform
across languages and domains. In short, we be-
lieve that the mwetoolkit is an important first
step toward robust and reliable MWE treatment.
It is a freely available core application providing
flexible tools and coherent up-to-date documenta-
tion, and these are essential characteristics for the
extension and support of any computer system.
Acknowledgements
This research was partly supported by CNPq
(Projects 479824/2009-6 and 309569/2009-5),
FINEP and SEBRAE (COMUNICA project
FINEP/SEBRAE 1194/07).
References
Evert, Stefan and Brigitte Krenn. 2005. Using small
random samples for the manual evaluation of statis-
tical association measures. Comp. Speech & Lang.
Special issue on MWEs, 19(4):450?466.
Jackendoff, Ray. 1997. Twistin? the night away. Lan-
guage, 73:534?559.
Pecina, Pavel. 2008. Reference data for czech colloca-
tion extraction. In Proc. of the LREC Workshop To-
wards a Shared Task for MWEs (MWE 2008), pages
11?14, Marrakech, Morocco, Jun.
Ramisch, Carlos, Helena de Medeiros Caseli, Aline
Villavicencio, Andr? Machado, and Maria Jos? Fi-
natto. 2010a. A hybrid approach for multiword ex-
pression identification. In Proc. of the 9th PROPOR
(PROPOR 2010), volume 6001 of LNCS (LNAI),
pages 65?74, Porto Alegre, RS, Brazil. Springer.
Ramisch, Carlos, Aline Villavicencio, and Christian
Boitet. 2010b. mwetoolkit: a framework for multi-
word expression identification. In Proc. of the Sev-
enth LREC (LREC 2010), Malta, May. ELRA.
Ramisch, Carlos, Aline Villavicencio, and Christian
Boitet. 2010c. Web-based and combined language
models: a case study on noun compound identifica-
tion. In Proc. of the 23th COLING (COLING 2010),
Beijing, China, Aug.
Sag, Ivan, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-
word expressions: A pain in the neck for NLP.
In Proc. of the 3rd CICLing (CICLing-2002), vol-
ume 2276/2010 of LNCS, pages 1?15, Mexico City,
Mexico, Feb. Springer.
Seretan, Violeta. 2008. Collocation extraction based
on syntactic parsing. Ph.D. thesis, University of
Geneva, Geneva, Switzerland.
Smadja, Frank A. 1993. Retrieving collocations from
text: Xtract. Comp. Ling., 19(1):143?177.
60
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419?424,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Nothing like Good Old Frequency:
Studying Context Filters for Distributional Thesauri
Muntsa Padr
?
o,
?
Marco Idiart
?
, Carlos Ramisch
?
, Aline Villavicencio
?
?
Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?
Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
?
Aix Marseille Universit?e, CNRS, LIF UMR 7279, 13288, Marseille (France)
muntsa.padro@inf.ufrgs.br, marco.idiart@gmail.com,
carlos.ramisch@lif.univ-mrs.fr, avillavicencio@inf.ufrgs.br
Abstract
Much attention has been given to the
impact of informativeness and similar-
ity measures on distributional thesauri.
We investigate the effects of context fil-
ters on thesaurus quality and propose the
use of cooccurrence frequency as a sim-
ple and inexpensive criterion. For eval-
uation, we measure thesaurus agreement
with WordNet and performance in answer-
ing TOEFL-like questions. Results illus-
trate the sensitivity of distributional the-
sauri to filters.
1 Introduction
Large-scale distributional thesauri created auto-
matically from corpora (Grefenstette, 1994; Lin,
1998; Weeds et al., 2004; Ferret, 2012) are an
inexpensive and fast alternative for representing
semantic relatedness between words, when man-
ually constructed resources like WordNet (Fell-
baum, 1998) are unavailable or lack coverage. To
construct a distributional thesaurus, the (colloca-
tional or syntactic) contexts in which a target word
occurs are used as the basis for calculating its sim-
ilarity with other words. That is, two words are
similar if they share a large proportion of contexts.
Much attention has been devoted to refin-
ing thesaurus quality, improving informativeness
and similarity measures (Lin, 1998; Curran and
Moens, 2002; Ferret, 2010), identifying and de-
moting bad neighbors (Ferret, 2013), or using
more relevant contexts (Broda et al., 2009; Bie-
mann and Riedl, 2013). For the latter in particular,
as words vary in their collocational tendencies, it
is difficult to determine how informative a given
context is. To remove uninformative and noisy
contexts, filters have often been applied like point-
wise mutual information (PMI), lexicographer?s
mutual information (LMI) (Biemann and Riedl,
2013), t-score (Piasecki et al., 2007) and z-score
(Broda et al., 2009). However, the selection of a
measure and of a threshold value for these filters
is generally empirically determined. We argue that
these filtering parameters have a great influence on
the quality of the generated thesauri.
The goal of this paper is to quantify the im-
pact of context filters on distributional thesauri.
We experiment with different filter methods and
measures to assess context significance. We pro-
pose the use of simple cooccurrence frequency as
a filter and show that it leads to better results than
more expensive measures such as LMI or PMI.
Thus we propose a cheap and effective way of fil-
tering contexts while maintaining quality.
This paper is organized as follows: in ?2 we
discuss evaluation of distributional thesauri. The
methodology adopted in the work and the results
are discussed in ?3 and ?4. We finish with some
conclusions and discussion of future work.
2 Related Work
In a nutshell, the standard approach to build a dis-
tributional thesaurus consists of: (i) the extraction
of contexts for the target words from corpora, (ii)
the application of an informativeness measure to
represent these contexts and (iii) the application of
a similarity measure to compare sets of contexts.
The contexts in which a target word appears can
be extracted in terms of a window of cooccurring
(content) words surrounding the target (Freitag et
al., 2005; Ferret, 2012; Erk and Pado, 2010) or in
terms of the syntactic dependencies in which the
target appears (Lin, 1998; McCarthy et al., 2003;
Weeds et al., 2004). The informativeness of each
context is calculated using measures like PMI, and
t-test while the similarity between contexts is cal-
culated using measures like Lin?s (1998), cosine,
Jensen-Shannon divergence, Dice or Jaccard.
Evaluation of the quality of distributional the-
sauri is a well know problem in the area (Lin,
419
1998; Curran and Moens, 2002). For instance, for
intrinsic evaluation, the agreement between the-
sauri has been examined, looking at the average
similarity of a word in the thesauri (Lin, 1998),
and at the overlap and rank agreement between the
thesauri for target words like nouns (Weeds et al.,
2004). Although much attention has been given to
the evaluation of various informativeness and sim-
ilarity measures, a careful assessment of the ef-
fects of filtering on the resulting thesauri is also
needed. For instance, Biemann and Riedl (2013)
found that filtering a subset of contexts based on
LMI increased the similarity of a thesaurus with
WordNet. In this work, we compare the impact of
using different types of filters in terms of thesaurus
agreement with WordNet, focusing on a distribu-
tional thesaurus of English verbs. We also propose
a frequency-based saliency measure to rank and
filter contexts and compare it with PMI and LMI.
Extrinsic evaluation of distributional thesauri
has been carried out for tasks such as En-
glish lexical substitution (McCarthy and Navigli,
2009), phrasal verb compositionality detection
(McCarthy et al., 2003) and the WordNet-based
synonymy test (WBST) (Freitag et al., 2005). For
comparative purposes in this work we adopt the
latter.
3 Methodology
We focus on thesauri of English verbs constructed
from the BNC (Burnard, 2007)
1
. Contexts are ex-
tracted from syntactic dependencies generated by
RASP (Briscoe et al., 2006), using nouns (heads
of NPs) which have subject and direct object rela-
tions with the target verb. Thus, each target verb
is represented by a set of triples containing (i) the
verb itself, (ii) a context noun and (iii) a syntac-
tic relation (object, subject). The thesauri were
constructed using Lin?s (1998) method. Lin?s ver-
sion of the distributional hypothesis states that two
words (verbs v
1
and v
2
in our case) are similar if
they share a large proportion of contexts weighted
by their information content, assessed with PMI
(Bansal et al., 2012; Turney, 2013).
In the literature, little attention is paid to context
filters. To investigate their impact, we compare
two kinds of filters, and before calculating similar-
ity using Lin?s measure, we apply them to remove
1
Even though larger corpora are available, we use a tradi-
tional carefully constructed corpus with representative sam-
ples of written English to control the quality of the thesaurus.
potentially noisy triples:
? Threshold (th): we remove triples that oc-
cur less than a threshold th. Threshold values
vary from 1 to 50 counts per triple.
? Relevance (p): we keep only the top p most
relevant contexts for each verb, were rele-
vance is defined according to the following
measures: (a) frequency, (b) PMI, and (c)
LMI (Biemann and Riedl, 2013). Values of
p vary between 10 and 1000.
In this work, we want to answer two ques-
tions: (a) Do more selective filters improve intrin-
sic evaluation of thesaurus? and (b) Do they also
help in extrinsic evaluation?
For intrinsic evaluation, we determine agree-
ment between a distributional thesaurus and Word-
Net as the path similarities for the first k distri-
butional neighbors of a verb. A single score is
obtained by averaging the similarities of all verbs
with their k first neighbors. The higher this score
is, the closer the neighbors are to the target in
WordNet, and the better the thesaurus. Several
values of k were tested and the results showed ex-
actly the same curve shapes for all values, with
WordNet similarity decreasing linearly with k. For
the remainder of the paper we adopt k = 10, as it
is widely used in the literature.
For extrinsic evaluation, we use the WBST set
for verbs (Freitag et al., 2005) with 7,398 ques-
tions and an average polysemy of 10.4. The task
consists of choosing the most suitable synonym
for a word among a set of four options. The the-
saurus is used to rank the candidate answers by
similarity scores, and select the first one as the
correct synonym. As discussed by Freitag et al.
(2005), the upper bound reached by English na-
tive speakers is 88.4% accuracy, and simple lower
bounds are 25% (random choice) and 34.5% (al-
ways choosing the most frequent option).
4 Results
Figure 1 shows average WordNet similarities for
thesauri built filtering by frequency threshold th
and by p most frequent contexts. Table 1 sum-
marizes the parametrization leading to the best
WordNet similarity for each kind of filter. In all
cases we show the results obtained for different
frequency ranges
2
as well as the results when av-
eraging over all verbs.
2
In order to study the influence of verb frequency on the
results, we divide the verbs in three groups: high-frequency
420
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 1  10
W
N
 
s
i
m
i
l
a
r
i
t
y
th 
WordNet path Similarity for different frequency ranges, k=10Filtering triples with frequency under th
all verbshigh frequent verbsmid frequent verbslow frequent verbs  0
 0.05
 0.1
 0.15
 0.2
 0.25
 10  100  1000
W
N
 
s
i
m
i
l
a
r
i
t
y
p 
WordNet path Similarity for different frequency ranges, k=10Keeping p most frequent triples per verb
all verbshigh frequent verbsmid frequent verbslow frequent verbs
Figure 1: WordNet scores for verb frequency ranges, filtering by frequency threshold th (left) and p most
frequent contexts (right).
Filter All verbs Frequency range
Low Mid High
No filter - 0.148 - 0.101 - 0.144 - 0.198
Filter low freq. contexts th = 50 0.164 th = 50 0.202 th = 50 0.154 th = 1 0.200
Keep p contexts (freq.) p = 200 0.158 p = 500 0.138 p = 200 0.149 p = 200 0.206
Keep p contexts (PMI) p = 1000 0.139 p = 1000 0.101 p = 1000 0.136 p = 1000 0.181
Keep p contexts (LMI) p = 200 0.155 p = 100 0.112 p = 200 0.147 p = 200 0.208
Table 1: Best scores obtained for each filter for all verbs and frequency ranges. Scores are given in terms
of WordNet path. Confidence interval is arround ? 0.002 in all cases.
When using a threshold filter (Figure 1 left),
high values lead to better performance for mid-
and low-frequency verbs. This is because, for high
th values, there are few low and mid-frequency
verbs left, since a verb that occurs less has less
chances to be seen often in the same context. The
similarity for verbs with no contexts over the fre-
quency threshold cannot be assessed and as a con-
sequence those verbs are not included in the fi-
nal thesaurus. As Figure 2 shows, the number
of verbs decreases much faster for low and mid
frequency verbs when th increases.
3
For exam-
ple, for th = 50, there are only 7 remaining low-
frequency verbs in the thesaurus and these tend
to be idiosyncratic multiword expressions. One
example is wreak, and the only triple contain-
ing this verb that appeared more than 50 times is
wreak havoc (71 occurrences). The neighbors of
this verb are cause and play, which yield a good
similarity score in WordNet. Therefore, although
higher thresholds result in higher similarities for
low and mid-frequency verbs, this comes at a cost,
as the number of verbs included in the thesaurus
decreases considerably.
(||v|| ? 500), mid-frequency (150 ? ||v|| < 500) and low-
frequency (||v|| < 150).
3
For p most salient contexts, the number of verbs does not
vary and is the same shown in Figure 2 for th = 1 (no filter).
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 1  10
N
u
m
b
e
r
 
o
f
 
v
e
r
b
s
th 
Number of verbs in WordNetFiltering triples with frequency under th
all verbshigh frequent verbsmid frequent verbslow frequent verbs
Figure 2: Number of verbs per frequency ranges
when filtering by context frequency threshold th
As expected, the best performance is obtained
for high-frequency verbs and no filter, since it re-
sults in more context information per verb. In-
creasing th decreases similarity due to the removal
of some of these contexts. In average, higher th
values lead to better overall similarity among the
frequency ranges (from 0.148 with th = 1 to
0.164 with th = 50). The higher the threshold,
the more high-frequency verbs will prevail in the
thesauri, for which the WordNet path similarities
are higher.
On the other hand, when adopting a relevance
421
 0
 0.2
 0.4
 0.6
 0.8
 1
 1  10
P
,
 
R
,
 
F
1
th 
WBST task: P, R and F1Filtering triples with frequency under th
PrecisionRecallF1  0
 0.2
 0.4
 0.6
 0.8
 1
 10  100  1000
P
,
 
R
,
 
F
1
p 
WBST task: P, R and F1Keeping p most frequent triples per verb
PrecisionRecallF1
Figure 3: WBST task scores filtering by frequency threshold th (left) and p most frequent contexts
(right).
filter of keeping the p most relevant contexts for
each verb (Figure 1 right), we obtain similar re-
sults, but more stable thesauri. The number of
verbs remains constant, since we keep a fixed
number of contexts for each verb and verbs are not
removed when the threshold is modified. Word-
Net similarity increases as more contexts are taken
into account, for all frequency ranges. There is a
maximum around p = 200, though larger values
do not lead to a drastic drop in quality. This sug-
gests that the noise introduced by low-frequency
contexts is compensated by the increase of infor-
mativeness for other contexts. An ideal balance
is reached by the lowest possible p that maintains
high WordNet similarity, since the lower the p the
faster the thesaurus construction.
In terms of saliency measure, when keeping
only the p most relevant contexts, sorting them
with PMI leads to much worse results than LMI
or frequency, as PMI gives too much weight to
infrequent combinations. This is consistent with
results of Biemann and Riedl (2013). Regarding
LMI versus frequency, the results using the latter
are slightly better (or with no significant differ-
ence, depending on the frequency range). The ad-
vantage of using frequency instead of LMI is that
it makes the process simpler and faster while lead-
ing to equal or better performance in all frequency
ranges. Therefore for the extrinsic evaluation us-
ing WBST task, we use frequency to select the
p most relevant contexts and then compute Lin?s
similarity using only those contexts.
Figure 3 shows the performance of the thesauri
in the WBST task in terms of precision, recall and
F1.
4
For precision, the best filter is to remove con-
4
Filters based on LMI and PMI were also tested with the
texts occurring less than th times, but, this also
leads to poor recall, since many verbs are left out
of the thesauri and their WSBT questions cannot
be answered. On the other hand, keeping the most
relevant p contexts leads to more stable results and
when p is high (right plot), they are similar to those
shown in the left plot of Figure 3.
4.1 Discussion
The answer to our questions in Section 3 is yes,
more selective filters improve intrinsic and extrin-
sic thesaurus quality. The use of both filtering
methods results in thesauri in which the neighbors
of target verbs are closer in WordNet and get better
scores in TOEFL-like tests. However, the fact that
filtering contexts with frequency under th removes
verbs in the final thesaurus is a drawback, as high-
lighted in the extrinsic evaluation on the WBST
task.
Furthermore, we demonstrated that competitive
results can be obtained keeping only the p most
relevant contexts per verb. On the one hand, this
method leads to much more stable thesauri, with
the same verbs for all values of p. On the other
hand, it is important to highlight that the best re-
sults to assess the relevance of the contexts are ob-
tained using frequency while more sophisticated
filters such as LMI do not improve thesaurus qual-
ity. Although an LMI filter is relatively fast com-
pared to dimensionality reduction techniques such
as singular value decomposition (Landauer and
Dumais, 1997), it is still considerably more expen-
sive than a simple frequency filter.
In short, our experiments indicate that a reason-
same results as intrinsic evaluation: sorting contexts by fre-
quency leads to better results.
422
able trade-off between noise, coverage and com-
putational efficiency is obtained for p = 200 most
frequent contexts, as confirmed by intrinsic and
extrinsic evaluation. Frequency threshold th is
not recommended: it degrades recall because the
contexts for many verbs are not frequent enough.
This result is useful for extracting distributional
thesauri from very large corpora like the UKWaC
(Ferraresi et al., 2008) by proposing an alterna-
tive that minimizes the required computational re-
sources while efficiently removing a significant
amount of noise.
5 Conclusions and Future Work
In this paper we addressed the impact of filters
on the quality of distributional thesauri, evaluat-
ing a set of standard thesauri and different filtering
methods. The results suggest that the use of fil-
ters and their parameters greatly affect the thesauri
generated. We show that it is better to use a filter
that selects the most relevant contexts for a verb
than to simply remove rare contexts. Furthermore,
the best performance was obtained with the sim-
plest method: frequency was found to be a simple
and inexpensive measure of context salience. This
is especially important when dealing with large
amounts of data, since computing LMI for all con-
texts would be computationally costly. With our
proposal to keep just the p most frequent contexts
per verb, a great deal of contexts are cheaply re-
moved and thus the computational power required
for assessing similarity is drastically reduced.
As future work, we plan to use these filters to
build thesauri from larger corpora. We would like
to generalize our findings to other syntactic con-
figurations (e.g. noun-adjective) as well as to other
similarity and informativeness measures. For in-
stance, ongoing experiments indicate that the same
parameters apply when Lin?s similarity is replaced
by cosine. Finally, we would like to compare the
proposed heuristics with more sophisticated filter-
ing strategies like singular value decomposition
(Landauer and Dumais, 1997) and non-negative
matrix factorization (Van de Cruys, 2009).
Acknowledgments
We would like to thank the support of projects
CAPES/COFECUB 707/11, PNPD 2484/2009,
FAPERGS-INRIA 1706-2551/13-7, CNPq
312184/2012-3, 551964/2011-1, 482520/2012-4
and 312077/2012-2.
References
Mohit Bansal, John DeNero, and Dekang Lin. 2012.
Unsupervised translation sense clustering. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
773?782, Montr?eal, Canada, June. Association for
Computational Linguistics.
Chris Biemann and Martin Riedl. 2013. Text: Now
in 2D! a framework for lexical expansion with con-
textual similarity. Journal of Language Modelling,
1(1).
Ted Briscoe, John Carroll, and Rebecca Watson. 2006.
The second release of the RASP system. In James
Curran, editor, Proc. of the COLING/ACL 2006 In-
teractive Presentation Sessions, pages 77?80, Sid-
ney, Australia, Jul. ACL.
Bartosz Broda, Maciej Piasecki, and Stan Szpakow-
icz. 2009. Rank-based transformation in mea-
suring semantic relatedness. In Proceedings of
the 22nd Canadian Conference on Artificial Intel-
ligence: Advances in Artificial Intelligence, Cana-
dian AI ?09, pages 187?190, Berlin, Heidelberg.
Springer-Verlag.
Lou Burnard. 2007. User Reference Guide for the
British National Corpus. Technical report, Oxford
University Computing Services, Feb.
James R. Curran and Marc Moens. 2002. Improve-
ments in automatic thesaurus extraction. In Proc.of
the ACL 2002 Workshop on Unsupervised Lexical
Acquisition, pages 59?66, Philadelphia, Pennsylva-
nia, USA. ACL.
Katrin Erk and Sebastian Pado. 2010. Exemplar-based
models for word meaning in context. In Proc. of the
ACL 2010 Conference Short Papers, pages 92?97,
Uppsala, Sweden, Jun. ACL.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database (Language, Speech, and
Communication). MIT Press, May. 423 p.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and
Silvia Bernardini. 2008. Introducing and evaluat-
ing UKWaC, a very large web-derived corpus of En-
glish. In In Proceedings of the 4th Web as Corpus
Workshop (WAC-4.
Olivier Ferret. 2010. Testing semantic similarity mea-
sures for extracting synonyms from a corpus. In
Proc. of the Seventh LREC (LREC 2010), pages
3338?3343, Valetta, Malta, May. ELRA.
Olivier Ferret. 2012. Combining bootstrapping and
feature selection for improving a distributional the-
saurus. In ECAI, pages 336?341.
Olivier Ferret. 2013. Identifying bad semantic neigh-
bors for improving distributional thesauri. In Proc.
of the 51st ACL (Volume 1: Long Papers), pages
561?571, Sofia, Bulgaria, Aug. ACL.
423
Dayne Freitag, Matthias Blume, John Byrnes, Ed-
mond Chow, Sadik Kapadia, Richard Rohwer, and
Zhiqiang Wang. 2005. New experiments in distri-
butional representations of synonymy. In Ido Dagan
and Dan Gildea, editors, Proc. of the Ninth CoNLL
(CoNLL-2005), pages 25?32, University of Michi-
gan, MI, USA, Jun. ACL.
Gregory Grefenstette. 1994. Explorations in Au-
tomatic Thesaurus Discovery. Springer, Norwell,
MA, USA.
Thomas K Landauer and Susan T. Dumais. 1997. A
solution to platos problem: The latent semantic anal-
ysis theory of acquisition, induction, and represen-
tation of knowledge. Psychological review, pages
211?240.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proc. of the 36th ACL and
17th COLING, Volume 2, pages 768?774, Montreal,
Quebec, Canada, Aug. ACL.
Diana McCarthy and Roberto Navigli. 2009. The en-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In Francis Bond, Anna Korhonen,
Diana McCarthy, and Aline Villavicencio, editors,
Proc. of the ACL Workshop on MWEs: Analysis, Ac-
quisition and Treatment (MWE 2003), pages 73?80,
Sapporo, Japan, Jul. ACL.
Maciej Piasecki, Stanislaw Szpakowicz, and Bartosz
Broda. 2007. Automatic selection of heterogeneous
syntactic features in semantic similarity of polish
nouns. In Proceedings of the 10th international
conference on Text, speech and dialogue, TSD?07,
pages 99?106, Berlin, Heidelberg. Springer-Verlag.
Peter D. Turney. 2013. Distributional semantics be-
yond words: Supervised learning of analogy and
paraphrase. 1:353?366.
Tim Van de Cruys. 2009. A non-negative tensor factor-
ization model for selectional preference induction.
In Proceedings of the Workshop on Geometrical
Models of Natural Language Semantics, pages 83?
90, Athens, Greece, March. Association for Compu-
tational Linguistics.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proc. of the 20th COLING (COL-
ING 2004), pages 1015?1021, Geneva, Switzerland,
Aug. ICCL.
424
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 49?56
Manchester, August 2008
Picking them up and Figuring them out:
Verb-Particle Constructions, Noise and Idiomaticity
Carlos Ramisch
??
, Aline Villavicencio
??
, Leonardo Moura
?
and Marco Idiart
?
?
Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)
?
GETALP Laboratory, Joseph Fourier University - Grenoble INP (France)
?
Department of Computer Sciences, Bath University (UK)
?
Institute of Physics, Federal University of Rio Grande do Sul (Brazil)
{ceramisch,avillavicencio,lfsmoura}@inf.ufrgs.br, idiart@if.ufrgs.br
Abstract
This paper investigates, in a first stage,
some methods for the automatic acquisi-
tion of verb-particle constructions (VPCs)
taking into account their statistical prop-
erties and some regular patterns found in
productive combinations of verbs and par-
ticles. Given the limited coverage pro-
vided by lexical resources, such as dictio-
naries, and the constantly growing number
of VPCs, possible ways of automatically
identifying them are crucial for any NLP
task that requires some degree of semantic
interpretation. In a second stage we also
study whether the combination of statis-
tical and linguistic properties can provide
some indication of the degree of idiomatic-
ity of a given VPC. The results obtained
show that such combination can success-
fully be used to detect VPCs and distin-
guish idiomatic from compositional cases.
1 Introduction
Considerable investigative effort has focused on
the automatic identification of Multiword Expres-
sions (MWEs), like compound nouns (science fic-
tion) and phrasal verbs (carry out) (e.g. Pearce
(2002), Evert and Krenn (2005) and Zhang et
al. (2006)). Some of them employ language
and/or type dependent linguistic knowledge for
the task, while others employ independent statis-
tical methods, such as Mutual Information and
Log-likelihood (e.g. Pearce (2002) and, Zhang et
al. (2006)), or even a combination of them (e.g.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
Baldwin (2005) and Sharoff (2004)), as basis for
helping to determine whether a given sequence
of words is in fact an MWE. Although some re-
search aims at developing methods for dealing
with MWEs in general (e.g. Zhang et al (2006),
Ramisch et al (2008)), there is also some work that
deals with specific types of MWEs (e.g. Pearce
(2002) on collocations and Villavicencio (2005)
on verb-particle constructions (VPCs)) as each of
these MWE types has distinct distributional and
linguistic characteristics.
VPCs are combinations of verbs and particles,
such as take off in Our plane took off late, that due
to their complex characteristics and flexible na-
ture, provide a real challenge for NLP. In particu-
lar, there is a lack of adequate resources to identify
and treat them, and those that are available provide
only limited coverage, in face of the huge number
of combinations in use. For tasks like parsing and
generation, it is essential to know whether a given
VPC is possible or not, to avoid for example us-
ing combinations that sound unnatural or ungram-
matical to native speakers (e.g. give/lend/?grant
out for the conveying of something to someone or
some place - (Fraser, 1976)).
1
Thus, the knowl-
edge of which combinations are possible is cru-
cial for precision grammar engineering. In ad-
dition, as the semantics of VPCs varies from the
idiomatic to the more compositional cases, meth-
ods for the automatic detection and handling of id-
iomaticity are very important for any NLP task that
involves some degree of semantic interpretation
such as Machine Translation (in this case avoiding
the problem of producing an unrelated translation
for a source sentence). Automatic methods for the
identification of idiomaticity in MWEs have been
1
See Baldwin et al (2004) for a discussion of the effects of
multiword expressions like VPCs on a parser?s performance.
49
proposed using a variety of approaches such as
statistical, substitutional, distributional, etc. (e.g.
McCarthy et al (2003), Bannard (2005) and Fa-
zly and Stevenson (2006)). In particular, Fazly
and Stevenson (2006) look at the correlation be-
tween syntactic fixedness (in terms of e.g. pas-
sivisation, choice of determiner type and pluralisa-
tion) and non-compositionality of verb-noun com-
pounds such as shoot the breeze.
In this work we investigate the automatic extrac-
tion of VPCs, looking into a variety of methods,
combining linguistic with statistical information,
ranging from frequencies to association measures:
Mutual Information (MI), ?
2
and Entropy. We also
investigate the determination of compositionality
of VPCs verifying whether the degree of semantic
flexibility of a VPC combined with some statisti-
cal information can be used to determine if it is
idiomatic or compositional.
This paper starts with a brief description of
VPCs, research on their automatic identification
and determination of their semantics (? 2). We then
explain the research questions and the assumptions
that serve as the basis for the application of statis-
tical measures (? 3) on the dataset (? 4). Our meth-
ods and experiments are then detailed (? 5), and
the results obtained are analysed (? 6). We con-
clude with a discussion of the contributions that
this work brings to the research on verb-particle
constructions (? 7).
2 Verb-Particle Constructions in Theory
and Practice
Particles in VPCs are characterised by containing
features of motion-through-location and of com-
pletion or result in their core meaning (Bolinger,
1971). VPCs can range from idiosyncratic or semi-
idiosyncratic combinations, such as get on (in e.g.
Bill got on well with his new colleagues), to more
regular ones, such as tear up (e.g. in In a rage she
tore up the letter Jack gave her). A three way clas-
sification is adopted by (Deh?e, 2002) and (Jack-
endoff, 2002), where a VPC can be classified as
compositional, idiomatic or aspectual, depending
on its sense. In compositional VPCs the meaning
of the construction is determined by the literal in-
terpretations of the particle and the verb. These
VPCs usually involve particles with directional or
spatial meaning, and these can often be replaced
by the appropriate directional PPs (e.g. carry in
in Sheila carried the bags in/into the house Deh?e
(2002)). Idiomatic VPCs, on the other hand, can-
not have their meaning determined by interpreting
their components literally (e.g. get on, meaning to
be on friendly terms with someone). The third class
is that of aspectual VPCs, which have the parti-
cle providing the verb with an endpoint, suggesting
that the action described by the verb is performed
completely, thoroughly or continuously (e.g. tear
up meaning to tear something into a lot of small
pieces).
From a syntactic point of view, a given combi-
nation can occur in several different subcategorisa-
tion frames. For example, give up can occur as an
intransitive VPC (e.g. in I give up! Tell me the an-
swer), where no other complement is required, or
it may occur as a transitive VPC which requires a
further NP complement (e.g. in She gave up alco-
hol while she was pregnant ). Since in English par-
ticles tend to be homographs with prepositions (up,
out, in), a verb followed by a preposition/particle
and an NP can be ambiguous between a transitive
VPC and a prepositional verb (e.g. rely on, in He
relies on his wife for everything). Some criteria
that characterise VPCs are discussed by Bolinger
(1971):
2
C1 In a transitive VPC the particle may come ei-
ther before or after the NP (e.g. He backed
up the team vs. He backed the team up).
However, whether a particle can be separated
or not from the verb may depend on the de-
gree of bonding between them, the size of the
NP, and the kind of NP. This is considered by
many to be sufficient condition for diagnos-
ing a VPC, as prepositions can only appear in
a position contiguous to the verb (e.g. *He
got the bus off ).
C2 Unstressed personal pronouns must precede
the particle (e.g. They ate it up but not *They
ate up it).
C3 If the particle precedes a simple definite NP,
the particle does not take the NP as its object
(e.g. in He brought along his girlfriend) un-
like with PP complements or modifiers (e.g.
in He slept in the hotel). This means that in
the first example the NP is not a complement
of the particle along, while in the second it is.
2
The distinction between a VPC and a prepositional verb
may be quite subtle, and as pointed out by Bolinger, many
of the criteria proposed for diagnosing VPCs give different
results for the same combination, frequently including un-
wanted combinations and excluding genuine VPCs.
50
In this paper we use the first two criteria, therefore
the candidates may contain noise (in the form of
prepositional verbs and related constructions).
VPCs have been the subject of a considerable
amount of interest, and some analysis has been
done on the subject of productive VPCs. In many
cases the particle seems to be compositionally
adding a specific meaning to the construction and
following a productive pattern (e.g. in tear up,
cut up and split up, where the verbs are seman-
tically related and up adds a sense of completion
to the action of these verbs). Fraser (1976) points
out that semantic properties of verbs can affect
their ability to combine with particles: for exam-
ple, bolt/cement/clamp/glue/paste/nail are seman-
tically similar verbs where the objects represented
by the verbs are used to join material, and they can
all combine with down. There is clearly a com-
mon semantic thread running through this list, so
that a new verb that is semantically similar to them
can also be reasonably assumed to combine with
down. Indeed, frequently new VPCs are formed by
analogy with existing ones, where often the verb is
varied and the particle remains (e.g. hang on, hold
on and wait on). Similarly, particles from a given
semantic class can be replaced by other particles
from the same class in compositional combina-
tions: send up/in/back/away (Wurmbrand, 2000).
By identifying classes of verbs that follow patterns
such as these in VPCs, we can help in the identi-
fication of a new unknown candidate combination,
using the degree of productivity of a class to which
the verb belongs as a back-off strategy.
In terms of methods for automatic identifica-
tion of VPCs from corpora, Baldwin (2005) pro-
poses the extraction of VPCs with valence infor-
mation from raw text, exploring a range of tech-
niques (using (a) a POS tagger, (b) a chunker, (c) a
chunk grammar, (d) a dependency parser, and (e) a
combination of all methods). Villavicencio (2005)
uses the Web as a corpus and productive patterns
of combination to generate and validate candidate
VPCs. The identification of compositionality in
VPCs is addressed by McCarthy et al (2003) who
examine the overlap of similar words in an auto-
matically acquired distributional thesaurus for verb
and VPCs, and by Bannard (2005) who uses a
distributional approach to determine when and to
what extent the components of a VPC contribute
their simplex meanings to the interpretation of the
VPC. Both report a correlation between some of
the measures and compositionality judgements.
3 The Underlying Hypotheses
The problem of the automatic detection and classi-
fication of VPCs can be summarised as, for a given
VPC candidate, to answer to the questions:
Q1 Is it a real VPC or some free combination
of verb and preposition/adverb or a preposi-
tional verb?
Q2 If it is a true VPC, is it idiomatic or composi-
tional?
In order to answer the first question, we use two
assumptions. Firstly, we consider that the elements
of a true VPC co-occur above chance. The greater
the correlation between the verb and the particle
the greater the chance that the candidate is a true
VPC. Secondly, based on criterion C1 we also as-
sume that VPCs have more flexible syntax and are
more productive than non-VPCs. This second as-
sumption goes against what is usually adopted for
general MWEs, since it is the prepositional verbs
that allow less syntactic configurations than VPCs
and are therefore more rigid (? 2). To further dis-
tinguish VPCs from prepositional verbs and other
related constructions we also verify the possibil-
ity of the particle to be immediately followed by
an indirect prepositional complement (like in The
plane took off from London), which is a good in-
dicator/delimiter of a VPC since in non-VPC con-
structions like prepositional verbs the preposition
needs to have an NP complement. Therefore, we
will assume that a true VPC occurs in the following
configurations, according to Villavicencio (2005)
and Ramisch et al (2008):
S1 VERB + PARTICLE + DELIMITER, for intran-
sitive VPCs;
S2 VERB + NP + PARTICLE + DELIMITER, for
transitive split VPCs and;
S3 VERB + PARTICLE + NP + DELIMITER, for
transitive joint VPCs.
In order to answer Q2, we look at the link be-
tween productivity and compositionality and as-
sume that a compositional VPC accepts the sub-
stitution of one of its members by a semantically
related term. This is in accordance to Fraser
(1976), who shows that semantic properties of
51
verbs can affect their ability to combine with par-
ticles: for example verbs of hunting combining
with the resultative down (hunt/track/trail/follow
down) and verbs of cooking with the aspectual up
(bake/cook/fry/broil up), forming essentially pro-
ductive VPCs. Idiomatic VPCs, however, will
not accept the substitution of one of its members
by a related term (e.g. get and its synonyms in
get/*obtain/*receive over), even if at first glance
this could seem natural. In our experiments, we
will consider that a VPC is compositional if it ac-
cepts: the replacement of the verb by a synonym,
or of the preposition by another preposition. Sum-
marising our hypothesis, we get:
? For Q1: Is the candidate syntactically flexi-
ble, i.e. does it allow the configurations S1
through S3?
? NO: non-VPC
? YES: VPC
? For Q2: Is the candidate semantically flexi-
ble, allowing the substitution of a member by
a related word?
? NO: idiomatic VPC
? YES: compositional VPC
4 Data Sources
To generate a gold standard, we used the Bald-
win VPC candidates dataset (henceforth Baldwin
CD)
3
, which contains 3,078 English VPC candi-
dates annotated with information about idiomatic-
ity (14.5% are considered idiomatic). We fur-
ther annotated this dataset with information about
whether each candidate is a genuine VPC or not,
where a candidate is consider genuine if it be-
longs to at least one of a set of machine-readable
dictionaries: the Alvey Natural Language Tools
(ANLT) lexicon (Carroll and Grover, 1989), the
Comlex lexicon (Macleod and Grishman, 1998),
and the LinGO English Resource Grammar (ERG)
(Copestake and Flickinger, 2000)
4
. With this crite-
rion 81.8% of them are considered genuine VPCs.
To gather information about the candidates in
this work we employ both a fragment of 1.8M
sentences from the British National Corpus (BNC
Burnard (2000)) and the Web as corpora. The
BNC fragment is used to calculate the correlation
3
This dataset was provided by Timothy Baldwin for the
MWE2008 Workshop.
4
Version of November 2001.
measures since they require a corpus with known
size. The Web is used to generate frequencies
for the entropy measures, as discussed in ? 5.2.
Web frequencies are approximated by the number
of pages containing a candidate and indexed by
Yahoo Search API. In order to keep the searches
as simple and self-sufficient as possible, no addi-
tional sources of information are used (Villavicen-
cio, 2005). Therefore, the frequencies are quite
conservative in the sense that by employing in-
flected forms of verbs, potentially much more evi-
dence could be gathered.
For the generation of semantic variational pat-
terns, we use both Wordnet 3.0 (Fellbaum, 1998)
and Levin?s English Verb Classes and Alternations
(Levin, 1993). Wordnet is organised as a graph of
concepts, called synsets, linked by relations of syn-
onymy, hyponymy, etc. Each synset contains a list
of words that represent the concept. The verbs in
a synset and its synonym synsets are used to gen-
erate variations of a VPC candidate. Likewise we
use Levin?s classes, which define 190 fine-grained
classes for English verbs, based on their syntactic
and semantic features.
It is important to highlight that the generation
of the semantic variations strongly relies on these
resources. Therefore, cross-language extension
would depend on the availability of similar tools
for the target language.
5 Carrying out the experiments
Our experiments are composed of two stages, each
one consisting of three steps (corresponding to the
next three sections). The first stage filters out ev-
ery candidate that is evaluated as not being a VPC,
while the second one intends to identify the id-
iomatic VPCs among the remaining candidates of
the previous stage.
5.1 Generating candidates
For each of the 3,078 items in the Baldwin CD we
generated 2 sets of variations, syntactic and seman-
tic, and we will refer to these as alternative forms
or variations of a candidate.
The syntactic variations are generated using the
patterns S1 to S3 described in section 3. Following
the work of Villavicencio (2005) 3 frequently used
prepositions for, from and with are used as delim-
iters and we search for NPs in the form of pronouns
like this and definite NPs like the boy. The use of
alternative search patterns also helps to give an in-
52
dication about the syntactic distribution of a can-
didate VPC, and consequently if it has a preferred
syntactic realisation. For instance, for eat up and
the delimiter with, we propose a list of Web search
queries for its respective variations v
i
, shown with
their corresponding Web frequencies in table 1.
5
Variation (v
i
) Frequency (n
Y ahoo
(v
i
))
eat up with 49200
eat the * up with 2240
eat this up with 1120
eat up the * with 3110
Table 1: Distribution of syntactic variations for the
candidate eat up.
For the semantic variations, in order to capture
the idiomaticity of VPCs we generate the alterna-
tive forms by replacing the verb by its synonym
verbs as follows:
WNS Wordnet Strict variations. When using Word-
net, we consider any verb that belongs to the
same synset of the candidate as a synonym.
WNL Wordnet Loose variations. This is an indi-
rect synonymy relation capturing any verb
in Wordnet that belongs either to the same
synset or to a synset that is synonym of the
synset in which the candidate verb is con-
tained.
Levin These include all verbs in the same Levin
class as the candidate.
Multiword synonyms are ignored in this step to
avoid noisy search patterns, (e.g. *eat up up). The
examples for these variations are shown in table 2
for the candidate act in.
Wordnet and Levin are considered ambiguous
resources because one verb is potentially contained
in several synsets or classes. However, as Word
Sense Disambiguation is not within the scope of
this work we employ some heuristics to select a
given sense for the candidate verb. In order to test
the effect of frequency, the first heuristic adopts the
first synset in the list, as Wordnet organises synsets
in descending order of frequency (denoted as first).
To study the influence of the number of synonyms,
the second and third heuristics use respectively the
biggest (max) and smallest (min) synsets. The last
5
The Yahoo wildcard used in these searches matches any
word occurring in that particular position.
Variation (v
i
) Source n
Y ahoo
(v
i
)
act in ? 2690
playact in WNS 0
play in WNS 167000
behave in WNL 98
do in WNL 24600
pose in Levin 1610
qualify in Levin 358
rank in Levin 706
rate in Levin 16700
serve in Levin 2240
Table 2: Distribution of syntactic variations for the
candidate eat up.
heuristic is the union of all synonyms (all). These
heuristics are indicated using a subscript notation,
where e.g. WNS
all
symbolizes the WNS varia-
tions set using the union of all synsets as disam-
biguation heuristic. Finally, we generated two
additional sets of candidates by replacing the par-
ticle by one of the 48 prepositions listed in the
ANLT dictionary (prep) and also by one of 9 cho-
sen locative prepositions (loc-prep). It is impor-
tant to also verify possible variations of the prepo-
sition because compositional VPCs combine pro-
ductively with one or more groups of particles, e.g.
locatives, and present consequently a wider prob-
ability distribution among the variations, while an
idiomatic VPC presents a higher frequency for a
chosen preposition.
5.2 Working the statistical measures out
The classifications of the candidate VPCs are done
using a set of measures: the frequencies of the
VPC candidates and of their individual words,
their Mutual Information (MI), ?
2
and Entropies.
We calculate the MI and ?
2
indices of a candidate
formed by a verb and a particle based on their in-
dividual frequencies and on their co-occurrence in
the BNC fragment.
The Entropy measure is given by
H(V ) = ?
n
?
i=1
p(v
i
) ln [ p(v
i
) ]
where
p(v
i
) =
n(v
i
)
?
? v
j
?V
n(v
j
)
is the probability of the variation v
i
to occur
among the set of all possible variations V =
53
H(V ) ? 0.001081
n
BNC
(p) ? 51611
n
Y ahoo
(v
transitive
) ? 1
n
Y ahoo
(v) ? 2020000000 : yes
n
Y ahoo
(v) > 2020000000
?
2
? 25.99
? ? ?
Figure 1: Fragment of the decision tree that filters
out non-VPCs.
{v
1
, v
2
, . . . , v
n
}, and n(v
i
) is the Web frequency
for the variation v
i
.
The entropy of a probability distribution gives
us some clues about its shape. A very low en-
tropy is a sign of a heterogeneous distribution that
contains a peak. On the other hand, a distribution
that presents uniformity will lead to a high entropy
value.
The interest of H(V ) for the detection of VPCs
is in that true instances are more likely to not prefer
a canonical form, more widely distributing proba-
bilities over all alternative syntactic frames (S1 to
S3), while non-VPCs are more likely to choose one
frame and present low frequencies for the proposed
variations.
For the semantic variations, the entropy is cal-
culated from a set V of variations generated by the
Wordnet synset, Levin class and preposition sub-
stitutions described in ? 5.1. The interpretation of
the entropy at this point is that high entropy indi-
cates compositionality while low entropy indicates
idiomaticity, since compositional VPCs are more
productive and distribute well over a class of verbs
or a class of prepositions and idiomatic VPCs pre-
fer a specific verb or preposition.
5.3 Bringing estimations together
Once we got a set of measures to predict
VPCs and another to predict their idiomatic-
ity/compositionality, we would like to know which
measures are useful. Therefore, we combine our
measures automatically by building a decision tree
with the J48 algorithm, a version of the traditional
entropy-based C4.5 algorithm implemented in the
Weka package.
6
6 Weighting the results up
The first stage of our experiments applied to the
3,078 VPC candidates generated a decision tree us-
6
http://www.cs.waikato.ac.nz/ml/weka/
ing 10-fold cross validation that is partially repro-
duced in figure 1. From these, 2,848 candidates
were considered genuine VPCs, with 2,419 true
positives, 100 false negatives and 429 false posi-
tives. This leads to a recall of 96% of the VPCs
being kept in the list with a precision of 84.9%,
and an f-measure of 90.1%. We interpret this as a
very positive result since although some false neg-
atives have been filtered out, the remaining candi-
dates are now less noisy.
Figure 1 shows that the entropy of the variations
is the best predictor since it is at the root of the
tree. We can also see that there are several types
of raw frequencies being used before a correlation
measure appears (?
2
). We can conclude that the
frequency of each transitive, intransitive and split
configurations are also good predictors to detect
false from true VPCs. At this point, MI does not
seem to contribute to the classification task.
For our second stage, we generated Wordnet
synonym, Levin class and preposition variations
for a list of the 2,867 VPC candidates classified
as genuine cases. We also took into account the
proportion of synonyms that are MWEs (vpc-syn)
and the proportion of synonyms that contain the
candidate itself (self-syn).
In order to know what kind of contribution each
measure gives to the construction of the decision
tree, we used a simple iterative algorithm that con-
structs the set U of useful attributes. It first ini-
tialises U with all attributes, then calculates the
precision for each class (yes and no)
7
on a cross
validation using all attributes in U . For each at-
tribute a ? U , it ignores a and recalculates preci-
sions. If both precisions decrease, the contribution
of a is positive, if both increase then a is negative,
else its contribution remains unknown. All fea-
tures that contribute negatively are removed from
U , and the algorithm is repeated until there is no
negative attribute left.
The step-by-step execution of the algorithm
can be observed in table 3, where the inconclu-
sive steps are hidden. We found out that the
optimal features are U
?
= {self-syn, H(prep),
H(Levin
first
), H(WNS
first
), H(WNS
min
),
H(Levin
max
), H(Levin
min
).} The self-syn in-
formation seems to be very important, as without
it precisions of both classes decrease considerably
7
We use the precision as a quality estimator since it gives
a good idea of the amount of work that a grammar engineer
or lexicographer must perform in order to clear the list from
false positives.
54
Precision
# Ignored No Yes +/?
1
st
iteration
0 ? 86.6% 54.9%
1 vpc-syn 86.7% 56.6% ?
2 self-syn 85.2% 28.7% +
4 H(loc-prep) 86.7% 56.1% ?
6 H(WNS
max
) 87.5% 57.4% ?
9 H(WNLfirst) 86.7% 57.9% ?
10 H(WNL
max
) 86.7% 57.8% ?
11 H(WNL
min
) 86.9% 57.6% ?
16 H(Levin
all
) 86.7% 55.1% ?
2
nd
iteration
17 ? 87.7% 60.3%
18 H(prep) 87.6% 59.2% +
21 H(WNS
all
) 87.8% 61.6% ?
22 H(WNL
all
) 87.8% 61.0% ?
23 H(Levin
first
) 87.5% 60.2% +
3
rd
iteration
26 ? 87.8% 61.9%
27 H(WNS
first
) 87.8% 61.9% ?
28 H(WNS
min
) 87.7% 61.1% +
29 H(Levin
max
) 87.8% 61.6 ?
30 H(Levin
min
) 87.7% 61.5% +
Table 3: Iterative attributes selection process. Pre-
cision in each class is used as quality estimator.
(experiment #2).
All entropies of the WNL heuristics are of little
or no utility. This could probably be explained by
either the choice of simple WSD heuristics for se-
lecting synsets, or because the indirect synonymy
information is too far related to the original verb to
be used in variational patterns. Inspecting the gen-
erated variations, we notice that most of the syn-
onym synsets are related to secondary senses or
very specific uses of a verb and are thus not cor-
rectly disambiguated.
In what concerns the WNS sets, only the small-
est and first synset were kept, suggesting again that
it may not be a good idea to maximise the syn-
onyms set and for future work, we intent to es-
tablish a threshold for a synset to be taken into
account. In addition, we can also infer a posi-
tive contribution of the frequency of a sense with
the choice of the first synset returned by Word-
net resulting in a reasonable WSD heuristic (which
is compatible with the results by McCarthy et al
(2004)).
On the other hand, the algorithm selected the
first, the smallest and the biggest of the Levin?s
sets. This probably happens because the major-
ity of these verbs belongs only to one or two, but
never to a great number of classes. Since the gran-
ularity of the classes is coarser than for synsets,
the heuristics often offer four equal or very close
entropies and thus redundant information. As an
overall result, the last iteration shown in table 3
indicates a precision of 61.9% for the classifier in
detecting idiomatic VPCs, that is to say that we au-
tomatically retrieved 176 VPCs where 67 are false
positives and 109 are truly idiomatic. This value is
a quality estimator for the resulting VPCs that will
potentially be used in the construction of a lexi-
con. Recall of idiomatic VPCs goes from 16.7%
to 24.9%.
7 Conclusions
One of the important challenges for robust natu-
ral language processing systems is to be able to
successfully deal with Multiword Expressions and
related constructions. We investigated the identifi-
cation of VPCs using a combination of statistical
methods and linguistic information, and whether
there is a correlation between the productivity of
VPCs and their semantics that could help us detect
if a VPC is idiomatic or compositional.
The results confirm that the use of statistical
and linguistic information to automatically iden-
tify verb-particle constructions presents a reason-
able way of improving coverage of existing lexi-
cal resources in a very simple and straightforward
manner. In terms of grammar engineering, the in-
formation about compositional candidates belong-
ing to productive classes provides us with the ba-
sis for constructing a family of fine-grained redun-
dancy rules for these classes. These rules are ap-
plied in a constrained way to verbs already in the
lexicon, according to their semantic classes. The
VPCs identified as idiomatic, on the other hand,
need to be explicitly added to the lexicon, after
their semantic is determined. This study can also
be complemented with the results of investigations
into the semantics of VPCs, as discussed by both
Bannard (2005) and McCarthy et al (2003).
In addition, the use of clustering methods is an
interesting possibility for automatically identify-
ing clusters of productive classes of both verbs and
of particles that combine well together.
55
Acknowledgments
This research was partly supported by the CNPq
research project Recuperac??ao de Informac??oes
Multil??ng?ues (CNPq Universal 484585/2007-0).
References
Baldwin, Timothy, Emily M. Bender, Dan Flickinger, Ara
Kim, and Stephan Oepen. 2004. Road-testing the English
Resource Grammar over the British National Corpus. In
Fourth International Conference on Language Resources
and Evaluation (LREC 2004), Lisbon, Portugal.
Baldwin, Timothy. 2005. Deep lexical acquisition of verb-
particle constructions. Computer Speech and Language,
19(4):398?414.
Bannard, Colin J. 2005. Learning about the meaning of verb-
particle constructions from corpora. Computer Speech and
Language, 19(4):467?478.
Bolinger, Dwight. 1971. The phrasal verb in English. Har-
vard University Press, Harvard, USA.
Burnard, Lou. 2000. User reference guide for the British Na-
tional Corpus. Technical report, Oxford University Com-
puting Services.
Carroll, John and Claire Grover. 1989. The derivation of a
large computational lexicon of English from LDOCE. In
Boguraev, B. and E. Briscoe, editors, Computational Lexi-
cography for Natural Language Processing. Longman.
Copestake, Ann and Dan Flickinger. 2000. An open-source
grammar development environment and broad-coverage
English grammar using HPSG. In Proceedings of the
2nd International Conference on Language Resources and
Evaluation (LREC 2000).
Deh?e, Nicole. 2002. Particle verbs in English: syntax, in-
formation structure and intonation. John Benjamins, Am-
sterdam/Philadelphia.
Evert, Stefan and Brigitte Krenn. 2005. Using small random
samples for the manual evaluation of statistical association
measures. Computer Speech and Language, 19(4):450?
466.
Fazly, Afsaneh and Suzanne Stevenson. 2006. Automatically
constructing a lexicon of verb phrase idiomatic combina-
tions. In EACL. The Association for Computer Linguis-
tics.
Fellbaum, Christiane, editor. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Communica-
tion). The MIT Press, May.
Fraser, Bruce. 1976. The Verb-Particle Combination in En-
glish. Academic Press, New York, USA.
Jackendoff, Ray. 2002. English particle constructions, the
lexicon, and the autonomy of syntax. In N. Deh?e, R. Jack-
endoff, A. McIntyre and S. Urban, editors, Verb-Particle
Explorations. Berlin: Mouton de Gruyter.
Levin, Beth. 1993. English Verb Classes and Alternations:
a preliminary investigation. University of Chicago Press,
Chicago and London.
Macleod, Catherine and Ralph Grishman. 1998. Comlex syn-
tax reference manual, Proteus Project.
McCarthy, Diana, Bill Keller, and John Carroll. 2003. De-
tecting a continuum of compositionality in phrasal verbs.
In Proceedings of the ACL 2003 workshop on Multiword
expressions, pages 73?80, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
McCarthy, Diana, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Finding predominant word senses in untagged
text. In Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics, page 279. Associa-
tion for Computational Linguistics.
Pearce, Darren. 2002. A comparative evaluation of colloca-
tion extraction techniques. In Third International Confer-
ence on Language Resources and Evaluation, Las Palmas,
Canary Islands, Spain.
Ramisch, Carlos, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for the ex-
traction of multiword expressions. In Proceedings of the
LREC Workshop - Towards a Shared Task for Multiword
Expressions (MWE 2008), pages 50?53, Marrakech, Mo-
rocco, June.
Sharoff, Serge. 2004. What is at stake: a case study of rus-
sian expressions starting with a preposition. pages 17?23,
Barcelona, Spain.
Villavicencio, Aline. 2005. The availability of verb-particle
constructions in lexical resources: How much is enough?
Journal of Computer Speech and Language Processing,
19(4):415?432.
Wurmbrand, S. 2000. The structure(s) of particle verbs. Ms.,
McGill University.
Zhang, Yi, Valia Kordoni, Aline Villavicencio, and Marco
Idiart. 2006. Automated multiword expression prediction
for grammar engineering. In Proceedings of the Workshop
on Multiword Expressions: Identifying and Exploiting Un-
derlying Properties, pages 36?44, Sydney, Australia. As-
sociation for Computational Linguistics.
56
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 74?82,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Identifying and Analyzing
Brazilian Portuguese Complex Predicates
Magali Sanches Duran? Carlos Ramisch? ? Sandra Maria Alu??sio? Aline Villavicencio?
? Center of Computational Linguistics (NILC), ICMC, University of Sa?o Paulo, Brazil
? Institute of Informatics, Federal University of Rio Grande do Sul, Brazil
? GETALP ? LIG, University of Grenoble, France
magali.duran@uol.com.br ceramisch@inf.ufrgs.br
sandra@icmc.usp.br avillavicencio@inf.ufrgs.br
Abstract
Semantic Role Labeling annotation task de-
pends on the correct identification of pred-
icates, before identifying arguments and as-
signing them role labels. However, most pred-
icates are not constituted only by a verb: they
constitute Complex Predicates (CPs) not yet
available in a computational lexicon. In order
to create a dictionary of CPs, this study em-
ploys a corpus-based methodology. Searches
are guided by POS tags instead of a limited list
of verbs or nouns, in contrast to similar stud-
ies. Results include (but are not limited to)
light and support verb constructions. These
CPs are classified into idiomatic and less id-
iomatic. This paper presents an in-depth anal-
ysis of this phenomenon, as well as an original
resource containing a set of 773 annotated ex-
pressions. Both constitute an original and rich
contribution for NLP tools in Brazilian Por-
tuguese that perform tasks involving seman-
tics.
1 Introduction
Semantic Role Labeling (SRL), independently of
the approach adopted, comprehends two steps be-
fore the assignment of role labels: (a) the delimita-
tion of argument takers and (b) the delimitation of
arguments. If the argument taker is not correctly
identified, the argument identification will propa-
gate the error and SRL will fail. Argument tak-
ers are predicates, frequently represented only by a
verb and occasionally by Complex Predicates (CPs),
that is, ?predicates which are multi-headed: they are
composed of more than one grammatical element?
(Alsina et al, 1997, p. 1), like give a try, take care,
take a shower. In SRL, the verbal phrases (VPs)
identified by a parser are usually used to automat-
ically identify argument takers, but do no suffice.
A lexicon of CPs, as well as the knowledge about
verbal chains composition, would complete a fully
automatic identification of argument takers. Con-
sequently, the possibility of disagreement between
SRL annotators would rely only on the assignment
of role labels to arguments. This paper reports the
investigation of such multi-word units, in order to
meet the needs arisen from an SRL annotation task
in a corpus of Brazilian Portuguese1.
To stress the importance of these CPs for SRL,
consider the sentence John takes care of his business
in three alternatives of annotation:
The first annotation shows care of his business as
a unique argument, masking the fact that this seg-
ment is constituted of a predicative noun, care, and
its internal argument, of his business. The second
annotation shows care and of his business as argu-
ments of take, which is incorrect because of his busi-
ness is clearly an argument of care. The third an-
notation is the best for SRL purposes: as a unique
predicate ? take care, take shares its external argu-
1CPs constituted by verbal chains (e.g. have been working)
are not focused here.
74
ment with care and care shares its internal argument
with take.
The goal of this paper is twofold: first, we briefly
describe our computer-aided corpus-based method
used to build a comprehensive machine-readable
dictionary of such expressions. Second and most
important, we analyze these expressions and their
behavior in order to shed some light on the most ad-
equate lexical representation for further integration
of our resource into an SRL annotation task. The re-
sult is a database of 773 annotated CPs, that can be
used to inform SRL and other NLP applications.
In this study we classify CPs into two groups: id-
iomatic CPs and less idiomatic CPs. Idiomatic CPs
are those whose sense may not be inferred from their
parts. Examples in Portuguese are fazer questa?o
(make a point), ir embora (go away), dar o fora (get
out), tomar conta (take care), dar para tra?s (give
up), dar de ombros (shrug), passar mal (get sick).
On the other hand, we use ?less idiomatic CPs? to
refer to those CPs that vary in a continuum of differ-
ent levels of compositionality, from fully composi-
tional to semi-compositional sense, that is, at least
one of their lexical components may be litterally
understood and/or translated. Examples of less id-
iomatic CPs in Portuguese are: dar instruc?a?o (give
instructions), fazer menc?a?o (make mention), tomar
banho (take a shower), tirar foto (take a photo), en-
trar em depressa?o (get depressed), ficar triste (be-
come sad).
Less idiomatic CPs headed by a predicative noun
have been called in the literature ?light verb con-
structions? (LVC) or ?support verb constructions?
(SVC). Although both terms have been employed as
synonyms, ?light verb? is, in fact, a semantic con-
cept and ?support verb? is a syntactic concept. The
term ?light verb? is attributed to Jespersen (1965)
and the term ?support verb? was already used by
Gross in 1981. A light verb is the use of a poly-
semous verb in a non prototypical sense or ?with a
subset of their [its] full semantic features?, North
(2005). On the other hand, a support verb is the
verb that combines with a noun to enable it to fully
predicate, given that some nouns and adjectives may
evoke internal arguments, but need to be associated
with a verb to evoke the external argument, that is,
the subject. As the function of support verb is almost
always performed by a light verb, attributes of LVCs
and SVCs have been merged, making them near syn-
onyms. Against this tendency, this study will show
cases of SVCs without light verbs (trazer preju??zo =
damage, lit. bring damage) and cases of LVCs with-
out support verbs (dar certo = work well, lit. give
correct).
To the best of our knowledge, to date, there is no
similar study regarding these complex predicates in
Brazilian Portuguese, focusing on the development
of a lexical resource for NLP tasks, such as SRL.
The remainder of this paper is organized as follows:
in ?2 we discuss related work, in ?3 we present the
corpus and the details about our methodology, in ?4
we present and discuss the resulting lists of candi-
dates, in ?5 we envisage further work and draw our
conclusions.
2 Related Work
Part of the CPs focused on here are represented by
LVCs and SVCs. These CPs have been studied in
several languages from different points of view: di-
acronic (Ranchhod, 1999; Marchello-Nizia, 1996),
language contrastive (Danlos and Samvelian, 1992;
Athayde, 2001), descriptive (Butt, 2003; Langer,
2004; Langer, 2005) and for NLP purposes (Salkoff,
1990; Stevenson et al, 2004; Barreiro and Cabral,
2009; Hwang et al, 2010). Closer to our study,
Hendrickx et al (2010) annotated a Treebank of 1M
tokens of European Portuguese with almost 2,000
CPs, which include LVCs and verbal chains. This
lexicon is relevant for many NLP applications, no-
tably for automatic translation, since in any task in-
volving language generation they confer fluency and
naturalness to the output of the system.
Work focusing on the automatic extraction of
LVCs or SVCs often take as starting point a list of re-
current light verbs (Hendrickx et al, 2010) or a list
of nominalizations (Teufel and Grefenstette, 1995;
Dras, 1995; Hwang et al, 2010). These approaches
are not adopted here because our goal is precisely
to identify which are the verbs, the nouns and other
lexical elements that take part in CPs.
Similar motivation to study LVCs/SVCs (for
SRL) is found within the scope of Framenet (Atkins
et al, 2003) and Propbank (Hwang et al, 2010).
These projects have taken different decisions on how
to annotate such constructions. Framenet annotates
75
the head of the construction (noun or adjective) as
argument taker (or frame evoker) and the light verb
separately; Propbank, on its turn, first annotates sep-
arately light verbs and the predicative nouns (as
ARG-PRX) and then merges them, annotating the
whole construction as an argument taker.
We found studies regarding Portuguese
LVCs/SVCs in both European (Athayde, 2001;
Rio-Torto, 2006; Barreiro and Cabral, 2009; Duarte
et al, 2010) and Brazilian Portuguese (Neves,
1996; Conejo, 2008; Silva, 2009; Abreu, 2011). In
addition to the variations due to dialectal aspects, a
brief comparison between these papers enabled us
to verify differences in combination patterns of both
variants. In addition, Brazilian Portuguese studies
do not aim at providing data for NLP applications,
whereas in European Portuguese there are at least
two studies focusing on NLP applications: Barreiro
and Cabral (2009), for automatic translation and
Hendrickx et al (2010) for corpus annotation.
3 Corpus, Extraction Tool and Methods
We employ a corpus-based methodology in order to
create a dictionary of CPs. After a first step in which
we use a computer software to automatically extract
candidate n-grams from a corpus, the candidate lists
have been analyzed by a linguist to distinguish CPs
from fully compositional word sequences.
For the automatic extraction, the PLN-BR-FULL2
corpus was used, consisting of news texts from
Folha de Sa?o Paulo from 1994 to 2005, with
29,014,089 tokens. The corpus was first prepro-
cessed for sentence splitting, case homogeniza-
tion, lemmatization and POS tagging using the
PALAVRAS parser (Bick, 2000).
Differently from the studies referred to in Sec-
tion 2, we did not presume any closed list of light
verbs or nouns as starting point to our searches. The
search criteria we used contain seven POS patterns
observed in examples collected during previous cor-
pus annotation tasks3:
1. V + N + PRP: abrir ma?o de (give up, lit. open
hand of );
2www.nilc.icmc.usp.br/plnbr
3V = VERB, N = NOUN, PRP = PREPOSITION, DET =
DETERMINER, ADV = ADVERB, ADJ = ADJECTIVE.
2. V + PRP + N: deixar de lado (ignore, lit. leave
at side);
3. V + DET + N + PRP: virar as costas para
(ignore, lit. turn the back to);
4. V + DET + ADV: dar o fora (get out, lit. give
the out);
5. V + ADV: ir atra?s (follow, lit. go behind);
6. V + PRP + ADV: dar para tra?s (give up, lit.
give to back);
7. V + ADJ: dar duro (work hard, lit. give hard).
This strategy is suitable to extract occurrences
from active sentences, both affirmative and negative.
Cases which present intervening material between
the verb and the other element of the CP are not cap-
tured, but this is not a serious problem considering
the size of our corpus, although it influences the fre-
quencies used in candidate selection. In order to fa-
cilitate human analysis of candidate lists, we used
the mwetoolkit4: a tool that has been developed
specifically to extract MWEs from corpora, which
encompasses candidate extraction through pattern
matching, candidate filtering (e.g. through associa-
tion measures) and evaluation tools (Ramisch et al,
2010). After generating separate lists of candidates
for each pattern, we filtered out all those occurring
less than 10 times in the corpus. The entries re-
sulting of automatic identification were classified by
their frequency and their annotation is discussed in
the following section.
4 Discussion
Each pattern of POS tags returned a large number
of candidates. Our expectation was to identify CPs
among the most frequent candidates. First we an-
notated ?interesting? candidates and then, in a deep
analysis, we judged their idiomaticity. In the Table
1, we show the total number of candidates extracted
before applying any threshold, the number of an-
alyzed candidates using a threshold of 10 and the
number of CPs by pattern divided into two columns:
idiomatic and less idiomatic CPs. Additionally, each
CP was annotated with one or more single-verb
4www.sf.net/projects/mwetoolkit
76
Pattern Extracted Analyzed Less idiomatic Idiomatic
V + N + PRP 69,264 2,140 327 8
V + PRP + N 74,086 1,238 77 8
V + DET + N + PRP 178,956 3,187 131 4
V + DET + ADV 1,537 32 0 0
V + ADV 51,552 3,626 19 41
V + PREP + ADV 5,916 182 0 2
V + ADJ 25,703 2,140 145 11
Total 407,014 12,545 699 74
Table 1: Statistics for the Patterns.
paraphrases. Sometimes it is not a simple task to
decide whether a candidate constitutes a CP, spe-
cially when the verb is a very polysemous one and
is often used as support verb. For example, fazer
exame em/de algue?m/alguma coisa (lit. make exam
in/of something/somebody) is a CP corresponding to
examinar (exam). But fazer exame in another use is
not a CP and means to submit oneself to someone
else?s exam or to perform a test to pass examina-
tions (take an exam). In the following sections, we
comment the results of our analysis of each of the
patterns.
4.1 VERB + NOUN + PREPOSITION
The pattern V + N is very productive, as every com-
plement of a transitive verb not introduced by prepo-
sition takes this form. For this reason, we restricted
the pattern, adding a preposition after the noun with
the aim of capturing only nouns that have their own
complements.
We identified 335 complex predicates, including
both idiomatic and less idiomatic ones. For exam-
ple, bater papo (shoot the breeze, lit. hit chat) or
bater boca (have an argument, lit. hit mouth) are
idiomatic, as their sense is not compositional. On
the other side, tomar conscie?ncia (become aware, lit.
take conscience) and tirar proveito (take advantage)
are less idiomatic, because their sense is more com-
positional. The candidates selected with the pattern
V + N + PRP presented 29 different verbs, as shown
in Figure 15.
Sometimes, causative verbs, like causar (cause)
5We provide one possible (most frequent sense) English
translation for each Portuguese verb.
and provocar (provoke) give origin to constructions
paraphrasable by a single verb. In spite of taking
them into consideration, we cannot call them LVCs,
as they are used in their full sense. Examples:
? provocar alterac?a?o (provoke alteration)= al-
terar (alter);
? causar tumulto (cause riot) = tumultuar (riot).
Some of the candidates returned by this pattern
take a deverbal noun, that is, a noun created from
the verb, as stated by most works on LVCs and
SVCs; but the opposite may also occur: some con-
structions present denominal verbs as paraphrases,
like ter simpatia por (have sympathy for) = simpati-
zar com (sympathize with) and fazer visita (lit. make
visit) = visitar (visit). These results oppose the idea
about LVCs resulting only from the combination of a
deverbal noun and a light verb. In addition, we have
identified idiomatic LVCs that are not paraphrasable
by verbs of the same word root, like fazer jus a (lit.
make right to) = merecer (deserve).
Moreover, we have found some constructions
that have no correspondent paraphrases, like fazer
sucesso (lit. make success) and abrir excec?a?o (lit.
open exception). These findings evidence that, the
most used test to identify LVCs and SVC ? the ex-
istence of a paraphrase formed by a single verb, has
several exceptions.
We have also observed that, when the CP has a
paraphrase by a single verb, the prepositions that in-
troduce the arguments may change or even be sup-
pressed, like in:
? Dar apoio a algue?m = apoiar algue?m (give sup-
port to somebody = support somebody);
77
atear (set (on fire))botar (put)
levar (carry)tornar-se (become)
tra?ar (trace)achar (find)
chamar (call)colocar (put)
ganhar (receive/win)lan?ar (throw)
pegar (take/grab)tirar (remove)
trazer (bring)bater (beat)
ficar (stay)p?r (put)
sentir (feel)firmar (firm)
pedir (ask)abrir (open)
causar (cause)fechar (close)
prestar (provide)provocar (provoke)
tomar (take)ser (be)
dar (give)ter (have)
fazer (make/do)
0 10 20 30 40 50 60 70 80
Idiomatic Non idiomatic
Figure 1: Distribution of verbs involved in CPs, consid-
ering the pattern V + N + PRP.
? Dar cabo de algue?m ou de alguma coisa =
acabar com algue?m ou com alguma coisa (give
end of somebody or of something = end with
somebody or with something).
Finally, some constructions are polysemic, like:
? Dar satisfac?a?o a algue?m (lit. give satisfaction
to somebody) = make somebody happy or pro-
vide explanations to somebody;
? Chamar atenc?a?o de algue?m (lit. call the at-
tention of somebody) = attract the attention of
somebody or reprehend somebody.
4.2 VERB + PREPOSITION + NOUN
The results of this pattern have too much noise, as
many transitive verbs share with this CP class the
same POS tags sequence. We found constructions
with 12 verbs, as shown in Figure 2. We classi-
fied seven of these constructions as idiomatic CPs:
dar de ombro (shrug), deixar de lado (ignore), po?r
de lado (put aside), estar de olho (be alert), ficar
de olho (stay alert), sair de fe?rias (go out on vaca-
tion). The later example is very interesting, as sair
de fe?rias is synonym of entrar em fe?rias (enter on
vacation), that is, two antonym verbs are used to ex-
press the same idea, with the same syntactic frame.
In the remaining constructions, the more frequent
ater (sonfansiter )ntonb)npf
nupter lnfvscter uptyf
ctser vt))funer lnf
)noter cteeyfc-n(ter teesonf
m?er mhpfpner -tonf
cd)dcter mhpfngpeter ngpnef
/ w k/ kw z/ zw ?/ ?w 0/
1asd2tpsc 3dgrsasd2tpsc
Figure 2: Distribution of verbs involved in CPs, consid-
ering the pattern V + PRP + N.
ateere (esonfieio)be (fsieio)rrn
pruie (aieeln)tvie ()icrn
)teare (yebofnuipre (-r yte)mn
ubeie ()seonamivie (aippn
i-ebe (t?ronhre (-r yte)mn
)re (miurndigre (vicr/wtn
k zk ?k 0k 1k 2k 3k
4wbtvi)ba 5to bwbtvi)ba
Figure 3: Distribution of verbs involved in CPs, consid-
ering the pattern V + DET + N + PRP.
verbs are used to give an aspectual meaning to the
noun: cair em, entrar em, colocar em, po?r em (fall
in, enter in, put in) have inchoative meaning, that is,
indicate an action starting, while chegar a (arrive at)
has a resultative meaning.
4.3 VERB + DETERMINER + NOUN +
PREPOSITION
This pattern gave us results very similar to the pat-
tern V + N + PRP, evidencing that it is possible
to have determiners as intervening material between
the verb and the noun in less idiomatic CPs. The
verbs involved in the candidates validated for this
pattern are presented in Figure 3.
The verbs ser (be) and ter (have) are special cases.
Some ter expressions are paraphrasable by an ex-
pression with ser + ADJ, for example:
? Ter a responsabilidade por = ser responsa?vel
por (have the responsibility for = be responsi-
ble for);
? Ter a fama de = ser famoso por (have the fame
of = be famous for);
78
? Ter a garantia de = ser garantido por (have the
guarantee of = be guaranteed for).
Some ter expressions may be paraphrased by a
single verb:
? Ter a esperanc?a de = esperar (have the hope of
= hope);
? Ter a intenc?a?o de = tencionar (have the inten-
tion of = intend);
? Ter a durac?a?o de = durar (have the duration of
= last).
Most of the ser expressions may be paraphrased
by a single verb, as in ser uma homenagem para =
homenagear (be a homage to = pay homage to). The
verb ser, in these cases, seems to mean ?to consti-
tute?. These remarks indicate that the patterns ser +
DET + N and ter + DET + N deserve further anal-
ysis, given that they are less compositional than they
are usually assumed in Portuguese.
4.4 VERB + DETERMINER + ADVERB
We have not identified any CP following this pattern.
It was inspired by the complex predicate dar o fora
(escape, lit. give the out). Probably this is typical in
spoken language and has no similar occurrences in
our newspaper corpus.
4.5 VERB + ADVERB
This pattern is the only one that returned more id-
iomatic than less idiomatic CPs, for instance:
? Vir abaixo = desmoronar (lit. come down =
crumble);
? Cair bem = ser adequado (lit. fall well = be
suitable);
? Pegar mal = na?o ser socialmente adequado (lit.
pick up bad = be inadequate);
? Estar de pe?6 = estar em vigor (lit. be on foot =
be in effect);
? Ir atra?s (de algue?m) = perseguir (lit. go behind
(somebody) = pursue);
6The POS tagger classifies de pe? as ADV.
? Partir para cima (de algue?m) = agredir (lit.
leave upwards = attack);
? Dar-se bem = ter sucesso (lit. give oneself well
= succeed);
? Dar-se mal = fracassar (lit. give oneself bad =
fail).
In addition, some CPs identified through this pat-
tern present a pragmatic meaning: olhar la? (look
there), ver la? (see there), saber la? (know there), ver
so? (see only), olhar so? (look only), provided they are
employed in restricted situations. The adverbials in
these expressions are expletives, not contributing to
the meaning, exception made for saber la?, (lit. know
there) which is only used in present tense and in first
and third persons. When somebody says ?Eu sei la??
the meaning is ?I don?t know?.
4.6 VERB + PREPOSITION + ADVERB
This is not a productive pattern, but revealed two
verbal expressions: deixar para la? (put aside) and
achar por bem (decide).
4.7 VERB + ADJECTIVE
Here we identified three interesting clusters:
1. Verbs of double object, that is, an object
and an attribute assigned to the object. These
verbs are: achar (find), considerar (con-
sider), deixar (let/leave), julgar (judge), man-
ter (keep), tornar (make) as in: Ele acha voce?
inteligente (lit. He finds you intelligent = He
considers you intelligent). For SRL annotation,
we will consider them as full verbs with two in-
ternal arguments. The adjective, in these cases,
will be labeled as an argument. However, con-
structions with the verbs fazer and tornar fol-
lowed by adjectives may give origin to some
deadjectival verbs, like possibilitar = tornar
poss??vel (possibilitate = make possible). Other
examples of the same type are: celebrizar
(make famous), esclarecer (make clear), evi-
denciar (make evident), inviabilizar (make un-
feasible), popularizar (make popular), respon-
sabilizar (hold responsible), viabilizar (make
feasible).
79
2. Expressions involving predicative adjectives,
in which the verb performs a functional role, in
the same way as support verbs do in relation to
nouns. In contrast to predicative nouns, pred-
icative adjectives do not select their ?support?
verbs: they combine with any verb of a restrict
set of verbs called copula. Examples of copula
verbs are: acabar (finish), andar (walk), con-
tinuar (continue), estar (be), ficar (stay), pare-
cer (seem), permanecer (remain), sair (go out),
ser (be), tornar-se (become), viver (live). Some
of these verbs add an aspect to the predica-
tive adjective: durative (andar, continuar, es-
tar, permanecer, viver) and resultative (acabar,
ficar, tornar-se, sair).
? The resultative aspect may be expressed
by an infix, substituting the combina-
tion of V + ADJ by a full verb: ficar
triste = entristecer (become sad) or by
the verbalization of the adjective in reflex-
ive form: ficar tranquilo = tranquilizar-se
(calm down); estar inclu??do = incluir-se
(be included).
? In most cases, adjectives preceded by cop-
ula verbs are formed by past participles
and inherit the argument structure of the
verb: estar arrependido de = arrepender-
se de (lit. be regretful of = regret).
3. Idiomatic CPs, like dar duro (lit. give hard =
make an effort), dar errado (lit. give wrong =
go wrong), fazer bonito (lit. make beautiful =
do well), fazer feio (make ugly = fail), pegar
leve (lit. pick up light = go easy), sair errado
(lit. go out wrong = go wrong), dar certo (lit.
give correct = work well).
4.8 Summary
We identified a total of 699 less idiomatic CPs
and observed the following recurrent pairs of para-
phrases:
? V = V + DEVERBAL N, e.g. tratar = dar trata-
mento (treat = give treatment);
? DENOMINAL V = V + N, e.g. amedrontar =
dar medo (frighten = give fear);
atear (set (on fire))botar (put)
lorrer (run)varantir (vuarantee)
cayer (be-caye)soar (sounm)
tornar?se (belohe)tradar (trale)
tratar (treat)lceirar (shegg)
fagar (shegg)ihavinar (spea/)
partir (ihavine)saber (geaye)
torler (/now)yager (wrinv)
yirar (be wortc)yogtar (turn)
vancar (vo bal/)gandar (releiye-win)
tirar (tcrow)traker (rehoye)
passar (brinv)alcar (pass-spenm)
sevuir (foggow)yer (see)
sentir (feeg)lair (fagg)
yir (lohe)bater (beat)
sair (vo out)firhar (firh)
pemir (as/)lcahar (lagg)
lcevar (arriye)ogcar (goo/)
lausar (lause)felcar (lgose)
geyar (geaye-get)ir (vo)
abrir (open)meizar (geaye-get)
pevar (ta/e-vrab)prestar (proyime)
proyolar (proyo/e)p?r (put)
tohar (ta/e)estar (be)
logolar (put)tornar (turn)
entrar (enter)ser (be)
mar (viye)filar (sta0)
ter (caye)faker (mo-ha/e)
1 21 31 41 51 611 621 631
7miohatil 8on imiohatil
Figure 4: Distribution of verbs involved in CPs, consid-
ering the total number of CPs (i.e. all patterns).
? DEADJECTIVAL V = V + ADJ, e.g. res-
ponsabilizar = tornar responsa?vel (lit. respon-
sibilize = hold responsible).
This will help our further surveys, as we may
search for denominal and deadjectival verbs (which
may be automatically recognized through infix and
suffix rules) to manually identify corresponding
CPs. Moreover, the large set of verbs involved in the
analyzed CPs, summarized in Figure 4, shows that
any study based on a closed set of light verbs will
be limited, as it cannot capture common exceptions
and non-prototypical constructions.
5 Conclusions and Future Work
This study revealed a large number of CPs and pro-
vided us insights into how to capture them with more
precision. Our approach proved to be very useful to
identify verbal MWEs, notably with POS tag pat-
80
terns that have not been explored by other studies
(patterns not used to identify LVCs/SVCs). How-
ever, due to the onus of manual annotation, we as-
sume an arbitrary threshold of 10 occurrences that
removes potentially interesting candidates. Our hy-
pothesis is that, in a machine-readable dictionary,
as well as in traditional lexicography, rare entries
are more useful than common ones, and we would
like to explore two alternatives to address this is-
sue. First, it would be straightforward to apply more
sophisticated filtering techniques like lexical asso-
ciation measures to our candidates. Second, we
strongly believe that our patterns are sensitive to
corpus genre, because the CPs identified are typical
of colloquial register. Therefore, the same patterns
should be applied on a corpus of spoken Brazilian
Portuguese, as well as other written genres like web-
crawled corpora. Due to its size and availability, the
latter would also allow us to obtain better frequency
estimators.
We underline, however, that we should not un-
derestimate the value of our original corpus, as it
contains a large amount of unexplored material. We
observed that only the context can tell us whether
a given verb is being used as a full verb or as a
light and/or support verb7. As a consequence, it
is not possible to build a comprehensive lexicon of
light and support verbs, because there are full verbs
that function as light and/or support verbs in spe-
cific constructions, like correr (run) in correr risco
(run risk). As we discarded a considerable number
of infrequent lexical items, it is possible that other
unusual verbs participate in similar CPs which have
not been identified by our study.
For the moment, it is difficult to assess a quan-
titative measure for the quality and usefulness of
our resource, as no similar work exists for Por-
tuguese. Moreover, the lexical resource presented
here is not complete. Productive patterns, the ones
involving nouns, must be further explored to enlarge
the aimed lexicon. A standard resource for English
like DANTE8, for example, contains 497 support
verb constructions involving a fixed set of 5 support
verbs, and was evaluated extrinsically with regard
to its contribution in complementing the FrameNet
7A verb is not light or support in the lexicon, it is light and/or
support depending on the combinations in which it participates.
8www.webdante.com
data (Atkins, 2010). Likewise, we intend to evalu-
ate our resource in the context of SRL annotation, to
measure its contribution in automatic argument taker
identification. The selected CPs will be employed in
an SRL project and, as soon as we receive feedback
from this experience, we will be able to report how
many CPs have been annotated as argument takers,
which will represent an improvement in relation to
the present heuristic based only on parsed VPs.
Our final goal is to build a broad-coverage lexicon
of CPs in Brazilian Portuguese that may contribute
to different NLP applications, in addition to SRL.
We believe that computer-assisted language learning
systems and other Portuguese as second language
learning material may take great profit from it. Anal-
ysis systems like automatic textual entailment may
use the relationship between CPs and paraphrases to
infer equivalences between propositions. Computa-
tional language generation systems may also want
to choose the most natural verbal construction to use
when generating texts in Portuguese. Finally, we be-
lieve that, in the future, it will be possible to enhance
our resource by adding more languages and by link-
ing the entries in each language, thus developing a
valuable resource for automatic machine translation.
Acknowledgements
We thank the Brazilian research foundation FAPESP
for financial support.
References
De?bora Ta??s Batista Abreu. 2011. A sema?ntica
de construc?o?es com verbos-suporte e o paradigma
Framenet. Master?s thesis, Sa?o Leopoldo, RS, Brazil.
1997. Complex Predicates. CSLI Publications, Stanford,
CA, USA.
Maria Francisca Athayde. 2001. Construc?o?es com
verbo-suporte (funktionsverbgefu?ge) do portugue?s e
do alema?o. Number 1 in Cadernos do CIEG Centro
Interuniversita?rio de Estudos German??sticos. Universi-
dade de Coimbra, Coimbra, Portugal.
Sue Atkins, Charles Fillmore, and Christopher R. John-
son. 2003. Lexicographic relevance: Selecting infor-
mation from corpus evidence. International Journal
of Lexicography, 16(3):251?280.
Sue Atkins, 2010. The DANTE Database: Its Contribu-
tion to English Lexical Research, and in Particular to
Complementing the FrameNet Data. Menha Publish-
ers, Kampala, Uganda.
81
Anabela Barreiro and Lu??s Miguel Cabral. 2009. ReE-
screve: a translator-friendly multi-purpose paraphras-
ing software tool. In Proceedings of the Workshop Be-
yond Translation Memories: New Tools for Transla-
tors, The Twelfth Machine Translation Summit, pages
1?8, Ottawa, Canada, Aug.
Eckhard Bick. 2000. The parsing system Palavras.
Aarhus University Press.
Miriam Butt. 2003. The light verb jungle. In Proceed-
ings of the Workshop on Multi-Verb Constructions,
pages 243?246, Trondheim, Norway.
Ca?ssia Rita Conejo. 2008. O verbo-suporte fazer na
l??ngua portuguesa: um exerc??cio de ana?lise de base
funcionalista. Master?s thesis, Maringa?, PR, Brazil.
Laurence Danlos and Pollet Samvelian. 1992. Transla-
tion of the predicative element of a sentence: category
switching, aspect and diathesis. In Proceedings of the
Fourth International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI),
pages 21?34, Montre?al, Canada.
Mark Dras. 1995. Automatic identification of support
verbs: A step towards a definition of semantic weight.
In Proceedings of the Eighth Australian Joint Confer-
ence on Artificial Intelligence, pages 451?458, Can-
berra, Australia. World Scientific Press.
Ine?s Duarte, Anabela Gonc?alves, Matilde Miguel,
Ama?lia Mendes, Iris Hendrickx, Fa?tima Oliveira,
Lu??s Filipe Cunha, Fa?tima Silva, and Purificac?a?o Sil-
vano. 2010. Light verbs features in European Por-
tuguese. In Proceedings of the Interdisciplinary Work-
shop on Verbs: The Identification and Representation
of Verb Features (Verb 2010), Pisa, Italy, Nov.
Iris Hendrickx, Ama?lia Mendes, S??lvia Pereira, Anabela
Gonc?alves, and Ine?s Duarte. 2010. Complex predi-
cates annotation in a corpus of Portuguese. In Pro-
ceedings of the ACL 2010 Fourth Linguistic Annota-
tion Workshop, pages 100?108, Uppsala, Sweden.
Jena D. Hwang, Archna Bhatia, Claire Bonial, Aous
Mansouri, Ashwini Vaidya, Yuping Zhou, Nianwen
Xue, and Martha Palmer. 2010. Propbank annota-
tion of multilingual light verb constructions. In Pro-
ceedings of the ACL 2010 Fourth Linguistic Annota-
tion Workshop, pages 82?90, Uppsala, Sweden.
Otto Jespersen. 1965. A Modern English Grammar on
Historical Principles. George Allen and Unwin Ltd.,
London, UK.
Stefan Langer. 2004. A linguistic test battery for sup-
port verb constructions. Special issue of Linguisticae
Investigationes, 27(2):171?184.
Stefan Langer, 2005. Semantik im Lexikon, chapter
A formal specification of support verb constructions,
pages 179?202. Gunter Naar Verlag, Tu?bingen, Ger-
many.
Christiane Marchello-Nizia. 1996. A diachronic survey
of support verbs: the case of old French. Langages,
30(121):91?98.
Maria Helena Moura Neves, 1996. Grama?tica do por-
tugue?s falado VI: Desenvolvimentos, chapter Estudo
das construc?o?es com verbos-suporte em portugue?s,
pages 201?231. Unicamp FAPESP, Campinas, SP,
Brazil.
Ryan North. 2005. Computational measures of the ac-
ceptability of light verb constructions. Master?s thesis,
Toronto, Canada.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. Multiword expressions in the wild? the
mwetoolkit comes in handy. In Proc. of the 23rd COL-
ING (COLING 2010) ? Demonstrations, pages 57?
60, Beijing, China, Aug. The Coling 2010 Organizing
Committee.
Elisabete Ranchhod, 1999. Lindley Cintra. Home-
nagem ao Homem, ao Mestre e ao Cidada?o, chap-
ter Construc?o?es com Nomes Predicativos na Cro?nica
Geral de Espanha de 1344, pages 667?682. Cosmos,
Lisbon, Portugal.
Grac?a Rio-Torto. 2006. O Le?xico: sema?ntica e
grama?tica das unidades lexicais. In Estudos sobre
le?xico e grama?tica, pages 11?34, Coimbra, Portugal.
CIEG/FLUL.
Morris Salkoff. 1990. Automatic translation of sup-
port verb constructions. In Proc. of the 13th COLING
(COLING 1990), pages 243?246, Helsinki, Finland,
Aug. ACL.
Hilda Monetto Flores Silva. 2009. Verbos-suporte ou
expresso?es cristalizadas? Soletras, 9(17):175?182.
Suzanne Stevenson, Afsaneh Fazly, and Ryan North.
2004. Statistical measures of the semi-productivity of
light verb constructions. In , Proc. of the ACL Work-
shop on MWEs: Integrating Processing (MWE 2004),
pages 1?8, Barcelona, Spain, Jul. ACL.
Simone Teufel and Gregory Grefenstette. 1995. Corpus-
based method for automatic identification of support
verbs for nominalizations. In Proc. of the 7th Conf.
of the EACL (EACL 1995), pages 98?103, Dublin, Ire-
land, Mar.
82
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 134?136,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Fast and Flexible MWE Candidate Generation
with the mwetoolkit
Vitor De Araujo? Carlos Ramisch? ? Aline Villavicencio?
? Institute of Informatics, Federal University of Rio Grande do Sul, Brazil
? GETALP ? LIG, University of Grenoble, France
{vbuaraujo,ceramisch,avillavicencio}@inf.ufrgs.br
Abstract
We present an experimental environment for
computer-assisted extraction of Multiword
Expressions (MWEs) from corpora. Candi-
date extraction works in two steps: generation
and filtering. We focus on recent improve-
ments in the former, for which we increased
speed and flexibility. We present examples
that show the potential gains for users and ap-
plications.
1 Project Description
The mwetoolkit was presented and demonstrated
in Ramisch et al (2010b) and in Ramisch et al
(2010a), and applied to several languages (Linardaki
et al, 2010) and domains (Ramisch et al, 2010c).
It is a downloadable open-source1 set of command-
line tools mostly written in Python. Our target users
are researchers with a background in computational
linguistics. The system performs language- and
type-independent candidate extraction in two steps2:
1. Candidate generation
? Pattern matching3
? n-gram counting
2. Candidate filtering
? Thresholds, stopwords and patterns
? Association measures, classifiers
1sf.net/projects/mwetoolkit
2For details, see previous papers and documentation
3The following attributes, if present, are supported for pat-
terns: surface form, lemma, POS, syntactic annotation.
The main contribution of our tool, rather than a
novel approach to MWE extraction, is an environ-
ment that systematically integrates the functionali-
ties found in other tools, that is, sophisticated cor-
pus queries like in CQP (Christ, 1994) and Manatee
(Rychly? and Smrz, 2004), candidate generation like
in Text::NSP (Banerjee and Pedersen, 2003), and fil-
tering like in UCS (Evert, 2004). The pattern match-
ing and n-gram counting steps are the focus of the
improvements described in this paper.
2 An Example
Our toy corpus, consisting of the first 20K sentences
of English Europarl v34, was POS-tagged and lem-
matized using the TreeTagger5 and converted into
XML. 6 As MWEs encompass several phenomena
(Sag et al, 2002), we define our target word se-
quences through the patterns shown in figure 1. The
first represents sequences with an optional (?) deter-
miner DET, any number (*) of adjectives A and one
or more (+) nouns N. This shallow pattern roughly
corresponds to noun phrases in English. The sec-
ond defines expressions in which a repeated noun is
linked by a preposition PRP. The backw element
matches a previous word, in this example the same
lemma as the noun identified as noun1.
After corpus indexing and n-gram pattern match-
ing, the resulting unique candidates are returned.
Examples of candidates captured by the first pattern
4statmt.org/europarl
5http://www.ims.uni-stuttgart.de/
projekte/corplex/TreeTagger
6For large corpora, XML imposes considerable overhead.
As corpora do not require the full flexibility of XML, we are
currently experimenting with plain-text, which is already in use
with the new C indexing routines.
134
<pat id="1">
<pat repeat="?"><w pos="DET"/></pat>
<pat repeat="*"><w pos="A"/></pat>
<pat repeat="+"><w pos="N"/></pat>
</pat>
<pat id="2">
<w pos="N" id="noun1"/>
<w pos="PRP"/>
<backw lemma="noun1" pos="noun1"/>
</pat>
Figure 1: Pattern 1 matches NPs, pattern 2 matches se-
quences N1 PRP N1.
are complicated administrative process, the clock,
the War Crimes Tribunal. The second pattern cap-
tures hand in hand, eye to eye, word for word. 7
3 New Features
Friendlier User Interface In the previous ver-
sion, one needed to manually invoke the Python
scripts passing the correct options. The current ver-
sion provides an interactive command-based inter-
face which allows simple commands to be run on
data files, while keeping the generation of interme-
diary files and the pipelining between the different
phases of MWE extraction implicit. At the end, a
user may want to save the session and restart the
work later.8
Regular Expression Support While in the previ-
ous version only wildcard words were possible, now
we support all the operators shown in figure 1 plus
repetition interval (2,3), multiple choice (either)
and in-word wildcards like writ* matching written,
writing, etc. All these extensions allow for much
more powerful candidate patterns to be expressed.
This means that one can also use syntax annotation if
the text is parsed: if two words separated by n words
share a syntactic head, they are extracted. Multi-
attribute patterns are correctly handled during pat-
tern matching, in spite of individual per-attribute in-
dices. Some scripts may fuse the individual indices
on the fly, producing a combined index (e.g. n-gram
counting).
7Currently only contiguous n-grams can be captured; non-
contiguous extraction (e.g., verb-noun pairs, with intervening
material, not part of the expression) is planned.
8Although it is not a graphical interface some users request,
it is far easier to use than the previous version.
Faster processing Candidate generation was not
able to deal with large corpora such as Europarl
and the BNC. The first optimization concerns pat-
tern matching: instead of using the XML corpus and
external matching procedures, now we match candi-
dates using Python?s builtin regular expressions di-
rectly on the corpus index. On a small corpus the
current implementation takes about 72% the origi-
nal time to perform pattern-based generation. On the
BNC, extraction of the two example patterns shown
before took about 4.5 hours and 1 hour, respectively.
The second optimization concerns the creation of
the index. The previous script allowed a static in-
dex to be created from the XML corpus, but it was
not scalable. Thus, we have rewritten index routines
in C. We still assume that the index must fit in main
memory, but the new routines provide faster index-
ing with reasonable memory consumption, propor-
tional to the corpus size. These scripts are still ex-
perimental and need extensive testing. With the C
index routines, indexing the BNC corpus took about
5 minutes per attribute on a 3GB RAM computer.
4 Future Improvements
Additionally to evaluation on several tasks and lan-
guages, we intend to develop several improvements
to the tool. First, we would like to rewrite the pattern
matching routines in C to speed the process up and
reduce memory consumption. Second, we would
like to test several heuristics to handle nested candi-
dates (current strategy returns all possible matches).
Third, we would like to perform more tests on us-
ing regular expressions to extract candidates based
on their syntax annotation. Fourth, we would like
to improve candidate filtering (not emphasized in
this paper) by testing new association measures, fil-
ters, context-based measures, etc. Last but most im-
portant, we are planning a new release version and
therefore we need extensive testing and documenta-
tion.
References
Satanjeev Banerjee and Ted Pedersen. 2003. The de-
sign, implementation, and use of the Ngram Statistic
Package. In Proceedings of the Fourth International
Conference on Intelligent Text Processing and Com-
135
putational Linguistics, pages 370?381, Mexico City,
Mexico, Feb.
Oli Christ. 1994. A modular and flexible architecture
for an integrated corpus query system. In COMPLEX
1994, Budapest, Hungary.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis, In-
stitut fu?r maschinelle Sprachverarbeitung, University
of Stuttgart, Stuttgart, Germany.
Evita Linardaki, Carlos Ramisch, Aline Villavicencio,
and Aggeliki Fotopoulou. 2010. Towards the con-
struction of language resources for greek multiword
expressions: Extraction and evaluation. In Stelios
Piperidis, Milena Slavcheva, and Cristina Vertan, ed-
itors, Proc. of the LREC Workshop on Exploitation
of multilingual resources and tools for Central and
(South) Eastern European Languages, pages 31?40,
Valetta, Malta. May.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010a. Multiword expressions in the wild?
the mwetoolkit comes in handy. In Proc. of the 23rd
COLING (COLING 2010) ? Demonstrations, pages
57?60, Beijing, China, Aug. The Coling 2010 Orga-
nizing Committee.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010b. mwetoolkit: a framework for multi-
word expression identification. In Proc. of the Seventh
LREC (LREC 2010), Malta, May. ELRA.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010c. Web-based and combined language
models: a case study on noun compound identification.
In Proc. of the 23rd COLING (COLING 2010), pages
1041?1049, Beijing, China, Aug. The Coling 2010 Or-
ganizing Committee.
Pavel Rychly? and Pavel Smrz. 2004. Manatee, bonito
and word sketches for czech. In Proceedings of
the Second International Conference on Corpus Lin-
guisitcs, pages 124?131, Saint-Petersburg, Russia.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-
take, and Dan Flickinger. 2002. Multiword expres-
sions: A pain in the neck for NLP. In Proc. of
the 3rd CICLing (CICLing-2002), volume 2276/2010
of LNCS, pages 1?15, Mexico City, Mexico, Feb.
Springer.
136
Proceedings of the EACL 2012 Workshop on Computational Models of Language Acquisition and Loss, pages 43?50,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
Get out but don?t fall down: verb-particle constructions in child language
Aline Villavicencio??, Marco A. P. Idiart?, Carlos Ramisch?,
V??tor Arau?jo?, , Beracah Yankama?, Robert Berwick?
?Federal University of Rio Grande do Sul (Brazil)
?MIT (USA)
alinev@gmail.com, marco.idiart@gmail.com, ceramisch@inf.ufrgs.br,
vbuaraujo@inf.ufrgs.br, beracah@mit.edu, berwick@csail.mit.edu
Abstract
Much has been discussed about the chal-
lenges posed by Multiword Expressions
(MWEs) given their idiosyncratic, flexi-
ble and heterogeneous nature. Nonethe-
less, children successfully learn to use them
and eventually acquire a number of Mul-
tiword Expressions comparable to that of
simplex words. In this paper we report
a wide-coverage investigation of a partic-
ular type of MWE: verb-particle construc-
tions (VPCs) in English and their usage
in child-produced and child-directed sen-
tences. Given their potentially higher com-
plexity in relation to simplex verbs, we
examine whether they appear less promi-
nently in child-produced than in child-
directed speech, and whether the VPCs
that children produce are more conserva-
tive than adults, displaying proportionally
reduced lexical repertoire of VPCs or of
verbs in these combinations. The results
obtained indicate that regardless of any ad-
ditional complexity VPCs feature widely in
children data following closely adult usage.
Studies like these can inform the develop-
ment of computational models for language
acquisition.
1 Introduction
There has been considerable discussion about
the challenges imposed by Multiword Expres-
sions (MWEs) which in addition to crossing word
boundaries act as a single lexical unit at some lev-
els of linguistic analysis (Calzolari et al, 2002;
Sag et al, 2002; Fillmore, 2003). They include a
wide range of grammatical constructions such as
verb-particle constructions (VPCs), idioms, com-
pound nouns and listable word configurations,
such as terminology and formulaic linguistic units
(Wray, 2009). Depending on the definition, they
may also include less traditional sequences like
copy of in They gave me a copy of the book (Fill-
more et al, 1988), greeting formulae like how
do you do?, and lexical bundles such as I dont
know whether or memorized poems and famil-
iar phrases from TV commercials (Jackendoff,
1997). These expressions may have reduced syn-
tactic flexibility, and be semantically more opaque
so that their semantics may not be easily inferred
from their component words. For instance, to play
down X means to (try to) make X seem less im-
portant than it really is and not literally a playing
event.
These expressions may also breach general
syntactic rules, sometimes spanning phrasal
boundaries and often having a high degree of lex-
icalisation and conventionality. They form a com-
plex of features that interact in various, often un-
tidy, ways and represent a broad continuum be-
tween non-compositional (or idiomatic) and com-
positional groups of words (Moon, 1998). In ad-
dition, they are usually sequences or groups of
words that co-occur more often than would be ex-
pected by chance, and have been argued to appear
in the same order of magnitude in a speaker?s lex-
icon as the simplex words (Jackendoff, 1997).
In terms of language acquisition difficulties
may arise as the interpretation of these expres-
sions often demands more knowledge than just
about (1) unitary words and (2) word-to-word re-
lations. This introduces a distinction between
what a learner is able to computationally disam-
biguate or figure out automatically from language
and what must be explicitly stored/memorized
and retrieved whole from memory at the time of
43
use, rather than being subject to generation or
analysis by the language grammar (Wray, 2009,
p. 9). Yet, according to Fillmore et al (1988),
in an ideal learning environment, most of the
knowledge about how to use a language should
be computable while explicitly memorized se-
quences should be kept to a minimum.
Due to these idiosyncrasies they have been
noted as easily phonetically mislearned: e.g. by
and large mistaken for by in large, to all in-
tents and purposes for to all intensive purposes,
and an arm and a leg for a nominal egg (Fill-
more, 2003). For second language (L2) learn-
ers in particular (Wray, 2002) MWEs are in-
deed a well-known cause of problems and less
likely to be used by them than by native speak-
ers in informal spoken contexts (Siyanova and
Schmitt, 2007). Even if L2 learners may be capa-
ble of producing a large number of MWEs, their
underlying intuitions and fluency do not match
those of native speakers (Siyanova and Schmitt,
2008) and they may produce marked combina-
tions that are not conventionally used together
(e.g. plastic surgery/?operation, strong/?powerful
tea) (Pearce, 2002; Siyanova and Schmitt, 2007).
Given the potential additional sources of com-
plexity of MWEs for learning, in this paper we
investigate whether children shy away from us-
ing them when they communicate. We focus on
a particular type of MWEs, VPCs, which present
a wide range of syntactic and semantic idyosin-
crasies examining whether children produce pro-
portionally less VPCs than adults. In addition, we
analyze whether any potential added processing
costs for VPCs are reflected in a reduced choice
of VPCs or verbs to form these combinations in
child-produced sentences compared to adult us-
age. Finally, given the possibility of flexible word
orders in VPCs with the verb and particle not only
occurring adjacently but also with an NP object
between them, we compare these two groups in
terms of distances between the verb and the par-
ticle in these combinations, to determine whether
there is a preference for a joint or a split config-
uration and if children and adults adopt distinct
strategies for their usage. By profiling the VPC
usage by children our aim is to provide the basis
for a computational modeling of the acquisition of
these constructions.
This paper is structured as follows: in sec-
tion 2 describes VPCs and related works; sec-
tion 3 presents the resources and methods used in
this paper. The analyses of VPCs in children and
adults sentences are in section 4. We finish with
conclusions and possibilities of future works.
2 Related Work
VPCs are combinations of verbs and prepositional
(up, down, ...), adverbial (away, back,...), adjecti-
val (short,...) or verbal (go, be,...) particles, and in
this work we focus on VPCs with prepositional or
adverbial particles like put off and move on. From
a language acquisition perspective, the complex-
ity of VPCs arises from their wide syntactic as
semantic variability.
Syntactically, like simplex verbs, VPCs can oc-
cur in different subcategorisation frames (e.g. in-
transitive in break down and transitive in print NP
up). However, the type of verb and the num-
ber of arguments of a VPC seem to have an
impact in learning as both children with typical
development and with specific language impair-
ments (SLI) seem to use obligatory arguments and
inflectional morphology more consistently with
general all purpose verbs, like make, go, do, put,
than with more specific verbs. Moreover, as the
number of obligatory arguments increases chil-
dren with SLI seem to produce more general and
fewer specific verbs (Boynton-Hauerwas, 1998).
Goldberg (1999b) refers to these verbs as light
verbs, suggesting that due to their frequency of
use, they are acquired earlier by children, and sub-
sequently act as centers of gravity from which
more specific instances can be learnt. These verbs
are very common and frequent in the everyday
communication, that could be used in place of
more specialized instances (e.g. make instead of
build).
In transitive VPCs there is the additional diffi-
culty of the particle appearing in different word
orders in relation to the verb: in a joint configu-
ration, adjacent to the verb (e.g. make up NP) or
in a split configuration after the NP complement
(make NP up) (Lohse et al, 2004). While some
VPCs can appear in both configurations, others
are inseparable (run across NP), and a learner has
to successfully account for these. Gries (2002)
using a multifactorial analysis to investigate 25
variables that could be linked to particle place-
ment like size of the direct object (in syllables
and words), type of NP (pronoun or lexical), type
of determiner (indefinite or definite). For a set
44
of 403 VPCs from the British National Corpus
he obtains 84% success in predicting (adult) na-
tive speakers? choice. Lohse et al (2004) propose
that these factors can be explained by consider-
ations of processing efficiency based on the size
of the object NP and on semantic dependencies
among the verb, the particle, and the object. In a
similar study for children Diessel and Tomasello
(2005) found that the type of the NP (pronoun vs
lexical NP) and semantics of the particle (spatial
vs non-spatial) were good predictors of placement
on child language data.
Semantically, one source of difficulties for
learners comes from the wide spectrum of compo-
sitionality that VPCs present. On one end of the
spectrum some combinations like take away com-
positionally combine the meaning of a verb with
the core meaning of a particle giving a sense of
motion-through-location (Bolinger, 1971). Other
VPCs like boil up are semi-idiomatic (or aspec-
tual) and the particle modifies the meaning of the
verb adding a sense of completion or result. At the
other end of the spectrum, idiomatic VPCs like
take off, meaning to imitate have an opaque mean-
ing that cannot be straightforwardly inferred from
the meanings of each of the components literally.
Moreover, even if some verbs form combinations
with almost every particle (e.g., get, fall, go,...),
others are selectively combined with only a few
particles (e.g., book and sober with up), or do not
combine well with them at all (e.g., know, want,
resemble,...) (Fraser, 1976). Although there are
some semi-productive patterns in these combina-
tions, like verbs of cooking and the aspectual up
(cook up, boil up, bake up), and stative verbs not
forming VPCs, for a learner it may not be clear
whether an unseen combination of verb and parti-
cle is indeed a valid VPC that can be produced or
not. Sawyer (1999) longitudinal analysis of VPCs
in child language found that children seem to treat
aspectual and compositional combinations differ-
ently, with the former being more frequent and
employing a larger variety of types than the lat-
ter. The sources of errors also differ and while
for compositional cases the errors tend to be lexi-
cal, for aspectuals there is a predominance of syn-
tactic errors such as object dropping, which ac-
counts for 92% of the errors in split configura-
tion for children under 5 (Sawyer, 1999). Chil-
dren with SLI tended to produce even more object
dropping errors for VPCs than children with typ-
ical development, despite both groups producing
equivalent numbers of VPCs (Juhasz and Grela,
2008). Given that compositionality seems to have
an impact on learning, to help reduce avoidance
of phrasal verbs Sawyer (2000) proposes a seman-
tic driven approach for second language learning
where transparent compositional cases would be
presented first to help familiarization with word
order variation, semi-idiomatic cases would be
taught next in groups according to the contribu-
tion of the particle (e.g telicity or completive-
ness), and lastly the idiomatic cases that need to
be memorized.
In this paper we present a wide coverage ex-
amination of VPC distributions in child produced
and child-directed sentences, comparing whether
children reproduce the linguistic environment to
which they are exposed or whether they present
distinct preferences in VPC usage.
3 Materials and Methods
For this work we use the English corpora from
the CHILDES database (MacWhinney, 1995)
containing transcriptions of child-produced and
child-directed speech from interactions involving
children of different age groups and in a variety
of settings, from naturalistic longitudinal studies
to task oriented latitudinal cases. These corpora
are available in raw, part-of-speech-tagged, lem-
matized and parsed formats (Sagae et al, 2010).
Moreover the English CHILDES Verb Construc-
tion Database (ECVCD) (Villavicencio et al,
2012) also adds for each sentence the RASP pars-
ing and grammatical relations (Briscoe and Car-
roll, 2006), verb semantic classes (Levin, 1993),
age of acquisition, familiarity, frequency (Colt-
heart, 1981) and other psycholinguistic and dis-
tributional characteristics. These annotated sen-
tences are divided into two groups according to
the speaker annotation available in CHILDES, the
Adults Set and the Children Set contain respec-
tively all the sentences spoken by adults and by
children1, as shown in table 1 as Parsed.
VPCs in these corpora are detected by look-
ing in the RASP annotation for all occurrences
of verbs followed by particles, prepositions and
adverbs up to 5 words to the right, following
Baldwin (2005), shown as Sentences with VPCs
1For the latter sentences which did not contain informa-
tion about age were removed.
45
Sentences Children Set Adults Set
Parsed 482,137 988,101
with VPCs 44,305 83,098
with VPCs Cleaned 38,326 82,796
% with VPCs 7.95 8.38
Table 1: VPCs in English Corpora in the Children
and Adults Sets
in table 1. The resulting sentences are subse-
quently automatically processed to remove noise
and words mistagged as verbs. For these candi-
dates with non-alphabetic characters, like @ in
a@l up, were removed as were those that did not
involve verbs (e.g. di, dat,), using the Comlex
Lexicon as reference for verb validity (Macleod
and Grishman, 1998). The resulting sets are listed
as Sentences with VPCs Cleaned in table 1. The
analyses reported in this paper use these sen-
tences, and the distribution of VPCs per children
age group is shown in table 2. Given the non-
uniform amounts of VPC for each age group, and
the larger proportion of VPC sentences in younger
ages in these corpora, we consider children as a
unique group. For these, the individual frequen-
cies of the verb, the particle and the VPC are col-
lected separately in the children set and in the
adult set, using the mwetoolkit (Ramisch et al,
2010).
Age in months VPC Sentences
0-24 2,799
24-48 26,152
48-72 8,038
72-96 1,337
>96 514
No age 4,841
Table 2: VPCs in Children Set per Age
To evaluate the VPCs in these sets, we use:
? English VPC dataset (Baldwin, 2008); which
lists 3,078 VPCs with valency (intransitive
and transitive) information;
? Comlex lexicon (Macleod and Grishman,
1998) containing 10,478 phrasal verbs;
? the Alvey Natural Language Tools (ANLT)
lexicon (Carroll and Grover, 1989) with
6,351 phrasal verbs.
4 VPCs in Child Language
To investigate whether any extra complexity in the
acquisition of VPCs is reflected in their reduced
presence in child-produced than in child-directed
sentences, we compare the proportion of VPCs in
the Children and Adults Sets, table 3. In absolute
terms adults produced more than double the num-
ber of VPCs that children did. However, given
the differences in size of the two sets, in relative
terms there was a similar proportion of VPC us-
age in these corpora for each of the groups: 7.95%
of the sentences produced by children contained
VPCs vs 8.38% of those by adults. Moreover, the
frequencies with which these VPCs are used by
both children and adults reflects the Zipfian distri-
bution found for the use of words in natural lan-
guages, with a large part of the VPCs occurring
just once in the data, table 4. In addition, in terms
of frequency, children?s production of VPCs re-
sembles that of the adults.
Total VPC Children Set Adults Set
Tokens 38,326 82,796
Types 1,579 2,468
Table 3: VPC usage in CHILDES
Frequency Children Set Adults Set
1 42.62% 43.03%
2 13.05% 15%
3 8.36% 6.48%
4 4.05% 4.5%
?5 31.92% 31%
Table 4: VPC types per frequency
Another possible source of divergence between
children and adults is in the lexical variety found
in VPCs. The potential difficulties with VPCs
may be manifested in children producing a re-
duced repertoire of VPCs or using a smaller set
of verbs to form these combinations. As shown in
table 3, adults, as expected, employ a larger VPC
vocabulary with 1.56 more types than children.
However, an examination of the distributions of
types reveals that they only differ by a scale. As
a result when children frequencies are multiplied
by a factor of 2.16, which corresponds to the ra-
tio between VPC tokens used by adults and chil-
dren (table 3), the resulting distribution has a very
46
good match with the adult distribution, see fig-
ure 1. Therefore, the lower number of VPC types
used by children can be explained totally by the
lower number of sentences they produced, and the
hypothesis that difficulties in VPCs would lead to
their avoidance is not confirmed by the data.
Nonetheless, there is a discrepancy between
the distributions found for the higher frequency
VPCs. Children have a more uniform distribution
and adults tend to repeat more often the higher
frequency combinations (top left corner of fig-
ure 1). An evidence that this discrepancy is partic-
ular for high frequency VPCs, and not their con-
stituent verbs, is shown in figure 2. This figure
displays the rank plot for the verbs present in the
VPCs, for both adults and children. The same
scale factor used in figure 1 is applied to compen-
sate for the lower number of VPC sentences in the
children set. This time the match is extraordinary,
spanning the whole vocabulary.
100 102 10410
0
101
102
103
104
105
rank
freq
uen
cy
VPC Usage
 
 adults
children*
Figure 1: VPC Usage Frequency vs Ranking. The
children frequency is scaled to match adult total
VPC usage.
Ranks however, might not tell the whole story.
It is important to verify if the same VPCs and
verbs are present in the both vocabularies, and fur-
ther if their orders in the ranks are similar. The
two groups have very similar preferences for VPC
usage, with a Kendall ? score of 0.63 which indi-
cates that they are highly correlated, as Kendall
? ranges from -1 to 1. Furthermore they use a
very similar set of verbs in VPCs, with a Kendall
100 102 10410
0
101
102
103
104
105
rank
freq
uen
cy
Verbs in VPCs Usage
 
 adultschildren*
Figure 2: Verbs in VPCs Usage Frequency vs
Ranking. The children frequency is scaled to
match adult total VPC usage.
? score of 0.84 pointing to a very strong corre-
lation. We find less agreement between the or-
ders of VPCs and verbs for both children and
adults, indicating that the order of the verbs in
the data is not predictive of the relative frequen-
cies of VPCs. We examined (a) if children?s VPC
ranks followed their verb ranks, (b) if adults VPC
ranks followed their verb ranks and (c) if chil-
dren?s VPC ranks followed adults? verb ranks.
The resulting Kendall scores were around 0.2 for
all three cases. Moreover, if the lower frequency
VPCs are removed to avoid potential cases of
noise, the Kendall ? score for VPCs by adults and
children increases with the threshold, second line
from the top in Figure 3, while it remains constant
for all the other cases. As an example, the top 10
VPC types used by children and adults are listed
in table 5. From these, 9 out of the 10 are the
same differing only in the order in which they ap-
pear. Most of these combinations are listed in one
of the dictionaries used for evaluation: 72% for
adults and 75.87% for children. When a thresh-
old of at least 5 counts is applied these values go
up to 87.72% for adults and 79.82% for children,
as would be expected. This indicates that besides
any possible lack of coverage for child-directed
VPCs in the lexicons or noise, it is in the lower
frequency combinations that novel and domains
specific non-standard usages can be found. Some
47
Rank Chidren Children Adult Adult Child
VPC Freq VPC Freq Rank
1 put on 2005 come on 6244 7
2 go in 1608 put on 4217 1
3 get out 1542 go on 2660 9
4 take off 1525 get out 2251 3
5 fall down 1329 take off 2249 4
6 put in 1284 put in 2177 6
7 come on 1001 sit down 2133 8
8 sit down 981 go in 1661 2
9 go on 933 come out 1654 10
10 come out 872 pick up 1650 18
Table 5: Top VPCs for Children and Adults
of the combinations not found in these dictionar-
ies include crawl in and creep up by adults and
erase off and crash down by children.
0
0.2
0.4
0.6
0.8
1
0 5 10 20
Lexical Choices for VPCs
K
e
n
d
a
l
l
 
t
a
u
threshold
Children / Adults VPCs Children VPCs / Verbs
Adults VPC / Verbs Children VPCs / Adult Verbs
Children /Adult Verbs
Figure 3: Kendall ? score per VPC frequency
threshold
Finally, despite adults having a larger verb vo-
cabulary used in VPCs than children, the two
groups have similar ratios of verb per VPCs: 2.81
VPCs for children and 2.79 for adults, table 6.
The top verbs used in VPCs types are also respon-
sible for very frequent VPC tokens (e.g. go, get,
come, take, put, make and move) accounting for
5.83% VPC types and 43.76% tokens for adults
and 7.02% of the types and 47.81% of the to-
kens for children, confirming the discrepancy dis-
cussed earlier. These are very general verbs and
some of the most frequent in the data, reported
among the first to be learned (Goldberg, 1999a)
which may facilitate their acquisition and use in
VPCs.
Comparing VPC types used by children and by
adults, this trend is confirmed: a large proportion
(72.32%) of the VPC types that children use is
also used by adults, Children ? Adult in table 6.
When low frequency VPCs types are removed,
this proportion increases (89.48%). Moreover,
when the VPCs used only by the adults are con-
sidered, most of these (93.44%) occur with fre-
quency lower than 5. This suggests that children
tend to follow quite closely the combinations em-
ployed by adults, and the lower frequency cases
may not yet be incorporated in their active vocab-
ulary.
In terms of the distance between verb and par-
ticle, there is a strong preference in the data for
joint combinations for both children and adults,
table 7. For the split cases, the majority contains
only one word between the verb and the particle.
Children in particular display a slight disprefer-
ence for longer distances between verbs and parti-
cles, and over 97% of VPCs have at most 2 words
between them.
Distance Children Set Adults Set
0 65.13% 64.14%
1 23.48% 22.15%
2 9.33% 10.90%
3 1.65% 2.15%
4 0.29% 0.47%
5 0.09% 0.16%
Table 7: Distance between verb and particle
5 Conclusions and future work
In this paper we presented an investigation of
VPCs in child-produced and child-directed sen-
tences in English to determine whether potential
complexities in the nature of these combinations
48
Children Adult Children ?Adult Children Adult
VPCs VPCs VPCs only VPCs only VPCs
VPCs 1579 2468 1142 437 1243
Verb in VPCs 561 884 401 160 483
Particle in VPCs 28 35 24 4 9
VPCs ? 5 504 766 451 53 278
Verb in VPCs ? 5 207 282 183 24 99
Particle in VPCs ? 5 18 20 17 1 3
Table 6: Number of VPC, Verb and Particle types by group, common usages
are reflected in their reduced usage by children.
The combination of these results shows that, de-
spite any additional difficulties, VPCs are as much
a feature in children?s data as in adults?. Children
follow very closely adult usage in terms of the
types and are sensitive to their frequencies, dis-
playing similar distributions to adults. They also
seem to use them in a similar manner in terms of
particle placement. Therefore no correction for
VPC complexity was found in this data.
Despite these striking similarities in many of
the distributions, there are still some discrepan-
cies between these two groups. In particular in the
VPC ranks, children present a more uniform dis-
tribution for higher frequency VPCs when com-
pared to adults. Moreover, there is a modest but
significant dispreference for longer distances be-
tween verb and particle for children. Whether
these reflect different strategies or efficiency con-
siderations deserves to be further investigated.
Acknowledgements
This research was partly supported by CNPq
Projects 551964/2011-1, 202007/2010-3,
305256/2008-4 and 309569/2009-5.
References
Timothy Baldwin. 2005. Deep lexical acquisition
of verb-particle constructions. Computer Speech &
Language Special issue on MWEs, 19(4):398?414.
Timothy Baldwin. 2008. A resource for evaluating
the deep lexical acquisition of english verb-particle
constructions. In Proceedings of the LREC Work-
shop Towards a Shared Task for Multiword Expres-
sions (MWE 2008), pages 1?2, Marrakech, Mo-
rocco, June.
Dwight Bolinger. 1971. The phrasal verb in English.
Harvard University Press, Harvard, USA.
L. S. Boynton-Hauerwas. 1998. The role of general
all purpose verbs in language acquisition: A com-
parison of children with specific language impair-
ments and their language-matched peers. 59.
Ted Briscoe and John Carroll. 2006. Evaluating the
accuracy of an unlexicalized statistical parser on
the PARC depbank. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics (COLING/ACL 2006),
pages 41?48, Sidney, Australia, July. Association
for Computational Linguistics.
Nicoleta Calzolari, Charles Fillmore, Ralph Grishman,
Nancy Ide, Alessandro Lenci, Catherine Macleod,
and Antonio Zampolli. 2002. Towards best prac-
tice for multiword expressions in computational
lexicons. In Third International Conference on
Language Resources and Evaluation (LREC 2002),
pages 1934?1940, Las Palmas, Canary Islands,
Spain. European Language Resources Association.
John Carroll and Claire Grover. 1989. The derivation
of a large computational lexicon of English from
LDOCE. In B. Boguraev and E. Briscoe, editors,
Computational Lexicography for Natural Language
Processing. Longman.
M. Coltheart. 1981. The MRC psycholinguistic
database. Quarterly Journal of Experimental Psy-
chology, 33A:497?505.
Holger Diessel and Michael Tomasello. 2005. Particle
placement in early child language : A multifactorial
analysis. Corpus Linguistics and Linguistic Theory,
1(1):89?112.
Charles J. Fillmore, Paul Kay, and Mary C. O?Connor.
1988. Regularity and idiomaticity in grammatical
constructions: The case of Let Alone. Language,
64(3):510?538.
Charles Fillmore. 2003. Multiword expressions: An
extremist approach. Presented at Collocations and
idioms 2003: linguistic, computational, and psy-
cholinguistic perspectives.
Bruce Fraser. 1976. The Verb-Particle Combination
in English. Academic Press, New York, USA.
49
Adele E. Goldberg, 1999a. The Emergence of Lan-
guage, chapter Emergence of the semantics of
argument structure constructions, pages 197?212.
Carnegie Mellon Symposia on Cognition Series.
Adele E. Goldberg. 1999b. The emergence of the
semantics of argument structure constructions. In
B. MacWhinney, editor, Emergence of language.
Lawrence Erlbaum Associates, Hillsdale, NJ.
Stefan Gries. 2002. The influence of processing on
syntactic variation: Particle placement in english.
In Nicole Dehe?, Ray Jackendoff, Andrew McIn-
tyre, and Silke Urban, editors, Verb-Particle Ex-
plorations, pages 269?288. New York: Mouton de
Gruyter.
Ray Jackendoff. 1997. Twistin? the night away. Lan-
guage, 73:534?559.
C. R. Juhasz and B. Grela. 2008. Verb particle errors
in preschool children with specific language impair-
ment. Contemporary Issues in Communication Sci-
ence & Disorders, 35:76?83.
Beth Levin. 1993. English Verb Classes and Alter-
nations: a preliminary investigation. University of
Chicago Press, Chicago, USA.
Barbara Lohse, John A Hawkins, and Thomas Wa-
sow. 2004. Domain minimization in english verb-
particle constructions. Language, 80(2):238?261.
Catherine Macleod and Ralph Grishman. 1998.
COMLEX syntax reference manual, Proteus
Project.
B. MacWhinney. 1995. The CHILDES project: tools
for analyzing talk. Hillsdale, NJ: Lawrence Erl-
baum Associates, second edition.
Rosamund E. Moon. 1998. Fixed Expressions and
Idioms in English: A Corpus-based Approach. Ox-
ford University Press.
Darren Pearce. 2002. A comparative evaluation of
collocation extraction techniques. In Third Inter-
national Conference on Language Resources and
Evaluation (LREC 2002), Las Palmas, Canary Is-
lands, Spain. European Language Resources Asso-
ciation.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. mwetoolkit: a framework for mul-
tiword expression identification. In Proceedings of
the Seventh International Conference on Language
Resources and Evaluation (LREC 2010), Malta,
May. European Language Resources Association.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2002. Multi-
word expressions: A pain in the neck for NLP.
In Proceedings of the 3rd International Conference
on Intelligent Text Processing and Computational
Linguistics (CICLing-2002), volume 2276/2010 of
Lecture Notes in Computer Science, pages 1?15,
Mexico City, Mexico, February. Springer.
K. Sagae, E. Davis, A. Lavie, B. MacWhinney, and
S. Wintner. 2010. Morphosyntactic annotation of
CHILDES transcripts. Journal of Child Language,
37(03):705?729.
J.H. Sawyer. 1999. Verb adverb and verb particle
constructions: their syntax and acquisition. s.n.
Joan H. Sawyer. 2000. Comments on clayton m. dar-
win and loretta s. gray?s ?going after the phrasal
verb: An alternative approach to classification?. a
reader reacts. TESOL Quarterly, 34(1):151?159.
Anna Siyanova and Norbert Schmitt. 2007. Na-
tive and nonnative use of multi-word vs. one-word
verbs. International Review of Applied Linguistics,
45:109139.
Anna Siyanova and Norbert Schmitt. 2008. L2 learner
production and processing of collocation: A multi-
study perspective. Canadian Modern Language Re-
view, 64(3):429458.
Aline Villavicencio, Beracah Yankama, Robert
Berwick, and Marco Idiart. 2012. A large scale
annotated child language construction database. In
Proceedings of the 8th LREC, Istanbul, Turkey.
Alison Wray. 2002. Formulaic Language and the Lex-
icon. Cambridge University Press, Cambridge, UK.
Alison Wray. 2009. Formulaic language in learn-
ers and native speakers. Language Teaching,
32(04):213?231.
50
Proceedings of the 2012 Student Research Workshop, pages 1?6,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Broad Evaluation of Techniques for Automatic
Acquisition of Multiword Expressions
Carlos Ramisch?, ?, Vitor De Araujo?, Aline Villavicencio?
?Federal University of Rio Grande do Sul (Brazil)
? GETALP ? LIG, University of Grenoble (France)
{ceramisch, vbuaraujo, avillavicencio}@inf.ufrgs.br
Abstract
Several approaches have been proposed for the au-
tomatic acquisition of multiword expressions from
corpora. However, there is no agreement about
which of them presents the best cost-benefit ratio, as
they have been evaluated on distinct datasets and/or
languages. To address this issue, we investigate
these techniques analysing the following dimen-
sions: expression type (compound nouns, phrasal
verbs), language (English, French) and corpus size.
Results show that these techniques tend to extract
similar candidate lists with high recall (? 80%) for
nominals and high precision (? 70%) for verbals.
The use of association measures for candidate filter-
ing is useful but some of them are more onerous and
not significantly better than raw counts. We finish
with an evaluation of flexibility and an indication of
which technique is recommended for each language-
type-size context.
1 Introduction
Taking into account multiword expressions (MWEs) is
important to confer naturalness to the output of NLP sys-
tems. An MT system, for instance, needs to be aware of
idiomatic expressions like raining cats and dogs to avoid
literal translations.1 Likewise, a parser needs to deal with
verb-particle expressions like take off from Paris and with
light verb constructions like take a walk along the river
in order to avoid PP-attachment errors.
Even though the last decade has seen considerable re-
search in the automatic acquisition of MWEs, both in
theoretical and in computational linguistics, to date there
are few NLP applications integrating explicit MWE treat-
ment. This may be partly explained by the complexity of
MWEs: as they are heterogeneous and flexible, there is
no unique push-button approach to identify all types of
MWEs in all languages (Sag et al, 2002). Existing ap-
proaches are either generic but present relatively low pre-
1The equivalent expressions in French would be raining ropes, in
German raining young dogs, in Portuguese raining Swiss knives, etc.
cision or they require a large amount of language-specific
resources to yield good results.
The goal of this paper is to evaluate approaches for the
automatic acquisition of MWEs from corpora (?2), exam-
ining as parameters of the experimental context the lan-
guage (English and French), type of target MWE (verbal
and nominal) and size of corpus (small, medium, large).
We focus on 4 approaches2 and the experimental setup is
presented in ?3. In ?4 we evaluate the following acqui-
sition dimensions: quality of extracted candidates and of
association measures, use of computational resources and
flexibility. Thus, this research presents a comparative in-
vestigation of available approaches and indicates the best
cost-benefit ratio in a given context (language, type, cor-
pus size), pointing out current limitations and suggesting
future avenues of research for the field.
2 MWE Acquisition Approaches
Efforts for the evaluation of MWE acquisition approaches
usually focus on a single technique or compare the qual-
ity of association measures (AMs) used to rank a fixed
annotated list of MWEs. For instance, Evert and Krenn
(2005) and Seretan (2008) specifically evaluate and anal-
yse the lexical AMs used in MWE extraction on small
samples of bigram candidates. Pearce (2002), systemat-
ically evaluates a set of techniques for MWE extraction
on a small test set of English collocations. Analogously,
Pecina (2005) and Ramisch et al (2008) present exten-
sive comparisons of individual AMs and of their combi-
nation for MWE extraction in Czech, German and En-
glish. There have also been efforts for the extrinsic eval-
uation of MWEs for NLP applications such as informa-
tion retrieval (Xu et al, 2010), word sense disambigua-
tion (Finlayson and Kulkarni, 2011) and MT (Carpuat
and Diab, 2010).
One recent initiative aiming at more comparable eval-
2We consider only freely available, downloadable and openly docu-
mented tools. Therefore, outside the scope of this work are proprietary
tools, terminology and lexicography tools, translation aid tools and pub-
lished techniques for which no available implementation is provided.
1
uations of MWE acquisition approaches was in the form
of a shared task (Gr?goire et al, 2008). However, the
present work differs from the shared task in its aims. The
latter considered only the ranking of precompiled MWE
lists using AMs or linguistic filters at the end of extrac-
tion. However, for many languages and domains, no such
lists are available. In addition, the evaluation results pro-
duced for the shared task may be difficult to generalise,
as some of the evaluations prioritized the precision of the
techniques without considering the recall or the novelty
of the extracted MWEs. To date little has been said about
the practical concerns involving MWE acquisition, like
computational resources, flexibility or availability. With
this work, we hope to help filling this gap by performing
a broad evaluation of the acquisition process as a whole,
considering many different parameters.
We focus on 4 approaches for MWE acquisition from
corpora, which follow the general trend in the area of us-
ing shallow linguistic (lemmas, POS, stopwords) and/or
statistical (counts, AMs) information to distinguishing
ordinary sequences (e.g. yellow dress, go to a concert)
from MWEs (e.g. black box, go by a name). In addition
to the brief description below, Section 4.4 underlines the
main differences between the approaches.
1. LocalMaxs3 extracts MWEs by generating all pos-
sible n-grams from a sentence and then filtering
them based on the local maxima of the AM?s dis-
tribution (Silva and Lopes, 1999). It is based
purely on word counts and is completely language
independent, but it is not possible to directly in-
tegrate linguistic information in order to target a
specific type of construction.4 The evaluation
includes both LocalMaxs Strict which prioritizes
high precision (henceforth LocMax-S) and Local-
Maxs Relaxed which focuses on high recall (hence-
forth LocMax-R). A variation of the original algo-
rithm, SENTA, has been proposed to deal with non-
contiguous expressions (da Silva et al, 1999). How-
ever, it is computationally costly5 and there is no
freely available implementation.
2. MWE toolkit6 (mwetk) is an environment for
type and language-independent MWE acquisition,
integrating linguistic and frequency information
(Ramisch et al, 2010). It generates a targeted list
of MWE candidates extracted and filtered according
to user-defined criteria like POS sequences and a set
3http://hlt.di.fct.unl.pt/luis/multiwords/
index.html
4Although this can be simulated by concatenating words and POS
tags together in order to form a token.
5It is based on the calculation of all possible n-grams in a sen-
tence, which explode in number when going from contiguous to non-
contiguous n-grams.
6http://mwetoolkit.sourceforge.net
Small Medium Large
# sentences 5,000 50,000 500,000
# en words 133,859 1,355,482 13,164,654
# fr words 145,888 1,483,428 14,584,617
Table 1: Number of sentences and of words of each fragment of
the Europarl corpus in fr and in en.
of statistical AMs. It is an integrated framework for
MWE treatment, providing from corpus preprocess-
ing facilities to the automatic evaluation of the re-
sulting list with respect to a reference. Its input is
a corpus annotated with POS, lemmas and depen-
dency syntax, or if these are not available, raw text.
3. Ngram Statistics Package7 (NSP) is a traditional
approach for the statistical analysis of n-grams in
texts (Pedersen et al, 2011). It provides tools for
counting n-grams and calculating AMs, where an n-
gram is a sequence of n words occurring either con-
tiguously or within a window of w words in a sen-
tence. While most of the measures are only appli-
cable to bigrams, some of them are also extended to
trigrams and 4-grams. The set of available AMs in-
cludes robust and theoretically sound measures such
as log-likelihood and Fischer?s exact test. Although
there is no direct support to linguistic information
such as POS, it is possible to simulate them to some
extent using the same workaround as for LocMax.
4. UCS toolkit8 provides a large set of sophisticated
AMs. It focuses on high accuracy calculations for
bigram AMs, but unlike the other approaches, it
starts from a list of candidates and their respec-
tive frequencies, relying on external tools for corpus
preprocessing and candidate extraction. Therefore,
questions concerning contiguous n-grams and sup-
port of linguistic filters are not dealt with by UCS. In
our experiments, we will use the list of candidates
generated by mwetk as input for UCS.
As the focus of this work is on MWE acquisition (iden-
tification and extraction), other tasks related to MWE
treatment, namely interpretation, classification and appli-
cations (Anastasiou et al, 2009), are not considered in
this paper. This is the case, for instance, of approaches
for dictionary-based in-context MWE token identification
requiring an initial dictionary of valid MWEs, like jMWE
(Kulkarni and Finlayson, 2011).
3 Experimental Setup
For comparative purposes, we investigate the acquisition
of MWEs in two languages, English (en) and French
7http://search.cpan.org/dist/Text-NSP
8http://www.collocations.de/software.html
2
(fr), analysing nominal and verbal expressions in en and
nominal in fr,9 obtained with the following rules:
? Nominal expressions en: a noun preceded by a se-
quence of one or more nouns or adjectives, e.g. Eu-
ropean Union, clock radio, clown anemone fish.
? Nominal expressions fr: a noun followed by either
an adjective or a prepositional complement (with the
prepositions de, ? and en) followed by an option-
ally determined noun, e.g. algue verte, ali?nation de
bien, allergie ? la poussi?re.
? Verbal expressions en: verb-particle constructions
formed by a verb (except be and have) followed by
a prepositional particle10 not further than 5 words
after it, e.g. give up, switch the old computer off.
To test the influence of corpus size on performance,
three fragments of the en and fr parts of the Eu-
roparl corpus v311 were used as test corpora: (S)mall,
(M)edium and (L)arge, summarised in Table 1.
The extracted MWEs were automatically evaluated
against the following gold standards: WordNet 3, the
Cambridge Dictionary of Phrasal Verbs, and the VPC
(Baldwin, 2008) and CN (Kim and Baldwin, 2008)
datasets 12 for en; the Lexique-Grammaire13 for fr. The
total number of entries is listed below, along with the
number of entries occurring at least twice in each cor-
pus (in parentheses), which was the denominator used to
calculate recall in ? 4.1:
? Nominal expressions en: 59,683 entries (S: 122, M:
764, L: 2,710);
? Nominal expressions fr: 69,118 entries (S: 220, M:
1,406, L: 4,747);
? Verbal expressions en: 1,846 entries (S: 699, M:
1,846, L: 1,846).
4 Evaluation Results
The evaluation of MWE acquisition is an open problem.
While classical measures like precision and recall assume
that a complete (or at least broad-coverage) gold standard
exists, manual annotation of top-n candidates and mean
average precision (MAP) are labour-intensive even when
applied to a small sample, emphasizing precision regard-
less of the number of acquired new expressions. As ap-
proaches differ in the way they allow the description of
extraction criteria, we evaluate candidate extraction sep-
arately from AMs.
9As fr does not present many verb-particle constructions and due
to the lack of availability of resource for other types of fr verbal ex-
pressions (e.g. light verb constructions), only nominal expressions are
considered.
10up, off, down, back, away, in, on.
11http://www.statmt.org/europarl/
12The latter are available from http://multiword.sf.net/
13http://infolingu.univ-mlv.fr/
LocM
ax-S
LocM
ax-R mwe
tk NSP UCS
LocM
ax-S
LocM
ax-R mwe
tk NSP UCS
LocM
ax-S
LocM
ax-R mwe
tk NSP UCS
en-noun                     fr-noun                     en-verb
0%10%
20%30%
40%50%
60%70%
80%90%
100%
Precision Recall F-measure
Figure 1: Quality of candidates extracted from medium corpus,
comparison across languages/MWE types.
4.1 Extracted Candidates
We consider as MWE candidates the initial set of se-
quences before any AM is applied. Candidate extraction
is performed through the application of patterns describ-
ing the target MWEs in terms of POS sequences, as de-
scribed in ? 3. To minimise potential cases of noise, can-
didates occurring only once in the corpus were discarded.
We compare the quality of these candidates in terms of
(P)recision, (R)ecall and (F)-measure using the gold stan-
dard references described in ? 3. These measures are un-
derestimations as they assume that candidates not in the
gold standard are false MWEs, whereas they may simply
be absent due to coverage limitations.
The quality of candidates extracted from the medium-
size corpus (M) varies across MWE types/languages, as
shown in Figure 1. The candidates for UCS are obtained
by keeping only the bigrams in the candidate list returned
by the mwetk. For nominal MWEs, the approaches have
similar patterns of performance in the two languages,
with high recall and low precision yielding an F-measure
of around 10 to 15%. The variation between en and fr
can be partly explained by the differences in size of the
gold standards for each of these languages. Further re-
search would be needed to determine to what degree the
characteristics of these languages and the set of extraction
patterns influence these results. For verbal expressions,
LocMax has high precision (around 70%) but low recall
while the other approaches have more balanced P and R
values around 20%. This is partly due to the need for
simulating POS filters for extraction of verbal MWE can-
didates with LocMax. The filter consists of keeping only
contiguous n-grams in which the first and the last words
matched verb+particle pattern and removing intervening
words.
The techniques differ in terms of extraction strategy:
(i) mwetk and NSP allow the definition of linguistic fil-
ters while LocMax only allows the application of grep-
3
S M L
LocMax-S
P 7.53% 6.18% 4.50%
R 42.62% 38.48% 37.42%
LocMax-R
P 7.46% 6.02% ?
R 42.62% 38.48% ?
P-mwetk
P 6.50% 4.40% 2.35%
R 83.61% 86.78% 89.23%
NSP
P 6.61% 4.46% 2.48%
R 83.61% 85.73% 89.41%
UCS
P 6.96% 4.91% 2.77%
R 96.19% 95.65% 96.88%
Table 2: (P)recision and (R)ecall of en nominal candidates,
comparison across corpus sizes (S)mall, (M)edium and (L)arge.
like filters after extraction; (ii) there is no preliminary fil-
tering in mwetk and NSP, they simply return all candi-
dates matching a pattern, while LocMax filters the candi-
dates based on the local maxima criterion; (iii) LocMax
only extracts contiguous candidates while the others al-
low discontiguous candidates. The way mwetk and NSP
extract discontiguous candidates differs: the former ex-
tracts all verbs with particles no further than 5 positions to
the right. NSP extracts bigrams in a window of 5 words,
and then filters the list keeping only those in which the
first word is a verb and that contain a particle. However,
the results are similar, with slightly better values for NSP.
The evaluation of en nominal candidates according to
corpus size is shown in Table 2.14 For all approaches,
precision decreases when the corpus size increases as
more noise is returned, while recall increases for all ex-
cept LocMax. This may be due to the latter ignoring
smaller n-grams when larger candidates containing them
become sufficiently frequent, as is the case when the cor-
pus increases. Table 3 shows that the candidates extracted
by LocMax are almost completely covered by the candi-
dates extracted by the other approaches. The relaxed ver-
sion extracts slighly more candidates, but still much less
than mwetk, NSP and UCS, which all extract a similar
set of candidates. In order to distinguish the performance
of the approaches, we need to analyse the AMs they use
to rank the candidates.
4.2 Association Measures
Traditionally, to evaluate an AM, the candidates are
ranked according to it and a threshold value is applied,
below which the candidates are discarded. However, if
we average the precision considering all true MWEs as
14It was not possible to evaluate LocMax-R on the large corpus as
the provided implementation did not support corpora of this magnitude.
L
o
c
M
a
x
-
S
L
o
c
M
a
x
-
R
m
w
e
t
k
N
S
P
U
C
S
To
ta
lv
er
bs
LocMax-S ? 124 124 122 124 124
LocMax-R 4747 ? 156 153 156 156
mwetk 4738 4862 ? 1565 1926 1926
NSP 4756 4879 14611 ? 1565 1629
UCS 4377 4364 13407 13045 ? 1926
Total nouns 4760 4884 15064 14682 13418
Table 3: Intersection of the candidate lists extracted from
medium corpus. Nominal candidates en in bottom left, verbal
candidates en in top right.
threshold points, we obtain the mean average precision
(MAP) of the measure without setting a hard threshold.
Table 4 presents the MAP values for the tested AMs15
applied to the candidates extracted from the large cor-
pus (L), where the larger the value, the better the perfor-
mance. We used as baseline the assignment of a random
score and the use of the raw frequency for the candidates.
Except for mwetk:t and mwetk:pmi, all MAP values
are significantly different from the two baselines, with a
two-tailed t test for difference of means assuming unequal
sample sizes and variances (p-value < 0.005).
The LocMax:glue AM performs best for all types
of MWEs, suggesting local maxima as a good generic
MWE indicator and glue as an efficient AM to generate
highly precise results (considering the difficulty of this
task). On the other hand this approach returns a small set
of candidates and this may be problematic depending on
the task (e.g. for building a wide-coverage lexicon). For
mwetk, the best overall AM is the Dice coefficient; the
other measures are not consistently better than the base-
line, or perform better for one MWE type than for the
other. The Poisson-Stirling (ps) measure performed quite
well, while the other two measures tested for NSP per-
formed below baseline for some cases. Finally, as we ex-
pected, the AMs applied by UCS perform all above base-
line and, for nominal MWEs, are comparable to the best
AM (e.g. Poisson.pv and local.MI). The MAP for verbal
expressions varies much for UCS (from 30% to 53% ), but
none of the measures comes close to the MAP of the glue
(87.06%). None of the approaches provides a straightfor-
ward method to choose or combine different AMs.
4.3 Computational resources
In the decision of which AM to adopt, factors like the de-
gree of MWE flexibility and computational performance
may be taken into account. For instance, the Dice coef-
ficient can be applied to any length of n-gram quite fast
15Due to length limitations, we cannot detail the calculation of the
evaluated AMs; please refer to the documentation of each approach,
cited in ? 2, for more details.
4
en noun fr noun en verb
Baseline
random 2.749 6.1072 17.2079
freq 4.7478 8.7946 22.7155
LocMax-S
glue 6.9901 12.9383 87.0614
mwetk
dice 5.7783 9.5419 46.3609
t-test 5.0907 8.6373 26.4185
pmi 2.7589 2.9173 53.5591
log-lik. 3.166 5.5176 45.8837
NSP
pmi 2.9902 7.6782 62.1689
ps 5.3985 12.3791 57.6238
tmi 2.108 4.8928 19.8009
UCS
z.score 6.1202 11.7657 46.8707
Poisson.pv 6.5858 12.8226 32.7737
MI 5.1465 9.3363 53.5591
relative.risk 5.0999 9.2919 46.6702
odds.ratio 5.0364 9.2104 50.2201
gmean 6.0101 11.524 45.6089
local.MI 6.4294 12.7779 29.9858
Table 4: Mean average precision of AMs in large corpus.
while more sophisticated measures like Poisson.pv can be
applied only to 2-grams and sometimes use much com-
putational resources. Even if one could argue that we can
be lenient towards a slow offline extraction process, the
extra waiting may not be worth a slight quality improve-
ment. Moreover, memory limitations are an issue if no
large computer clusters are available.
In Figure 2, we plotted in log-scale the time in sec-
onds used by each approach to extract nominal and ver-
bal expressions in en, using a dedicated 2.4GHz quad-
core Linux machine with 4Gb RAM. For nominal expres-
sions, time increases linearly with the size of the corpus,
whereas for verbal expressions it seems to increase faster
than the size of the corpus. UCS is the slowest approach
for both MWE types while NSP and LocMax-S are the
fastest. However, it is important to emphasize that NSP
consumed more than 3Gb memory to extract 4- and 5-
grams from the large corpus and LocMax-R could not
handle the large corpus at all. In theory, all techniques can
be applied to arbitrarily large corpora if we used a map-
reduce approach (e.g. NSP provides tools to split and join
the corpus). However, the goal of this evaluation is to dis-
cover the performance of the techniques with no manual
optimization. In this sense, mwetk seems to provide an
average trade-off between quality and resources used.
4.4 Flexibility
Table 5 summarises the characteristics of the approaches.
Among them, UCS does not extract candidates from cor-
pora but takes as input a list of bigrams and their counts.
S M LLocMax-xSRmwe
wee
weee
weeee
tSkm
-NPoU
-xmC
onux
 
mn-fm
cr-----
----mn
-noan voCb0%12voCb0%13k4m567289L2voCb0%12voCb0%13k4m567289L2
Figure 2: Time (seconds, log scale) to extract en nouns (bold
line) and verbs (dashed line) from corpora.
LocMax mwetk NSP UCS
Candidate extraction Yes Yes Yes No
N-grams with n > 2 Yes Yes Yes No
Discontiguous MWE No Yes Yes ?
Linguistic filter No Yes No No
Robust AMs No No Yes Yes
Large corpora Partly Yes Yes No
Availability Free Free Free Free
Table 5: Summary of tools for MWE acquisition.
While it only supports n-grams of size 2, NSP imple-
ments some of the AMs for 3 and 4-grams and mwetk
and LocMax have no constraint on the number of words.
LocMax extracts only contiguous MWEs while mwetk
allows the extraction of unrestrictedly distant words and
NSP allows the specification of a window of maximum w
ignored words between each two words of the candidate.
Only mwetk integrates linguistic filters on the lemma,
POS and syntactic annotation, but this was performed us-
ing external tools (sed/grep) for the other approaches with
similar results. The AMs implemented by LocMax and
mwetk are conceived for any size of n-gram and are thus
less statistically sound than the clearly designed measures
used by UCS and, to some extent, by NSP (Fisher test).
The large corpus used in our experiments was not sup-
ported by LocMax-R version, but LocMax-S has a ver-
sion that deals with large corpora, as well as mwetk and
NSP. Finally, all of these approaches are freely available
for download and documented on the web.
5 Conclusions and future work
We evaluated the automatic acquisition of MWEs from
corpora. The dimensions evaluated were type of
construction (for flexibility and contiguity), language
and corpus size. We evaluated two steps separately:
candidate extraction and filtering with AMs. Can-
didate lists are very similar, with approaches like
5
mwetk and NSP returning more candidates (they cover
most of the nominal MWEs in the corpus) but hav-
ing lower precision. LocMax-S presented a remark-
ably high precision for verbal expressions. However,
the choice of an AM may not only take into ac-
count its MAP but also its flexibility and the compu-
tational resources used. Our results suggest that the
approaches could be combined using machine learn-
ing (Pecina, 2005). The data used in our experi-
ments is available at http://www.inf.ufrgs.br/
~ceramisch/?page=downloads/mwecompare.
In the future, we would like to develop this evaluation
further by taking into account other characteristics such
as the domain and genre of the source corpus. Such eval-
uation would be useful to guide future research on spe-
cialised multiword terminology extraction, determining
differences with respect to generic MWE extraction. We
would also like to evaluate other MWE-related tasks (e.g.
classification, interpretation) and also dictionary-based
identification (Kulkarni and Finlayson, 2011) and bilin-
gual MWE acquisition (Carpuat and Diab, 2010). Fi-
nally, we believe that an application-based extrinsic eval-
uation involving manual validation of candidates would
ultimately demonstrate the usefulness of current MWE
acquisition techniques.
Acknowledgements
This work was partly funded by the CAMELEON project
(CAPES?COFECUB 707-11).
References
Dimitra Anastasiou, Chikara Hashimoto, Preslav Nakov, and
Su Nam Kim, editors. 2009. Proc. of the ACL Workshop on
MWEs: Identification, Interpretation, Disambiguation, Ap-
plications (MWE 2009), Suntec, Singapore, Aug. ACL.
Timothy Baldwin. 2008. A resource for evaluating the deep
lexical acquisition of english verb-particle constructions. In
Gr?goire et al (Gr?goire et al, 2008), pages 1?2.
Marine Carpuat and Mona Diab. 2010. Task-based evaluation
of multiword expressions: a pilot study in statistical machine
translation. In Proc. of HLT: The 2010 Annual Conf. of the
NAACL (NAACL 2003), pages 242?245, Los Angeles, Cali-
fornia, Jun. ACL.
Joaquim Ferreira da Silva, Ga?l Dias, Sylvie Guillor?, and Jos?
Gabriel Pereira Lopes. 1999. Using localmaxs algorithm for
the extraction of contiguous and non-contiguous multiword
lexical units. In Proceedings of the 9th Portuguese Confer-
ence on Artificial Intelligence: Progress in Artificial Intelli-
gence, EPIA ?99, pages 113?132, London, UK. Springer.
Stefan Evert and Brigitte Krenn. 2005. Using small random
samples for the manual evaluation of statistical association
measures. Comp. Speech & Lang. Special issue on MWEs,
19(4):450?466.
Mark Finlayson and Nidhi Kulkarni. 2011. Detecting multi-
word expressions improves word sense disambiguation. In
Kordoni et al (Kordoni et al, 2011), pages 20?24.
Nicole Gr?goire, Stefan Evert, and Brigitte Krenn, editors.
2008. Proc. of the LREC Workshop Towards a Shared Task
for MWEs (MWE 2008), Marrakech, Morocco, Jun.
Su Nam Kim and Timothy Baldwin. 2008. Standardised evalu-
ation of english noun compound interpretation. In Gr?goire
et al (Gr?goire et al, 2008), pages 39?42.
Valia Kordoni, Carlos Ramisch, and Aline Villavicencio, edi-
tors. 2011. Proc.of the ACL Workshop on MWEs: from Pars-
ing and Generation to the Real World (MWE 2011), Portland,
OR, USA, Jun. ACL.
Nidhi Kulkarni and Mark Finlayson. 2011. jMWE: A java
toolkit for detecting multi-word expressions. In Kordoni
et al (Kordoni et al, 2011), pages 122?124.
?ric Laporte, Preslav Nakov, Carlos Ramisch, and Aline Villav-
icencio, editors. 2010. Proc.of the COLING Workshop on
MWEs: from Theory to Applications (MWE 2010), Beijing,
China, Aug. ACL.
Darren Pearce. 2002. A comparative evaluation of collocation
extraction techniques. In Proc. of the Third LREC (LREC
2002), Las Palmas, Canary Islands, Spain, May. ELRA.
Pavel Pecina. 2005. An extensive empirical study of collo-
cation extraction methods. In Proc. of the ACL 2005 SRW,
pages 13?18, Ann Arbor, MI, USA, Jun. ACL.
Ted Pedersen, Satanjeev Banerjee, Bridget McInnes, Saiyam
Kohli, Mahesh Joshi, and Ying Liu. 2011. The ngram
statistics package (text::NSP) : A flexible tool for identify-
ing ngrams, collocations, and word associations. In Kordoni
et al (Kordoni et al, 2011), pages 131?133.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and Aline
Villavicencio. 2008. An evaluation of methods for the ex-
traction of multiword expressions. In Gr?goire et al (Gr?-
goire et al, 2008), pages 50?53.
Carlos Ramisch, Aline Villavicencio, and Christian Boitet.
2010. Multiword expressions in the wild? the mwetoolkit
comes in handy. In Yang Liu and Ting Liu, editors, Proc.
of the 23rd COLING (COLING 2010) ? Demonstrations,
pages 57?60, Beijing, China, Aug. The Coling 2010 Orga-
nizing Committee.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and
Dan Flickinger. 2002. Multiword expressions: A pain in the
neck for NLP. In Proc. of the 3rd CICLing (CICLing-2002),
volume 2276/2010 of LNCS, pages 1?15, Mexico City, Mex-
ico, Feb. Springer.
Violeta Seretan. 2008. Collocation extraction based on syn-
tactic parsing. Ph.D. thesis, University of Geneva, Geneva,
Switzerland.
Joaquim Silva and Gabriel Lopes. 1999. A local maxima
method and a fair dispersion normalization for extracting
multi-word units from corpora. In Proceedings of the Sixth
Meeting on Mathematics of Language (MOL6), pages 369?
381, Orlando, FL, USA, Jul.
Ying Xu, Randy Goebel, Christoph Ringlstetter, and Grzegorz
Kondrak. 2010. Application of the tightness continuum
measure to chinese information retrieval. In Laporte et al
(Laporte et al, 2010), pages 54?62.
6
Proceedings of the 2012 Student Research Workshop, pages 61?66,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Generic Framework for Multiword Expressions Treatment:
from Acquisition to Applications
Carlos Ramisch
Federal University of Rio Grande do Sul (Brazil)
GETALP ? LIG, University of Grenoble (France)
ceramisch@inf.ufrgs.br
Abstract
This paper presents an open and flexible method-
ological framework for the automatic acquisition of
multiword expressions (MWEs) from monolingual
textual corpora. This research is motivated by the
importance of MWEs for NLP applications. Af-
ter briefly presenting the modules of the framework,
the paper reports extrinsic evaluation results con-
sidering two applications: computer-aided lexicog-
raphy and statistical machine translation. Both ap-
plications can benefit from automatic MWE acquisi-
tion and the expressions acquired automatically from
corpora can both speed up and improve their quality.
The promising results of previous and ongoing ex-
periments encourage further investigation about the
optimal way to integrate MWE treatment into these
and many other applications.
1 Introduction
Multiword expressions (MWEs) range over linguistic
constructions such as idioms (to pay an arm and a leg),
fixed phrases (rock ?n? roll) and noun compounds (dry
ice). There is no unique and widely accepted definition
for the term multiword expression. It can be an ?arbi-
trary and recurrent word combination? (Smadja, 1993)
or ?a syntactic and semantic unit whose exact and un-
ambiguous meaning or connotation cannot be derived di-
rectly from the meaning or connotation of its compo-
nents? (Choueka, 1988) or simply an ?idiosyncratic inter-
pretation that crosses word boundaries (or spaces)? (Sag
et al, 2002). MWEs lie in the fuzzy zone between lexi-
con and syntax, thus constituting a real challenge for NLP
systems. In addition, they are very pervasive, occurring
frequently in everyday language as well as in specialised
communications. Some common properties of MWEs
are:1
1These are not binary yes/no flags, but values in a continuum going
from flexible word combinations to prototypical fixed expressions.
SRC I paid my poor parents a visit
MT J?ai pay? mes pauvres parents une visite
REF J?ai rendu visite ? mes pauvres parents
SRC Students pay an arm and a leg to park on campus
MT Les ?tudiants paient un bras et une jambe pour
se garer sur le campus
REF Les ?tudiants paient les yeux de la t?te pour se
garer sur le campus
SRC It shares the translation-invariance and homo-
geneity properties with the central moment
MT Il partage la traduction-invariance et propri?t?s
d?homog?n?it? avec le moment central
REF Il partage les propri?t?s d?invariance par trans-
lation et d?homog?n?it? avec le moment central
Table 1: Examples of SMT errors due to MWEs.
? Arbitrariness: sometimes valid constructions are
not acceptable because people do not use them.
Smadja (1993, p. 143?144) illustrates this by pre-
senting 8 different ways of referring to the Dow
Jones index, among which only 4 are used.
? Institutionalisation: MWEs are recurrent, as they
correspond to conventional ways of saying things.
Jackendoff (1997) estimates that they compose half
of the entries of a speaker?s lexicon, and Sag et al
(2002) point out that this may be an underestimate if
we consider domain-specific MWEs.
? Limited semantic variability: MWEs do not un-
dergo the same semantic compositionality rules as
ordinary word combinations. This is expressed in
terms of (i) non-compositionality, as the meaning
of the whole expression often cannot be directly in-
ferred from the meaning of the parts composing it,
(ii) non-substitutability, as it is not possible to re-
place part of an MWE by a related (synonym/equiv-
alent) word or construction, and (iii) no word-for-
word translation.
61
? Limited syntactic variability: standard grammati-
cal rules do not apply to MWEs. This can be ex-
pressed in terms of (i) lexicalisation, as one can-
not list all MWEs in the lexicon (undergeneration)
nor include them all in the grammar (overgenera-
tion) and (ii) extragrammaticality, as MWEs are
unpredictable and seem ?weird? for a second lan-
guage learner who only knows general rules.2
? Heterogeneity: MWEs are hard to define because
they encompass a large amount of phenomena.
Thus, NLP applications cannot use a unified ap-
proach and need to rely on some typology3.
In this paper, I adopt the definition by Calzolari et al
(2002), who define MWEs as:
different but related phenomena [which] can be
described as a sequence4 of words that acts as a
single unit at some level of linguistic analysis.
This generic and intentionally vague definition can be
narrowed down according to the application needs. For
example, for the statistical machine translation (MT) sys-
tem5 used in the examples shown in Table 1, an MWE is
any sequence of words which, when not translated as a
unit, generates errors: ungrammatical or unnatural verbal
constructions (sentence 1), awkward literal translations of
idioms (sentence 2) and problems of lexical choice and
word order in specialised texts (sentence 3). These ex-
amples illustrate the importance of correctly dealing with
MWEs in MT applications and, more generally, MWEs
can speed up and help remove ambiguities in many cur-
rent NLP applications, for example:
? Lexicography: Church and Hanks (1990) used a
lexicographic environment as their evaluation sce-
nario, comparing manual and intuitive research with
the automatic association ratio they proposed.
? Word sense disambiguation: MWEs tend to be
less polysemous than simple words. Finlayson and
Kulkarni (2011) exemplify that the word world has
9 senses in Wordnet 1.6, record has 14, but world
record has only 1.
? POS tagging and parsing: recent work in parsing
and POS tagging indicates that MWEs can help re-
move syntactic ambiguities (Seretan, 2008).
? Information retrieval: when MWEs like pop star
are indexed as a unit, the accuracy of the system im-
proves on multiword queries (Acosta et al, 2011).
2Examples of MWEs that breach standard grammatical rules include
kingdom come and by and large.
3For example, Smadja (1993) classifies them according to syntactic
function while Sag et al (2002) classify them according to flexibility.
4Although they define MWEs as ?sequences?, assuming contiguity,
we assume ?sets? of words for greater generality.
5Automatic translations (MT) by Google (http://translate.
google.com/) on 2012/02/18. Reference (REF) by native speaker.
2 Thesis contributions
Despite the importance of MWEs in several applications,
they are often neglected in the design and construction
of real-life systems. In 1993, Smadja pointed out that
?. . . although disambiguation was originally considered
as a performance task, the collocations retrieved have not
been used for any specific computational task.? Most
of the recent and current research in the MWE commu-
nity still focuses on MWE acquisition instead of integra-
tion of automatically acquired or manually compiled re-
sources into applications. The main contribution of my
thesis is that it represents a step toward the integration
of automatically extracted MWEs into real-life applica-
tions. Concretely, my contributions can be classified in
two categories: first, I propose a unified, open and flexi-
ble methodological framework (? 3) for automatic MWE
acquisition from corpora; and second, I am performing
an intrinsic and extrinsic evaluation of MWE acquisition
(? 4), dissecting the influence of the different types of re-
sources employed in the acquisition on the quality of the
MWEs. The results of ongoing experiments are interest-
ing but further work is needed to better understand the
contributions of MWEs to the systems (? 5).
Methodological Framework To date, there is no
agreement on whether there is a single best method for
MWE acquisition, or whether a different subset of meth-
ods works better for a given MWE type. Most of recent
work on MWE treatment focuses on candidate extraction
from preprocessed text (Seretan, 2008) and on the auto-
matic filtering and ranking through association measures
(Evert, 2004; Pecina, 2010), but few authors provide a
whole picture of the MWE treatment pipeline. One of
the advantages of the framework I propose is that it mod-
els the whole acquisition process with modular tasks that
can be chained in several ways, each task having multiple
available techniques. Therefore, it is highly customisable
and allows for a large number of parameters to be tuned
according to the target MWE types. Moreover, the tech-
niques I have developed do not depend on a fixed length
of candidate expression nor on adjacency assumptions,
as the words in an expression might occur several words
away. Thanks to this flexibility, this methodology can
be easily applied to virtually any language, MWE type
and domain, not strictly depending on a given formal-
ism or tool6. Intuitively, for a given language, if some
preprocessing tools like POS taggers and/or parsers are
available, the results will be much better than running the
methods on raw text. But since such tools are not avail-
able for all languages, the methodology was conceived to
be applicable even in the absence of preprocessing.
6However, it is designed to deal with languages that use spaces to
separate words. Thus, when working with Chinese, Japanese, or even
with German compounds, some additional preprocessing is required.
62
Evaluation of MWE Acquisition Published results
comparing MWE extraction techniques usually evaluate
them on small controlled data sets using objective mea-
sures such as precision, recall and mean average preci-
sion (Schone and Jurafsky, 2001; Pearce, 2002; Evert and
Krenn, 2005). On the one hand, the results of intrinsic
evaluation are often vague or inconclusive: although they
shed some light on the optimal parameters for the given
scenario, they are hard to generalise and cannot be di-
rectly applied to other configurations. The quality of ac-
quired MWEs as measured by objective criteria depends
on the language, domain and type of the target construc-
tion, on corpus size and genre, on already available re-
sources7, on the applied filters, preprocessing steps, etc.
On the other hand, extrinsic evaluation consists of insert-
ing acquired MWEs into a real NLP application and eval-
uating the impact of this new data on the overall perfor-
mance of the system. For instance, it may be easier to ask
a human annotator to evaluate the output of an MT sys-
tem than to ask whether a sequence of words constitutes
an MWE. Thus, another original contribution of my the-
sis is application-oriented extrinsic evaluation of MWE
acquisition on two study cases: computer-aided lexicog-
raphy and statistical machine translation. My goal is to
investigate (1) how much the MWEs impact on the appli-
cation and (2) what is (are) the best way(s) of integrating
them in the complex pipeline of the target application.
3 MWE Extraction
Among early work on developing methods for MWE
identification, there is that of Smadja (1993). He pro-
posed and developed a tool called Xtract, aimed at
general-purpose collocation extraction from text using a
combination of n-grams and a mutual information mea-
sure. On general-purpose texts, Xtract has a precision of
around 80%. Since then, many advances have been made,
either looking at MWEs in general (Dias, 2003), or focus-
ing on specific MWE types, such as collocations, phrasal
verbs and compound nouns. A popular type-independent
approach to MWE identification is to use statistical as-
sociation measures, which have been applied to the task
with varying degrees of success (Evert and Krenn, 2005).
One of the advantages of this approach is that it is lan-
guage independent. This is particularly important since
although work on MWEs in several languages has been
reported, e.g. Dias (2003) for Portuguese and Evert and
Krenn (2005) for German, work on English still seems to
predominate.
I propose a new framework called mwetoolkit, de-
scribed in Figure 1, which integrates multiple techniques
and covers the whole pipeline of MWE acquisition. One
can preprocess a raw monolingual corpus, if tools are
7It is useless to acquire MWEs already present in the dictionary.
raw corpus preprocessedcorpusPREPROCESSING  * POS tagging  * Lemmatisation  * Dependency parsing indexedcorpusINDEXING
MWEcandidates
extractionpatterns
CANDIDATE EXTRACTION  * n-grams  * POS sequences     (multilevel RegExp)  * syntax
Filteredcandidates
web as corpus
filteringpatterns
CANDIDATE FILTERING  * Counting  * Statistical features  * Filters:    - Stopwords    - Patterns    - Thresholds  * Ranking and croping
Validatedcandidates
VALIDATION  * Sampling  * Human annotation  * Comparison with     gold standardMWE acquisitionmodel NLPapplication
LEARNING
Contributivedictionarymanagement
Figure 1: Framework for MWE acquisition from corpora
available for the target language, enriching it with POS
tags, lemmas and dependency syntax. Then, based on
expert linguistic knowledge, intuition, empiric observa-
tion and/or examples, one defines multilevel patterns in a
formalism similar to regular expressions to describe the
target MWEs. The application of these patterns on an in-
dexed corpus generates a list of candidate MWEs. For
filtering, a plethora of methods is available, ranging from
simple frequency thresholds to stopword lists and sophis-
ticated association measures. Finally, the resulting fil-
tered candidates are either directly injected into an NLP
application or further manually validated before applica-
tion. An alternative use for the validated candidates is
to train a machine learning model which can be applied
on new corpora in order to automatically identify and ex-
tract MWEs based on the characteristics of the previously
acquired ones. For further details, please refer to the
website of the framework8 and to previous publications
(Ramisch et al, 2010a; Ramisch et al, 2010b).
4 Application-oriented evaluation
In this section, I present summarised results of extrinsic
quantitative and qualitative evaluation of the framework
for MWE acquisition propose in ? 3. The target applica-
tions are computer-aided lexicography (? 4.1) and statis-
tical machine translation (? 4.2).
8http://mwetoolkit.sf.net
63
Language Type Corpus (words) Candidates Final MWEs Publication
English PV Europarl (13M) 5.3K 875 (Ramisch et al, 2012)
French NC Europarl (14.5M) 104K 3,746 (Ramisch et al, 2012)
Greek NC Europarl (26M) 25K 815 (Linardaki et al, 2010)
Portuguese CP PLN-BR-FULL (29M) 407K 773 (Duran et al, 2011)
Table 2: MWE acquisition applied to lexicography
4.1 Computer-aided Lexicography
In this evaluation, I collaborated with colleagues who are
experienced linguists and lexicographers, in order to cre-
ate new lexical resources containing MWEs. The lan-
guages of the resources are English, French, Greek and
Portuguese. Table 2 summarises the outcomes of each
evaluation. The created data sets are freely available.9, 10
We extracted English phrasal verbs (PVs) from the En-
glish portion of the Europarl corpus11. We considered a
PV as being formed by a verb (except to be and to have)
followed by a prepositional particle12 not further than 5
words after it13 This resulted in 5,302 phrasal verb candi-
dates occurring more than once in the corpus, from which
875 were automatically identified as true PVs and the oth-
ers are currently under manual validation. Analogously,
the French noun compounds (NCs) were extracted from
Europarl using the following pattern: a noun followed by
either an adjective or a prepositional complement14. Af-
ter filtering out candidates that occur once in the corpus,
we obtained 3,746 MWE candidates and part of the re-
maining candidates will be manually analysed in the fu-
ture.
For Greek, in particular, considerable work has been
done to study the linguistic properties of MWEs, but
computational approaches are still limited (Fotopoulou
et al, 2008). In our experiments, we extracted from
the POS-tagged Greek part of the Europarl corpus
words matching the following patterns: adjective-noun,
noun-noun, noun-determiner-noun, noun-preposition-
noun, preposition-noun-noun, noun-adjective-noun and
noun-conjunction-noun. The candidates were counted in
two corpora and annotated with four association mea-
sures, and the top 150 according to each measure where
annotated by three native speakers, that is, each annotator
judged around 1,200 candidates and in the end the anno-
tations were joined, creating a lexicon with 815 Greek
nominal MWEs.
9http://multiword.sourceforge.net/PHITE.php?
sitesig=FILES&page=FILES_20_Data_Sets
10http://www.inf.ufrgs.br/~ceramisch/?page=
downloads/mwecompare
11http://statmt.org/europarl
12up, off, down, back, away, in, on.
13Even though the particle might occur further than 5 positions away,
such cases are sufficiently rare to be ignored in this experiment.
14Prepositions de, ? and en followed by optionally determined noun.
Finally, the goal of the work with Portuguese com-
plex predicates (CPs) was to perform a qualitative
analysis of these constructions. Therefore, we POS-
tagged the PLN-BR-FULL corpus15 and extracted
sequences of words matching the patterns: verb-
[determiner]-noun-preposition, verb-preposition-noun,
verb-[preposition/determiner]-adverb and verb-adjective.
The extraction process resulted in a list of 407,014
candidates which were further filtered using statistical
association measures. Thus, an expert human annotator
manually validated 12,545 candidates from which 699
were annotated as compositional verbal expressions
and 74 as idiomatic verbal expressions. Afterwards,
a fine-grained analysis of each extraction pattern was
conducted with the goal of finding correlations between
syntactic flexibility and semantic properties such as
compositionality.
4.2 Statistical Machine Translation (SMT)
Incorporating even simple treatments for MWEs in
SMT systems can improve translation quality. For in-
stance, Carpuat and Diab (2010) adopt two complemen-
tary strategies for integrating MWEs: a static strategy
of single-tokenisation that treats MWEs as word-with-
spaces and a dynamic strategy that adds a count for the
number of MWEs in the source phrase. They found that
both strategies result in improvement of translation qual-
ity, which suggests that SMT phrases alone do not model
all MWE information. Morin and Daille (2010) obtained
an improvement of 33% in the French?Japanese trans-
lation of MWEs with a morphologically-based composi-
tional method for backing-off when there is not enough
data in a dictionary to translate a MWE (e.g. chronic fa-
tigue syndrome decomposed as [chronic fatigue] [syn-
drome], [chronic] [fatigue syndrome] or [chronic] [fa-
tigue] [syndrome]). For translating from and to mor-
phologically rich languages like German, where a com-
pound is in fact a single token formed through concate-
nation, Stymne (2011) splits the compound into its sin-
gle word components prior to translation and then applies
some post-processing, like the reordering or merging of
the components, after translation. She obtains improve-
ments in BLEU from 21.63 to 22.12 in English?Swedish
and from 19.31 to 19.73 in English?German.
15www.nilc.icmc.usp.br/plnbr
64
% Good % Acceptable % Incorrect
Baseline 0.53 0.36 0.11
TOK 0.55 0.29 0.16
PV? 0.50 0.39 0.11
PART 0.53 0.36 0.11
VERB 0.53 0.36 0.11
BILEX 0.50 0.29 0.20
Table 3: Evaluation of translation of phrasal verbs in test set.
In the current experiments, a standard non factored
phrase-based SMT system was built using the open-
source Moses toolkit with parameters similar to those of
the baseline system for the 2011 WMT campaign. 16.
For training, we used the English?Portuguese Europarl
v6 (EP) corpus, with 1.7M sentences and around 50M
words. The training data contains the first 200K sen-
tences tokenized and lowercased, resulting in 152,235
parallel sentences and around 3.1M words. The whole
Portuguese corpus was used as training data for 5-gram
language model built with SRILM. Phrasal verbs were
automatically identified using the jMWE tool and a dic-
tionary of PVs. We compared the following five strate-
gies for the integration of automatically identified phrasal
verbs in the system:
? TOK: before translation, rearrange the verb and the
particle in a joint configuration and transform them
into a single token with underscore (e.g. call him up
into call_up him).
? PV?: add a binary feature to each bi-phrase indicat-
ing whether a source phrasal verb has been detected
in it or not.
? PART: replace the particle by the one most fre-
quently used with the target verb, using a web-based
language model with a symmetric windows of 1 to 5
words around the particle.
? VERB: modify the form of the Portuguese verb
(gerund or infinitive), according to the form detected
on the English side.
? BILEX (or bilingual lexicon): augment the phrase ta-
ble of the baseline system with 179,133 new bilin-
gual phrases from an English?Portuguese phrasal
verb lexicon.
Table 3 shows the preliminary results of a human eval-
uation performed on a test set of 100 sentences. The sen-
tences were inspected and we verified that, while some
translations improve with the integration strategies, oth-
ers are degraded. No absolute improvement was ob-
served, but we believe that this is due to the fact that our
evaluation needs to consider more fine-grained classes of
16www.statmt.org/wmt11/baseline.html
phrasal verbs instead of mixing them all in the same test
set. Additionally, we would need to annotate more data
in order to obtain more representative results. These hy-
potheses motivate us to continue our investigation in or-
der to obtain a deeper understanding the impact of each
integration strategy on each step of the SMT system.
5 Future Experiments and Perspectives
In this paper, I described an open framework for the au-
tomatic acquisition of MWEs from corpora. What dis-
tinguishes it from related work is that it provides an
integrated environment covering the whole acquisition
pipeline. For each module, there are multiple available
techniques which are flexible, portable and can be com-
bined in several ways. The usefulness of the framework
is then presented in terms of extrinsic application-based
evaluation. I presented summarised results of ongoing
experiments in computer-aided lexicography and in SMT.
Although our results are promising, the experiments
on SMT need further investigation. I am currently apply-
ing syntax-based identification and analysing word align-
ment and translation table entries for a set of prototypi-
cal MWEs, in order to obtain a better understanding of
the impact of each integration strategy on the system.
Moreover, I would like to pursue previous experiments
on bilingual MWE acquisition from parallel and compa-
rable resources. Finally, I would like to experiment on
MWE simplification (e.g. replacing a multiword verb like
go back by its simplex form regress) as preprocessing for
SMT, in order to improve translation quality by making
the source language look more like the target language.As
these improvements depend in the MT paradigm, I would
also like to evaluate strategies for the integration of verbal
MWEs in expert MT systems.
In spite of a large amount of work in the area, the
treatment of MWEs in NLP applications is still an open
and challenging problem. This is not surprising, given
their complex and heterogeneous behaviour (Sag et al,
2002). At the beginning of the 2000?s, Schone and Juraf-
sky (2001) asked whether the identification of MWEs was
a solved problem, and the answer that paper gave was ?no,
it is not?. The MWE workshop series have shown that this
is still the case, listing several challenges in MWE treat-
ment like lexical representation and application-oriented
evaluation. Therefore, I believe that my thesis will be a
significant step toward the full integration of MWE treat-
ment in NLP applications, but there is still a long road to
go.
Acknowledgements
This work was partly funded by the CAMELEON project
(CAPES?COFECUB 707-11) and by a Ph.D. grant from the
French Ministry for Higher Education and Research. I would
65
like to thank my supervisors Aline Villavicencio and Christian
Boitet, as well as the colleagues who contributed to this work:
Evita Linardaki, Valia Kordoni, Magali Sanchez Duran and Vi-
tor De Araujo.
References
Otavio Acosta, Aline Villavicencio, and Viviane Moreira. 2011.
Identification and treatment of multiword expressions ap-
plied to information retrieval. In Valia Kordoni, Carlos
Ramisch, and Aline Villavicencio, editors, Proc.of the ALC
Workshop on MWEs: from Parsing and Generation to the
Real World (MWE 2011), pages 101?109, Portland, OR,
USA, Jun. ACL.
Nicoleta Calzolari, Charles Fillmore, Ralph Grishman, Nancy
Ide, Alessandro Lenci, Catherine Macleod, and Antonio
Zampolli. 2002. Towards best practice for multiword ex-
pressions in computational lexicons. In Proc. of the Third
LREC (LREC 2002), pages 1934?1940, Las Palmas, Canary
Islands, Spain, May. ELRA.
Marine Carpuat and Mona Diab. 2010. Task-based evaluation
of multiword expressions: a pilot study in statistical machine
translation. In Proc. of HLT: The 2010 Annual Conf. of the
NAACL (NAACL 2003), pages 242?245, Los Angeles, Cali-
fornia, Jun. ACL.
Yaacov Choueka. 1988. Looking for needles in a haystack or
locating interesting collocational expressions in large textual
databases. In RIAO?88, pages 609?624.
Kenneth Ward Church and Patrick Hanks. 1990. Word asso-
ciation norms mutual information, and lexicography. Comp.
Ling., 16(1):22?29.
Ga?l Dias. 2003. Multiword unit hybrid extraction. In Francis
Bond, Anna Korhonen, Diana McCarthy, and Aline Villavi-
cencio, editors, Proc. of the ACL Workshop on MWEs: Anal-
ysis, Acquisition and Treatment (MWE 2003), pages 41?48,
Sapporo, Japan, Jul. ACL.
Magali Sanches Duran, Carlos Ramisch, Sandra Maria Alu?sio,
and Aline Villavicencio. 2011. Identifying and analyzing
brazilian portuguese complex predicates. In Valia Kordoni,
Carlos Ramisch, and Aline Villavicencio, editors, Proc.of the
ALC Workshop on MWEs: from Parsing and Generation to
the Real World (MWE 2011), pages 74?82, Portland, OR,
USA, Jun. ACL.
Stefan Evert and Brigitte Krenn. 2005. Using small random
samples for the manual evaluation of statistical association
measures. Comp. Speech & Lang. Special issue on MWEs,
19(4):450?466.
Stefan Evert. 2004. The Statistics of Word Cooccur-
rences: Word Pairs and Collocations. Ph.D. thesis, Institut
f?r maschinelle Sprachverarbeitung, University of Stuttgart,
Stuttgart, Germany.
Mark Finlayson and Nidhi Kulkarni. 2011. Detecting multi-
word expressions improves word sense disambiguation. In
Valia Kordoni, Carlos Ramisch, and Aline Villavicencio, ed-
itors, Proc.of the ALC Workshop on MWEs: from Parsing
and Generation to the Real World (MWE 2011), pages 20?
24, Portland, OR, USA, Jun. ACL.
Aggeliki Fotopoulou, Giorgos Giannopoulos, Maria Zourari,
and Marianna Mini. 2008. Automatic recognition and ex-
traction of multiword nominal expressions from corpora (in
greek). In Proceedings of the 29th Annual Meeting, De-
partment of Linguistics, Aristotle University of Thessaloniki,
Greece.
Ray Jackendoff. 1997. Twistin? the night away. Language,
73:534?559.
Evita Linardaki, Carlos Ramisch, Aline Villavicencio, and
Aggeliki Fotopoulou. 2010. Towards the construction of
language resources for greek multiword expressions: Extrac-
tion and evaluation. In Stelios Piperidis, Milena Slavcheva,
and Cristina Vertan, editors, Proc. of the LREC Workshop
on Exploitation of multilingual resources and tools for Cen-
tral and (South) Eastern European Languages, pages 31?40,
Valetta, Malta. May.
Emmanuel Morin and B?atrice Daille. 2010. Compositionality
and lexical alignment of multi-word terms. Lang. Res. &
Eval. Special Issue on Multiword expression: hard going or
plain sailing, 44(1-2):79?95, Apr.
Darren Pearce. 2002. A comparative evaluation of collocation
extraction techniques. In Proc. of the Third LREC (LREC
2002), Las Palmas, Canary Islands, Spain, May. ELRA.
Pavel Pecina. 2010. Lexical association measures and colloca-
tion extraction. Lang. Res. & Eval. Special Issue on Multi-
word expression: hard going or plain sailing, 44(1-2):137?
158, Apr.
Carlos Ramisch, Aline Villavicencio, and Christian Boitet.
2010a. Multiword expressions in the wild? the mwetoolkit
comes in handy. In Yang Liu and Ting Liu, editors, Proc.
of the 23rd COLING (COLING 2010) ? Demonstrations,
pages 57?60, Beijing, China, Aug. The Coling 2010 Orga-
nizing Committee.
Carlos Ramisch, Aline Villavicencio, and Christian Boitet.
2010b. mwetoolkit: a framework for multiword expression
identification. In Proc. of the Seventh LREC (LREC 2010),
Malta, May. ELRA.
Carlos Ramisch, Vitor De Araujo, and Aline Villavicencio.
2012. A broad evaluation of techniques for automatic acqui-
sition of multiword expressions. In Proc. of the ACL 2012
SRW, Jeju, Republic of Korea, Jul. ACL.
Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and
Dan Flickinger. 2002. Multiword expressions: A pain in the
neck for NLP. In Proc. of the 3rd CICLing (CICLing-2002),
volume 2276/2010 of LNCS, pages 1?15, Mexico City, Mex-
ico, Feb. Springer.
Patrick Schone and Daniel Jurafsky. 2001. Is knowledge-free
induction of multiword unit dictionary headwords a solved
problem? In Lillian Lee and Donna Harman, editors, Proc.
of the 2001 EMNLP (EMNLP 2001), pages 100?108, Pitts-
burgh, PA USA, Jun. ACL.
Violeta Seretan. 2008. Collocation extraction based on syn-
tactic parsing. Ph.D. thesis, University of Geneva, Geneva,
Switzerland.
Frank A. Smadja. 1993. Retrieving collocations from text:
Xtract. Comp. Ling., 19(1):143?177.
Sara Stymne. 2011. Pre- and postprocessing for statistical ma-
chine translation into germanic languages. In Proc. of the
ACL 2011 SRW, pages 12?17, Portland, OR, USA, Jun. ACL.
66
Proceedings of the First Workshop on Multilingual Modeling, pages 25?31,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Comparable Corpus Based on Aligned Multilingual Ontologies
Roger Granada
PUCRS (Brazil)
roger.granada@acad.pucrs.br
Lucelene Lopes
PUCRS (Brazil)
lucelene.lopes@pucrs.br
Carlos Ramisch
University of Grenoble (France)
ceramisch@inf.ufrgs.br
Cassia Trojahn
University of Grenoble (France)
cassia.trojahn@inria.fr
Renata Vieira
PUCRS (Brazil)
renata.vieira@pucrs.br
Aline Villavicencio
UFRGS (Brazil)
alinev@gmail.com
Abstract
In this paper we present a methodology for
building comparable corpus, using multilin-
gual ontologies of a scpecific domain. This
resource can be exploited to foster research on
multilingual corpus-based ontology learning,
population and matching. The building re-
source process is exemplified by the construc-
tion of annotated comparable corpora in En-
glish, Portuguese, and French. The corpora,
from the conference organization domain, are
built using the multilingual ontology concept
labels as seeds for crawling relevant docu-
ments from the web through a search engine.
Using ontologies allows a better coverage of
the domain. The main goal of this paper is
to describe the design methodology followed
by the creation of the corpora. We present a
preliminary evaluation and discuss their char-
acteristics and potential applications.
1 Introduction
Ontological resources provide a symbolic model of
the concepts of a scientific, technical or general
domain (e.g. Chemistry, automotive industry, aca-
demic conferences), and of how these concepts are
related to one another. However, ontology creation
is labour intensive and error prone, and its mainte-
nance is crucial for ensuring the accuracy and util-
ity of a given resource. In multilingual contexts, it
is hard to keep the coherence among ontologies de-
scribed in different languages and to align them ac-
curately. These difficulties motivate the use of semi-
automatic approaches for cross-lingual ontology en-
richment and population, along with intensive reuse
and interoperability between ontologies. For that, it
is crucial to have domain-specific corpora available,
or the means of automatically gathering them.
Therefore, this paper describes an ontology-based
approach for the generation of multilingual compa-
rable corpora. We use a set of multilingual domain-
dependent ontologies, which cover different aspects
of the conference domain. These ontologies provide
the seeds for building the domain specific corpora
from the web. Using high-level background knowl-
edge expressed in concepts and relations, which are
represented as natural language descriptions in the
labels of the ontologies, allow focused web crawl-
ing with a semantic and contextual coverage of the
domain. This approach makes web crawling more
precise, which is crucial when exploiting the web as
a huge corpus.
Our motivation is the need of such resources
in tasks related to semi-automatic ontology cre-
ation and maintenance in multilingual domains.
We exemplify our methodology focusing on the
construction of three corpora, one in English,
one in Portuguese, and one in French. This
effort is done in the context of a larger re-
search project which aims at investigating meth-
ods for the construction of lexical resources, in-
tegrating multilingual lexica and ontologies, fo-
cusing on collaborative and automatic techniques
(http://cameleon.imag.fr/xwiki/bin/view/Main/).
In the next section, we present some relevant re-
lated work (?2). This is followed by a description
of the methodology used to build the corpora (?3).
Finally, the application example expressed by the
resulting corpora are evaluated (?4) and discussed
25
(?5). We conclude by outlining their future applica-
tions (? 6).
2 Related Work
Web as corpus (WAC) approaches have been suc-
cessfully adopted in many cases where data sparse-
ness plays a major limiting role, either in specific
linguistic constructions and words in a language
(e.g. compounds and multiword expressions), or for
less resourced languages in general1.
For instance, Grefenstette (1999) uses WAC for
machine translation of compounds from French into
English, Keller et al (2002) for adjective-noun,
noun-noun and verb-object bigram discovery, and
Kim and Nakov (2011) for compound interpretation.
Although a corpus derived from the web may con-
tain noise, the sheer size of data available should
compensate for that. Baroni and Ueyama (2006)
discuss in details the process of corpus construc-
tion from web pages for both generic and domain-
specific corpora. In particular, they focus on the
cleaning process applied to filter the crawled web
pages. Much of the methodology applied in our
work is similar to their proposed approach (see ?3).
Moreover, when access to parallel corpora is lim-
ited, comparable corpora can minimize data sparse-
ness, as discussed by Skadina et al (2010). They
create bilingual comparable corpora for a variety of
languages, including under-resourced ones, with 1
million words per language. This is used as ba-
sis for the definition of metrics for comparability of
texts. Forsyth and Sharoff (2011) compile compa-
rable corpora for terminological lexicon construc-
tion. An initial verification of monolingual compa-
rability is done by partitioning the crawled collec-
tion into groups. Those are further extended through
the identification of representative archetypal texts
to be used as seeds for finding documents of the
same type.
Comparable corpora is a very active research sub-
ject, being in the core of several European projects
(e.g. TTC2, Accurat3). Nonetheless, to date most of
1Kilgarriff (2007) warns about the dangers of statistics heav-
ily based on a search engine. However, since we use the down-
loaded texts of web pages instead of search engine count esti-
mators, this does not affect the results obtained in this work.
2www.ttc-project.eu
3www.accurat-project.eu
the research on comparable corpora seems to focus
on lexicographic tasks (Forsyth and Sharoff, 2011;
Sharoff, 2006), bilingual lexicon extraction (Morin
and Prochasson, 2011), and more generally on ma-
chine translation and related applications (Ion et al,
2011). Likewise, there is much to be gained from
the potential mutual benefits of comparable corpora
and ontology-related tasks.
Regarding multilingually aligned ontologies, very
few data sets have been made available for use in
the research community. Examples include the vlcr4
and the mldirectory5 datasets. The former con-
tains a reduced set of alignments between the the-
saurus of the Netherlands Institute for Sound and
Vision and two other resources, English WordNet
and DBpedia. The latter consists of a set of align-
ments between web site directories in English and
in Japanese. However, these data sets provide sub-
sets of bilingual alignments and are not fully pub-
licly available. The MultiFarm dataset6, a multilin-
gual version of the OntoFarm dataset (S?va?b et al,
2005), has been designed in order to overcome the
lack of multilingual aligned ontologies. MultiFarm
is composed of a set of seven ontologies that cover
the different aspects of the domain of organizing sci-
entific conferences. We have used this dataset as the
basis for generating our corpora.
3 Methodology
The main contribution of this paper is the proposal
of the methodology to build corpora. This sec-
tion describes the proposed methodology present-
ing our own corpus crawler, but also its application
to construct three corpora, in English, Portuguese,
and French. These corpora are constructed from the
MultiFarm dataset.
3.1 Tools and Resources
Instead of using an off-the-shelf web corpus tool
such as BootCaT (Baroni and Bernardini, 2004), we
implemented our own corpus crawler. This allowed
us to have more control on query and corpus con-
struction process. Even though our corpus construc-
4www.cs.vu.nl/?laurah/oaei/2009
5oaei.ontologymatching.org/2008/
mldirectory
6web.informatik.uni-mannheim.de/
multifarm
26
tion strategy is similar to the one implemented in
BootCaT, there are some significant practical issues
to take into account, such as:
? The predominance of multiword keywords;
? The use of the fixed keyword conference;
? The expert tuning of the cleaning process;
? The use of a long term support search AP[b].
Besides, BootCaT uses the Bing search API,
which will no longer work in 2012. As our work
is part of a long-term project, we preferred to use
Google?s search API as part of the University Re-
search Program.
The set of seed domain concepts comes from
the MultiFarm dataset. Seven ontologies from the
OntoFarm project (Table 1), together with the align-
ments between them, have been translated from En-
glish into eight languages (Chinese, Czech, Dutch,
French, German, Portuguese, Russian, and Span-
ish). As shown in Table 1, the ontologies differ
in numbers of classes, properties, and in their log-
ical expressivity. Overall, the ontologies have a high
variance with respect to structure and size and they
were based upon three types of resources:
? actual conferences and their web pages (type
?web?),
? actual software tools for conference organisa-
tion support (type ?tool?), and
? experience of people with personal participa-
tion in organisation of actual conferences (type
?insider?).
Currently, our comparable corpus generation ap-
proach focuses on a subset of languages, namely En-
glish (en), Portuguese (pt) and French (fr). The
labels of the ontology concepts, like conference and
call for papers, are used to generate queries and re-
trieve the pages in our corpus. In the current imple-
mentation, the structure and relational properties of
the ontologies were ignored. Concept labels were
our choice of seed keywords since we intended to
have comparable, heterogeneous and multilingual
domain resources. This means that we need a corpus
and an ontology referring to the same set of terms or
concepts. We want to ensure that the concept labels
Name Type C DP OP
Ekaw insider 74 0 33
Sofsem insider 60 18 46
Sigkdd web 49 11 17
Iasted web 140 3 38
ConfTool tool 38 23 13
Cmt tool 36 10 49
Edas tool 104 20 30
Table 1: Ontologies from the OntoFarm dataset in terms
of number of classes (C), datatype properties (DP) and
object properties (OP).
are present in the corresponding natural language,
textual sources. This combination of resources is es-
sential for our goals, which involve problems such as
ontology learning and enriching from corpus. Thus,
the original ontology can serve as a reference for
automatically extracted resources. Moreover, we
intend to use the corpus as an additional resource
for ontology (multilingual) matching, and again the
presence of the labels in the corpus is of great rele-
vance.
3.2 Crawling and Preprocessing
In each language, a concept label that occurs in
two or more ontologies provides a seed keyword
for query construction. This results in 49 en key-
words, 54 pt keywords and 43 fr keywords. Be-
cause many of our keywords are formed by more
than one word (average length of keywords is re-
spectively 1.42, 1.81 and 1.91 words), we combine
three keywords regardless of their sizes to form a
query. The first keyword is static, and corresponds
to the word conference in each language. The query
set is thus formed by permuting keywords two by
two and concatenating the static keyword to them
(e.g. conference reviewer program committee). This
results in 1 ? 48 ? 47 = 2, 256 en queries, 2,756
pt queries and 1,892 fr queries. Average query
length is 3.83 words for en, 4.62 words for pt and
4.91 words for fr. This methodology is in line with
the work of Sharoff (2006), who suggests to build
queries by combining 4 keywords and downloading
the top 10 URLs returned for each query.
The top 10 results returned by Google?s search
27
API7 are downloaded and cleaned. Duplicate URLs
are automatically removed. We did not filter out
URLs coming from social networks or Wikipedia
pages because they are not frequent in the corpus.
Results in formats other than html pages (like .doc
and .pdf documents) are ignored. The first clean-
ing step is the extraction of raw text from the html
pages. In some cases, the page must be discarded for
containing malformed html which our page cleaner
is not able to parse. In the future, we intend to im-
prove the robustness of the HTML parser.
3.3 Filtering and Linguistic Annotation
After being downloaded and converted to raw text,
each page undergoes a two-step processing. In the
first step, markup characters as interpunctuation,
quotation marks, etc. are removed leaving only let-
ters, numbers and punctuation. Further heuristics
are applied to remove very short sentences (less than
3 words), email addresses, URLs and dates, since
the main purpose of the corpus is related to concept,
instance and relations extraction. Finally, heuristics
to filter out page menus and footnotes are included,
leaving only the text of the body of the page. The
raw version of the text still contains those expres-
sions in case they are needed for other purposes.
In the second step, the text undergoes linguistic
annotation, where sentences are automatically lem-
matized, POS tagged and parsed. Three well-known
parsers were employed: Stanford parser (Klein and
Manning, 2003) for texts in English, PALAVRAS
(Bick, 2000) for texts in Portuguese, and Berkeley
parser (Petrov et al, 2006) for texts in French.
4 Evaluation
The characteristics of the resulting corpora are sum-
marized in tables 2 and 3. Column D of table 2
shows that the number of documents retrieved is
much higher in en than in pt and fr, and this is
not proportional to the number of queries (Q). In-
deed, if we look in table 3 at the average ratio of
documents retrieved per query (D/Q), the en queries
return much more documents than queries in other
languages. This indicates that the search engine re-
turns more distinct results in en and more duplicate
URLs in fr and in pt. The high discrepancy in
7research.google.com/university/search
Q D W token W type
en 2,256 10,127 15,852,650 459,501
pt 2,756 5,342 12,876,344 405,623
fr 1,892 5,154 9,482,156 362,548
Table 2: Raw corpus dimensions: number of queries (Q),
documents (D), and words (W).
D/Q S/D W/S TTR
en 4.49 110.59 14.15 2.90%
pt 1.94 120.08 20.07 3.15%
fr 2.72 115.63 15.91 3.82%
Table 3: Raw corpus statistics: average documents per
query (D/Q), sentences per document (S/D), words per
sentence (W/S) and type-token ration (TTR).
the number of documents has a direct impact in the
size of the corpus in each language. However, this
is counterbalanced by the average longer documents
(S/D) and longer sentences (W/S) in pt and fr with
respect to en. The raw corpus contains from 9.48
million words in fr, 12.88 million words in pt to
15.85 million words in en, constituting a large re-
source for research on ontology-related tasks.
A preliminary semi-automated analysis of the cor-
pus quality was made by extracting the top-100 most
frequent n-grams and unigrams for each language.
Using the parsed corpora, the extraction of the top-
100 most frequent n-grams for each language fo-
cused on the most frequent noun phrases composed
by at least two words. The lists with the top-100
most frequent unigrams was generated by extract-
ing the most frequent nouns contained in the parsed
corpus for each language. Four annotators manually
judged the semantic adherence of these lists to the
conference domain.
We are aware that semantic adherence is a vague
notion, and not a straightforward binary classifica-
tion problem. However, such a vague notion was
considered useful at this point of the research, which
is ongoing work, to give us an initial indication
of the quality of the resulting corpus. Examples
of what we consider adherent terms are appel a?
communication (call for papers), conference pro-
gram and texto completo (complete text), examples
28
# of adherent terms
Lower Upper
en words 46 85
en n-grams 57 94
fr words 21 69
fr n-grams 24 45
pt words 32 70
pt n-grams 11 45
Table 4: Number of words and n-grams judged as seman-
tically adherent to the domain.
of nonadherent terms extracted from the corpus were
produits chimiques (chemical products), following
case, projeto de lei (law project). In the three lan-
guages, the annotation of terms included misparsed
and mistagged words (ad hoc), places and dates typ-
ical of the genre (but not necessarily of the domain),
general-purpose terms frequent in conference web-
sites (email, website) and person names.
Table 4 shows the results of the annotation. The
lower bound considers an n-gram as semantically
adherent if all the judges agree on it. The upper
bound, on the other hand, considers as relevant n-
grams all those for which at least one of the four
judges rated it as relevant. As a result of our anal-
ysis, we found indications that the English corpus
was more adherent, followed by French and Por-
tuguese. This can be explained by the fact that
the amount of internet content is larger for English,
and that the number of international conferences
is higher than national conferences adopting Por-
tuguese and French as their official languages. We
considered the adherence of Portuguese and French
corpora rather low. There are indications that mate-
rial related to political meetings, law and public in-
stitutions was also retrieved on the basis of the seed
terms.
The next step in our evaluation is verifying its
comparable nature, by counting the proportion of
translatable words. Thus, we will use existing bilin-
gual dictionaries and measure the rank correlation of
equivalent words in each language pair.
5 Discussion
The first version of the corpus containing the to-
tality of the raw pages, the tools used to process
them, and a sample of 1,000 annotated texts for
each language are freely available for download at
the CAMELEON project website8. For the raw
files, each page is represented by an URL, a lan-
guage code, a title, a snippet and the text of the
page segmented into paragraphs, as in the original
HTML file. A companion log file contains informa-
tion about the download dates and queries used to re-
trieve each URL. The processed files contain the fil-
tered and parsed texts. The annotation format varies
for each language according to the parser used. The
final version of this resource will be available with
the totality of pages parsed.
Since the texts were extracted from web pages,
there is room for improvement concerning some im-
portant issues in effective corpus cleaning. Some of
these issues were dealt with as described in the ? 3,
but other issues are still open and are good candi-
dates for future refinements. Examples already fore-
seen are the removal of foreign words, special char-
acters, and usual web page expressions like ?site un-
der construction?, ?follow us on twitter?, and ?click
here to download?. However, the relevance of some
of these issues depends on the target application. For
some domains, foreign expressions may be genuine
part of the vocabulary (e.g. parking or weekend in
colloquial French and deadline in Portuguese), and
as such, should be kept, while for other domains
these expressions may need to be removed, since
they do not really belong to the domain. Therefore,
the decision of whether to implement these filters
or not, and to deal with truly multilingual texts, de-
pends on the target application.
Another aspect that was not taken into account in
this preliminary version is related to the use of the
relations between concepts in the ontologies to guide
the construction of the queries. Exploiting the con-
textual and semantic information expressed in these
relations may have an impact in the set of retrieved
documents and will be exploited in future versions
of the corpus.
6 Conclusions and Future Work
This paper has described an ontology-based ap-
proach for the generation of a multilingual compara-
8cameleon.imag.fr/xwiki/bin/view/Main/
Resources
29
ble corpus in English, Portuguese and French. The
corpus constructed and discussed here is an impor-
tant resource for ontology learning research, freely
available to the research community. The work on
term extraction that we are doing for the initial as-
sessment of the corpus is indeed the initial step to-
wards more ambitious research goals such as multi-
lingual ontology learning and matching in the con-
text of our long-term research project.
The initial ontologies (originally built by hand)
and resulting corpora can serve as a reference, a re-
search resource, for information extraction tasks re-
lated to ontology learning (term extraction, concept
formation, instantiation, etc). The resource also al-
lows the investigation of ontology enriching tech-
niques, due to dynamic and open-ended nature of
language, by which relevant terms found in the cor-
pus may not be part of the original ontology. We can
also assess the frequencies (relevance) of the labels
of the ontology element with respect to the corpus,
thus assessing the quality of the ontology itself. An-
other research that can be developed on the basis of
our resource is to evaluate the usefulness of a corpus
in the improvement of existing multilingual ontol-
ogy matching techniques9.
Regarding to our own crawler implementation,
we plan to work on its evaluation by using other
web crawlers, as BootCaT, and compare both ap-
proaches, specially on what concerns the use of on-
tologies.
From the point of view of NLP, several techniques
can be compared showing the impact of adopting
different tools in terms of depth of analysis, from
POS tagging to parsing. This is also an important re-
source for comparable corpora research, which can
be exploited for other tasks such as natural language
translation and ontology-based translation. So far
this corpus contains English, Portuguese and French
versions, but the ontology data set includes 8 lan-
guages, to which this corpus may be extended in the
future.
9An advantage of this resource is that the Multilingual Onto-
Farm is to be included in the OAEI (Ontology Alignment Eval-
uation Initiative) evaluation campaign.
References
Marco Baroni and Silvia Bernardini. 2004. BootcaT:
Bootstrapping corpora and terms from the web. In
Proc. of the Fourth LREC (LREC 2004), Lisbon, Por-
tugal, May. ELRA.
Marco Baroni and Motoko Ueyama. 2006. Building
general- and special-purpose corpora by web crawling.
In Proceedings of the 13th NIJL International Sympo-
sium on Language Corpora: Their Compilation and
Application, pages 31?40.
Eckhard Bick. 2000. The parsing system Palavras.
Aarhus University Press.
Richard Forsyth and Serge Sharoff. 2011. From crawled
collections to comparable corpora: an approach based
on automatic archetype identification. In Proc. of Cor-
pus Linguistics Conference, Birmingham, UK.
Gregory Grefenstette. 1999. The World Wide Web as a
resource for example-based machine translation tasks.
In Proc. of the Twenty-First Translating and the Com-
puter, London, UK, Nov. ASLIB.
Radu Ion, Alexandru Ceaus?u, and Elena Irimia. 2011.
An expectation maximization algorithm for textual
unit alignment. In Zweigenbaum et al (Zweigenbaum
et al, 2011), pages 128?135.
Frank Keller, Maria Lapata, and Olga Ourioupina. 2002.
Using the Web to overcome data sparseness. In Jan
Hajic? and Yuji Matsumoto, editors, Proc. of the 2002
EMNLP (EMNLP 2002), pages 230?237, Philadel-
phia, PA, USA, Jul. ACL.
Adam Kilgarriff. 2007. Googleology is bad science.
Comp. Ling., 33(1):147?151.
Su Nam Kim and Preslav Nakov. 2011. Large-scale noun
compound interpretation using bootstrapping and the
web as a corpus. In Proc. of the 2011 EMNLP
(EMNLP 2011), pages 648?658, Edinburgh, Scotland,
UK, Jul. ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proc. of the 41st ACL (ACL
2003), pages 423?430, Sapporo, Japan, Jul. ACL.
Emmanuel Morin and Emmanuel Prochasson. 2011.
Bilingual lexicon extraction from comparable corpora
enhanced with parallel corpora. In Zweigenbaum et al
(Zweigenbaum et al, 2011), pages 27?34.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of the 21st COLING
and 44th ACL (COLING/ACL 2006), pages 433?440,
Sidney, Australia, Jul. ACL.
Serge Sharoff, 2006. Creating general-purpose cor-
pora using automated search engine queries. Gedit,
Bologna, Italy.
30
Inguna Skadina, Ahmed Aker, Voula Giouli, Dan Tufis?,
Robert Gaizauskas, Madara Mieirina, and Nikos Mas-
tropavlos. 2010. A Collection of Comparable Cor-
pora for Under-resourced Languages. In Inguna Skad-
ina and Andrejs Vasiljevs, editors, Frontiers in Artifi-
cial Intelligence and Applications, volume 219, pages
161?168, Riga, Latvia, Oct. IOS Press.
Ondr?ej S?va?b, Vojte?ch Sva?tek, Petr Berka, Dus?an Rak,
and Petr Toma?s?ek. 2005. Ontofarm: Towards an ex-
perimental collection of parallel ontologies. In Poster
Track of ISWC 2005.
Pierre Zweigenbaum, Reinhard Rapp, and Serge Sharoff,
editors. 2011. Proc.of the 4th Workshop on Building
and Using Comparable Corpora: Comparable Cor-
pora and the Web (BUCC 2011), Portland, OR, USA,
Jun. ACL.
31
Proceedings of the 3rd Workshop on the People?s Web Meets NLP, ACL 2012, pages 10?14,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Serious Game for Building a Portuguese Lexical-Semantic Network
Mathieu Mangeot? Carlos Ramisch??
? GETALP ? LIG, University of Grenoble (France)
? Federal University of Rio Grande do Sul (Brazil)
FirstName.LastName@imag.fr
Abstract
This paper presents a game with a purpose
for the construction of a Portuguese lexical-
semantic network. The network creation is
implicit, as players collaboratively create links
between words while they have fun. We de-
scribe the principles and implementation of
the platform. As this is an ongoing project,
we discuss challenges and long-term goals.We
present the current network in terms a quanti-
tative and qualitative analysis, comparing it to
other resources. Finally, we describe our tar-
get applications.
1 Introduction
The creation of lexical resources like wordnets is
time consuming and very costly in terms of man-
power. Funding agencies and publishing houses are
very reluctant to launch new projects. Ironically, in
our globalized nowadays world, the need of such re-
sources for communication is growing. In this con-
text, there is hope for building resources via commu-
nities of voluntary contributors. But is it possible to
use the Wikipedia paradigm for building a rich and
broad-coverage lexical resource reusable by humans
and machines in NLP projects? Wordnets are very
interesting resources, but they suffer of several limi-
tations. First, even if the English wordnet (Miller et
al., 1990) is open source and freely available, this is
not the case of the EuroWordnets. Second, wordnets
and other manually built thesauri are based on lin-
guists? intuition. Information about up-to-date en-
tities (Facebook, Costa Concordia, etc.) and real-
world facts are missing. Third, relations between the
synsets of wordnets are of limited semantic kinds.
We would like to build other relations at the syntac-
tic and lexical level (e.g. collocations).
Our first goal is to build a rich lexical network
for the Portuguese language. The relations between
nodes (words) is represented in a sophisticated way,
by using lexical-semantical functions ? la Mel?c?uk
(Mel?c?uk, 1995) such as the Magn function repre-
senting the notion of intensifier: Magn(smoker) =
heavy, Magn(bachelor) = confirmed. The resulting
network represents the usage of the language, not the
norm. Thus, it may contain frequent spelling mis-
takes or neologisms. This resource is open-source
and freely available. It can be used in several appli-
cations: lexicography, printed dictionary, text gen-
eration, semantic information extraction, ontology
learning, etc. The construction of the resource is
done indirectly by contributors through a game.
In the next section, the concept of using serious
games for building NLP resources will be explained
(? 2). The following section will detail the construc-
tion of the Portuguese version of the game (? 3). Af-
terwards, we will discuss some preliminary results
(? 4) and finally we present future work (? 5).
2 Serious Games and NLP
The concept of human contribution, collaboration
and computation has been utilized in many applica-
tions and scenarios. The work of Luis von Ahn made
a breakthrough, especially in ESP game (von Ahn,
2006; von Ahn and Dabbish, 2008). Human com-
putation (crowdsourcing, volunteer contribution) is
now seriously considered to be able to solve large
computational problems (Speer, 2007). The idea
of collecting massive contributions from volunteers
through an online game took off recently. Nowa-
10
days, many serious games or GWAP ?Game With
A Purpose? (von Ahn and Dabbish, 2008) projects
exist in different domains, like Open Mind Com-
mon Sense (Singh et al, 2002), ESP games, Learner
(Chklovski and Gil, 2005), or CYC project 1. Con-
cerning more specifically lexical networks, similar
projects exist like ?small world of words?2 launched
in 2003 by KU Leuwen. For the moment, this
project is limited to building relations of only one
kind: associated ideas.
Looking at the Wikipedia project, the idea of
building lexical resources with the help voluntary
contributors comes to mind. Unfortunately, the
Wikipedia paradigm cannot be easily applied to
build a dictionary with rich lexical information. In
Wikipedia, articles do not need to follow the same
structure, while in a dictionary, the same structure
and linguistic theory must be applied to all the arti-
cles. Moreover, while it is easy to contribute to an
encyclopedia entry, not everyone has the linguistic
knowledge to contribute to a dictionary. On read-
ing Wiktionary entries, one realizes that the quality
cannot be compared to existing paper dictionaries.
When looking at people playing online games
through the Internet, one could think that it would
be interesting to use this time for playing a game that
would build lexical data in the background, specifi-
cally data that is difficult to find in existing dictionar-
ies. In this context, the idea of a serious lexical game
emerged. The first version was launched for French
in 2007 (Lafourcade and Joubert, 2008), which has
now around 250,000 nodes and 1,330,000 relations.
Our game aims at building a rich and evolving
lexical network comparable to the famous English
wordnet (Miller et al, 1990). The principle is as
follows: a player A initiates a game, an instruc-
tion is displayed concerning a type of competency
corresponding to a lexical relation (e.g. synonym,
antonym, domain, intensifier) and a word W is cho-
sen randomly in the database. Player A has then a
limited amount of time for giving propositions that
answer the instruction applied to the word W .
The same word W with the same instruction is
proposed to another player B and the process is the
same. The two half-games of player A and player
1http://game.cyc.com/
2http://www.smallworldofwords.com/
B are asynchronous. For each common answer in
A and B?s propositions, the two players earn a cer-
tain amount of points and credits. For the word W ,
the common answers of A and B players are entered
into the database. This process participates in the
construction of a lexical network linking terms with
typed and weighted relations, validated by pairs of
players. The relations are typed by the instructions
given to the players and weighted with the number
of pair players that proposed them. A more detailed
description of the game in French is provided by
Lafourcade and Zampa (2009).
3 Portuguese Version
The game interface was translated by a native Por-
tuguese speaker. A preliminary step was to interna-
tionalize the text messages by separating them from
the interface and storing them in an array, allowing
for easy translation in any other language. Simul-
taneously, we developed, and tested an easy step-
by-step installer which makes the deployment of the
game as easy as installing a content management
system software on a server.
A list of seed words must be provided from which
the game will chose the proposed terms at the be-
ginning. As the game evolves, people suggest new
words not necessarily in the initial dictionary, thus
helping the vocabulary to grow. Two resources
were used to compose this list of seed words. The
first is the DELAS?PB dictionary from NILC (Mu-
niz, 2004). All nouns, verbs, adjectives and ad-
verbs were extracted, resulting in 67,062 words. As
these include a large number of rare words, pilot
tests showed that the game became annoying when
the player ignored the meaning of most of the pro-
posed words. Therefore, the number of Google
hits for every word was obtained and only the 20%
most common ones were kept, resulting in a list of
13,413 words. To this, the entries of the Brazilian
Open Mind Common Sense network (Anacleto et
al., 2008) were added, in order to allow future com-
parison with this resource. Apertium?s lt-toolbox3
was used in order to obtain the most frequent POS
tag for each entry, resulting in 5,129 nouns, 3,672
verbs, 1,176 adjectives, and 201 adverbs. The union
with the preceding dictionary resulted in a final seed
3http://wiki.apertium.org/wiki/Lttoolbox
11
list of 20,854 words.
Once the game is deployed, one of the big chal-
lenges is to gather volunteer players. We gave pre-
sentations about the game in the academic context
and spread the word among Portuguese teachers, ar-
guing that the game could be used to enrich the vo-
cabulary of their students. We also created a Face-
book page and linked it in our website. One way to
motivate subscribed players to come back is to offer
gift words. Each player can offer a friend a game
with a specific word. For example, if I have a friend
fan of baseball, I will offer him/her this word.
Once the first challenge of gathering a commu-
nity of players is overcome, the main difficulty is
to keep the motivation going. For succeeding, the
project needs a person that will animate the com-
munity, motivate gamers and publicize the game for
recruiting new contributors. Games were launched
in other languages, but due to social factors (lack of
community animator), they are in a sleeping state.4
Internally, each word is represented as a node in
a graph. The directed edges are the lexico-syntactic
relations created by the game. Each edge has a type
(associated idea, hypernym, hyponym, typical ob-
ject, etc.) and a weight, corresponding to the num-
ber of times the two words co-occurred. Each node
has also a weight corresponding to its popularity
(proportional to its degree). Part of speech is en-
coded as edges going from a term to special POS
nodes. In addition to the standard attributes, each
edge also contains counters that represent the coun-
try of players who contributed to its creation. There-
fore, we would like to investigate dialectal varia-
tions of Portuguese in Portugal, Brasil and other lu-
sophone countries. This information can be impor-
tant for using the resource in semantic extraction,
according to the variation of the analyzed text.
4 Preliminary Evaluation
To date, 61 players participated in the game. In this
preliminary quantitative and qualitative evaluation,
we consider only the nodes for which some rela-
tion was created, thus excluding all the seed words
that were not connected to other words yet. Figure 1
shows a fragment of the network. Green edges rep-
resent associated words, red edges represent hypo-
4http://jeuxdemots.liglab.fr/
Figure 1: Overview of part of the network.
and hypernyms. Most relations created are standard,
like feij?, andr?, am?lia and jean are associated with
name. However, non-standard relations are also cre-
ated, like Cuba is the antonym of United States or
tatoo is associated to eternal. While purists may
consider these as noise, we regard it as relations rep-
resenting real-world semantics and language use.
The network contains 19,473 word nodes and
20,854 occurrences of POS relations (a word may
have several POS). Among those, 347 nodes do
not contain POS edges, meaning that they are new
words. A sample of the 20 most popular terms is
presented in Table 1. They include common hyper-
nym nodes like thing and person and animal but also
everyday language words like drink, car and sea.
From all the nodes in the network, only 1,408
(7.23%) have a degree greater than 1 (excluding
POS edges). For the remaining 18,065 nodes, no
relation was created. Figure 2 shows user activity
in number of games played per day. The number of
games is unsteady but it does not seem to increase
nor decrease. Analysis of the log files show that
players tend to participate a lot and the beginning
and then, after one or two weeks, they stop. Thus, it
is important not only to attract new players but also
to keep them active.
Word w Word w Word w
comida 110 hotel 80 pintura 74
*** 100 bebida 80 ?gua 72
pessoa 96 mulher 78 porta 72
dinheiro 92 casa 78 mar 72
carne 82 carro 78 empresa 72
nome 80 animal 76 coisa 72
Table 1: Top-20 most connected words and weights (w).
12
25/12 02/01 10/01 18/01 26/01 03/02 11/02 19/02 27/02 06/03 14/03 22/03 30/03 07/04Date
0
10
20
30
40
50
Numb
er of 
game
s
Figure 2: User activity from Dec 27, 2011 to Apr 4, 2012.
For the moment, the most common edge is of the
associated idea type. It corresponds to more than
80% of the edges. Some players bought compe-
tences in hypo- and hypernym, which together ac-
count for 15.39% of the edges. As these relations
are dual, it would be easy to infer new edges.The
total number of edges acquired is 1,344.
There is one large connected component in the
graph and a large number of small connected com-
ponents with two or three nodes. The total number
of connected components in the graph is 281 (ig-
noring disconnected nodes), yielding a high mod-
ularity of 0.898. The average degree of a node is
0.955, as more than 750 nodes have only one rela-
tion and around 200 have 2 relations, and the degree
decreases exponentially.
The trend is that, as more relations are added,
the current small components will be attached to
the larger ones, but also more smaller unconnected
graphs will be created. However, we expect that
once a large proportion of the nodes has been cov-
ered, the network will converge to a single large con-
nected component.
For the moment, the coverage of our resource is
limited. But a previous comparison done for the
French game versus the French Euro WordNet (F-
EWN) was very promising. The french game con-
tains 10 times more terms than F-EWN (240,000 vs
Relation type Count %
Associated 1,126 83.78
Hypernym 115 8.56
Hyponym 81 6.83
Domain 12 0.89
Antonym 10 0.74
Total 1,344 100
Table 2: Number of edges according to types.
23,000) and relations (1,359,000 vs 100,000). On a
sample of 100 terms frequently played in the game,
3% of them contain errors (spelling mistakes or con-
fusions). Data collected with the French game bring
a lot of originality but the precision rate is much
lower that data collected manually in F-EWN.
Our resource now only contains as much as 0.91%
of the nodes in English Wordnet. As precise num-
bers about the size of the Portuguese wordnet (Mar-
rafa et al, 2011) are not available, we also queried
the online service for the nodes in our network. We
found out that 35.87% of the nodes are covered by
the Portuguese wordnet. Thus, we believe that col-
laborative methods can considerably speed up the
creation of lexical resources, as in only three months
we already have some information complimentary to
a 13-years old project.
5 Future work
We presented the deployment and a preliminary
evaluation of JeuxDeMots?pt,5 a game that aims
at the construction of a Portuguese lexical-semantic
network. The coverage of the resource is still lim-
ited, but the network keeps growing.
For the moment, we have made available a simple
interface in which the user can query for a word and
retrieve all the words related to it. For instance, if
one searches for the word loja (store), the result is:
? store is a place
? cell phone store is a store
? store is associated to buy shirt
? store is associated to sell toys
? store is associated to manager
? store is associated to clothes
The creation of the network is much less onerous
and faster (and more entertaining) than traditional
thesauri construction, that can take years of the work
of many experts. Our long-term goal is the creation
of a large network comparable to existing resources
for English. This resource would be extremely use-
ful in many NLP tasks. Once we will have enough
data, our goal is to apply it to many other applica-
tions like information extraction, WSD, semantic in-
ference and textual entailment. This would help to
bridge the gap of missing lexical resources for NLP
applications dealing with Portuguese language.
5http://jeuxdemots.liglab.fr/por
13
Acknowledgements
This work was partly funded by the CAMELEON
project (CAPES?COFECUB 707-11).
References
Junia Coutinho Anacleto, Aparecido Fabiano P. de Car-
valho, Alexandre M. Ferreira, Eliane N. Pereira, and
Alessandro J. F. Carlos. 2008. Common sense based
applications to advance personalized learning. In
PROC of the IEEE International Conference on Sys-
tems, Man and Cybernetics (SMC 2008), pages 3244?
3249, Singapore.
Timothy Chklovski and Yolanda Gil. 2005. An anal-
ysis of knowledge collected from volunteer contribu-
tors. In Twentieth National Conference on Artificial
Intelligence (AAAI-05), Pittsburgh, Pennsylvania.
Mathieu Lafourcade and Alain Joubert. 2008. Jeuxde-
mots : un prototype ludique pour l??mergence de re-
lations entre termes. In JADT 2008 : 9es Journ?es
internationales d?Analyse statistique des Donn?es
Textuelles, pages 657?666, Lyon, France, 12-14 mars.
Mathieu Lafourcade and Virginie Zampa. 2009. Jeuxde-
mots and pticlic: games for vocabulary assessment and
lexical acquisition. In Computer Games, Multimedia
& Allied technology 09 (CGAT?09), Singapore, 11th-
13th May.
Palmira Marrafa, Raquel Amaro, and Sara Mendes.
2011. Wordnet.pt global ? extending wordnet.pt to
portuguese varieties. In Proc. of the First Workshop
on Algorithms and Resources for Modelling of Di-
alects and Language Varieties, pages 70?74, Edin-
burgh, Scotland, July. ACL.
Igor Mel?c?uk. 1995. Lexical functions: A tool for
the description of lexical relations in the lexicon. In
Leo Wanner, editor, Lexical Functions in Lexicogra-
phy and Natural Language Processing, pages 37?102.
John Benjamins, Amsterdam/Philadelphia.
George A. Miller, Richard Beckwith, Christiane Fell-
baum, Derek Gross, and Katherine Miller. 1990. In-
troduction to wordnet: an on-line lexical database. In-
ternational Journal of Lexicography, 3(4):235?244.
Marcelo C. M. Muniz. 2004. A constru??o de recursos
linguistico-computacionais para o portugues do brasil:
o projeto de unitex-pb. Master?s thesis, Instituto de
Ciencias Matematicas de Sao Carlos, USP, S?o Carlos,
SP, Brazil.
Push Singh, Thomas Lin, et al 2002. Open mind
common sense: Knowledge acquisition from the gen-
eral public. In Proceedings of the First International
Conference on Ontologies, Databases, and Applica-
tions of Semantics for Large Scale Information Sys-
tems, Irvine, CA, USA.
Robert Speer. 2007. Open mind commons: An inquis-
itive approach to learning common sense. In Work-
shop on Common Sense and Intelligent User Inter-
faces, Honolulu, Hawaii, USA., January 28-31.
Luis von Ahn and Laura Dabbish. 2008. General tech-
niques for designing games with a purpose. Commu-
nications of the ACM, pages 58?67.
Luis von Ahn. 2006. Games with a purpose. IEEE Com-
puter Magazine, pages 96?98.
14
Proceedings of the 9th Workshop on Multiword Expressions (MWE 2013), pages 93?100,
Atlanta, Georgia, 13-14 June 2013. c?2013 Association for Computational Linguistics
Identifying Pronominal Verbs: Towards Automatic
Disambiguation of the Clitic se in Portuguese
Magali Sanches Duran?, Carolina Evaristo Scarton?,
Sandra Maria Alu??sio?, Carlos Ramisch?
? University of Sa?o Paulo (Brazil)
? Joseph Fourier University (France)
magali.duran@uol.com.br, carol.scarton@gmail.com
sandra@icmc.usp.br, carlosramisch@gmail.com
Abstract
A challenging topic in Portuguese language
processing is the multifunctional and ambigu-
ous use of the clitic pronoun se, which impacts
NLP tasks such as syntactic parsing, semantic
role labeling and machine translation. Aiming
to give a step forward towards the automatic
disambiguation of se, our study focuses on the
identification of pronominal verbs, which cor-
respond to one of the six uses of se as a clitic
pronoun, when se is considered a CONSTITU-
TIVE PARTICLE of the verb lemma to which
it is bound, as a multiword unit. Our strategy
to identify such verbs is to analyze the results
of a corpus search and to rule out all the other
possible uses of se. This process evidenced
the features needed in a computational lexicon
to automatically perform the disambiguation
task. The availability of the resulting lexicon
of pronominal verbs on the web enables their
inclusion in broader lexical resources, such as
the Portuguese versions of Wordnet, Propbank
and VerbNet. Moreover, it will allow the revi-
sion of parsers and dictionaries already in use.
1 Introduction
In Portuguese, the word se is multifunctional. POS
taggers have succeeded in distinguishing between se
as a conjunction (meaning if or whether) and se as
a pronoun (see Martins et al (1999) for more details
on the complexity of such task). As a clitic1 pro-
1A clitic is a bound form, phonologically unstressed, at-
tached to a word from an open class (noun, verb, adjective, ad-
verbial). It belongs to closed classes, that is, classes that have
grammatical rather than lexical meaning (pronouns, auxiliary
verbs, determiners, conjunctions, prepositions, numerals).
noun, however, se has six uses:
1. marker of SUBJECT INDETERMINATION:
Ja? se falou muito nesse assunto.
*Has-SE already spoken a lot about this matter.
One has already spoken a lot about this matter.
2. marker of pronominal PASSIVE voice (syn-
thetic passive voice):
Sugeriram-se muitas alternativas.
*Have-SE suggested many alternatives.
Many alternatives have been suggested.
3. REFLEXIVE pronoun (-self pronouns):
Voce? deveria se olhar no espelho.
*You should look-SE on the mirror.
You should look at yourself on the mirror.
4. RECIPROCAL pronoun (each other):
Eles se cumprimentaram com um aperto de ma?o.
*They greeted-SE with a handshake.
They greeted each other with a handshake.
5. marker of causative-INCHOATIVE alternation2:
Esse esporte popularizou-se no Brasil.
*This sport popularED-SE in Brazil.
This sport became popular in Brazil.
6. CONSTITUTIVE PARTICLE of the verb lexical
item (pronominal verb):
Eles se queixaram de dor no joelho.
*They complained-SE about knee pain.
They complained about knee pain.
2Causative-inchoative alternation: a same verb can be used
two different ways, one transitive, in which the subject position
is occupied by the argument which causes the action or process
described by the verb (causative use), and one intransitive, in
which the subject position is occupied by the argument affected
by the action or process (inchoative use).
93
Clitic se uses Syntactic
function
Semantic
function
SUBJECT INDE-
TERMINATION
NO YES3
PASSIVE
YES YES3
REFLEXIVE
YES YES
RECIPROCAL
YES YES
INCHOATIVE
YES NO
CONSTITUTIVE
PARTICLE
NO NO
Table 1: Uses of the clitic se from the point of view of
syntax and semantics.
The identification of these uses is very important
for Portuguese language processing, notably for syn-
tactic parsing, semantic role labeling (SRL) and ma-
chine translation. Table 1 shows which of these six
uses support syntactic and/or semantic functions.
Since superficial syntactic features seem not suffi-
cient to disambiguate the uses of the pronoun se, we
propose the use of a computational lexicon to con-
tribute to this task. To give a step forward to solve
this problem, we decided to survey the verbs un-
dergoing se as an integral part of their lexical form
(item 6), called herein pronominal verbs, but also
known as inherent reflexive verbs (Rosa?rio Ribeiro,
2011). Grammars usually mention this kind of verbs
and give two classical examples: queixar-se (to com-
plain) and arrepender-se (to repent). For the best of
our knowledge, a comprehensive list of these multi-
word verbs is not available in electronic format for
NLP uses, and not even in a paper-based format,
such as a printed dictionary.
An example of the relevance of pronominal verbs
is that, in spite of not being argumental, that is, not
being eligible for a semantic role label, the use of se
as a CONSTITUTIVE PARTICLE should integrate the
verb that evokes the argumental structure, as may be
seen in Figure 1.
The identification of pronominal verbs is not a
trivial task because a pronominal verb has a nega-
3In these cases, the clitic may support the semantic role label
of the suppressed external argument (agent).
Figure 1: Sentence The broadcasters refused to apologize
includes pronominal verbs negar-se (refuse) and retratar-
se (apologize) that evoke frames in SRL.
tive definition: if se does not match the restrictions
imposed by the other five uses, so it is a CONSTI-
TUTIVE PARTICLE of the verb, that is, it composes a
multiword. Therefore, the identification of pronom-
inal verbs requires linguistic knowledge to distin-
guish se as a CONSTITUTIVE PARTICLE from the
other uses of the the pronoun se (SUBJECT INDE-
TERMINATION, PASSIVE, REFLEXIVE, RECIPRO-
CAL and INCHOATIVE.)
There are several theoretical linguistic studies
about the clitic pronoun se in Portuguese. Some of
these studies present an overview of the se pronoun
uses, but none of them prioritized the identification
of pronominal verbs. The study we report in this pa-
per is intended to fill this gap.
2 Related Work
From a linguistic perspective, the clitic pronoun
se has been the subject of studies focusing on:
SUBJECT INDETERMINATION and PASSIVE uses
(Morais Nunes, 1990; Cyrino, 2007; Pereira-Santos,
2010); REFLEXIVE use (Godoy, 2012), and IN-
CHOATIVE use (Fonseca, 2010; Nunes-Ribeiro,
2010; Rosa?rio Ribeiro, 2011). Despite none of these
works concerning specifically pronominal verbs,
they provided us an important theoretical basis for
the analysis undertaken herein.
The problem of the multifunctional use of clitic
pronouns is not restricted to Portuguese. Romance
languages, Hebrew, Russian, Bulgarian and oth-
ers also have similar constructions. There are
94
crosslinguistic studies regarding this matter reported
in Siloni (2001) and Slavcheva (2006), showing
that there are partial coincidence of verbs taking
clitic pronouns to produce alternations and reflexive
voice.
From an NLP perspective, the problem of the
ambiguity of the clitic pronoun se was studied by
Martins et al (1999) to solve a problem of catego-
rization, that is, to decide which part-of-speech tag
should be assigned to se. However, we have not
found studies regarding pronominal verbs aiming at
Portuguese automatic language processing.
Even though in Portuguese all the uses of the clitic
pronoun se share the same realization at the surface
form level, the use as a CONSTITUTIVE PARTICLE of
pronominal verbs is the only one in which the verb
and the clitic form a multiword lexical unit on its
own. In the other uses, the clitic keeps a separate
syntactic and/or semantic function, as presented in
Table 1.
The particle se is an integral part of pronominal
verbs in the same way as the particles of English
phrasal verbs. As future work, we would like to in-
vestigate possible semantic contributions of the se
particle to the meaning of pronominal verbs, as done
by Cook and Stevenson (2006), for example, who try
to automatically classify the uses of the particle up in
verb-particle constructions. Like in the present pa-
per, they estimate a set of linguistic features which
are in turn used to train a Support Vector Machine
(SVM) classifier citecook:2006:mwe.
3 Methodology
For the automatic identification of multiword
verb+se occurrences, we performed corpus searches
on the PLN-BR-FULL corpus (Muniz et al, 2007),
which consists of news texts extracted from a ma-
jor Brazilian newspaper, Folha de Sa?o Paulo, from
1994 to 2005, with 29,014,089 tokens. The cor-
pus was first preprocessed for sentence splitting,
case homogenization, lemmatization, morphologi-
cal analysis and POS tagging using the PALAVRAS
parser (Bick, 2000). Then, we executed the corpus
searches using the mwetoolkit (Ramisch et al,
2010). The tool allowed us to define two multilevel
word patterns, for proclitic and enclitic cases, based
on surface forms, morphology and POS. The pat-
terns covered all the verbs in third person singular
(POS=V*, morphology=3S) followed/preceded by
the clitic pronoun se (surface form=se, POS=PERS).
The patterns returned a set of se occurrences, that
is, for each verb, a set of sentences in the corpus in
which this verb is followed/preceded by the clitic se.
In our analysis, we looked at all the verbs tak-
ing an enclitic se, that is, where the clitic se is at-
tached after the verb. We could as well have in-
cluded the occurrences of verbs with a proclitic se
(clitic attached before the verb). However, we sus-
pected that this would increase the number of occur-
rences (sentences) to analyze without a proportional
increase in verb lemmas. Indeed, our search for pro-
clitic se occurrences returned 40% more verb lem-
mas and 264% more sentences than for the enclitic
se (59,874 sentences), thus confirming our hypothe-
sis. Moreover, as we could see at a first glance, pro-
clitic se results included se conjunctions erroneously
tagged as pronouns (when the parser fails the cate-
gorial disambiguation). This error does not occur
when the pronoun is enclitic because Portuguese or-
thographic rules require a hyphen between the verb
and the clitic when se is enclitic, but never when it
is proclitic.
We decided to look at sentences as opposed to
looking only at candidate verb lemmas, because we
did not trust that our intuition as native speakers
would be sufficient to identify all the uses of the
clitic se for a given verb, specially as some verbs
allow more than one of the six uses we listed herein.
For performing the annotation, we used a table
with the verb lemmas in the lines and a column for
each one of the six uses of se as a clitic pronoun.
Working with two screens (one for the table and the
other for the sentences), we read the sentences and,
once a new use was verified, we ticked the appro-
priate column. This annotation setup accelerated the
analyses, as we only stopped the reading when we
identified a new use. The annotation was performed
manually by a linguist, expert in semantics of Por-
tuguese verbs, and also an author of this paper.
After having summarized the results obtained
from corpus analysis, we realized that some cliti-
cized verb uses that we know as native speakers did
not appear in the corpus (mainly reflexive and recip-
rocal uses). In these cases, we added a comment on
our table which indicates the need to look for the use
95
in another corpus aiming to confirm it.
For example, the most frequent cliticized verb,
tratar-se has no occurrence with the meaning of to
take medical treatment. We checked this meaning in
another corpus and found one example: O senador
se tratou com tecido embriona?rio. . . (*The senator
treated himself with embryonic tissue. . . ), proving
that our intuition may help us to improve the results
with specific corpus searches. A comparative multi-
corpus extension of the present study is planned as
future work.
The strategy we adopted to analyze the sentences
in order to identify pronominal verbs was to make a
series of questions to rule out the other possible se
uses.
Question 1 Does the se particle function as a
marker of PASSIVE voice or SUBJECT INDETERMI-
NATION?
In order to answer this question, it is important to
know that both uses involve the suppression of the
external argument of the verb. The difference is that,
in the pronominal PASSIVE voice, the remaining NP
(noun phrase) is shifted to the subject position (and
the verb must then be inflected according to such
subject), whereas in SUBJECT INDETERMINATION,
the remaining argument, always a PP (prepositional
phrase), remains as an indirect object. For example:
? Pronominal PASSIVE voice:
Fizeram-se va?rias tentativas.
*Made-SE several trials.
Several trials were made.
? SUBJECT INDETERMINATION:
Reclamou-se de falta de hygiene.
*Complained-SE about the lack of hygiene.
One has complained about the lack of hygiene.
Question 2 Is it possible to substitute se for a si
mesmo (-self )?
If so, it is a case of REFLEXIVE use. A clue for
this is that it is always possible to substitute se for
another personal pronoun, creating a non-reflexive
use keeping the same subject. For example:
? Ele perguntou-se se aquilo era certo.
He asked himself whether that was correct.
? Ele perguntou-me se aquilo era certo.
He asked me whether that was correct.
Question 3 Is it possible to substitute se for um ao
outro (each other)?
If so, it is a case of RECIPROCAL use. A clue for
this interpretation is that, in this case, the verb is al-
ways in plural form as the subject refers to more than
one person. RECIPROCAL uses were not included in
the corpus searches, as we only looked for cliticized
verbs in third person singular. However, aiming to
gather data for future work, we have ticked the table
every time we annotated sentences of a verb that ad-
mits reciprocal use. The reciprocal use of such verbs
have been later verified in other corpora.
? Eles se beijaram.
They kissed each other.
Question 4 Has the verb, without se, a transi-
tive use? If so, are the senses related to causative-
inchoative alternation? In other words, is the mean-
ing of the transitive use to cause X become Y?
If so, it is a case of INCHOATIVE use, for example:
? A porta abriu-se.
The door opened.
Compare with the basic transitive use:
? Ele abriu a porta.
He opened the door.
It is important to mention that verbs which allow
causative-inchoative alternation in Portuguese may
not have an equivalent in English that allows this al-
ternation, and vice-versa. For example, the inchoa-
tive use of the verb tornar corresponds to the verb
to become and the causative use corresponds to the
verb to make:
? Esse fato tornou-se conhecido em todo o
mundo.
This fact became known all around the world.
? A imprensa tornou o fato conhecido em todo o
mundo.
The press made the fact known all around the world.
If the verb being analyzed failed the four tests, the
clitic se has neither semantic nor syntactic function
and is considered a CONSTITUTIVE PARTICLE of the
verb, for example:
96
? Ele vangloriou-se de seus talentos.
He boasted of his talents.
Therefore, we made the identification of pronom-
inal verbs based on the negation of the other possi-
bilities.
4 Discussion
The corpus search resulted in 22,618 sentences of
cliticized verbs, corresponding to 1,333 verb lem-
mas. Some verbs allow only one of the uses of
the clitic se (unambiguous cliticized verbs), whereas
others allow more than one use (ambiguous cliti-
cized verbs), as shown in Table 2. Therefore, a
lexicon can only disambiguate part of the cliticized
verbs (others need additional features to be disam-
biguated).
The analysis of the verbs? distribution reveals that
10% of them (133) account for 73% of the sentences.
Moreover, among the remaining 90% verb lemmas,
there are 477 hapax legomena, that is, verbs that oc-
cur only once. Such distribution indicates that com-
putational models which focus on very frequently
cliticized verbs might significantly improve NLP ap-
plications.
Contrary to our expectations, very frequently
cliticized verbs did not necessarily present high pol-
ysemy. For example, the most frequent verb of our
corpus is tratar, with 2,130 occurrences. Although
tratar-se has more than one possible use, only one
appeared in the corpus, as a marker of SUBJECT IN-
DETERMINATION, for example:
? Trata-se de uma nova tende?ncia.
It is the case of a new tendency.
Despite being very frequent, when we search for
translations of tratar-se de in bilingual (parallel)
Portuguese-English corpora and dictionaries avail-
able on the web,4,5,6 we observed that there are sev-
eral solutions to convey this idea in English (deter-
mining a subject, as English does not allow subject
omission). Six examples extracted from the Com-
para corpus illustrate this fact:
4http://www.linguateca.pt/COMPARA/
5http://www.linguee.com.br/
portugues-ingles
6http://pt.bab.la/dicionario/
portugues-ingles
se uses Unamb. Amb. Total
SUBJECT INDE-
TERMINATION
17 6 23
PASSIVE
467 630 1097
REFLEXIVE
25 333 358
INCHOATIVE
190 64 254
RECIPROCAL
0 33 33
CONSTITUTIVE
PARTICLE
83 104 187
Total 782 1170 1952
Table 2: Proportion of unambiguous (Unamb.) and am-
biguous (Amb.) verbs that allow each se use.
? Trata-se de recriar o pro?prio passado.
It?s a question of re-creating your own past.
? Mas o assunto era curioso, trata-se do casa-
mento, e a viu?va interessa-me.
But the subject was a curious one; it was about her
marriage, and the widow interests me.
? Na?o ha? mais du?vidas, trata-se realmente de um
louco.
There?s no longer any doubt; we?re truly dealing
with a maniac.
? Trata-se realmente de uma emerge?ncia, Sr.
Hoffman.
This really is a matter of some urgency, Mr Hoff-
man.
? Trata-se de um regime repousante e civilizado.
It is a restful, civilized re?gime.
? Trata-se de um simples caso de confusa?o de
identidades, dizem voce?s.
(??) Simple case of mistaken identity.
In what concerns specifically pronominal verbs,
our analysis of the data showed they are of three
kinds:
1. Verbs that are used exclusively in pronominal
form, as abster-se (to abstain). This does not
mean that the pronominal form is unambigu-
ous, as we found some pronominal verbs that
present more than one sense, as for example the
verb referir-se, which means to refer or to con-
cern, depending on the subject?s animacy status
[+ human] or [? human], respectively;
97
2. Verbs that have a non-pronominal and a pro-
nominal form, but both forms are not related,
e.g.: realizar (to make or to carry on, which
allows the passive alternation realizar-se); and
the pronominal form realizar-se (to feel ful-
filled);
3. Verbs that have pronominal form, but accept
clitic drop in some varieties of Portuguese
without change of meaning, as esquecer-se and
esquecer (both mean to forget)
We did not study the clitic drop (3), but we un-
covered several pronominal verbs of the second kind
above (2). The ambiguity among the uses of se in-
creases with such cases. The verb desculpar (to
forgive), for example, allows the REFLEXIVE use
desculpar-se (to forgive oneself ), but also consti-
tutes a pronominal verb: desculpar-se (to apolo-
gize). The verb encontrar (to find) allows the RE-
FLEXIVE use (to find oneself, from a psychological
point of view) and the PASSIVE use (to be found).
The same verb also constitutes a pronominal verb
which means to meet (1) or functions as a copula
verb, as to be (2):
1. Ele encontrou-se com o irma?o.
He met his brother.
2. Ele encontra-se doente.
He is ill.
In most sentences of cliticized verbs? occurrences,
it is easy to observe that, as a rule of thumb:7
? SUBJECT INDETERMINATION uses of se do not
present an NP before the verb, present a PP af-
ter the verb and the verb is always inflected in
the third person singular;
? PASSIVE uses of se present an NP after the verb
and no NP before the verb;
? INCHOATIVE uses of se present an NP before
the verb and almost always neither a PP nor a
NP after the verb;
? CONSTITUTIVE PARTICLE uses of se present
an NP before the verb and a PP after the verb;
7Syntactic clues do not help to identify REFLEXIVE verbs.
The distinction depends on the semantic level, as the reflexive
use requires a [+ animate] subject to play simultaneously the
roles of agent and patient.
? RECIPROCAL uses of se only occur with verbs
taking a plural inflection.
Problems arise when a sentence follows none of
these rules. For example, subjects in PASSIVE use
of se usually come on the right of the verb. Thus,
when the subject appears before the verb, it looks, at
a first glance, to be an active sentence. For example:
? O IDH baseia-se em dados sobre renda, esco-
laridade e expectativa de vida.
*The HDI bases-SE on income, education and life
expectancy data.
The HDI is based on income, education and life ex-
pectancy data.
These cases usually occur with stative passives
(see Rosa?rio Ribeiro (2011, p. 196)) or with ditran-
sitive action verbs8 when a [? animate] NP takes
the place usually occupied by a [+ animate] NP. Se-
mantic features, again, help to disambiguate and to
reveal a non-canonical passive.
The opposite also occurs, that is, the subject, usu-
ally placed on the left of the verb in active voice,
appears on the right, giving to the sentence a false
passive appearance:
? Desesperaram-se todos os passageiros.
*Fell-SE into despair all the passengers.
All the passengers fell into despair.
Sometimes the meaning distinctions of a verb are
very subtle, making the matter more complex. In
the following sections, we comment two examples
of difficult disambiguation.
4.1 Distinguishing Pronominal PASSIVE Voice
from Pronominal Verbs
The verb seguir (to follow) conveys the idea of obey-
ing when it has a [+ human] subject in the active
voice (an agent). The passive voice may be con-
structed using se, like in (2). Additionally, this verb
has a pronominal active use, seguir-se, which means
to occur after, as shown in (3):
1. Active voice:
? [Eles]Agent seguem [uma se?rie de conven-
c?o?es]Theme - thing followed.
They follow a series of conventions.
8Ditransitive verbs take two internal arguments: an NP as
direct object and a PP as indirect object.
98
2. PASSIVE voice:
? Segue-se [uma se?rie de conven-
c?o?es]Theme - thing followed.
A series of conventions are followed.
3. Pronominal verb ? active voice:
? [A queda]Theme - thing occurring after seguiu-
se [a` divulgac?a?o dos dados de desemprego
em o pa??s]Theme - thing occurring before.
The drop followed the announcement of unem-
ployment figures in the country.
The preposition a introducing one of the argu-
ments in (3) distinguishes the two meanings, as the
PASSIVE voice presents an NP and not a PP imme-
diately after or before the verb.
4.2 Distinguishing REFLEXIVE, INCHOATIVE
and PASSIVE Uses
The verb transformar, when cliticized, may be in-
terpreted as a PASSIVE (to be transformed), as a RE-
FLEXIVE (to transform oneself ) or as an INCHOA-
TIVE use (to become transformed). The PASSIVE
voice is identified by the subject position, after the
verb (1). The difference between the REFLEXIVE (2)
and INCHOATIVE (3) uses, on its turn, is a semantic
feature: only a [+ human] subject may act to be-
come something (REFLEXIVE use):
1. PASSIVE:
Transformou-se o encontro em uma
grande festa.
The meeting was transformed into a big party.
2. REFLEXIVE:
? A mulher jovem transformou-se em uma
pessoa sofisticada.
The young woman transformed herself into a
sophisticated person.
3. INCHOATIVE:
? O encontro transformou-se em uma gran-
de festa.
The meeting transformed into a big party.
5 Conclusions and Future Work
The lexicon gathered through this research will par-
tially enable disambiguating the uses of the clitic
pronoun se, as there are several verbs that allow only
one of the se clitic uses. For the other verbs, whose
polysemy entails more than one possible use of se, it
is necessary to add further information on each verb
sense.
The analysis we reported here evidenced the need
for enriching Portuguese computational lexicons,
encompassing (a) the semantic role labels assigned
by each verb sense, (b) the selectional restrictions
a verb imposes to its arguments, and (c) the alter-
nations a verb (dis)allows. The semantic predicate
decomposition used by Levin (1993) has proved to
be worthy to formalize the use of se in reflexive con-
structions (Godoy, 2012) and we think it should be
adopted to describe other uses of the pronoun se.
Another alternative is to construct a detailed com-
putational verb lexicon along the lines suggested
by Gardent et al (2005), based on Maurice Gross?
lexicon-grammar.
The data generated by this study can also be used
to automatically learn classifiers for ambiguous uses
of the clitic se. On the one hand, the annotation
of uses can be semi-automatically projected on the
sentences extracted from the corpus. On the other
hand, the findings of this work in terms of syntac-
tic and semantic characteristics can be used to pro-
pose features for the classifier, trying to reproduce
those that can be automatically obtained (e.g., sub-
categorization frame) and to simulate those that can-
not be easily automated (e.g., whether the subject
is animate). For these future experiments, we in-
tend to compare different learning models, based on
SVM and on sequence models like conditional ran-
dom fields (Vincze, 2012).
As languages are different in what concerns al-
lowed alternations, the use of clitic se in Portuguese
becomes even more complex when approached from
a bilingual point of view. Depending on how differ-
ent the languages compared are, the classification of
se adopted here may be of little use. For example,
several verbs classified as reflexive in Portuguese,
like vestir-se (to dress), barbear-se (to shave) and
demitir-se (to resign) are not translated into a re-
flexive form in English (*to dress oneself, *to shave
oneself and *to dismiss oneself ). Similarly, typical
inchoative verb uses in Portuguese need to be trans-
lated into a periphrasis in English, like surpreender-
se (to be surprised at), orgulhar-se (to be proud of )
and irritar-se (to get angry). Such evidences lead
99
us to conclude that it would be useful to count on
a bilingual description not only of pronominal, but
also of the other se uses.
The results of this work are available at www.
nilc.icmc.usp.br/portlex.
Acknowledgments
This study was funded by FAPESP (process
2011/22337-1) and by the CAMELEON project
(CAPES-COFECUB 707-11).
References
Eckhard Bick. 2000. The parsing system Palavras.
Aarhus University Press. 411 p.
Paul Cook and Suzanne Stevenson. 2006. Classifying
particle semantics in English verb-particle construc-
tions. In Proceedings of MWE 2006, pages 45?53,
Sydney, Australia.
Sonia Maria Lazzarino Cyrino. 2007. Construc?o?es com
SE e promoc?a?o de argumento no portugue?s brasileiro:
Uma investigac?a?o diacro?nica. Revista da ABRALIN,
6:85?116.
Paula Fonseca. 2010. Os verbos pseudo-reflexos em
Portugue?s Europeu. Master?s thesis, Universidade do
Porto.
Claire Gardent, Bruno Guillaume, Guy Perrier, and In-
grid Falk. 2005. Maurice gross? grammar lexicon and
natural language processing. In Proceedings of the
2nd Language and Technology Conference, Poznan?,
Poland.
Luisa Andrade Gomes Godoy. 2012. A reflexivizac?a?o no
PB e a decomposic?a?o sema?ntica de predicados. Ph.D.
thesis, Universidade Federal de Minas Gerais.
Beth Levin. 1993. English Verb Classes and Alterna-
tions: a preliminary investigation. The University of
Chicago Press, Chicago, USA.
Ronaldo Teixeira Martins, Gisele Montilha, Lucia He-
lena Machado Rino, and Maria da Grac?a Volpe Nunes.
1999. Dos modelos de resoluc?a?o da ambiguidade cat-
egorial: o problema do SE. In Proceedings of IV
Encontro para o Processamento Computacional da
L??ngua Portuguesa Escrita e Falada (PROPOR 1999),
pages 115?128, E?vora, Portugal, September.
Jairo Morais Nunes. 1990. O famigerado SE: uma
ana?lise sincro?nica e diacro?nica das construc?o?es com
SE apassivador e indeterminador. Master?s thesis,
Universidade Estadual de Campinas.
Marcelo Muniz, Fernando V. Paulovich, Rosane
Minghim, Kleber Infante, Fernando Muniz, Renata
Vieira, and Sandra Alu??sio. 2007. Taming the tiger
topic: an XCES compliant corpus portal to generate
subcorpus based on automatic text topic identification.
In Proceedings of The Corpus Linguistics Conference
(CL 2007), Birmingham, UK.
Pablo Nunes-Ribeiro. 2010. A alterna?ncia causativa
no Portugue?s do Brasil: a distribuic?a?o do cl??tico SE.
Master?s thesis, Universidade Federal do Rio Grande
do Sul.
Jose? Ricardo Pereira-Santos. 2010. Alterna?ncia pas-
siva com verbos transitivos indiretos do portugue?s do
Brasil. Master?s thesis, Universidade de Bras??lia.
Carlos Ramisch, Aline Villavicencio, and Christian
Boitet. 2010. Multiword expressions in the wild?
the mwetoolkit comes in handy. In Proceedings of
the 23rd COLING (COLING 2010) - Demonstrations,
pages 57?60, Beijing, China.
S??lvia Isabel do Rosa?rio Ribeiro. 2011. Estruturas
com ?se? Anafo?rico, Impessoal e Decausativo em Por-
tugue?s. Ph.D. thesis, Faculdade de Letras da Universi-
dade de Coimbra.
Tal Siloni. 2001. Reciprocal verbs. In Online Proceed-
ings of IATL 17, Jerusalem, Israel.
Milena Slavcheva. 2006. Semantic descriptors: The
case of reflexive verbs. In Proceedings of LREC 2006,
pages 1009?1014, Genoa, Italy.
Veronika Vincze. 2012. Light verb constructions in the
szegedparalellFX English?Hungarian parallel corpus.
In Proceedings of LREC 2012, Istanbul, Turkey.
100
