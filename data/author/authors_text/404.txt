Named Entity Recognition in Bengali: A Conditional Random Field
Approach
Asif Ekbal
Department of CSE
Jadavpur University
Kolkata-700032, India
asif.ekbal@gmail.com
Rejwanul Haque
Department of CSE
Jadavpur University
Kolkata-700032, India
rejwanul@gmail.com
Sivaji Bandyopadhyay
Department of CSE
Jadavpur University
Kolkata-700032, India
sivaji cse ju@yahoo.com
Abstract
This paper reports about the development of
a Named Entity Recognition (NER) system
for Bengali using the statistical Conditional
Random Fields (CRFs). The system makes
use of the different contextual information
of the words along with the variety of fea-
tures that are helpful in predicting the var-
ious named entity (NE) classes. A portion
of the partially NE tagged Bengali news cor-
pus, developed from the archive of a lead-
ing Bengali newspaper available in the web,
has been used to develop the system. The
training set consists of 150K words and has
been manually annotated with a NE tagset
of seventeen tags. Experimental results of
the 10-fold cross validation test show the ef-
fectiveness of the proposed CRF based NER
system with an overall average Recall, Pre-
cision and F-Score values of 93.8%, 87.8%
and 90.7%, respectively.
1 Introduction
Named Entity Recognition (NER) is an impor-
tant tool in almost all Natural Language Process-
ing (NLP) application areas. Proper identifica-
tion and classification of named entities (NEs) are
very crucial and pose a very big challenge to the
NLP researchers. The level of ambiguity in NER
makes it difficult to attain human performance.
NER has applications in several domains includ-
ing information extraction, information retrieval,
question-answering, automatic summarization, ma-
chine translation etc.
The current trend in NER is to use the machine-
learning approach, which is more attractive in that
it is trainable and adoptable and the maintenance
of a machine-learning system is much cheaper than
that of a rule-based one. The representative ma-
chine-learning approaches used in NER are Hid-
den Markov Model (HMM) (BBN?s IdentiFinder
in (Bikel et al, 1999)), Maximum Entropy (New
York University?s MENE in (Borthwick, 1999)) and
Conditional Random Fields (CRFs) (Lafferty et al,
2001; McCallum and Li, 2003).
There is no concept of capitalization in Indian
languages (ILs) like English and this fact makes
the NER task more difficult and challenging in ILs.
There has been very little work in the area of NER
in ILs. In Indian languages particularly in Ben-
gali, the work in NER can be found in (Ekbal and
Bandyopadhyay, 2007a; Ekbal and Bandyopadhyay,
2007b) with pattern directed shallow parsing ap-
proach and in (Ekbal et al, 2007c) with HMM.
Other than Bengali, a CRF based NER system can
be found in (Li and McCallum, 2004) for Hindi.
2 Conditional Random Fields
Conditional Random Fields (CRFs) (Lafferty et al,
2001) are used to calculate the conditional proba-
bility of values on designated output nodes given
values on other designated input nodes. The con-
ditional probability of a state sequence S =<
s1, s2, . . . , sT > given an observation sequence
O =< o1, o2, . . . , oT > is calculated as:
P?(s|o) =
1
Z0
exp(
T
?
t=1
?
k
?k ? fk(st?1, st, o, t)),
589
where, fk(st?1, st, o, t) is a feature function whose
weight ?k, is to be learned via training. The val-
ues of the feature functions may range between
??, . . .+?, but typically they are binary. To make
all conditional probabilities sum up to 1, we must
calculate the normalization factor,
Z0 =
?
s
exp(
T
?
t=1
?
k
?k ? fk(st?1, st, o, t)),
which as in HMMs, can be obtained efficiently by
dynamic programming.
To train a CRF, the objective function to be maxi-
mized is the penalized log-likelihood of the state se-
quences given the observation sequences:
L? =
N
?
i=1
log(P?(s(i)|o(i))) ?
?
k
?2k
2?2 ,
where {< o(i), s(i) >} is the labeled training data.
The second sum corresponds to a zero-mean, ?2
-variance Gaussian prior over parameters, which
facilitates optimization by making the likelihood
surface strictly convex. Here, we set parameters
? to maximize the penalized log-likelihood using
Limited-memory BFGS (Sha and Pereira, 2003), a
quasi-Newton method that is significantly more ef-
ficient, and which results in only minor changes in
accuracy due to changes in ?.
When applying CRFs to the NER problem, an ob-
servation sequence is a token of a sentence or docu-
ment of text and the state sequence is its correspond-
ing label sequence. While CRFs generally can use
real-valued functions, in our experiments maximum
of the features are binary valued. A feature function
fk(st?1, st, o, t) has a value of 0 for most cases and
is only set to be 1, when st?1, st are certain states
and the observation has certain properties. We have
used the C++ based OpenNLP CRF++ package 1.
3 Named Entity Recognition in Bengali
Bengali is one of the widely used languages all over
the world. It is the seventh popular language in the
world, second in India and the national language of
Bangladesh. A partially NE tagged Bengali news
corpus (Ekbal and Bandyopadhyay, 2007d), devel-
oped from the archive of a widely read Bengali news
1http://crfpp.sourceforge.net
paper available in the web, has been used in this
work to identify and classify NEs. The corpus con-
tains around 34 million word forms in ISCII (Indian
Script Code for Information Interchange) and UTF-
8 format. The location, reporter, agency and differ-
ent date tags (date, ed, bd, day) in the partially NE
tagged corpus help to identify some of the location,
person, organization and miscellaneous names, re-
spectively, that appear in some fixed places of the
newspaper. These tags cannot detect the NEs within
the actual news body. The date information obtained
from the news corpus provides example of miscella-
neous names. A portion of this partially NE tagged
corpus has been manually annotated with the seven-
teen tags as described in Table 1.
NE tag Meaning Example
PER Single-word sachin/ PER
person name
LOC Single-word jadavpur/LOC
location name
ORG Single-word infosys/ ORG
organization name
MISC Single-word 100%/ MISC
miscellaneous name
B-PER Beginning, Internal sachin/B-PER
I-PER or End of a multiword ramesh/I-PER
E-PER person name tendulkar/E-PER
B-LOC Beginning, Internal or mahatma/B-LOC
I-LOC End of a multiword gandhi/I-LOC
E-LOC location name road/E-LOC
B-ORG Beginning, Internal or bhaba/B-ORG
I-ORG End of a multiword atomic/I-ORG
E-ORG organization name research/I-ORG
center/E-ORG
B-MISC Beginning, Internal or 10e/B-MISC
I-MISC End of a multiword magh/ I-MISC
E-MISC miscellaneous name 1402/E-MISC
NNE Words that are not NEs neta/NNE
Table 1: Named Entity Tagset
3.1 Named Entity Tagset
A CRF based NER system has been developed
in this work to identify NEs in Bengali and clas-
sify them into the predefined four major categories,
namely, ?Person name?, ?Location name?, ?Organi-
zation name? and ?Miscellaneous name?. In order to
590
properly denote the boundaries of NEs and to apply
CRF in NER task, sixteen NE and one non-NE tags
have been defined as shown in Table 1. In the out-
put, sixteen NE tags are replaced appropriately with
the four major NE tags by some simple heuristics.
3.2 Named Entity Features
Feature selection plays a crucial role in CRF frame-
work. Experiments were carried out to find out the
most suitable features for NER in Bengali. The
main features for the NER task have been iden-
tified based on the different possible combination
of available word and tag context. The features
also include prefix and suffix for all words. The
term prefix/suffix is a sequence of first/last few
characters of a word, which may not be a lin-
guistically meaningful prefix/suffix. The use of
prefix/suffix information works well for highly in-
flected languages like the Indian languages. In
addition, various gazetteer lists have been devel-
oped for use in the NER task. We have consid-
ered different combination from the following set
for inspecting the best feature set for NER task:
F={wi?m, . . . , wi?1, wi, wi+1, . . . wi+n, |prefix| ?
n, |suffix| ? n, previous NE tag, POS tags, First
word, Digit information, Gazetteer lists}.
Following are the details of the set of features that
were applied to the NER task:
? Context word feature: Previous and next words of
a particular word might be used as a feature.
? Word suffix: Word suffix information is helpful
to identify NEs. This feature can be used in two
different ways. The first and the nai?ve one is, a
fixed length word suffix of the current and/or the sur-
rounding word(s) might be treated as feature. The
second and the more helpful approach is to modify
the feature as binary valued. Variable length suf-
fixes of a word can be matched with predefined lists
of useful suffixes for different classes of NEs. The
different suffixes that may be particularly helpful in
detecting person (e.g., -babu, -da, -di etc.) and lo-
cation names (e.g., -land, -pur, -lia etc.) have been
considered also. Here, both types of suffixes have
been used.
? Word prefix: Prefix information of a word is also
helpful. A fixed length prefix of the current and/or
the surrounding word(s) might be treated as features.
? Part of Speech (POS) Information: The POS of
the current and/or the surrounding word(s) can be
used as features. Multiple POS information of the
words can be a feature but it has not been used in the
present work. The alternative and the better way is
to use a coarse-grained POS tagger.
Here, we have used a CRF-based POS tagger,
which was originally developed with the help of 26
different POS tags2, defined for Indian languages.
For NER, we have considered a coarse-grained POS
tagger that has only the following POS tags:
NNC (Compound common noun), NN (Com-
mon noun), NNPC (Compound proper noun), NNP
(Proper noun), PREP (Postpositions), QFNUM
(Number quantifier) and Other (Other than the
above).
The POS tagger is further modified with two
POS tags (Nominal and Other) for incorporating
the nominal POS information. Now, a binary val-
ued feature ?nominalPOS? is defined as: If the cur-
rent/previous/next word is ?Nominal? then the ?nom-
inalPOS? feature of the corresponding word is set to
1; otherwise, it is set to 0. This ?nominalPOS? fea-
ture has been used additionally with the 7-tag POS
feature. Sometimes, postpositions play an important
role in NER as postpositions occur very frequently
after a NE. A binary valued feature ?nominalPREP?
is defined as: If the current word is nominal and the
next word is PREP then the feature ?nomianlPREP?
of the current word is set to 1, otherwise set to 0.
? Named Entity Information: The NE tag of the pre-
vious word is also considered as the feature. This is
the only dynamic feature in the experiment.
? First word: If the current token is the first word of
a sentence, then the feature ?FirstWord? is set to 1.
Otherwise, it is set to 0.
? Digit features: Several binary digit features
have been considered depending upon the presence
and/or the number of digits in a token (e.g., Con-
tainsDigit [token contains digits], FourDigit [token
consists of four digits], TwoDigit [token consists
of two digits]), combination of digits and punctu-
ation symbols (e.g., ContainsDigitAndComma [to-
ken consists of digits and comma], ConatainsDigi-
tAndPeriod [token consists of digits and periods]),
combination of digits and symbols (e.g., Contains-
DigitAndSlash [token consists of digit and slash],
2http://shiva.iiit.ac.in/SPSAL2007/iiit tagset guidelines.pdf
591
ContainsDigitAndHyphen [token consists of digits
and hyphen], ContainsDigitAndPercentage [token
consists of digits and percentages]). These binary
valued features are helpful in recognizing miscella-
neous NEs such as time expressions, monetary ex-
pressions, date expressions, percentages, numerical
numbers etc.
? Gazetteer Lists: Various gazetteer lists have been
developed from the partially NE tagged Bengali
news corpus (Ekbal and Bandyopadhyay, 2007d).
These lists have been used as the binary valued fea-
tures of the CRF. If the current token is in a particu-
lar list then the corresponding feature is set to 1 for
the current and/or the surrounding word(s); other-
wise, set to 0. The following is the list of gazetteers:
(i) Organization suffix word (94 entries): This list
contains the words that are helpful in identifying or-
ganization names (e.g., kong, limited etc). The fea-
ture ?OrganizationSuffix? is set to 1 for the current
and the previous words.
(ii) Person prefix word (245 entries): This is useful
for detecting person names (e.g., sriman, sree, sri-
mati etc.). The feature ?PersonPrefix? is set to 1 for
the current and the next two words.
(iii) Middle name (1,491 entries): These words gen-
erally appear inside the person names (e.g., chandra,
nath etc.). The feature ?MiddleName? is set to 1 for
the current, previous and the next words.
(iv) Surname (5,288 entries): These words usually
appear at the end of person names as their parts. The
feature ?SurName? is set to 1 for the current word.
(v) Common location word (547 entries): This list
contains the words that are part of location names
and appear at the end (e.g., sarani, road, lane etc.).
The feature ?CommonLocation? is set to 1 for the
current word.
(vi) Action verb (221 entries): A set of action verbs
like balen, ballen, ballo, shunllo, haslo etc. often
determines the presence of person names. The fea-
ture ?ActionVerb? is set to 1 for the previous word.
(vii) Frequent word (31,000 entries): A list of most
frequently occurring words in the Bengali news cor-
pus has been prepared using a part of the corpus.
The feature ?RareWord? is set to 1 for those words
that are not in this list.
(viii) Function words (743 entries): A list of func-
tion words has been prepared manually. The feature
?NonFunctionWord? is set to 1 for those words that
are not in this list.
(ix) Designation words (947 entries): A list of com-
mon designation words has been prepared. This
helps to identify the position of the NEs, partic-
ularly person names (e.g., neta, sangsad, kheloar
etc.). The feature ?DesignationWord? is set to 1 for
the next word.
(x) Person name (72, 206 entries): This list contains
the first name of person names. The feature ?Person-
Name? is set to 1 for the current word.
(xi) Location name (7,870 entries): This list contains
the location names and the feature ?LocationName?
is set to 1 for the current word.
(xii) Organization name (2,225 entries): This list
contains the organization names and the feature ?Or-
ganizationName? is set to 1 for the current word.
(xiii) Month name (24 entries): This contains the
name of all the twelve different months of both En-
glish and Bengali calendars. The feature ?Month-
Name? is set to 1 for the current word.
(xiv) Weekdays (14 entries): It contains the name of
seven weekdays in Bengali and English both. The
feature ?WeekDay? is set to 1 for the current word.
4 Experimental Results
A partially NE tagged Bengali news corpus (Ekbal
and Bandyopadhyay, 2007d) has been used to cre-
ate the training set for the NER experiment. Out of
34 million wordforms, a set of 150K wordforms has
been manually annotated with the 17 tags as shown
in Table 1 with the help of Sanchay Editor 3, a text
editor for Indian languages. Around 20K NE tagged
corpus has been selected as the development set and
the rest 130K wordforms has been used as the train-
ing set of the CRF based NER system.
We define the baseline model as the one where
the NE tag probabilities depend only on the cur-
rent word: P (t1, t2, . . . , tn|w1, w2, . . . , wn) =
?
i=1,...,n P (ti, wi).
In this model, each word in the test data will be
assigned the NE tag which occurred most frequently
for that word in the training data. The unknown
word is assigned the NE tag with the help of vari-
ous gazetteers and NE suffix lists.
Ninety-five different experiments were conducted
taking the different combinations from the set ?F? to
3Sourceforge.net/project/nlp-sanchay
592
Feature (word, tag) FS
(in %)
pw, cw, nw, FirstWord 71.31
pw2, pw, cw, nw, nw2, FirstWord 72.23
pw3, pw2, pw, cw, nw, nw2, nw3, 71.12
FirstWord
pw2, pw, cw, nw, nw2, FirstWord, pt 74.91
pw2, pw, cw, nw, nw2, FirstWord, pt, 77.61
|pre| ? 4, |suf| ? 4
pw2, pw, cw, nw, nw2, FirstWord, pt, 79.70
|suf| ? 3, |pre| ? 3
pw2, pw, cw, nw, nw2, FirstWord, pt, 81.50
|suf| ? 3, |pre| ? 3, Digit features
pw2, pw, cw, nw, nw2, FirstWord, pt, 83.60
|suf| ? 3, |pre| ? 3, Digit features, pp,
cp, np
pw2, pw, cw, nw, nw2, FirstWord, pt, 82.20
|suf| ? 3, |pre| ? 3, Digit features,
pp2, pp, cp, np, np2
pw2, pw, cw, nw, nw2, FirstWord, pt, 83.10
|suf| ? 3, |pre| ? 3, Digit features, pp, cp
pw2, pw, cw, nw, nw2, FirstWord, pt, 83.70
|suf| ? 3, |pre| ? 3, Digit features, cp, np
pw2, pw, cw, nw, nw2, FirstWord, pt, 89.30
|suf| ? 3,|pre| ? 3, Digit features, pp,
cp, np, nominalPOS, nominalPREP,
Gazetteer lists
Table 2: Results on Development Set
identify the best suited set of features for the NER
task. From our empirical analysis, we found that the
following combination gives the best result with 744
iterations:
F=[wi?2, wi?1, wi, wi+1, wi+2, |prefix| ? 3,
|sufix| ? 3, NE information of the previous word,
POS information of the window three, nominalPOS
of the current word, nominalPREP, FirstWord, Digit
features, Gazetteer lists].
The meanings of the notations, used in experi-
mental results, are defined as below:
cw, pw, nw: Current, previous and next word; pwi,
nwi: Previous and the next ith word from the current
word; pre, suf: Prefix and suffix of the current word;
pt: NE tag of the previous word; cp, pp, np: POS tag
of the current, previous and the next word; ppi, npi:
POS tag of the previous and the next ith word.
Evaluation results of the system for the develop-
ment set in terms of overall F-Score (FS) are pre-
sented in Table 2. It is observed from Table 2 that
word window [?2,+2] gives the best result with
?FirstWord? feature only and the further increase of
the window size reduces the overall F-Score value.
Results of Table 2 (3rd and 5th rows) show that
the inclusion of NE information of the previous
word increases the overall F-Score by 2.68%. It is
also indicative from the evaluation results that the
performance of the system can be improved by in-
cluding the prefix and suffix features. Results (6th
and 7th rows) also show the fact that prefix and suf-
fix of length upto three of the current word is more
effective. In another experiment, it has been also ob-
served that the surrounding word suffixes and/or pre-
fixes do not increase the F-Score value. The overall
F-Score value is further improved by 1.8% (7th and
8th rows) with the inclusion of various digit features.
Results (8th and 9th rows) show that POS in-
formation of the words improves the overall F-score
by 2.1%. In the above experiment, the POS tag-
ger was developed with 26 POS tags. Experimen-
tal results (9th, 10th, 11th and 12th rows) suggest
that the POS tags of the previous, current and the
next words, i.e., POS information of the window
[?1,+1] is more effective than POS information of
the window [?2,+2], [?1, 0] or [0,+1]. In another
experiment, we also observed that the POS informa-
tion of the current word alone is less effective than
the window [?1,+1]. The modified POS tagger that
is developed with 7 POS tags increases the overall F-
Score to 85.2%, while other set of features are kept
unchanged. So, it can be decided that smaller POS
tagset is more effective than the larger POS tagset
in NER. We have observed from two separate ex-
periments that the overall F-Score values can further
be improved by 0.4% and 0.2%, respectively, with
the ?nominalPOS? and ?nominalPREP? features. Fi-
nally, an overall F-Score value of 89.3% is obtained
by including the gazetteer lists.
The best set of features is identified by training
the system with 130K wordforms and testing with
the development set of 20K wordforms. Now, the
development set is included as part of the train-
ing set and resultant training set is thus consists of
150K wordforms. The training set has 20,455 per-
son names, 11,668 location names, 963 organization
593
names and 11,554 miscellaneous names. We have
performed 10-fold cross validation test on this train-
ing set. The Recall, Precision and F-Score values
for the 10 different experiments in the 10-fold cross
validation test are presented in Table 3. The over-
all average Recall, Precision and F-Score values are
93.8%, 87.8% and 90.7%, respectively.
The other existing Bengali NER systems along
with the baseline model are also trained and tested
under the same experimental setup. The baseline
model has demonstrated the overall F-Score value of
56.3%. The overall F-Score value of the CRF based
NER system is 90.7%, which is an improvement of
more than 6% over the HMM based system, best re-
ported Bengali NER system (Ekbal et al, 2007c).
The reason behind the rise in overall F-Score value
might be its better capability than HMM to capture
the morphologically rich and overlapping features of
Bengali language. The system has been evaluated
also for the four individual NE classes and it has
shown the average F-Score values of 91.2%, 89.7%,
87.1% and 99.2%, respectively, for person, location,
organization and miscellaneous names.
5 Conclusion
In this paper, we have developed a NER system us-
ing CRF with the help of a partially NE tagged Ben-
gali news corpus, developed from the archive of a
leading Bengali newspaper available in the web. Ex-
perimental results with the 10-fold cross validation
test have shown reasonably good Recall, Precision
and F-Score values. It has been shown that the con-
textual window [-2, +2], prefix and suffix of length
upto three, first word of the sentence, POS informa-
tion of the window [-1, +1], current word, NE infor-
mation of the previous word, different digit features
and the various gazetteer lists are the best-suited fea-
tures for the Bengali NER.
Analyzing the performance using other methods
like MaxEnt and Support Vector Machines (SVMs)
will be other interesting experiments.
References
Daniel M. Bikel, Richard L. Schwartz, and Ralph M.
Weischedel. 1999. An Algorithm that Learns What?s
in a Name. Machine Learning, 34(1-3):211?231.
A. Borthwick. 1999. Maximum Entropy Approach to
Test set no. Recall Precision FS (%)
1 92.4 87.3 89.78
2 92.3 87.4 89.78
3 91.4 86.6 88.94
4 95.2 87.7 91.29
5 91.6 86.7 89.08
6 92.2 87.1 89.58
7 94.5 87.9 91.08
8 93.8 89.3 91.49
9 96.9 88.4 92.45
10 97.7 89.6 93.47
Average 93.8 87.8 90.7
Table 3: Results for the 10-fold Cross Validation
Test
Named Entity Recognition. Ph.D. thesis, New York
University.
A. Ekbal and S. Bandyopadhyay. 2007a. Lexical Pattern
Learning from Corpus Data for Named Entity Recog-
nition. In Proceedings of ICON, pages 123?128, India.
A. Ekbal and S. Bandyopadhyay. 2007b. Pattern Based
Bootstrapping Method for Named Entity Recognition.
In Proceedings of ICAPR, pages 349?355, India.
A. Ekbal and S. Bandyopadhyay. 2007d. A Web-
based Bengali News Corpus for Named Entity Recog-
nition. Language Resources and Evaluation Journal
(accepted).
A. Ekbal, S.K. Naskar, and S. Bandyopadhyay. 2007c.
Named Entity Recognition and Transliteration in Ben-
gali. Named Entities: Recognition, Classification
and Use, Special Issue of Lingvisticae Investigationes
Journal, 30(1):95?114.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
Data. In ICML, pages 282?289.
Wei Li and Andrew McCallum. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition using Con-
ditional Random Fields and Feature Induction. ACM
TALIP, 2(3):290?294.
A. McCallum and W. Li. 2003. Early results for Named
Entity Recognition with Conditional Random Fields,
Feature Induction and Web-enhanced Lexicons. In
Proceedings of CoNLL, pages 188?191, Canada.
Fei Sha and Fernando Pereira. 2003. Shallow Parsing
with Conditional Random Fields. In Proceedings of
NAACL ?03, pages 134?141, Canada.
594
Generation of Referring Expression Using Prefix Tree Structure 
                       Sibabrata Paladhi                                   Sivaji Bandyopadhyay 
      Department of Computer Sc. & Engg.             Department of Computer Sc. & Engg.          
               Jadavpur University, India                              Jadavpur University, India 
             sibabrata_paladhi@yahoo.com                        sivaji_cse_ju@yahoo.com 
 
Abstract 
This paper presents a Prefix Tree (Trie) 
based model for Generation of Referring 
Expression (GRE). The existing algorithms 
in GRE lie in two extremities. Incremental 
algorithm is simple and speedy but less ex-
pressive in nature whereas others are com-
plex and exhaustive but more expressive in 
nature. Our prefix tree based model not 
only incorporates all relevant features of 
GRE (like describing set, generating Boo-
lean and context sensitive description etc.) 
but also try to attain simplicity and speed 
properties of Incremental algorithm. Thus 
this model provides a simple and linguisti-
cally rich approach to GRE. 
1 Introduction 
Generation of referring expression (GRE) is an 
important task in the field of Natural Language 
Generation (NLG) systems (Reiter and Dale, 
1995). The task of any GRE algorithm is to find a 
combination of properties that allow the audience 
to identify an object (target object) from a set of 
objects (domain or environment). The properties 
should satisfy the target object and dissatisfy all 
other objects in the domain. We sometimes call it 
distinguishing description because it helps us to 
distinguish the target from potential distractors, 
called contrast set. When we generate any natural 
language text in a particular domain, it has been 
observed that the text is centered on certain objects 
for that domain. When we give introductory de-
scription of any object, we generally give full 
length description (e.g. ?The large black hairy 
dog?). But the later references to that object tend to 
be shorter and only support referential communica-
tion goal of distinguishing the target from other 
objects. For example the expression ?The black 
dog? suffices if the other dogs in the environment 
are all non black. Grice, an eminent philosopher of 
language, has stressed on brevity of referential 
communication to avoid conversational implica-
ture. Dale (1992) developed Full Brevity algorithm 
based on this observation. It always generates 
shortest possible referring description to identify 
an object. But Reiter and Dale (1995) later proved 
that Full Brevity requirement is an NP-Hard task, 
thus computationally intractable and offered an 
alternative polynomial time Incremental Algo-
rithm. This algorithm adds properties in a prede-
termined order, based on the observation that hu-
man speakers and audiences prefer certain kinds of 
properties when describing an object in a domain 
(Krahmer et al 2003). The Incremental Algorithm 
is accepted as state of the art algorithm in NLG 
domain. Later many refinements (like Boolean de-
scription and set representation (Deemter 2002), 
context sensitivity (Krahmer et al2002) etc) have 
been incorporated into this algorithm. Several ap-
proaches have also been made to propose an alter-
native algorithmic framework to this problem like 
graph-based (Krahmer et al 2003), conceptual 
graph based (Croitoru and Deemter 2007) etc that 
also handle the above refinements. In this paper we 
propose a new Prefix Tree (Trie) based framework 
for modeling GRE problems. Trie is an ordered 
tree data structure which allows the organization of 
prefixes in such a way that the branching at each 
level is guided by the parts of prefixes. There are 
several advantages of this approach: 1) Trie data 
structure has already been extensively used in 
many domains where search is the key operation. 
2) The structure is scalable and various optimized 
algorithms are there for time, space optimizations.  
  In this paper it is shown how scenes can be 
represented using a Trie (section 2) and how de-
scription generation can be formalized as a search 
problem (section 3). In section 4 the algorithm is 
explained using an example scene. In section 5, the 
basic algorithm is extended to take care of different 
scenarios. The algorithm is analyzed for time com-
697
plexity in section 6 and conclusion is drawn in sec-
tion 7. 
2 Modeling GRE Using Trie Structure 
In this section, it is shown how a scene can be rep-
resented using a trie data structure. The scheme is 
based on Incremental algorithm (Reiter and Dale 
1995) and incorporates the attractive properties 
(e.g. speed, simplicity etc) of that algorithm. Later 
it is extended to take care of different refinements 
(like relational, boolean description etc) that could 
not be handled by Incremental algorithm. Reiter 
and Dale (1995) pointed out the notion of 
?PreferredAttributes? (e.g. Type, Size, Color etc) 
which is a sequence of attributes of an object that 
human speakers generally use to identify that ob-
ject from the contrast set. We assume that the ini-
tial description of an entity is following this se-
quence (e.g. ?The large black dog?) then the later 
references will be some subset of initial description 
(like ?The dog? or ?The large dog?) which is de-
fined as the prefix of the initial description. So, we 
have to search for a prefix of the initial full length 
description so that it is adequate to distinguish the 
target object. Following the Incremental version 
we will add properties one by one from the 
?PreferredAttributes? list. In our model, the root 
consists of all entities in the domain and has empty 
description. Then at each level, branching is made 
based on different values of corresponding pre-
ferred attribute. The outgoing edge is labeled with 
that value. For example, at the first level, branch-
ing is made based on different values of ?Type? 
attribute like ?Dog?, ?Cat?, ?Poodle? etc. A node in 
Trie will contain only those objects which have the 
property(s) expressed by the edges, constituting the 
path from root to that node. After construction of 
the Trie structure for a given domain in this way, 
referring expression generation problem for an ob-
ject r is reduced to search the tree for a node which 
consists of r and no other object. Description for r 
can be found from the search path itself as we have 
said earlier. Now we will introduce some notations 
that we will use to describe the actual algorithm. 
Let D be the Domain, r be the target object and P 
be the ?PreferredAttributes? List.  Ni   = {d | 
d?D and d is stored at node Ni} where Ni is an i-th 
level node. Obviously  No   = D since No is root 
node. E(Ni, Nki+1) is an edge between parent node 
Ni and Nki+1, k-th child of that node (considering an 
enumeration among children nodes). Since every 
edges in Trie are labeled, thus {E} ? {N} x L x 
{N}, where {E} and {N} are set of all edges and 
nodes respectively in the tree and L is the set of 
attribute values. Let Val(E(Ni, Nki+1)) denotes the 
label or value of the edge and  Val(E(Ni, Nki+1))    
= {d | d?D and d is satisfied by the edge value} 
i.e. the set contains those objects who have this 
property. We define  Nki+1  =  Ni   
?  Val(E(Ni, Nki+1)) where Ni and Nki+1 are par-
ent and child node respectively. Similarly  Nki   = 
 Ni-1   ?   Val(E(Ni-1, Nki))  . Ultimately, we 
can say that ? i  Ni   =  No ?  Val(E(No,N1)) 
 ? ?? ?  Val(E(Ni-1,Ni))  . Since our con-
struction is basically a tree, each node is reachable 
from root and there exists a unique path from root 
to that node. So, for each node in the tree we will 
get some description. We will formulate referring 
expression construction as search in the con-
structed tree for the node min(k){Nk} such that  Nk   
= {r}. If Nk is leaf node then description of r will 
be same with the full description but if it is an in-
termediate node then description is some proper 
prefix of initial description. But the point is that, in 
both cases the later reference is prefix of initial one 
(as both ?ab? and ?abc? are prefixes of ?abc?).  
3 Basic Algorithm  
Based on above discussions, algorithms are devel-
oped for construction of Trie from the domain and 
generation of reference description for any object 
in that domain. The Trie construction algorithm 
ConstructTrie(D,P,T) is shown in figure 1, Refer-
ring expression generation algorithm MakeRe-
fExpr(r,p,T,L) is shown in figure 2, where T is a 
node pointer and p is pointer to parent of that node. 
Our algorithm MakeRefExpr returns set of attrib-
ute-values L to identify r in the domain. As dis-
cussed earlier, it is basically a node searching algo-
rithm. In course of searching, if it is found that an 
intermediate node N doesn?t have r i.e. r?   N   
then our search will not move forward through the 
subtree rooted at N. Our search will proceed 
through next level iff r?  N  . For a node Nk, if 
we get  Nk   = {r} then we have succeeded and 
our algorithm will return L, set of descriptions for 
that node. If there is no distinguishing description 
exists for r, then ? (null) will be returned.  We 
698
would like to point out that our algorithm will find 
out only one description that exists at the minimum 
level of the tree. Moreover, a description is added 
to L only if it is distinguishing i.e. the connecting 
edge must remove some contrasting object(s). 
Thus, the child node should contain less number of 
objects than that of parent node. In this case, cardi-
nality of parent Ni (Card(Ni)) will be greater than 
that of child (Card(Ni+1)). This condition is in-
cluded in our algorithm and if (Card (P?N)) > 
Card (T?N) holds then only the value is added 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
P->N and T->N respectively represents parent and 
child node. After finding a distinguishing descrip-
tion for r, search will neither move further down 
the tree nor explore the remaining branches of the 
current node. Search will explore the next branch 
only if the search in current branch returned NULL 
description i.e. when L? = ?  in the algorithm. If 
we reach a leaf node and that contains r along with 
other objects then it is not possible to distinguish 
r?. In that case, the algorithm returns NULL indi-
cating that no description exists at all. It has been 
later shown that some distinguishing description 
may still exist and the algorithm will be modified 
to find that. It should be mentioned that once the 
prefix tree is constructed offline, it can be used 
repetitively to find description for any object in the 
domain throughout the text generation phase. Our 
MakeRefExpr() algorithm is very simple and it 
doesn?t employ any set theoretic operation, which 
is a non trivial task, to find current contrast set at 
every steps of algorithm. In existing algorithms, 
computing referential description for every object 
require computing similar things (like finding cur-
rent contrast set, ruled out objects) again and again. 
And it has to be repeated every time the object is 
referred. It is not possible to generate description 
once, store it and use it later because of the fact 
that domain may also change in course of time 
(Krahmer, 2002). That?s why every time we want 
to refer to ?r?, such rigorous set operations need to 
be computed. But in our prefix tree structure, once 
the tree is constructed, it is very easy to find de-
scription for that object using simple tree search 
function. It is also very easy to add/delete objects 
to/from domain. We have to follow just the initial 
properties of that object to find the proper branch-
ing at each level, followed by addition /deletion of 
that object to /from relevant nodes, which is essen-
tially a search operation. The disadvantage of our 
algorithm is that space complexity is high but it 
can be tackled using bit Vector representation of 
individual nodes of the prefix tree. Besides, several 
methods are there for compressing Trie structure. 
But these optimization techniques are beyond the 
scope of our current discussion. 
4 Formalizing A Scene using Prefix Tree  
Consider an example scene in figure 3, from 
[Krahmer 2002]. In this scene, there is a finite do-
main of entities D. Let D = {d1, d2, d3, d4}, P = 
{Type, Size, Color} and values are Type = {dog, 
cat}; Size = {small, large}; Color = {black, white}. 
A scene is usually represented as a database (or 
ConstructTrie(D, P, T) { 
  If (D = ?  ?  P = ? )  
  Then Stop 
  Else 
     Create a node N at T 
     Set  N   = D 
     Extract front attribute Ai from list P 
     P?  =   P ? { Ai } 
     For each value Vj  of  attribute  Ai  do 
          Create Edge Ej with label Vj as T?Nextj 
      
      Dj?  = D ?    Val(Ej)     
          ConstructTrie(Dj?  , P?, T?Nextj) 
        
End For 
   End If 
} 
 
 
  
Figure 2. Expression Generation Algorithm 
Figure 1. Prefix Tree Generation Algorithm 
MakeRefExpr(r, P, T, L) { 
    If ( r ?  T?N  ) 
            Then  L ? ?  
             Return L 
     Else If ({r} =  T?N  ) 
            L = L ? Val(P?Ej )     
            Return L 
    Else If (isLeaf (T) ? {r} ?  N  ) 
             Then L ? ?  
             Return L 
    Else { 
         If (Card(P?N) > Card (T?N))  
             Then L = L ? Val(P?Ej ) 
         P = T 
         For each outgoing edge T? Nextj (Ej)  do 
            L? = MakeRefExpr(r, P,T? Childj, L) 
                If (L? ?? ) 
                Then Return L? 
        } } 
                                                                                
699
knowledge base) listing the properties of each ele-
ment in D. Thus: 
d1 : ? Type : dog ? , ? Size : small ? , ? Color: white ? 
d2 : ? Type : dog ? , ? Size : large ? , ? Color: white ? 
d3 : ? Type : dog ? , ? Size : large ? , ? Color: black ? 
d4:  ? Type : cat ? ,  ? Size: small ? ,  ? Color: white ? 
Now it will be shown how our MakeRefExpr() 
algorithm will find a description for a target object 
r. Let r = {d1}. In the first phase, starting from 
root, edge labeled D is traversed. Since d1 exists in 
the node and D discards some objects (d4), D is 
distinguishing description and it is added to L. In 
the next phase the node connected by the edge la-
beled L does not contain d1 so search will not pro-
ceed further. Rather the node connected by the 
edge labeled S contains d1. Since, d1 is the only 
object, then we are done and the referring expres-
sion is ?The small dog?. But for d2, we have to 
search upto the leaf node which generates the de-
scription ?The large white dog?. 
 
 
  
            Figure 3.  Scene Representation  
5 Extension of Basic Algorithm  
5.1 Specifying Overlapping Values  
Deemter (2002) has shown incompleteness of In-
cremental algorithm in case of overlapping values. 
Due to vagueness of properties, sometimes it is 
hard to classify an object in a particular class. Con-
sider the example scene D = {a,b,c,d} Color: 
{Red(a,b); Orange(a,c,d)} Size: {Large(a,b); 
Small(c,d)}. In this case a can not be properly clas-
sified by Color type. Incremental algorithm always 
select Red(a,b) at first phase, since it rules out 
maximum distractors and returns failure because it 
can?t distinguish a from b at second phase. Deem-
ter(2002) suggested inclusion of all overlapping 
values that are true of target while also removing 
some distractors. So, referring expression for a is 
?The red orange desk?. But it fails to obey Gricean 
maxims of conversational implicature. We con-
sider the failure as ?Early Decision? problem and 
defer the decision making in our model. We keep 
in our mind the fact that human beings seldom take 
instantaneous decision. Rather they consider all 
opportunities in parallel and take decision in the 
favor of the best one at later point of time. Since, 
our algorithm searches in parallel through all 
promising branches until some description is 
found; it mimics the capabilities of human mind to 
consider in parallel. Our algorithm will generate 
?The large orange desk? which will help audiences 
to better identify the desk. The execution sequence 
is shown in figure 4.  
 
       
 
         Figure 4.  Dealing with overlapping values 
 
5.2 Describing Set of Objects 
Generation of referring description for a set of ob-
jects is very important in NLG. Deemter?s (2002) 
suggestion can be easily incorporated into our 
framework. We will represent target r as set of ob-
jects. Now our algorithm will try to find a node in 
the tree which only consists of all objects in the set 
r. In this way, we can find a distinguishing de-
scription for any set, for which description exists. 
In figure 3, the description for the set {d2,d3} is 
?The large dogs?. Thus, our basic algorithm is able 
to describe set of objects. In case of set like {d2, d3, 
d4} where there is no separate node consisting all 
the object, we need to partition the set and find 
description for individual set. In our case the pos-
sible partitions are {d2, d3} and {d4} for which 
separate nodes exist.  
700
5.3 Boolean Descriptions     
Deemter (2002) shown that Incremental algorithm 
is only intersectively complete. But he argues that 
other Boolean combination of properties can be 
used to generate description for an object. Consider 
the example from (Deemter, 2002).  Let D = {a, b, 
c, d, e} Type: {Dog(a,b,c,d,e); Poodle(a,b)} Color: 
{Black(a,b,c); White(d,e)} and r = {c}. In this sce-
nario Incremental algorithm is not able to indi-
viduate any of the animals. However a description 
for c exists, ?The black dog that is not a poodle?. 
Since {c} = [[Black]] ? [[ ? Poodle]]. Deemter 
(2002) has modified the Incremental algorithm by 
adding negative values for each attribute. Now we 
will show that our basic algorithm can be modified 
to take care of this situation. In our basic algorithm 
ConstructTrie(), we add branches at each level for 
negative values also. In this case our simple rou-
tine MakeRefExpr() is able to find boolean de-
scription while remaining as close as to Incre-
mental algorithm. In figure 5, we show part of the 
trie structure, which is generated for the above 
scene. The dashed arrows show the alternative 
search paths for node containing {c}. 
   
 
Figure 5.  Trie structure (Partial) incorporating  
negation of  properties  
 
For referring objects using disjunction of proper-
ties we have do same thing as negations. We have 
to extend our prefix tree structure by adding extra 
edges at different levels for making implicit infor-
mation explicit as described in [Krahmer 2002]. 
5.4 Incorporating Context Sensitivity     
Krahmer and Theune [2002] have added the notion 
of context sensitivity into GRE. Earlier algorithms 
assumed that all objects in environment are equally 
salient. Krahmer and Theune refined the idea by 
assigning some degree of salience to each object. 
They proposed that during referring any object, the 
object needs to be distinguished only from those 
objects which are more salient (having higher sali-
ence weight). An object that has been mentioned 
recently, is linguistically more salient than other 
objects and can be described using fewer proper-
ties (?The dog? instead of ?The large black hairy 
dog?). They introduced the concept of centering 
theory, hierarchical focus constraints in the field of 
NLG and devised a constant function mapping sw: 
D ?? , where sw is salience weight function, D is 
domain and ?  is set of natural numbers. We can 
incorporate this idea into our model easily. In each 
node of the prefix tree we keep a field ?salience 
weight? (sw) for each of the object stored in that 
node in the form (di, swi). During describing an 
object if we find a node that is containing r where 
it is the most salient then we need not traverse 
higher depth of the tree. So, we have to modify 
MakeRefExpr() algorithm by adding more condi-
tions. If the current node is N and both 1) r?  N   
and 2) ? d?  N   (d ? r ? sw(d) < sw(r)) hold 
then r is the most salient and the edges constituting 
the path from root to N represents distinguishing 
description for r. In figure 6, a is most salient dog 
and referred to as ?The dog? whereas b is referred 
to as ?The small dog?. 
 
 
 
Figure 6:  Trie structure (Partial) representing Con-
text Sensitivity 
5.5 Relational Descriptions 
Relational descriptions are used to single out an 
object with reference to other one. For example 
?The cup on the table? is used to distinguish a cup 
from other cups which are not on the table. Dale 
and Haddock (1991) first offer the idea of rela-
701
tional description and extend Full Brevity algo-
rithm to incorporate this idea. Later Krahmer et al 
(2003) Graph based framework for generating rela-
tional description. We follow Krahmer (2002) and 
denote relations as Spatial: {In(a,b); Left_of(c,d)} 
etc. Then we treat ?Spatial? as another attribute and 
consider ?In?, ?Left_of? as different values for that 
attribute. In this way, our basic algorithm itself is 
capable of handling relational descriptions. The 
only modification that we add that when a relation 
R is included, the MakeRefExpr() should be 
called again for the relatum. Thus, if Val(E(Ni, 
Nki+1)) expresses a relation of r with r? then we 
have to call MakeRefExpr (r?,p,T,L) again to find 
description for  r?. 
5.6 Modeling Full Brevity 
In this section, we will show that our prefix tree 
structure can be so modified that it can generate 
shortest possible description which is requirement 
of Full Brevity (Dale, 1992). Consider a scene 
where a domain is identified by set of n attributes 
{A1, A2?An}. We can generate n! number of dif-
ferent permutations of Ai?s ? i? [1,n]. We con-
sider each permutation as different PreferredAt-
tributes list Pk and generate all possible prefix 
trees Tk for each Pk ? k? [1,n!] for same domain 
D. Now, we connect roots of all trees with a com-
mon dummy root node with edges having empty 
description (?). Now, if we search the branches of 
new combined tree in parallel, it?s obvious that we 
can always find the target node at lowest possible 
level. Thus we can generate shortest length de-
scription using our algorithm. 
6 Complexity of The Algorithm  
Let the domain entities are identified by a number 
of attributes and each attribute has on the average 
k number of different values. So, our Con-
structTrie() algorithm takes ?(ka) time. Now we 
will consider different cases for analyzing the time 
complexity of our MakeRefExpr() algorithm.          
 1) In case of non overlapping properties, our 
search tree will be pruned at each level by a factor 
of k. Thus the time complexity will be ?(logk(ka)) 
= ?(a) which is linear. 
2) In case of overlapping properties, we have to 
search whole tree in worst case (although in aver-
age cases also there will be large pruning, as found 
from test cases) which will take ?(ka) time.                    
3) In case of achieving full brevity requirement, 
both time and space complexity will be exponen-
tial as in the original algorithm by Dale (1992).  
7 Conclusions 
In this paper, we present a new Prefix tree (Trie) 
based approach for modeling GRE problems. We 
construct the trie in such a way that a node at a par-
ticular level consists of only those objects which 
are satisfied by values of the edges, constituting 
the path from root to that node. We formulate de-
scription generation as a search problem. So, when 
we reach the target node, the attribute values corre-
sponding to the edges in the path automatically 
form the distinguishing description. Different sce-
narios of GRE problems like representation of set, 
boolean descriptions etc. is taken care of in this 
paper. We have shown that in simple non overlap-
ping scenarios, our algorithm will find distinguish-
ing description in linear time. 
8 References 
E. Krahmer and M. Theune. 2002. Efficient Context 
Sensitive Generation of Referring Expressions. CSLI 
Publ, Stanford : 223 ? 264 
E. Krahmer, S. van Erk and A. Verlag. 2003. Graph 
based Generation of Referring Expressions Computa-
tional Linguistics, 29(1): 53-72 
H. Horacek. 2004. On Referring to Set of Objects Natu-
rally.  Proceedings of Third INLG, Brokenhurst, U.K: 
70-79 
M. Croitoru  and van Deemter. 2007. A conceptual 
Graph Approach to the Generation of Referring Ex-
pressions. Proceedings of IJCAI 2007 : 2456-2461  
R. Dale and N. Haddock. 1991.  Generating Referring 
Expressions containing Relations. Proceedings of 
Fifth ACL- EACL conference, 161-166 
R. Dale. 1992. Generating Referring Expressions: 
Building Descriptions in a Domain of Objects and 
Processes. MIT Press 
R. Dale  and E. Reiter. 1995. Computational Interpreta-
tions of the Gricean Maxims in the generation of Re-
ferring Expressions. Cognitive Science (18): 233 ? 
263 
van Deemter. 2002. Generating Referring Expressions: 
Boolean Extensions of Incremental Algorithm. Com-
putational Linguistics 28(1): 37-52 
702
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 65?72,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Design of a Rule-based Stemmer for Natural Language Text in Bengali
Sandipan Sarkar 
IBM India 
sandipan.sarkar@in.ibm.com, 
sandipansarkar@gmail.com 
Sivaji Bandyopadhyay 
Computer Science and Engineering Department 
Jadavpur University, Kolkata 
sbandyopadhyay@cse.jdvu.ac.in 
 
 
Abstract 
This paper presents a rule-based approach 
for finding out the stems from text in Ben-
gali, a resource-poor language. It starts by 
introducing the concept of orthographic 
syllable, the basic orthographic unit of 
Bengali. Then it discusses the morphologi-
cal structure of the tokens for different 
parts of speech, formalizes the inflection 
rule constructs and formulates a quantita-
tive ranking measure for potential candi-
date stems of a token. These concepts are 
applied in the design and implementation 
of an extensible architecture of a stemmer 
system for Bengali text. The accuracy of 
the system is calculated to be ~89% and 
above. 
1 Introduction 
While stemming systems and algorithms are being 
studied for European, Middle Eastern and Far 
Eastern languages for sometime, such studies in 
Indic scripts are quite limited. Ramanathan and 
Rao (2003) reported a lightweight rule-based 
stemmer in Hindi. Garain et. al. (2005) proposed a 
clustering-based approach to identify stem from 
Bengali image documents. Majumdar et. al. (2006) 
accepted the absence of rule-based stemmer in 
Bengali and proposed a statistical clustering-based 
approach to discover equivalence classes of root 
words from electronic texts in different languages 
including Bengali. We could not find any publica-
tion on Bengali stemmer following rule-based ap-
proach. 
Our approach in this work is to identify and 
formalize rules in Bengali to build a stemming sys-
tem with acceptable accuracy. This paper deals 
with design of such a system to stem Bengali 
words tokens tagged with their respective parts of 
speech (POS). 
2 Orthographic Syllable 
Unlike English or other Western-European lan-
guages, where the basic orthographic unit is a 
character, Bengali uses syllable. A syllable is typi-
cally a vowel core, which is preceded by zero or 
more consonants and followed by an optional dia-
critic mark.  
However, the syllable we discuss here is ortho-
graphic and not phonological, which can be differ-
ent. As for example, the phonological syllables of 
word ???? ? [kartaa] are ?r [kar_] and ?? [taa]. 
Whereas, the orthographic syllables will be ? [ka] 
and ??? ? [rtaa] respectively. Since the term 'syllable' 
is more used in phonological context, we use 'o-
syllable' to refer orthographic syllables, which will 
be a useful tool in this discussion. 
Formally, using regular expression syntax, an o-
syllable can be represented as where C 
is a consonant, V is a vowel and D is a diacritic 
mark or halant. If one or more consonants are pre-
sent, the vowel becomes a dependent vowel sign 
[maatraa].  
* ? ?C V D
We represent the o-syllables as a triple (C, V, D) 
where C is a string of consonant characters, V is a 
vowel character and D is a diacritic mark. All of 
these elements are optional and their absence will 
be denoted by ?. V will be always represented in 
independent form. 
We define o-syllabic length |?| of token (?) as 
the number of o-syllables in ?. 
Few examples are provided below: 
 
Token (?) O-syllable Form |?| 
?? [maa] (?,?,?) 1 
???? [chaa`nd] (?,?,??)(?,a,?) 2 
a?s?? [agastya] (?,a,?)(?,a,?)(???,a,?) 3 
65
Token (?) O-syllable Form |?| 
????? [aaT_kaa] (?,?,?) (?,?,??) (?,?,?) 3 
Table 1: O-syllable Form Examples 
3 Morphological Impact of Inflections 
Like English, the inflections in Bengali work as a 
suffix to the stem. It typically takes the following 
form: 
<token> ::= <stem><inflections> 
<inflections> ::= <inflection> |  
<inflection><inflections> 
Typically Bengali word token are formed with 
zero or single inflection. Example: ????? [maayer] 
< ?? [maa] (stem) + ??? [yer] (inflection) 
However, examples are not rare where the token 
is formed by appending multiple inflections to the 
stem: ????o [karaleo] < ?r [kar_] (stem) + ?? [le] 
(inflection) + o [o] (inflection), ??i?????i [bhaaid-
erakei] < ??i [bhaai] (stem) + ??? [der] (inflec-
tion) + ?? [ke] (inflection) + i [i] (inflection). 
3.1 Verb 
Verb is the most complex POS in terms of in-
flected word formation. It involves most number of 
inflections and complex formation rules.  
Like most other languages, verbs can be finite 
and non-finite in Bengali. While inflections for 
non-finite verbs are not dependent on tense or per-
son; finite verbs are inflected based on person (first, 
second and third), tense (past, present and future), 
aspect (simple, perfect, habitual and progressive), 
honour (intimate, familiar and formal), style (tradi-
tional [saadhu], standard colloquial [chalit] etc.) 
mood (imperative etc.) and emphasis. Bengali verb 
stems can yield more than 100 different inflected 
tokens.  
Some examples are: ?????? [karaatis] < ??? 
[karaa] (stem) + ??? [tis] (inflection representing 
second person, past tense, habitual aspect, intimate 
honour and colloquial style), ??i? [khaaiba] < ?? 
[khaa] (stem) +i? [iba] (inflection representing 
first person, future tense, simple aspect and tradi-
tional style) etc. 
A verb token does not contain more than two in-
flections at a time. Second inflection represents 
either emphasis or negation.  
Example: ???i [aasabai] < ?s [aas_] (stem) + ? 
[ba] (inflection representing first person, future 
tense, simple aspect and colloquial style) + i [i] 
(inflection representing emphasis). 
While appended, the inflections may affect the 
verb stem in four different ways: 
1. Inflections can act as simple suffix and do not 
make any change in the verb stem. Examples: ??? 
(stem) + ?c [chchhi] (inflection) > ????c [karaach-
chhi], ?? (stem) + ? (inflection) > ??? [khaaba] etc.  
2. Inflections can change the vowel of the first 
o-syllable of the stem. Example (the affected vow-
els are in bold and underlined style): ???? ?? 
[shudh_raa] (stem) + ? [sa] (inflection) > (?,u,?) 
(?,?,??) (?,?,?) + ? > (?,o,?) (?,?,??) (?,?,?) + ? > 
????? ?? [shodh_raa] + ? > ????? ??? [shodh_raasa]. 
3. Inflections can change the vowel of the last o-
syllable of the stem. Example: ??? ?? [aaT_kaa] 
(stem) + ?? [chhi] (inflection) > (?,?,?) (?,?,??) 
(?,?,?) + ?? > (?,?,?) (?,?,??) (?,e,?) + ?? > ???
?? [aaT_ke] + ?? > ??????? [aaT_kechhi]. 
4. Inflections can change the vowel of both first 
and last o-syllable of the stem. Example: ???k ?? 
[Thok_raa] (stem) + o [o] (inflection) > (?,o,?) 
(?,?,??) (?,?,?) + o > (?,u,?) (?,?,??) (?,i,?)  
+ o > ??k?? [Thuk_ri] + o > ??k??o [Thuk_rio]. 
3.2 Noun 
Noun is simpler in terms of inflected token forma-
tion. Zero or more inflections are applied to noun 
stem to form the token. Nouns are inflected based 
on number (singular, plural), article and case [k?-
raka] (nominative, accusative, instrumental, dative, 
ablative, genitive, locative and vocative). Unlike 
verbs, stems are not affected when inflections are 
applied. The inflections applicable to noun is a dif-
ferent set than verb and the number of such inflec-
tions also less in count than that of verb. 
Example: ???????i [baarhiTaarai] < ???? [baarhi] 
(stem) + ?? [Taa] (inflection representing article) + 
? [ra] (inflection representing genitive case) + i [i] 
(inflection representing emphasis), ???????????? 
[maanushhaguloke] < ????? [maanushha] (stem) + 
????? [gulo] (inflection representing plural number) + 
?? [ke] (inflection representing accusative case) etc.  
3.3 Pronoun 
Pronoun is almost similar to noun. However, there 
are some pronoun specific inflections, which are 
not applicable to noun. These inflections represent 
location, time, amount, similarity etc. 
66
Example: ???? [sethaa] < ?? [se] (stem) + ?? [thaa] 
(inflection representing location). This inflection is 
not applicable to nouns. 
Moreover, unlike noun, a pronoun stem may 
have one or more post-inflection forms. 
Example: stem ??? [aami] becomes ??? [aamaa] 
(????? < ??? + ??) or ??? [mo] (?????? < ??? + ???) once 
inflected.  
3.4 Other Parts of Speeches 
Other POSs in Bengali behave like noun in their 
inflected forms albeit the number of applicable 
inflections is much less comparing to that of noun.  
Example: ????? [shreshhThatama] < ??? 
[shreshhTha] (adjective stem) + ?? [tama] (inflec-
tion representing superlative degree), ????? [madhye] 
< ???? [madhya] (post-position stem) + ?? [e] 
(inflection) etc. 
4 Design 
4.1 Context 
As we identified in the previous section, the impact 
of inflections on stem are different for different 
POSs. Also the applicable list of inflections varies 
a lot among the POSs. Hence, if the system is POS 
aware, it will be able to generate more accurate 
result. This can be achieved by sending POS 
tagged text to the stemmer system, which will ap-
ply POS specific rules to discover stems. This 
proposition is quite viable as statistical POS tag-
gers like TnT (Brants, 2000) are available. 
The context of the proposed system is provided 
below: 
 
Figure 1: Context of Proposed Stemmer 
4.2 Inflection Rule Observations 
To discover the rules, we took the help of the 
seminal work by Chatterji (1939). For this work 
we limited our study within traditional and stan-
dard colloquial styles (dialects) of Bengali.  For 
each of the POSs, we prepared the list of applica-
ble inflections considering these dialects only. We 
studied these inflections and inflected tokens and 
framed the rules inspired by the work of Porter 
(1981). We had following observations:  
1. To find out the stem, we need to replace the 
inflection with empty string in the word token. 
Hence all rules will take the following form:  
                           <inflection> ? "" 
2. For rules related to verbs, the conditionals are 
present but they are dependent on the o-syllables 
instead of 'm' measure, as defined and described in 
Porter (1981). 
3. For pronouns the inflection may change the 
form of the stems. The change does not follow any 
rule. However, the number of such changes is 
small enough to handle on individual basis instead 
of formalizing it through rules. 
4. A set of verb stems, which are called incom-
plete verbs, take a completely different form than 
the stem. Such verbs are very limited in number. 
Examples: ?? [Jaa] (????? [gelaam] etc. are valid 
tokens for this verb), ?s (e??? [elaam] etc. are 
valid tokens), ??? [aachh_] (?????? [thaakalaam], 
??? [chhila] etc. are valid tokens) 
5. For non-verb POSs, there is no conditional. 
6. Multiple inflections can be applied to a token. 
7. The inflections may suggest mutually contra-
dictory results. As for example token ???? [kheli] 
can be derived by applying two legitimate inflec-
tions ?? [li] and ?? [i] on two different stems ?? 
[khaa] and ??l [khel_] respectively. Finding out the 
correct stem can be tricky.  
8. Because of contradictory rules and morpho-
logical similarities in different stems there will be 
ambiguities. 
Tagged 
Text 
Plain Text Stemmed Text 
POS Tagger Stemmer 
4.3 Analysis and Design Decisions 
Based on the observations above we further ana-
lyzed and crafted a few design decisions, which are 
documented below: 
POS Group Specific Inflection Sets: It is ob-
served that multiple POSs behave similarly while 
forming inflected word tokens. We decided to 
group them together and keep a set of inflections 
for each such group. By separating out inflection 
sets, we are minimizing the ambiguity.  
We identified following inflection sets based on 
the tagset developed by IIIT Hyderabad for Indic 
languages. The tags not mentioned in the table be-
low do not have any inflected forms. Size indicates 
the number of inflections found for that set. 
67
Set Comment Size
IN  The inflection set for noun group. It 
covers NN, NNP, NVB, NNC and 
NNPC tags. 
40 
IP The inflection set for pronoun group. 
It covers PRP and QW tags. This is a 
superset of IN. 
54 
IV The inflection set for verb group. It 
covers VFM, VAUX, VJJ, VRB and 
VNN tags. 
184 
IJ The inflection set for adjective 
group. It covers JJ, JVB, QF and 
QFNUM tags. 
14 
IR The inflection set for adverb, post-
position, conjunction and noun-
location POSs. It covers RB, RBVB, 
PREP, NLOC and CC tags. 
6 
Table 2: POS Groups 
Pronoun ? Post-inflection vs. Actual Stem 
Map: For pronoun we decided to keep a map of 
post-inflection stems and actual stems. After in-
flection stripping, this map will be consulted to 
discover the stem. Since number of pronouns in 
Bengali is limited in number, this approach will 
provide the most effective and performance 
friendly mechanism. 
Verb ? Morphological Rules: Based on obser-
vation 2, we further studied the verb POS and iden-
tified four classes of stems that exhibits own char-
acteristics of morphological changes when inflec-
tions are applied. These classes can be identified 
for a stem ? based on the following two meas-
ures: 
n = |?| and 
2
n
j
j
c?
=
=?  
where cj is the number of consonants in j-th o-
syllable of the stem. 
 
Class Identification Characteristics 
I If n = 1. Example: ?? [khaa], ?? [de] etc. 
II If n > 1 and the n-th o-syllable has halant 
as diacritic mark. Only this class of verb 
stems can have halant at the last o-
syllable. Example: ?r, ???? [shikh_] etc. 
III If n > 1, ? = 1 and vowel of the n-th o-
syllable is '?'. Example: ???, ???? [shik-
haa], ????? [dourhaa] etc. 
IV If n > 1, ? > 1 and vowel of the n-th o-
Class Identification Characteristics 
syllable is '?'. Example: ?????, ?m?? 
[dham_kaa] etc. 
Table 3: Verb Stem Classes 
Since the verb inflections may affect the stems 
by changing the vowels of first and last o-syllable, 
a rule related to verb inflection is presented as a 5-
tuple:  
(L1, R1, Ln, Rn, i)  
where 
? L1 is the vowel of the first o-syllable of post-
inflection stem 
? R1 is the vowel of the first o-syllable of ac-
tual stem 
? Ln is the vowel of the last (n-th) o-syllable of 
post-inflection stem 
? Rn is the vowel of the last (n-th) o-syllable of 
actual stem 
? i  is the inflection 
The vowels are always presented in their inde-
pendent form instead of maatraa. This is because, 
we are going to apply these rules in the context of 
o-syllables, which can deterministically identify, 
which form a vowel should take. However, for in-
flection, we decided to differentiate between de-
pendent and independent forms of vowel to mini-
mize the ambiguity. 
As for example, for the token ??k ??o, inflection is 
o, post-inflection stem is ??k ??, and the actual stem 
is ???k ??. Hence the rule for this class IV verb will 
be (u, o, i, ?, o).    
Absence of an element of the 5-tuple rule is rep-
resented by '?'. Example: for token ???? [kheye], 
which is derived from stem ??, a class I verb stem; 
the rule will be (e, ?, ?, ?, ??). 
After completion of analysis, we captured 731 
such rules. The distribution was 261, 103, 345 and 
22 for class I, II, III & IV combined and IV respec-
tively. 
Map for Incomplete Verbs: For incomplete 
verbs, we decided to maintain a map. This data 
structure will relate the tokens to an imaginary to-
ken, which can be generated from the stem using a 
5-tuple rule. Taking the example of token ?????, 
which is an inflected form of stem ??, will be 
mapped to ????? [Jelaam], which can be generated 
by applying rule (e, ?, ?, ?, ???). The system 
will consult this map for each input verb token. If 
68
it is found, it will imply that the token is an incom-
plete verb. The corresponding imaginary token will 
be retrieved to be processed by rules. 
Recursive Stem Discovery Process: Since mul-
tiple inflections can be applied to a token, we de-
cided to use a stack and a recursive process to dis-
cover the inflections and the possible stems for a 
token. However, we do special processing for verb 
tokens, which cannot have more than two inflec-
tions attached at a time and require extra morpho-
logical rule processing. 
Ranking: Since there will be ambiguity, we de-
cided to capture all candidate stems discovered and 
rank them. The client of the system will be ex-
pected to pick up the highest ranked stem. 
Our observation was ? stems discovered by 
stripping a lengthier inflection are more likely to 
be correct. We decided to include the o-syllabic 
length of the inflection as a contributing factor in 
rank calculation. 
Additionally, for verb stems, the nature of the 5-
tuple rule will play a role. There is a degree of 
strictness associated with these rules. The strict-
ness is defined by the number of non-? elements 
in the 5-tuple. The stricter the rule, chances are 
more that the derived stem is accurate. 
Taking an example ? token ???? [kheye] can be 
derived from two rules: ?? [khaa] + ?? [ye] is de-
rived from (e, ?, ?, ?, ??) and ??? [khaay_]+ ?? [e] 
is derived from (?, ?, ?, ?, ??). Since rule (e, ?, 
?, ?, ??) is stricter, ?? should be the correct stem, 
and that matches with our knowledge also. 
Let ? be a token and ? is one of the candidate 
stem derived from inflection ?. 
For non-verb cases the rank of ? will be:  
R? ?=  
For verb, the strictness of the rule that generated 
? has to be considered. Let that rule be  
1 1( , , , , )n nL R L R i? =  
The strictness can be measured as the number of 
non-? elements in the 5-tuple. Element i always 
demands an exact match. Moreover, (L1, R1) and 
(Ln, Rn) always come in pair. Hence the strictness 
S? of rule ? can be calculated as  
1 n 
1 n
1 n 
1 n
1,  if L  L
2,  if L   L
2,  if L =   L
3,  if L   L
and
S
and
and
?
?
Hence for verb stems the rank of ? will be: 
R S? ??= +  
Overchanged Verb Stems and Compensation: 
Because of the rule strictness ranking some verb 
stems might be overchanged. As for example, to-
ken ??????? [bhejaalaam] is an inflected form of 
stem ???? [bhejaa]. This is a class III stem. There 
are two relevant rules ?1 = (?, ?, ?, ?, ???) and ?2 
= (e, i, ?, ?, ???) which identifies the candidate 
stems ???? and ???? [bhijaa] respectively. Since the 
?2 has higher strictness, ???? will rank better, which 
is wrong.  
This type of situation only happens if the ap-
plied rule satisfies following condition:  
(L1, R1) ? ((i, e), (e, i), (u, o), (o, u)). 
This effect comes because the verbs with first 
vowel of these pairs at first o-syllable exhibits 
morphologically similar behaviour with such verbs 
for the last vowel of the pair once inflected.  
???? and ???? are example of such behaviour. 
With inflection ???, both of them produce similar 
morphological structure (??????? [shekhaalaam] and 
???????) even though their morphology is different 
at their actual stem. 
To compensate that, we decided to include a 
stem to the result set without changing the first o-
syllable, with same calculated rank, once such rule 
is encountered. Going back to example of ???????, 
even though we identified ???? as the stem with 
highest rank, since ?2 satisfies the above condition, 
???? will be included with same rank as compensa-
tion.  
Dictionary: To reduce ambiguity further, we 
decided to introduce a stem dictionary, which will 
be compared with potential stems. If a match 
found, the rank of that stem will be increased with 
a higher degree, so that they can take precedence. 
Bengali word can have more than one correct 
spelling. As for example, ?n? [jan_ma] and ?n 
[janma] are both correct. Similarly, ???? ? [garjaa] 
and ?r ?? [gar_jaa], ?r?? [bar_shhaa] and ????? [bar-
shhaa] etc. 
To take care of the above problem, instead of 
exact match in the dictionary, we decided to intro-
duce a quantitative match measure, so that some 
tolerance threshold can be adopted during the 
search in the dictionary. 
 
 
? ?
? ?
? ?
= =?? ? =?= ? ??? ? ??
 
Edit-distance measure (Levenshtein, 1966) was 
a natural choice for this. However direct usage of 
69
this algorithm may not be useful because of the 
following. For any edit operation the cost is always 
calculated 1 in edit-distance algorithm. This may 
mislead while calculating the edit-distance of a 
pair of Bengali tokens. As for example: The edit-
distance for (?????, ?r ??) and (?????, ????? [barshaa]) 
pairs are same, which is 1. However, intuitively we 
know that ?r ?? should be closer to ????? than ?????.  
To address the above problem we propose that 
the edit cost for diacritic marks, halant and de-
pendent vowel marks should be less than that of 
consonants or independent vowels. Similarly, edit 
cost for diacritic marks and halant should be less 
than that of dependent vowel marks. 
Formally, let VO, CO, VS and DC be the set of 
vowels, consonants, dependent vowel signs and 
diacritic marks (including halant) in Bengali al-
phabet.  
We define the insertion cost Ci and deletion cost 
Cd of character ? as: 
1, if ( ) or ( )
0.5, if ( )
( ) ( )
0.25, if ( )
0, otherwise
i d
CO VO
VS
C C
DC
? ?
?? ? ?
? ??? ??= = ? ????
 
We also define the substitution cost Cs of char-
acter ?1 by character ?2 as: 
1 2
1 2
1 2
0, if ( )
( , )
( ( ), ( )), otherwises i i
C
Min C C
? ?? ? ? ?
=?= ??
 
We refer this modified distance measure as 
weighted edit-distance (WED) hereafter. 
Going back to the previous example, the WED 
between ????? and ????? is 1 and between ????? and ?r ?? 
is 0.25. This result matches our expectation. 
We proposed that the discovered stems will be 
compared against the dictionary items. If the WED 
is below the threshold value ?, we enhance the 
previous rank value of that stem.  
Let D = (w1, w2, ... wM) be the dictionary of size 
M. Let us define ?? for stem ? as below: 
1
( , ( ( , )))
M
kk
Min Min WED w?? ? ?==  
The modified rank of ? is: 
100( )
, if  is verb
100( )
,  otherwise
S
R
?
?
?
?
? ?? ?
? ?? ?
?? + +??= ? ?? +??
The match score is raised by a factor of 100 to 
emphasise the dictionary match and dampen the 
previous contributing ranking factors, which are 
typically in the range between 0 - 20. 
5 System Architecture 
The proposed system structure is provided below 
using Architecture Description Standard notation 
(Youngs et. al., 1999): 
?system?
POS Tagger
StemmingEngine
OrthosyllableHandler
?stack?
InflectionTracker
?set?
InflectionSets
?map?
PostinflectionPronouns
?map?
IncompleteVerbs
?set?
Lexicon
Stemmer system boundary
?table?
VerbRules
Figure 2: Stemmer Architecture 
The components of the system are briefly de-
scribed below: 
StemmingEngine: It receives a tagged token 
and produces a set of candidate stems with their 
assigned ranks and associated inflection. 
OrthosyllableHandler: This component is re-
sponsible for converting a token into o-syllables 
and vice-versa. It also allows calculating the WED 
between two Bengali tokens. 
InflectionTracker: While discovering the in-
flections recursively, this stack will help the 
Stemming Engine to keep track of the inflections 
discovered till now. 
InflectionSets: Contains the POS group specific 
inflection sets (IN, IP, IV, IJ and IR). 
PostinflectionPronouns: A map of post-
inflection pronoun stems against their correspond-
ing actual stem form. 
VerbRules: A table of 5-tuple verb rules along 
with their verb stem class association. 
?
 
IncompleteVerbs: A map of incomplete verb 
tokens against their formal imaginary forms. 
Lexicon: The dictionary where a discovered 
stem will be searched for rank enhancement. 
As presented, the above design is heavily de-
pendent on persisted rules, rather than hard-coded 
70
logic. This will bring in configurability and 
adaptability to the system for easily accommodat-
ing other dialects to be considered in future. 
The high level algorithm to be used by the Stem-
mingEngine is provided below: 
 
global stems; 
 
Stem(token, pos) { 
Search(token, pos); 
return stems; 
} 
 
Search(token, pos) { 
if (pos is verb and token ? IncompleteVerbs)  
  token ? IncompleteVerbs[token]; 
 
for (i = 1; i < token.length; i++) { 
  candidate ? first i characters of token;   
  inflection ? remaining characters of token; 
 
  if (inflection ? InflectionSets)  
    continue; 
 
  if (pos is verb) { 
    if (inflection is representing emphasis or negation) { 
      InflectionTracker.push(inflection); 
      Search(candidate, pos); 
      InflectionTracker.pop(inflection); 
    } 
 
    class ? verb stem class of candidate; 
     
    for each matching rule R in VerbRules for  
    candidate and class { 
        modify candidate by applying R;  
        a ? inflection + inflections in InflectionTracker; 
        r ? rank of the candidate based on |inflection|,  
        strictness of R and match in Lexicon;    
        Add candidate, a and r to stems; 
 
        if (R is an overchanging rule) 
          Modify candidate by compensation logic; 
          Add candidate, a and r to stems; 
    } // for each 
  } // if pos is verb 
  else { 
    a ? inflection + inflections in InflectionTracker; 
 
    if (pos is pronoun and  
    candidate ? Postinflection Pronouns) { 
      candidate ?  PostinflectionPronouns[candidate]; 
    } 
 
    r ? rank of the candidate based on |inflection|  
    and match in Lexicon;    
    Add candidate, a and r  to stems; 
 
    if (inflection != "") { 
      InflectionTracker.push(inflection); 
      Search(candidate, pos); 
      InflectionTracker.pop(inflection); 
    } 
  } // else 
} // for 
} 
6 Evaluation 
Based on the above mentioned approach and de-
sign, we developed a system using C#, XML 
and .NET Framework 2.0. We conducted the fol-
lowing experiment on it. 
The goal of our experiment was to calculate the 
level of accuracy the proposed stemmer system can 
achieve. Since the system can suggest more than 
one stems, we sorted the suggested stems based on 
ranking in descending order and picked up the first 
(s'i) and the next (s''i) stems. We compared these 
stems against truthed data and calculated the accu-
racy measures A' and A'' as below: 
Let T = (t1, t2, ... tN) be the set of tokens in a cor-
pus of size N, S = (?1, ?2, ... ?N) be the set of 
truthed stems for those tokens. Let s'i and s''i be the 
best and second-best stems suggested by the pro-
posed stemmer system for token ti. Then we define 
1
'( )
'
N
i
f i
A
N
==
?
, where  i i
1, if  = s'
'( )
0, otherwise
f i
??= ??
and 
1
''( )
''
N
i
f i
A
N
==
?
, where  i i
1, if   (s' , s'' )
''( )
0, otherwise
f i
? ??=??
i
                                                
A' and A'' will be closer to 1 as the system accu-
racy increases. 
Initially we ran it for three classic short stories 
by Rabindranath Tagore1. Since the proposed sys-
tem accuracy will also depend upon the accuracy 
of the POS tagger and the dictionary coverage, to 
rule these factors out we manually identified the 
POS of the test corpus to emulate a 100% accurate 
POS tagger and used an empty dictionary. Apart 
from calculating the individual accuracies, we also 
calculated overall accuracy by considering the 
three stories as a single corpus: 
 
 
1 i?????? ???? [i`ndurer bhoj], ??????o?? [denaapaaonaa], 
and ???????i??? ???????d?? [raamakaanaaiyer nirbuddhitaa] 
respectively
71
Corpus N A' A'' 
RT1 519 0.888 0.988
RT2 1865 0.904 0.987
RT3 1416 0.903 0.999
Overall 3800 0.902 0.992
Table 4: Accuracies for Short Stories by Tagore 
As shown above, while A'' is very good, A' is 
also quite satisfactory. We could not compare this 
result with other similar Bengali stemmer systems 
due to unavailability. The closest stemmer system 
we found is the Hindi stemmer by Ramanathan et. 
al. (2003). It did not use a POS tagger and was run 
on a different corpus. The recorded accuracy of 
that stemmer was 0.815.  
To check whether we can further improve on A', 
we introduced lexicon of 352 verb stems, ran it on 
the above three pieces with ? = 0.6 to tolerate 
only the changes in maatraa and diacritic mark. 
We calculated A' for verbs tokens only with and 
without lexicon scenarios. We received the follow-
ing result: 
0.969
0.997
0.955
0.973
0.957
0.907
0.9900.991
0.860
0.880
0.900
0.920
0.940
0.960
0.980
1.000
RT1 (Verb) RT2 (Verb) RT3 (Verb) Overall
A
cc
u
ra
cy
A' (w/o Lexicon) A' (w/ Lexicon)
 
Figure 3: Comparison of Accuracies with and 
without Verb Lexicon 
Above graph suggests that a lexicon can im-
prove the accuracy significantly. 
7 Conclusion  
This paper proposed a system and algorithm for 
stripping inflection suffixes from Bengali word 
tokens based on a rule-based approach. The con-
ducted experiments produced encouraging results.  
Currently, our work is limited to the traditional 
and standard colloquial dialects of Bengali. Future 
works can be carried out to include other dialects 
by including more inflections in the respective data 
structure of this system. 
The system suggests a set of ranked stems for a 
word token. The client of this system is expected to 
choose the highest ranked stem. This can be mis-
leading for some of the cases where tokens derived 
from different stems share low or zero edit-
distance among each other. As for example, when 
the verb token ???? can be derived from both ?? and 
??l, the system will suggest ?? over ??l.  
This problem can be addressed by taking hints 
from word sense disambiguation (WSD) compo-
nent as an input. Further studies can be devoted 
towards this idea. Moreover, a blend of rule-based 
and statistical approaches may be explored in fu-
ture to improve the resultant accuracy of the stem-
mer. 
While input from POS tagger helped to achieve 
a good performance of this system, it is yet to be 
studied how the system will perform without a 
POS tagger.  
References 
S. Chatterji. 1939. Bhasha-prakash Bangla Vyakaran. 
Rupa & Co. New Delhi, India 
M. F. Porter. 1980. An algorithm for suffix stripping. 
Program 14(3):130-137. 
U. Garain and A. K. Datta. 2005. An Approach for 
Stemming in Symbolically Compressed Indian Lan-
guage Imaged Documents. Proceedings of the 2005 
Eight International Conference on Document Analy-
sis and Recognition (ICDAR?05). IEEE Computer 
Society  
P. Majumder, M. Mitra, S. Parui, G. Kole, P. Mitra, and 
K. Datta. 2006. YASS: Yet Another Suffix Stripper. 
ACM Transactions on Information Systems.  
T. Brants . 2000. TnT: a statistical part-of-speech tag-
ger. Proceedings of the sixth conference on Applied 
natural language processing: 224-231. Morgan Kauf-
mann Publishers Inc.   San Francisco, CA, USA 
V. I. Levenshtein. 1966. Binary codes capable of cor-
recting deletion, insertions and reversals. Cybernet-
ics and Control Theory, 10:707-710.  
R. Youngs, D. Redmond-Pyle, P. Spaas, and E. Kahan. 
1999. A standard for architecture description. IBM 
System Journal 38(1). 
A. Ramanathan and D. D. Rao. 2003. A lightweight 
stemmer for hindi. In Proc. Workshop of Computa-
tional Linguistics for South Asian Languages -
Expanding Synergies with Europe, EACL-2003: 42?
48. Budapest, Hungary. 
72
Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 91?98,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Morphology Driven Manipuri POS Tagger 
 
 
Thoudam Doren Singh             Sivaji Bandyopadhyay 
      Computer Science Department       Computer Science & Engineering Department 
  St. Anthony?s College                 Jadavpur University 
              Shillong-793001, Meghalaya, India                     Kolkata ? 700 032, India 
        thoudam_doren@rediffmail.com                     sivaji_cse_ju@yahoo.com
 
 
Abstract 
 
A good POS tagger is a critical component 
of a machine translation system and other 
related NLP applications where an 
appropriate POS tag will be assigned to 
individual words in a collection of texts. 
There is not enough POS tagged corpus 
available in Manipuri language ruling out 
machine learning approaches for a POS 
tagger in the language. A morphology 
driven Manipuri POS tagger that uses three 
dictionaries containing root words, prefixes 
and suffixes has been designed and 
implemented using the affix information 
irrespective of the context of the words. We 
have tested the current POS tagger on 3784 
sentences containing 10917 unique words. 
The POS tagger demonstrated an accuracy 
of 69%. Among the incorrectly tagged 31% 
words, 23% were unknown words (includes 
9% named entities) and 8% known words 
were wrongly tagged.  
 
1 Introduction 
 
Manipuri (Meiteilon or Meiteiron) belongs to the 
Tibeto-Burman language family and is highly 
agglutinative in behavior, monosyllabic, influenced 
and enriched by the Indo-Aryan languages of 
Sanskrit origin and English. The affixes play the 
most important role in the structure of the language. 
A clear -cut demarcation between morphology and 
syntax is not possible. In Manipuri, words are formed 
in three processes called affixation, derivation and 
compounding (Thoudam, 2006). The majority of the 
roots found in the language are bound and the affixes 
are the determining factor of the class of the words in 
the language. Classification of words using the role 
of affix helps to implement the tagger for a resource 
poor language like Manipuri with high performance.  
There are many POS taggers developed using 
different techniques for many major languages such 
as transformation-based error-driven learning (Brill, 
1995), decision trees (Black et al, 1992), Markov 
model (Cutting et al, 1992), maximum entropy 
methods (Ratnaparkhi, 1996) etc for English. 
Decision trees are used to estimate marginal 
probabilities in a maximum entropy model for 
predicting the parts-of-speech of a word given the 
context in which it appears (Black et al, 1992). The 
rules in a rule-based system are usually difficult to 
construct and typically are not very robust (Brill, 
1992). Large tables of statistics are not needed for the 
rule-based tagger. In a stochastic tagger, tens of 
thousands of lines of statistical information are 
needed to capture the contextual information (Brill, 
1992). For a tagger to function as a practical 
component in a language processing system, a tagger 
must be robust, efficient, accurate, tunable and 
reusable (Cutting, 1992).  
 
2 Previous work on Manipuri POS tagger 
 
Morphology based POS tagging of some languages 
like Turkish (Oflazer and Kuruoz, 1994), Czech 
(Hajic, et al, 2001) has been tried out using a 
combination of hand-crafted rules and statistical 
learning. A Marathi rule based POS tagger used a 
technique called SRR (suffix replacement rule) 
(Burange et al, 2006) with considerable accuracy. A 
POS tagger for Hindi overcomes the handicap of 
annotated corpora scarcity by exploiting the rich 
morphology of the language (Singh et al, 2006). To 
the best of our knowledge, there is no record 
available of work done on a Manipuri POS tagger. A 
related work of word class and sentence type 
identification in a Manipuri Morphological Analyzer 
91
is found in (Thoudam and Bandyopadhyay, 2006) 
where the classification of few word categories and 
sentence type identification are discussed based on 
affix rules.  
 
3 Manipuri Morphemes 
 
There are free and bound roots in Manipuri. All the 
verb roots are bound roots. There are also a few 
bound noun roots, the interrogative and 
demonstrative pronoun roots. They cannot occur 
without some particle prefixed or suffixed to it. The 
bound root may form a compound by the addition of 
another root. The free roots are pure nouns, 
pronouns, time adverbials and some numerals. The 
bound roots are mostly verb roots although there are 
a few noun and other roots. The suffixes, which are 
attached to the nouns, derived nouns, to the 
adjectives in noun phrases including numerals, the 
case markers and the bound coordinators are the 
nominal suffixes. In Manipuri, the nominal suffixes 
are always attached to the numeral in a noun phrase 
and the noun cannot take the suffixes. Since 
numerals are considered as adjectives, the position 
occupied by the numerals in Manipuri may be 
regarded adjective position (Thoudam, 2006). There 
are a few prefixes in Manipuri. These prefixes are 
mostly attached to the verb roots. They can also be 
attached to the derived nouns and bound noun roots. 
There are also a few prefixes derived from the 
personal pronouns.  
In this agglutinative language the numbers of 
verbal suffixes are more than that of the nominal 
suffixes (Singh, 2000). New words are easily formed 
in Manipuri using morphological rules. Inflectional 
morphology is more productive than derivative 
morphology (Chelliah, 1997). There are 8 
inflectional (INFL) suffixes and 23 enclitics (ENC). 
There are 5 derivational prefixes out of which 2 are 
category changing and 3 are non-category changing. 
There are 31 non-category changing derivational 
suffixes and 2 category changing suffixes. The non-
category changing derivational suffixes may be 
divided into first level derivatives (1st LD) of 8 
suffixes, second level derivatives (2nd LD) of 16 
suffixes and third level derivatives (3rd LD) of 7 
suffixes.  Enclitics in Manipuri fall in six categories: 
determiners, case markers, the copula, mood 
markers, inclusive/exclusive and pragmatic peak 
markers and attitude markers. The categories are 
determined on the basis of position in the word 
(category 1 occurs before category 2, category 2 
occurs before category 3 and so on). 
 
4 Dictionaries  
 
Three different dictionaries namely prefix which 
contains prefix information, suffix which contains 
suffix information and root containing 2051 entries 
are used for the system. The format of root is 
<root><category>. 
A bilingual dictionary consisting of Manipuri 
word and its corresponding pronunciation, POS, 1st 
English (Eng1) word meaning, 2nd English (Eng2) 
word meaning (if any), 3rd English (Eng3) word 
meaning (if any), a Manipuri sentence or phrase 
using the word and corresponding English meaning 
has been developed based on the work of Manipuri to 
English Dictionary (Imoba, 2004). The bilingual 
parallel dictionary is used for testing POS tagger and 
later on will be used for EBMT system. The 
Manipuri sentences/phrases using a particular word 
are used as the input to the POS tagger thus enabling 
to sort out words with multiple meaning.   
 
5 Morphological analysis of Major Lexical 
categories  
 
The lexical categories in Manipuri can be of two 
types ? major and minor (Chelliah, 1997). Major 
lexical categories can be of two types, namely 
?actual? and ?potential?. The lexicon of actual lexical 
categories i.e., actual lexicon consists of an 
unordered list of roots and affixes and lexicalized 
forms. Each lexical entry in the actual lexicon 
consists of what lexical category it belongs to and 
what its meaning is. On the other hand, the output of 
the potential lexicon consists of words created 
through productive morphological processes. In the 
actual lexicon, roots may be bound or free. Nouns 
and verbs from the actual lexicon can be 
distinguished on formal grounds in that bound roots 
are verbs and free roots are nouns. In the potential 
lexicon, adjectives, adverbs and nominal forms can 
be derived from verb roots and stative verbs can be 
derived from noun roots. There are several instances 
where the words belonging to some class or category 
plays the role of some other category sometimes 
based on its position in the sentences (P.C. Thoudam, 
92
2006) Some of the generalized handcrafted rules to 
identify the lexical are given as below.  
 
5.1 Nouns 
 
Nouns can be distinguished from other lexical 
categories on morphological grounds. Unlike verbs, 
nouns can be suffixed by gender, number or case 
markers. Proper nouns and common nouns are free 
standing forms.  
The following is the list of word structure rules 
for nouns (Chelliah, 1997) 
N ? Root INFL (ENC) 
Root ? Root (2nd LD) 
Root ? Root (1st LD) 
Root ? (prefix) root (root) 
Figure 1 shows the general form of noun 
morphology in Manipuri. Examples of some 
singular/plural noun forms are listed in Table 1. 
 
Prono
minal 
prefix 
Root gende
r 
number Quant
ifier 
Cas
e 
Figure 1. General form of Noun Morphology 
 
Singular Form Plural Form 
=??J?E?  -Uchek (bird) =??J?E??`e -Ucheksing(birds) 
]   -Ma (He/She) ]?F??^  -Makhoy (they) 
]?  -Mi (man) ]??^?] -Mi-yaam (men) 
Table 1: Singular/Plural forms 
 
Although case markers are functionally 
inflectional, they exhibit the clitic like characteristic 
of docking at the edge of a phrase. The word 
structure of rules of verbs and nouns are identical 
except for the category of the word level node, the 
possible terminal elements of the derivational and 
inflectional categories and the lack of the third level 
nominal derivation. Two examples to demonstrate 
the noun morphology are given below:- 
]J??X?Y??`eX? (m?-ca-nu-pi-si?-n?) ?by his/her 
daughters? 
]J??X?Y??`eX? (m?-ca-nu-pa-si?- n?) ?by his/her sons? 
 
The ] -m? ?his/her? is the pronominal suffix and J?? -ca 
?child? is the noun root. The X? -nu ?human? is suffixed 
by Y? -pi to indicate a female human and Y??pa to 
indicate a male human. ?`e ?si? or ?F?+ -khoy or ?^?]?
yaam can be used to indicate plurality. -si? cannot be 
used with pronouns or proper nouns and -khoy 
cannot be used with nonhuman nouns. X? -n? meaning 
?by the? is the instrumental case marker.  
 
5.2 Pronouns 
 
The singular personal pronouns are B -?y ?I?, Xe -n?? 
?you? and ]? -ma ?he/she?. Possessive pronouns are 
formed through the suffixation of ?E? -ki ?genitive? on 
these personal pronouns.  Indefinite pronouns are 
also lexicalized forms that consists of a question 
word which may be followed by a? -su ?also? or the 
sequence E????? -kumb? composed of E??] ?kum, ?like?, 
?kind of? and [? ?b? ?nominalizer?. The strategy for 
creating relative clause in Manipuri is to place the 
relativized noun directly after a normalized clause; 
there is no relative pronoun to mark the relative 
clause. The determiner may occur either as an 
independent pronoun or encliticized on the noun 
phrase with no difference in meaning. The 
determiners ?a ?si ?proximate? and T?  ?tu ?distal? are 
stems that function as enclitics. ?a ?si indicated that 
the object or person being spoken of is near or 
currently seen or known to be near., even if not 
viewable by the speaker, or is currently the topic of 
conversation; T? ?-tu signifies something or someone 
not present at the time of speech or newly introduced 
in the conversation. Possessive pronominal prefix 
may be affixed to the root `? sa ?body? to form 
pronouns emphasizing that the subject of the verb is 
a particular person or thing and no one or nothing 
else: +`?X? isan? ?by myself? X`?X? n?san? ?by 
yourself? and ]`?X? m?san? ?by him/her/itself/.  The 
set of Manipuri Pronominal prefixes differ for 
different persons (+  {I} for 1st person, X {Na} for 2nd 
person and ]  {Ma} for 3rd person) while the set of 
pronominal suffixes differ only on gender (Y? ?pa for 
masculine gender,  Y? -Pi for feminine gender). 
 
5.3 Verbs 
 
Verbs roots are in the actual lexicon and are bound 
forms. A verb may be free standing word if it is 
minimally suffixed by an inflectional marker. The 
verb root may also be followed by one of the 
enclitics. Three derivational categories may 
optionally precede the final inflectional suffix. The 
1st LD suffixes signal adverbial meanings, the 2nd LD 
suffixes indicate evidentiality, the deitic reference of 
93
a verb, or the number of persons performing the 
action and the 3rd LD suffixes signal aspect and 
mood. Verb roots may also be used to form verbal 
nouns, adjectives and adverbs. Verbal nouns are 
formed through the suffixation of the nominalizer Y? 
?p? to the verb root. 
The following is the list of word structure rules 
for verbs (Chelliah, 1997) 
 
a. Verb ? Root INFL 
b. Root ? Root (3rd LD) 
c. Root ? Root (2nd LD) 
d. Root ? Root (1st LD) 
e. Root ? root (root) 
f. 3rd LD ? (mood1)(mood2)(aspect) 
g. 2nd LD ? (2nd LD1),(2nd LD2),(2nd LD3).. 
h. 1st LD ? 1st LD 
 
Derivat
ional 
Prefixa
tion 
Ro
ot 
1st 
Level 
derivati
on 
2nd 
level 
derivati
on 
3rd 
level 
deriv
ation 
Infle
ction 
Figure 2. General form of Verb Morphology 
 
There are 3 categories (mood1, mood2, and 
aspect) belonging to the third level derivational (3rd 
LD) markers. The general form of verb morphology 
is shown in figure 2. 
The sub-categorization frames of affixes will 
restrict that only nominal affixes occur with a noun 
and verbal affixes occur with a verb root. The 
derivational suffix order of the word ??J?E?F?+?[?h??X is 
given below:- 
 
??J?E?         F?+?            ?[?E?        E            ?X 
cek        ?khay                -r?k          -k?              -ni 
crack   -totally affect    -distal      -potential   ?copula 
 (1st LD)          (2nd LD)     (3rd LD)  
The ?[?E? -r?k  has allomorph _E?-l?k.  ?[?E? -r?k occurs 
after vowels while _E?-l?k occurs after consonants. 
Such allomorph is an example of orthographic 
change and it is taken care by the system by making 
individual entries into the dictionary.  
 
?J???[?E?A ?ca-r?k-y (ate there and came here)  
?J?????E?A ? cam-l?k-y (washed there and came here) 
 
The formation of verb can be of the form 
 
Verb stem + aspect/mood ? verb 
UE? -th?k (drink)  + ?_ -le- ? UE??_ th?kle (has drunk) 
 
The verbal noun is formed with the rule as given as 
 
Verb Stem + Nominalizer ? Verbal noun 
?U?e -thong (cook)+ [?? -ba  ? ?U?e[??? thongba  (to cook) 
  
5.4 Adjectives 
 
An adjective is derived through the affixation of the 
attributive, derivational prefix % ?- to a verbal noun.  
e.g. 
% -? + Verbal noun ? Adjective 
% -? + ?a -si (die) + [?? -ba?>%?a[?? ?siba (something 
dead) 
Adjectives may appear before or after the nouns 
they modify.  Possessive adjectives are formed 
through the suffixation of the genitive marker ?E? ?ki 
to the possessor of a noun.  
 
5.5 Adverbs 
 
Manner adverbs are formed through suffixation of X? 
?n? ?adverbial? to a verb root. e.g. ?_??^X?  loyn? 
?completely, all? from loy ?complete,finish?. e.g.,  
 
Stem  +   X? - na   ? Adverb 
E?Y -K?p (cry)+ X? - na  ?> E?YX? -k?p-na (cryingly) 
 
Locative adverbs are derived through the 
prefixation of ] m? ?noun marker? to a noun or verb 
roots. e.g. ]F? m?kha ?below, underneath? from  F?  
kha ?south? 
 
 
6 Morphological analyses of some minor 
lexical categories 
 
The three minor lexical categories of Manipuri are 
quantifiers, numerals and interjections. These are 
considered minor categories because these lexical 
items are closed sets which express meanings most 
often encoded by affixal morphology. The lexical 
items in interjection is defined on the semantic 
similarity of its members, all express strong emotion.  
 
 
 
 
94
6.1 Quantifiers 
 
Most quantifiers in Manipuri are lexicalized forms 
consisting of the unproductive prefix khV- (where the 
vowel can be a, i, u). These are F?[?? -kh?ra ?some? 
which indicates an indeterminate amount; ?FTe -
khit?? ?ever so little, a particle? of some tangible 
material. These quantifiers can be combined as in  
 
<?`e   F?[??     ?FT?e     Y??[?E?=. 
Ishi?   kh?ra    khit?? pur?k-u 
 ?Bring me just a little bit of water?. 
 
6.2 Numerals 
 
The numerals are nouns. Ordinal numerals are 
adjectives, derived through the affixation of the 
attributive prefix % ?? and the nominalizer [?? ?b? to 
any numeral with ? ?su ?also?: thus %?X?[?? ?nisub? 
?second one?. 
 
6.3 Interjections 
 
The lexical items of this category which is defined on 
the semantic similarity of its members, all express 
strong emotion. Some of these are composite forms 
where one syllable is identifiable as the exasperative 
enclitic ?c? ?he and the second syllable is not 
identifiable as a productive affix or stem. 
 
7 Manipuri Tagset  
 
The basic Manipuri POS tag set used in the POS 
tagger is listed below. E??y?? E??y?? kukru kukru (a 
pigeon?s cry) is ideophone. T?? tu ?that? is a 
determiner. c???^[??` haybasi is a determiner 
complementizer.  
 
Sl. No. Category 
name 
Tag 
1 adjective ADJ 
2 adverb ADV 
3 conjunction CONJ 
4 complementizer CMP 
5 determiner DET 
6 ideophone IDEO 
7 interjection INTJ 
8 noun N 
9 pronoun PN 
10 quantifier QU 
11 verb VB 
12 Verbal noun VN 
13 Unknown  UNK 
Table 2. Manipuri POS tagset 
 
8 Design of Manipuri POS tagger 
 
In Manipuri, the basic POS tags are assigned to the 
words on the basis of morphological rules. Figure 3 
shows the system diagram of Manipuri POS tagger.  
 
         Input sentence 
 
    
 
 
 
 
 
 
 
 
 
 
 
 
Lexical  
Rules 
 
              Tagged Output Sentence 
 
Figure 3. System Diagram 
 
The different parts involved in the system are:- 
a. Tokenizer: Words are separated based on 
the space given between consecutive words. 
b. Stemmer: It separates the prefixes and 
suffixes from the words. 
c. Engine: Different analysis and treatment of 
different words are performed based on the 
category. 
d. Tag Generator: Tags are assigned to the 
words in the sentence input based on the 
tagset and morphology rules. 
e. Dictionaries:  Prefix, suffix and word 
dictionary along with sentences using the 
words are maintained.  
 
 
Tokenizer Stemmer 
                      
 
Engine 
Major 
Lexical 
Category 
Module 
Minor 
Lexical 
Category 
module 
Dictionaries
   Tag Generator 
95
8.1 Algorithm of POS tagging 
 
Algorithm used for tagging is as follows:- 
 
1. Input the Manipuri input texts to the 
Tokenizer. 
2. Repeat steps 3 to 6 until the end of the texts 
for each token. 
3. Feed the tokens to the stemmer. 
4. Check the patterns and order of the different 
morphemes by looking at the stem category. 
5. Apply the handcrafted morphological rules 
for identifying the category using the engine. 
6. Generate the POS tags using Tag generator. 
7. End. 
The Visual C++, MsAccess and GIST SDK are 
used to develop the system. The Manipuri words are 
entered into the dictionary using Bengali script (BN1 
TTBidisha font).  
 
9 Evaluation 
 
In Manipuri, word category is not so distinct except 
Noun. The verbs are also under bound category. 
Another problem is to classify basic root forms 
according to word class although the distinction 
between the noun class and verb classes is relatively 
clear, the distinction between nouns and adjectives is 
often vague. Distinction between a noun and an 
adverb becomes unclear because structurally a word 
may be a noun but contextually it is adverb. Thus, 
the assumption made for word categories are 
depending upon the root category and affix 
information available from the dictionaries. At the 
moment, we use a sequential search of a stem from 
the root dictionary in alphabetical order. It is found 
to be suitable for small size dictionary. Further a part 
of root may also be a prefix which leads to wrong 
tagging. The verb morphology is more complex than 
that of noun. A comparative study on the number of 
words tagged by the system and manually tagged had 
been carried out. The inputs of 3784 Manipuri 
sentences of 10917 unique words as input to the 
tagger engine. Sometimes two words get fused to 
form a complete word. Handling such collocations is 
difficult. Conjuncts require a separate dealing using a 
table. Verbs, nouns and noun phrases, subordinate 
sentences, and root sentences can be affixed by 
enclitics. Table 4 shows the percentage statistics of 
tagging output based on the actual and correctly 
tagged words. The accuracy of tagging can be further 
improved by populating more root morphemes to the 
root dictionary.  
             No. of single correct tags 
Accuracy percentage=            X 100 
    Total no. of tokens 
 
Group Types Percentage 
Single tagged correct words 65% 
Multiple tagged correct words  4% 
Unknown words 23% ( 9% 
Named Entities) 
Wrong tagged words 8% 
Table 4. Tagger output statistics 
 
The unknown words are the words which could not 
be tagged based on the linguistic rules and 
unavailability of entries mainly in root dictionary. In 
the process of word formation, only affixation: 
prefixing, suffixing or compounding takes the role of 
formation of new words in this language. Due to the 
fact that new words are easily formed in Manipuri, 
thus the number of unknown words (out of 
vocabulary) is relatively large (Sirajul et al, 2004). 
  
10 Challenges for future work 
 
The noun group words handling are not incorporated. 
For example %F?E? %?[??C (pronounced as ?khak ?raw) 
meaning thunderbolt, %I?] %?[??+ FI?V[?? (pronounced as 
??am ?ray kh??d?ba) meaning wanton are noun 
group words and are not tagged by the POS tagger 
correctly. The Noun-Adjective ambiguity 
disambiguation scheme is required as a separate 
module and implementations are to be included in the 
future work. The Manipuri tagging is very much 
dependent on the morphological analysis and lexical 
rules of each category. There is a cleaning process of 
all word and morphemes specially the spelling to 
ensure that the lexical rules are implemented. This 
has not yet been implemented. Collocations handling 
and more disambiguation rules will be developed in 
further phases of the work. The output of the POS 
tagger will be used in a Manipuri-English machine 
translation system.  
  
References 
 
E. Black, F. Jelinek, J. Lafferty, R. Mercer and S. Roukos. 
1992. Decision tree models applied to labeling of texts 
96
with parts of speech. In DARPA Workshop on Speech 
and Natural Language. San Mateo, CA, 1992, Morgan 
Kaufman. 
 
Eric Brill. 1992. A simple rule-based part of speech 
tagger. In Proceedings Third Conference on Applied 
Natural Language Processing, ACL, Trento, Italy. 
 
Eric Brill. 1995. Transformation-Based Error Driven 
Learning and Natural Language Processing: A case 
study in Parts-Of-Speech tagging. Computational 
Linguistics 21(94): pp 543-566. 
 
Sachin Burange, Sushant Devlakar, Pushpak 
Bhattacharyya. 2006. Rule Governed Marathi POS 
Tagging. In Proceeding of  MSPIL, IIT Bombay, pp 69-
78. 
 
Shobhana L. Chelliah. 1997. A Grammar of Meithei. 
Mouton de  Gruyter, Berlin, pp 77-92. 
 
Sirajul Islam Choudhury, Leihaorambam Sarbajit Singh, 
Samir Borgohain, P.K. Das. 2004. Morphological 
Analyzer for Manipuri: Design and Implementation. In 
Proceedings of AACC, Kathmandu, Nepal, pp 123-129. 
 
D. Cutting. 1992. A practical part-of-speech tagger. In 
Proceeding of third conference on Applied Natural 
Language Processing. ACL, 1992. pp 133-140. 
 
J. Hajic, P. Krbec, P. Kveton, K. Oliva, V.Petkevic, 2001. 
A Case Study in Czech Tagging. In proceedings of the 
39th Annual Meeting of the ACL. 
 
S. Imoba. 2004. Manipuri to English Dictionary.  S. 
Ibetombi Devi, Imphal. 
 
K. Oflazer, I Kuruoz. 1994. Tagging and morphological 
disambiguation of Turkish text. In Proceedings of 4th 
ACL conference on Applied Natural Language 
Processing Conference. 
 
A. Ratnaparakhi. 1996. A maximum entropy Parts-Of-
Speech Tagger. In Proceedings EMNLP-ACL. pp 133-
142. 
 
Smriti Singh, Kuhoo Gupta, Manish Shrivastava, Pushpak 
Bhattacharya. 2006. Morphological Richness offsets 
Resource Demand ? Experiences in constructing a POS 
tagger for Hindi. In Proceedings of COLING-ACL, 
Sydney, Australia. 
 
Ch. Yashawanta Singh. 2000. Manipuri Grammar. Rajesh 
Publications, New Delhi. 
 
P.C. Thoudam. 2006. Problems in the Analysis of 
Manipuri Language. www.ciil-ebooks.net, CIIL, 
Mysore. 
 
D. S. Thoudam and S. Bandyopadhyay. 2006. Word Class 
and Sentence Type Identification in Manipuri 
Morphological Analyzer.  In Proceedings of MSPIL, 
IIT Bombay, pp 11-17. 
97
 98
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 3?4,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Multilingual Named Entity Recognition  
Sivaji Bandyopadhyay 
Computer Science & Engineering Department  
Jadavpur University  
Kolkata, INDIA. 
sbandyopadhyay@cse.jdvu.ac.in 
 
 
 
 
Abstract 
The computational research aiming at automatically identifying named entities (NE) in texts forms a vast 
and heterogeneous pool of strategies, techniques and representations from hand-crafted rules towards ma-
chine learning approaches. Hand-crafted rule based systems provide good performance at a relatively high 
system engineering cost. The availability of a large collection of annotated data is the prerequisite for us-
ing supervised learning techniques. Semi-supervised and unsupervised learning techniques promise fast 
deployment for many NE types without the prerequisite of an annotated corpus. The main technique for 
semi-supervised learning is called bootstrapping and involves a small degree of supervision, such as a set 
of seeds, for starting the learning process. The typical approach in unsupervised learning is clustering 
where systems can try to gather NEs from clustered groups based on the similarity of context. The tech-
niques rely on lexical resources (e.g., Wordnet), on lexical patterns and on statistics computed on a large 
unannotated corpus.  
In multilingual named entity recognition (NER), it must be possible to use the same method for many 
different languages and the extension to new languages must be easy and fast. Person names can be rec-
ognized in text through a lookup procedure, by analyzing the local lexical context, by looking at part of a 
sequence of candidate words that is a known name component etc. Some organization names can be iden-
tified by looking at contain organization-specific candidate words. Identification of place names necessar-
ily involves lookup against a gazetteer, as most context markers are too weak and ambiguous.  
An important feature in multilingual person name detection is that the same person can be referred to by 
different name variants. The main reasons for these variations are: the reuse of name parts to avoid repeti-
tion, morphological variants such as the added suffixes, spelling mistakes, adaptation of names to local 
spelling rules, transliteration differences due to different transliteration rules or different target languages 
etc.. Name variants can be found within the same language documents.  
The major challenges for looking up place names in a multilingual gazetteer are the following: place 
names are frequently homographic with common words or with person names, presence of a number of 
exonyms (foreign language equivalences), endonyms (local variants) and historical variants for many 
place names etc..  
Application of NER to multilingual document sets helps to find more and more accurate informa-tion 
on each NE, while at the same time rich in-formation about NEs is helpful and can even be a crucial 
ingredient for text analysis applications that cross the language barrier.  
3
 4
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 33?40,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
 
Language Independent Named Entity Recognition in  Indian Languages  
Asif Ekbal, Rejwanul Haque, Amitava Das, Venkateswarlu Poka 
and Sivaji Bandyopadhyay 
 Department of Computer Science and Engineering 
Jadavpur University 
Kolkata-700032, India 
asif.ekbal@gmail.com, rejwanul@gmail.com, 
amit_santu_kuntal@yahoo.com, venkat.ju@gmail.com and 
sivaji_cse_ju@yahoo.com 
            
Abstract 
This paper reports about the development 
of a Named Entity Recognition (NER) sys-
tem for South and South East Asian lan-
guages, particularly for Bengali, Hindi, Te-
lugu, Oriya and Urdu as part of the 
IJCNLP-08 NER Shared Task
1
. We have 
used the statistical Conditional Random 
Fields (CRFs). The system makes use of 
the different contextual information of the 
words along with the variety of features 
that are helpful in predicting the various 
named entity (NE) classes. The system uses 
both the language independent as well as 
language dependent features. The language 
independent features are applicable for all 
the languages. The language dependent 
features have been used for Bengali and 
Hindi only. One of the difficult tasks of 
IJCNLP-08 NER Shared task was to iden-
tify the nested named entities (NEs) though 
only the type of the maximal NEs were 
given. To identify nested NEs, we have 
used rules that are applicable for all the 
five languages. In addition to these rules, 
gazetteer lists have been used for Bengali 
and Hindi. The system has been trained 
with Bengali (122,467 tokens), Hindi 
(502,974 tokens), Telugu (64,026 tokens), 
Oriya (93,173 tokens) and Urdu (35,447 
tokens) data. The system has been tested 
with the 30,505 tokens of Bengali, 38,708 
tokens of Hindi, 6,356 tokens of Telugu, 
                                                
1
http://ltrc.iiit.ac.in/ner-ssea-08  
24,640 tokens of Oriya and 3,782 tokens of 
Urdu. Evaluation results have demonstrated 
the highest maximal F-measure of 53.36%, 
nested F-measure of 53.46% and lexical F-
measure of 59.39% for Bengali.   
1 Introduction 
Named Entity Recognition (NER) is an impor-
tant tool in almost all Natural Language Proc-
essing (NLP) application areas. Proper identifi-
cation and classification of named entities are 
very crucial and pose a very big challenge to 
the NLP researchers. The level of ambiguity in 
named entity recognition (NER) makes it diffi-
cult to attain human performance.     
NER has drawn more and more attention 
from the named entity (NE) tasks (Chinchor 
95; Chinchor 98) in Message Understanding 
Conferences (MUCs) [MUC6; MUC7]. The 
problem of correct identification of named enti-
ties is specifically addressed and benchmarked 
by the developers of Information Extraction 
System, such as the GATE system (Cunning-
ham, 2001). NER also finds application in 
question-answering systems (Maldovan et al, 
2002) and machine translation (Babych and 
Hartley, 2003).  
The current trend in NER is to use the ma-
chine-learning approach, which is more attrac-
tive in that it is trainable and adoptable and the 
maintenance of a machine-learning system is 
much cheaper than that of a rule-based one. 
The representative machine-learning ap-
proaches used in NER are HMM (BBN?s Iden-
tiFinder in (Bikel, 1999)), Maximum Entropy 
33
(New York University?s MENE in (Borthwick, 
1999)), Decision Tree (New York University?s 
system in (Sekine 1998), SRA?s system in 
(Bennet, 1997) and Conditional Random Fields 
(CRFs) (Lafferty et al, 2001; McCallum and 
Li, 2003).       
There is no concept of capitalization in Indian 
languages (ILs) like English and this fact makes 
the NER task more difficult and challenging in 
ILs. There has been very little work in the area of 
NER in Indian languages. In Indian languages par-
ticularly in Bengali, the work in NER can be found 
in (Ekbal and Bandyopadhyay, 2007a) and  (Ekbal 
and Bandyopadhyay, 2007b). These two systems 
are based on the pattern directed shallow parsing 
approach. An HMM-based NER in Bengali can be 
found in (Ekbal et al, 2007c). Other than Bengali, 
the work on NER can be found in (Li and 
McCallum, 2004) for Hindi. This system is based 
on CRF.  
In this paper, we have reported a named entity 
recognition system for the south and south east 
Asian languages, particularly for Bengali, Hindi, 
Telugu, Oriya and Urdu. Bengali is the seventh 
popular language in the world, second in India and 
the national language of Bangladesh. Hindi is the 
third popular language in the world and the na-
tional language of India. Telugu is one of the popu-
lar languages and predominantly spoken in the 
southern part of India. Oriya and Urdu are the 
other two popular languages of India and widely 
used in the eastern and the northern part, respec-
tively. The statistical Conditional Random Field 
(CRF) model has been used to develop the system, 
as it is more efficient than HMM to deal with the 
non-independent and diverse overlapping features 
of the highly inflective Indian languages. We have 
used a fine-grained named entity tagset
2
, defined as 
part of the IJCNLP-08 NER Shared Task for 
SSEA. The system makes use of the different con-
textual information of the words along with the 
variety of orthographic word level features that are 
helpful in predicting the various named entity 
classes. In this work, we have considered language 
independent features as well as the language de-
pendent features. Language independent features 
include the contextual words, prefix and suffix in-
formation of all the words in the training corpus, 
several digit features depending upon the presence 
                                                
 
2
http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=3  
and/or the number of digits in a token and the fre-
quency features of the words. The system consid-
ers linguistic features particularly for Bengali and 
Hindi. Linguistic features of Bengali include the 
set of known suffixes that may appear with named 
entities, clue words that help in predicating the lo-
cation and organization names, words that help to 
recognize measurement expressions, designation 
words that help in identifying person names, the 
various gazetteer lists like the first names, middle 
names, last names, location names and organiza-
tion names. As part of linguistic features for Hindi, 
the system uses only the lists of first names, middle 
names and last names along with the list of words 
that helps to recognize measurements. No linguis-
tic features have been considered for Telugu, Oriya 
and Urdu. It has been observed from the evaluation 
results that the use of linguistic features improves 
the performance of the system. A number of ex-
periments have been carried out to find out the 
best-suited set of features for named entity recog-
nition in Bengali, Hindi, Telugu, Oriya and Urdu.  
2 Conditional Random Fields 
Conditional Random Fields (CRFs) (Lafferty et al, 
2001) are undirected graphical models, a special 
case of which corresponds to conditionally trained 
probabilistic finite state automata. Being 
conditionally trained, these CRFs can easily 
incorporate a large number of arbitrary, non-
independent features while still having efficient 
procedures for non-greedy finite-state inference 
and training. CRFs have shown success in various 
sequence modeling tasks including noun phrase 
segmentation (Sha and Pereira, 2003) and table 
extraction (Pinto et al, 2003).     
CRFs are used to calculate the conditional 
probability of values on designated output nodes 
given values on other designated input nodes. The 
conditional probability of a state sequence 
1, 2, , TS s s s given an observation 
sequence 1 2,, ....., )TO o o o  is calculated as: 
1 ,
1
1
( | ) exp( ( , , )),
T
k k t t
o
t k
P s o f s s o t
Z
where
1 ,( , , )k t tf s s o t is a feature function whose weight 
k is to be learned via training. The values of the 
feature functions may range between ..... , 
but typically they are binary. To make all 
34
conditional probabilities sum up to 1, we must 
calculate the normalization 
factor, 0 1 ,
1
exp( ( , , ))
T
s k k t t
t k
Z f s s o t , 
which, as in HMMs, can be obtained efficiently by 
dynamic programming. 
To train a CRF, the objective function to be 
maximized is the penalized log-likelihood of the 
state sequences given observation sequences: 
2
( ) ( )
2
1
log( ( | ))
2
N
i i
k
i k
L P s o , 
where, {
( ) ( )
,
i i
o s } is the labeled training 
data. The second sum corresponds to a zero-mean,  
2
-variance Gaussaian prior over parameters, 
which facilitates optimization by making the like-
lihood surface strictly convex. Here, we set pa-
rameters 
 
to maximize the penalized log-
likelihood using Limited-memory BFGS (Sha and 
Pereira, 2003), a quasi-Newton method that is sig-
nificantly more efficient, and which results in only 
minor changes in accuracy due to changes in .  
When applying CRFs to the named entity 
recognition problem, an obsevation sequence is a 
token of a sentence or document of text and the 
state sequence is its corresponding label sequence. 
While CRFs generally can use real-valued 
functions, in our experiments maximum of the 
features are binary. A feature function 
1 ,( , , )k t tf s s o t has a value of 0 for most cases and 
is only set to be 1, when 1,t ts s are certain states 
and the observation has certain properties. We 
have used the C
++ 
based OpenNLP CRF++ pack-
age
3
, a simple, customizable, and open source im-
plementation of Conditional Random Fields 
(CRFs) for segmenting /labeling sequential data. 
3 Named Entity Recognition in Indian 
Languages 
Named Entity Recognition in Indian languages 
(ILs) is difficult and challenging as capitalization 
is not a clue in ILs. The training data were pro-
vided for five different Indian languages, namely 
Bengali, Hindi, Telugu, Oriya and Urdu in Shakti 
Standard Format
4
. The training data in all the lan-
                                                
3
http://crfpp.sourceforge.net  
4
http://shiva.iiit.ac.in/SPSAL 2007/ssf.html  
guages were annotated with the twelve NE tags, as 
defined for the IJCNLP-08 NER shared task taget
5
. 
Only the maximal named entities and not the inter-
nal structures of the entities were annotated in the 
training data. For example, mahatma gandhi road 
was annotated as location and assigned the tag 
?NEL? even if mahatma and gandhi  are named 
entity title person (NETP) and person name (NEP) 
respectively, according to the IJCNLP-08 shared 
task tagset. These internal structures of the entities 
were to be identified during testing. So, mahatma 
gandhi road will be tagged as mahatma /NETP 
gandhi/NEP road/NEL. The structure of the tagged 
element using the SSF form will be as follows:  
1 (( NP <ne=NEL>  
1.1 (( NP <ne=NEP> 
1.1.1 (( NP  <ne=NETP> 
1.1.1.1 mahatma 
)) 
1.1.2 gandhi 
)) 
1.2 road 
)) 
3.1 Training Data Preparation for CRF  
Training data for all the languages required some 
preprocessing in order to use in the Conditional 
Random Field framework. The training data is 
searched for the multiword NEs. Each component 
of the multiword NE is searched in the training set 
to find whether it occurs as a single-word NE. The 
constituent components are then replaced by their 
NE tags (NE type of the single-word NE). For ex-
ample, mahatma gandhi road/NEL will be tagged 
as mahatma/NETP gandhi/NEP road/NEL if the 
internal components are found to appear with these 
NE tags in the training set. Each component of a 
multiword NE is also checked whether the compo-
nent is made up of digits only. If a component is 
made up digits only, then it is assigned the tag 
?NEN?. Various gazetteers for Bengali and Hindi 
have been also used in order to identify the internal 
structure of the NEs properly.  The list of gazet-
teers, which have been used in preparing the train-
ing data, is shown in Table 1. 
The individual components (not occurring as a 
single-word NE in the training data) of a multi-
word NE are searched in the gazetteer lists and 
                                                
5
http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=3  
35
assigned the appropriate NE tags. Other than NEs 
are marked with the NNE tags. The procedure is 
given below:  
Gazetteer list Number of entries 
First person name in Ben-
gali 
27,842 
Last person name in Ben-
gali 
5,288 
Middle name in Bengali 1,491 
Person name designation 
in Bengali 
947 
Location name in Bengali 7,870 
First person name in Hindi 1,62,881 
Last person name in Hindi 3,573 
Middle name in Hindi 450 
Cardinals in Bengali, 
Hindi and Telugu 
100 
Ordinals in Bengali, Hindi 
and Telugu 
65 
Month names in Bengali, 
Hindi and Telugu 
24 
Weekdays in Bengali, 
Hindi and Telugu 
14 
Words that denote meas-
urement in Bengali, Hindi 
and Telugu 
52 
Table 1. Gazetteer lists used during training data 
preparation  
 Step 1: Search the multiword NE in the training 
data 
Step 2: Extract each component from the mult-
word NE. 
Step 3: Check whether the constituent individual 
component (except the last one) appears in the 
training data as a single-word NE. 
Step 4: If the constituent NE appears in the training 
data as a single-word NE then 
Step 4.1: Assign the NE tag, extracted from the 
single-word NE, to the component of the multi-
word NE. 
else 
Step 4.2: Search the component in the gazetteer 
lists and assign the appropriate NE tag. 
Step 4.2.1: If the component is not found to appear 
in the gazetteer list then assign the NE tag of the 
maximal NE to the individual component.  
For example, if mahatma gandhi road is tagged 
as NEL, i.e., mahatma gandhi road/NEL then each 
component except the last one (road ) of this mult-
word NE is searched in the training set to look for 
it?s appearance (Step 3). Gazetteer lists are 
searched in case the component is not found in the 
training set (Step 4.2). If the components are found 
either in the training set or in the gazetteer list, 
then mahatma gandhi road/NEL will be tagged as: 
mahatma/NETP gandhi/NEP road/NEL. 
3.2 Named Entity Features 
Feature selection plays a crucial role in CRF 
framework. Experiments were carried out to find 
out most suitable features for NE tagging task. The 
main features for the NER task have been identi-
fied based on the different possible combination of 
available word and tag context. The features also 
include prefix and suffix for all words. The term 
prefix/suffix is a sequence of first/last few charac-
ters of a word, which may not be a linguistically 
meaningful prefix/suffix. The use of prefix/suffix 
information works well for highly inflected lan-
guages like the Indian languages. In addition, vari-
ous gazetteer lists have been developed to use in 
the NER task particularly for Bengali and Hindi. 
We have considered different combination from 
the following set for inspecting the best feature set 
for the NER task: 
 F={
1 1
,..., , , ,...,
i m i i i i n
w w w w w , |prefix| n, 
|suffix| n, previous NE tag, POS tags, First word, 
Digit information, Gazetteer lists} 
     Following is the details of the set of features 
that were applied to the NER task: 
 
Context word feature: Previous and next words 
of a particular word might be used as a feature. We 
have considered the word window of size five, i.e., 
previous and next two words from the current word 
for all the languages.  
Word suffix: Word suffix information is helpful 
to identify NEs. A fixed length word suffix of the 
current and surrounding words might be treated as 
feature. In this work, suffixes of length up to three 
the current word have been considered for all the 
languages. More helpful approach is to modify the 
feature as binary feature. Variable length suffixes 
of a word can be matched with predefined lists of 
useful suffixes for different classes of NEs. For 
Bengali, we have considered the different suffixes 
that may be particularly helpful in detecting person 
(e.g., -babu, -da, -di etc.). 
36
Word prefix: Prefix information of a word is also 
helpful. A fixed length prefix of the current and the 
surrounding words might be treated as features. 
Here, the prefixes of length up to three have been 
considered for all the language. 
Rare word: The lists of most frequently occurring 
words in the training sets have been calculated for 
all the five languages. The words that occur more 
than 10 times are considered as the frequently oc-
curring words in Bengali and Hindi. For Telugu, 
Oriya and Urdu, the cutoff frequency was chosen 
to be 5. Now, a binary feature ?RareWord? is de-
fined as: If current word is found to appear in the 
frequent word list then it is set to 1; otherwise, set 
to 0.   
First word: If the current token is the first word of 
a sentence, then this feature is set to 1. Otherwise, 
it is set to 0. 
Contains digit: For a token, if it contains digit(s) 
then the feature ?ContainsDigit? is set to 1. This 
feature is helpful for identifying the numbers.  
Made up of four digits: For a token if all the char-
acters are digits and having 4 digits then the fea-
ture ?FourDigit? is set to 1. This is helpful in iden-
tifying the time (e.g., 2007sal) and numerical (e.g., 
2007) expressions. 
Made up of two digits: For a token if all the char-
acters are digits and having 2 digits then the fea-
ture ?TwoDigit? is set to 1. This is helpful for iden-
tifying the time expressions (e.g., 12 ta, 8 am, 9 pm) 
in general. 
Contains digits and comma: For a token, if it con-
tains digits and commas then the feature ?Con-
tainsDigitsAndComma? is set to 1. This feature is 
helpful in identifying named entity measurement 
expressions (e.g., 120,45,330 taka) and numerical 
numbers (e.g., 120,45,330) 
Contains digits and slash: If the token contains 
digits and slash then the feature ?ContainsDigi-
tAndslash? is set to 1. This helps in identifying 
time expressions (e.g., 15/8/2007). 
Contains digits and hyphen: If the token contains 
digits and hyphen then the feature ?ContainsDigit-
sAndHyphen? is set to 1. This is helpful for the 
identification of time expressions (e.g., 15-8-2007). 
Contains digits and period: If the token contains 
digits and periods then the feature ?ContainsDigit-
sAndPeriod? is set to 1. This helps to recognize 
numerical quantities (e.g., 120453.35) and meas-
urements (e.g., 120453.35 taka). 
Contains digits and percentage: If the token con-
tains digits and percentage symbol then the feature 
?ContainsDigitsAndPercentage? is set to 1. This 
helps to recognize measurements (e.g., 120%). 
Named Entity Information: The NE tag of the 
previous word is also considered as the feature, i.e., 
the combination of the current and the previous 
output token has been considered. This is the only 
dynamic feature in the experiment. 
Gazetteer Lists: Various gazetteer lists have been 
created from a tagged Bengali news corpus (Ekbal 
and Bandyopadhyay, 2007d) for Bengali. The first, 
last and middle names of person for Hindi have 
been created from the election commission data
6
. 
The person name collections had to be processed in 
order to use it in the CRF framework. The simplest 
approach of using these gazetteers is to compare 
the current word with the lists and make decisions. 
But this approach is not good, as it can?t resolve 
ambiguity. So, it is better to use these lists as the 
features of the CRF. If the current token is in a par-
ticular list, then the corresponding feature is set to 
1 for the current/previous/next token; otherwise, 
set to 0. The list of gazetteers is shown in Table 2. 
3.3 Nested Named Entity Identification 
One of the important tasks of the IJCNLP-NER 
shared task was to identify the internal named enti-
ties within the maximal NEs. In the training data, 
only the type of the maximal NEs were given. In 
order to identify the internal NEs during testing, 
we have defined some rules. After testing the un-
annotated test data with the CRF based NER sys-
tem, it is searched to find the sequence of NE tags. 
The last NE tag in the sequence is assigned as the 
NE tag of the maximal NE. The NE tags of the 
constituent NEs may either be changed or may not 
be changed. The NE tags are changed with the help 
of rules and various gazetteer lists. We identified 
NEM (Named entity measurement), NETI (Named 
entity time expressions), NEO (Named entity or-
ganization names), NEP (Named entity person 
names) and NEL (Named entity locations) to be 
the potential NE tags, where nesting could occur. 
A NEM expression may contain NEN, an NETI 
may contain NEN, an NEO may contain NEP/ 
NEL, an NEL may contain NEP/NETP/NED and 
an NEP may contain NEL expressions. The nested 
                                                
 
6 
http://www.eci.gov.in/DevForum/Fullname.asp 
37
NEN tags could be identified by simply checking 
whether it contains digits only and checking the 
lists of cardinal and ordinal numbers.  
  Gazetteer  Number 
of entries
 
 Feature Descrip-
tions 
Designation 
words in Bengali 
947 ?Designation? set to 
1, otherwise 0  
Organization 
names in Bengali 
2, 225 ?Organization? set 
to 1, otherwise 0. 
Organization 
suffixes in Ben-
gali 
94 ?OrgSuffix? set to 
1, otherwise 0 
Person prefix for 
Bengali 
245 ?PersonPrefix? set 
to 1, otherwise set 
to 0 
First person 
names in Bengali
27,842 ?FirstName? set to 
1, otherwise 0 
Middle names in 
Bengali 
1,491 ?MiddleName? set 
to 1, otherwise 0 
Surnames in 
Bengali 
5,288 ?SurName? set to 1, 
otherwise 0 
Common loca-
tion word in 
Bengali 
75 ?CommonLocation? 
set 1, otherwise 0 
Action verb in 
Bengali 
215 ?ActionVerb? set to 
1, otherwise 0 
First person 
names in Hindi 
1,62,881 ?FirstName? set to 
1, otherwise 0 
Middle person 
names in Hindi 
450 ?MiddleName? set 
to 1, otherwise 0 
Last person 
names in Hindi 
3,573 ?SurName? set to 1, 
otherwise 0 
Location names 
in Bengali 
7,870 ?LocationName? 
set to 1, otherwise 
0 
Week days in 
Bengali, Hindi 
and Telugu 
14 ?WeekDay? set to 
1, otherwise 0 
Month names in 
Bengali, Hindi 
and Telugu 
24 ?MonthName? set 
to 1, otherwise 0 
Measurements in 
Bengali, Hindi 
and Telugu 
52 ?Measurement? set 
to 1, otherwise 0. 
 Table 2. Named entity gazetteer list   
The procedure for identifying the nested NEs are 
shown below: 
Step1: Test the unannotated test set. 
Step 2: Look for the sequence of NE tags. 
Step 3: All the words in the sequence will belong     
to a maximal NE. 
Step 4: Assign the last NE tag in the sequence to 
the maximal NE. 
Step 5: The test set is searched to look whether 
each component word appears with a NE tag.  
Step 6: Assign the particular NE tag to the compo-
nent if it appears in the test set with that NE tag. 
Otherwise, search the gazetteer lists as shown in 
Tables 1-2 to assign the tag. 
4 Evaluation 
The evaluation measures used for all the five lan-
guages are precision, recall and F-measure. These 
measures are calculated in three different ways: 
(i). Maximal matches: The largest possibles 
named entities are matched with the reference data. 
(ii). Nested matches: The largest possible as 
well as nested named entities are matched. 
(iii). Maximal lexical item matches: The lexical 
items inside the largest possible named entities are 
matched. 
(iv). Nested lexical item matches: The lexical 
items inside the largest possible as well as nested 
named entities are matched.  
5 Experimental Results 
The CRF based NER system has been trained and 
tested with five different Indian languages namely, 
Bengali, Hindi, Telugu, Oriya and Urdu data.  The 
training and test sets statistics are presented in Ta-
ble 3. Results of evaluation as explained in the 
previous section are shown in Table 4. The F-
measures for the nested lexical match are also 
shown individually for each named entity tag sepa-
rately in Table 5. 
Experimental results of Table 4 show that the 
CRF based NER system performs best for Bengali 
with maximal F-measure of 55.36%, nested F-
measure of 61.46% and lexical F-measure 59.39%. 
The system has demonstrated the F-measures of 
35.37%, 36.75% and 33.12%, respectively for 
maximal, nested and lexical matches. The system 
has shown promising precision values for Hindi. 
But due to the low recall values, the F-measures 
get reduced. The large difference between the re-
call and precision values in the evaluation results 
of Hindi indicates that the system is not able to 
retrieve a significant number of NEs from the test 
38
data. In comparison to Hindi, the precision values 
are low and the recall values are high for Bengali. 
It can be decided from the evaluation results that 
system retrieves more NEs in Bengali than Hindi 
but involves more errors. The lack of features in 
Oriya, Telugu and Urdu might be the reason be-
hind their poor performance.   
Language
 
Number of 
tokens in the 
training set 
Number of to-
kens in the test 
set 
Bengali 122,467 30,505 
Hindi 502,974 38,708 
Telugu 64,026 6,356 
Oriya 93,173 24,640 
Urdu 35,447 3,782 
Table 3: Training and Test Sets Statistics  
Tag Bengali Hindi Oriya Telugu Urdu 
NEP 85.68 21.43 43.76 1.9 7.69 
NED 35.9 38.70 NF NF NF 
NEO 52.53 NF 5.60 NF 22.02 
NEA 26.92 30.77 NF NF NF 
NEB NF NF NF NF NF 
NETP 61.44 NF 12.55 NF NF 
NETO 45.98 NF NF NF NF 
NEL 80.00 22.70 31.49 0.73 50.14 
NETI 53.43 49.60 27.08 7.64 49.28 
NEN 30.12 85.40 9.19 9.16 NF 
NEM 79.08 36.64 7.56 NF 79.27 
NETE 18.06 1.64 NF 5.74 NF 
Table 4. Evaluation for Specific NE Tags (F-
Measures for nested lexical match) [NF: Nothing 
found]  
Experimental results of Table 5 show the F-
measures for the nested lexical item matches for 
individual NE tags. For Bengali, the system has 
shown reasonably high F-measures for NEP, NEL 
and NEM tags and medium F-measures for NETP, 
NETI, NEO and NETO tags. The overall F-
measures in Bengali might have reduced due to 
relatively poor F-measures for NETE, NEN, NEA 
and NED tags. For Hindi, the highest F-measure 
obtained is 85.4% for NEN tag followed by NETI, 
NED, NEM, NEA, NEL and NEP tags. In some 
cases, the system has shown better F-measures for 
Hindi than Bengali also. The system has performed 
better for NEN, NED and NEA tags in Hindi than 
all other languages. 
6 Conclusion  
We have developed a named entity recognition 
system using Conditional Random Fields for the 
five different Indian languages, namely Bengali, 
Hindi, Telugu, Oriya and Urdu. We have consid-
ered the contextual window of size five, prefix and 
suffix of length upto three of the current word, NE 
information of the previous word, different digit 
features and the frequently occurring word lists. 
The system also uses linguistic features extracted 
from the various gazetteer lists for Bengali and 
Hindi. Evaluation results show that the system per-
forms best for Bengali. The performance of the 
system for Bengali can further be improved by in-
cluding the part of speech (POS) information of the 
current and/or the surrounding word(s). The per-
formance of the system for other languages can be 
improved with the use of different linguistic fea-
tures as like Bengali.  
The system did not perform as expected due to 
the problems faced during evaluation regarding the 
tokenization. We have tested the system for Ben-
gali with 10-fold cross validation and obtained im-
pressive results.  
References 
Babych, Bogdan, A. Hartley. Improving machine trans-
lation quality with automatic named entity recogni-
tion. In Proceedings of EAMT/EACL 2003 Workshop 
on MT and other language technology tools, 1-8, 
Hungary. 
Bennet, Scott W.; C. Aone; C. Lovell. 1997. Learning to 
Tag Multilingual Texts Through Observation. In 
Proceedings of EMNLP, 109-116,  Rhode Island.  
Bikel, Daniel M., R. Schwartz, Ralph M. Weischedel. 
1999. An Algorithm that Learns What?s in Name. 
Machine Learning (Special Issue on NLP), 1-20. 
Bothwick, Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. Thesis, 
New York University. 
Chinchor, Nancy. 1995. MUC-6 Named Entity Task 
Definition (Version 2.1). MUC-6, Columbia, Mary-
land.   
39
     
Table 5. Evaluation of the Five Languages  
Chinchor, Nancy. 1998. MUC-7 Named Entity Task 
Definition (Version 3.5). MUC-7. Fairfax, Vir-
ginia. 
Cunningham, H. 2001. GATE: A general architecture 
for text engineering. Comput.  Humanit. (36), 223-
254. 
Ekbal, Asif, and S. Bandyopadhyay. 2007a. Pattern 
Based Bootstrapping Method for Named Entity 
Recognition. In Proceedings of 6
th 
International 
Conference on Advances in Pattern Recognition, 
Kolkata,    India, 349-355. 
Ekbal, Asif, and S. Bandyopadhyay. 2007b. Lexical 
Pattern Learning from Corpus Data for Named En-
tity Recognition. In Proceedings of the 5
th 
Interna-
tional Conference on Natural Language Process-
ing, Hyderabad, India, 123-128. 
Ekbal, Asif, Naskar, Sudip and S. Bandyopadhyay.   
2007c. Named Entity Recognition and Translitera-
tion in Bengali. Named Entities: Recognition, 
Classification and Use, Special Issue of Lingvisti-
cae Investigationes Journal, 30:1 (2007), 95-114. 
Ekbal, Asif, and S. Bandyopadhyay. 2007d. A Web-
based Bengali News Corpus for Named Entity 
Recognition. Language Resources and Evaluation 
Journal (Accepted) 
Lafferty, J., McCallum, A., and Pereira, F. 2001. 
Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. In 
Proc. of 18
th
 International Conference on Machine 
learning. 
Li, Wei and Andrew McCallum. 2003. Rapid Devel-
opment of Hindi Named Entity Recognition Using 
Conditional Random Fields and Feature Induc-
tions, ACM TALIP, 2(3), (2003), 290-294. 
McCallum, A.; W. Li. 2003. Early Results for Named 
Entity Recognition with Conditional Random 
Fields, Feature Induction and Web-Enhanced 
Lexicons. In Proceedings CoNLL-03, Edmanton, 
Canada. 
Moldovan, Dan I., Sanda M. Harabagiu, Roxana 
Girju, P. Morarescu, V. F. Lacatusu, A. Novischi, 
A. Badulescu, O. Bolohan. 2002. LCC Tools for 
Question Answering. In Proceedings of the TREC, 
Maryland, 1-10. 
Pinto, D., McCallum, A., Wei, X., and Croft, W. B. 
2003. Table extraction using conditional random 
fields. In Proceedings of SIGIR 03 Conference, 
Toronto, Canada. 
Sekine, Satoshi. 1998. Description of the Japanese 
NE System Used for MET-2, MUC-7, Fairfax, 
Virginia. 
Sha, F. and Pereira, F. 2003. Shallow parsing with 
conditional random fields. In Proceedings of Hu-
man Language Technology, NAACL. 
Measure
Precision Recall F-measure 
Language P
m 
P
n 
P
l 
R
m 
R
n 
R
l 
F
m 
F
n 
F
l 
Bengali 51.63 47.74 52.90 59.60 61.46 67.71 55.36 61.46 59.39
Hindi 71.05 76.08 80.59 23.54 24.23 20.84 35.37 36.75 33.12
Oriya 27.12 27.18 50.40 12.88 10.53 20.07 17.47 15.18 28.71
Telugu 1.70 2.70 8.10 0.538 0.539 3.34 0.827 0.902 4.749
Urdu 49.16 48.93 54.45 21.95 20.15 26.36 30.35 28.55 35.52
M: Maximal,  n: Nested, l: Lexical 
40
Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 51?58,
Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language Processing
Bengali Named Entity Recognition using Support Vector Machine 
Asif Ekbal  
Department of Computer Science and 
Engineering, Jadavpur University 
Kolkata-700032, India 
asif.ekbal@gmail.com 
Sivaji Bandyopadhyay 
Department of Computer Science and  
Engineering, Jadavpur University 
Kolkata-700032, India 
svaji_cse_ju@yahoo.com 
Abstract 
Named Entity Recognition (NER) aims to 
classify each word of a document into prede-
fined target named entity classes and is nowa-
days considered to be fundamental for many 
Natural Language Processing (NLP) tasks 
such as information retrieval, machine transla-
tion, information extraction, question answer-
ing systems and others. This paper reports 
about the development of a NER system for 
Bengali using Support Vector Machine 
(SVM). Though this state of the art machine 
learning method has been widely applied to 
NER in several well-studied languages, this is 
our first attempt to use this method to Indian 
languages (ILs) and particularly for Bengali. 
The system makes use of the different contex-
tual information of the words along with the 
variety of features that are helpful in predicting 
the various named entity (NE) classes. A por-
tion of a partially NE tagged Bengali news 
corpus, developed from the archive of a lead-
ing Bengali newspaper available in the web, 
has been used to develop the SVM-based NER 
system. The training set consists of approxi-
mately 150K words and has been manually 
annotated with the sixteen NE tags. Experi-
mental results of the 10-fold cross validation 
test show the effectiveness of the proposed 
SVM based NER system with the overall av-
erage Recall, Precision and F-Score of 94.3%, 
89.4% and 91.8%, respectively. It has been 
shown that this system outperforms other ex-
isting Bengali NER systems. 
1 Introduction 
Named Entity Recognition (NER) is an important 
tool in almost all NLP application areas such as 
information retrieval, machine translation, ques 
tion-answering system, automatic summarization 
etc. Proper identification and classification of NEs 
are very crucial and pose a very big challenge to 
the NLP researchers. The level of ambiguity in 
NER makes it difficult to attain human perform-
ance 
NER has drawn more and more attention from 
the NE tasks (Chinchor 95; Chinchor 98) in Mes-
sage Understanding Conferences (MUCs) [MUC6; 
MUC7]. The problem of correct identification of 
NEs is specifically addressed and benchmarked by 
the developers of Information Extraction System, 
such as the GATE system (Cunningham, 2001). 
NER also finds application in question-answering 
systems (Maldovan et al, 2002) and machine 
translation (Babych and Hartley, 2003).  
The current trend in NER is to use the machine-
learning approach, which is more attractive in that 
it is trainable and adoptable and the maintenance of 
a machine-learning system is much cheaper than 
that of a rule-based one. The representative ma-
chine-learning approaches used in NER are Hidden 
Markov Model (HMM) (BBN?s IdentiFinder in 
(Bikel, 1999)), Maximum Entropy (New York 
University?s MEME in (Borthwick, 1999)), Deci-
sion Tree (New York University?s system in (Se-
kine, 1998) and Conditional Random Fields 
(CRFs) (Lafferty et al, 2001). Support Vector Ma-
chines (SVMs) based NER system was proposed 
by Yamada et al (2002) for Japanese. His system 
is an extension of Kudo?s chunking system (Kudo 
and Matsumoto, 2001) that gave the best perform-
ance at CoNLL-2000 shared tasks. The other 
SVM-based NER systems can be found in (Takeu-
chi and Collier, 2002) and (Asahara and Matsu-
moto, 2003).  
 Named entity identification in Indian languages 
in general and particularly in Bengali is difficult 
and challenging. In English, the NE always ap-
pears with capitalized letter but there is no concept 
of capitalization in Bengali. There has been a very
51
little work in the area of NER in Indian languages. 
In Indian languages, particularly in Bengali, the 
works in NER can be found in (Ekbal and 
Bandyopadhyay, 2007a; Ekbal and Bandyop-
adhyay, 2007b) with the pattern directed shallow 
parsing approach and in (Ekbal et al, 2007c) with 
the HMM. Other than Bengali, a CRF-based Hindi 
NER system can be found in (Li and McCallum, 
2004).  
The rest of the paper is organized as follows.  
Support Vector Machine framework is described 
briefly in Section 2. Section 3 deals with the 
named entity recognition in Bengali that describes 
the named entity tagset and the detailed descrip-
tions of the features for NER. Experimental results 
are presented in Section 4. Finally, Section 5 con-
cludes the paper.  
2 Support Vector Machines 
Support Vector Machines (SVMs) are relatively 
new machine learning approaches for solving two-
class pattern recognition problems. SVMs are well 
known for their good generalization performance, 
and have been applied to many pattern recognition 
problems. In the field of NLP, SVMs are applied to 
text categorization, and are reported to have 
achieved high accuracy without falling into over-
fitting even though with a large number of words 
taken as the features. 
Suppose we have a set of training data for a two-
class problem: 1 1{( , ),.....( , )}N Nx y x y , where 
D
ix R  is a feature vector of the i-th sample in the 
training   data and { 1, 1}iy     is the class to which 
ix belongs. The goal is to find a decision function 
that accurately predicts class y for an input vector 
x. A non-linear SVM classifier gives a decision 
function f(x) sign(g(x) for an input vector 
where, 
1
( ) ( , )i
m
i
i
g x wK x z b

 

 
Here, f(x) +1 means x is a member of a cer-
tain class and f(x)  -1 means x is not a member. 
zi s are called support vectors and are representa-
tives of training examples, m is the number of sup-
port vectors. Therefore, the computational com-
plexity of ( )g x  is proportional to m. Support vec-
tors and other constants are determined by solving 
a certain quadratic programming problem. 
( , )iK x z is a kernel that implicitly maps vectors 
into a higher dimensional space. Typical kernels 
use dot products: ( , ) ( . )iK x z k x z . A polynomial 
kernel of degree d is given by 
( , )iK x z =(1 )
d
x . We can use various kernels, 
and the design of an appropriate kernel for a par-
ticular application is an important research issue.  
We have developed our system using SVM 
(Jochims, 1999) and (Valdimir, 1995), which per-
forms classification by constructing an N-
dimensional hyperplane that optimally separates 
data into two categories. Our general NER system 
includes two main phases: training and classifica-
tion. Both the training and classification processes 
were carried out by YamCha1 toolkit, an SVM 
based tool for detecting classes in documents and 
formulating the NER task as a sequential labeling 
problem. Here, the pair wise multi-class decision 
method and second degree polynomial kernel func-
tion were used. We have used TinySVM-0.072 
classifier that seems to be the best optimized 
among publicly available SVM toolkits. 
3 Named Entity Recognition in Bengali 
Bengali is one of the widely used languages all 
over the world. It is the seventh popular language 
in the world, second in India and the national lan-
guage of Bangladesh. A partially NE tagged Ben-
gali news corpus (Ekbal and Bandyopadhyay, 
2007d), developed from the archive of a widely 
read Bengali newspaper. The corpus contains 
around 34 million word forms in ISCII (Indian 
Script Code for Information Interchange) and 
UTF-8 format. The location, reporter, agency and 
different date tags (date, ed, bd, day) in the par-
tially NE tagged corpus help to identify some of 
the location, person, organization and miscellane-
ous names, respectively that appear in some fixed 
places of the newspaper. These tags cannot detect 
the NEs within the actual news body. The date in-
formation obtained from the news corpus provides 
example of miscellaneous names. A portion of this 
partially NE tagged corpus has been manually an-
notated with the sixteen NE tags as described in 
Table 1. 
3.1 Named Entity Tagset 
A SVM based NER system has been developed in 
this work to identify NEs in Bengali and classify 
                                                 
1http://chasen-org/~taku/software/yamcha/  
2http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM  
52
them into the predefined four major categories, 
namely, ?Person name?, ?Location name?, ?Organi-
zation name? and ?Miscellaneous name?. In order 
to properly denote the boundaries of the NEs and 
to apply SVM in NER task, sixteen NE and one 
non-NE tags have been defined as shown in Table 
1. In the output, sixteen NE tags are replaced ap-
propriately with the four major NE tags by some 
simple heuristics. 
 
NE tag Meaning Example 
PER Single word per-
son name 
sachin / PER 
LOC Single word loca-
tion name 
jdavpur/LOC 
ORG Single word or-
ganization name 
infosys / ORG 
MISC Single word mis-
cellaneous name 
100%/ MISC 
B-PER 
I-PER 
E-PER 
Beginning, Inter-
nal or the End of 
a multiword per-
son name 
sachin/B-PER 
ramesh/I-PER  
tendulkar/E-PER 
B-LOC 
I-LOC 
E-LOC 
Beginning, Inter-
nal or the End of 
a multiword loca-
tion name 
mahatma/B-LOC 
gandhi/I-LOC   
road/E-LOC 
B-ORG 
I-ORG 
E-ORG 
Beginning, Inter-
nal or the End of 
a multiword or-
ganization name 
bhaba/B-ORG 
atomic/I-ORG  
research/I-ORG 
center/E-ORG 
B-MISC 
I-MISC 
E-MISC 
Beginning, Inter-
nal or the End of 
a multiword mis-
cellaneous name 
10e/B-MISC 
magh/I-MISC 
1402/E-MISC 
NNE Words that are 
not named enti-
ties  
neta/NNE,  
bidhansabha/NNE
Table 1. Named Entity Tagset 
3.2 Named Entity Feature Descriptions 
Feature selection plays a crucial role in the Support 
Vector Machine (SVM) framework. Experiments 
have been carried out in order to find out the most 
suitable features for NER in Bengali. The main 
features for the NER task have been identified 
based on the different possible combination of 
available word and tag context. The features also 
include prefix and suffix for all words. The term 
prefix/suffix is a sequence of first/last few charac-
ters of a word, which may not be a linguistically 
meaningful prefix/suffix. The use of prefix/suffix 
information works well for highly inflected lan-
guages like the Indian languages. In addition, vari-
ous gazetteer lists have been developed for use in 
the NER task. We have considered different com-
bination from the following set for inspecting the 
best feature set for NER task: 
F={ 1 1,..., , , ,...,i m i i i i nw w w w w    , |prefix|n, |suffix|n, 
previous NE tags, POS tags, First word, Digit in-
formation, Gazetteer lists} 
Following are the details of the set of features 
that have been applied to the NER task: 
Context word feature: Previous and next words of 
a particular word might be used as a feature.  
Word suffix: Word suffix information is helpful 
to identify NEs. This feature can be used in two 
different ways. The first and the na?ve one is, a 
fixed length word suffix of the current and/or the 
surrounding word(s) might be treated as feature. 
The second and the more helpful approach is to 
modify the feature as binary valued. Variable 
length suffixes of a word can be matched with pre-
defined lists of useful suffixes for different classes 
of NEs. The different suffixes that may be particu-
larly helpful in detecting person (e.g., -babu, -da, -
di etc.) and location names (e.g., -land, -pur, -lia 
etc.) are also included in the lists of variable length 
suffixes. Here, both types of suffixes have been 
used. 
Word prefix: Prefix information of a word is also 
helpful. A fixed length prefix of the current and/or 
the surrounding word(s) might be treated as fea-
tures.  
Part of Speech (POS) Information: The POS of 
the current and/or the surrounding word(s) can be 
used as features. Multiple POS information of the 
words can be a feature but it has not been used in 
the present work. The alternative and the better 
way is to use a coarse-grained POS tagger.  
Here, we have used a CRF-based POS tagger, 
which was originally developed with the help of 26 
different POS tags3, defined for Indian languages.  
For NER, we have considered a coarse-grained 
POS tagger that has only the following POS tags: 
NNC (Compound common noun), NN (Com-
mon noun), NNPC (Compound proper noun), NNP 
(Proper noun), PREP (Postpositions), QFNUM 
(Number quantifier) and Other (Other than the 
above). 
                                                 
3http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.pdf 
53
 The POS tagger is further modified with two 
POS tags (Nominal and Other) for incorporating 
the nominal POS information. Now, a binary val-
ued feature ?nominalPOS? is defined as: If the cur-
rent/surrounding word is ?Nominal? then the  
?nominalPOS? feature of the corresponding word is 
set to ?+1?; otherwise, it is set to ?-1?. This binary 
valued ?nominalPOS? feature has been used in ad-
dition to the 7-tag POS feature.  Sometimes, post-
positions play an important role in NER as postpo-
sitions occur very frequently after a NE. A binary 
valued feature ?nominalPREP? is defined as: If the 
current word is nominal and the next word is PREP 
then the feature ?nomianlPREP? of the current 
word is set to ?+1?, otherwise, it is set to ?-1?. 
Named Entity Information: The NE tag(s) of the 
previous word(s) can also be considered as the fea-
ture. This is the only dynamic feature in the ex-
periment. 
First word: If the current token is the first word of 
a sentence, then the feature ?FirstWord? is set to 
?+1?; Otherwise, it is set to ?-1?. 
Digit features: Several digit features have been 
considered depending upon the presence and/or the 
number of digit(s) in a token (e.g., ContainsDigit 
[token contains digits], FourDigit [token consists 
of four digits], TwoDigit [token consists of two 
digits]), combination of digits and punctuation 
symbols (e.g., ContainsDigitAndComma [token 
consists of digits and comma], ConatainsDigi-
tAndPeriod [token consists of digits and periods]), 
combination of digits and symbols (e.g., Con-
tainsDigitAndSlash [token consists of digit and 
slash], ContainsDigitAndHyphen [token consists 
of digits and hyphen], ContainsDigitAndPercent-
age [token consists of digits and percentages]). 
These binary valued features are helpful in recog-
nizing miscellaneous NEs such as time expres-
sions, monetary expressions, date expressions, per-
centages, numerical numbers etc.     
Gazetteer Lists: Various gazetteer lists have been 
developed from the partially NE tagged Bengali 
news corpus (Ekbal and Bandyopadhyay, 2007d). 
These lists have been used as the binary valued 
features of the SVM framework. If the current to-
ken is in a particular list, then the corresponding 
feature is set to ?+1? for the current and/or sur-
rounding word(s); otherwise, it is set to ?-1?. The 
following is the list of gazetteers: 
 (i). Organization suffix word (94 entries): This list 
contains the words that are helpful in identifying 
organization names (e.g., kong, limited etc.). The 
feature ?OrganizationSuffix? is set to ?+1? for the 
current and the previous words.  
 (ii). Person prefix word (245 entries): This is use-
ful for detecting person names (e.g., sriman, sree, 
srimati etc.). The feature ?PersonPrefix? is set to 
?+1? for the current and the next two words.  
 (iii). Middle name (1,491 entries): These words 
generally appear inside the person names (e.g., 
chandra, nath etc.). The feature ?MiddleName? is 
set to ?+1? for the current, previous and the next 
words.  
 (iv). Surname (5,288 entries): These words usually 
appear at the end of person names as their parts. 
The feature ?SurName? is set to ?+1? for the current 
word. 
(v). Common location word (547 entries): This list 
contains the words that are part of location names 
and appear at the end (e.g., sarani, road, lane etc.). 
The feature ?CommonLocation? is set to ?+1? for 
the current word. 
(vi). Action verb (221 entries): A set of action 
verbs like balen, ballen, ballo, shunllo, haslo etc. 
often determines the presence of person names. 
The feature ?ActionVerb? is set to ?+1? for the 
previous word. 
(vii). Frequent word (31,000 entries): A list of 
most frequently occurring words in the Bengali 
news corpus has been prepared using a part of the 
corpus. The feature ?RareWord? is set to ?+1? for 
those words that are not in this list. 
(viii). Function words (743 entries): A list of func-
tion words has been prepared manually. The fea-
ture ?NonFunctionWord? is set to ?+1? for those 
words that are not in this list. 
(ix). Designation words (947 entries): A list of 
common designation words has been prepared. 
This helps to identify the position of the NEs, par-
ticularly person names (e.g., neta, sangsad, 
kheloar etc.). The feature ?DesignationWord? is set 
to ?+1? for the next word. 
(x). Person name (72, 206 entries): This list con-
tains the first name of person names. The feature 
?PersonName? is set to ?+1? for the current word. 
(xi). Location name (7,870 entries): This list con-
tains the location names and the feature ?Loca-
tionName? is set to ?+1? for the current word. 
(xii). Organization name (2,225 entries): This list 
contains the organization names and the feature 
?OrganizationName? is set to ?+1? for the current 
word.  
(xiii). Month name (24 entries): This contains the 
name of all the twelve different months of both 
54
English and Bengali calendars. The feature 
?MonthName? is set to ?+1? for the current word.  
(xiv). Weekdays (14 entries): It contains the name 
of seven weekdays in Bengali and English both. 
The feature ?WeekDay? is set to ?+1? for the cur-
rent word. 
4 Experimental Results 
A partially NE tagged Bengali news corpus (Ekbal 
and Bandyopadhyay, 2007d) has been used to cre-
ate the training set for the NER experiment. Out of 
34 million wordforms, a set of 150K wordforms 
has been manually annotated with the 17 tags as 
shown in Table 1 with the help of Sanchay Editor4, 
a text editor for Indian languages. Around 20K NE 
tagged corpus is selected as the development set 
and the rest 130K wordforms are used as the train-
ing set of the SVM based NER system.  
We define the baseline model as the one where 
the NE tag probabilities depend only on the current 
word: 
1 2 3 1 2 3
1...
( , , ..., | , , ..., ) ( , )n n i i
i n
P t t t t w w w w P t w


	
 
In this model, each word in the test data is as-
signed the NE tag that occurs most frequently for 
that word in the training data. The unknown word 
is assigned the NE tag with the help of various 
gazetteers and NE suffix lists. 
Seventy four different experiments have been 
conducted taking the different combinations from 
the set ?F? to identify the best-suited set of features 
for NER in Bengali. From our empirical analysis, 
we found that the following combination gives the 
best result for the development set.  
F={ 3 2 1 1 2i i i i i iw w w w w w     , |prefix|<=3, 
|suffix|<=3, NE information of the window [-2, 0], 
POS information of the window [-1, +1], nominal-
POS of the current word, nominalPREP, 
FirstWord, Digit features, Gazetteer lists} 
The meanings of the notations, used in experi-
mental results, are defined below: 
pw, cw, nw: Previous, current and the next 
word; pwi, nwi: Previous and the next ith word 
from the current word; pt: NE tag of the previous 
word; pti: NE tag of the previous ith word; pre, 
suf: Prefix and suffix of the current word; ppre, 
psuf: Prefix and suffix of the previous word; npre, 
nsuf: Prefix and suffix of the next word; pp, cp, np: 
POS tag of the previous, current and the next word; 
                                                 
4Sourceforge.net/project/nlp-sanchay 
ppi, npi: POS tag of the previous and the next ith 
word; cwnl: Current word is nominal. 
Evaluation results of the development set are 
presented in Tables 2-4. 
 
Feature (word, tag)  FS (%) 
pw, cw, nw, FirstWord 71.23 
pw2, pw, cw, nw, nw2, FirstWord 73.23 
pw3, pw2, pw, cw, nw, nw2, 
FirstWord 
74.87 
pw3, pw2, pw, cw, nw, nw2, nw3, 
FirstWord 
74.12 
pw4, pw3, pw2, pw, cw, nw, nw2, 
FirstWord 
74.01 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt 
75.30 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2 
76.23 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, pt3 
75.48 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, | |suf|<=4, pre|<=4 
78.72 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3 
81.2 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3 
|psuf|<=3 
80.4 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
|psuf|<=3, |nsuf|<=3, |ppre|<=3, 
|npre|<=3 
78.14 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
|nsuf|<=3, |npre|<=3 
79.90 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
|psuf|<=3, |ppre|<=3, 
80.10 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit  
82.8 
Table 2. Results on the Development Set  
 
It is observed from Table 2 that the word win-
dow [-3, +2] gives the best result (4th row) with the 
?FirstWord? feature and further increase or de-
crease in the window size reduces the overall F-
Score value. Results (7th-9th rows) show that the 
inclusion of NE information increases the F-Score 
value and the NE information of the previous two 
words gives the best results (F-Score=81.2%). It is 
indicative from the evaluation results (10th and 11th 
55
rows) that prefixes and suffixes of length up to 
three of the current word are very effective. It is 
also evident (12th-15th rows) that the surrounding 
word prefixes and/or suffixes do not increase the 
F-Score value. The F-Score value is improved by 
1.6% with the inclusion of various digit features 
(15th and 16th rows). 
 
Feature (word, tag)  FS ( %) 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit, pp, cp, np 
    87.3 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit, pp2, pp, cp, np, np2 
85.1 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit,  pp, cp 
86.4 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit, cp, np 
85.8 
pp2, pp, cp, np, np2, pt, pt2, 
|pre|<=3, |suf|<=3, FirstWord, Digit 
41.9 
pp, cp, np, pt, pt2, |pre|<=3, |suf|<=3, 
FirstWord, Digit 
36.4 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit, cp 
86.1 
Table 3. Results on the Development Set 
 
Experimental results (2nd-5th rows) of Table 3 
suggest that the POS tags of the previous, current 
and the next words, i.e., POS information of the 
window [-1, +1] is more effective than the window 
[-2, +2], [-1, 0], [0, +1] or the current word alone. 
In the above experiment, the POS tagger was de-
veloped with 7 POS tags. Results (6th and 7th rows) 
also show that POS information with the word is 
helpful but only the POS information without the 
word decreases the F-Score value significantly. 
Results (4th and 5th rows) also show that the POS 
information of the window [-1, 0] is more effective 
than the POS information of the window [0, +1]. 
So, it can be argued that the POS information of 
the previous word is more helpful than the POS 
information of the next word. 
In another experiment, the POS tagger was de-
veloped with 26 POS tags and the use of this tag-
ger has shown the F-Score value of 85.6% with the 
feature (word, tag)=[pw3, pw2, pw, cw, nw, nw2, 
FirstWord, pt, pt2, |suf|<=3, |pre|<=3, Digit, pp, cp, 
np]. So, it can be decided that the smaller POS 
tagset is more effective than the larger POS tagset 
in NER. We have observed from two different ex-
periments that the overall F-Score values can fur-
ther be improved by 0.5% and 0.3%, respectively, 
with the ?nominalPOS? and ?nominalPREP? fea-
tures. It has been also observed that the ?nominal-
POS? feature of the current word is only helpful 
and not of the surrounding words. The F-Score 
value of the NER system increases to 88.1% with 
the feature: feature (word, tag)=[pw3, pw2, pw, 
cw, nw, nw2, FirstWord, pt, pt2, |suf|<=3, |pre|<=3, 
Digit pp, cp, np, cwnl, nominalPREP]. 
Experimental results with the various gazetteer 
lists are presented in Table 4 for the development 
set. Results demonstrate that the performance of 
the NER system can be improved significantly 
with the inclusion of various gazetteer lists. The 
overall F-Score value increases to 90.7%, which is 
an improvement of 2.6%, with the use of gazetteer 
lists. 
The best set of features is identified by training 
the system with 130K wordforms and tested with 
the help of development set of 20K wordforms. 
Now, the development set is included as part of the 
training set and resultant training set is thus con-
sisting of 150K wordforms. The training set has 
20,455 person names, 11,668 location names, 963 
organization names and 11,554 miscellaneous 
names. We have performed 10-fold cross valida-
tion test on this resultant training set. The Recall, 
Precision and F-Score values of the 10 different 
experiments for the 10-fold cross validation test 
are presented in Table 5. The overall average Re-
call, Precision and F-Score values are 94.3%, 
89.4% and 91.8%, respectively. 
The other existing Bengali NER systems along 
with the baseline model have been also trained and 
tested with the same data set. Comparative evalua-
tion results of the 10-fold cross validation tests are 
presented in Table 6 for the four different models. 
It presents the average F-Score values for the four 
major NE classes: ?Person name?, ?Location 
name?, ?Organization name? and ?Miscellaneous 
name?. Two different NER models, A and B, are 
defined in (Ekbal and Bandyopadhyay, 2007b). 
The model A denotes the NER system that does 
not use linguistic knowledge and B denotes the 
system that uses linguistic knowledge. Evaluation 
results of Table 6 show that the SVM based NER 
model has reasonably high F-Score value. The av-
erage F-Score value of this model is 91.8%, which 
is an improvement of 7.3% over the best-reported 
56
HMM based Bengali NER system (Ekbal et al, 
2007c). The reason behind the rise in F-Score 
value might be its better capability to capture the 
morphologically rich and overlapping features of 
Bengali language.  
 
Feature (word, tag) FS (%) 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit pp, cp, np, cwnl, nominal-
PREP, DesignationWord, Non-
FunctionWord
 
89.2 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit pp, cp, np, cwnl, nominal-
PREP, DesignationWord, Non-
FunctionWord
 
89.5 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit pp, cp, np, cwnl, nominal-
PREP, DesignationWord, Non-
FunctionWord OrganizationSuf-
fix, PersonPrefix
 
90.2 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit pp, cp, np, cwnl, nominal-
PREP, DesignationWord, Non-
FunctionWord OrganizationSuf-
fix, PersonPrefix MiddleName, 
CommonLocation 
90.5 
pw3, pw2, pw, cw, nw, nw2, First 
Word, pt, pt2, |suf|<=3, |pre|<=3, 
Digit pp, cp, np, cwnl, nominal-
PREP, DesignationWord, No-
FunctionWord OrganizationSuf-
fix, PersonPrefix MiddleName, 
CommonLocation,  Other gazet-
teers 
90.7 
 Table 4. Results on the Development Set  
 
The F-Score value of the system increases with 
the increment of training data. This fact is repre-
sented in Figure 1. Also, it is evident from Figure 1 
that the value of ?Miscellaneous name? is nearly 
close to 100% followed by ?Person name?, ?Loca-
tion name? and ?Organization name? NE classes 
with the training data of 150K words. 
 
Test set no. Recall Precision FS (%) 
1 92.5 87.5 89.93 
2 92.3 87.6 89.89 
3 94.3 88.7 91.41 
4 95.4 87.8 91.40 
5 92.8 87.4 90.02 
6 92.4 88.3 90.30 
7 94.8 91.9 93.33 
8 93.8 90.6 92.17 
9 96.9 91.8 94.28 
10 97.8 92.4 95.02 
Average 94.3 89.4 91.8 
 Table 5. Results of the 10-fold cross validation 
test  
      
Model F_P F_L F_O F_M F_T 
Baseline 
 
61.3 58.7 58.2 52.2 56.3 
A 75.3 74.7 73.9 76.1 74.5 
B 79.3 78.6 78.6 76.1 77.9 
HMM 85.5 82.8 82.2 92.7 84.5 
SVM 91.4 89.3 87.4 99.2 91.8 
 Table 6. Results of the 10-fold cross validation 
test (F_P: Avg. f-score of ?Person?, F_L: Avg. f-
score of ?Location?, F_O: Avg. f-score of ?Organi-
zation?, F_M: Avg. f-score of ?Miscellaneous? and 
F_T: Overall avg. f-score of all classes)   
5 Conclusion 
We have developed a NER system using the SVM 
framework with the help of a partially NE tagged 
Bengali news corpus, developed from the archive 
of a leading Bengali newspaper available in the 
web. It has been shown that the contextual window 
of size six, prefix and suffix of length up to three 
of the current word, POS information of the win-
dow of size three, first word, NE information of 
the previous two words, different digit features and 
the various gazetteer lists are the best-suited fea-
tures for NER in Bengali. Experimental results 
with the 10-fold cross validation test have shown 
reasonably good Recall, Precision and F-Score 
values. The performance of this system has been 
compared with the existing three Bengali NER sys-
tems and it has been shown that the SVM-based 
system outperforms other systems. One possible 
reason behind the high Recall, Precision and F-
Score values of the SVM based system might be its 
effectiveness to handle the diverse and overlapping 
features of the highly inflective Indian languages.    
57
The proposed SVM based system is to be 
trained and tested with the other Indian languages, 
particularly Hindi, Telugu, Oriya and Urdu. Ana-
lyzing the performance of the system using other 
methods like MaxEnt and CRFs will be other in-
teresting experiments. 
 
F-Score(%) vs Training file size(K)
0
20
40
60
80
100
120
0 100 200
Number of Words (K) 
F-
Sc
or
e 
(%
)
Person
Location
Organisation
Miscellaneous
 Fig. 1. F-Score VS Training file size 
References 
Anderson, T. W. and Scolve, S. 1978. Introduction to 
the Statistical Analysis of Data. Houghton Mifflin. 
Asahara, Masayuki and Matsumoto, Yuji. 2003. Japa-
nese Named Entity Extraction with Redundant Mor-
phological Analysis.  In Proc. of HLT-NAACL. 
Babych, Bogdan, A. Hartley. 2003. Improving Machine 
Translation Quality with Automatic Named Entity 
Recognition. In Proceedings of EAMT/EACL 2003 
Workshop on MT and other language technology 
tools, 1-8, Hungary. 
Bikel, Daniel M., R. Schwartz, Ralph M. Weischedel. 
1999. An Algorithm that Learns What?s in Name. 
Machine Learning (Special Issue on NLP), 1-20. 
Bothwick, Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. Thesis, 
New York University. 
Chinchor, Nancy. 1995. MUC-6 Named Entity Task 
Definition (Version 2.1). MUC-6, Maryland. 
Chinchor, Nancy. 1998. MUC-7 Named Entity Task 
Definition (Version 3.5). MUC-7, Fairfax, Virginia. 
Cunningham, H. 2001. GATE: A General Architecture 
for Text Engineering. Comput.  Humanit. (36), 223-254. 
Ekbal, Asif, and S. Bandyopadhyay. 2007a. Pattern 
Based Bootstrapping Method for Named Entity Rec-
ognition. In Proceedings of ICAPR, India, 349-355. 
Ekbal, Asif, and S. Bandyopadhyay. 2007b. Lexical 
Pattern Learning from Corpus Data for Named Entity 
Recognition. In Proc.  of ICON, India, 123-128. 
Ekbal, Asif, Naskar, Sudip and S. Bandyopadhyay.   
2007c. Named Entity Recognition and Transliteration 
in Bengali. Named Entities: Recognition, Classifica-
tion and Use, Special Issue of Lingvisticae Investiga-
tiones Journal, 30:1 (2007), 95-114. 
Ekbal, Asif, and S. Bandyopadhyay. 2007d. A Web-
based Bengali News Corpus for Named Entity Rec-
ognition. Language Resources and Evaluation Jour-
nal (To appear December). 
Joachims , T. 1999. Making Large Scale SVM Learning 
Practical. In B. Scholkopf, C. Burges and A. Smola 
editions, Advances in Kernel Methods-Support Vec-
tor Learning, MIT Press. 
Kudo, Taku and Matsumoto, Yuji. 2001. Chunking with 
Support Vector Machines. In Proceedings of NAACL, 
192-199. 
Kudo, Taku and Matsumoto, Yuji. 2000. Use of Support 
Vector Learning for Chunk Identification. In Pro-
ceedings of CoNLL-2000. 
Lafferty, J., McCallum, A., and Pereira, F. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proc. of 
18th International Conference on Machine learning, 
282-289. 
Li, Wei and Andrew McCallum. 2003. Rapid Develop-
ment of Hindi Named Entity Recognition Using 
Conditional Random Fields and Feature Inductions. 
ACM TALIP, 2(3), (2003), 290-294. 
Moldovan, Dan I., Sanda M. Harabagiu, Roxana Girju, 
P. Morarescu, V. F. Lacatusu, A. Novischi, A. 
Badulescu, O. Bolohan. 2002. LCC Tools for Ques-
tion Answering. In Proceedings of the TREC, 1-10. 
Sekine, Satoshi. 1998. Description of the Japanese NE 
System Used for MET-2. MUC-7, Fairfax, Virginia. 
Takeuchi, Koichi and Collier, Nigel. 2002. Use of Sup-
port Vector Machines in Extended Named Entity 
Recognition. In Proceedings of  6th CoNLL, 119-125. 
Vapnik, Valdimir N. 1995. The Nature of Statistical 
Learning Theory. Springer. 
Yamada, Hiroyasu, Taku Kudo and Yuji Matsumoto. 
2002. Japanese Named Entity Extraction using Sup-
port Vector Machine. In Transactions of IPSJ, Vol. 
43, No. 1, 44-53.  
 
58
A Document Graph Based Query Focused Multi-Document Summarizer 
                  Sibabrata Paladhi                                       Sivaji Bandyopadhyay 
      Department of Computer Sc. & Engg.             Department of Computer Sc. & Engg.             
               Jadavpur University, India                              Jadavpur University, India                                  
            sibabrata_paladhi@yahoo.com                        sivaji_cse_ju@yahoo.com 
 
 
Abstract 
This paper explores the research issue and 
methodology of a query focused multi-
document summarizer. Considering its pos-
sible application area is Web, the computa-
tion is clearly divided into offline and 
online tasks. At initial preprocessing stage 
an offline document graph is constructed, 
where the nodes are basically paragraphs of 
the documents and edge scores are defined 
as the correlation measure between the 
nodes. At query time, given a set of key-
words, each node is assigned a query de-
pendent score, the initial graph is expanded 
and keyword search is performed over the 
graph to find a spanning tree identifying 
relevant nodes satisfying the keywords. 
Paragraph ordering of the output summary 
is taken care of so that the output looks co-
herent. Although all the examples, shown 
in this paper are based on English language, 
we show that our system is useful in gener-
ating query dependent summarization for 
non- English languages also. We also pre-
sent the evaluation of the system. 
 
1 Introduction 
With the proliferation of information in the Inter-
net, it is becoming very difficult for users to iden-
tify the exact information. So many sites are pro-
viding same piece of information and a typical 
query based search in Google results in thousands 
of links if not million. Web Search engines gener-
ally produce query dependent snippets for each 
result which help users to explore further. An 
automated query focused multi-document summar-
izer, which will generate a query based short  
 
 
summary of web pages will be very useful to get a 
glimpse over the complete story. Automated multi-
document summarization has drawn much atten-
tion in recent years. Most multi-document sum-
marizers are query independent, which produce 
majority of information content from multiple 
documents using much less lengthy text. Each of 
the systems fall into two different categories: either 
they are sentence extraction based where they just 
extract relevant sentences and concatenate them to 
produce summary or they fuse information from 
multiple sources to produce a coherent summary.  
In this paper, we propose a query focused multi-
document summarizer, based on paragraph extrac-
tion scheme. Unlike traditional extraction based 
summarizers which do not take into consideration 
the inherent structure of the document, our system 
will add structure to documents in the form of 
graph. During initial preprocessing, text fragments 
are identified from the documents which constitute 
the nodes of the graph. Edges are defined as the 
correlation measure between nodes of the graph.              
We define our text fragments as paragraph rather 
than sentence with the view that generally a para-
graph contains more correlated information 
whereas sentence level extraction might lead to 
loss of some coherent information.  
 Since the system produces multi-document 
summary based on user?s query, the response time 
of the system should be minimal for practical pur-
pose. With this goal, our system takes following 
steps: First, during preprocessing stage (offline) it 
performs some query independent tasks like identi-
fying seed summary nodes and constructing graph 
over them. Then at query time (online), given a set 
of keywords, it expands the initial graph and per-
forms keyword search over the graph to find a 
spanning tree identifying relevant nodes (para-
graphs) satisfying the keywords. The performance 
of the system depends much on the identification 
of the initial query independent nodes (seed nodes). 
Although, we have presented all the examples in 
the current discussion for English language only,    
we argue that our system can be adapted to work in 
multilingual environment (i.e. Hindi, Bengali, 
Japanese etc.) with some minor changes in imple-
mentation of the system like incorporating lan-
guage dependent stop word list, stemmer, WodrNet 
like lexicon etc. 
 In section 2, related works in this field is pre-
sented. In section 3 the overall approach is de-
scribed. In section 4 query independent preprocess-
ing steps are explained. In section 5 query depend-
ent summary generation and paragraph ordering 
scheme is presented. Section 6 presents the evalua-
tion scheme of the system. In section 7 we discuss 
how our system can be modified to work in multi-
lingual scenario. In section 8 we have drawn con-
clusion and discussed about future work in this 
field. 
2 Related Work 
A lot of research work has been done in the do-
main of multi-document summarization (both 
query dependent/independent). MEAD (Radev et 
al., 2004) is centroid based multi-document sum-
marizer which generates summaries using cluster 
centroids produced by topic detection and tracking 
system. NeATS (Lin and Hovy, 2002) selects im-
portant content using sentence position, term fre-
quency, topic signature and term clustering. XDoX 
(Hardy et al, 2002) identifies the most salient 
themes within the document set by passage cluster-
ing and then composes an extraction summary, 
which reflects these main themes.  
Graph based methods have been proposed for 
generating query independent summaries. Web-
summ (Mani and Bloedorn, 2000) uses a graph-
connectivity model to identify salient information. 
Zhang et al(2004) proposed the methodology of 
correlated summarization for multiple news arti-
cles. In the domain of single document summariza-
tion a system for query-specific document summa-
rization has been proposed (Varadarajan and Hris-
tidis, 2006) based on the concept of document 
graph. 
In this paper, the graph based approach has been 
extended to formulate a framework for generating 
query dependent summary from related  multiple 
document set describing same event.  
3 Graph Based Modeling 
The proposed graph based multi-document sum-
marization method consists of following steps: (1) 
The document set D = {d1,d2, ?  dn} is processed 
to extract text fragments, which are paragraphs in 
our case as it has been discussed earlier. Here, we 
assume that the entire document in a particular set 
are related i.e. they describe the same event. Some 
document clustering techniques may be adopted to 
find related documents from a large collection. 
Document clustering is out of the scope of our cur-
rent discussion and is itself a research interest. Let 
for a document di, the paragraphs are 
{pi1,pi2,?pim}. But the system can be easily modi-
fied to work with sentence level extraction.  Each 
text fragment becomes a node of the graph. (2) 
Next, edges are created between nodes across the 
document where edge score represents the degree 
of correlation between inter documents nodes. (3) 
Seed nodes are extracted which identify the rele-
vant paragraphs within D and a search graph is 
built offline to reflect the semantic relationship 
between the nodes. (4) At query time, each node is 
assigned a query dependent score and the search 
graph is expanded. (5) A query dependent multi-
document summary is generated from the search 
graph which is nothing but constructing a total 
minimal spanning tree T (Varadarajan and Hristi-
dis, 2006). For a set of keywords Q = {q1,q2, .. qn} , 
T is total if ?q?Q, T consists of at least one node 
satisfying q and T is  minimal if no node can be 
removed from T while getting the total T. 
4 Building Query Independent Compo-
nents  
Mainly there are two criteria for the performance 
evaluation of such systems: First it?s accuracy i.e. 
the quality of output with respect to specific que-
ries and next of course the turn around time i.e., 
how fast it can produce the result. Both are very 
important aspects of such system, and we will 
show how these aspects are taken care of in our 
system.  Runtime of such system greatly depends 
on how well the query independent graph is con-
structed. At one extreme, offline graph can be built 
connecting all the nodes from each of the docu-
ments, constituting a total document graph. But 
keyword search over such large graph is time con-
suming and practically not plausible. On the other 
hand, it is possible to select query specific nodes at 
runtime and to create a graph over those nodes. But 
if the number of such nodes is high, then calculat-
ing similarity scores between all the nodes will                                                                          
take large computing time, thus resulting in slower  
performance.  
We will take an intermediate approach to attack 
the problem. It can be safely assumed that signifi-
cant information for a group of keywords can be 
found in ?relevant/topic paragraphs? of the docu-
ments. So, if relevant/topic nodes can be selected 
from document set D during offline processing, 
then the significant part of the search graph can be 
constructed offline which greatly reduce the online 
processing time. For example, if a user wants to 
find the information about the IBM Hindi speech 
recognition system, then the keywords are likely to 
be {IBM, speech recognition, accuracy}. For a set 
of news articles about this system, the topic para-
graphs, identified offline, naturally satisfy first two 
keywords and theoretically, they are the most in-
formative paragraphs for those keywords. The last 
term ?accuracy? (relevant for accuracy of the sys-
tem) may not be satisfied by seed nodes. So, at run 
time, the graph needs to be expanded purposefully 
by including nodes so that the paragraphs, relevant 
to ?accuracy of the system? are included. 
4.1 Identification of Seed/ Topic Nodes 
At the preprocessing stage, text is tokenized, stop 
words are eliminated, and words are stemmed 
(Porter, 1980). The text in each document is split 
into paragraphs and each paragraph is represented 
with a vector of constituent words. If we consider 
pair of related document, then the inter document 
graph can be represented as a set of nodes in the 
form of bipartite graph. The edges connect two 
nodes corresponding to paragraphs from different 
documents. The similarity between two nodes is 
expressed as the edge weight of the bipartite graph. 
Two nodes are related if they share common words 
(except stop words) and the degree of relationship 
can be measured by adapting some traditional IR 
formula (Varadarajan and Hristidis, 2006). 
 
( ( ( ( ) , ) ( ( ) , ) ) . ( ) )( ) ( ( ) ) ( ( ) )
t f t u w t f t v w id f w
S c o r e e
s iz e t u s i z e t v
+
=
+
?  
Where ( , )tf d w  is number of occurrence of w in 
d, ( )id f w is the inverse of the number of docu-
ments containing w, and ( )size d is the size of the 
documents in words. The score can be accurately 
set if stemmer and lexicon are used to match the 
equivalent words. With the idea of page ranking 
algorithms, it can be easily observed that a para-
graph in a document is relevant if it is highly re-
lated to many relevant paragraphs of other docu-
ment. If some less stringent rules are adopted, then 
a node from a document is selected as seed/topic 
node if it has high edge scores with nodes of other 
document. Actually for a particular node, total 
edge score is defined as the sum of scores of all out 
going edges from that node. The nodes with higher 
total edge scores than some predefined threshold 
are included as seed nodes. In Figure 1. correlation 
between two news articles is shown as a bipartite 
graph.   
But the challenge for multi-document summari-
zation is that the information stored in different 
documents inevitably overlap with each other.  So, 
before inclusion of a particular node (paragraph), it 
has to be checked whether it is being repeated or 
not. Two paragraphs are said to be similar if they 
share for example, 70% words (non stop words) in 
common.   
 
 
Figure 1.  A bipartite graph representing correlation 
among two news articles on same event.  
 
4.2 Offline Construction of Search Graph 
After detection of seed/topic nodes a search graph 
is constructed. For nodes, pertaining to different 
documents, edge scores are already calculated, but 
for intra document nodes, edge scores are calcu-
lated in the similar fashion as said earlier. Since, 
highly dense graph leads to higher 
search/execution time, only the edges having edge 
scores well above the threshold value might be 
considered. The construction of query independent 
part of the search graph completes the offline proc-
essing phase of the system. 
5 Building Query Dependent Compo-
nents  
At query time, first, the nodes of the already con-
structed search graph are given a query dependent 
score. The score signifies the relevance of the 
paragraph with respect to given queries. During 
evaluation if it is found that any keyword is not 
satisfied by the seed nodes, then system goes back 
to individual document structure and collects rele-
vant nodes. Finally, it expands the offline graph by 
adding those nodes, fetched from individual docu-
ments. Next, the expanded search graph is proc-
essed to find the total minimum spanning tree T 
over the graph. 
5.1 Expanding Search Graph 
When query arrives, system evaluates nodes of the 
offline search graph and computes query depend-
ent score. This computation is based on ranking 
principals from IR community. The most popular 
IR ranking is okapi equation (Varadarajan and 
Hristidis, 2006) which is based on tf-idf principle.  
          
1 1 3
3
, 1
0 .5 ( ). ( 1).ln . .
0 .5 ( (1 ) )t Q d
N df k tf k q tf
d ld f k q tfk b b tf
avd l
+
?
? + +
+ +
? + +
?
                                                                                                                                                        
 
tf is the term?s frequency in document, qtf is term?s 
frequency in the query, N is the total no. of docu-
ments in collection, df is the number of documents 
that contain the term, dl is the document length 
(number of words), avdl is the average document 
length and k1 (1.0 ? 2.0), b (0.75), k3 (0 -1000) are 
constants. 
During node score computation, the system in-
telligently partitions the query set Q into two parts. 
One part consists of qi?s which are satisfied by at 
least one node from offline search graph. The other 
part consists of qi?s which are not satisfied by any 
node from offline search graph. The system then 
computes query dependent scores for the nodes of 
all the individual documents for the unsatisfied 
keyword set and relevant nodes (having score 
above threshold) are added to the search graph. 
Edge scores are computed only for edges connect-
ing newly added nodes with the existing ones and 
between the new nodes. In this way, the offline 
graph is expanded by adding some query depend-
ent nodes at runtime. Query dependent scoring can 
be made faster using a full text indexing which is a 
mapping Ki ? (Di , Ni); where Ki?s are content 
words (i.e., not stop words)
 
and Di?s and Ni?s are 
respectively the document ids and the node ids 
within the document set. Since, the node score is 
calculated at runtime, it needs to be accelerated. 
Thus a full text index developed offline will be of 
great help. 
5.2 Summary Generation 
Summary generation is basically a keyword search 
technique in the expanded search graph. This is to 
mention that the search technique discussed here is 
basically based on AND semantic, i.e. it requires 
all the keywords to be present in the summary, but 
the algorithm can be modified to take care of OR 
semantic also. Keyword search in graph structure 
is itself a research topic and several efficient algo-
rithms are there to solve the problem. DBXplorer 
(Agrawal et al, 2002), BANKS (Bhalotia et al, 
2002), are popular algorithms in this field which 
consider relational database as graph and devise 
algorithms for keyword based search in the graph. 
Finally, Varadarajan and Hristidis (2006) has pro-
posed Top-k Enumeration and MultiResultExpand-
ing search for constructing total minimum span-
ning tree over a document graph. Any of the above 
popular algorithms can be adapted to use within 
our framework. 
In our system we have used a search algorithm 
which finds different combinations of nodes that 
represent total spanning tree. For each of the com-
bination we compute score of the summary based 
on some IR principle (Varadarajan and Hristidis, 
2006). Then we take the one having best score 
(minimal in our case). If the graph is not too dense, 
then the response time will be small enough. The 
equation given below is used to compute the score 
of individual spanning tree T. 
 
1 1
s c o r e
s c o r e s c o r e
e T
n T
T a b
ne?
?
= +? ?
 
Where scoreT the score of the spanning tree, e and 
n is are edge and node of T respectively, scoree  
and scoren  are edge score and individual node 
score respectively. a and b are non zero positive 
constants in the range of [0 ? 1]. For a particular 
search graph, it is possible to find many total span-
ning trees, having different summary scores. In our 
system, the summary with the best score is consid-
ered. 
In Figure 2 two sample news stories are shown 
along with system identified seed nodes, shown in 
bold. A query based summary from that related 
document set is shown in Figure 3. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
5.3 Paragraph Ordering Scheme 
In the previous sections, the techniques for genera-
tion of summary nodes have been discussed. Here,  
we will investigate the method for ordering them 
into a coherent text.  In case of single document 
summarization, sentence/paragraph ordering is 
done based on the position of extracted paragraphs/ 
sentences in the original document. But in multi-
document scenario, the problem is non trivial since 
information is extracted from different documents 
and no single document can provide ordering. Be-
sides, the ordering of information in two different 
documents may be significantly varying because  
 
       
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Paragraphs of two news articles with five extracted seed/ topic paragraphs (in bold). Un-
derlined paragraphs are added later during graph expansion phase. 
 
Software major IBM has developed a speech recognition technology in Hindi which would help physically challenged and 
less literate Hindi speakers access information through a variety of systems.  [Doc-2, Para - 0 ] 
Besides, the technology could also enable C-DAC to ensure a high level of accuracy in Hindi translation in a number of do-
mains like administration, finance, agriculture and the small-scale industry.   [Doc-1, Para-5] 
A spellchecker to correct spoken-word errors also enhances the accuracy of the system.  [Doc-2, Para - 4 ] 
 
Figure 3. Automatic summary based on {speech recognition, accuracy, spellchecker} query 
P0: Software giant IBM has developed a speech recognition 
software in Hindi. 
P1 : The company hopes that this development will help 
physically challenged and less literate Hindi speakers to 
access information using a variety of applications. 
P2 : The Desktop Hindi Speech Recognition Technology 
developed by the IBM India Software Lab in collabora-
tion with Centre for Development of Advanced Com-
puting would provide a natural interface for human-
computer interaction. 
P3 : The new IBM technology could help to provide a natu-
ral interface for human-computer interaction. 
P4: According to Dr. Daniel Dias, Director, IBM Indian 
Research Laboratory, the technology which helps tran-
scribe continuous Hindi speech instantly into text form, 
could find use in a variety of appli In Figure 1. corre-
lation between two news articles is shown 
as a bipartite graph. cations like voice-enabled 
ATMs, car navigation systems, banking, telecom, railways, 
and airlines. 
P5: Besides, the technology could also enable C-DAC to 
ensure a high level of accuracy in Hindi translation in a 
number of domains like administration, finance, agri-
culture and the small-scale industry. 
P6: The IBM Desktop Hindi Speech Recognition software 
is capable of recognizing over 75,000 Hindi words with 
dialectical variations, providing an accuracy of 90 to 95%. 
P7: What?s more; this software also has an integrated spell-
checker that corrects spoken-word errors, enhancing the 
accuracy to a great extent. 
P8:  The Desktop Hindi Speech Recognition Technology 
also integrates a number of user-friendly features such as 
the facility to convert text to digits and decimals, date and 
currency format, and into fonts which could be imported to 
any Windows-based application. 
P9: ?IBM believes in taking high-end research to the benefit 
of the masses and bridging the digital divide through a 
faster diffusion process,? concluded Dias. 
P0: Software major IBM has developed a speech recog-
nition technology in Hindi which would help physically 
challenged and less literate Hindi speakers access in-
formation through a variety of systems. 
P1 : Called the Desktop Hindi Speech Recognition technol-
ogy, this software was developed by the IBM India Soft-
ware Lab jointly with the Centre for Development of Ad-
vanced Computing. 
P2 : The technology, which helps transcribe continuous 
Hindi speech instantly into text form, could find use in a 
variety of applications like voice-enabled ATMs, car 
navigation systems, banking, telecom, railways and 
airlines, said Dr Daniel Dias, Director, IBM India Re-
search Laboratory. 
P3 : The system can recognize more than 75,000 Hindi 
words with dialectical variations, providing an accuracy 
level of 90-95 per cent, he said. 
P4:  A spellchecker to correct spoken-word errors also 
enhances the accuracy of the system. 
P5:  The technology also has integrated many user-
friendly features such as facility to convert text to digits 
and decimals, date and currency format, and into fonts 
which could be imported to any windows-based applica-
tion. 
P6: "IBM believes in taking high-end research to the benefit 
of the masses and bridging the digital divide through a 
faster diffusion process", Dias said. 
P7: The technology also would enable C-DAC to ensure 
high-level accuracy in Hindi translation in a host of do-
mains, including administration, finance, agriculture and 
small scale industry. 
the writing styles of different authors are different. 
In case of news event summarization, chronologi-
cal ordering is a popular choice which considers 
the temporal sequence of information pieces, when 
deciding the ordering process. 
In this paper, we will propose a scheme of or-
dering which is different from the above two ap-
proaches in that, it only takes into consideration 
the semantic closeness of information pieces 
(paragraphs) in deciding the ordering among them. 
First, the starting paragraph is identified which is 
the paragraph with lowest positional ranking 
among selected ones over the document set. Next 
for any source node (paragraph) we find the sum-
mary node that is not already selected and have 
(correlation value) with the source node. This node 
will be selected as next source node in ordering. 
This ordering process will continue until the nodes 
are totally ordered. The above ordering scheme 
will order the nodes independent of the actual or-
dering of nodes in the original document, thus 
eliminating the source bias due to individual writ-
ing style of human authors. Moreover, the scheme 
is logical because we select a paragraph for posi-
tion p at output summary, based on how coherent it 
is with the (p-1)th paragraph. 
6 Evaluation 
Evaluation of summarization methods is generally 
performed in two ways. Evaluation measure based 
on information retrieval task is termed as the ex-
trinsic method, while the evaluation based on user 
judgments is called the intrinsic measure. We 
adopted the latter, since we concentrated more on 
user?s satisfaction. We measure the quality of out-
put based on the percentage of overlap of system 
generated output with the manual extract. Salton et 
al (1997) observed that an extract generated by one 
person is likely to cover 46% of the information 
that is regarded as most important by another per-
son. Mitra et. al. (1998) proposed an interesting 
method for evaluation of paragraph based auto-
matic summarization and identified the following 
four quality-measures ? Optimistic (O), Pessimistic 
(P), Intersection (I) and Union (U) based evalua-
tion. For evaluation purpose, we identify different 
related document set (D) from different domains 
like technical, business etc and keyword (query) 
list for each domain. Users are asked to manually 
prepare the multi-document summarization based 
on the given queries. They prepared it by marking 
relevant paragraphs over D. Based on the excerpts 
prepared by the users; the above scores are calcu-
lated as O: Percentage overlap with that manual 
extract for which the number of common para-
graphs is highest, P: Percentage overlap with that 
manual extract for which the number of common 
paragraphs is lowest; I: Percentage overlap with 
the intersection of manual extracts; U: Percentage 
overlap with the union of manual extracts. The re-
sults are shown in Table 1. A comparative survey 
of quality measures for the set of articles is shown 
in Figure 3. 
 
              Table 1.  Evaluation score 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  D Omeasure Pmeasure Imeasure Umeasure 
article1 
&    
article2 
44.4 27 33.3 66.6 
article3 
&    
article4 
75 60 50 100 
article5 
&    
article6 
50 35.5 25 66 
article7 
&    
article8 
45.5 28.7 33.3 56.4 
O -  M easure
0
10
20
30
40
50
60
70
80
1 2 3 4
Dat a Set
Sc
or
e
Sc
or
e
Sc
or
e
Sc
or
e
P - Measure
0
20
40
60
80
1 2 3 4
Data Set
Sc
o
re
  
 
 
 
 
 
 
 
 
 
 
     
 
 
 
 
 
 
 
 
 
 
 
Figure 3.  Comparative measure scores for set 
                 of articles 
7 Baseline Approach to Multilingual 
Summarization 
Our baseline approach to multilingual multidocu-
ment summarization is to apply our English based 
multi-document summarization system to other 
non-English languages like Hindi, Bengali, Japa-
nese etc. We have initially implemented the system 
for English language only, but it can be modified 
to work in multilingual scenario also. To work 
with other languages, the system requires some 
language dependent tools for that particular lan-
guage: 
1) A stop word list of that language is required be-
cause they have no significance in finding similar-
ity between the paragraphs and need to be removed 
during initial preprocessing stage. 
2) A language dependent stemmer is required. In 
most of the languages, stemmer is yet to be devel-
oped. Another problem is that suffix stripping is 
not the only solution for all languages because 
some languages have affix, circumfix etc. in their 
inflected form. A morphological analyzer to find 
the root word may be used for those languages. 
3) A lexicon for that language is required to match 
the similar words. For English, WordNet is widely 
available. For other languages also, similar type of 
lexicons are required. 
If these tools are available then our system can 
be tuned to generate query dependent multilingual 
multi-document summary. 
8 Conclusion and Future Work 
In this work we present a graph based approach for 
query dependent multi-document summarization 
system. Considering its possible application in the 
web document, we clearly divided the computation 
into two segments. Extraction of seed/topic sum-
mary nodes and construction of offline graph is a 
part of query independent computation. At query 
time, the precomputed graph is processed to extract 
the best multi-document summary. We have tested 
our algorithm with news articles from different 
domains. The experimental results suggest that our 
algorithm is effective. Although we experimented 
with pair of articles, the proposed algorithm can be 
improved to handle more than two articles simul-
taneously. 
The important aspect of our system is that it can 
be modified to compute query independent sum-
mary which consists of topic nodes, generated dur-
ing preprocessing stage. The paragraph ordering 
module can be used to define ordering among 
those topic paragraphs. Another important aspect is 
that our system can be tuned to generate summary 
with custom size specified by users. The spanning 
tree generation algorithm can be so modified that it 
produces not only total spanning tree but also takes 
care of the size requirement of user. Lastly, it is 
shown that our system can generate summary for 
other non-English documents also if some lan-
guage dependent tools are available. 
The performance of our algorithm greatly de-
pends on quality of selection of topic nodes. So if 
we can improve the identification of topic para-
graphs and shared topics among multiple docu-
ments it would surely enhance the quality of our 
system. 
9 References 
A. Singhal , M. Mitra, and C. Buckley. 1997. Automatic 
Text Summarization by Paragraph Extraction. Pro-
ceedings of ACL/EACL Workshop.  
C.-Y. Lin and E.H. Hovy. 2002. From Single to Multi-
document Summarization: A Prototype System and 
its Evaluation. Proceedings of ACL: 457?464. 
I -  M easure
0
10
20
30
40
50
60
1 2 3 4
Dat a Set
Sc
or
e
U  -  M easure
0
20
40
60
80
100
120
1 2 3 4
Dat a Set
Sc
or
e
D.R. Radev, H. Jing, M. Sty? and D. Tam. 2004. Cen-
troid-based summarization of multiple documents. 
Information Processing and Management, 
Vol.40:919?938. 
G. Salton , A. Singhal , M. Mitra, and C. Buckley. 1997. 
Automatic text structuring and summarization. In-
formation Processing and Management: Vol. 33, No. 
2: 193-207. 
G. Bhalotia, C. Nakhe, A. Hulgeri, S. Chakrabarti and 
S.Sudarshan. 2002. Keyword Searching and Brows-
ing in Databases using BANKS. Proceedings of 
ICDE : 431-440. 
 
H. Hardy, N. Shimizu, T. Strzalkowski, L. Ting, G. B. 
Wise and X. Zhang. 2002. Cross-document summari-
zation by concept classification. Proceedings of 
SIGIR.02: 65-69 . 
I. Mani and E. Bloedorn. 2000. Summarizing Similari-
ties and Differences Among Related Documents. In-
formation Retrieval, Vol. 1(1): 35-67. 
M. Porter. 1980. An algorithm for suffix stripping. Pro-
gram, 14(3):130?137. 
R. Varadarajan,. V. Hristidis. 2006. A system for query-
specific document summarization. Proceedings of 
CIKM 2006: 622-631. 
S. Agrawal, S. Chaudhuri, and G. Das.2002. DBXplorer: 
A System for Keyword-Based Search over Relational 
Databases. Proceedings of ICDE: 5-16. 
Y. Zhang, X. Ji, C. H. Chu, and H. Zha. 2004. Correlat-
ing Summarization of Multisource News with K-
Way Graph Biclustering. SIGKDD Explorations 
6(2): 34-42. 
Bengali, Hindi and Telugu to English Ad-hoc Bilingual task  
 
Sivaji Bandyopadhyay, Tapabrata Mondal, Sudip Kumar Naskar, 
Asif Ekbal, Rejwanul Haque, Srinivasa Rao Godavarthy 
 
Abstract 
 
This paper presents the experiments carried out at Jadavpur University as 
part of participation in the CLEF 2007 ad-hoc bilingual task. This is our first 
participation in the CLEF evaluation task and we have considered Bengali, 
Hindi and Telugu as query languages for the retrieval from English 
document collection. We have discussed our Bengali, Hindi and Telugu to 
English CLIR system as part of the ad-hoc bilingual task, English IR system 
for the ad-hoc monolingual task and the associated experiments at CLEF. 
Query construction was manual for Telugu-English ad-hoc bilingual task, 
while it was automatic for all other tasks. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Development of Bengali Named Entity Tagged Corpus and its Use in 
NER Systems 
Asif Ekbal 
Department of Computer Science and 
Engineering, Jadavpur University 
Kolkata-700032, India 
asif.ekbal@gmail.com 
Sivaji Bandyopadhyay 
Department of Computer Science and  
Engineering, Jadavpur University 
Kolkata-700032, India 
sivaji_cse_ju@yahoo.com 
 
Abstract 
The rapid development of language tools 
using machine learning techniques for less 
computerized languages requires appropri-
ately tagged corpus. A Bengali news cor-
pus has been developed from the web ar-
chive of a widely read Bengali newspaper. 
A web crawler retrieves the web pages in 
Hyper Text Markup Language (HTML) 
format from the news archive. At present, 
the corpus contains approximately 34 mil-
lion wordforms. The date, location, re-
porter and agency tags present in the web 
pages have been automatically named en-
tity (NE) tagged. A portion of this partially 
NE tagged corpus has been manually anno-
tated with the sixteen NE tags with the help 
of Sanchay Editor1, a text editor for Indian 
languages. This NE tagged corpus contains 
150K wordforms. Additionally, 30K word-
forms have been manually annotated with 
the twelve NE tags as part of the IJCNLP-
08 NER Shared Task for South and South 
East Asian Languages 2 . A table driven 
semi-automatic NE tag conversion routine 
has been developed in order to convert the 
sixteen-NE tagged corpus to the twelve-NE 
tagged corpus. The 150K NE tagged cor-
pus has been used to develop Named Entity 
Recognition (NER) system in Bengali us-
ing pattern directed shallow parsing ap-
proach, Hidden Markov Model (HMM), 
Maximum Entropy (ME) Model, Condi-
                                                 
1 sourceforge.net/project/nlp-sanchay 
2 http://ltrc.iiit.ac.in/ner-ssea-08 
tional Random Field (CRF) and Support 
Vector Machine (SVM). Experimental re-
sults of the 10-fold cross validation test 
have demonstrated that the SVM based 
NER system performs the best with an 
overall F-Score of 91.8%. 
1 Introduction 
The mode of language technology work has been 
changed dramatically since the last few years with 
the web being used as a data source in a wide 
range of research activities. The web is anarchic, 
and its use is not in the familiar territory of compu-
tational linguistics. The web walked into the ACL 
meetings started in 1999. The use of the web as a 
corpus for teaching and research on language tech-
nology has been proposed a number of times 
(Rundel, 2000; Fletcher, 2001; Robb, 2003; 
Fletcher, 2003). There is a long history of creating 
a standard for western language resources. The 
human language technology (HLT) society in 
Europe has been particularly zealous for the stan-
dardization, making a series of attempts such as 
EAGLES3, PROLE/SIMPLE (Lenci et al, 2000), 
ISLE/MILE (Calzolari et al, 2003; Bertagna et al, 
2004) and more recently multilingual lexical data-
base generation from parallel texts in 20 European 
languages (Giguet and Luquet, 2006). On the other 
hand, in spite of having great linguistic and cul-
tural diversities, Asian language resources have 
received much less attention than their western 
counterparts. A new project (Takenobou et al, 
2006) has been started to create a common stan-
dard for Asian language resources. They have ex-
tended an existing description framework, the 
                                                 
3 http://www.ilc.cnr.it/Eagles96/home.html 
The 6th Workshop on Asian Languae Resources, 2008
1
MILE (Bertagna et al, 2004), to describe several 
lexical entries of Japanese, Chinese and Thai. India 
is a multilingual country with the enormous cul-
tural diversities. (Bharati et al, 2001) reports on 
efforts to create lexical resources such as transfer 
lexicon and grammar from English to several In-
dian languages and dependency tree bank of anno-
tated corpora for several Indian languages. Corpus 
development work from web can be found in (Ek-
bal and Bandyopadhyay, 2007d) for Bengali. 
Named Entity Recognition (NER) is one of the 
core components in most Information Extraction 
(IE) and Text Mining systems. During the last few 
years, the probabilistic machine learning methods 
have become state of the art for NER (Bikel et al, 
1999; Chieu and Ng, 2002) and for field extraction 
(McCallum et al, 2000). Most prominently, Hid-
den Markov Models (HMMs) have been used for 
the information extraction task (Bikel et al, 1999). 
Beside HMM, there are other systems based on 
Support Vector Machine (Sun et al, 2003) and 
Na?ve Bayes (De Sitter and Daelemans, 2003). 
Maximum Entropy (ME) conditional models like 
ME Markov models (McCallum et al, 2000) and 
Conditional Random Fields (CRFs) (Lafferty et al, 
2001) were reported to outperform the generative 
HMM models on several IE tasks.  
The existing works in the area of NER are 
mostly in non-Indian languages. There has been a 
very little work in the area of NER in Indian lan-
guages (ILs). In ILs, particularly in Bengali, the 
work in NER can be found in (Ekbal and 
Bandyopadhyay, 2007a; Ekbal and Bandyop-
adhyay, 2007b; Ekbal et al, 2007c). Other than 
Bengali, the work on NER can be found in (Li and 
McCallum, 2003) for Hindi. 
Newspaper is a huge source of readily available 
documents. In the present work, the corpus has 
been developed from the web archive of a very 
well known and widely read Bengali newspaper. 
Bengali is the seventh popular language in the 
world, second in India and the national language of 
Bangladesh. A code conversion routine has been 
written that converts the proprietary codes used in 
the newspaper into the standard Indian Script Code 
for Information Interchange (ISCII) form, which 
can be processed for various tasks. A separate code 
conversion routine has been developed for convert-
ing ISCII codes to UTF-8 codes. A portion of this 
corpus has been manually annotated with the six-
teen NE tags as described in Table 3. Another por-
tion of the corpus has been manually annotated 
with the twelve NE tags as part of the IJCNLP-08 
NER Shared Task for South and South East Asian 
Languages. A table driven semi-automatic NE tag 
conversion routine has been developed in order to 
convert this corpus to a form tagged with the 
twelve NE tags. The NE tagged corpus has been 
used to develop Named Entity Recognition (NER) 
system in Bengali using pattern directed shallow 
parsing approach, HMM, ME, CRF and SVM 
frameworks.  
A number of detailed experiments have been 
conducted to identify the best set of features for 
NER in Bengali. The ME, CRF and SVM based 
NER models make use of the language independ-
ent as well as language dependent features. The 
language independent features could be applicable 
for NER in other Indian languages also. The sys-
tem has demonstrated the highest F-Score value of 
91.8% with the SVM framework. One possible 
reason behind its best performance may be the 
flexibility of the SVM framework to handle the 
morphologically rich Indian languages.    
2 Development of the Named Entity 
Tagged Bengali News Corpus  
2.1 Language Resource Acquisition 
A web crawler has been developed that retrieves 
the web pages in Hyper Text Markup Language 
(HTML) format from the news archive of a leading 
Bengali newspaper within a range of dates 
provided as input. The crawler generates the 
Universal Resource Locator (URL) address for the 
index (first) page of any particular date. The index 
page contains actual news page links and links to 
some other pages (e.g., Advertisement, TV 
schedule, Tender, Comics and Weather etc.) that 
do not contribute to the corpus generation. The 
HTML files that contain news documents are 
identified and the rest of the HTML files are not 
considered further. 
2.2 Language Resource Creation 
The HTML files that contain news documents are 
identified by the web crawler and require cleaning 
to extract the Bengali text to be stored in the 
corpus along with relevant details. The HTML file 
is scanned from the beginning to look for tags like 
<fontFACE=BENGALI_FONT_NAME>...<font>, 
where the BENGALI_FONT_NAME is the name 
The 6th Workshop on Asian Languae Resources, 2008
2
of one of the Bengali font faces as defined in the 
news archive. The Bengali text enclosed within 
font tags are retrieved and stored in the database 
after appropriate tagging. Pictures, captions and 
tables may exist anywhere within the actual news. 
Tables are integral part of the news item. The 
pictures, its captions and other HTML tags that are 
not relevant to our text processing tasks are 
discarded during the file cleaning. The Bengali 
news corpus has been developed in both ISCII and 
UTF-8 codes.  The tagged news corpus contains 
108,305 number of news documents with about 
five (5) years (2001-2005) of news data collection. 
Some statistics about the tagged news corpus are 
presented in Table 1. 
 
Total number of news documents 
in the corpus 
108, 305 
Total number of sentences in the 
corpus 
2, 822, 737 
Avgerage number of sentences in 
a document  
27 
Total number of wordforms in 
the corpus 
33, 836, 736 
Avgerage number of wordforms 
in a document 
313 
Total number of distinct 
wordforms in the corpus 
467, 858 
Table 1.  Corpus Statistics 
2.3 Language Resource Annotation 
The Bengali news corpus collected from the web is 
annotated using a tagset that includes the type and 
subtype of the news, title, date, reporter or agency 
name, news location and the body of the news. A 
part of this corpus is then tagged with a tagset, 
consisting of sixteen NE tags and one non-NE tag.  
Also, a part of the corpus has been tagged with a 
tagest of twelve NE tags4, defined for the IJCNLP-
08 NER Shared Task for South and South East 
Asian Languages. 
A news corpus, whether in Bengali or in any 
other language, has different parts like title, date, 
reporter, location, body etc. To identify these parts 
in a news corpus the tagset, described in Table 2, 
has been defined. The reporter, agency, location, 
date, bd, day and ed tags help to recognize the 
person name, organization name, location name 
                                                 
4http://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=3 
and the various date expressions that appear in the 
fixed places of the newspaper. These tags are not 
able to recognize the various NEs that appear 
within the actual news body. 
In order to identify NEs within the actual news 
body, we have defined a tagset consisting of 
seventeen tags. We have considered the major four 
NE classes, namely ?Person name?, ?Location 
name?, ?Organization name? and ?Miscellaneous 
name?. Miscellaneous names include the date, 
time, number, percentage and monetary 
expressions. The four major NE classes are further 
divided in order to properly denote each 
component of the multiword NEs. The NE tagset is 
shown in Table 3 with the appropriate examples. 
We have also tagged a portion of the corpus as 
part of the IJCNLP-08 NER Shared Task for South 
and South East Asian Languages. This tagset has 
twelve different tags. The underlying reason for 
adopting these tags was the necessity of a slightly 
finer tagset for various natural language processing 
(NLP) applications and particularly for machine 
translation. The IJCNLP-08 NER shared task 
tagset is shown in Table 4.      
One important aspect of IJCNLP-08 NER 
shared task was to annotate only the maximal NEs 
and not the structures of the entities. For example, 
mahatma gandhi road is annotated as location and 
assigned the tag ?NEL? even if  mahatma and gan-
dhi are NE title person and person name, respec-
tively, according to the IJCNLP-08 shared task 
tagset. These internal structures of the entities need 
to be identified during testing. So, mahatma gan-
dhi road will be tagged as mahatma/NETP gan-
dhi/NEP road/NEL. The structure of the tagged 
element using the Shakti Standard Format (SSF)5 
will be as follows: 
1 (( NP <ne=NEL>  
1.1 (( NP <ne=NEP> 
1.1.1 (( NP  <ne=NETP> 
1.1.1.1 mahatma 
)) 
1.1.2 gandhi 
)) 
1.2 road 
)) 
                                                 
5http://shiva.iiit.ac.in/SPSAL 2007/ssf.html  
The 6th Workshop on Asian Languae Resources, 2008
3
2.4 Partially Tagged News Corpus Develop-
ment 
A news document is stored in the corpus in XML 
format using the tagset, mentioned in Table 2. In 
the HTML news file, the date is stored at first and 
is divided into three parts. The first one is the date 
according to Bengali calendar, second one is the 
day in Bengali and the last one is the date accord-
ing to English calendar. Both Bengali and English 
dates are stored in the form ?day month year?.  
A sequence of four Bengali digits separates the 
Bengali date from the Bengali day. The English 
date starts with one/two digits in Bengali font. 
Bengali date, day and English date can be distin-
guished by checking the appearance of the numer-
als and these are tagged as <bd>, <day> and <ed>, 
respectively. For e.g., 25 sraban 1412  budhbar  10 
august 2005 is tagged as shown in Table 5. 
 
Table 2. News Corpus Tagset 
 
 
Tag Meaning Example 
PER Single word person name sachin / PER, manmohan/PER 
LOC Single word location name jadavpur / LOC, delhi/LOC 
ORG Single word organization name infosys / ORG, tifr/ORG 
MISC Single word miscellaneous name 100% / MISC, 100/MISC 
B-PER 
I-PER 
E-PER 
Beginning, Internal or the end of 
a multiword person name 
sachin/ B-PER ramesh / I-PER  
tendulkar / E- PER 
B-LOC 
I-LOC 
E-LOC 
Beginning, Internal or the end of 
a multiword location name 
mahatma/ B-LOC gandhi / I-LOC  road / E-LOC 
B-ORG 
I-ORG 
E-ORG 
Beginning, Internal or the end of 
a multiword organization name 
bhaba / B-ORG atomic / I-ORG   
research / I-ORG centre / E-ORG 
B-MISC 
I-MISC 
E-MISC 
Beginning, Internal or the end of 
a multiword miscellaneous name 
10 e / B-MISC   magh / I-MISC  
1402 / E-MISC 
NNE Words that are not named entities neta/NNE, bidhansabha/NNE 
 
 
Table 3. Named Entity Tagset 
 
 
Tag Definition Tag Definition Tag Definition 
header Header of the news 
documents 
day Day body Body of the news 
document 
title Headline of the news 
document 
ed English date p Paragraph 
t1 1st headline of the 
title 
reporter Reporter name table Information in tabular 
form 
t2 
2nd headline of the 
title 
agency Agency 
providing news 
tc 
Table column 
date 
Date of the news 
document 
location News location tr Table row 
bd Bengali date  
The 6th Workshop on Asian Languae Resources, 2008
4
NE tag Meaning Example 
NEP Person name sachin ramesh  tendul-
kar / NEP 
NEL Location 
name 
mahatma gandhi road / 
NEL 
NEO Organization 
name 
bhaba atomic   
research   centre / NEO 
NED Designation chairman/NED, 
sangsad/NED  
NEA Abbreviation b a/NEA, c m d a/NEA, 
b j p/NEA 
NEB Brand fanta/NEB, 
windows/NEB 
NETP Title-person sriman/NED, sree/NED 
NETO Title-object american beauty/NETO
NEN Number 10/NEN, dash/NEN 
NEM Measure tin din/NEM, panch 
keji/NEM 
NETE Terms hidden markov 
model/NETE 
NETI Time 10 e   magh   1402/ 
NETI 
Table 4. IJCNLP-08 NER Shared Task Tagset 
 
Original date pattern Tagged date pattern 
 <date> 
   25 sraban 1412 <bd>25 sraban 1412 
</bd> 
    budhbar <day>budhbar 
</day> 
10 august 2005 <ed>10 august 2005 
</ed> 
 </date> 
Table 5. Example of a Tagged Date Pattern 
2.5 Named Entity Tagged Corpus 
Development 
The partially NE tagged corpus contains 34 
million wordforms and are in both ISCII and UTF-
8 forms. A portion of this corpus, containing 150K 
wordforms, has been manually annotated with the 
sixteen NE tags that are listed in Table 3. The 
corpus has been annotated with the help of 
Sanchay Editor, a text editor for Indian languages. 
The detailed statistics of this NE-tagged corpus are 
given in Table 6. The corpus is in SSF form, 
which has the following structure: 
 
 
 
<Story id=""> 
<Sentence id=""> 
1 biganni NNE  
2 newton PER 
3 .  
</Sentence id=""> 
 .  
</Story id=""> 
 
Another portion of the partially NE tagged 
Bengali news corpus has been manually annotated 
as part of the IJCNLP-08 NER shared task with 
the twelve NE tags, as listed in Table 4. The 
annotation process has been very difficult due to 
the presence of a number of ambiguous NE tags. 
The potential ambiguous NE tags are: NED vs 
NETP, NEO vs NEB, NETE vs NETO, NETO vs 
NETP and NEN vs NEM. For example, it is 
difficult to decide whether ?Agriculture? is 
?NETE?, and if no then whether ?Horticulture? is 
?NETE? or not. In fact, this the most difficult class 
to identify. This NE tagged corpus contains 
approximately 30K wordforms. Details statistics 
of this tagged corpus are shown in Table 7. This 
NE tagged corpus is in the following SSF form.  
   
   <Story id=""> 
   <Sentence id=""> 
1 (( NP <ne=NEP> 
1.1 (( NP <ne=NED> 
1.1.1 biganni 
)) 
   1.1.2 newton NEP 
 )) 
2 . 
   </Sentence id=""> 
   </Story id=""> 
   
NE Class Number of 
wordforms 
Number of dis-
tinct wordforms 
Person name 20, 455 15, 663 
Location 
name 
11, 668    5, 579 
Organization 
name 
963 867 
Miscellane-
ous name 
11,554 3, 227 
Table 6. Statistics of the 150K-tagged Corpus 
The 6th Workshop on Asian Languae Resources, 2008
5
2.6 Tag Conversion 
A tag conversion routine has been developed in 
order to convert the sixteen-NE tagged corpus of 
150K wordforms to the corpus, tagged with the 
IJCNLP-08 twelve-NE tags. This conversion is a 
semi-automatic process. Some of our sixteen NE 
tags can be automatically mapped to some of the 
IJCNLP-08 shared task tags. The tags that repre-
sent person, location and organization names can 
be directly mapped to the NEP, NEL and NEO 
tags, respectively. Other IJCNLP-08 shared task 
tags can be obtained with the help of gazetteer lists 
and simple heuristics. In our earlier NER experi-
ments, we have already developed a number of 
gazetteer lists such as: lists of person, location and 
organization names; list of prefix words (e.g., sree, 
sriman etc.) that predict the left boundary of a per-
son name; list of designation words (e.g., mantri, 
sangsad etc.) that helps to identify person names. 
The lists of prefix and designation words are help-
ful to find the NETP and NED tags. The sixteen-
NE tagged corpus is searched for the person name 
tags and the previous word is matched against the 
lists of prefix and designation words. The previous 
word is tagged as NED or NETP if there is a 
match with the lists of designation words and pre-
fix words, respectively. The NEN and NETI tags 
can be obtained by looking at our miscellaneous 
tags and using some simple heuristics. The NEN 
tags can be simply obtained by checking whether 
the MISC tagged element consists of digits only. 
The lists of cardinal and ordinal numbers have 
been also kept to recognize NETI tags. A list of 
words that denote the measurements (e.g., kilo-
gram, taka, dollar etc.) has been kept in order to 
get the NEM tag. The lists of words, denoting the 
brand names, title-objects and terms, have been 
prepared to get the NEB, NETO and NETE tags. 
The NEA tags can be obtained with the help of a 
gazetteer list and using some simple heuristics 
(whether the word contains the dot and there is no 
space between the characters). The mapping from 
our NE tagset to the IJCNLP-08 NER shared task 
tagset is shown in Table 8. 
3 Use of Language Resources 
The NE tagged news corpus, developed in this 
work, has been used to develop the Named Entity 
Recognition (NER) systems in Bengali using pat-
tern directed shallow parsing, HMM, ME, CRF 
and SVM frameworks. 
 
NE Class Number of 
wordforms 
Number of dis-
tinct wordforms 
Person name 5, 123 3, 201 
Location 
name 
1, 675 1, 119 
Organization 
name 
168 131 
Designation 231 102 
Abbreviation 32 21 
Brand 15 12 
Title-person 79 51 
Title-object 63 42 
Number 324 126 
Measure 54 31 
Time 337 212 
Terms 46 29 
Table 7. Statistics of the 30K-tagged Corpus 
 
Sixteen-NE tagset IJCNLP-08 
tagset 
PER, LOC, ORG NEP, NEL, 
NEO 
B-PER, I-PER, E-PER NEP 
B-LOC, I-LOC, E-LOC NEL 
B-ORG, I-ORG, E-ORG NEO 
MISC  NEN  
B-MISC, I-MISC, E-MISC NETI, NEM 
Brand name gazetteer  NEB 
Title-object gazetteer  NETO 
Term gazetteer  NETE 
Person prefix word + PER/B-
PER, I-PER, E-PER 
NETP 
Designation word +PER/B-
PER, I-PER, E-PER 
NED 
Abbreviation gazetteer + 
Heuristics 
NEA 
Table 8. Tagset Mapping Table  
 
 
We have considered the sixteen NE tags to de-
velop these systems. Named entity recognition in 
Indian Languages (ILs) in general and particularly 
in Bengali is difficult and challenging as there is 
no concept of capitalization in ILs. 
The Bengali NER systems that use pattern di-
rected shallow parsing approach can be found in 
The 6th Workshop on Asian Languae Resources, 2008
6
(Ekbal and Bandyopadhyay, 2007a) and (Ekbal 
and Bandyopadhyay, 2007b). An HMM-based 
Bengali NER system can be found in (Ekbal and 
Bandyopadhyay, 2007c). These systems have been 
trained and tested with the corpus tagged with the 
sixteen NE tags.  
A number of experiments have been conducted 
in order to find out the best feature set for NER in 
Bengali using the ME, CRF and SVM frameworks. 
In all these experiments, we have used a number of 
gazetteer lists such as: first names (72, 206 en-
tries), middle names (1,491 entries) and sur names 
(5,288 entries) of persons; prefix (245 entries) and 
designation words (947 entries) that help to detect 
person names; suffixes (45 and 23 entries) that 
help to identify person and location names; clue 
words (94 entries) that help to detect organization 
names; location name (7, 870 entries) and organi-
zation name (2,225 entries). Apart from these gaz-
etteer lists, we have used the prefix and suffix 
(may not be linguistically meaningful suf-
fix/prefix) features, digit features, first word fea-
ture and part of speech information of the words 
etc. We have used the C++ based Maximum En-
tropy package6, C++ based OpenNLP CRF++ pack-
age7 and open source YamCha8 tool for ME based 
NER, CRF based NER and SVM based NER, re-
spectively. For SVM based NER system, we have 
used TinySVM 9  classifier, pair wise multi-class 
decision method and the second-degree polynomial 
kernel function. The brief descriptions of all the 
models are given below: 
?A: Pattern directed shallow parsing approach 
without linguistic knowledge. 
?B: Pattern directed shallow parsing approach with 
linguistic knowledge. 
?HMM based NER: Trigram model with additional 
context dependency, NE suffix lists for handling 
unknown words. 
?ME based NER: Contextual window of size three 
(current, previous and the next word), prefix and 
suffix of length upto three of the current word, 
POS information of the current word, NE informa-
tion of the previous word (dynamic feature), dif-
ferent digit features and the various gazetteer liststs. 
                                                 
6http://homepages.inf.ed.ac.uk/s0450736/software/maxe
nt/maxent-20061005.tar.bz2 
7 http://crfpp.sourceforge.net 
8 http://chasen.org/~taku/software/yamcha/ 
9http://cl.aist-nara.ac.jp/taku-ku/software/TinySVM 
?CRF based NER: Contextual window of size five 
(current, previous two words and the next two 
words), prefix and suffix of length upto three of the 
current word, POS information of window three 
(current word, previous word and the next word), 
NE information of the previous word (dynamic 
feature), different digit features and the various 
gazetteer lists. 
?SVM based NER: Contextual window of size six 
(current, previous three words and the next two 
words), prefix and suffix of length upto three of the 
current word, POS information of window three 
(current word, previous word and the next word), 
NE information of the previous two words (dy-
namic feature), different digit features and the vari-
ous gazetteer lists. 
Evaluation results of the 10-fold cross validation 
test for all the models are presented in Table 9. 
Evaluation results clearly show that the SVM 
based NER model outperforms other models due to 
it?s efficiency to handle the non-independent, di-
verse and overlapping features of Bengali lan-
guage.    
 
Model F-Score (in %) 
A 74.5 
B 77.9 
HMM 84.5 
ME 87.4 
CRF 90.7 
SVM 91.8 
Table 9.Results of 10-fold Cross Validation Test 
4 Conclusion 
In this work we have developed a Bengali news 
corpus of approximately 34 million wordforms 
from the web archive of a leading Bengali newspa-
per. The date, location, reporter and agency tags 
present in the web pages have been automatically 
NE tagged. Around 150K wordforms of this par-
tially NE tagged corpus has been manually anno-
tated with the sixteen NE tags. We have also 
tagged around 30K wordforms with the twelve NE 
tags, defined for the IJCNLP-08 NER shared task. 
A tag conversion routine has also been developed 
in order to convert any sixteen-NE tagged corpus 
to the twelve-NE tagged corpus.  The sixteen-NE 
tagged corpus of 150K wordforms has been used to
The 6th Workshop on Asian Languae Resources, 2008
7
develop the NER systems using various machine-
learning approaches. 
This NE tagged corpus can be used for other 
NLP research activities such as machine 
translation, information retrieval, cross-lingual 
event tracking, automatic summarization etc. 
References 
Bertagna, M. and A. Lenci, M. Monachini and N. Cal-
zolari. 2004. CotentInteroperability of Lexical Re-
sources, Open Issues and ?MILE? Perspectives, In 
Proceedings of the LREC, 131-134. 
Bharthi, A., D.M. Sharma, V. Chaitnya, A. P. Kulkarni 
and R. Sanghal. 2001. LERIL: Collaborative Effort 
for Creating Lexical Resources. In Proceedings of 
the 6th NLP Pacific Rim Symposium Post-Conference 
Workshop, Japan. 
Bikel, D. M., Schwartz, R., Weischedel, R. M. 1999. An 
Algorithm that Learns What?s in a Name. Machine 
Learning, 34, 211-231. 
Calzolari, N., F. Bertagna, A. Lenci and M. Monachni. 
2003. Standards and best Practice for Miltilingual 
Computational Lexicons, MILE (the multilingual 
ISLE lexical entry). ISLE Deliverable D2.2 &3.2.  
Chieu, H. L., Tou Ng, H. 2002. Named Entity Recogni-
tion: A Maximum Entropy Approach Using Global 
Information, In Proc. of the 6th  Workshop on Very 
Large Corpora.  
De Sitter, A., Daelemans W. 2003. Information Extrac-
tion via Double Classification. In Proeedings of In-
ternational Workshop on Adaptive Text Extraction 
and Mining, Dubronik. 
Ekbal, Asif, and S. Bandyopadhyay. 2007a. Pattern 
Based Bootstrapping Method for Named Entity Rec-
ognition. In Proceedings of the 6th International Con-
ference on Advances in Pattern Recognition, 2007, 
India, 349-355. 
Ekbal, Asif, and S. Bandyopadhyay. 2007b. Lexical 
Pattern Learning from Corpus Data for Named Entity 
Recognition. In Proceedings of the 5th International 
Conference on Natural Language Processing 
(ICON), India, 123-128. 
Ekbal, Asif, Naskar, Sudip and S. Bandyopadhyay.   
2007c. Named Entity Recognition and Transliteration 
in Bengali, Named Entities: Recognition, 
Classification and Use, Special Issue of Lingvisticae 
Investigationes Journal, 30:1 (2007), 95-114. 
Ekbal, Asif, and S. Bandyopadhyay. 2007d. A Web-
based Bengali News Corpus for Named Entity Rec-
ognition. Language Resources and Evaluation Jour-
nal (Accepted and to appear by December 2007). 
Fletcher, W. H. 2001. Making the Web More Useful as 
Source for Linguistics Corpora. In Ulla Conor and 
Thomas A. Upton (eds.), Applied corpus Linguistics: 
A Multidimensional Perspective, 191-205. 
Fletcher, W. H. 2003. Concording the Web with 
KwiCFinder. In Proceedings of the Third North 
American Symposium on Corpus Linguistics and 
Language Teaching, Boston. 
Giguet, E., and P. Luquet. 2006. Multilingual Lexical 
Database Generation from Parallel Texts in 20 Euro-
pean Languages with Endogeneous Resources. In 
Proceedings of the COLING/ACL, Sydney, 271-278. 
Lafferty, J., McCallum, A., and Pereira, F. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data, In Proceedings 
of the 18th International Conference on Machine 
Learning, 282-289. 
Lenci, A., N. Bel, F. Busu, N. Calzolari, E. Gola, M. 
Monachini, A. Monachini, A. Ogonowski, I. Peters, 
W. Peters, N. Ruimy, M. Villegas and A. Zampolli. 
2000. SIMPLE: A general Framework for the Devel-
opment of Multilingual Lexicons. International 
Journal of Lexicography, Special Issue, Dictionaries, 
Thesauri and Lexical-Semantic Relations, XIII(4): 
249-263. 
Li, Wei and Andrew McCallum. 2004. Rapid Develop-
ment of Hindi Named Entity Recognition Using 
Conditional Random Fields and Feature Inductions. 
ACM TALIP, Vol. 2(3), 290-294. 
McCallum, A., Freitag, D., Pereira, F. 2000. Maximum 
Entropy Markov Models for Information Extraction 
and Segmentation.  In Proceedings of the 17th Inter-
national Conference Machine Learning. 
Robb, T. 2003. Google as a Corpus Tool? ETJ Journal, 
4(1), Spring 2003. 
Rundell, M. 2000. The Biggest Corpus of All. Humanis-
ing Language Teaching, 2(3). 
Sun, A., et al 2003. Using Support Vector Machine for 
Terrorism Information Extraction. In Proceedings of 
1st NSF/NIJ Symposium on Intelligence and Security.   
Takenobou, T., V. Sornlertlamvanich, T. Charoenporn, 
N. Calzolari, M. Monachini, C. Soria, C. Huang, X. 
YingJu, Y. Hao, L. Prevot and S. Kiyoaki. 2006. In-
frastructure for Standardization of Asian Languages 
Resources. In Proceedings of the COLING/ACL 
2006, Sydney, 827-834. 
The 6th Workshop on Asian Languae Resources, 2008
8
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 191?198,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Modified Joint Source-Channel Model for Transliteration 
 
 
Asif Ekbal 
Comp.  Sc. & Engg. Deptt. 
 Jadavpur University 
India 
ekbal_asif12@ 
yahoo.co.in 
Sudip Kumar Naskar 
Comp.  Sc. & Engg. Deptt. 
Jadavpur University 
India 
sudip_naskar@ 
hotmail.com 
Sivaji Bandyopadhyay 
Comp.  Sc. & Engg. Deptt.  
Jadavpur University 
India 
sivaji_cse_ju@ 
yahoo.com 
 
 
Abstract 
Most machine transliteration systems 
transliterate out of vocabulary (OOV) 
words through intermediate phonemic 
mapping. A framework has been 
presented that allows direct 
orthographical mapping between two 
languages that are of different origins 
employing different alphabet sets. A 
modified joint source?channel model 
along with a number of alternatives have 
been proposed. Aligned transliteration 
units along with their context are 
automatically derived from a bilingual 
training corpus to generate the 
collocational statistics. The transliteration 
units in Bengali words take the pattern 
C+M where C represents a vowel or a 
consonant or a conjunct and M represents 
the vowel modifier or matra. The English 
transliteration units are of the form C*V* 
where C represents a consonant and V 
represents a vowel. A Bengali-English 
machine transliteration system has been 
developed based on the proposed models. 
The system has been trained to 
transliterate person names from Bengali 
to English. It uses the linguistic 
knowledge of possible conjuncts and 
diphthongs in Bengali and their 
equivalents in English. The system has 
been evaluated and it has been observed 
that the modified joint source-channel 
model performs best with a Word 
Agreement Ratio of 69.3% and a 
Transliteration Unit Agreement Ratio of 
89.8%.    
1 Introduction 
In Natural Language Processing (NLP) 
application areas such as information retrieval, 
question answering systems and machine 
translation, there is an increasing need to 
translate OOV words from one language to 
another. They are translated through 
transliteration, the method of translating into 
another language by expressing the original 
foreign words using characters of the target 
language preserving the pronunciation in their 
original languages. Thus, the central problem in 
transliteration is predicting the pronunciation of 
the original word. Transliteration between two 
languages, that use the same set of alphabets, is 
trivial: the word is left as it is. However, for 
languages that use different alphabet sets, the 
names must be transliterated or rendered in the 
target language alphabets.  
Technical terms and named entities make up 
the bulk of these OOV words. Named entities 
hold a very important place in NLP applications. 
Proper identification, classification and 
translation of named entities are very crucial in 
many NLP applications and pose a very big 
challenge to NLP researchers. Named entities are 
usually not found in bilingual dictionaries and 
they are very productive in nature. Translation of 
named entities is a tricky task: it involves both 
translation and transliteration. Transliteration is 
commonly used for named entities, even when 
the words could be translated. Different types of 
named entities are translated differently. 
Numerical and temporal expressions typically 
use a limited set of vocabulary words (e.g., 
names of months, days of the week etc.) and can 
be translated fairly easily using simple 
translation patterns. The named entity machine 
transliteration algorithms presented in this work 
191
focus on person names, locations and 
organizations. A machine transliteration system 
that is trained on person names is very important 
in a multilingual country like India where large 
name collections like census data, electoral roll 
and railway reservation information must be 
available to multilingual citizens of the country 
in their vernacular. In the present work, the 
various proposed models have been evaluated on 
a training corpus of person names. 
A hybrid neural network and knowledge-based 
system to generate multiple English spellings for 
Arabic personal names is described in (Arbabi et 
al., 1994). (Knight and Graehl, 1998) developed 
a phoneme-based statistical model using finite 
state transducer that implements transformation 
rules to do back-transliteration. (Stalls and 
Knight, 1998) adapted this approach for back 
transliteration from Arabic to English for English 
names. A spelling-based model is described in 
(Al-Onaizan and Knight, 2002a; Al-Onaizan and 
Knight, 2002c) that directly maps English letter 
sequences into Arabic letter sequences with 
associated probability that are trained on a small 
English/Arabic name list without the need for 
English pronunciations. The phonetics-based and 
spelling-based models have been linearly 
combined into a single transliteration model in 
(Al-Onaizan and Knight, 2002b) for 
transliteration of Arabic named entities into 
English.  
Several phoneme-based techniques have been 
proposed in the recent past for machine 
transliteration using transformation-based 
learning algorithm (Meng et al, 2001; Jung et 
al., 2000; Vigra and Khudanpur, 2003). 
(Abduljaleel and Larkey, 2003) have presented a 
simple statistical technique to train an English-
Arabic transliteration model from pairs of names. 
The two-stage training procedure first learns 
which n-gram segments should be added to 
unigram inventory for the source language, and 
then a second stage learns the translation model 
over this inventory. This technique requires no 
heuristic or linguistic knowledge of either 
language. 
 (Goto et al, 2003) described an English-
Japanese transliteration method in which an 
English word is divided into conversion units 
that are partial English character strings in an 
English word and each English conversion unit is 
converted into a partial Japanese Katakana 
character string. It calculates the likelihood of a 
particular choice of letters of chunking into 
English conversion units for an English word by 
linking them to Katakana characters using 
syllables. Thus the English conversion units 
consider phonetic aspects. It considers the 
English and Japanese contextual information 
simultaneously to calculate the plausibility of 
conversion from each English conversion unit to 
various Japanese conversion units using a single 
probability model based on the maximum 
entropy method. 
 (Haizhou et al, 2004) presented a framework 
that allows direct orthographical mapping 
between English and Chinese through a joint 
source-channel model, called n-gram 
transliteration model. The orthographic 
alignment process is automated using the 
maximum likelihood approach, through the 
Expectation Maximization algorithm to derive 
aligned transliteration units from a bilingual 
dictionary. The joint source-channel model tries 
to capture how source and target names can be 
generated simultaneously, i.e., the context 
information in both the source and the target 
sides are taken into account. 
A tuple n-gram transliteration model (Marino 
et al, 2005; Crego et al, 2005) has been log-
linearly combined with feature functions to 
develop a statistical machine translation system 
for Spanish-to-English and English-to-Spanish 
translation tasks. The model approximates the 
joint probability between source and target 
languages by using trigrams. 
The present work differs from (Goto et al, 
2003; Haizhou et al, 2004) in the sense that 
identification of the transliteration units in the 
source language is done using regular 
expressions and no probabilistic model is used. 
The proposed modified joint source-channel 
model is similar to the model proposed by (Goto 
et. al., 2003) but it differs in the way the 
transliteration units and the contextual 
information are defined in the present work. No 
linguistic knowledge is used in (Goto et al, 
2003; Haizhou et al, 2004) whereas the present 
work uses linguistic knowledge in the form of 
possible conjuncts and diphthongs in Bengali. 
The paper is organized as follows. The 
machine transliteration problem has been 
formulated under both noisy-channel model and 
joint source-channel model in Section 2. A 
number of transliteration models based on 
collocation statistics including the modified joint 
source-channel model and their evaluation 
scheme have been proposed in Section 3. The 
Bengali-English machine transliteration scenario 
has been presented in Section 4. The proposed 
192
models have been evaluated and the result of 
evaluation is reported in Section 5. The 
conclusion is drawn in Section 6. 
2 Machine Transliteration and Joint 
Source-Channel Model 
A transliteration system takes as input a character 
string in the source language and generates a 
character string in the target language as output. 
The process can be conceptualized as two levels 
of decoding: segmentation of the source string 
into transliteration units; and relating the source 
language transliteration units with units in the 
target language, by resolving different 
combinations of alignments and unit mappings. 
The problem of machine transliteration has been 
studied extensively in the paradigm of the noisy 
channel model.  
For a given Bengali name B as the observed 
channel output, we have to find out the most 
likely English transliteration E that maximizes 
P(E?B). Applying Bayes? rule, it means to find 
E to maximize 
  P(B,E) = P(B?E) * P(E)                             (1) 
with equivalent effect. This is equivalent to 
modelling two probability distributions: P(B|E), 
the probability of transliterating E to B through a 
noisy channel, which is also called 
transformation rules, and P(E), the probability 
distribution of source, which reflects what is 
considered good English transliteration in 
general. Likewiswe, in English to Bengali (E2B) 
transliteration, we could find B that maximizes 
P(B,E) = P(E?B) * P(B)                               (2) 
for a given English name. In equations (1) and 
(2), P(B) and P(E) are usually estimated using n-
gram language models. Inspired by research 
results of grapheme-to-phoneme research in 
speech synthesis literature, many have suggested 
phoneme-based approaches to resolving P(B?E) 
and P(E?B), which approximates the probability 
distribution by introducing a phonemic 
representation. In this way, names in the source 
language, say B, are converted into an 
intermediate phonemic representation P, and then 
the phonemic representation is further converted 
into the target language, say English E. In 
Bengali to English (B2E) transliteration, the 
phoneme-based approach can be formulated as 
P(E?B) = P(E?P) * P(P?B) and conversely we 
have P(B?E) = P(B?P) * P(P?E) for E2B back-
transliteration. 
However, phoneme-based approaches are 
limited by a major constraint that could 
compromise transliteration precision. The 
phoneme-based approach requires derivation of 
proper phonemic representation for names of 
different origins. One may need to prepare 
multiple language-dependent grapheme-to-
phoneme(G2P) and phoneme-to-grapheme(P2G) 
conversion systems accordingly, and that is not 
easy to achieve. 
In view of close coupling of the source and 
target transliteration units, a joint source-channel 
model, or n-gram transliteration model (TM) has 
been proposed in (Haizhou et al, 2004). For K 
alligned transliteration units, we have 
P(B,E) = P(
  
b1, b2.....bk, e1, e2......ek ) 
           = P (<b,e>1, <b,e>2, .....<b,e>k) 
              K   
           = ? P ( <b,e>k? <b,e>1k-1)               (3) 
              k=1 
which provides an alternative to the phoneme-
based approach for resolving equations (1) and 
(2) by eliminating the intermediate phonemic 
representation. 
Unlike the noisy-channel model, the joint 
source-channel model does not try to capture 
how source names can be mapped to target 
names, but rather how source  and target names 
can be generated simultaneously. In other words, 
a joint probability model is estimated  that can be 
easily marginalized in order to yield conditional 
probability models for both transliteration  and 
back-transliteration. 
Suppose that we have a Bengali name ? = 
x1x2............xm  and an English transliteration ? = 
y1y2........yn where xi, i = 1: m are Bengali 
transliteration units and yj, j = 1: n are English 
transliteration units. An English transliteration 
unit may correspond to zero, one or more than 
one transliteration unit in Bengali. Often the 
values of m and n are different. 
 
x1 x2x3..... xi-1xixi+1....xm 
      
 
         y1      y2 ..yi .... yn 
 
where there exists an alignment ? with <b,e>1 
= <x1,y1>; <b,e>2 = <x2x3, y2>; ?. and <b,e>k = 
<xm,yn>. A transliteration unit correspondence 
<b, e> is called a transliteration pair. Thus B2E 
transliteration can be formulated as    
 
         ?  = argmax P (?, ?, ? )          (4) 
                   ?, ?  
 
and similarly the E2B back-transliteration as  
193
  ?   = argmax P (?, ?, ? )         (5) 
                   ?, ?  
An n-gram transliteration model is defined as 
the conditional probability or transliteration 
probability of a transliteration pair <b, e>k 
depending on its immediate n predecessor pairs: 
 
  P (B, E) = P (?, ?, ?) 
                         
               K   
           = ? P ( <b, e>k? <b, e>k-n+1k-1)     (6) 
             k=1   
3 Proposed Models and Evaluation 
Scheme 
  Machine transliteration has been viewed as a 
sense disambiguation problem. A number of 
transliteration models have been proposed that 
can generate the English transliteration from a 
Bengali word that is not registered in any 
bilingual or pronunciation dictionary. The 
Bengali word is divided into Transliteration 
Units (TU) that have the pattern C+M, where C 
represents a vowel or a consonant or conjunct 
and M represents the vowel modifier or matra. 
An English word is divided into TUs that have 
the pattern C*V*, where C represents a 
consonant and V represents a vowel. The TUs 
are considered as the lexical units for machine 
transliteration. The system considers the Bengali 
and English contextual information in the form 
of collocated TUs simultaneously to calculate the 
plausibility of transliteration from each Bengali 
TU to various English candidate TUs and 
chooses the one with maximum probability. This 
is equivalent to choosing the most appropriate 
sense of a word in the source language to identify 
its representation in the target language. The 
system learns the mappings automatically from 
the bilingual training corpus guided by linguistic 
features. The output of this mapping process is a 
decision-list classifier with collocated TUs in the 
source language and their equivalent TUs in 
collocation in the target language along with the 
probability of each decision obtained from a 
training corpus. The machine transliteration of 
the input Bengali word is obtained using direct 
orthographic mapping by identifying the 
equivalent English TU for each Bengali TU in 
the input and then placing the English TUs in 
order. The various proposed models differ in the 
nature of collocational stastistics used during 
machine transliteration process: monogram 
model with no context, bigram model with 
previous (with respect to the current TU to be 
transliterated) source TU as the context, bigram 
model with next source TU as the context, 
bigram model with previous source and target 
TUs as the context (this is the joint source 
channel model), trigram model with previous and 
next source TUs as the context and the modified 
joint source-channel model with previous and 
next source TUs and the previous target TU as 
the context.  
 
? Model A 
 
In this model, no context is considered in 
either the source or the target side. This is 
essentially the monogram model. 
                K 
P(B,E) = ? P(<b,e>k) 
                k=1 
 
? Model B 
 
This is essentially a bigram model with 
previous source TU, i.e., the source TU occurring 
to the left of the current TU to be transliterated, 
as the context. 
                K 
P(B,E) = ? P(<b,e>k | bk-1) 
              k=1  
 
?Model C 
 
 This is  essentially a bigram model with next 
source TU, i.e., the source TU occurring to the 
right of the current TU to be transliterated, as the 
context. 
                K 
P(B,E) =  ?  P(<b,e>k? bk+1 )           
               k=1   
 
? Model D 
 
This is essentially the joint source-channel 
model where the previous TUs in both the source 
and the target sides are considered as the context. 
The previous TU on the target side refers to the 
transliterated TU to the immediate left of the 
current target TU to be transliterated. 
                 K 
P(B,E) =  ? P( <b,e>k ?? | <b,e>k-1) 
                k=1 
 
 
 
194
? Model E 
 
This is basically the trigram model where the 
previous and the next source TUs are considered 
as the context  
                K 
P(B,E) =  ? P(<b,e>k | bk-1, bk+1) 
                k=1 
  
? Model F 
 
In this model, the previous and the next TUs in 
the source and the previous target TU are 
considered as the context. This is the modified 
joint source-channel model . 
                K 
P(B,E) = ? P (<b,e>k | <b,e>k-1, bk+1) 
              k=1  
 
The performance of the system is evaluated in 
terms of Transliteration Unit Agreement Ratio 
(TUAR) and Word Agreement Ratio (WAR) 
following the evaluation scheme in (Goto et al, 
2003). The evaluation parameter Character 
Agreement Ratio in (Goto et al, 2003) has been 
modified to Transliteration Unit Agreement 
Ratio as vowel modifier matra symbols in 
Bengali words are not independent and must 
always follow a consonant or a conjunct in a 
Transliteration Unit. Let, B be the input Bengali 
word, E be the English transliteration given by 
the user in open test and E/ be the system 
generates the transliteration..TUAR is defined as, 
TUAR = (L-Err)/ L, where L is the number of 
TUs in E, and Err is the number of wrongly 
transliterated TUs in E/ generated by the system. 
WAR is defined as, WAR= (S-Err/) / S, where S 
is the test sample size and Err/ is is the number of 
erroneous names generated by the system (when 
E/ does not match with E). Each of these models 
has been evaluated with linguistic knowledge of 
the set of possible conjuncts and diphthongs in 
Bengali and their equivalents in English. It has 
been observed that the Modified Joint Source 
Channel Model with linguistic knowledge 
performs best in terms of Word Agreement Ratio 
and Transliteration Unit Agreement Ratio. 
4 Bengali-English Machine 
Transliteration 
Translation of named entities is a tricky task: it 
involves both translation and transliteration. 
Transliteration is commonly used for named 
entities, even when the words could be translated 
[LXT?? V_ (janata dal) is translated to Janata Dal 
(literal translation) although LXT?? (Janata) and 
V_ (Dal) are vocabulary words]. On the other 
hand ^?V[?Y??[? ?[? ?`?[?V??_?^  (jadavpur 
viswavidyalaya) is translated to Jadavpur 
University in which ^?V[?Y??[? (Jadavpur) is 
transliterated to Jadavpur and ?[? ?`?[?V??_?^  
(viswavidyalaya) is translated to University.  
A bilingual training corpus has been kept that 
contains entries mapping Bengali names to their 
respective English transliterations. To 
automatically analyze the bilingual training 
corpus to acquire knowledge in order to map new 
Bengali names to English, TUs are extracted 
from the Bengali names and the corresponding 
English names, and Bengali TUs are associated 
with their English counterparts. 
Some examples are given below: 
%?\?X?VX (abhinandan) ? [% | ?\? | X | ?V | X] 
abhinandan  ? [a | bhi | na | nda | n ]  
E??b?]??T?? (krishnamoorti) ?  [E?? | b? | ]? | ?T??]  
krishnamurthy ? [ kri | shna | mu | rthy ]  
?`?E????? (srikant) ? [ ?`? | E?? | ??? ] 
srikant ? [ sri | ka | nt ]  
 
After retrieving the transliteration units from a 
Bengali-English name pair, it associates the     
Bengali TUs to the English TUs along with the 
TUs in context. 
For example, it derives the following 
transliteration pairs or rules from the name-pair: 
?[??[???V?X?U (rabindranath)  ?   rabindranath 
  
Source Language                 Target Language 
                      
previous TU  TU  next TU       previous TU    TU        
          -            ?[?      [??   ?       -                ra 
     ?[          [??     ?V?  ?           ra               bi  
     [??      ?V?     X?   ?        bi             ndra  
          ?V?      X?     U    ?       ndra            na 
        X?      U       -    ?        na              th 
                                              
195
But, in some cases, the number of 
transliteration units retrieved from the Bengali 
and English words may differ. The [ [??L?]?c?X 
(brijmohan) ? brijmohan ] name pair yields  5 
TUs  in Bengali side and  4 TUs in English side   
[ [?? | L | ?]? | c? | X ?  bri | jmo | ha | n]. In such 
cases, the system cannot align the TUs 
automatically and linguistic   knowledge is used 
to resolve the confusion. A knowledge base that 
contains a list of Bengali conjuncts and 
diphthongs and their possible English 
representations has been kept. The hypothesis 
followed in the present work is that the problem 
TU in the English side has always the maximum 
length.  If more than one English TU has the 
same length, then system starts its analysis from 
the first one.  In the above example, the TUs bri 
and jmo have the same length. The system 
interacts with the knowledge base and ascertains 
that bri is valid and jmo cannot be a valid TU in 
English since there is no corresponding conjunct 
representation in Bengali. So jmo is split up into 
2 TUs j and mo, and the system aligns the 5 TUs 
as [[?? | L | ?]? | c? | X ?  bri | j | mo | ha | n]. 
Similarly, [?_?E?X?U (loknath) ? loknath] is 
initially split as [ ?_? | E? | X? | U ]   ?   lo | kna | 
th], and then as [ lo | k | na | th ] since kna has the 
maximum length and it does not have any valid 
conjunct representation in Bengali. 
In some cases, the knowledge of Bengali 
diphthong resolves the problem. In the following           
example, [ ?[?? | + | ]? (raima) ? rai | ma], the 
number of TUs on both sides do not                  
match. The English TU rai is chosen for analysis 
as its length is greater than the other TU ma. The 
vowel sequence ai corresponds to a diphthong in 
Bengali that has two valid representations < %?+, 
B >. The first representation signifies that a 
matra is associated to the previous character 
followed by the character +. This matches the 
present Bengali input. Thus, the English vowel 
sequence ai is separated from the TU rai (rai ? r 
| ai) and the intermediate form of the name pair 
appears to be [?[?? | + | ]? (raima) ? r | ai | ma].  
Here, a matra is associated with the Bengali TU 
that corresponds to English TU r and so there 
must be a vowel attached with the TU r. TU ai is 
further splitted as a and i (ai ? a | i) and the first 
one (i.e. a) is assimilated with the previous TU 
(i.e. r) and finally the name pair appears as: [ ?[?? | 
+ | ]? (raima) ? ra | i | ma]. 
In the following two examples, the number of 
TUs on both sides does not match. 
[ ?V | [? | ?[?? | L (devraj)    ?   de | vra | j ]   
[ ?a? | ] | X? | U (somnath) ? so | mna | th] 
 
It is observed that both vr and mn represent 
valid conjuncts in Bengali but these examples 
contain the constituent Bengali consonants in 
order and not the conjunct representation. During 
the training phase, if, for some conjuncts, 
examples with conjunct representation are 
outnumbered by examples with constituent 
consonants representation, the conjunct is 
removed from the linguistic knowledge base and 
training examples with such conjunct 
representation are moved to a Direct example 
base which contains the English words and their 
Bengali transliteration. The above two name 
pairs can then be realigned as  
[ ?V | [? | ?[?? | L (devraj)    ?   de | v | ra | j ]   
[ ?a? | ] | X? | U (somnath) ? so | m | na | th] 
 
Otherwise, if such conjuncts are included in 
the linguistic knowledge base, training examples 
with constituent consonants representation are to 
be moved to the Direct example base. 
The Bengali names and their English 
transliterations are split into TUs in such a way 
that, it   results in a one-to-one correspondence 
after using the linguistic information. But in 
some       cases there exits zero-to-one or many-
to-one relationship. An example of Zero-to-One 
relationship [? ? h] is the name-pair [%? | {? 
(alla) ?  a | lla | h] while the name-pair [%? | + | 
?\? (aivy)   ? i | vy] is an example of Many-to-
One relationship [%?, + ? i]. These bilingual 
examples should also be included in the Direct 
example base. 
In some cases, the linguistic knowledge 
apparently solves the mapping problem, but not        
always. From the name-pair [[??[?F? (barkha) ? 
barkha], the system initially generates the       
mapping [[? | ?[? | F? ? ba | rkha] which is not 
one-to-one. Then it consults the linguistic          
knowledge base and breaks up the transliteration 
unit as (rkha ? rk | ha ) and generates the final 
196
aligned transliteration pair [[? | ?[? | F? ? ba | rk | 
ha ] (since it finds out that rk has a valid conjunct 
representation in Bengali but not rkh), which is 
an incorrect transliteration pair to train   the 
system. It should have been [[? | ?[? | F? ?  ba | r | 
kha]. Such type of errors can be detected by 
following the alignment process from the target 
side during the training phase. Such training 
examples may be either manually aligned or 
maintained in the Direct Example base. 
5 Results of the Proposed Models 
Approximately 6000 Indian person names have 
been collected and their English transliterations 
have been stored manually. This set acts as the 
training corpus on which the system is trained to 
generate the collocational statistics. These 
statistics serve as the decision list classifier to 
identify the target language TU given the source 
language TU and its context. The system also 
includes the linguistic knowledge in the form of 
valid conjuncts and diphthongs in Bengali and 
their English representation.  
All the models have been tested with an open 
test corpus of about 1200 Bengali names that 
contains their English transliterations. The total 
number of transliteration units (TU) in these 
1200 (Sample Size, i.e., S) Bengali names is 
4755 (this is the value of L), i.e., on an average a 
Bengali name contains 4 TUs. The test set was 
collected from users and it was checked that it 
does not contain names that are present in the 
training set. The total number of transliteration 
unit errors (Err) in the system-generated 
transliterations and the total number of words 
erroneously generated (Err/) by the system have 
been shown in Table 1 for each individual model. 
The models are evaluated on the basis of the two 
evaluation metrics, Word Agreement Ratio 
(WAR) and Transliteration Unit Agreement 
Ratio (TUAR). The results of the tests in terms 
of the evaluation metrics are shown in Table 2. 
The modified joint source-channel model (Model 
F) that incorporates linguistic knowledge 
performs best among all the models with a Word 
Agreement Ratio (WAR) of 69.3% and a 
Transliteration Unit Agreement Ratio (TUAR) of 
89.8%. The joint source-channel model with 
linguistic knowledge (Model D) has not 
performed well in the Bengali-English machine 
transliteration whereas the trigram model (Model 
E) needs further attention as its result are 
comparable to the modified joint source-channel 
model (Model F). All the models were also tested 
for back-transliteration, i.e., English to Bengali 
transliteration, with an open test corpus of 1000 
English names that contain their Bengali 
transliterations. The results of these tests in terms 
of the evaluation metrics WAR and TUAR are 
shown in Table 3. It is observed that the 
modified joint source-channel model performs 
best in back-transliteration with a WAR of 
67.9% and a TUAR of 89%.  
 
Model Error in TUs 
(Err) 
Error words 
(Err/) 
A 990 615 
B 795 512 
C 880 532 
D 814 471 
E 604 413 
F 486 369 
 
Table 1: Value of Err and Err/ for each model 
(B2E  transliteration) 
 
Model WAR 
(in %) 
TUAR 
(in %) 
A 48.8 79.2 
B 57.4 83.3 
C 55.7 81.5 
D 60.8 82.9 
E 65.6 87.3 
F 69.3 89.8 
 
Table 2: Results with Evaluation Metrics 
(B2E  transliteration) 
 
Model WAR 
(in %) 
TUAR 
(in %) 
A 49.6 79.8 
B 56.2 83.8 
C 53.9 82.2 
D 58.2 83.2 
E 64.7 87.5 
F 67.9 89.0 
 
Table 3: Results with Evaluation Metrics 
(E2B transliteration) 
6.    Conclusion 
It has been observed that the modified joint 
source-channel model with linguistic knowledge 
performs best in terms of Word Agreement Ratio 
(WAR) and Transliteration Unit Agreement 
Ratio (TUAR). Detailed examination of the 
197
evaluation results reveals that Bengali has 
separate short and long vowels and the 
corresponding matra representation while these 
may be represented in English by the same 
vowel. It has been observed that most of the 
errors are at the matra level i.e., a short matra 
might have been replaced by a long matra or vice 
versa. More linguistic knowledge is necessary to 
disambiguate the short and the long vowels and 
the matra representation in Bengali. The system 
includes conjuncts and diphthongs as part of the 
linguistic knowledge base. Triphthongs or 
tetraphthongs usually do not appear in Indian 
names. But, inclusion of them will enable the 
system to transliterate those few names that may 
include them. The models are to be trained 
further on sets of additional person names from 
other geographic areas. Besides person names, 
location and organization names are also to be 
used for training the proposed models. 
Acknowledgement 
Our thanks go to Council of Scientific and 
Industrial Research, Human Resource 
Development Group, New Delhi, India for 
supporting Sudip Kumar Naskar under Senior 
Research Fellowship Award (9/96(402) 2003-
EMR-I). 
References 
Abdul Jaleel Nasreen and Leah S. Larkey. 2003. 
Statistical Transliteration for English-Arabic Cross 
Language Information Retrieval. Proceedings of 
the Twelfth International Conference on 
Information and Knowledge Management (CIKM 
2003), New Orleans, USA, 139-146. 
Al-Onaizan Y. and Knight K. 2002a. Named Entity 
Translation: Extended Abstract. Proceedings of the 
Human Language Technology Conference (HLT 
2002), 122-124. 
Al-Onaizan Y. and Knight K.2002b. Translating 
Named Entities Using Monolingual and Bilingual 
Resources.  Proceedings of the 40th Annual 
Meeting of the ACL (ACL 2002), 400-408. 
Al-Onaizan Y. and Knight K. 2002c. Machine 
Transliteration of Names in Arabic Text. 
Proceedings of the ACL Workshop on 
Computational Approaches to Semitic Languages. 
Arbabi Mansur, Scott M. Fischthal, Vincent C. 
Cheng, and Elizabeth Bar. 1994. Algorithms for 
Arabic name transliteration. IBM Journal of 
Research and Development, 38(2): 183-193. 
Crego J.M., Marino J.B. and A. de Gispert. 2005. 
Reordered Search and Tuple Unfolding for Ngram-
based SMT. Proceedings of the MT-Summit X, 
Phuket, Thailand, 283-289. 
Marino J. B., Banchs R., Crego J. M., A. de Gispert, 
P.  Lambert, J. A. Fonollosa and M. Ruiz, Bilingual 
N-gram Statistical Machine Translation.  
Proceedings of the MT-Summit X, Phuket, 
Thailand, 275-282. 
Goto I., N. Kato, N. Uratani, and T. Ehara. 2003. 
Transliteration considering Context Information 
based on the Maximum Entropy Method. 
Proceeding of the MT-Summit IX, New Orleans, 
USA, 125?132. 
Haizhou Li, Zhang Min, Su Jian. 2004. A Joint 
Source-Channel Model for Machine 
Transliteration. Proceedings of the 42nd Annual 
Meeting of the ACL (ACL 2004), Barcelona, 
Spain, 159-166. 
Jung Sung Young, Sung Lim Hong, and Eunok Paek. 
2000. An English to Korean Transliteration Model 
of Extended Markov Window. Proceedings of 
COLING 2000, 1, 383-389. 
Knight K. and J. Graehl. 1998. Machine 
Transliteration, Computational Linguistics, 24(4): 
599-612. 
Meng Helen M., Wai-Kit Lo, Berlin Chen and Karen 
Tang. 2001. Generating Phonetic Cognates to 
handle Name Entities in English-Chinese Cross-
language Spoken Document Retrieval. Proceedings 
of the Automatic Speech Recognition and 
Understanding (ASRU) Workshop, Trento, Italy. 
Stalls, Bonnie Glover and Knight K. 1998. 
Translating names and technical terms in Arabic 
text. Proceedings of the COLING/ACL Workshop 
on Computational Approaches to Semitic 
Languages, Montral, Canada, 34-41. 
Virga Paola and Sanjeev Khudanpur. 2003. 
Transliteration of Proper Names in Crosslingual 
Information Retrieval. Proceedings of the ACL 
2003 Workshop on Multilingual and Mixed-
language Named Entity Recognition, Sapporo, 
Japan, 57-60.  
 
198
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 149?152,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Word to Sentence Level Emotion Tagging for Bengali Blogs  
 
 
Dipankar Das 
Department of Computer Science & 
Engineering, Jadavpur University, India 
dipankar.dipnil2005@gmail.com
Sivaji Bandyopadhyay 
Department of Computer Science & 
Engineering, Jadavpur University, India 
sivaji_cse_ju@yahoo.com 
 
 
Abstract 
 
In this paper, emotion analysis on blog texts 
has been carried out for a less privileged lan-
guage like Bengali. Ekman?s six basic emotion 
types have been selected for reliable and semi 
automatic word level annotation. An automatic 
classifier has been applied for recognizing six 
basic emotion types for different words in a 
sentence. Application of different scoring 
strategies to identify sentence level emotion 
tag based on the acquired word level emotion 
constituents have produced satisfactory per-
formance.  
1 Introduction 
Emotion is a private state that is not open to ob-
jective observation or verification. So, the identi-
fication of the emotional state of natural lan-
guage texts is really a challenging issue. Most of 
the related work has been conducted for English.   
    The approach in this paper is to assign emo-
tion tags on the Bengali blog sentences with one 
of the Ekman?s (1993) six basic emotion types 
such as happiness, sadness, anger, fear, surprise 
and disgust. The system consists of two phases, 
machine learning based word level emotion clas-
sification followed by assignment of sentence 
level emotion tags based on the word level con-
stituents using sense based scoring mechanism. 
The classifier accuracy has been measured 
through confusion matrix. Corpus based and 
sense based tag weights have been calculated for 
each of the six emotion tags and then these emo-
tion tag weights have been used to identify sen-
tence level emotion tag. The tuned reference 
ranges selected from the development set have 
proved effective on the test set.  
The rest of the paper is organized as follows. 
Section 2 describes the related work. Section 3 
briefly describes the resource preparation.  Ma-
chine learning based word level emotion tagging 
system framework and its evaluation results have 
been discussed in section 4. Section 5 describes 
the calculation of tag weights, sentence level 
emotion detection process based on the tag 
weights, evaluation strategies and results. Finally 
section 6 concludes the paper.  
2 Related Work 
(Mishne et al, 2006) used several supervised and 
unsupervised machine learning techniques on 
blog data for comparative evaluation. Importance 
of verbs and adjectives in identifying emotion 
has been explained in (Chesley et al, 2006). 
(Yang et al, 2007) has used Yahoo! Kimo Blog 
corpora containing emoticons associated with 
textual keywords to build emotion lexicons. 
(Chen et al, 2007) has experimented the emotion 
classification task on web blog corpora using 
Support Vector Machine (SVM) and Conditional 
Random Field (CRF) and the observed results 
have shown that the CRF classifiers outperform 
SVM classifiers in case of document level emo-
tion detection. 
3 Resource Preparation  
Bengali is a less computerized language and 
there is no existing emotion word list or Senti-
WordNet in Bengali. The English WordNet Af-
fect lists, (Strapparava et al, 2004) based on Ek-
man?s six basic emotion types have been updated 
with the synsets retrieved from the English Sen-
tiWordNet to have adequate number of emotion 
word entries.  
These lists have been converted to Bengali us-
ing English to Bengali bilingual dictionary 1 . 
These six lists have been termed as Emotion lists. 
A Bengali SentiWordNet is being developed by 
replacing each word entry in the synonymous set 
of the English SentiWordNet (Esuli et al, 2006) 
                                                 
1 http://home.uchicago.edu/~cbs2/banglainstruction.html 
149
by its equivalent Bengali meaning using the same 
English to Bengali bilingual dictionary.  
A knowledge base for the emoticons has been 
prepared by experts after minutely analyzing the 
Bengali blog data. Each image link of the emoti-
con in the raw corpus has been mapped into its 
corresponding textual entity in the tagged corpus 
with the proper emotion tags using the knowl-
edge base. The Bengali blog data have been col-
lected from the web blog archive 
(www.amarblog.com) containing 1300 sentences 
on 14 different topics and their corresponding 
user comments have been retrieved.   
4 Word Level Emotion Classification 
Primarily, the word level annotation has been 
semi-automatically carried out using Ekman?s six 
basic emotion tags. The assignment of emotion 
tag to a word has been done based on the type of 
the Emotion Word lists in which that word is pre-
sent. Other non-emotional words have been 
tagged with neutral type. 1000 sentences have 
been considered for training of the CRF based 
word level emotion classification module. Rest 
200 and 100 sentences, verified by language ex-
perts to perform evaluation have been considered 
as development and test data respectively.  
4.1 Feature Selection and Training  
The Conditional Random Field (CRF) 
(McCallum, 2001) framework has been used for 
training as well as for the classification of each 
word of a sentence into the above-mentioned six 
emotion tags and one neutral tag. By manually 
reviewing the Bengali blog data and different 
language specific characteristics, 10 active fea-
tures have been selected heuristically for our 
classification task. Each feature value is boolean 
in nature, with discrete value for intensity feature 
at the word level. 
x POS information: We are interested with 
the verb, noun, adjective and adverb words 
as these are emotion informative constitu-
ents. For this feature, total 1300 sentences 
has been passed through a Bengali part of 
speech tagger (Ekbal et al 2008) based on 
Support Vector Machine (SVM) tech-
nique. The POS tagger was developed 
with a tagset of 26 POS tags2, defined for 
the Indian languages. The POS tagger has 
demonstrated an overall accuracy of ap-
proximately 90%.  
                                                 
2http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.pdf  
x First sentence in a topic: It has been ob-
served that first sentence of the topic gen-
erally contains emotion (Roth et.al., 2005). 
x SentiWordNet emotion word: A word 
appearing in the SentiWordNet (Bengali) 
contains an emotion. 
x Reduplication: The reduplicated words 
(e.g., bhallo bhallo [good good], khokhono 
khokhono [when when] etc.) in Bengali are 
most likely emotion words. 
x Question words: It has been observed 
that the question words generally contrib-
ute to the emotion in a sentence. 
x Colloquial / Foreign words: The collo-
quial words (e.g., kshyama [pardon] etc.) 
and foreign words (e.g. Thanks, gossya 
[anger] etc.) are highly rich with their 
emotional contents. 
x Special punctuation symbols: The sym-
bols (e.g. !, ?, @ etc ) appearing at the 
word / sentence level convey emotions.  
x Quoted sentence: The sentences espe-
cially remarks or direct speech always 
contain emotion. 
x Negative word: Negative words such as 
na (no), noy (not) etc. reverse the meaning 
of the emotion in a sentence. Such words 
are appropriately tagged. 
x Emoticons: The emoticons and their con-
secutive occurrences generally contribute 
as much as real sentiment to the words or 
sentences that precede or follow it.  
Features  Training       Testing 
Parts of Speech 
First Sentence  
Word in SentiWordNet 
Reduplication 
Question Words 
Coll. / Foreign Words 
Special Symbols  
Quoted Sentence 
Negative Words 
Emoticons 
432              221 
96                13 
684              157   
18                7 
23                11   
      35                9 
      16                4  
      22                8 
      67                27 
      87                33  
        Table 1: Frequencies of different features  
 
Different unigram and bi-gram context fea-
tures (word level as well as POS tag level) and 
their combination has been generated from the 
training corpus. The following sentence contains 
four features (Colloquial word (khyama), special 
150
symbol (!), quoted sentence and emotion word 
(????? [happy])) together and all these four fea-
tures are important to identify the emotion of this 
sentence. 
      k????   ??o!    ??? ??    ?????     ?????  
    (khyama) (dao)!   ?(tumi)  (bhalo)  (lok)?      
    (Forgive)!            ?(you)   (good)   (person)? 
4.2 Evaluation Results of the Word-level 
Emotion Classification   
Evaluation results of the development set have 
demonstrated an accuracy of 56.45%. Error 
analysis has been conducted with the help of 
confusion matrix as shown in Table 2. A close 
investigation of the evaluation results suggests 
that the errors are mostly due to the uneven dis-
tribution between emotion and non-emotion tags.   
 
Tags happy   sad   ang     dis    fear  sur    ntrl 
happy   
sad    
ang      
dis     
fear    
sur    
ntrl 
            0.01   0.05   0.0    0.0    0.0   0.03 
0.006             0.02   0.03  0.0    0.0   0.02 
0.0       0.03             0.0    0.02  0.0   0.01 
0.0       0.0     0.01            0.01  0.0   0.01 
0.0       0.0     0.0     0.0             0.0   0.01 
0.02     0.007 0.0     0.0    0.0            0.01 
0.0       0.0     0.0     0.0    0.0    0.0  
Table 2: Confusion matrix for development set  
 
The number of non-emotional or neutral type 
tags is comparatively higher than other emotional 
tags in a sentence. So, one solution to this unbal-
anced class distribution is to split the ?non-
emotion? (emo_ntrl) class into several subclasses. 
That is, given a POS tagset POS, we generate 
new emotion classes, ?emo_ntrl-C?|C?POS. We 
have 26 sub-classes, which correspond, to non-
emotion tags such as ?emo_ntrl-NN? (common 
noun), ?emo_ntrl-VFM? (verb finite main) etc. 
Evaluation results of the system with the inclu-
sion of this class splitting technique have shown 
the accuracies of 64.65% and 66.74% on the de-
velopment and test data respectively.   
5 Sentence Level Emotion Tagging 
This module has been developed to identify sen-
tence level emotion tags based on the word level 
emotion tags. 
5.1 Calculation of Emotion Tag weights 
Sense_Tag_Weight (STW): The tag weight has 
been calculated using SentiWordNet. We have 
selected the basic six words ?happy?, ?sad?, 
?anger?, ?disgust?, ?fear? ?surprise? as the seed 
words corresponding to each emotion type. The 
positive and negative scores in the English Sen-
tiWordNet for each synset in which each of these 
seed words appear have been retrieved and the 
average of the scores has been fixed as the 
Sense_Tag_Weight of that particular emotion tag.   
Corpus_Tag_Weight (CTW): This tag weight 
for each emotion tag has been calculated based 
on the frequency of occurrence of an emotion tag 
with respect to the total number of occurrences 
of all six types of emotion tags in the annotated 
corpus. 
 
Tag Types        CTW                     STW   
emo_happy 
emo_sad 
emo_ang 
emo_dis 
emo_fear 
emo_sur 
emo_ntrl 
      0.5112                     0.0125 
      0.2327              ( - ) 0.1022 
      0.0959              ( - ) 0.5 
      0.1032              ( - ) 0.075 
      0.0465                     0.0131 
      0.0371                     0.0625 
      0.0                           0.0 
Table 3: CTW and STW for each of six emotion 
tags with neutral tag 
5.2 Scoring Techniques 
The following two scoring techniques depending 
on two calculated tag weights (in section 5.1) 
have been adopted for selecting the best possible 
sentence level emotion tags.  
(1) Sense_Weight_Score (SWS): Each sen-
tence is assigned a Sense_Weight_Score (SWS) 
for each emotion tag which is calculated by di-
viding the total Sense_Tag_Weight (STW)of all 
occurrences of an emotion tag in the sentence by 
the total Sense_Tag_Weight (STW) of all types 
of emotion tags present in that sentence. The 
Sense_Weight_Score is calculated as  
SWSi = (STWi * Ni) / (? j=1 to 7 STWj * Nj) | i ? j 
where SWSi is the Sentence level 
Sense_Weight_Score for the emotion tag i in the 
sentence and Ni is the number of occurrences of 
that emotion tag in the sentence. STWi and STWj 
are the Sense_Tag_Weights for the emotion tags i 
and j respectively. Each sentence has been as-
signed with the sentence level emotion tag SETi 
for which SWSi is highest, i.e., 
SETi = [max i=1 to 6(SWSi)].  
(2) Corpus_Weight_Score (CWS): This meas-
ure is calculated in a similar manner by using the 
CTW of each emotion tag. The corresponding 
Bengali sentence is assigned with the emotion 
tag for which the sentence level CWS is highest. 
The scoring mechanism has been considered for 
verifying any domain related biasness of emotion 
and their influence in emotion detection process.  
151
5.3 Evaluation Results of Sentence Level 
Emotion Tagging 
Each sentence in the development and test sets 
have been annotated with positive or negative or 
neutral valence and with any of the six emotion 
tags. The SWS has been used in identifying va-
lence scores as there is no valence information 
carried by CWS. The sentences for which the 
total SWS produced positive, negative and zero 
(0) values have been tagged as positive, negative 
and neutral type. Any domain biasness through 
CWS has been re-evaluated through SWS also. 
We have taken the Bengali corpus from comic 
related background. So, during analysis on the 
development set, the CWS outperforms the SWS 
significantly in identifying happy, disgust, fear 
and surprise sentence level emotion tags. The 
other SETs have been identified through SWS as 
the CWS for these SETs are significantly less 
than their corresponding SWS as shown in Table 
5. The knowledge and information of the refer-
ence ranges (shown in Table 4) of SWS and 
CWS for assigning valence and six other emotion 
tags, acquired after tuning of development set, 
have been applied on the test set. The valence 
and emotion tag assignment process has been 
evaluated using accuracy measure on test data. 
The difference in the accuracies for the develop-
ment and test sets is negligible. It signifies that 
the best possible reference range for valence and 
other emotion tags have been selected. Results in 
Table 5 show that the system has performed sat-
isfactorily for valence identification as well as 
for sentence level emotion tagging.   
Table 4: Reference ranges 
6 Conclusion  
The hierarchical ordering of the word level to 
sentence level and from sentence level to docu-
ment level can be considered as the well favored 
route to track the document level emotional ori-
entation. The handling of negative words and 
metaphors and their impact in detecting sentence 
level emotion along with document level analysis 
are the future areas to be explored. 
Table 5: Accuracies (in %) of valence and six   
emotion tags in development set before and after 
applying the reference range and in test set 
References  
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A Publicly Available Lexical Re-
source for Opinion Mining.LREC-06. 
Andrew McCallum, Fernando Pereira and John 
Lafferty. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and labeling Se-
quence Data. ISBN, 282 ? 289.    
A. Ekbal and S. Bandyopadhyay. 2008. Web-based 
Bengali News Corpus for Lexicon Development 
and POS Tagging. POLIBITS, 37(2008):20-29. 
Mexico. 
B. Vincent, L. Xu, P. Chesley and R. K. Srhari. 2006. 
Using verbs and adjectives to automatically clas-
sify blog sentiment.AAAI-CAAW-06. 
Carlo Strapparava, Rada Mihalcea .2007. SemEval-
2007 Task 14: Affective Text. 45th Aunual Meet-
ing of ACL. 
C. Yang, K. H.-Y. Lin, and H.-H. Chen. 2007. Build-
ing Emotion Lexicon from Weblog Corpora, 45th 
Annual Meeting of ACL, pp. 133-136. 
C. Yang, K. H.-Y. Lin, and H.-H. Chen.2007. Emo-
tion Classification from Web Blog Corpora, 
IEEE/WIC/ACM, 275-278. 
Cecilia Ovesdotter Alm, Dan Roth, Richard Sproat. 
2005. Emotions from text: machine learning for 
text-based emotion prediction. Human Language 
Technology and EMNLP, 579-586.Canada. 
G. Mishne and M. de Rijke. 2006. Capturing Global 
Mood Levels using Blog Posts, AAAI, Spring 
Symposium on Computational Approaches to 
Analysing Weblogs, 145-152. 
Paul Ekman. 1993. Facial expression and emotion. 
American Psychologist, 48(4):384?392. 
Category  Reference Range        
Valence (SWS) 
 
happy 
sad 
angry 
disgust 
fear 
surprise 
0 to 2.35 (+ve), 0 to -0.56 
(-ve) and  0.0  neutral)        
0.31 to 1 (CWS)           
-0.15 to -1.6 (SWS)     
-0.5 to -1.9 (SWS)       
0.18 to 1 (CWS)          
0.14 to 1.9 (CWS)       
0.15 to 1.76 (CWS)     
 
Category 
     
        Development         Test         
      Before        After 
CWS    SWS          
Valence  
happy 
sad 
angry 
disgust 
fear 
surprise 
  --        49.56    65.43     66.54 
54.15    10.33    63.88     64.28 
7.66      42.93    64.56     66.42 
15.47    53.44    61.48     60.28 
60.13    17.18    70.19     72.18 
55.57    11.54    66.04     67.14 
50.25    12.39    65.45     66.45 
152
Dialogue based Question Answering System in Telugu 
 
 
 
Abstract 
A dialogue based Question Answering 
(QA) system for Railway information in 
Telugu has been described. Telugu is an 
important language in India belonging to 
the Dravidian family. The main compo-
nent of our QA system is the Dialogue 
Manager (DM), to handle the dialogues 
between user and system. It is necessary 
in generating dialogue for clarifying par-
tially understood questions, resolving 
Anaphora and Co-reference problems. 
Besides, different modules have been de-
veloped for processing the query and its 
translation into formal database query 
language statement(s). Based on the re-
sult from the database, a natural language 
answer is generated. The empirical re-
sults obtained on the current system are 
encouraging. Testing with a set of ques-
tions in Railway domain, the QA system 
showed 96.34% of precision and 83.96% 
of dialogue success rate. Such a question 
answering system can be effectively util-
ized when integrated with a speech input 
and speech output system. 
1 Introduction  
Ever since Question Answering (QA) emerged as 
an active research field, the community has 
slowly diversified question types, increased 
question complexity, and refined evaluation met- 
rics, as reflected by the TREC (Text Retrieval 
Conference) QA track (Voorhees, 2004). Several 
QA systems have responded to these changes in 
the nature of the QA task by incorporating vari-
ous knowledge resources (Hovy et al, 2002), 
handling of additional types of questions tapping 
 
 
 
 
 
 
 
 
 
 
 
 
 
into external data sources such as web, encyclo-
pedia, and databases in order to find the answer 
candidates, which may then be located in the 
specific corpus being searched (Xu et al, 2003). 
   The most popular classes of technique for 
QA are open-domain and restricted-domain 
(Diekema et al, 2004, Doan-Nguyen et al, 
2004). These two domains use thesauri and lexi-
cons in classifying documents and categorizing 
the questions. Open domain question answering 
deals with questions about nearly everything and 
can only rely on general ontology. It has become 
a very active research area over the past few 
years. On the other hand, Restricted-domain 
question answering (RDQA) deals with ques-
tions under a specific domain. If we create such a 
RDQA interface for structured e.g. relational 
database, we call it as Natural language interface 
to database system (NLIDB) (Androutsopoulos 
et al, 1995), where it allows the user to access 
the information stored in database by typing re-
quests expressed in some natural language. 
RDQA has a long history, beginning with sys-
tems working over databases (e.g., BASEBALL 
(Green et al, 1961), and LUNAR (woods et al, 
1972)). 
 In practice, current QAs can only understand 
limited subsets of natural language. Therefore, 
some training is still needed to teach the end-user 
what kinds of questions the system can or cannot 
understand. There are kinds of questions (e.g. 
questions involving negation, or quantification) 
that can be easily expressed in natural language, 
but that seem difficult (or at least tedious) to ex-
press using graphical or form based interfaces. 
Anaphoric and elliptical expressions are also 
handled by the QA systems. In recent years a 
large part of the research in QAs has been de-
voted to portability, i.e., to the design of QAs 
that can be used in different knowledge domains 
 
    Rami Reddy Nandi Reddy 
    Dept. of Comp. Sc. & Engg, 
Jadavpur University,  
Kolkata, India 
nramireddy@gmail.com 
 
 
Sivaji Bandyopadhyay 
    Dept. of Comp. Sc. & Engg, 
Jadavpur University,  
Kolkata, India 
sivaji_cse_ju@yahoo.com 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
53
(Knowledge domain portability), with different 
underlying Database Management System 
(DBMS) (DBMS portability), or even with dif-
ferent natural languages (Natural language port-
ability). There is a growing body of research on 
integrating speech recognition, robust interpreta-
tion with the goal being to implement systems 
that engage users in spoken dialogue to help 
them perform certain tasks. We expect that this 
line of research will have a significant influence 
on future QAs, giving rise to systems that will 
allow users to access databases by spoken dia-
logue, in situations for which graphic and form-
based interfaces are difficult to use. 
A practical question answering system in re-
stricted domain (Hoojung et al, 2004) and our 
system handles user questions similarly. How-
ever, our system extracts the information from a 
relational database. Moreover, our system keeps 
track of user dialogue and handles clarifications, 
elaborations and confirmations needed from the 
user with respect to the query. Along with it re-
turns natural language answer in user-friendly 
format.  
ARISE (Automatic Railway Information Sys-
tem for Europe) is a spoken dialogue system to 
provide train timetable information over the 
phone.  Prototypes have been developed in four 
languages: Dutch, French, English, and Italian. 
ARISE uses a mixed initiative Dialogue Manager 
(DM). A mix of implicit and explicit confirma-
tion is used, based on how confident the system 
is in deciding whether an item has been correctly 
understood. 
We relate this paper as an experiment for de-
signing a keyword based QA system for a huge 
domain (i.e. for Railways), which aims at reply-
ing users questions in their native language (Te-
lugu). The system generates SQL query out of 
the natural language question, executes the SQL 
query over a relational database and then provide 
the answer. Dialogue Manager (DM) is main-
tained to generate dialogues with user and to 
handle the anaphoric and elliptical expression in 
our query. This system is implemented on a rela-
tively restricted domain that includes a number 
of aspects of railway information system (Arri-
val/Departure time, Fare between for particular 
stations, Trains between important stations etc.). 
The precision of the information extraction stage 
is essential to the success of a QA system, be-
cause it places an upper bound on the precision 
of the entire system. 
The empirical results obtained on the current 
system are encouraging. Testing with a set of 
questions in Railway domain, the QA system 
showed 96.34% of precision and 83.96% of dia-
logue success rate.    
   Section 2 deals with the System Architecture 
of the QA system. Section 3 details about the QA 
system design in the Railway information do-
main using the Keyword based approach. The 
evaluation has been carried out in Section 4. Sec-
tion 5 concludes with some directions for future 
work. 
2 System Architecture  
In this keyword based approach the input query 
statement is analyzed by the query analyzer, 
which uses domain ontology stored as knowl-
edge base, generating tokens and keywords. The 
appropriate query frame is selected based on the 
keywords and the tokens in the query statement. 
Each query frame is associated with a SQL gen-
eration procedure. The appropriate SQL state-
ment(s) is generated using the tokens retrieved 
from the input query.  
The QA system architecture is shown in Fig-
ure 1. The Dialogue Manager keeps track of the 
elliptical queries from the user that constitute the 
dialogue and helps in the SQL generation proce-
dure using dialogue history (Flycht-Erikson et 
al., 2000), which contains information about pre-
vious tokens and their types as well as other dia-
logue information like answers retrieved by the 
current SQL statements and the answers for pre-
vious queries in the dialogue. The SQL state-
ments used to retrieve the correct answer from 
the database.  Based on the result of the DBMS, 
a natural language answer is generated. This an-
swer is forwarded to the DM for onward trans-
mission to the user. 
 
 
Figure 1. QA System Architecture 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
54
If the system cannot decide on the query frame 
by using the keywords extracted from the input 
query, the system enters into a dialogue with the 
user through the DM. During SQL generation if 
it is detected that more information is needed 
from the user to generate the SQL statement then 
an interactive message is sent to the user through 
the DM. The user will then send the needed in-
formation to the system. If user could not pro-
vide correct information then DM sends an error 
message to the user indicating the error in the 
user query. In case, the SQL statement generates 
a null response from the database the DM will 
send a cooperative message depending on the 
user query.  
3 Design of Railway Information Sys-
tem  
The most important issue in the design of the 
Railway information system is the design of the 
Railway database and the Knowledge base. 
These are detailed in Sections 3.1 & 3.2 respec-
tively. The different components of the dialogue 
based QA system, i.e., Query Analyzer, Query 
Frame Decision, Dialogue Manager, SQL Gen-
eration and Answer Generation sub systems are 
described in subsequent sections. 
3.1 Railway Database Management 
The system as a whole is engaged in data access, 
and is a hybrid system with subsystem to analyze 
the natural language query and formal query lan-
guage SQL, and a data retrieval and database 
management system. The database is structured 
and contains the information to provide the rail-
way information service. For example in a Rail-
way information system, database contains in-
formation about the arrival/departure time of 
trains, their fares and their running information 
etc. The aim of database management is to de-
scribe the information, in order to offer the ser-
vice.  
For our purposes the relational model has im-
portant advantage: The relational model stresses 
on data independence. This means that the user 
and front-end programs are effectively isolated 
from the actual database organization. 
The main tables used here are schedule table 
for each train, fare tables for special trains like 
Rajdhani, Shatabdi etc. that have a different fare 
structure, Route tables for each route and tables 
that include train running frequency details etc. 
Some temporal tables are maintained in order to 
check the status of the railway ticket (which is 
known as checking the Passenger Name Record 
or PNR status of the ticket) and reservation 
availability information of a particular train. 
3.2 Design of the Knowledge Base 
The system maintains a knowledge base of the 
domain to facilitate question answering. For a 
system operating on a restricted domain this is 
quite obvious since it will greatly improve the 
disambiguation and parsing. 
The words that occur in the database query for 
Railway information system includes words de-
scribing train name, station name, reservation 
class, and date and/or period of journey or key-
words that specify the topic of the query. Hence 
we stored a domain dependent ontology in the 
knowledge base.  
Knowledge base, which contains tables for 
train name, station name and alias tables for train 
name and station name. We have stored possible 
Telugu inflections (? ?? (ke [to]), ? ??? (ku [to]), ?? 
(loo [in]), ????????? (tundi [ing]), ?? (vi [have]) etc. 
for ex: ??????????????? ??? (gunturku [to Guntur])), which 
can be used in morphological analysis of input 
query. We have considered possible postposi-
tions like ????????? (nundi [from]), ? ??????? (nunchi 
[from] etc. (For ex: ?????????????? ????????? (newdelhi nundi 
[from New Delhi])), which can be used to iden-
tify the source station in the input query and 
route words like ?????????? (daggara [near]), ????? 
(dwara [through]),  ???????? (gunda [through]), ??????? 
(vadda [at]), ?????????? (meedugaa [via]) etc. (For 
ex: ????????? ?????????? (gaya meedugaa [via Gaya])), 
which can be used to identify the route station of 
the journey. We kept a list of keywords in a table 
in order to identify the proper query frame. 
3.3  Query Analyzer 
During query analysis, Morphological analysis of 
the input query statement is carried out to iden-
tify the root words / terms. Analyzing the whole 
input query, the system identifies several tokens 
such as Train name, Station name, Reservation 
class, date and period of the day etc. and a set of 
keywords.  
The query analyzer consults the domain-
dependent ontology i.e. knowledge base for rec-
ognizing these tokens and keywords. It may hap-
pen that some words/terms may not found in the 
knowledge base. Those words do not contain any 
semantic information and are simply discarded. 
 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
55
For example: If our input query is  
??????????? ??????? ? ? ???????? ??? ?????????? ??????????????? ??? ????????????? (ep-
pudu falaknuma express gunturuku veltundi 
[When the Falaknuma Express goes to Guntur])  
Here query is parsed based on spaces. After 
parsing each word, it is searched in the knowl-
edge base until the word is found. After search-
ing each word/term in the knowledge base, their 
types and semantic information are put in a list 
of tokens. Each token has three properties: the 
token value, its type and semantic information 
that it contains. These tokens and keywords are 
used to decide the proper query frame. 
For the above example, the tokens identified 
are ??????? ? ? ???????? ??? ?????????? (Falaknuma Express) as 
Train name and ?????????????? (Guntur) as Station 
name. Whereas ??????????? (eppudu [when]), ????????????? 
(veltundi [goes]) are under keywords list.  
3.4 Query Frame Decision 
During the analysis of query, the keywords in the 
input query are detected. In this step, based on 
the tokens and keywords, we identify the appro-
priate query frame. 
   Restricting the query domain and informa-
tion resource, the scope of the user request can 
be focused. That is, there are a finite number of 
expected question topics. Each expected question 
topic is defined under a single query frame. 
Some query frame examples for Railway infor-
mation system are fare of a journey [Fare], arri-
val [Arr_Time] or departure time [Dep_Time] of 
a train, trains between important stations 
[Trains_Imp_Stations], scheduled time 
[Sched_Time], weekly frequency of a train 
[Arr_Frequency / Dep_Frequency], Availability 
of reservation class in a particular train [Reserva-
tion_Availability] and PNR enquiry 
[PNR_Enquiry]. 
It is important to select the appropriate query 
frame for the user request; because in some cases 
ambiguity will occur i.e. a single natural lan-
guage query statement may belong to one or 
more query frames means same keywords are 
used to identify the query frames. 
 For example keywords like ???????? (vellu [go]), 
???????? (vachhu [come]), ???????? (cheru [reach]), and 
?????????????????? (bayuluderu [start]) etc. are used to 
identify the query frames [Arr_Time], 
[Dep_Time], and [Trains_Imp_Stations]. To re-
solve this ambiguity, we consider what/which 
(question having words ? (ee [what]), ?? (eeee 
[what]), ??? (evi [which]) etc.) type of questions 
like ?????????????? ????????? ??????? ??? ?????????????????? ????????? ??? 
(newdelhi nundi howrahku bayaluderu raillu evi 
[What are the trains starts from New Delhi to 
Howrah]) are under [Trains_Imp_Stations] query 
frame. Where as, when (questions having words 
??????????? (eppudu [when]), ??????????????? ??? (enniganta-
laku [at what time]), ??????????? ?? (ennintiki [at what 
time]) etc.) type of questions like ??????????????? ??? ??????? ?
??? ????????? ??? ?????????? ??????????????????????????? (ennigantalaku 
kolkata rajadhani express bayaluderutundi 
[When Kolkata Rajdhani Express starts]) are un-
der [Dep_Time] query frame. Similarly, week-
day names like ???????????????? (somavaaramu) [Mon-
day], ?????????????????????? (mangalavaaramu) [Tuesday] 
etc. and keywords used in [Arr_Time]/ 
[Dep_Time] query frame are used to identify the 
[Arr_Frequency]/ [Dep_Frequency] query frame. 
In contrast, separate keywords are used to 
identify [Arr_Time] and [Dep_Time] query 
frames. But keywords like ??????????? (potundi [go]), 
???????? (vellu [go]) etc. are used to identify both 
[Arr_Time] and [Dep_Time] query frames. To 
resolve this ambiguity, we consider the station 
type, i.e. whether the station is source or destina-
tion. If the station is source station (station name 
succeeded by postpositions like ????????? (nundi 
[from]), ? ??????? (nunchi [from])), then we conclude 
that our query is under [Dep_Time] query frame. 
Otherwise query will be under [Arr_Time] query 
frame. For example, questions like ??????????? ??????? ? 
????????? ??? ?????????? ??????????????? ??? ????????????? (eppudu falak-
numa express gunturuku veltundi [When the Fa-
laknuma Express goes to Guntur]) is under 
[Arr_Time] query frame. But, questions like 
??????????? ??????? ? ????????? ??? ?????????? ?????????????? ? ???????? ????????????? 
(eppudu falaknuma express gunturu nundi vel-
tundi [When the Falaknuma Express goes from 
Guntur]) is under [Dep_Time] query frame. 
The selection process of query frame has a 
great influence on the precision of the system, 
while there is not much likelihood of errors in 
other processes, such as getting the information 
from the dialogue history or generating SQL 
statement(s) from the selected query frame 
and/or retrieving the answer from the database 
and generating natural language answer from the 
retrieved result.  
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
56
3.5 Dialogue Manager  
The role of the Dialogue Manager (DM) differs 
slightly between different dialogue systems. But 
the primary responsibility of the DM is to control 
the flow of dialogue by deciding how the system 
should respond to a user request and the coordi-
nation of the other components in the system. If 
some information is missing or a request is am-
biguous, clarification questions are specified by 
the DM and posed to the user. 
For example in general, users ask questions 
about Arrival/Departure time without mentioning 
journey of train i.e. Upward/Downward journey, 
then system asks the user for proper information. 
 Sometimes user may not give correct informa-
tion (like missing Train name, Station name or 
query does not belong to any of the query frames 
etc.). At that time DM generates error message 
describing that missed information. In another 
case user asks questions without knowledge. In 
this case DM generates a cooperative message, 
which will help the user in further requests. 
As a basis for the above tasks the DM utilizes 
the dialogue history. Here dialogue history re-
cords the focal information, i.e what has been 
talked in the past and what is talking at present. 
It is used for dialogue control and disambigua-
tion of context dependent requests. The DM gets 
a semantic frame from the other system compo-
nents. This frame is filled by interpreting the re-
quest in the context of the ongoing dialogue, 
domain knowledge, and dialogue history. The 
DM then prompts for missing information or 
sends a SQL query. Before the query is sent off, 
DM checks whether new information is con-
tained in the query or the information is contra-
dictory to information given before. If this is the 
case then the DM can either keep the original 
information or replace it with the new one in the 
dialogue history or engage in a confirmation sub-
dialogue. 
The DM looks at the query after language 
processing has been completed (but before the 
formal query is issued), as well as after the result 
has been obtained from the formal query. The 
accuracy of the system mainly depends on the 
representation of the dialogue history and how 
the DM responds to the user?s dialogue. 
3.6 SQL Generation 
Once the query frame is selected for a ques-
tion, the corresponding procedure for the SQL 
query generation is called. For each query frame 
there is a procedure for SQL statement(s) genera-
tion. In order to generate the SQL query, it needs 
the tokens generated by the query analyzer. 
If the tokens are presented in the current 
query, it uses them. Otherwise it gets the token 
information from the dialogue history. For ex-
ample, in the arrival time queries user has to 
specify Train name/no and station/city name 
where he/she needs to go. If he/she did not men-
tion that information, SQL generation procedure 
gets the information from the dialogue history. 
Figure 2 depicts the conversion of natural lan-
guage query to its SQL query.  
 
 
  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: Interpreting the natural language   
question to the SQL query 
 
For the fare related query, SQL generation 
procedure would be called depending on the type 
of train. The procedure considers that the user 
will provide the train name and reservation class. 
If the train is of Express type, it considers that 
the user may provide either the source and desti-
nation stations of journey or the distance of jour-
ney. If it is of Rajdhani type, it considers that the 
user may provide source and destination station 
of journey. Similarly for the other query frames, 
SQL generation procedure considers that the user 
provide the necessary information. 
??????????? ??????? ? ? ???????? ??? ?????????? ??????????????? ??? ????????????? (ep-
pudu falaknuma express gunturuku veltundi 
When the Falaknuma Express goes to Gun-
tur])? 
 
Train name: ??????? ? ????????? ??? ?????????? (Falaknuma Ex-
press) 
Station name: ?????????????? (Guntur)  
Keywords: ??????????? (eppudu [when]), ????????????? 
(veltundi [goes]).  
 
The [Arr_Time] Query frame is selected. 
 
The system checks with the user for up/down 
journey of the train  
 
Let user asked about upward journey of train 
via DM. 
 
SELECT Arr_Time FROM Schedule2703 
WHERE Station Name=????????????????. 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
57
3.7 Answer Generation 
Once the SQL statement for an input query 
statement is generated, it is triggered on the da-
tabase and the retrieved information is used to 
represent the answer. The retrieved information 
is updated in the dialogue history for further ref-
erence.  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Generating answer from the retrieved 
result. 
 
Each query frame has its corresponding An-
swer generator. We use template based answer 
generation method. Each template consists of 
several slots. Those slots are filled by the re-
trieved answer and the tokens generated from the 
query. Figure 3 shows the answer generation 
from the SQL query generated from the natural 
language query shown in Figure 2. The answer 
will be sent to Dialogue Manager, which will 
further send it to the user. 
4 Evaluation 
For evaluating our system we have taken queries 
from our Telugu-speaking friends. We have de-
scribed the Railway Information system to them. 
They have also been told about the constraints on 
the nature of queries in the systems. They have 
also been shown list of example queries for the 
systems. 
Here we are considering two measures for 
evaluating our system: Dialogue success rate and 
Precision. The QA system was evaluated by giv-
ing 26 sets of dialogue consisting 95 natural lan-
guage queries in total. The two evaluation meas-
ures are defined as follows:  
 
Dialogue success rate for each set=Number of 
Answers or Responses generated by the system 
/Number of turns issued by the user. 
Dialogue success rate = (? Dialogue success rate 
for each set / Number of sets of dialogues)*100. 
Precision= (Number of correct answers given by 
the system/Number of answers given by the sys-
tem)*100. 
The number of turns issued by the user in a 
dialogue is the total of the number of questions 
issued to the system and the number of responses 
provided by the user to the system. 
Each set of dialogue consisted of around 3 to 5 
natural language queries. The total dialogue suc-
cess rate for the 26 sets was obtained as 21.83. 
The dialogue success rate for the system is calcu-
lated as  
Dialogue success rate= (21.83/26)*100= 83.96%. 
Out of 95 questions, system generated answers 
for 82 questions of which 79 were correct an-
swers. So, the precision of the system is calcu-
lated as  
Precision= (79/82)*100= 96.34%. 
This low dialogue success rate is due to the 
fact that the system coverage of the domain is 
not extensive enough, i.e., query frames for some 
natural language queries were not correctly iden-
tified. The information given by the user in the 
query was sometimes inadequate and the system 
was not able to identify the missing information 
because of the incorrect choice of the query 
frame. Sometimes the system is unable to obtain 
tokens correctly from the input query even if it 
had identified the right query frame, thereby 
generating wrong answers. Misinterpretation of 
dialogue history is also another problem.   
5 Conclusion 
In this dialogue based QA system following the 
keyword based approach, each word need not be 
found in the knowledge base. Only the words 
that contain semantic information needs to be 
found in the knowledge base. 
  By restricting the coverage of questions, our 
system could achieve relatively high dialogue 
success rate. However, for a real practical system 
this success rate must be improved. 
In extension to our work we are developing 
the modules for the remaining query frames. The 
system needs to be upgraded so that a user can 
query for railway information over phone. The 
speech input can be converted to textual query. 
This textual query can be input of our system and 
the textual out can be converted to speech again 
to answer the user. 
References 
Androutsopoulos I, Ritchie G. D, and Thanisch P. 
1995. Natural Language Interfaces to Databases ? 
SELECT Arr_Time FROM Schedule2703 
WHERE Station_Name=????????????????. 
 
DBMS returns ?04:33 hrs?. 
 
4:33 ?????????? ??? ??????? ??? ???????? ??? ?????????? ??????????????? ??? 
????????????? (04:33 gantalaku falaknuma express 
gunturku veltundi [At 04:33hrs Falaknuma 
Express goes to Guntur]). 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
58
An Introduction. Natural Language Engineer-
ing, Vol 1, Part1, 29?81. 
Diekema A.R, Yilmazel Ozgur, and Liddy E.D. 2004. 
Evaluation of Restricted Domain Question-
Answering Systems. In Proceedings of the 
ACL2004 Workshop on Question Answering 
in Restricted Domain, 2-7. 
Doan-Nguyen Hai and Leila Kosseim. 2004. The 
Problem of Precision in Restricted-Domain Ques-
tion Answering. Some Proposed Methods of Im-
provement. In Proceedings of the ACL 2004 
Workshop on Question Answering in Re-
stricted Domain, 8-15. 
Flycht-Eriksson Annika and Jonsson Arne. 2000 Dia-
logue and Domain Knowledge Management in 
Dialogue Systems. In proceedings of 1st SIG-
DIAL workshop at ACL2000. 
Green W, Chomsky C, and Laugherty K. 1961. 
BASEBALL: An automatic question answerer. 
Proceedings of the Western Joint Computer 
Conference, 219-224.  
Hoojung Chung, Young-In Song, Kyoung-Soo Han, 
Do-Sang Yoon, Joo-Young Lee, Hae-Chang Rim 
and Soo-Hong Kim. 2004. A Practical QA System 
in Restricted Domains. In Proceedings of the 
ACL 2004 Workshop on Question Answering 
in Restricted Domain, 39-45. 
Hovy E, Hermjakob U, Lin C.Y, and Ravichnadran D. 
2002. Using knowledge to facilitate factoid answer 
pinpointing .COLING. 
Voorhees E.M. 2004. Overview of the TREC 2003 
question answering track. In Proceedings of the 
12th Text REtrieval Conference. 
 Woods W.A, Kaplan R.M, and Webber B.N.1972 
The Lunar Sciences Natural Language Information 
System: Final Report. BBN Report 2378, Bolt 
Beranek and Newman Inc., Cambridge, Massa-
chusetts. 
Xu J, Licuanan A and Weischedel R. 2003. TREC 
2003 qa at bbn: Answering definitional questions. 
TREC. 
Appendix: Examples of QA in the system 
 
Example 1: 
U: ??????????????? ??? ??????? ???? ????????? ??? ??????????  
  ??????????????????????????? (ennigantalaku kolkata rajad-
hani express bayaluderutundi [When Kolkata 
Rajdhani Express starts])? 
S: ??????(pikaa [is it up])/?? ????????? (krindikaa [is it 
down]) 
U: ????? ??? (piku [up]) 
S: 16:15 ?????????? ??? ??????? ???? ????????? ??? ?????????? ?????? 
?????????   ??????????????????????????? (16:15 gantalaku kolkata 
rajadhani express howrah nundi bayaluder-
utundi [At 16:15hrs Kolkata Rajdhani Express 
starts from Howrah]). 
 
Example 2: 
U: ??????? ???? ????????? ??? ????????????? ???????? ?????? ??? ?????????? ?????? ??? 
?????? ? ???????? ???????????????????????? ?? ???? ??????? ??????? ??????? 
(Kolkata rajadhani expressloo first acku twotier 
acku howrah nundi mughulsaraike tikkettu dhara 
enta [What is the fare between Howrah to 
Mughalsarai in Kolkata Rajdhani Express for 
First A.C, Two Tier A.C])? 
S: ??????? ???? ????????? ??? ????????????? ???????? ?????? ??? ?????????? ?????? ??? 
?????? ? ???????? ???????????????????????? ?? ???? ??????? ??????? 2061, 1060  
??????????????. (Kolkata rajadhani expressloo first 
acku twotier acku howrah nundi mughulsaraike 
tikeettu dhara 2061, 1060 rupaayulu [Fare from 
Howrah to Mughulsarai in Kolkata Rajdhani Ex-
press for First A.C, Two Tier A.C is 2061, 1060 
respectively]). 
U: ?????????? ? ?????? ??? (sleeper classku [to Sleeper Class] 
(Here it refers to the previous request of the user) 
S: ??????? ???? ????????? ??? ????????????? ???????? ?????, ?????????? ???? , 
??????????? ????? ?????????????? ?????????. (kolkata rajadhani ex-
pressloo first ac, twotier ac, three tier ac ma-
tramee untayi [Rajdhani Express has First AC, 
Two Tier AC and Three Tier AC only]). Because 
in India, Rajdhani Express won?t has Sleeper 
Class. This cooperative message generated by 
the DM. 
 
Example 3: 
U: ???? ???????????? ????????? ???????????? ??? ?? ?????????   ???????? 
(secundrabad nundi tirupatiki eeee raillu unnayi 
[What are the trains from Secunderabad to Tiru-
pati])? 
S: ? ?????? ??? ?????????? (Krishna Express),???????? ??? ?????????? 
(Sabari Express), ??????????? ??? ?????????? (Padmavati Ex-
press), ??????????????? ??? ?????????? (Narayanadri Ex-
press). 
 U: ??? ???????????????? ???????????????????????? (evi 
somavaaramu bayaluderutayi [Which start on 
Monday])? 
S: ? ?????? ??? ?????????? (Krishna Express),???????? ??? ?????????? 
(Sabari Express), ??????????????? ??? ?????????? (Narayana-
dri Express). 
U: ??????????? ?? ??????????????? ??? ?????????? ????????????? (ennintiki 
narayanadri express veltundi [When Narayana-
dri Express goes])? 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
59
S: 07:15 ?????????? ??? ??????????????? ??? ?????????? ??????????? 
????????????? (07:15 gantalaku narayanadri express 
tirupati veltundi [At 07:15 hrs Narayanadri Ex-
press goes to Tirupati]). 
 
Example 4: 
U: ???? ???????????? ????????? ???????????? ??? ? ????????? ?????????? 
(secundrabad nundi tirupatiki ee raillu veltayi 
[What are the trains from Secunderabad to Tiru-
pati])? 
S: ? ?????? ??? ?????????? (Krishna Express),???????? ??? ?????????? 
(Sabari Express), ??????????? ??? ?????????? (Padmavati Ex-
press), ??????????????? ??? ?????????? (Narayanadri Ex-
press). 
 U: ???? ????????? ????????????? (edi twaragaa veltundi 
[Which Goes Early])? 
S: No answer. Because keyword ????????? (twara-
gaa [early]) was not found in the knowledge 
base.i.e query belongs to another query frame. 
 
 
EACL 2006 Workshop on Multilingual Question Answering - MLQA06
60
Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 89?94,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
Handling of Prepositions in English to Bengali  
Machine Translation 
 
Sudip Kumar Naskar 
Dept. of Comp. Sc. & Engg., 
Jadavpur University, 
Kolkata, India 
sudip_naskar@hotmail.com 
Sivaji Bandyopadhyay 
Dept. of Comp. Sc. & Engg.,  
Jadavpur University, 
Kolkata, India 
sivaji_cse_ju@yahoo.com 
 
  
 
Abstract 
The present study focuses on the lexical 
meanings of prepositions rather than on 
the thematic meanings because it is in-
tended for use in an English-Bengali ma-
chine translation (MT) system, where the 
meaning of a lexical unit must be pre-
served in the target language, even 
though it may take a different syntactic 
form in the source and target languages. 
Bengali is the fifth language in the world 
in terms of the number of native speakers 
and is an important language in India. 
There is no concept of preposition in 
Bengali. English prepositions are trans-
lated to Bengali by attaching appropriate 
inflections to the head noun of the prepo-
sitional phrase (PP), i.e., the object of the 
preposition. The choice of the inflection 
depends on the spelling pattern of the 
translated Bengali head noun. Further 
postpositional words may also appear in 
the Bengali translation for some preposi-
tions. The choice of the appropriate post-
positional word depends on the WordNet 
synset information of the head noun. 
Idiomatic or metaphoric PPs are trans-
lated into Bengali by looking into a bi-
lingual example base. The analysis pre-
sented here is general and applicable for 
translation from English to many other 
Indo-Aryan languages that handle prepo-
sitions using inflections and postposi-
tions. 
1 Introduction 
Prepositions have been studied from a variety of 
perspectives. Both linguistic and computational 
(monolingual and cross-lingual) aspects of 
prepositions have been contemplated by several 
researchers. Jackendoff (1977), Emonds (1985), 
Rauh (1993) and Pullum and Huddleston (2002) 
have investigated the syntactic characteristics of 
preposition. Cognitive theorists have examined 
the polysemous nature of prepositions and ex-
plored the conceptual relationships of the 
polysemy, proposing the graphical mental im-
ages (Lakoff and Johnson, 1980; Brugman, 1981, 
1988; Herskovits, 1986; Langacker, 1987; Tyler 
and Evans, 2003). Fauconnier (1994) and Visetti 
and Cadiot (2002) have canvassed the pragmatic 
aspects of prepositions. A practical study of the 
usage of prepositions was carried out for the pur-
pose of teaching English as a second language 
(Wahlen, 1995; Lindstromberg, 1997; Yates, 
1999). The deictic properties of spatial preposi-
tions have been studied by Hill (1982), while the 
geographical information provided by them was 
an interest of computational research (Xu and 
Badler, 2000; Tezuka et al, 2001). 
In the fields of natural language processing, 
the problem of PP attachment has been a topic 
for research for quite a long time, and in recent 
years, the problem was explored with a neural 
network-based approach (Sopena et al, 1998) 
and with a syntax-based trainable approach (Yeh 
and Vilain, 1998). Although past research has 
revealed various aspects of prepositions, there is 
not much semantic research of prepositions 
available for computational use, which requires a 
vigorous formalization of representing the se-
mantics. A recent semantic study of prepositions 
for computational use is found in (Voss, 2002), 
with a focus on spatial prepositions. Spatial 
prepositions are divided into three categories ac-
cording to which one of the two thematic mean-
ings between place and path they acquire when 
they are in argument, adjunct and non-
subcategorized positions of particular types of 
89
verbs. The semantics of spatial prepositions dealt 
with in (Voss, 2002) is not lexical but thematic. 
There are some prepositions (e.g., over, with), 
which have many senses as preposition. By mak-
ing use of the semantic features of the Comple-
ments (reference object) and Heads (verb, verb 
phrase, noun or noun phrase governing a preposi-
tion or a PP), the meaning of the polysemous 
prepositions can be computationally disambigu-
ated. The different meanings of over call for dif-
ferent semantic features in its heads and com-
plements [Alam, 04]. 
Prepositional systems across languages vary to 
a considerably degree, and this cross-linguistic 
diversity increases as we move from core, physi-
cal senses of prepositions into the metaphoric 
extensions of prepositional meaning  (metaphor 
or rather, idiomaticity is one of the main realms 
of usage with prepositions) (Brala, 2000). 
The present study focuses on the lexical mean-
ings of prepositions rather than on the thematic 
meanings because it is intended for use in an 
English-Bengali machine translation (MT) sys-
tem, where the meaning of a sentence, a phrase 
or a lexical entry of the source language must be 
preserved in the target language, even though it 
may take a different syntactic form in the source 
and target languages. Bengali is the fifth lan-
guage in the world in terms of the number of na-
tive speakers and is an important language in 
India. It is the official language of neighboring 
Bangladesh. There is no concept of preposition 
in Bengali. English prepositions are translated to 
Bengali by attaching appropriate inflections to 
the head noun of the PP, i.e., the object of the 
preposition. The choice of the inflection depends 
on the spelling pattern of the translated Bengali 
head noun. Further postpositional words may 
also appear in the Bengali translation for some 
prepositions. The choice of the appropriate post-
positional word depends on the WordNet (Fell-
baum, 1998) synset information of the head 
noun. Idiomatic or metaphoric PPs are translated 
into Bengali by looking into a bilingual example 
base. 
A brief overview of the English-Bengali MT 
System is presented in Section 2. Different types 
of English prepositions and their identification in 
the MT system are described in Section 3. Inflec-
tions and postpositions in Bengali are outlined in 
Section 4. Translation of English prepositions to 
inflections and postpositions in Bengali are de-
tailed in Section 5. The conclusion is drawn in 
Section 6.  
2 A Brief Overview of the English-
Bengali MT System 
The handling of English prepositions during 
translation to Bengali has been studied with re-
spect to an English-Bengali MT system (Naskar 
and Bandyopadhyay, 2005) being developed. In 
order to translate from English to Bengali, the 
first thing we do is lexical analysis of the English 
sentence using the WordNet, to gather the lexical 
features of the morphemes. During morphologi-
cal analysis, the root words / terms (including 
idioms and named entities), along with associ-
ated grammatical information and semantic cate-
gories are extracted. A shallow parser identifies 
the constituent phrases of the source language 
sentence and tags them to encode all relevant 
information that might be needed to translate 
these phrases and perhaps resolve ambiguities in 
other phrases. Then these phrases are translated 
individually to the target language (Bengali) us-
ing Bengali synthesis rules. The noun phrases 
and PPs are translated using Example bases of 
syntactic transfer rules. Verb phrase translation 
scheme is rule based and uses Morphologi-
cal Paradigm Suffix Tables. Finally, those target 
language phrases are arranged using some 
heuristics, based on the word ordering rules of 
Bengali, to form the target language representa-
tion of the source language sentence.  
3 Prepositions in English  
A preposition is a word placed before a ?noun? 
to show in what relation the noun stands with 
regard to the other noun and verb words in the 
same sentence. The noun that follows a preposi-
tion, i.e., the reference object is in the accusative 
case and is governed by the preposition. Preposi-
tions can also be defined as words that begin 
prepositional phrases (PP). A PP is a group of 
words containing a preposition, an object of the 
preposition, and any modifiers of the object. 
Syntactically, prepositions can be arranged 
into three classes ? simple prepositions (e.g., at, 
by, for, from etc.), compound prepositions and 
phrase prepositions. A compound preposition is 
made up of a set of words which starts with and 
acts like a preposition (e.g., in spite of, in favor 
of, on behalf of etc.). A phrase preposition is a 
simple preposition preceded by a word from an-
other category, such as an adverb, adjective, or 
conjunction (e.g., instead of, prior to, because of, 
according to etc.). 
Frequently prepositions follow the verbs to-
gether forming phrasal verbs and remain sepa-
90
rate. A word that looks like a preposition but is 
actually part of a phrasal verb is often called a 
particle. E.g. ?Four men held up the bank.? Here 
held up is a verb [?to rob?]. Therefore, up is not 
a preposition, and bank is not the object of a 
preposition. Instead, bank is a direct object of the 
verb held up. A particle may not always appear 
immediately after the verb with which it makes 
up a phrasal verb (e.g., Four men held the bank 
up.). 
An idiomatic (metaphoric) PP starts with a 
preposition, but its meaning cannot be ascer-
tained from the meaning of its components. Ex-
amples of idiomatic PPs are: at times, by hook or 
crook etc. 
All these syntactical characteristics are used to 
identify prepositions in the English-Bengali MT 
system. Moreover, the inventory of prepositions 
in English is a close set. So, identification of 
prepositions is not much of a problem in English. 
A simple list serves the purpose. The preposi-
tions, compound prepositions, phrase preposi-
tions and idiomatic PPs are identified during 
morphological analysis. Some of the phrasal 
verbs (when the phrasal verb appears as a whole) 
are identified during the morphological analysis 
phase and some during parsing (when the parti-
cle does not accompany the verb). 
However, there are some words that act as 
prepositions and fall into other POS categories as 
well. For example, the word before can be used 
as an adverb (e.g., I could not come before), 
preposition (e.g., He came before me) or a con-
junction (e.g., He came before I came). Simi-
larly, the word round can be used as an adjective 
(e.g., Rugby is not played with a round ball), 
noun (e.g., Rafter was knocked out of the tour-
nament in the third round), adverb (e.g., They 
have moved all the furniture round), preposition 
(e.g., The earth revolves round the sun) and verb 
(e.g., His eyes rounded with anger). But depend-
ing on the POS of the neighboring words/terms, 
the parser easily identifies the correct POS of the 
word in the particular context. 
A preposition is usually placed in front of (is 
?pre-positioned? before) its object, but some-
times however may follow it (e.g., What are you 
looking at?). The preposition is often placed at 
the end when the reference object is an interroga-
tive pronoun (e.g., Where are you coming 
from?) or a relative pronoun (e.g., My grandfa-
ther was a collector of coins, which we used to 
fight over). In such cases, the system finds out 
that the preposition is not a particle and is not 
followed by a noun either, so it must be a 
stranded preposition. It searches the pronoun 
(relative or interrogative) that appears at its left 
and relates the stranded preposition to the pro-
noun. Thus during translation, the following 
conversion takes place.  
(1) Where are you coming 
from? ?? From where are you 
coming? 
(2) My grandfather was a 
collector of coins, which we 
used to fight over. ?? My 
grandfather was a collector 
of coins, over which we used 
to fight. 
But if the pronoun is missing, then the system 
has to find out the elliptical pronoun first.  
(3) I am grateful to the man 
I have spoken to. ? I am 
grateful to the man [whom] I 
have spoken to. ? I am 
grateful to the man to 
[whom] I have spoken. 
Prepositions represent several relations with 
the nouns governed by them. Spatial and tempo-
ral prepositions (which indicate a place or time 
relation) have received a relatively in-depth 
study for a number of languages. The semantics 
of other types of prepositions describing manner, 
instrument, amount or accompaniment largely 
remain unexplored. In case of an MT system, 
when a preposition has different representations 
in the target language for different relations indi-
cated by it, identification of the relation is neces-
sary. The WordNet synset information of the 
head noun of the PP, i.e., the object of the prepo-
sition serves to identify the relation. 
4 Inflections and Postpositions in Ben-
gali 
In Bengali, there is no concept of preposition. 
English prepositions are handled in Bengali us-
ing inflections (vibhaktis) to the reference objects 
and/or post-positional words after them. Inflec-
tions get attached to the reference objects. An 
inflection has no existence of its own in the lan-
guage, and it does not have any meaning as well. 
There are only a few inflections in Bengali: ? 
(null), -?#(-e), -?^ (-y), -??^ (-ye), -?T? (-te), -
?#?T? (-ete), -?E? (-ke), -??[? (-re), -?#??[? (-ere), 
-?[? (-r) and -?#?[? (-er) (an inflection is repre-
91
sented as a word with a leading ?-? in this paper). 
The placeholder indicated by a dashed circle 
represents a consonant or a conjunct. For exam-
ple, if -?# inflection is attached to the word 
[??L??[? (bazar [market]) the inflected word is 
[??L???[? (bazar-e [market-to]). On the other hand, 
post-positional words are independent words. 
They have meanings of their own and are used 
independently like other words. A post-positional 
word is positioned after an inflected noun (the 
reference object). Some examples of the post-
positional words in (colloquial) Bengali are: ?V??^ 
(diye [by]), ?U?E? (theke [from]), LX? (jonno 
[for]), E????K? (kachhe [near]), a?]?X  (samne [in 
front of]) etc. 
5 Translating English prepositions to 
Bengali 
When an English PP is translated into Bengali, 
the following transformation takes place: (prepo-
sition) (reference object) ?? (reference object) 
[(inflection)] [(postpositional-word)]. The corre-
spondence between English prepositions and 
Bengali postpositions (inflections and post-
positional words) is not direct. As far as the se-
lection of the appropriate target language repre-
sentation of a preposition is concerned the refer-
ence object plays a major role in determining the 
correct preposition sense. Deciding whether the 
preposition is used in a spatial sense, as opposed 
to a temporal or other senses, is determined by 
the semantics of the head noun of the reference 
object. A noun phrase (NP) denoting a place 
gives rise to a spatial PP. Similarly, an object 
referring to a time entity produces a temporal 
expression. These relationships can be estab-
lished by looking at the WordNet synset informa-
tion of the head noun of the PP. 
5.1 Translating English prepositions using 
Inflections in Bengali 
The translation of the three English prepositions 
'in', 'on', and 'at' involves identifying the possible 
inflection to be attached to the head noun of the 
PP. No postpositional words are placed after the 
head noun for these prepositions. The three 
prepositions 'in', 'on', and 'at' (which are both 
spatial and temporal in nature) can be translated 
into the Bengali inflections '-?#' (-e), '-?T?? (-te), 
-?#?T? (-ete) and '?^ '  (-y). Any of these 4 Bengali 
inflections can be placed after the reference ob-
ject for any of these 3 English spatial and tempo-
ral prepositions. The choice depends on the spell-
ing of the translated reference object. The rule is: 
if the last letter of the Bengali representation of 
the reference object is a consonant, ??#? (-e) or -
?#?T? (-ete) is added to it (e.g., at/in market? 
[??L???[? [bazar-e / bazar-ete]), else if the last let-
ter of the Bengali word is a matra (vowel modi-
fier) and if the matra is ?#??  (-a), any of ??T??    
(-te), or  '?^ '  (-y) can be added to the Bengali ref-
erence word (e.g., in evening? a?????T? / a?????^ 
[sandhya-te / sandhya-y]), otherwise '?T??  (-te) is 
added to it (e.g., at home? [???Q???T? [badi-te]). 
When translating the temporal expressions, if 
?on? is followed by a day (like Sunday, Monday 
etc.) or by a date in English, null inflection is 
added. 
To translate this type of PPs, we take the help 
of an example base, which contains bilingual 
translation examples. Here are some translation 
examples from the example base (TLR ? target 
language representation of the reference object). 
(1) at / in (place) ?? 
(TLR) - ?(?# / ??^ / ?T? ) [ - ( e / 
ye / te )] 
(2) of (NP) ?? (TLR) - ?( ?[? / 
?#?[? / ??^?[? ) [ - ( r / er / yer 
)] 
5.2 Translating English prepositions using 
Inflections and Postpositions in Bengali 
Most of the English prepositions are translated to 
Bengali as inflections and postpositions to the 
noun word representing the reference object. To 
translate this type of PPs, we take the help of an 
example base, which contains bilingual transla-
tion examples. Here are some translation exam-
ples from the example base (TLR ? target lan-
guage representation of the reference object).  
(1) before (artifact) ?? 
(TLR) - ?( ?[? / ?#?[? / ??^?[? ) a?]?X [ - 
( r / er / yer ) samne ] 
(2) before (!artifact) ?? 
(TLR) - ?( ?[? / ?#?[? / ??^?[? ) %??G [ - 
( r / er / yer ) age ] 
92
(3) round (place / physical 
object) ?? (TLR) - ?( ?[? / ?#?[? / 
??^?[? ) ?J???[??V?E? [ - ( r / er / yer 
) chardike ] 
(4) after (time) ?? (TLR) - 
?( ?[? / ?#?[? / ??^?[? ) Y??[?? [ - ( r / er 
/ yer ) pare ] 
(5) since (place / physical 
object / time) ?? (TLR) ?U?E? 
[theke] 
The choice of inflection depends on the spell-
ing of the translated reference object as said be-
fore. If the translated reference object ends with 
a vowel, ??^?[? is added to it; else if ends with a 
consonant, ?#?[? (er)is added to it; otherwise (it 
ends with a matra) ?[? (r)is appended with it. The 
postpositional word is placed after the inflected 
reference object in Bengali. The choice of the 
postpositional word depends on the semantic 
information about the reference objects as col-
lected from the WordNet. In cases with one 
postpositional word, there is no need to know the 
semantic features of the reference objects. For 
example, ?since?, as a preposition, is always 
translated as ?U?E? (theke) in Bengali, irrespec-
tive of the reference object. Again in some cases, 
this semantic information about the reference 
object does not suffice to translate the preposi-
tion properly.  
Consider the following examples that include 
the preposition before in two different senses. 
(1) He stood before the 
door. ?? ?a V?[?L??[? a?]?X V??Q???_ 
(se [he] darja-r samne [the 
door before] dandalo 
[stood]) 
(2) He reached before eve-
ning.  ?? ?a a?????[? %??G 
?Yg??K??_ (se [he] sondhya-r age 
[evening before] pouchhalo 
[reached]) 
(3) He reached before John. 
?? ?a L?X?[? %??G ?Yg??K??_ (se [he] 
jan-er age [John before] 
pouchhalo [reached]) 
From the WordNet, the system acquires the 
semantic information that ?door? is a hyponym of 
?artifact?, whereas ?evening? and ?me? (which 
represents a person) are not. Thus ?with? is trans-
lated to Bengali as - ?[? a?]?X in sentence (1), and 
takes the meaning - ?( ?[? / ?#?[? / ??^?[? ) %??G in 
sentence (2) and (3). 
As there is no ambiguity in the meaning of 
compound prepositions and phrase prepositions, 
a simple listing of them (along with their Bengali 
representations) suffices to translate them. We 
have prepared a list that contains the phrase 
prepositions and compound prepositions in Eng-
lish along with their Bengali translations. 
 
English Bengali 
in spite of a????C [satteo] 
away from - ?( ?[? / ?#?[? / ??^?[? ) ?U?E? V???[? 
[ - ( r / er / yer ) theke dure ] 
owing to - ?( ?[? / ?#?[? / ??^?[? ) E???[??S 
[ - ( r / er / yer ) karane ] 
apart from K??Q???C [ chhadao ] 
Instead of - ( ?[? / ?#?[? / ??^?[? ) Y??[?[??T?? 
[ - ( r / er / yer ) paribarte ] 
along with - ( ?[? / ?#?[? / ??^?[? ) a??U 
[ - ( r / er / yer ) sathhe ] 
5.3 Translation of English Idiomatic PPs 
The meaning of an idiomatic PP cannot be de-
rived from the meanings of its components. The 
simplest way to tackle them is to maintain a list-
ing of them. A list or a direct Example Base is 
used which contains idioms, which start with 
prepositions, along with their Bengali transla-
tions.  Such an idiom is treated like any other PP 
during the word-reordering phase. Here are some 
examples of them: 
(1) at times ?? a]??^ a]??^ 
(samaye samaye) 
(2) by hook or crook ?? 
?^\???[?+ ?c??E? (jebhabei hok) 
(3) to a fault ?? ]?y??T???[?N? 
(matratirikto) 
6 Conclusion 
In the present study, the handling of English 
prepositions in Bengali has been studied with 
reference to a machine translation system from 
English to Bengali. English prepositions are han-
93
dled in Bengali using inflections and / or using 
post-positional words. In machine translation, 
sense disambiguation of preposition is necessary 
when the target language has different represen-
tations for the same preposition. In Bengali, the 
choice of the appropriate inflection depends on 
the spelling of the reference object. The choice 
of the postpositional word depends on the se-
mantic information about the reference object 
obtained from the WordNet.   
Acknowledgements  
Our thanks go to Council of Scientific and In-
dustrial Research, Human Resource Develop-
ment Group, New Delhi, India for supporting 
Sudip Kumar Naskar under Senior Research Fel-
lowship Award (9/96(402) 2003-EMR-I). 
References  
Alam, Yukiko Sasaki. 2004. Decision Trees for Sense 
Disambiguation of Prepositions: Case of Over. In 
HLT/NAACL-04. 
Brala, Marija M. 2000. Understanding and trans-
lating (spatial) prepositions: An exercise in 
cognitive semantics for lexicographic pur-
poses. 
Brugman, Claudia. 1988. The story of over: 
Polysemy, semantics and the structure of the 
lexicon. New York: Garland Press. [1981. The 
story of over. Berkely, CA: UC-Berkely MA the-
sis.] 
Emonds, Joseph. 1985. A unified theory of syntac-
tic categories. Dordrecht: Foris. 
Fellbaum, Christiane D. ed. 1998. WordNet ? An 
Electronic Lexical Database, MIT Press, Cam-
bridge, MA.  
Fauconnier, Gilles. 1994. Mental spaces. Cam-
bridge: Cambridge University Press. 
Herskovits, Annette. 1986. Language and spatial 
cognition An interdisciplinary study of the 
prepositions in English. Cambridge: Cambridge 
University Press.  
Hill, Cliffort 1982. Up/down, front/back, left/right. 
A contrastive study of Hausa and English. In 
Weissenborn and Klein, 13-42. 
Jackendoff, Ray. 1977. The architecture of the lan-
guage. Cambridge, MA: MIT Press. 
Lakoff, George and Mark Johnson. 1980. Metaphors 
we live by. Chicago: University of Chicago Press. 
Langacker, Ronald. 1987. Foundations of cognitive 
grammar, vol. 1. Stanford, CA: Stanford Univer-
sity Press. 
Lindstromberg, Seth. 1997. English prepositions 
explained. Amsterdam: John Benjamins. 
Naskar, Sudip Kr. and Bandyopadhyay. Sivaji. 2005. 
A Phrasal EBMT System for Translating English 
to Bangla. In MT Summit X. 
Pullum, Geoffrey and Rodney Huddleston. 2002. 
Prepositions and prepositional phrases. In 
Huddleston and Pullum (eds.), 597-661. 
Rauh, Gisa. 1993. On the grammar of lexical and 
nonlexical prepositions in English. In Ze-
linskiy-Wibbelt (eds.), 99-150. 
Sopena, Joseph M., Agusti LLoberas and Joan L. 
Moliner. 1998. A connectionist approach to prepo-
sitional phrase attachment for real world texts. In 
COLING-ACL ?98, 1233-1237. 
Tezuka, Taro, Ryong Lee, Yahiko Kambayashi and 
Hiroki Takakura. 2001. Web-based inference rules 
for processing conceptual geographical relation-
ships. Proceedings of Web Information Sys-
tems Engineering, 14-21. 
Tyler A. and Evans V. 2003. Reconsidering prepo-
sitional polysemy networks: the case of over*. 
In B. Nerlich, Z. Todd, V. Herman, & D. D. Clarke 
(Eds.), Polysemy: Flexible patterns of meanings in 
mind and language, pp. 95-160. Berlin: Mouton de 
Gruyter. 
Visetti, Yves-Marie and Pierre Cadiot. 2002. Insta-
bility and the theory of semantic forms Start-
ing from the case of prepositions. In Fei-
genbaum, Susanne and Dennis Kurzon (eds.), 9-39. 
Voss, Clare. 2002. Interlingua-based machine 
translation of spatial expressions. University of 
Maryland: Ph.D. Dissertation. 
Wahlen, Gloria. 1995. Prepositions illustrated. 
Michigan: The University of Michigan Press. 
Xu, Yilun Dianna and Norman Badler. 2000. Algo-
rithms for generating motion trajectories described 
by prepositions. Proceedings of Computer Ani-
mation 2000, 30-35. 
Yates, Jean. 1999. The ins and outs of prepositions 
A guidebook for ESL students. New York: Bar-
ron?s. 
Yeh, Alexander S. and Marc B. Vilain. 1998. Some 
properties of preposition and subordinate conjunc-
tion attachments. In COLING-ACL ?98, 1436-
1442. 
94
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 203?206,
Prague, June 2007. c?2007 Association for Computational Linguistics
JU-SKNSB: Extended WordNet Based WSD on the English All-Words 
Task at SemEval-1 
Sudip Kumar Naskar 
Computer Sc. & Engg. Dept., 
Jadavpur University, 
Kolkata, India 
sudip.naskar@gmail.com 
Sivaji Bandyopadhyay 
Computer Sc. & Engg. Dept., 
Jadavpur University, 
Kolkata, India 
sivaji_cse_ju@yahoo.com 
 
 
Abstract 
This paper presents an Extended WordNet 
based word sense disambiguation system 
using a major modification to the Lesk al-
gorithm. The algorithm tries to disambigu-
ate nouns, verbs and adjectives. The algo-
rithm relies on the POS-sense tagged syn-
set glosses provided by the Extended 
WordNet. The basic unit of disambiguation 
of our algorithm is the entire sentence un-
der consideration. It takes a global ap-
proach where all the words in the target 
sentence are simultaneously disambigu-
ated. The context includes previous and 
next sentence. The system assigns the de-
fault WordNet first sense to a word when 
the algorithm fails to predict the sense of 
the word. The system produces a precision 
and recall of .402 on the SemEval-2007 
English All-Words test data. 
1 Introduction 
In Senseval 1, most of the systems disambiguating 
English words, were outperformed by a Lesk vari-
ant serving as baseline(Kilgariff & Rosenzweig, 
2000). On the other hand, during Senseval 2 and 
Senseval 3, Lesk baselines were outperformed by 
most of the systems in the lexical sample track 
(Edmonds, 2002). 
In this paper, we explore variants of the Lesk al-
gorithm on the English All Words SemEval 2007 
test data (465 instances), as well as on the first 10 
Semcor 2.0 files (9642 instances). The proposed 
WSD algorithm is POS-sense-tagged gloss (from 
Extended WordNet) based and is a major modifi-
cation of the original Lesk algorithm. 
2 Extended WordNet 
The eXtended WordNet (Harabagiu et al, 1999) 
project aims to transform the WordNet glosses into 
a format that allows the derivation of additional 
semantic and logic relations. It intends to syntacti-
cally parse the glosses, transform glosses into logi-
cal forms and tag semantically the nouns, verbs, 
adjectives and adverbs of the glosses automati-
cally. The last release of the Extended WordNet is 
based on WordNet 2.0 and has three stages: POS 
tagging and parsing, logic form transformation, 
and semantic disambiguation. 
3 Related Works 
Banerjee and Pedersen (2002) reports an adapta-
tion of Lesk?s dictionary-based WSD algorithm 
which makes use of WordNet glosses and tests on 
English lexical sample from SENSEVAL-2. They de-
fine overlap as the longest sequence of one or more 
consecutive content words that occurs in both 
glosses. Each overlap contributes a score equal to 
the square of the number of words in the overlap. 
A version of Lesk algorithm in combination 
with WordNet has been reported for achieving 
good results in (Ramakrishnan et al, 2004). 
Vasilescu et al (2004) carried on a series of ex-
periments on the Lesk algorithm, adapted to 
WordNet, and on some variants. They studied the 
effect of varying the number of words in the con-
texts, centered around the target word. 
But till now no work has been reported which 
makes use of Extended WordNet for Lesk-like 
gloss-oriented approach. 
203
4 Proposed Sense Disambiguation Algo-
rithm 
The proposed sense disambiguation algorithm is a 
major modification of the Lesk algorithm (Lesk, 
1986). WordNet and Extended WordNet are the 
main resources. 
4.1 Modifications to the Lesk Algorithm 
We modify the Lesk algorithm (Lesk, 1986) in 
several ways to create our baseline algorithm. The 
Lesk algorithm relies on glosses found in tradi-
tional dictionaries which often do not have enough 
words for the algorithm to work well. We choose 
the lexical database WordNet, to take advantage of 
the highly inter?connected set of relations among 
different words that WordNet offers, and Extended 
WordNet to capitalize on its (POS and sense) 
tagged glosses. 
The Lesk algorithm takes a local approach for 
sense disambiguation. The disambiguation of the 
various words in a sentence is a series of inde-
pendent problems and has no effect on each other. 
We propose a global approach where all the words 
(we mean by word, an open-class lemma) in the 
context window are simultaneously disambiguated 
in a bid to get the best combination of senses for 
all the words in the window instead of only the 
target word. The process can be thought of as sense 
disambiguation of the whole context, instead of a 
word.  
The Lesk algorithm disambiguates words in short 
phrases. But, the basic unit of disambiguation of 
our algorithm is the entire sentence under consid-
eration. We later modify the context to include the 
previous and next sentence. 
Another major change is that the dictionary 
definition or gloss of each of its senses is com-
pared to the glosses of every other word in the con-
text by the Lesk algorithm. But in the present 
work, the words themselves are compared with the 
glosses of every other word in the context. 
4.2 Choice of Which Glosses to Use 
While Lesk?s algorithm restricts its comparisons to 
the dictionary meanings of the words being disam-
biguated, our choice of dictionary allows us to also 
compare the meanings (i.e., glosses) of the words, 
as well as the words that are related to them 
through various relationships defined in WordNet. 
For each POS we choose a relation if links of its 
kind form at least 5% of the total number of links 
for that part of speech, with two exceptions. We 
use the attribute relation although there are not 
many links of its kind. But this relation links adjec-
tives, which are not well developed in WordNet, to 
nouns which have a lot of data about them. This 
potential to tap into the rich noun data prompted us 
to use this relation. Another exception is the an-
tonymy relationship. Although there are sufficient 
antonymy links for adjectives and adverbs, we 
have not utilized these relations. 
 
Noun Verb Adjective 
Hypernym 
Hyponym 
Holonym 
Meronym 
Attribute 
Hyponym 
Troponym 
Also see 
Attribute 
Also see 
Similar to 
Pertainym of 
Table 1. WordNet relations chosen for the disam-
biguation algorithm 
4.3 The Algorithm 
The gloss bag is constructed for every sense of 
every word in the sentence. The gloss-bag is con-
structed from the POS and sense tagged glosses of 
synsets, obtained from the Extended WordNet. For 
any synset, the words forming the synset and the 
gloss definition contribute to the gloss-bag. The 
non-content words are left out. Example sentences 
do not contribute to the gloss bag since they are not 
(POS and sense) tagged.  Each word along with its 
POS and sense-tag are stored in the gloss bag. For 
words with different POS, different relations are 
taken into account (according to Table 1) for build-
ing the corresponding gloss-bag. 
This gloss-bag creation process can be per-
formed offline or online. It can be performed dy-
namically on a as-when-needed basis. Or, gloss-
bags can be created for all WordNet entries only 
once and stored in a data file in prior. The issue is 
time versus space. 
Once, this gloss-bag creation process is over, the 
comparison process starts. Each word (say Wi) in 
the context is compared with each word in the 
gloss-bag for every sense (say Sk) of every other 
word (say Wj) in the context. If a match is found, 
they are checked further for part-of-speech match. 
If the words match in part-of-speech as well, a 
score is assigned to both the words: the word being 
matched (Wi) and the word whose gloss-bag con-
tains the match (Wj). This matching event indicates 
204
mutual confidence towards each other, so both 
words are rewarded for this event. Two two-
dimensional (one for word index and the other for 
sense index) vectors are maintained: sense_vote for 
the word in context, and sense_score for the word 
in gloss-bag. Say, for example, the context word 
(Wi # noun) matches with gloss word (Wn # noun # 
m) (i.e., Wi = Wn) in the gloss bag for kth sense of 
Wj. Then, a score of 1/(gloss bag size of (Wjk)) is 
assigned to both sense_vote[i][m] and 
sense_score[j][k]. Scores are normalized before 
assigning because of huge discrepancy in gloss-bag 
sizes. This process continues until each context 
word is matched against all gloss-bag words for 
each sense of every other context words. 
Once all the comparisons have been made, we add 
sense_vote value with the sense_score linearly 
value for each sense of every word to arrive at the 
combination score for this word-sense pair.  
The algorithm assigns a word the nth sense for 
which the corresponding sense_vote and 
sense_score produces the maximum sum, and it 
does not assign a word any sense when the corre-
sponding sense_vote and sense_score values are 0, 
even if the word has only one sense. In the event of 
a tie, we choose the one that is more frequent, as 
specified by WordNet.  
Assuming that there are N words in the window 
of context (i.e. the sentence), and that, on an aver-
age there are S senses per word, and G number of 
gloss words in each gloss bag per sense, N * S 
gloss bags need to be constructed, giving rise to a 
total of N * S * G gloss words. Now these many 
gloss words are compared against each of the N 
context words. Thus, N2 * S * G pairs of word 
comparisons need to be performed. Both, S and G 
vary heavily. 
5 Variants of the Algorithm 
The algorithm discussed thus far is our baseline 
algorithm. We made some changes, as described in 
the following two subsections, to investigate 
whether the performance of the algorithm can be 
improved. 
5.1 Increasing the Context Size 
The poor performance of the algorithm perhaps 
suggests that sentential context is not enough for 
this algorithm to work. So we went for a larger 
context: a context window containing the current 
sentence under consideration (target sentence), its 
preceding sentence and the succeeding sentence. 
This increment in context size indeed performed 
better than the baseline algorithm. 
5.2 Assigning Different Scores 
When constructing the gloss-bags for a word-sense 
pair, some words may appear in more than one 
gloss (by gloss we mean to say synonyms as well 
as gloss). So, we added another parameter with 
every (word#pos#sense) in a gloss bag: noc - the 
number of occurrence of this (word#pos#sense) 
combination in this gloss-bag. 
And, in case of a match of context word (say Wi) 
with a gloss-bag word (of say kth sense of word 
Wj), we scored the words in four ways to see if this 
phenomenon has any effect on the sense disam-
biguation process. Say, for example, the context 
word (Wi # noun) matches with gloss word (Wn # 
noun # m # noc) in the gloss bag for kth sense of Wj 
(i.e., the particular word appears noc times in the 
said gloss-bag) and the gloss bag size is gbs. Then, 
we reward Wi and Wj for this event in four ways 
given below. 
 
1. Assign 1/gbs to 
sense_vote[i][m] and 1/gbs 
to sense_score[j][k]. 
2. Assign 1/gbs to 
sense_vote[i][m] and noc/gbs 
to sense_score[j][k]. 
3. Assign noc/gbs  to 
sense_vote[i][m] and 1/gbs 
to sense_score[j][k]. 
4. Assign noc/gbs to 
sense_vote[i][m] and noc/gbs 
to sense_score[j][k]. 
 
The results of this four-way scoring proved that 
this indeed has influence on the disambiguation 
process. 
The WSD system is based on Extended Word-
Net version 2.0-1.1 (the latest release), which is in 
turn based on WordNet version 2.0. So, the system 
returns WordNet 2.0 sense indexes. These Word-
Net sense indexes are then mapped to WordNet 2.1 
sense indexes using sensemap 2.0 to 2.1. 
6 Evaluations 
The system has been evaluated on the SemEval-
2007 English All-Words Tasks (465 test in-
205
stances), as well as on the first 10 Semcor 2.0 
files, which are manually disambiguated text 
corpora using WordNet senses. 
We compute F-Score as 2*P*R / (P+R). Ta-
ble 2 shows the performance of the four variants of 
the system (with a context size of 3 sentences) 
on the first 10 Semcor 2.0 files. From table 2, it 
is clearly evident that model C produces the best 
result (precision - .621, recall - .533) among the 4 
scoring schemes. POS-wise evaluation results for 
model C on Semcor 2.0 data is given in table 3. 
 
Model  
A B C D 
Precision .618 .602 .621 .604 
Recall .531 .517 .533 .519 
F-Score .571 .556 .574 .558 
Table 2. Evaluation of the four models on Sem-
cor Data 
 
 Noun Verb Adj Overall
Precision .6977 .4272 .6694 .6211 
Recall .6179 .3947 .4602 .5335 
F-Score .6554 .4103 .5454 .574 
Table 3. POS-wise Evaluation for model C on 
Semcor Data 
 
Model C produced a precision of .393 and a re-
call of .359 on the SemEval-2007 English All-
Words test data (465 test instances). Table 4 
shows POS-wise evaluation results for this test 
data. 
 
 Noun Verb Overall 
Precision .507 .331 .393 
Recall .472 .299 .359 
F-Score .489 .314 .375 
Table 3. POS-wise Evaluation on SemEval-2007 
English All-Words test data 
 
When default WordNet first senses were as-
signed to the (40) words for which the algorithm 
failed to predict senses, both the precision and re-
call values went up to .402 (this result has been 
submitted in SemEval-2007). The WSD system 
stood 10th in the SemEval-2007 English All-
Words task. 
7 Discussions 
We believe that this somewhat poor showing can 
be partially attributed to the brevity of definitions 
in WordNet in particular and dictionaries in gen-
eral. The Lesk algorithm is crucially dependent on 
the lengths of glosses. However lexicographers 
aim to create short and precise definitions which, 
though a desirable quality in dictionaries, is disad-
vantageous to this algorithm. Nouns have the long-
est average glosses in WordNet, and indeed the 
highest recall obtained is on nouns. The character-
istics of the gloss bags need to be further investi-
gated. Again many of the sense tagged gloss words 
in Extended WordNet, which are determinant fac-
tors in this algorithm, are of  ?silver? or ?normal? 
quality. And finally, since the system returns 
WordNet 2.0 sense indexes which are mapped to 
WordNet 2.1 indexes with certain amount of con-
fidence using sensemap 2.0 to 2.1, there may be 
some loss of information during this mapping 
process. 
References 
A. Kilgarriff, and J. Rosenzweig. 2000. Framework and 
Results for English SENSEVAL. Computers and the 
Humanities, 34, 15-48. 
Florentina Vasilescu, Philippe Langlais, and Guy La-
palme. 2004. Evaluating Variants of the Lesk Ap-
proach for Disambiguating Words. LREC, Portugal. 
G. Ramakrishnan, B. Prithviraj, and P. Bhattacharyya. 
2004. A Gloss Centered Algorithm for Word Sense 
Disambiguation. Proceedings of the ACL SEN-
SEVAL 2004, Barcelona, Spain, 217-221. 
M. Lesk. 1986. Automatic sense disambiguation using 
machine readable dictionaries: How to tell a pine 
cone from a ice cream cone. Proceedings of SIGDOC 
?86. 
P. Edmonds. 2002. SENSEVAL : The Evaluation of 
Word Sense Disambiguation Systems, ELRA News-
letter, Vol. 7, No. 3. 
S. Banerjee. 2002. Adapting the Lesk Algorithm for 
Word Sense Disambiguation to WordNet. MS Thesis, 
University of Minnesota. 
S. Banerjee, and T. Pedersen. 2002. An Adapted Lesk 
Algorithm for Word Sense Disambiguation Using 
WordNet. CICLing, Mexico. 
S. Harabagiu, G. Miller, and D. Moldovan. 1999. 
WordNet2 - a morphologically and semantically en-
hanced resource. Proceedings of SIGLEX-99, Univ of 
Mariland. 1-8. 
206
Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 103?104,
Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLP
JUNLG-MSR: A Machine Learning Approach of Main Subject          
Reference Selection with Rule Based Improvement 
Samir Gupta 
Department of Computer Science and 
Engineering, Jadavpur University. 
Kolkata-700032, India. 
samir.ju@gmail.com 
Sivaji Bandopadhyay 
Department of Computer Science and 
Engineering, Jadavpur University. 
Kolkata-700032, India. 
sivaji_cse_ju@yahoo.com 
 
 
Abstract 
The GREC-MSR task is to generate appropri-
ate references to an entity in the context of a 
piece of discourse longer than a sentence. In 
MSR ?09 run of this task, the main aim is to 
select the actual main subject reference 
(MSR) from a list of given referential expres-
sions that is appropriate in context. We used a 
machine learning approach augmented with 
some rules to select the most appropriate ref-
erential expression. Our approach uses the 
training set for learning and then combines 
some of the rules found by observation to im-
prove the system. 
1 Introduction 
In this paper we provide a description of our sys-
tem for the GREC MSR task of Generation Chal-
lenges 2009. GREC-2.0 Corpus of 2,000 
Wikipedia introduction sections in which refer-
ences to the main subject of the Wikipedia article 
have been annotated was provided to us by the or-
ganizers. The corpus was divided into five differ-
ent domains like cities, countries, mountains, 
people and rivers. 
The basic approach we used was to develop a 
baseline system first by training the system on the 
training set. This system then selects the most fre-
quent referential expression based on a number of 
parameters of the corresponding reference. After 
evaluation on the development set we used the de-
velopment set to deduce certain rules based on ob-
servation and iteratively added these rules to the 
system and evaluated resulting performance. Thus 
the system development can be divided into two 
phases which are discussed in sections 2 and 3. 
2 Baseline System: Training and Classifi-
cation 
The machine learning approach we used for the 
baseline system was domain independent and 
hence was build by populating a single database 
with the training set data. First we parsed the con-
tents of the XML files of the training sets using a 
Java DOM XML Parser. Then we inserted the 
training set data into the database named grec 
which had two tables: parsed_ref and possi-
ble_refex. There is a one to many mapping from 
possible_refex to parsed_ref. The possible_refex 
contains all possible REFEX elements i.e. referen-
tial expressions  possible while parsed_ref contains 
all the parsed references of the training set with 
attributes such as syncat, semcat, paragraph num-
ber, reference number (with respect to a para-
graph), sentence number and a foreign key refex id 
referring to the possible_refex table.  
The prediction of the referential expression was 
done based on features such as the semantic cate-
gory, syntactic category, paragraph number, refer-
ence number with respect to a paragraph and 
sentence number of the referent. One example 
from the database is, if the semcat of the reference 
is cities, syncat is np-subj, paragraph number is 2, 
ref number is 1 and sentence number equals 1 then 
in 74% of the cases of the training set the referen-
tial expression was with refex id=1 (i.e. 
type=common, emphatic=no, head=nominal and 
103
case= plain) and refex id = 4 (i.e. type=name, em-
phatic=no, head=nominal and case= plain)  had the 
second highest count (19.6%). Thus we selected 
the most frequent refex from the possible referen-
tial expressions corresponding to the feature set of 
the reference, based on their count in the training 
set populated database. These decision rules with 
their associated probabilities are stored in a table 
which served as our model for classification. When 
a number of referential expressions from the 
alt_refex match from the list of the given refexes 
then we select the refex with the longest surface 
form.  In certain case when the refex was not in the 
alt_refex element we select the second best case 
from our decision model. Results of this intermedi-
ate baseline system are given in Table 1. 
 
Domain String 
Acc. 
Reg 
08 
type 
Acc. 
Mean 
Edit 
Dis-
tance 
Norm. 
mean 
edit dis-
tance 
Cities 0.404 0.495 1.657 0.575 
Countr. 0.468 0.576 1.467 0.471 
Mount. 0.567 0.646 1.192 0.380 
People 0.576 0.673 0.902 0.379 
Rivers 0.6 0.6 1.06 0.36 
Overall 0.532 0.62 1.205 0.421 
 
Table 1: Baseline Results 
3 Rule based Improvement 
After the baseline system was evaluated on the 
development set we iteratively added some rules to 
optimize the system output.  These rules are ap-
plied only when a reference matches the below 
stated condition, otherwise the result from the 
baseline system was used. 
The different rules that we deduced are as follows: 
? The referential expression is empty if its 
immediate preceding word is a conjunction 
and the referent?s syncat is np-subj. Thus 
the surface form of the refex is null. 
? In the people domain if the best case out-
put from the baseline results in Reg-type  = 
?name? and if earlier in the paragraph the 
person?s full name has been referred to, 
then subsequent references will have a 
shorter version of the referential expres-
sion i.e. shorter surface form (example: 
Zinn?s instead of Howard Zinn?s) 
? If the same sentence spans two or more 
references then generally a pronoun form 
is used if a noun has been used earlier. 
? Generally common form of the noun is 
used instead of the baseline pronoun out-
put if words like in, for, to, of, in precedes 
the reference (maximum distance 3 
words). This rule is applied to all domains 
except people. 
The first and the last rules had some effect to 
the system but the improvement from the other 
rules was very negligible. Final results are tabu-
lated in Table 2. 
4 Results 
We provide final results of our system in Table 2 
Script geval.pl was provided by the organizers for 
this purpose. We see that inclusion of the above 
rules in the system increased it?s accuracy by al-
most 4-5%. More rules can be added to system by 
studying cases of the training set which do not get 
classified correctly by the best case baseline sys-
tem. Overall reg08 accuracy, precision and recall 
were 66.4 %. 
 
Domain String 
Acc. 
Reg 
08 
type  
Acc. 
Mean 
Edit 
Dist. 
Norm. 
mean 
edit 
Dist. 
Cities 0.434 0.525 1.596 0.544 
Countr. 0.5 0.619 1.381 0.431 
Mount. 0.583 0.663 1.158 0.363 
People 0.659 0.756 0.746 0.296 
Rivers 0.65 0.65 0.95 0.31 
Overall 0.575 0.664 1.12 0.377 
 
Table 2: Final Results 
References 
 
Anja Belz and Albert Gatt. 2008. Grec Main Subject 
Reference Generation Challenge 2009: Participants? 
Pack. 
http://www.nltg.brighton.ac.uk/research/genchal09 
Anja Belz, Eric Kow, Jette Viethen, Albert Gatt. 2008.  
The GREC Challenge 2008: Overview and Evalua-
tion Results. In Proceedings of the Fifth Interna-
tional Natural Language Generation Conference 
(INLG-2008) pages 183-192. 
104
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 76?83,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Bengali Verb Subcategorization Frame Acquisition - A Baseline Model 
 
Somnath Banerjee                      Dipankar Das                      Sivaji Bandyopadhyay 
Department of Computer Science & Engineering 
Jadavpur University, Kolkata-700032, India 
s.banerjee1980@gmail.com, dipankar.dipnil2005@gmail.com, 
sivaji_cse_ju@yahoo.com 
 
 
 
Abstract 
 
Acquisition of verb subcategorization frames 
is important as verbs generally take different 
types of relevant arguments associated with 
each phrase in a sentence in comparison to 
other parts of speech categories. This paper 
presents the acquisition of different subcate-
gorization frames for a Bengali verb Kara 
(do). It generates compound verbs in Bengali 
when combined with various noun phrases. 
The main hypothesis here is that the subcate-
gorization frames for a Bengali verb are same 
with the subcategorization frames for its 
equivalent English verb with an identical 
sense tag.  Syntax plays the main role in the 
acquisition of Bengali verb subcategorization 
frames. The output frames of the Bengali 
verbs have been compared with the frames of 
the equivalent English verbs identified using 
a Bengali-English bilingual lexicon. The 
flexible ordering of different phrases, addi-
tional attachment of optional phrases in Ben-
gali sentences make this frames acquisition 
task challenging. This system has demon-
strated precision and recall values of 77.11% 
and 88.23% respectively on a test set of 100 
sentences. 
1 Introduction  
A subcategorization frame is a statement of what 
types of syntactic arguments a verb (or an adjec-
tive) takes, such as objects, infinitives, that-
clauses, participial clauses, and subcategorized 
prepositional phrases (Manning,1993). The verb 
phrase in a sentence usually takes various types 
of subcategorization frames compared to phrases 
of other types and hence the acquisition of such 
frames for verbs are really challenging. 
A subcategorization dictionary obtained auto-
matically from corpora can be updated quickly 
and easily as different usages develop. Several 
large, manually developed subcategorization 
lexicons are available for English, e.g. the 
COMLEX Syntax (Macleod et al, 1994), AC-
QUILEX (Copestake, 1992) and the ANLT 
(Briscoe et al, 1987) dictionaries. VerbNet (VN) 
(Kipper-Schuler, 2005) is the largest online verb 
lexicon with explicitly stated syntactic and se-
mantic information based on Levin?s verb classi-
fication (Levin, 1993). It is a hierarchical do-
main-independent, broad-coverage verb lexicon 
with mappings to other lexical resources such as 
WordNet (Miller, 1990), XTAG (XTAG Re-
search Group, 2001) and FrameNet (Baker et al, 
1998). But, there is no existing subcategorization 
lexicon available for Bengali language. The sub-
categorization of verbs is an essential issue in 
parsing for the free phrase order languages such 
as Bengali. As there is no such existing parser 
available in Bengali, the acquisition as well as 
evaluation of the acquired subcategorization 
frames are difficult but crucial tasks. The main 
difference between English and Bengali sentence 
is the variation in the ordering of various 
phrases. The pivotal hypothesis here is that the 
subcategorization frames obtained for a Bengali 
verb are same with the subcategorization frames 
that may be acquired for its equivalent verb with 
an identical sense tag in English. 
The present work deals with the acquisition of 
verb subcategorization frames of a verb kara 
(do) from a Bengali newspaper corpus. This verb 
generates various types of compound verbs in 
combination with other preceding noun phrases 
in Bengali. The sentences containing these types 
of compound verb entries have been retrieved 
from the Bengali corpus. The Bengali verb sub-
categorization frame acquisition task has been 
carried out for the ten most frequent compound 
verbs that contain kara (do) as a component. The 
number of occurrences of other compound verbs 
76
is negligible in the corpus. So, for evaluation 
purpose, we have not considered those verbs. 
Each of the ten Bengali compound verbs has 
been searched in the Bengali-English bilingual 
lexicon1 and the equivalent English verb mean-
ings with its synonyms have been identified and 
retrieved. All possible subcategorization frames 
for each of the English synonyms of the Bengali 
verb have been acquired from the English 
VerbNet2. These frames have been mapped to the 
Bengali sentences that contain the compound 
verb. Evaluation results with a test set of 100 
sentences show the effectiveness of the model 
with precision, recall and F-Measure values of 
77.11%, 88.23% and 79.24% respectively. There 
are some frames that have not been identified 
due to their absence in the corpus. Linguists have 
suggested that these frames do appear in Bengali 
and hence can be acquired.   
The rest of the paper is organized as follows. 
Section 2 gives the description of the related 
works carried out in this area.  Section 3 de-
scribes the framework for the acquisition of sub-
categorization frames for ten compound Bengali 
verbs. Evaluation results of the system are dis-
cussed in section 4. Finally section 5 concludes 
the paper. 
2 Related Work 
One of the early works for identifying verbs that 
resulted in extremely low yields for subcategori-
zation frame acquisition is described in (Brent, 
1991). A rule based system for automatically 
acquiring six verb subcategorization frames and 
their frequencies from a large corpus is men-
tioned in (Ushioda et al, 1993). An open class 
vocabulary of 35,000 words was analyzed manu-
ally in (Briscoe and Carroll, 1997) for subcatego-
rization frames and predicate associations. The 
result was compared against associations in 
ANLT and COMLEX. Variations of subcatego-
rization frequencies across corpus type (written 
vs. spoken) have been studied in (Carroll and 
Rooth, 1998). A mechanism for resolving verb 
class ambiguities using subcategorization frames 
is reported in (Lapata and Brew, 1999). All these 
works deal with English. Several works on the 
term classification of verb diathesis roles or the 
lexical semantics of predicates in natural lan-
guage have been reported in ((McCarthy, 2001), 
                                                                                                 
1 http://home.uchicago.edu/~cbs2/banglainstruction.html 
2 http://verbs.colorado.edu/~mpalmer/projects/verbnet.html 
(Korhonen, 2002), (Stevenson and Merlo, 1999) 
and (Walde, 1998)).  
A cross lingual work on learning verb-
argument structure for Czech language is de-
scribed in (Sarkar and Zeman, 2000).  (Samanta-
ray, 2007) gives a method of acquiring different 
subcategorization frames for the purpose of ma-
chine aided translation system for Indian lan-
guages. The work on subcategorization frame 
acquisition of Japanese verbs using breadth-first 
algorithm is described in (Muraki et al, 1997). 
3     System Outline 
We have developed several modules for the ac-
quisition of verb subcategorization frames from 
the Bengali newspaper corpus. The modules con-
sist of POS tagging and chunking, Identification 
and Selection of Verbs, English Verb Determina-
tion, Frames Acquisition from VerbNet and 
Bengali Verb Subcategorization Frame Acquisi-
tion. 
3.1 POS Tagging and Chunking 
We have used a Bengali news corpus (Ekbal and 
Bandyopadhyay, 2008) developed from the web-
archives of a widely read Bengali newspaper.  A 
portion of the Bengali news corpus containing 
1500 sentences have been POS tagged using a 
Maximum Entropy based POS tagger (Ekbal et 
al., 2008). The POS tagger was developed with a 
tagset of 26 POS tags3, defined for the Indian 
languages. The POS tagger demonstrated an ac-
curacy of 88.2%. We have also developed a rule-
based chunker to chunk the POS tagged data 
with an overall accuracy of 89.4%. 
3.2 Identification and Selection of Verbs 
Our previous work (Das et.al., 2009) on the ac-
quisition of Bengali subcategorization frames 
from the same Bengali news corpus was carried 
out for the most frequent verb ?????? (dekha) 
(see) in that corpus. The next highest frequent 
verb in this corpus is ????? (kara) (do) which is 
a special verb in Bengali. However to the best of 
our knowledge, no frame acquisition task has 
been carried out yet for this Bengali verb. The 
single occurrence of ????? (kara) as a main verb 
in a sentence takes completely different subcate-
gorization frames in comparison with the ac-
quired frames for the compound verbs consisting 
of ????? (kara) as a component. Hence, we have 
 
3http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.pdf  
 
77
concentrated our focus to acquire subcategoriza-
tion frames for the Bengali verb ????? (kara).   
For this purpose, we have manually analyzed 
the tagged and chunked data to identify the word 
????? (kara) that are tagged as main verb (VM) 
and belong to the verb group chunk (VG) in the 
corpus. The preceding noun phrase of ????? 
(kara) generally produces completely different 
verbs in Bengali (e.g. [???? ??? (tairi(NN) 
kara(VM))(make)], [?????? ??? (byabahar (NN) 
kara(VM))(use)] etc.).  
Bengali, like any other Indian languages, is 
morphologically very rich. Different suffixes 
may be attached to a verb depending on the vari-
ous features such as Tense, Aspect, and Person. 
The Bengali stemmer uses a suffix list to identify 
the stem form of the verb ????? (kara). Another 
table stores the stem form and the corresponding 
root form. Stemming process has correctly iden-
tified 234 occurrences of the verb ????? (kara) 
from its 241 occurrences in the corpus with an 
accuracy of 97.09%. The sentences where the 
verb ????? (kara) appears in any inflected form 
but has been tagged as main verb (VM) have 
been retrieved. These sentences have been con-
sidered for fine-grained analysis of verb subcate-
gorization frames. It is expected that the corpus 
will have adequate number of occurrences for 
each subcategorization frame of the verb. The 
passive occurrences of ????? (kara) such as 
???????? (karano), ????? (kariye) have been fil-
tered out and the sentences containing the pas-
sive entries of ????? have not been considered in 
the present work. 
The compound verb phrases with pattern such 
as {[XXX] (NN) [kara] (VM)} have been identi-
fied and retrieved from the Bengali POS tagged 
and chunked corpus. It has been observed that 
most of these compound verb phrases are indi-
vidually different verbs in Bengali. Around 182 
various kinds of verbs have been identified. Cer-
tain typical and distinct occurrences of ????? 
(kara) have also been identified. But, linguistic 
verification shows that these typical verbs are 
formed by attaching the verb ????? (kara) to an 
adjective or an adverb word, like ???? ??? 
(jhakjhak kara) , ???? ??? (taktak kara), ??? 
??? (sheet kara) etc. Such types of around 48 
verb entries have been identified and filtered out 
from the corpus. The rest 134 distinct types of 
Bengali compound verbs (CV) with ????? (kara) 
as a component have been considered as target 
verbs for analysis. 
We have identified the frequencies of these 
verbs in the corpus. It has to be mentioned that 
only a few verbs have an adequate number of 
sentences in the corpus. For this reason, only the 
top ten compound verbs that have the largest 
number of occurrences in the corpus have been 
selected. Table 1 represents the top 10 different 
Bengali compound verbs and their frequencies 
obtained from the corpus.  
 
  Table 1. Top 10 Bengali Compound Verbs and 
their frequencies obtained from the corpus 
Bengali Verbs Freq. 
???? ??? (tairi kara) (make) 23 
?????? ??? (byabahar kara) (use) 18 
??? ??? (bas kara) (live) 17 
??? ??? (kaj kara) (work) 15 
??g? ??? (sangraha kara) (collect) 13 
?n ??? (bandha kara) (shut) 13 
???????   ??? (chitkar kara) (shout) 3 
??? ??? (bhul kara) (mistake) 3 
??j??? ??? (jigyasa kara) (ask) 3 
?????k? ??? (parjabekkhan kara) 
(observe) 
3 
3.3 English Verb Determination  
The verb subcategorization frames for the 
equivalent English verbs (in the same sense) of a 
Bengali verb are the initial set of verb subcatego-
rization frames that have been considered as 
valid for that Bengali verb. The root forms of the 
target verbs appearing in different inflected 
forms in the Bengali corpus have been identified 
by the process described in section 3.2. The de-
termination of equivalent English verbs has been 
carried out using a Bengali-English bilingual 
lexicon. We have used the available Bengali-
English bilingual dictionary that has been for-
matted for the text processing tasks. Various syn-
tactical representations of a word entry in the 
lexicon have been analyzed to identify its syno-
nyms and meanings. The example of an entry in 
the bilingual lexicon for our target verb ????? 
(kara) is given as follows. 
<??? [kar?] v to do, to per-
form, to accomplish, to exe-
cute (??? ???); to build, to 
make (???? ???) ;.....> 
 
But, the various distinct verbs, with ????? 
(kara) as a component have individual separate 
78
entries in the bilingual dictionary. We have iden-
tified the equivalent English verbs from those 
Bengali verb entries in the dictionary. For exam-
ple,  
<???? ??? v. to build, to 
make; ?> 
<?????? ??? v. to apply, to 
use; to behave; to treat (a 
person), to behave towards; 
?> 
<??? ??? v. to work; to 
serve; to be effective ;?> 
 
Different synonyms for a verb having the 
same sense are separated using ?,? and different 
senses are separated using ?;? in the lexicon. The 
synonyms including different senses of the target 
verb have been extracted from the lexicon. This 
yields a resulting set called Synonymous Verb 
Set (SVS). For example, the English synonyms 
(apply, use) and synonym with another sense 
(behave) have been selected for Bengali verb 
??????? ???? (byabahar kara) and have been 
categorized as two different SVS for the Bengali 
verb ??????? ????. Two synonyms (make, build) 
for the Bengali verb ????? ???? (tairi kara) are 
thus present in the same SVS. Now, the task is to 
acquire all the possible existing frames for each 
member of the SVS from the VerbNet. The 
????? (kara) verb may also appear in passive 
form in Bengali sentences. For example,  
 ?????          ???        
(Ramke)NNP  (kaj)NN   
 ??????        ?????? 
(karano)VM  (hayechilo)VAUX 
 
The corresponding dictionary entry for the 
passive form of ????? (kara) is as follows. But in 
this work, we have concentrated only on those 
sentences where ????? (kara) appears in active 
form.  
<?????? [kar?n?] v to cause to 
do or perform or accomplish 
or execute or build or 
make?> 
3.4 Frames Acquisition from VerbNet  
VerbNet associates the semantics of a verb with 
its syntactic frames and combines traditional 
lexical semantic information such as thematic 
roles and semantic predicates, with syntactic 
frames and selectional restrictions. Verb entries 
in the same VerbNet class share common syntac-
tic frames, and thus they are believed to have the 
same syntactic behavior. The VerbNet files con-
taining the verbs with their possible subcategori-
zation frames and membership information is 
stored in XML file format. The Bengali verb ???? 
??? (tairi kora) (make) has no direct class in 
VerbNet. The verb ?make? and its synonymous 
verb ?build? are members of one of the sub-
classes of the build-26.1 class and ?make? is also 
a member of the dub-29.3 class. A snapshot of 
XML file for the build-26.1 class is given below.
..... 
<VNCLASS ID="build-26.1"  
.....<SUBCLASSES> 
   <VNSUBCLASS ID="build-26.1-1"> 
<MEMBERS> 
    <MEMBER name="build" 
wn="build%2:36:00"/> 
    <MEMBER name="make" 
wn="make%2:36:01 make%2:36:05 
..... 
make%2:42:13 make%2:36:10"/> 
.....  
</MEMBERS> 
..... 
<FRAME> 
    <DESCRIPTION descriptionNum-
ber="3.9" primary="NP-PP" secon-
dary="Asset-PP" xtag=""/>            
<EXAMPLES> 
    <EXAMPLE> The contractor 
builds houses for $100,000.     
    </EXAMPLE> 
    .....  
</EXAMPLES> 
.....</FRAME> 
..... 
The verbs in VerbNet that take same type of sub-
categorization frames are stored in the  <MEM-
BER> tag and the possible primary and secon-
dary subcategorization frames are kept in <DE-
SCRIPTION> tag with proper English examples 
for each frame. The example for each of the sub-
categorization frames for the English verb 
"make" has been given in the "build-26.1-1" sub-
class of the ?build-26.1? class in the VerbNet. 
The sentence tagged within <EXAMPLE>.. 
</EXAMPLE> shows that after the occurrence 
of the verb "build/make", one noun phrase (NP) 
and one prepositional phrase (PP) have occurred 
as the arguments of the verb. The frame cor-
responding to this sentence has been described as 
the primary frame "NP-PP" in the frame descrip-
tion <DESCRIPTION> tag. 
79
Sense wise separated SVS members occupy 
the membership of same class or subclass in 
VerbNet. It has been observed that the verbs 
?build? and ?make? are members of the same 
SVS (extracted from the Bengali-English bilin-
gual dictionary) and they are also members of the 
same subclass build-26.1-1. Therefore, both of 
the verbs take same subcategorization frames. 
 
SVS (VerbNet 
classes) 
Primary and Secondary 
Frames for a SVS 
Make (build-
26.1-1) 
Build (build-
26.1-1) 
NP-PP, NP, NP-NP, NP-
NP-PP, Asset-PP 
Asset-Subject 
Use (use-105, 
consume-66, fit-
54.3) 
Apply (use-105) 
NP-ADVP, NP-PP, NP-
TO-INF-VC, Basic 
Transitive, NP-ING-SC, 
Location Subject 
Alternation, NP-PP 
for-PP, Location-PP 
Behave (mas-
querade-29.6, 
29.6-1) 
PP, Basic Transitive 
as-PP, like-PP, in-PP 
Table 2. The SVS members and their subcatego-
rization frames for the corresponding Bengali 
verbs ???? ??? (tairi kara) and  
?????? ??? (byabahar kara) 
 
The xml files of VerbNet have been preproc-
essed to build up a general list that contains all 
members (verbs) and their possible subcategori-
zation frames (primary as well as secondary) in-
formation. This preprocessed list is searched to 
acquire the subcategorization frames for each 
member of the SVS of the ten Bengali verbs 
(identified in section 3.3).  As the verbs are clas-
sified according to their semantics in the 
VerbNet, the frames for the particular Bengali 
verb are assumed to be similar to the frames ob-
tained for the members of its SVS.  It has also 
been observed that the same verb with a different 
sense can belong to a separate class in the 
VerbNet. For example, the acquired frames (pri-
mary and secondary) for each member of the 
SVS of the target verbs (??????? ???? and ????? 
????) have been shown in Table 2. In this way, 
all possible subcategorization frames for each 
member of a SVS have been extracted from the 
generalized search list for our ten target verbs. 
3.5 Bengali Verb Subcategorization Frames 
Acquisition  
The acquired VerbNet frames have been mapped 
to the Bengali verb subcategorization frames by 
considering the position of the verb as well as its 
general co-existing nature with other phrases in 
Bengali sentences. 
    The syntax of ?NP-PP? frame for a Bengali 
sentence has been acquired by identifying the 
target verb followed by a NP chunk and a PREP 
chunk. The sentences containing prepositional 
frame ?PP? do not appear in the Bengali corpus, 
as there is no concept of preposition in Bengali. 
But, when we compare the sentences containing 
postpositional markers, i.e. PREP (postpositions) 
as a probable argument of the verb, the system 
gives the desired output.  
???       ????         ??????? 
(jar)PRP (theke)PREP (hat-
pakha)NN                      
 ??      ?c???        ???? 
(ar)CC   (achhadon)QF 
(toiri)NN                                
????????         ???k 
(korechilen)VM (Max)NN 
 
All the frames of a SVS corresponding to a 
Bengali verb have been considered. The Bengali 
verb ??????? ???? (byabahar kara) in the fol-
lowing sentence has taken the frame ?ADVP-
PRED? (the word with RB tag) from a different 
SVS.  
 ?????????            
(karmachari ra)NN 
 ?nh??? ??          
(bondhuttwapurno)RB                      
??????          ???? 
(byabahar)NN  (karen)VM 
   
Another form of ?ADVP-PRED? frame has 
been obtained by considering the Bengali mean-
ing of the corresponding English adverbial 
phrase. ?There? is an adverbial phrase taken by 
the ?live? verb in English. The corresponding 
representation in the equivalent Bengali verb is 
o????i (okhanei) as shown in the following sen-
tence. Hence, the frame has been identified. 
o????i        ???  
(okhanei)RB (bas)NN         
????        ??? 
(karte)VM  (habe)VAUX  
 
80
The NNPC (Compound proper noun), NNP 
(Proper noun), NNC (Compound common noun) 
and NN (Common noun) POS tags help to de-
termine the subjects, objects as well as the loca-
tive information related to the verb. In simple 
sentences the occurrence of these POS tags pre-
ceded by the PRP (Pronoun) or NNPC tags and 
followed by the verb gives similar frame syntax 
for ?Basic Transitive? frame of the VerbNet. 
Only the components like subject, object and a 
single verb in Bengali as well as in English sen-
tence can be signified as simple ?Basic Transi-
tive? frame. 
 ??            ???         
(se)PRP   NP((rakam)NN   
 ????i???       ???      ???       
(designer)NN) (kaj)NN (kare)VM 
 
The following example shows that the frame 
identified from the sentence is also a ?transitive 
frame? and the secondary frame component is a 
?material object? for that sentence. 
   e??       ??????                                 
The set of acquired subcategorization frames or 
the frame lexicon can be evaluated against a gold 
standard corpus obtained either through manual 
analysis of corpus data or from subcategorization 
frame entries in a large dictionary or from the 
output of the parser made for that language. As 
there is no parser available for the Bengali and 
also no existing dictionary for Bengali that con-
tains subcategorization frames, manual analysis 
from corpus data is the only method for evalua-
tion. The chunked sentences that contain the ten 
most frequent verbs have been evaluated manu-
ally to prepare the gold standard data.  
(ekti)QC (bagaze)NNP 
     ??g?          ????? 
 VGNF((sangroho)NN (korlam)VM) 
 
The PREP (postposition) followed by a NP 
phrase and the target verb gives similar syntax 
for a NP-PP frame but it has been noticed that 
the secondary frame here can be a component of 
?Location-PP?. 
????        ????                                          
We have identified 45 different kinds of verbs 
in the corpus. A detailed statistics of the verb 
????? (kara) is presented in Table 3. During the 
Bengali verb subcategorization frame acquisition 
process, it has been observed that the simple sen-
tences contain most of the frames that the Eng-
lish verb form usually takes in VerbNet. Analysis 
of a simple Bengali sentence to identify the verb 
subcategorization frames is easier in the absence 
of a parser than analyzing complex and com-
pound sentences. There are only three occur-
rences of ????? (kara) as auxiliary in the corpus. 
These are chunking errors as the verb ????? 
(kara) does not occur as auxiliary verb.  
(setu)NNP (theke)PREP 
    ????      u?d? 
NP((nana)JJ (udvid)NN)) 
  p????         ?????k?            
  (projati)JJ (porjobekkhon)NN 
????? 
(korlam)VM  
 
The sentences where the determiner (DEM) 
and a NP chunk follow the target verb the se-
quence (Target verb DEM NP) is considered as 
the frame of sentential complement "S" for that 
target verb. 
 ???        ?????? 
(Ram)NNP (chitkar)(NN)           
???           ??    ?? 
(korlo)VM(je)(DEM) (se)(PRP) 
 ??         ???o 
(ar)CC      (kokhono)NN 
????      ?? 
(asbe)VM (na)NEG  
 
The presence of JJ (Adjective) generally does 
not play any role in the acquisition process of 
verb subcategorization frames. There are some 
frames that did not have any instance in our cor-
pus. Such frames are ?Asset-PP?, ?After-PP?, 
?Location Subject Alternation? and ?NP-TO-
INF-VC? etc. A close linguistic analysis shows 
that these frames can also be acquired from the 
Bengali sentences. They have not occurred in the 
corpus that has been considered for the analysis 
in the present work. 
4 Evaluation 
The verb subcategorization frames acquisition 
process is evaluated using type precision (the 
percentage of subcategorization frame types that 
the system proposes are correct according to the 
gold standard), type recall (the percentage of 
subcategorization frame types in the gold stan-
dard that the system proposes) and F-measure:   
          
 
 
81
The system has been evaluated with 100 gold 
standard test sentences containing ten most fre-
quent verbs and the evaluation results are shown 
in Table 4. The recall of the system shows a sat-
isfactory performance in producing Bengali verb 
subcategorization frames but the precision value 
requires more improvement.   
 
  Information Freq. 
Number of sentences in the corpus      1500 
Number of different verbs in the 
corpus 
45 
Number of inflected forms of the 
verb ????? in the corpus 
49 
Total number of occurrences of the 
verb ????? (before stemming ) in the 
corpus  
241  
Total number of occurrences of the 
verb ????? (after stemming) in the 
corpus  
234 
Number of sentences where ????? 
occurs as a  Main Verb (VM) 
206 
Number of sentences where ????? 
occurs as a Simple Main Verb 
(SVM) 
2 
Number of sentences where ????? 
occurs as a Compound Main Verb 
(CVM) 
204 
Number of sentences where ????? 
occurs as a Passive Verb 
(??????)(done) 
25 
Number of sentences where ????? 
occurs as a  Auxiliary Verb (VAUX) 
3 
Number of simple sentences where 
????? occurs as a Simple Main Verb 
(SVM) 
0 
Number of simple sentences where 
?????  occurs as a Compound Main 
Verb (CVM) 
127 
Table 3. The frequency information of the verb 
????? (kara) acquired from the corpus 
 
Measures Results 
Recall 88.23% 
Precision 71.11% 
F-Measure 79.24 
Table 4. The Precision, Recall and F-Measure 
values of the system 
 
It has been noticed that the absence of other 
frames in the Bengali corpus is due to the free 
phrase ordering characteristics of Bengali Lan-
guage. The proper alignment of the phrases is 
needed to cope up with this language specific 
problem. The number of different frames ac-
quired for these ten verbs is shown in Table 5.  
 
Bengali Verbs Subcategory 
Frames 
No. of 
Frames  
???? ??? 
(toiri kora)  
NP-PP 
NP-NP 
15 
3 
?????? ??? 
(babohar kora) 
NP-ADVP 
NP-PP 
NP-ING-SC 
NP-PP 
Location-PP 
1 
2 
1 
1 
1 
??? ??? (bas 
kora) 
Basic  
Transitive 
PP 
ADVP-
PRED 
 
12 
1 
1 
??? ??? (kaj 
kora) 
PP 
NP-PP 
1 
11 
??g? ??? 
(sangroho 
kora) 
Transitive  
(Material 
obj) 
PP 
1 
 
 
2 
?n ??? 
(bondho kora) 
Basic 
Transitive 
NP-PP 
 
1 
1 
???????   ??? 
(chitkar kora) 
S 
PP 
1 
1 
??? ??? (bhul 
kora) 
Nil 0 
??j??? ??? 
(jigyasa kora) 
BT 1 
?????k? ??? 
(porjobekkhon 
kora) 
Transitive 
(Location-
PP) 
NP-PP 
1 
 
 
1 
Table 5. The frequencies of different frames ac-
quired from corpus 
5 Conclusion 
The acquisition of subcategorization frames for 
more number of verbs and clustering them will 
help us to build a verb lexicon for Bengali lan-
guage. We need to find out Bengali verb sub-
categorization frames that may not be supported 
for the corresponding English verb with identical 
sense. 
82
There is no restriction for domain dependency 
in this system. For the free-phrase-order lan-
guages like Bengali, the overall performance can 
be increased by proper assumptions, rules and 
implementation procedures. Verb morphological 
information, synonymous sets and their possible 
subcategorization frames are all important in-
formation to develop a full-fledged parser for 
Bengali. This system can be used for solving 
alignment problems in Machine Translation for 
Bengali as well as to identify possible argument 
selection for Question and Answering systems.  
References  
Anna Korhonen. 2002. Semantically motivated sub-
categorization acquisition. ACL Workshop on 
Unsupervised Lexical Acquisition. Philadelphia. 
Anoop Sarkar and Daniel Zeman. 2000. Automatic 
extraction of subcategorization frames for czech. 
COLING-2000.
A. Ekbal and S. Bandyopadhyay. 2008. A Web-based 
Bengali News Corpus for Named Entity Recogni-
tion. LRE Journal. Springer. 
A.Ekbal, R. Haque and S. Bandyopadhyay. 2008. 
Maximum Entropy Based Bengali Part of Speech 
Tagging. RCS Journal, (33): 67-78.  
Akira Ushioda, David A. Evans, Ted Gibson, Alex 
Waibel. 1993. The Automatic Acquisition of Fre-
quencies of Verb Subcategorization Frames from 
Tagged Corpora. Workshop on Acquisition of 
Lexical Knowledge from Text, 95-106. Colum-
bus, Ohio.  
B. K. Boguraev and E. J. Briscoe.1987. Large lexi-
cons for natural language processing utilising the 
grammar coding system of the Longman Diction-
ary of Contemporary English. Computational 
Linguistics, 13(4): 219-240.  
Christopher D. Manning. 1993. Automatic Acquisi-
tion of a Large Subcategorization Dictionary from 
Corpora. 31st Meeting of the ACL, 235-242. Co-
lumbus, Ohio. 
Collin F. Baker, Charles J. Fillmore, and John B. 
Lowe.1998. The Berkeley FrameNet project. 
COLING/ACL-98, 86-90. Montreal. 
Copestake A.1992. The ACQUILEX LKB: Represen-
tation Issues in the Semi-automatic Acquisition of 
Large Lexicons. ANLP. Trento, Italy. 
D.Das, A.Ekbal, and S.Bandyopadhyay. 2009. Ac-
quiring Verb Subcategorization Frames in Bengali 
from Corpora. ICCPOL-09, LNAI-5459, 386-
393.Hong Kong.  
 Dan Gusfield. 1997. Algorithms on Strings, Trees 
and Sequences. Cambridge University Press, 
Cambridge, UK. 
Diana McCarthy. 2001. Lexical Acquisition at the 
Syntax-Semantics Interface: Diathesis Alter-
nations, Subcategorization Frames and Selec-
tional Preferences. University of Sussex.  
Grishman, R., Macleod, C., and Meyers, A. 1994. 
Comlex syntax : building a computational lexicon. 
COLING-94, 268-272. Kyoto, Japan.   
George A. Miller. 1990. WordNet: An on-line lexical 
database. International Journal of Lexicogra-
phy, 3(4):235-312.  
Glenn Carroll, Mats Rooth. 1998. Valence induction 
with a head-lexicalized PCFG. EMNLP. Granada.  
Karin Kipper-Schuler.2005. VerbNet: A broad-
coverage, comprehensive verb lexicon. Ph.D. 
thesis, Computer and Information Science Dept., 
University of Pennsylvania, Philadelphia, PA. 
Kazunori Muraki, Shin'ichiro Kamei, Shinichi 
Doi.1997. A Left-to-right Breadth-first Algo-
rithm for. Subcategorization Frame Selection 
of Japanese Verbs. TMI.  
Levin, B. 1993. English Verb Classes and Alterna-
tion: A Preliminary Investigation. The Univer-
sity of  Chicago Press. 
Michael Brent.1991. Automatic acquisition of sub-
categorization frames from untagged text.   29th 
Meeting of the ACL, 209-214. California.  
Maria Lapata, Chris Brew.1999. Using subcategoriza-
tion to resolve verb class ambiguity. 
WVLC/EMNLP, 266-274.  
Suzanne Stevenson, Paola Merlo. 1999. Automatic 
Verb Classification using Distributions of Gram-
matical Features. EACL-99, 45-52. Norge.  
Sabine Schulte im Walde. 1998. Automatic Seman-
tic Classification of Verbs According to Their 
Alternation Behavior. Master's thesis,  Stuttgart. 
S.D. Samantaray.2007. A Data mining approach for 
resolving cases of Multiple Parsing in Machine 
Aided Translation of Indian Languages. ITNG'07 
? IEEE. 
Ted Briscoe, John Carroll.1997. Automatic Extraction 
of Subcategorization from Corpora. ANLP-ACL, 
356-363.  Washington, D.C. 
XTAG Research Group. 2001. A lexicalized tree ad-
joining grammar for English. IRCS. University of 
Pennsylvania. 
83
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 80?83,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
English to Hindi Machine Transliteration System at NEWS 2009 
 
Amitava Das, Asif Ekbal, Tapabrata Mandal and Sivaji Bandyopadhyay 
Computer Science and Engineering Department 
Jadavpur University, Kolkata-700032, India 
amitava.research@gmail.com, asif.ekbal@gmail.com, ta-
pabratamondal@gmail.com, sivaji_cse_ju@yahoo.com 
 
 
Abstract 
 
This paper reports about our work in the 
NEWS 2009 Machine Transliteration Shared 
Task held as part of ACL-IJCNLP 2009. We 
submitted one standard run and two non-
standard runs for English to Hindi translitera-
tion. The modified joint source-channel model 
has been used along with a number of alterna-
tives. The system has been trained on the 
NEWS 2009 Machine Transliteration Shared 
Task datasets. For standard run, the system 
demonstrated an accuracy of 0.471 and the 
mean F-Score of 0.861. The non-standard runs 
yielded the accuracy and mean F-scores of 
0.389 and 0.831 respectively in the first one 
and 0.384 and 0.828 respectively in the second 
one. The non-standard runs resulted in sub-
stantially worse performance than the standard 
run. The reasons for this are the ranking algo-
rithm used for the output and the types of to-
kens present in the test set. 
1 Introduction 
Technical terms and named entities (NEs) consti-
tute the bulk of the Out Of Vocabulary (OOV) 
words. Named entities are usually not found in 
bilingual dictionaries and are very generative in 
nature. Proper identification, classification and 
translation of Named entities (NEs) are very im-
portant in many Natural Language Processing 
(NLP) applications. Translation of NEs involves 
both translation and transliteration. Translitera-
tion is the method of translating into another lan-
guage by expressing the original foreign word 
using characters of the target language preserv-
ing the pronunciation in their source language. 
Thus, the central problem in transliteration is 
predicting the pronunciation of the original word. 
Transliteration between two languages that use 
the same set of alphabets is trivial: the word is 
left as it is. However, for languages those use 
different alphabet sets the names must be transli-
terated or rendered in the target language alpha-
bets. Transliteration of NEs is necessary in many 
applications, such as machine translation, corpus 
alignment, cross-language Information Retrieval, 
information extraction and automatic lexicon 
acquisition. In the literature, a number of transli-
teration algorithms are available involving Eng-
lish (Li et al, 2004; Vigra and Khudanpur, 2003; 
Goto et al, 2003), European languages (Marino 
et al, 2005) and some of the Asian languages, 
namely Chinese (Li et al, 2004; Vigra and Khu-
danpur, 2003), Japanese (Goto et al, 2003; 
Knight and Graehl, 1998), Korean (Jung et al, 
2000) and Arabic (Al-Onaizan and Knight, 
2002a; Al-Onaizan and Knight, 2002c). Recent-
ly, some works have been initiated involving 
Indian languages (Ekbal et al, 2006; Ekbal et al, 
2007; Surana and Singh, 2008). 
 
2 Machine Transliteration Systems  
Three transliteration models have been used that 
can generate the Hindi transliteration from an 
English named entity (NE). An English NE is 
divided into Transliteration Units (TUs) with 
patterns C*V*, where C represents a consonant 
and V represents a vowel. The Hindi NE is di-
vided into TUs with patterns C+M?, where C 
represents a consonant or a vowel or a conjunct 
and M represents the vowel modifier or matra. 
The TUs are the lexical units for machine transli-
teration. The system considers the English and 
Hindi contextual information in the form of col-
located TUs simultaneously to calculate the plau-
sibility of transliteration from each English TU 
to various Hindi candidate TUs and chooses the 
one with maximum probability. This is equiva-
lent to choosing the most appropriate sense of a 
word in the source language to identify its repre-
sentation in the target language. The system 
learns the mappings automatically from the bi-
lingual NEWS training set being guided by lin-
80
guistic features/knowledge. The system consid-
ers the linguistic knowledge in the form of con-
juncts and/or diphthongs in English and their 
possible transliteration in Hindi. The output of 
the mapping process is a decision-list classifier 
with collocated TUs in the source language and 
their equivalent TUs in collocation in the target 
language along with the probability of each deci-
sion obtained from the training set. Linguistic 
knowledge is used in order to make the number 
of TUs in both the source and target sides equal. 
A Direct example base has been maintained that 
contains the bilingual training examples that do 
not result in the equal number of TUs in both the 
source and target sides during alignment. The 
Direct example base is checked first during ma-
chine transliteration of the input English word. If 
no match is obtained, the system uses direct or-
thographic mapping by identifying the equivalent 
Hindi TU for each English TU in the input and 
then placing the Hindi TUs in order. The transli-
teration models are described below in which S 
and T denotes the source and the target words 
respectively: 
 
? Model A 
This is essentially the joint source-channel model 
(Hazhiou et al, 2004) where the previous TUs 
with reference to the current TUs in both the 
source (s) and the target sides (t) are considered 
as the context.  
1
1
( | ) ( , | , )k k
k
K
P S T P s t s t
?
=
= < > < >?  
( ) arg max { ( ) ( | )}S T S P T P S TT? = ?  
? Model B 
This is basically the trigram model where the 
previous and the next source TUs are considered 
as the context.  
 1, 1
1
( | ) ( , | )k k k
k
K
P S T P s t s s
? +
=
= < >?  
  ( ) arg max { ( ) ( | )}S T S P T P S TT? = ?  
? Model C 
In this model, the previous and the next TUs in 
the source and the previous target TU are 
considered as the context. This is the  improved 
modified joint source-channel model. 
1, 1
1
( | ) ( , | , )k k k
k
K
P S T P s t s t s
? +
=
= < > < >?   
 ( ) arg max { ( ) ( | )}S T S P T P S TT? = ?               
For NE transliteration, P(T), i.e., the 
probability of transliteration in the target 
language, is calculated from a English-Hindi 
bilingual database of approximately 961,890 
English person names, collected from the web1.  
If, T is not found in the dictionary, then a very 
small value is assigned to P(T). These models 
have been desribed in details in Ekbal et al 
(2007). 
 
? Post-Processing 
Depending upon the nature of errors involved in 
the results, we have devised a set of translitera-
tion rules. A few rules have been devised to pro-
duce more spelling variations. Some examples 
are given below. 
Spelling variation rules 
Badlapur ??????? | ??????? 
Shree | Shri ? 
 
3 Experimental Results   
We have trained our transliteration models using 
the English-Hindi datasets obtained from the 
NEWS 2009 Machine Transliteration Shared 
Task (Li et al, 2009). A brief statistics of the 
datasets are presented in Table 1. Out of 9975 
English-Hindi parallel examples in the training 
set, 4009 are multi-words. During training, we 
have split these multi-words into collections of 
single word transliterations. It was observed that 
the number of tokens in the source and target 
sides mismatched in 22 multi-words and these 
cases were not considered further. Following are 
some examples:  
Paris Charles de Gaulle ????  
???? ??? ? ?????  
South Arlington Church of 
Christ ???? ???? 
In the training set, some multi-words were partly 
translated and not transliterated. Such examples 
were dropped from the training set. Finally, the 
training set consists of 15905 single word Eng-
lish-Hindi parallel examples.  
                                                 
1http://www.eci.gov.in/DevForum/Fullname.asp  
81
      
Set Number of examples 
Training 9975 
Development 974 
Test 1000 
Table 1. Statistics of Dataset 
 
The output of the modified joint source-
channel model is given more priority during out-
put ranking followed by the trigram and the joint 
source-channel model. During testing, the Direct 
example base is searched first to find the transli-
teration. Experimental results on the develop-
ment set yielded the accuracy of 0.442 and mean 
F-score of 0.829. Depending upon the nature of 
errors involved in the results, we have devised a 
set of transliteration rules. The use of these trans-
literation rules increased the accuracy and mean 
F-score values up to 0.489 and 0.881 respective-
ly.  
The system has been evaluated for the test set 
and the detailed reports are available in Li et al 
(2009). There are 88.88% unknown examples in 
the test set. We submitted one standard run in 
which the outputs are provided for the modified 
joint source-channel model (Model C), trigram 
model (Model B) and joint source-channel model 
(Model A). The same ranking procedure (i.e., 
Model C, Model B and Model A) has been fol-
lowed as that of the development set. The output 
of each transliteration model has been post-
processed with the set of transliteration rules. For 
each word, three different outputs are provided in 
a ranked order. If the outputs of any two models 
are same for any word then only two outputs are 
provided for that particular word. Post-
processing rules generate more number of possi-
ble transliteration output. Evaluation results of 
the standard run are shown in Table 2.  
 
Parameters Accuracy 
Accuracy in top-1 0.471 
Mean F-score 0.861 
Mean Reciprocal Rank 
(MRR) 
0.519 
Mean Average Preci-
sion (MAP)ref 
0.463 
MAP10 0.162 
MAPsys 0.383 
Table 2. Results of the standard run  
 
The results of the two non-standard runs are 
presented in Table 3 and Table 4 respectively.  
Parameters Accuracy 
Accuracy in top-1 0.389 
Mean F-score 0.831 
Mean Reciprocal Rank 
(MRR) 
0.487 
Mean Average Preci-
sion (MAP)ref 
0.385 
MAP10 0.16 
MAPsys 0.328 
  
Table 3. Results of the non-standard run 1 
 
Parameters Accuracy 
Accuracy in top-1 0.384 
Mean F-score 0.823 
Mean Reciprocal Rank 
(MRR) 
0.485 
Mean Average Precision 
(MAP)ref 
0.380 
MAP10 0.16 
MAPsys 0.325 
 
Table 4. Results of the non-standard run2 
 
In both the non-standard runs, we have used 
an English-Hindi bilingual database of approx-
imately 961, 890 examples that have been col-
lected from the web2. This database contains the 
(frequency) of the corresponding English-Hindi 
name pair. Along with the outputs of three mod-
els, the output obtained from this bilingual data-
base has been also provided for each English 
word. In the first non-standard run, only the most 
frequent transliteration has been considered. But, 
in the second non-standard run all the possible 
transliteration have been considered. It is to be 
noted that in these two non-standard runs, the 
transliterations obtained from the bilingual data-
base have been kept first in the ranking. Results 
of the tables show quite similar performance in 
both the runs. But the non-standard runs resulted 
in substantially worse performance than the stan-
dard run. The reasons for this are the ranking 
algorithm used for the output and the types of 
tokens present in the test set. The additional da-
                                                 
2http://www.eci.gov.in/DevForum/Fullname.asp  
82
taset used for the non-standard runs is mainly 
census data consisting of only Indian person 
names. The NEWS 2009 Machine Transliteration 
Shared Task training set is well distributed with 
foreign names (Ex. Sweden, Warren), common 
nouns (Mahfuz, Darshanaa) and a few non 
named entities. Hence the training set for the 
non-standard runs was biased towards the Indian 
person name transliteration pattern. Additional 
training set was quite larger (961, 890) than the 
shared task training set (9,975). Actually outputs 
of non-standard runs have more alternative trans-
literation outputs than the standard set. That 
means non-standard sets are superset of standard 
set. Our observation is that the ranking algorithm 
used for the output and biased training are the 
main reasons for the worse performance of the 
non-standard runs. 
4 Conclusion  
This paper reports about our works as part of the 
NEWS 2009 Machine Transliteration Shared 
Task. We have used the modified joint source-
channel model along with two other alternatives 
to generate the Hindi transliteration from an Eng-
lish word (to generate more spelling variations of 
Hindi names). We have also devised some post-
processing rules to remove the errors. During 
standard run, we have obtained the word accura-
cy of 0.471 and mean F-score of 0.831. In non-
standard rune, we have used a bilingual database 
obtained from the web. The non-standard runs 
yielded the word accuracy and mean F-score 
values of 0.389 and 0.831 respectively in the first 
run and 0.384 and 0.823 respectively in the 
second run. 
 
References  
Al-Onaizan, Y. and Knight, K. 2002a. Named 
Entity Translation: Extended Abstract. In 
Proceedings of the Human Language Tech-
nology Conference, 122? 124. 
Al-Onaizan, Y. and Knight, K. 2002b. Translat-
ing Named Entities using Monolingual and 
Bilingual Resources. In Proceedings of the 
40th Annual Meeting of the ACL, 400?408, 
USA. 
Ekbal, A. Naskar, S. and Bandyopadhyay, S. 
2007. Named Entity Transliteration. Interna-
tional Journal of Computer Processing of 
Oriental Languages (IJCPOL), Volume 
(20:4), 289-310, World Scientific Publishing 
Company, Singapore. 
Ekbal, A., Naskar, S. and Bandyopadhyay, S. 
2006. A Modified Joint Source Channel 
Model for Transliteration. In Proceedings of 
the COLING-ACL 2006, 191-198, Australia. 
Goto, I., Kato, N., Uratani, N. and Ehara, T. 
2003. Transliteration Considering Context 
Information based on the Maximum Entropy 
Method. In Proceeding of the MT-Summit 
IX, 125?132, New Orleans, USA.  
Jung, Sung Young , Sung Lim Hong and Eunok 
Paek. 2000. An English to Korean Translite-
ration Model of Extended Markov Window. 
In Proceedings of International Conference 
on Computational Linguistics (COLING 
2000), 383-389. 
Knight, K. and Graehl, J. 1998. Machine Transli-
teration, Computational Linguistics, Volume 
(24:4), 599?612. 
Kumaran, A. and Tobias Kellner. 2007. A gener-
ic framework for machine transliteration. In 
Proc. of the 30th SIGIR. 
Li, Haizhou, A Kumaran, Min Zhang and Vla-
dimir Pervouchine. 2009. Whitepaper of 
NEWS 2009 Machine Transliteration Shared 
Task. In Proceedings of ACL-IJCNLP 2009 
Named Entities Workshop (NEWS 2009), Sin-
gapore. 
Li, Haizhou, A Kumaran, Vladimir Pervouchine 
and Min Zhang. 2009.  Report on NEWS 2009 
Machine Transliteration Shared Task. In Pro-
ceedings of ACL-IJCNLP 2009  amed Entities 
Workshop (NEWS 2009), Singapore. 
Li, Haizhou, Min Zhang and Su Jian. 2004. A 
Joint Source-Channel Model for Machine 
Transliteration. In Proceedings of the 42nd 
Annual Meeting of the ACL, 159-166. Spain. 
Marino, J. B., R. Banchs, J. M. Crego, A. de 
Gispert, P. Lambert, J. A. Fonollosa and M. 
Ruiz. 2005.  Bilingual n-gram Statistical 
Machine Translation. In Proceedings of the 
MT-Summit X, 275?282. 
Surana, Harshit, and Singh, Anil Kumar. 2008. A 
More Discerning and Adaptable Multilingual 
Transliteration Mechanism for Indian Lan-
guages. In Proceedings of the 3rd Interna-
tional Joint Conference on Natural Lan-
guage Processing (IJCNLP-08), 64-71, In-
dia. 
Vigra, Paola and Khudanpur, S. 2003. Translite-
ration of Proper Names in Cross-Lingual In-
formation Retrieval. In Proceedings of the 
ACL 2003 Workshop on Multilingual and 
Mixed-Language Named Entity Recognition, 
57?60. 
83
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 202?210,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
Voted NER System using Appropriate Unlabeled Data 
 
Asif Ekbal 
Dept. of Computer Science &Engg., 
Jadavpur University, Kolkata-700032, 
India 
asif.ekbal@gmail.com 
Sivaji Bandyopadhyay 
Dept. of Computer Science &Engg., 
Jadavpur University, Kolkata-700032, 
India 
sivaji_cse_ju@yahoo.com 
 
Abstract 
 
This paper reports a voted Named Entity Rec-
ognition (NER) system with the use of appro-
priate unlabeled data. The proposed method is 
based on the classifiers such as Maximum En-
tropy (ME), Conditional Random Field (CRF) 
and Support Vector Machine (SVM) and has 
been tested for Bengali. The system makes use 
of the language independent features in the 
form of different contextual and orthographic 
word level features along with the language 
dependent features extracted from the Part of 
Speech (POS) tagger and gazetteers. Context 
patterns generated from the unlabeled data us-
ing an active learning method have been used 
as the features in each of the classifiers. A 
semi-supervised method has been used to de-
scribe the measures to automatically select ef-
fective documents and sentences from unla-
beled data. Finally, the models have been 
combined together into a final system by 
weighted voting technique. Experimental re-
sults show the effectiveness of the proposed 
approach with the overall Recall, Precision, 
and F-Score values of 93.81%, 92.18% and 
92.98%, respectively. We have shown how the 
language dependent features can improve the 
system performance. 
1 Introduction 
Named Entity Recognition (NER) is an impor-
tant tool in almost all Natural Language Process-
ing (NLP) application areas. Machine learning 
(ML) approaches are more popularly used in 
NER because these are easily trainable, adopt-
able to different domains and languages as well 
as their maintenance are also less expensive. 
Some of the very effective ML approaches used 
in NER are ME (Borthwick, 1999), CRF 
(Lafferty et al, 2001) and SVM (Yamada et al, 
2002). In the earlier work (Florian et al, 2003), it 
has been shown that combination of several ML 
models yields better performance than any single 
ML model. One drawback of the ML techniques 
to NLP tasks is the requirement of a large 
amount of annotated data to achieve a reasonable 
performance. 
Indian languages are resource-constrained and 
the manual preparation of NE annotated data is 
both time consuming and cost intensive. It is im-
portant to decide how the system should effec-
tively select unlabeled data and how the size and 
relevance of data impact the performance. India 
is a multilingual country with great cultural di-
versities. Named Entity (NE) identification in 
Indian languages in general and Bengali in par-
ticular is difficult and challenging as: 
1. Unlike English and most of the European 
languages, Bengali lacks capitalization infor-
mation, which plays a very important role in 
identifying NEs. 
2. Indian person names are generally found in 
the dictionary as common nouns with some 
specific meanings. For example, kabitA 
[Kabita] is a person name and can also be 
found in the dictionary as a common noun with 
the meaning ?poem?. 
3.  Bengali is an inflectional language provid-
ing one of the richest and most challenging sets 
of linguistic and statistical features resulting in 
long and complex wordforms. For example, the 
person name sachin [root] can appear as sa-
chiner [inflection:-er], sachInke [inflection:-
ke], sachInbAbu [inflection: -bAbu], sachIndA 
[ inflection:-dA] etc. The location name kol-
kAtA [root] can appear in different wordforms 
like kolkAtAr  [inflection:-r], kolkAtAte [inflec-
tion:-te], kolkAtAi  [inflection:-i] etc. 
4. Bengali is a relatively free phrase order lan-
guage. Thus, NEs can appear in any position of 
the sentence making the NER task more diffi-
cult.   
5. Bengali, like other Indian languages, is a re-
source-constrained language. The annotated 
corpus, name dictionaries, good morphological 
202
analyzers, POS taggers etc. are not yet avail-
able in the required measure. 
6. Although Indian languages have a very old 
and rich literary history, technological devel-
opments are of recent origin. 
7. Web sources for name lists are available in 
English, but such lists are not available in Ben-
gali. This necessitates the use of transliteration 
for creating such lists. 
A HMM based NER system for Bengali has 
been reported in Ekbal et al (2007b), where ad-
ditional contextual information has been consid-
ered during emission probabilities and NE suf-
fixes are used for handling the unknown words. 
More recently, the works in the area of Bengali 
NER can be found in Ekbal et al (2008a), and 
Ekbal and Bandyopadhyay (2008b) with the CRF, 
and SVM approach, respectively. Other than 
Bengali, the works on Hindi can be found in Li 
and McCallum (2004) with CRF and Saha et al 
(2008) with a hybrid feature set based ME ap-
proach. Various works of NER involving Indian 
languages are reported in IJCNLP-08 NER 
Shared Task on South and South East Asian 
Languages (NERSSEAL) 1  using various tech-
niques. 
2 Named Entity Recognition in Bengali  
We have used a Bengali news corpus (Ekbal and 
Bandyopadhyay, 2008c), developed from the 
web-archive of a widely read Bengali newspaper 
for NER. A portion of this corpus containing 
200K wordforms has been manually annotated 
with the four NE tags namely, Person, Location, 
Organization and Miscellaneous. We have also 
used the NE annotated data of 122K wordforms, 
collected from the NERSSEAL shared task. The 
shared task data was originally annotated with a 
fine-grained NE tagset of twelve tags. We con-
sider only those tags that represent person, loca-
tion, organization, and miscellaneous names 
(NEN [number], NEM [Measurement] and NETI 
[Time]). Other tags have been mapped to the 
NNE tags that represent the ?other-than-NE? 
category. In order to properly denote the bounda-
ries of NEs, four NE tags are further divided into 
the following forms:  
 B-XXX: Beginning of a multiword NE, I-
XXX: Internal of a multiword NE consisting of 
more than two words, E-XXX: End of a multi-
word NE, XXX?PER/LOC/ORG/MISC. For 
example, the name sachin ramesh tendulkar is 
                                                 
1 http://ltrc.iiit.ac.in/ner-ssea-08/proc/index.html 
tagged as sachin/B-PER ramesh/I-PER tendul-
kar/E-PER. The single word NE is tagged as, 
PER: Person name, LOC: Location name, ORG: 
Organization name and MISC: Miscellaneous 
name. In the output, sixteen NE tags are replaced 
with the four NE tags. 
2.1 Our Approaches 
Initially, we started with the development of a 
NER system using an active learning method. 
This is used as the baseline model. Four super-
vised NER systems based on ME, CRF and SVM 
have been developed. Two different systems with 
the SVM model, one using forward parsing 
(SVM-F) that parses from left to right and other 
using backward parsing (SVM-B) that parses 
from right to left, have been developed. The 
SVM system has been developed based on 
(Valdimir, 1995), which perform classification 
by constructing an N-dimensional hyperplane 
that optimally separates data into two categories. 
We have used YamCha toolkit (http://chasen-
org/~taku/software/yamcha), an SVM based tool 
for detecting classes in documents and formulat-
ing the NER task as a sequential labeling prob-
lem. Here, the pairwise multi-class decision 
method and polynomial kernel function have 
been used. The TinySVM-0.02 classifier has been 
used for classification. The C++ based CRF++ 
package (http://crfpp.sourceforge.net) and the 
C++ based ME package 3 have been used for NER.  
Performance of the supervised NER models is 
limited in part by the amount of labeled training 
data available. A part of the available unlabeled 
corpus (Ekbal and Bandyopadhyay, 2008c) has 
been used to address this problem. Based on the 
original training on the labeled corpus, there will 
be some tags in the unlabeled corpus that the 
taggers will be very sure about. We have pro-
posed a semi-supervised learning technique that 
selects appropriate data from the available large 
unlabeled corpora and adds to the initial training 
set in order to improve the performance of the 
taggers. The models are retrained with this new 
training set and this process is repeated in a boot-
strapped manner. 
2.2 Named Entity Features 
The main features for the NER task have been 
identified based on the different possible combi-
nations of available word and tag contexts. In 
                                                 
2http://cl.aist-nara.ac.jp/~taku ku/software/TinySVM  
3http://homepages.inf.ed.ac.uk/s0450736/software/ma
xent/maxent-20061005.tar.bz2 
203
addition to these, various gazetteer lists have 
been developed for use in the NER tasks.  
The set of features ?F? contains language inde-
pendent as well as language dependent features. 
The set of language independent features in-
cludes the context words, fixed length prefixes 
and suffixes of all the words, dynamic NE infor-
mation of the previous word(s), first word, length 
of the word, digit and infrequent word informa-
tion. Language dependent features include the set 
of known suffixes that may appear with the vari-
ous NEs, clue words that help in predicting the 
location and organization names, words that help 
to recognize measurement expressions, designa-
tion words that help to identify person names, 
various gazetteer lists that include the first 
names, middle names, last names, location 
names, organization names, function words, 
weekdays and month names. We have also used 
the part of speech (POS) information of the cur-
rent and/or the surrounding word(s) as the fea-
tures. 
Language independent NE features can be ap-
plied for NER in any language without any prior 
knowledge of that language. The lists or gazet-
teers are basically language dependent at the 
lexical level and not at the morphology or syntax 
level. Also, we include the POS information in 
the set of language dependent features as the 
POS information depends on some language spe-
cific phenomenon such as person, number, tense, 
gender etc. Also, the particular POS tagger, used 
in this work, makes use of the several language 
specific resources such as lexicon, inflection lists 
and a NER system to improve its performance. 
Evaluation results have demonstrated that the use 
of language specific features is helpful to im-
prove the performance of the NER system. In the 
resource-constrained Indian language environ-
ment, the non-availability of language specific 
resources acts as a stimulant for the development 
of such resources for use in NER systems. This 
leads to the necessity of apriori knowledge of the 
language. The features are described below very 
briefly. 
  ?Context words: Such words include the pre-
ceding and succeeding words of the current 
word. This is based on the observation that the 
surrounding words carry effective information 
for the identification of NEs. 
?Word suffix and prefix: Fixed length word 
suffixes and prefixes are helpful to identify NEs. 
In addition, variable length word suffixes are 
also used. Word suffixes and prefixes are the ef-
fective features and work well for the inflective 
Indian languages like Bengali. 
?Named Entity Information: This is the only 
dynamic feature in the experiment.  The previous 
word NE tag is very informative in deciding the 
current word NE tag. 
?First word (binary valued): This feature 
checks whether the current token is the first word 
of the sentence or not. Though Bengali is a rela-
tively free phrase order language, the first word 
of the sentence is most likely a NE as it appears 
most of the time in the subject position. 
?Length of the word (binary valued): This fea-
ture checks whether the length of the token is 
less than three or not. We have observed that 
very short words are most probably not the NEs.  
?Infrequent word (binary valued): A cut off 
frequency has been chosen in order to consider 
the infrequent words in the training corpus. This 
is based on the observation that the infrequent 
words are rarely NEs. 
?Digit features: Several digit features have 
been considered depending upon the presence 
and/or the number of digit(s) in a token. These 
binary valued features are helpful in recognizing 
miscellaneous NEs such as time, monetary and 
date expressions, percentages, numerical num-
bers etc.     
?Position of the word (binary valued):  Posi-
tion of the word (whether last word or not) in a 
sentence is a good indicator of NEs.  
?Part of Speech (POS) Information: We have 
used an SVM-based POS tagger (Ekbal and 
Bandyopadhyay, 2008d) that was originally de-
veloped with 26 POS tags, defined for the Indian 
languages. For SVM models, we have used this 
POS tagger. However, for the ME and CRF 
models, we have considered a coarse-grained 
POS tagger that has the following tags: Nominal, 
PREP (Postpositions) and Other.  
?Gazetteer Lists: Gazetteer lists, developed 
manually as well as semi-automatically from the 
news corpus (Ekbal and Bandyopadhyay, 2008c), 
have been used as the features in each of the 
classifiers. The set of gazetteers along with the 
number of entries are as follows: 
 (1). Organization clue word (e.g., ko.m [Co.], 
limited [Limited] etc): 94, Person prefix words 
(e.g., shrimAn [Mr.], shrImati [Mrs.] etc.): 145, 
Middle names: 2,491, Surnames: 5,288, NE suf-
fixes (e.g., -bAbu [-babu], -dA [-da], -di [-di] for 
person and  -lyAnd [-land] -pur[-pur],  -liyA [-lia] 
etc for location):115, Common location (e.g., 
sarani [Sarani], roDa [Road] etc.): 147, Action 
204
verb (e.g., balen [says], ballen [told] etc.):141, 
Function words:743, Designation words (e.g., 
netA[leader], sA.msad [MP] etc.): 139, First 
names:72,206, Location names:7,870, Organiza-
tion names:2,225, Month name (English and 
Bengali calendars):24, Weekdays (English and 
Bengali calendars):14 
 (2). Common word (521 entries): Most of the 
Indian language NEs appears in the dictionary 
with some meanings. For example, the word ka-
mol may be the name of a person but also ap-
pears in the dictionary with another meaning lo-
tus, the name of a flower; the word dhar may be 
a verb and also can be the part of a person name. 
We have manually created a list, containing the 
words that can be NEs as well as valid dictionary 
words.  
3  Active Learning Method for Baseline 
NER System  
We have used a portion, containing 35,143 news 
documents and approximately 10 million word-
forms, of the Bengali news corpus (Ekbal and 
Bandyopadhyay, 2008c) for developing the base-
line NER system. 
The frequently occurring words have been col-
lected from the reporter, location and agency 
tags of the Bengali news corpus. The unlabeled 
corpus is tagged with the elements from the seed 
lists. In addition, various gazetteers have been 
used that include surname, middle name, person 
prefix words, NE suffixes, common location and 
designations for further tagging of the NEs in the 
training corpus. The following linguistic rules 
have been used to tag the training corpus: 
  (i). If there are two or more words in a se-
quence that represent the characters of Bengali or 
English alphabet, then such words are part of 
NEs. For example, bi e (B A), ci em di e (C M D 
A), bi je pi (B J P) are all NEs. 
  (ii). If at the end of a word, there are strings like 
- era(-er),  -eraa (-eraa),  -ra (-ra), -rA (-raa), -ke 
(-ke), -dera (-der) then the word is likely to be a 
person name. 
  (iii). If a clue word like saranI (sarani), ro.Da 
(road), lena (lane) etc. is found after an unknown 
word then the unknown word along with the clue 
word may be a location name. 
  (iv). A few names or words in Bengali consist 
of the characters chandrabindu or khanda ta. So, 
if a particular word W is not identified as NE by 
any of the above rules but includes any of these 
two characters, then W may be a NE. For 
example o.NrI (onry) is a person name.  
  (v). The set of action verbs like balen (says), 
ballen (told), ballo (told), shunla (heared), 
ha.Nslo (haslo) etc. often determines the 
presence of person names. If an unknown word 
W appears in the sentence followed by the action 
verbs, then W is most likely a person name. 
Otherwise, W is not likely to be a NE. 
  (vi). If there is reduplication of a word W in a 
sentence then W is not likely to be a NE. This is 
so because rarely name words are reduplicated. 
In fact, reduplicated name words may signify 
something else. For example, rAm rAm (ram 
ram)  is used to greet a person. 
  (vii). If at the end of any word W there are 
suffixes like -gulo (-gulo), -guli (guli), -khAnA (-
khana) etc., then W is not a NE. 
For each tag T inserted in the training corpus, 
the algorithm generates a lexical pattern p using 
a context window of maximum width 6 (exclud-
ing the tagged NE) around the left and the right 
tags, e.g.,  
    p = [l-3l-2 l-1  <T> ...</T> l+1 l+2 l+3],  
 where, l?i   are the context of p. All these pat-
terns, derived from the different tags of the la-
beled and unlabeled training corpora, are stored 
in a Pattern Table (or, set P), which has four dif-
ferent fields namely, pattern id (identifies any 
particular pattern), pattern example (pattern), pat-
tern type (Person/Location/Organization) and 
relative frequency (indicates the number of times 
any pattern of a particular type appears in the 
entire training corpus relative to the total number 
of patterns generated of that type). This table has 
20,967 distinct entries.  
Every pattern p in the set P is matched against 
the same unlabeled corpus. In a place, where the 
context of p matches, p predicts the occurrence 
of the left or right boundary of name. POS in-
formation of the words as well as some linguistic 
rules and/or length of the entity have been used 
in detecting the other boundary. The extracted 
entity may fall in one of the following categories: 
? positive example: The extracted entity is 
of the same NE type as that of the pattern. 
? negative example: The extracted entity is 
of the different NE type as that of the pattern. 
? error example: The extracted entity is 
not at all a NE. 
The type of the extracted entity is determined 
by checking whether it appears in any of the seed 
lists; otherwise, its type is determined manually. 
The positive and negative examples are then 
added to the appropriate seed lists. The accuracy 
of the pattern is calculated as follows:  
205
     accuracy(p)= |positive (p)|/[| positive (p)| + 
|negative (p)| + |error(p)|] 
A threshold value of accuracy has been cho-
sen in order to discard the patterns below this 
threshold. A pattern is also discarded if its total 
positive count is less than a predetermined 
threshold value. The remaining patterns are 
ranked by their relative frequency values. The n 
top high frequent patterns are retained in the pat-
tern set P and this set is denoted as Accept Pat-
tern.  
All the positive and negative examples ex-
tracted by a pattern p can be used to generate 
further patterns from the same training corpus. 
Each new positive or negative instance (not ap-
pearing in the seed lists) is used to further tag the 
training corpus. We repeat the previous steps for 
each new NE until no new patterns can be gener-
ated. A newly generated pattern may be identical 
to a pattern that is already in the set P. In such a 
case, the type and relative frequency fields in the 
set P are updated accordingly. Otherwise, the 
newly generated pattern is added to the set with 
the type and relative frequency fields set prop-
erly. The algorithm terminates after 13 iterations 
and there are 20,176 distinct entries in the set P.   
 
4 Semi-supervised Approach for Unla-
beled Document and Sentence Selec-
tion 
A method for automatically selecting the appro-
priate unlabeled data from a large collection of 
unlabeled documents for NER has been de-
scribed in Ekbal and Bandyopadhyay (2008e). 
This work reported the selection of unlabeled 
documents based on the overall F-Score value of 
the individual system. In this work, the unlabeled 
documents have been selected based on the Re-
call, Precision as well as the F-Score values of 
the participating systems. Also, we have consid-
ered only the SVM-F model trained with the lan-
guage independent, language dependent and con-
text features for selecting the appropriate sen-
tences to be included into the initial training data. 
The use of single model makes the training faster 
compared to Ekbal and Bandyopadhyay (2008e). 
The SVM-F model has been considered as it 
produced the best results for the development set 
as well as during the 10-fold cross validation test. 
The unlabeled 35,143 news documents have been 
divided based on news sources/types in order to 
create segments of manageable size, separately 
evaluate the contribution of each segment using a 
gold standard development test set and reject 
those that are not helpful and to apply the latest 
updated best model to each subsequent segment. 
It has been observed that incorporation of unla-
beled data can only be effective if it is related to 
the target problem, i.e., the test set. Once the ap-
propriate documents are selected, it is necessary 
to select the tagged sentences that are useful to 
improve both the Recall and Precision values of 
the system. Appropriate sentences are selected 
using the SVM-F model depending upon the 
structure and/or contents of the sentences. 
4.1 Unlabeled Document Selection 
The unlabeled data supports the acquisition of 
new names and contexts to provide new evi-
dences to be incorporated in the models. Unla-
beled data can degrade rather than improve the 
classifier?s performance on the test set if it is ir-
relevant to the test document. So, it is necessary 
to measure the relevance of the unlabeled data to 
our target test set. We construct a set of key 
words from the test set T to check whether an 
unlabeled document d is useful or not.     
 
? We do not use all words in the test set T as 
the key words since we are only concerned 
about the distribution of name candidates. 
So, each document is tested with the CRF 
model using the language independent fea-
tures, language dependent features and the 
context features.  
? We take all the name candidates in the top N 
best hypotheses (N=10) for each sentence of 
the test set T to construct a query set Q. Us-
ing this query set, we find all the relevant 
documents that include three (heuristically 
set) names belonging to the set Q. In addi-
tion, the documents are not considered if 
they contain fewer than seven (heuristic) 
names.   
4.2 Sentence Selection 
All the tagged sentences of a relevant document 
are not added to training corpus as incorrectly 
tagged or irrelevant sentences can lead to the 
degradation in model performance. Our main 
concern is on how much new information is ex-
tracted from each sentence of the unlabeled data 
compared to the training corpus that already we 
have in our hand.  
The SVM-F model has been used to select the 
relevant sentences. All the relevant documents 
are tagged with the SVM-F model developed 
with the language independent, language de-
206
pendent and context features along with the class 
decomposition technique. If both Recall and Pre-
cision values of the SVM-F model increase then 
that sentence is selected to be added to the initial 
training corpus. A close investigation reveals the 
fact that this criterion often selects a number of 
sentences which are too short or do not include 
any name. These words may make the model 
worse if added to the training data. For example, 
the distribution of non-names may increase sig-
nificantly that may lead to degradation of model 
performance. In this experiment, we have not 
included the sentences that include fewer than 
five words or do not include any names. The 
bootstrapping procedure is given as follows: 
1. Select a relevant document RelatedD 
from a large corpus of unlabeled data 
with respect to the test set T using the 
document selection method described in 
Section 4.1. 
2. Split RelatedD into n subsets and mark 
them C1, C2, ?., Cn.    
3. Call the development set DevT. 
4. For I=1 to n 
4.1. Run SVM-F model, developed with the 
language independent features, language 
dependent feature and context features 
along with the class decomposition tech-
nique, on Ci. 
4.2. If the length of each tagged sentence S is 
less than five or it does not contain any 
name then discard S. 
4.3. Add Ci to the training data and retrain 
SVM-F model. This produces the up-
dated model. 
4.4. Run the updated model on DevT; if the 
Recall and Precision values reduce then 
don?t use Ci and use the old model. 
5. Repeat steps 1-4 until Recall and Precision 
values of the SVM-F model either become equal 
or differ by some threshold values (set to 0.01) in 
consecutive two iterations.  
5 Evaluation Results and Discussions 
Out of 200K wordforms, 150K wordforms along 
with the IJCNLP-08 shared task data has been 
used for training the models. Out of 200K word-
forms, 50K wordforms have been used as the 
development data. The system has been tested 
with a gold standard test set of 35K wordforms. 
Each of the models has been evaluated in two 
different ways, being guided by language inde-
pendent features (language independent system 
denoted as LI) and being guided by language 
independent as well as language dependent fea-
tures (language dependent system denoted as 
LD).  
5.1 Language Independent Evaluation 
A number of experiments have been carried out 
in order to identify the best-suited set of lan-
guage independent features for NER in each of 
models. Evaluation results of the development 
set for the NER models are presented in Table 1 
in terms of percentages of Recall (R), Precision 
(P) and F-Score (FS). The ME based system has 
demonstrated the F-Score value of 74.67% for 
the context word window of size three, i.e., pre-
vious one word, current word and the next word, 
prefixes and suffixes of length up to three char-
acters of only the current word, dynamic NE tag 
of the previous word, first word, infrequent word, 
length and the various digit features. The CRF 
based system yielded the highest F-Score value 
of 76.97% for context window of size five, i.e., 
two preceding, current and two succeeding words 
along with the other set of features as in the ME 
model. Both the SVM based systems have dem-
onstrated the best performance for the context 
window of size seven, i.e., three preceding, cur-
rent and two succeeding words, dynamic NE in-
formation of the previous two words along with 
the other set of features as in the ME and CRF 
based systems. In SVM models, we have con-
ducted experiments with the different polynomial 
kernel functions and observed the highest F-
Score value with degree 2. It has been also ob-
served that pairwise multiclass decision method 
performs better than the one vs rest method. For 
all the models, context words and prefixes and/or 
suffixes have been found to be the most effective 
features. 
 
Model R  P  FS  
ME 76.82 72.64 74.67 
CRF 78.17 75.81 76.97 
SVM-F 79.14 77.26 78.19 
SVM-B 79.09 77.15 78.11 
Table 1. Results on the development set for 
the language independent supervised models 
5.2 Language Dependent Evaluation 
Evaluation results of the systems that include the 
POS information and other language dependent 
features are presented in the Table 2. During the 
experiments, it has been observed that all the 
language dependent features are not equally im-
portant. POS information is the most effective 
207
followed by NE suffixes, person prefix words, 
designations, organization clue words and loca-
tion clue words. Table 1 and Table 2 show that 
the language dependent features can improve the 
overall performance of the systems significantly. 
 
Model R  P  FS  
ME 87.02 80.77 83.78 
CRF 87.63 84.03 85.79  
SVM-F 87.74 85.89 86.81  
SVM-B 87.69 85.17 86.72  
Table 2. Results on the development set for the 
language dependent supervised models 
 
5.3 Use of Context Features as Features 
Now, the high ranked patterns of the Accept Pat-
tern set (Section 3) can be used as the features of 
the individual classifier. A feature ?ContextInf? is 
defined by observing the three preceding and 
succeeding words of the current word. Evalua-
tion results are presented in Table 3. Clearly, it is 
evident from the results of Table 2 and Table 3 
that context features are very effective to im-
prove the Precision values in each of the models.  
 
Model R  P  FS  
ME 88.22 83.71 85.91 
CRF 89.51 85.94 87.69 
SVM-F 89.67 86.49 88.05 
SVM-B 89.61 86.47 88.01 
Table 3. Results on the development set by in-
cluding context features 
5.4 Results on the Test Set 
A gold standard test set of 35K wordforms has 
been used to report the evaluation results. The 
models have been trained with the language in-
dependent, language dependent and the context 
features. Results have been presented in Table 4 
for the test set. In the baseline model, each pat-
tern of the Accept Pattern set is matched against 
the test set. Results show that SVM-F model per-
forms best for the test set. 
Error analyses have been conducted with the 
help of confusion matrix. In order to improve the 
performance of the classifiers, we have used 
some post-processing techniques.  
Output of the ME based system has been post-
processed with a set of heuristics (Ekbal and 
Bandyopadhyay, 2009) to improve the perform-
ance further. The post-processing as described in 
Ekbal and Bandyopadhyay (2008e) tries to as-
sign the correct tag according to the n-best re-
sults for every sentence of the test set in the CRF 
framework. In order to remove the unbalanced 
class distribution between names and non-names 
in the training set, we have considered the class 
decomposition technique (Ekbal and Bandyop-
adhyay, 2008e) for SVM. Evaluation results of 
the post-processed systems are presented in Ta-
ble 5.  
 
 Model R  P  FS  
Baseline 68.11 71.37 69.32 
ME 86.04 84.98 85.51 
CRF 87.94 87.12 87.53 
SVM-F 89.91 85.97 87.89 
SVM-B 89.82 85.93 87.83 
      Table 4. Results on the test set 
 
Model R  P  FS  
ME 87.29 86.81 87.05 
CRF 89.19 88.85 89.02 
SVM-F 90.23 88.62 89.41 
SVM-B 90.05 88.61 89.09 
Table 5. Results of the post-processed models 
on the test set 
Each of the models has been also evaluated for 
the 10-fold cross validation tests. Initially all the 
models have been developed with the language 
independent features along with the context fea-
tures. Then, language dependent features have 
been included into the models. In each run of the 
10 tests, the outputs have been post-processed 
with the several post-processing techniques as 
described earlier. Results are shown in Table 6.  
  
 Model R  P  FS  
ME  81.34 79.01 80.16 
CRF 82.66 80.75 81.69 
SVM-F 83.87 81.83 82.83 
LI 
SVM-B 83.87 81.77 82.62 
ME  87.54 87.97 87.11 
CRF 89.5 88.73 89.19 
SVM-F 89.97 88.61 89.29 
LD
SVM-B 89.76 88.51 89.13 
Table 6. Results of the 10-fold cross validation 
tests   
Statistical ANOVA tests (Anderson and 
Scolve, 1978) demonstrated that the performance 
improvement in each of the language dependent 
model is statistically significant over the lan-
guage independent model. We have also carried 
out the statistical tests to show that performance 
improvement in CRF over ME and SVM-F over 
CRF are statistically significant.    
208
5.5 Impact of Unlabeled Data Selection 
In order to investigate the contribution of 
document selection in bootstrapping, the post-
processed models are run on 35,143 news 
documents. This yields the gradually improving 
performance for the SVM-F model as shown in 
Table 7. After selection of the appropriate 
unlabeled data, all the models have been 
retrained by including the unlabeled documents. 
Results have been presented in Table 8. 
 
Itera-
tion 
Sentences 
added 
R  P FS 
0 0 89.97 88.61 89.29 
1 129 90.19 88.97 89.58 
2 223 90.62 89.14 89.87 
3 332 90.89 89.73 90.31 
4 416 91.24 90.11 90.67 
5 482 91.69 90.65 91.16 
6 543 91.88 90.97 91.42 
7 633 92.07 91.05 91.56 
8 682 92.33 91.31 91.82 
9 712 92.52 91.39 91.95 
10 723 92.55 91.44 91.99 
11 729 92.57 91.45 92.01 
12 734 92.58 91.45 92.01 
Table 7. Incremental improvement of perform-
ance 
 
Model R  P  FS  
ME 90.7 89.78 90.24 
CRF 92.02 91.66 91.84 
SVM-B 92.34 91.42 91.88 
SVM-F 92.58 91.45 92.01 
Table 8. Results after unlabeled data selection 
5.6 Voting Techniques 
In order to obtain higher performance, we have 
applied weighted voting to the four models. We 
have used the following weighting methods: 
 (1). Uniform weights (Majority voting): All 
the models are assigned the same voting weight. 
The combined system selects the classifications, 
which are proposed by the majority of the mod-
els. In case of a tie, the output of the SVM-F 
model is selected. The output of the SVM-F 
model has been selected due to its highest per-
formance among all the models.  
  (2). Cross validation Precision values: Two 
different types of weights have been defined de-
pending on the 10-fold cross validation Precision 
on the training data as follows:  
   (a). Total Precision: In this method, the 
overall average Precision of any classifier is as-
signed as the weight for it.  
  (b). Tag Precision: In this method, the aver-
age Precision value of the individual tag is as-
signed as the weight for the corresponding model. 
 
Experimental results of the voted system are 
presented in Table 9. Evaluation results show 
that the system achieves the highest performance 
for the voting scheme ?Tag Precision?. Voting 
shows (Tables 8-9) an overall improvement of 
2.74% over the least performing ME based sys-
tem and 0.97% over the best performing SVM-F 
system. This also shows an improvement of 
23.66% F-Score over the baseline model. 
 
Voting  R  P  FS  
Majority 92.59 91.47 92.03 
Total Precision 93.08 91.79 92.43 
Tag Precision 93.81 92.18 92.98 
Table 9. Results of the voted system 
 
6 Conclusion 
In this paper, we have reported a voted system 
with the use of appropriate unlabeled data. We 
have also demonstrated how language dependent 
features can improve the system performance. It 
has been experimentally verified that effective 
measures to select relevant documents and useful 
labeled sentences are important. The system has 
demonstrated the overall Recall, Precision, and 
F-Score values of 93.81%, 92.18%, and 92.98%, 
respectively.   
Future works include the development of NER 
system using other machine learning techniques 
such as decision tree, AdaBoost etc. We would 
like to apply the proposed voted technique for 
the development of NER systems in other Indian 
languages. Future direction of the work will be to 
investigate an appropriate clustering technique 
that can be very effective for the development of 
NER systems in the resource-constrained Indian 
language environment. Instead of the words, the 
cluster of words can be used as the features of 
the classifiers. It may reduce the cost of training 
as well as may be helpful to improve the per-
formance. We would like to explore other voting 
techniques.  
 
 
209
References 
Anderson, T. W. and Scolve, S. Introduction to the 
Statistical Analysis of Data. Houghton Mifflin, 
1978. 
Bikel, Daniel M., R. Schwartz, Ralph M. Weischedel. 
1999. An Algorithm that Learns What?s in Name.  
Machine Learning (Special Issue on NLP), 1-20. 
Bothwick, Andrew. 1999. A Maximum Entropy Ap-
proach to Named Entity Recognition. Ph.D. Thesis, 
NYU.  
Ekbal, Asif, Naskar, Sudip and S. Bandyopadhyay. 
2007b. Named Entity Recognition and Translitera-
tion in Bengali. Named Entities: Recognition, 
Classification and Use, Special Issue of Lingvisti-
cae Investigationes Journal, 30:1 (2007), 95-114. 
Ekbal, Asif, Haque, R and S. Bandyopadhyay. 2008a. 
Named Entity Recognition in Bengali: A Condi-
tional Random Field Approach. In Proceedings of 
3rd International Joint Conference on Natural Lan-
guage Processing (IJCNLP-08), 589-594. 
Ekbal, Asif, and S. Bandyopadhyay. 2008b. Bengali 
Named Entity Recognition using Support Vector 
Machine. In Proceedings of the Workshop on 
Named Entity Recognition on South and South East 
Asian Languages (NERSSEAL), IJCNLP-08, 51-58. 
Ekbal, Asif, and S. Bandyopadhyay. 2008c. A Web-
based Bengali News Corpus for Named Entity 
Recognition. Language Resources and Evaluation 
Journal, Volume (40), 173-182. 
Ekbal, Asif and S. Bandyopadhyay. 2008d. Web-
based Bengali News Corpus for Lexicon Develop-
ment and POS Tagging. In POLIBITS, an Interna-
tional Journal, Volume (37), 20-29, ISSN: 1870-
9044.  
Ekbal, Asif and S. Bandyopadhyay. 2008e. Appropri-
ate Unlabeled Data, Post-processing and Voting 
Can Improve the Performance of NER System. In 
Proceedings of the 6th International Conference on 
Natural Language Processing (ICON-08), 234-
239, India. 
Ekbal, Asif and S. Bandyopadhyay. 2009. Improving 
the Performance of a NER System by Post-
processing, Context Patterns and Voting. In W. Li 
and D. Molla-Aliod (Eds): ICCPOL 2009, Lecture 
Notes in Artificial Intelligence (LNAI), Springer 
Berlin/Heidelberg, Volume (5459), 45-56. 
Florian, Radu, Ittycheriah, A., Jing, H. and Zhang, T. 
2003. Named Entity Recognition through Classifier 
Combination. In Proceedings of CoNLL-2003. 
Lafferty, J., McCallum, A., and Pereira, F. 2001. 
Conditional Random Fields: Probabilistic Models 
for Segmenting and Labeling Sequence Data. In 
Proceedings  of 18th International Conference on 
Machine Learning (ICML), 282-289. 
Li, Wei and Andrew McCallum. 2003. Rapid Devel-
opment of Hindi Named Entity Recognition Using 
Conditional Random Fields and Feature Induc-
tions. ACM TALIP, 2(3), (2003), 290-294. 
Saha, Sujan, Sarkar, S and Mitra, P. 2008. A Hybrid 
Feature Set based Maximum Entropy Hindi Named 
Entity Recognition. In Proceedings of the 3rd Inter-
national Joint Conference on Natural Language 
Processing (IJCNLP-08), 343-349. 
Valdimir N., Vapnik 1995. The Nature of Statistical 
Learning Theory. Springer. 
Yamada, Hiroyasu, Taku Kudo and Yuji Matsumoto. 
2002. Japanese Named Entity Extraction using 
Support Vector Machine. In Transactions of IPSJ, 
Vol. 43 No. 1, 44-53. 
 
210
Coling 2010: Poster Volume, pages 232?240,
Beijing, August 2010
Topic-Based Bengali Opinion Summarization 
Amitava Das 
Department of Computer Science 
and Engineering  
Jadavpur University 
amitava.santu@gmail.com 
Sivaji Bandyopadhyay 
Department of Computer Science 
and Engineering  
Jadavpur University 
sivaji_cse_ju@yahoo.com 
 
Abstract 
In this paper the development of an opi-
nion summarization system that works on 
Bengali News corpus has been described. 
The system identifies the sentiment in-
formation in each document, aggregates 
them and represents the summary infor-
mation in text. The present sys-tem fol-
lows a topic-sentiment model for senti-
ment identification and aggregation. Top-
ic-sentiment model is designed as dis-
course level theme identification and the 
topic-sentiment aggregation is achieved 
by theme clustering (k-means) and Doc-
ument level Theme Relational Graph re-
presentation. The Document Level 
Theme Relational Graph is finally used 
for candidate summary sentence selection 
by standard page rank algorithms used in 
Information Retrieval (IR). As Bengali is 
a resource constrained language, the 
building of annotated gold standard cor-
pus and acquisition of linguistics tools 
for lexico-syntactic, syntactic and dis-
course level features extraction are de-
scribed in this paper. The reported accu-
racy of the Theme detection technique is 
83.60% (precision), 76.44% (recall) and 
79.85% (F-measure). The summarization 
system has been evaluated with Precision 
of 72.15%, Recall of 67.32% and F-
measure of 69.65%. 
1 Introduction 
The Web has become a rich source of various 
opinions in the form of product reviews, travel 
advice, social issue discussions, consumer com-
plaints, movie review, stock market predictions, 
real estate market predictions, etc. Present com-
putational systems need to extend the power of 
understanding the sentiment/opinion expressed in 
an electronic text to act properly in the society 
rather than dealing with the topic of a document. 
The topic-document model of information re-
trieval has been studied for a long time and sys-
tems are available publicly since last decade. On 
the contrary Opinion Mining/Sentiment Analysis 
is still an unsolved research problem. Although a 
few systems like Twitter Sentiment Analysis 
Tool1, TweetFeel2 are available in World Wide 
Web since last few years still more research ef-
forts are necessary to match the user satisfaction 
level and social need. 
Researchers have taken multiple approaches 
towards the problem of Opinion Summarization 
like Topic-sentiment model, Textual summaries 
at single document or multiple document pers-
pective and graphical summaries or visualization. 
The works on opinion tracking systems have ex-
plicitly incorporated temporal dimension. The 
topic-sentiment model is well established for 
opinion retrieval. 
The concept of reputation system was first in-
troduced in (Resnick et al, 2000). Reputation 
systems for both buyers and sellers are needed to 
earn each other?s trust in online interactions.  
Ku et al, (2005) selects representative words 
from a document set to identify the main con-
cepts in the document set. A term is considered 
to represent a topic if it appears frequently across 
documents or in each document. Different me-
thodologies have been used to assign weights to 
each word both at document level and paragraph 
level. The precision and recall values of the sys-
tem have been reported as 0.56 and 0.85. 
                                                 
1
 http://twittersentiment.appspot.com/ 
2http://www.tweetfeel.com/ 
232
Zhou et al (2006) have proposed the architec-
ture for generative summary from blogosphere. 
Typical multi-document summarization (MDS) 
systems focus on content selection followed by 
synthesis by removing redundancy across mul-
tiple input documents. The online discussion 
summarization system (Zhou et al, 2006) work 
on an online discussion corpus involving mul-
tiple participants and discussion topics are passed 
back and forth by various participants. MDS sys-
tems are insufficient in representing this aspect 
of the interactions. Due to the complex structure 
of the dialogue, similar subtopic structure identi-
fication in the participant-written dialogues is 
essential. Maximum Entropy Model (MEMM) 
and Support Vector Machine (SVM) have been 
used with a number of relevant features. 
Carenini et al (2006) present and compare 
two approaches to the task of multi document 
opinion summarization on evaluative texts. The 
first is a sentence extraction based approach 
while the second one is a natural language gener-
ation-based approach. Relevant extracted fea-
tures are categorized in two types: User Defined 
Features (UDF) and Crude Features (CF) as de-
scribed in (Hu and Liu, 2004).  
The summary generation technique uses the 
aggregation of the extracted features, CF and 
UDF. Opinion aggregation has been done by the 
two relevant features: opinion strength and polar-
ity. A new opinion distribution function feature 
has been introduced to capture the overall opi-
nion distributed in corpus. 
Kawai et al (2007) developed a news portal 
site called Fair News Reader (FNR) that recom-
mends news articles with different sentiments for 
a user in each of the topics in which the user is 
interested. FNR can detect various sentiments of 
news articles and determine the sentimental pre-
ferences of a user based on the sentiments of 
previously read articles by the user. News ar-
ticles crawled from various news sites are stored 
in a database. The contents are integrated as 
needed and the summary is presented on one 
page. A sentiment vector on the basis of word 
lattice model has been generated for every doc-
ument. A user sentiment model has been pro-
posed based on user sentiment state. The user 
sentiment state model works on the browsing 
history of the user. The intersection of the docu-
ments under User Vector and Sentiment Vector 
are the results. 
2 Resource Organization 
Resource acquisition is one of the most challeng-
ing obstacles to work with resource constrained 
languages like Bengali. Bengali is the fifth popu-
lar language in the World, second in India and 
the national language in Bangladesh. Extensive 
NLP research activities in Bengali have started 
recently but resources like annotated corpus, var-
ious linguistic tools are still unavailable for Ben-
gali in the required measure. The manual annota-
tion of gold standard corpus and acquisition of 
various tools used in the feature extraction for 
Bengali are described in this section. 
2.1 Gold Standard Data Acquisition 
2.1.1 Corpus 
For the present task a Bengali news corpus has 
been developed from the archive of a leading 
Bengali news paper available on the Web 
(http://www.anandabazar.com/). A portion of the 
corpus from the editorial pages, i.e., Reader?s 
opinion section or Letters to the Editor Section 
containing 28K word forms has been manually 
annotated with sentence level subjectivity and 
discourse level theme words. Detailed reports 
about this news corpus development in Bengali 
can be found in (Das and Bandyopadhyay, 
2009b). 
2.1.2 Annotation 
From the collected document set (Letters to the 
Editor Section), some documents have been cho-
sen for the annotation task. Some statistics about 
the Bengali news corpus is represented in the 
Table 1. Documents that have appeared within an 
interval of four months are chosen on the hypo-
thesis that these letters to the editors will be on 
related events. A simple annotation tool has been 
designed for annotating the sentences considered 
to be important for opinion summarization. 
Three annotators (Mr. X, Mr. Y and Mr. Z) par-
ticipated in the present task.  
<Story> 
?????????????????????.. 
?????????????????????.. 
<SS><TW>Sargeant O?Leary</TW> said ?the 
<TW>incident</TW> took place at 2:00pm.?</SS> 
?????????????????????.. 
</Story> 
Figure 1: XML Annotation Format 
 Annotators were asked to annotate sentences 
for summary and to mark the theme words (topi-
cal expressions) in those sentences. The docu-
ments with such annotated sentences are saved in 
233
XML format. Figure 1 shows the XML annota-
tion format. ?<SS>? marker denotes subjective 
sentences and ?<TW>? denotes the theme words. 
 Bengali NEWS Corpus Statistics 
Total number of  documents in the corpus 100 
Total number of sentences in the corpus 2234 
Average number of sentences in a document 22 
Total number of wordforms in the corpus 28807 
Average number of wordforms in a document 288 
Total number of distinct wordforms in the 
corpus 
17176 
Table 1: Bengali News Corpus Statistics 
The annotation tool highlights the sentiment 
words (Das and Bandyopadhyay, 2010a)3 by four 
different colors within a document according to 
their POS categories (Noun, Adjective, Adverb 
and Verb). This technique helps to increase the 
speed of annotation process. Finally 100 anno-
tated documents have been developed. 
2.1.3 Inter-annotator Agreement 
The agreement of annotations among three anno-
tators has been evaluated. The agreements of tag 
values at theme words level and sentence levels 
are listed in Tables 2 and 3 respectively. 
 
Annotators X vs. Y X Vs. Z Y Vs. Z Avg 
Percentage 82.64% 71.78% 80.47% 78.30% 
All Agree 69.06% 
Table 2: Agreement of annotators at theme 
words level 
 
Annotators X vs. Y X Vs. Z Y Vs. Z Avg 
Percentage 73.87% 69.06% 60.44% 67.8% 
All Agree 58.66% 
Table 3: Agreement of annotators at sentence 
level 
From the analysis of inter-annotator agree-
ment, it is observed that the agreement drops fast 
as the number of annotator?s increases. It is less 
possible to have consistent annotations when 
more annotators are involved. In the present task 
the inter-annotator agreement is better for theme 
words annotation rather than candidate sentence 
identification for summary though a small num-
ber of documents have been considered. 
Further discussion with annotators reveals that 
the psychology of annotators is to grasp as many 
as possible theme words identification during 
annotation but the same groups of annotators are 
more cautious during sentence identification for 
summary as they are very conscious to find out 
the most concise set of sentences that best de-
scribe the opinionated snapshot of any document. 
                                                 
3
 http://www.amitavadas.com/sentiwordnet.php 
The annotators were working independent of 
each other and they were not trained linguists.  
2.2 Subjectivity Classifier 
Work in opinion mining and classification often 
assumes the incoming documents to be opinio-
nated. Opinion mining system makes false hits 
while attempting to summarize non-subjective or 
factual sentences or documents. It becomes im-
perative to decide whether a given document 
contains subjective information or not as well as 
to identify which portions of the document are 
subjective or factual. This task is termed as sub-
jectivity detection in sentiment literature. The 
subjectivity classifier that uses SVM machine 
learning technique and described in (Das and 
Bandyopadhyay, 2009a) has been used here. The 
recall measure of the present classifier is greater 
than its precision value. The evaluation results of 
the classifier are 72.16% (Precision) and 76.00 
(recall) on the News Corpus.  
2.3 Feature Organization 
The set of features used in the present task have 
been categorized as Lexico-Syntactic, Syntactic 
and Discourse level features. These are listed in 
the Table 4 below and have been described in the 
subsequent subsections. 
 
Types Features 
Lexico-Syntactic 
POS 
SentiWordNet 
Frequency 
Stemming 
Syntactic Chunk Label Dependency Parsing Depth 
Discourse Level 
Title of the Document 
First Paragraph 
Term Distribution 
Collocation 
Table 4: Features 
2.3.1 Lexico-Syntactic Features 
2.3.1.1 Part of Speech (POS) 
It has been shown in (Hatzivassiloglou et. al., 
2000), (Chesley et. al., 2006) etc. that opinion 
bearing words in sentences are mainly adjective, 
adverb, noun and verbs. Many opinion mining 
tasks, like (Nasukawa et. al., 2003) are mostly 
based on adjective words. Details of the Bengali 
POS tagger used can be found in (Das and Ban-
dyopadhyay 2009b). 
234
2.3.1.2 SentiWordNet (Bengali) 
Words that are present in the SentiWordNet car-
ry opinion information. The developed Senti-
WordNet (Bengali) (Das and Bandyopadhyay, 
2010a) is used as an important feature during the 
learning process. These features are individual 
sentiment words or word n-grams (multiword 
entities) with strength measure as strong subjec-
tive or weak subjective. Strong and weak subjec-
tive measures are treated as a binary feature in 
the supervised classifier. Words which are col-
lected directly from SentiWordNet (Bengali) are 
tagged with positivity or negativity score. The 
subjectivity score of these words are calculated 
as:                 
| | | |s p nE S S= +  
where sE  is the resultant subjective measure 
and pS , nS  are the positivity and negativity 
scores respectively. 
2.3.1.3 Frequency 
Frequency always plays a crucial role in identify-
ing the importance of a word in the document. 
The system generates four separate high frequent 
word lists for four POS categories: Adjective, 
Adverb, Verb and Noun after function words are 
removed. Word frequency values are then effec-
tively used as a crucial feature in the Theme De-
tection technique. 
2.3.1.4 Stemming 
Several words in a sentence that carry opinion 
information may be present in inflected forms 
and stemming is necessary for them before they 
can be searched in appropriate lists. Due to non 
availability of good stemmers in Indian languag-
es especially in Bengali, a stemmer (Das and 
Bandyopadhyay, 2010b) based on stemming 
cluster technique has been used. This stemmer 
analyzes prefixes and suffixes of all the word 
forms present in a particular document. Words 
that are identified to have the same root form are 
grouped in a finite number of clusters with the 
identified root word as cluster center.  
2.3.2 Syntactic Features 
2.3.2.1 Chunk Label 
Chunk level information is effectively used as a 
feature in supervised classifier. Chunk labels are 
defined as B-X (Beginning), I-X (Intermediate) 
and E-X (End), where X is the chunk label. In 
the task of identification of Theme expressions, 
chunk label markers play a crucial role. Further 
details of development of chunking system could 
be found in (Das and Bandyopadhyay 2009b).  
2.3.2.2 Dependency Parser 
Dependency depth feature is very useful to iden-
tify Theme expressions. A particular Theme 
word generally occurs within a particular range 
of depths in a dependency tree. Theme expres-
sions may be a Named Entity (NE: person, or-
ganization or location names), a common noun 
(Ex: accident, bomb blast, strike etc) or words of 
other POS categories. It has been observed that 
depending upon the nature of Theme expressions 
it can occur within a certain depth in the depen-
dency tree for the sentence. A statistical depen-
dency parser has been used for Bengali as de-
scribed in (Ghosh et al, 2009). 
2.3.3 Discourse Level Features 
2.3.3.1 Positional Aspect 
Depending upon the position of the thematic 
clue, every document is divided into a number of 
zones. The features considered for each docu-
ment are Title words of the document, the first 
paragraph words and the words from the last two 
sentences. A detailed study was done on the 
Bengali news corpus to identify the roles of the 
positional aspect features of a document (first 
paragraph, last two sentences) in the detection of 
theme words and subjective sentences for gene-
rating the summary of the document. The impor-
tance of these positional features is shown in 
Tables 5 on the Bengali gold standard set. 
2.3.3.2 Title Words 
Title words of a document always carry some 
meaningful thematic information. The title word 
feature has been used as a binary feature during 
CRF based machine learning. 
2.3.3.3 First Paragraph Words 
People usually give a brief idea of their beliefs 
and speculations in the first paragraph of the 
document and subsequently elaborate or support 
them with relevant reasoning or factual informa-
tion. Hence first paragraph words are informative 
in the detection of Thematic Expressions.  
2.3.3.4 Words From Last Two Sentences 
Generally every document concludes with a 
summary of the opinions expressed in the docu-
ment. 
235
Positional Factors Bengali 
First Paragraph 56.80% 
Last Two Sentences 78.00% 
Table 5: Statistics on Positional Aspect. 
2.3.3.5 Term Distribution Model 
An alternative to the classical TF-IDF weighting 
mechanism of standard IR has been proposed as 
a model for the distribution of a word. The model 
characterizes and captures the informativeness of 
a word by measuring how regularly the word is 
distributed in a document. As discussed in Sec-
tion 1, Carenini et al (2006) have introduced the 
opinion distribution function feature to capture 
the overall opinion distributed in the corpus. 
Thus the objective is to estimate ( )d if w  that 
measures the distribution pattern of the k occur-
rences of the word wi in a document d. Zipf's law 
describes distribution patterns of words in an 
entire corpus. In contrast, term distribution mod-
els capture regularities of word occurrence in 
subunits of a corpus (e.g., documents, paragraphs 
or chapters of a book). A good understanding of 
the distribution patterns is useful to assess the 
likelihood of occurrences of a word in some spe-
cific positions (e.g., first paragraph or last two 
sentences) of a unit of text. Most term distribu-
tion models try to characterize the informative-
ness of a word identified by inverse document 
frequency (IDF). In the present work, the distri-
bution pattern of a word within a document for-
malizes the notion of topic-sentiment informa-
tiveness. This is based on the Poisson distribu-
tion. Significant Theme words are identified us-
ing TF, Positional and Distribution factor. The 
distribution function for each theme word in a 
document is evaluated as follows: 
( )1 1
1 1
( ) / ( ) /
n n
d i i i i i
i i
f w S S n TW TW n
? ?
= =
= ? + ?? ?
 
where n=number of sentences in a document 
with a particular theme word, Si=sentence id of 
the current sentence containing the theme word 
and Si-1=sentence id of the previous sentence 
containing the query term, iTW is the positional id 
of current Theme word and 1iTW ? is the positional 
id of the previous Theme word. 
2.3.3.6 Collocation 
Collocation with other thematic word/expression 
is undoubtedly an important clue for identifica-
tion of theme sequence patterns in a document. A 
window size of 5 including the present word is 
considered during training to capture the colloca-
tion with other thematic words/expressions. 
 
3 Theme Detection 
Term Frequency (TF) plays a crucial role to 
identify document relevance in Topic-Based In-
formation Retrieval. The motivation behind de-
veloping Theme detection technique is that in 
many documents relevant words may not occur 
frequently or irrelevant words may occur fre-
quently. Moreover for sentiment analysis topic 
words should have sentiment conceptuality. The 
Theme detection technique has been proposed to 
resolve these issues to identify discourse level 
relevant topic-semantic nodes in terms of word 
or expressions using a standard machine learning 
technique. The machine learning technique used 
here is Conditional Random Field (CRF)4. The 
theme word detection is defined as a sequence 
labeling problem. Depending upon the series of 
input feature, each word is tagged as either 
Theme Word (TW) or Other (O). 
4 Theme Clustering 
Theme clustering algorithms partition a set of 
documents into finite number of topic based 
groups or clusters in terms of theme 
words/expressions. The task of document cluster-
ing is to create a reasonable set of clusters for a 
given set of documents. A reasonable cluster is 
defined as the one that maximizes the within-
cluster document similarity and minimizes be-
tween-cluster similarities. There are two princip-
al motivations for the use of this technique in 
theme clustering setting: efficiency, and the clus-
ter hypothesis. 
The cluster hypothesis (Jardine and van Rijs-
bergen, 1971) takes this argument a step further 
by asserting that retrieval from a clustered col-
lection will not only be more efficient, but will in 
fact improve retrieval performance in terms of 
recall and precision. The basic notion behind this 
hypothesis is that by separating documents ac-
cording to topic, relevant documents will be 
found together in the same cluster, and non-
relevant documents will be avoided since they 
will reside in clusters that are not used for re-
trieval. Despite the plausibility of this hypothe-
sis, there is only mixed experimental support for 
it. Results vary considerably based on the clus-
                                                 
4
 http://crfpp.sourceforge.net 
236
tering algorithm and document collection in use 
(Willett, 1988; Shaw et al, 1996). 
Application of the clustering technique to the 
three sample documents results in the following 
theme-by-document matrix, A, where the rows 
represent Docl, Doc7 and Doc13 and the col-
umns represent the themes politics, sport, and 
travel.  
election cricket hotel
A parliament sachin vacation
governor soccer tourist
? ?
? ?
= ? ?
? ?? ?
 
The similarity between vectors is calculated 
by assigning numerical weights to these words 
and then using the cosine similarity measure as 
specified in the following equation.  
, ,
1
, .
N
k j k j i k i j
i
s q d q d w w
? ? ? ?
=
? ?
= = ?? ?? ? ? ---- (1) 
This equation specifies what is known as the 
dot product between vectors.  Now, in general, 
the dot product between two vectors is not par-
ticularly useful as a similarity metric, since it is 
too sensitive to the absolute magnitudes of the 
various dimensions. However, the dot product 
between vectors that have been length norma-
lized has a useful and intuitive interpretation: it 
computes the cosine of the angle between the 
two vectors. When two documents are identical 
they will receive a cosine of one; when they are 
orthogonal (share no common terms) they will 
receive a cosine of zero. Note that if for some 
reason the vectors are not stored in a normalized 
form, then the normalization can be incorporated 
directly into the similarity measure as follows.  
, ,1
2 2
, ,1 1
,
N
i k i ji
k j N N
i k i ki i
w w
s q d
w w
? ?
=
= =
?? ?
=? ?? ? ?
?
? ?  ----(2) 
Of course, in situations where the document 
collection is relatively static, it makes sense to 
normalize the document vectors once and store 
them, rather than include the normalization in the 
similarity metric. 
Calculating the similarity measure and using a 
predefined threshold value, documents are classi-
fied using standard bottom-up soft clustering k-
means technique. The predefined threshold value 
is experimentally set to 0.5 as shown in Table 6. 
A set of initial cluster centers is necessary in 
the beginning. Each document is assigned to the 
cluster whose center is closest to the document. 
After all documents have been assigned, the cen-
ter of each cluster is recomputed as the centroid 
or mean ?
?
 (where ?
?
 is the clustering coeffi-
cient) of its members, that 
is ( )1/
jj x c
c x?
? ?
?
= ? . The distance function is 
the cosine vector similarity function. 
ID Themes 1 2 3 
1  (administration) 0.63 0.12 0.04 
1  (good-government) 0.58 0.11 0.06 
1  (Society) 0.58 0.12 0.03 
1  (Law) 0.55 0.14 0.08 
2  !"# (Research) 0.11 0.59 0.02 
2 & ' (College) 0.15 0.55 0.01 
2 	
 (Higher Study) 0.12 0.66 0.01 
3  +,- (Jehadi) 0.13 0.05 0.58 
3 ,- (Mosque) 0.05 0.01 0.86 
3 23 (Musharaf) 0.05 0.01 0.86 
3 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 50?55,
Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational Linguistics
Dr Sentiment Knows Everything! 
 
Amitava Das and Sivaji Bandyopadhyay 
Department of Computer Science and Engineering 
Jadavpur University 
India 
amitava.santu@gmail.com sivaji_cse_ju@yahoo.com 
 
 
Abstract 
Sentiment analysis is one of the hot de-
manding research areas since last few dec-
ades. Although a formidable amount of 
research have been done, the existing re-
ported solutions or available systems are 
still far from perfect or do not meet the sa-
tisfaction level of end users?. The main is-
sue is the various conceptual rules that 
govern sentiment and there are even more 
clues (possibly unlimited) that can convey 
these concepts from realization to verbali-
zation of a human being. Human psycholo-
gy directly relates to the unrevealed clues 
and governs the sentiment realization of us. 
Human psychology relates many things 
like social psychology, culture, pragmatics 
and many more endless intelligent aspects 
of civilization. Proper incorporation of hu-
man psychology into computational senti-
ment knowledge representation may solve 
the problem. In the present paper we pro-
pose a template based online interactive 
gaming technology, called Dr Sentiment to 
automatically create the PsychoSenti-
WordNet involving internet population. 
The PsychoSentiWordNet is an extension 
of SentiWordNet that presently holds hu-
man psychological knowledge on a few as-
pects along with sentiment knowledge. 
1 Introduction 
In order to identify sentiment from a text, lexical 
analysis plays a crucial role. For example, words 
like love, hate, good and favorite directly indicate 
sentiment or opinion. Previous works (Pang et al, 
2002; Wiebe and Mihalcea, 2006; Baccianella et. 
al., 2010) have already proposed various tech-
niques for making dictionaries for those sentiment 
words. But polarity assignment of such sentiment 
lexicons is a hard semantic disambiguation prob-
lem. The regulating aspects which govern the lexi-
cal level semantic orientation are natural language 
context (Pang et al, 2002), language properties 
(Wiebe and Mihalcea, 2006), domain pragmatic 
knowledge (Aue and Gamon, 2005), time dimen-
sion (Read, 2005), colors and culture (Strapparava 
and Ozbal, 2010) and many more unrevealed hid-
den aspects. Therefore it is a challenging and 
enigmatic research problem. 
The current trend is to attach prior polarity to 
each entry at the sentiment lexicon level. Prior po-
larity is an approximation value based on heuristics 
based statistics collected from corpus and not ex-
act. The probabilistic fixed point prior polarity 
scores do not solve the problem completely rather 
it places the problem into next level, called contex-
tual polarity classification.  
We start with the hypothesis that the summation 
of all the regulating aspects of sentiment orienta-
tion is human psychology and thus it is a multi-
faceted problem (Liu, 2010). More precisely what 
we mean by human psychology is the union of all 
known and unknown aspects that directly or indi-
rectly govern the sentiment orientation knowledge 
of us. The regulating aspects wrapped in the 
present PsychoSentiWordNet are Gender, Age, 
City, Country, Language and Profession.  
The PsychoSentiWordNet is an extension of the 
existing SentiWordNet 3.0 (Baccianella et. al., 
2010) to hold the possible psychological ingre-
dients and govern the sentiment understandability 
of us. The PsychoSentiWordNet holds variable 
prior polarity scores that could be fetched depend-
ing upon those psychological regulating aspects. 
50
An example with the input word ?High? may illu-
strate the definition better:  
 
Aspects (Profession)   Polarity 
Null     Positive 
Businessman    Negative 
Share Broker   Positive 
 
In this paper, we propose an interactive gaming 
(Dr Sentiment) technology to collect psycho-
sentimental polarity for lexicons. This technology 
has proven itself as an excellent technique to col-
lect psychological sentiment of human society 
even at multilingual level. Dr Sentiment presently 
supports 56 languages and therefore we may call it 
Global PsychoSentiWordNet. The supported lan-
guages by Dr Sentiment are reported in Table 1. 
In this section we have philosophically argued 
about the necessity of developing PsychoSenti-
WordNet. In the next section 2 we will describe the 
technical details of the proposed architecture for 
building the lexical resource. Section 3 explains 
about some exciting outcomes of PsychoSenti-
WordNet. The developed PsychoSentiWordNet(s) 
are expected to help automatic sentiment analysis 
research in many aspects and other disciplines as 
well and have been described in section 4.The data 
structure and the organization are described in sec-
tion 5. The conclusion is drawn in section 6. 
2 Dr Sentiment 
Dr Sentiment1 is a template based interactive on-
line game, which collects player?s sentiment by 
asking a set of simple template based questions and 
finally reveals a player?s sentimental status. Dr 
Sentiment fetches random words from Senti-
WordNet synsets and asks every player to tell 
about his/her sentiment polarity understanding re-
garding the concept behind the word fetched by it.  
There are several motivations behind developing 
the intuitive game to automatically collect human 
psycho-sentimental orientation information.  
In the history of Information Retrieval research 
there is a milestone when ESP game2 (Ahn et al, 
2004) innovated the concept of a game to automat-
ically label images available in the World Wide 
Web. It has been identified as the most reliable 
strategy to automatically annotate the online im-
                                                          
1
 http://www.amitavadas.com/Sentiment%20Game/index.php 
2
 http://www.espgame.org/ 
ages. We are highly motivated by the success of 
the Image Labeler game.  
A number of research endeavors could be found 
in the literature for creation of Sentiment Lexicon 
in several languages and domains. These tech-
niques can be broadly categorized into two classes, 
one follows classical manual annotation techniques  
(Andreevskaia and Bergler, 2006);(Wiebe and Ri-
loff, 2006) while the other follows various auto-
matic techniques (Mohammad et al, 2008). Both 
types of techniques have few limitations. Manual 
annotation techniques are undoubtedly trustable 
but it generally takes time. Automatic techniques 
demand manual validations and are dependent on 
the corpus availability in the respective domain. 
Manual annotation techniques require a large num-
ber of annotators to balance one?s sentimentality in 
order to reach agreement. But human annotators 
are quite unavailable and costly. 
Sentiment is a property of human intelligence 
and is not entirely based on the features of a lan-
guage. Thus people?s involvement is required to 
capture the sentiment of the human society. We 
have developed an online game to attract internet 
population for the creation of PsychoSentiWord-
Net automatically. Involvement of Internet popula-
tion is an effective approach as the population is 
very high in number and ever growing (approx. 
360,985,492) 3 . Internet population consists of 
people with various languages, cultures, age etc 
and thus not biased towards any domain, language 
or particular society. A detailed statistics on the 
Internet usage and population has been reported in 
the Table 2. 
The lexicons tagged by this system are credible 
as it is tagged by human beings. It is not a static 
sentiment lexicon set [polarity changes with time 
(Read, 2005)] as it is updated regularly. Around 
10-20 players each day are playing it throughout 
the world in different languages. The average 
number of tagging per word is about 7.47 till date. 
The Sign Up form of the ?Dr Sentiment? game 
asks the player to provide personal information 
such as Sex, Age, City, Country, Language and 
Profession. These collected personal details of a 
player are kept as a log record in the database. 
The gaming interface has four types of question 
templates. The question templates are named as 
Q1, Q2, Q3 and Q4. 
                                                          
3
 http://www.internetworldstats.com/stats.htm 
51
Languages 
Afrikaans Bulgarian Dutch German Irish Malay Russian Thai 
Albanian Catalan Estonian Greek Italian Maltese Serbian Turkish 
Arabic Chinese Filipino Haitian Japanese Norwegian Slovak Ukrainian 
Armenian Croatian Finnish Hebrew Korean Persian Slovenian Urdu 
Azerbaijani Creole French Hungarian Latvian Polish Spanish Vietnamese 
Basque Czech Galician Icelandic Lithuanian Portuguese Swahili Welsh 
Belarusian Danish Georgian Indonesian Macedonian Romanian Swedish Yiddish 
Table 1: Languages
 
WORLD INTERNET USAGE AND POPULATION STATISTICS 
World Regions Population ( 2010 Est.) 
Internet Users 
Dec. 31, 2000 
Internet Users 
Latest Data 
Penetration 
(Population) 
Growth 
2000-2010 
Users % 
of Table 
Africa 1,013,779,050 4,514,400 110,931,700 10.9 % 2,357.3 % 5.6 % 
Asia 3,834,792,852 114,304,000 825,094,396 21.5 % 621.8 % 42.0 % 
Europe  813,319,511 105,096,093 475,069,448 58.4 % 352.0 % 24.2 % 
Middle East  212,336,924 3,284,800 63,240,946 29.8 % 1,825.3 % 3.2 % 
North America 344,124,450 108,096,800 266,224,500 77.4 % 146.3 % 13.5 % 
Latin America/Caribbean  592,556,972 18,068,919 204,689,836 34.5 % 1,032.8 % 10.4 % 
Oceania / Australia  34,700,201 7,620,480 21,263,990 61.3 % 179.0 % 1.1 % 
WORLD TOTAL 6,845,609,960 360,985,492 1,966,514,816 28.7 % 444.8 % 100.0 % 
Table 2: Internet Usage and Population Statistics 
 
To make the gaming interface more interesting 
images have been added. These images have been 
retrieved by Google image search API 4  and to 
avoid biasness we have randomized among the 
first ten images retrieved by Google. 
2.1 Gaming Strategy 
Dr Sentiment asks 30 questions to each player. 
There are predefined distributions of each question 
type as 11 for Q1, 11 for Q2, 4 for Q3 and 4 for 
Q4. These numbers are arbitrarily chosen and ran-
domly changed for experimentation. The questions 
are randomly asked to keep the game more inter-
esting. For word based translation Google transla-
tion5 service has been used. At each Question (Q) 
level translation service has been used to display 
the sentiment word into player?s own language. 
Google API provides multiple senses for word lev-
el translation and currently only the first sense has 
been picked automatically.  
2.2 Q1 
An English word from the English SentiWordNet 
synset is randomly chosen. The Google image 
search API is fired with the word as a query. An 
image along with the word itself is shown in the 
Q1 page of the game.  
                                                          
4
 http://code.google.com/apis/imagesearch/ 
5
 http://translate.google.com/ 
Players press the different emoticons (Figure 1) 
to express their sentimentality. The interface keeps 
log records of each interaction. 
Extreme 
Positive Positive Neutral Negative 
Extreme 
Negative 
 
 
 
 
 
Figure 1: Emoticons to Express Player?s Senti-
ment 
2.3 Q2 
This question type is specially designed for relative 
scoring technique. For example: good and better 
both are positive but we need to know which one is 
more positive than other. Table 3 shows how in 
SentiWordNet relative scoring has been made. 
With the present gaming technology relative polar-
ity scoring has been assigned to each n-n word pair 
combination. 
Randomly n (presently 2-4) words have been 
chosen from the source SentiWordNet synsets 
along with their images as retrieved by Google 
API. Each player is then asked to select one of 
them that he/she likes most. The relative score is 
calculated and stored in the corresponding log ta-
ble. 
Word Positivity Negativity 
Good 0.625 0.0 
Better 0.875 0.0 
Best 0.980 0.0 
Table 3: Relative Sentiment Scores in Senti-
WordNet 
52
2.4 Q3 
The player is asked for any positive word in his/her 
mind. This technique helps to increase the cover-
age of existing SentiWordNet. The word is then 
added to the existing PsychoSentiWordNet and 
further used in Q1 to other users to note their sen-
timentality about the particular word. 
2.5 Q4 
A player is asked by Dr Sentiment about any nega-
tive word. The word is then added to the existing 
PsychoSentiWordNet and further used in Q1 to 
other users to note their sentimentality about the 
particular word. 
2.6 Comment Architecture 
There are three types of Comments, Comment type 
1 (CMNT1), Comment type 2 (CMNT2) and the 
final comment as Dr Sentiment?s prescription. 
CMNT1 type and CMNT2 type comments are as-
sociated with question types Q1 and Q2 respective-
ly. 
2.6.1 CMNT1 
Comment type 1 has 5 variations as shown in the 
Comment table in Table 4. Comments are random-
ly retrieved from comment type table according to 
their category: 
 
? Positive word has been tagged as negative (PN) 
? Positive word has been tagged as positive (PP) 
? Negative word has been tagged as positive (NP) 
? Negative word has been tagged as negative (NN) 
? Neutral. (NU) 
2.6.2 CMNT2 
The strategy here is as same as the CMNT 1. 
Comment type 2 has only two variations as. 
? Positive word has been tagged as negative (PN) 
? Negative word has been tagged as positive (NP) 
2.7 Dr Sentiment?s Prescription 
The final prescription depends on various factors 
such as total number of positive, negative or neu-
tral comments and the total time taken by any 
player. The final prescription also depends on the 
range of the accumulated values of all the above 
factors.  
This is the most important appealing factor to a 
player. The motivating message for players is that 
Dr Sentiment can reveal their sentimental status: 
whether they are extreme negative or positive or 
very much neutral or diplomatic etc. It is not 
claimed that the revealed status of a player by Dr 
Sentiment is exact or ideal. It is only to make the 
players motivated but the outcomes of the game 
effectively helps to store human sentimental psy-
chology in terms of computational lexicon. 
A word previously tagged by a player is avoided 
by the tracking system during subsequent turns by 
the same player. The intension is to tag more and 
more words involving Internet population. We ob-
serve that the strategy helps to keep the game in-
teresting as a large number of players return to 
play the game after this strategy was implemented. 
3 Senti-Mentality 
PsychoSentiWordNet gives a good sketch to un-
derstand the psycho-sentimental behavior of the 
human society depending upon proposed psycho-
logical dimensions. The PsychoSentiWordNet is 
basically the log records of every player?s tagged 
words.  
3.1 Concept-Culture-Wise Analysis 
The word ?blue? gets tagged by different players 
around the world. But surprisingly it has been 
tagged as positive from one part of the world and 
negative from another part of the world. The 
graphical illustration in Figure 2 may explain the 
situation better. The observation is that most of the 
negative tags are coming from the middle-east and 
especially from the Islamic countries. 
PN PP NP NN NU 
You don?t like 
<word>! 
Good you have a good 
choice! Is <word> good! 
Yes <word> is too 
bad! 
You should speak out 
frankly! 
You should like 
<word>! I love <word> too! 
I hope it is a bad 
choice! You are quite right! 
You are too diplomat-
ic! 
But <word> is a good 
itself! I support your view! 
I don?t agree with 
you! 
I also don?t like 
<word>! 
Why you hiding from 
me? I am Dr Senti-
ment. 
Table 4: Comments 
53
We found a line in Wiki6 (see in Religion Section) 
that may provide a good explanation: ?Blue in Is-
lam: In verse 20:102 of the Qur?an, the word ??? 
zurq (plural of azraq 'blue') is used metaphorically 
for evil doers whose eyes are glazed with fear?. 
But other explanations may be there for this situa-
tion. This is an interesting observation that sup-
ports the effectiveness of the developed 
PsychoSentiWordNet. This information could be 
further retrieved from the developed source by giv-
ing information like (blue, Italy), (blue, Iraq) or 
(blue, USA) etc. 
 
Figure 2: Geospatial Senti-Mentality 
3.2 Age-Wise Analysis 
Another interesting observation is that sentimental-
ity may vary age-wise. For better understanding we 
look at the total statistics and the age wise distribu-
tion of all the players. Total 533 players have taken 
part till date. The total number of players for each 
range of age is shown at the top of every bar.  
 
Figure 3: Age-Wise Senti-Mentality 
In Figure 3 the horizontal bars are divided into two 
colors (Green depicts the Positivity and Red de-
picts the negativity) according to the total positivi-
ty and negativity scores, gathered during playing. 
                                                          
6
 http://en.wikipedia.org/wiki/Blue 
This sociological study gives an idea on the varia-
tion of sentimentality with age.  This information 
may be retrieved from the developed source by 
giving information like (X, 36-39) or (X, 45-49) 
etc where X denotes any arbitrary lexicon synset. 
3.3 Gender-Wise Analysis 
It is observed from the collected statistics that 
women are more positive than men! The variations 
in sentimentality among men and women are 
shown in the following Figure 4.  
 
Figure 4: Gender Specific Senti-Mentality 
3.4 Other-Wise 
We have described several important observations 
in the previous sections and there are other impor-
tant observations as well. Studies on the combina-
tions of the proposed psychological dimensions, 
such as, location-age, location-profession and 
gender-location may reveal some interesting re-
sults.  
4 Expected Impact of the Resource 
Undoubtedly the generated PsychoSentiWord-
Net(s) are important resources for senti-
ment/opinion or emotion analysis task. Moreover 
the other non linguistic psychological dimensions 
are very much important for further analysis as 
well as for several newly discovered sub-
disciplines such as: Geospatial Information retriev-
al (Egenhofer, 2002), Personalized search (Gaucha 
et al, 2003), Recommender System (Adomavicius 
and Tuzhilin, 2005), Sentiment Tracking (Tong, 
2001) etc. 
5 The Data Structure and Organization 
Deciding on the data structure for the PsychoSen-
tiWordNet was not trivial. Presently RDBMS (Re-
lational Database Management System) has been 
54
used. Several tables are being used to keep user?s 
clicking log and their personal information.  
As one of the research motivations was to gen-
erate up-to-date prior polarity scores across various 
dimensions, we decided to generate web service 
API through which the people can access latest 
prior polarity scores. The developed PsychoSenti-
WordNet is expected to perform better than a static 
sentiment lexicon. 
6 Conclusion and Future Directions 
In the present paper the development of the Psy-
choSentiWordNet for 56 languages has been de-
scribed. No evaluation has been done yet as there 
is no data available for this kind of experimenta-
tion and to the best of our knowledge this is the 
first endeavor where sentiment analysis meets AI 
and psychology.  
Our present goal is to collect such corpus and 
carry out experiments to check whether variable 
prior polarity scores of PsychoSentiWordNet excel 
over the fixed point prior polarity score of Senti-
WordNet. 
Automatically picked first sense from Google 
translation API may cause difficulties for cross 
lingual projection of sentiment synsets. Erroneous 
outputs from API may also cause some problems. 
But these problems lead to another research issue 
that may be termed as cross lingual sentiment syn-
set linking. Presently we are giving a closer look to 
the qualitative analysis of developed multilingual 
psycho-sentiment lexicons. 
Acknowledgment 
The work reported in this paper was supported by a 
grant from the India-Japan Cooperative Program 
(DST-JST) Research project entitled ?Sentiment 
Analysis where AI meets Psychology? funded by 
Department of Science and Technology (DST), 
Government of India. 
References  
Adomavicius Gediminas and Alexander Tuzhilin. To-
ward the Next Generation of Recommender Systems: 
A Survey of the State-of-the-Art and Possible Exten-
sions. In the Proc. of IEEE Transactions on Know-
ledge and Data Engineering, VOL. 17, NO. 6, June 
2005. ISSN 1041-4347/05. Pages 734-749. 
Ahn Luis von and Laura Dabbish. Labeling Images with 
a Computer Game.In the Proc. of ACM CHI 2004. 
Andreevskaia Alina and Bergler Sabine. CLaC and 
CLaC-NB: Knowledge-based and corpus-based ap-
proaches to sentiment tagging. In the Proc. of the 4th 
SemEval-2007, Pages 117?120, Prague, June 2007. 
Aue A. and Gamon M., Customizing sentiment classifi-
ers to new domains: A case study. In the Proc. Of 
RANLP, 2005. 
Baccianella Stefano, Andrea Esuli, and Fabrizio Sebas-
tiani. SENTIWORDNET 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion Min-
ing. In the Proc. of LREC-10. 
Bo Pang, Lee Lillian, and Vaithyanathan Shivakumar. 
Thumbs up? Sentiment classification using machine 
learning techniques. In the Proc. of EMNLP, Pages 
79?86, 2002. 
Egenhofer M.. Toward the Semantic Geospatial Web. 
ACM-GIS 2002, McLean, VI A. Voisard and S.-C. 
Chen (eds.), Pages. 1-4, November 2002. 
Gaucha Susan, Jason Chaffeeb and Alexander Pret-
schnerc. Ontology-based personalized search and 
browsing. In Proc. of Web Intelligence and Agent 
Systems: An international journal. 2003. Pages 219?
234. ISSN 1570-1263/03. 
Liu Bing . Sentiment Analysis: A Multi-Faceted Prob-
lem.In the IEEE Intelligent Systems, 2010. 
Read Jonathon. Using emoticons to reduce dependency 
in machine learning techniques for sentiment classi-
fication. In the Proc. of the ACL Student Research 
Workshop, 2005. 
Richard M. Tong. An operational system for detecting 
and tracking opinions in online discussion. In the 
Proc. of the Workshop on Operational Text Classifi-
cation (OTC), 2001. 
Saif Mohammad, Dorr Bonnie and Hirst Graeme. Com-
puting Word-Pair Antonymy. In the Proc. of 
EMNLP-2008. 
Strapparava, C. and Valitutti, A. WordNet-Affect: an 
affective extension of WordNet. In Proc. of LREC 
2004, Pages 1083 ? 1086 
Wiebe Janyce and Mihalcea Rada. Word sense and sub-
jectivity. In the Proc. of COLING/ACL-06. Pages 
1065-1072. 
55
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 206?209,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
JU: A Supervised Approach to Identify Semantic Relations from Paired 
Nominals 
 
Santanu Pal         Partha Pakray             Dipankar Das          Sivaji Bandyopadhyay
Department of Computer Science & Engineering, Jadavpur University, Kolkata, India 
santanupersonal1@gmail.com,parthapakray@gmail.com,     
dipankar.dipnil2005@gmail.com,sivaji_cse_ju@yahoo.com 
Abstract 
This article presents the experiments carried 
out at Jadavpur University as part of the 
participation in Multi-Way Classification of 
Semantic Relations between Pairs of Nomi-
nals in the SemEval 2010 exercise. Separate 
rules for each type of the relations are iden-
tified in the baseline model based on the 
verbs and prepositions present in the seg-
ment between each pair of nominals. Inclu-
sion of WordNet features associated with 
the paired nominals play an important role 
in distinguishing the relations from each 
other. The Conditional Random Field (CRF) 
based machine-learning framework is 
adopted for classifying the pair of nominals.  
Application of dependency relations, 
Named Entities (NE) and various types of 
WordNet features along with several com-
binations of these features help to improve 
the performance of the system. Error analy-
sis suggests that the performance can be im-
proved by applying suitable strategies to 
differentiate each paired nominal in an al-
ready identified relation. Evaluation result 
gives an overall macro-averaged F1 score of 
52.16%.     
1 Introduction 
Semantic Relations describe the relations between 
concepts or meanings that are crucial but hard to 
identify. The present shared task aims to develop 
the systems for automatically recognizing semantic 
relations between pairs of nominals. Nine relations 
such as Cause-Effect, Instrument-Agency, Product-
Producer, Content-Container, Entity-Origin, En-
tity-Destination, Component-Whole, Member-
Collection and Message-Topic are given for Se-
mEval-2010 Task #8 (Hendrix et al, 2010). The 
relation that does not belong to any of the nine re-
lations is tagged as Other. The first five relations 
also featured in the previous SemEval-2007 Task 
#4.  
The present paper describes the approach of 
identifying semantic relations between pair of 
nominals. The baseline system is developed based 
on the verbs and prepositions present in the senten-
tial segment between the two nominals. Some 
WordNet (Miller, 1990) features are also used in 
the baseline for extracting the relation specific at-
tributes (e.g. Content type hypernym feature used 
for extracting the relation of Content-Container). 
The performance of the baseline system is limited 
due to the consideration of only the verb and 
preposition words in between the two nominals 
along with a small set of WordNet features. Hence, 
the Conditional Random Field (CRF) (McCallum 
et al, 2001) based framework is considered to ac-
complish the present task. The incorporation of 
different lexical features (e.g. WordNet hyponyms, 
Common-parents, distance), Named Entities (NE) 
and syntactic features (direct or transitive depend-
ency relations of parsing) has noticeably improved 
the performance of the system. It is observed that 
nominalization feature plays an effective role for 
identifying as well as distinguishing the relations. 
The test set containing 2717 sentences is evaluated 
against four different training sets. Some of the 
relations, e.g. Cause-Effect, Member-Collection 
perform well in comparison to other relations in all 
the four test results. Reviewing of the confusion 
matrices suggests that the system performance can 
be improved by reducing the errors that occur in 
distinguishing the two individual nominals in each 
relation. 
The rest of the paper is organized as follows. 
The pre-processing of resources and the baseline 
system are described in Section 2 and Section 3 
respectively. Development of CRF-based model is 
discussed in Section 4. Experimental results along 
206
with error analysis are specified in Section 5. Fi-
nally Section 6 concludes the paper. 
2 Resource Pre-Processing 
The annotated training corpus containing 8000 sen-
tences was made available by the respective task 
organizers. The objective is to evaluate the effec-
tiveness of the system in terms of identifying se-
mantic relations between pair of nominals. The 
rule-based baseline system is evaluated against the 
whole training corpus. But, for in-house experi-
ments regarding CRF based framework, the devel-
opment data is prepared by randomly selecting 500 
sentences from the 8000 training sentences. Rest 
7500 sentences are used for training of the CRF-
model. The format of one example entry in training 
file is as follows.  
"The system as described above has its greatest 
application in an arrayed <e1>configuration</e1> 
of antenna <e2>elements</e2>."  
Component-Whole (e2, e1)  
Comment: Not a collection: there is structure 
here, organisation. 
    Each of the training sentences is annotated by 
the paired nominals tagged as <e1> and <e2>. 
The relation of the paired nominals and a comment 
portion describing the detail of the input type fol-
lows the input sentence. 
The sentences are filtered and passed through 
Stanford Dependency Parser (Marneffe et al, 
2006) to identify direct as well as transitive de-
pendencies between the nominals. The direct de-
pendency is identified based on the simultaneous 
presence of both nominals, <e1> as well as <e2> 
in the same dependency relation whereas the tran-
sitive dependencies are verified if <e1> and <e2> 
are connected via one or more intermediate de-
pendency relations.  
Each of the sentences is passed through a Stan-
ford Named Entity Recognizer (NER)1 for identi-
fying the named entities. The named entities are 
the useful hints to separately identify the relations 
like Entity-Origin and Entity-Destination from 
other relations as the Origin and Destination enti-
ties are tagged by the NER frequently than other 
entities. 
Different seed lists are prepared for different 
types of verbs. For example, the lists for causal 
                                                          
1  http://nlp.stanford.edu/software/CRF-NER.shtml 
and motion verbs are developed by processing the 
XML files of English VerbNet (Kipper-Schuler, 
2005). The list of the causal and motion verbs are 
prepared by collecting the member verbs if their 
corresponding class contain the semantic type  
?CAUSE? or ?MOTION?. The other verb lists are 
prepared manually by reviewing the frequency of 
verbs in the training corpus. The WordNet stem-
mer is used to identify the root forms of the verbs.   
3 Baseline Model 
The baseline model is developed based on the 
similarity clues present in the phrasal pattern con-
taining verbs and prepositions. Different rules are 
identified separately for the nine different rela-
tions. A few WordNet features such as hypernym, 
meronym, distance and Common-Parents are 
added into the rule-based baseline model. Some of 
the relation specific rules are mentioned below. 
For example, if any of the nominals contain 
their meronym property as ?whole? and if the hy-
pernym tree for one of the nominals contains the 
word ?whole?, the relation is identified as a Com-
ponent-Whole relation.   But, the ordering of the 
nominals <e1> and <e2> is done based on the 
combination of ?has?, ?with? and ?of? with other 
word level components.  
The relations Cause-Effect, Entity-Destination 
are identified based on the causal verbs (cause, 
lead etc.) and motion verbs (go, run etc.) respec-
tively. One of the main criteria for extracting these 
relations is to verify the presence of causal and 
motion verbs in between the text segment of <e1> 
and <e2>. Different types of specific relaters (as, 
because etc.) are identified from the text segment 
as well. It is observed that such specific causal re-
laters help in distinguishing other relations from 
Cause-Effect.  
If one of the nominals is described as instrument 
type in its hypernym tree, the corresponding rela-
tion is identified as Instrument-Agency but the base 
level filtering criterion is applied if both the nomi-
nals belong to instrument type. On the other hand, 
if any of the nominals belong to the hypernym tree 
as content or container or hold type, it returns the 
relation Content-Container as a probable answer. 
Similarly, if both of them belong to the same type, 
the condition is fixed as false criterion for that par-
ticular category. The nominals identified as the 
part of collective nouns and associated with 
207
phrases like "of", "in", "from" between <e1> and 
<e2> contain the relation of Member-Collection. 
The relations e.g. Message-Topic uses seed list of 
verbs that satisfy the communication type in the 
hypernym tree and Product-Producer relation con-
cerns the hypernym feature as Product type. 
But, the identification of the proper ordering of 
the entities in the relation, i.e., whether the relation 
is valid between <e1, e2> or <e2, e1> is done by 
considering the passive sense of the sentence with 
the help of the keyword ?by? as well as by some 
passive dependency relations.  
The evaluation of the rule-based baseline sys-
tem on the 8000 training data gives an average F1-
score of 22.45%. The error analysis has shown that 
use of lexical features only is not sufficient to ana-
lyze the semantic relation between two nominals 
and the performance can be improved by adopting 
strategies for differentiating the nominals of a par-
ticular pair. 
4 CRF-based Model 
To improve the baseline system performance, 
CRF-based machine learning framework 
(McCallum et al, 2001) is considered for classify-
ing the semantic relations that exist among the or-
dered pair of nominals. Identification of appropri-
ate features plays a crucial role in any machine-
learning framework. The following features are 
identified heuristically by manually reviewing the 
corpus and based on the frequency of different 
verbs in different relations. 
? 11 WordNet features (Synset, Synonym, 
Gloss, Hyponym, Nominalization, Holo-
nym, Common-parents, WordNet distance, 
Sense ID, Sense count, Meronym) 
? Named Entities (NE) 
? Direct Dependency 
? Transitive Dependency 
? 9 separate verb list containing relation spe-
cific verbs, each for 9 different semantic 
relations  
Different singleton features and their combinations 
are generated from the training corpus. Instead of 
considering the whole sentence as an input to the 
CRF-based system, only the pairs of nominals are 
passed for classification. The previous and next 
token of the current token with respect to each of 
the relations are added in the template to identify 
their co-occurrence nature that in turn help in the 
classification process. Synsets containing synony-
mous verbs of the same and different senses are 
considered as individual features.   
4.1 Feature Analysis  
The importance of different features varies accord-
ing to the genre of the relations. For example, the 
Common-parents WordNet feature plays an effec-
tive role in identifying the Content-Container and 
Product-Producer relations. If the nominals in a 
pair share a common Sense ID and Sense Count  
then this is considered as a feature. The combina-
tion of multiple features in comparison with a sin-
gle feature generally shows a reasonable perform-
ance enhancement of the present classification sys-
tem. Evaluation on the development data for the 
various feature combinations has shown that the 
nominalization feature effectively performs for all 
the relations. WordNet distance feature is used for 
capturing the relations like Content-Container and 
Component-Whole. The direct and transitive de-
pendency syntactic features contribute in identify-
ing the relation as well as identify the ordering of 
the entities <e1> and <e2> in the relation. 
The Named-Entity (NE) relation plays an impor-
tant role in distinguishing the relations, e.g., Entity-
Origin and Entity-Destination from other relations. 
The person tagged NEs have been excluded from 
the present task as such NEs are not present in the  
Entity-Origin and Entity-Destination relations. It 
has been observed that the relation specific verbs 
supply useful clues to the training phrase for dif-
ferentiating relations among nominals.   
The system is trained on 7500 sentences and the 
evaluation is carried out on 500 development sen-
tences achieving an F1-Score of 57.56% F1-Score. 
The tuning on the development set has been carried 
out based on the performance produced by the 
individual features that effectively contains 
WordNet relations. In addition to that, the 
combination of dependency features with verb 
feature plays an contributory role on the system 
evaluation results. 
208
Table 1: Precision, Recall and F1-scores (in %) of semantic relations in (9+1) way directionality-based evaluation 
 
5 Experimental Results 
The active feature list is prepared after achieving 
the best possible F1-score of 61.82% on the devel-
opment set of 500 sentences. The final training of 
the CRF-based model is carried out on four differ-
ent sets containing 1000, 2000, 4000 and 8000 sen-
tences. These four training sets are prepared by 
extracting sentences from the beginning of the 
training corpus and the final evaluation is carried 
out on 2717 test sentences as provided by the or-
ganizers. The results on the four test sets termed as 
TD1, TD2, TD3 and TD4 are shown in Table 1. 
The error analysis is done based on the information 
present in the confusion matrices. The fewer occur-
rence of Entity-Destination (e2, e1) instance in the 
training corpus plays the negative role in identify-
ing the relation. Mainly, the strategy used for as-
signing the order among the entities, i.e., either 
<e1, e2> or <e2, e1> in the already identified re-
lations is the main cause of errors of the system. 
The Entity-Origin, Product-Producer and Mes-
sage-Topic relations suffer from overlapping prob-
lem with other relations. Each of the tested nomi-
nal pairs is tagged with more than one relation. 
But, selecting the first output tag produced by CRF 
is considered as the final relational tag for each of 
the nominal pairs. Hence, a distinguishing strategy 
needs to be adopted for fine-grained selection.  
6 Conclusion and Future Task 
In our approach to automatic classification of se-
mantic relations between nominals, the system 
achieves its best performance using the lexical fea-
ture such as nominalization of WordNet and syn-
tactic information such as dependency relations. 
These facts lead us to conclude that semantic fea-
tures from WordNet, in general, play a key role in 
the classification task. The present system aims for 
assigning class labels to discrete word level entities 
but the context feature is not taken into considera-
tion. The future task is to evaluate the performance 
of the system by capturing the context present be-
tween the pair of nominals.  
References  
Andrew McCallum, Fernando Pereira and John 
Lafferty. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and labeling Se-
quence Data. ICML-01, 282 ? 289. 
George A. Miller. 1990. WordNet: An on-line lexical 
database. International Journal of Lexicography, 
3(4): 235?312. 
Karin Kipper-Schuler. 2005. VerbNet. A broad-
coverage, comprehensive verb lexicon. Ph.D. thesis, 
University of Pennsylvania, Philadelphia, PA. 
Marie-Catherine de Marneffe, Bill MacCartney, and 
Christopher D. Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 
(LREC 2006). 
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, 
Preslav Nakov, Diarmuid ?O S?eaghdha, Sebastian 
Padok , Marco Pennacchiotti, Lorenza Romano, Stan 
Szpakowicz. 2010. SemEval-2010 Task 8: Multi-
Way Classification of Semantic Relations Between 
Pairs of Nominals. 5th SIGLEX Workshop. 
TD1 TD2 TD3 TD4 Relations 
Prec. Recall F1 Prec. Recall F1 Prec. Recall F1 Prec. Recall F1 
Cause-Effect 76.33 65.85 70.70 78.55 65.85 71.64 79.86 68.90 73.98 79.26 72.26 75.60
Component-Whole 49.25 31.41 38.36 48.76 37.82 42.60 50.77 42.31 46.15 58.40 49.04 53.31
Content-Container 31.35 30.21 30.77 37.93 34.38 36.07 40.65 32.81 36.31 51.15 34.90 41.49
   Entity-Destination 37.58 62.67 46.98 43.43 63.36 51.53 43.09 63.01 51.18 
 
47.07 60.62 52.99
Entity-Origin 62.50 46.51 53.33 61.95 49.22 54.86 60.18 52.71 56.20 64.02 53.10 58.05
Instrument-Agency 19.46 23.08 21.11 21.18 27.56 23.96 26.43 23.72 25.00 32.48 24.36 27.84
Member-Collection 50.97 67.81 58.20 54.82 70.82 61.80 59.93 72.53 65.63 66.80 71.67 69.15
Message-Topic 41.70 41.38 41.54 50.23 42.15 45.83 52.81 46.74 49.59 57.78 49.81 53.50
Product-Producer 52.94 7.79 13.58 48.94 9.96 16.55 59.09 16.88 26.26 53.17 29.00 37.54
Other 21.10 27.09 23.72 24.48 33.70 28.36 26.28 37.44 30.88 26.64 42.07 32.62
Average F1 score 42.62 44.98 47.81 52.16 
209
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 345?350,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
JU_CSE_TEMP: A First Step towards Evaluating Events, Time Ex-
pressions and Temporal Relations 
Anup Kumar Kolya1, Asif Ekbal2 and Sivaji Bandyopadhyay3 
 
1,3Department of Computer Science and Engineering, Jadavpur University,  
Kolkata-700032, India 
2Department of Computational Linguistics, Heidelberg University,  
Heidelberg-69120, Germany 
Email: anup.kolya@gmail.com1, asif.ekbal@gmail.com2  
and sivaji_cse_ju@yahoo.com3  
 
Abstract 
Temporal information extraction is a 
popular and interesting research field in 
the area of Natural Language Processing 
(NLP). In this paper, we report our works 
on TempEval-2 shared task. This is our 
first participation and we participated in 
all the tasks, i.e., A, B, C, D, E and F. We 
develop rule-based systems for Tasks A 
and B, whereas the remaining tasks are 
based on a machine learning approach, 
namely Conditional Random Field 
(CRF). All our systems are still in their 
development stages, and we report the 
very initial results. Evaluation results on 
the shared task English datasets yield the 
precision, recall and F-measure values of 
55%, 17% and 26%, respectively for 
Task A and 48%, 56% and 52%, respec-
tively for Task B (event recognition).  
The rest of tasks, namely C, D, E and F 
were evaluated with a relatively simpler 
metric: the number of correct answers di-
vided by the number of answers. Experi-
ments on the English datasets yield the 
accuracies of 63%, 80%, 56% and 56% 
for tasks C, D, E and F, respectively.        
1 Introduction 
Temporal information extraction is, nowadays, a 
popular and interesting research area of Natural 
Language Processing (NLP). Generally, events 
are described in different newspaper texts, sto-
ries and other important documents where 
events happen in time and the temporal location 
and ordering of these events are specified. One 
of the important tasks of text analysis clearly re-
quires identifying events described in a text and 
locating these in time. This is also important in a 
wide range of NLP applications that include 
temporal question answering, machine transla-
tion and document summarization.  
   In the literature, temporal relation identifica-
tion based on machine learning approaches can 
be found in Boguraev et el. (2005), Mani et al 
(2006), Chambers et al (2007) and some of the 
TempEval 2007 participants (Verhagen et al, 
2007). Most of these works tried to improve 
classification accuracies through feature engi-
neering. The performance of any machine learn-
ing based system is often limited by the amount 
of available training data. Mani et al (2006) in-
troduced a temporal reasoning component that 
greatly expands the available training data. The 
training set was increased by a factor of 10 by 
computing the closure of the various temporal 
relations that exist in the training data. They re-
ported significant improvement of the classifica-
tion accuracies on event-event and event-time 
relations. Their experimental result showed the 
accuracies of 62.5%-94.95% and 73.68%-
90.16% for event-event and event-time relations, 
respectively. However, this has two shortcom-
ings, namely feature vector duplication caused 
by the data normalization process and the unreal-
istic evaluation scheme.  The solutions to these 
issues are briefly described in Mani et al (2007).  
In TempEval 2007 task, a common standard da-
taset was introduced that involves three temporal 
relations. The participants reported F-measure 
scores for event-event relations ranging from 
42% to 55% and for event-time relations from 
73% to 80%. Unlike (Mani et al, 2007; 2006), 
event-event temporal relations were not dis-
course-wide (i.e., any pair of events can be tem-
porally linked) in TempEval 2007. Here, the 
event-event relations were restricted to events 
within two consecutive sentences. Thus, these 
two frameworks produced highly dissimilar re-
345
sults for solving the problem of temporal relation 
classification.  
   In order to apply various machine learning al-
gorithms, most of the authors formulated tempo-
ral relation as an event paired with a time or an-
other event and translated these into a set of fea-
ture values. Some of the popularly used machine 
learning techniques were Naive-Bayes, Decision 
Tree (C5.0), Maximum Entropy (ME) and Sup-
port Vector Machine (SVM). Machine learning 
techniques alone cannot always yield good accu-
racies. To achieve reasonable accuracy, some 
researchers (Mao et al, 2006) used hybrid ap-
proach. The basic principle of hybrid approach is 
to combine the rule-based component with ma-
chine learning.  It has been shown in (Mao et al, 
2006) that classifiers make most mistakes near 
the decision plane in feature space. The authors 
carried out a series of experiments for each of the 
three tasks on four models, namely naive-Bayes, 
decision tree (C5.0), maximum entropy and sup-
port vector machine. The system was designed in 
such a way that they can take the advantage of 
rule-based as well as machine learning during 
final decision making. But, they did not explain 
exactly in what situations machine learning or 
rule based system should be used given a particu-
lar instance. They had the option to call either 
component on the fly in different situations so 
that they can take advantage of the two empirical 
approaches in an integrated way. 
The rest of the paper is structured as follows. 
We present very brief descriptions of the differ-
ent tasks in Section 2. Section 3 describes our 
approach in details with rule-based techniques 
for tasks A and B in Subsection 3.1, CRF based 
techniques in Subsection 3.2 for tasks C, D, E 
and F, and features in Subsection 3.3. Detailed 
evaluation results are reported in Section 4. Fi-
nally, Section 5 concludes the paper with a direc-
tion to future works.  
2 Task Description 
The main research in this area involves identifi-
cation of all temporal referring expressions, 
events and temporal relations within a text. The 
main challenges involved in this task were first 
addressed during TempEval-1 in 2007 (Verhagen 
et al, 2007). This was an initial evaluation exer-
cise based on three limited tasks that were con-
sidered realistic both from the perspective of as-
sembling resources for development and testing 
and from the perspective of developing systems 
capable of addressing the tasks. In TempEval 
2007, following types of event-time temporal 
relations were considered: Task A (relation be-
tween the events and times within the same sen-
tence), Task B (relation between events and 
document creation time) and Task C (relation 
between verb events in adjacent sentences). The 
data sets were based on TimeBank, a hand-built 
gold standard of annotated texts using the Ti-
meML markup scheme1. The data sets included 
sentence boundaries, timex3 tags (including the 
special document creation time tag), and event 
tags. For tasks A and B, a restricted set of events 
was used, namely those events that occur more 
than 5 times in TimeBank. For all three tasks, the 
relation labels used were before, after, overlap, 
before-or-overlap, overlap-or-after and vague. 
Six teams participated in the TempEval tasks. 
Three of the teams used statistics exclusively, 
one used a rule-based system and the other two 
employed a hybrid approach. For task A, the 
range of F-measure scores were from 0.34 to 
0.62 for the strict scheme and from 0.41 to 0.63 
for the relaxed scheme. For task B, the scores 
were from 0.66 to 0.80 (strict) and 0.71 to 0.81 
(relaxed). Finally, task C scores range from 0.42 
to 0.55 (strict) and from 0.56 to 0.66 (relaxed). 
   In TempEval-2, the following six tasks were 
proposed:  
 A:  The main task was to determine the extent of 
the time expressions in a text as defined by the 
TimeML timex3 tag. In addition, values of the 
features type and val had to be determined. The 
possible values of type are time, date, duration, 
and set; the value of val is a normalized value as 
defined by the timex2 and timex3 standards. 
B. Task was to determine the extent of the events 
in a text as defined by the TimeML event tag. In 
addition, the values of the features tense, aspect, 
polarity, and modality had to be determined. 
C. Task was to determine the temporal relation 
between an event and a time expression in the 
same sentence. 
D. Temporal relation between an event and the 
document creation time had to be determined. 
E. Temporal relation between two main events in 
consecutive sentences had to be determined.  
F. Temporal relation between two events, where 
one event syntactically dominates the other 
event.  
     In our present work, use handcrafted rules for 
Task A and Task B. All the other tasks, i.e., C, 
D, E and F are developed based on the well 
known statistical algorithm, Conditional Random 
                                                 
1www.timeml.org for details on TimeML  
346
Field (CRF). For CRF, we use only those fea-
tures that are available in the training data. All 
the systems are evaluated on the TempEval-
2 shared task English datasets. Evaluation results 
yield the precision, recall and F-measure values 
of 55%, 17% and 26%, respectively for Task A 
and 48%, 56% and 52%, respectively for Task B. 
Experiments on the other tasks demonstrate the 
accuracies of 63%, 80%, 56% and 56% for C, D, 
E and F, respectively.   
3 Our Approach  
In this section, we present our systematic ap-
proach for evaluating events, time expressions 
and temporal relations as part of our first partici-
pation in the TempEval shared task. We partici-
pated in all the six tasks of TempEval-2. Rule-
based systems are developed using a preliminary 
handcrafted set of rules for tasks A and B. We 
use machine learning approach, namely CRF for 
solving the remaining tasks, i.e., C, D, E and F.  
 
3.1 Rules for Task A and Task B 
We manually identify a set of rules studying the 
various features available in the training data. 
There were some exceptions to these rules. How-
ever, a rule is used if it is found to be correct 
most of the time throughout the training data. It 
is to be noted that these are the very preliminary 
rules, and we are still working on finding out 
more robust rules. Below, we present the rules 
for tasks A and B.  
 
Task A. The time expression is identified by de-
fining appropriate regular expression. The regu-
lar expressions are based on several entities that 
denote month names, year, weekdays and the 
various digit expressions. We also use a list of 
keywords (e.g., day, time, AM, PM etc.) that de-
note the various time expressions. The values of 
various attributes (e.g., type and value) of time 
expressions are computed by some simple tem-
plate matching algorithms.  
 
Task B. In case of Task B, the training data is 
initially passed through the Stanford PoS tagger2. 
We consider the tokens as the events that are 
tagged with POS tags such as VB, VBG, VBN, 
VBP, VBZ and VBD, denoting the various verb 
expressions. Values of different attributes are 
computed as follows.  
                                                 
2 http://nlp.stanford.edu/software/tagger.shtml 
 
a. Tense: A manually augmented suffix list such 
as: "ed","d","t" etc. is used to capture the proper 
tense of any event verb from surface level ortho-
graphic variations. 
b. Aspect: The Tense-Aspect-Modality (TAM) 
for English verbs is generally associated with 
auxiliaries. A list is manually prepared. Any oc-
currence of main verb with continuous aspect 
leads to search for the adjacent previous auxil-
iary and rules are formulated to extract TAM 
relation using the manually generated checklist. 
A separate list of auxiliaries is prepared and suc-
cessfully used for detection of progressive verbs.  
c. Polarity: Verb-wise polarity is assigned by the 
occurrence of previous negation words. If any 
negation word appears before any event verb 
then the resultant polarity is negative; otherwise, 
the verb considered as positive by default. 
d. Modality: We prepare a manual list that con-
tains the words such as: may, could, would etc. 
The presence of these modal auxiliaries gives 
modal tag to the targeted verb in a sentence oth-
erwise it is considered a non-modal. 
e. Class: We select ?occurrence? to be class val-
ue by default.  
 
3.2 Machine Learning Approach for Tasks 
C, D, E and F 
 
For tasks C-F, we use a supervised machine 
learning approach that is based on CRF. We con-
sider the temporal relation identification task as a 
pair-wise classification problem in which the 
target pairs?a TIMEX3 tag and an EVENT?are 
modelled using CRF, which can include arbitrary 
set of features, and still can avoid overfitting in a 
principled manner.  
 
Introduction to CRF.  CRF (Lafferty et al, 
2001), is used to calculate the conditional prob-
ability of values on designated output nodes 
given values on other designated input nodes. 
The conditional probability of a state sequence 
1, 2, ..., TS s s s=<
1 2,O o
>  given an observation se-
quence , ....., )To o=<  is calculated as: 
1 ,
1 1
1
( | ) exp( ( , , ))
T K
k k t t
o t k
P s o f s s o t
Z
??
= =
= ?? ?
)
                                 
where, 1 ,( , ,k t tf s s o t?
k
is a feature function 
whose weight ? is to be learned via training. 
The values of the feature functions may range 
between .....? ? + ? , but typically they are 
347
binary. To make all conditional probabilities sum 
up to 1, we must calculate the normalization 
factor, 
0
1 1
exp( ( , , ))
T K
s k k t
t k
1 ,tZ f s s o t? ?
= =
= ? ? ? ,                                             
which, as in HMMs, can be obtained efficiently 
by dynamic programming. 
   To train a CRF, the objective function to be 
maximized is the penalized log-likelihood of the 
state sequences given the observation sequence: 
2
( ) ( )
2
1
log( ( | ))
2
N
i i k
i
L P s o
1
K
k
?
?? ?==? =??
>
,                                         
where, { } is the labeled training da-
ta. The second sum corresponds to a zero-mean, 
( ) ( ),i io s<
2? -variance Gaussian prior over parameters, 
which facilitates optimization by making the li-
kelihood surface strictly convex.  
  CRFs generally can use real-valued functions 
but it is often required to incorporate the binary 
valued features. A feature function 
1 ,( , ,k t t )f s s o t? has a value of 0 for most cases 
and is only set to  1, when 1,t ts s?  are certain 
states and the observation has certain properties. 
Here, we set parameters ?  to maximize the pe-
nalized log-likelihood using Limited-memory 
BFGS (Sha and Pereira, 2003) a quasi-Newton 
method that is significantly more efficient, and 
which results in only minor changes in accuracy 
due to changes in ? . 
   We use the OpenNLP C++ based CRF++ pack-
age 3 , a simple, customizable, and open source 
implementation of CRF for segmenting /labeling 
sequential data.  
 
3.3 Features of Tasks C, D, E and F 
 
We extract the gold-standard TimeBank features 
for events and times in order to train/test the 
CRF. In the present work, we mainly use the 
various combinations of the following features:  
 
(i). Part of Speech (POS) of event terms: It de-
notes the POS information of the event. The fea-
tures values may be either of ADJECTIVE, 
NOUN, VERB, and PREP. 
 (ii). Event Tense: This feature is useful to cap-
ture the standard distinctions among the gram-
matical categories of verbal phrases. The tense 
attribute can have values, PRESENT, PAST, 
                                                 
3http://crfpp.sourceforge.net  
FUTURE, INFINITIVE, PRESPART, PAST-
PART, or NONE. 
 (iii). Event Aspect: It denotes the aspect of the 
events. The aspect attribute may take values, 
PROGRESSIVE, PERFECTIVE and PERFEC-
TIVE PROGRESSIVE or NONE. 
(iv). Event Polarity: The polarity of an event 
instance is a required attribute represented by the 
boolean attribute, polarity. If it is set to ?NEG?, 
the event instance is negated.  If it is set to ?POS? 
or not present in the annotation, the event in-
stance is not negated. 
(v). Event Modality: The modality attribute is 
only present if there is a modal word that modi-
fies the instance. 
(vi). Event Class: This is denoted by the 
?EVENT? tag and used to annotate those ele-
ments in a text that mark the semantic events 
described by it. Typically, events are verbs but 
can be nominal also. It may belong to one of the 
following classes:  
 REPORTING: Describes the action of a person 
or an organization declaring something, narrating 
an event, informing about an event, etc.  For ex-
ample, say, report, tell, explain, state etc. 
 PERCEPTION: Includes events involving the 
physical perception of another event. Such 
events are typically expressed by verbs like: see, 
watch, glimpse, behold, view, hear, listen, over-
hear etc. 
ASPECTUAL: Focuses on different facets of 
event history. For example, initiation, reinitia-
tion, termination, culmination, continuation etc. 
 I_ACTION: An intentional action. It introduces 
an event argument which must be in the text ex-
plicitly describing an action or situation from 
which we can infer something given its relation 
with the I_ ACTION. 
I_STATE: Similar to the I_ACTION class. This 
class includes states that refer to alternative or 
possible words, which can be introduced by sub-
ordinated clauses, nominalizations, or untensed 
verb phrases (VPs). 
 STATE: Describes circumstances in which 
something obtains or holds true. 
 Occurrence: Includes all of the many other 
kinds of events that describe something that hap-
pens or occurs in the world. 
(vii). Type of temporal expression: It repre-
sents the temporal relationship holding between 
events, times, or between an event and a time of 
the event.  
(viii). Event Stem:  It denotes the stem of the 
head event.  
348
(ix). Document Creation Time: The document 
creation time of the event.  
4 Evaluation Results 
Each of the tasks is evaluated with the Tem-
pEval-2 shared task datasets. 
  
4.1 Evaluation Scheme 
 
For the extents of events and time expressions 
(tasks A and B), precision, recall and the F-
measure are used as evaluation metrics, using the 
following formulas: 
Precision (P) = tp/ (tp + fp) 
Recall (R) = tp/ (tp + fn) 
F-measure = 2 *(P * R)/ (P + R) 
   Where, tp is the number of tokens that are part 
of an extent in both keys and response,  
fp is the number of tokens that are part of an ex-
tent in the response but not in the key, and  
fn is the number of tokens that are part of an ex-
tent in the key but not in the response. 
  An even simpler evaluation metric similar to 
the definition of ?accuracy? is used to evaluate 
the attributes of events and time expressions (the 
second part of tasks, A and B) and for relation 
types (tasks C through F). The metric, henceforth 
referred to as ?accuracy?, is defined as below:  
    Number of correct answers/ Number of an-
swers present in the test data  
 
4.2 Results 
 
For tasks A and B, we identify a set of rules from 
the training set and apply them on the respective 
test sets.  
   The tasks C, D, E and F are based on CRF. We 
develop a number of models based on CRF using 
the different features included into it. A feature 
vector consisting of the subset of the available 
features as described in Section 2.3 is extracted 
for each of <event, timex>, <event, DCT>, 
<event, event> and <event, event> pairs in tasks 
C, D, E and F, respectively. Now, we have a 
training data in the form ( , , where,  is 
the ith pair along with its feature vector and  is 
it?s corresponding TempEval relation class. 
Models are built based on the training data and 
the feature template. The procedure of training is 
summarized below: 
)i iW T iW
iT
1. Define the training corpus, C. 
2. Extract the corresponding relation from 
the training corpus. 
3. Create a file of candidate features, in-
cluding lexical features derived from the 
training corpus. 
4. Define a feature template.  
5. Compute the CRF weights ?k for every fK 
using the CRF toolkit with the training 
file and feature template as input. 
  During evaluation, we consider the following 
feature templates for the respective tasks:  
 
(i) Task C: Feature vector consisting of current 
token, polarity, POS, tense, class and value; 
combination of token and type, combination of 
tense and value of the current token, combination 
of aspect and type of current token, combination 
of aspect, value and type of the current token.      
(ii) Task D: Feature vector consisting of current 
token and POS; combination of POS and tense of 
the current token, combination of polarity and 
POS of the current token, combination of POS 
and aspect of current token, combination of po-
larity and POS of current token, combination of 
POS, tense and aspect of the current token.      
(iii). Task E: Current token, combination of 
event-class and event-id of the current token, 
combination of POS tags of the pair of events, 
combination of (tense, aspect) values of the event 
pairs. 
(iv). Task F: Current token, combination of POS 
tags of the pair of events, combination of tense 
values of the event pairs, combination of the as-
pect values of the event pairs, combination of the 
event classes of the event pairs. 
  Experimental results of tasks A and B are re-
ported in Table 1 for English datasets. The re-
sults for task A, i.e., recognition and normaliza-
tion of time expressions, yield the precision, re-
call and F-measure values of 55%, 17% and 
26%, respectively. For task B, i.e., event recogni-
tion, the system yields precision, recall and F-
measure values of 48%, 56% and 52%, respec-
tively. Event attribute identification shows the 
accuracies of 98%, 98%, 30%, 95% and 53% for 
polarity, mood, modality, tense, aspect and class, 
respectively. These systems are the baseline 
models, and the performance can further be im-
proved with a more carefully handcrafted set of 
robust rules. In further experiments, we would 
also like to apply machine learning methods to 
these problems.  
 
 
 
349
Task  precision 
(in %)  
recall   
(in %) 
F-measure  
(in %) 
A 55% 17% 26% 
B 48% 56% 52% 
 
Table 1. Experimental results on tasks A and B 
 
  Evaluation results on the English datasets for 
tasks C, D, E and F are presented in Table 2. Ex-
periments show the accuracies of 63%, 80%, 
56% and 56% for tasks C, D, E and F, respec-
tively. Results show that our system performs 
best for task D, i.e., relationships between event 
and document creation time. The system 
achieves an accuracy of 63% for task C that finds 
the temporal relation between an event and a time 
expression in the same sentence. The system per-
forms quite similarly for tasks E and F. It is to be 
noted that there is still the room for performance 
improvement. In the present work, we did not 
carry out sufficient experiments to identify the 
most suitable feature templates for each of the 
tasks. In future, we would experiment after se-
lecting a development set for each task; and find 
out appropriate feature template depending upon 
the performance on the development set.  
 
 
Task  Accuracy (in %) 
C 63%  
D 80% 
E 56% 
F 56% 
 
Table 2. Experimental results on tasks C, D, E 
and F 
   
5 Conclusion and Future Works 
In this paper, we report very preliminary results 
of our first participation in the TempEval shared 
task. We participated in all the tasks of Tem-
pEval-2, i.e., A, B, C, D, E and F for English. 
We develop the rule-based systems for tasks A 
and B, whereas the remaining tasks are based on 
a machine learning approach, namely CRF. All 
our systems are still in their development stages. 
Evaluation results on the shared task English 
datasets yield the precision, recall and F-measure 
values of 55%, 17% and 26%, respectively for 
Task A and 48%, 56% and 52%, respectively for 
Task B (event recognition).  Experiments on the 
English datasets yield the accuracies of 63%, 
80%, 56% and 56% for tasks C, D, E and F, re-
spectively. 
  Future works include identification of more 
precise rules for tasks A and B. We would also 
like to experiment with CRF for these two tasks.  
We would experiment with the various feature 
templates for tasks C, D, E and F. Future works 
also include experimentations with other ma-
chine learning techniques like maximum entropy 
and support vector machine.          
References  
Boguraev, B. and R. K. Ando. 2005. TimeML 
Compliant Text Analysis for Temporal Rea-
soning. In Proceedings of Nineteenth Interna-
tional Joint Conference on Artificial Intelli-
gence (IJCAI-05), Edinburgh, Scotland, Au-
gust, pages 997?1003. 
Chambers, N., S., Wang, and D., Jurafsky. , 
2007. Classifying Temporal Relations between 
Events. In Proceedings of the ACL 2007 Demo 
and Poster Sessions, Prague, Czech Republic, 
June, pages 173?176. 
 Lafferty, J., McCallum, A., and Pereira, F. 
Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Se-
quence Data. In Proceedings of 18th Interna-
tional Conference on Machine Learning, 
2001. 
Mani, I., B., Wellner, M., Verhagen, and J. 
Pustejovsky. 2007. Three Approaches to 
Learning TLINKs in TimeML. Technical Re-
port CS-07-268, Computer Science Depart-
ment, Brandeis University, Waltham, USA. 
Mani, I., Wellner, B., Verhagen, M., Lee C.M.,   
Pustejovsky, J. 2006. Machine Learning of 
Temporal Relation. In Proceedings of the 
COLING/ACL, Sydney, Australia, ACL. 
Mao, T., Li., T., Huang, D., Yang, Y. 2006. Hy-
brid Models for Chinese Named Entity Rec-
ognition. In Proceedings of the Fifth SIGHAN 
Workshop on Chinese Language Processing. 
Sha, F., Pereira, F. 2003. Shallow  Parsing  with  
Conditional Random Fields. In Proceedings of  
HLT-NAACL, 2003. 
Verhagen, M., Gaizauskas, R., Schilder, F., Hep-
ple, M., Katz, G., Pustejovsky, and J.: SemE-
val-2007 Task 15: TempEval Temporal Rela-
tion Identification. 2007. In Proceedings of the 
SemEval-2007, Prague, June 2007, pages 75-
80. 
350
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 571?574,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
JU_CSE_NLP: Multi-grade Classification of Semantic Similarity  
Between Text Pairs 
 
    Snehasis Neogi1, Partha Pakray2, Sivaji Bandyopadhyay1            Alexander Gelbukh 
                           1Computer Science & Engineering Department                        Center for Computing Research 
                           Jadavpur University, Kolkata, India                                 National Polytechnic Institute         
                  2Computer Science & Engineering Department                               Mexico City, Mexico 
                           Jadavpur University, Kolkata, India                                 gelbukh@gelbukh.com 
                        Intern at Xerox Research Centre Europe 
                                          Grenoble, France 
                  {snehasis1981,parthapakray}@gmail.com 
            sbandyopadhyay@cse.jdvu.ac.in 
 
Abstract 
This article presents the experiments car-
ried out at Jadavpur University as part of 
the participation in Semantic Textual Si-
milarity (STS) of Task 6 @ Semantic 
Evaluation Exercises (SemEval-2012). 
Task-6 of SemEval- 2012 focused on se-
mantic relations of text pair. Task-6 pro-
vides five different text pair files to 
compare different semantic relations and 
judge these relations through a similarity 
and confidence score. Similarity score is 
one kind of multi way classification in the 
form of grade between 0 to 5. We have 
submitted one run for the STS task. Our 
system has two basic modules - one deals 
with lexical relations and another deals 
with dependency based syntactic relations 
of the text pair. Similarity score given to a 
pair is the average of the scores of the 
above-mentioned modules. The scores 
from each module are identified using rule 
based techniques. The Pearson Correlation 
of our system in the task is 0.3880. 
1 Introduction 
Task-61 [1] of SemEval-2012 deals with seman-
tic similarity of text pairs. The task is to find the 
similarity between the sentences in the text pair 
(s1 and s2) and return a similarity score and an 
optional confidence score. There are five datasets 
                                                          
1 http://www.cs.york.ac.uk/semeval-2012/task6/ 
in the test data and with tab separated text pairs. 
The datasets are as follows: 
 
? MSR-Paraphrase, Microsoft Research Pa-
raphrase Corpus (750 pairs of sentences.) 
? MSR-Video, Microsoft Research Video De-
scription Corpus (750 pairs of sentences.) 
? SMTeuroparl: WMT2008 development data-
set (Europarl section) (459 pairs of sen-
tences.)  
? SMTnews: news conversation sentence pairs 
from WMT.(399 pairs of sentences.) 
? OnWN: pairs of sentences where the first 
comes from Ontonotes and the second from a 
WordNet definition. (750 pairs of sentences.) 
 
Similarity score ranges from 0 to 5 and confi-
dence score from 0 to 100. An s1-s2 pair gets a 
similarity score of 5 if they are completely 
equivalent. Similarity score 4 is allocated for 
mostly equivalent s1-s2 pair. Similarly, score 3 is 
allocated for roughly equivalent pair. Score 2, 1 
and 0 are allocated for non-equivalent details 
sharing, non-equivalent topic sharing and totally 
different pairs respectively. Major challenge of 
this task is to find the similarity score based simi-
larity for the text pair. Generally text entailment 
tasks refer whether sentence pairs are entailed or 
not: binary classification (YES, NO) [2] or multi-
classification (Forward, Backward, bidirectional 
or no entailment) [3][4]. But multi grade classifi-
cation of semantic similarity assigns a score to 
the sentence pair. Our system considers lexical 
and dependency based syntactic measures for 
semantic similarity. Similarity scores are the ba-
sic average of these module scores. A subsequent 
571
section describes the system architecture. Section 
2 describes JU_NLP_CSE system for STS task. 
Section 3 describes evaluation and experimental 
results. Conclusions are drawn in Section 4.  
2 System Architecture  
The system of Semantic textual similarity task 
has two main modules: one is lexical module and 
another one is dependency parsing based syntac-
tic module. Both these module have some pre-
processing tasks such as stop word removal, co-
reference resolution and dependency parsing etc. 
Figure 1 displays the architecture of the system.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: System Architecture 
2.1 Pre-processing Module 
The system separates the s1-s2 sentence pairs 
contained in the different STS task datasets. 
These separated pairs are then passed through the 
following sub modules: 
i. Stop word Removal: Stop words are removed 
from s1 - s2 sentence pairs. 
ii. Co-reference: Co-reference resolutions are 
carried out on the datasets before passing through 
the TE module. The objective is to increase the 
score of the entailment percentage. A word or 
phrase in the sentence is used to refer to an entity 
introduced earlier or later in the discourse and 
both having same things then they have the same 
referent or co reference. When the reader must 
look back to the previous context, reference is 
called "Anaphoric Reference". When the reader 
must look forward, it is termed "Cataphoric Ref-
erence". To address this problem we used a tool 
called JavaRAP2 (A java based implementation 
of Anaphora Procedure (RAP) - an algorithm by 
Lappin and Leass (1994)). 
iii. Dependency Parsing: Separated s1 ? s2 sen-
tences are parsed using Stanford dependency 
parser3 to produce the dependency relations in 
the texts. These dependency relations are used 
for WordNet based syntactic matching.     
2.2 Lexical Matching Module 
In this module the TE system calculates different 
matching scores such as N ? Gram match, Text 
Similarity, Chunk match, Named Entity match 
and POS match.  
 
i. N-Gram Match module: The N-Gram match 
basically measures the percentage match of the 
unigram, bigram and trigram of hypothesis 
present in the corresponding text. These scores 
are simply combined to get an overall N ? Gram 
matching score for a particular pair.  
 
ii. Chunk Match module: In this sub module 
our system evaluates the key NP-chunks of both 
text (s1) and hypothesis (s2) using NP Chunker 
v1.13 (The University of Sheffield). The hypo-
thesis NP chunks are matched in the text NP 
chunks. System calculates an overall value for 
the chunk matching, i.e., number of text NP 
chunks that match the hypothesis NP chunks. If 
the chunks are not similar in their surface form 
then our system goes for wordnet synonyms 
matching for the words and if they match in 
wordnet synsets information, it will be encoun-
tered as a similar chunk. WordNet [5] is one of 
most important resource for lexical analysis. The 
WordNet 2.0 has been used for WordNet based 
chunk matching. The API for WordNet Search-
ing (JAWS)4 is an API that provides Java appli-
cations with the ability to retrieve data from the 
WordNet synsets. 
iii. Text Similarity Module: System takes into 
consideration several text similarities calculated 
                                                          
2 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html 
3 http://www.dcs.shef.ac.uk/~mark/phd/software/ 
4 http://lyle.smu.edu/~tspell/jaws/index.html 
 
 
572
over the s1-s2 pair. These text similarity values 
are summed up to produce a total score for a par-
ticular s1-s2 pair. Major Text similarity measures 
that our system considers are: 
 
? Cosine Similarity 
? Lavenstine Distance 
? Euclidean Distance 
? MongeElkan Distance 
? NeedlemanWunch Distance 
? SmithWaterman Distance 
? Block Distance 
? Jaro Similarity 
? MatchingCoefficient Distance 
? Dice Similarity 
? OverlapCoefficient 
? QGrams Distance 
 
iv. Named Entity Matching: It is based on the 
detection and matching of Named Entities in the 
s1-s2 pair. Stanford Named Entity Recognizer5 is 
used to tag the named entities in both s1 and s2. 
System simply maps the number of hypothesis 
(s2) NEs present in the text (s1). A score is allo-
cated for the matching. 
 
NE_match = (Number of common NEs in Text 
and Hypothesis) / (Number of NE in Hypothesis). 
 
v. Part ?of ? Speech (POS) Matching: This 
module basically deals with matching the com-
mon POS tags between s1 and s2 sentences. 
Stanford POS tagger6 is used to tag the part of 
speech in both s1 and s2. System matches the 
verb and noun POS words in the hypothesis that 
match in the text. A score is allocated based on 
the number of POS matching. 
 
POS_match = (Number of common verb and 
noun POS in Text and Hypothesis) / (Total num-
ber of verb and noun POS in hypothesis). 
 
System calculates the sum of the entire sub mod-
ule (modules described in section 2.2) scores and 
forms a single percentage score for the lexical 
matching. This score is then compared with some 
predetermined threshold value to assign a final 
lexical score for each pair. If percentage value is 
                                                          
5 http://nlp.stanford.edu/software/CRF-NER.shtml 
6 http://nlp.stanford.edu/software/tagger.shtml 
above 0.80 then lexical score 5 is allocated. If the 
value is between 0.60 to 0.80 then lexical score 4 
is allocated. Similarly, lexical score 3 is allocated 
for percentage score of 0.40 to 0.60 and so on. 
One lexical score is finally generated for each 
text pair.     
2.3. Syntactic Matching Module: 
TE system considers the preprocessed dependen-
cy parsed text pairs (s1 ? s2) and goes for word 
net based matching technique. After parsing the 
sentences, they have some attributes like subject, 
object, verb, auxiliaries and prepositions tagged 
by the dependency parser tag set. System uses 
these attributes for the matching procedure and 
depending on the nature of matching a score is 
allocated to the s1-s2 pair. Matching procedure is 
basically done through comparison of the follow-
ing features that are present in both the text and 
the hypothesis.    
? Subject ? Subject comparison. 
? Verb ? Verb Comparison. 
? Subject ? Verbs Comparison. 
? Object ? Object Comparison. 
? Cross Subject ? Object Comparison. 
? Object ? Verbs Comparison. 
? Prepositional phrase comparison. 
 
Each of these comparisons produces one match-
ing score for the s1-s2 pair that are finally com-
bined with previously generated lexical score to 
generate the final similarity score by taking sim-
ple average of lexical and syntactic matching 
scores. The basic heuristics are as follows: 
(i) If the feature of the text (s1) directly matches 
the same feature of the hypothesis (s2), matching 
score 5 is allocated for the text pair. 
(ii) If the feature of either text (s1) or hypothesis 
(s2) matches with the wordnet synsets of the cor-
responding text (s1) or hypothesis (s2), matching 
score 4 is allocated.     
(iii) If wordnet synsets of the feature of the text 
(s1) match with one of the synsets of the feature 
of the hypothesis (s2), matching score 3 is given 
to the pair. 
(iv) If wordnet synsets of the feature of either 
text (s1) or hypothesis (s2) match with the syn-
sets of the corresponding text (s1) or hypothesis 
(s2) then matching score 2 is allocated for the 
pair. 
573
(v) Similarly if in both the cases match occurs in 
the second level of wordnet synsets, matching 
score 1is allocated. 
(vi) Matching score 0 is allocated for the pair 
having no match in their features. 
After execution of the module, system generates 
some scores. Lexical module generates one lexi-
cal score and wordnet based syntactic matching 
module generates seven matching scores. At the 
final stage of the system all these scores are 
combined and the mean is evaluated on this 
combined score. This mean gives the similarity 
score for a particular s1-s2 pair of different data-
sets of STS task. Optional confidence score is 
also allocated which is basically the similarity 
score multiplied by 10, i.e., if the similarity score 
is 5.22, the confidence score will be 52.2.     
3. Experiments on Dataset and Result  
We have submitted one run in SemEval-2012 
Task 6. The results for Run on STS Test set are 
shown in Table 1. 
 
task6-JU_CSE_NLP-
Semantic_Syntactic_Approach 
Correlations 
ALL    0.3880 
ALLnrm 0.6706 
Mean 0.4111 
MSRpar  0.3427 
MSRvid 0.3549 
SMT-eur 0.4271 
On-WN 0.5298 
SMT-news 0.4034 
Table 1: Results of Test Set 
ALL: Pearson correlation with the gold standard 
for the five datasets and the corresponding rank 
82. 
ALLnrm: Pearson correlation after the system 
outputs for each dataset are fitted to the gold 
standard using least squares and the correspond-
ing rank 86. 
Mean: Weighted mean across the 5 datasets, 
where the weight depends on the number of pairs 
in the dataset and the corresponding rank 76. 
The subsequent rows show the pearson correla-
tion scores for each of the individual datasets. 
 
4. Conclusion 
Our JU_CSE_NLP system for the STS task 
mainly focus on lexical and syntactic approaches. 
There are some limitations in the lexical match-
ing module that shows a correlation that is not 
higher in the range. In case of simple sentences 
lexical matching is helpful for entailment but for 
complex and compound sentences the lexical 
matching module loses its accuracy. Semantic 
graph matching or conceptual graph implementa-
tion can improve the system. That is not consi-
dered in our present work. Machine learning 
tools can be used to learn the system based on the 
features. It can also improve the correlation. In 
future work our system will include semantic 
graph matching and a machine-learning module.  
Acknowledgments 
The work was done under support of the DST 
India-CONACYT Mexico project ?Answer Vali-
dation through Textual Entailment? funded by 
DST, Government of India.  
References  
[1] Eneko Agirre, Daniel Cer, Mona Diab and Aitor 
Gonzalez.  SemEval-2012 Task 6: A Pilot on Se-
mantic Textual Similarity. In Proceedings of the 
6th International Workshop on Semantic Evalua-
tion (SemEval 2012), in conjunction with the First 
Joint Conference on Lexical and Computational 
Semantics (*SEM 2012). (2012) 
[2] Dagan, I., Glickman, O., Magnini, B.: The 
PASCAL Recognising Textual Entailment Chal-
lenge. Proceedings of the First PASCAL Recogniz-
ing Textual Entailment Workshop. (2005). 
[3] H. Shima, H. Kanayama, C.-W. Lee, C.-J. Lin,T. 
Mitamura, S. S. Y. Miyao, and K. Takeda. Over-
view of ntcir-9 rite: Recognizing inference in text. 
In NTCIR-9 Proceedings,2011. 
[4] Pakray, P., Neogi, S., Bandyopadhyay, S., Gel-
bukh, A.: A Textual Entailment System using Web 
based Machine Translation System. NTCIR-9, Na-
tional Center of Sciences, Tokyo, Japan. December 
6-9, 2011. (2011). 
[5] Fellbaum, C.: WordNet: An Electronic Lexical 
Database. MIT Press (1998). 
574
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689?695,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
JU_CSE_NLP: Language Independent Cross-lingual  
Textual Entailment System 
 
Snehasis Neogi1, Partha Pakray2, Sivaji Bandyopadhyay1, 
Alexander Gelbukh3 
1
Computer Science & Engineering Department 
Jadavpur University, Kolkata, India 
2
Computer Science & Engineering Department 
Jadavpur University, Kolkata, India 
Intern at Xerox Research Centre Europe 
Grenoble, France 
3
Center for Computing Research 
National Polytechnic Institute 
Mexico City, Mexico 
{snehasis1981,parthapakray}@gmail.com 
sbandyopadhyay@cse.jdvu.ac.in 
gelbukh@gelbukh.com 
 
Abstract 
This article presents the experiments car-
ried out at Jadavpur University as part of 
the participation in Cross-lingual Textual 
Entailment for Content Synchronization 
(CLTE) of task 8 @ Semantic Evaluation 
Exercises (SemEval-2012). The work ex-
plores cross-lingual textual entailment as a 
relation between two texts in different lan-
guages and proposes different measures 
for entailment decision in a four way clas-
sification tasks (forward, backward, bidi-
rectional and no-entailment). We set up 
different heuristics and measures for eva-
luating the entailment between two texts 
based on lexical relations. Experiments 
have been carried out with both the text 
and hypothesis converted to the same lan-
guage using the Microsoft Bing translation 
system. The entailment system considers 
Named Entity, Noun Chunks, Part of 
speech, N-Gram and some text similarity 
measures of the text pair to decide the en-
tailment judgments. Rules have been de-
veloped to encounter the multi way 
entailment issue. Our system decides on 
the entailment judgment after comparing 
the entailment scores for the text pairs. 
Four different rules have been developed 
for the four different classes of entailment. 
The best run is submitted for Italian ? 
English language with accuracy 0.326. 
1 Introduction 
Textual Entailment (TE) (Dagan and Glick-
man, 2004) is one of the recent challenges of 
Natural Language Processing (NLP). The Task 
8 of SemEval-20121 [1] defines a textual en-
tailment system that specifies two major as-
pects: the task is based on cross-lingual 
corpora and the entailment decision must be 
four ways. Given a pair of topically related text 
fragments (T1 and T2) in different languages, 
the CLTE task consists of automatically anno-
tating it with one of the following entailment 
judgments: 
i. Bidirectional (T1 ->T2 & T1 <- T2): the two 
fragments entail each other (semantic equiva-
lence)  
ii. Forward (T1 -> T2 & T1!<- T2): unidirec-
tional  entailment from T1 to T2 . 
iii. Backward (T1! -> T2 & T1 <- T2): unidirec-
tional entailment from T2 to T1.  
iv. No Entailment (T1! -> T2 & T1! <- T2): 
there is no entailment between T1 and T2. 
CLTE (Cross Lingual Textual Entailment) task 
consists of 1,000 CLTE dataset pairs (500 for 
                                                          
1http://www.cs.york.ac.uk/semeval2012/index.php?id=tasks 
689
training and 500 for test) available for the fol-
lowing language combinations: 
     - Spanish/English (spa-eng)  
     - German/English (deu-eng). 
     - Italian/English (ita-eng)  
     - French/English (fra-eng) 
 
Seven Recognizing Textual Entailment (RTE) 
evaluation tracks have already been held: RTE-1 
in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 
2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, 
RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE 
task produces a generic framework for entail-
ment task across NLP applications. The RTE 
challenges have moved from 2 ? way entailment 
task (YES, NO) to 3 ? way task (YES, NO, 
UNKNOWN). EVALITA/IRTE [9] task is simi-
lar to the RTE challenge for the Italian language. 
So far, TE has been applied only in a monolin-
gual setting. Cross-lingual Textual Entailment 
(CLTE) has been proposed ([10], [11], [12]) as 
an extension of Textual Entailment. In 2010, 
Parser Training and Evaluation using Textual 
Entailment [13] was organized by SemEval-2. 
Recognizing Inference in Text (RITE)2 orga-
nized by NTCIR-9 in 2011 is the first to expand 
TE as a 5-way entailment task (forward, back-
ward, bi-directional, contradiction and indepen-
dent) in a monolingual scenario [14].  
We have participated in RTE-5 [15], RTE-6 
[16], RTE-7 [17], SemEval-2 Parser Training 
and Evaluation using Textual Entailment Task 
and RITE [18]. 
Section 2 describes our Cross-lingual Textual 
Entailment system. The various experiments 
carried out on the development and test data sets 
are described in Section 3 along with the results. 
The conclusions are drawn in Section 4. 
2 System Architecture  
Our system for CLTE task is based on a set of 
heuristics that assigns entailment scores to a text 
pair based on lexical relations. The text and the 
hypothesis in a text pair are translated to the 
same language using the Microsoft Bing ma-
chine translation system. The system separates 
the text pairs (T1 and T2) available in different 
languages and preprocesses them. After prepro-
                                                          
2 http://artigas.lti.cs.cmu.edu/rite/Main_Page 
cessing we have used several techniques such as 
Word Overlaps, Named Entity matching, Chunk 
matching, POS matching to evaluate the sepa-
rated text pairs. These modules return a set of 
score statistics, which helps the system to go for 
multi-class entailment decision based on the 
predefined rules. We have submitted 3 runs for 
each language pair for the CLTE task and there 
are some minor differences in the architectures 
that constitute the 3 runs. The three system ar-
chitectures are described in section 2.1, section 
2.2 and section 2.3. 
2.1 System Architecture 1: CLTE Task 
with  Translated English Text  
The system architecture of Cross-lingual textual 
entailment consists of various components such 
as Preprocessing Module, Lexical Similarity 
Module, Text Similarity Module. Lexical Simi-
larity module again is divided into subsequent 
modules like POS matching, Chunk matching 
and Named Entity matching. Our system calcu-
lates these measures twice once considering T1 
as text and T2 as hypothesis and once T2 as text 
and T1 as hypothesis. The mapping is done in 
both directions T1-to-T2 and T2-to-T1 to arrive 
at the appropriate four way entailment decision 
using a set of rules. Each of these modules is 
now being described in subsequent subsections. 
Figure 1 shows our system architecture where 
the text sentence is translated to English. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: System Architecture 
 
CLTE Task Data 
(T1, T2) 
T1.txt 
T2.txt 
 
Translated in 
Eng.  Using Bing 
Translator 
Preprocessing 
(Stop word removal, 
Co referencing) 
 
N-Gram Module 
Preprocessing 
(Stop word removal, 
Co referencing) 
Chunking Module 
Text Similarity Module Named Entity POS Module 
? Lexical Score (S1) 
S1 
? Lexical Score (S2) 
S1 
If (S1>S2) Then Entailment = ?forward? 
If (S1<S2) Then Entailment = ?backward? 
If (S1=S2) or (abs (S1-S2) <Threshold) Then Entailment = ?bidirectional? 
(fra, ita, deu, 
spa language) 
T1- Text 
T2- Hypothesis 
 
T1 ? Hypothesis 
T2 - Text 
 
If (S1=S2 and (S1=S2) <Threshold) Then Entailment = ?no_entailment? 
(English 
language) 
 
690
2.1.1 Preprocessing Module 
The system separates the T1 and T2 pair from 
the CLTE task data. T1 sentences are in differ-
ent languages (In French, Italian, German and 
Spanish) where as T2 sentences are in English. 
Microsoft Bing translator3 API for Bing transla-
tor (microsoft-translator-java-api-0.4-jar-with-
dependencies.jar) is being used to translate the 
T1 text sentences into English. The translated 
T1 and T2 sentences are passed through the two 
sub modules. 
i. Stop word Removal: Stop words are removed 
from the T1 and T2 sentences. 
ii. Co-reference: Co?reference chains are eva-
luated for the datasets before passing them to the 
TE module. The objective is to increase the en-
tailment score after substituting the anaphors 
with their antecedents. A word or phrase in the 
sentence is used to refer to an entity introduced 
earlier or later in the discourse and both having 
same things then they have the same referent or 
co-reference. When the reader must look back to 
the previous context, co-reference is called 
"Anaphoric Reference". When the reader must 
look forward, it is termed "Cataphoric Refer-
ence". To address this problem we used a tool 
called JavaRAP4 (A java based implementation 
of Anaphora Procedure (RAP) - an algorithm by 
Lappin and Leass (1994)). It has been observed 
that the presence of co ? referential expressions 
are very small in sentence based paradigm.   
2.1.2 Lexical Based Textual Entailment 
(TE) Module 
T1 - T2 pairs are the inputs to the system. The 
TE module is executed once by considering T1 
as text and T2 as hypothesis and again by consi-
dering T2 as text and T1 as hypothesis. The 
overall TE module is a collection of several lex-
ical based sub modules.  
i. N-Gram Match module: The N-Gram match 
basically measures the percentage match of the 
unigram, bigram and trigram of hypothesis 
present in the corresponding text. These scores 
are simply combined to get an overall N ? Gram 
matching score for a particular pair. By running 
                                                          
3 http://code.google.com/p/microsoft-translator-java-api/ 
4 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html 
the module we get two scores, one for T1-T2 
pair and another for T2-T1 pair. 
       
ii. Chunk Similarity module: In this sub mod-
ule our system evaluates the key NP-chunks of 
both text and hypothesis identified using NP 
Chunker v1.15. Then our system checks the 
presence of NP-Chunks of hypothesis in the cor-
responding text. System calculates the overall 
value for the chunk matching, i.e., number of 
text NP-chunks that match with hypothesis NP-
chunks. If the chunks are not similar in their sur-
face form then our system goes for WordNet 
matching for the words and if they match in 
WordNet synsets information, the chunks are 
considered as similar. 
WordNet [19] is one of most important resource 
for lexical analysis. The WordNet 2.0 has been 
used for WordNet based chunk matching. The 
API for WordNet Searching (JAWS)6 is an API 
that provides Java applications with the ability 
to retrieve data from the WordNet database. Let 
us consider the following example taken from 
training data: 
 
T1: Due/JJ to/TO [an/DT error/NN of/IN com-
munication/NN] between/IN [the/DT police/NN] 
? 
T2: On/IN [Tuesday/NNP] [a/DT failed/VBN 
communication/NN] between/IN? 
 
The chunk in T1 [error communication] matches 
with T2 [failed communication] via WordNet 
based synsets information. A weight is assigned 
to the score depending upon the nature of chunk 
matching. 
 
 
 
                   M[i] = Wm[i] * ? / Wc[i] 
Where N= Total number of chunk containing 
hypothesis. 
M[i] = Match Score of the ith  Chunk. 
Wm[i] = Number of words matched in the i
th 
chunk. 
Wc[i] = Total words in the i
th chunk. 
                    1 if surface word matches. 
and ? = 
                ? if matche via WordNet 
                                                          
5 http://www.dcs.shef.ac.uk/~mark/phd/software/ 
6 http://lyle.smu.edu/~tspell/jaws/index.html 
691
System takes into consideration several text si-
milarity measures calculated over the T1-T2 
pair. These text similarity measures are summed 
up to produce a total score for a particular text 
pair. Similar to the Lexical module, text simi-
larity module is also executed for both T1-T2 
and T2-T1 pairs.   
iii. Text Distance Module: The following major 
text similarity measures have been considered 
by our system. The text similarity measure 
scores are added to generate the final text dis-
tance score. 
 
?   Cosine Similarity 
?   Levenshtein Distance 
?   Euclidean Distance 
?   MongeElkan Distance 
?   NeedlemanWunch Distance 
?   SmithWaterman Distance 
?   Block Distance 
?   Jaro Similarity 
?   MatchingCoefficient Similarity 
?   Dice Similarity 
?   OverlapCoefficient 
?   QGrams Distance 
 
iv. Named Entity Matching: It is based on the 
detection and matching of Named Entities in the 
T1-T2 pair. Stanford Named Entity Recognizer7 
(NER) is used to tag the Named Entities in both 
T1 and T2. System simply matches the number 
of hypothesis NEs present in the text. A score is 
allocated for the matching. 
NE_match = (Number of common NEs in Text 
and Hypothesis)/(Number of NEs in Hypothe-
sis). 
v. Part-of-Speech (POS) Matching: This mod-
ule basically deals with matching the common 
POS tags between T1 and T2 pair. Stanford POS 
tagger8 is used to tag the part of speech in both 
T1 and T2. System matches the verb and noun 
POS words in the hypothesis that match in the 
text. A score is allocated based on the number of 
POS matching.  
 
POS_match = (Number of verb and noun                            
POS in Text and Hypothesis)/(Total number of 
verb and noun POS in hypothesis).    
                                                          
7 http://nlp.stanford.edu/software/CRF-NER.shtml 
8 http://nlp.stanford.edu/software/tagger.shtml 
System adds all the lexical matching scores to 
evaluate the total score for a particular T1- T2 
pair, i.e.,  
    Pair1:  (T1 ? Text and T2 ? Hypothesis) 
    Pair2:   (T1 ? Hypothesis and T2 - Text). 
Total lexical score for each pair can be mathe-
matically represented by: 
 
 
where S1 represents the score for the pair with 
T1 as text and T2 as hypothesis while S2 
represents the score from T1 to T2. The figure 2 
shows the sample output values of the TE mod-
ule. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: output values of this module 
 
The system finally compares the above two val-
ues S1 and S2 as obtained from the lexical mod-
ule to go for four-class entailment decision. If 
score S1, i.e., the mapping score with T1 as text 
and T2 as hypothesis is greater than the score 
S2, i.e., mapping score with T2 as text and T1 as 
hypothesis, then the entailment class will be 
?forward?. Similarly if S1 is less than S2, i.e., 
T2 now acts as the text and T1 acts as the hypo-
thesis then the entailment class will be ?back-
ward?. Similarly if both the scores S1 and S2 are 
equal the entailment class will be ?bidirectional? 
(entails in both directions). Measuring ?bidirec-
tional? entailment is much more difficult than 
any other entailment decision due to combina-
tions of different lexical scores. As our system 
produces a final score (S1 and S2) that is basi-
cally the sum over different similarity measures, 
 
692
the tendency of identical S1 ? S2 will be quite 
small. As a result we establish another heuristic 
for ?bidirectional? class. If the absolute value 
difference between S1 and S2 is below the thre-
shold value, our system recognizes the pair as 
?bidirectional? (abs (S1 ? S2) < threshold). This 
threshold has been set as 5 based on observation 
from the training file. If the individual scores S1 
and S2 are below a certain threshold, again set 
based on the observation in the training file, then 
system concludes the entailment class as 
?no_entailment?. This threshold has been set as 
20 based on observation from the training file. 
2.2 System Architecture 2: CLTE Task 
with translated hypothesis  
System Architecture 2 is based on lexical match-
ing between the text pairs (T1, T2) and basically 
measures the same attributes as in the architec-
ture 1. In this architecture, the English hypothe-
sis sentences are translated to the language of 
the text sentence (French, Italian, Spanish and 
German) using the Microsoft Bing Translator. 
The CLTE dataset is preprocessed after separat-
ing the (T1, T2) pairs. Preprocessing module 
includes stop word removal and co-referencing. 
After preprocessing, the system executes the TE 
module for lexical matching between the text 
pairs. This module comprises N-Gram matching, 
Text Similarity, Named Entity Matching, POS 
matching and Chunking. The TE module is ex-
ecuted once with T1 as text and T2 as hypothe-
sis and again with T1 as hypothesis and T2 as 
text. But in this architecture N-Gram matching 
and text similarity modules differ from the pre-
vious architecture. In system architecture 1, the 
N-Gram matching and text similarity values are 
calculated on the English text translated from T1 
(i.e., Text in Spanish, German, French and Ital-
ian languages). In system architecture 2, the Mi-
crosoft Bing translator is used to translate T2 
texts (in English) to different languages (i.e. in 
Spanish, German, French and Italian) and calcu-
late N ? Gram matching and Text Similarity 
values on these (T1 ? newly translated T2) pairs. 
Other lexical sub modules are executed as be-
fore. These lexical matching scores are stored 
and compared according to the heuristic defined 
in section 2.1.    
2.3 System Architecture 3: CLTE task 
using Voting 
The system considers the output of the previous 
two systems (Run 1 from System architecture 1 
and Run 2 from System architecture 2) as input. 
If the entailment decision of both the runs agrees 
then this is output as the final entailment label. 
Otherwise, if they do not agree, the final entail-
ment label will be ?no_entailment?. The voting 
rule can be defined as the ANDing rule where 
logical AND operation of the two inputs are 
considered to arrive at the final evaluation class. 
3 Experiments on Datasets and Results   
Three runs (Run 1, Run 2 and Run 3) for each 
language were submitted for the SemEval-3 
Task 8. The descriptions of submissions for the 
CLTE task are as follows: 
 
? Run1: Lexical matching between text pairs 
(Based on system Architecture ? 1). 
? Run2: Lexical matching between text pairs  
    (Based on System Architecture ? 2). 
? Run3: ANDing Module between Run1 and  
          Run2. (Based on System Architecture ?3). 
 
The CLTE dataset consists of 500 training 
CLTE pairs and 500 test CLTE pairs. The re-
sults for Run 1, Run 2 and Run 3 for each lan-
guage on CLTE Development set are shown in 
Table 1.  
 
Run Name Accuracy 
JU-CSE-NLP_deu-eng_run1 0.284 
JU-CSE-NLP_deu-eng_run2 0.268 
JU-CSE-NLP_deu-eng_run3 0.270 
JU-CSE-NLP_fra-eng_run1 0.290 
JU-CSE-NLP_fra-eng_run2 0.320 
JU-CSE-NLP_fra-eng_run3 0.278 
JU-CSE-NLP_ita-eng_run1 0.302 
JU-CSE-NLP_ita-eng_run2 0.298 
JU-CSE-NLP_ita-eng_run3 0.298 
JU-CSE-NLP_spa-eng_run1 0.270 
JU-CSE-NLP_spa-eng_run2 0.262 
JU-CSE-NLP_spa-eng_run3 0.262 
 
Table 1: Results on Development set 
 
693
The comparison of the runs for different lan-
guages shows that in case of deu-eng language 
pair system architecture ? 1 is useful for devel-
opment data whereas system architecture ? 2 is 
more accurate for test data. For fra-eng language 
pair, system architecture - 2 is more accurate for 
development data whereas voting helps to get 
more accurate results for test data. Similar to the 
deu-eng language pair, ita-eng language pair 
shows same trends, i.e., system architecture ? 1 
is more helpful for development data and system 
architecture ? 2 is more accurate for test data. In 
case of spa-eng language pair system architec-
ture ? 1 is helpful for both development and test 
data. 
 
The results for Run 1, Run 2 and Run 3 for each 
language on CLTE Test set are shown in Table 
2. 
 
Run Name Accuracy 
JU-CSE-NLP_deu-eng_run1 0.262 
JU-CSE-NLP_deu-eng_run2 0.296 
JU-CSE-NLP_deu-eng_run3 0.264 
JU-CSE-NLP_fra-eng_run1 0.288 
JU-CSE-NLP_fra-eng_run2 0.294 
JU-CSE-NLP_fra-eng_run3 0.296 
JU-CSE-NLP_ita-eng_run1 0.316 
JU-CSE-NLP_ita-eng_run2 0.326 
JU-CSE-NLP_ita-eng_run3 0.314 
JU-CSE-NLP_spa-eng_run1 0.274 
JU-CSE-NLP_spa-eng_run2 0.266 
JU-CSE-NLP_spa-eng_run3 0.272 
 
Table 2: Results on Test Set 
4 Conclusions and Future Works 
We have participated in Task 8 of Semeval-2012 
named Cross Lingual Textual Entailment mainly 
based on lexical matching and translation of text 
and hypothesis sentences in the cross lingual 
corpora. Both lexical matching and translation 
have their limitations. Lexical matching is useful 
for simple sentences but fails to retain high ac-
curacy for complex sentences with number of 
clauses. Semantic graph matching or conceptual 
graph is a good substitution to overcome these 
limitations. Machine learning technique is 
another important tool for multi-class entailment 
task. Features can be trained by some machine 
learning tools (such as SVM, Na?ve Bayes or 
Decision tree etc.) with multi-way entailment 
(forward, backward, bi-directional, no-
entailment) as its class. Works have been started 
in these directions. 
Acknowledgments 
The work was carried out under partial support 
of the DST India-CONACYT Mexico project 
?Answer Validation through Textual Entail-
ment? funded by DST, Government of India and 
partial support of the project CLIA Phase II 
(Cross Lingual Information Access) funded by 
DIT, Government of India. 
References  
[1] Negri, M., Marchetti, A., Mehdad, Y., Bentivogli, 
L., and Giampiccolo, D.: Semeval-2012 Task 8: 
Cross-lingual Textual Entailment for Content Syn-
chronization. In Proceedings of the 6th International 
Workshop on Semantic Evaluation (SemEval 2012). 
[2]  Dagan, I., Glickman, O., Magnini, B.: The 
PASCAL Recognising Textual Entailment Chal-
lenge. Proceedings of the First PASCAL Recog-
nizing Textual Entailment Workshop. (2005). 
[3] Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., 
Giampiccolo, D., Magnini, B., Szpektor, I.: The-
Seond PASCAL Recognising Textual Entailment 
Challenge. Proceedings of the Second PASCAL 
Challenges Workshop on Recognising Textual En-
tailment, Venice, Italy (2006). 
[4] Giampiccolo, D., Magnini, B., Dagan, I., Dolan, 
B.: The Third PASCAL Recognizing Textual En-
tailment Challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and 
Paraphrasing, Prague, Czech Republic. (2007). 
[5] Giampiccolo, D., Dang, H. T., Magnini, B., Da-
gan, I., Cabrio, E.: The Fourth PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2008 
Proceedings. (2008) 
[6] Bentivogli, L., Dagan, I., Dang. H.T., Giampicco-
lo, D., Magnini, B.: The Fifth PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2009 
Workshop, National Institute of Standards and 
Technology Gaithersburg, Maryland USA. (2009). 
[7] Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa 
Trang Dang,Danilo Giampiccolo: The Sixth 
PASCAL Recognizing Textual Entailment Chal-
694
lenge. In TAC 2010 Notebook Proceedings. 
(2010) 
[8] Bentivogli, L., Clark, P., Dagan, I., Dang, H., 
Giampiccolo, D.: The Seventh PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2011 
Notebook Proceedings. (2011) 
[9] Bos, Johan, Fabio Massimo Zanzotto, and Marco 
Pennacchiotti. 2009. Textual Entailment at 
EVALITA 2009: In Proceedings of EVALITA 
2009. 
[10] Mehdad, Yashar, Matteo Negri, and Marcello 
Federico.2010. Towards Cross-Lingual Textual 
entailment. In Proceedings of the 11th Annual 
Conference of the North American Chapter of the 
Association for Computational Linguistics, 
NAACL-HLT 2010. LA, USA. 
[11] Negri, Matteo, and Yashar Mehdad. 2010. 
Creating a Bilingual Entailment Corpus through 
Translations with Mechanical Turk: $100 for a 
10-day Rush. In Proceedings of the NAACL-HLT 
2010, Creating Speech and Text Language Data 
With Amazon's Mechanical Turk Workshop. LA, 
USA. 
[12] Mehdad, Yashar, Matteo Negri, Marcello Fede-
rico. 2011. Using Bilingual Parallel Corpora for 
Cross-Lingual Textual Entailment. In Proceedings 
of ACL 2011. 
[13] Yuret, D., Han, A., Turgut, Z.: SemEval-2010 
Task 12: Parser Evaluation using Textual Entail-
ments. Proceedings of the SemEval-2010 Evalua-
tion Exercises on Semantic Evaluation. (2010).  
 
[14] H. Shima, H. Kanayama, C.-W. Lee, C.-J. Lin,T. 
Mitamura, S. S. Y. Miyao, and K. Takeda. Over-
view of ntcir-9 rite: Recognizing inference in text. 
In NTCIR-9 Proceedings,2011. 
[15]  Pakray, P., Bandyopadhyay, S., Gelbukh, A.: 
Lexical based two-way RTE System at RTE-5. Sys-
tem Report, TAC RTE Notebook. (2009) 
 
[16] Pakray, P., Pal, S., Poria, S., Bandyopadhyay, S., 
, Gelbukh, A.: JU_CSE_TAC: Textual Entailment 
Recognition System at TAC RTE-6. System Re-
port, Text Analysis Conference Recognizing Tex-
tual Entailment Track (TAC RTE) Notebook. 
(2010) 
 
[17] Pakray, P., Neogi, S., Bhaskar, P., Poria, S., 
Bandyopadhyay, S., Gelbukh, A.: A Textual En-
tailment System using Anaphora Resolution. Sys-
tem Report. Text Analysis Conference 
Recognizing Textual Entailment Track Notebook, 
November 14-15. (2011) 
 
[18] Pakray, P., Neogi, S., Bandyopadhyay, S., Gel-
bukh, A.: A Textual Entailment System using Web 
based Machine Translation System. NTCIR-9, Na-
tional Center of Sciences, Tokyo, Japan. Decem-
ber 6-9, 2011. (2011) 
 
[19]  Fellbaum, C.: WordNet: An Electronic Lexical 
Database. MIT Press (1998). 
695
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 64?72, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
JU_CSE: A CRF Based Approach to Annotation of Temporal Expres-
sion, Event and Temporal Relations 
 
 
Anup Kumar Kolya1, Amitava Kundu1, 
 Rajdeep Gupta1  
Asif Ekbal2, Sivaji Bandyopadhyay1 
1Dept. of Computer Science & Engineering 2Dept. of Computer Science & Engineering 
Jadavpur Univeristy IIT Patna 
Kolkata-700 032, India Patna-800 013, India 
{anup.kolya,amitava.jucse, 
rajdeepgupta20}@gmail.com 
asif@iitp.ac.in, 
sivaji_ju_cse@yahoo.com 
 
 
 
 
 
Abstract 
In this paper, we present the JUCSE system, 
designed for the TempEval-3 shared task. The 
system extracts events and temporal infor-
mation from natural text in English. We have 
participated in all the tasks of TempEval-3, 
namely Task A, Task B & Task C. We have 
primarily utilized the Conditional Random 
Field (CRF) based machine learning tech-
nique, for all the above tasks. Our system 
seems to perform quite competitively in Task 
A and Task B. In Task C, the system?s per-
formance is comparatively modest at the ini-
tial stages of system development. We have 
incorporated various features based on differ-
ent lexical, syntactic and semantic infor-
mation, using Stanford CoreNLP and Wordnet 
based tools. 
1 Introduction 
Temporal information extraction has been a popu-
lar and interesting research area of Natural Lan-
guage Processing (NLP) for quite some time. 
Generally, a lot of events are described in a variety 
of newspaper texts, stories and other important 
documents where the different events described 
happen at different time instants. The temporal 
location and ordering of these events are either 
specified or implied. Automatic identification of 
time expressions and events and annotation of 
temporal relations constitute an important task in 
text analysis. These are also important in a wide 
range of NLP applications that include temporal 
question answering, machine translation and doc-
ument summarization.  
A lot of research in the area of temporal infor-
mation extraction has been conducted on multiple 
languages, including English and several European 
languages. The TimeML was first developed in 
2002 in an extended workshop called TERQAS 
(Time and Event Recognition for Question An-
swering Systems) and, in 2003, it was further de-
veloped in the context of the TANGO workshop 
(TimeML Annotation Graphical Organizer). Since 
then most of the works in this research arena have 
been conducted in English. The variety of works 
include TimeML (Pustejovsky et al, 2003), the 
development of a temporally annotated corpus 
Time-Bank (Pustejovsky et al, 2003), the temporal 
evaluation challenges TempEval-1 (Verhagen et 
al., 2007), TempEval-2 (Pustejovsky and Verha-
gen, 2010). In the series of Message Understanding 
Conferences (MUCs) that started from 1987 and 
the Sheffield Temporal Annotation scheme 
(STAG) (Setzer &Gaizauskas, 2000) the  aim  was 
to identify events in news text and determine their 
relationship with points on a temporal line. 
In the series of TempEval evaluation exercises, 
TempEval-1 was the first one where the focus was 
on identification of three types of temporal rela-
tion: relation between an event and a time expres-
sion in the same sentence, relation between an 
64
event and the document creation time, and relation 
between two main events in consecutive sentences. 
 TempEval-2 was a follow up to TempEval-1 
and consisted of six subtasks rather than three. It 
added (i) identification of time expressions and 
determination of values of the attributes TYPE and 
VAL (ii) identification of event expressions and 
determination of its attribute values. It included the 
previous three relation tasks from TempEval-1 and 
an additional task of annotating temporal relation 
between a pair of events where one subordinates 
the other.  
We have participated in all three tasks of 
TempEval-3- Task A, Task B and Task C. A com-
bination of CRF based machine learning and rule 
based techniques has been adopted for temporal 
expression extraction and determination of attrib-
ute values of the same   (Task A). We have used a 
CRF based technique for event extraction (Task 
B), with the aid of lexical, semantic and syntactic 
features. For determination of event attribute val-
ues we have used simple rule based techniques. 
Automatic annotation of temporal relation between 
event-time in the same sentence, event-DCT rela-
tions, mainevent-mainevent relations in consecu-
tive sentences and subevent-subevent relations in 
the same sentences has been introduced as a new 
task (Task-C) in the TempEval-3 exercise. We 
have adopted a CRF based technique for the same 
as well. 
2 The JU_CSE System Approach  
The JU_CSE system for the TempEval-3 shared 
task uses mainly a Conditional Random Field 
(CRF) machine learning approach to achieve Task 
A, Task B & Task C. The workflow of our system 
is illustrated in Figure 1. 
2.1 Task A: Temporal Expression Identifica-
tion and Normalization 
Temporal Expression Identification: 
 We have used CRF++ 0.571, an open source im-
plementation of the Conditional Random Field 
(CRF) machine learning classifier for our experi-
ments. CRF++ templates have been used to capture 
the relation between the different features in a se-
quence to identify temporal expressions. Temporal 
                                                        
1 http://crfpp.googlecode.com/svn/trunk/doc/index.html 
expressions mostly appear as multi-word entities 
such as ?the next three days?. Therefore the use of 
CRF classifier that uses context information of a 
token seemed most appropriate.  
 Initially, all the sentences have been changed to 
a vertical token-by-token level sequential structure 
for temporal expressions representation by a B-I-O 
encoding, using a set of mostly lexical features. In 
this encoding of temporal expression, ?B? indi-
cates the ?beginning of sequence?, ?I? indicates a 
token inside a sequence and ?O? indicates an out-
side word. We have carefully chosen the features 
list based on the several entities that denote month 
names, year, weekdays, various digit expressions 
(day, time, AM, PM etc.) In certain temporal ex-
pression patterns (several months, last evening) 
some words (several, last) act as modifiers to the 
following words that represent the time expression. 
Temporal expressions include time expression 
modifiers, relative days, periodic temporal set, 
year-eve day, month name with their short pattern 
forms, season of year, time of day, decade list and 
so on. We have used the POS information of each 
token as a feature. We have carefully accounted for 
a simple intuition revelation that most temporal 
expressions contain some tokens conveying the 
?time? information while others possibly convey-
ing the ?quantity? of time. For example, in the ex-
pression ?next three days?, ?three? quantifies 
?days?. Following are the different temporal ex-
pressions lists that have been utilized: 
 
? A list of time expression modifiers: this, 
mid, recent, earlier, beginning, late etc. 
? A list of relative days: yesterday, tomor-
row etc. 
? A list of periodic temporal set: hourly, 
nightly etc. 
? A list of year eve day: Christmas Day, 
Valentine Day etc. 
? A list of month names with their short pat-
tern forms: April, Apr. etc. 
? A list of season of year: spring, winter etc. 
? A list of time of day: morning, afternoon, 
evening etc. 
? A list of decades list: twenties, thirties etc. 
 
 
65
  
  
Raw Text: 
For his part, Fidel Castro is the ultimate political 
survivor. People have predicted his demise so 
many times, and the US has tried to hasten it on 
several occasions. Time and again, he endures.  
? Tokenize with Stanford CoreNLP 
? Obtain POS tags of tokens 
? Extract features from tokens 
? Identify the features for event annotation and 
temporal annotation separately 
 
CRF  
 
Event & 
Time 
 Features 
T
ag E
V
E
N
T
 
tokens 
Tag 
TIMEX3 
tokens 
. 
       For???  OTHERS 
  nearly ???.. TIMEX3 
       forty?. ?  TIMEX3 
years??.. TIMEX3 
. 
. 
 
. 
People???  OTHERS 
have ???..   OTHERS 
      predicted ?. ?  EVENT 
his ????.. OTHERS 
. 
. 
Annotated Text 
 
For his part, Fidel Castro is the ultimate political survivor. 
People have <EVENT class="I_ACTION" 
eid="e1">predicted</EVENT> his <EVENT 
class="OCCURRENCE" eid="e2">demise</EVENT> so 
many times, and the US has <EVENT class="I_ACTION" 
eid="e3">tried</EVENT> to <EVENT 
class="OCCURRENCE" eid="e4">hasten</EVENT> it on 
several occasions. 
D
eterm
ine 
E
vent 
C
lass 
CoreNLP 
for ?type? 
& ?velue? 
<MAKEINSTANCE eiid="ei1? eventID="e1" pos="VERB" 
tense="PRESENT" aspect="PERFECTIVE" polarity="POS" /> 
 
<MAKEINSTANCE eiid="ei2? eventID="e2" pos="NOUN" 
tense="PRESENT" aspect="PERFECTIVE" polarity="POS" /> 
 
<MAKEINSTANCE eiid="ei3? eventID="e3" pos="VERB" 
tense="PRESENT" aspect="PERFECTIVE" polarity="POS" /> 
R
ule based approach to obtain tense, as-
pect, polarity, m
odality etc. for events 
 
Enlist entity pairs with features 
<mainevent-mainevent> 
<event-event> 
<event-dct>  
<event-time> 
 
 
CRF  
 
Temporal Relations: 
 
<TLINK lid="l1" relType="BEFORE" 
eventInstanceID="ei1" relatedTo-
Time="t0" /> 
 
<TLINK lid="l2" relType="BEFORE" 
eventInstanceID="ei2" relatedToEven-
tInstance="ei1" /> 
Figure 1.The JU_CSE System Architecture 
66
Determination of Normalized value and type 
of Temporal Expressions: 
 Temporal expressions in documents are generally 
defined with the type and value attributes. All the 
temporal expressions can be differentiated into 
three types (i) explicit (ii) relative and (iii) implicit 
temporal expressions. For example, the expression 
?October 1998? refers to a specific month of the 
year which can be normalized without any addi-
tional information. On the other hand, the relative 
expression ?yesterday? can?t be normalized with-
out the knowledge of a corresponding reference 
time. The reference time can either be a temporal 
expression or the Document Creation Time marked 
in the document. Consider the following piece of 
text: ?Yesterday was the 50th independence of In-
dia?. The First Independence Day of India is 15th 
august 1947.? Here ?Yesterday? can be normal-
ized as ?15-08-1997?. It may be noted that infor-
mation such as ?First Independence Day of India? 
can be directly accessed from the timestamp calen-
dar, through the metadata of a document. The third 
type of temporal expressions includes implicit ex-
pressions such as names of festival days, birthdays 
and holidays or events. These expressions are 
mapped to available calendar timeline to find out 
their normalized values. 
 
Temporal 
Expression 
Type Value 
A couple of 
years 
 
DURATION P2Y 
October DATE ?1997-10? 
Every day SET P1D 
2 P.M. TIME 2013-02-01T14:00 
Now DATE PRESENT_REF" 
Table 1: TimeML normalized type and value attributes 
for temporal expressions 
 
We have implemented a combined technique us-
ing our handcrafted rules and annotations given by 
the Stanford CoreNLP tool to determine the ?type?-
s and ?value?-s. Four types TIME, DATE, 
DURATION and SET of temporal expressions are 
defined in the TimeML framework. Next, we have 
evaluated the normalized value of temporal expres-
sions using Document Creation Time (DCT) from 
the documents.  In this way, values of different 
dates have been inferred e.g. last year, Monday, 
and today. 
2.2 Task B: Extraction of Event Words and 
Determination of Event Attribute Values  
Event Extraction 
In our evaluation framework, we have used the 
Stanford CoreNLP tool extensively to tokenize, 
lemmatize, named-entity annotate and part-of-
speech tag the text portions of the input files. For 
event extraction, the features have been considered 
at word level, where each word has its own set of 
features. The general features used to train our 
CRF model are: 
Morphological Features: Event words are rep-
resented mostly as verbs and nouns. The major 
problem is detecting the events having non-verbal 
PoS labels. Linguistically, non-verbal wordforms 
are derived from verbal wordforms. Various inflec-
tional and derivational morphological rules are 
involved in the process of evolving from verbal to 
non-verbal wordforms. We have used a set of 
handcrafted rules to identify the suffixes such as (?-
ci?n?, ?-tion? or ?-ion?), i.e., the morphological 
markers of word token, where Person, Location 
and Organization words are not considered. The 
POS and lemma, in a 5-window (-2, +2), has been 
used for event extraction. 
Syntactic Feature: Different event words no-
tions are contained in the sentences such as: verb-
noun combinations structure, the complements of 
aspectual prepositional phrases (PPs) headed by 
prepositions and a particular type of complex 
prepositions. These notions are captured to be used 
as syntactic features for event extraction. 
WordNet Feature: The RiTa Wordnet2 package 
has been effectively used to extract different prop-
erties of words, such as Synonyms, Antonyms, 
Hypernyms, & Hyponyms, Holonyms, Meronyms, 
Coordinates, & Similars, Nominalizations, Verb-
Groups, & Derived-terms. We have used these 
Wordnet properties in the training file for the CRF 
in the form of binary features for verbs and nouns 
indicating if  the words like ?act?, ?activity?, ?phe-
nomenon? etc. occur  in different relations of the 
Wordnet ontology. 
                                                        
2 http://www.rednoise.org/rita/wordnet/documentation/ 
67
Features using Semantic Roles: We use Se-
mantic Role Label (SRL) (Gildea et el, 2002; Pra-
dhan et al 2004; Gurevich et al 2006) to identify 
different useful features for event extraction. For 
each predicate in a sentence acting as event word, 
semantic roles extract all constituents; determine 
their arguments (agent, patient, etc.) and adjuncts 
(locative, temporal, etc.). Some of the other fea-
tures like predicate, voice and verb sub-
categorization are shared by all the nodes in the 
tree. In the present work, we use predicate as an 
event.  Semantic roles can be used to detect the 
events that are nominalizations of verbs such as 
agreement for agree or construction for construct.  
Event nominalizations often share the same seman-
tic roles as verbs, and often replace them in written 
language. Noun words, morphologically derived 
from verbs, are commonly defined as deverbal 
nouns. Event and result nominalizations constitute 
the bulk of deverbal nouns. The first class refers to 
an event/activity/process, with the nominal ex-
pressing this action (e.g., killing, destruction etc.). 
Nouns in the second class describe the result or 
goal of an action (e.g., agreement, consensus etc.). 
Many nominals denote both the event and result 
(e.g., selection). A smaller class is agent/patient 
nominalizations that are usually identified by suf-
fixes such as -er, -or etc., while patient nominaliza-
tions end with -ee, -ed (e.g. employee).   
Object information of Dependency Relations 
(DR): We have developed handcrafted rules to 
identify features for CRF training, based on the 
object information present in the dependency rela-
tions of parsed sentences. Stanford Parser (de 
Marneffe et al, 2006), a probabilistic lexicalized 
parser containing 45 different Part-of-Speech 
(PoS) tags of Penn Treebank is used to get the 
parsed sentences with dependency relations. The 
dependency relations are found out for the predi-
cates ?dobj? so that the direct object related com-
ponents in the ?dobj? predicate is considered as the 
feature for the event expression. Initially the input 
sentences are passed to the dependency parser3.  
From the parsed output verb noun combination 
direct object (dobj) dependency relations are ex-
tracted. These dobj relations basically inform us 
that direct object of a VP is the noun phrase which 
is the (accusative) object of the verb; the direct 
object of a clause is the direct object of the VP 
                                                        
3 http://nlp.stanford.edu:8080/parser/ 
which is the predicate of that clause. Within the 
dobj relation governing verb word and dependent 
noun words are acting as important features for 
event identification when dependent word is not 
playing any role in other dependency relation 
(nsubj, prep_of, nn ,etc.) of the sentence. 
 
In this way, we have set list of word tokens and 
its features to train the recognition model. Then the 
model will give to each word one of the valid la-
bels.  
Determination of various Event Attribute 
Values: 
Values of different event attributes have been 
computed as follows: 
Class: Identification of the class of an event has 
been done using a simple, intuitive, rule based ap-
proach. Here too, the hypernym list of an event 
token from RitaWordnet has been deployed to de-
termine the class of the respective event. In this 
case, OCCURRENCE has been considered the de-
fault class. 
Tense, Aspect, POS: These three attributes are 
the obligatory attributes of MAKEINSTANCE 
tags. To determine the tense, aspect and polarity of 
an event, we have used the ?parse? annotator in 
CoreNLP. We annotated each sentence with the 
Stanford dependency relations using the above an-
notator. Thereafter various specific relations were 
used to determine the tense, aspect and POS of an 
event token, with another rule based approach. For 
example, in the phrase ?has been abducted?, the 
token ?been? appears as the dependent in an ?aux? 
relation with the event token ?abducted?; and 
hence the aspect ?PERFECTIVE? is inferred. The 
value ?NONE? has been used as the default value 
for both tense and aspect. 
Polarity and Modality: Polarity of event tokens 
are determined using Stanford dependency rela-
tions too; here the ?neg? relation. To determine the 
modality we search for modal words in ?aux? rela-
tions with the event token. 
2.3 Task C: Temporal Relation Annotation 
We have used the gold-standard TimeBank fea-
tures for events and times for training the CRF. In 
the present work, we mainly use the various com-
binations of the following features:  
68
 
(i)  Part of Speech (POS) 
(ii)  Event Tense 
(iii)  Event Aspect 
(iv)  Event Polarity 
(v)  Event Modality 
(vi)  Event Class 
(vii)       Type of temporal expression 
(vii)  Event Stem 
(viii)  Document Creation Time (DCT). 
 
The following subsections describe how various 
temporal relations are computed. 
Event-DCT 
We take the combined features of every event pre-
sent in the text and the DCT for this purpose. 
 
Derived Features: We have identified different 
types of context based syntactic features which are 
derived from text to distinguish the different types 
of temporal relations. In this task, following fea-
tures help us to identify the event-DCT relations, 
specially ?AFTER? temporal relations: 
(i)Modal Context: Whether or not the event word 
has one of the modal context words like- will, 
shall, can, may, or any of their variants (might, 
could, would, etc.).In the sentence: ?The entire 
world will [EVENT see] images of the Pope in Cu-
ba?. Here ?will? context word helps us to deter-
mine event-DCT relation ?AFTER?. 
(ii)Preposition Context: Any prepositions preced-
ing an event or time expression. We consider an 
example:?Children and invalids would be permit-
ted to [EVENT leave] Iraq?. Here the preposition 
to helps us to determine event-DCT relation 
?AFTER?. The same principle goes for time too: in 
the expressions on Friday and for nearly forty 
years, the prepositions on and for governs the time.  
(iii)Context word before or after temporal expres-
sion: context words like before, after, less than, 
greater than etc. help us to determine event-time 
temporal relation identification. Consider an ex-
ample: ?After ten years of [EVENT boom] ?.? 
Event-Time 
Derived Features: We extract all events from eve-
ry sentence. For every temporal expression in a 
sentence, we pair an event in the sentence with the 
former so that the temporal relation can be deter-
mined. 
Similar to annotation of event-DCT relations, 
here too, we have identified different types of con-
text based temporal expression features which are 
derived from text to distinguish the different types 
of temporal relations. In this task, the following 
features help us to distinguish between event and 
time relations, specially ?AFTER? and ?BEFORE? 
temporal relations. The following features are de-
rived from text. 
(i)Type of temporal expression: Represents the 
temporal relationship holding between events, 
times, or between an event and a time of the event.   
(ii)Temporal signal: Represents temporal preposi-
tions ?on? (on this coming Sunday) and slightly 
contribute to the overall score of classifiers 
(iii)Temporal Expression in the target sentence: 
Takes the values greater than, less than, equal or 
none. These values contribute to the overall score 
of classifiers. 
Mainevent-Mainevent and Subevent-
Subevent 
The task demands that the main event of every sen-
tence be determined. As a heuristic decision, we 
have assumed that the first event that appears in a 
sentence is its main event. We pair up main events 
(if present) from consecutive sentences and use 
their combined features to determine their temporal 
relation. For the events belonging to a single sen-
tence, we take into account the combined features 
of all possible pairs of sentential events. 
   
Derived Features: We have identified different 
types of context based syntactic features which are 
derived from text to distinguish the different types 
of temporal relations. 
(i)Relational context: If a relation holding be-
tween the previous event and the current event is 
?AFTER?, the current one is in the past. This in-
formation helps us to identify the temporal relation 
between the current event and successive event. 
(ii)Modal Context: Whether or not the event word 
has one of the context words like, will, shall, can, 
may, or any of their variants (might, could, would, 
etc.).  The verb and auxiliaries governing the next 
event play as an important feature in event-event 
temporal relation identification.   
69
(iii)Ordered based context: In event-event rela-
tion identification, when EVENT-1, EVENT-2, 
and EVENT-3 are linearly ordered, then we have 
assigned true/false as feature value from tense and 
aspect shifts in this ordered pair.  
(iv) Co-reference  based feature: We have used 
co-referential features as derived feature using our 
in-house system based on Standford CoreNLP tool, 
where two event words within or outside one sen-
tence are referring to the same event, i.e. two event 
words co-refer in a discourse.  
(v)Event-DCT relation based feature: We have 
included event-document creation times (DCT) 
temporal relation types as feature of event-event 
relation identification. 
(ii) Preposition Context: Any prepositions before 
the event or time, we consider an exam-
ple:?Children and invalids would be permitted to 
[EVENT leave] Iraq?. Here the preposition to 
helps us determine the event-DCT relation 
?AFTER?.  
(vi) Context word before or after temporal ex-
pression: Context words like before, after, less 
than, greater than help us determine event- event 
temporal relations .We consider an example:?After 
ten years of [EVENT boom] ?.? 
(vii)Stanford parser based clause boundaries 
features: The two consecutive sentences are first 
parsed using Stanford dependency parser and then 
clause boundaries are identified. Then, considering 
the prepositional context and tense verb of the 
clause, temporal relations are identified where all 
temporal expressions are situated in the same 
clause.  
 
 
3 Results and Evaluation 
For the extraction of time expressions and events 
(tasks A and B), precision, recall and F1-score 
have been used as evaluation metrics, using the 
following formulae: 
 
precision (P) = tp/(tp + fp) 
recall (R) = tp/(tp + fn) 
F-measure = 2 *(P * R) / (P + R). 
 
Where, tp is the number of tokens that are part of 
an extent in keys and response, fp is the number of 
tokens that are part of an extent in the response but 
not in the key, and fn is the number of tokens that 
are part of an extent in the key but not in the re-
sponse. Additionally attribute accuracies computed 
according to the following formulae have also been 
reported. 
 
Attr. Accuracy = Attr. F1 / Entity Extraction F1  
Attr. R = Attr. Accuracy * Entity R 
Attr. P = Attr. Accuracy * Entity P 
 
Performance in task C is judged with the aid of the 
Temporal Awareness score proposed by UzZaman 
and Allen (2011) 
The JU_CSE system was evaluated on the TE-3 
platinum data. Table 2 reports JU_CSE?s perfor-
mance in timex extraction Task A. Under the re-
laxed match scheme, the F1-score stands at 
86.38% while the strict match scheme yields a F1-
score of 75.41%. As far as TIMEX attributes are 
concerned, the F1-scores are 63.81% and 73.15% 
for value and type respectively.  
 
Timex Extraction Timex Attribute 
F1 P R Strict F1 Strict P Strict R 
Value 
F1 
Type 
F1 
Value 
Accuracy 
Type 
Accuracy 
86.38 93.28 80.43 75.49 81.51 70.29 63.81 73.15 73.87 84.68 
Table 2:JU_CSE system?s TE-3 Results on Timex Task A 
 
 
 
 
Event Extraction Event Attribute 
F1 P R 
Class 
F1 
Tense 
F1 
Aspect 
F1 
Class 
Accuracy 
Tense 
Accuracy 
Aspect 
Accuracy 
78.57 80.85 76.41 52.65 58.58 72.09 67.01 74.56 91.75 
Table 3:JU_CSE system?s TE-3 Results on Event Task B 
  
70
  
Table 3 reports the system?s performance in 
event extraction (Task B) on TE-3 platinum da-
ta. F1-score for event extraction is 78.57%. At-
tribute F1-scores are 52.65%, 58.58% and 
72.09% for class, tense and aspect respectively.  
In both entities extraction tasks recall is nota-
bly lower than precision. The F1-scores for 
event attributes are modest given that the attrib-
utes were computed using handcrafted rules. 
However, the handcrafted approach can be treat-
ed as a good baseline to start with. Normaliza-
tion is proved to be a challenging task. 
 
Task F1 P R 
Task-ABC 24.61 19.17 34.36 
Task-C 26.41 21.04 35.47 
Task-C-relation-only 34.77 35.07 34.48 
 
Table 4: JU_CSE system?s TE-3 Temporal Aware-
ness results on Task ABC, TaskC-only & TaskC-
relation-only 
 
 
Table 4 presents the Temporal Awareness F1-
score for TaskABC, TaskC and TaskC-relation-
only. For TaskC-only evaluation, the event and 
timex annotated data was provided and one had 
to identify the TLINKs and classify the temporal 
relations. In the TaskC-relation-only version the 
timex and event annotations including their at-
tributes as well as TLINKs were provided save 
the relation classes. Only the relation classes had 
to be determined. The system yielded a temporal 
awareness F1-score of 24.6% for TaskABC, 
26.41% for TaskC-only and 34.77% for TaskC-
relation-only version. 
 
4 Conclusions and Future Directions 
  
In this paper, we have presented the JU_CSE 
system for the TempEval-3 shared task. Our sys-
tem in TempEval-3 may be seen upon as an im-
provement over our earlier endeavor in 
TempEval-2. We have participated in all tasks of 
the TempEval-3 exercise. We have incorporated 
a CRF based approach in our system for all 
tasks. The JU_CSE system for temporal infor-
mation extraction is currently undergoing a lot 
of extensive experimentation. The one reported 
in this article seemingly has a significant scope 
of improvement. Preliminarily, the results yield-
ed are quite competitive and encouraging. Event 
extraction and Timex extraction F1-scores at 
78.58% and 86.38% encourage us to further de-
velop our CRF based scheme. We expect better 
results with additional features and like to con-
tinue our experimentations with other semantic 
features for the CRF classifier. Our rule-based 
approach for event attribute determination how-
ever yields modest F1-scores- 52.65% & 
58.58% for class and tense. We intend to explore 
other machine learning techniques for event at-
tribute classification. We also intend to use parse 
tree based approaches for temporal relation an-
notation. 
Acknowledgments 
This work has been partially supported by a 
grant from the English to Indian language Ma-
chine Translation (EILMT) project funded by 
the Department of Information and Technology 
(DIT), Government of India. We would also like 
to thank to Mr. Jiabul Sk. for his technical con-
tribution.  
 
References  
A. Setzer, and R. Gaizauskas. 2000. Annotating 
Events and Temporal Information in Newswire 
Texts. In LREC 2000, pages 1287?1294, Athens. 
D. Gildea, and D. Jurafsky. 2002. Automatic Label-
ing of Semantic Roles. Computational Linguistics, 
28(3):245?288. 
James Pustejovsky, Jos? Castano, Robert Ingria, 
Roser Sauri, Robert Gaizauskas, Andrea Setzer, 
Graham Katz, and Dragomir Radev. 2003. 
TimeML: Robust specification of event and tem-
poral expressions in text. New directions in ques-
tion answering, 3: 28-34. 
Marc Verhagen, Robert Gaizauskas, Frank Schilder, 
Mark Hepple, Graham Katz, and James 
Pustejovsky. 2007. Semeval-2007 task 15: 
Tempeval temporal relation identification. In Pro-
ceedings of the 4th International Workshop on 
Semantic Evaluations, pages 75-80, ACL. 
71
Marc Verhagen, Roser Sauri, Tommaso Caselli, and 
James Pustejovsky. 2010. Semeval-2010 task 13: 
Tempeval-2. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 
57- 62. ACL. 
Olga Gurevich, Richard Crouch, Tracy H. King, and 
V. de Paiva. 2006. Deverbal Nouns in Knowledge 
Representation. Proceedings of FLAIRS, pages 
670?675, Melbourne Beach, FL. 
Sameer Pradhan, Wayne Ward, Kadri Hacioglu, 
James H. Martin, and Daniel Jurafsky. 2004. Shal-
low Semantic Parsing using Support Vector Ma-
chine. Proceedings of HLT/NAACL-2004, 
Boston, MA. 
UzZaman, N. and J.F. Allen (2011), ?Temporal 
Evaluation.? In Proceedings of The 49th Annual 
Meeting of the Association for Computational 
Linguistics: Human Language Technologies 
(Short Paper), Portland, Oregon, USA.
   
 
72
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 370?374,
Dublin, Ireland, August 23-24, 2014.
JU_CSE: A Conditional Random Field (CRF) Based Approach to    
Aspect Based Sentiment Analysis 
 
Braja Gopal Patra, Soumik Mandal, Dipankar Das and Sivaji Bandyopadhyay 
Department of Computer Science & Engineering,  
Jadavpur University, Kolkata, India 
brajagopal.cse@gmail.com, mandal.soumik@gmail.com, 
dipankar.dipnil2005@gmail.com, sivaji_cse_ju@yahoo.com 
 
  
Abstract 
The fast upswing of online reviews and their 
sentiments on the Web became very useful 
information to the people. Thus, the opin-
ion/sentiment mining has been adopted as a 
subject of increasingly research interest in 
the recent years. Being a participant in the 
Shared Task Challenge, we have developed a 
Conditional Random Field based system to 
accomplish the Aspect Based Sentiment 
Analysis task. The aspect term in a sentence 
is defined as the target entity. The present 
system identifies aspect term, aspect catego-
ries and their sentiments from the Laptop 
and Restaurants review datasets provided by 
the organizers. 
1 Introduction 
In recent times, the research activities in the 
areas of Opinion Mining/Sentiment Analysis in 
natural language texts and other media are gain-
ing ground under the umbrella of subjectivity 
analysis and affect computing1. The reason may 
be the huge amount of available text data in So-
cial Web in the forms of news, reviews, blogs, 
chat and twitter etc. Majority of research efforts 
are being carried out for the identification of pos-
itive or negative polarity from the textual con-
tents like sentence, paragraph, or text span re-
gardless of the entities (e.g., laptops, restaurants) 
and their aspects (e.g., battery, screen; food, ser-
vice). 
                                                 
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details: 
http://creativecommons.org/licenses/by/4.0/ 
1http://www.saaip.org/ 
Aspect is a multinomial distribution over 
words that represent a more specific topic in re-
views (Jo and Oh, 2011). For example, in case of 
Laptop reviews, ?touchpad? is considered an 
aspect. Similarly, given a predefined entity, an 
aspect term describes a specific aspect of that 
entity (e.g., for the entity ?restaurant?, ?wine? 
can be an aspect term). Aspect term can be ap-
peared as a single word (e.g., ?menu?) or multi-
ple words (?side dish?). 
It is observed that for a particular entity, one 
or more number of aspect terms can be grouped 
into a single category (e.g., aspect terms 
?drinks?, ?main course? belongs to the same cat-
egory, ?food?).  
The main goal of the Aspect Based Sentiment 
Analysis (ABSA) (Pontiki et al., 2014) task is to 
identify the aspect terms and their categories 
from the given target entities as well as to identi-
fy the sentiments expressed towards each of the 
aspect terms. The datasets provided by the 
shared task organizers consist of customer re-
views with human-annotations. 
We have participated in all of the four tasks. A 
combination of Conditional Random Field (CRF) 
based machine learning algorithm and rule based 
techniques has been adopted for identifying the 
aspect term, aspect category and their senti-
ments. We have used several features like Part of 
Speech (POS), Stanford dependency relations2, 
WordNet information, and sentiment lexicon 
(SentiWordNet3) to accomplish these tasks. 
The rest of the paper is organized in the fol-
lowing manner. Section 2 provides the details of 
previous works. Section 3 provides an elabora-
tive description of the data used in the task. Fea-
tures used in these experiments are described in 
Section 4. The detailed setup of experimentation 
and analysis of the results are described in Sec-
                                                 
2 http://nlp.stanford.edu/software/lex-parser.shtml 
3 http://sentiwordnet.isti.cnr.it/ 
370
tion 5. Finally, conclusions and future directions 
are presented. 
2 Related Work 
It has been observed that most of the previous 
works on aspect detection were based on infor-
mation extraction, to find the most frequent noun 
phrases (Hu and Liu, 2004). This approach is 
generally useful in finding aspects which are 
strongly associated with a single noun. But, one 
principal disadvantage of this approach is that it 
cannot detect the aspect terms which are of low 
frequency and noun phrases (e.g., different 
names of dishes like Biryani, Dosa and Uttapam 
etc. for the aspect category, ?food?). The pro-
posed work of such problem involves semantic 
hierarchy, rule-based or combination of both 
(Popescu and Etzioni 2005). More recent ap-
proaches of aspect detection are based on topic 
modelling, that use Latent Dirichlet Allocation 
(LDA) (Brody and Elhadad, 2010). But, the 
standard Latent Dirichlet Allocation (LDA) is 
not exactly suitable for the task of aspect detec-
tion due to their inherent nature of capturing 
global topics in the data, rather than finding local 
aspects related to the predefined entity. This ap-
proach was further modified in Sentence-LDA 
(SLDA) and Aspect and Sentiment Unification 
Model (ASUM) (Jo and Oh, 2011). Similarly, the 
identification of focussed text spans for opinion 
topics and targets were identified in (Das and 
Bandyopadhyay, 2010). 
Snyder and Barzilay (2007) addressed the 
problem of identifying categories for multiple 
related aspect terms appeared in the text. For 
instance, in a restaurant review, such categories 
may include food, ambience and service etc. In 
our task, we call them as aspect or review cate-
gories. The authors implemented the Good Grief 
decoding algorithm on a corpus collected on res-
taurant review4, which outperforms over the fa-
mous PRank algorithm (Crammer and Singer, 
2001). 
Ganu et al., (2009) have classified the restau-
rant reviews collected from City search New 
York5 into six categories namely Food, Service, 
Price, Ambience, Anecdotes, and Miscellaneous. 
Sentiment associated with each category has also 
been identified and both the experiments were 
carried out using Support Vector Machine classi-
fiers. Finally, they implemented the regression 
based model containing MATLAB regression 
                                                 
4 http://people.csail.mit.edu/bsnyder/naacl07/ 
5 http://www.citysearch.com/guide/newyork-ny-metro 
function (mvregress) to give rating (1 to 5) to 
each review.  
To determine the sentiment or polarity of the 
aspect term and aspect category, we need a prior 
sentiment annotated lexicon. Several works have 
been conducted on building emotional corpora in 
different English languages such as SentiWord-
Net (Baccianella et al., 2010), WordNet Affect 
(Strapparava and Valitutti, 2004) (Patra et al., 
2013) etc. Among all these publicly available 
sentiment lexicons, SentiWordNet is one of the 
well-known and widely used ones (number of 
citations is higher than other resources6) that has 
been utilized in several applications such as sen-
timent analysis, opinion mining and emotion 
analysis.  
Several works have been performed on the au-
tomated opinion detection or polarity identifica-
tion from reviews (Yu and Hatzivassiloglou, 
2003; Hu and Liu, 2004). Yu and Hatzivass-
iloglou (2003) has focused on characterizing 
opinions and facts in a generic manner, without 
examining who the opinion holder is or what the 
opinion is about. Then, they have identified the 
polarity or sentiment of the fact using Naive 
Bayes classifier. Hu and Liu, (2004) has summa-
rized the customer review and then identified the 
sentiment of that review. They have achieved 
promising accuracy in case of identifying polari-
ty of the reviews.  
3 Data 
The sentences collected from the customer re-
views of Restaurants and Laptops are used in 
these tasks. The training data of Restaurant re-
views contains 3041 English sentences annotated 
with aspect terms and aspect categories along 
with their polarity. The training data of Laptop 
reviews contains 3045 sentences annotated with 
aspect terms along with their polarity. The test 
data contains 800 sentences from each of the re-
view sets.  
An example extracted from the corpus is as 
follows:  
But the staff was so horrible to us.  
Here, "staff" is the aspect term and its polarity 
is "negative". The aspect category is "service" 
and polarity of the aspect category is also "nega-
tive". 
                                                 
6 http://citeseerx.ist.psu.edu/index 
371
4 Feature Analysis 
In general, the feature selection always plays 
an important role in any machine learning 
framework and depends upon the data set used 
for the experiments. Based on a preliminary in-
vestigation of the dataset, we have identified 
some of the following features. Different combi-
nations of the features have also been used to get 
the best results from the classification task. 
Parts-of-Speech (POS): the aspect terms are 
basically represented by the noun phrases. On the 
other hand, the POS tag plays an important role 
in aspect term identification (Hu and Liu, 2004; 
Brody and Elhadad, 2010). Thus, we have used 
the Stanford CoreNLP7 tool to parse each of the 
review sentences to find out the part-of-speech 
tag of each word and included them as a feature 
in all of our experiments.  
POS Frequency: We have observed that the 
aspect terms surrounded by a noun or adjective 
are also denoted as aspect terms. Therefore, we 
have utilized this information in our system. For 
example, in the phrase ?external_JJ mouse_NN?. 
Here the word ?mouse? is an object and aspect 
term. The word ?external? is also tagged as as-
pect term. 
Before be verb: We have observed that the 
nouns occur before the ?be? verbs denote the 
aspect terms in most of the cases. e.g. ?The hard 
disk is noisy?. Here ?hark disk? is an aspect term 
and is followed by the ?be? verb "is". 
Inanimate words: In case of the Restaurant 
and Laptop reviews, we observed that many of 
the inanimate nouns occur as aspect terms. We 
have used the hyponym tree of RiTa.WordNet8 to 
identify the inanimate words. For example, in the 
following sentence, the words food, kitchen and 
menu are inanimate nouns occurred as aspect 
terms. 
?The food is uniformly exceptional, with a 
very capable kitchen which will proudly whip up 
whatever you feel like eating, whether it's on the 
menu or not.?  
Dependency Relation for finding Object: We 
have identified the object based dependency rela-
tions from parsed sentences, as we have observed 
that the words occupied in such relations are rep-
resented as aspect terms in many cases. ?dobj?, 
?obj? and ?xobj? are considered as the probable 
candidate relations for identifying the aspect 
                                                 
7
http://nlp.stanford.edu/software/corenlp.shtml 
8www.rednoise.org/rita/reference/RiWordNet.html 
terms. Here, the Stanford Parser9 has been used 
to get the dependency relations. 
Ontology Information (Liu, 2012): We have 
counted the aspect terms in the training data. The 
aspect terms occurred more than five times in the 
corpus are considered during our experiments. At 
first, we have tested this ontology information on 
the development set and observed that the aspect 
terms with frequency five or more also give bet-
ter results in the test set. 
Sentiment Words: We have used the senti-
ment words as a feature for the sentiment identi-
fication tasks (Liu, 2012; Brody and Elhadad, 
2010). Words are identified as positive, negative 
or neutral using SentiWordNet10. 
WordNet Information: The RiTa.WordNet 
package has been used to extract different prop-
erties of the words.  
For aspect category identification, we have 
matched the hypernym tree of each word with 
the four categories (service, price, food, and am-
bience). If the hypernym tree does not contain 
any of such words, we check the next level hy-
pernym tree of the words derived from hypernym 
of previous word. We have checked up to the 
second degree hypernym tree. We also searched 
hypernym tree of the synset of each word.  
Number of Sentence: It has been found that 
many reviews contain more than one sentence. 
Therefore, we have included the number of sen-
tence as a feature based on the output of Stanford 
Parser. We have split the output of Stanford 
Parser by the mark, ?(S?.  
In case of our experiments, the stop words are 
excluded. Total of 329 stop words was prepared 
manually.  
5 Experimentation and Result Analysis 
We have used the CRF++ 0.58 11 , an open 
source tool for implementing the machine learn-
ing framework for our experiments. CRF is well 
known for sequence labeling tasks (Lafferty et 
al., 2001). Similarly, in the present task, the as-
pect terms use the context information and are 
represented in sequences. Many of the aspect 
terms are multiword expressions such as ?hard 
disk?. We have created different templates for 
different subtasks to capture all the relations be-
tween different sequence related features.  
 
                                                 
9http://nlp.stanford.edu/software/lex-parser.shtml 
10http://sentiwordnet.isti.cnr.it/ 
11http://crfpp.googlecode.com/svn/trunk/doc/index.htm 
372
a. Classification of Aspect Term 
Features used in case of identifying aspect 
terms are POS, POS Frequency, Before be verb, 
Inanimate word, objects of the sentence, ontolo-
gy information. We have used several rules to 
identify these features. Then, we have used the 
CRF++ to identify the aspect terms. Some post 
processing techniques are also used in order to 
get better accuracy. The present system identifies 
only single word aspect terms. But it is found in 
the training data that many aspect terms consist 
of multiple words. Therefore, if there is a stop 
word in between two system identified aspect 
words, the stop word is also considered as a part 
of the aspect term. We have joined the aspect 
words along with the stop words to form a single 
but multiword aspect terms.  
Precisions, Recalls and F-scores are recorded 
for our system in Table 1. The maximum F-
scores achieved in the aspect term identification 
task for Laptop and Restaurant are 0.7455012 
and 0.84012544, respectively. Our system per-
forms better on Restaurant reviews than Laptop 
reviews.  
 Laptop Restaurant 
Precision 0.4938838 0.6481481 
Recall 0.7442396 0.8184855 
F-score 0.59375 0.72342515 
Table 1: JU_CSE system result for aspect 
term identification. 
b. Classification of Aspect Category 
Features used in this experiment are POS, De-
pendency relations for object and a few semantic 
relations of WordNet. In this subtask, we have 
also used aspect term knowledge as a feature. 
We identified the POS of the words using Stan-
ford CoreNLP tool and used the words which are 
not listed in our stop-word list. The objects are 
identified from the dependency relations. The 
hpernym trees of these words are searched up to 
second degree to find four aspect categories 
(service, price, food, and ambience). If we don?t 
find these four categories in the hypernym tree, 
we increase the frequency of anecdotes/ miscel-
laneous category. Frequency counts of these 
matched words are listed as a feature. The accu-
racy of the system for aspect categories in the 
Restaurant reviews are shown in Table 2.  
Maximum F-score achieved in this aspect cat-
egory identification is 0.8857715. The main 
problem faced in this task was to assign the an-
ecdotes/ miscellaneous category to the respective 
reviews. There are many cases in which the an-
ecdotes/miscellaneous categories occurred with 
other categories. In these cases, our system fails 
to identify the anecdotes/miscellaneous category.  
 
Restaurant 
Precision Recall F-score 
0.7307317 0.68029064 0.7046096 
Table 2: JU_CSE system result for aspect 
category identification. 
We have also observed that every review has 
at least one category. If any word of the review 
does not belong to any of the four categories, we 
assign these reviews with anecdotes/ miscellane-
ous category at the time of post processing.  
c. Classification of Sentiment of Aspect 
term and category 
Features used in these experiments are POS, 
Positive, Negative and Neutral words and num-
ber of sentences. Some reviews with multiple 
sentences contain different sentiments associated 
with different aspect terms. This observation also 
leads to conflict sentiment. Therefore, we have 
also included the aspect term and aspect catego-
ry information during sentiment identification. 
The accuracy of the system is given in the Table 
3. 
Accuracy 
? 
Aspect 
Term  
Sentiment 
Aspect 
Category 
Sentiment 
Laptop 0.5321101 NaN 
Restaurant 0.65547705 0.6409756 
Table 3: JU_CSE system result for aspect 
term and category sentiment identification. 
Our system performs moderate in case of sen-
timent identification. Mainly, the system was 
biased towards the positive tags. It is found that 
the number of positive tags in the training data 
was more as compared to others. We have ob-
served that a conflict tag occurs when an aspect 
term was present as both positive and negative. 
As the present system identifies the sentiment 
based on word level only, it was unable to detect 
the conflict tags. The feature, number of sentenc-
es fails to identify the conflict tags. Therefore, 
we need to find more suitable features for our 
system to improve the accuracy. 
373
6 Conclusion 
In this paper, we have presented a CRF based 
system for identifying the aspect terms, aspect 
categories and their sentiments. We believe that 
this problem will become increasingly important 
for common people. This task will not only be 
useful to common shoppers, but also crucial to 
product manufacturers and restaurateurs.  
Overall accuracies of our system were moder-
ate. In future, we will include more suitable fea-
tures to improve accuracy of our system. We also 
intend to explore different machine learning al-
gorithms for these tasks in future.  
Reference 
Benjamin Snyder and Regina Barzilay. 2007. Multi-
ple Aspect Ranking Using the Good Grief Algo-
rithm. In Proceedings of the Human Language 
Technologies: The Annual Conference of the North 
American Chapter of the Association for Computa-
tional Linguistics (NAACL-HLT 2007), pp. 300-
307. 
Bing Liu. 2012. Sentiment Analysis and Opinion 
Mining. Synthesis Lectures on Human Language 
Technologies 5, no. 1 (2012): 1-167. 
Braja G. Patra, Hiroya Takamura, Dipankar Das, 
Manabu Okumura, and Sivaji Bandyopadhyay. 
2013. Construction of Emotional Lexicon Using 
Potts Model. In Proceedings of the 6th Internation-
al Joint Conference on Natural Language Pro-
cessing (IJCNLP-2013), Nagoya, Japan, pp. 674?
679. 
Carlo Strapparava, and Alessandro Valitutti. 2004. 
WordNet Affect: an Affective Extension of Word-
Net. In LREC, vol. 4, pp. 1083-1086. 
Dipankar Das and Sivaji Bandyopadhyay. 2010. Ex-
tracting emotion topics from blog sentences: use of 
voting from multi-engine supervised classifiers. In 
Proceedings of the 2nd international workshop on 
Search and mining user-generated contents, pp. 
119-126. 
Ganu Gayatree, Noemie Elhadad, and Amelie Marian. 
2009. Beyond the stars: Improving rating predic-
tions using review text content. In Proceedings of 
the 12th International Workshop on the Web and 
Databases, Providence, Rhode Island. 
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating 
facts from opinions and identifying the polarity of 
opinion sentences. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language 
Processing (EMNLP-2013), pp. 129-136.  
Koby Crammer and Yoram Singer. 2001. Pranking 
with ranking. In NIPS, vol. 14, pp. 641-647. 
John Lafferty, Andrew McCallum, Fernando C.N. 
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of the 18th Interna-
tional Conference on Machine Learning (ICML 
2001), pp. 282-289.  
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th 
ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining, pp. 168-
177. 
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In 
proceedings of the Human Language Technology 
Conference: Conference on Empirical Methods in 
Natural Language Processing (HLT-EMNLP). 
Morristown, NJ, USA, pp. 339?346. 
Samaneh Moghaddam and Martin Ester. 2010. Opin-
ion digger: an unsupervised opinion miner from 
unstructured product reviews. In Proceedings of the 
19th ACM international conference on Information 
and knowledge management, pp. 1825-1828. 
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online re-
views. In Proceedings of the Human Language 
Technologies: The 2010 Annual Conference of the 
North American Chapter of the Association for 
Computational Linguistics (HLT-NAACL). 
Soo-Min Kim and Eduard Hovy. 2006. Extracting 
opinions, opinion holders, and topics expressed in 
online news media text. In Proceedings of the 
Workshop on Sentiment and Subjectivity in Text, 
pp. 1-8. 
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. SentiWordNet 3.0: An Enhanced 
Lexical Resource for Sentiment Analysis and 
Opinion Mining. In LREC, vol. 10, pp. 2200-2204. 
Yohan Jo and Alice H. Oh. 2011. Aspect and senti-
ment unification model for online review analysis. 
In Proceedings of the fourth ACM international 
conference on Web search and data mining.  
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 
2011. Clustering product features for opinion min-
ing. In Proceedings of the fourth ACM internation-
al conference on Web search and data mining, pp. 
347-354. 
Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, 
Haris Papageorgiou, Ion Androutsopoulos, and 
Suresh Manandhar. 2014. SemEval-2014 Task 4: 
Aspect Based Sentiment Analysis. In Proceedings 
of the 8th International Workshop on Semantic 
Evaluation (SemEval 2014), Dublin, Ireland. 
374
JU-PTBSGRE:  GRE Using Prefix Tree Based Structure  
                            Abstract 
This paper presents a Prefix Tree based model 
of Generation of Referring Expression (RE). 
Our algorithm PTBSGRE works in two phas-
es. First, an encoded prefix tree is constructed 
describing the domain structure. Subsequent-
ly, RE is generated using that structure. We 
evaluated our system using Dice, MASI, Ac-
curacy, Minimality and Uniqueness scoring 
method using standard TEVAl tool and the re-
sult is encouraging. 
1 Introduction 
Generation of referring expression (GRE) is an 
important task in the field of Natural Language 
Generation (NLG) systems. The existing algo-
rithms in GRE lie in two extremities. Incremental 
Algorithm is simple and speedy but less expressive 
in nature whereas others are complex and exhaus-
tive but more expressive in nature. We propose a 
new Prefix Tree (Trie) based framework for mod-
eling GRE problems. It incorporates intricate fea-
tures of GRE (like set and boolean descriptions, 
context sensitivity, relational description etc.) 
while achieving attractive properties of Incremen-
tal algorithm (simplicity, speed etc.). The prefix 
tree based algorithm works in two phases. First, it 
encodes the description, stored in the knowledge 
base, in the form of prefix tree structure. Secondly, 
it generates the referring expression identifying the 
target object, which is basically a node search 
problem in the tree. The edges in our encoded trie 
structure are labeled and the path from root to that 
node forms the distinguishing description for the 
target object.  
Let D be the Domain, r be the target object and 
P be the ?PreferredAttributes? List.The Trie con-
structionn algorithm  ConstructTrie(D,P,T) is 
shown in figure 1, Referring expression generation 
algorithm MakeRefExpr(r,p,T,L) is shown in 
figure 2, where T is a node pointer and p is pointer 
to parent of that node. Our algorithm MakeRe-
fExpr returns set of attribute-values L to identify r  
in the domain. [[Ni]]= {d |d?D and d is stored at 
node Ni where Ni is an i-th level node}. Card(N) is 
cardinality of set of objects in node N. 
 
Figure 1. Prefix Tree Generation Algorithm 
 
Figure 2. Expression Generation Algorithm 
The significant achievement is that incompleteness 
of previous algorithms can be tackled in this model 
in a straightforward way. For example, in case of 
vague descriptions (overlapping properties), In-
cremental and other algorithms are unable to find 
unambiguous description even if it exists but our 
prefix tree model takes into account hearer model      
                         
                     Sibabrata Paladhi 
 
Sivaji Bandyopadhyay 
            Department of Computer Sc. & Engg. Department of Computer Sc. & Engg. 
Jadavpur University, India Jadavpur University, India 
            sibabrata_paladhi@yahoo.com            sivaji_cse_ju@yahoo.com 
 
 
 
230
and generate description for identifying the target 
object. Besides, in case of Boolean, plural, context 
sensitive and relational description generation our 
model provides a simple and linguistically rich 
approach to GRE. 
2 Evaluation Results  
In Table 1 and 2 the evaluation results for Furni-
ture and People data has been shown. 
 
 
Table1: Evaluation Result of Furniture data 
 
Table2: Evaluation Result of People data 
References  
R. Dale and E. Reiter. 1995. Computational Interpretations of 
the Gricean Maxims in the generation of Referring Expres-
sions. Cognitive Science (18): 233 ?263 
S. Paladhi and S. Bandyopadhyay. 2008. Generation of Refer-
ring Expression Using Prefix Tree Structure. Proceedings 
of  IJCNLP: 697-702 
van Deemter. 2002. Boolean Extensions of Incremental Algo-
rithm. Computational Linguistics 28(1): 37-52 
231
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 71?75,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
English to Indian Languages Machine Transliteration System at 
NEWS 2010 
Amitava Das1, Tanik Saikh2, Tapabrata Mondal3, Asif Ekbal4, Sivaji Bandyopadhyay5 
Department of Computer Science and Engineering1,2,3,5 
Jadavpur University,  
Kolkata-700032, India  
amitava.santu@gmail.com1, tanik4u@gmail.com2, tapabratamon-
dal@gmail.com3, sivaji_cse_ju@yahoo.com5  
Department of Computational Linguistics4 
University of Heidelberg 
Im Neuenheimer Feld 325 
69120 Heidelberg, Germany 
ekbal@cl.uni-heidelberg.de 
 
Abstract 
 
This paper reports about our work in the 
NEWS 2010 Shared Task on Transliteration 
Generation held as part of ACL 2010. One 
standard run and two non-standard runs were 
submitted for English to Hindi and Bengali 
transliteration while one standard and one non-
standard run were submitted for Kannada and 
Tamil. The transliteration systems are based 
on Orthographic rules and Phoneme based 
technology. The system has been trained on 
the NEWS 2010 Shared Task on Translitera-
tion Generation datasets. For the standard run, 
the system demonstrated mean F-Score values 
of 0.818 for Bengali, 0.714 for Hindi, 0.663 
for Kannada and 0.563 for Tamil. The reported 
mean F-Score values of non-standard runs are 
0.845 and 0.875 for Bengali non-standard run-
1 and 2, 0.752 and 0.739 for Hindi non-
standard run-1 and 2, 0.662 for Kannada non-
standard run-1 and 0.760 for Tamil non-
standard run-1. Non-Standard Run-2 for Ben-
gali has achieved the highest score among all 
the submitted runs. Hindi Non-Standard Run-1 
and Run-2 runs are ranked as the 5th and 6th 
among all submitted Runs. 
1 Introduction 
Transliteration is the method of translating one 
source language word into another target lan-
guage by expressing and preserving the original 
pronunciation in their source language. Thus, the 
central problem in transliteration is predicting the 
pronunciation of the original word. Translitera-
tion between two languages that use the same set 
of alphabets is trivial: the word is left as it is. 
However, for languages those use different al-
phabet sets the names must be transliterated or 
rendered in the target language alphabets. Trans-
literation of words is necessary in many applica-
tions, such as machine translation, corpus align-
ment, cross-language Information Retrieval, in-
formation extraction and automatic lexicon ac-
quisition. In the literature, a number of translite-
ration algorithms are available involving English 
(Li et al, 2004; Vigra and Khudanpur, 2003; Go-
to et al, 2003), European languages (Marino et 
al., 2005) and some of the Asian languages, 
namely Chinese (Li et al, 2004; Vigra and Khu-
danpur, 2003), Japanese (Goto et al, 2003; 
Knight and Graehl, 1998), Korean (Jung et al, 
2000) and Arabic (Al-Onaizan and Knight, 
2002a; Al-Onaizan and Knight, 2002c). Recent-
ly, some works have been initiated involving 
Indian languages (Ekbal et al, 2006; Ekbal et al, 
2007; Surana and Singh, 2008). The detailed re-
port of our participation in NEWS 2009 could be 
found in (Das et al, 2009).  
One standard run for Bengali (Bengali 
Standard Run: BSR), Hindi (Hindi Standard 
Run: HSR), Kannada (Kannada Standard Run: 
KSR) and Tamil (Tamil Standard Run: TSR) 
were submitted. Two non-standard runs for Eng-
lish to Hindi (Hindi Non-Standard Run 1 & 2: 
HNSR1 & HNSR2) and Bengali (Bengali Non-
Standard Run 1 & 2: BNSR1 & BNSR1) transli-
teration were submitted. Only one non-standard 
run were submitted for Kannada (Kannada Non-
Standard Run-1: KNSR1) and Tamil (Tamil 
Non-Standard Run-1: TNSR1). 
71
2 Machine Transliteration Systems  
Five different transliteration models have been 
proposed in the present report that can generate 
the transliteration in Indian language from an 
English word. The transliteration models are 
named as Trigram Model (Tri), Joint Source-
Channel Model (JSC), Modified Joint Source-
Channel Model (MJSC), Improved Modified 
Joint Source-Channel Model (IMJSC) and Inter-
national Phonetic Alphabet Based Model (IPA). 
Among all the models the first four are catego-
rized as orthographic model and the last one i.e. 
IPA based model is categorized as phoneme 
based model. 
An English word is divided into Translitera-
tion Units (TUs) with patterns C*V*, where C 
represents a consonant and V represents a vowel. 
The targeted words in Indian languages are di-
vided into TUs with patterns C+M?, where C 
represents a consonant or a vowel or a conjunct 
and M represents the vowel modifier or matra. 
The TUs are the basic lexical units for machine 
transliteration. The system considers the English 
and Indian languages contextual information in 
the form of collocated TUs simultaneously to 
calculate the plausibility of transliteration from 
each English TU to various Indian languages 
candidate TUs and chooses the one with maxi-
mum probability. The system learns the map-
pings automatically from the bilingual NEWS 
2010 training set being guided by linguistic fea-
tures/knowledge. The output of the mapping 
process is a decision-list classifier with collo-
cated TUs in the source language and their 
equivalent TUs in collocation in the target lan-
guage along with the probability of each decision 
obtained from the training set. A Direct example 
base has been maintained that contains the bilin-
gual training examples that do not result in the 
equal number of TUs in both the source and tar-
get sides during alignment. The Direct example 
base is checked first during machine translitera-
tion of the input English word. If no match is 
obtained, the system uses direct orthographic 
mapping by identifying the equivalent TU in In-
dian languages for each English TU in the input 
and then placing the target language TUs in or-
der. The IPA based model has been used for 
English dictionary words. Words which are not 
present in the dictionary are handled by other 
orthographic models as Trigram, JSC, MJSC and 
IMJSC. 
The transliteration models are described below 
in which S and T denotes the source and the tar-
get words respectively: 
3 Orthographic Transliteration models 
The orthographic models work on the idea of 
TUs from both source and target languages. The 
orthographic models used in the present system 
are described below. For transliteration, P(T), 
i.e., the probability of transliteration in the target 
language, is calculated from a English-Indian 
languages bilingual database If, T is not found in 
the dictionary, then a very small value is 
assigned to P(T). These models have been 
desribed in details in Ekbal et al (2007). 
3.1 Trigram 
This is basically the Trigram model where the 
previous and the next source TUs are considered 
as the context.  
( | ) ( , | )1, 11
K
P S T P s t s sk k kk
= < >?
? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.2  Joint Source-Channel Model (JSC) 
This is essentially the Joint Source-Channel 
model (Hazhiou et al, 2004) where the 
previous TUs with reference to the current TUs 
in both the source (s) and the target sides (t) are 
considered as the context.  
( | ) ( , | , )11
K
P S T P s t s tk kk
= < > < >?
?
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.3 Modified Joint Source-Channel Model 
(MJSC) 
In this model, the previous and the next TUs in 
the source and the previous target TU are 
considered as the context. This is the Modified 
Joint Source-Channel model. 
( | ) ( , | , )1, 11
K
P S T P s t s t sk k kk
= < > < >?
? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
3.4 Improved Modified Joint Source-
Channel Model (IMJSC) 
In this model, the previous two and the next TUs 
in the source and the previous target TU are 
considered as the context. This is the  Improved 
Modified Joint Source-Channel model. 
72
( | ) ( , | , )1 1, 11
K
P S T P s t s s t sk k k kk
= < > < >? + ? +
=
 
( ) arg max { ( ) ( | )}S T S P T P S T
T
? = ?  
4 International Phonetic Alphabet 
(IPA) Model 
The NEWS 2010 Shared Task on Transliteration 
Generation challenge addresses general domain 
transliteration problem rather than named entity 
transliteration. Due to large number of dictionary 
words as reported in Table 1 in NEWS 2010 data 
set a phoneme based transliteration algorithm  
has been devised.  
 Train Dev Test 
Bengali 7.77% 5.14% 6.46% 
Hindi 27.82% 15.80% 3.7% 
Kannada 27.60% 14.63% 4.4% 
Tamil 27.87% 17.31% 3.0% 
Table 1: Statistics of Dictionary Words 
The International Phonetic Alphabet (IPA) is a 
system of representing phonetic notations based 
primarily on the Latin alphabet and devised by 
the International Phonetic Association as a 
standardized representation of the sounds of 
spoken language. The machine-readable 
Carnegie Mellon Pronouncing Dictionary 1  has 
been used as an external resource to capture 
source language IPA structure. The dictionary 
contains over 125,000 words and their 
transcriptions with mappings from words to their 
pronunciations in the given phoneme set. The 
current phoneme set contains 39 distinct 
phonemes. As there is no such parallel IPA 
dictionary available for Indian languages, 
English IPA structures have been mapped to TUs 
in Indian languages during training. An example 
of such mapping between phonemes and TUs are 
shown in Table 3, for which the vowels may 
carry lexical stress as reported in Table 2. This 
phone set is based on the ARPAbet2 symbol set 
developed for speech recognition uses.  
Representation Stress level 
0 No 
1 Primary 
2 Secondary 
Table 2: Stress Level on Vowel 
A pre-processing module checks whether a 
targeted source English word is a valid 
dictionary word or not. The dictionary words are 
then handled by phoneme based transliteration 
module. 
                                                 
1
 www.speech.cs.cmu.edu/cgi-bin/cmudict 
2
 http://en.wikipedia.org/wiki/Arpabet 
Phoneme Example Translation TUs 
AA odd AA0-D - 
AH hut HH0-AH-T - 
D dee D-IY1 -?	 
Table 3: Phoneme Map Patterns of English 
Words and TUs 
In the target side we use our TU segregation 
logic to get phoneme wise transliteration pattern. 
We present this problem as a sequence labelling 
problem, because transliteration pattern changes 
depending upon the contextual phonemes in 
source side and TUs in the target side. We use a 
standard machine learning based sequence 
labeller Conditional Random Field (CRF)3 here. 
IPA based model increased the performance 
for Bengali, Hindi and Tamil languages as 
reported in Section 6. The performance has 
decreased for Kannada. 
5 Ranking 
The ranking among the transliterated outputs 
follow the order reported in Table 4: The ranking 
decision is based on the experiments as described 
in (Ekbal et al, 2006) and additionally based on 
the experiments on NEWS 2010 development 
dataset. 
Word Type  Ranking Order 1 2 3 4 5 
Dictionary IPA IMJSC MJSC JSC Tri 
Non-
Dictionary IMJSC MJSC JSC Tri - 
Table 4: Phoneme Patterns of English Words 
In BSR, HSR, KSR and TSR the orthographic 
TU based models such as: IMJSC, MJSC, JSC 
and Tri have been used only trained by NEWS 
2010 dataset. In BNSR1 and HNSR1 all the or-
thographic models have been trained with addi-
tional census dataset as described in Section 6. In 
case of BNSR2, HNSR2, KNSR1 and TNSR1 
the output of the IPA based model has been add-
ed with highest priority. As no census data is 
available for Kannada and Tamil therefore there 
is only one Non-Standard Run was submitted for 
these two languages only with the output of IPA 
based model along with the output of Standard 
Run. 
6 Experimental Results  
We have trained our transliteration models using 
the NEWS 2010 datasets obtained from the 
NEWS 2010 Machine Transliteration Shared 
Task (Li et al, 2010). A brief statistics of the 
                                                 
3
 http://crfpp.sourceforge.net 
73
datasets are presented in Table 5. During train-
ing, we have split multi-words into collections of 
single word transliterations. It was observed that 
the number of tokens in the source and target 
sides mismatched in various multi-words and 
these cases were not considered further. Follow-
ing are some examples:  
Paris Charles de Gaulle  ???? 
???? ??	?
 ? ?????  
Suven Life Scie  ??? ??
	??
 
Delta Air Lines  ???? 
???	
 
In the training set, some multi-words were 
partly translated and not transliterated. Such ex-
amples were dropped from the training set. In the 
following example the English word ?National? 
is being translated in the target as ??????. 
Australian National Univer-
sity  ????? ???? 
?Proceedings of the 8th Workshop on Asian Language Resources, pages 47?55,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
Labeling Emotion in Bengali Blog Corpus ? A Fine Grained 
Tagging at Sentence Level 
Dipankar Das 
Department of Computer Science  
& Engineering,  
Jadavpur University 
dipankar.dipnil2005@gmail.com 
Sivaji Bandyopadhyay 
Department of Computer Science  
& Engineering,  
Jadavpur University  
sivaji_cse_ju@yahoo.com 
 
Abstract 
Emotion, the private state of a human 
entity, is becoming an important topic 
in Natural Language Processing (NLP) 
with increasing use of search engines. 
The present task aims to manually an-
notate the sentences in a web based 
Bengali blog corpus with the emotional 
components such as emotional expres-
sion (word/phrase), intensity, associ-
ated holder and topic(s). Ekman?s six 
emotion classes (anger, disgust, fear, 
happy, sad and surprise) along with 
three types of intensities (high, general 
and low) are considered for the sen-
tence level annotation. Presence of dis-
course markers, punctuation marks, 
negations, conjuncts, reduplication, 
rhetoric knowledge and especially 
emoticons play the contributory roles 
in the annotation process. Different 
types of fixed and relaxed strategies 
have been employed to measure the 
agreement of the sentential emotions, 
intensities, emotional holders and top-
ics respectively. Experimental results 
for each emotion class at word level on 
a small set of the whole corpus have 
been found satisfactory.    
1 Introduction 
Human emotion described in texts is an impor-
tant cue for our daily communication but the 
identification of emotional state from texts is 
not an easy task as emotion is not open to any 
objective observation or verification (Quirk et 
al., 1985). Emails, weblogs, chat rooms, online 
forums and even twitter are considered as the 
affective communication substrates to analyze 
the reaction of emotional catalysts. Among 
these media, blog is one of the communicative 
and informative repository of text based emo-
tional contents in the Web 2.0 (Lin et al, 
2007).  
Rapidly growing web users from multilin-
gual communities focus the attention to im-
prove the multilingual search engines on the 
basis of sentiment or emotion. Major studies 
on Opinion Mining and Sentiment Analyses 
have been attempted with more focused per-
spectives rather than fine-grained emotions. 
The analyses of emotion or sentiment require 
some basic resource. An emotion-annotated 
corpus is one of the primary ones to start with.  
The proposed annotation task has been car-
ried out at sentence level. Three annotators 
have manually annotated the Bengali blog sen-
tences retrieved from a web blog archive1 with 
Ekman?s six basic emotion tags (anger (A),
disgust (D), fear (F), happy (H), sad (Sa) and 
surprise (Su)). The emotional sentences are 
tagged with three types of intensities such as 
high, general and low. The sentences of non-
emotional (neutral) and multiple (mixed) cate-
gories are also identified. The identification of 
emotional words or phrases and fixing the 
scope of emotional expressions in the sen-
tences are carried out in the present task. Each 
of the emoticons is also considered as individ-
ual emotional expressions. The emotion holder 
and relevant topics associated with the emo-
tional expressions are annotated considering 
the punctuation marks, conjuncts, rhetorical 
structures and other discourse information. The 
knowledge of rhetorical structure helps in re-
moving the subjective discrepancies from the 
                                                 
1 www.amarblog.com 
47
writer?s point of view. The annotation scheme 
is used to annotate 123 blog posts containing 
4,740 emotional sentences having single emo-
tion tag and 322 emotional sentences for mixed 
emotion tagss along with 7087 neutral sen-
tences in Bengali. Three types of standard 
agreement measures such as Cohen?s kappa 
() (Cohen, 1960; Carletta, 1996), Measure of 
Agreement on Set-valued Items (MASI) (Pas-
sonneau, 2004) and agr (Wiebe et al, 2005) 
metrics are employed for annotating the emo-
tion related components. The relaxed agree-
ment schemes like MASI and agr are specially 
considered for fixing the boundaries of emo-
tional expressions and topic spans in the emo-
tional sentences. The inter annotator agreement 
of some emotional components such as senten-
tial emotions, holders, topics show satisfactory 
performance but the sentences of mixed emo-
tion and intensities of general and low show 
the disagreement. A preliminary experiment 
for word level emotion classification on a 
small set of the whole corpus yielded satisfac-
tory results.
The rest of the paper is organized as fol-
lows. Section 2 describes the related work. The 
annotation of emotional expressions, sentential 
emotion and intensities are described in Sec-
tion 3. In Section 4, the annotation scheme for 
emotion holder is described. The issues of 
emotional topic annotation are discussed in 
Section 5. Section 6 describes the preliminary 
experiments carried out on the annotated cor-
pus. Finally, Section 7 concludes the paper.     
2 Related Work 
One of the most well known tasks of annotat-
ing the private states in texts is carried out by 
(Wiebe et al, 2005).  They manually annotated 
the private states including emotions, opinions, 
and sentiment in a 10,000-sentence corpus (the 
MPQA corpus) of news articles. The opinion 
holder information is also annotated in the 
MPQA corpus but the topic annotation task has 
been initiated later by (Stoyanov and Cardie, 
2008a). In contrast, the present annotation 
strategy includes the fine-grained emotion 
classes and specially handles the emoticons 
present in the blog posts. 
(Alm et al, 2005) have considered eight 
emotion categories (angry, disgusted, fearful, 
happy, sad, positively surprised, negatively 
surprised) to accomplish the emotion annota-
tion task at sentence level. They have manually 
annotated 1580 sentences extracted from 22 
Grimms? tales. The present approach discusses 
the issues of annotating unstructured blog text 
considering rhetoric knowledge along with the 
attributes, e.g. negation, conjunct, reduplica-
tion etc.  
Mishne (2005) experimented with mood 
classification in a blog corpus of 815,494 posts 
from Livejournal 
(http://www.livejournal.com), a free weblog 
service with a large community. (Mihalcea and 
Liu, 2006) have used the same data source for 
classifying the blog posts into two particular 
emotions ? happiness and sadness. The blog 
posts are self-annotated by the blog writers 
with happy and sad mood labels. In contrast, 
the present approach includes Ekman?s six 
emotions, emotion holders and topics to ac-
complish the whole annotation task. 
(Neviarouskaya et al, 2007) collected 160 
sentences labeled with one of the nine emo-
tions categories (anger, disgust, fear, guilt, in-
terest, joy, sadness, shame, and surprise) and a 
corresponding intensity value from a corpus of 
online diary-like blog posts. On the other hand, 
(Aman and Szpakowicz, 2007) prepare an 
emotion-annotated corpus with a rich set of 
emotion information such as category, inten-
sity and word or phrase based expressions. The 
present task considers all the above emotion 
information during annotation. But, the present 
annotation task additionally includes the com-
ponents like emotion holder, single or multiple 
topic spans. 
The emotion corpora for Japanese were built 
for recognizing emotions (Tokuhisa et al, 
2008). An available emotion corpus in Chinese 
is Yahoo!?s Chinese news 
(http://tw.news.yahoo.com), which is used for 
Chinese emotion classification of news readers 
(Lin, et al, 2007). The manual annotation of 
eight emotional categories (expect, joy, love, 
surprise, anxiety, sorrow, angry and hate) 
along with intensity, holder, word/phrase, de-
gree word, negative word, conjunction, rheto-
ric, punctuation and other linguistic expres-
sions are carried out at sentence, paragraph as 
well as document level on 1,487 Chinese blog 
documents (Quan and Ren, 2009). In addition 
48
to the above emotion entities, the present ap-
proach also includes the annotation of single or 
multiple emotion topics in a target span. 
Recent study shows that non-native English 
speakers support the growing use of the Inter-
net 2.  This raises the demand of linguistic re-
sources for languages other than English. Ben-
gali is the fifth popular language in the World, 
second in India and the national language in 
Bangladesh but it is less computerized com-
pared to English. To the best of our knowl-
edge, at present, there is no such available cor-
pus that is annotated with detailed linguistic 
expressions for emotion in Bengali or even for 
other Indian languages. Thus we believe that 
this corpus would help the development and 
evaluation of emotion analysis systems in 
Bengali. 
3 Emotion Annotation 
Random collection of 123 blog posts contain-
ing a total of 12,149 sentences are retrieved 
from Bengali web blog archive 3  (especially 
from comics, politics, sports and short stories) 
to prepare the corpus. No prior training was 
provided to the annotators but they were in-
structed to annotate each sentence of the blog 
corpus based on some illustrated samples of 
the annotated sentences. Specially for annotat-
ing the emotional expressions and topic(s) in 
emotional sentences, the annotators are free in 
selecting the texts spans. This annotation 
scheme is termed as relaxed scheme. For other 
emotional components, the annotators are 
given items with fixed text spans and in-
structed to annotation the items with definite 
tags. 
3.1 Identifying Emotional Expressions for 
Sentential Emotion and Intensity 
The identification of emotion or affect affixed 
in the text segments is a puzzle. But, the puzzle 
can be solved partially using some lexical 
clues (e.g. discourse markers, punctuation 
marks (sym), negations (NEG), conjuncts 
(CONJ), reduplication (Redup)), structural 
clues (e.g. rhetoric and syntactic knowledge) 
and especially some direct affective clues (e.g. 
                                                 
2 http://www.internetworldstats.com/stats.htm 
3 www.amarblog.com 
emoticons (emo_icon)). The identification of 
structural clues indeed requires the identifica-
tion of lexical clues.  
Rhetorical Structure Theory (RST) de-
scribes the various parts of a text, how they 
can be arranged and connected to form a whole 
text (Azar, 1999). The theory maintains that 
consecutive discourse elements, termed text 
spans, which can be in the form of clauses, 
sentences, or units larger than sentences, are 
related by a relatively small set (20?25) of rhe-
torical relations (Mann and Thompson, 1988). 
RST distinguishes between the part of a text 
that realizes the primary goal of the writer, 
termed as nucleus, and the part that provides 
supplementary material, termed satellite. The 
separation of nucleus from satellite is done 
based on punctuation marks (, ! @?), emoti-
cons, discourse markers (  jehetu [as], 	 
jemon [e.g.], 
 karon [because], 	 mane
[means]), conjuncts (e ebong [and], 
n 
kintu [but], a athoba [or]), causal verbs 
( ghotay [caused]) if they are explicitly 
specified in the sentences.  
Use of emotion-related words is not the sole 
means of expressing emotion. Often a 
sentence, which otherwise may not have an 
emotional word, may become emotion bearing 
depending on the context or underlying 
semantic meaning (Aman and Szpakowicz, 
2007). An empirical analysis of the blog texts 
shows two types of emotional expressions. The 
first category contains explicitly stated 
emotion word (EW) or phrases (EP) mentioned 
in the nucleus or in the satellite. Another 
category contains the implicit emotional clues 
that are identified based on the context or from 
the metaphoric knowledge of the expressions. 
Sometimes, the emotional expressions contain 
direct emotion words (EW) (

 koutuk 
[joke], 	 ananda [happy],  
ashcharjyo [surprise]), reduplication (Redup) 
(Proceedings of the 8th Workshop on Asian Language Resources, pages 56?63,
Beijing, China, 21-22 August 2010. c?2010 Asian Federation for Natural Language Processing
SentiWordNet for Indian Languages 
Amitava Das1 and Sivaji Bandyopadhyay2 
Department of Computer Science and Engineering  
Jadavpur University 
amitava.santu@gmail.com1 sivaji_cse_ju@yahoo.com2 
 
Abstract 
The discipline where sentiment/ opi-
nion/ emotion has been identified and 
classified in human written text is well 
known as sentiment analysis. A typical 
computational approach to sentiment 
analysis starts with prior polarity lex-
icons where entries are tagged with 
their prior out of context polarity as 
human beings perceive using their 
cognitive knowledge. Till date, all re-
search efforts found in sentiment lex-
icon literature deal mostly with English 
texts. In this article, we propose mul-
tiple computational techniques like, 
WordNet based, dictionary based, cor-
pus based or generative approaches for 
generating SentiWordNet(s) for Indian 
languages. Currently, SentiWordNet(s) 
are being developed for three Indian 
languages: Bengali, Hindi and Telugu. 
An online intuitive game has been de-
veloped to create and validate the de-
veloped SentiWordNet(s) by involving 
Internet population. A number of au-
tomatic, semi-automatic and manual 
validations and evaluation methodolo-
gies have been adopted to measure the 
coverage and credibility of the devel-
oped SentiWordNet(s). 
1 Introduction 
Sentiment analysis and classification from 
electronic text is a hard semantic disambigua-
tion problem. The regulating aspects of seman-
tic orientation of a text are natural language 
context information (Pang et al, 2002) lan-
guage properties (Wiebe and Mihalcea, 2006), 
domain pragmatic knowledge (Aue and Ga-
mon, 2005) and lastly most challenging is the 
time dimension (Read, 2005). 
The following example shows that the polar-
ity tag associated with a sentiment word de-
pends on the time dimension. During 90?s mo-
bile phone users generally reported in various 
online reviews about their color phones but in 
recent times color phone is not just enough. 
People are fascinated and influenced by touch 
screen and various software(s) installation fa-
cilities on these new generation gadgets. 
In typical computational approaches (Higa-
shinaka et al, 2007; Hatzivassiloglou et al, 
2000) to sentiment analysis researchers con-
sider the problem of learning a dictionary that 
maps semantic representations to verbaliza-
tions, where the data comes from opinionated 
electronic text. Although lexicons in these dic-
tionaries are not explicitly marked up with re-
spect to their contextual semantics, they con-
tain only explicit polarity rating and aspect 
indicators. Lexicon-based approaches can be 
broadly classified into two categories firstly 
where the discriminative polarity tag of lex-
icons is determined on labeled training data 
and secondly where the lexicons are manually 
compiled, the later constitutes the main effec-
tive approach.  
It is undoubted that the manual compilation 
is always the best way to create monolingual 
semantic lexicons, but manual methods are 
expensive in terms of human resources, it in-
volves a substantial number of human annota-
tors and it takes lot of time as well. In this pa-
per we propose several computational tech-
niques to generate sentiment lexicons in Indian 
languages automatically and semi-
automatically. In the present task, SentiWord-
56
Net(s) are being developed for the Bengali, 
Hindi and Telugu languages.  
Several prior polarity sentiment lexicons are 
available for English such as SentiWordNet 
(Esuli et. al., 2006), Subjectivity Word List 
(Wilson et. al., 2005), WordNet Affect list 
(Strapparava et al, 2004), Taboada?s adjective 
list (Taboada et al, 2006).  
Among these publicly available sentiment 
lexicon resources we find that SentiWordNet is 
most widely used (number of citation is higher 
than other resources1) in several applications 
such as sentiment analysis, opinion mining and 
emotion analysis. Subjectivity Word List is 
most trustable as the opinion mining system 
OpinionFinder2 that uses the subjectivity word 
list has reported highest score for opi-
nion/sentiment subjectivity (Wiebe and Riloff, 
2006). SentiWordNet is an automatically con-
structed lexical resource for English that as-
signs a positivity score and a negativity score 
to each WordNet synset.  
The subjectivity word list is compiled from 
manually developed resources augmented with 
entries learned from corpora. The entries in the 
subjectivity word list have been labeled with 
part of speech (POS) tags as well as either 
strong or weak subjective tag depending on the 
reliability of the subjective nature of the entry.  
These two resources have been merged au-
tomatically and the merged resource is used for 
SentiWordNet(s) generation in the present 
task.  
The generated sentiment lexicons or Senti-
WordNet(s) for several Indian languages most-
ly contain synsets (approximately 60%) of re-
spective languages. Synset based method is 
robust for any kind of monolingual lexicon 
creation and useful to avoid further word sense 
disambiguation problem in application domain.  
Additionally we have developed an online 
intuitive game to create and validate the devel-
oped SentiWordNet(s) by involving Internet 
population.  
The proposed approaches in this paper are 
easy to adopt for any new language. To meas-
ure the coverage and credibility of generated 
SentiWordNet(s) in Indian languages we have 
                                                 
1
 http://citeseerx.ist.psu.edu/ 
2
 http://www.cs.pitt.edu/mpqa/ 
developed several automatic and semi-
automatic evaluation methods. 
2 Related Works 
Various methods have been used in the litera-
ture such as WordNet based, dictionary based, 
corpus based or generative approaches for sen-
timent lexicon generation in a new target lan-
guage.  
Andreevskaia and Bergler, (2006) present a 
method for extracting sentiment-bearing adjec-
tives from WordNet using the Sentiment Tag 
Extraction Program (STEP). They did 58 
STEP runs on unique non-intersecting seed 
lists drawn from manually annotated list of 
positive and negative adjectives and evaluated 
the results against other manually annotated 
lists.  
The proposed methods in (Wiebe and Riloff, 
2006) automatically generate resources for 
subjectivity analysis for a new target language 
from the available resources for English. Two 
techniques have been proposed for the genera-
tion of target language lexicon from English 
subjectivity lexicon. The first technique uses a 
bilingual dictionary while the second method 
is a parallel corpus based approach using exist-
ing subjectivity analysis tools for English. 
Automatically or manually created lexicons 
may have limited coverage and do not include 
most semantically contrasting word pairs like 
antonyms. Antonyms are broadly categorized 
(Saif Mohammed, 2008) as gradable adjec-
tives (hot?cold, good?bad, friend?enemy) and 
productive adjectives (normal?abnormal, for-
tune?misfortune, implicit?explicit). The first 
type contains the semantically contrasting 
word pairs but the second type includes ortho-
graphic suffix/affix as a clue. The second type 
is highly productive using very less number of 
affixation rules.  
Degree of antonymy (Mohammad et al, 
2008) is defined to encompass the complete 
semantic range as a combined measure of the 
contrast in meaning conveyed by two antony-
my words and is identified by distributional 
hypothesis. It helps to measure relative senti-
ment score of a word and its antonym.   
Kumaran et al, (2008) introduced a beauti-
ful method for automatic data creation by on-
line intuitive games. A methodology has been 
57
proposed for community creation of linguistic 
data by community collaborative framework 
known as wikiBABEL3. It may be described as 
a revolutionary approach to automatically 
create large credible linguistic data by involv-
ing Internet population for content creation.  
For the present task we prefer to involve all 
the available methodologies to automatically 
and semi-automatically create and validate 
SentiWordNet(s) for three Indian languages. 
Automatic methods involve only computation-
al methods. Semi-automatic methods involve 
human interruption to validate system?s output. 
3 Source Lexicon Acquisition  
SentiWordNet and Subjectivity Word List 
have been identified as the most reliable source 
lexicons. The first one is widely used and the 
second one is robust in terms of performance. 
A merged sentiment lexicon has been devel-
oped from both the resources by removing the 
duplicates. It has been observed that 64% of 
the single word entries are common in the Sub-
jectivity Word List and SentiWordNet. The 
new merged sentiment lexicon consists of 
14,135 numbers of tokens. Several filtering 
techniques have been applied to generate the 
new list. 
A subset of 8,427 sentiment words has been 
extracted from the English SentiWordNet, by 
selecting those whose orientation strength is 
above the heuristically identified threshold 
value of 0.4. The words whose orientation 
strength is below 0.4 are ambiguous and may 
lose their subjectivity in the target language 
after translation. A total of weakly subjective 
2652 words are discarded from the 
Subjectivity word list as proposed in (Wiebe 
and Riloff, 2006). 
In the next stage the words whose POS 
category in the Subjectivity word list is 
undefined and tagged as ?anypos? are 
considered. These words may generate sense 
ambiguity issues in the next stages of 
subjectivity detection. The words are checked 
in the SentiWordNet list for validation. If a 
match is found with certain POS category, the 
word is added to the new merged sentiment 
                                                 
3
 http://research.microsoft.com/en-
us/projects/wikibabel/ 
lexicon. Otherwise the word is discarded to 
avoid ambiguities later. 
Some words in the Subjectivity word list are 
inflected e.g., memories. These words would 
be stemmed during the translation process, but 
some words present no subjectivity property 
after stemming (memory has no subjectivity 
property). A word may occur in the 
subjectivity list in many inflected forms. 
Individual clusters for the words sharing the 
same root form are created and then checked in 
the SentiWordNet for validation. If the root 
word exists in the SentiWordNet then it is 
assumed that the word remains subjective after 
stemming and hence is added to the new list. 
Otherwise the cluster is completely discarded 
to avoid any further ambiguities. 
Various statistics of the English Senti-
WordNet and Subjectivity Word List are re-
ported in Table 1.  
 SentiWordNet Subjectivity Word List 
En
tr
ie
s Single Multi Single Multi 
115424 79091 5866 990 
U
am
bi
-
gu
o
u
s 
W
o
rd
s 
20789 30000 4745 963 
D
isc
ar
de
d 
A
m
-
bi
gu
o
u
s 
W
o
rd
s Threshold Orientation  Strength 
Subjectivity 
Strength POS 
86944 30000 2652 928 
Table 1: English SentiWordNet and Subjec-
tivity Word List Statistics 
4 Target Lexicon Generation 
4.1 Bilingual Dictionary Based Approach 
A word-level translation process followed by 
error reduction technique has been adopted for 
generating the Indian languages 
SentiWordNet(s) from the English sentiment 
lexicon merged from the English 
SentiWordNet and the Subjectivity Word List.  
English to Indian languages synsets are be-
ing developed under Project English to Indian 
Languages Machine Translation Systems 
58
(EILMT)4, a consortia project funded by De-
partment of Information Technology (DIT), 
Government of India. These synsets are robust 
and reliable as these are created by native 
speakers as well as linguistics experts of the 
specific languages. For each language we have 
approximately 9966 synsets along with the 
English WordNet offset. These bilingual syn-
set dictionaries have been used along with lan-
guage specific dictionaries. 
A word level synset/lexical transfer tech-
nique is applied to each English synset/word in 
the merged sentiment lexicon. Each dictionary 
search produces a set of Indian languages syn-
sets/words for a particular English synset/word.  
4.1.1 Hindi 
Two available manually compiled English-
Hindi electronic dictionaries have been identi-
fied for the present task. First is the SHABD-
KOSH5  and the second one is Shabdanjali6 .  
These two dictionaries have been merged au-
tomatically by replacing the duplicates. The 
merged English-Hindi dictionary contains ap-
proximately 90,872 unique entries. The posi-
tive and negative sentiment scores for the Hin-
di words are copied from their English Senti-
WordNet.  
The bilingual dictionary based translation 
process has resulted 22,708 Hindi entries.  
4.1.2 Bengali 
An English-Bengali dictionary (approx-
imately 102119 entries) has been developed 
using the Samsad Bengali-English dictionary7. 
The positive and negative sentiment scores for 
the Bengali words are copied from their Eng-
lish SentiWordNet equivalents.  
The bilingual dictionary based translation 
process has resulted in 35,805 Bengali entries. 
A manual checking is done to identify the re-
liability of the words generated from automatic 
process. After manual checking only 1688 
                                                 
4
 http://www.cdacmumbai.in/e-ilmt 
5
 http://www.shabdkosh.com/ 
6
 
http://www.shabdkosh.com/content/category/downl
oads/ 
7
 
http://dsal.uchicago.edu/dictionaries/biswas_bengal
i/ 
words are discarded i.e., the final list consists 
of 34,117 words.  
4.2 Telugu 
Charles Philip Brown English-Telugu Dictio-
nary 8 , Aksharamala 9  English-Telugu Dictio-
nary and English-Telugu Dictionary 10  devel-
oped by Language Technology Research Cen-
ter (LTRC), International Institute of Hydera-
bad (IITH) have been chosen for the present 
task. There is no WordNet publicly available 
for Telugu and the corpus (Section 4.5) we 
used is small in size. Dictionary based ap-
proach is the main process for Telugu Senti-
WordNet generation.  
These three dictionaries have been merged 
automatically by replacing the duplicates. The 
merged English-Telugu dictionary contains 
approximately 112310 unique entries. The pos-
itive and negative sentiment scores for the Te-
lugu words are copied from their English Sen-
tiWordNet equivalents. 
The dictionary based translation process has 
resulted in 30,889 Telugu entries, about 88% 
of final Telugu SentiWordNet synsets. An on-
line intuitive game has been proposed in Sec-
tion 4.6 to automatically validate the devel-
oped Telugu SentiWordNet by involving In-
ternet population.  
4.3 WordNet Based Approach 
WordNet(s) are available for Hindi11  (Jha et 
al., 2001) and Bengali12 (Robkop et al, 2010) 
but publicly unavailable for Telugu. 
A WordNet based lexicon expansion strate-
gy has been adopted to increase the coverage 
of the generated SentiWordNet(s) through the 
dictionary based approach. The present algo-
rithm starts with English SentiWordNet syn-
sets that is expanded using synonymy and an-
tonymy relations in the WordNet. For match-
ing synsets we keep the exact score as in the 
source synset in the English SentiWordNet. 
The calculated positivity and negativity score 
                                                 
8
 http://dsal.uchicago.edu/dictionaries/brown/ 
9
 https://groups.google.com/group/aksharamala 
10
 
http://ltrc.iiit.ac.in/onlineServices/Dictionaries/Dict
_Frame.html 
11
 http://www.cfilt.iitb.ac.in/wordnet/webhwn/ 
12
 http://bn.asianwordnet.org/ 
59
for any target language antonym synset is cal-
culated as: 
1
1
p p
n n
T S
T S
= ?
= ?
 
where pS , nS  are the positivity and negativ-
ity score for the source language (i.e, English) 
and pT , nT  are the positivity and negativity 
score for target languages (i.e., Hindi and Ben-
gali) respectively. 
4.3.1 Hindi 
Hindi WordNet is a well structured and ma-
nually compiled resource and is being updated 
since last nine years. There is an available 
API13  for accessing the Hindi WordNet. Al-
most 60% of final SentiWordNet synsets in 
Hindi are generated by this method. 
4.3.2 Bengali 
The Bengali WordNet is being developed by 
the Asian WordNet (AWN) community. It on-
ly contains 1775 noun synsets as reported in 
(Robkop et al, 2010). A Web Service14  has 
been provided for accessing the Bengali 
WordNet. There are only a few number of 
noun synsets in the Bengali WordNet and other 
important POS category words for sentiment 
lexicon such as adjective, adverb and verb are 
absent. Only 5% new lexicon entries have been 
generated in this process.  
4.4 Antonym Generation 
Automatically or manually created lexicons 
have limited coverage and do not include most 
semantically contrasting word pairs. To over-
come the limitation and increase the coverage 
of the SentiWordNet(s) we present automatic 
antonymy generation technique followed by 
corpus validation to check orthographically 
generated antonym does really exist. Only 16 
hand crafted rules have been used as reported 
in Table 2. About 8% of Bengali, 7% of Hindi 
and 11% of Telugu SentiWordNet entries are 
generated in this process. 
 
                                                 
13
 
http://www.cfilt.iitb.ac.in/wordnet/webhwn/API_do
wnloaderInfo.php 
14
 http://bn.asianwordnet.org/services 
Affix/Suffix Word Antonym 
abX  Normal  Ab-normal 
misX Fortune Mis-fortune 
imX-exX  Im-plicit Ex-plicit 
antiX Clockwise Anti-clockwise 
nonX  Aligned Non-aligned 
inX-exX  In-trovert Ex-trovert 
disX Interest Dis-interest 
unX  Biased Un-biased 
upX-downX  Up-hill Down-hill 
imX  Possible Im-possible 
illX  Legal Il-legal 
overX-underX  Overdone Under-done 
inX  Consistent In-consistent 
rX-irX  Regular Ir-regular 
Xless-Xful  Harm-less Harm-ful 
malX  Function Mal-function 
Table 2: Rules for Generating Productive An-
tonyms 
4.5 Corpus Based Approach 
Language/culture specific words such as those 
listed below are to be captured in the devel-
oped SentiWordNet(s). But sentiment lexicon 
generation techniques via cross-lingual projec-
tion are unable to capture these words. As ex-
ample:  
????? (Sahera: A marriage-
wear) 
 (Durgapujo: A festiv-
al of Bengal) 
To increase the coverage of the developed 
SentiWordNet(s) and to capture the lan-
guage/culture specific words an automatic cor-
pus based approach has been proposed. At this 
stage the developed SentiWordNet(s) for the 
three Indian languages have been used as a 
seed list. Language specific corpus is automat-
ically tagged with these seed words and we 
have a simple tagset as SWP (Sentiment Word 
Positive) and SWN (Sentiment Word Nega-
tive). Although we have both positivity and 
negativity scores for the words in the seed list 
but we prefer a word level tag as either posi-
tive or negative following the highest senti-
ment score. 
A Conditional Random Field (CRF15) based 
Machine Learning model is then trained with 
the seed list corpus along with multiple lin-
guistics features such as morpheme, parts-of-
                                                 
15
 http://crfpp.sourceforge.net 
60
speech, and chunk label. These linguistics fea-
tures have been extracted by the shallow pars-
ers16  for Indian languages. An n-gram (n=4) 
sequence labeling model has been used for the 
present task.  
The monolingual corpuses used have been 
developed under Project English to Indian 
Languages Machine Translation Systems 
(EILMT). Each corpus has approximately 10K 
of sentences. 
4.6 Gaming Methodology 
There are several motivations behind develop-
ing an intuitive game to automatically create 
multilingual SentiWordNet(s). The assigned 
polarity scores to each synset may vary in time 
dimension. Language specific polarity scores 
may vary and it should be authenticated by 
numbers of language specific annotators. 
In the history of Information Retrieval re-
search there is a milestone when ESP17 game 
(Ahn et al, 2004) innovate the concept of a 
game to automatically label images available 
in World Wide Web. Highly motivated by the 
historical research we proposed a intuitive 
game to create and validate SentiWordNet(s) 
for Indian languages by involving internet 
population. 
 
Figure 1: Intuitive Game for SentiWordNet(s) 
Creation 
In the gaming interface a simple picture (re-
trieved by Google Image API18) along with a 
sentiment bearing word (retrieved randomly 
                                                 
16
 
http://ltrc.iiit.ac.in/showfile.php?filename=downloa
ds/shallow_parser.php 
17
 http://www.espgame.org/ 
18
 
http://code.google.com/apis/ajaxsearch/multimedia.
html 
from SentiWordNet) is displayed to a player 
and he/she is then been asked to capture his 
immediate sentiment as extreme positive, posi-
tive, extreme negative, negative or neutral by 
pressing appropriate emoticon buttons. A snap 
of the game is shown in the Figure 1. The sen-
timent score is calculated by the different emo-
ticons based on the inputs from the different 
players and then is assigned the scale as fol-
lows: extreme positive (pos: 0.5, neg: 0.0), 
positive (pos: 0.25, neg: 0.0), neutral (pos: 0.0, 
neg: 0.0), negative (pos: 0.0, 0.25), extreme 
negative (pos: 0.0, neg: 0.5). 
The score of a particular player is calculated 
on the basis of pre-stored sentiment lexicon 
scores in the generated SentiWordNet(s).  
5 Evaluation 
Andera Esuli and Fabrizio Sebastiani (2006) 
have calculated the reliability of the sentiment 
scores attached to every synsets in the English 
SentiWordNet. They have tagged sentiment 
words in the English WordNet with positive 
and negative sentiment scores. In the present 
task, these sentiment scores from English 
WordNet have been directly copied to the In-
dian language SentiWordNet(s).  
Two extrinsic evaluation strategies have 
been adopted for the developed Bengali Sen-
tiWordNet based on the two main usages of 
the sentiment lexicon as subjectivity classifier 
and polarity identifier. The Hindi and Telugu 
SentiWordNet(s) have not completely been 
evaluated.  
5.1 Coverage 
 NEWS BLOG 
Total number of  documents 100 - 
Total number of sentences 2234 300 
Avgerage number of sentences in 
a document 22 - 
Total number of wordforms 28807 4675 
Avgerage number of wordforms 
in a document 288 - 
Total number of distinct 
wordforms 17176 1235 
Table 3: Bengali Corpus Statistics 
We experimented with NEWS and BLOG 
corpora for subjectivity detection. Sentiment 
lexicons are generally domain independent but 
it provides a good baseline while working with 
sentiment analysis systems. The coverage of 
61
the developed Bengali SentiWordNet is eva-
luated by using it in a subjectivity classifier 
(Das and Bandyopadhyay, 2009). The statistics 
of the NEWS and BLOG corpora is reported in 
Table 3. 
For comparison with the coverage of Eng-
lish SentiWordNet the same subjectivity clas-
sifier (Das and Bandyopadhyay, 2009) has 
been applied on Multi Perspective Question 
Answering (MPQA) (NEWS) and IMDB Mov-
ie review corpus along with English Senti-
WordNet. The result of the subjectivity clas-
sifier on both the corpus proves that the cover-
age of the Bengali SentiWordNet is reasonably 
good. The subjectivity word list used in the 
subjectivity classifier is developed from the 
IMDB corpus and hence the experiments on 
the IMDB corpus have yielded high precision 
and recall scores. The developed Bengali Sen-
tiWordNet is domain independent and still its 
coverage is very good as shown in Table 4. 
 
Languages Domain Precision Recall 
English MPQA 76.08% 83.33% 
IMDB 79.90% 86.55% 
Bengali NEWS 72.16% 76.00% BLOG 74.6% 80.4% 
Table 4: Subjectivity Classifier using Senti-
WordNet 
5.2 Polarity Scores 
This evaluation metric measures the reliability 
of the associated polarity scores in the senti-
ment lexicons. To measure the reliability of 
polarity scores in the developed Bengali Sen-
tiWordNet, a polarity classifier (Das and Ban-
dyopadhyay, 2010) has been developed using 
the Bengali SentiWordNet alng with some 
other linguistic features. 
 
Features Overall Performance Incremented By 
SentiWordNet 47.60% 
Table 5: Polarity Performance Using Bengali 
SentiWordNet 
 
Feature ablation method proves that the as-
sociated polarity scores in the developed Ben-
gali SentiWordNet are reliable. Table 5 shows 
the performance of a polarity classifier using 
the Bengali SentiWordNet. The polarity wise 
overall performance of the polarity classifier is 
reported in Table 6. 
 
Polarity Precision Recall 
Positive 56.59% 52.89% 
Negative 75.57% 65.87% 
Table 6: Polarity-wise Performance Using 
Bengali SentiWordNet 
 
Comparative study with a polarity classifier 
that works with only prior polarity lexicon is 
necessary but no such works have been identi-
fied in literature.  
An arbitrary 100 words have been chosen 
from the Hindi SentiWordNet for human eval-
uation. Two persons are asked to manually 
check it and the result is reported in Table 7. 
The coverage of the Hindi SentiWordNet has 
not been evaluated, as no manually annotated 
sentiment corpus is available. 
 
Polarity Positive Negative 
Percentage 88.0% 91.0% 
Table 7: Evaluation of Polarity Score of De-
veloped Hindi SentiWordNet 
 
For Telugu we created a version of the game 
with Telugu words on screen. Only 3 users 
have played the Telugu language specific 
game till date. Total 92 arbitrary words have 
been tagged and the accuracy of the polarity 
scores is reported in Table 8. The coverage of 
Telugu SentiWordNet has not been evaluated, 
as no manually annotated sentiment corpus is 
available. 
Polarity Positive Negative 
Percentage 82.0% 78.0% 
Table 8: Evaluation of Polarity Score of De-
veloped Telugu SentiWordNet 
6 Conclusion 
SentiWordNet(s) for Indian languages are be-
ing developed using various approaches. The 
game based technique may be directed towards 
a new way for the creation of linguistic data 
not just only for SentiWordNet(s) but in either 
areas of NLP too.  
Presently only the Bengali SentiWordNet19 
is downloadable from the author?s web page. 
                                                 
19
 http://www.amitavadas.com/sentiwordnet.php 
62
References 
Andreevskaia Alina and Bergler Sabine. CLaC and 
CLaC-NB: Knowledge-based and corpus-based 
approaches to sentiment tagging. In Proceedings 
of the 4th International Workshop on Semantic 
Evaluations (SemEval-2007), pages 117?120, 
Prague, June 2007. 
Aue A. and Gamon M., Customizing sentiment 
classifiers to new domains: A case study. In Pro-
ceedings of Recent Advances in Natural Lan-
guage Processing (RANLP), 2005. 
Das A. and Bandyopadhyay S. (2010). Phrase-level 
Polarity Identification for Bengali, In Interna-
tional Journal of Computational Linguistics and 
Applications (IJCLA), Vol. 1, No. 1-2, Jan-Dec 
2010, ISSN 0976-0962, Pages 169-182. 
Das A. and Bandyopadhyay S. Subjectivity Detec-
tion in English and Bengali: A CRF-based Ap-
proach. In the Proceeding of ICON 2009. 
Esuli Andrea and Sebastiani Fabrizio. SentiWord-
Net: A publicly available lexical resource for 
opinion mining. In Proceedings of Language Re-
sources and Evaluation (LREC), 2006. 
Hatzivassiloglou, Vasileios and Wiebe Janyce. Ef-
fects of adjective orientation and gradability on 
sentence subjectivity. In Proceedings of COL-
ING-00, 18th International Conference on Com-
putational Linguistics. Saarbru?cken, GE. Pages 
299-305. 2000. 
Higashinaka Ryuichiro, Walker Marilyn, and Pra-
sad Rashmi. Learning to generate naturalistic ut-
terances using reviews in spoken dialogue sys-
tems. ACM Transactions on Speech and Lan-
guage Processing (TSLP), 2007. 
Jha S., Narayan D., Pande P. and Bhattacharyya P. 
A WordNet for Hindi, International Workshop 
on Lexical Resources in Natural Language 
Processing, Hyderabad, India, January 2001. 
Kumaran A., Saravanan K. and Maurice Sandor. 
WikiBABEL: Community Creation of Multilin-
gual Data, in the WikiSYM 2008 Conference, 
Porto, Portugal, Association for Computing Ma-
chinery, Inc., September 2008. 
Mihalcea Rada, Banea Carmen and Wiebe Janyce. 
Learning multilingual subjective language via-
cross-lingual projections. In Proceedings of the 
Association for Computational Linguistics 
(ACL), pages 976?983, Prague, Czech Republic, 
June 2007. 
Mohammad Saif, Dorr Bonnie, and Hirst Graeme. 
Computing Word-Pair Antonymy. In Proceed-
ings of the Conference on Empirical Methods in 
Natural Language Processing and Computational 
Natural Language Learning (EMNLP-2008), Oc-
tober 2008, Waikiki, Hawaii. 
Pang Bo, Lee Lillian, and Vaithyanathan Shivaku-
mar. Thumbs up? Sentiment classification using 
machine learning techniques. In Proceedings of 
the Conference on Empirical Methods in Natural 
Language Processing (EMNLP), pages 79?86, 
2002. 
Read Jonathon. Using emoticons to reduce depen-
dency in machine learning techniques for senti-
ment classification. In Proceedings of the ACL 
Student Research Workshop, 2005. 
Robkop Kergrit, Thoongsup Sareewan, Charoen-
porn Thatsanee, Sornlertlamvanich Virach and 
Isahara Hitoshi.WNMS: Connecting the Distri-
buted WordNet in the Case of Asian WordNet. . 
In the Proceeding of 5th International Confe-
rence of the Global WordNet Association 
(GWC-2010), Mumbai, India , 31st Jan. - 4th 
Feb., 2010. 
Wiebe Janyce and Mihalcea Rada. Word sense and 
subjectivity. In Proceedings of COLING/ACL-
06 the 21st Conference on Computational Lin-
guistics/Association for Computational Linguis-
tics. Sydney, Australia. Pages 1065--1072. 
Wiebe Janyce and Riloff Ellen. Creating Subjective 
and Objective Sentence Classifiers from Unan-
notated Texts. In Proceeding of International 
Conference on Intelligent Text Processing and 
Computational Linguistics, Mexico City, Pages 
475?486, 2006. 
Wilson Theresa, Wiebe Janyce and Hoffmann Paul 
(2005). Recognizing Contextual Polarity in 
Phrase-Level Sentiment Analysis. In Proceed-
ings of HLT/EMNLP 2005, Vancouver, Canada. 
63
Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), pages 2?11,
Beijing, August 2010
SemanticNet-Perception of Human Pragmatics   
Amitava Das1 and Sivaji Bandyopadhyay2 
Department of Computer Science and Engineering  
Jadavpur University 
amitava.santu@gmail.com1 sivaji_cse_ju@yahoo.com2  
 
Abstract 
SemanticNet is a semantic network of 
lexicons to hold human pragmatic 
knowledge. So far Natural Language 
Processing (NLP) research patronized 
much of manually augmented lexicon 
resources such as WordNet. But the 
small set of semantic relations like 
Hypernym, Holonym, Meronym and 
Synonym etc are very narrow to cap-
ture the wide variations human cogni-
tive knowledge. But no such informa-
tion could be retrieved from available 
lexicon resources. SemanticNet is the 
attempt to capture wide range of con-
text dependent semantic inference 
among various themes which human 
beings perceive in their pragmatic 
knowledge, learned by day to day cog-
nitive interactions with the surrounding 
physical world. SemanticNet holds 
human pragmatics with twenty well es-
tablished semantic relations for every 
pair of lexemes. As every pair of rela-
tions cannot be defined by fixed num-
ber of certain semantic relation labels 
thus additionally contextual semantic 
affinity inference in SemanticNet could 
be calculated by network distance and 
represented as a probabilistic score. 
SemanticNet is being presently devel-
oped for Bengali language. 
1 Historical Motivation 
Semantics (from Greek "??????????" - seman-
tikos) is the study of meaning, usually in lan-
guage. The word "semantics" itself denotes a 
range of ideas, from the popular to the highly 
technical. It is often used in ordinary language 
to denote a problem of understanding that 
comes down to word selection or connotation. 
We studied with various Psycholinguistics ex-
periments to understand how human natural 
intelligence helps to understand general se-
mantic from nature. Our study was to under-
stand the human psychology about semantics 
beyond language. We were haunting for the 
intellectual structure of the psychological and 
neurobiological factors that enable humans to 
acquire, use, comprehend and produce natural 
languages. Let?s come with an example of 
simple conversation about movie between two 
persons. 
Person A: Have you seen the 
movie ?No Man's Land?? How 
is it? 
Person B: Although it is 
good but you should see 
?The Hurt Locker?? 
May be the conversation looks very casual, 
but our intension was to find out the direction 
of the decision logic on the Person B?s brain. 
We start digging to find out the nature of hu-
man intelligent thinking. A prolonged discus-
sion with Person B reveals that the decision 
logic path to recommend a good movie was as 
the Figure 1. The highlighted red paths are the 
shortest semantic affinity distances of the hu-
man brain. 
We call it semantic thinking. Although the 
derivational path of semantic thinking is not 
such easy as we portrait in Figure 1 but we 
keep it easier for understandability. Actually a 
human try to figure out the closest semantic 
affinity node into his pragmatics knowledge by 
natural intelligence. In the previous example 
Person B find out with his intelligence that No 
Man's Land is a war movie and got Oscar 
2
award. Oscar award generally cracked by Hol-
lywood movies and thus Person B start search-
ing his pragmatics network to find out a movie 
fall into war genre, from Hollywood and may 
be got Oscar award. Person B finds out the 
name of a movie The Hurt Locker at nearer 
distance into his pragmatics knowledge net-
work which is an optimized recommendation 
that satisfy all the criteria. Noticeably Person B 
didn?t choice the other paths like Bollywood, 
Foreign movie etc. 
 
Figure 1: Semantic Thinking 
And thus our aim was to develop a computa-
tional lexicon structure for semantics as human 
pragmatics knowledge. We spare long time to 
find out the most robust structure to represent 
pragmatics knowledge properly and it should 
be easy understandable for next level of search 
and usability. 
We look into literature that probably direct 
to the direction of our ideological thinking. We 
found that in the year of 1996 Push Singh and 
Marvin Minsky proposed the field has shat-
tered into subfields populated by researchers 
with different goals and who speak very differ-
ent technical languages. Much has been 
learned, and it is time to start integrating what 
we've learned, but few researchers are widely 
versed enough to do so. They had a proposal 
for how to do so in their ConceptNet work. 
They developed lexicon resources like Con-
ceptNet (Liu and Singh, 2004). ConceptNet- 
ConceptNet is a large-scale semantic network 
(over 1.6 million links) relating a wide variety 
of ordinary objects, events, places, actions, and 
goals by only 20 different link types, mined 
from corpus. 
The present task of developing SemanticNet 
is to capture semantic affinity knowledge of 
human pragmatics as a lexicon database.  We 
extend our vision from the human common 
sense (as in ConceptNet) to human pragmatics 
and have proposed semantic relations for every 
pair of lexemes that cannot be defined by fixed 
number of certain semantic relation labels. 
Contextual semantic affinity inference in Se-
manticNet could be calculated by network dis-
tance and represented as a probabilistic score. 
SemanticNet is being presently developed for 
Bengali language. 
2 Semantic Roles 
The ideological study of semantic roles started 
age old ago since Panini?s karaka theory that 
assigns generic semantic roles to words in a 
natural language sentence. Semantic roles are 
generally domain specific in nature such as 
FROM_DESTINATION,TO_DESTINATION, 
DEPARTURE_TIME etc. Verb-specific se-
mantic roles have also been defined such as 
EATER and EATEN for the verb eat. The 
standard datasets that are used in various Eng-
lish SRL systems are: PropBank (Palmer et al, 
2005), FrameNet (Fillmore et al, 2003) and 
VerbNet (Kipper et al, 2006). These collec-
tions contain manually developed well-trusted 
gold reference annotations of both syntactic 
and predicate-argument structures.  
PropBank defines semantic roles for each 
verb. The various semantic roles identified 
(Dowty, 1991) are Agent, patient or theme etc. 
In addition to verb-specific roles, PropBank 
defines several more general roles that can ap-
ply to any verb (Palmer et al, 2005). 
FrameNet is annotated with verb frame se-
mantics and supported by corpus evidence. 
The frame-to-frame relations defined in Fra-
meNet are Inheritance, Perspective_on, Sub-
frame, Precedes, Inchoative_of, Causative_of 
and Using. Frame development focuses on pa-
raphrasability (or near paraphrasability) of 
words and multi-words.  
VerbNet annotated with thematic roles refer 
to the underlying semantic relationship be-
tween a predicate and its arguments. The se-
mantic tagset of VerbNet consists of tags as 
agent, patient, theme, experiencer, stimulus, 
instrument, location, source, goal, recipient, 
benefactive etc. 
It is evident from the above discussions that 
no adequate semantic role set exists that can be 
defines across various domains. Hence pro-
3
posed SemanticNet does not only rely on fixed 
type of semantics roles as ConceptNet. For 
semantic relations we followed the 20 relations 
defined in ConceptNet. Additionally we pro-
posed semantic relations for every pair of lex-
icons cannot be defined by exact semantic role 
and thus we formulated a probabilistic score 
based technique. Semantic affinity in Seman-
ticNet could be calculated by network distance. 
Details could be found in relevant Section 8. 
3 Corpus 
Present SemanticNet has been developed for 
Bengali language. Resource acquisition is one 
of the most challenging obstacles to work with 
electronically resource constrained languages 
like Bengali. Although Bengali is the sixth1 
popular language in the World, second in India 
and the national language in Bangladesh.  
There was another issue drive us long way 
to find out the proper corpus for the develop-
ment of SemanticNet. As the notion is to cap-
ture and store human pragmatic knowledge so 
the hypothesis was chosen corpus should not 
be biased towards any specific domain know-
ledge as human pragmatic knowledge is not 
constricted to any domain rather it has a wide 
spread range over anything related to universe 
and life on earth. Additionally it must be larger 
in size to cover mostly available general con-
cepts related to any topic. After a detail analy-
sis we decided it is better to choose NEWS 
corpus as various domains knowledge like Pol-
itics, Sports, Entertainment, Social Issues, 
Science, Arts and Culture, Tourism, Adver-
tisement, TV schedule, Tender, Comics and 
Weather etc are could be found only in NEWS 
corpus.  
Statistics NEWS 
Total no. of news documents in the 
corpus 108,305 
Total no. of sentences in the corpus 2,822,737 
Avg no. of sentences in a document 27 
Total no. of wordforms in the corpus 33,836,736 
Avg. no. of wordforms in a document 313 
Total no. of distinct wordforms in the 
corpus 467,858 
Table 1:  Bengali Corpus Statistics 
                                                 
1
 
http://en.wikipedia.org/wiki/List_of_languages_by_
number_of_native_speakers 
Fortunately such corpus development could 
be found in (Ekbal and Bandyopadhyay, 2008) 
for Bengali. We obtained the corpus from the 
authors. The Bengali NEWS corpus consisted 
of consecutive 4 years of NEWS stories with 
various sub domains as reported above. For the 
present task we have used the Bengali NEWS 
corpus, developed from the archive of a lead-
ing Bengali NEWS paper 2  available on the 
Web. The NEWS corpus is quite larger in size 
as reported in Table 1. 
4 Annotation 
From the collected document set 200 docu-
ments have been chosen randomly for the an-
notation task. Three annotators (Mr. X, Mr. Y 
and Mr. Z) participated in the present task.  
Annotators were asked to annotate the theme 
words (topical expressions) which best de-
scribe the topical snapshot of the document. 
The agreement of annotations among three 
annotators has been evaluated. The agreements 
of tag values at theme words level is reported 
in Table 2. 
 
Annotators X vs. Y X Vs. Z Y Vs. Z Avg 
Percentage 82.64% 71.78% 80.47% 78.3% 
All Agree 75.45% 
Table 2: Agreement of annotators at theme 
words level 
5 Theme Identification 
Term Frequency (TF) plays a crucial role to 
identify document relevance in Topic-Based 
Information Retrieval. The motivation behind 
developing Theme detection technique is that 
in many documents relevant words may not 
occur frequently or irrelevant words may occur 
frequently. Moreover for the lexicon affinity 
inference, topic or theme words are the only 
strong clue to start with. The Theme detection 
technique has been proposed to resolve these 
issues to identify discourse level most relevant 
thematic nodes in terms of word or lexicon 
using a standard machine learning technique. 
The machine learning technique used here is 
Conditional Random Field (CRF)3. The theme 
word detection has been defined as a sequence 
                                                 
2
 http://www.anandabazar.com/ 
3
 http://crfpp.sourceforge.net 
4
labeling problem using various useful depend-
ing features. Depending upon the series of in-
put features, each word is tagged as either 
Theme Word (TW) or Other (O). 
5.1 Feature Organization 
The set of features used in the present task 
have been categorized as Lexico-Syntactic, 
Syntactic and Discourse level features. These 
are listed in the Table 3 below and have been 
described in the subsequent subsections. 
 
Types Features 
Lexico-Syntactic 
POS 
Frequency 
Stemming 
Syntactic Chunk Label Dependency Parsing Depth 
Discourse Level 
Title of the Document 
First Paragraph 
Term Distribution 
Collocation 
Table 3: Features 
5.2 Lexico-Syntactic Features 
5.2.1 Part of Speech (POS) 
It has been shown by Das and Bandyopadhyay, 
(2009), that theme bearing words in sentences 
are mainly adjective, adverb, noun and verbs 
as other POS categories like pronoun, preposi-
tion, conjunct, article etc. have no relevance 
towards thematic semantic of any document. 
The detail of the POS tagging system chosen 
for the present task could be found in (Das and 
Bandyopadhyay 2009). 
5.3 Frequency 
Frequency always plays a crucial role in identi-
fying the importance of a word in the docu-
ment or corpus. The system generates four 
separate high frequent word lists after function 
words are removed for four POS categories: 
adjective, adverb, verb and noun. Word fre-
quency values are then effectively used as a 
crucial feature in the Theme Detection tech-
nique. 
5.4 Stemming 
Several words in a sentence that carry thematic 
information may be present in inflected forms. 
Stemming is necessary for such inflected 
words before they can be searched in appropri-
ate lists. Due to non availability of good stem-
mers in Indian languages especially in Bengali, 
a stemmer based on stemming cluster tech-
nique has been used as described in (Das and 
Bandyopadhyay, 2010). This stemmer analyz-
es prefixes and suffixes of all the word forms 
present in a particular document. Words that 
are identified to have the same root form are 
grouped in a finite number of clusters with the 
identified root word as cluster center.  
5.5 Syntactic Features 
5.5.1 Chunk Label 
We found that Chunk level information is very 
much effective to identify lexicon inference 
affinity. As an example: 
 
( 	)/NP (
 
)/NP ()/NP 
()/JJP (?)/SYM 
The movies released by Sa-
tyajit Roy are excellent. 
 
In the above example two lexicons 
?/release? and ?/movie? are collo-
cated in a chunk and they are very much se-
mantically neighboring in human pragmatic 
knowledge. Chunk feature effectively used in 
supervised classifier. Chunk labels are defined 
as B-X (Beginning), I-X (Intermediate) and E-
X (End), where X is the chunk label. In the 
task of identification of Theme expressions, 
chunk label markers play a crucial role. Fur-
ther details of development of chunking sys-
tem could be found in (Das and Bandyopad-
hyay 2009).  
5.5.2 Dependency Parser 
Dependency depth feature is very useful to 
identify Theme expressions. A particular 
Theme word generally occurs within a particu-
lar range of depth in a dependency tree. Theme 
expressions may be a Named Entity (NE: per-
son, organization or location names), a com-
mon noun (Ex: accident, bomb blast, strike etc) 
or words of other POS categories. It has been 
observed that depending upon the nature of 
Theme expressions it can occur within a cer-
tain depth in the dependency tree in the sen-
tences. A statistical dependency parser has 
5
been used for Bengali as described in (Ghosh 
et al, 2009). 
5.6 Discourse Level Features 
5.6.1 Positional Aspect 
Depending upon the position of the thematic 
clue, every document is divided into a number 
of zones. The features considered for each 
document are Title words of the document, the 
first paragraph words and the words from the 
last two sentences. A detailed study was done 
on the Bengali news corpus to identify the 
roles of the positional aspect features of a doc-
ument (first paragraph, last two sentences) in 
the detection of theme words. The importance 
of these positional features has been described 
in the following section.  
5.6.2 Title Words 
It has been observed that the Title words of a 
document always carry some meaningful the-
matic information. The title word feature has 
been used as a binary feature during CRF 
based machine learning. 
5.6.3 First Paragraph Words 
People usually give a brief idea of their beliefs 
and speculations about any related topic or 
theme in the first paragraph of the document 
and subsequently elaborate or support their 
ideas with relevant reasoning or factual infor-
mation. Hence first paragraph words are in-
formative in the detection of Thematic Expres-
sions.  
5.6.4 Words From Last Two Sentences 
It is a general practice of writing style that 
every document concludes with a summary of 
the overall story expressed in the document. 
We found that it is very obvious that every 
document ended with dense theme/topic words 
in the last two sentences. 
5.6.5 Term Distribution Model 
An alternative to the classical TF-IDF weight-
ing mechanism of standard IR has been pro-
posed as a model for the distribution of a word. 
The model characterizes and captures the in-
formativeness of a word by measuring how 
regularly the word is distributed in a document. 
Thus the objective is to estimate  that measures 
the distribution pattern of the k occurrences of 
the word wi in a document d. Zipf's law de-
scribes distribution patterns of words in an en-
tire corpus. In contrast, term distribution mod-
els capture regularities of word occurrence in 
subunits of a corpus (e.g., documents, para-
graphs or chapters of a book). A good under-
standing of the distribution patterns is useful to 
assess the likelihood of occurrences of a theme 
word in some specific positions (e.g., first pa-
ragraph or last two sentences) of a unit of text. 
Most term distribution models try to character-
ize the informativeness of a word identified by 
inverse document frequency (IDF). In the 
present work, the distribution pattern of a word 
within a document formalizes the notion of 
theme inference informativeness. This is based 
on the Poisson distribution. Significant Theme 
words are identified using TF, Positional and 
Distribution factor. The distribution function 
for each theme word in a document is eva-
luated as follows: 
( )1 1
1 1
( ) / ( ) /
n n
d i i i i i
i i
f w S S n TW TW n
? ?
= =
= ? + ?? ?  
where n=number of sentences in a document 
with a particular theme word Si=sentence id of 
the current sentence containing the theme word 
and Si-1=sentence id of the previous sentence 
containing the query term, iTW is the positional 
id of current Theme word and 1iTW ? is the posi-
tional id of the previous Theme word. 
5.6.6 Collocation 
Collocation with other thematic 
words/expressions is undoubtedly an important 
clue for identification of theme sequence pat-
terns in a document. As we used chunk level 
collocation to capture thematic words (as de-
scribed in 5.5.1) and in this section we are in-
troducing collocation feature as inter-chunk 
collocation or discourse level collocation with 
various granularity as sentence level, para-
graph level or discourse level.  
6 Theme Clustering 
Theme clustering algorithms partition a set of 
documents into finite number of topic based 
groups or clusters in terms of theme 
words/expressions. The task of document clus-
tering is to create a reasonable set of clusters 
6
for a given set of documents. A reasonable 
cluster is defined as the one that maximizes the 
within-cluster document similarity and mini-
mizes between-cluster similarities. There are 
two principal motivations for the use of this 
technique in the theme clustering setting: effi-
ciency, and the cluster hypothesis. 
The cluster hypothesis (Jardine and van 
Rijsbergen, 1971) takes this argument a step 
further by asserting that retrieval from a clus-
tered collection will not only be more efficient, 
but will in fact improve retrieval performance 
in terms of recall and precision. The basic no-
tion behind this hypothesis is that by separat-
ing documents according to topic, relevant 
documents will be found together in the same 
cluster, and non-relevant documents will be 
avoided since they will reside in clusters that 
are not used for retrieval. Despite the plausibil-
ity of this hypothesis, there is only mixed ex-
perimental support for it. Results vary consi-
derably based on the clustering algorithm and 
document collection in use (Willett, 1988). We 
employ the clustering hypothesis only to 
measure inter-document level thematic affinity 
inference on semantics. 
Application of the clustering technique to 
the three sample documents results in the fol-
lowing theme-by-document matrix, A, where 
the rows represent various documents and the 
columns represent the themes politics, sport, 
and travel.  
election cricket hotel
A parliament sachin vacation
governor soccer tourist
? ?
? ?
= ? ?
? ?? ?
 
The similarity between vectors is calculated 
by assigning numerical weights to these words 
and then using the cosine similarity measure as 
specified in the following equation.  
, ,
1
, .
N
k j k j i k i j
i
s q d q d w w
? ? ? ?
=
? ?
= = ?? ?? ? ? ---- (1) 
This equation specifies what is known as the 
dot product between vectors.  Now, in general, 
the dot product between two vectors is not par-
ticularly useful as a similarity metric, since it is 
too sensitive to the absolute magnitudes of the 
various dimensions. However, the dot product 
between vectors that have been length norma-
lized has a useful and intuitive interpretation: it 
computes the cosine of the angle between the 
two vectors. When two documents are identic-
al they will receive a cosine of one; when they 
are orthogonal (share no common terms) they 
will receive a cosine of zero. Note that if for 
some reason the vectors are not stored in a 
normalized form, then the normalization can 
be incorporated directly into the similarity 
measure as follows.  
Of course, in situations where the document 
collection is relatively static, it makes sense to 
normalize the document vectors once and store 
them, rather than include the normalization in 
the similarity metric. 
, ,1
2 2
, ,1 1
,
N
i k i ji
k j N N
i k i ki i
w w
s q d
w w
? ?
=
= =
?? ?
=? ?? ? ?
?
? ?  ----(2) 
Calculating the similarity measure and using 
a predefined threshold value, documents are 
classified using standard bottom-up soft clus-
tering k-means technique. The predefined thre-
shold value is experimentally set as 0.5 as 
shown in Table 4. 
 
ID Theme 1 2 3 
1 	
 (administration) 0.63 0.12 0.04 
1  (good-government) 0.58 0.11 0.06 
1  (society) 0.58 0.12 0.03 
1  (law) 0.55 0.14 0.08 
2 Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), pages 17?25,
the 23rd International Conference on Computational Linguistics (COLING), Beijing, August 2010
Clause Identification and Classification in Bengali  
Aniruddha Ghosh1 Amitava Das2 Sivaji Bandyopadhyay3 
Department of Computer Science and Engineering  
Jadavpur University 
arghyaonline@gmail.com1 amitava.santu@gmail.com2 si-
vaji_cse_ju@yahoo.com3  
 
Abstract 
This paper reports about the develop-
ment of clause identification and classi-
fication techniques for Bengali language. 
A syntactic rule based model has been 
used to identify the clause boundary. For 
clause type identification a Conditional 
random Field (CRF) based statistical 
model has been used. The clause identi-
fication system and clause classification 
system demonstrated 73% and 78% pre-
cision values respectively.  
1 Introduction 
The clause identification is one of the shallow 
semantic parsing tasks, which is important in 
various NLP applications such as Machine 
Translation, parallel corpora alignment, Informa-
tion Extraction and speech applications. Gram-
matically a clause is a group of words having a 
subject and a predicate of its own, but forming 
part of a sentence. Clause boundary identifica-
tion of natural language sentences poses consi-
derable difficulties due to the ambiguous nature 
of natural languages. Clause classification is a 
convoluted task as natural language is generally 
syntactically rich in formation of sentences or 
clauses. 
By the classical theory of Panini (Paul and 
Staal, 1969) a clause is the surface level basic 
syntactic element which holds the basic depen-
dent semantics (i.e. lexical semantic have no 
dependency) to represent the overall meaning of 
any sentence. This syntactic to semantic deriva-
tion proceeds through two intermediate stages: 
the level of karaka relations, which are compa-
rable to the thematic role types and the level of 
inflectional or derivational morphosyntax. 
Fillmore?s Case Grammar (Fillmore et. al, 
2003), and much subsequent work, revived the 
Panini?s proposals in a modern setting. A main 
objective of Case Grammar was to identify syn-
tactic positions of semantic arguments that may 
have different realizations in syntax.  
In the year of 1996 Bharati et al (1996) de-
fines the idea of Chunk or local word group for 
Indian languages. After the successful imple-
mentation of Shakti1 , the first publicly available 
English-Hindi machine translation system the 
idea of chunk became the most acceptable syn-
tactic/semantic representation format for Indian 
languages, known as Shakti Standard Format 
(SSF).   
In 2009 Bali et al (2009) redefines the idea of 
chunk and establishes that the idea of chunking 
varies with prosodic structure of a language. 
Boundary of chunk level is very ambiguous it-
self and can differ by writer or speaker accord-
ing to their thrust on semantic. 
Therefore it is evident that automatic clause 
identification for Indian languages needs more 
research efforts. In the present task, clause 
boundary identification is attempted using the 
classical theory of Panini and the Case Grammar 
approach of Fillmore on the shallow parsed out-
put in SSF structure. It may be worth mentioning 
that several basic linguistic tools in Indian lan-
guages such as part of speech tagger, chunker, 
and shallow parser follow SSF2  as a standard.  
Previous research on clause identification was 
done mostly on the English language (Sang and 
Dejean, 2001). There have been limited efforts 
on clause identification for Indian languages. 
One such effort is proposed in Ram and Devi, 
                                                 
1
 http://shakti.iiit.ac.in/ 
2
 http://ltrc.iiit.ac.in/MachineTrans/research/tb/shakti-
analy-ssf.pdf 
17
(2008) with statistical method. The idea of ge-
nerative grammar based on rule-based descrip-
tions of syntactic structures introduced by 
Chomsky (Chomsky, 1956) points out that every 
language has its own peculiarities that cannot be 
described by standard grammar. Therefore a new 
concept of generative grammar has been pro-
posed by Chomsky. Generative grammar can be 
identified by statistical methods. In the present 
task, conditional random field (CRF) 3  -based 
machine learning method has been used in 
clause type classification. According to the best 
of our knowledge this is the first effort to identi-
fy and classify clauses in Bengali. 
The present system is divided into two parts. 
First, the clause identification task aims to iden-
tify the start and the end boundaries of the claus-
es in a sentence. Second, Clause classification 
system identifies the clause types. 
Analysis of corpus and standard grammar of 
Bengali revealed that clause boundary identifica-
tion depends mostly on syntactic dependency. 
For this reason, the present clause boundary 
identification system is rule based in nature. 
Classification of clause is a semantic task and 
depends on semantic properties of Bengali lan-
guage. Hence we follow the theory of 
Chomsky?s generative grammar to disambiguate 
among possible clause types. The present classi-
fication system of clause is a statistics-based 
approach. A conditional random field (CRF) 
based machine learning method has been used in 
the clause classification task. The output of the 
rule based identification system is forwarded to 
the machine learning model as input. 
The rest of the paper is organized as follows. 
In section 2 we elaborate the rule based clause 
boundary identification. The next section 3 de-
scribes the implementation detail with all identi-
fied features for the clause classification prob-
lem. Result section 4 reports about the accuracy 
of the hybrid system. In error analysis section 
we reported the limitations of the present sys-
tem. The conclusion is drawn in section 5 along 
with the future task direction. 
2 Resource Acquisition 
Bengali belongs to Indo-Aryan language family. 
A characteristic of Bengali is that it is under-
                                                 
3
 http://crf.sourceforge.net/ 
resourced. Language research for Bengali got 
attention recently. Resources like annotated cor-
pus and linguistics tools for Bengali are very 
rarely available in the public domain. 
2.1 Corpus 
We used the NLP TOOLS CONTEST: ICON 
20094 dependency relation marked training data-
set of 980 sentences for training of the present 
system. The data has been further annotated at 
the clause level. According to the standard 
grammar there are two basic clause types such as 
Principal clause and Subordinate clause. Subor-
dinate clauses have three variations as Noun 
clause, Adjective clause and Adverbial clause. 
The tagset defined for the present task consists 
of four tags as Principal clause (PC), Noun 
clause (NC), Adjective clause (AC) and Adver-
bial clause (RC). The annotation tool used for 
the present task is Sanchay5. The detailed statis-
tics of the corpus are reported in Table 1. 
 
 Train Dev Test 
No of Sentences 980 150 100 
Table 1: Statistics of Bengali Corpus 
2.1.1 Annotation Agreement 
Two annotators (Mr. X and Mr. Y) participated 
in the present task. Annotators were asked to 
identify the clause boundaries as well as the type 
of the identified clause. The agreement of anno-
tations among two annotators has been eva-
luated. The agreements of tag values at clause 
boundary level and clause type levels are listed 
in Table 2. 
 
 
Boundary Type 
Percentage 76.54% 89.65% 
Table 2: Agreement of annotators at clause 
boundary and type level 
It is observed from the Table 2 that clause 
boundary identification task has lower agree-
ment value. A further analysis reveals that there 
are almost 9% of cases where clause boundary 
has nested syntactic structure. These types of 
clause boundaries are difficult to identify. One 
of such cases is Inquisitive semantic (Groenen-
dijk, 2009) cases, ambiguous for human annota-
                                                 
4
 http://ltrc.iiit.ac.in/nlptools2009/ 
5
 http://ltrc.iiit.ac.in/nlpai_contest07/Sanchay/ 
18
tors too. It is better to illustrate with some spe-
cific example. 
If John goes to the party, 
will Mary go as well? 
In an inquisitive semantics for a language of 
propositional logic the interpretation of disjunc-
tion is the source of inquisitiveness. Indicative 
conditionals and conditional questions are 
treated both syntactically and semantically. The 
semantics comes with a new logical-
pragmatically notion that judges and compares 
the compliance of responses to an initiative in 
inquisitive dialogue (Groenendijk, 2009). Hence 
it is evident that these types of special cases 
need special research attention. 
2.2 Shallow Parser 
Shallow parser6 for Indian languages, developed 
under a Government of India funded consortium 
project named Indian Language to Indian Lan-
guage Machine Translation System (IL-ILMT), 
are now publicly available. It is a well developed 
linguistic tool and produce good credible analy-
sis. For the present task the linguistic analysis is 
done by the tool and it gives output as pruned 
morphological analysis at each word level, part 
of speech at each word level, chunk boundary 
with type-casted chunk label, vibhakti computa-
tion and chunk head identification. 
2.3 Dependency parser 
A dependency parser for Bengali has been used 
as described in Ghosh et al (2009). The depen-
dency parser follows the tagset7  identified for 
Indian languages as a part of NLP TOOLS 
CONTEST 2009 as a part of ICON 2009. 
3 Rule-based Clause Boundary Identi-
fication 
Analysis of a Bengali corpus and standard 
grammar reveals that clause boundaries are di-
rectly related to syntactic relations at sentence 
level. The present system first identifies the 
number of verbs present in a sentence and sub-
sequently finds out dependant chunks to each 
verb. The set of identified chunks that have rela-
tion with a particular verb is considered as a 
clause. But some clauses have nested syntactic 
                                                 
6
 http://ltrc.iiit.ac.in/analyzer/bengali/ 
7
 http://ltrc.iiit.ac.in/nlptools2009/CR/intro-husain.pdf 
formation, known as inquisitive semantic. These 
clauses are difficult to identify by using only 
syntactic relations. The present system has limi-
tations on those inquisitive types of clauses. 
Bengali is a verb final language. Most of the 
Bengali sentences follow a Subject-Object-Verb 
(SOV) pattern. In Bengali, subject can be miss-
ing in a clause formation. Missing subjects and 
missing keywords lead to ambiguities in clause 
boundary identification. In sentences which do 
not follow the SOV pattern, chunks that appear 
after the finite verb are not considered with that 
clause. For example:  
 
wAra AyZawana o parimANa 
xeKe buJawe asubiXA hayZa ei  
paWa hAwi geCe. 
 
After seeing the size and 
effect, it is hard to under-
stand that an elephant went 
through this way. 
 
In the above example, there is hardly any clue 
to find beginning of subordinate clause. To solve 
this type of problem, capturing only the tree 
structure of a particular sentence has been 
treated as the key factor to the goal of disambig-
uation. One way to capture the regularity of 
chunks over different sentences is to learn a ge-
nerative grammar that explains the structure of 
the chunks one finds. These types of language 
properties make the clause identification prob-
lem difficult.  
3.1 Karaka relation 
Dependency parsing generates the inter chunk 
relation and generates the tree structure. The de-
pendency parser as described in Section 2.3 used 
as a supportive tool for the present problem.   
In the output of the dependency parsing sys-
tems, most of the chunks have a dependency 
relation with the verb chunk. These relations are 
called as karaka relation. Using dependency re-
lations, the chunks having dependency relation 
i.e. karaka relation with same verb chunk are 
grouped. The set of chunks are the members of a 
clause. Using this technique, identification of 
chunk members of a certain clause becomes in-
dependent of SOV patterns of sentences. An ex-
ample is shown in Figure 1. 
19
 Figure 1: Karaka Relations 
3.2 Compound verbs  
In Bengali language a noun chunk with an infi-
nite verb chunk or a finite verb chunk can form a 
compound verb. An example is shown in Figure 
2. 
 
Figure 2: Compound Verb 
In the above example, the noun chunk and the 
VGF chunk form a compound verb. These two 
consecutive noun and verb chunks appearing in 
a sentence are merged to form a compound verb. 
These chunks are connected with a part-of rela-
tion in Dependency Parsing. The set of related 
chunks with these noun and verb chunks are 
merged.  
3.3 Shasthi Relation (r6) 
In dependency parsing the genitive relation are 
marked with shasthi (r6) relation. The chunk 
with shasthi (r6) (see the tagset of NLP Tool 
Contest: ICON 2009) relation always has a rela-
tion with the succeeding chunk. An example is 
shown in Figure 3. 
In the example as mentioned in Figure 3, the 
word ?wadera?(their) has a genitive relation 
with the word in the next chunk ?manera?(of 
mind). These chunks are placed in a set. It forms 
a set of two chunks members. The system gene-
rates two different types of set. In one forms a 
set of members having relation with verb 
chunks. Another set contains two noun chunks 
with genitive relation. Now the sets containing 
only noun chunks with genitive relation does not 
form a clause. Those sets are merged with the set 
containing verb chunk and having dependency 
relation with the noun chunks. An example is 
shown in Figure 3. 
 
Figure 3: Shasthi Relation 
 
Consider ? is set of all sets containing two 
chunk members connected with genitive marker. 
Consider ? is a set of all sets consisting of re-
lated chunks with a verb chunk. ? is a element of 
?. ? is a element of ?. Now, If a set ? which can 
have common chunks from a ? set then ? set is 
associated with the proper ? set. So, ? ? ? ? 
Null then ? = ? ? ?. If a set ? which can have 
common chunks from two ? sets which leads to 
ambiguity of associability of the ? set with the 
proper ? set. If ? ? ? = verb chunk, then ? set 
will be associated with ? set containing the verb 
chunk. From the related set of chunk of verb 
chunks, system has identified the clauses in the 
sentence. Afterwards, the clauses are marked 
with the B-I-E (Beginning-Intermediate-End) 
notation.  
4 Case Grammar-Identification of Ka-
raka relations 
The classical Sanskrit grammar Astadhyayi 8  
(?Eight Books?), written by the Indian gramma-
                                                 
8
 
http://en.wikipedia.org/wiki/P%C4%81%E1%B9%87
ini 
20
rian Panini sometime during 600 or 300 B.C. 
(Robins, 1979), includes a sophisticated theory 
of thematic structure that remains influential till 
today. Panini?s Sanskrit grammar is a system of 
rules for converting semantic representations of 
sentences into phonetic representations (Ki-
parsky, 1969). This derivation proceeds through 
two intermediate stages: the level of karaka rela-
tions, which are comparable to the thematic role 
types described above; and the level of morpho-
syntax. 
Fillmore?s Case Grammar (Fillmore, 1968), 
and much subsequent work, revived the Panini?s 
proposals in a modern setting. A main objective 
of Case Grammar was to identify semantic ar-
gument positions that may have different realiza-
tions in syntax. Fillmore hypothesized ?a set of 
universal, presumably innate, concepts which 
identify certain types of judgments human be-
ings are capable of making about the events that 
are going on around them?. He posited the fol-
lowing preliminary list of cases, noting however 
that ?additional cases will surely be needed?.  
? Agent: The typically animate perceived 
instigator of the action. 
? Instrument: Inanimate force or object 
causally involved in the action or state. 
? Dative: The animate being affected by 
the state or action. 
? Factitive: The object or being resulting 
from the action or state. 
? Locative: The location or time-spatial 
orientation of the state or action. 
? Objective: The semantically most neu-
tral case, the concept should be limited to 
things which are affected by the action or 
state. 
The SSF specification handles this syntactic 
dependency by a coarse-grain tagset of Nomini-
tive, Accusative, Genitive and Locative case 
markers. Bengali shallow parser identifies the 
chunk heads as part of the chunk level analysis. 
Dependency parsing followed by a rule based 
module has been developed to analyze the inter-
chunk relationships depending upon each verb 
present in a sentence. Described theoretical as-
pect can well define the problem definition of 
clause boundary identification but during prac-
tical implementation of the solution we found 
some difficulties. Bengali has explicit case 
markers and thus long distant chunk relations are 
possible as valid grammatical formation. As an 
example: 
bAjAre yAoyZAra samayZa xeKA 
kare gela rAma. 
 
bAjAre yAoyZAra samayZa rAma 
xeKA kare gela. 
 
rAma bAjAre yAoyZAra samayZa 
xeKA kare gela. 
 
Rama came to meet when he 
was going to market. 
 
In the above example rAma could be placed 
anywhere and still all the three syntactic forma-
tion are correct. For these feature of Bengali 
many dependency relation could be missed out 
located at far distance from the verb chunk in a 
sentence. Searching for uncountable numbers of 
chunks have dependency relation with a particu-
lar verb may have good idea theoretically but we 
prefer a checklist strategy to resolve the problem 
in practice. At this level we decided to check all 
semantic probable constituents by the definition 
of universal, presumably innate, concepts list. 
We found this is a nice fall back strategy to iden-
tify the clause boundary. Separately rules are 
written as described below. 
4.1 Agent 
Bengali is a verb final language. Most of the 
Bengali sentences follow a Subject-Object-Verb 
(SOV) pattern. In Bengali, subject can be miss-
ing in a clause formation. Missing subjects and 
missing keywords lead to ambiguities in clause 
boundary identification. 
 
  	? 
Close the door. 
 
In the previous case system marks 
?/door? as an ?Agent? whereas the 
?Agent? is ?you? (2nd person singular number), 
silent here.  
We developed rules using case marker, Gend-
er-Number-Person (GNP), morphological fea-
ture and modality features to disambiguate these 
21
types of phenomena. These rules help to stop 
false hits by identifying no 2nd person phrase 
was there in the example type sentences and em-
power to identify proper phrases by locating 
proper verb modality matching with the right 
chunk.  
4.2 Instrument 
Instrument identification is ambiguous for the 
same type of case marker (nominative) taken by 
agent and instrument. There is no ani-
mate/inanimate information is available at syn-
tactic level. 

	 
  ? 
The music of Shyam?s messme-
rized me. 
 ? 
The umbrella of Sumi. 
 
Bengali sentences follow a Subject-Object-
Verb (SOV) pattern. Positional information is 
helpful to disambiguate between agent and in-
strument roles. 
4.3 Dative 
G
en
er
a
l 
Bengali English Gloss 
/	//	
... 
Morn-
ing/evening/night/da
wn? 
_ 
//Proceedings of the 1st Workshop on South and Southeast Asian Natural Language Processing (WSSANLP), pages 35?42,
the 23rd International Conference on Computational Linguistics (COLING), Beijing, August 2010
Web Based Manipuri Corpus for Multiword NER and        
Reduplicated MWEs Identification using SVM 
Thoudam Doren Singh 
Department of Computer Science and 
Engineering 
Jadavpur University  
thoudam.doren@gmail.com 
Sivaji Bandyopadhyay 
Department of Computer Science and 
Engineering 
Jadavpur University 
sivaji_cse_ju@yahoo.com 
 
  
 
Abstract 
A web based Manipuri corpus is devel-
oped for identification of reduplicated 
multiword expression (MWE) and mul-
tiword named entity recognition (NER). 
Manipuri is one of the rarely investi-
gated language and its resources for 
natural language processing are not 
available in the required measure. The 
web content of Manipuri is also very 
poor. News corpus from a popular Ma-
nipuri news website is collected. Ap-
proximately four and a half million Ma-
nipuri wordforms have been collected 
from the web. The mode of corpus col-
lection and the identification of redupli-
cated MWEs and multiword NE based 
on support vector machine (SVM) 
learning technique are reported. The 
SVM based reduplicated MWE system 
is evaluated with recall, precision and F-
Score values of 94.62%, 93.53% and 
94.07% respectively outperforming the 
rule based approach. The recall, preci-
sion and F-Score for multiword NE are 
evaluated as 94.82%, 93.12% and 
93.96% respectively. 
1 Introduction  
The NER and MWE identification are important 
tasks for natural language applications that in-
clude machine translation and information re-
trieval. The present work reports the NER and 
reduplicated MWE identification of Manipuri 
on web based news corpus. The use of web as a 
corpus for teaching and research on languages 
has been proposed several times (Rundell, 2000; 
Fletcher, 2001; Robb, 2003; Fletcher 2004). A 
special issue of the Computational Linguistics 
journal on web as corpus (Kilgarriff and Gre-
fenstette, 2003) was published. Several studies 
have used different methods to mine web data. 
The web walked into the ACL meetings starting 
in 1999. The special interest group of ACL on 
web as corpus is promoting interest in the use of 
the web as a source of linguistic data, and as an 
object of study in its own right. India is a multi-
lingual country with a lot of cultural diversity. 
Bharati et al (2001) reports an effort to create 
lexical resources such as transfer lexicon and 
grammar from English to several Indian lan-
guages and dependency Treebank of annotated 
corpora for several Indian languages. In Indian 
context, a web based Bengali corpus develop-
ment work from web is reported in Ekbal and 
Bandyopadhyay (2008) and Manipuri-English 
semi automatic parallel corpora extraction by 
Singh et. al., (2010). Newspaper is a huge 
source of readily available documents. In the 
present work, the Manipuri monolingual corpus 
has been developed from web for NLP and re-
lated tasks. 
Manipuri is a scheduled Indian language 
spoken approximately by three million people 
mainly in the state of Manipur in India and in 
the neighboring countries namely Bangladesh 
and Myanmar. It is a Tibeto-Burman language 
and highly agglutinative in nature, influenced 
and enriched by the Indo-Aryan languages of 
Sanskrit origin and English. The affixes play the 
most important role in the structure of the lan-
guage. In Manipuri, words are formed in three 
processes called affixation, derivation and com-
pounding. The majority of the roots found in the 
35
language are bound and the affixes are the de-
termining factor of the class of the words in the 
language. Annotated corpus, bilingual dictiona-
ries, name dictionaries, WordNet, morphologi-
cal analyzers etc. are not yet available in Mani-
puri in the required measure. 
In the present work, the tasks of identifica-
tion of Manipuri multiword named entity (MNE) 
and reduplicated multiword expression (RMWE) 
identification using support vector machine 
(SVM) learning technique on the corpus col-
lected from web is reported. 
Works on multiword expressions (MWEs) 
have started with a momentum in different lan-
guages. In the Indian context, some of the 
works can be seen in (Dandapat et. al., 2006; 
Kunchukuttan and Damani, 2008; Kishorjit et. 
al., 2010). The identification of MWEs in sev-
eral languages concentrate on compound nouns, 
noun-verb combination, some on idioms and 
phrases and so on but not much on RMWEs. 
The reason may be that the reduplicated words 
are either rare or easy to identify for these lan-
guages since only complete duplication and 
some amount of partial reduplication may be 
present in these languages. On the other hand, 
reduplicated MWEs are quite large in number in 
Manipuri and there are wide varieties of redup-
licated MWEs in Manipuri. 
2 Manipuri News Corpus and Statis-
tics 
The content of Manipuri language on the web is 
very poor. One of the sources is the daily news 
publications. Again, there is no repository. Thus, 
the possibility of deploying web crawler and 
mining the web corpus is not possible. The Ma-
nipuri news corpus is collected from 
http://www.thesangaiexpress.com/ covering the 
period from May 2008 to May 2010 on daily 
basis. The Manipuri news is available in PDF 
format. A tool has been developed to convert 
contents from PDF documents to Unicode for-
mat. There are 15-20 articles in each day. Con-
sidering the Manipuri corpus covering the pe-
riod from May 2008 to May 2010, there are 
4649016 wordforms collected1. 
                                                 
1
There are no publications on some occasions. 
2.1 Conversion from PDF to UTF-8  
The general Manipuri news collected is in PDF 
format. A tool has been developed to convert 
Manipuri news PDF articles to Bengali Unicode. 
The Bengali Unicode characters are used to 
represent Manipuri as well. The conversion of 
PDF format into Unicode involves the conver-
sion to ASCII and then into Unicode using 
mapping tables between the ASCII characters 
and corresponding Bengali Unicode. The map-
ping tables have been prepared at different le-
vels with separate tables for single characters 
and conjuncts with two or more than two cha-
racters. The single character mapping table con-
tains 72 entries and the conjunct characters 
mapping table consists of 738 entries. There are 
conjuncts of 2, 3 and 4 characters. Sub-tables 
for each of the conjuncts are prepared. English 
words are present on the Manipuri side of the 
news and they are filtered to avoid unknown 
character features. 
2.2 Use of language resources 
The Manipuri web corpus collected from the 
web is cleaned by removing the unknown cha-
racters. After the cleaning process, the running 
texts are picked up followed by spelling correc-
tion. The web based news corpus is POS tagged 
using the 26 tagset2 defined for the Indian lan-
guages based on the work of (Singh et. al. , 
2008). The Manipuri news corpus developed in 
this work has been used to identify MNE and 
RMWEs identification. 
3 Support Vector Machine 
The SVM (Vapnik, 1995) is based on discr i-
minative approach and makes use of both pos i-
tive and negative examples to learn the distinc-
tion between the two classes. The SVMs are 
known to robustly handle large feature sets and 
to develop models that maximize their generali-
zability.   Suppose we have a set of training data 
for a two-class problem: 1 1{( , ),.....( , )}N Nx y x y, 
where xi ? RD is a feature vector of the i
th sam-
ple in the training data and yi ? {+1, -1} is the 
class to which xi belongs. The goal is to find a 
decision function that accurately predicts class y 
                                                 
2http://shiva.iiit.ac.in/SPSAL2007/iiit_tagset_guidelines.p
df 
36
for an input vector x. A non-linear SVM clas-
sifier gives a decision function f (x)= sign (g (x)) 
for an input vector where,  
1
( ) ( , )im i
i
g x wK x z b
?
? ??  Here, f(x)=+1 means 
x is a member of a certain class and f(x)=-1 
means x is not a member. The support vector is 
represented by zi and stands for the training ex-
amples; m is the number of support vectors 
Therefore, the computational complexity of g(x) 
is proportional to m. Support vectors and other 
constants are determined by solving a certain 
quadratic programming problem. ( , )iK x z is a 
kernel that implicitly maps vectors into a higher 
dimensional space. Typical kernels use dot 
products: ( , ) ( . )iK x z k x z? .A polynomial ker-
nel of degree d is given by ( , )iK x z = (1+x)d. 
We can use various kernels, and the design of 
an appropriate kernel for a particular application 
is an important research issue. 
The MNE/RMWE tagging system includes 
two main phases: training and classification. 
The training process has been carried out by 
YamCha3 toolkit, an SVM based tool for detect-
ing classes in documents and formulating the 
MNE/RMWE tagging task as a sequence labe-
ling problem. Here, both one vs rest and pair-
wise multi-class decision methods have been 
used. Different experiments with the various 
degrees of the polynomial kernel function have 
been carried out. In one vs rest strategy, K bi-
nary SVM classifiers may be created where 
each classifier is trained to distinguish one class 
from the remaining K-1 classes. In pairwise 
classification, we constructed K (K-1)/2 clas-
sifiers considering all pairs of classes, and the 
final decision is given by their weighted voting. 
For classification, the TinySVM-0.074 classifier 
has been used that seems to be the best opti-
mized among publicly available SVM toolkits.  
4 Multiword Named Entity Recogni-
tion  
Named Entity Recognition for Manipuri is re-
ported in (Singh et. al., 2009). The present work 
focuses and reports on the recognition of mul-
tiword NEs. In order to identify the MNEs,  
                                                 
3http://chasen-org/~taku/software/yamcha/  
4http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM  
28,629 wordforms from Manipuri news corpus 
has been manually annotated and used as train-
ing data with the major named entity (NE) tags, 
namely person name, location name, organiza-
tion name and miscellaneous name to apply 
Support Vector Machine (SVM) based machine 
learning technique. Miscellaneous name in-
cludes the festival name, name of objects, name 
of building, date, time, measurement expression 
and percentage expression etc. The SVM based 
system makes use of the different contextual 
information of the words along with the variety 
of word-level orthographic features that are 
helpful in predicting the MNE classes. 
MNE identification in Indian languages as 
well as in Manipuri is difficult and challenging 
as: 
? Unlike English and most of the European lan-
guages, Manipuri lacks capitalization infor-
mation, which plays a very important role in 
identifying MNEs. 
? A lot of MNEs in Manipuri can appear in the 
dictionary with some other specific meanings. 
? Manipuri is a highly inflectional language 
providing one of the richest and most chal-
lenging sets of linguistic and statistical fea-
tures resulting in long and complex word-
forms. 
? Manipuri is a relatively free word order lan-
guage. Thus MNEs can appear in subject and 
object positions making the NER task more 
difficult compared to others. 
? Manipuri is a resource-constrained language. 
Annotated corpus, name dictionaries, sophis-
ticated morphological analyzers, POS taggers 
etc. are not yet available.  
 
MNE 
Tag 
Meaning MNE Exam-
ples 
B-LOC 
 
 
 
Beginning, 
Internal or the 
End of 
a multiword 
location name 
?????? (Thanga) 
I-LOC ? ???????? (Moi-
rangthem) 
E-LOC ????? (Leikai) 
B-PER 
 
Beginning, 
Internal or the 
End of a mul-
tiword person 
name 
????? (Oinam) 
I-PER 
 
?????? (Ibobi) 
E-PER ???? (Meetei) 
 Table 1. Named entity examples 
37
In the present work, the NE tagset used 
have been further subdivided into the detailed 
categories in order to denote the boundaries of 
MNEs properly.  Table 1 shows examples. 
5 Reduplicated MWEs Identification 
Manipuri is very rich in RMWEs like other Ti-
beto-Burman languages. The work of (Singh, 
2000) describes the linguistic rules for identify-
ing reduplicated words. A rule based Manipuri 
RMWE identification is reported in (Kishorjit 
and Bandyopadhyay, 2010). The process of re-
duplication (Singh, 2000) is defined as: ?redup-
lication is that repetition, the result of which 
constitutes a unit word?.  These single unit 
words are the MWEs. The RMWEs in Manipuri 
are classified as: 1) Complete RMWEs, 2) Par-
tial RMWEs, 3) Echo RMWEs and 4) Mimic 
RMWEs. Apart from these four types of 
RMWEs, there are also cases of a) Double 
RMWEs and b) Semantic RMWEs. 
Complete RMWEs: In the complete 
RMWEs the single word or clause is repeated 
once forming a single unit regardless of phono-
logical or morphological variations. 
???? ???? (?marik marik?) which means 
?drop by drop?. 
 ???? ?????? (?atek atek-pa? ) which 
means ?fresh? 
??? ??? (?kari kari?) means ?what/which?.  
Partial RMWEs: In case of partial 
reduplication the second word carries some part 
of the first word as an affix to the second word, 
either as a suffix or a prefix. 
For example, ????? ????? (?chat-thok chat-
sin?) means ?to go to and fro?; ???? ????? (?saa-mi 
laan-mi?) means ?army?. 
Mimic RMWEs: In the mimic  
reduplication the words are complete 
reduplication but the morphemes are 
onomatopoetic, usually emotional or natural 
sounds. For example, ??? ???  (?krak krak? ) 
means ?cracking sound of earth in drought?. 
Echo RMWEs: The second word does not 
have a dictionary meaning and is basically an 
echo word of the first word. For example, ???? 
???? (?thak-si kha-si?) means ?good manner?. 
Double RMWEs: Such type of reduplica-
tion generally consists of three words where the 
prefix or suffix of the first two words is redupli-
cated but in the third word the prefix or suffix is 
absent. An example of double prefix reduplica-
tion is ???? ???? ????? (?i-mun i-mun mun-ba?) 
which means, ?completely ripe?. 
Semantic RMWEs: Both the reduplication 
words have the same meaning and so also is the 
MWE. Such types of MWEs are very special to 
the Manipuri language. For example,  ????? ?? 
(?paamba kei?) means ?tiger? and each of the 
component words means ?tiger?. 
5.1 Role of suffix and prefix 
Apart from the above cases meaningful prefixes 
or suffixes are used with RMWEs otherwise 
they are ungrammatical. 
Suffixes/ wh- duplicating 
words 
Part of 
Speech 
?? (?da),  ?? (?gi) and  ?? (?ki) 
 
Beginning, Internal or the End 
of 
a multiword location name 
Noun 
?? (?ba) and?? (?pa) Adjective 
?? (?na) Adverb 
??? ??? (?kari kari?), ??? ??? 
(?kanaa kanaa?), ???? ???? 
(?kadaay kadaay?) and ??? 
??? (?karam karam?) 
Wh- ques-
tion type 
Table 2. Suffixes/wh- duplicating words list 
used in Complete Reduplication and  POS 
tagging 
 
Prefix: With such prefixes the semantic 
shapes are different and sometimes even the 
same prefix carries a different meaning. By 
these prefixation, the root is reduplicated as 
given below: 
 
{[?(i)-/??(pang)-/??(khang)-/?(ta)-/???(pum)-/ 
???(suk)] + Root }  
?  
{[?(i)-/??(pang)-/??(khang)-/?(ta)-/???(pum)-/ 
???(suk)] + Root  + Root} 
 
?????   ????  ???? 
 mahaak-na       i-waang    waang-ngi 
 he/she-nom    ?tall          tall-asp 
 He/She is the tallest 
 
Suffix: There are some suffixes that carry 
certain meaning when used with RMWEs. 
Commonly used suffixes are, ??? (-trik) / ??? (-
drik), ???? (-throk), ???? (-drong), ??? (-suk), ?? (-
sang), ??? (-dring), ??? (-sit), ??? (-sin), ??? (-
38
dreng), ???? (-sroke) etc. Generally these suffix-
es indicate a superlative degree or emphatic 
meaning. 
Some examples are as follows, 
??????       ????? 
mun-trik   mun-ba 
ripe         ripe 
?very ripe? 
 
Role of affix in Partial Reduplication: 
Character-wise comparisons are done with not 
less than two characters either from front or rear 
for both the words since the second word is not 
a complete repetition.  
Also the last few characters of the first 
word and the same number of first characters of 
the second word are compared to check the par-
tial reduplication. The prefixes or suffixes are 
verified with a list of accepted suffixes and pre-
fixes (see table 2) to validated the reduplication.  
Role of affix in Echo Reduplication: 
Identification of echo reduplication is done by 
comparing the equality of suffixes of   consecu-
tive two words w1 and w2. 
6 Best Feature Selection for SVM 
The use of prefix/suffix information works well 
for the highly inflected languages like the In-
dian languages. Different combinations from 
the following set for identifying  the best feature 
set for MNE/RMWE are experimented: 
F={ , .., 1, , 1, ....,i m i i i i nw w w w w? ? ? ?, |prefix|<=n, 
|suffix|<=n, MNE/RMWE tag(s) of previous 
word(s), POS tag(s) of the current and/or the 
surrounding word(s), First word, Length of the 
word, Digit information, Infrequent word}, 
where iw  is the current word; i mw ?  is the 
previous mth word and i nw ?  is the next n
th 
word. Following are the details of the features: 
1 Context word feature: Preceding and fol-
lowing words of a particular word since 
the surrounding words carry effective in-
formation for the identification of 
MNE/RMWEs. 
2 Word suffix: Word suffix information is 
helpful to identify MNE/RMWEs. This is 
based on the observation that the 
MNE/RMWEs share some common suf-
fixes. The fixed length (say, n) word suf-
fix of the current and/or the surrounding 
word(s) can be treated as the feature. If 
the length of the corresponding word is 
less than or equal to n ? 1 then the feature 
values are not defined and are denoted by 
?ND?. The feature value is also not de-
fined (ND) if the token itself is a punctua-
tion symbol or contains any special sym-
bol or digit. Word suffixes are the effec-
tive features and work well for the highly 
inflective Indian languages like Manipuri.  
3 Word prefix: Word prefixes are also help-
ful to identify MNE/RMWEs. It is based 
on the observation that MNE/RMWEs 
share some common prefix strings. This 
feature has been defined in a similar way 
as that of the fixed length suffixes.  
4 MNE and RMWE Information: The 
MNE/RMWE tag(s) of the previous 
word(s) have been used as the only dy-
namic feature in the experiment. The out-
put tag of the previous word is very in-
formative in deciding the MNE/RMWE 
tag of the current word. 
5 Digit features: Several binary valued digit 
features have been defined depending 
upon the  
(i). Presence and/or the exact number 
of digits in a token. 
(a). CntDgtCma: Token consists of 
digits and comma 
 (b). CntDgtPrd: Token consists of 
digits and periods 
(ii). Combination of digits and sym-
bols. For example, 
(a). CntDgtSlsh: Token consists of 
digit and slash 
(b). CntDgtHph: Token consists of 
digits and hyphen 
(c). CntDgtPrctg: Token consists of 
digits and percentages 
(iii). Combination of digit and special 
symbols. For example, 
(a). CntDgtSpl: Token consists of 
digit and special symbol such as $, 
# etc. 
39
These binary valued digit features are 
helpful in recognizing miscellaneous 
NEs such as measurement expression 
and percentage expression. 
6 Infrequent word: The frequencies of the 
words in the training corpus have been 
calculated. A cut off frequency has been 
chosen in order to consider the words that 
occur with less than the cut off frequency 
in the training corpus. A binary valued 
feature ?Infrequent? is defined to check 
whether the current word appears in this 
infrequent word list or not. This is based 
on the observation that the infrequent 
words are most probably MNE/RMWEs. 
7 Length of a word: This binary valued fea-
ture is used to check whether the length 
of the current word is less than three or 
not. We have observed that very short 
words are most probably not the 
MNE/RMWEs. 
8 Part of Speech (POS) information: 
We have used an SVM-based POS 
tagger (Singh et. al., 2008) that was 
originally developed with 26 POS 
tags, defined for the Indian languages. 
The POS information of the current 
and/or the surrounding words can be 
effective for MNE/RMWE identifica-
tion. 
 
The Table 3 gives the statistics of training, 
development and test sets. The various nota-
tions used in the experiments are presented in 
Table 4. The Table 5 shows the recall (R), pre-
cision (P) and F-Score (FS) in percentage in the 
development set.  
 
Table 3. Statistics of the training, development 
and test sets 
 
Notation Meaning 
W[-i,+j]  Words spanning from the ith left 
position to the jth right position  
POS[-i, +j] POS tags of the words spanning 
from the ith left to the jth right 
positions 
Pre Prefix of the word 
Suf Suffix of the word 
NE [-i, -j] NE tags of the words spanning 
from the ith left to the jth left 
positions 
Table 4. Meaning of the notations 
 
Feature  R 
% 
P 
% 
FS 
% 
Static: W[-2,+2], POS[-
2,+2], |Pre|<=3, |Suf|<=3, 
Length, Infrequent, 
FirstWord, Digit  
Dynamic: 
MNE/RMWE[-2,-1]  
94.
26 
96.
72 
95.
47 
Static: W[-3,+3], POS[-
3,+3], |Pre|<=3, |Suf|<=3, 
Length, Infrequent, 
FirstWord, Digit 
Dynamic: 
MNE/RMWE[-3,-1] 
88.
23 
94.
82 
91.
40 
Static: W[-3,+2], POS[-
3,+2], |Pre|<=3, |Suf|<=3, 
Length, Infrequent, 
FirstWord, Digit 
Dynamic: 
MNE/RMWE[-3,-1] 
90.
32 
93.
18 
91.
72 
Static: W[-4,+3], POS[-
4,+3], |Pre|<=3, |Suf|<=3, 
Length, Infrequent, 
FirstWord, Digit 
Dynamic: 
MNE/RMWE[-2,-1] 
88.
15 
92.
62 
90.
32 
Static: W[-4,+3], POS[-
4,+3], |Pre|<=3, |Suf|<=3, 
Length, Infrequent, 
FirstWord, Digit 
Dynamic: 
MNE/RMWE[-3,-1] 
86.
24 
92.
31 
89.
17 
Static: W[-2,+2], POS[- 88. 91. 90.
 Training Devel-
opment 
Test 
# of sentences 1235 732 189 
#of wordforms 28,629 15,000 4,763 
# of distinct 
wordforms 
8671 4,212 2,207 
40
2,+2], |Pre|<=4, |Suf|<=4, 
Length, Infrequent, 
FirstWord, Digit 
Dynamic: 
MNE/RMWE[-2,-1] 
70 49 07 
Static: W[-3,+3], POS[-
3,+3], |Pre|<=4, |Suf|<=4, 
Length, Infrequent, 
FirstWord, Digit 
Dynamic: 
MNE/RMWE[-3,-1] 
85.
05 
90.
09 
87.
49 
Static: W[-4,+3], POS[-
4,+2], |Pre|<=4, |Suf|<=4, 
Length, Infrequent, 
FirstWord, Digit 
Dynamic: 
MNE/RMWE[-2,-1] 
78.
55 
89.
54 
83.
68 
Static: W[-4,+4], POS[-
4,+4], |Pre|<=4, |Suf|<=4, 
Length, Infrequent, 
FirstWord, Digit 
Dynamic: 
MNE/RMWE[-3,-1] 
73.
71 
89.
44 
80.
81 
Table 5. Results on the development set 
7 Results on the Test Set 
The best feature set (F) of Manipuri MNER and 
RMWE is identified as F=[prefixes and suffixes 
of length upto three characters of the current 
word, dynamic NE tags of the previous two 
words, POS tags of the previous two and next 
two words, digit information, length of the 
word]. After the selection of the best feature set, 
the SVM based system for MNE and RMWEs 
is tested on the test set of 4,763 wordforms. 
 
Reduplicated 
MWE type  
Recall 
% 
Precision 
% 
F-
Score 
% 
Complete and 
mimic 
96.21 95.12 95.66 
Partial 88.32 85.03 86.64 
Echo 97.76 96.45 97.10 
Double 93.23 94.23 93.72 
Semantic 74.45 81.56 77.84 
Table 6. Result on RMWE test set 
 
In this work, SVM that parses from left to 
right is considered. The break-up of the 
RMWEs and the scores are given in Table 6. 
The handling of semantic RMWEs requires fur-
ther investigation to improve the performance. 
The rule based RMWE identification (Kishorjit 
and Bandyopadhyay, 2010) shows a recall, pre-
cision and F-Score of 94.24%, 82.27% and 
87.68% respectively. 
 
Multiword 
NE  
Recall 
% 
Precision 
% 
F-
Score% 
Person 94.21 95.12 94.66 
Location 94.32 95.03 94.67 
Organization 95.76 93.45 94.59 
Miscellaneous 92.23 91.23 91.72 
Table 7. Result on MNE test set 
 
It is observed that the SVM based system 
outperforms the rule based system. Table 7 
shows the break-up scores of different types of 
MNEs and Table 8 shows the overall scores of 
MNE and RMWE. 
 
 Recall 
% 
Precision 
% 
F-Score 
% 
MNE 94.82 93.12 93.96 
RMWE 94.62 93.53 94.07 
Table 8. Overall recall, precision and F-Scores 
on test set 
8 Conclusion 
In this paper, the development of RMWEs iden-
tification and recognition of MNE for a re-
source-constrained language using web based 
corpus of Manipuri is reported. This training 
data of 28,629 is then manually annotated with 
a coarse-grained tagset of four NE tags and six 
RMWEs in order to apply SVM and tested with 
4,763 wordforms. The SVM classifier makes 
use of the different contextual information of 
the words along with the various orthographic 
word-level features. A number of experiments 
have been carried out to find out the best set of 
features for MWE in Manipuri. The SVM based 
RMWE system outperforms the rule based ap-
proach. The SVM based RMWE shows recall, 
precision and F-Score of 94.62%, 93.53% and 
94.07% respectively. The rule based RMWE 
41
identification shows a recall, precision and F-
Score of 94.24%, 82.27% and 87.68% respec-
tively. The overall performance of the system 
shows reasonable output for both MNE and 
RMWE. 
References 
Bharati, A., Sharma, D. M., Chaitanya, V., Kulkarni, 
A. P., & Sangal, R., 2001. LERIL: Collaborative 
effort fo r creating lexical resources. In Proceed-
ings of the 6th NLP Pacific Rim Symposium Post-
Conference Workshop, Japan. 
Dandapat, S., Mitra, P., and Sarkar, S., 
2006. Statistical investigation of Bengali noun-
verb (N-V) collocations as multi-word-
expressions, In Proceedings of Modeling and 
Shallow Parsing of Indian Languages (MSPIL), 
Mumbai, pp 230-233 
Ekbal, A., and Bandyopadhyay, S., 2008. A web 
based Bengali news corpus for Named Entity 
Recognition, Lang Resources & Evaluation 
(2008) 42:173?182, Springer 
Fletcher, W. H., 2001. Concordancing the web with 
KWiCFinder. In Proceedings of the Third North 
American Symposium on Corpus Linguistics and 
Language Teaching, Boston, MA, 23?25 March 
2001. 
Fletcher, W. H., 2004. Making the web more use-ful 
as source for linguists corpora. In U. Conor & T. 
A. Upton (Eds.), Applied corpus linguists: A mul-
tidimensional perspective (pp. 191?205). Amster-
dam: Rodopi. 
Kilgarriff, A., and Grefenstette, G., 2003. Introduc-
tion to the special issue on the web as corpus. 
Computational Linguistics, 29(3), 333?347. 
Kishorjit, N., and Bandyopadhyay, S., 2010. Identi-
fication of Reduplicated MWEs in Manipuri: A 
Rule Based Approch, In proceedings of 23rd 
International Conference on the Computer 
Processing of Oriental Languages (ICCPOL 
2010) - New Generation in Asian Information 
Processing , Redmond City, CA  
Kunchukuttan, A., and Damani, O. P., 2008. A Sys-
tem for Compound Nouns Multiword Expression 
Extraction for Hindi, In  Proceedings of 6th Inter-
national conference on Natural Language 
Processing (ICON 2008), Pune, India  
Robb, T., 2003. Google as a corpus tool? ETJ 
Journal, 4(1), Spring. 
Rundell, M., 2000. The biggest corpus of all. Huma-
nising Language Teaching, 2(3) 
Singh. Chungkham Y., 2000. Manipuri Grammar, 
Rajesh Publications, Delhi, pp 190-204 
Singh, Thoudam D., Ekbal, A., Bandyopadhyay, S. 
2008. Manipuri POS tagging using CRF and 
SVM: A language independent approach, In pro-
ceeding of 6th International conference on Natural 
Language Processing (ICON -2008), Pune, India, 
pp 240-245 
Singh, Thoudam D., Kishorjit, N., Ekbal, A., Ban-
dyopadhyay, S., 2009. Named Entity Recognition 
for Manipuri using Support Vector Machine, In 
proceedings of 23rd Pacific Asia Conference on 
Language, Information and Computation (PAC-
LIC 23), Hong Kong, pp 811-818 
Singh, Thoudam D., Singh, Yengkhom R. and Ban-
dyopadhyay, S., 2010. Manipuri-English Semi 
Automatic Parallel Corpora Extraction from Web, 
In proceedings of 23rd International Conference 
on the Computer Processing of Oriental Lan-
guages (ICCPOL 2010) - New Generation in 
Asian Information Processing , Redmond City, 
CA 
Vapnik, Vladimir N. 1995: The nature of Statistical 
learning theory. Springer 
42
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 37?45,
Beijing, August 2010
Automatic Extraction of Complex Predicates in Bengali  
Dipankar Das     Santanu Pal      Tapabrata Mondal       Tanmoy Chakraborty   
 
  Sivaji Bandyopadhyay 
Department of Computer Science and Engineering 
Jadavpur University 
dipankar.dipnil2005@gmail.com, 
santanupersonal1@gmail.com, 
tapabratamondal@gmail.com, its_tanmoy@yahoo.co.in, 
sivaji_cse_ju@yahho.com 
 
 
Abstract 
This paper presents the automatic ex-
traction of Complex Predicates (CPs) 
in Bengali with a special focus on 
compound verbs (Verb + Verb) and 
conjunct verbs (Noun /Adjective + 
Verb). The lexical patterns of com-
pound and conjunct verbs are extracted 
based on the information of shallow 
morphology and available seed lists of 
verbs. Lexical scopes of compound and 
conjunct verbs in consecutive sequence 
of Complex Predicates (CPs) have 
been identified. The fine-grained error 
analysis through confusion matrix 
highlights some insufficiencies of lexi-
cal patterns and the impacts of different 
constraints that are used to identify the 
Complex Predicates (CPs). System 
achieves F-Scores of 75.73%, and 
77.92% for compound verbs and 
89.90% and 89.66% for conjunct verbs 
respectively on two types of Bengali 
corpus.      
1 Introduction 
Complex Predicates (CPs) contain [verb] + 
verb (compound verbs) or [noun/ 
adjective/adverb] +verb (conjunct verbs) 
combinations in South Asian languages (Hook, 
1974). To the best of our knowledge, Bengali  
 
 
is not only a language of South Asia but also 
the sixth popular language in the World 1 , 
second in India and the national language of 
Bangladesh. The identification of Complex 
Predicates (CPs) adds values for building 
lexical resources (e.g. WordNet (Miller et al, 
1990; VerbNet (Kipper-Schuler, 2005)), 
parsing strategies and machine translation 
systems.  
Bengali is less computerized compared to 
English due to its morphological enrichment. 
As the identification of Complex Predicates 
(CPs) requires the knowledge of morphology, 
the task of automatically extracting the Com-
plex Predicates (CPs) is a challenge. Complex 
Predicates (CPs) in Bengali consists of two 
types, compound verbs (CompVs) and conjunct 
verbs (ConjVs). 
The compound verbs (CompVs) (e.g. ? ?? 
? ?? mere phela ?kill?, ???? ???? bolte laglo 
?started saying?) consist of two verbs. The first 
verb is termed as Full Verb (FV) that is present 
at surface level either as conjunctive participial 
form -e ?e or the infinitive form -?  ?te. The 
second verb bears the inflection based on 
Tense, Aspect and Person. The second verbs 
that are termed as Light Verbs (LV) are 
polysemous, semantically bleached and 
confined into some definite candidate seeds 
(Paul, 2010).  
On the other hand, each of the Bengali con-
junct verbs (ConjVs) (e.g. ???? ??? bharsha 
                                                 
1http://www.ethnologue.com/ethno_docs/distributio
n.asp?by=size 
37
kara ?to depend?, ???? ??? jhakjhak kara ?to 
glow?) consists of noun or adjective followed 
by a Light Verb (LV). The Light Verbs (LVs) 
bear the appropriate inflections based on 
Tense, Aspect and Person.   
According to the definition of multi-word 
expressions (MWEs)(Baldwin and Kim, 2010), 
the absence of conventional meaning of the 
Light Verbs in Complex Predicates (CPs) 
entails us to consider the Complex Predicates 
(CPs) as MWEs (Sinha, 2009). But, there are 
some typical examples of Complex Predicates 
(CPs), e.g. ? ?? ??? dekha kara ?see-do? that 
bear the similar lexical pattern as Full Verb 
(FV)+ Light Verb (LV) but both of the Full 
Verb (FV) and Light Verb (LV) loose their 
conventional meanings and generate a 
completely different meaning (?to meet? in this 
case).  
In addition to that, other types of predicates 
such as ???? ? ? niye gelo ?take-go? (took and 
went), ???? ? ? diye gelo ?give-go? (gave and 
went) follows the similar lexical patterns 
FV+LV as of Complex Predicates (CPs) but 
they are not mono-clausal. Both the Full Verb 
(FV) and Light Verb (LV) behave like 
independent syntactic entities and they belong 
to non-Complex Predicates (non-CPs). The 
verbs are also termed as Serial Verb (SV) 
(Mukherjee et al, 2006). 
Butt (1993) and Paul (2004) have also 
mentioned the following criteria that are used 
to check the validity of complex predicates 
(CPs) in Bengali. The following cases are the 
invalid criteria of complex predicates (CPs). 
1. Control Construction (CC): ????? ??? 
likhte bollo ?asked to write?, ????? ???? 
??? likhte badhyo korlo ?forced to 
write? 
2. Modal Control Construction (MCC): 
? ?? ??? jete hobe ?have to go? ? ?? ??? 
khete hobe ?have to eat? 
3. Passives (Pass) : ??? ??? dhora porlo 
?was caught?, ???? ?? mara holo ?was 
beaten? 
4. Auxiliary Construction (AC): ??? ??? 
bose ache ?is sitting?, ???? ??? niye chilo 
?had taken?. 
Sometimes, the successive sequence of the 
Complex Predicates (CPs) shows a problem of 
deciding the scopes of individual Complex 
Predicates (CPs) present in that sequence. For 
example the sequence, u?? ??? ? ???? uthe pore 
dekhlam ?rise-wear-see? (rose and saw) seems 
to contain two Complex Predicates (CPs) (u?? 
??? uthe pore ?rose? and ??? ? ???? pore 
dekhlam ?wore and see?). But there is actually 
one Complex Predicate (CP). The first one u?? 
??? uthe pore ?rose? is a compound verb 
(CompV) as well as a Complex Predicate (CP). 
Another one is ? ???? dekhlam ?saw? that is a 
simple verb. As the sequence is not mono-
clausal, the Complex Predicate (CP) u?? ??? 
uthe pore ?rose? associated with ? ???? dekhlam 
?saw? is to be separated by a lexical boundary. 
Thus the determination of lexical scopes of 
Complex Predicates (CPs) from a long con-
secutive sequence is indeed a crucial task.      
 The present task therefore not only aims to 
extract the Complex Predicates (CPs) 
containing compound and conjunct verbs but 
also to resolve the problem of deciding the 
lexical scopes automatically. The compound 
verbs (CompVs) and conjunct verbs (ConjVs) 
are extracted from two separate Bengali 
corpora based on the morphological 
information (e.g. participle forms, infinitive 
forms and inflections) and list of Light Verbs 
(LVs). As the Light Verbs (LVs) in the 
compound verbs (CompVs) are limited in 
number, fifteen predefined verbs (Paul, 2010) 
are chosen as Light Verbs (LVs) for framing 
the compound verbs (CompVs).  A manually 
prepared seed list that is used to frame the 
lexical patterns for conjunct verbs (ConjVs) 
contains frequently used Light Verbs (LVs).  
An automatic method is designed to identify 
the lexical scopes of compound and conjunct 
verbs in the long sequences of Complex 
Predicates (CPs). The identification of lexical 
scope of the Complex Predicates (CPs) 
improves the performance of the system as the 
number of identified Complex Predicates 
(CPs) increases.  
Manual evaluation is carried out on two 
types of Bengali corpus. The experiments are 
carried out on 800 development sentences 
from two corpora but the final evaluation is 
carried out on 1000 sentences. Overall, the 
system achieves F-Scores of 75.73%, and 
77.92% for compound verbs and 89.90% and 
89.66% for conjunct verbs respectively.  
38
The error analysis shows that not only the 
lexical patterns but also the augmentation of 
argument structure agreement (Das, 2009), the 
analysis of Non-MonoClausal Verb (NMCV) or 
Serial Verb, Control Construction (CC), 
Modal Control Construction (MCC), Passives 
(Pass) and Auxiliary Construction (AC) (Butt, 
1993; Paul, 2004) are also necessary to 
identify the Complex Predicates (CPs). The 
error analysis shows that the system suffers in 
distinguishing the Complex Predicates (CPs) 
from the above constraint constructions.  
The rest of the paper is organized as fol-
lows. Section 2 describes the related work 
done in this area. The automatic extraction of 
compound and conjunct verbs is described in 
Section 3. In Section 4, the identification of 
lexical scopes of the Complex Predicates 
(CPs) is mentioned. Section 5 discusses the 
results of evaluation along with error analysis. 
Finally, Section 6 concludes the paper. 
2 Related Work 
The general theory of complex predicate is 
discussed in Alsina (1996). Several attempts 
have been organized to identify complex 
predicates in South Asian languages (Abbi, 
1991; Bashir, 1993; Verma, 1993) with a spe-
cial focus to Hindi (Burton-Page, 1957; Hook, 
1974), Urdu (Butt, 1995), Bengali (Sarkar, 
1975; Paul, 2004), Kashmiri (Kaul, 1985) and 
Oriya (Mohanty, 1992). But the automatic ex-
traction of Complex Predicates (CPs) has been 
carried out for few languages, especially 
Hindi. 
The task described in (Mukherjee et al, 
2006) highlights the development of a database 
based on the hypothesis that an English verb is 
projected onto a multi-word sequence in Hindi. 
The simple idea of projecting POS tags across 
an English-Hindi parallel corpus considers the 
Complex Predicate types, adjective-verb (AV), 
noun-verb (NV), adverb-verb (Adv-V), and 
verb-verb (VV) composites. A similar task 
(Sinha, 2009) presents a simple method for 
detecting Complex Predicates of all kinds us-
ing a Hindi-English parallel corpus. His simple 
strategy exploits the fact that Complex Predi-
cate is a multi-word expression with a meaning 
that is distinct from the meaning of the Light 
Verb. In contrast, the present task carries the 
identification of Complex Predicates (CPs) 
from monolingual Bengali corpus based on 
morphological information and lexical pat-
terns. 
The analysis of V+V complex predicates 
termed as lexical compound verbs (LCpdVs) 
and the linguistic tests for their detection in 
Hindi are described in (Chakrabarti et al, 
2008). In addition to compound verbs, the pre-
sent system also identifies the conjunct verbs 
in Bengali. But, it was observed that the identi-
fication of Hindi conjunct verbs that contain 
noun in the first slot is puzzling and therefore a 
sophisticated solution was proposed in (Das, 
2009) based on the control agreement strategy 
with other overtly case marked noun phrases. 
The present task also agrees with the above 
problem in identifying conjunct verbs in Ben-
gali although the system satisfactorily identi-
fies the conjunct verbs (ConjVs). 
Paul (2003) develops a constraint-based 
mechanism within HPSG framework for com-
posing Indo-Aryan compound verb construc-
tions with special focus on Bangla (Bengali) 
compound verb sequences. Postulating seman-
tic relation of compound verbs, another work 
(Paul, 2009) proposed a solution of providing 
lexical link between the Full verb and Light 
Verb to store the Compound Verbs in Indo 
WordNet without any loss of generalization. 
To the best of our knowledge, ours is the first 
attempt at automatic extraction of Complex 
Predicates (CPs) in Bengali.  
3 Identification of Complex Predi-
cates (CPs) 
The compound verbs (CompVs) and conjunct 
verbs (ConjVs) are identified from the shallow 
parsed result using a lexical pattern matching 
technique. 
3.1 Preparation of Corpora 
Two types of Bengali corpus have been con-
sidered to carry out the present task. One cor-
pus is collected from a travel and tourism do-
main and another from an online web archive 
of Rabindranath Rachanabali 2 . Rabindra 
Rachanabali corpus is a large collection of 
short stories of Rabindranath Tagore. The for-
                                                 
2 www.rabindra-rachanabali.nltr.org 
39
mer EILMT travel and tourism corpus is ob-
tained from the consortium mode project ?De-
velopment of English to Indian Languages 
Machine Translation (EILMT 3) System?. The 
second type of corpus is retrieved from the 
web archive and pre-processed accordingly. 
Each of the Bengali corpora contains 400 and 
500 development and test sentences respec-
tively.   
The sentences are passed through an open 
source Bengali shallow parser 4. The shallow 
parser gives different morphological informa-
tion (root, lexical category of the root, gender, 
number, person, case, vibhakti, tam, suffixes 
etc.) that help in identifying the lexical patterns 
of Complex Predicates (CPs).  
3.2 Extracting Complex Predicates (CPs) 
Manual observation shows that the Complex 
Predicates (CPs) contain the lexical pattern 
{[XXX] (n/adj) [YYY] (v)} in the shallow 
parsed sentences where XXX and YYY repre-
sent any word. But, the lexical category of the 
root word of XXX is either noun (n) or adjec-
tive (adj) and the lexical category of the root 
word of YYY is verb (v). The shallow parsed 
sentences are pre-processed to generate the 
simplified patterns. An example of similar 
lexical pattern of the shallow parsed result and 
its simplified output is shown in Figure 1.  
 
((NP  a????  NN  <fs 
?f='a???? ,n,,sg,,d,?? ? ,?? ? '>  ))              
   
((VGF  ???????      VM       <fs 
?f='?r,v,,,5,,? ,? '>     )) 
a???? |no?n|a???? /NN/NP/ 
(a???? ^n^*^sg^*^d^?? ^?? ? )_ 
???????|v??b|???????/VM/VGF/              
(?r^v^*^*^1^*^? ^? ) 
         
 Figure 1. Example of a pre-processed shallow 
parsed result. 
 
                                                 
3 The EILMT project is funded by the Department of 
Information Technology (DIT), Ministry of Communica-
tions and Information Technology (MCIT), Government 
of India. 
4http://ltrc.iiit.ac.in/showfile.php?filename=downloads/sh
allow_parser.php 
The corresponding lexical categories of the 
root words a???? adhyan ?study? (e.g. noun 
for ?n?) and '?r  kar, ?do? (e.g. verb for ?v?) are 
shown in bold face in Figure 1. The f ollowing 
example is of conjunct verb (ConjV).  
The extraction of Bengali compound verbs 
(CompVs) is straightforward rather than con-
junct verbs (ConjVs). The lexical pattern of 
compound verb is {[XXX](v) [YYY] (v)} where 
the lexical or basic POS categories of the root 
words of  ?XXX? and ?YYY? are only verb. If 
the basic POS tags of the root forms of ?XXX? 
and ?YYY? are verbs (v) in shallow parsed sen-
tences, then only the corresponding lexical 
patterns are considered as the probable candi-
dates of compound verbs (CompVs).  
Example 1: 
??i??|v??b|??i??/VM/VGNF/? ? ^v^*^*^?ny^*^i??^i??)
#??????|v??b|??????/VM/VGF/(??^v^*^*^1^*^?^?) 
Example 1 is a compound verb (CompV) but 
Example 2 is not. In Example 2, the lexical 
category or the basic POS of the Full Verb 
(FV) is noun (n) and hence the pattern is dis-
carded as non-compound verb (non-CompV). 
Example 2: 
?k? |noun|?k? /NN/NP/(?k? ^n^*^*^*^*^*^pos
lcat="NM") #  
?????|verb|?????/VM/VGNF/(?r^v^*^*^any^*^i??
^i??) 
Bengali, like any other Indian languages, is 
morphologically very rich. Different suffixes 
may be attached to a Light Verb (LVs) (in this 
case [YYY]) depending on the various features 
such as Tense, Aspect, and Person.  
In case of extracting compound verbs 
(CompVs), the Light Verbs are identified from 
a seed list (Paul, 2004). The list of Light Verbs 
is specified in Table 1. The dictionary forms of 
the Light Verbs are stored in this list. As the 
Light Verbs contain different suffixes, the pri-
mary task is to identify the root forms of the 
Light Verbs (LVs) from shallow parsed result. 
Another table that stores the root forms and the 
corresponding dictionary forms of the Light 
Verbs is used in the present task. The table 
contains a total number of 378 verb entries 
including Full Verbs (FVs) and Light Verbs 
(LVs). The dictionary forms of the Light Verbs 
(LVs) are retrieved from the Table. 
On the other hand, the conjunctive particip-
ial form -e/i?? -e/iya or the infinitive form -
? /i?? ?te/ite are attached with the Full Verbs 
40
(FVs) (in this case [XXX]) in compound verbs 
(CompVs). i?? / iya and i??/ ite are also used 
for conjunctive participial form -e ?e or the 
infinitive form -?  ?te respectively in litera-
ture. The participial and infinitive forms are 
checked based on the morphological informa-
tion (e.g. suffixes of the verb) given in the 
shallow parsed results. In Example 1, the Full 
Verb (FV) contains -i?? -iya suffix. If the dic-
tionary forms of the Light Verbs (LVs) are pre-
sent in the list of Light Verbs and the Full 
Verbs (FVs) contain the suffixes of -e/i?? -
e/iya or ? /i?? ?te/ite, both verbs are combined 
to frame the patterns of compound verbs 
(CompVs). 
 
aSa ?come?          d?Ra ?stand? 
rakha ?keep?       ana ?bring?  
deoya ?give?        pOra ?fall?  
paTha ?send?       bERano ?roam? 
neoya ?take?        tola ?lift? 
bOSa ?sit?           oTha ?rise? 
jaoya ?go?           chaRa ?leave? 
phEla ?drop?       mOra ?die? 
 
Table 1. List of Light Verbs for compound 
verbs. 
The identification of conjunct verbs 
(ConjVs) requires the lexical pattern (Noun / 
Adjective + Light Verb) where a noun or an 
adjective is followed by a Light Verb (LV). The 
dictionary forms of the Light Verbs (LVs) that 
are frequently used as conjunct verbs (ConjVs) 
are prepared manually. The list of Light Verbs 
(LVs) is given in Table 2. The detection of 
Light Verbs (LVs) for conjunct verbs (ConjVs) 
is similar to the detection of the Light Verbs 
(LVs) for compound verbs (CompVs) as de-
scribed earlier in this section.  If the basic POS 
of the root of the first words ([XXX]) is either 
?noun? or ?adj? (n/adj) and the basic POS of 
the following word ([YYY]) is ?verb? (v), the 
patterns are considered as conjunct verbs 
(ConjVs). The Example 2 is an example of 
conjunct verb (ConjV). 
For example, ???? ??? (jhakjhak kara ?to 
glow?), ???? ??? (taktak ?to glow?), ?? ??? ??? 
(chupchap kara ?to silent?) etc are identified as 
conjunct verbs (ConjVs) where the basic POS 
of the former word is an adjective (adj) fol-
lowed by ??? kara ?to do?, a common Light 
Verb.  
deoya ?give?        kara  ?do?    
neoya ?take?        laga  ?start?            
paoya ?pay?         kata  ?cut?   
  
Table 2. List of Light Verbs for conjunct verbs. 
 
Example 3:  
  ????|?dj|???? /JJ/JJP/(???? ^?dj) # 
????|v??b|????/VM/VGF/(?r^v^*^*^5^*^?^?) 
But, the extraction of conjunct verbs 
(ConjVs) that have a ?noun+verb? construction 
is descriptively and theoretically puzzling 
(Das, 2009). The identification of lexical pat-
terns is not sufficient to recognize the com-
pound verbs (CompVs). For example, ?i ? ??? 
boi deoya ?give book? and ???? ? ??? bharsa 
deyoa ?to assure? both contain similar lexical 
pattern (noun+verb) and same Light Verb ? ??? 
deyoa. But, ???? ? ??? bharsa deyoa ?to assure? 
is a conjunct verb (ConjV) whereas ?i ? ??? boi 
deoya ?give book? is not a conjunct verb 
(ConjV). Linguistic observation shows that the 
inclusion of this typical category into conjunct 
verbs (ConjVs) requires the additional knowl-
edge of syntax and semantics.  
In connection to conjunct verbs (ConjVs), 
(Mohanty, 2010) defines two types of conjunct 
verbs (ConjVs), synthetic and analytic. A syn-
thetic conjunct verb is one in which both the 
constituents form an inseparable whole from 
the semantic point of view or semantically 
non-compositional in nature. On the other 
hand, an analytic conjunct verb is semantically 
compositional. Hence, the identification of 
conjunct verbs requires knowledge of seman-
tics rather than only the lexical patterns. 
It is to be mentioned that sometimes, the 
negative markers (?? no, ??i nai) are attached 
with the Light Verbs u?????  uthona ?do not get 
up? ? ?????  phelona ?do not throw?. Negative 
attachments are also considered in the present 
task while checking the suffixes of Light Verbs 
(LVs). 
4 Identification of Lexical Scope for 
Complex Predicates (CPs) 
The identification of lexical scopes of the 
Complex Predicates (CPs) from their succes-
sive sequences shows that multiple Complex 
41
Predicates (CPs) can occur in a long sequence. 
An automatic method is employed to identify 
the Complex Predicates (CPs) along with their 
lexical scopes. The lexical category or basic 
POS tags are obtained from the parsed sen-
tences. 
If the compound and conjunct verbs occur 
successively in a sequence, the left most two 
successive tokens are chosen to construct the 
Complex Predicate (CP). If successive verbs 
are present in a sequence and the dictionary 
form of the second verb reveals that the verb is 
present in the lists of compound Light Verbs 
(LV), then that Light Verb (LV) may be a part 
of a compound verb (CompV). For that reason, 
the immediate previous word token is chosen 
and tested for its basic POS in the parsed result. 
If the basic POS of the previous word is ?verb 
(v)? and any suffixes of either conjunctive par-
ticipial form -e/i?? -e/iya or the infinitive form 
-? /i?? ?te/ite is attached to the previous verb, 
the two successive verbs are grouped together 
to form a compound verb (CompV) and the 
lexical scope is fixed for the Complex Predi-
cate (CP).  
If the previous verb does not contain -e/i?? 
-e/iya or -? /i?? ?te/ite inflections, no com-
pound verb (CompV) is framed with these two 
verbs. But, the second Light Verb (LV) may be 
a part of another Complex Predicate (CP). This 
Light Verb (LV) is now considered as the Full 
Verb (FV) and its immediate next verb is 
searched in the list of compound Light Verbs 
(LVs) and the formation of compound verbs 
(CompVs) progresses similarly.  If the verb is 
not in the list of compound Light Verbs, the 
search begins by considering the present verb 
as Full Verb (FV) and the search goes in a 
similar way. 
The following examples are given to illus-
trate the formation of compound verbs 
(CompVs) and find the lexical scopes of the 
compound verbs (CompVs). 
 
???      ????       ????     ???      ? ??? 
(ami)       (chalte)      (giye)    (pore)    (gelam). 
I <fell down while walking>. 
 
Here, ?chalte giye pore gelam? is a verb 
group. The two left most verbs ???? ???? chalte 
giye are picked and the dictionary form of the 
second verb is searched in the list of com-
pound Light Verbs. As the dictionary form 
(jaoya ?go?) of the verb ???? giye is present in 
the list of compound Light Verbs (as shown in 
Table 1), the immediate previous verb ???? 
chalte is checked for inflections -e/i?? -e/iya 
or -? /i?? ?te/ite.  As the verb ???? chalte con-
tains the inflection -?  -te , the verb group ???? 
???? chalte giye is a compound verb (CompV) 
where ???? giye is a Light Verb and ???? chalte 
is the Full Verb with inflection (-?  -te).  Next 
verb group, ???   ? ??? pore gelam is identified 
as compound verb (CompV) in a similar way 
(??+ (-e) por+ (-e) + ? ??? gelam (jaoya ?go?)).  
Another example is given as follows.  
 
???    u??      ???       ? ????       ?   
(ami)   (uthe)      (pore)      (dekhlam)    (je)  
?? ??      e????        ? i 
(tumi)     (ekhane)       (nei) 
I <get up and saw> that you are not here 
 
Here, u?? ??? ? ???? uthe pore dekhlam is 
another verb group. The immediate next verb 
of u?? uthe is ??? pore that is chosen and its 
dictionary form is searched in the list of com-
pound Light Verbs (LV) similarly. As the dic-
tionary form (???  pOra) of the verb ??? pore 
is present in the list of Light Verbs and the 
verb u??   uthe contains the inflection -e ?e, 
the consecutive verbs frame a compound verb 
(CompV) u?? ??? where u?? uthe is a Full Verb 
with inflection -e ?e and ??? pore is a Light 
Verb. The final verb ? ????      dekhlam is 
chosen and as there is no other verb present, 
the verb ? ???? dekhlam is excluded from any 
formation of compound verb (CompV) by con-
sidering it as a simple verb.  
Similar technique is adopted for identifying 
the lexical scopes of conjunct verbs (ConjVs). 
The method seems to be a simple pattern 
matching technique in a left-to-right fashion 
but it helps in case of conjunct verbs (ConjVs). 
As the noun or adjective occur in the first slot 
of conjunct verbs (ConjVs) construction, the 
search starts from the point of noun or adjec-
tive. If the basic POS of a current token is ei-
ther ?noun? or ?adjective? and the dictionary 
form of the next token with the basic POS 
?verb (v)? is in the list of conjunct Light Verbs 
(LVs), then the two consecutive tokens are 
42
combined to frame the pattern of a conjunct 
verb (ConjV). 
For example, the identification of lexical 
scope of a conjunct verb (ConjV) from a se-
quence such as u????  ???? ? ??? uparjon korte 
gelam ?earn-do-go? (went to earn) identifies 
the conjunct verb (ConjV) u????  ???? uparjon 
korte. There is another verb group ???? ? ??? 
korte gelam that seems to be a compound verb 
(CompV) but is excluded by considering ?? ??? 
gelam as a simple verb. 
5 Evaluation 
The system is tested on 800 development sen-
tences and finally applied on a collection of 
500 sentences from each of the two Bengali 
corpora. As there is no annotated corpus avail-
able for evaluating Complex Predicates (CPs), 
the manual evaluation of total 1000 sentences 
from the two corpora is carried out in the pre-
sent task.  
The recall, precision and F-Score are con-
sidered as the standard metrics for the present 
evaluation. The extracted Complex Predicates 
(CPs) contain compound verb (CompV) and 
conjunct verbs (ConjVs). Hence, the metrics 
are measured for both types of verbs individu-
ally. The separate results for two separate cor-
pora are shown in Table 3 and Table 4 respec-
tively. The results show that the system identi-
fies the Complex Predicates (CPs) satisfacto-
rily from both of the corpus. In case of Com-
pound Verbs (CompVs), the precision value is 
higher than the recall. The lower recall value 
of Compound Verbs (CompVs) signifies that 
the system fails to capture the other instances 
from overlapping sequences as well as non-
Complex predicates (non-CPs).  
But, it is observed that the identification of 
lexical scopes of compound verbs (CompVs) 
and conjunct verbs (ConjVs) from long se-
quence of successive Complex Predicates 
(CPs) increases the number of Complex Predi-
cates (CPs) entries along with compound verbs 
(CompVs) and conjunct verbs (ConjVs). The 
figures shown in bold face in Table 3 and Ta-
ble 4 for the Travel and Tourism corpus and 
Short Story corpus of Rabindranath Tagore 
indicates the improvement of identifying lexi-
cal scopes of the Complex Predicates (CPs).  
In comparison to other similar language 
such as Hindi (Mukerjee et al, 2006) (the re-
ported precision and recall are 83% and 46% 
respectively), our results (84.66% precision 
and 83.67% recall) are higher in case of ex-
tracting Complex Predicates (CPs). The reason 
may be of resolving the lexical scope and han-
dling the morphosyntactic features using shal-
low parser.  
In addition to Non-MonoClausal Verb 
(NMCV) or Serial Verb, the other criteria 
(Butt, 1993; Paul, 2004) are used in our pre-
sent diagnostic tests to identify the complex 
predicates (CPs). The frequencies of 
Compound Verb (CompV), Conjunct Verb 
(ConjV) and the instances of other constraints 
of non Complex Predicates (non-CPs) are 
shown in Figure 2. It is observed that the num-
bers of instances of Conjunct Verb (ConjV), 
Passives (Pass), Auxiliary Construction (AC) 
and Non-MonoClausal Verb (NMCV) or Serial 
Verb are comparatively high than other in-
stances in both of the corpus. 
 
EILMT  Recall Precision F-
Score 
Compound  
Verb 
(CompV) 
65.92% 
70.31% 
 
80.11% 
82.06% 
72.32% 
75.73%
Conjunct 
Verb 
(ConjV) 
94.65% 
96.96% 
80.44% 
83.82% 
86.96% 
89.90%
 
Table 3. Recall, Precision and F-Score of the 
system for acquiring the CompVs and ConjVs 
from EILMT Travel and Tourism Corpus. 
 
Rabindra 
Rachana-
bali 
Recall Precision F-
Score 
Compound  
Verb 
(CompV) 
68.75% 
72.22% 
 
81.81% 
84.61% 
74.71% 
77.92%
Conjunct 
Verb 
(ConjV) 
94.11% 
95.23% 
83.92% 
84.71% 
88.72% 
89.66%
 
Table 4. Recall, Precision and F-Score of the 
system for acquiring the CompVs and ConjVs 
from Rabindra Rachanabali corpus. 
 
43
 
 CompV ConjV NMCV CC MCC Pass AC 
CompV 0.76 0.00 0.02 0.00 0.00 0.03 0.02 
ConjV 0.04 0.72 0.03 0.01 0.02 0.02 0.02 
NMCV 0.17 0.18 0.65 0.00 0.02 0.02 0.02 
CC 0.01 0.00 0.00 0.56 0.01 0.02 0.02 
MCC 0.00 0.00 0.00 0.07 0.65 0.00 0.02 
Pass 0.12 0.01 0.00 0.00 0.00 0.78 0.00 
AC 0.06 0.07 0.04 0.00 0.00 0.08 0.54 
Table 5. Confusion Matrix for CPs and constraints of non-CPs (in %).  
 
0
50
100
150
200
CompV
ConjV
CC MCC
Pass
AC NMCV
EILMTRabindra
  
Figure 2. The frequencies of Complex Predi-
cates (CPs) and different constrains of non-
Complex Predicates (non-CPs). 
 
The error analysis is conducted on both of 
the corpus. Considering both corpora as a 
whole single corpus, the confusion matrix is 
developed and shown in Table 5. The bold face 
figures in Table 5 indicate that the percentages 
of non-Complex Predicates (non-CPs) such as 
Non-MonoClausal Verbs (NMCV), Passives 
(Pass) and Auxiliary Construction (AC) that 
are identified as compound verbs (CompVs). 
The reason is the frequencies of the non-
Complex Predicates (non-CPs) that are rea-
sonably higher in the corpus.  In case of con-
junct verbs (ConjVs), the Non-MonoClausal 
Verbs (NMCV) and Auxiliary Construction 
(AC) occur as conjunct verbs (ConjVs).  The 
system also suffers from clausal detection that 
is not attempted in the present task. The Pas-
sives (Pass) and Auxiliary Construction (AC) 
requires the knowledge of semantics with ar-
gument structure knowledge. 
6 Conclusion 
In this paper, we have presented a study of 
Bengali Complex Predicates (CPs) with a spe-
cial focus on compound verbs, proposed auto-
matic methods for their extraction from a cor-
pus and diagnostic tests for their evaluation. 
The problem arises in case of distinguishing 
Complex Predicates (CPs) from Non-Mono-
Clausal verbs, as only the lexical patterns are 
insufficient to identify the verbs. In future task, 
the subcategorization frames or argument 
structures of the sentences are to be identified 
for solving the issues related to the errors of 
the present system.  
References 
Abbi, Anvita. 1991. Semantics of Explicator Com-
pound Verbs. In South Asian Languages, Lan-
guage Sciences, 13(2): 161-180. 
Alsina, Alex. 1996. Complex Predicates: Structure 
and Theory. Center for the Study of Language 
and Information Publications, Stanford, CA. 
Bashir, Elena. 1993. Causal chains and compound 
verbs. In M. K. Verma ed. (1993) Complex 
Predicates in South Asian Languages, Manohar 
Publishers and Distributors, New Delhi. 
Burton-Page, John. 1957. Compound and conjunct 
verbs in Hindi. Bulletin of the School of Oriental 
and African Studies, 19: 469-78. 
Butt, Miriam. 1995. The Structure of Complex 
Predicates in Urdu. Doctoral Dissertation, Stan-
ford University. 
Chakrabarti, Debasri, Mandalia Hemang, Priya 
Ritwik, Sarma Vaijayanthi, Bhattacharyya Push-
pak. 2008. Hindi Compound Verbs and their 
Automatic Extraction. International Conference 
on Computational Linguistics ?2008, pp. 27-30. 
44
Das, Pradeep Kumar. 2009. The form and function 
of Conjunct verb construction in Hindi. Global 
Association of Indo-ASEAN Studies, Daejeon, 
South Korea. 
Hook, Peter. 1974. The Compound Verbs in Hindi. 
The Michigan Series in South and South-east 
Asian Language and Linguistics. The University 
of Michigan. 
Kaul, Vijay Kumar. 1985. The Compound Verb in 
Kashmiri. Unpublished Ph.D. dissertation. Ku-
rukshetra University. 
Kipper-Schuler, Karin. 2005. VerbNet: A broad-
coverage,  comprehensive verb lexicon. Ph.D. 
thesis, Computer and Information Science Dept., 
University of Pennsylvania, Philadelphia,PA  
Miller, George, Richard Beckwith, Christiane Fell-
baum, Derek Gross and Katherine Miller. 1990. 
Five Papers on WordNet. CSL Report 43, Cogni-
tive Science Laboratory, Princeton University, 
Princeton. 
Mohanty, Gopabandhu. 1992. The Compound 
Verbs in Oriya. Ph. D. dissertation, Deccan Col-
lege Post-Graduate and Research Institute, Pune. 
Mohanty, Panchanan. 2010. WordNets for Indian 
Languages: Some Issues. Global WordNet Con-
ference-2010, pp. 57-64. 
Mukherjee, Amitabha, Soni Ankit and Raina Achla 
M. 2006. Detecting Complex Predicates in Hindi 
using POS Projection across Parallel Corpora. 
Multiword Expressions: Identifying and Exploit-
ing Underlying Properties Association for Com-
putational Linguistics, pp. 28?35, Sydney.  
Paul, Soma. 2010. Representing Compound Verbs 
in Indo WordNet. Golbal Wordnet Conference-
2010, pp. 84-91. 
Paul, Soma. 2004. An HPSG Account of Bangla 
Compound Verbs with LKB Implementation. 
Ph.D dissertation, University of Hyderabad, Hy-
derabad. 
Paul, Soma. 2003. Composition of Compound 
Verbs in Bangla. Multi-Verb constructions. 
Trondheim  Summer School. 
Sarkar, Pabitra. 1975. Aspects of Compound Verbs 
in Bengali. Unpublished M.A. dissertation, Chi-
cago University. 
Sinha, R. Mahesh, K. 2009. Mining Complex 
Predicates In Hindi Using A Parallel Hindi-
English Corpus. Multiword Expression Work-
shop, Association of Computational Linguistics-
International Joint Conference on Natural Lan-
guage Processing-2009, pp. 40-46, Singapore. 
Timothy, Baldwin, Su Nam Kim. 2010. Multiword 
Expressions. In Nitin Indurkhya and Fred J. 
Damerau (eds.) Handbook of Natural Language 
Processing, Second Edition, Chapman & 
Hall/CRC, London, UK, pp. 267-292. 
Verma, Manindra K.1993. Complex Predicates in 
South Asian Languages. Manohar Publishers and 
Distributors, New Delhi. 
 
45
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 46?54,
Beijing, August 2010
Handling Named Entities and Compound Verbs in             
Phrase-Based Statistical Machine Translation 
Santanu Pal*, Sudip Kumar Naskar?, Pavel Pecina?,  
Sivaji Bandyopadhyay* and Andy Way? 
*Dept. of Comp. Sc. & Engg. 
Jadavpur University 
santanupersonal1@gmail.com, sivaji_cse_ju@yahoo.com 
?CNGL, School of Computing 
Dublin City University 
{snaskar, ppecina, away}@computing.dcu.ie 
 
Abstract 
Data preprocessing plays a crucial role in 
phrase-based statistical machine transla-
tion (PB-SMT). In this paper, we show 
how single-tokenization of two types of 
multi-word expressions (MWE), namely 
named entities (NE) and compound 
verbs, as well as their prior alignment 
can boost the performance of PB-SMT. 
Single-tokenization of compound verbs 
and named entities (NE) provides sig-
nificant gains over the baseline PB-SMT 
system. Automatic alignment of NEs 
substantially improves the overall MT 
performance, and thereby the word 
alignment quality indirectly. For estab-
lishing NE alignments, we transliterate 
source NEs into the target language and 
then compare them with the target NEs. 
Target language NEs are first converted 
into a canonical form before the com-
parison takes place. Our best system 
achieves statistically significant im-
provements (4.59 BLEU points absolute, 
52.5% relative improvement) on an Eng-
lish?Bangla translation task. 
1 Introduction 
Statistical machine translation (SMT) heavily 
relies on good quality word alignment and 
phrase alignment tables comprising translation 
knowledge acquired from a bilingual corpus. 
Multi-word expressions (MWE) are defined 
as ?idiosyncratic interpretations that cross word 
boundaries (or spaces)? (Sag et al, 2002). Tradi-
tional approaches to word alignment following 
IBM Models (Brown et al, 1993) do not work 
well with multi-word expressions, especially 
with NEs, due to their inability to handle many-
to-many alignments. Firstly, they only carry out 
alignment between words and do not consider 
the case of complex expressions, such as multi-
word NEs. Secondly, the IBM Models only al-
low at most one word in the source language to 
correspond to a word in the target language 
(Marcu, 2001, Koehn et al, 2003). 
In another well-known word alignment ap-
proach, Hidden Markov Model (HMM: Vogel et 
al., 1996), the alignment probabilities depend on 
the alignment position of the previous word. It 
does not explicitly consider many-to-many 
alignment either. 
We address this many-to-many alignment 
problem indirectly. Our objective is to see how 
to best handle the MWEs in SMT. In this work, 
two types of MWEs, namely NEs and compound 
verbs, are automatically identified on both sides 
of the parallel corpus. Then, source and target 
language NEs are aligned using a statistical 
transliteration method. We rely on these auto-
matically aligned NEs and treat them as transla-
tion examples. Adding bilingual dictionaries, 
which in effect are instances of atomic transla-
tion pairs, to the parallel corpus is a well-known 
practice in domain adaptation in SMT (Eck et 
al., 2004; Wu et al, 2008). We modify the paral-
lel corpus by converting the MWEs into single 
tokens and adding the aligned NEs in the parallel 
corpus in a bid to improve the word alignment, 
and hence the phrase alignment quality. This 
46
preprocessing results in improved MT quality in 
terms of automatic MT evaluation metrics. 
The remainder of the paper is organized as 
follows. In section 2 we discuss related work. 
The System is described in Section 3.  Section 4 
includes the results obtained, together with some 
analysis. Section 5 concludes, and provides ave-
nues for further work. 
2 Related Work 
Moore (2003) presented an approach for si-
multaneous NE identification and translation. He 
uses capitalization cues for identifying NEs on 
the English side, and then he applies statistical 
techniques to decide which portion of the target 
language corresponds to the specified English 
NE. Feng et al (2004) proposed a Maximum 
Entropy model based approach for English?
Chinese NE alignment which significantly out-
performs IBM Model4 and HMM. They consid-
ered 4 features: translation score, transliteration 
score, source NE and target NE's co-occurrence 
score, and the distortion score for distinguishing 
identical NEs in the same sentence. Huang et al 
(2003) proposed a method for automatically ex-
tracting NE translingual equivalences between 
Chinese and English based on multi-feature cost 
minimization. The costs considered are translit-
eration cost, word-based translation cost, and NE 
tagging cost. 
Venkatapathy and Joshi (2006) reported a dis-
criminative approach of using the compositional-
ity information about verb-based multi-word 
expressions to improve word alignment quality. 
(Ren et al, 2009) presented log likelihood ratio-
based hierarchical reducing algorithm to auto-
matically extract bilingual MWEs, and investi-
gated the usefulness of these bilingual MWEs in 
SMT by integrating bilingual MWEs into Moses 
(Koehn et al, 2007) in three ways. They ob-
served the highest improvement when they used 
an additional feature to represent whether or not 
a bilingual phrase contains bilingual MWEs. 
This approach was generalized in Carpuat and 
Diab (2010). In their work, the binary feature 
was replaced by a count feature representing the 
number of MWEs in the source language phrase. 
Intuitively, MWEs should be both aligned in 
the parallel corpus and translated as a whole. 
However, in the state-of-the-art PB-SMT, it 
could well be the case that constituents of an 
MWE are marked and aligned as parts of con-
secutive phrases, since PB-SMT (or any other 
approaches to SMT) does not generally treat 
MWEs as special tokens. Another problem SMT 
suffers from is that verb phrases are often 
wrongly translated, or even sometimes deleted in 
the output in order to produce a target sentence 
considered good by the language model. More-
over, the words inside verb phrases seldom show 
the tendency of being aligned one-to-one; the 
alignments of the words inside source and target 
verb phrases are mostly many-to-many, particu-
larly so for the English?Bangla language pair. 
These are the motivations behind considering 
NEs and compound verbs for special treatment 
in this work. 
By converting the MWEs into single tokens, 
we make sure that PB-SMT also treats them as a 
whole. The objective of the present work is two-
fold; firstly to see how treatment of NEs and 
compound verbs as a single unit affects the 
overall MT quality, and secondly whether prior 
automatic alignment of these single-tokenized 
MWEs can bring about any further improvement 
on top of that. 
We carried out our experiments on an Eng-
lish?Bangla translation task, a relatively hard 
task with Bangla being a morphologically richer 
language. 
3 System Description 
3.1 PB-SMT 
Translation is modeled in SMT as a decision 
process, in which the translation Ie1 = e1 . . . ei . . 
. eI of a source sentence
Jf1 = f1 . . . fj . . . fJ is 
chosen to maximize (1): 
)().|(maxarg)|(maxarg 111
,
11
, 11
IIJ
eI
JI
eI
ePefPfeP
II
=      (1)  
where )|( 11
IJ efP  and )( 1
IeP  denote respec-
tively the translation model and the target lan-
guage model (Brown et al, 1993). In log-linear 
phrase-based SMT, the posterior probability 
)|( 11
JI feP  is directly modeled as a log-linear 
combination of features (Och and Ney, 2002), 
that usually comprise M translational features, 
and the language model, as in (2): 
 
47
?
=
=
M
m
KIJ
mm
JI sefhfeP
1
11111 ),,()|(log ?  
)(log 1
I
LM eP?+        (2)     
where k
k sss ...11 =  denotes a segmentation of the 
source and target sentences respectively into the 
sequences of phrases )?,...,?( 1 kee  and )?,...,?( 1 kff  
such that (we set i0 = 0) (3): 
,1 Kk ???  sk = (ik, bk, jk), 
          
kk iik
eee ...? 11 +?= , 
         
kk jbk
fff ...? = .          (3) 
and each feature mh?  in (2) can be rewritten as in 
(4): 
?
=
=
K
k
kkkm
KIJ
m sefhsefh
1
111 ),?,?(?),,(                  (4) 
where mh? is a feature that applies to a single 
phrase-pair. It thus follows (5): 
? ??
= ==
=
K
k
K
k
kkkkkkm
M
m
m sefhsefh
1 11
),?,?(?),?,?(??      (5) 
where m
M
m
mhh ??
1
?
=
= ? .            
3.2 Preprocessing of the Parallel Corpus 
The initial English?Bangla parallel corpus is 
cleaned and filtered using a semi-automatic 
process. We employed two kinds of multi-word 
information: compound verbs and NEs. Com-
pound verbs are first identified on both sides of 
the parallel corpus. Chakrabarty et al (2008) 
analyzed and identified a category of V+V com-
plex predicates called lexical compound verbs 
for Hindi. We adapted their strategy for identifi-
cation of compound verbs in Bangla. In addition 
to V+V construction, we also consider N+V and 
ADJ+V structures. 
NEs are also identified on both sides of trans-
lation pairs. NEs in Bangla are much harder to 
identify than in English (Ekbal and Bandyop-
adhyay, 2009). This can be attributed to the fact 
that (i) there is no concept of capitalization in 
Bangla; and (ii) Bangla common nouns are often 
used as proper names. In Bangla, the problem is 
compounded by the fact that suffixes (case 
markers, plural markers, emphasizers, specifiers) 
are also added to proper names, just like to any 
other common nouns. As a consequence, the ac-
curacy of Bangla NE recognizers (NER) is much 
poorer compared to that for English. Once the 
compound verbs and the NEs are identified on 
both sides of the parallel corpus, they are con-
verted into and replaced by single tokens. When 
converting these MWEs into single tokens, we 
replace the spaces with underscores (?_?). Since 
there are already some hyphenated words in the 
corpus, we do not use hyphenation for this pur-
pose; besides, the use of a special word separator 
(underscore in our case) facilitates the job of 
deciding which single-token (target language) 
MWEs to detokenize into words comprising 
them, before evaluation. 
3.3 Transliteration  Using Modified Joint 
Source-Channel Model 
Li et al (2004) proposed a generative framework 
allowing direct orthographical mapping of trans-
literation units through a joint source-channel 
model, which is also called n-gram translitera-
tion model. They modeled the segmentation of 
names into transliteration units (TU) and their 
alignment preferences using maximum likeli-
hood via EM algorithm (Dempster et al, 1977). 
Unlike the noisy-channel model, the joint 
source-channel model tries to capture how 
source and target names can be generated simul-
taneously by means of contextual n-grams of the 
transliteration units. For K aligned TUs, they 
define the bigram model as in (6): 
 )...,,...,(),( 2121 KK bbbeeePBEP =  
  ),...,,,( 21 KbebebeP ><><><=  
   ? ><><= K
=k
k bebeP
1
1-k
1 ),|,(         (6) 
where E refers to the English name and B the 
transliteration in Bengali, while ei and bi refer to 
the ith English and Bangla segment (TU) respec-
tively. 
Ekbal et al (2006) presented a modification to 
the joint source-channel model to incorporate 
different contextual information into the model 
for Indian languages. They used regular expres-
sions and language-specific heuristics based on 
consonant and vowel patterns to segment names 
into TUs. Their modified joint source-channel 
model, for which they obtained improvement 
48
over the original joint source-channel model, 
essentially considers a trigram model for the 
source language and a bigram model for the tar-
get, as in (7). 
 ? +><><= K
=k
kk ebebePBEP
1
11-k ),,|,(),(   (7) 
Ekbal et al (2006) reported a word agreement 
ratio of 67.9% on an English?Bangla translit-
eration task. In the present work, we use the 
modified joint source-channel model of (Ekbal 
et al, 2006) to translate names for establishing 
NE alignments in the parallel corpus. 
3.4 Automatic Alignment of NEs through 
Transliteration 
We first create an NE parallel corpus by extract-
ing the source and target (single token) NEs 
from the NE-tagged parallel translations in 
which both sides contain at least one NE. For 
example, we extract the NE translation pairs 
given in (9) from the sentence pair shown in (8), 
where the NEs are shown as italicized. 
(8a) Kirti_Mandir , where Mahatma_Gandhi 
was born , today houses a photo exhibition on 
the life and times of the Mahatma , a library, a 
prayer hall and other memorabilia . 
(8b) ??????_??n? , ?????? ???t?_??n? ??n????? , 
???????? ?????? ???t?? ???? o ??i ????? 
?????????? e??? ??tp????????? , e??? ??i?b?? o 
e??? p?????? ?? e?? a????? s ????????? ??????t 
??? ? 
(9a) Kirti_Mandir Mahatma_Gandhi Mahatma 
(9b) ??????_??n? ???t?_??n? ???t?? 
Then we try to align the source and target NEs 
extracted from a parallel sentence, as illustrated 
in (9). If both sides contain only one NE then the 
alignment is trivial, and we add such NE pairs to 
seed another parallel NE corpus that contains 
examples having only one token in both side. 
Otherwise, we establish alignments between the 
source and target NEs using transliteration. We 
use the joint source-channel model of translitera-
tion (Ekbal et al, 2006) for this purpose.  
If both the source and target side contains n 
number of NEs, and the alignments of n-1 NEs 
can be established through transliteration or by 
means of already existing alignments, then the 
nth alignment is trivial. However, due to the rela-
tive performance difference of the NERs for the 
source and target language, the number of NEs 
identified on the source and target sides is al-
most always unequal (see Section 4). Accord-
ingly, we always use transliteration to establish 
alignments even when it is assumed to be trivial. 
Similarly, for multi-word NEs, intra-NE word 
alignments are established through translitera-
tion or by means of already existing alignments. 
For a multi-word source NE, if we can align all 
the words inside the NE with words inside a tar-
get NE, then we assume they are translations of 
each other. Due to the relatively poor perform-
ance of the Bangla NER, we also store the im-
mediate left and right neighbouring words for 
every NE in Bangla, just in case the left or the 
right word is a valid part of the NE but is not 
properly tagged by the NER. 
As mentioned earlier, since the source side 
NER is much more reliable than the target side 
NER, we transliterate the English NEs, and try 
to align them with the Bangla NEs. For aligning 
(capitalized) English words to Bangla words, we 
take the 5 best transliterations produced by the 
transliteration system for an English word, and 
compare them against the Bangla words. Bangla 
NEs often differ in their choice of matras (vowel 
modifiers). Thus we first normalize the Bangla 
words, both in the target NEs and the transliter-
ated ones, to a canonical form by dropping the 
matras, and then compare the results. In effect, 
therefore, we just compare the consonant se-
quences of every transliteration candidate with 
that of a target side Bangla word; if they match, 
then we align the English word with the Bangla 
word. 
???? (? + ??+ ? + ?) -- ????? (? + ?? + ? + ?? + ?) 
      (10) 
The example in (10) illustrates the procedure. 
Assume, we are trying to align ?Niraj? with 
???????. The transliteration system produces 
?????? from the English word ?Niraj? and we 
compare ?????? with ???????. Since the conso-
nant sequences match in both words, ?????? is 
considered a spelling variation of ???????, and 
the English word ?Niraj? is aligned to the 
Bangla word ???????. 
In this way, we achieve word-level align-
ments, as well as NE-level alignments. (11) 
shows the alignments established from (8). The 
word-level alignments help to establish new 
49
word / NE alignments. Word and NE alignments 
obtained in this way are added to the parallel 
corpus as additional training data. 
(11a) Kirti-Mandir  ? ??????-??n?  
(11b) Kirti ? ?????? 
(11c) Mandir  ? ??n? 
(11d) Mahatma-Gandhi ? ???t?-??n?  
(11e) Mahatma ? ???t? 
(11f) Gandhi ? ??n? 
(11g) Mahatma ? ???t?? 
3.5 Tools and Resources Used 
A sentence-aligned English?Bangla parallel 
corpus containing 14,187 parallel sentences from 
a travel and tourism domain was used in the pre-
sent work. The corpus was obtained from the 
consortium-mode project ?Development of Eng-
lish to Indian Languages Machine Translation 
(EILMT) System? 1. 
The Stanford Parser2 and the CRF chunker3 
were used for identifying compound verbs in the 
source side of the parallel corpus. The Stanford 
NER4 was used to identify NEs on the source 
side (English) of the parallel corpus. 
The sentences on the target side (Bangla) 
were POS-tagged by using the tools obtained 
from the consortium mode project ?Develop-
ment of Indian Languages to Indian Languages 
Machine Translation (ILILMT) System?. NEs in 
Bangla are identified using the NER system of 
Ekbal and Bandyopadhyay (2008). We use the 
Stanford Parser, Stanford NER and the NER for 
Bangla along with the default model files pro-
vided, i.e., with no additional training. 
The effectiveness of the MWE-aligned paral-
lel corpus developed in the work is demonstrated 
by using the standard log-linear PB-SMT model 
as our baseline system: GIZA++ implementation 
of IBM word alignment model 4, phrase-
extraction heuristics described in (Koehn et al, 
2003), minimum-error-rate training (Och, 2003) 
on a held-out development set, target language 
model with Kneser-Ney smoothing (Kneser and 
                                                 
1 The EILMT and ILILMT projects are funded by the De-
partment of Information Technology (DIT), Ministry of 
Communications and Information Technology (MCIT), 
Government of India. 
2 http://nlp.stanford.edu/software/lex-parser.shtml 
3 http://crfchunker.sourceforge.net/ 
4 http://nlp.stanford.edu/software/CRF-NER.shtml 
Ney, 1995) trained with SRILM (Stolcke, 2002), 
and Moses decoder (Koehn et al, 2007). 
4 Experiments and Results 
We randomly extracted 500 sentences each for 
the development set and testset from the initial 
parallel corpus, and treated the rest as the train-
ing corpus. After filtering on maximum allow-
able sentence length of 100 and sentence length 
ratio of 1:2 (either way), the training corpus con-
tained 13,176 sentences. In addition to the target 
side of the parallel corpus, a monolingual Bangla 
corpus containing 293,207 words from the tour-
ism domain was used for the target language 
model. We experimented with different n-gram 
settings for the language model and the maxi-
mum phrase length, and found that a 4-gram 
language model and a maximum phrase length 
of 4 produced the optimum baseline result. We 
therefore carried out the rest of the experiments 
using these settings. 
English Bangla In training set 
T U T U 
Compound verbs 4,874 2,289 14,174 7,154
Single-word NEs 4,720 1,101 5,068 1,175
2-word NEs 4,330 2,961 4,147 3,417
>2 word NEs 1,555 1,271 1,390 1,278
Total NEs 10,605 5,333 10,605 5,870
Total NE words 22,931 8,273 17,107 9,106
Table 1.  MWE statistics (T - Total occur-
rence, U ? Unique). 
Of the 13,676 sentences in the training and 
development set, 13,675 sentences had at least 
one NE on both sides, only 22 sentences had 
equal number of NEs on both sides, and 13,654 
sentences had an unequal number of NEs. Simi-
larly, for the testset, all the sentences had at least 
one NE on both sides, and none had an equal 
number of NEs on both sides. It gives an indica-
tion of the relative performance differences of 
the NERs. 6.6% and 6.58% of the source tokens 
belong to NEs in the training and testset respec-
tively. These statistics reveal the high degree of 
NEs in the tourism domain data that demands 
special treatment. Of the 225 unique NEs ap-
pearing on the source side of the testset, only 65 
NEs are found in the training set.  
50
Experiments Exp BLEU METEOR NIST WER PER TER 
Baseline 1 8.74 20.39 3.98 77.89 62.95 74.60
NEs of any length as Single 
Token (New-MWNEaST) 
2 9.15 18.19 3.88 77.81 63.85 74.61
NEs of length >2 as  
Single Tokens (MWNE-
aST) 
3 8.76 18.78 3.86 78.31 63.78 75.15
 
 
NEs as Single  
Tokens  
(NEaST) 
2-Word NEs as Single To-
kens (2WNEaST) 
4 9.13 17.28 3.92 78.12 63.15 74.85
Compound Verbs as  Single Tokens 
(CVaST) ? 
5 9.56 15.35 3.96 77.60 63.06 74.46
Alignment of NEs of any 
length (New-MWNEA) ? 
6 13.33 24.06 4.44 74.79 60.10 71.25
Alignment of NEs of length 
upto 2 (New-2WNEA) ? 
7 10.35 20.93 4.11 76.49 62.20 73.05
Alignment of NEs of length 
>2 (MWNEA) ? 
8 12.39 23.13 4.36 75.51 60.58 72.06
 
 
 
 
NE Alignment 
(NEA) 
Alignment of NEs of length 
2 (2WNEA) ? 
9 11.2 23.14 4.26 76.13 60.72 72.57
New-MWNEaST 10 8.62 16.64 3.73 78.41 65.21 75.47
MWNEaST 11 8.74 14.68 3.84 78.40 64.05 75.40
 
CVaST 
+NEaST 2WNEaST 12 8.85 16.60 3.86 78.17 63.90 75.33
New-MWNEA? 13 11.22 21.02 4.16 75.99 61.96 73.06
New-2WNEA? 14 10.07 17.67 3.98 77.08 63.35 74.18
MWNEA? 15 10.34 16.34 4.07 77.12 62.38 73.88
 
CVaST +NEA 
2WNEA? 16 10.51 18.92 4.08 76.77 62.28 73.56
Table 2.  Evaluation results for different experimental setups (The ??? marked systems produce 
statistically significant improvements on BLEU over the baseline system).
Table 1 shows the MWE statistics of the 
parallel corpus as identified by the NERs. The 
average NE length in the training corpus is 
2.16 for English and 1.61 for Bangla. As can 
be seen from Table 1, 44.5% and 47.8% of the 
NEs are single-word NEs in English and 
Bangla respectively, which suggests that prior 
alignment of the single-word NEs, in addition 
to multi-word NE alignment, should also be 
beneficial to word and phrase alignment. 
Of all the NEs in the training and develop-
ment sets, the transliteration-based alignment 
process was able to establish alignments of 
4,711 single-word NEs, 4,669 two-word NEs 
and 1,745 NEs having length more than two. 
It is to be noted that, some of the single-word 
NE alignments, as well as two-word NE 
alignments, result from multi-word NE align-
ment. 
We analyzed the output of the NE align-
ment module and observed that longer NEs 
were aligned better than the shorter ones, 
which is quite intuitive, as longer NEs have 
more tokens to be considered for intra-NE 
alignment. Since the NE alignment process is 
based on transliteration, the alignment method 
does not work where NEs involve translation 
or acronyms. We also observed that English 
multi-word NEs are sometimes fused together 
into single-word NEs. 
We performed three sets of experiments: 
treating compound verbs as single tokens, 
treating NEs as single tokens, and the combi-
nation thereof. Again for NEs, we carried out 
three types of preprocessing: single-
tokenization of (i) two-word NEs, (ii) more 
than two-word NEs, and (iii) NEs of any 
length. We make distinctions among these 
three to see their relative effects. The devel-
opment and test sets, as well as the target lan-
guage monolingual corpus (for language mod-
eling), are also subjected to the same preproc-
essing of single-tokenizing the MWEs. For 
NE alignment, we performed experiments us-
ing 4 different settings: alignment of (i) NEs 
of length up to two, (ii) NEs of length two, 
51
(iii) NEs of length greater than two, and (iv) 
NEs of any length. Before evaluation, the sin-
gle-token (target language) underscored 
MWEs are expanded back to words compris-
ing the MWEs. 
Since we did not have the gold-standard 
word alignment, we could not perform intrin-
sic evaluation of the word alignment. Instead 
we carry out extrinsic evaluation on the MT 
quality using the well known automatic MT 
evaluation metrics: BLEU (Papineni et al, 
2002), METEOR (Banerjee and Lavie, 2005), 
NIST (Doddington, 2002), WER, PER and 
TER (Snover et al, 2006). As can be seen 
from the evaluation results reported in Table 
2, baseline Moses without any preprocessing 
of the dataset produces a BLEU score of 8.74. 
The low score can be attributed to the fact that 
Bangla, a morphologically rich language, is 
hard to translate into. Moreover, Bangla being 
a relatively free phrase order language (Ekbal 
and Bandyopadhyay, 2009) ideally requires 
multiple set of references for proper evalua-
tion. Hence using a single reference set does 
not justify evaluating translations in Bangla. 
Also the training set was not sufficiently large 
enough for SMT. Treating only longer than 2-
word NEs as single tokens does not help im-
prove the overall performance much, while 
single tokenization  of two-word NEs as single 
tokens produces some improvements (.39 
BLEU points absolute, 4.5% relative). Con-
sidering compound verbs as single tokens 
(CVaST) produces a .82 BLEU point im-
provement (9.4% relative) over the baseline. 
Strangely, when both compound verbs and 
NEs together are counted as single tokens, 
there is hardly any improvement. By contrast, 
automatic NE alignment  (NEA) gives a huge 
impetus to system performance, the best of 
them (4.59 BLEU points absolute, 52.5% rela-
tive improvement) being the alignment of NEs 
of any length that produces the best scores 
across all metrics. When NEA is combined 
with CVaST, the improvements are substan-
tial, but it can not beat the individual im-
provement on NEA. The (?) marked systems 
produce statistically significant improvements 
as measured by bootstrap resampling method 
(Koehn, 2004) on BLEU over the baseline 
system. Metric-wise individual best scores are 
shown in bold in Table 2. 
5 Conclusions and Future Work 
In this paper, we have successfully shown 
how the simple yet effective preprocessing of 
treating two types of MWEs, namely NEs and 
compound verbs, as single-tokens, in conjunc-
tion with prior NE alignment can boost the 
performance of PB-SMT system on an Eng-
lish?Bangla translation task. Treating com-
pound verbs as single-tokens provides signifi-
cant gains over the baseline PB-SMT system. 
Amongst the MWEs, NEs perhaps play the 
most important role in MT, as we have clearly 
demonstrated through experiments that auto-
matic alignment of NEs by means of translit-
eration improves the overall MT performance 
substantially across all automatic MT evalua-
tion metrics. Our best system yields 4.59 
BLEU points improvement over the baseline, 
a 52.5% relative increase. We compared a 
subset of the output of our best system with 
that of the baseline system, and the output of 
our best system almost always looks better in 
terms of either lexical choice or word order-
ing. The fact that only 28.5% of the testset 
NEs appear in the training set, yet prior auto-
matic alignment of the NEs brings about so 
much improvement in terms of MT quality, 
suggests that it not only improves the NE 
alignment quality in the phrase table, but word 
alignment and phrase alignment quality must 
have also been improved significantly. At the 
same time, single-tokenization of MWEs 
makes the dataset sparser, but yet improves 
the quality of MT output to some extent. Data-
driven approaches to MT, specifically for 
scarce-resource language pairs for which very 
little parallel texts are available, should benefit 
from these preprocessing methods. Data 
sparseness is perhaps the reason why single-
tokenization of NEs and compound verbs, 
both individually and in collaboration, did not 
add significantly to the scores. However, a 
significantly large parallel corpus can take 
care of the data sparseness problem introduced 
by the single-tokenization of MWEs. 
The present work offers several avenues for 
further work. In future, we will investigate 
how these automatically aligned NEs can be 
52
used as anchor words to directly influence the 
word alignment process. We will look into 
whether similar kinds of improvements can be 
achieved for larger datasets, corpora from dif-
ferent domains and for other language pairs. 
We will also investigate how NE alignment 
quality can be improved, especially where 
NEs involve translation and acronyms. We 
will also try to perform morphological analy-
sis or stemming on the Bangla side before NE 
alignment. We will also explore whether dis-
criminative approaches to word alignment can 
be employed to improve the precision of the 
NE alignment. 
Acknowledgements 
This research is partially supported by the Sci-
ence Foundation Ireland (Grant 07/CE/I1142) 
as part of the Centre for Next Generation Lo-
calisation (www.cngl.ie) at Dublin City Uni-
versity, and EU projects PANACEA (Grant 
7FP-ITC-248064) and META-NET (Grant 
FP7-ICT-249119). 
References 
Banerjee, Satanjeev, and Alon Lavie. 2005. An 
Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In 
proceedings of the ACL-2005 Workshop on In-
trinsic and Extrinsic Evaluation Measures for 
MT and/or Summarization, pp. 65-72. Ann Ar-
bor, Michigan., pp. 65-72. 
Brown, Peter F., Stephen A. Della Pietra, Vincent 
J. Della Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: 
parameter estimation. Computational Linguis-
tics, 19(2):263-311. 
Carpuat, Marine, and Mona Diab. 2010. Task-
based Evaluation of Multiword Expressions: a 
Pilot Study in Statistical Machine Translation. 
In Proceedings of Human Language Technology 
conference and the North American Chapter of 
the Association for Computational Linguistics 
conference (HLT-NAACL 2010), Los Angeles, 
CA, pp. 242-245. 
Chakrabarti, Debasri, Hemang Mandalia, Ritwik 
Priya, Vaijayanthi Sarma, and Pushpak Bhat-
tacharyya. 2008. Hindi compound verbs and 
their automatic extraction. In Proceedings 
of  the 22nd International Conference on Com-
putational Linguistics (Coling 2008), Posters 
and demonstrations, Manchester, UK, pp. 27-
30. 
Dempster, A.P., N.M. Laird, and D.B. Rubin. 
1977). Maximum Likelihood from Incomplete 
Data via the EM Algorithm. Journal of the 
Royal Statistical Society, Series B (Methodo-
logical) 39 (1): 1?38. 
Doddington, George. 2002. Automatic evaluation 
of machine translation quality using n-gram 
cooccurrence statistics. In Proceedings of the 
Second International Conference on Human 
Language Technology Research (HLT-2002), 
San Diego, CA, pp. 128-132. 
Eck, Matthias, Stephan Vogel, and Alex Waibel. 
2004. Improving statistical machine translation 
in the medical domain using the Unified Medi-
cal Language System. In Proceedings of  the 
20th International Conference on Computational 
Linguistics (COLING 2004), Ge-
neva, Switzerland, pp. 792-798. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2009. 
Voted NER system using appropriate unlabeled 
data. In proceedings of the ACL-IJCNLP-2009 
Named Entities Workshop (NEWS 2009), 
Suntec, Singapore, pp. 202-210. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2008. 
Maximum Entropy Approach for Named Entity 
Recognition in Indian Languages. International 
Journal for Computer Processing of Lan-
guages (IJCPOL), Vol. 21(3):205-237. 
Feng, Donghui, Yajuan Lv, and Ming Zhou. 2004. 
A new approach for English-Chinese named en-
tity alignment. In Proceedings of the 2004 Con-
ference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-2004), Barcelona, 
Spain, pp. 372-379. 
Huang, Fei, Stephan Vogel, and Alex Waibel. 
2003. Automatic extraction of named entity 
translingual equivalence based on multi-feature 
cost minimization. In Proceedings of the ACL-
2003 Workshop on Multilingual and Mixed-
language Named Entity Recognition, 2003, 
Sapporo, Japan, pp. 9-16. 
Kneser, Reinhard, and Hermann Ney. 1995. Im-
proved backing-off for m-gram language model-
ing. In Proceedings of the IEEE Internation 
Conference on Acoustics, Speech, and Signal 
Processing (ICASSP), vol. 1, pp. 181-184. De-
troit, MI. 
Koehn, Philipp, Franz Josef Och, and Daniel 
Marcu. 2003. Statistical phrase-based transla-
tion. In Proceedings of HLT-NAACL 2003: 
53
conference combining Human Language Tech-
nology conference series and the North Ameri-
can Chapter of the Association for Computa-
tional Linguistics conference series,  Edmonton, 
Canada, pp. 48-54. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, 
Chris Callison-Burch, Marcello Federico, Ni-
cola Bertoldi, Brooke Cowan, Wade Shen, 
Christine Moran, Richard Zens, Chris Dyer, 
Ond?ej Bojar, Alexandra Constantin, and Evan 
Herbst. 2007. Moses: open source toolkit for 
statistical machine translation. In Proceedings of 
the 45th Annual meeting of the Association for 
Computational Linguistics (ACL 2007): Pro-
ceedings of demo and poster sessions, Prague, 
Czech Republic, pp. 177-180. 
Koehn, Philipp. 2004. Statistical significance tests 
for machine translation evaluation. In  EMNLP-
2004: Proceedings of the 2004 Conference on 
Empirical Methods in Natural Language Proc-
essing, 25-26 July 2004, Barcelona, Spain, pp. 
388-395. 
Marcu, Daniel. 2001. Towards a Unified Approach 
to Memory- and Statistical-Based Machine 
Translation. In Proceedings of the 39th Annual 
Meeting of the Association for Computational 
Linguistics (ACL 2001), Toulouse, France, pp. 
386-393. 
Moore, Robert C. 2003. Learning translations of 
named-entity phrases from parallel corpora. In 
Proceedings of 10th Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics (EACL 2003), Budapest, 
Hungary; pp. 259-266. 
Och, Franz J. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), Sap-
poro, Japan, pp. 160-167. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for 
automatic evaluation of machine translation. In 
Proceedings of the 40th Annual Meeting of the 
Association for Computational Linguistics 
(ACL-2002), Philadelphia, PA, pp. 311-318. 
Ren, Zhixiang, Yajuan L?, Jie Cao, Qun Liu, and 
Yun Huang. 2009. Improving statistical ma-
chine translation using domain bilingual multi-
word expressions. In Proceedings of the 2009 
Workshop on Multiword Expressions, ACL-
IJCNLP 2009, Suntec, Singapore, pp. 47-54. 
Sag, Ivan A., Timothy Baldwin, Francis Bond, 
Ann Copestake and Dan Flickinger. 2002. Mul-
tiword expressions: A pain in the neck for NLP. 
In Proceedings of the 3rd International Confer-
ence on Intelligent Text Processing and Compu-
tational Linguistics (CICLing-2002), Mexico 
City, Mexico, pp. 1-15. 
Snover, Matthew, Bonnie Dorr, Richard Schwartz, 
Linnea Micciulla, and John Makhoul. 2006. A 
study of translation edit rate with targeted hu-
man annotation. In Proceedings of the 7th Con-
ference of the Association for Machine Transla-
tion in the Americas (AMTA 2006), Cambridge, 
MA, pp. 223-231. 
Vogel, Stephan, Hermann Ney, and Christoph 
Tillmann. 1996. HMM-based word alignment in 
statistical translation. In Proceedings of the 16th 
International Conference on Computational 
Linguistics (COLING 1996), Copenhagen, pp. 
836-841. 
Venkatapathy, Sriram, and Aravind K. Joshi. 2006. 
Using information about multi-word expres-
sions for the word-alignment task. In Proceed-
ings of Coling-ACL 2006: Workshop on Multi-
word Expressions: Identifying and Exploiting 
Underlying Properties, Sydney, pp. 20-27. 
Wu, Hua Haifeng Wang, and Chengqing Zong. 
2008. Domain adaptation for statistical machine 
translation with domain dictionary and mono-
lingual corpora. In Proceedings of the 22nd In-
ternational Conference on Computational Lin-
guistics (COLING 2008),  Manchester, UK, pp. 
993-1000. 
54
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 73?76,
Beijing, August 2010
Identification  of  Reduplication  in  Bengali  Corpus and their 
Semantic Analysis: A Rule-Based Approach 
Tanmoy Chakraborty 
Department of Computer Science and  
Engineering 
Jadavpur University 
its_tanmoy@yahoo.co.in 
Sivaji Bandyopadhyay 
Department of Computer Science and 
Engineering 
Jadavpur University 
sivaji_cse_ju@yahoo.co.in 
 
Abstract 
In linguistic studies, reduplication gener-
ally means the repetition of any linguis-
tic unit such as a phoneme, morpheme, 
word, phrase, clause or the utterance as a 
whole. The identification of reduplica-
tion is a part of general task of identifica-
tion of multiword expressions (MWE). 
In the present work, reduplications have 
been identified from the Bengali corpus 
of the articles of Rabindranath Tagore. 
The present rule-based approach is di-
vided into two phases. In the first phase, 
identification of reduplications has been 
done mainly at general expression level 
and in the second phase, their structural 
and semantics classifications are ana-
lyzed. The system has been evaluated 
with average Precision, Recall and F-
Score values of 92.82%, 91.50% and 
92.15% respectively. 
1 Introduction 
In all languages, the repetition of noun, pronoun, 
adjective and verb are broadly classified under 
two coarse-grained categories: repetition at the 
(a) expression level, and at the (b) contents or 
semantic level. The repetition at both the levels 
is mainly used for emphasis, generality, intensity 
or to show continuation of an act.  The paper 
deals with the identification of reduplications at 
both levels in Bengali. Reduplication phenome-
non is not an exotic feature of Indian Languages. 
For instance, Yiddish English has duplication of 
the form X schm-X, as in "duplication schmu-
plication". Semantic duplication is also rich in 
English and Onomatopoeic repetition is not un-
common either (e.g., ha-ha, blah-blah etc).  
Reduplication carries various semantic mean-
ings and sometime helps to identify the mental 
state of the speaker as well. Some correlative 
words are used in Bengali to express the posses-
siveness, relative or descriptiveness. They are 
called ?secondary descriptive compounds?.  
The related studies on MWEs are discussed in 
Section 2. Various types of reduplications in 
Bengali and their semantic interpretations are 
discussed in Section 3. The proposed system 
architecture and the procedures are discussed in 
Section 4. The evaluation metrics used for eva-
luating the system are discussed in Section 5.  
Experimental results are presented in Section 6 
and conclusions are drawn in Section 7.  
2 Related Work 
The works on MWE identification and extrac-
tion have been continuing in English (Fillmore, 
2003; Sag et. al, 2002). After tokenization, mul-
tiword expressions are important in understand-
ing the meaning in applications like Machine 
Translation, Information Retrieval system etc. 
Some of the MWE extraction tasks in English 
can be seen in (Diab and Bhutada, 2009; Enivre 
and Nilson, 2004). Among Indian languages, 
Hindi compound noun MWE extraction has been 
studied in (Kunchukuttan and Damani, 2008). 
Manipuri reduplicated MWE identification is 
discussed in (Nongmeikapam and Bandyop-
adhyay, 2010). There are no published works on 
reduplicated MWE identification in Bengali.  
3 Reduplication of Words in Bengali 
Identification of MWEs is done during the to-
kenization phase and is absolutely necessary 
73
during POS tagging as is outlined in (Thoudam 
and Bandyopadhyay, 2008). POS tagger identi-
fies MWE as unknown word at token level. 
Bengali Shallow Parser 1  can only identify hy-
phened reduplication and gives them separate 
tags like RDP (reduplication) or ECH (echo).  
Another objective for identifying reduplicated 
MWEs is to extract correct sense of reduplicated 
MWEs as discussed in Section 3.2. Sometime, 
reduplication is used for sentiment marking to 
identify whether the speaker uses it in positive or 
negative sense. For example,  
(i) Eto Bara Bara Asha Kisher?(Why are you 
thinking so high?)  (Positive Sense) 
(ii) Ki Bara Bara Bari Ekhane! (Here, the 
buildings are very large.) (Negative Sense)  
3.1 Expression Level Classification of Redu-
plication 
Four classes of reduplications commonly occur 
in the Indian language (Bengali, Hindi, Tamil2, 
Manipuri etc.). In Bengali, another type called 
correlated word is also classified as reduplica-
tion.  
Onomatopoeic expressions: Such words rep-
resent an imitation of a particular sound or imita-
tion of an action along with the sound, etc. For 
example, khat khat, (knock knock). 
Complete Reduplication: The individual 
words carry certain meaning, and they are re-
peated. e.g. bara-bara (big big), dheere dheere, 
slowly). In some cases, both the speaker and the 
listener repeat certain clauses or phrases in long 
utterances or narrations. The repetition of such 
utterances breaks the monotony of the narration, 
allows a pause for the listener to comprehend the 
situation, and also provides an opportunity to the 
speaker to change the style of narration. 
Partial Reduplication: Only one of the 
words is meaningful, while the second word is 
constructed by partially reduplicating the first 
word. Most common type in Bengali is one 
where the first letter or the associated matra or 
both is changed, e.g. thakur-thukur (God), 
boka-soka ( Foolish) etc.  
Semantic Reduplication: The most common 
forms of semantic relations between paired 
words are synonym (matha-mundu, head), an-
                                                 
1 http://ltrc.iiit.ac.in/analyzer/bengali 
2 http://users.ox.ac.uk/~sjoh0535/thesis.html 
tonym (din-rat, day and night), class representa-
tive (cha-paani, snacks)). 
Correlative Reduplication: To express a 
sense of exchange or barter or interchange, the 
style of corresponding correlative words is used 
just preceding the main root verb. For example, 
maramari( fighting).  
3.2 Reduplication at the Sense Level 
Different types of reduplication at the sense 
level are described below: 
i. Sense of repetition:  
       Bachar Bachar Ek Kaj Kara . 
       ( Do the same job every year.)  
ii. Sense of plurality: 
 Ki Bara Bara Bari Ekhane. 
 (Here, the houses are very large.) 
iii. Sense of Emphatic or Modifying Meaning:   
         Lala-Lala phul. (Deep red rose) 
iv. Sense of completion:  
        Kheye Deye Ami Shute Jaba. 
         After eating, I shall go to sleep. 
v. Sense of hesitation or softness:  
       Eta Hasi Hasi Mukh Kena? 
 Why does your face smiling? 
vi. Sense of incompleteness of the verbs: 
  Katha Bolte Bolte Hatat Se Chup Kore Gelo. 
    Talking about something, suddenly he      
stopped. 
vii. Sense of corresponding correlative words: 
     Nijera  Maramari Kara  Na.  
 Don?t fight among yourselves. 
viii.  Sense of Onomatopoeia:  
     Shyamal Darja Khata khata Karchhe .  
 Shyamal is knocking at the door. 
4 System Design 
The system is designed in two phases. The first 
phase identifies mainly five cases of reduplica-
tion discussed in Section 3.1 and the second 
phase attempts to extract the associated sense or 
semantics discussed in Section 3.2.     
4.1 Identifying Reduplications 
 Reduplication is considered as two consecutive 
words W1 and W2. For complete reduplica-
tion, after removing matra, comparison for com-
plete equality of two words is checked. 
74
  In partial reduplication, three cases are pos-
sible- (i) change of the first vowel or the matra 
attached with first consonant, (ii) change of con-
sonant itself in first position or (iii) change of 
both matra and consonant. Exception is reported 
where vowel in first position is changed to con-
sonant and its corresponding matra is added. For 
example, ???-???? (abal-tabal, incoherent or 
irrelevant). Linguistic study (Chattopadhyay, 
1992) reveals that the consonants that can be 
produced after changing are ???, ???, ???, ???. 
For onomatopoeic expression, mainly words 
are repeated twice and may be with some matra 
(mainly ?e?-matra is added with the first word to 
make second word). In this case, after removing 
inflection, words are divided equally and then 
the comparison is done.  
For correlative reduplication, the formative 
affixes ???? and ?-i? are added with the root to 
form w1 and w2 respectively and agglutinated 
together to make a single word. 
For semantic reduplication, a dictionary 
based approach has been taken. List of inflec-
tions identified for the semantic reduplication is 
shown in Table 1.  
 
Set of identified inflections and matra 
0(?? ? ), e(-? , -?), -? (-e??), -? , ? (-e??), -
?, -e?(? ?), e??, -? ?,  -??, -?, -?? ?? , -o, -i, 
 
Table 1. Inflections identified for semantic redu-
plication. 
This system has identified those consecutive 
words having same part-of-speech. Then, mor-
phological analysis has been done to identify the 
roots of both components. In synonymous redu-
plication, w2 is the synonym of w1. So, at first 
in Bengali monolingual dictionary, the entry of 
w1 is searched to have any existence of w2. For 
antonym words, they are mainly gradable oppo-
site (pap-purna, Vice and Virtue) where the 
word and its antonyms are entirely different 
word forms. The productive opposites (garraji, 
disagree is the opposite of raji, agree) are easy 
to identify because the opposite word is gener-
ated by adding some fixed number of prefixes or 
suffixes with the original. In dictionary based 
approach, English meaning of both w1 and w2 
are extracted and opposite of w1 is searched in 
English WordNet3 for any entry of w2. The first 
model for identifying the five types of reduplica-
tions is shown in Figure 1.  
 
   
Figure 1. System Architecture of first phase. 
4.2 Semantics (Sense) Analysis 
Mainly eight types of semantic classifications 
are identified in Section 3.2. If the reduplication 
is an onomatopoeic expression, its sense is easily 
identified as the sense of onomatopoeia. When 
infinite verb with complete reduplication is iden-
tified in a sentence, it obviously expresses the 
sense of incompleteness. The semantic or partial 
reduplicated words belong to the sense of com-
pletion. The correlative word is classified as the 
sense of corresponding correlative word because 
it is generally associated with the full verb in the 
sentence. The problem arises when grouping the 
complete reduplication. Sometime they are used 
as sense of repetition, plurality and sometime 
they express some kind of hesitation, incom-
pleteness or softness. Sense disambiguation for 
this case has been identified as a future work.  
5 Evaluation Metrics 
The corpus is collected from some selected arti-
cles of Rabindranath Tagore4. Standard IR met-
rics like Precision, Recall and F-score are used 
to evaluate the system. Total number of relevant 
                                                 
3 http://wordnetweb.princeton.edu/perl/webwn 
4 http://www.rabindra-rachanabali.nltr.org 
75
reduplication is identified manually. For each 
type of expression level classification, Precision, 
Recall and F-score are calculated separately. The 
overall system score is the average of these 
scores. Statistical co-occurrence measures like 
frequency, hyphen and closed form count are 
calculated on each of the types as an evidence of 
their MWEhood.  
6 Experimental Results 
The collected corpus includes 14,810 tokens for 
3675 distinct word forms at the root level.  Pre-
cision, Recall, F-score are calculated for each 
class as well as for the reduplication identifica-
tion system and are shown in Table 2. 
Reduplications Precision Recall F-
Score 
Onomatopoeic 99.85 99.77 99.79 
Complete 99.98 99.92 99.95 
Partial 79.15 75.80 77.44 
Semantic 85.20 82.26 83.71 
Correlative 99.91 99.73 99.82 
System 92.82 91.50 92.15 
 
Table 2. Evaluation results for various reduplica-
tions (in %). 
The scores of partial and semantic evaluation 
are not satisfactory because of some wrong tag-
ging by the shallow parser (adjective, adverb and 
noun are mainly interchanged). Some synony-
mous reduplication (????-?? ??, dhire-susthe, 
slowly and steadily, leisurely) implies some 
sense of the previous word but not its exact 
synonym. These words are not identified prop-
erly.  Figure 2 shows that the use of complete 
reduplication is more in this corpus. In this cor-
pus, only 8.52% reduplications are hyphened. It 
shows that the trend of writing reduplications is 
to use the space as separator. Also the percent-
age of closed reduplications is 33.09% where 
maximum of them are onomatopoeic, correlative 
and semantic reduplications. 100% of correlative 
reduplications are closed. 
7 Conclusion 
The reduplication phenomenon has been studied 
for Bengali at the expression as well as at the 
semantic levels. The semantics of the redupli-
cated words indicate some sort of sense disam-
biguation that cannot be handled by only rule-
based approach. More works need to be done for 
identifying semantic reduplication using statisti-
cal and morphological approaches. 
 
8.51
51.06
26.6
12.7
18.08 Onomatopoeic
Complete
Partial 
Semantic
Correlative   
  Figure 2. Frequencies (in %) of different redu-
plications. 
References 
Bhaskararao, Peri. 1977. Reduplication and Ono-
matopoeia in Telugu. Deccan College Post-
Graduate and research Institute, Pune, India. 
Chattopadhyay Suniti Kumar. 1992. Bhasa-Prakash 
Bangala Vyakaran, Third Edition.  
Diab, Mona and Pravin Bhutada. 2009. Verb Noun 
Construction MWE Token Supervised Classifica-
tion, In Proceedings of the Joint conference of As-
sociation for Computational Linguistics and  In-
ternational Joint Conference on Natural Language 
Processing, Workshop on Multiword Expression., 
Singapore,   pp.17-22. 
Enivre, Joakim and Jens Nilson. 2004. Multiword 
Units in Syntactic Parsing. In Proceedings of Me-
thodologies and Evaluation of Multiword Units in 
Real-World Applications, 2004 Workshop, Lisbon, 
pp. 39-46. 
Kunchukuttan, Anoop and Om Prakash Damani, 
2008. A System for Compound Noun Multiword 
Expression Extraction for Hindi. 6th International. 
Conference on Natural Language Processing, 
Pune, pp. 20-29. 
Nongmeikapam, Kishorjit and Sivaji Bandyopadhyay. 
2010.  Identification of Reduplication MWEs in 
Manipuri, a rule-based approach, In Proceedings 
of the 23rd International Conference on the Com-
puter Processing of Oriental Languages, Califor-
nia, USA, pp. 49-54. 
Thoudam, Doren Singh and Sivaji Bandyopadhyay. 
2008. Morphology Driven Manipuri POS Tagger. 
In workshop on NLP for Less Privileged Lan-
guages, International Joint conference of Natural 
Language Processing, Hyderabad, pp. 91-98 
76
Proceedings of SSST-4, Fourth Workshop on Syntax and Structure in Statistical Translation, pages 83?91,
COLING 2010, Beijing, August 2010.
Manipuri-English Bidirectional Statistical Machine  
Translation Systems using Morphology and Dependency Relations 
Thoudam Doren Singh 
Department of Computer Science and  
Engineering 
Jadavpur University  
thoudam.doren@gmail.com 
Sivaji Bandyopadhyay 
Department of Computer Science and  
Engineering 
Jadavpur University 
sivaji_cse_ju@yahoo.com 
 
 
 
Abstract 
The present work reports the develop-
ment of Manipuri-English bidirectional 
statistical machine translation systems. In 
the English-Manipuri statistical machine 
translation system, the role of the suffixes 
and dependency relations on the source 
side and case markers on the target side 
are identified as important translation 
factors. A parallel corpus of 10350 sen-
tences from news domain is used for 
training and the system is tested with 500 
sentences. Using the proposed translation 
factors, the output of the translation qual-
ity is improved as indicated by baseline 
BLEU score of 13.045 and factored 
BLEU score of 16.873 respectively. Si-
milarly, for the Manipuri English system, 
the role of case markers and POS tags in-
formation at the source side and suffixes 
and dependency relations at the target 
side are identified as useful translation 
factors. The case markers and suffixes 
are not only responsible to determine the 
word classes but also to determine the 
dependency relations. Using these trans-
lation factors, the output of the transla-
tion quality is improved as indicated by 
baseline BLEU score of 13.452 and fac-
tored BLEU score of 17.573 respectively. 
Further, the subjective evaluation indi-
cates the improvement in the fluency and 
adequacy of both the factored SMT out-
puts over the respective baseline systems. 
 
1 Introduction 
Manipuri has little resource for NLP related re-
search and development activities. Manipuri is a 
less privileged Tibeto-Burman language spoken 
by approximately three million people mainly in 
the state of Manipur in India as well as its neigh-
boring states and in the countries of Myanmar 
and Bangladesh. Some of the unique features of 
this language are tone, the agglutinative verb 
morphology and predominance of aspect than 
tense, lack of grammatical gender, number and 
person. Other features are verb final word order 
in a sentence i.e., Subject Object Verb (SOV) 
order, extensive suffix with more limited prefixa-
tion. In Manipuri, identification of most of the 
word classes and sentence types are based on the 
markers. All sentences, except interrogatives end 
with one of these mood markers, which may or 
may not be followed by an enclitic. Basic sen-
tence types in Manipuri are determined through 
illocutionary mood markers, all of which are 
verbal inflectional suffixes, with the exception of 
the interrogatives that end with an enclitic. Two 
important problems in applying statistical ma-
chine translation (SMT) techniques to English-
Manipuri bidirectional MT systems are: (a) the 
wide syntactic divergence between the language 
pairs, and (b) the richer morphology and case 
marking of Manipuri compared to English. The 
first problem manifests itself in poor word-order 
in the output translations, while the second one 
leads to incorrect inflections and case marking. 
The output Manipuri sentences in case of Eng-
lish-Manipuri system suffer badly when mor-
phology and case markers are incorrect in this 
free word order and morphologically rich lan-
guage. 
83
The parallel corpora used is in news domain 
which have been collected, cleaned and aligned 
(Singh et al , 2010b) from the Sangai Express 
newspaper website www.thesangaiexpress.com 
available in both Manipuri and English. A daily 
basis collection was done covering the period 
from May 2008 to November 2008 since there is 
no repository. 
2 Related Works  
Koehn and Hoang (2007) developed a frame-
work for statistical translation models that tightly 
integrates additional morphological, syntactic, or 
semantic information. Statistical Machine Trans-
lation with scarce resources using morpho-
syntactic information is discussed in (Nie?en and 
Ney, 2004). It introduces sentence level restruc-
turing transformations that aim at the assimila-
tion of word order in related sentences and 
exploitation of the bilingual training data by ex-
plicitly taking into account the interdependencies 
of related inflected forms thereby improving the 
translation quality. Popovic and Ney (2006) dis-
cussed SMT with a small amount of bilingual 
training data. Case markers and morphology are 
used to address the crux of fluency in the Eng-
lish-Hindi SMT system (Ramanathan et al, 
2009). Work on translating from rich to poor 
morphology using factored model is reported in 
(Avramidis and Koehn, 2008). In this method of 
enriching input, the case agreement for nouns, 
adjectives and articles are mainly defined by the 
syntactic role of each phrase. Resolution of verb 
conjugation is done by identifying the person of 
a verb and using the linguistic information tag. 
Manipuri to English Example Based Machine 
Translation system is reported in (Singh and 
Bandyopadhyay, 2010a) on news domain. For 
this, POS tagging, morphological analysis, NER 
and chunking are applied on the parallel corpus 
for phrase level alignment. Chunks are aligned 
using a dynamic programming ?edit-distance 
style? alignment algorithm. The translation 
process initially looks for an exact match in the 
parallel example base and returns the retrieved 
target output. Otherwise, the maximal match 
source sentence is identified. For word level 
mismatch, the unmatched words in the input are 
either translated from the lexicon or translite-
rated. Unmatched phrases are looked into the 
phrase level parallel example base; the target 
phrase translations are identified and then re-
combined with the retrieved output. English-
Manipuri SMT system using morpho-syntactic 
and semantic information is reported in (Singh 
and Bandyopadhyay, 2010c). In this system, the 
role of the suffixes and dependency relations on 
the source side and case markers on the target 
side are identified as important translation fac-
tors. 
3 Syntactic Reordering 
This is a preprocessing step applied to the in-
put English sentences for English-Manipuri SMT 
system. The program for syntactic reordering 
uses the parse trees generated by Stanford parser1 
and applies a handful of reordering rules written 
using perl module Parse::RecDescent. By doing 
this, the SVO order of English is changed to 
SOV order for Manipuri, and post modifiers are 
converted to pre-modifiers. The basic difference 
of Manipuri phrase order compared to English is 
handled by reordering the input sentence follow-
ing the rule (Rao et al, 2000): 
 
SSmV VmOOmCm  ??C'mS'mS'O'mO'V'mV'  
where,    S: Subject 
O: Object 
V : Verb 
Cm: Clause modifier 
X': Corresponding constituent in Manipuri, 
where X is S, O, or V 
Xm: modifier of X 
 
There are two reasons why the syntactic reor-
dering approach improves over the baseline 
phrase-based SMT system (Wang et al, 2007). 
One obvious benefit is that the word order of the 
transformed source sentence is much closer to 
the target sentence, which reduces the reliance on 
the distortion model to perform reordering during 
decoding. Another potential benefit is that the 
alignment between the two sides will be of high-
er quality because of fewer ?distortions? between 
the source and the target, so that the resulting 
phrase table of the reordered system would be 
better. However, a counter argument is that the 
reordering is very error prone, so that the added 
noise in the reordered data actually hurts the 
alignments and hence the phrase tables. 
                                                                 
1 http://nlp.stanford.edu/software/lex-parser.shtml 
84
4 Morphology 
The affixes are the determining factor of the 
word class in Manipuri. In this agglutinative lan-
guage the number of verbal suffixes is more than 
that of nominal suffixes. Works on Manipuri 
morphology are found in (Singh and Bandyo-
padhyay, 2006) and (Singh and Bandyopadhyay, 
2008). In this language, a verb must minimally 
consist of a verb root and an inflectional suffix. 
A noun may be optionally affixed by derivational 
morphemes indicating gender, number and quan-
tity. Further, a noun may be prefixed by a pro-
nominal prefix which indicates its possessor. 
Words in Manipuri consist of stems or bound 
roots with suffixes (from one to ten suffixes), 
prefixes (only one per word) and/or enclitics.  
(a) ?? ????-??  ? ??-? ?      ???? 
Ibomcha-na  Ball-du  kao-i 
Ibomcha-nom Ball-distal kick 
Ibomcha kicks the ball.   
(b) ? ??-? ?  ?? ????-??  ???? 
Ball-du  Ibomcha-na kao-i 
Ball-distal Ibomcha-nom kick 
Ibomcha kicks the ball.   
The identification of subject and object in both 
the sentences are done by the suffixes ?? (na) and 
? ?(du) as given by the examples (a) and (b). The 
case markers convey the right meaning during 
translation though the most acceptable order of 
Manipuri sentence is SOV. In order to produce a 
good translation output all the morphological 
forms of a word and its translations should be 
available in the training data and every word has 
to appear with every possible suffixes. This will 
require a large training data. By learning the gen-
eral rules of morphology, the amount of training 
data could be reduced. Separating lemma and 
suffix allows the system to learn more about the 
different possible word formations.  
 
Manipuri  Gloss English Meaning 
? ????? Tom-na by Tom 
? ????? Tom-dagi from Tom 
? ??? ? Tom-su Tom also 
? ???? Tom-gi of Tom 
? ???? Tom-ga with Tom 
Table 1: Some of the inflected forms of names in 
Manipuri and its corresponding English meaning 
 
Table 1 gives some examples of the inflected 
forms of a person name and its corresponding 
English meaning. The Manipuri stemmer sepa-
rates the case markers such as ??? (-na), -??? (-
dagi), -? ? (-su), -?? (-gi), -?? (-ga) etc. from 
surface forms so that ?? ??? (Tom) from Manipu-
ri side matches with ?Tom? at English side help-
ing to overcome the data sparseness. Enclitics in 
Manipuri fall into six categories: determiners, 
case markers, the copula, mood markers, inclu-
sive / exclusive and pragmatic peak markers and 
attitude markers. The role of the enclitics used 
and its meaning differs based on the context. 
5  Factored Model of Translation 
Using factored approach, a tighter integration of 
linguistic information into the translation model 
is done for two reasons2: 
? Translation models that operate on more 
general representations, such as lemma in-
stead of surface forms of words, can draw on 
richer statistics and overcome the data 
sparseness problem caused by limited train-
ing data. 
? Many aspects of translation can be best ex-
plained at a morphological, syntactic or se-
mantic level. Having such information 
available to the translation model allows the 
direct modeling of these aspects. For in-
stance, reordering at the sentence level is 
mostly driven by general syntactic principles, 
local agreement constraints that show up in 
morphology, etc.  
5.1 Combination of Components in Fac-
tored Model 
Factored translation model is the combination of 
several components including language model, 
reordering model, translation steps and genera-
tion steps in a log-linear model3: 
Z is a normalization constant that is ignored in 
practice. To compute the probability of a transla-
tion e given an input sentence f, we have to eva-
luate each feature function hi. The feature weight 
                                                                 
2http://www.statmt.org/moses/?n=Moses.FactoredModels 
3http://www.statmt.org/moses/?n=Moses.FactoredModels 
 
    
      (1 ) 
85
?i in the log linear model is determined by using 
minimum error rate training method (Och, 2003). 
For a translation step component, each feature 
function ht is defined over the phrase pairs (f j,ej) 
given a scoring function ?:  
 
   
  (2) 
For the generation step component, each fea-
ture function hg given a scoring function ? is de-
fined over the output words ek only: 
 
 
 (3) 
  
5.2 Stanford Dependency Parser  
The dependency relations used in the experiment 
are generated by the Stanford dependency parser 
(Marie-Catherine de Marneffe and Manning, 
2008). This parser uses 55 relations to express 
the dependencies among the various words in a 
sentence. The dependencies are all binary rela-
tions: a grammatical relation holds between a 
governor and a dependent. These relations form a 
hierarchical structure with the most general rela-
tion at the root.  
 
Figure 1. Dependency relation graph of the sen-
tence ?Sources said that Tom was shot by police? 
generated by Stanford Parser 
There are various argument relations like sub-
ject, object, objects of prepositions and clausal 
complements, modifier relations like adjectival, 
adverbial, participial, infinitival modifiers and 
other relations like coordination, conjunct, exple-
tive and punctuation. Let us consider an example 
?Sources said that Tom was shot by police?. 
Stanford parser produces the dependency rela-
tions, nsubj(said, sources) and agent (shot, po-
lice) . Thus, sources|nsubj and police|agent are 
the factors used. ?Tom was shot by police? forms 
the object of the verb ?said?. The Stanford parser 
represents these dependencies with the help of a 
clausal complement relation which links ?said? 
with ?shot? and uses the complementizer relation 
to introduce the subordination conjunction. Fig-
ure 1 shows the dependency relation graph of the 
sentence ?Sources said that Tom was shot by po-
lice?. 
5.3 Factorization approach of English-
Manipuri SMT system 
Manipuri case markers are decided by dependen-
cy relation and aspect information of English. 
Figure 2 shows the translation factors used in the 
translation between English and Manipuri.  
 
(i) Tomba drives the car. 
   ? ?????? ????? ???? 
     Tomba-na car-du thou-i 
    (Tomba)  (the car)  (drives) 
Tomba|empty|nsubj drive|s|empty the|empty|det 
car|empty|dobj 
A subject requires a case marker in a clause 
with a perfective form such as ??? (na). It can be 
represented as, 
suffix+ dependency relation ? case marker  
    s|empty  + empty|dobj ? ?? (na) 
 
(ii) Birds are flying. 
   ????????? ???????  
      ucheksing payri 
     (birds are)  (flying) 
      Bird|s|nsubj are|empty|aux fly|ing|empty 
 
Thus, English-Manipuri factorization consists of  
 
? a lemma to lemma translation factor [i.e., 
Bird ? ???? (uchek) ] 
? a suffix + dependency relation ? suffix [i.e.,  
s + nsubj ? ????? (sing)] 
? a lemma + suffix ? surface form generation 
factor  
[i.e., ???? (uchek) + ????? (sing) ? ????????? 
(ucheksing)] 
said 
source
s 
shot 
that 
Tom was 
Police 
nsubj 
ccomp  
complm 
nsubjpass auxpass 
agent 
86
 
Figure 2. English to Manipuri translation factors 
5.4 Factorization approach of Manipuri-
English SMT system 
Manipuri case markers are responsible to identify 
dependency relation and aspect information of 
English. Figure 3 shows the translation factors 
used in the translation between Manipuri and 
English. The Manipuri- English factorization 
consists of: 
 
? Translation factor: lemma to lemma  
[e.g., ???? (uchek) ? Bird] 
? Translation factor: suffix + POS ? depen-
dency relation + POS + suffix  
[e.g., ????? (sing) + NN ? nsubj + NN + s] 
? Generation factor: lemma + POS + depen-
dency Relation +suffix ? surface form gen-
eration factor  
[e.g., ???? (uchek) + NN  + nsubj + ????? (sing) 
?  ????????? (ucheksing ] 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 3. The Manipuri-English translation factors 
5.5 Syntactically enriched output 
High-order sequence models (just like n-gram 
language models over words) are used in order to 
support syntactic coherence of the output (Koehn 
and Hoang, 2007).  
                 Input             Output 
 
          word                                word 
 
 
                3-gram                       Parts-of-speech 
                
                7-gram 
Figure 4. By generating additional linguistic factors 
on the output side, high-order sequence models over 
these factors support syntactical coherence of the out-
put. 
Adding part-of-speech factor on the output 
side and exploiting them with 7-gram sequence 
models (as shown in Figure 4) results in minor 
improvements in BLEU score. 
6 Experimental Setup 
A number of experiments have been carried out 
using factored translation framework and incor-
porating linguistic information. The toolkits used 
in the experiment are: 
? Stanford Dependency Parser4 was used to (i) 
generate the dependency relations and (ii) 
syntactic reordering of the input English sen-
tences using Parse::RecDescent module. 
? Moses5  toolkit (Koehn, 2007) was used for 
training with GIZA++6, decoding and mini-
mum error rate training (Och, 2003) for tun-
ing. 
? SRILM7 toolkit (Stolcke, 2002) was used to 
build language models with 10350 Manipuri 
sentences for English-Manipuri system and 
four and a half million English wordforms 
collected from the news domain for Manipu-
ri-English system. 
? English morphological analyzer morpha 8 
(Minnen et al, 2001) was used and the 
                                                                 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://www.statmt.org/moses/ 
6 http://www.fjoch.com/GIZA++.html 
7 http://www.speech.sri.com/projects/srilm 
8  
ftp://ftp.informatics.susx.ac.uk/pub/users/johnca/morph.tar.
gz 
Word 
 
 
Lemma 
 
 
 
Suffix  
 
 
Dependency 
Relation 
Word 
 
 
 
Lemma 
 
 
 
 
Case 
Marker 
 
  Input              Output 
Word 
 
 
Lemma 
 
 
 
POS 
 
 
Suffix/ 
Case 
Marker 
Word 
 
 
Lemma 
 
 
 
POS 
 
 
Dependen-
cy Relation  
 
Suffix 
 
87
stemmer from Manipuri Morphological ana-
lyzer (Singh and Bandyopadhyay, 2006) was 
used for the Manipuri side.  
? Manipuri POS tagger (Singh et. al., 2008) is 
used to tag the POS (Parts of speech) factors 
of the input Manipuri sentences. 
7 Evaluation 
7.1 English-Manipuri SMT System 
The evaluation of the machine translation sys-
tems developed in the present work is done in 
two approaches using automatic scoring with 
reference translation and subjective evaluation as 
discussed in (Ramanathan et al, 2009). 
Evaluation Metrics: 
? NIST (Doddington, 2002): A high score 
means a better translation by measuring the 
precision of n-gram. 
? BLEU (Papineni et al 2002): This metric 
gives the precision of n-gram with respect to 
the reference translation but with a brevity 
penalty.  
 
 No of sentences No of words 
Training 10350 296728 
Development 600 16520 
Test 500 15204 
Table 2. Train ing, development and testing corpus 
statistics 
 
Table 2 shows the corpus statistics used in the 
experiment. The corpus is annotated with the 
proposed factors. The following models are de-
veloped for the experiment. 
Baseline: 
The model is developed using the default setting 
values in MOSES.  
Lemma +Suffix: 
It uses lemma and suffix factors on the source 
side, lemma and suffix on the target side for 
lemma to lemma and suffix to suffix translations 
with generation step of lemma plus suffix to sur-
face form. 
Lemma + Suffix + Dependency Relation: 
Lemma, suffix and dependency relations are used 
on the source side.  The translation steps are (a) 
lemma to lemma (b) suffix + dependency rela-
tion to suffix and generation step is lemma + suf-
fix to surface form. Table 3 shows the BLEU and 
NIST scores of the system using these factors. 
Table 4 shows the BLEU and NIST scores of 
the English-Manipuri SMT systems using lexica-
lized and syntactic reordering.  
 
Model BLEU NIST 
Baseline (surface) 13.045 4.25 
Lemma + Suffix 15.237 4.79 
Lemma + Suffix + De-
pendency Relation 
16.873 5.10 
Table 3. Evaluation Scores of English - Manipuri 
SMT System using various translation factors 
 
Model Reordering BLEU NIST 
Baseline 
(surface) 
 13.045 4.25 
Surface Lexicalized 13.501 4.32 
Surface Syntactic 14.142 4.47 
Table 4. Evaluation Scores of English-Manipuri SMT 
system using Lexicalized and Syntactic Reordering  
 
Input/Output of English-Manipuri SMT: 
 
(1a) Input: Going to school is obligatory for stu-
dents. 
   ???  ????  ?????????? ? ?? ??? ? ???????? | 
    School chatpa shatra-sing-gi touda ya     
    draba mathouni. 
 Baseline output:  ??? ???? ???? ?? ??? 
              school mathou chatpa oy shatra 
     gloss : school duty going is student. 
 Syntactic Reorder output: ??? ??? ???? ? ?? ??? ? 
                 shatra school chatpa touda yadraba 
     gloss: Student school going compulsory.  
 Dependency output: ???????? ??? ???? ???????? 
     shatrasing schoolda chatpa mathouni 
     gloss: Students going to the school is duty. 
 
(1b) Input: Krishna has a flute in his hand.  
           ?????     ????    ? ?????   ??? ?? | 
                 Krishna-gi khut-ta toudri ama lei. 
 Syntactic Reorder output:  ??? ?? ???? ??? ? ?????  
                           Krishna lei khut ama toudri 
      gloss : Krishna has a hand flute 
 Dependency output: ????? ?? ? ????? ??? ????  
                           krishnagi lei toudri ama  khutta   
      gloss : Krishna has a flute in his hand 
88
One of the main aspects required for the fluen-
cy of a sentence is agreement. Certain words 
have to match in gender, case, number, person 
etc. within a sentence. The rules of agreement are 
language dependent and are closely linked to the 
morphological structure of language. Subjective 
evaluations on 100 sentences have been per-
formed for fluency and adequacy by two judges. 
The fluency measures how well formed the sen-
tences are at the output and adequacy measures 
the closeness of the output sentence with the ref-
erence translation. The Table 5 and Table 6 show 
the adequacy and fluency scales used for evalua-
tion and Table 7 shows the scores of the evalua-
tion. 
 
Level Interpretation 
4 Full meaning is conveyed 
3 Most of the meaning is conveyed 
2 Poor meaning is conveyed 
1 No meaning is conveyed 
Table 5. Adequacy scale 
 
Level Interpretation 
4 Flawless with no grammatical error  
3 Good output with minor errors 
2 Disfluent ungrammatical with correct 
phrase 
1 Incomprehensible 
Table 6. Fluency scale 
 
 Sentence 
length 
Fluency Adequacy 
Baseline <=15 
words 
1.95 2.24 
>15 words 1.49 1.75 
Reordered <=15 
words 
2.58 2.75 
>15 words 1.82 1.96 
Dependency 
Relation 
<=15 
words 
2.83 2.91 
>15 words 1.94 2.10 
Table 7. Scale o f Fluency and Adequacy on sentence 
length basis of English-Manipuri SMT system 
7.2 Manipuri-English SMT System 
The system uses the corpus statistics shown in 
Table 2. The corpus is annotated with the pro-
posed factors. The following models are devel-
oped for the experiment. The baseline and 
lemma+suffix systems follow same factors as 
English-Manipuri.  
Lemma + Suffix + POS: 
Lemma, suffix and POS are used on the source 
side.  The translation steps are (a) lemma to 
lemma (b) suffix + POS to POS + suffix + de-
pendency relation and generation step is lemma 
+ suffix + POS + dependency relation to surface 
form. 
 
Model BLUE NIST 
Baseline (surface) 13.452 4.31 
Lemma + Suffix 16.137 4.89 
Lemma + Suffix + POS 17.573 5.15 
Table 8. Evaluation Scores of Manipuri-English SMT 
system using various translation factors 
 
Table 8 shows the BLEU and NIST scores of 
the Manipuri-English systems using the different 
factors. Table 9 shows the scores of using lexica-
lized reordering and POS language model. 
 
Model BLUE NIST 
Baseline + POS LM 14.341 4.52 
Baseline + Lexicalized 13.743 4.46 
Baseline + Lexicalized 
+POS LM 
14.843 4.71 
Table 9. Evaluation Scores of Manipuri-English SMT 
system using Lexicalized reordering and POS Lan-
guage Model 
 
Input/Output of Manipuri-English SMT: 
 
 (2a) Input: ??? ???? ?????????? ? ?? ??? ? ???????? | 
     gloss: School chatpa shatra-sing-gi touda 
yadraba mathouni. 
     Going to school is obligatory for students. 
Baseline output: school going to the students 
important 
Lexicalized Reordered output: school going 
important to the students 
Lemma+Suffix+POS+lexicalized reordered 
output: School going important to the students 
 
(2b) Input: ????? ???? ? ????? ??? ?? | 
     gloss: Krishna-gi khut-ta toudri ama lei. 
     Krishna has a flute in his hand.  
 Baseline output: Krishna is flute and hand  
Lexicalized Reordered output: Krishna flute 
has his hand  
89
Lemma+Suffix+POS+lexicalized reordered 
output: Krishna has flute his hand 
By considering the lemma along with suffix 
and POS factors, the fluency and adequacy of the 
output is better addressed as given by the sample 
input and output (2a) and (2b) over the baseline 
system. Using the Manipuri stemmer, the case 
markers and suffixes are taken into account for 
different possible word forms thereby helping to 
overcome the data sparseness problem. Table 10 
shows the scores of adequacy and fluency of the 
evaluation. 
 
 Sentence 
length 
Fluency Adequacy 
Baseline <=15 
words 
1.93 2.31 
>15 words 1.51 1.76 
Reordered <=15 
words 
2.48 2.85 
>15 words 1.83 1.97 
Lemma + 
Suffix  
+ POS 
<=15 
words 
2.86 2.92 
>15 words 2.01 2.11 
Table 10. Scale of Fluency and Adequacy on sen-
tence length basis of Manipuri-English SMT system 
Subjective evaluations on 100 sentences have 
been performed for fluency and adequacy. In the 
process of subjective evaluation, sentences were 
judged on fluency, adequacy and the number of 
errors in case marking/morphology. It is ob-
served that poor word-order makes the baseline 
output almost incomprehensible, while lexica-
lized reordering solves the problem correctly 
along with parts-of-speech language model (POS 
LM). Statistical significant test is performed to 
judge if a change in score that comes from a 
change in the system reflects a change in overall 
translation quality. It is found that all the differ-
ences are significant at the 99% level. 
8 Discussion 
The factored approach using the proposed factors 
show improved fluency and adequacy at the Ma-
nipuri output for English-Manipuri system as 
shown in the Table 6. Using the Stanford gener-
ated relations shows an improvement in terms of 
fluency and adequacy for shorter sentences than 
the longer ones.  
Input : Khamba pushed the stone with a lever. 
       ?????? ???????? ??? ?? ????????? | 
Outputs: 
Syntactic Reordered: ???? ??? ??????  ?? ???????? | 
 Khamba nung jamfat adu illi 
gloss:  Khamba stone the lever push 
Dependency: ?????? ??? ?? ????????? ??????? | 
 Khambana nung adu jamfatna illi 
gloss: Khamba the stone pushed with lever 
 
By the use of semantic relation, ?? (na) is at-
tached to ???? (Khamba), which makes the mean-
ing ??????  ?by Khamba?  instead of  just ???? 
?Khamba?. 
Input : Suddenly the woman burst into tears. 
       ??? ???? ??? ????? ????? ?????????? | 
Outputs: 
Syntactic Reordered: ???? ???? ????????? ????? | 
 Nupi thuna pirang-ga kappi 
gloss: woman soon tears cry 
Dependency:  ?? ??? ????? ????????? | 
 Athubada nupidu kaplammi 
gloss: suddenly the woman cried 
 
Here, in this example, the ???? (nupi) is suf-
fixed by the ?? (du), to produce ????? ??the wom-
an? instead of just ???? ?woman?. 
The factored approach of Manipuri-English 
SMT system also shows improved BLEU and 
NIST scores using the proposed factors as shown 
in Table 8 not only gain in fluency and adequacy 
scores as shown in Table 10.  
9 Conclusion 
A framework for Manipuri and English bidirec-
tional SMT system using factored model is expe-
rimented with a goal to improve the translation 
output and reduce the amount of training data. 
The output of the translation is improved by in-
corporating morphological information and se-
mantic relations by tighter integration. The 
systems are evaluated using automatic scoring 
techniques BLEU and NIST. The subjective 
evaluation of the systems is done to find out the 
fluency and adequacy. The fluency and adequacy 
are also addressed better for the shorter sentences 
than the longer ones using semantic relations. 
The improvement is statistically significant. 
 
90
References  
Avramidis, E. and Koehn, P. 2008. Enriching mor-
phologically poor languages for Statistical Machine 
Translation, Proceedings of ACL-08: HLT 
 
Callison-Burch, Chris., Osborne, M. and Koehn, P. 
2006. Re-evaluating the Role of Bleu in Machine 
Translation Research" In Proceedings of EACL-
2006 
 
Doddington, G. 2002. Automat ic evaluation of Ma-
chine Translation quality using n-gram co-
occurrence statistics. In Proceedings of HLT 2002, 
San Diego, CA. 
 
Koehn. P., and Hoang, H. 2007.  Factored Translation 
Models, In Proceedings of EMNLP-2007 
 
Koehn, P., Hieu, H., Alexandra, B., Chris, C., Marcel-
lo, F., Nicola, B., Brooke, C., Wade, S., Christine, 
M., Richard, Z., Chris, D., Ondrej, B., A lexandra, 
C., Evan, H. 2007. Moses: Open Source Toolkit  for 
Statistical Machine Translation, Proceedings of the 
ACL 2007 Demo and Poster Sessions, pages 177?
180, Prague. 
 
Marie-Catherine de Marneffe and Manning, C. 2008.  
Stanford Typed Dependency Manual 
 
Minnen, G., Carro ll, J., and Pearce, D. 2001. Applied 
Morphological Processing of English, Natural 
Language Engineering, 7(3), pages 207-223 
 
Nie?en, S., and Ney, H. 2004. Statistical Machine 
Translation with Scarce Resources Using Morpho-
syntactic Information, Computational Linguistics, 
30(2), pages 181-204 
 
Och, F. 2003. Minimum error rate train ing in Statis-
tical Machine Translation , Proceedings of ACL 
 
Papineni, K., Roukos, S., Ward, T., and Zhu, W. 
2002. BLEU: a method for automat ic evaluation of 
machine translation. In Proceedings of 40th  ACL, 
Philadelphia, PA 
 
Popovic, M., and Ney, H. 2006.  Statistical Machine 
Translation with a s mall amount of bilingual train-
ing data, 5th LREC SALTMIL Workshop on Minor-
ity Languages 
Ramanathan, A., Choudhury, H., Ghosh, A., and 
Bhattacharyya, P. 2009. Case markers and Mor-
phology: Addressing the crux of the fluency prob-
lem in  English-Hindi SMT, Proceedings of the 
Joint Conference of the 47th Annual Meeting of the 
ACL and the 4th International Joint Conference on 
Natural Language Processing of the AFNLP: Vo-
lume 2, pages: 800-808  
 
Rao, D., Mohanraj, K., Hegde, J., Mehta, V. and Ma-
hadane, P. 2000. A practical framework for syntac-
tic transfer of compound-complex sentences for 
English-Hindi Machine Translation, Proceedings 
of KBCS 2000, pages 343-354 
 
Singh, Thoudam D., and Bandyopadhyay, S. 2006. 
Word Class and Sentence Type Identification in  
Manipuri Morphological Analyzer, Proceeding of 
Modeling and Shallow Parsing of Indian Languag-
es(MSPIL) 2006, IIT Bombay, pages 11-17, Mum-
bai, India  
 
Singh, Thoudam D., and Bandyopadhyay, S. 2008. 
Morphology Driven Manipuri POS Tagger,  In pro-
ceedings International Joint Conference on Natu-
ral Language Processing (IJCNLP-08) Workshop 
on Natural Language Processing of Less Privi-
leged Languages (NLPLPL) 2008, pages 91-98, 
Hyderabad, India  
 
Singh, Thoudam D., and Bandyopadhyay, S. 2010a. 
Manipuri-English Example Based Machine Trans-
lation System, International Journal of Computa-
tional Linguistics and Applications (IJCLA), ISSN 
0976-0962, pages 147-158 
 
Singh, Thoudam D., Singh, Yengkhom R. and Ban-
dyopadhyay, S., 2010b. Manipuri-English Semi 
Automatic Parallel Corpora Extraction from Web, 
In proceedings of 23rd International Conference 
on the Computer Processing of Oriental Languag-
es (ICCPOL 2010) - New Generation in Asian In-
formation Processing , San Francisco Bay, CA, 
USA, Pages 45-48 
Singh, Thoudam D. and Bandyopadhyay, S., 2010c. 
Statistical Machine Translation of English-
Manipuri using Morpho-Syntactic and Semantic 
Information, In  the proceedings of Ninth Confe-
rence  of the Association for Machine Translation 
in Americas (AMTA 2010), Denver, Colorado, 
USA. (To appear) 
Stolcke. A. 2002. SRILM - An Extensible Language 
Modeling Toolkit. In  Proc. Intl. Conf. Spoken Lan-
guage Processing, Denver, Colorado, September.  
 
Wang, C., Collin,  M., and Koehn, P. 2007. Ch inese 
syntactic reordering for statistical machine transla-
tion, Proceedings of EMNLP-CoNLL 
91
JU_CSE_GREC10: Named Entity Generation at GREC 2010 
Amitava Das1, Tanik Saikh2, Tapabrata Mondal3, Sivaji Bandyopadhyay4 
Department of Computer Science and Engineering 
Jadavpur University,  
Kolkata-700032, India  
amitava.santu@gmail.com1, tanik4u@gmail.com2, tapabratamon-
dal@gmail.com3, sivaji_cse_ju@yahoo.com4  
 
Abstract 
 
This paper presents the experiments carried 
out at Jadavpur University as part of the par-
ticipation in the GREC Named Entity Genera-
tion Challenge 2010. The Baseline system is 
based on the SEMCAT, SYNCAT and SYN-
FUNC features of REF and REG08-TYPE and 
CASE features of REFEX elements. The dis-
course level system is based on the additional 
positional features: paragraph number, sen-
tence number, word position in the sentence 
and mention number of a particular named ent-
ity in the document. The inclusion of discourse 
level features has improved the performance 
of the system. 
1 Baseline System 
The baseline system is based on the following 
linguistic features of REF elements: SEMCAT 
(Semantic Category), SYNCAT (Syntactic Cate-
gory) and SYNFUNC (Syntactic Function) (Anja 
Belz, 2010) and the following linguistic features 
of REFEX elements: REG08-TYPE (Entity type) 
and CASE (Case marker). The baseline system 
has been separately trained on the training set 
data for the three domains: chefs, composers and 
inventors. The system has been tested on each 
development set by identifying the most probable 
REFEX element among the possible alternatives 
based on the REF element feature combination. 
The probability assigned to a REFEX element 
corresponding to a certain feature combination of 
REF element is calculated as follows:  
( )
i
i
D
REFEX
v D
REF
Np R
N
=  
where ( )vp R is the probability of the targeted 
REFEX element to be assigned, iDREFN is the total 
number of occurrences of REF element feature 
combinations, iD denotes the domain i.e., Chefs, 
Composers and Inventors and iDREFN denotes the 
total number of occurrences of the REFEX ele-
ment corresponding to the REF feature combina-
tion. 
It has been observed that many times the most 
probable REFEX element as identified from the 
training set is not present among the alternative 
REFEX elements. In these cases the system as-
signs the next highest probable REFEX element 
learnt from the training set that matches with one 
of the REFEX elements among the alternatives. 
In some cases more than one REFEX element get 
same probability in the training set. In these cas-
es, the REFEX element that occurs earlier in the 
alternative set is assigned. The experimental re-
sult of Baseline system is reported in Table 1. 
 Chefs Composers Inventors 
Precision 0.63 0.68 0.70 
Recall 0.69 0.60 0.64 
F-Measure 0.66 0.64 0.68 
Table 1: Result of Baseline System 
2 Discourse Level System 
The discourse level features like paragraph num-
ber, sentence number and position of a particular 
word in a sentence have been added with the fea-
tures considered in the baseline system. As men-
tioned in Section 1, more than one REFEX ele-
ment can have the same probability value. This 
happens as REFEX elements are identified by 
two features only REG08-TYPE and CASE.  
 Nam
e 
Pro-
noun 
Com-
mon 
Emp-
ty 
Chefs 2317 3071 55 646 
Composers 2616 4037 92 858 
Inventors 1959 2826 75 621 
Table 2: Distribution of REFEX Types among 
three domains. 
The above problem occurs mainly for Name 
type. Pronouns are very frequent in all the three 
domains but they have small number of varia-
tions as: he, her, him, himself, his, she, who, 
whom and whose. Common type REFEX ele-
ments are too infrequent in the training set and 
they are very hard to generalize. Empty type has 
only one REFEX value as: ?_?.  The distribution 
of the various REFEX types among the three 
domains in the training set is shown in Table 2. 
2.1 Analysis of Name type entities 
Table 2 shows that name types are very frequent 
in all the three domains. Name type entities are 
further differentiated by adding more features 
derived from the analysis of the name type ele-
ment.  
Firstly, the full name of each named entity has 
been identified by Entity identification number 
(id), maximum length among all occurrences of 
that named entity and case marker as plain. For 
example, in Figure 1, the REFEX element of id 3 
has been chosen as a full name of entity ?0? as it 
has the longest string with case ?plain?. 
After identification of full name of each RE-
FEX entity, the following features are identified 
for each occurrence of an entity:: Complete 
Name Genitive (CNG), Complete Name (CN), 
First Name Genitive (FNG), First Name (FN), 
Last Name Genitive (LNG), Last Name (LN), 
Middle Name Genitive (MNG) and Middle 
Name (MN). These features are binary in nature 
and for each occurrence of an entity only one of 
the above features will be true. 
Pronouns are kept as the REFEX element fea-
ture with its surface level pattern as they have 
only 9 variations. Common types are considered 
with tag level ?common? as they hard to general-
ize. Empty types are tagged as ?empty? as they 
have only one tag value ?_?.  
1 
<REFEX ENTITY="0" REG08-
TYPE="name" CASE="genitive">Alain 
Senderens's</REFEX> 
CNG 
2 
<REFEX ENTITY="0" REG08-
TYPE="name" 
CASE="genitive">Senderens's</REFEX> 
LNG 
3 
<REFEX ENTITY="0" REG08-
TYPE="name" CASE="plain">Alain 
Senderens</REFEX> 
CN 
4 
<REFEX ENTITY="0" REG08-
TYPE="name" 
CASE="plain">Senderens</REFEX> 
LN 
Figure 1: Example of Full Name Identification 
3 Experimental Results 
The experimental results of the discourse level 
system on the development set are reported in the 
Table 3 and Table 4 respectively. Table 3 reports 
the results when the system has been trained sep-
arately with domain specific training set and Ta-
ble 4 reports the results when the training has 
been carried out on the complete training set.  
The comparison of the results of the baseline 
and the discourse level system shows an overall 
improvement. But there are some interesting ob-
servations when comparing the results in Table 3 
and Table 4. Currently detailed analyses of the 
results are being carried out.
 Chefs Composers Inventors 
P R F P R F P R F 
Name 0.69 0.74 0.71 0.78 0.61 0.69 0.77 0.67 0.71 
Pronoun 0.81 0.76 0.79 0.70 0.84 0.76 0.76 0.87 0.81 
Common 0.76 0.87 0.81 0.37 0.44 0.40 0.44 0.65 0.68 
Empty 0.92 0.88 0.90 0.86 0.92 0.89 0.72 0.65 0.68 
 
Table 3: Experimental Results of Discourse Level System on the Development Set (Training with 
Domain Specific Training Set) 
 
 
Reg08 Type 
   String  
Accuracy 
BLEU 
NIST 
String Edit 
Distance 
Precision Recall Mean Mean Normalized 1 2 3 4 
Chefs 0.66 0.70 0.57 0.68 0.70 0.76 0.81 3.70 0.77 0.38 
Composers 0.63 0.67 0.56 0.61 0.57 0.54 0.50 3.34 1.07 0.40 
Inventors 0.60 0.62 0.50 0.55 0.54 0.52 0.49 2.90 1.25 0.47 
Total 0.63 0.66 0.54 0.61 0.58 0.57 0.55 3.83 1.03 0.42 
 
Table 4: Table 4: Experimental Results of Discourse Level System on the Development Set (Training 
with Complete Training Set) 
References 
Anja Belz. 2010. GREC Named Entity Generation Challenge 2010: Participants? Pack. 
Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 8?13,
Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational Linguistics
Semantic Clustering: an Attempt to Identify Multiword Expressions in 
Bengali 
Tanmoy Chakraborty        Dipankar Das        Sivaji Bandyopadhyay 
Department of Computer Science and Engineering 
Jadavpur University, Kolkata 700 032, India 
its_tanmoy@yahoo.co.in, dipankar.dipnil2005@gmail.com  
sivaji_cse_ju@yahoo.com 
 
Abstract 
One of the key issues in both natural lan-
guage understanding and generation is the 
appropriate processing of Multiword Ex-
pressions (MWEs). MWE can be defined 
as a semantic issue of a phrase where the 
meaning of the phrase may not be obtained 
from its constituents in a straightforward 
manner. This paper presents an approach of 
identifying bigram noun-noun MWEs from 
a medium-size Bengali corpus by cluster-
ing the semantically related nouns and in-
corporating a vector space model for 
similarity measurement. Additional inclu-
sion of the English WordNet::Similarity 
module also improves the results consider-
ably. The present approach also contributes 
to locate clusters of the synonymous noun 
words present in a document. Experimental 
results draw a satisfactory conclusion after 
analyzing the Precision, Recall and F-score 
values.  
1 Introduction 
Over the past two decades or so, Multi-Word Ex-
pressions (MWEs) have been identified with an 
increasing amount of interest in the field of Com-
putational linguistics and Natural Language 
Processing (NLP). The term MWE is used to refer 
the various types of linguistic units and expres-
sions including idioms (kick the bucket, ?to die?), 
noun compounds (village community), phrasal 
verbs (find out, ?search?) and other habitual collo-
cations like conjunction (as well as), institutiona-
lized phrases (many thanks) etc. They can also be 
grossly defined as ?idiosyncratic interpretations 
that cross the word boundaries? (Sag et al, 2002).  
MWE is considered as a special issue of seman-
tics where the individual components of an expres-
sion often fail to keep their meanings intact within 
the actual meaning of the expression. This opaque-
ness in meaning may be partial or total depending 
on the degree of compositionality of the whole ex-
pression. In Bengali, an analogous scenario can be 
observed when dealing with the expressions like 
compound nouns (taser ghar, ?house of cards?, 
?fragile?), complex predicates such as conjunct 
verbs (anuvab kara, ?to feel?) and compound verbs 
(uthe para, ?to arise?), idioms (matir manus, ?down 
to the earth?), Named Entities (NEs) (Rabindra-
nath Thakur, ?Rabindranath Tagore?) etc.  
In this paper, we analyze MWEs from the pers-
pective of semantic interpretation. We have fo-
cused mainly on the fact that the individual 
meanings of the components are totally or partially 
diminished in order to form the actual semantics of 
the expression. A constellation technique has been 
employed to group all nouns that are somehow 
related to the meaning of the component of any 
expression in the corpus and hence to build cluster 
for that component. Two types of vector space 
based similarity techniques are applied to make a 
binary classification of the candidate nouns. The 
intuition was that more the similarity of the com-
ponents of an expression, less the probability of the 
candidate to become a MWE. We have also shown 
the results using WordNet::Similarity module.  
The remainder of the paper is organized as fol-
lows. In the next section, we review the related 
work on MWE and graph-clustering approach for 
detecting compositionality. Section 3 proposes a 
brief description of the semantic clustering ap-
proach. The system framework is elaborated in 
Section 4. Experimental results and the various 
observations derived from our research are dis-
cussed in Section 5. Finally, Section 6 concludes 
the paper. 
8
2 Related Work 
A number of research activities regarding MWE 
identification have been carried out in various lan-
guages like English, German and many other Eu-
ropean languages. The statistical co-occurrence 
measurements such as Mutual Information (MI) 
(Church and Hans, 1990), Log-Likelihood (Dun-
ning, 1993) and Salience (Kilgarriff and Rosenz-
weig, 2000) have been suggested for identification 
of MWEs. An unsupervised graph-based algorithm 
to detect the compositionality of MWEs has been 
proposed in (Korkontzelos and Manandhar 2009). 
In case of Indian languages, an approach in 
compound noun MWE extraction (Kunchukuttan 
and Damani, 2008) and a classification based ap-
proach for Noun-Verb collocations (Venkatapathy 
and Joshi, 2009) have been reported. In Bengali, 
the works on automated extraction of MWEs are 
limited in number. One method of automatic ex-
traction of Noun-Verb MWE in Bengali (Agarwal 
et al, 2004) has been carried out using significance 
function. In contrast, we have proposed a cluster-
ing technique to identify Bengali MWEs using se-
mantic similarity measurement. It is worth noting 
that the conducted experiments are useful for iden-
tifying MWEs for the electronically resource con-
strained languages.   
3 Semantic Clustering Approach 
Semantic clustering aims to cluster semantically 
related tokens present in a document. Identifying 
semantically related words for a particular token is 
carried out by looking the surrounding tokens and 
finding the synonymous words within a fixed con-
text window. Statistical idiomaticity demands fre-
quent occurrence of a particular expression as one 
or few occurrences of a particular word cannot in-
fer all its meaning. However, the semantics of a 
word may be obtained by analyzing its similarity 
sets called synset.  Higher value of the similarity 
coefficient between two synonymous sets of the 
multi-word components indicates more affinity of 
the components to each other.  
For individual component of a bigram expres-
sion, semantically related words of the documents 
are extracted by using a monolingual dictionary (as 
discussed in Section 4.4). Count of elements in an 
intersection of two synsets indicates the commo-
nality of the two sets and its absolute value stands 
for their commonality measure. Considering the 
common elements as the dimensions of the vector 
space, similarity based techniques are applied to 
measure the semantic affection of the two compo-
nents present in a bigram. 
4 System Framework 
4.1 Corpus Preparation and Candidate Selec-
tion 
The system uses a large number of Bengali articles 
written by the noted Indian Nobel laureate Rabin-
dranath Tagore 1 . We are primarily interested in 
single document term affinity rather than document 
information and document length normalization. 
Merging all of the articles, a medium size raw cor-
pus consisting of 393,985 tokens and 283,533 
types has been prepared. Basic pre-processing of 
the crawled corpus is followed by parsing with the 
help of an open source shallow parser2 developed 
for Bengali. Parts-of-Speech (POS), chunk, root, 
inflection and other morphological information for 
each token have been retrieved. Bigram noun se-
quence within a noun chunk is extracted and 
treated as candidates based on their POS, chunk 
categories and the heuristics described as follows.   
1. POS:   POS of each token is either ?NN? or         
?NNP? 
2. Chunk: w1 and w2 must be in the same ?NP?   
chunk 
3. Inflection: Inflection 3  of w1 must be                
?-    ????(null), ?-??(-r), ?-???(-er), ?-
??(-e), ?-??(-y) or ?-????(-yr) and for 
w2, any inflection is considered. 
4.2 Dictionary Restructuring 
To the best of our knowledge, no full-fledged 
WordNet resource is available for Bengali. Hence, 
the building of Bengali synsets from a monolingual 
Bengali dictionary not only aims to identify the 
meaning of a token, but also sets up the framework 
towards the development of Bengali WordNet. 
Each word present in the monolingual dictionary 
(Samsada Bengali Abhidhana)4 contains its POS, 
                                                        
1 http://www.rabindra-rachanabali.nltr.org 
2  http://ltrc.iiit.ac.in/analyzer/bengali 
3  Linguistic study (Chattopadhyay, 1992) reveals that for 
compound noun MWE, considerable inflections of first noun 
are only those which are mentioned above. 
4  http://dsal.uchicago.edu/dictionaries/biswas-bangala/ 
9
phonetics and synonymous sets. An automatic 
technique has been devised to identify the synsets 
of a particular word based on the clues (?,? comma 
and ?;? semi-colon) provided in the dictionary to 
distinguish words of similar and different sense 
from the synonymous sets. The symbol tilde (~) 
indicates that the suffix string followed by the tilde 
(~) notation makes another new word concatenat-
ing with the original entry word. A partial snapshot 
of the synsets for the Bengali word ????? (Ang-
shu) is shown in Figure 1. In Table 1, the frequen-
cies of different synsets according to their POS are 
shown. 
Dictionary Entry: 
??? [a??u] ??. 1 ????, ???, ???; ~ ? 
??. ?? , ??? ?? ; ???? ??? ???????? ??? 
???  ~ ??? ??. ????????, ?????????  
Synsets: 
??? ????/???/???_??.#25_1_1  
???? ??/???_??_??.#26_1_1  
???? ????_???_????????_???_??_??.#26_2_2 
?????? ????????/????????_??.#27_1_1 
Figure 1: A partial snapshot of the Bengali mono-
lingual dictionary entry (word and synsets) 
 
Total 
#Word  
Total 
#Synset 
Noun Adj- 
ective 
Pro- 
noun 
Verb 
33619 63403 28485 11023 235 1709 
Table 1: Total number of words, synsets and Fre-
quencies of different POS based synsets 
4.3 Generating Semantic Clusters of Nouns 
In the first phase, we have generated the synonym-
ous sets for all nouns present in the corpus using 
the synset based dictionary whereas in the second 
phase, the task is to identify the semantic distance 
between two nouns. The format of the dictionary 
can be thought of as follows:  
W1=n1
1, n2
1, n3
1,  ?????? = {ni
1} 
     . 
 . 
Wm=n1
m, n2
m, n3
m,  ?????. = {np
m} 
where, W1, W2, ?.,W
m are the dictionary word en-
tries and nj
m (for all j) are the elements of the syn-
sets of Wm. Now, each noun entry identified by the 
shallow parser in the document is searched in the 
dictionary. For example, if a noun N present the 
corpus becomes an entry of the synsets, W1, W
3
 and 
W5, the synset of N is as follows,  
            SynSet (N) = {Wl, W3, W5}??? (1) 
To identify the semantic similarity between two 
nouns, we have applied simple intersection rule. 
The number of common elements between the syn-
sets of the two noun words denotes the similarity 
between them. If Ni and Nj are the two noun words 
in the document and Wi and Wj are their corres-
ponding synsets, the similarity of the two words 
can be defined as, 
               Similarity (Ni,Nj) = |W
i ? Wj|???.(2) 
We have clustered all the nouns present in the 
document for a particular noun and have identified 
the similarity score for every pair of nouns ob-
tained using equation 2. 
4.4 Checking of Candidate Bigram as MWE  
The identification of candidates as MWE is done 
using the results obtained from the previous phase. 
The algorithm to identify the noun-noun bigram 
<M1 M2> as MWE is discussed below with an 
example shown in Figure 2. 
 
ALOGRITHM:  MWE-CHECKING 
    INPUT: Noun-noun bigram <M1 M2> 
    OUTPUT: Return true if MWE, or return false. 
1. Extract semantic clusters of M1 and M2 
2. Intersection of the clusters of both M1 and M2 
(Figure 2.1 shows the common synset entries of 
M1 and M2 using rectangle). 
3. For measuring the semantic similarity between 
M1 and M2: 
3.1. In an n-dimensional vector space (here 
n=2), the common entries act as the axes. Put 
M1 and M2 as two vectors and associated 
weights as their co-ordinates. 
3.2. Calculate cosine-similarity measurement 
and Euclidean distance (Figure 2.2). 
4. Final decision taken individually for two differ-
ent measurements- 
4.1 If cosine-similarity > m, return false;            
Else return true; 
  4.2 If Euclidean-distance >  p, return false; 
                Else return true; 
(Where m and p are the pre-defined cut-off values) 
 
 
    We have also employed English WordNet 5  to 
measure   the   semantic   similarity   between   two  
                                                        
5 http://www.d.umn.edu/tpederse/similarity.html 
10
 
 
 
 
 
 
 
 
 
Figure 2.1: Intersection of the clusters of the con-
stituents (left side); Figure 2.2: Similarity between 
two constituents Evaluation (right side) 
Bengali words translated into English. Word-
Net::Similarity is an open-source package for cal-
culating the lexical similarity between word (or 
sense) pairs based on various similarity measures. 
Basically, WordNet measures the relative distance 
between two nodes denoted by two words in the 
WordNet tree which can vary from -1 to 1 where    
-1 indicates total dissimilarity between two nodes. 
The equation used to calculate this distance is men-
tioned below- 
    Normalized_Distance= minDistToCommonPa-
rent / (DistFromCommonParentToRoot + min-
DistToCommonParent)                    ????..(3) 
We have translated the root of the two compo-
nents of a Bengali candidate into their English 
equivalents using a Bengali to English bilingual 
dictionary. They are passed into the WordNet 
based similarity module for measuring similarity 
between the components.  
If we take an example of a Bengali idiom hater 
panch (remaining resource) to describe our intui-
tion, we have seen that the WordNet defines two 
components of the idiom hat (hand) as ?a part of a 
limb that is farthest from the torso? and panch 
(five) as ?a number which is one more than four?. 
So from these two glosses it is quite clear that they 
are not at all semantically related in any sense.   
 
 
 
 
 
 
The synonymous sets for these two components 
extracted from the formatted dictionary are shown 
below ? 
Synset (??? ?hat?) = { ??, ??, ????, ???, ???, 
?????, ?????, ????, ????, ?????, ?????, 
?????, ???? } 
Synset (???? ?panch?) = {??, ?????, ???, ???, 
???, ????, ??, ????, ??, ????, ???, ??????, 
???????, ???? } 
It is clearly seen from the above synonymous 
sets that there is no common element and hence its 
similarity score is obviously zero. In this case, the 
vector space model cannot be drawn using zero 
dimensions. For them, a marginal weight is as-
signed to show them as completely non-
compositional phrase. To identify their non-
compositionality, we have to show that their occur-
rence is not certain only in one case; rather they 
can occur side by side in several occasions. But 
this statistical proof can be determined better using 
a large corpus. Here, for those candidate phrases, 
which show zero similarity, we have seen their 
existence more than one time in the corpus. Taking 
any decision using single occurrence may give in-
correct result because they can be unconsciously 
used by the authors in their writings. That is why, 
the more the similarity between two components in 
a bigram, the less the probability to be a MWE. 
4.5 Annotation Agreement 
Three annotators identified as A1, A2 and A3 were 
engaged to carry out the annotation. The annota-
tion agreement of 628 candidate phrases is meas-
ured using standard Cohen's kappa coefficient (?) 
(Cohen, 1960). It is a statistical measure of inter-
rater agreement for qualitative (categorical) items. 
In addition to this, we also choose the measure of 
agreements on set-valued items (MASI) (Passon-
neau, 2006) that was used for measuring agreement 
in the semantic and pragmatic annotation.  Annota-
tion results as shown in Table 2 are satisfactory.  
 
Cut-off 
Cosine-Similarity Euclidean Distance WordNet Similarity 
P R FS P R FS P R FS 
0.6 70.75 64.87 67.68 70.57 62.23 66.14 74.60 61.78 67.58 
0.5 78.56 59.45 67.74 72.97 58.79 65.12 80.90 58.75 68.06 
0.4 73.23 56.97 64.08 79.78 53.03 63.71 75.09 52.27 61.63 
 
Table 3: Precision (P), Recall (R) and F-score (FS) (in %) for various measurements 
11
The list of noun-noun collocations are extracted 
from the output of the parser for manual checking. 
It is observed that 39.39% error occurs due to 
wrong POS tagging or extracting invalid colloca-
tions by considering the bigrams in a n-gram chunk 
where n > 2. We have separated these phrases from 
the final list. 
Table 2: Inter-Annotator Agreement (in %) 
4.6 Experimental  Results 
We have used the standard IR matrices like Preci-
sion (P), Recall (R) and F-score (F) for evaluating 
the final results obtained from three modules. Hu-
man annotated list is used as the gold standard for 
the evaluation. The present system results are 
shown in Table 3.  These results are compared with 
the statistical baseline system described in (Cha-
kraborty, 2010). Our baseline system is reported 
with the precision of 39.64%. The predefined thre-
shold has been varied to catch individual results in 
each case. Increasing Recall in accordance with the 
increment of cut-off infers that the maximum 
numbers of MWEs are identified in a wide range 
of threshold. But the Precision does not increase 
considerably. It shows that the higher cut-off de-
grades the performance. The reasonable results for 
Precision and Recall have been achieved in case of 
cosine-similarity at the cut-off value of 0.5 where 
Euclidean distance and WordNet Similarity give 
maximum precision at cut-off values of 0.4 and 0.5 
respectively. In all cases, our system outperforms 
the baseline system.   
It is interesting to observe that English WordNet 
becomes a very helpful tool to identify Bengali 
MWEs. WordNet detects maximum MWEs cor-
rectly at the cut-off of 0.5. Baldwin et al, (2003) 
suggested that WordNet::Similarity measure is ef-
fective to identify empirical model of Multiword 
Expression Decomposability. This is also proved 
in this experiment as well and even for Bengali 
language. There are also candidates with very low 
value of similarity between their constituents (for 
example, ganer gajat (earth of song, affectionate 
of song), yet they are discarded from this experi-
ment because of their low frequency of occurrence 
in the corpus which could not give any judgment 
regarding collocation. Whether such an unexpec-
tedly low frequent high decomposable elements 
warrant an entry in the lexicon depends on the type 
of the lexicon being built. 
5 Conclusions 
We hypothesized that sense induction by analyzing 
synonymous sets can assist the identification of 
Multiword Expression. We have introduced an 
unsupervised approach to explore the hypothesis 
and have shown that clustering technique along 
with similarity measures can be successfully em-
ployed to perform the task. This experiment addi-
tionally contributes to the following scenarios - (i) 
Clustering of words having similar sense, (ii) Iden-
tification of MWEs for resource constraint lan-
guages and (iii) Reconstruction of Bengali 
monolingual dictionary towards the development 
of Bengali WordNet. However, in our future work, 
we will apply the present techniques for other type 
of MWEs (e.g., adjective-noun collocation, verbal 
MWEs) as well as for other languages.    
Acknowledgement 
The work reported in this paper is supported by a 
grant from the ?Indian Language to Indian Lan-
guage Machine Translation (IL-ILMT) System 
Phrase II?, funded by Department of Information 
and Technology (DIT), Govt. of India. 
References 
Agarwal, Aswini, Biswajit Ray, Monojit Choudhury, 
Sudeshna Sarkar and Anupam Basu. 2004. Automat-
ic Extraction of Multiword Expressions in Bengali: 
An Approach for Miserly Resource Scenario. In Pro-
ceedings of International Conference on Natural 
Language Processing (ICON), pp. 165-174. 
Baldwin, Timothy, Colin Bannard, Takaaki Tanaka and 
Dominic Widdows. 2003. An Empirical Model of 
Multiword Expression Decomposability. Proceed-
ings of the Association for Computational Linguis-
tics-2003, Workshop on Multiword Expressions: 
Analysis, Acquisition and Treatment, Sapporo, Japan, 
pp. 89?96. 
Ckakraborty, Tanmoy, 2010, Identification of Noun-
Noun (N-N) Collocations as Multi-Word Expressions 
in Bengali Corpus. Student Session, International 
Conference of Natural Language Processing (ICON), 
IIT Kharagpur, India 
MWEs 
[# 628] 
Agreement  between pair of annotators  
A1-A2      A2-A3    A1-A3        Avg 
KAPPA 87.23 86.14 88.78 87.38 
MASI 87.17 87.02 89.02 87.73 
12
Chakraborty, Tanmoy and Sivaji Bandyopadhyay. 2010. 
Identification of Reduplication in Bengali Corpus 
and their Semantic Analysis: A Rule Based Ap-
proach.  In proceedings of the Workshop on Multi-
word Expressions: from Theory to Applications 
(MWE 2010), 23rd International Conference on 
Computational Linguistics (COLING 2010), pp.73-
76, Beijing, China. 
Chattopadhyay Suniti K. 1992. Bhasa-Prakash Bangala 
Vyakaran, Third Edition.  
Church, Kenneth Wrad and Patrick Hans. 1990. Word 
Association Norms, Mutual Information and Lexico-
graphy.  Proceedings of 27th Association for Compu-
tational Linguistics (ACL), 16(1). pp. 22-29. 
Cohen, J. 1960. A coefficient of agreement for nominal 
scales. Educational and Psychological Measurement, 
vol. 20, pp. 37?46. 
Dunning, T. 1993. Accurate Method for the Statistic of 
Surprise and Coincidence. In Computational Linguis-
tics, pp. 61-74. 
Kilgarriff, Adam and Joseph Rosenzweig. 2000. 
Framework and results for English SENSEVAL. 
Computers and the Humanities. Senseval Special Is-
sue, 34(1-2). pp. 15-48. 
Korkontzelos,Ioannis and Suresh Manandhar. 2009. 
Detecting Compositionality in Multi-Word Expres-
sions. Proceedings of the Association for Computa-
tional Linguistics-IJCNLP, Singapore, pp. 65-68. 
Kunchukuttan F. A. and Om P. Damani. 2008. A Sys-
tem for Compound Noun Multiword Expression Ex-
traction for Hindi. Proceeding of 6th International 
Conference on Natural Language Processing 
(ICON). pp. 20-29. 
Passonneau, R.J. 2006. Measuring agreement on set-
valued items (MASI) for semantic and pragmatic an-
notation. Language Resources and Evaluation. 
Sag, Ivan A., Timothy Baldwin, Francis Bond, Ann 
Copestake and Dan Flickinger. 2002. Multiword Ex-
pressions: A Pain in the Neck for NLP. In Proceed-
ings of Conference on Intelligent Text Processing 
and Computational Linguistics (CICLING), pp. 1-15. 
Venkatapathy, Sriram and Aravind Joshi. 2005. Measur-
ing the relative compositionality of verb-noun (V-N) 
collocations by integrating features. Proceedings of 
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language 
Processing (HLT/EMNLP), Association for Compu-
tational Linguistics. pp. 899 - 906.  
13
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 19?27,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
Identifying Event ? Sentiment Association using Lexical Equivalence and 
Co-reference Approaches 
 
 
Anup Kumar Kolya1       Dipankar Das1      Asif Ekbal2      Sivaji Bandyopadhyay1 
1 Computer Science and Engineering Department, Jadavpur University, India  
2 Indian Institute of Technology, Patna (IITP), India 
anup.kolya@gmail.com, dipankar.dipnil2005@gmail.com 
asif.ekbal@gmail.com, sivaji_cse_ju@yahoo.com 
 
 
 
Abstract 
In this paper, we have identified event and sen-
timent expressions at word level from the sen-
tences of TempEval-2010 corpus and evaluated 
their association in terms of lexical equivalence 
and co-reference. A hybrid approach that con-
sists of Conditional Random Field (CRF) based 
machine learning framework in conjunction 
with several rule based strategies has been 
adopted for event identification within the 
TimeML framework. The strategies are based 
on semantic role labeling, WordNet relations 
and some handcrafted rules. The sentiment ex-
pressions are identified simply based on the 
cues that are available in the sentiment lexicons 
such as Subjectivity Wordlist, SentiWordNet 
and WordNet Affect. The identification of lexi-
cal equivalence between event and sentiment 
expressions based on the part-of-speech (POS) 
categories is straightforward. The emotional 
verbs from VerbNet have also been employed 
to improve the coverage of lexical equivalence. 
On the other hand, the association of sentiment 
and event has been analyzed using the notion of 
co-reference. The parsed dependency relations 
along with basic rhetoric knowledge help to 
identify the co-reference between event and 
sentiment expressions. Manual evaluation on 
the 171 sentences of TempEval-2010 dataset 
yields the precision, recall and F-Score values 
of 61.25%, 70.29% and 65.23% respectively.  
1 Introduction 
Event and Sentiment are two abstract entities 
closely coupled with each other from social, psy-
chological and commercial perspectives. Some 
kind of action that is going on or something that is 
being happened are addressed as events in general 
by the Natural Language (NL) researchers. The 
events are described in texts where the time, tem-
poral location and ordering of the events are speci-
fied. Event entities are represented by finite 
clauses, nonfinite clauses, nominalizations, event-
referring nouns, adjectives and even some kinds of 
adverbial clauses.  
On the other hand, text not only contains the in-
formative contents, but also some attitudinal pri-
vate information that includes sentiments. 
Nowadays, in the NLP communities, research ac-
tivities on sentiment analysis are in full swing. But, 
the identification of sentiment from texts is not an 
easy task as it is not open to any objective observa-
tion or verification (Quirk et al, 1985).  
Sometimes, similar or different types of senti-
ments are expressed on a single or multiple events. 
Sentiment of people over different events is impor-
tant as it has great influence on our society. Track-
ing users? sentiments about products or events or 
about political candidates as expressed in online 
forums, customer relationship management, stock 
market prediction, social networking etc., temporal 
question answering, document summarization, in-
formation retrieval systems are some of the impor-
tant applications of sentiment analysis.  
The identification of the association between 
event and sentiment is becoming more popular and 
interesting research challenge in the area of Natu-
ral Language Processing (NLP). Our present task is 
to identify the event and sentiment expressions 
from the text, analyze their associative relationship 
19
and investigate the insides of event-sentiment rela-
tions.  
For example, in the following sentence, the an-
notated events are, talked, sent and hijacked .But, 
it also shows the presence of underlying sentiments 
(as shown in underlined script) inscribed in the 
sentence. Here, sentiment helps to evoke the event 
property at lexical entity level (e.g. negative (-ve) 
sentiment for only the event word hijacked) as well 
as at context level (e.g. positive (+ve) sentiment 
associated with the event hijacked as the event 
word appears with the evaluative expression, re-
cover that gives the +ve polarity).  
 
?The prime minister of India told Friday that he 
has talked with top commander of Indian military 
force and sent a team to recover the host of Taj 
Hotel hijacked.?  
 
 Hence, we have organized the entire task into 
three different steps i) event identification, ii) sen-
timent expression identification and iii) identifica-
tion of event sentiment relationships at context 
level using lexical equivalence and co-reference 
approaches.  
In the first step, we propose a hybrid approach 
for event extraction from the text under the Tem-
pEval-2010 framework. Initially, we have used a 
Conditional Random Field (CRF) (Lafferty et al, 
2001) machine learning framework but we observe 
that it often makes the errors in extracting the 
events denoted by deverbial entities. This observa-
tion prompts us to employ several strategies in 
conjunction with machine learning. These strate-
gies are implemented based on semantic role labe-
ling, WordNet (Miller, 1990) and some 
handcrafted rules. We have experimented with the 
TempEval-2010 evaluation challenge setup (Kolya 
et al, 2010).  Evaluation results yield the preci-
sion, recall and F-measure values of approximate-
ly 93.00%, 96.00% and 94.47% respectively. This 
is approximately 12% higher F-measure in com-
parison to the best system (Llorens et al, 2010) of 
TempEval-2010. 
    On the other hand, the identification of the sen-
timent expressions is carried out based on the sen-
timent word. The words are searched in three 
different sentiment lexicons, the Subjectivity Word 
lists (Banea et al, 2008), SentiWordNet (Baccia-
nella et al, 2010) and WordNet Affect (Strapparava 
and Valitutti, 2004). The coarse-grained (positive 
and negative) as well as Ekman?s (1993) six fine- 
grained sentiment or emotion expressions (happy, 
sadness, anger, disgust, fear and surprise) are 
tagged in the corpus. As there is no annotation in 
the TemEval-2010 corpus for sentiment expres-
sions, the evaluation has been carried out by the 
authors and it achieves the precision, recall and F-
measure values of approximately 73.54%, 86.04% 
and 79.30% respectively 
Determining the lexical equivalence of event 
and sentiment expressions based on the POS prop-
erty at the lexical entity level is straightforward. If 
an event word also expresses the sentiment word, 
we have associated the corresponding sentiment 
type with the event word directly. In addition to the 
sentiment lexicons, the emotional verbs extracted 
from the VerbNet (Kipper-Schuler, 2005) are used 
in this phase. It improves the coverage of lexical 
equivalence by 12.76%. 
But, if the event and sentiment expressions oc-
cupy separate text spans in a sentence, we have 
adopted a co-reference approach for identifying 
their association. The parsed dependency relations 
along with some basic rhetoric components, such 
as nucleus, satellite and locus help in identifying 
the co-reference between the event and sentiment 
expressions. The text span containing sentiment 
word is hypothesized as the locus, the main effec-
tive part of the nucleus or satellite. The text span 
that reflects the primary goal of the writer is 
termed as nucleus (marked as ?{ }?) whereas the 
span that provides supplementary material is 
termed as satellite (marked as ?[ ]?). The distin-
guished identification of nucleus and satellite as 
well as their separation from each other is carried 
out based on the direct and transitive dependency 
relations, causal verbs, relaters or discourse mark-
ers. If both the locus and event are identified to-
gether in either nucleus or satellite, we term their 
association as co-referenced. If they occur sepa-
rately in nucleus and satellite and share at least one 
direct dependency relation, we consider their asso-
ciation as co-referenced.  
The evaluation of the lexical equivalence as 
well as co-reference systems has been performed 
by the authors. Primarily, the evaluation of both 
systems has been conducted on the random sam-
ples of 200 sentences of the TempEval-2010 train-
ing dataset.  Finally, the co-reference system 
achieves the precision, recall and F-Scores of 
20
61.25%, 70.29% and 65.23% respectively on 171 
sentences of the TempEval-2010 test corpus.  
The rest of the paper is organized as follows. 
Section 2 describes the related work. The event 
identification is discussed in Section 3. The identi-
fication of sentiment expressions is described in 
Section 4. Determination of lexical equivalence 
between event and sentiment expressions is speci-
fied in Section 5. The co-reference approach for 
identifying the association between event and sen-
timent is described in Section 6. Finally Section 7 
concludes the paper. 
2 Related Work 
The existing works on event extraction are based 
either on pattern-matching rules (Mani and Wilson 
2000), or on the machine learning approach (Bo-
guraev and Ando, 2005). But, still the problems 
persist with the high complexities involved in the 
proper extractions of events. The events expres-
sions were annotated in the TempEval 2007 
source in accordance with the TimeML standard 
(Pustejovsky et al, 2003). On the other hand, the 
Task B of TempEval-2010 evaluation challenge 
setup (Verhagen et al, 2010) was aimed at identi-
fying events from text. The best achieved result 
was obtained by (Llorens et al, 2010). 
The majority of subjective analysis methods 
that are related to emotion is based on textual key-
words spotting that use specific lexical resources. 
A lexicon that provides appraisal attributes for 
terms was constructed and the features were used 
for emotion classification (Whitelaw et al, 2005). 
The features along with the bag-of-words model 
give 90.2% accuracy. UPAR7 (Chaumartin, 2007), 
a rule-based system uses a combination of Word-
Net Affect and SentiWordNet. The system was 
semi-automatically enriched with the original trial 
data provided during the SemEval task (Strappara-
va and Mihalcea, 2007). SWAT (Katz et al, 2007) 
is another supervised system that uses a unigram 
model trained to annotate emotional content. 
Our motivation is that though events and senti-
ments are closely coupled with each other from 
social, psychological and commercial perspectives, 
very little attention has been given about their de-
tection and analysis. To the best of our knowledge, 
only a few tasks have been attempted (Fukuhara et 
al., 2007) (Das et al, 2010).  
Sometimes, the opinion topics are not neces-
sarily spatially coherent as there may be two opi-
nions in the same sentence on different topics, as 
well as opinions that are on the same topic sepa-
rated by opinions that do not share that topic 
(Stoyanov and Cardie 2008). The authors have es-
tablished their hypothesis by applying the co-
reference technique. Similarly, we have adopted 
the co-reference technique based on basic rhetoric 
components for identifying the association be-
tween event and sentiment expressions.  In addi-
tion to that, we have also employed the lexical 
equivalence approach for identifying their associa-
tion.  
3 Event Identification 
In this work, we propose a hybrid approach for 
event identification from the text under the Tem-
pEval-2010 framework. We use Conditional Ran-
dom Field (CRF) as the underlying machine 
learning algorithm. We observe that this machine 
learning based system often makes the errors in 
identifying the events denoted by deverbial enti-
ties. This observation prompts us to employ several 
strategies in conjunction with machine learning 
techniques. These strategies have been imple-
mented based on semantic role labeling, WordNet 
senses and some handcrafted rules.  
We have experiment with the TempEval-2010 
evaluation challenge setup (Kolya et al, 2010).  
Evaluation results yield the precision, recall and F-
measure values of approximately 93.00%, 96.00% 
and 94.47% respectively. This is approximately 
12% higher F-measure in comparison to the best 
system (Llorens et al, 2010) of TempEval-2010. 
3.1 CRF based Approach for Event Identifi-
cation 
We extract the gold-standard TimeBank features 
for events in order to train/test the CRF model. In 
the present work, we mainly use the various com-
binations of the following features:  
Part of Speech (POS) of event terms (e.g. Ad-
jective, Noun and Verb), Tense (Present, Past, Fu-
ture, Infinitive, Present part, Past part, or NONE), 
Aspect (Progressive, Perfective and Perfective 
Progressive or NONE), Class (Reporting, Percep-
tion, Aspectual, I_action, I_state, State, Occur-
rence), Stem (e.g., discount /s/).  
21
3.2 Use of Semantic Roles for Event Identifi-
cation 
We use an open source Semantic Role Labeler 
1(SRL) (Gildea et al, 2002) (Pradhan et al, 2004) 
to identify different features of the sentences. For 
each predicate in a sentence acting as event word, 
semantic roles extract all constituents, determining 
their arguments (agent, patient etc.) and adjuncts 
(locative, temporal etc.). Semantic roles can be 
used to detect the events that are the nominaliza-
tions of verbs such as agreement for agree or con-
struction for construct. Nominalizations (or, 
deverbal nouns) are commonly defined as nouns 
that are morphologically derived from verbs, 
usually by suffixation (Quirk et al, 1985). Event 
nominalizations often afford the same semantic 
roles as verbs and often replace them in written 
language (Gurevich et al, 2006).  Event nominali-
zations constitute the bulk of deverbal nouns.  The 
following example sentence shows how semantic 
roles can be used for event identification.  
 
[ARG1 All sites] were [TARGET inspected] to the satis-
faction of the inspection team and with full coope-
ration of Iraqi authorities, [ARG0 Dacey] [TARGET 
said]. 
 
   The extracted target words are treated as the 
event words. It has been observed that many of 
these target words are identified as the event ex-
pressions by the CRF model. But, there exists ma-
ny nominalised event expressions (i.e., deverbal 
nouns) that are not identified as events by the su-
pervised CRF. These nominalised expressions are 
correctly identified as events by SRL.  
3.3 Use of WordNet for Event Identification 
WordNet is mainly used to identify non-deverbal 
event nouns. We observed that the event entities 
like ?war?, ?attempt?, ?tour? are not properly identi-
fied. These words have noun (NN) POS informa-
tion as the previous approaches, i.e., CRF and SRL 
can only identify those event words that have verb 
(VB) POS information. We know from the lexical 
information of WordNet that the words like ?war? 
and ?tour? are generally used as both noun and 
verb forms in the sentence. Therefore, we have 
                                                        
1 http://cemantix.org/assert.html 
designed the following two rules based on the 
WordNet: 
 
Rule 1: The word tokens having Noun (NN) POS 
categories are looked into the WordNet. If it ap-
pears in the WordNet with noun and verb senses, 
then that word token is considered as an event.  For 
example, war has both noun and verb senses in the 
WordNet, and hence war is considered as an event.  
 
Rule 2: The stems of the noun word tokens are 
looked into the WordNet. If one of the WordNet 
senses is verb then the token is considered as verb. 
For example, the stem of proposal, i.e., propose 
has two different senses, noun and verb in the 
WordNet, and thus it is considered as an event.  
3.4    Use of Rules for Event Identification 
Here, we mainly concentrate on the identification 
of specific lexical classes like ?inspection? and 
?resignation?. These can be identified by the suf-
fixes such as (?-ci?n?), (?-tion?) or (?-ion?), i.e., the 
morphological markers of deverbal derivations. 
  Initially, we have employed the CRF based Stan-
ford Named Entity (NE) tagger2 on the TempEval-
2 test dataset. The output of the system is tagged 
with Person, Location, Organization and Other 
classes. The words starting with the capital letters 
are also considered as NEs. Thereafter, we came 
up with the following rules for event identification: 
  
Cue-1: The deverbal nouns are usually identified 
by the suffixes like ?-tion?, ?-ion?, ?-ing? and ?-ed? 
etc. The nouns that are not NEs, but end with these 
suffixes are considered as the event words. 
  
Cue 2: The verb-noun combinations are searched 
in the sentences of the test set. The non-NE noun 
word tokens are considered as the events.  
 
Cue 3: Nominals and non-deverbal event nouns 
can be identified by the complements of aspectual 
PPs headed by prepositions like during, after and 
before, and complex prepositions such as at the 
end of and at the beginning of etc.  The next word 
token(s) appearing after these clue word(s) or 
phrase(s) are considered as events.  
                                                        
2 http://nlp.stanford.edu/software/CRF-NER.shtml 
22
Cue 4: The non-NE nouns occurring after the ex-
pressions such as frequency of, occurrence of and 
period of are most probably the event nouns. 
 
Cue 5: Event nouns can also appear as objects of 
aspectual and time-related verbs, such as have be-
gun a campaign or have carried out a campaign 
etc. The non-NEs that appear after the expressions 
like ?have begun a?, ?have carried out a? etc.  are 
also denoted as the events.   
4 Sentiment Expression Identification 
Sentiment is an important cue that effectively de-
scribes the events associated with it. The binary 
classification of the sentiments (positive and nega-
tive) as well as the fine-grained categorization into 
Ekman?s (1993) six emotions is therefore em-
ployed for identifying the sentiment expressions. 
200 sentences are randomly selected from the 
training dataset of the TempEval-2010 corpus. 
These sentences have been considered as our de-
velopment set. On the other hand, 171 sentences 
were already provided as the test sentences in the 
TempEval-2010 evaluation challenge.   
The events are already annotated in the Tem-
pEval-2010 corpus. But, no sentiment or emotion 
related annotation is available in the corpus. 
Hence, we have annotated the sentiment expres-
sions at word level in a semi-supervised way. The 
word level entities are tagged by their coarse and 
fine grained sentiment tags using the available sen-
timent related lexical resources. Then the automat-
ic annotation has been evaluated manually by the 
authors. The semi-supervised sentiment annotation 
agreements were 90.23% for the development set 
and 92.45% for the test sets respectively.  
4.1 Lexicon based Approach 
The tagging of the evaluative expressions or more 
specifically the sentiment expressions on the Tem-
pEval-2010 corpus has been carried out using the 
available sentiment lexicons. We passed the sen-
tences through three sentiment lexicons, Subjectivi-
ty Wordlists (Banea et al, 2008), SentiWordNet 
(Baccianella et al, 2010) and WordNet Affect 
(Strapparava and Valitutti, 2004). Subjectivity 
Wordlist assigns words with the strong or weak 
subjectivity and prior polarities of types positive, 
negative and neutral. SentiWordNet, used in opi-
nion mining and sentiment analysis, assigns three 
sentiment scores such as positive, negative and 
objective to each synset of WordNet. WordNet Af-
fect, a small well-used lexical resource but valua-
ble for its affective annotation contains the words 
that convey emotion.  
The algorithm is that, if a word in a sentence is 
present in any of these resources; the word is 
tagged as the sentiment expression. But, if any 
word is not found in any of them, each word of the 
sentence is passed through the WordNet Morpho-
logical analyzer (Miller, 1990) to identify its root 
form and the root form is searched through the re-
sources again. If the root form is found, the corres-
ponding word is tagged as sentiment expression 
accordingly.  
The identified sentiment expressions have been 
evaluated by the authors and it achieves the preci-
sion, recall and F-Score of 73.54%, 86.04% and 
79.30%, respectively on a total of 171 test sen-
tences of the TempEval-2010 corpus.   
The identification of event words that also ex-
press sentiment is straightforward. But, the prob-
lem arises when the event and sentiment 
expressions are present separately in a sentence 
and the sentiment is either closely associated with 
the event or affects it. In case of the former, we 
have adopted the approach of lexical equivalence 
between the event and sentiment entities whereas 
the co-reference technique has been introduced for 
resolving the latter case.  
5 Lexical Equivalence between Event and 
Sentiment Expressions  
It is observed that in general the verbs, nouns and 
adjectives represent events. The sentences are 
passed through an open source Stanford Maximum 
Entropy based POS tagger (Manning and Toutano-
va, 2000). The best reported accuracy for the POS 
tagger on the Penn Treebank is 96.86% overall and 
86.91% on previously unseen words. Our objective 
was to identify the event words that also express 
sentiments. Hence, we have identified the event 
words that have also been tagged as the sentiment 
expressions. The coverage of these lexical re-
sources in identifying the event sentiment associa-
tion is shown in Table 1. 
On the other hand, not only the adjectives or 
nouns, the sentiment or emotional verbs play an 
important role in identifying the sentiment expres-
23
sions. Hence, in addition to the above mentioned 
sentiment resources, we have also incorporated 
English VerbNet (Kipper-Schuler, 2005) for the 
automatic annotation process. VerbNet associates 
the semantics of a verb with its syntactic frames 
and combines traditional lexical semantic informa-
tion such as thematic roles and semantic predi-
cates, with syntactic frames and selectional 
restrictions. Verb entries in the same VerbNet class 
share common syntactic frames and thus they are 
believed to have the same syntactic behavior. For 
example, the emotional verbs ?love? and ?enjoy? 
are members of the admire-31.2-1 class and ?en-
joy? also belongs to the class want-32.1-1.  
The XML files of VerbNet are preprocessed to 
build up a general list that contains all member 
verbs and their available syntax information re-
trieved from VerbNet. The main criterion for se-
lecting the member verbs as sentiment expressions 
is the presence of ?emotional_state? type predicate 
in their frame semantics. The frequencies of the 
event words matched against the above said four 
resources are shown in Table 1.  It has been ob-
served that the adjective events are not identified 
by the lexical resources as their frequency in the 
test corpus was very low. But, the lexical coverage 
has been improved by 12.76% by incorporating 
VerbNet. 
 
Resources Noun   Adjective  Verb 
#114    #4              #380 
Subjectivity Wordlists 
SentiWordNet 
WordNet Affect List 
VerbNet (emotional 
verbs) 
24            --             35 
32            --             59  
12            --             25 
 --            --             79 
Accuracy (in %) 59.64                    52.57 
 
Table 1: Results of Lexical Equivalence between 
Event and Sentiment based on different resources  
6 Co-reference between Event and Senti-
ment Expressions  
The opinion and/or sentiment topics are not neces-
sarily spatially coherent as there may be two opi-
nions in the same sentence on different topics. 
Sometimes, the opinions that are on the same topic 
are separated by opinions that do not share that 
topic (Stoyanov and Cardie, 2008). We observe the 
similar situation in case of associating sentiments 
with events. Hence, the hypothesis for opinion top-
ic is established for sentiment events by applying 
the co-reference technique along with the rhetori-
cal structure. We have proposed two different sys-
tems for identifying the association of sentiments 
with the events at context level. 
6.1 Baseline Co-reference System 
The baseline system has been developed based on 
the object information present in the dependency 
relations of the parsed sentences. Stanford Parser 
(Marneffe et al, 2006), a probabilistic lexicalized 
parser containing 45 different part of speech (POS) 
tags of Pen Treebank tagset  has been used to get 
the parsed sentences and dependency relations. 
The dependency relations are checked for the pre-
dicates ?dobj? so that the related components 
present in the predicate are considered as the prob-
able candidates for the events.  
If a dependency relation contains both the event 
and sentiment words, we have considered the pres-
ence of co-reference between them. But, it has 
been observed that the event and sentiment expres-
sions are also present in two different relations that 
share a common word element. Hence, if the event 
and sentiment words appear in two different rela-
tions but both of the relations contain at least one 
common element, the event and sentiment words 
are termed as co-referenced.    
Overall, the baseline co-reference system 
achieves the precision, recall and F-Scores of 
40.03%, 46.10% and 42.33% for event-sentiment 
co-reference identification. For example in the fol-
lowing sentence, the writer?s direct as well as indi-
rect emotional intentions are reflected by 
mentioning one or more topics or events (spent, 
thought) and their associated sentiments (great).  
 
?When Wong Kwan spent seventy million dol-
lars for this house, he thought it was a great deal.? 
 
The baseline co-reference system fails to asso-
ciate the sentiment expressions with their corres-
ponding event expressions. Hence, we aimed for 
the rhetoric structure based co-reference system to 
identify their association. 
6.2  Rhetoric Co-reference System 
The distribution of events and sentiment expres-
sions in different text spans of a sentence needs the 
24
analysis of sentential structure. We have incorpo-
rated the knowledge of Rhetorical Structure 
Theory (RST) (Mann and Thompson 1987) for 
identifying the events that are co-referred by their 
corresponding sentiment expressions.  
The theory maintains that consecutive discourse 
elements, termed text spans, are related by a rela-
tively small set (20?25) of rhetorical relations. 
But, instead of identifying the rhetorical relations, 
the present task acquires the basic and coarse rhe-
torical components such as locus, nucleus and sa-
tellite from a sentence.  These rhetoric clues help 
in identifying the individual event span associated 
with the span denoting the corresponding senti-
ment expression in a sentence. The text span that 
reflects the primary goal of the writer is termed as 
nucleus (marked as ?{ }?) whereas the span that 
provides supplementary material is termed as satel-
lite (marked as ?[ ]?). For example, the nucleus and 
satellite textual spans are shown in the following 
sentence as, 
 
{Traders said the market remains extremely 
nervous} because [the wild swings seen on the 
New York Stock Exchange last week]. 
 
The event or topic of an opinion or sentiment 
depends on the context in which the associated 
opinion or sentiment expression occurs (Stoyanov 
and Cardie 2008). Considering the similar hypo-
thesis in case of events instead of topics, the co-
reference between an event and a sentiment ex-
pression is identified from the nucleus and/or satel-
lite by positioning the sentiment expression as 
locus. We have also incorporated the WordNet?s 
(Miller 1990) morphological analyzer to identify 
the stemmed forms of the sentiment words.  
The preliminary separation of nucleus from sa-
tellite was carried out based on the list of frequent-
ly used causal keywords (e.g., as, because, that, 
while, whether etc) and punctuation markers (,) (!) 
(?).The discourse markers and causal verbs are 
also the useful clues if they are explicitly specified 
in the text. The identification of discourse markers 
from written text itself is a research area (Azar 
1999). Hence, our task was restricted to identify 
only the explicit discourse markers that are tagged 
by conjunctive_() or mark_() type dependency re-
lations of the parsed constituents. The dependency 
relations containing conjunctive markers (e.g., 
conj_and(), conj_or(), conj_but()) were considered 
for separating nucleus from satellite if the markers 
are present in between two successive clauses. 
Otherwise, the word token contained in the 
mark_() type dependency relation was considered 
as a discourse marker. 
The list of causal verbs is prepared by 
processing the XML files of VerbNet. If any Verb-
Net class file contains any frame with semantic 
type as Cause, we collect the member verbs of that 
XML class file and term the member verbs as 
causal verbs. We used a list that contains a total 
number of 253 causal verbs.  
If any clause tagged as S or SBAR in the parse 
tree contains any causal verb, that clause is consi-
dered as the nucleus and the rest of the clauses de-
note the satellites. Considering the basic theory of 
rhetorical structure (Mann and Thompson 1987), 
the clauses were separated into nucleus and satel-
lite to identify the event and sentiment expressions. 
The direct dependency is identified based on the 
simultaneous presence of locus and the event word 
in the same dependency relation whereas the tran-
sitive dependency is verified if the word is con-
nected to locus and event via one or more 
intermediate dependency relations.  
If the event and sentiment words are together 
present in either nucleus or satellite, the associa-
tion between the two expressions is considered as 
co-referenced. If they occur in nucleus and satellite 
separately, but the event and sentiment words are 
present in at least one direct dependency relation, 
the expressions are termed as co-referenced.  
In the previous example, the event expressions, 
?said? and ?remains? are associated with the sen-
timent expression ?nervous? as both the event ex-
pressions share the direct dependency relations 
?cop(nervous-7, remains-5)? and ?ccomp(said-2, 
nervous-7)? in the nucleus segment. Similarly, the 
event word, ?seen? and sentiment word ?wild? are 
present in the satellite part and they share a direct 
dependency relation ?partmod(swings-12, seen-
13)?. But, no direct dependency relation is present 
between the ?nervous? and ?seen? or ?said? and 
?wild? or ?remains? and ?wild?.  
6.3 Results 
Though the event annotation is specified in the 
TempEval-2010 corpus, the association between 
the event and sentiment expressions was not speci-
fied in the corpus. Hence, we have carried out the 
25
evaluation manually. The 200 random samples of 
the training set that were used in sentiment expres-
sion identification task have been considered as 
our development set. The Evaluation Vectors 
(EvalV) are prepared manually from each sentence 
of the development and test sets. The vectors 
<EvExp, SentiExp> are filled with the annotated 
events and sentiment expressions by considering 
their association. The annotation of sentiment ex-
pressions using the semi-supervised process has 
been described in Section 4. 
    The rule based baseline and rhetoric based co-
reference systems identify the event and sentiment 
expressions from each sentence and stores them in 
a Co-reference Vector (CorefV). The evaluation is 
carried out by comparing the system generated Co-
reference Vectors (CorefV) with their correspond-
ing Evaluation Vectors (EvalV). The evaluation 
results on 171 test sentences are shown in Table 2. 
 
Co-reference  
Approaches 
Prec.     Rec.    F-Score 
(in %) 
Baseline System 40.03    46.10       42.33 
Rhetoric System 61.25    70.29       65.23 
 
Table 2: Precision (Prec.), Recall (Rec.) and F-
Scores (in %) of the event-sentiment co-reference 
systems  
 
Overall, the precision, recall and F-Scores are 
61.25%, 70.29% and 65.23% for event-sentiment 
co-reference identification using rhetoric clues. 
Though the co-reference technique performs satis-
factorily for identifying the event-sentiment co-
reference, the problem arises in distinguishing the 
corresponding spans of events from an overlapped 
text span of multi-word tokens.  
7 Conclusion  
In this present work, we have identified event and 
sentiment expressions at word level from the sen-
tences of TempEval-2010 corpus and evaluated 
their association in terms of lexical equivalence 
and co-reference. It has been observed that the lex-
ical equivalence based on lexicons performs satis-
factorily but overall, the co-reference entails that 
the presence of indirect affective clues can also be 
traced with the help of rhetoric knowledge and de-
pendency relations. The association of the senti-
ments with their corresponding events can be used 
in future concerning the time based sentiment 
change over events.  
Acknowledgments 
The work is supported by a grant from the India-
Japan Cooperative Programme (DST-JST) 2009 
Research project entitled ?Sentiment Analysis 
where AI meets Psychology? funded by Depart-
ment of Science and Technology (DST), Govern-
ment of India. 
References  
Baccianella Stefano, Esuli Andrea and Sebas-tiani Fa-
brizio. 2010. SentiWordNet 3.0: An Enhanced Lexi-
cal Re-source for Sentiment Analysis and Opinion 
Mining. In Proceedings of the 7th Conference on 
Language Resources and Evaluation, pp. 2200-2204. 
Banea, Carmen, Mihalcea Rada, Wiebe Janyce. 2008.  
A Bootstrapping Method for Building Subjectivity 
Lexicons for Languages with Scarce Resources. The 
Sixth International Conference on Language Re-
sources and Evaluation. 
Boguraev, B., Ando, R. K. 2005. TimeBank-
DrivenTimeML Analysis. Annotating, Extracting and 
Reasoning about Time and Events 2005. 
Chaumartin, F. 2007. Upar7: A knowledge-based sys-
tem for headline sentiment tagging. SemEval-200,  
Czech Republic. 
Ekman Paul. 1993. An argument for basic emotions, 
Cognition and Emotion, 6(3-4):169-200. 
Fukuhara T., Nakagawa, H. and Nishida, T. 2007. Un-
derstanding Sentiment of People from News Articles: 
Temporal Sentiment Analysis of Social Events. 
ICWSM?2007, Boulder, Colorado. 
Gildea, D. and Jurafsky, D. 2002. Automatic Labeling 
of Semantic Roles. Computational Linguistics, 
28(3):245?288. 
Gurevich, O., R. Crouch, T. King, and V. de Paiva. 
2006. Deverbal Nouns in Knowledge Representation. 
Proceedings of FLAIRS, pages 670?675, Melbourne 
Beach, FL. 
Katz, P., Singleton, M. and Wicentowski, R. 2007. 
Swat-mp: the semeval-2007 systems for task 5 and 
task SemEval-2007.  
Kipper-Schuler, K. 2005.  VerbNet: A broad-coverage, 
comprehensive verb lexicon. Ph.D. thesis, Computer 
and Information Science Dept., University of Penn-
sylvania, Philadelphia, PA. 
26
Kolya, A., Ekbal, A. and Bandyopadhyay, S. 2010. 
JU_CSE_TEMP: A First Step towards Evaluating 
Events, Time Expressions and Temporal Relations. 
In Proceedings of the 5th International Workshop on 
Semantic Evaluation, ACL 2010, July 15-16, Swe-
den, pp. 345?350. 
Lafferty, J., McCallum, A.K., Pereira, F. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. International 
Conference on Machine Learning. 
Llorens Hector, Estela Saquete, Borja Navarro. 2010. 
TIPSem (English and Spanish): Evaluating CRFs and 
Semantic Roles. Proceedings of the 5th International 
Workshop on Semantic Evaluation, ACL 2010, pages 
284?291, Uppsala, Sweden, 15-16 July 2010. 
Mani, I., and Wilson G. 2000. Processing of News. In 
Proceedings of the 38th Annual Meeting of the Asso-
ciation for Computational Linguistics, pp. 69-76. 
Mann, W. and S. Thompson. 1987. Rhetorical Structure 
Theory: Description and Construction of Text Struc-
ture. In G. Kempen (ed.), Natural Language Genera-
tion, Martinus Nijhoff, The Hague, pp. 85?96. 
Manning Christopher and Toutanova, Kristina. 2000. 
Enriching the Knowledge Sources Used in a Maxi-
mum Entropy Part-of-Speech Tagger. Proceedings of 
the Joint SIGDAT Conference on Empirical Methods 
in Natural Language Processing and Very Large 
Corpora (EMNLP/VLC)  
Marneffe, Marie-Catherine de, Bill MacCartney, and 
Christopher D.Manning. 2006. Generating Typed 
Dependency Parses from Phrase Structure Parses. 5th 
International Conference on Language Resources 
and Evaluation.  
Miller George A. 1990. WordNet: An on-line lexical 
database. International Journal of Lexicography, 
3(4): 235?312 
Pradhan S., Wayne W., Hacioglu, K., Martin, J.H. and 
Jurafsky, D. 2004. Shallow Semantic Parsing using 
Support Vector Machines. Proceedings of the Human 
Language Technology Conference/North American 
chapter of the Association for Computational Lin-
guistics annual meeting Boston, MA, May 2-7. 
Pustejovsky, J., Castano, J., Ingria, R., Sauri, R., Gai-
zauskas, R., Setzer, A., Katz, G. and Radev, D. 
TimeML: Robust specification of event and temporal 
expressions in text. In AAAI Spring Symposium on 
New Directions in Question-Answering, pp. 28-34, 
CA, 2003. 
Quirk, R., Greenbaum, S. Leech, G. and Svartvik, J. 
1985. A Comprehensive Grammar of the English 
Language. Longman.  
Strapparava C. and Valitutti, A. 2004. Wordnet-affect: 
an affective extension of wordnet. In 4th Internation-
al Conference on Language Resources and Evalua-
tion, pp. 1083-1086. 
Strapparava Carlo and Mihalcea Rada. 2007. SemEval-
2007 Task 14: Affective Text. 45th Aunual Meeting 
of Association for Computational linguistics. 
Stoyanov, V., and Cardie, C. 2008. Annotating topics of 
opinions. In Proceedings of LREC.  
 
27
Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 38?42,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Shared task system description: Measuring the Compositionality of 
Bigrams using Statistical Methodologies 
Tanmoy Chakraborty, Santanu Pal, Tapabrata Mondal, Tanik Saikh, 
 Sivaji Bandyopadhyay 
Department of Computer Science and Engineering 
Jadavpur University 
its_tanmoy@yahoo.co.in, santanu.pal.ju@gmail.com, 
tapabratamondal@gmail.com, tanik4u@gmail.com, 
sivaji_cse_ju@yahoo.com 
 
Abstract 
The measurement of relative 
compositionality of bigrams is crucial to 
identify Multi-word Expressions 
(MWEs) in Natural Language 
Processing (NLP) tasks. The article 
presents the experiments carried out as 
part of the participation in the shared 
task ?Distributional Semantics and 
Compositionality (DiSCo)? organized as 
part of the DiSCo workshop in ACL-
HLT 2011. The experiments deal with 
various collocation based statistical 
approaches to compute the relative 
compositionality of three types of 
bigram phrases (Adjective-Noun, Verb-
subject and Verb-object combinations). 
The experimental results in terms of both 
fine-grained and coarse-grained 
compositionality scores have been 
evaluated with the human annotated gold 
standard data. Reasonable results have 
been obtained in terms of average point 
difference and coarse precision.  
1 Introduction 
The present work examines the relative 
compositionality of Adjective-Noun (ADJ-NN; 
e.g., blue chip), Verb-subject (V-SUBJ; where 
noun acting as a subject of a verb, e.g., name 
imply) and Verb-object (V-OBJ; where noun 
acting as an object of a verb, e.g., beg question) 
combinations using collocation based statistical 
approaches. Measuring the relative 
compositionality is useful in applications such as 
machine translation where the highly non-
compositional collocations can be handled in a 
special way (Hwang and Sasaki, 2005). 
Multi-word expressions (MWEs) are 
sequences of words that tend to co-occur more 
frequently than chance and are either 
idiosyncratic or decomposable into multiple 
simple words (Baldwin, 2006). Deciding 
idiomaticity of MWEs is highly important for 
machine translation, information retrieval, 
question answering, lexical acquisition, parsing 
and language generation. Compositionality 
refers to the degree to which the meaning of a 
MWE can be predicted by combining the 
meanings of its components. Unlike syntactic 
compositionality (e.g. by and large), semantic 
compositionality is continuous (Baldwin, 2006).   
Several studies have been carried out for 
detecting compositionality of noun-noun MWEs 
using WordNet hypothesis (Baldwin et al, 
2003), verb-particle constructions using 
statistical similarities (Bannard et al, 2003; 
McCarthy et al, 2003) and verb-noun pairs 
using Latent Semantic Analysis (Katz and 
Giesbrecht, 2006).  
Our contributions are two-fold: firstly, we 
experimentally show that collocation based 
statistical compositionality measurement can 
assist in identifying the continuum of 
compositionality of MWEs. Secondly, we show 
that supervised weighted parameter tuning 
results in accuracy that is comparable to the best 
manually selected combination of parameters.  
38
2 Proposed Methodologies 
The present task was to identify the numerical 
judgment of compositionality of individual 
phrase. The statistical co-occurrence features 
used in this experiment are described.     
Frequency:  If two words occur together 
quite frequently, the lexical meaning of the 
composition may be different from the 
combination of their individual meanings. The 
frequency of an individual phrase is directly 
used in the following methods. 
Point-wise Information (PMI): An 
information-theoretic motivated measure for 
discovering interesting collocations is point-wise 
mutual information (Church and Hanks, 1990). 
It is originally defined as the mutual information 
between particular events X and Y and in our 
case the occurrence of particular words, as 
follows: 
   = log ,. ? log ,.   1  
PMI represents the amount of information 
provided by the occurrence of the event 
represented by X about the occurrence of the 
event represented by Y. 
T-test:  T-test has been widely used for 
collocation discovery. This statistical test tells us 
the probability of a certain constellation 
(Nugues, 2006). It looks at the mean and 
variance of a sample of measurements. The null 
hypothesis is that the sample is drawn from a 
distribution with mean. T-score is computed 
using the equation (2): 
,  = Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 80?86,
24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational Linguistics
Developing Japanese WordNet Affect for Analyzing Emotions 
 
 
Yoshimitsu Torii1       Dipankar Das2       Sivaji Bandyopadhyay2      Manabu Okumura1 
1Precision and Intelligence Laboratory, Tokyo Institute of Technology, Japan 
2Computer Science and Engineering Department, Jadavpur University, India 
torii@lr.pi.titech.ac.jp, dipankar.dipnil2005@gmail.com 
sivaji_cse_ju@yahoo.com, oku@pi.titech.ac.jp 
 
Abstract 
This paper reports the development of Jap-
anese WordNet Affect from the English 
WordNet Affect lists with the help of Eng-
lish SentiWordNet and Japanese WordNet. 
Expanding the available synsets of the 
English WordNet Affect using SentiWord-
Net, we have performed the translation of 
the expanded lists into Japanese based on 
the synsetIDs in the Japanese WordNet. A 
baseline system for emotion analysis of 
Japanese sentences has been developed 
based on the Japanese WordNet Affect. The 
incorporation of morphology improves the 
performance of the system. Overall, the 
system achieves average precision, recall 
and F-scores of 32.76%, 53% and 40.49% 
respectively on 89 sentences of the Japa-
nese judgment corpus and 83.52%, 49.58% 
and 62.22% on 1000 translated Japanese 
sentences of the SemEval 2007 affect sens-
ing test corpus. Different experimental out-
comes and morphological analysis suggest 
that irrespective of the google translation 
error, the performance of the system could 
be improved by enhancing the Japanese 
WordNet Affect in terms of coverage.  
1 Introduction 
Emotion analysis, a recent sub discipline at the 
crossroads of information retrieval (Sood et al, 
2009) and computational linguistics (Wiebe et al, 
2006) is becoming increasingly important from 
application view points of affective computing.  
The majority of subjective analysis methods that 
are related to emotion is based on textual keywords 
spotting that use specific lexical resources. Senti-
WordNet (Baccianella et al, 2010) is a lexical re-
source that assigns positive, negative and objective 
scores to each WordNet synset (Miller, 1995). Sub-
jectivity wordlist (Banea et al, 2008) assigns 
words with the strong or weak subjectivity and 
prior polarities of types positive, negative and neu-
tral.  Affective lexicon (Strapparava and Valitutti, 
2004), one of the most efficient resources of emo-
tion analysis, contains emotion words. To the best 
of our knowledge, these lexical resources have 
been created for English. A recent study shows that 
non-native English speakers support the growing 
use of the Internet1. Hence, there is a demand for 
automatic text analysis tools and linguistic re-
sources for languages other than English.  
In the present task, we have prepared the Japa-
nese WordNet Affect from the already available 
English WordNet Affect (Strapparava and Valitutti, 
2004). Entries in the English WordNet Affect are 
annotated using Ekman?s (1993) six emotional 
categories (joy, fear, anger, sadness, disgust, sur-
prise). The collection of the English WordNet Af-
fect 2 synsets that are used in the present work was 
provided as a resource in the ?Affective Text? 
shared task of SemEval-2007 Workshop.  
The six WordNet Affect lists that were provided 
in the shared task contain only 612 synsets in total 
with 1536 words. The words in each of the six 
emotion lists have been observed to be not more 
than 37.2% of the words present in the correspond-
ing SentiWordNet synsets. Hence, these six lists 
are expanded with the synsets retrieved from the 
                                                        
1 http://www.internetworldstats.com/stats.htm 
2 http://www.cse.unt.edu/~rada/affectivetext/ 
80
English SentiWordNet (Baccianella et al, 2010). 
We assumed that the new sentiment bearing words 
in English SentiWordNet might have some emo-
tional connotation in Japanese even keeping their 
part-of-speech (POS) information unchanged. The 
numbers of entries in the expanded word lists are 
increased by 69.77% and 74.60% at synset and 
word levels respectively. We have mapped the 
synsetID of the WordNet Affect lists with the syn-
setID of the WordNet 3.03. This mapping helps in 
expanding the WordNet Affect lists with the recent 
version of SentiWordNet 3.0 4 as well as translating 
with the Japanese WordNet (Bond et al, 2009). 
Some affect synsets (e.g., 00115193-a huffy, mad, 
sore) are not translated into Japanese as there are 
no equivalent synset in the Japanese WordNet.  
Primarily, we have developed a baseline system 
based on the Japanese WordNet Affect and carried 
out the evaluation on a Japanese judgement corpus 
of 89 sentences. The system achieves the average 
F-score of 36.39% with respect to six emotion 
classes. We have also incorporated an open source 
Japanese morphological analyser 5 . The perform-
ance of the system has been increased by 4.1% in 
average F-score with respect to six emotion classes. 
Scarcity of emotion corpus in Japanese moti-
vated us to apply an open source google translator6 
to build the Japanese emotion corpus from the 
available English SemEval-2007 affect sensing 
corpus. The baseline system based on the Japanese 
WordNet Affect achieves average precision, recall 
and F-score of 83.52%, 49.58% and 62.22% re-
spectively on 1000 translated test sentences. The 
inclusion of morphological processing improves 
the performance of the system. Different experi-
ments have been carried out by selecting different 
ranges of annotated emotion scores. Error analysis 
suggests that though the system performs satisfac-
torily in identifying the sentential emotions based 
on the available words of the Japanese WordNet 
Affect, the system suffers from the translated ver-
sion of the corpus. In addition to that, the Japanese 
WordNet Affect also needs an improvement in 
terms of coverage.  
 The rest of the paper is organized as follows. 
Different developmental phases of the Japanese 
WordNet Affect are described in Section 3. Prepa-
                                                        
3 http://wordnet.princeton.edu/wordnet/download/ 
4 http://sentiwordnet.isti.cnr.it/ 
5 http://mecab.sourceforge.net/ 
6 http://translate.google.com/# 
ration of the translated Japanese corpus, different 
experiments and evaluations based on morphology 
and the annotated emotion scores are elaborated in 
Section 4. Finally Section 5 concludes the paper. 
2 Related Works  
The extraction and annotation of subjective terms 
started with machine learning approaches (Hat-
zivassiloglou and McKeown, 1997). Some well 
known sentiment lexicons have been developed, 
such as subjective adjective list (Baroni and Veg-
naduzzo, 2004), English SentiWordNet (Esuli et. 
al., 2006), Taboada?s adjective list (Voll and 
Taboada, 2007), SubjectivityWord List (Banea et 
al., 2008) etc. Andreevskaia and Bergler (2006) 
present a method for extracting positive or negative 
sentiment bearing adjectives from WordNet using 
the Sentiment Tag Extraction Program (STEP). 
The proposed methods in (Wiebe and Riloff, 2006) 
automatically generate resources for subjectivity 
analysis for a new target language from the avail-
able resources for English. On the other hand, an 
automatically generated and scored sentiment lexi-
con, SentiFul (Neviarouskaya et al, 2009), its 
expansion, morphological modifications and dis-
tinguishing sentiment features also shows the con-
tributory results.   
But, all of the above mentioned resources are in 
English and have been used in coarse grained sen-
timent analysis (e.g., positive, negative or neutral). 
The proposed method in (Takamura et al, 2005) 
extracts semantic orientations from a small number 
of seed words with high accuracy in the experi-
ments on English as well as Japanese lexicons. 
But, it was also aimed for sentiment bearing words. 
Instead of English WordNet Affect (Strapparava 
and Valitutti, 2004), there are a few attempts in 
other languages such as, Russian and Romanian 
(Bobicev et al, 2010), Bengali (Das and Bandyop-
adhyay, 2010) etc. Our present approach is similar 
to some of these approaches but in contrast, we 
have evaluated our Japanese WordNet Affect on the 
SemEval 2007 affect sensing corpus translated into 
Japanese. In recent trends, the application of me-
chanical turk for generating emotion lexicon (Mo-
hammad and Turney, 2010) shows promising 
results. In the present task, we have incorporated 
the open source, available and accessible resources 
to achieve our goals.   
81
3 Developmental Phases  
3.1 WordNet Affect 
The English WordNet Affect, based on Ekman?s six 
emotion types is a small lexical resource compared 
to the complete WordNet but its affective annota-
tion helps in emotion analysis. Some collection of 
WordNet Affect synsets was provided as a resource 
for the shared task of Affective Text in SemEval-
2007. The whole data is provided in six files 
named by the six emotions. Each file contains a list 
of synsets and one synset per line. An example 
synset entry from WordNet Affect is as follows. 
a#00117872 angered  enraged  furious  infuri-
ated  maddened 
The first letter of each line indicates the part of 
speech (POS) and is followed by the affectID. The 
representation was simple and easy for further 
processing. We have retrieved and linked the com-
patible synsetID from the recent version of Word-
Net 3.0 with the affectID of the WordNet Affect 
synsets. We have searched each WordNet Affect 
synset in WordNet 3.0. If a matching WordNet 3.0 
synset is found, the WordNet 3.0 synsetID is 
mapped to the WordNet Affect affectID.  The link-
ing between two synsets of WordNet Affect and 
WordNet 3.0 is shown in Figure 1.  
 
WordNet Affect: 
n#05587878 anger choler ire 
a#02336957 annoyed harassed harried pestered 
vexed 
WordNet:  
07516354-n anger, ire, choler 
02455845-a annoyed harassed harried pestered 
vexed 
Linked Synset ID with Affect ID:  
   n#05587878 ?? 07516354-n anger choler ire  
  a#02336957 ?? 02455845-a annoyed harassed 
harried pestered vexed 
Figure 1: Linking between the synsets of Word-
Net Affect and WordNet 
3.2 Expansion of WordNet Affect using Sen-
tiWordNet 
It has been observed that the WordNet Affect con-
tains fewer number of emotion word entries. The 
six lists provided in the SemEval 2007 shared task 
contain only 612 synsets in total with 1536 words. 
The detail distribution of the emotion words as 
well as the synsets in the six different lists accord-
ing to their POS is shown in Table 1. Hence, we 
have expanded the lists with adequate number of 
emotion words using SentiWordNet before at-
tempting any translation of the lists into Japanese. 
SentiWordNet assigns each synset of WordNet with 
two coarse grained subjective scores such as posi-
tive and negative along with an objective score. 
SentiWordNet contains more number of coarse 
grained emotional words than WordNet Affect. We 
assumed that the translation of the coarse grained 
emotional words into Japanese might contain more 
or less fine-grained emotion words. One example 
entry of the SentiWordNet is shown below. The 
POS of the entry is followed by a synset ID, posi-
tive and negative scores and synsets containing 
sentiment words.   
SentiWordNet:  
a 121184  0.25 0.25 infuri-
ated#a#1 furious#a#2 maddened#a#1 en-
raged#a#1 angered#a#1 
Our aim is to increase the number of emotion 
words in the WordNet Affect using SentiWordNet, 
both of which are developed from the WordNet. 
Hence, each word of the WordNet Affect is re-
placed by the equivalent synsets retrieved from 
SentiWordNet if the synset contains that emotion 
word. The POS information in the WordNet Affect 
is kept unchanged during expansion. A related ex-
ample is shown in Figure 2. The distributions of 
expanded synsets and words for each of the six 
emotion classes based on four different POS types 
(noun N, verb V, adjective Adj. and adverb Adv.) 
are shown in Table 1. But, we have kept the dupli-
cate entries at synset level for identifying the emo-
tion related scores in our future attempts by 
utilizing the already associated positive and nega-
tive scores of SentiWordNet. The percentage of 
entries in the updated word lists are increased by 
69.77 and 74.60 at synset and word levels.  
3.3 Translation of Expanded WordNet Affect 
into Japanese  
We have mapped the affectID of the WordNet Af-
fect to the corresponding synsetID of the WordNet 
3.0. This mapping helps to expand the WordNet 
Affect with the recent version of SentiWordNet 3.0 
as well as translating the expanded lists into Japa-
nese using the Japanese WordNet (Bond et al, 
2009).
82
Emotion 
Classes 
WordNet Affect Synset (S) and Word (W) [After SentiWordNet updating] 
N V Adj Adv 
S W S W S W S W 
Anger 48 [198] 99 [403] 19 [103] 64 [399] 39 [89] 120 [328] 21 [23] 35 [50] 
Disgust 3 [17] 6 [21] 6 [21] 22 [62] 6  [38] 34  [230] 4  [5] 10 [19] 
Fear 23[89] 45 [224] 15  [48] 40 [243] 29  [62] 97  [261] 15 [21] 26 [49] 
Joy 73 [375] 149 [761] 40 [252] 122 [727] 84  [194] 203 [616] 30  [45] 65 [133] 
Sadness 32 [115] 64 [180] 10  [43] 33 [92] 55 [129] 169 [779] 26 [26] 43 [47] 
Surprise 5 [31]    8 [28] 7  [42] 28 [205] 12  [33] 41  [164] 4  [6] 13 [28] 
Table 1: Number of POS based Synsets and Words in six WordNet Affect lists before and after updating 
using SentiWordNet 
 
Linked Affect word:  
n#05587878 ?? 07516354-n anger choler ire  
 
SentiWordNet synsets containing  ?anger?:  
07516354-n anger, ire, choler 
14036539-n angriness, anger 
00758972-n anger, ira, ire, wrath 
01785971-v anger 
01787106-v see_red, anger 
 
SentiWordNet synsets containing  ?choler?:  
07552729-n fretfulness, fussiness, crossness, pe-
tulance, peevishness, irritability, choler 
05406958-n choler, yellow_bile 
 
Expanded Affect word:  
n#05587878?? 07516354-n anger choler ire 
14036539-n angriness anger 00758972-n anger 
ira, ire wrath 01785971-v anger  
? 05406958-n choler 
Figure 2: Expansion of WordNet Affect synset 
using SentiWordNet 
 
As the Japanese WordNet 7  is freely available 
and it is being developed based on the English 
WordNet, the synsets of the expanded lists are au-
tomatically translated into Japanese equivalent 
synsets based on the synsetIDs. The number of 
translated Japanese words and synsets for six affect 
lists are shown in Table 2 and Table 3 respectively. 
The following are some translated samples that 
contain word as well as phrase level translations. 
07510348-n surprise ? ??, ?? 
07503260-n disgust ? ????, ?? 
07532440-n unhappiness, sadness ? ????
?, ??, ???, ????, ????  
                                                        
7 http://nlpwww.nict.go.jp/wn-ja/index.en.html 
07527352-n joy, joyousness, joyfulness ? ??
?, ??, ??????, ??, ????, ??, 
??, ?, ???, ??, ?????? 
 
Emotion 
Classes 
Translated WordNet Affect list 
in Japanese (#Words) 
N V Adj Adv 
Anger 861 501 231 9 
Disgust 49 63 219 10 
Fear 375 235 334 104 
Joy 1959 1831 772 154 
Sadness 533 307 575 39 
Surprise 144 218 204 153 
Table 2: Number of POS based translated word 
entries in six Japanese WordNet Affect lists 
 
Emotion 
Classes 
Japanese WordNet Affect list 
Trans 
(#Syn) 
Non-
Trans 
(#Syn) 
Translated 
Morphemes 
(#W) (#P) 
Anger 254 159 1033 450 
Disgust 57 24 218 97 
Fear 146 74 615 315 
Joy 628 238 2940 1273 
Sadness 216 97 846 519 
Surprise 112 25 456 216 
Table 3: Number of translated (Trans) and non-
translated (Non-Trans) synsets (Syn), words (W) 
and phrases (P) in six Japanese WordNet Affects. 
3.4 Analyzing Translation Errors  
Some SentiWordNet synsets (e.g., 00115193-a huf-
fy, mad, sore) are not translated into Japanese as 
there are no equivalent synset entries in the Japa-
nese WordNet. There were a large number of word 
combinations, collocations and idioms in the Japa-
nese WordNet Affect. These parts of synsets show 
problems during translation and therefore manual 
83
translation is carried out for these types. Some of 
the English synsets (?07517292-n lividity?) were 
not translated into Japanese. But, an equivalent 
gloss of the word ?lividity? that is present in the 
Japanese WordNet is ?a state of fury so great the 
face becomes discolored?. One of the reasons of 
such translation problems may be that no equiva-
lent Japanese word sense is available for such Eng-
lish words. 
4 Evaluation and Analysis 
We have evaluated the lexical coverage of the de-
veloped Japanese WordNet Affect on a small emo-
tional judgment corpus and SemEval 2007 affect 
sensing corpus.  
4.1 Evaluation on Judgment Corpus    
The judgment corpus that is being developed by 
the Japan System Applications Co. Ltd. 8 contains 
only 100 sentences of emotional judgments. But, 
this corpus is not an open source till date. We have 
evaluated our Japanese WordNet Affect based base-
line system on these 100 sentences and the results 
for each of the six emotion classes are shown in 
Table 4. We have also incorporated an open source 
morphological analyzer9 in our baseline system.   
The algorithm is that, if a word in a sentence is 
present in any of the Japanese WordNet Affect lists; 
the sentence is tagged with the emotion label cor-
responding to that affect list. But, if any word is 
not found in any of the six lists, each word of the 
sentence is passed through the morphological 
process to identify its root form which is searched 
through the Japanese WordNet Affect lists again. If 
the root form is found in any of the six Japanese 
WordNet Affect lists, the sentence is tagged accor-
dingly. Otherwise, the sentence is tagged as non-
emotional or neutral. The average F-Score of the 
baseline system has been improved by 4.1% with 
respect to the six emotion classes. Due to the fewer 
number of sentential instances in some emotion 
classes (e.g., joy, sadness, surprise), the perfor-
mance of the system gives poor results even after 
including the morphological knowledge. One of 
the reasons may be the less number of words and 
synset entries in some WordNet Affect lists (e.g., 
fear). Hence, we have aimed to translate the Eng-
                                                        
8 http://www.jsa.co.jp/ 
9 http://mecab.sourceforge.net/ 
lish SemEval 2007 affect sensing corpus into Japa-
nese and evaluate our system on the translated cor-
pus. 
 
Emotion 
Classes  
(#Sentences) 
Judgment Corpus (in %) 
Before Morphology [After Mor-
phology] 
Precision Recall F-Score 
Anger 
 (#32) 
51.61 
[64.29] 
50.00 
[68.12] 
50.79 
[66.14] 
disgust 
 (#18) 
25.00 
[45.00] 
5.56 
[10.56] 
9.09 
[17.10] 
fear (#33) NULL 
joy  
(#3) 
3.45 
[8.08] 
66.67 
[100.00] 
6.56 
[14.95] 
Sadness  (#5) NULL 
surprise  
(#9) 
6.90 
[13.69] 
22.22 
[33.33] 
10.53 
[19.41] 
Table 4: Precision, Recall and F-Scores (in %) 
of the system per emotion class on the Judgment 
corpus by including and excluding morphology. 
4.2 Evaluation on Translated SemEval 2007 
Affect Sensing Corpus    
The English SemEval 2007 affect sensing corpus 
consists of news headlines only. Each of the news 
headlines is tagged with a valence score and scores 
for all the six Ekman?s emotions. The six emotion 
scores for each sentence are in the range of 0 to 
100. We have considered that each sentence is as-
signed a single sentential emotion tag based on the 
maximum emotion score out of six annotated emo-
tion scores. We have used the Google translator 
API 10to translate the 250 and 1000 sentences of 
the trial and test sets of the SemEval 2007 corpus 
respectively. The experiments regarding morphol-
ogy and emotion scores are conducted on the trial 
corpus. We have carried out different experiments 
on 1000 test sentences by selecting different ranges 
of emotion scores. The corresponding experimental 
results are also shown in Table 5. Incorporation of 
morphology improves the performance of the sys-
tem. On the other hand, it is observed that the per-
formance of the system decreases by increasing the 
range of Emotion Scores (ES). The reason may be 
that the numeric distribution of the sentential in-
stances in each of the emotion classes decreases as 
the range in emotion scores increases. 
                                                        
10 http://translate.google.com/# 
 
84
Emotion 
Classes 
Japanese Translated SemEval 2007 Test Corpus (in %) 
Before Morphology [After Morphology] 
Emotion Score (ES) ? 0 Emotion Score (ES) ? 10 
Precision Recall F-Score Precision Recall F-Score 
Anger 61.01[68.75] 18.83[31.16] 28.78[42.88] 44.65[52.08] 25.54[33.32] 32.49[40.35] 
disgust 79.55[85.05] 8.35[16.06] 15.12[27.01] 40.91[41.46] 9.89[18.07] 15.93[24.97] 
Fear 93.42[95.45] 10.26[16.77] 18.49[28.52] 77.63[81.82] 13.32[21.42] 22.74[34.03] 
Joy 69.07[72.68] 57.03[80.30] 62.48[76.29] 53.89[55.61] 56.50[96.22] 55.17[70.40] 
sadness 83.33[84.29] 10.58[19.54] 18.77[31.67] 67.78[69.87] 11.78[19.88] 20.07[30.86] 
surprise 94.94[94.94] 7.84[13.65] 14.48[23.99] 72.15[74.58] 8.25[15.87] 14.81[26.30] 
Emotion Score (ES) ? 30 Emotion Score (ES) ? 50 
Anger 21.38[28.12] 39.08[62.45] 27.64[38.59] 6.92[10.42] 57.89[78.02] 12.36[18.26] 
disgust 2.27[5.04] 3.70[6.72] 2.82[6.15] NIL NIL NIL 
Fear 44.74[56.82] 16.67[28.76] 24.29[38.45] 21.05[29.55] 17.98[31.26] 19.39[30.79] 
Joy 31.48[33.42] 56.86[97.08] 40.52[50.53] 12.04[24.98] 61.32[87.66] 20.12[39.10] 
sadness 37.78[69.86] 15.60[25.31] 22.08[37.22] 13.33[23.07] 12.12[22.57] 12.70[18.71] 
surprise 17.72[20.34] 8.14[18.56] 11.16[20.35] 3.80[8.50] 7.50[12.50] 5.04[10.11] 
Table 6: Precision, Recall and F-Scores (in %) of the system per emotion class on the translated Japanese 
SemEval 2007 test corpus before and after including morphology on different ranges of Emotion Scores. 
4.3 Analysis of Morphology  
Japanese affect lists include words as well as 
phrases. We deal with phrases using Japanese 
morphology tool to find affect words in a sentence 
and substitute an affect word into its original con-
jugated form. One of the main reasons of using a 
morphology tool is to analyze the conjugated form 
and to identify the phrases. For example, the Japa-
nese word for the equivalent English word ?anger? 
is "?? (o ko ru)" but there are other conjugated 
word forms such as "???(o ko tta)" that means 
?angered? and it is used in past tense. Similarly, 
other conjugated form "????? (o ko tte i ta)" 
denotes the past participle form ?have angered? of 
the original word ?anger?. The morphological form 
of its passive sense is "???? (o ko ra re ru)" 
that means ?be angered?. We identify the word 
forms from their corresponding phrases by using 
the morpheme information. For example, the 
phrase "???? (o ko ra re ru)" consists of two 
words, one is ??? (o ko ra) that is in an imper-
fective form and other word is "?? (re ru) which 
is in an original form. The original form of the im-
perfective word ?? (o ko ra) is "?? (o ko 
ru)". It has been found that some of the English 
multi-word phrases have no equivalent Japanese 
phrase available. Only the equivalent Japanese 
words are found in Japanese WordNet. For exam 
 
ple, the following synset contains a multi-word 
phrase ?see-red?. Instead of any equivalent phrases, 
only words are found in Japanese WordNet. 
01787106-v anger, see -red ? ??, ??, ?? 
5 Conclusion 
The present paper describes the preparation of Jap-
anese WordNet Affect containing six types of emo-
tion words in six separate lists. The automatic 
approach of expanding, translating and sense dis-
ambiguation tasks reduces the manual effort. The 
resource is still being updated with more number 
of emotional words to increase the coverage. The 
sense disambiguation task needs to be improved 
further in future by incorporating more number of 
translators and considering their agreement into 
account. In future we will adopt a corpus-driven 
approach for updating the resource with more 
number of emotion words and phrases for extend-
ing the emotion analysis task in Japanese. 
Acknowledgments 
The work reported in this paper is supported by a 
grant from the India-Japan Cooperative Pro-
gramme (DST-JST) 2009 Research project entitled 
?Sentiment Analysis where AI meets Psychology? 
funded by Department of Science and Technology 
(DST), Government of India. 
85
References  
Andreevskaia A. and Bergler Sabine. 2007. CLaC and 
CLaC-NB: Knowledge-based and corpus-based ap-
proaches to sentiment tagging. 4th International 
Workshop on Semantic Evaluations (SemEval-2007), 
pp. 117?120, Prague. 
Baccianella Stefano, Esuli Andrea and Sebas-tiani Fa-
brizio. 2010. SentiWordNet 3.0: An Enhanced Lexi-
cal Re-source for Sentiment Analysis and Opinion 
Mining. In Proceedings of the 7th Conference on 
Language Resources and Evaluation, pp. 2200-2204. 
Banea, Carmen, Mihalcea Rada, Wiebe Janyce. 2008.  
A Bootstrapping Method for Building Subjectivity 
Lexicons for Languages with Scarce Resources. The 
Sixth International Conference on Language Re-
sources and Evaluation (LREC 2008). 
Baroni M. and Vegnaduzzo S. 2004. Identifying subjec-
tive adjectives through web-based mutual informa-
tion. Proceedings of the German Conference on NLP. 
Bobicev Victoria, Maxim Victoria, Prodan Tatiana, 
Burciu Natalia, Anghelus Victoria. 2010. Emotions 
in words: developing a multilingual WordNet-Affect. 
CICLING 2010.  
Bond, Francis, Hitoshi Isahara, Sanae Fujita, Kiyotaka 
Uchimoto, Takayuki Kuribayashi and Kyoko Kanza-
ki. 2009. Enhancing the Japanese WordNet. 7th 
Workshop on Asian Language Resources, ACL-
IJCNLP 2009, Singapore.  
Das Dipankar and Bandyopadhyay Sivaji. 2010. Devel-
oping Bengali WordNet Affect for Analyzing Emo-
tion. 23rd International Conference on the Computer 
Processing of Oriental Languages (ICCPOL-2010), 
pp. 35-40, California, USA. 
Ekman Paul. 1992. An argument for basic emotions, 
Cognition and Emotion, 6(3-4):169-200. 
Esuli, Andrea. and Sebastiani, Fabrizio. 2006. 
SENTIWORDNET: A Publicly Available Lexical 
Resource for Opinion Mining, LREC. 
Hatzivassiloglou V. and McKeown K. R. 1997. Predict-
ing the semantic orientation of adjectives. 35th An-
nual Meeting of the ACL and the 8th Conference of 
the European Chapter of the ACL, pp. 174?181. 
Miller, A. G. 1995. WordNet: a lexical database for 
English. In Communications of the ACM, vol. 38 
(11), November, pp. 39-41. 
Mohammad, S. and Turney, P.D. 2010. Emotions 
evoked by common words and phrases: Using Me-
chanical Turk to create an emotion lexicon. Proceed-
ings of the NAACL-HLT 2010 Workshop on 
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, LA, California, 26-34. 
Neviarouskaya, Alena, Prendinger Helmut, and Ishizuka 
Mitsuru. 2009. SentiFul: Generating a Reliable Lex-
icon for Sentiment Analysis. International Confe-
rence on Affective Computing and Intelligent 
Interaction (ACII'09), IEEE, pp. 363-368. 
Sood S. and Vasserman, L. 2009. ESSE: Exploring 
Mood on the Web. 3rd International AAAI Confe-
rence on Weblogs and Social Media (ICWSM) Data 
Challenge Workshop. 
Strapparava Carlo and Valitutti, A. 2004. Wordnet-
affect: an affective extension of wordnet, In 4th In-
ternational Conference on Language Resources and 
Evaluation, pp. 1083-1086. 
Strapparava Carlo and Mihalcea Rada. 2007. SemEval-
2007 Task 14: Affective Text. 45th Aunual Meeting 
of Association for Computational linguistics. 
Takamura Hiroya, Inui Takashi, Okumura Manabu. 
2005. Extracting Semantic Orientations of Words us-
ing Spin Model. 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pp.133-140.  
Voll, K. and M. Taboada. 2007. Not All Words are 
Created Equal: Extracting Semantic Orientation as a 
Function of Adjective Relevance. In Proceedings of 
the 20th Australian Joint Conference on Artificial In-
telligence. pp. 337-346, Gold Coast, Australia. 
Wiebe Janyce and Riloff Ellen. 2006. Creating Subjec-
tive and Objective Sentence Classifiers from Unan-
notated Texts. International Conference on 
Intelligent Text Processing and Computational Lin-
guistics, Mexico City, pp. 475?486. 
 
 
 
86
Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 93?100,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
Bootstrapping Method for Chunk Alignment in Phrase Based SMT 
 
Santanu Pal Sivaji Bandyopadhyay 
  
Department of Computer Science and Engi-
neering 
Department of Computer Science and Engi-
neering 
Jadavpur University Jadavpur University 
santanu.pal.ju@gmail.com sivaji_cse@yahoo.com 
 
 
 
 
 
 
Abstract 
The processing of parallel corpus plays 
very crucial role for improving the over-
all performance in Phrase Based Statisti-
cal Machine Translation systems (PB-
SMT). In this paper the automatic align-
ments   of different kind of chunks have 
been studied that boosts up the word 
alignment as well as the machine transla-
tion quality. Single-tokenization of 
Noun-noun MWEs, phrasal preposition 
(source side only) and reduplicated 
phrases (target side only) and the align-
ment of named entities and complex 
predicates provide the best SMT model 
for bootstrapping. Automatic bootstrap-
ping on the alignment of various chunks 
makes significant gains over the previous 
best English-Bengali PB-SMT system. 
The source chunks are translated into the 
target language using the PB-SMT sys-
tem and the translated chunks are com-
pared with the original target chunk. The 
aligned chunks increase the size of the 
parallel corpus. The processes are run in 
a bootstrapping manner until all the 
source chunks have been aligned with the 
target chunks or no new chunk alignment 
is identified by the bootstrapping process. 
The proposed system achieves significant 
improvements (2.25 BLEU over the best 
System and 8.63 BLEU points absolute 
over the baseline system, 98.74% relative 
improvement over the baseline system) 
on an English- Bengali translation task.  
1 Introduction 
The objective of the present research work is to 
analyze effects of chunk alignment in English ? 
Bengali parallel corpus in a Phrase Based Statis-
tical Machine Translation system. The initial sen-
tence level aligned English-Bengali corpus is 
cleaned and filtered using a semi-automatic 
process. More effective chunk level alignments 
are carried out by bootstrapping on the training 
corpus to the PB-SMT system. 
The objective in the present task is to align the 
chunks in a bootstrapping manner using a Single 
tokenized MWE aligned SMT model and then 
modifying the model by inserting the aligned 
chunks to the parallel corpus after  each iteration 
of the bootstrapping process, thereby enhancing 
the performance of the SMT system. In turn, this 
method deals with the many-to-many word 
alignments in the parallel corpus. Several types 
of MWEs like phrasal prepositions and Verb-
object combinations are automatically identified 
on the source side while named-entities and 
complex predicates are identified on both sides 
of the parallel corpus. In the target side only, 
identification of the Noun-noun MWEs and re-
duplicated phrases are carried out. Simple rule-
based and statistical approaches have been used 
to identify these MWEs. The parallel corpus is 
modified by considering the MWEs as single 
tokens. Source and target language NEs are 
aligned using a statistical transliteration tech-
nique. These automatically aligned NEs and 
Complex predicates are treated as translation ex-
amples, i.e., as additional entries in the phrase 
table (Pal.et al2010, 2011). Using this aug-
mented phrase table each individual source 
chunk is translated into the target chunk and then 
validated with the target chunks on the target 
side. The validated source-target chunks are con-
93
sidered as further parallel examples, which in 
effect are instances of atomic translation pairs to 
the parallel corpus. This is a well-known practice 
in domain adaptation in SMT (Eck et al, 2004; 
Wu et al, 2008).  The preprocessing of the paral-
lel corpus results in improved MT quality in 
terms of automatic MT evaluation metrics. 
The remainder of the paper is organized as fol-
lows. Section 2 briefly elaborates the related 
work. The PB-SMT system is described in Sec-
tion 3. The resources used in the present work 
are described in Section 4. The various experi-
ments carried out and the corresponding evalua-
tion results have been reported in Section 5. The 
conclusions are drawn in Section 6 along with 
future work roadmap.  
2 Related work 
A multi lingual filtering algorithm generates bi-
lingual chunk alignment from Chinese-English 
parallel corpus (Zhou.et al 2004). The algorithm 
has  three steps, first, the most frequent bilingual 
chunks are extracted from the parallel corpus, 
second, a clustering algorithm has been used for 
combining chunks which are participating for 
alignment and finally one English chunk is gen-
erated corresponding to a Chinese chunk by ana-
lyzing the highest co-occurrences of English 
chunks. Bilingual knowledge can be extracted 
using chunk alignment (Zhou.et al 2004). The 
alignment strategies include the comparison of 
dependency relations between source and target 
sentences. The dependency related candidates are 
then compared with the bilingual dictionary and 
finally the chunk is aligned using the extracted 
dependency related words. Ma.et al (2007) sim-
plified the task of automatic word alignment as 
several consecutive words together correspond to 
a single word in the opposite language by using 
the word aligner itself, i.e., by bootstrapping on 
its output. Zhu and Chang (2008) extracted a dic-
tionary from the aligned corpus, used the dic-
tionary to re-align the corpus and then extracted 
the new dictionary from the new alignment re-
sult. The process goes on until the threshold is 
reached.  
An automatic extraction of bilingual MWEs is 
carried out by Ren et al (2009), using a log like-
lihood ratio based hierarchical reducing algo-
rithm to investigate the usefulness of bilingual 
MWEs in SMT by integrating bilingual MWEs 
into the Moses decoder (Koehn et al, 2007). The 
system has observed the highest improvement 
with an additional feature that identifies whether 
or not a bilingual phrase contains bilingual 
MWEs. This approach was generalized in Car-
puat and Diab (2010) where the binary feature is 
replaced by a count feature which is representing 
the number of MWEs in the source language 
phrase. 
MWEs on the source and the target sides 
should be both aligned in the parallel corpus and 
translated as a whole. However, in the state-of-
the-art PB-SMT systems, the constituents of an 
MWE are marked and aligned as parts of con-
secutive phrases, since PB-SMT (or any other 
approaches to SMT) does not generally treat 
MWEs as special tokens. Another problem with 
SMT systems is the wrong translation of some 
phrases. Sometimes some phrases are not found 
in the output sentence. Moreover, the source and 
target phrases are mostly many-to-many, particu-
larly so for the English?Bengali language pair.  
The main objective of the present work is to see 
whether prior automatic alignment of chunks can 
bring any improvement in the overall perform-
ance of the MT system.  
3 PB-SMT System Description 
The system follows three steps; the first step is 
prepared an SMT system with improved word 
alignment that produces a best SMT model for 
bootstrapping. And the second step is produced a 
chunk level parallel corpus by using the best 
SMT model. These chunk level parallel corpuses 
are added with the training corpus to generate the 
new SMT model in first iteration. And finally the 
whole process repeats to achieve better chunk 
level alignments as well as the better SMT 
model. 
3.1 SMT System with improved Word 
Alignment 
The initial English-Bengali parallel corpus is 
cleaned and filtered using a semi-automatic 
process.  Complex predicates are first extracted 
on both sides of the parallel corpus. The analysis 
and identification of various complex predicates 
like, compound verbs (Verb + Verb), conjunct 
verbs (Noun /Adjective/Adverb + Verb) and se-
rial verbs (Verb + Verb + Verb) in Bengali are 
done following the strategy in Das.et al (2010). 
 Named-Entities and complex predicates are 
aligned following a similar technique as reported 
in Pal.et al(2011). Reduplicated phrases do not 
occur very frequently in the English corpus; 
some of them (like correlatives, semantic redu-
plications) are not found in English (Chakraborty 
94
and Bandyopadhyay, 2010).  But reduplication 
plays a crucial role on the target Bengali side as 
they occur with high frequency. These redupli-
cated phrases are considered as a single-token so 
that they may map to a single word on the source 
side. Phrasal prepositions and verb object combi-
nations are also treated as single tokens. Once the 
compound verbs and the NEs are identified on 
both sides of the parallel corpus, they are assem-
bled into single tokens. When converting these 
MWEs into single tokens, the spaces are replaced 
with underscores (?_?). Since there are already 
some hyphenated words in the corpus, hyphena-
tion is not used for this purpose. Besides, the use 
of a special word separator (underscore in this 
case) facilitates the job of deciding which single-
token MWEs to be de-tokenized into its constitu-
ent words, before evaluation. 
3.1.1 MWE Identification on Source Side 
 The UCREL1 Semantic analysis System 
(USAS) developed by Lancaster University 
(Rayson.et al 2004) has been adopted for MWE 
identification.  The USAS is a software tool for 
the automatic semantic analysis of English spo-
ken and written data. Various types of Multi-
Word Units (MWU) that are identified by the 
USAS software include: verb-object combina-
tions (e.g. stubbed out), noun phrases (e.g. riding 
boots), proper names (e.g. United States of 
America), true idioms (e.g. living the life of Ri-
ley) etc. In English, Noun-Noun (NN) com-
pounds, i.e., noun phrases occur with high fre-
quency and high lexical and semantic variability 
(Tanaka.et al 2003). The USAS software has a 
reported precision value of 91%. 
3.1.2 MWE Identification on Target Side 
Compound nouns are identified on the target 
side. Compound nouns are nominal compounds 
where two or more nouns are combined to form a 
single phrase such as ?golf club? or ?computer 
science department? (Baldwin.et al 2010). Each 
element in a compound noun can function as a 
lexeme in independent of the other lexemes in 
different context. The system uses Point-wise 
Mutual Information (PMI), Log-likelihood Ratio 
(LLR) and Phi-coefficient, Co-occurrence meas-
urement and Significance function (Agarwal.et 
al, 2004) measures for identification of com-
pound nouns. Final evaluation has been carried 
out by combining the results of all the methods. 
A predefined cut-off score has been considered 
                                                        
1  http://www.comp.lancs.ac.uk/ucrel 
and the candidates having scores above the 
threshold value have been considered as MWEs. 
The repetition of noun, pronoun, adjective and 
verb are generally classified as two categories: 
repetition at the (a) expression level and at the 
(b) contents or semantic level. In case of Bengali, 
The expression-level reduplication are classified 
into five fine-grained subcategories:  (i) Ono-
matopoeic expressions (khat khat, knock knock), 
(ii) Complete Reduplication (bara-bara, big big), 
(iii) Partial Reduplication (thakur-thukur, God), 
(iv) Semantic Reduplication (matha-mundu, 
head) and (v) Correlative Reduplication 
(maramari, fighting). 
For identifying reduplications, simple rules 
and morphological properties at lexical level 
have been used (Chakraborty and Bandyop-
adhyay, 2010). The Bengali monolingual dic-
tionary has been used for identification of seman-
tic reduplications.  
An NE and Complex Predicates parallel cor-
pus is created by extracting the source and the 
target (single token) NEs from the NE-tagged 
parallel corpus and aligning the NEs using the 
strategies as applied in (Pal.et al 2010, 2011).  
3.1.3 Verb Chunk / Complex Predicate 
Alignment 
Initially, it is assumed that all the members of the 
English verb chunk in an aligned sentence pair 
are aligned with the members of the Bengali 
complex predicates. Verb chunks are aligned 
using a statistical aligner. A pattern generator 
extracts patterns from the source and the target 
side based on the correct alignment list.  The root 
form of the main verb, auxiliary verb present in 
the verb chunk and the associated tense, aspect 
and modality information are extracted for the 
source side token. Similarly, root form of the 
Bengali verb and the associated vibhakti (inflec-
tion) are identified on the target side token. Simi-
lar patterns are extracted for each alignment in 
the doubtful alignment list.  
Each pattern alignment for the entries in the 
doubtful alignment list is checked with the pat-
terns identified in the correct alignment list. If 
both the source and the target side patterns for a 
doubtful alignment match with the source and the 
target side patterns of a correct alignment, then 
the doubtful alignment is considered as a correct 
one.  
The doubtful alignment list is checked again to 
look for a single doubtful alignment for a sen-
tence pair. Such doubtful alignments are consid-
ered as correct alignment. 
95
The above alignment list as well as NE 
aligned lists are added with the parallel corpus 
for creating the SMT model for chunk alignment. 
The system has reported 15.12 BLEU score for 
test corpus and 6.38 (73% relative) point im-
provement over the baseline system (Pal.et al 
2011). 
3.2 Automatic chunk alignment 
 
3.2.1 Source chunk extraction 
The source corpus is preprocessed after identify-
ing the MWEs using the UCREL tool and single 
tokenizing the extracted MWEs. The source sen-
tences of the parallel corpus have been parsed 
using Stanford POS tagger and then the chunks 
of the sentences are extracted using CRF chun-
ker2 The CRF chunker detects the chunk bounda-
ries of noun, verb, adjective, adverb and preposi-
tional chunks from the sentences. After detection 
of the individual chunks by the CRF chunker, the 
boundary of the prepositional phrase chunks are 
expanded by examining the series of  noun 
chunks separated by conjunctions such as 
'comma', 'and' etc. or a single noun chunk fol-
lowed by a preposition.  For each individual 
chunk, the head words are identified. A synony-
mous bag of words is generated for each head 
word. These bags of words produce more alter-
native chunks which are decoded using the best 
SMT based system (Section 3.1). Additional 
translated target chunks for a single source chunk 
are generated. 
 
CRF Chunker output 
 
bodies/NNS/B-NP of/IN/B-PP all/DT/B-NP 
ages/NNS/I-NP ,/,/O colors/NNS/I-NP and/CC/O 
sizes/NNS/I-NP don/VB/B-VP the/DT/B-NP 
very/JJ/I-NP minimum/NN/I-NP in/IN/B-PP beach-
wear/NN/B-NP and/CC/O idle/VB/B-VP away/RP/B-
PRT the/DT/B-NP days/NNS/I-NP on/IN/B-PP 
the/DT/B-NP sun/NN/I-NP kissed/VBN/I-NP co-
pacabana/NN/I-NP and/CC/O ipanema/NN/I-NP 
beaches/NNS/I-NP ././O  
 
Noun chunk Expansion and boundary detection 
 
(bodies/NNS/B-NP) (of/IN/B-PP) (all/DT/B-NP 
ages/NNS/I-NP ,/,/I-NP colors/NNS/I-NP and/CC/I-
NP sizes/NNS/I-NP) (don/VB/B-VP) (the/DT/B-NP 
very/JJ/I-NP minimum/NN/I-NP) (in/IN/B-PP) 
(beachwear/NN/B-NP) (and/CC/B-O) (idle/VB/B-VP) 
(away/RP/B-PRT) (the/DT/B-NP days/NNS/I-NP) 
                                                        
2  http://crfchunker.sourceforge.net/ 
(on/IN/B-PP) (the/DT/B-NP sun/NN/I-NP 
kissed/VBN/I-NP copacabana/NN/I-NP and/CC/I-NP 
ipanema/NN/I-NP beaches/NNS/I-NP) (././B-O) 
 
Prepositional phrase expansion and extraction 
 
bodies 
of all ages , colors and sizes 
don 
the very minimum 
in beachwear 
and 
idle 
away 
the days 
on the sun kissed copacabana and ipanema 
beaches  
 
 
Figure 1.System architecture of the Automatic chunk 
alignment model  
3.2.2 Target chunk extraction 
The target side of the parallel corpus is cleaned 
and parsed using the shallow parser developed by 
the consortia mode project ?Development of In-
dian Language to Indian Language Machine 
Translation (IL-ILMT) System Phase II? funded 
by Department of Information Technology, Gov-
ernment of India. The individual chunks are ex-
tracted from the parsed output. The individual 
chunk boundary is expanded if any noun chunk 
contains only single word and several noun 
chunks occur consecutively.  The content of the 
individual chunks are examined by checking 
their POS categories. At the time of boundary 
expansion, if the system detects other POS cate-
gory words except noun or conjunction then the 
expansion process stops immediately and new 
chunk boundary beginning is identified. The IL-
ILMT system generates the head word for each 
individual chunk. The chunks for each sentence 
are stored in a separate list. This list is used as a 
96
validation resource for validate the output of the 
statistical chunk aligner. 
3.2.3 Source-Target chunk Alignment 
The extracted source chunks are translated using 
the generated SMT model. The translated chunks 
as well as their alternatives are validated with the 
original target chunk. During validation check-
ing, if any match is found between the translated 
chunk and the target chunk then the source chunk 
is directly aligned with the original target chunk. 
Otherwise, the source chunk is ignored in the 
current iteration for any possible alignment. The 
source chunk will be considered in the next 
alignment. After the current iteration is com-
pleted, two lists are produced: a chunk level 
alignment list and an unaligned source chunk list. 
The produced alignment lists are added with the 
parallel corpus as the additional training corpus 
to produce new SMT model for the next iteration 
process. The next iteration process translates the 
source chunks that are in the unaligned list pro-
duced by the previous iteration. This process 
continues until the unaligned source chunk list is 
empty or no further alignment is identified.  
3.2.4 Source-Target chunk Validation 
The translated target chunks are validated with 
the original target list of the same sentence. The 
extracted noun, verb, adjective, adverb and 
prepositional chunks of the source side may not 
have a one to one correspondence with the target 
side except for the verb chunk. There is no con-
cept of prepositional chunks on the target side. 
Some time adjective or adverb chunks may be 
treated as noun chunk on the target side. So, 
chunk level validation for individual categories 
of chunks is not possible. Source side verb 
chunks are compared with the target side verb 
chunks while all the other chunks on the source 
side are compared with all the other chunks on 
the target side. Head words are extracted for each 
source chunk and the translated head words are 
actually compared on the target side taking into 
the consideration the synonymous target words. 
When the validation system returns positive, the 
source chunk is aligned with the identified origi-
nal target chunk.  
4 Tools and Resources used 
A sentence-aligned English-Bengali parallel cor-
pus containing 14,187 parallel sentences from the 
travel and tourism domain has been used in the 
present work. The corpus has been collected 
from the consortium-mode project ?Development 
of English to Indian Languages Machine Trans-
lation (EILMT) System Phase II3?. The Stanford 
Parser4, Stanford NER, CRF chunker5 and the 
Wordnet 3.06 have been used for identifying 
complex predicates in the source English side of 
the parallel corpus.  
The sentences on the target side (Bengali) are 
parsed and POS-tagged by using the tools ob-
tained from the consortium mode project ?De-
velopment of Indian Language to Indian Lan-
guage Machine Translation (IL-ILMT) System 
Phase II?. NEs in Bengali are identified using the 
NER system of Ekbal and Bandyopadhyay 
(2008).  
The effectiveness of the MWE-aligned and 
chunk aligned parallel corpus is demonstrated by 
using the standard log-linear PB-SMT model as 
our baseline system: GIZA++ implementation of 
IBM word alignment model 4, phrase-extraction 
heuristics described in (Koehn et al, 2003), 
minimum-error-rate training (Och, 2003) on a 
held-out development set, target language model 
trained using SRILM toolkit  (Stolcke, 2002) 
with Kneser-Ney smoothing (Kneser and Ney, 
1995) and the Moses decoder (Koehn et al, 
2007). 
5 Experiments and Evaluation Results 
We have randomly identified 500 sentences each 
for the development set and the test set from the 
initial parallel corpus. The rest are considered as 
the training corpus. The training corpus was fil-
tered with the maximum allowable sentence 
length of 100 words and sentence length ratio of 
1:2 (either way). Finally the training corpus con-
tains 13,176 sentences. In addition to the target 
side of the parallel corpus, a monolingual Ben-
gali corpus containing 293,207 words from the 
tourism domain was used for the target language 
model. The experiments have been carried out 
with different n-gram settings for the language 
model and the maximum phrase length and found 
that a 4-gram language model and a maximum 
phrase length of 4 produce the optimum baseline 
result. The rest of the experiments have been car-
ried out using these settings. 
                                                        
3    The EILMT and ILILMT projects are funded by 
the Department of Information Technology (DIT), Ministry 
of Communications and Information Technology (MCIT), 
Government of India. 
4    http://nlp.stanford.edu/software/lex-parser.shtml 
5    http://crfchunker.sourceforge.net/ 
6    http://wordnet.princeton.edu/ 
97
The system continues with the various pre-
processing of the corpus. The hypothesis is that 
as more and more MWEs and chunks are identi-
fied and aligned properly, the system shows the 
improvement in the translation procedure. Table 
1 shows the MWE statistics of the parallel train-
ing corpus. It is observed from Table 1 that NEs 
occur with high frequency in both sides com-
pared to other types of MWEs. It suggests that 
prior alignment of the NEs and complex predi-
cates plays a role in improving the system per-
formance. 
 
 
English Bengali Training set 
T U T U 
CPs 4874 2289 14174 7154 
redupli-
cated word 
- - 85 50 
Noun-noun 
compound 
892 711 489 300 
Phrasal 
preposition 
982 779 - - 
Phrasal 
verb 
549 532 - - 
Total NE 
words 
22931 8273 17107 9106 
 
Table 1. MWE Statistics. (T - Total occurrence, 
U ? Unique, CP ? complex predicates, NE ? 
Named Entities) 
 
 
Single tokenization of NEs and MWEs of any 
length on both the sides followed by GIZA++ 
alignment has given a huge impetus to system 
performance (6.38 BLEU points absolute, 73% 
relative improvement over the baseline). In the 
source side, the system treats the phrasal preposi-
tions, verb-object combinations and noun-noun 
compounds as a single token. In the target side, 
single tokenization of reduplicated phrases and 
noun-noun compounds has been done followed 
by alignments using the GIZA++ tool. From the 
observation of Table 2, during first iteration there 
are 81821 chunks are identified from the source 
corpus and 14534 has been aligned by the sys-
tem. For iteration 2, there are 67287 source 
chunks are remaining to align. At the final itera-
tion almost 65% of the source chunks have been 
aligned. 
 
 
 
Training 
set 
English Bengali 
Iteration T U T U 
1 81821 70321 65429 59627 
2 67287 62575 50895 47139 
final 32325 31409 15933 15654 
 
Table 2. Chunk Statistics. (T - Total occurrence, 
U ? Unique) 
 
The system performance improves when the 
alignment list of NEs and complex predicates as 
well as sentence level aligned chunk are incorpo-
rated in the baseline best system. It achieves the 
BLEU score of 17.37 after the final iteration. 
This is the best result obtained so far with respect 
to the baseline system (8.63 BLEU points abso-
lute, 98.74% relative improvement in Table 3). It 
may be observed from Table 3 that baseline 
Moses without any preprocessing of the dataset 
produces a BLEU score of 8.74. 
 
Experiments Exp BLEU NIST 
Baseline 1 8.74 3.98 
Best System (Alignment 
of NEs and Complex 
Predicates and Single 
Tokenization of various 
MWEs) 
2 15.12 4.48 
Iteration 1 3 15.87 4.49 
Iteration 2 4 16.28 4.51 
Iteration 3 5 16.40 4.51 
Iteration 4 6 16.68 4.52 
Base-
line 
Best 
Sys-
tem + 
Chunk 
Align
ment 
Final Iteration? 7 17.37 4.55 
 
Table 3.  Evaluation results for different experi-
mental setups. (The ??? marked systems produce 
statistically significant improvements on BLEU 
over the baseline system) 
 
Intrinsic evaluation of the chunk alignment 
could not be performed as gold-standard word 
alignment was not available. Thus, extrinsic 
evaluation was carried out on the MT quality 
using the well known automatic MT evaluation 
metrics: BLEU (Papineni et al, 2002) and NIST 
(Doddington, 2002). Bengali is a morphologi-
cally rich language and has relatively free phrase 
order. Proper evaluation of the English-Bengali 
98
MT evaluation ideally requires multiple set of 
reference translations. Moreover, the training set 
was smaller in size.  
6. Conclusions and Future work 
A methodology has been presented in this paper  
to show how the simple yet effective preprocess-
ing of various types of MWEs and alignment of 
NEs, complex predicates and chunks can boost 
the performance of PB-SMT system on an Eng-
lish?Bengali translation task. The best system 
yields 8.63 BLEU points improvement over the 
baseline, a 98.74% relative increase.  A subset of 
the output from the best system has been com-
pared with that of the baseline system, and the 
output of the best system almost always looks 
better in terms of either lexical choice or word 
ordering. It is observed that only 28.5% of the 
test set NEs appear in the training set, yet prior 
automatic alignment of the NEs complex predi-
cates and chunk improves the translation quality. 
This suggests that not only the NE alignment 
quality in the phrase table but also the word 
alignment and phrase alignment quality improves 
significantly. At the same time, single-
tokenization of MWEs makes the dataset sparser, 
but improves the quality of MT output to some 
extent. Data-driven approaches to MT, specifi-
cally for scarce-resource language pairs for 
which very little parallel texts are available, 
should benefit from these preprocessing meth-
ods. Data sparseness is perhaps the reason why 
single-tokenization of NEs and compound verbs, 
both individually and in collaboration, did not 
add significantly to the scores. However, a sig-
nificantly large parallel corpus can take care of 
the data sparseness problem introduced by the 
single-tokenization of MWEs. 
Acknowledgement 
The work has been carried out with support from 
the consortium-mode project ?Development of 
English to Indian Languages Machine Transla-
tion (EILMT) System funded by Department of 
Information Technology, Government of India. 
References 
Agarwal, Aswini, Biswajit Ray, Monojit Choudhury, 
Sudeshna Sarkar and Anupam Basu. Automatic 
Extraction of Multiword Expressions in Bengali: 
An Approach for Miserly Resource Scenario. In 
Proc. of International Conference on Natural Lan-
guage Processing (ICON), pp. 165-174.( 2004) 
Baldwin, Timothy and Su Nam Kim Multiword Ex-
pressions, in Nitin Indurkhya and Fred J. Damerau 
(eds.) Handbook of Natural Language Processing,  
Second Edition, CRC Press, Boca Raton, USA, pp. 
267?292 (2010) 
Banerjee, Satanjeev, and Alon Lavie.. An Automatic 
Metric for MT Evaluation with Improved Correla-
tion with Human Judgments. In proceedings of the 
ACL-2005 Workshop on Intrinsic and Extrinsic 
Evaluation Measures for MT and/or Summariza-
tion, pp. 65-72. Ann Arbor, Michigan., pp. 65-72. 
(2005) 
Carpuat, Marine, and Mona Diab. Task-based Evalua-
tion of Multiword Expressions: a Pilot Study in 
Statistical Machine Translation. In Proc. of Human 
Language Technology conference and the North 
American Chapter of the Association for Computa-
tional Linguistics conference (HLT-NAACL 
2010), Los Angeles, CA (2010) 
Chakraborty, Tanmoy and Sivaji Bandyopadhyay. 
Identification of Reduplication in Bengali Corpus 
and their Semantic Analysis: A Rule Based Ap-
proach. In proc. of the 23rd International Confer-
ence on Computational Linguistics (COLING 
2010), Workshop on Multiword Expressions: from 
Theory to Applications (MWE 2010). Beijing, 
China. (2010) 
Das, Dipankar, Santanu Pal, Tapabrata Mondal, Tan-
moy Chakraborty, Sivaji Bandyopadhyay. Auto-
matic Extraction of Complex Predicates in Bengali 
In proc. of the workshop on Multiword expression: 
from theory to application (MWE-2010), The 23rd 
International conference of computational linguis-
tics (Coling 2010),Beijing, Chaina, pp. 37-
46.(2010) 
Doddington, George. Automatic evaluation of ma-
chine translation quality using n-gram cooccur-
rence statistics. In Proc. of the Second International 
Conference on Human Language Technology Re-
search (HLT-2002), San Diego, CA, pp. 128-
132(2002) 
Eck, Matthias, Stephan Vogel, and Alex Waibel. Im-
proving statistical machine translation in the medi-
cal domain using the Unified Medical Language 
System. In Proc. of the 20th International Confer-
ence on Computational Linguistics (COLING 
2004), Geneva, Switzerland, pp. 792-798 (2004) 
Ekbal, Asif, and Sivaji Bandyopadhyay. Voted NER 
system using appropriate unlabeled data. In proc. 
of the ACL-IJCNLP-2009 Named Entities Work-
shop (NEWS 2009), Suntec, Singapore, pp.202-
210 (2009). 
Huang, Young-Sook, Kyonghee Paik, Yutaka Sasaki, 
?Bilingual Knowledge Extraction Using Chunk 
Alignment?, PACLIC 18, Tokiyo, pp. 127-138, 
(2004). 
99
Kneser, Reinhard, and Hermann Ney. Improved back-
ing-off for m-gram language modeling. In Proc. of 
the IEEE Internation Conference on Acoustics, 
Speech, and Signal Processing (ICASSP), vol. 1, 
pp. 181?184. Detroit, MI. (1995) 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
Statistical phrase-based translation. In Proc. of 
HLT-NAACL 2003: conference combining Human 
Language Technology conference series and the 
North American Chapter of the Association for 
Computational Linguistics conference series,  Ed-
monton, Canada, pp. 48-54. (2003) 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine 
Moran, Richard Zens, Chris Dyer, Ond?ej Bojar, 
Alexandra Constantin, and Evan Herbst. Moses: 
open source toolkit for statistical machine transla-
tion. In Proc. of the 45th Annual meeting of the 
Association for Computational Linguistics (ACL 
2007): Proc. of demo and poster sessions, Prague, 
Czech Republic, pp. 177-180. (2007) 
Koehn, Philipp. Statistical significance tests for ma-
chine translation evaluation. In  EMNLP-2004: 
Proc. of the 2004 Conference on Empirical Meth-
ods in Natural Language Processing, 25-26 July 
2004, Barcelona, Spain, pp 388-395. (2004) 
Ma, Yanjun, Nicolas Stroppa, AndyWay. Proceedings 
of the 45th Annual Meeting of the Association of 
Computational Linguistics, ,Prague, Czech Repub-
lic, June 2007, pp. 304?311 (2007). 
Moore, Robert C. Learning translations of named-
entity phrases from parallel corpora. In Proc. of 
10th Conference of the European Chapter of the 
Association for Computational Linguistics (EACL 
2003), Budapest, Hungary; pp. 259-266. (2003) 
Och, Franz J. Minimum error rate training in statisti-
cal machine translation. In Proc. of the 41st Annual 
Meeting of the Association for Computational Lin-
guistics (ACL-2003), Sapporo, Japan, pp. 160-167. 
(2003) 
Pal Santanu, Sudip Kumar Naskar, Pavel Pecina, 
Sivaji Bandyopadhyay and Andy Way. Handling 
Named Entities and Compound Verbs in Phrase-
Based Statistical Machine Translation, In proc. of 
the workshop on Multiword expression: from the-
ory to application (MWE-2010), The 23rd Interna-
tional conference of computational linguistics (Col-
ing 2010),Beijing, Chaina, pp. 46-54 (2010) 
Pal, Santanu Tanmoy Chakraborty , Sivaji Bandyop-
adhyay, ?Handling Multiword Expressions in 
Phrase-Based Statistical Machine Translation?, 
Machine Translation Summit XIII(2011),Xiamen, 
China, pp. 215-224 (2011) 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. BLEU: a method for automatic 
evaluation of machine translation. In Proc. of the 
40th Annual Meeting of the Association for Com-
putational Linguistics (ACL-2002), Philadelphia, 
PA, pp. 311-318 (2002) 
Rayson, Paul, Dawn Archer, Scott Piao, and Tony 
McEnery. The UCREL Semantic Analysis System. 
In proc. Of LREC-04 Workshop: Beyond Named 
Entity Recognition Semantic Labeling for NLP 
Tasks, pages 7-12, Lisbon, Porugal (2004) 
Ren, Zhixiang, Yajuan L?, Jie Cao, Qun Liu, and Yun 
Huang. Improving statistical machine translation 
using domain bilingual multiword expressions. 
In Proc. of the 2009 Workshop on Multiword Ex-
pressions, ACL-IJCNLP 2009, Suntec, Singapore, 
pp. 47-54 (2009). 
Stolcke, A. SRILM?An Extensible Language Mod-
eling Toolkit. Proc. Intl. Conf. on Spoken Lan-
guage Processing, vol. 2, pp. 901?904, Denver 
(2002). 
Tanaka, Takaaki and Timothy Baldwin. Noun- Noun 
Compound Machine Translation: A Feasibility 
Study on Shallow Processing. In Proc. of the Asso-
ciation for Computational Linguistics- 2003, 
Workshop on Multiword Expressions: Analysis, 
Acquisition and Treatment, Sapporo, Japan, pp. 
17?24 (2003) 
Wu, Hua Haifeng Wang, and Chengqing Zong. Do-
main adaptation for statistical machine translation 
with domain dictionary and monolingual cor-
pora. In Proc. of the 22nd International Conference 
on Computational Linguistics (COLING 
2008),  Manchester, UK, pp. 993-1000 (2008) 
Xuan-Hieu Phan, "CRFChunker: CRF English Phrase 
Chunker", http://crfchunker.sourceforge.net/, 
(2006) 
Zhou, Yu, chengquing Zong, Bo Xu, ?Bilingual 
Chunk Aliment in Statistical Machine Translation?,  
IEEE International Conference on Systems, Man 
and Cybernetics, pp. 1401-1406, (2004)
 
100
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 201?207,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Detection and Correction of Preposition and  Determiner Errors in English: HOO 2012   Pinaki Bhaskar Aniruddha Ghosh Santanu Pal Sivaji Bandyopadhyay Department of Computer Science and Engineering, Jadavpur University 188, Raja S. C. Mallick Road Kolkata ? 700032, India pinaki.bhaskar @gmail.com arghyaonline @gnail.com santanu.pal.ju @gmail.com sivaji_cse_ju @yahoo.com       Abstract 
This paper reports on our work in the HOO 2012 shared task. The task is to automatically detect, recognize and correct the errors in the use of prepositions and determiners in a set of given test documents in English. For that, we have developed a hybrid system of an n-gram statistical model along with some rule-based techniques. The system has been trained on the HOO shared task?s training datasets and run on the test set given. We have submitted one run, which has demonstrated an F-score of 7.1, 6.46 and 2.58 for detection, recognition and correction respectively before revision and F-score of 8.22, 7.59 and 3.16 for detec-tion, recognition and correction respectively after revision. 
1 Introduction Writing research papers or theses in English is a very challenging task for those researchers and scientists whose first language or mother tongue is not English. Depicting their research works proper-ly in English is a hard job for them. Generally their papers, which are submitted to conferences, may be rejected not because of their research works but because of the English writing, which makes the papers harder for the reviewer to understand the intentions of author. This kind of problem will be faced in any field where someone has to provide 
material in a language other than his/her first lan-guage. The mentoring service of Association for Com-putational Linguistics (ACL) is one part of a re-sponse. This service can address a wider range of problems than those related purely to writing. The aim of this service is that a research paper should be judged only on its research content. The organizer of ?Help Our Own? (HOO) pro-posed and initiated a shared task in 2011 (Dale and Kilgarriff, 2010), which attempts to tackle the problem by developing tools or techniques for the non-native speaker of English, which will automat-ically correct the English prose of the papers so that they can be accepted. This tools and tech-niques may also help native English speakers. This task is simply expressed as text-to-text generation or Natural language Generation (NLG). In the 2011 shared task, all possible errors were covered which made the task enormously huge. In 2012, the task is more specific and only deals with de-terminers and prepositions as described in (Dale and Kilgarriff, 2011). For this shared task, HOO, we have developed two models, one is rule-based model and the other is the statistical model for both determiners and prepositions. Then we have combined both these models and developed our system for HOO 2012. 2 Related Work The English language belongs to the Germanic languages branch of the Indo-European language family, widely spoken on six continents. The HOO 
201
shared task is organized to help authors with writ-ing tasks. Identifying grammatical and linguistic errors in text is an open challenge to researchers. In recent times, researchers (Heidorn, 2000) have provided quite a benchmark for spell checker and grammar checkers, which is commonly available. In this task it is aimed to correct errors beyond the scope of these commonly available checkers i.e. detection and correction of jarring errors at part-of-speech (POS) level, syntax level and semantic lev-el. Earlier Heidorn (1975) developed augmented phrase structure grammar. (Tetreault et. al., 2008) has dealt with error pattern with preposition by non-native speakers. Meurers and Wunsch (2010) showed a surface based state-of-the-art machine learning technique, which deals with some fre-quently used prepositions. (Elghafari et al, 2010) worked on Data-Driven Prediction of Prepositions in English. Boyd et al (2011) used an n-gram based machine-learning approach. Last year we have also participated in this shared task; our sys-tem report was reported in (Bhaskar et. al., 2011).  3 Corpus Statistics There are two sets of data, training set and test set provided by the organizer. The training set has 1000 documents, which are collected from the FCE dataset. The publicly available dataset was in the native FCE format. So, the organizer first convert-ed it to the HOO data format. Then CUP annota-tors found the errors and marked them up in the dataset. This year the task is only about the errors related to prepositions and determiners. So the or-ganizer set only six types of errors, listed in table 1, which were dealt with this year. Hence, the other errors were discarded and replace with its corre-sponding standoff annotation in the training set. The training set consists of 1000 documents of to-tal 374680 words, which means 375 words per document. All the standoff annotations of training set were provided and an example of the standoff annotation is shown in the figure 1. Table 2 gives the error statistics of training set as reported in (Dale et. al., 2012). The test dataset has another 100 documents, which contain total of 18013 words at an average of 180 words per document. The test data was pro-cessed as the training data was done, but the stand-off annotation of the test documents was not provided before the task completion. The docu-
ments were provided in XML format as shown in the figure 2.   Error Type Tag Original Correction Replacement Preposition  RT He was born on January He was born in January Missing Preposition  MT Because it reminds me my child-hood. 
Because it reminds me of my child-hood. Unwanted Preposition  UT Regarding to the accom-modation Regarding the accom-modation Replacement Determiner  RD I used to go-ing with my friends to the camp. 
I used to go-ing with my friends to a camp. Missing Determiner  MD That will be nice to go on 1st of July That will be nice to go on the 1st of July Unwanted Determiner  UD The most suitable time for shopping is weekend when parents don't work and children haven't got a school. 
The most suitable time for shopping is weekend when parents don't work and children haven't got school.  Table 1. Examples of the six types of error.   Error Type # Training # Test # before Revised # after Revised UT  822  43 39 MT  1104 57 56 RT  2618 136 148 Prep  4545 236 243 UD  1048 53 62 MD  2230 125 131 RD  609 39 37 Det  3887 217 230 Total  8432 453 473 Words/  Error  44.18 39.77 38.08  Table 2. Error Statistics in the Training set. 
202
 <edit end="779" file="0004" in-dex="0008" part="1" start="775" type="UD">   <original>the </original>     <corrections>       <correction>         <empty/>       </correction>   </corrections> </edit>  <edit end="1041" file="0004" in-dex="0010" part="1" start="1039" type="RT">   <original>in</original>     <corrections>       <correction>at</correction>   </corrections> </edit>   Figure 1: An example of a standoff error annotation  4 System Description  The task is consisted of two coarse parts ? Preposi-tion and Determiner detection, recognition and cor-rection. In our previous year?s hybrid model, to resolve preposition errors, a rule-based model was developed and for determiner errors, a linear statis-tical method was used. There was no linear statisti-cal model for prepositions. So this year we have induced a statistical model to incorporate larger coverage of preposition error detection, which is not detected by the appropriate preposition list de-scribed in section 4.1.2. To resolve preposition errors and determiner er-rors we have built a hybrid model for both of them and used a voting technique among the rule based and statistical model for determiners and rule based post processing for prepositions. The system architecture is shown in the figure 3.  
 <?xml version="1.0" encod-ing="utf-8"?> <HOO version="2.1">   <HEAD sortkey="" source-type="FCE">     <CANDIDATE>       <AGE>20-30</AGE>     </CANDIDATE>   </HEAD>   <BODY>     <PART id="1">       <P>Dear Chris</P>       <P>I was great ?</P>       .       .       .     </PART>   </BODY> </HOO>   Figure 2: An example of the XML format of documents  4.1 Preposition Error Detection 
4.1.1 Statistical Model for Preposition An n-gram based linear statistical model is used. From the training corpus, it was trained with 3, 5 and 7-gram models. After testing, the 5-gram mod-el is performing best as from 3-gram, the statistical model fails to classify since probability distance is too small among the probable set to distinguish proper one while in 7-gram it fails to score high as training data set is relatively small and there are no similar occurrences. For the statistical model, dif-ferent linguistic information is taken as features. Initially, surface words are only considered which actually is similar to fingerprinting technique. Due to different inflected forms, the system fails to identify possible cases for a similar type of error with different inflected forms. Hence the root form of the word is included as a feature. Chunk infor-mation is included as a feature. The preposition with same word varies with if following word is animate or inanimate. As example, collaborate with SB collaborate in/on ST
203
  Figure 3. System Architecture  The text is parsed using the Stanford Dependen-cy parser1 to retrieve animate and inanimate infor-mation. After including animate and inanimate information the system didn?t improve much as training data set is quite small and animate infor-mation is not correct for names. Hence, this feature is discarded from the statistical model. 4.1.2 Appropriate Preposition List An appropriate preposition list consists of list of words along with preposition. The list is prepared in different corpus and training data. In the list, all possible formation with a word and preposition is stored. Let us take an example: admit ST to SB admit to From corpus, two patterns for admit are found. Between admit and preposition something (ST) 
                                                            1 http://nlp.stanford.edu/software/lex-parser.shtml 
may come. Hence both of the entries are combined and formed in a regular expression format. 
admit (ST)* to SB 4.1.3 Rule Based model for Preposition Rule based post processing was applied on output of statistical model. For the rule based post pro-cessing, an appropriate preposition list was pre-pared manually. The list contains 1567 entries. The list is associated with animate and inanimate in-formation. Hence, we aim to use dependency par-ser to identify subject object relation. Since the test data was in XML format, raw text was extracted from the XML document and the extracted sen-tences were parsed using Stanford dependency par-ser. After parsing the document with the dependency parser, subject and object information was extract-ed. From all the sentences, proposition are detected and cross-validated with the appropriate preposi-tion list. The preposition is dependent of the local association of the word around it. For the baseline
204
model, we have found that due to the list being small, few errors are being detected. Hence from the training corpus, the appropriate proposition list is enriched. The list is prepared in regular expres-sion format. Here is an example: ask * out + invite on a date  In the above example, + means the two phrases have a similar meaning and * means one or more words can appear between the two words. Hence, when a match is found from the appropriate propo-sition list with the first word or the preposition, the words local to it are validated. Since the task is about correcting preposition errors, only words are matched with the list.  grateful to SB for ST  In the above example, ST means something or an object and SB means somebody or a subject, this information being retrieved from the depend-ency parser.  4.2 Determiner Error Detection At the beginning of the determiner error detection task, we found that generation of list of rules to detect and correct the probable linguistic errors is a non-exhaustive set. Hence, we have decided to use a statistical model. After the statistical model, a rule based system is implemented with a few rules for the determiner devised from grammar books as for certain patterns statistical model fails to identi-fy. 4.2.1 Statistical Model for Determiner Similarly to preposition error detection, here a 5-gram linear statistical model is used. As same au-thors are prone to repeat same types of mistakes, we have decided to list out the errors from the training corpus documents. We have listed the er-rors document wise. In the training corpus, age information of author is mentioned. Hence docu-ments are grouped according to age. After a close inspection of the document wise error list, the age group is prone to make similar type of errors, which depicts the attributes of the age group. Our statistical model is trained with every set of train-ing data grouped by age separately. Hence differ-ent statistical models are prepared for different age 
groups. Now statistical model are applied accord-ing to the age group. It is found that age wise train-ing incurred better result than single statistical model over whole data.  4.2.2 Rule Based Model for Determiner It is found that statistical models works best for detecting the a and an determiner whereas perfor-mance drops for the determiner. Hence, rules for the are crafted manually from grammar books. A few rules for a and an are defined based on the first letter of the following word.  Among the determiners, usage of the is the most complicated one. For the rule based system differ-ent lists like nation, nationalities, unique objects, etc are produced. A few of the rules, which have been developed for the the determiner are men-tioned below. 1. In most cases, if a sentence starts with a proper noun or common noun the is dropped. 2. Before a country name, the is dropped except if starts with kingdom or republic. 3. They system checks whether a common noun is appeared in a previous line of the docu-ment, i.e. it has already been referred to, in which case the is added. 4. If subject and object belong to same class i.e. they share the same hyponym class, the is added to the subject. 5. In case of superlatives like best, worst etc. the is added. 6. Before numerals, the is added. 7. Before unique things, the is added. Unique-ness is defined if a thing has single embod-iment like moon etc. 8.  It is found that if some geographical location is mentioned at a position other than start of sentence, the is added. For different rules word lists are prepared such as a unique things list, superlatives, common nouns, country names, citizenships etc.  For a and an determiner correction, a list of dif-ferent phonemes is prepared. Rule based system 
205
trims the first two characters and maps them into a phoneme to decide between a and an. 4.2.3 Voting Technique The voting technique is used on the output of the rule based model and the statistical model. For a and an determiners, statistical model works best, especially in missing determiner and unnecessary determiner but for wrong determiner the rule based model performs better. For the determiner, the sta-tistical model identified missing determiner and unnecessary determiner cases to some extent whereas list based rule-based system elevates the accuracy.  5 Evaluation The system was evaluated for its performance in detecting, recognizing and correcting preposition and determiner errors in English documents. Sepa-rate scores were calculated for detection, recogni-tion and correction for both the errors of preposition and determiner separately and then combined scores were also calculated. For all re-sults, the organizer has provided three measures: Precision, Recall and F-Score. The precise defini-tions of these measures as implemented in the evaluation tool, and further details on the evalua-tion process are provided in (Dale and Narroway, 2012) and elaborated on at the HOO website.4 Each team was allowed to submit up to 10 sepa-rate runs over the test data, thus allowing them to have different configurations of their systems eval-
                                                            4 See www.correcttext.org/hoo2012. 
uated. Teams were asked to indicate whether they had used only publicly available data to train their systems, or whether they had made use of privately held data. We have submitted only one run (JU_run1) which has demonstrated F-scores of 7.1, 6.46 and 2.58 for detection, recognition and correc-tion respectively before revision. And after revi-sion it has demonstrated F-scores of 8.22, 7.59 and 3.16 for detection, recognition and correction re-spectively. Table 3 shows all the results of our run. We had used only publicly available data to train our systems, which are provided by the organizer as training set; we didn?t use any privately held data. 6 Conclusion and Future Works Our system has achieved F-scores of 8.22, 7.59 and 3.16 in detection, recognition and correction respectively. Our system failed to detect and cor-rect many syntactic and semantic errors like wrong a determiner. Since the data consists of mostly mail conversation, it retains huge number of spelling mistakes, which misdirected the statistical, and rule based model to detect probable errors. For the determiner, if the size of the produced lists in-creases, better accuracy can be achieved with the rule-based system. Co-reference is another issue to identify, as the determiner is used mostly subse-quent references. Anaphora resolution might there-fore be of some help.    
 
    Element Task Before Revision After Revision Precision Recall F-score Precision Recall F-score 
Preposition Detection 6.10 7.63 6.78 7.12 8.61 7.79 Recognition 5.42 6.78 6.03 6.44 7.79 7.05 Correction 3.05 3.81 3.39 3.73 4.51 4.08 
Determiner Detection 7.73 6.45 7.04 9.39 7.42 8.29 Recognition 7.73 6.45 7.04 9.39 7.42 8.29 Correction 1.66 1.38 1.51 2.21 1.75 1.95 
Combined Detection 6.93 7.28 7.10 8.19 8.25 8.22 Recognition 6.30 6.62 6.46 7.56 7.61 7.59 Correction 2.52 2.65 2.58 3.15 3.17 3.16  Table 3. Results for Preposition, Determiner and Combined (preposition and determiner) errors.  
206
Acknowledgments We acknowledge the support of the IFCPAR fund-ed Indo-French project ?An Advanced Platform for Question Answering Systems? and the DIT, Gov-ernment of India funded project ?Development of English to Indian Language Machine Translation (EILMT) System Phase II?. References  Adriane Boyd and Detmar Meurers. Data-Driven Cor-rection of FunctionWords in Non-Native English. In 2011 Generation Challenges, HOO: Helping Our Own in the Proceedings of the 13th European Work-shop on Natural Language Generation (ENLG), 28th ? 30th September, 2011, Nancy, France. Anas Elghafari, Detmar Meurers and Holger Wunsch, 2010. Exploring the Data-Driven Prediction of Prep-ositions in English. In the Proceedings of the 23rd In-ternational Conference on Computational Linguistics, Beijing, China, 2010. George Heidorn. 2000. Intelligent writing assistance. In R Dale, H Moisl, and H Somers, editors, Handbook of Natural Language Processing, pages 181?207. Marcel Dekker Inc.  GE Heidorn. 1975. Augmented phrase structure gram-mars. In: BL Webber, RC Schank, eds. Theoretical Issues in Natural Language Processing. Assoc. for Computational Linguistics, pp.1-5.  J R Tetreault and M S Chodorow. 2008. The ups and downs of preposition error detection in ESL writing. In Proceedings of the 22nd International Conference on Computational Linguistics, pp-865-872, Manches-ter,2008. Pinaki Bhaskar, Aniruddha Ghosh, Santanu Pal and Sivaji Bandyopadhyay. May I correct the English of your paper!!!. In 2011 Generation Challenges, HOO: Helping Our Own in the Proceedings of the 13th Eu-ropean Workshop on Natural Language Generation (ENLG), pp 250-253, 28th ? 30th September, 2011, Nancy, France. Robert Dale and A Kilgarriff. 2010. Helping Our Own: Text massaging for computational linguistics as a new shared task. In Proceedings of the 6th Interna-tional Natural Language Generation Conference, Dublin, Ireland, pages 261?266, 7th-9th July 2010. Robert Dale and A Kilgarriff. 2011. Helping our own: The HOO 2011 pilot shared task. In Proceedings of the 13th European Workshop on Natural Language Generation (ENLG), 28th ? 30th September, 2011, Nancy, France. 
Robert Dale and George Narroway. 2012. A framework for evaluating text correction. In Proceedings of the Eighth International Conference on Language Re-sources and Evaluation (LREC2012), 21?27 May 2012. Robert Dale, Ilya Anisimoff and George Narroway (2012) HOO 2012: A Report on the Preposition and Determiner Error Correction Shared Task.  In Pro-ceedings of the Seventh Workshop on Innovative Use of NLP for Building Educational Applications, Mon-treal, Canada, 7th June 2012.             
207
Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 69?76,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
Improving MT System Using Extracted Parallel Fragments of Text 
from Comparable Corpora 
 
 
Rajdeep Gupta, Santanu Pal, Sivaji Bandyopadhyay 
Department of Computer Science & Engineering 
Jadavpur University 
Kolkata ? 700032, India 
{rajdeepgupta20, santanu.pal.ju}@gmail.com,  
sivaji_cse_ju@yahoo.com 
 
Abstract 
In this article, we present an automated ap-
proach of extracting English-Bengali parallel 
fragments of text from comparable corpora 
created using Wikipedia documents. Our ap-
proach exploits the multilingualism of Wiki-
pedia. The most important fact is that this ap-
proach does not need any domain specific cor-
pus. We have been able to improve the BLEU 
score of an existing domain specific English-
Bengali machine translation system by 
11.14%. 
1 Introduction 
Recently comparable corpora have got great at-
tention in the field of NLP. Extracting parallel 
fragments of texts, paraphrases or sentences from 
comparable corpora are particularly useful for 
any statistical machine translation system (SMT) 
(Smith et al 2010) as the size of the parallel cor-
pus plays major role in any SMT performance. 
Extracted parallel phrases from comparable cor-
pora are added with the training corpus as addi-
tional data that is expected to facilitate better per-
formance of machine translation systems specifi-
cally for those language pairs which have limited 
parallel resources available. In this work, we try 
to extract English-Bengali parallel fragments of 
text from comparable corpora. We have devel-
oped an aligned corpus of English-Bengali doc-
ument pairs using Wikipedia. Wikipedia is a 
huge collection of documents in many different 
languages. We first collect an English document 
from Wikipedia and then follow the inter-
language link to find the same document in Ben-
gali (obviously, if such a link exists). In this way, 
we create a small corpus. We assume that such 
English-Bengali document pairs from Wikipedia 
are already comparable since they talk about the 
same entity. Although each English-Bengali 
document pair talks about the same entity, most 
of the times they are not exact translation of each 
other. And as a result, parallel fragments of text 
are rarely found in these document pairs. The 
bigger the size of the fragment the less probable 
it is to find its parallel version in the target side. 
Nevertheless, there is always chance of getting 
parallel phrase, tokens or even sentences in com-
parable documents. The challenge is to find those 
parallel texts which can be useful in increasing 
machine translation performance. 
In our present work, we have concentrated on 
finding small fragments of parallel text instead of 
rigidly looking for parallelism at entire sentential 
level. Munteanu and Marcu (2006) believed that 
comparable corpora tend to have parallel data at 
sub-sentential level. This approach is particularly 
useful for this type of corpus under 
consideration, because there is a very little 
chance of getting exact translation of bigger 
fragments of text in the target side. Instead, 
searching for parallel chunks would be more 
logical. If a sentence in the source side has a 
parallel sentence in the target side, then all of its 
chunks need to have their parallel translations in 
the target side as well. 
It is to be noted that, although we have 
document level alignment in our corpus, it is 
somehow ad-hoc i.e. the documents in the corpus 
do not belong to any particular domain. Even 
with such a corpus we have been able to improve 
the performance of an existing machine 
translation system built on tourism domain. This 
also signifies our contribution towards domain 
adaptation of machine translation systems. 
The rest of the paper is organized as follows. 
Section 2 describes the related work. Section 3 
describes the preparation of the comparable 
corpus. The system architecture is described in 
section 4. Section 5 describes the experiments we 
69
conducted and presents the results. Finally the 
conclusion is drawn in section 6. 
2 Related Work 
There has been a growing interest in approaches 
focused on extracting word translations from 
comparable corpora (Fung and McKeown, 1997; 
Fung and Yee, 1998; Rapp, 1999; Chiao and 
Zweigenbaum, 2002; Dejean et al, 2002; Kaji, 
2005; Gamallo, 2007; Saralegui et al, 2008). 
Most of the strategies follow a standard method 
based on context similarity. The idea behind this 
method is as follows: A target word t is the 
translation of a source word s if the words with 
which t co-occurs are translations of words with 
which s co-occurs. The basis of the method is to 
find the target words that have the most similar 
distributions with a given source word. The 
starting point of this method is a list of bilingual 
expressions that are used to build the context 
vectors of all words in both languages. This list 
is usually provided by an external bilingual 
dictionary. In Gamallo (2007), however, the 
starting list is provided by bilingual correlations 
which are previously extracted from a parallel 
corpus. In Dejean (2002), the method relies on a 
multilingual thesaurus instead of an external 
bilingual dictionary. In all cases, the starting list 
contains the ?seed expressions? required to build 
context vectors of the words in both languages. 
The works based on this standard approach 
mainly differ in the coefficients used to measure 
the context vector similarity. 
Otero et al (2010) showed how Wikipedia 
could be used as a source of comparable corpora 
in different language pairs. They downloaded the 
entire Wikipedia for any two language pair and 
transformed it into a new collection: 
CorpusPedia. However, in our work we have 
showed that only a small ad-hoc corpus 
containing Wikipedia articles could be proved to 
be beneficial for existing MT systems. 
3 Tools and Resources Used 
A sentence-aligned English-Bengali parallel 
corpus containing 22,242 parallel sentences from 
a travel and tourism domain was used in the 
preparation of the baseline system. The corpus 
was obtained from the consortium-mode project 
?Development of English to Indian Languages 
Machine Translation (EILMT) System?. The 
Stanford Parser and the CRF chunker were used 
for identifying individual chunks in the source 
side of the parallel corpus. The sentences on the 
target side (Bengali) were POS-tagged/chunked 
by using the tools obtained from the consortium 
mode project ?Development of Indian Languages 
to Indian Languages Machine Translation 
(ILILMT) System?.  
For building the comparable corpora we have 
focused our attention on Wikipedia documents. 
To collect comparable English-Bengali 
document pairs we designed a crawler. The 
crawler first visits an English page, saves the raw 
text (in HTML format), and then finds the cross-
lingual link (if exists) to find the corresponding 
Bengali document. Thus, we get one English-
Bengali document pair. Moreover, the crawler 
visits the links found in each document and 
repeats the process. In this way, we develop a 
small aligned corpus of English-Bengali 
comparable document pairs. We retain only the 
textual information and all the other details are 
discarded. It is evident that the corpus is not 
confined to any particular domain. The challenge 
is to exploit this kind of corpus to help machine 
translation systems improve. The advantage of 
using such corpus is that it can be prepared easily 
unlike the one that is domain specific. 
The effectiveness of the parallel fragments of 
text developed from the comparable corpora in 
the present work is demonstrated by using the 
standard log-linear PB-SMT model as our 
baseline system: GIZA++ implementation of 
IBM word alignment model 4, phrase extraction 
heuristics described in (Koehn et al, 2003), 
minimum-error-rate training (Och, 2003) on a 
held-out development set, target language model 
with Kneser-Ney smoothing (Kneser and Ney, 
1995) trained with SRILM (Stolcke, 2002), and 
Moses decoder (Koehn et al, 2007). 
4 System Architecture 
4.1 PB-SMT(Baseline System) 
Translation is modeled in SMT as a decision 
process, in which the translation e1
I = e1..ei..eI of 
a source sentence f1
J = f1..fj..fJ  is chosen to 
maximize (1) 
)().|(maxarg)|(maxarg 111,11, 11
IIJ
eI
JI
eI
ePefPfeP
II
?
     (1)  
where )|( 11 IJ efP  and )( 1IeP  denote 
respectively the translation model and the target 
language model (Brown et al, 1993). In log-
linear phrase-based SMT, the posterior 
probability )|( 11 JI feP  is directly modeled as a 
log-linear combination of features (Och and Ney, 
70
2002), that usually comprise of M translational 
features, and the language model, as in (2): 
?
?
?
M
m
KIJ
mm
JI sefhfeP
1
11111 ),,()|(log ?
)(log 1ILM eP??        (2)     
where kk sss ...11 ?  denotes a segmentation of the 
source and target sentences respectively into the 
sequences of phrases )?,...,?( 1 kee  and )?,...,?( 1 kff  
such that (we set i0 = 0) (3): 
,1 Kk ???  sk = (ik, bk, jk), 
          kk iik eee ...? 11??? , 
         kk jbk fff ...? ? .          (3) 
and each feature mh?  in (2) can be rewritten as in 
(4): 
?
?
?
K
k
kkkm
KIJ
m sefhsefh
1
111 ),?,?(?),,(
                 (4) 
where mh? is a feature that applies to a single 
phrase-pair. It thus follows (5): 
? ??
? ??
?K
k
K
k
kkkkkkm
M
m
m sefhsefh
1 11
),?,?(?),?,?(??
     (5) 
where 
m
M
m
mhh ??
1
?
?
? ?
. 
4.2 Chunking of English Sentences 
We have used CRF-based chunking algorithm to 
chunk the English sentences in each document. 
The chunking breaks the sentences into linguistic 
phrases. These phrases may be of different sizes. 
For example, some phrases may be two words 
long and some phrases may be four words long. 
According to the linguistic theory, the interme-
diate constituents of the chunks do not usually 
take part in long distance reordering when it is 
translated, and only intra chunk reordering oc-
curs. Some chunks combine together to make a 
longer phrase. And then some phrases again 
combine to make a sentence. The entire process 
maintains the linguistic definition of a sentence. 
Breaking the sentences into N-grams would have 
always generated phrases of length N but these 
phrases may not be linguistic phrases. For this 
reason, we avoided breaking the sentences into 
N-grams. 
The chunking tool breaks each English sentence 
into chunks. The following is an example of how 
the chunking is done. 
Sentence: India , officially the Republic of India , 
is a country in South Asia. 
After Chunking: (India ,) (officially) (the 
Republic ) (of) (India , ) (is) (a country ) (in 
South Asia ) (.) 
We have further merged the chunks to form 
bigger chunks. The idea is that, we may 
sometimes find the translation of the merged 
chunk in the target side as well, in which case, 
we would get a bigger fragment of parallel text. 
The merging is done in two ways: 
Strict Merging: We set a value ?V?. Starting 
from the beginning, chunks are merged such that 
the number of tokens in each merged chunk does 
not exceed V. 
 
 
Figure 1. Strict-Merging Algorithm. 
 
Figure 1 describes the pseudo-code for strict 
merging. 
For example, in our example sentence the 
merged chunks will be as following, where V=4: 
(India , officially) (the Republic of ) (India , is) 
(a country) (in South Asia .) 
 
 
Figure 2. Window-Based Merging Algorithm. 
Procedure Window_Merging() 
begin 
Set_Chunk?Set of all English Chunks 
L?Number of chunks in Set_Chunk 
for i = 0 to L-1 
 Words?Set of tokens in i-th Chunk in Set_Chunk 
 Cur_wc?number of tokens in Words 
Ol?i-th chunk in Set_Chunk 
for j = (i+1) to (L-1) 
  C?j-th chunk in Set_Chunk 
  w?set of tokens in C 
  l?number of tokens in w 
  if(Cur_wc + l ? V) 
   Append C at the end of Ol 
   Add l to Cur_wc 
  end if 
 end for 
 Output Ol as the next merged chunk 
end for 
end   
 
Procedure Strict_Merge() 
begin 
Oline ? null 
Cur_wc ? 0 
repeat 
Iline?Next Chunk 
Length?Number of Tokens in Iline 
if(Cur_wc + Length > V) 
Output Oline as the next merged chunk 
  Cur_wc?Length 
 else 
  Append Iline at the end of Oline 
  Add Length to Cur_wc 
 end if 
while (there are more chunks) 
end 
71
 
Figure 3. System Architecture for Finding Parallel Fragments
Window-Based Merging: In this type of 
chunking also, we set a value ?V?, and for each 
chunk we try to merge as many chunks as 
possible so that the number of tokens in the 
merged chunk never exceeds V. 
So, we slide an imaginary window over the 
chunks. For example, for our example sentence 
the merged chunks will be as following, where V 
= 4 : 
(India , officially) (officially the Republic of) 
(the Republic of) (of India , is) (India , is) (is a 
country) (a country) (in South Asia .) 
The pseudo-code of window-based merging is   
described in Figure 2. 
4.3 Chunking of Bengali Sentences 
Since to the best of our knowledge, there is no 
good quality chunking tool for Bengali we did 
not use chunking explicitly. Instead, strict 
merging is done with consecutive V number of 
tokens whereas window-based merging is done 
sliding a virtual window over each token and 
merging tokens so that the number of tokens 
does not exceed V. 
4.4 Finding Parallel Chunks 
After finding the merged English chunks they are 
translated into Bengali using a machine 
translation system that we have already 
developed. This is also the same machine 
translation system whose performance we want 
to improve. Chunks of each of the document 
pairs are then compared to find parallel chunks. 
Each translated source chunk (translated from 
English to Bengali) is compared with all the 
target chunks in the corresponding Bengali-
chunk document. When a translated source 
chunk is considered, we try to align each of its 
token to some token in the target chunk. Overlap 
between token two Bengali chunks B1 and B2, 
where B1 is the translated chunk and B2 is the 
chunk in the Bengali document, is defined as 
follows: 
Overlap(B1,B2) = Number of tokens in B1 for 
which an alignment can be found in B2.  
It is to be noted that Overlap(B1,B2) ? 
Overlap(B2 ,B1). Overlap between chunks is 
found in both ways (from translated source 
chunk to target and from target to translated 
source chunk). If 70% alignment is found in both 
the overlap measures then we declare them as 
parallel. Two issues are important here: the com-
parison of two Bengali tokens and in case an 
alignment is found, which token to retrieve 
(source or target) and how to reorder them. We 
address these two issues in the next two sections. 
4.5 Comparing Bengali Tokens 
For our purpose, we first divide the two tokens 
into their matra (vowel modifiers) part and 
consonant part keeping the relative orders of 
characters in each part same. For example, 
Figure 4 shows the division of the word . 
 
English 
Documents 
English 
Chunks 
Merging 
Translation 
Bengali 
Documents 
Bengali 
Chunks 
Find Parallel Chunks and Reorder  
Merging 
72
 
 
 
 
 
Figure 4. Division of a Bengali Word. 
 
Respective parts of the two words are then 
compared. Orthographic similarities like 
minimum edit distance ratio, longest common 
subsequence ratio, and length of the strings are 
used for the comparison of both parts. 
Minimum Edit Distance Ratio: It is defined 
as  follows: 
 
 
where |B| is the length of the string B and ED is 
the minimum edit distance or levenshtein 
distance calculated as the minimum number of 
edit operations ? insert, replace, delete ? needed 
to transform B1 into B2. 
Longest Common Subsequence Ratio: It is 
defined as follows: 
 
 
 
where LCS is the longest common subsequence 
of two strings. 
Threshold for matching is set empirically. We 
differentiate between shorter strings and larger 
strings. The idea is that, if the strings are short 
we cannot afford much difference between them 
to consider them as a match. In those cases, we 
check for exact match. Also, the threshold for 
consonant part is set stricter because our 
assumption is that consonants contribute more 
toward the word?s pronunciation. 
4.6 Reordering of Source Chunks 
When a translated source chunk is compared 
with a target chunk it is often found that the 
ordering of the tokens in the source chunk and 
the target chunk is different. The tokens in the 
target chunk have a different permutation of 
positions with respect to the positions of tokens 
in the source chunk. In those cases, we reordered 
the positions of the tokens in the source chunk so 
as to reflect the positions of tokens in the target 
chunk because it is more likely that the tokens 
will usually follow the ordering as in the target 
chunk. For example, the machine translation 
output of the English chunk ?from the Atlantic 
Ocean? is ? theke  atlantic  
 (mahasagar)?. We found a target 
chunk ?  (atlantic)  (maha-
sagar)  (theke)  (ebong)? with which 
we could align the tokens of the source chunk 
but in different relative order. Figure 5 shows the 
alignment of tokens.  
 
Figure 5. Alignment of Bengali Tokens. 
 
We reordered the tokens of the source chunk 
and the resulting chunk was ?  
 ?.Also, the token ? ? in the 
target chunk could not find any alignment and 
was discarded. The system architecture of the 
present system is described in figure 3. 
5 Experiments And Results 
5.1 Baseline System 
We randomly extracted 500 sentences each for 
the development set and test set from the initial 
parallel corpus, and treated the rest as the 
training corpus. After filtering on the maximum 
allowable sentence length of 100 and sentence 
length ratio of 1:2 (either way), the training 
corpus contained 22,492 sentences.  
 
V=4 V=7 
Number of English 
Chunks(Strict-Merging) 
579037 376421 
Number of English 
Chunks(Window-
Merging) 
890080 949562 
Number of Bengali 
Chunks(Strict-Merging) 
69978 44113 
Number of Bengali 
Chunks(Window-
Merging) 
230025 249330 
Table 1. Statistics of the Comparable Corpus 
 
V=4 V=7 
Number of Parallel 
Chunks(Strict-Merging) 
1032 1225 
Number of Parallel 
Chunks(Window-Merging) 
1934 2361 
Table 2. Number of Parallel Chunks found 
 
 
Kolkata  
matra
73
 BLEU NIST 
Baseline System(PB-SMT) 10.68 4.12 
Baseline + Parallel 
Chunks(Strict-
Merging) 
V=4 10.91 4.16 
V=7 11.01 4.16 
Baseline + Parallel 
Chunks(Window-
Merging) 
V=4 11.55 4.21 
V=7 11.87 4.29 
 
Table 3.Evaluation of the System 
 
In addition to the target side of the parallel cor-
pus, a monolingual Bengali corpus containing 
406,422 words from the tourism domain was 
used for the target language model. We 
experimented with different n-gram settings for 
the language model and the maximum phrase 
length, and found that a 5-gram language model 
and a maximum phrase length of 7 produced the 
optimum baseline result. We therefore carried 
out the rest of the experiments using these 
settings. 
5.2 Improving Baseline System 
The comparable corpus consisted of 582 English-
Bengali document pairs.  
We experimented with the values V=4 and 
V=7 while doing the merging of chunks both in 
English and Bengali. All the single token chunks 
were discarded. Table 1 shows some statistics 
about the merged chunks for V=4 and V=7.It is 
evident that number of chunks in English 
documents is far more than the number of chunks 
in Bengali documents. This immediately 
suggests that Bengali documents are less 
informative than English documents. When the 
English merged chunks were passed to the 
translation module some of the chunks could not 
be translated into Bengali. Also, some chunks 
could be translated only partially, i.e. some 
tokens could be translated while some could not 
be. Those chunks were discarded. Finally, the 
number of (Strict-based) English merged-chunks 
and number of (Window-based) English merged-
chunks were 285756 and 594631 respectively. 
Two experiments were carried out separately. 
Strict-based  merged English chunks were 
compared with Strict-Based merged Bengali 
chunks. Similarly, window-based merged Eng-
lish chunks were compared with window-based 
merged Bengali chunks. While searching for 
parallel chunks each translated source chunk was 
compared with all the target chunks in the 
corresponding document. Table 2 displays the 
number of parallel chunks found. Compared to 
the number of chunks in the original documents 
the number of parallel chunks found was much 
less. Nevertheless, a quick review of the parallel 
list revealed that most of the chunks were of 
good quality. 
5.3 Evaluation 
We carried out evaluation of the MT quality 
using two automatic MT evaluation metrics: 
BLEU (Papineni et al, 2002) and NIST 
(Doddington, 2002). Table 3 presents the ex-
perimental results. For the PB-SMT experiments, 
inclusion of the extracted strict merged parallel 
fragments from comparable corpora as additional 
training data presented some improvements over 
the PB-SMT baseline. Window based extracted 
fragments are added separately with parallel cor-
pus and that also provides some improvements 
over the PB baseline; however inclusion of win-
dow based extracted phrases in baseline system 
with phrase length 7 improves over both strict 
and baseline in term of BLEU score and NIST 
score. 
Table 3 shows the performance of the PB-
SMT system that shows an improvement over 
baseline with both strict and window based 
merging even if,  we change their phrase length 
from 4 to 7. Table 3 shows that the best 
improvement is achieved when we add parallel 
chunks as window merging with phrase length 7. 
It gives 1.19 BLEU point, i.e., 11.14% relative 
improvement over baseline system. The NIST 
score could be improved  up to 4.12%. Bengali is 
a morphologically rich language and has 
74
relatively free phrase order. The strict based 
extraction does not reflect much improvement 
compared to the window based extraction 
because strict-merging (Procedure Strict_Merge) 
cannot cover up all the segments on either side, 
so very few parallel extractions have been found 
compared to window based extraction.  
6 Conclusion 
In this work, we tried to find English-Bengali 
parallel fragments of text from a comparable 
corpus built from Wikipedia documents. We 
have successfully improved the performance of 
an existing machine translation system. We have 
also shown that out-of-domain corpus happened 
to be useful for training of a domain specific MT 
system. The future work consists of working on 
larger amount of data. Another focus could be on 
building ad-hoc comparable corpus from WEB 
and using it to improve the performance of an 
existing out-of-domain MT system. This aspect 
of work is particularly important because the 
main challenge would be of domain adaptation. 
Acknowledgements 
This work has been partially supported by a grant 
from the English to Indian language Machine 
Translation (EILMT) project funded by the 
Department of Information and Technology 
(DIT), Government of India.  
Reference 
Chiao, Y. C., & Zweigenbaum, P. (2002, August). 
Looking for candidate translational equivalents in 
specialized, comparable corpora. In Proceedings of 
the 19th international conference on Computation-
al linguistics-Volume 2 (pp. 1-5). Association for 
Computational Linguistics. 
D?jean, H., Gaussier, ?., & Sadat, F. (2002). Bilin-
gual terminology extraction: an approach based on 
a multilingual thesaurus applicable to comparable 
corpora. In Proceedings of the 19th International 
Conference on Computational Linguistics COLING 
(pp. 218-224). 
Doddington, G. (2002, March). Automatic evaluation 
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second 
international conference on Human Language 
Technology Research (pp. 138-145). Morgan 
Kaufmann Publishers Inc.. 
Fung, P., & McKeown, K. (1997, August). Finding 
terminology translations from non-parallel corpora. 
In Proceedings of the 5th Annual Workshop on 
Very Large Corpora (pp. 192-202). 
Fung, P., & Yee, L. Y. (1998, August). An IR ap-
proach for translating new words from nonparallel, 
comparable texts. In Proceedings of the 17th inter-
national conference on Computational linguistics-
Volume 1 (pp. 414-420). Association for Computa-
tional Linguistics. 
Hiroyuki, K. A. J. I. (2005). Extracting translation 
equivalents from bilingual comparable corpora. 
IEICE Transactions on information and systems, 
88(2), 313-323. 
 Kneser, R., & Ney, H. (1995, May). Improved back-
ing-off for m-gram language modeling. In Acous-
tics, Speech, and Signal Processing, 1995. 
ICASSP-95., 1995 International Conference on 
(Vol. 1, pp. 181-184). IEEE. 
Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., 
Federico, M., Bertoldi, N., ... & Herbst, E. (2007, 
June). Moses: Open source toolkit for statistical 
machine translation. In Proceedings of the 45th 
Annual Meeting of the ACL on Interactive Poster 
and Demonstration Sessions (pp. 177-180). 
Association for Computational Linguistics. 
Koehn, P., Och, F. J., & Marcu, D. (2003, May). Sta-
tistical phrase-based translation. In Proceedings of 
the 2003 Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1 
(pp. 48-54). Association for Computational Lin-
guistics. 
 Munteanu, D. S., & Marcu, D. (2006, July). Extract-
ing parallel sub-sentential fragments from non-
parallel corpora. In Proceedings of the 21st Inter-
national Conference on Computational Linguistics 
and the 44th annual meeting of the Association for 
Computational Linguistics (pp. 81-88). Association 
for Computational Linguistics.. 
Och, F. J. (2003, July). Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting on Association for Com-
putational Linguistics-Volume 1 (pp. 160-167). As-
sociation for Computational Linguistics. 
Och, F. J., & Ney, H. (2000). Giza++: Training of 
statistical translation models. 
Otero, P. G. (2007). Learning bilingual lexicons from 
comparable english and spanish corpora. Proceed-
ings of MT Summit xI, 191-198. 
Otero, P. G., & L?pez, I. G. (2010). Wikipedia as 
multilingual source of comparable corpora. In Pro-
ceedings of the 3rd Workshop on Building and Us-
ing Comparable Corpora, LREC (pp. 21-25). 
 Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. 
(2002, July). BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of the 
40th annual meeting on association for computa-
75
tional linguistics (pp. 311-318). Association for 
Computational Linguistics. 
Rapp, R. (1999, June). Automatic identification of 
word translations from unrelated English and Ger-
man corpora. In Proceedings of the 37th annual 
meeting of the Association for Computational Lin-
guistics on Computational Linguistics (pp. 519-
526). Association for Computational Linguistics. 
Saralegui, X., San Vicente, I., & Gurrutxaga, A. 
(2008). Automatic generation of bilingual lexicons 
from comparable corpora in a popular science do-
main. In LREC 2008 workshop on building and us-
ing comparable corpora. 
 Smith, J. R., Quirk, C., & Toutanova, K. (2010, 
June).Extracting parallel sentences from 
comparable corpora using document level 
alignment. In Human Language Technologies: The 
2010 Annual Conference of the North American 
Chapter of the Association for Computational 
Linguistics (pp. 403-411). Association for 
Computational Linguistics. 
Stolcke, A. (2002, September). SRILM-an extensible 
language modeling toolkit. In Proceedings of the 
international conference on spoken language 
processing (Vol. 2, pp. 901-904). 
 
 
 
76
Proceedings of the Second Workshop on Hybrid Approaches to Translation, pages 94?101,
Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational Linguistics
A Hybrid Word Alignment Model for Phrase-Based Statistical Ma-
chine Translation 
 
 
Santanu Pal*, Sudip Kumar Naskar? and Sivaji Bandyopadhyay* 
*Department of Computer Science & Engineering 
Jadavpur University, Kolkata, India 
santanu.pal.ju@gmail.com, sivaji_cse_ju@yahoo.com 
?
 Department of Computer & System Sciences 
Visva-Bharati University, Santiniketan, India 
sudip.naskar@gmail.com 
 
  
 
Abstract 
This paper proposes a hybrid word alignment 
model for Phrase-Based Statistical Machine 
translation (PB-SMT). The proposed hybrid 
alignment model provides most informative 
alignment links which are offered by both un-
supervised and semi-supervised word align-
ment models. Two unsupervised word align-
ment models (GIZA++ and Berkeley aligner) 
and a rule based aligner are combined togeth-
er. The rule based aligner only aligns named 
entities (NEs) and chunks. The NEs are 
aligned through transliteration using a joint 
source-channel model. Chunks are aligned 
employing a bootstrapping approach by trans-
lating the source chunks into the target lan-
guage using a baseline PB-SMT system and 
subsequently validating the target chunks us-
ing a fuzzy matching technique against the 
target corpus. All the experiments are carried 
out after single-tokenizing the multi-word 
NEs.  Our best system provided significant 
improvements over the baseline as measured 
by BLEU.  
1 Introduction 
Word alignment is the backbone of PB-SMT sys-
tem or any data driven approaches to Machine 
Translation (MT) and it has received a lot of at-
tention in the area of statistical machine transla-
tion (SMT) (Brown et al, 1993; Och and Ney, 
2003; Koehn et al, 2003). Word alignment is not 
an end task in itself and is usually used as an in-
termediate step in SMT. Word alignment is de-
fined as the detection of corresponding alignment 
of words from parallel sentences that are transla-
tion of each other. Statistical machine translation 
usually suffers from many-to-many word links 
which existing statistical word alignment algo-
rithms can not handle well.  
The unsupervised word alignment models are 
based on IBM models 1?5 (Brown et al, 1993) 
and the HMM model (Ney and Vogel, 1996; Och 
and Ney, 2003). Models 3, 4 and 5 are based on 
fertility based models which are asymmetric. To 
improve alignment quality, the Berkeley Aligner 
is based on the symmetric property by intersect-
ing alignments induced in each translation direc-
tion. 
In the present work, we propose improvement 
of word alignment quality by combining three 
word alignment tables (i) GIZA++ alignment (ii) 
Berkeley Alignment and (iii) rule based align-
ment. Our objective is to perceive the effective-
ness of the Hybrid model in word alignment by 
improving the quality of translation in the SMT 
system. In the present work, we have implement-
ed a rule based alignment model by considering 
several types of chunks which are automatically 
extracted on the source side. Each individual 
source chunk is translated using a baseline PB-
SMT system and validated with the target chunks 
on the target side. The validated source-target 
chunks are added in the rule based alignment 
table. Work has been carried out into three direc-
tions: (i) three alignment tables are combined 
together by taking their union; (ii) extra align-
ment pairs are added into the alignment table. 
This is a well-known practice in domain adapta-
tion in SMT (Eck et al, 2004; Wu et al, 2008); 
(iii) the alignment table is updated through semi-
supervised alignment technique. 
94
The remainder of the paper is organized as fol-
lows. Section 2 discusses related work. The pro-
posed hybrid word alignment model is described 
in Section 3. Section 4 presents the tools and re-
sources used for the various experiments. Section 
5 includes the results obtained, together with 
some analysis. Section 6 concludes and provides 
avenues for further work. 
2 Related Works  
Zhou et al (2004) proposed a multi lingual filter-
ing algorithm that generates bilingual chunk 
alignment from Chinese-English parallel corpus. 
The algorithm has three steps, first, from the par-
allel corpus; the most frequent bilingual chunks 
are extracted. Secondly, the participating chunks 
for alignments are combined into a cluster and 
finally one English chunk is generated corre-
sponding to a Chinese chunk by analyzing the 
highest co-occurrences of English chunks. Bilin-
gual knowledge can be extracted using chunk 
alignment (Zhou et. al., 2004). Pal et, al. (2012) 
proposed a bootstrapping method for chunk 
alignment; they used an SMT based model for 
chunk translation and then aligned the source-
target chunk pairs after validating the translated 
chunk. Ma et. al. (2007) simplified the task of 
automatic word alignment as several consecutive 
words together correspond to a single word in the 
opposite language by using the word aligner it-
self, i.e., by bootstrapping on its output. A Max-
imum Entropy model based approach for Eng-
lish?Chinese NE alignment which significantly 
outperforms IBM Model4 and HMM has been 
proposed by Feng et al (2004). They considered 
4 features: translation score, transliteration score, 
source NE and target NE's co-occurrence score 
and the distortion score for distinguishing identi-
cal NEs in the same sentence. Moore (2003) pre-
sented an approach where capitalization cues 
have been used for identifying NEs on the Eng-
lish side. Statistical techniques are applied to de-
cide which portion of the target language corre-
sponds to the specified English NE, for simulta-
neous NE identification and translation. 
To improve the learning process of unlabeled 
data using labeled data (Chapelle et al, 2006), 
the semi-supervised learning method is the most 
useful learning technique. Semi-supervised 
learning is a broader area of Machine Learning. 
Researchers have begun to explore semi-
supervised word alignment models that use both 
labeled and unlabeled data. Fraser and Marcu 
(2006) proposed a semi-supervised training algo-
rithm. The weighting parameters are learned 
from discriminative error training on labeled da-
ta, and the parameters are estimated by maxi-
mum-likelihood EM training on unlabeled data. 
They have also used a log-linear model which is 
trained on the available labeled data to improve 
performance. Interpolating human alignments 
with automatic alignments has been proposed by 
Callison-Burch et al (2004), where the align-
ments of higher quality have gained much higher 
weight than the lower-quality alignments. Wu et 
al. (2006) have developed two separate models 
of standard EM algorithm which learn separately 
from both labeled and unlabeled data. Two mod-
els are then interpolated as a learner in the semi-
supervised Ada-Boost algorithm to improve 
word alignment. Ambati et al (2010) proposed 
active learning query strategies to identify highly 
uncertain or most informative alignment links 
under an unsupervised word alignment model. 
Intuitively, multiword NEs on the source and 
the target sides should be both aligned in the par-
allel corpus and translated as a whole. However, 
in the state-of-the-art PB-SMT systems, the con-
stituents of multiword NE are marked and 
aligned as parts of consecutive phrases, since 
PB-SMT (or any other approaches to SMT) does 
not generally treat multiword NEs as special to-
kens. This is the motivations behind considering 
NEs for special treatment in this work by con-
verting into single tokens that makes sure that 
PB-SMT also treats them as a whole 
Another problem with SMT systems is the er-
roneous word alignment. Sometimes some words 
are not translated in the SMT output sentence 
because of the mapping to NULL token or erro-
neous mapping during word alignment. Verb 
phrase translation also creates major problems. 
The words inside verb phrases are generally not 
aligned one-to-one; the alignments of the words 
inside source and target verb phrases are mostly 
many-to-many particularly so for the English?
Bengali language pair.  
The first objective of the present work is to see 
how single tokenization and alignment of NEs on 
both the sides affects the overall MT quality. The 
second objective is to see whether Hybrid word 
alignment model of both unsupervised and semi-
supervised techniques enhance the quality of 
translation in the SMT system rather than the 
single tokenized NE level parallel corpus applied 
to the hybrid model.  
We carried out the experiments on English?
Bengali translation task. Bengali shows high 
morphological richness at lexical level. Lan-
95
guage resources in Bengali are not widely avail-
able. 
3 Hybrid Word Alignment Model 
The hybrid word alignment model is described as 
the combination of three word alignment models 
as follows: 
3.1 Word Alignment Using GIZA++ 
GIZA++ (Och and Ney, 2003) is a statistical 
word alignment tool which incorporates all the 
IBM 1-5 models. GIZA++ facilitates fast devel-
opment of statistical machine translation (SMT) 
systems. In case of low-resource language pairs 
the quality of word alignments is typically quite 
low and it also deviates from the independence 
assumptions made by the generative models. 
Although huge amount of parallel data enables 
the model parameters to acquire better estimation, 
a large number of language pairs still lacks from 
the unavailability of sizeable amount of parallel 
data. GIZA++ has some draw-backs. It allows at 
most one source word to be aligned with each 
foreign word. To resolve this issue, some tech-
niques have already been applied such as: the 
parallel corpus is aligned bidirectionally; then the 
two alignment tables are reconciled using differ-
ent heuristics e.g., intersection, union, and most 
recently grow-diagonal-final and grow-diagonal-
final-and heuristics have been applied. In spite of 
these heuristics, the word alignment quality for 
low-resource language pairs is still low and calls 
for further improvement. We describe our ap-
proach of improving word alignment quality in 
the following three subsections. 
3.2 Word Alignment Using Berkley Aligner 
The recent advancements in word alignment is 
implemented in Berkeley Aligner (Liang et al, 
2006) which allows both unsupervised and su-
pervised approach to align word from parallel 
corpus. We initially train the parallel corpus us-
ing unsupervised technique. We make a few 
manual corrections to the alignment table pro-
duced by the unsupervised aligner. Then we ap-
ply this corrected alignment table as gold stand-
ard training data for the supervised aligner. The 
Berkeley aligner is an extension of the Cross Ex-
pectation Maximization word aligner. Berkeley 
aligner is a very useful word aligner because it 
allows for supervised training, enabling us to 
derive knowledge from already aligned parallel 
corpus or we can use the same corpus by updat-
ing the alignments using some rule based meth-
ods. Our approach deals with the latter case. The 
supervised technique of Berkeley aligner helps 
us to align those words which could not be 
aligned by rule based word aligner.  
3.3 Rule Based Word Alignment 
The proposed Rule based aligner aligns Named 
Entities (NEs) and chunks. For NE alignment, 
we first identify NEs from the source side (i.e. 
English) using Stanford NER.  The NEs on the 
target side (i.e. Bengali) are identified using a 
method described in (Ekbal and Bandyopadhyay, 
2009). The accuracy of the Bengali Named Enti-
ty recognizers (NER) is much poorer compared 
to that of English NER due to several reasons: (i) 
there is no capitalization cue for NEs in Bengali; 
(ii) most of the common nouns in Bengali are 
frequently used as proper nouns; (iii) suffixes 
(case markers, plural markers, emphasizers, 
specifiers) get attached to proper names as well 
in Bengali. Bengali shallow parser 1  has been 
used to improve the performance of NE identifi-
cation by considering proper names as NE.  
Therefore, NER and shallow parser are jointly 
employed to detect NEs from the Bengali sen-
tences. The source NEs are then transliterated 
using a modified joint source-channel model 
(Ekbal et al, 2006) and aligned to their target 
side equivalents following the approach of Pal et 
al. (2010). The target side equivalents NEs are 
transformed into canonical form after omitting 
their ?matras?. Similarly Bengali NEs are also 
transformed into canonical forms as Bengali NEs 
may differ in their choice of matras (vowel mod-
ifiers). The transliterated NEs are then matched 
with the corresponding parallel target NEs and 
finally we align the NEs if match is found.   
After identification of multiword NEs on both 
sides, we pre-processed the corpus by replacing 
space with the underscore character (?_?). We 
have used underscore (?_?) instead of hyphen (?-
?) since there already exists some hyphenated 
words in the corpus.  The use of the underscore 
(?_?) character also facilitates to de-tokenize the 
single-tokenized NEs after decoding. 
For chunk alignment, the source sentences of 
the parallel corpus are parsed using Stanford 
POS tagger. The chunks of the sentences are ex-
tracted using CRF chunker2. The chunker detects 
the boundaries of noun, verb, adjective, adverb 
                                                 
1 
http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallo
w_parser.php 
2 http://crfchunker.sourceforge.net/ 
96
and prepositional chunks from the sentences. In 
case of prepositional phrase chunks, we have 
taken a special attention: we have expanded the 
prepositional phrase chunk by examining a single 
noun chunk followed by a preposition or a series 
of noun chunks separated by conjunctions such 
as 'comma', 'and' etc.  For each individual chunk, 
the head word is identified. Similarly target side 
sentences are parsed using a shallow parser. The 
individual target side Bengali chunks are extract-
ed from the parsed sentences.  The head words 
for all individual chunks on the target side are 
also marked. If the translated head word of a 
source chunk matches with the headword of a 
target chunk then we hypothesize that these two 
chunks are translations of each other.  
The extracted source chunks are translated us-
ing a baseline SMT model trained on the same 
corpus. The translated chunks are validated 
against the target chunks found in the corre-
sponding target sentence. During the validation 
process, if any match is found between the trans-
lated chunk and a target chunk then the source 
chunk is directly aligned with the original target 
chunk. Otherwise, the source chunk is ignored in 
the current iteration for any possible alignment 
and is considered in the next iterations. 
 
 
 
 
 
 
Figure 1.a: Rule based alignments 
 
 
 
 
 
 
Figure 1.b: Gold standard alignments 
 
Figure 1: Establishing alignments through Rule 
based methods. 
 
The extracted chunks on the source side may 
not have a one to one correspondence with the 
target side chunks. The alignment validation pro-
cess is focused on the proper identification of the 
head words and not between the translated 
source chunk and target chunk. The matching 
process has been carried out using a fuzzy 
matching technique. If both sides contain only 
one chunk after aligning the remaining chunks 
then the alignment is trivial. After aligning the 
individual chunks, we also establish word align-
ments between the matching words in those 
aligned chunks. Thus we get a sentence level 
source-target word alignment table.  
Figure 1 shows how word alignments are es-
tablished between a source-target sentence pair 
using the rule based method. Figure 1.a shows 
the alignments obtained through rule based 
method. The solid links are established through 
transliteration (for NEs) and translation. The dot-
ted arrows are also probable candidates for intra-
chunk word alignments; however they are not 
considered in the present work. Figure 1.b shows 
the gold standard alignments for this sentence 
pair.  
3.4  Hybrid Word alignment Model  
The hybrid word alignment method combines 
three different kinds of word alignments ? Gi-
za++ word alignment with grow-diag-final-and 
(GDFA) heuristic, Berkeley aligner and rule 
based aligner. We have followed two different 
strategies to combine the three different word 
alignment tables.  
 
Union 
In the union method all the alignment tables are 
united together and duplicate entries are removed. 
 
ADD additional Alignments  
In this method we consider either of the align-
ments generated by GIZA++ GDFA (A1) or 
Berkeley aligner (A2) as the standard alignment 
as the rule based aligner fails to align all words 
in the parallel sentences. From the three set of 
alignments A1, A2 and A3, we propose an 
alignment combination method as described in 
algorithm 1. 
 
ALGORITHM: 1 
 
Step 1: Choose either A1 or A2 as the standard 
alignment (SA). 
Step 2: Correct the alignments in SA using the 
alignment table of A3. 
Step 3: if A2 is considered as SA then find addi-
tional alignment from A1 and A3 using intersec-
tion method (A1?A3) otherwise find additional 
alignment from A2 and A3 (using A2?A3).   
Step 4: Add additional entries with SA. 
[Jaipur] [golapi sohor name] [porichito] [.] 
[Jaipur] [is known] [as [Pink City]] [.] 
 
[Jaipur] [golapi sohor name] [porichito] [.] 
[Jaipur] [is known] [as [Pink City]] [.] 
 
97
3.5 Berkeley Semi-supervised Alignment 
The correctness of the alignments is verified by 
manually checking the performance of the vari-
ous alignment system. We start with the com-
bined alignment table which is produced by Al-
gorithm 1. Iinitially, we take a subset of the 
alignments by manually inspecting from the 
combined alignment table. Then we train the 
Barkley supervised aligner with this labeled data. 
A subset of the unlabeled data from the com-
bined alignment table is tested with the super-
vised model. The output is then added as addi-
tional labeled training data for the supervised 
training method for the next iteration. Using this 
bootstrapping approach, the amount of labeled 
training data for the supervised aligner is gradu-
ally increased. The process is continued until 
there are no more unlabelled training data. In this 
way we tune the whole alignment table for the 
entire parallel corpus. The process is carried out 
in a semi-supervised manner. 
4 Tools and resources Used  
A sentence-aligned English-Bengali parallel cor-
pus containing 23,492 parallel sentences from 
the travel and tourism domain has been used in 
the present work. The corpus has been collected 
from the consortium-mode project ?Development 
of English to Indian Languages Machine Trans-
lation (EILMT) System - Phase II? 3. The Stan-
ford Parser4 and CRF chunker5 have been used 
for identifying chunks and Stanford NER has 
been used to identify named entities in the source 
side of the parallel corpus.  
The target side (Bengali) sentences are parsed 
by using the tools obtained from the consortium 
mode project ?Development of Indian Language 
to Indian Language Machine Translation (IL-
ILMT) System - Phase II6?. 
The effectiveness of the present work has been 
tested by using the standard log-linear PB-SMT 
model as our baseline system: phrase-extraction 
heuristics described in (Koehn et al, 2003), , 
MERT (minimum-error-rate training) (Och, 
2003) on a held-out development set, target 
                                                 
3  The EILMT project is funded by the Department of Elec-
tronics and Information Technology (DEITY), Ministry of 
Communications and Information Technology (MCIT), 
Government of India. 
4 http://nlp.stanford.edu/software/lex-parser.shtml 
5 http://crfchunker.sourceforge.net/ 
6   The IL-ILMT project is funded by the Department of 
Electronics and Information Technology (DEITY), Ministry 
of Communications and Information Technology (MCIT), 
Government of India. 
language model trained using SRILM toolkit 
(Stolcke, 2002) with Kneser-Ney smoothing 
(Kneser and Ney, 1995) and the Moses decoder 
(Koehn et al, 2007) have been used in the 
present study. 
5 Experiments and Results 
We have randomly selected 500 sentences each 
for the development set and the test set from the 
initial parallel corpus. The rest are considered as 
the training corpus. The training corpus was fil-
tered with the maximum allowable sentence 
length of 100 words and sentence length ratio of 
1:2 (either way). Finally the training corpus con-
tained 22,492 sentences. In addition to the target 
side of the parallel corpus, a monolingual Benga-
li corpus containing 488,026 words from the 
tourism domain was used for building the target 
language model. We experimented with different 
n-gram settings for the language model and the 
maximum phrase length and found that a 4-gram 
language model and a maximum phrase length of 
7 produced the optimum baseline result. We car-
ried out the rest of the experiments using these 
settings. 
We experimented with the system  over 
various combinations of word alignment models. 
Our hypothesis focuses mainly on the theme that 
proper alignment of words will result in 
improvement of the system performance in terms 
of translation quality.  
141,821 chunks were identified from the 
source corpus, of which 96,438 (68%) chunks 
were aligned by the system. 39,931 and 28,107 
NEs were identified from the source and target 
sides of the parallel corpus respectively, of which 
22,273 NEs are unique in English and 22,010 
NEs in Bengali. A total of 14,023 NEs have been 
aligned through transliteration.  
The experiments have been carried out with 
various experimental settings: (i) single 
tokenization of NEs on both sides of the parallel 
corpus, (ii) using Berkeley Aligner with 
unsupervised training, (iii) union of the three 
alignment models: rule based, GIZA++ with 
GDFA and Berkeley Alignment, (iv) 
hybridization of the three alignment models and 
(v) supervised Berkeley Aligner. Eextrinsic 
evaluation was carried out on the MT quality 
using BLEU (Papineni et al, 2002) and NIST 
(Doddington, 2002). 
98
 
 
Experiment Exp 
no. 
BLEU NIST 
Baseline system using GIZA++ with GDFA 1 10.92 4.13 
PB-SMT system using Berkeley Aligner 2 11.42 4.16 
Union of all Alignments 3 11.12 4.14 
PB-SMT System with Hybrid Alignment by considering (a) 
GIZA++ as the standard alignment) (b) Berkeley alignment 
as the standard alignment) 
4a? 15.38 4.30 
4b? 15.92 4.36 
Single tokenized NE + Exp 1 5 11.68 4.17 
Single tokenized NE + Exp 2 6 11.82 4.19 
Single tokenized NE + (a) Exp 4a (b) Exp 4b 7a? 16.58 4.45 
7b? 17.12 4.49 
PB-SMT System with semi-supervised Berkeley Aligner + 
Single tokenized NE 
8? 20.87 4.71 
 
Table: 1 Evaluation results for different experimental setups. (The ??? marked systems produce statis-
tically significant improvements on BLEU over the baseline system) 
 
 
The baseline system (Exp 1) is the state-of-art 
PB-SMT system where GIZA++ with grow-diag-
final-and has been used as the word alignment 
model. Experiment 2 provides better results than 
experiment 1 which signifies that Berkeley 
Aligner performs better than GIZA++ for the 
English-Bengali translation task. The union of all 
thee alignments (Exp 3) provides better scores 
than the baseline; however it cannot beat the re-
sults obtained with the Berkeley Aligner alone. 
Hybrid alignment model with GIZA++ as the  
standard alignment (Exp 4a) produces statistical-
ly significant improvements over the baseline. 
Similarly the use of Berkeley Aligner as the 
standard alignment for hybrid alignment model 
(Exp 4b) also results in statistically significant 
improvements over Exp 2. These two experi-
ments (Exp 4a and 4b) demonstrate the effec-
tiveness of the hybrid alignment model. It is to 
be noticed that hybrid alignment model works 
better with the Berkeley Aligner than with 
GIZA++. 
Single-tokenization of the NEs (Exp 5, 6, 7a 
and 7b) improves the system performance to 
some extent over the corresponding experiments 
without single-tokenization (Exp 1, 2, 4a and 
4b); however, these improvements are not statis-
tically significant. The Berkeley semi-supervised 
alignment method using a bootstrapping ap-
proach together with single-tokenization of NEs 
provided the overall best performance in terms of 
both BLEU and NIST and the corresponding im-
provement is statistically significant on BLEU 
over rest of the experiments. 
6 Conclusion and Future Work 
The paper proposes a hybrid word alignment 
model for PB-SMT. The paper also shows how 
effective pre-processing of NEs in the parallel 
corpus and direct incorporation of their align-
ment in the word alignment model can improve 
SMT system performance. In data driven ap-
proaches to MT, specifically for scarce resource 
data, this approach can help to upgrade the state-
of-art machine translation quality as well as the 
word alignment quality. . The hybrid model with 
the use of the semi-supervised technique of the 
Berkeley word aligner in a bootstrapping manner, 
together with single tokenization of NEs, pro-
vides substantial improvements (9.95 BLEU 
points absolute, 91.1% relative) over the base-
line. On manual inspection of the output we 
found that our best system provides more accu-
99
rate lexical choice as well as better word order-
ing than the baseline system.  
As future work we would like to explore how 
to get the best out of multiple word alignments. 
Furthermore, integrating the knowledge about 
multi-word expressions into the word alignment 
models is another future direction for this work. 
 
Acknowledgement 
 
The work has been carried out with support from 
the project ?Development of English to Indian 
Languages Machine Translation (EILMT) Sys-
tem - Phase II? funded by Department of Infor-
mation Technology, Government of India. 
References  
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. 
In ACL-44: Proceedings of the 21st International 
Conference on Computational Linguistics and the 
44th annual meeting of the Association for Compu-
tational Linguistics (ACL-2006), Morristown, NJ, 
USA. pages 769?776. 
Brown, Peter F., Stephen A. Della Pietra, Vincent J. 
Della Pietra, and Robert L. Mercer. 1993. The 
mathematics of statistical machine translation: pa-
rameter estimation. Computational Linguistics, 
19(2):263-311. 
Chris Callison-Burch, David Talbot, and Miles Os-
borne. 2004. Statistical machine translation with 
word- and sentence-aligned parallel corpora. In 
ACL 2004, page 175, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics. 
Dempster, A.P., N.M. Laird, and D.B. Rubin. 1977). 
Maximum Likelihood from Incomplete Data via 
the EM Algorithm. Journal of the Royal Statistical 
Society, Series B (Methodological) 39 (1): 1?38. 
Doddington, George. 2002. Automatic evaluation of 
machine translation quality using n-gram cooccur-
rence statistics. In Proceedings of the Second In-
ternational Conference on Human Language Tech-
nology Research (HLT-2002), San Diego, CA, pp. 
128-132. 
Eck, Matthias, Stephan Vogel, and Alex Waibel. 
2004. Improving statistical machine translation in 
the medical domain using the Unified Medical 
Language System. In Proc. of the 20th Internation-
al Conference on Computational Linguistics (COL-
ING 2004), Geneva, Switzerland, pp. 792-798. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2008. Maxi-
mum Entropy Approach for Named Entity Recog-
nition in Indian Languages. International Journal 
for Computer Processing of Languages (IJCPOL), 
Vol. 21 (3), 205-237. 
Ekbal, Asif, and Sivaji Bandyopadhyay. 2009. Voted 
NER system using appropriate unlabeled data. In 
proceedings of the ACL-IJCNLP-2009 Named En-
tities Workshop (NEWS 2009), Suntec, Singapore, 
pp.202-210. 
Feng, Donghui, Yajuan Lv, and Ming Zhou. 2004. A 
new approach for English-Chinese named entity 
alignment. In Proceedings of the 2004 Conference 
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2004), Barcelona, Spain, pp. 
372-379. 
Feng, Donghui, Yajuan Lv, and Ming Zhou. 2004. A 
new approach for English-Chinese named entity 
alignment. In Proceedings of the 2004 Conference 
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2004), Barcelona, Spain, pp. 
372-379. 
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment 
models. Computational Linguistics, pages 19?51. 
Huang, Fei, Stephan Vogel, and Alex Waibel. 2003. 
Automatic extraction of named entity translingual 
equivalence based on multi-feature cost minimiza-
tion. In Proceedings of the ACL-2003 Workshop 
on Multilingual and Mixed-language Named Entity 
Recognition, 2003, Sapporo, Japan, pp. 9-16. 
HuaWu, HaifengWang, and Zhanyi Liu. 2006. Boost-
ing statistical word alignment using labeled and un-
labeled data. In Proceedings of the COLING/ACL 
on Main conference poster sessions, pages 913?
920, Morristown, NJ, USA. Association for Com-
putational Linguistics.  
Kneser, Reinhard, and Hermann Ney. 1995. Improved 
backing-off for m-gram language modeling. In 
Proceedings of the IEEE Internation Conference on 
Acoustics, Speech, and Signal Processing 
(ICASSP), vol. 1, pp. 181?184. Detroit, MI. 
Koehn, Philipp, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT-NAACL 2003: conference com-
bining Human Language Technology conference 
series and the North American Chapter of the As-
sociation for Computational Linguistics conference 
series,  Edmonton, Canada, pp. 48-54. 
Koehn, Philipp, Hieu Hoang, Alexandra Birch, Chris 
Callison-Burch, Marcello Federico, Nicola Ber-
toldi, Brooke Cowan, Wade Shen, Christine Mo-
ran, Richard Zens, Chris Dyer, Ond?ej Bojar, Alex-
andra Constantin, and Evan Herbst. 2007. Moses: 
open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual meeting of 
the Association for Computational Linguistics 
(ACL 2007): Proceedings of demo and poster ses-
sions, Prague, Czech Republic, pp. 177-180. 
Koehn, Philipp. 2004. Statistical significance tests for 
machine translation evaluation. In  EMNLP-2004: 
100
Proceedings of the 2004 Conference on Empirical 
Methods in Natural Language Processing, 25-26 
July 2004, Barcelona, Spain, pp 388-395. 
O. Chapelle, B. Sch?olkopf, and A. Zien, editors. 
2006. Semi-Supervised Learning. MIT Press, 
Cambridge, MA. 
Och, Franz J. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics (ACL-2003), Sapporo, 
Japan, pp. 160-167. 
Pal, Santanu, Sivaji Bandyopadhyay. 2012, ?Boot-
strapping Chunk Alignment in Phrase-Based Sta-
tistical Machine Translation?, Joint Workshop on 
Exploiting Synergies between Information Retriev-
al and Machine Translation (ESIRMT) and Hybrid 
Approaches to Machine Translation (HyTra), 
EACL-2012, Avignon, France, pp. 93-100 . 
Pal, Santanu., Sudip Kumar Naskar, Pavel Pecina, 
Sivaji Bandyopadhyay and Andy Way. 2010, Han-
dling Named Entities and Compound Verbs in 
Phrase-Based Statistical Machine Translation, In 
proc. of the workshop on Multiword expression: 
from theory to application (MWE-2010), The 23rd 
International conference of computational linguis-
tics (Coling 2010),Beijing, Chaina, pp. 46-54. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a method for automat-
ic evaluation of machine translation. In Proceed-
ings of the 40th Annual Meeting of the Association 
for Computational Linguistics (ACL-2002), Phila-
delphia, PA, pp. 311-318. 
Percy Liang, Ben Taskar, Dan Klein. 2006.  6th Pro-
ceedings of the main conference on Human Lan-
guage Technology Conference of the North Ameri-
can Chapter of the Association of Computational 
Linguistics, HLT-NAACL-2006, Pages 104-111 
Stolcke, A. SRILM?An Extensible Language Mod-
eling Toolkit. Proc. Intl. Conf. on Spoken Lan-
guage Processing, vol. 2, pp. 901?904, Denver 
(2002). 
Vamshi Ambati, Stephan Vogel, Jaime Carbonell. 
2010, 10th Proceedings of the NAACL HLT 2010 
Workshop on Active Learning for Natural Lan-
guage Processing (ALNLP-2010), Pages 10-17. 
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in statis-
tical translation. In Proc. of the 16th International 
Conference on Computational Linguistics (COL-
ING 1996), Copenhagen, pp. 836-841. 
Wu, Hua Haifeng Wang, and Chengqing Zong. 2008. 
Domain adaptation for statistical machine transla-
tion with domain dictionary and monolingual cor-
pora. In Proc. of the 22nd International Conference 
on Computational Linguistics (COLING 2008),  
Manchester, UK, pp. 993-1000. 
X. Zhu. 2005. Semi-Supervised Learning Literature 
Survey. Technical Report 1530, Computer Scienc-
es, University of Wisconsin-Madison. 
http://www.cs.wisc.edu/_jerryzhu/pub/ssl_survey.p
df. 
 
101
