Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 533?540, Prague, June 2007. c?2007 Association for Computational Linguistics
 
Phrase Reordering Model Integrating Syntactic Knowledge for SMT 
Dongdong Zhang, Mu Li, Chi-Ho Li, Ming Zhou 
 
Microsoft Research Asia 
Beijing, China 
{dozhang,muli,chl,mingzhou}@microsoft.com 
 
 
Abstract 
Reordering model is important for the sta-
tistical machine translation (SMT). Current 
phrase-based SMT technologies are good at 
capturing local reordering but not global 
reordering. This paper introduces syntactic 
knowledge to improve global reordering 
capability of SMT system. Syntactic know-
ledge such as boundary words, POS infor-
mation and dependencies is used to guide 
phrase reordering. Not only constraints in 
syntax tree are proposed to avoid the reor-
dering errors, but also the modification of 
syntax tree is made to strengthen the capa-
bility of capturing phrase reordering. Fur-
thermore, the combination of parse trees 
can compensate for the reordering errors 
caused by single parse tree. Finally, expe-
rimental results show that the performance 
of our system is superior to that of the 
state-of-the-art phrase-based SMT system. 
1 Introduction 
In the last decade, statistical machine translation 
(SMT) has been widely studied and achieved good 
translation results. Two kinds of SMT system have 
been developed, one is phrase-based SMT and the 
other is syntax-based SMT.  
In phrase-based SMT systems (Koehn et al, 
2003; Koehn, 2004), foreign sentences are firstly 
segmented into phrases which consists of adjacent 
words. Then source phrases are translated into tar-
get phrases respectively according to knowledge 
usually learned from bilingual parallel corpus. Fi-
nally the most likely target sentence based on a 
certain statistical model is inferred by combining 
and reordering the target phrases with the aid of 
search algorithm. On the other hand, syntax-based 
SMT systems (Liu et al, 2006; Yamada et al, 
2001) mainly depend on parse trees to complete 
the translation of source sentence.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: A reordering example 
 
As studied in previous SMT projects, language 
model, translation model and reordering model are 
the three major components in current SMT sys-
tems. Due to the difference between the source and 
target languages, the order of target phrases in the 
target sentence may differ from the order of source 
phrases in the source sentence.  To make the trans-
lation results be closer to the target language style, 
a mathematic model based on the statistic theory is 
constructed to reorder the target phrases. This sta-
tistic model is called as reordering model. As 
shown in Figure 1, the order of the translations of 
???? and ??? is changed. The order of the 
IP 
VP 
ADVP 
NP 
DNP VP 
NP 
NN 
AD DEG 
?? 
VV 
? ?? ?? 
the Euro the significant appreciation of 
533
translation of ???/?? and ???/??? is al-
tered as well. The former reordering case with the 
smaller distance is usually referred as local reor-
dering and the latter with the longer distance reor-
dering as global reordering. Phrase-based SMT 
system can effectively capture the local word reor-
dering information which is common enough to be 
observed in training data. But it is hard to model 
global phrase reordering. Although syntactic 
knowledge used in syntax-based SMT systems can 
help reorder phrases, the resulting model is usually 
much more complicated than a phrase-based sys-
tem. 
There have been considerable amount of efforts 
to improve the reordering model in SMT systems, 
ranging from the fundamental distance-based dis-
tortion model (Och and Ney, 2004; Koehn et al, 
2003), flat reordering model (Wu, 1996; Zens et al, 
2004; Kumar et al, 2005), to lexicalized reordering 
model (Tillmann, 2004; Kumar et al, 2005; Koehn 
et al, 2005), hierarchical phrase-based model 
(Chiang, 2005), and maximum entropy-based 
phrase reordering model (Xiong et al, 2006). Due 
to the absence of syntactic knowledge in these sys-
tems, the ability to capture global reordering know-
ledge is not powerful. Although syntax-based SMT 
systems (Yamada et al, 2001; Quirk et al, 2005; 
Liu et al, 2006) are good at modeling global reor-
dering, their performance is subject to parsing er-
rors to a large extent. 
In this paper, we propose a new method to im-
prove reordering model by introducing syntactic 
information. Syntactic knowledge such as boun-
dary of sub-trees, part-of-speech (POS) and depen-
dency relation is incorporated into the SMT system 
to strengthen the ability to handle global phrase 
reordering. Our method is different from previous 
syntax-based SMT systems in which the translation 
process was modeled based on specific syntactic 
structures, either phrase structures or dependency 
relations. In our system, syntactic knowledge is 
used just to decide where we should combine adja-
cent phrases and what their reordering probability 
is. For example, according to the syntactic infor-
mation in Figure 1, the phrase translation combina-
tion should take place between ???? and ???? 
rather than between ??? and ????. Moreover, 
the non-monotone phrase reordering should occur 
between ???/?? and ???/??? rather than 
between ???/?? and ????. We train a maxi-
mum entropy model, which is able to integrate rich 
syntactic knowledge, to estimate phrase reordering 
probabilities. To enhance the performance of 
phrase reordering model, some modification on the 
syntax trees are also made to relax the phrase reor-
dering constraints. Additionally, the combination 
of other kinds of syntax trees is introduced to over-
come the deficiency of single parse tree. The expe-
rimental results show that the performance of our 
system is superior to that of the state-of-art phrase-
based SMT system.  
The roadmap of this paper is: Section 2 gives the 
related work. Section 3 introduces our model.  Sec-
tion 4 explains the generalization of reordering 
knowledge. The procedures of training and decod-
ing are described in Section 5 and Section 6 re-
spectively. The experimental results are shown in 
Section 7. Section 8 concludes the paper. 
2 Related Work  
The Pharaoh system (Koehn et al, 2004) is well 
known as the typical phrase-based SMT system. Its 
reordering model is designed to penalize transla-
tion according to jump distance regardless of lin-
guistic knowledge. This method just works well for 
language pairs that trend to have similar word-
orders and it has nothing to do with global reorder-
ing. 
A straightforward reordering model used in (Wu, 
1996; Zens et al, 2004; Kumar et al, 2005) is to 
assign constant probabilities to monotone reorder-
ing and non-monotone reordering, which can be 
flexible depending on the different language pairs. 
This method is also adopted in our system for non-
peer phrase reordering. 
The lexicalized reordering model was studied in 
(Tillmann, 2004; Kumar et al, 2005; Koehn et al, 
2005). Their work made a step forward in integrat-
ing linguistic knowledge to capture reordering. But 
their methods have the serious data sparseness 
problem. 
Beyond standard phrase-based SMT system, a 
CKY style decoder was developed in (Xiong et al, 
2006). Their method investigated the reordering of 
any two adjacent phrases. The limited linguistic 
knowledge on the boundary words of phrases is 
used to construct the phrase reordering model.  The 
basic difference to our method is that no syntactic 
knowledge is introduced to guide the global phrase 
reordering in their system. Besides boundary 
534
words, our phrase reordering model also integrates 
more significant syntactic knowledge such as POS 
information and dependencies from the  syntax tree, 
which can avoid some intractable phrase reorder-
ing errors. 
A hierarchical phrase-based model was pro-
posed by (Chiang, 2005). In his method, a syn-
chronous CFG is used to reorganize the phrases 
into hierarchical ones and grammar rules are auto-
matically learned from corpus. Different from his 
work, foreign syntactic knowledge is introduced 
into the synchronous grammar rules in our method 
to restrict the arbitrary phrase reordering.   
Syntax-based SMT systems (Yamada et al, 
2001; Quirk et al, 2005; Liu et al, 2006) totally 
depend on syntax structures to complete phrase 
translation. They can capture global reordering by 
simply swapping the children nodes of a parse tree. 
However, there are also reordering cases which do 
not agree with syntactic structures. Furthermore, 
their model is usually much more complex than a 
phrase-based system. Our method exactly attempts 
to integrate the advantages of phrase-based SMT 
system and syntax-based SMT system to improve 
the phrase reordering model. Phrase translation in 
our system is independent of syntactic structures. 
3 The Model 
In our work, we focus on building a better reorder-
ing model with the help of source parsing informa-
tion. Although we borrow some fundamental ele-
ments from a phrase-based SMT system such as 
the use of bilingual phrases as basic translation unit, 
we are more interested in introducing syntactic 
knowledge to strengthen the ability to handle glob-
al reordering phenomena in translation.  
3.1 Definitions 
Given a foreign sentence f and its syntactic parse 
tree T, each leaf in T corresponds to a single word 
in f and each sub-tree of T exactly covers a phrase 
fi in f which is called as linguistic phrase.  Except 
linguistic phrases, any other phrase is regarded as 
non-linguistic phrase. The height of phrase fi is 
defined as the distance between the root node of T 
and the root node of the maximum sub-tree which 
exactly covers fi. For example, in Figure 1 the 
phrase ???? has the maximum sub-tree rooting 
at ADJP and its height is 3. The height of phrase 
??? is 4 since its maximum sub-tree roots at 
ADBP instead of AD. If two adjacent phrases have 
the same height, we regard them as peer phrases.  
In our model, we make use of bilingual phrases 
as well, which refer to source-target algned phrase 
pairs extracted using the same criterion as most 
phrase-based systems (Och and Ney, 2004). 
3.2 Model 
Similar to the work in Chiang (2005), our transla-
tion model can be formulated as a weighted syn-
chronous context free grammar derivation process. 
Let D be a derivation that generates a bilingual 
sentence pair ?f, e?, in which f is the given source 
sentence, the statistical model that is used to pre-
dict the translation probability p(e|f) is defined over 
Ds as follows: 
? ? ? ? ? ? ? ???  ? 
???
?  ?? ? ? ??,?? 
??
???? ,????
 
?
 
where plm(e) is the language model, ?i(X ???,??) 
is a feature function defined over the derivation 
rule X???,??, and ?i is its weight.  
Although theoretically it is ideal for translation 
reorder modeling by constructing a synchronous 
context free grammar based on bilingual linguistic 
parsing trees, it is generally a very difficult task in 
practice. In this work we propose to use a small 
synchronous grammar constructed on the basis of 
bilingual phrases to model translation reorder 
probability and constraints by referring to the 
source syntactic parse trees. In the grammar, the 
source / target words serve as terminals, and the 
bilingual phrases and combination of bilingual 
phrases are presented with non-terminals. There 
are two non-terminals in the grammar except the 
start symbol S: Y and Z. The general derivation 
rules are defined as follows: 
a) Derivations from non-terminal to non-
terminals are restricted to binary branching 
forms; 
b) Any non-terminals that derives a list of termin-
als, or any combination of two non-terminals, 
if the resulting source string won?t cause any 
cross-bracketing problems in the source parse 
tree (it exactly corresponds to a linguistic 
phrase in binary parse trees), are reduced to Y; 
c) Otherwise, they are reduced to Z. 
Table 1 shows a complete list of derivation rules 
in our synchronous context grammar. The first nine 
grammar rules are used to constrain phrase reor-
535
dering during phrase combination. The last two 
rules are used to represent bilingual phrases. Rule 
(10) is the start grammar rule to generate the entire 
sentence translation.  
 
Rule Name Rule Content 
Rule (1) Y??Y1Y2, Y1Y2? 
Rule (2) Y??Y1Y2, Y2Y1? 
Rule (3) Y??Z1Z2, Z1Z2? 
Rule (4) Y??Y1Z2,Y1Z2? 
Rule (5) Y??Z1Y2, Z1Y2? 
Rule (6) Z??Y1Z2, Y1Z2? 
Rule (7) Z??Z1Y2, Z1Y2? 
Rule (8) Z??Z1Z2, Z1Z2? 
Rule (9) Z??Y1Y2, Y1Y2? 
Rule (10) S??Y1,Y1? 
Rule (11) Z??Z1, Z1? 
Rule (12) Y??Y1,Y1? 
 
Table 1: Synchronous grammar rules 
 
Rule (1) and Rule (2) are only applied to two ad-
jacent peer phrases. Note that, according to the 
constraints of foreign syntactic structures, only 
Rule (2) among all rules in Table 1 can be applied 
to conduct non-monotone phrase reordering in our 
framework. This can avoid arbitrary phrase reor-
dering. For example, as shown in Figure 1, Rule (1) 
is applied to the monotone combination of phrases 
???? and ???, and Rule (2) is applied to the 
non-monotone combination of phrases ???/?? 
and ??? /???. However, the non-monotone 
combination of ??? and ???? is not allowed in 
our method since there is no proper rule for it.  
Non-linguistic phrases are involved in Rule 
(3)~(9). We do not allow these grammar rules for 
non-monotone combination of non-peer phrases, 
which really harm the translation results as proved 
in experimental results. Although these rules vi-
olate the syntactic constraints, they not only pro-
vide the option to leverage non-linguistic transla-
tion knowledge to avoid syntactic errors but also 
take advantage of phrase local reordering capabili-
ties. Rule (3) and Rule (8) are applied to the com-
bination of two adjacent non-linguistic phrases. 
Rule (4)~(7) deal with the situation where one is a 
linguistic phrase and the other is a non-linguistic 
phrase. Rule (9) is applied to the combination of 
two adjacent linguistic phrases but their combina-
tion result is not a linguistic phrase.  
Rule (11) and Rule (12) are applied to generate 
bilingual phrases learned from training corpus. 
Table 2 demonstrates an example how these 
rules are applied to translate the foreign sentence 
???/?/??/??? into the English sentence 
?the significant appreciation of the Euro?. 
 
Step Partial derivations Rule 
1 S??Y1, Y1?   (10) 
2 ??Y2Y3, Y3Y2? (2) 
3 ??Y4Y5Y3, Y3Y5Y4? (2) 
4 ???? Y5Y3, Y3Y5 the Euro? (12) 
5 ???? ? Y3, Y3 of the Euro? (12) 
6 ???? ? Y6Y7, Y6Y7 of the Euro? (1) 
7 ???? ? ?? Y7, the significant 
Y7 of  the Euro? 
(12) 
8 ???? ? ?? ??, the signifi-
cant appreciation of  the Euro? 
(12) 
 
Table 2: Example of application for rules  
 
However, there are always other kinds of bilin-
gual phrases extracted directly from training cor-
pus, such as ???, the Euro? and ?? ?? ?
?, ?s significant appreciation?, which can produce 
different candidate sentence translations. Here, the 
phrase ?? ?? ??? is a non-linguistic phrase. 
The above derivations can also be rewritten as 
S??Y1, Y1???Y2Z3,Y2Z3??? ?? Z3, the Euro 
Z3?????? ?? ??, the Euro ?s significant 
appreciation?, where Rule (10), (4), (12) and (11) 
are applied respectively. 
3.3 Features 
Similar to the default features in Pharaoh (Koehn, 
Och and Marcu 2003), we used following features 
to estimate the weight of our grammar rules. Note 
536
that different rules may have different features in 
our model. 
? The lexical weights plex(?|?) and plex(?|?) esti-
mating how well the words in ? translate the 
words in ?. This feature is only applicable to 
Rule (11) and Rule (12). 
? The phrase translation weights pphr(?|?) and 
pphr(?|?) estimating how well the terminal 
words of ? translate the terminal words of ?, 
This feature is only applicable to Rule (11) and 
Rule (12). 
? A word penalty exp(|?|), where |?| denotes the 
count of terminal words of ?. This feature is 
only applicable to Rule (11) and Rule (12). 
? A penalty exp(1) for grammar rules analogous 
to Pharaoh?s penalty which allows the model to 
learn a preference for longer or shorter deriva-
tions. This feature is applicable to all rules in 
Table 1. 
? Score for applying the current rule. This feature 
is applicable to all rules in Table 1. We will ex-
plain the score estimation in detail in Section 
3.4. 
3.4 Scoring of Rules 
Based on the syntax constraints and involved non-
terminal types, we separate the grammar rules into 
three groups to estimate their application scores 
which are also treated as reordering probabilities.  
For Rule (1) and Rule (2), they strictly comply 
with the syntactic structures. Given two peer 
phrases, we have two choices to use one of them. 
Thus, we use maximum entropy (ME) model algo-
rithm to estimate their reordering probabilities sep-
arately, where the boundary words of foreign 
phrases and candidate target translation phrases, 
POS information and dependencies are integrated 
as features. As listed in Table 3, there are totally 
twelve categories of features used to train the ME 
model. In fact, the probability of Rule (1) is just 
equal to the supplementary probability of Rule (2), 
and vice versa. 
For Rule (3)~(9), according to the syntactic 
structures, their application is determined since 
there is only one choice to complete reordering, 
which is similar to the ?glue rules? in Chiang 
(2005). Due to the appearance of non-linguistic 
phrases, non-monotone phrase reordering is not 
allowed in these rules. We just assign these rules a 
constant score trained using our implementation of 
Minimum Error Rate Training (Och, 2003b), 
which is 0.7 in our system. 
For Rule (10)~(12), they are also determined 
rules since there is no other optional rules compet-
ing with them. Constant score is simply assigned to 
them as well, which is 1.0 in our system. 
 
Fea. Description 
LS1 First word of first foreign phrase 
LS2 First word of second foreign phrase 
RS1 Last word of first foreign phrase 
RS2 Last word of second foreign phrase 
LT1 First word of first target phrase 
LT2 First word of second target phrase 
RT1 Last word of first target phrase 
RT2 Last word of second target phrase 
LPos 
POS of the node covering first foreign 
phrase 
RPos 
POS of the node covering second foreign 
phrase  
Cpos 
POS of the node covering the combina-
tion of foreign phrases 
DP 
Dependency between the nodes covering 
two single foreign phrases respectively 
 
Table 3: Feature categories used for ME model 
4 The Generalization of Reordering 
Knowledge 
4.1 Enriching Parse Trees 
The grammar rules proposed in Section 3 are only 
applied to binary syntax tree nodes. For n-ary syn-
tax trees (n>2), some modification is needed to 
generate more peer phrases. As shown in Figure 
2(a), the syntactic tree of Chinese sentence ???
? /???? /?? /?? ? (Guangdong/high-
tech/products/export), parsed by the Stanford Pars-
er (Klein, 2003), has a 3-ary sub-tree. Referring to 
its English translation result ?export of high-tech 
products in Guangdong?, we understand there 
should be a non-monotone combination between 
the phrases ????? and ?????/???. How-
ever, ?????/??? is not a linguistic phrase 
537
though its component phrases ?????? and ??
?? are peer phrases. To avoid the conflict with the 
Rule (2), we just add some extra virtual nodes in 
the n-ary sub-trees to make sure that only binary 
sub-trees survive in the modified parse tree. Figure 
2(b) is the modification result of the syntactic tree 
from Figure 2(a), where two virtual nodes with the 
new distinguishable POS of M are added.  
In general, we add virtual nodes for each set of 
the continuous peer phrases and let them have the 
same height. Thus, for a n-ary sub-tree, there are 
? ?? ?11 )(ni in
= (n?1)2/2 virtual nodes being added 
where n>2. The phrases exactly covered by the 
virtual nodes are called as virtual peer phrases.  
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2: Example of syntax tree modification 
4.2 Combination of Parse Trees 
It is well known that parse errors in syntactic trees 
always are inescapable even if the state-of-the-art 
parser is used.  Incorrect syntactic knowledge may 
harm the reordering probability estimation. To mi-
nimize the impact of parse error of a single tree, 
more parse trees are introduced. To support the 
combination of parse trees, the synchronous 
grammar rules are applied independently, but they 
will compete against each other with the effect of 
other models such as language model. 
In our system, we combine the parse trees gen-
erated respectively by Stanford parser (Klein, 2003) 
and a dependency parser developed by (Zhou, 
2000). Compared with the Stanford parser, the de-
pendency parser only conducts shallow syntactic 
analysis. It is powerful to identify the base NPs and 
base VPs and their dependencies. Additionally, 
dependency parser runs much faster. For example, 
it took about three minutes for the dependency 
parser to parse one thousand sentences with aver-
age length of 25 words, but the Stanford parser 
needs about one hour to complete the same work. 
More importantly, as shown in the experimental 
results, the dependency parser can achieve the 
comparable quality of final translation results with 
Stanford parser in our system.  
5 The Decoder 
We developed a CKY style decoder to complete 
the sentence translation. A two-dimension array 
CA is constructed to store all the local candidate 
phrase translation and each valid cell CAij in CA 
corresponds to a foreign phrase where i is the 
phrase start position and j is the phrase end posi-
tion. The cells in CA are filled in a bottom-up way. 
Firstly we fill in smaller cells with the translation 
in bilingual phrases learned from corpus. Then the 
candidate translation in the larger cell CAij is gen-
erated based on the content in smaller adjacent 
cells CAik and CAk+1j with the monotone combina-
tion and non-monotone combination, where i?k?j. 
To reduce the cost of system resources, the well 
known pruning methods, such as histogram prun-
ing, threshold pruning and recombination, are used 
to only keep the top N candidate translation in each 
cell.  
6 Training 
Similar to most state-of-the-art phrase-based SMT 
systems, we use the SRI toolkit (Stolcke, 2002) for 
language model training and Giza++ toolkit (Och 
and Ney, 2003) for word alignment. For reordering 
model training, two kinds of parse trees for each 
foreign sentence in the training corpus were ob-
tained through the Stanford parser (Klein, 2003) 
and a dependency parser (Zhou, 2000). After that, 
we picked all the foreign linguistic phrases of the 
same sentence according to syntactic structures. 
Based on the word alignment results, if the aligned 
target words of any two adjacent foreign linguistic 
phrases can also be formed into two valid adjacent 
phrase according to constraints proposed in the 
phrase extraction algorithm by Och (2003a), they 
will be extracted as a reordering training sample. 
Finally, the ME modeling toolkit developed by 
Zhang (2004) is used to train the reordering model 
over the extracted samples. 
?? 
NP 
NP 
NP 
NN 
NP 
NR JJ 
ADJP 
NN 
NP 
??
? 
?? 
M M 
?? 
NP 
NP 
NP 
NN 
NP 
NR JJ 
ADJP 
NN 
NP 
??
? 
??
?? 
?? 
(a) (b) 
??
?? 
538
7 Experimental Results and Analysis 
We conducted our experiments on Chinese-to-
English translation task of NIST MT-05 on a 
3.0GHz system with 4G RAM memory. The bilin-
gual training data comes from the FBIS corpus. 
The Xinhua news in GIGAWORD corpus is used 
to train a four-gram language model. The devel-
opment set used in our system is the NIST MT-02 
evaluation test data.  
For phrase extraction, we limit the maximum 
length of foreign and English phrases to 3 and 5 
respectively. But there is no phrase length con-
straint for reordering sample extraction. About 
1.93M and 1.1M reordering samples are extracted 
from the FBIS corpus based on the Stanford parser 
and the dependency parser respectively. To reduce 
the search space in decoder, we set the histogram 
pruning threshold to 20 and relative pruning thre-
shold to 0.1.  
In the following experiments, we compared our 
system performance with that of the other state-of-
the-art systems. Additionally, the effect of some 
strategies on system performance is investigated as 
well. Case-sensitive BLEU-4 score is adopted to 
evaluate system performance.  
7.1 Comparing with Baseline SMT system 
Our baseline system is Pharaoh (Koehn, 2004). 
Xiong?s system (Xiong, et al, 2006) which used 
ME model to train the reordering model is also 
regarded as a competitor. To have a fair compari-
son, we used the same language model and transla-
tion model for these three systems. The experimen-
tal results are showed in Table 4. 
 
System Bleu Score 
Pharaoh 0.2487 
Xiong?s System 0.2616 
Our System 0.2737 
Table 4: Performance against baseline system 
 
These three systems are the same in that the fi-
nal sentence translation results are generated by the 
combination of local phrase translation. Thus, they 
are capable of local reordering but not global reor-
dering. The phrase reordering in Pharaoh depends 
only on distance distortion information which does 
not contain any linguistic knowledge. The experi-
mental result shows that the performance of both 
Xiong?s system and our system is better than that 
of Pharaoh. It proves that linguistic knowledge can 
help the global reordering probability estimation. 
Additionally, our system is superior to Xiong?s 
system in which only use phrase boundary words 
to guide global reordering. It indicates that syntac-
tic knowledge is more powerful to guide global 
reordering than boundary words. On the other hand, 
it proves the importance of syntactic knowledge 
constraints in avoiding the arbitrary phrase reorder-
ing.  
7.2 Syntactic Error Analysis 
Rule (3)~(9) in Section 3 not only play the role to 
compensate for syntactic errors, but also take the 
advantage of the capability of capturing local 
phrase reordering. However, the non-monotone 
combination for non-peer phrases is really harmful 
to system performance. To prove these ideas, we 
conducted experiments with different constrains.  
 
Constraints Bleu Score 
All rules in Table 1 used  0.2737 
Allowing the non-monotone 
combination of non-peer phrases 
0.2647 
Rule (3)~(9) are prohibited 0.2591 
Table 5:  About non-peer phrase combination 
 
From the experimental results shown in Table 5, 
just as claimed in other previous work, the combi-
nation between non-linguistic phrases is useful and 
cannot be abandoned. On the other hand, if we re-
lax the constraint of non-peer phrase combination 
(that is, allowing non-monotone combination for 
on-peer phrases), some more serious errors in non-
syntactic knowledge is introduced, thereby degrad-
ing performance from 0.2737 to 0.2647. 
7.3 Effect of Virtual Peer Phrases 
As discussed in Section 4, for n-ary nodes (n>2) in 
the original syntax trees, the relationship among n-
ary sub-trees is always not clearly captured. To 
give them the chance of free reordering, we add the 
virtual peer nodes to make sure that the combina-
tion of a set of peer phrases can still be a peer 
phrase. An experiment was done to compare with 
the case where the virtual peer nodes were not 
added to n-ary syntax trees. The Bleu score 
539
dropped to 26.20 from 27.37, which shows the vir-
tual nodes have great effect on system performance. 
7.4 Effect of Mixed Syntax Trees 
In this section, we conducted three experiments to 
investigate the effect of constituency parse tree and 
dependency parse tree. Over the same platform, we 
tried to use only one of them to complete the trans-
lation task. The experimental results are shown in 
Table 6.  
Surprisingly, there is no significant difference in 
performance. The reason may be that both parsers 
produce approximately equivalent parse results. 
However, the combination of syntax trees outper-
forms merely only one syntax tree. This suggests 
that the N-best syntax parse trees may enhance the 
quality of reordering model. 
 
Situation Bleu Score 
Dependency parser only 0.2667 
Stanford parser only 0.2670 
Mixed parsing trees 0.2737 
 
Table 6: Different parsing tree 
8 Conclusion and Future Work 
In this paper, syntactic knowledge is introduced 
to capture global reordering of SMT system. This 
method can not only inherit the advantage of local 
reordering ability of standard phrase-based SMT 
system, but also capture the global reordering as 
the syntax-based SMT system. The experimental 
results showed the effectiveness of our method. 
In the future work, we plan to improve the reor-
dering model by introducing N-best syntax trees 
and exploiting richer syntactic knowledge. 
References 
David Chiang. 2005. A hierarchical phrase-based mod-
el for statistical machine translation. In Proceedings 
of ACL 2005. 
Franz Josef Och. 2003a. Statistical Machine Translation: 
From Single-Word Models to Alignment Templates 
Thesis. 
Franz Josef Och. 2003b. Minmum Error Rate Training 
in Statistical Machine Translation. In Proceedings 
for ACL 2003. 
Franz Josef Och, Hermann Ney. 2003. A Systematic 
Comparison of Various Statistical Alignment Models. 
Computational Linguistics, 29:19-51. 
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine trans-
lation. Computational Linguistics, 30:417-449. 
Dan Klein and Christopher D. Manning. 2003. Accurate 
Unlexicalized Parsing. In Proceedings of ACL 2003. 
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of HLT/NAACL 2003. 
Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder 
for Phrased-Based Statistical Machine Translation 
Models. In Proceedings of AMTA 2004. 
Shankar Kumar and William Byrne. 2005. Local phrase 
reordering models for statistical machine translation. 
In Proceedings of HLT-EMNLP 2005. 
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-String 
Alignment Template for Statistical Machine Transla-
tion. In Proceedings of COLING-ACL 2006. 
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. 
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of ACL 2005. 
Andreas Stolcke. 2002. SRILM-An Extensible Language 
Modeling Toolkit. In Proceedings of ICSLP 2002. 
Christoph Tillmann. 2004. A block orientation model 
for statistical machine translation. In Proceedings of 
HLT-NAACL 2004. 
Dekai Wu. 1996. A Polynomial-Time Algorithm for Sta-
tistical Machine Translation. In Proceedings of ACL 
1996. 
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for 
Statistical Machine Translation. In Proceedings of 
COLING-ACL 2006. 
Kenji Yamada and Kevin Knight. 2001. A syntax based 
statistical translation model. In Proceedings of ACL 
2001. 
Le Zhang. 2004. Maximum Entropy Modeling Toolkit 
for Python and C++. Available at http://homepa 
ges.inf.ed.ac.uk/s0450736/maxent_toolkit.html. 
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. 
Reordering Constraints for Phrase-Based Statistical 
Machine Translation. In Proceedings of CoLing 2004. 
Ming Zhou. 2000. A block-based robust dependency 
parser for unrestricted Chinese text. The second 
Chinese Language Processing Workshop attached to 
ACL2000. 
540
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 720?727,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
A Probabilistic Approach to Syntax-based Reordering
for Statistical Machine Translation
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou
Microsoft Research Asia
Beijing, China
chl, dozhang@microsoft.com
muli, mingzhou@microsoft.com
Minghui Li, Yi Guan
Harbin Institute of Technology
Harbin, China
mhli@insun.hit.edu.cn
guanyi@insun.hit.edu.cn
Abstract
Inspired by previous preprocessing ap-
proaches to SMT, this paper proposes a
novel, probabilistic approach to reordering
which combines the merits of syntax and
phrase-based SMT. Given a source sentence
and its parse tree, our method generates,
by tree operations, an n-best list of re-
ordered inputs, which are then fed to stan-
dard phrase-based decoder to produce the
optimal translation. Experiments show that,
for the NIST MT-05 task of Chinese-to-
English translation, the proposal leads to
BLEU improvement of 1.56%.
1 Introduction
The phrase-based approach has been considered the
default strategy to Statistical Machine Translation
(SMT) in recent years. It is widely known that the
phrase-based approach is powerful in local lexical
choice and word reordering within short distance.
However, long-distance reordering is problematic
in phrase-based SMT. For example, the distance-
based reordering model (Koehn et al, 2003) al-
lows a decoder to translate in non-monotonous or-
der, under the constraint that the distance between
two phrases translated consecutively does not ex-
ceed a limit known as distortion limit. In theory the
distortion limit can be assigned a very large value
so that all possible reorderings are allowed, yet in
practise it is observed that too high a distortion limit
not only harms efficiency but also translation per-
formance (Koehn et al, 2005). In our own exper-
iment setting, the best distortion limit for Chinese-
English translation is 4. However, some ideal trans-
lations exhibit reorderings longer than such distor-
tion limit. Consider the sentence pair in NIST MT-
2005 test set shown in figure 1(a): after translating
the word ?V/mend?, the decoder should ?jump?
across six words and translate the last phrase ?
? ?_/fissures in the relationship?. Therefore,
while short-distance reordering is under the scope
of the distance-based model, long-distance reorder-
ing is simply out of the question.
A terminological remark: In the rest of the paper,
we will use the terms global reordering and local
reordering in place of long-distance reordering and
short-distance reordering respectively. The distinc-
tion between long and short distance reordering is
solely defined by distortion limit.
Syntax1 is certainly a potential solution to global
reordering. For example, for the last two Chinese
phrases in figure 1(a), simply swapping the two chil-
dren of the NP node will produce the correct word
order on the English side. However, there are also
reorderings which do not agree with syntactic anal-
ysis. Figure 1(b) shows how our phrase-based de-
coder2 obtains a good English translation by reorder-
ing two blocks. It should be noted that the second
Chinese block ??e? and its English counterpart
?at the end of? are not constituents at all.
In this paper, our interest is the value of syntax in
reordering, and the major statement is that syntactic
information is useful in handling global reordering
1Here by syntax it is meant linguistic syntax rather than for-
mal syntax.
2The decoder is introduced in section 6.
720
Figure 1: Examples on how syntax (a) helps and (b) harms reordering in Chinese-to-English translation
The lines and nodes on the top half of the figures show the phrase structure of the Chinese sentences, while the links on the bottom
half of the figures show the alignments between Chinese and English phrases. Square brackets indicate the boundaries of blocks
found by our decoder.
and it achieves better MT performance on the ba-
sis of the standard phrase-based model. To prove it,
we developed a hybrid approach which preserves the
strength of phrase-based SMT in local reordering as
well as the strength of syntax in global reordering.
Our method is inspired by previous preprocessing
approaches like (Xia and McCord, 2004), (Collins
et al, 2005), and (Costa-jussa` and Fonollosa, 2006),
which split translation into two stages:
S ? S? ? T (1)
where a sentence of the source language (SL), S,
is first reordered with respect to the word order of
the target language (TL), and then the reordered SL
sentence S? is translated as a TL sentence T by
monotonous translation.
Our first contribution is a new translation model
as represented by formula 2:
S ? n? S? ? n? T ? T? (2)
where an n-best list of S?, instead of only one S?, is
generated. The reason of such change will be given
in section 2. Note also that the translation process
S??T is not monotonous, since the distance-based
model is needed for local reordering. Our second
contribution is our definition of the best translation:
argmax
T
exp(?rlogPr(S?S?)+
?
i
?iFi(S??T ))
where Fi are the features in the standard phrase-
based model and Pr(S ? S?) is our new feature,
viz. the probability of reordering S as S?. The de-
tails of this model are elaborated in sections 3 to 6.
The settings and results of experiments on this new
model are given in section 7.
2 Related Work
There have been various attempts to syntax-
based SMT, such as (Yamada and Knight, 2001)
and (Quirk et al, 2005). We do not adopt these
models since a lot of subtle issues would then be in-
troduced due to the complexity of syntax-based de-
coder, and the impact of syntax on reordering will
be difficult to single out.
There have been many reordering strategies un-
der the phrase-based camp. A notable approach is
lexicalized reordering (Koehn et al, 2005) and (Till-
mann, 2004). It should be noted that this approach
achieves the best result within certain distortion limit
and is therefore not a good model for global reorder-
ing.
There are a few attempts to the preprocessing
approach to reordering. The most notable ones
are (Xia and McCord, 2004) and (Collins et al,
2005), both of which make use of linguistic syntax
in the preprocessing stage. (Collins et al, 2005) an-
alyze German clause structure and propose six types
721
of rules for transforming German parse trees with
respect to English word order. Instead of relying
on manual rules, (Xia and McCord, 2004) propose
a method in learning patterns of rewriting SL sen-
tences. This method parses training data and uses
some heuristics to align SL phrases with TL ones.
From such alignment it can extract rewriting pat-
terns, of which the units are words and POSs. The
learned rewriting rules are then applied to rewrite SL
sentences before monotonous translation.
Despite the encouraging results reported in these
papers, the two attempts share the same shortcoming
that their reordering is deterministic. As pointed out
in (Al-Onaizan and Papineni, 2006), these strategies
make hard decisions in reordering which cannot be
undone during decoding. That is, the choice of re-
ordering is independent from other translation fac-
tors, and once a reordering mistake is made, it can-
not be corrected by the subsequent decoding.
To overcome this weakness, we suggest a method
to ?soften? the hard decisions in preprocessing. The
essence is that our preprocessing module generates
n-best S?s rather than merely one S?. A variety of
reordered SL sentences are fed to the decoder so
that the decoder can consider, to certain extent, the
interaction between reordering and other factors of
translation. The entire process can be depicted by
formula 2, recapitulated as follows:
S ? n? S? ? n? T ? T? .
Apart from their deterministic nature, the two
previous preprocessing approaches have their own
weaknesses. (Collins et al, 2005) count on man-
ual rules and it is suspicious if reordering rules for
other language pairs can be easily made. (Xia and
McCord, 2004) propose a way to learn rewriting
patterns, nevertheless the units of such patterns are
words and their POSs. Although there is no limit to
the length of rewriting patterns, due to data sparse-
ness most patterns being applied would be short
ones. Many instances of global reordering are there-
fore left unhandled.
3 The Acquisition of Reordering
Knowledge
To avoid this problem, we give up using rewriting
patterns and design a form of reordering knowledge
which can be directly applied to parse tree nodes.
Given a node N on the parse tree of an SL sentence,
the required reordering knowledge should enable the
preprocessing module to determine how probable
the children of N are reordered.3 For simplicity, let
us first consider the case of binary nodes only. Let
N1 and N2, which yield phrases p1 and p2 respec-
tively, be the child nodes of N . We want to deter-
mine the order of p1 and p2 with respect to their TL
counterparts, T (p1) and T (p2). The knowledge for
making such a decision can be learned from a word-
aligned parallel corpus. There are two questions in-
volved in obtaining training instances:
? How to define T (pi)?
? How to define the order of T (pi)s?
For the first question, we adopt a similar method
as in (Fox, 2002): given an SL phrase ps =
s1 . . . si . . . sn and a word alignment matrix A, we
can enumerate the set of TL words {ti : ti?A(si)},
and then arrange the words in the order as they ap-
pear in the TL sentence. Let first(t) be the first word
in this sorted set and last(t) be the last word. T (ps)
is defined as the phrase first(t) . . . last(t) in the TL
sentence. Note that T (ps) may contain words not in
the set {ti}.
The question of the order of two TL phrases is not
a trivial one. Since a word alignment matrix usu-
ally contains a lot of noises as well as one-to-many
and many-to-many alignments, two TL phrases may
overlap with each other. For the sake of the quality
of reordering knowledge, if T (p1) and T (p2) over-
lap, then the node N with children N1 and N2 is
not taken as a training instance. Obviously it will
greatly reduce the amount of training input. To rem-
edy data sparseness, less probable alignment points
are removed so as to minimize overlapping phrases,
since, after removing some alignment point, one of
the TL phrases may become shorter and the two
phrases may no longer overlap. The implementation
is similar to the idea of lexical weight in (Koehn et
al., 2003): all points in the alignment matrices of the
entire training corpus are collected to calculate the
probabilistic distribution, P (t|s), of some TL word
3Some readers may prefer the expression the subtree rooted
at node N to node N . The latter term is used in this paper for
simplicity.
722
t given some SL word s. Any pair of overlapping
T (pi)s will be redefined by iteratively removing less
probable word alignments until they no longer over-
lap. If they still overlap after all one/many-to-many
alignments have been removed, then the refinement
will stop and N , which covers pis, is no longer taken
as a training instance.
In sum, given a bilingual training corpus, a parser
for the SL, and a word alignment tool, we can collect
all binary parse tree nodes, each of which may be an
instance of the required reordering knowledge. The
next question is what kind of reordering knowledge
can be formed out of these training instances. Two
forms of reordering knowledge are investigated:
1. Reordering Rules, which have the form
Z : X Y ?
{
X Y Pr(IN-ORDER)
Y X Pr(INVERTED)
where Z is the phrase label of a binary node
and X and Y are the phrase labels of Z?s chil-
dren, and Pr(INVERTED) and Pr(IN-ORDER)
are the probability that X and Y are inverted on
TL side and that not inverted, respectively. The
probability figures are estimated by Maximum
Likelihood Estimation.
2. Maximum Entropy (ME) Model, which does
the binary classification whether a binary
node?s children are inverted or not, based on a
set of features over the SL phrases correspond-
ing to the two children nodes. The features that
we investigated include the leftmost, rightmost,
head, and context words4, and their POSs, of
the SL phrases, as well as the phrase labels of
the SL phrases and their parent.
4 The Application of Reordering
Knowledge
After learning reordering knowledge, the prepro-
cessing module can apply it to the parse tree, tS ,
of an SL sentence S and obtain the n-best list of
S?. Since a ranking of S? is needed, we need some
way to score each S?. Here probability is used as
the scoring metric. In this section it is explained
4The context words of the SL phrases are the word to the left
of the left phrase and the word to the right of the right phrase.
how the n-best reorderings of S and their associated
scores/probabilites are computed.
Let us first look into the scoring of a particular
reordering. Let Pr(p?p?) be the probability of re-
ordering a phrase p into p?. For a phrase q yielded by
a non-binary node, there is only one ?reordering? of
q, viz. q itself, thus Pr(q?q) = 1. For a phrase p
yielded by a binary node N , whose left child N1 has
reorderings pi1 and right child N2 has the reorder-
ings pj2 (1 ? i, j ? n), p? has the form pi1pj2 or pj2pi1.
Therefore, Pr(p?p?) =
{
Pr(IN-ORDER)? Pr(pi1?pi
?
1 )? Pr(pj2?pj
?
2 )
Pr(INVERTED)? Pr(pj2?pj
?
2 )? Pr(pi1?pi
?
1 )
The figures Pr(IN-ORDER) and Pr(INVERTED) are
obtained from the learned reordering knowledge. If
reordering knowledge is represented as rules, then
the required probability is the probability associated
with the rule that can apply to N . If reordering
knowledge is represented as an ME model, then the
required probability is:
P (r|N) = exp(
?
i ?ifi(N, r))?
r? exp(
?
i ?ifi(N, r?))
where r?{IN-ORDER, INVERTED}, and fi?s are fea-
tures used in the ME model.
Let us turn to the computation of the n-best re-
ordering list. Let R(N) be the number of reorder-
ings of the phrase yielded by N , then:
R(N) =
{
2R(N1)R(N2) if N has children N1, N2
1 otherwise
It is easily seen that the number of S?s increases ex-
ponentially. Fortunately, what we need is merely an
n-best list rather than a full list of reorderings. Start-
ing from the leaves of tS , for each node N covering
phrase p, we only keep track of the n p?s that have
the highest reordering probability. Thus R(N) ? n.
There are at most 2n2 reorderings for any node and
only the top-scored n reorderings are recorded. The
n-best reorderings of S, i.e. the n-best reorderings
of the yield of the root node of tS , can be obtained
by this efficient bottom-up method.
5 The Generalization of Reordering
Knowledge
In the last two sections reordering knowledge is
learned from and applied to binary parse tree nodes
723
only. It is not difficult to generalize the theory of
reordering knowledge to nodes of other branching
factors. The case of binary nodes is simple as there
are only two possible reorderings. The case of 3-ary
nodes is a bit more complicated as there are six.5 In
general, an n-ary node has n! possible reorderings
of its children. The maximum entropy model has the
same form as in the binary case, except that there are
more classes of reordering patterns as n increases.
The form of reordering rules, and the calculation of
reordering probability for a particular node, can also
be generalized easily.6 The only problem for the
generalized reordering knowledge is that, as there
are more classes, data sparseness becomes more se-
vere.
6 The Decoder
The last three sections explain how the S?n?S?
part of formula 2 is done. The S??T
part is simply done by our re-implementation
of PHARAOH (Koehn, 2004). Note that non-
monotonous translation is used here since the
distance-based model is needed for local reordering.
For the n?T? T? part, the factors in consideration
include the score of T returned by the decoder, and
the reordering probability Pr(S ? S?). In order
to conform to the log-linear model used in the de-
coder, we integrate the two factors by defining the
total score of T as formula 3:
exp(?r logPr(S?S?) +
?
i
?iFi(S??T )) (3)
The first term corresponds to the contribution of
syntax-based reordering, while the second term that
of the features Fi used in the decoder. All the fea-
ture weights (?s) were trained using our implemen-
tation of Minimum Error Rate Training (Och, 2003).
The final translation T? is the T with the highest total
score.
5Namely, N1N2N3, N1N3N2, N2N1N3, N2N3N1,
N3N1N2, and N3N2N1, if the child nodes in the original order
are N1, N2, and N3.
6For example, the reordering probability of a phrase p =
p1p2p3 generated by a 3-ary node N is
Pr(r)?Pr(pi1)?Pr(pj2)?Pr(pk3)
where r is one of the six reordering patterns for 3-ary nodes.
It is observed in pilot experiments that, for a lot of
long sentences containing several clauses, only one
of the clauses is reordered. That is, our greedy re-
ordering algorithm (c.f. section 4) has a tendency to
focus only on a particular clause of a long sentence.
The problem was remedied by modifying our de-
coder such that it no longer translates a sentence at
once; instead the new decoder does:
1. split an input sentence S into clauses {Ci};
2. obtain the reorderings among {Ci}, {Sj};
3. for each Sj , do
(a) for each clause Ci in Sj , do
i. reorder Ci into n-best C ?is,
ii. translate each C ?i into T (C
?
i),
iii. select T? (C ?i);
(b) concatenate {T? (C ?i)} into Tj ;
4. select T?j .
Step 1 is done by checking the parse tree if there
are any IP or CP nodes7 immediately under the root
node. If yes, then all these IPs, CPs, and the remain-
ing segments are treated as clauses. If no, then the
entire input is treated as one single clause. Step 2
and step 3(a)(i) still follow the algorithm in sec-
tion 4. Step 3(a)(ii) is trivial, but there is a subtle
point about the calculation of language model score:
the language model score of a translated clause is not
independent from other clauses; it should take into
account the last few words of the previous translated
clause. The best translated clause T? (C ?i) is selected
in step 3(a)(iii) by equation 3. In step 4 the best
translation T?j is
argmax
Tj
exp(?rlogPr(S?Sj)+
?
i
score(T (C ?i))).
7 Experiments
7.1 Corpora
Our experiments are about Chinese-to-English
translation. The NIST MT-2005 test data set is used
for evaluation. (Case-sensitive) BLEU-4 (Papineni
et al, 2002) is used as the evaluation metric. The
7 IP stands for inflectional phrase and CP for complementizer
phrase. These two types of phrases are clauses in terms of the
Government and Binding Theory.
724
Branching Factor 2 3 >3
Count 12294 3173 1280
Percentage 73.41 18.95 7.64
Table 1: Distribution of Parse Tree Nodes with Dif-
ferent Branching Factors Note that nodes with only one
child are excluded from the survey as reordering does not apply
to such nodes.
test set and development set of NIST MT-2002 are
merged to form our development set. The training
data for both reordering knowledge and translation
table is the one for NIST MT-2005. The GIGA-
WORD corpus is used for training language model.
The Chinese side of all corpora are segmented into
words by our implementation of (Gao et al, 2003).
7.2 The Preprocessing Module
As mentioned in section 3, the preprocessing mod-
ule for reordering needs a parser of the SL, a word
alignment tool, and a Maximum Entropy training
tool. We use the Stanford parser (Klein and Man-
ning, 2003) with its default Chinese grammar, the
GIZA++ (Och and Ney, 2000) alignment package
with its default settings, and the ME tool developed
by (Zhang, 2004).
Section 5 mentions that our reordering model can
apply to nodes of any branching factor. It is inter-
esting to know how many branching factors should
be included. The distribution of parse tree nodes
as shown in table 1 is based on the result of pars-
ing the Chinese side of NIST MT-2002 test set by
the Stanford parser. It is easily seen that the major-
ity of parse tree nodes are binary ones. Nodes with
more than 3 children seem to be negligible. The 3-
ary nodes occupy a certain proportion of the distri-
bution, and their impact on translation performance
will be shown in our experiments.
7.3 The decoder
The data needed by our Pharaoh-like decoder are
translation table and language model. Our 5-gram
language model is trained by the SRI language mod-
eling toolkit (Stolcke, 2002). The translation table
is obtained as described in (Koehn et al, 2003), i.e.
the alignment tool GIZA++ is run over the training
data in both translation directions, and the two align-
Test Setting BLEU
B1 standard phrase-based SMT 29.22
B2 (B1) + clause splitting 29.13
Table 2: Experiment Baseline
Test Setting BLEU BLEU
2-ary 2,3-ary
1 rule 29.77 30.31
2 ME (phrase label) 29.93 30.49
3 ME (left,right) 30.10 30.53
4 ME ((3)+head) 30.24 30.71
5 ME ((3)+phrase label) 30.12 30.30
6 ME ((4)+context) 30.24 30.76
Table 3: Tests on Various Reordering Models
The 3rd column comprises the BLEU scores obtained by re-
ordering binary nodes only, the 4th column the scores by re-
ordering both binary and 3-ary nodes. The features used in the
ME models are explained in section 3.
ment matrices are integrated by the GROW-DIAG-
FINAL method into one matrix, from which phrase
translation probabilities and lexical weights of both
directions are obtained.
The most important system parameter is, of
course, distortion limit. Pilot experiments using the
standard phrase-based model show that the optimal
distortion limit is 4, which was therefore selected for
all our experiments.
7.4 Experiment Results and Analysis
The baseline of our experiments is the standard
phrase-based model, which achieves, as shown by
table 2, the BLEU score of 29.22. From the same
table we can also see that the clause splitting mech-
anism introduced in section 6 does not significantly
affect translation performance.
Two sets of experiments were run. The first set,
of which the results are shown in table 3, tests the
effect of different forms of reordering knowledge.
In all these tests only the top 10 reorderings of
each clause are generated. The contrast between
tests 1 and 2 shows that ME modeling of reordering
outperforms reordering rules. Tests 3 and 4 show
that phrase labels can achieve as good performance
as the lexical features of mere leftmost and right-
most words. However, when more lexical features
725
Input 0 2005#?R????q?Z??/???=?
Reference Hainan province will continue to increase its investment in the public services and
social services infrastructures in 2005
Baseline Hainan Province in 2005 will continue to increase for the public service and social
infrastructure investment
Translation with
Preprocessing
Hainan Province in 2005 will continue to increase investment in public services
and social infrastructure
Table 4: Translation Example 1
Test Setting BLEU
a length constraint 30.52
b DL=0 30.48
c n=100 30.78
Table 5: Tests on Various Constraints
are added (tests 4 and 6), phrase labels can no longer
compete with lexical features. Surprisingly, test 5
shows that the combination of phrase labels and lex-
ical features is even worse than using either phrase
labels or lexical features only.
Apart from quantitative evaluation, let us con-
sider the translation example of test 6 shown in ta-
ble 4. To generate the correct translation, a phrase-
based decoder should, after translating the word
?? as ?increase?, jump to the last word ?=
?(investment)?. This is obviously out of the capa-
bility of the baseline model, and our approach can
accomplish the desired reordering as expected.
By and large, the experiment results show that no
matter what kind of reordering knowledge is used,
the preprocessing of syntax-based reordering does
greatly improve translation performance, and that
the reordering of 3-ary nodes is crucial.
The second set of experiments test the effect of
some constraints. The basic setting is the same as
that of test 6 in the first experiment set, and reorder-
ing is applied to both binary and 3-ary nodes. The
results are shown in table 5.
In test (a), the constraint is that the module does
not consider any reordering of a node if the yield
of this node contains not more than four words.
The underlying rationale is that reordering within
distortion limit should be left to the distance-based
model during decoding, and syntax-based reorder-
ing should focus on global reordering only. The
result shows that this hypothesis does not hold.
In practice syntax-based reordering also helps lo-
cal reordering. Consider the translation example
of test (a) shown in table 6. Both the baseline
model and our model translate in the same way up
to the word ??w? (which is incorrectly translated
as ?and?). From this point, the proposed preprocess-
ing model correctly jump to the last phrase ??q?
?X/discussed?, while the baseline model fail to do
so for the best translation. It should be noted, how-
ever, that there are only four words between ??w?
and the last phrase, and the desired order of decod-
ing is within the capability of the baseline system.
With the feature of syntax-based global reordering,
a phrase-based decoder performs better even with
respect to local reordering. It is because syntax-
based reordering adds more weight to a hypothesis
that moves words across longer distance, which is
penalized by the distance-based model.
In test (b) distortion limit is set as 0; i.e. reorder-
ing is done merely by syntax-based preprocessing.
The worse result is not surprising since, after all,
preprocessing discards many possibilities and thus
reduce the search space of the decoder. Some local
reordering model is still needed during decoding.
Finally, test (c) shows that translation perfor-
mance does not improve significantly by raising the
number of reorderings. This implies that our ap-
proach is very efficient in that only a small value of
n is capable of capturing the most important global
reordering patterns.
8 Conclusion and Future Work
This paper proposes a novel, probabilistic approach
to reordering which combines the merits of syntax
and phrase-based SMT. On the one hand, global
reordering, which cannot be accomplished by the
726
Input ?$3 ,?)Z?C?wOcu??q??X
Reference Meanwhile , Yushchenko and his assistants discussed issues concerning the estab-
lishment of a new government
Baseline The same time , Yushchenko assistants and a new Government on issues discussed
Translation with
Preprocessing
The same time , Yushchenko assistants and held discussions on the issue of a new
government
Table 6: Translation Example 2
phrase-based model, is enabled by the tree opera-
tions in preprocessing. On the other hand, local re-
ordering is preserved and even strengthened in our
approach. Experiments show that, for the NIST MT-
05 task of Chinese-to-English translation, the pro-
posal leads to BLEU improvement of 1.56%.
Despite the encouraging experiment results, it
is still not very clear how the syntax-based and
distance-based models complement each other in
improving word reordering. In future we need to
investigate their interaction and identify the contri-
bution of each component. Moreover, it is observed
that the parse trees returned by a full parser like
the Stanford parser contain too many nodes which
seem not be involved in desired reorderings. Shal-
low parsers should be tried to see if they improve
the quality of reordering knowledge.
References
Yaser Al-Onaizan, and Kishore Papineni. 2006. Distor-
tion Models for Statistical Machine Translation. Pro-
ceedings for ACL 2006.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause Restructuring for Statistical Machine
Translation. Proceedings for ACL 2005.
M.R. Costa-jussa`, and J.A.R. Fonollosa. 2006. Statis-
tical Machine Reordering. Proceedings for EMNLP
2006.
Heidi Fox. 2002. Phrase Cohesion and Statistical Ma-
chine Translation. Proceedings for EMNLP 2002.
Jianfeng Gao, Mu Li, and Chang-Ning Huang 2003.
Improved Source-Channel Models for Chinese Word
Segmentation. Proceedings for ACL 2003.
Dan Klein and Christopher D. Manning. 2003. Accurate
Unlexicalized Parsing. Proceedings for ACL 2003.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-based Translation. Proceedings for
HLT-NAACL 2003.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. Proceedings for AMTA 2004.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation.
Proceedings for IWSLT 2005.
Franz J. Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. Proceedings for ACL
2003.
Franz J. Och, and Hermann Ney. 2000. Improved Statis-
tical Alignment Models. Proceedings for ACL 2000.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. Proceedings for ACL
2002.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency Treelet Translation: Syntactically Informed
Phrasal SMT. Proceedings for ACL 2005.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. Proceedings for the Interna-
tional Conference on Spoken Language Understand-
ing 2002.
Christoph Tillmann. 2004. A Unigram Orientation
Model for Statistical Machine Translation. Proceed-
ings for ACL 2004.
Fei Xia, and Michael McCord 2004. Improving a Statis-
tical MT System with Automatically Learned Rewrite
Patterns. Proceedings for COLING 2004.
Kenji Yamada, and Kevin Knight. 2001. A syntax-
based statistical translation model. Proceedings for
ACL 2001.
Le Zhang. 2004. Maximum Entropy
Modeling Toolkit for Python and C++.
http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.
727
Proceedings of ACL-08: HLT, pages 89?96,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Measure Word Generation for English-Chinese SMT Systems 
 
 
Dongdong Zhang1, Mu Li1, Nan Duan2, Chi-Ho Li1, Ming Zhou1 
1Microsoft Research Asia 2Tianjin University 
Beijing, China Tianjin, China 
{dozhang,muli,v-naduan,chl,mingzhou}@microsoft.com 
 
 
 
 
 
 
Abstract 
Measure words in Chinese are used to indi-
cate the count of nouns. Conventional sta-
tistical machine translation (SMT) systems do 
not perform well on measure word generation 
due to data sparseness and the potential long 
distance dependency between measure words 
and their corresponding head words. In this 
paper, we propose a statistical model to gen-
erate appropriate measure words of nouns for 
an English-to-Chinese SMT system. We mod-
el the probability of measure word generation 
by utilizing lexical and syntactic knowledge 
from both source and target sentences. Our 
model works as a post-processing procedure 
over output of statistical machine translation 
systems, and can work with any SMT system. 
Experimental results show our method can 
achieve high precision and recall in measure 
word generation. 
1 Introduction 
In linguistics, measure words (MW) are words or 
morphemes used in combination with numerals or 
demonstrative pronouns to indicate the count of 
nouns1, which are often referred to as head words 
(HW). 
Chinese measure words are grammatical units 
and occur quite often in real text. According to our 
survey on the measure word distribution in the 
Chinese Penn Treebank and the test datasets distri-
buted by Linguistic Data Consortium (LDC) for 
Chinese-to-English machine translation evaluation, 
the average occurrence is 0.505 and 0.319 measure 
                                                 
1 The uncommon cases of verbs are not considered. 
words per sentence respectively. Unlike in Chinese, 
there is no special set of measure words in English. 
Measure words are usually used for mass nouns 
and any semantically appropriate nouns can func-
tion as the measure words. For example, in the 
phrase three bottles of water, the word bottles acts 
as a measure word. Countable nouns are almost 
never modified by measure words2. Numerals and 
indefinite articles are directly followed by counta-
ble nouns to denote the quantity of objects.  
Therefore, in the English-to-Chinese machine 
translation task we need to take additional efforts 
to generate the missing measure words in Chinese. 
For example, when translating the English phrase 
three books into the Chinese phrases ?????, 
where three corresponds to the numeral ??? and 
books corresponds to the noun ???, the Chinese 
measure word ??? should be generated between 
the numeral and the noun.  
In most statistical machine translation (SMT) 
models (Och et al, 2004; Koehn et al, 2003; 
Chiang, 2005), some of measure words can be 
generated without modification or additional 
processing. For example, in above translation, the 
phrase translation table may suggest the word three 
be translated into ???, ????, ????, etc, and 
the word books into ???, ????, ???? (scroll), 
etc. Then the SMT model selects the most likely 
combination ????? as the final translation re-
sult. In this example, a measure word candidate set 
consisting of ??? and ??? can be generated by 
bilingual phrases (or synchronous translation rules), 
and the best measure word ??? from the measure  
                                                 
2 There are some exceptional cases, such as ?100 head of cat-
tle?. But they are very uncommon. 
89
  
 
 
 
 
 
 
 
 
 
 
 
 
word candidate set can be selected by the SMT 
decoder. However, as we will show below, existing 
SMT systems do not deal well with the measure 
word generation in general due to data sparseness 
and long distance dependencies between measure 
words and their corresponding head words.  
Due to the limited size of bilingual corpora, 
many measure words, as well as the collocations 
between a measure and its head word, cannot be 
well covered by the phrase translation table in an 
SMT system. Moreover, Chinese measure words 
often have a long distance dependency to their 
head words which makes language model ineffec-
tive in selecting the correct measure words from 
the measure word candidate set. For example, in 
Figure 1 the distance between the measure word 
??? and its head word ???? (undertaking) is 15. 
In this case, an n-gram language model with n<15 
cannot capture the MW-HW collocation. Table 1 
shows the relative position?s distribution of head 
words around measure words in the Chinese Penn 
Treebank, where a negative position indicates that 
the head word is to the left of the measure word 
and a positive position indicates that the head word 
is to the right of the measure word. Although lots 
of measure words are close to the head words they 
modify, more than sixteen percent of measure 
words are far away from their corresponding head 
words (the absolute distance is more than 5). 
To overcome the disadvantage of measure word 
generation in a general SMT system, this paper 
proposes a dedicated statistical model to generate 
measure words for English-to-Chinese translation. 
We model the probability of measure word gen-
eration by utilizing rich lexical and syntactic 
knowledge from both source and target sentences. 
Three steps are involved in our method to generate 
measure words: Identifying the positions to gener-
ate measure words, collecting the measure word 
candidate set and selecting the best measure word. 
Our method is performed as a post-processing pro-
cedure of the output of SMT systems. The advan-
tage is that it can be easily integrated into any SMT 
system. Experimental results show our method can 
significantly improve the quality of measure word 
generation. We also compared the performance of 
our model based on different contextual informa-
tion, and show that both large-scale monolingual 
data and parallel bilingual data can be helpful to 
generate correct measure words. 
Position Occurrence Position Occurrence
1 39.5% -1 0 
2 15.7% -2 0 
3 4.7% -3 8.7% 
4 1.4% -4 6.8% 
5 2.1% -5 4.3% 
>5 8.8% <-5 8.0% 
Table 1. Position distribution of head words 
2 Our Method 
2.1 Measure word  generation in Chinese 
In Chinese, measure words are obligatory in cer-
tain contexts, and the choice of measure word 
usually depends on the head word?s semantics (e.g., 
shape or material). The set of Chinese measure 
words is a relatively close set and can be classified 
into two categories based on whether they have a 
corresponding English translation. Those not hav-
ing an English counterpart need to be generated 
during translation. For those having English trans-
lations, such as ??? (meter), ??? (ton), we just 
use the translation produced by the SMT system 
itself. According to our survey, about 70.4% of 
measure words in the Chinese Penn Treebank need 
Figure 1.  Example of long distance dependency between MW and its modified HW 
??/??/ 
??/ ?/ 
? ?? 
Pudong 's de-
velopment and 
opening up is a century-spanning 
/?/?
?/ 
for vigorously promoting shanghai 
and constructing a modern econom-
ic , trade , and financial center  undertaking
??/??/ ?/ ?? /??? /??
/ ?/ ??/ ? /??/ ??/ ?/ 
? 
. 
?
90
 to be explicitly generated during the translation 
process. 
In Chinese, there are generally stable linguistic 
collocations between measure words and their head 
words. Once the head word is determined, the col-
located measure word can usually be selected ac-
cordingly. However, there is no easy way to identi-
fy head words in target Chinese sentences since for 
most of the time an SMT output is not a well 
formed sentence due to translation errors. Mistake 
of head word identification may cause low quality 
of measure word generation. In addition, some-
times the head word itself is not enough to deter-
mine the measure word. For example, in Chinese 
sentences ???? 5??? (there are five people 
in his family) and ???? 5???????? (a 
total of five people attended the meeting), where 
??? (people) is the head word collocated with two 
different measure words ??? and ???, we cannot 
determine the measure word just based on the head 
word ???.   
2.2 Framework 
In our framework, a statistical model is used to 
generate measure words. The model is applied to 
SMT system outputs as a post-processing proce-
dure. Given an English source sentence, an SMT 
decoder produces a target Chinese translation, in 
which positions for measure word generation are 
identified. Based on contextual information con-
tained in both input source sentence and SMT sys-
tem?s output translation, a measure word candidate 
set M is constructed. Then a measure word selec-
tion model is used to select the best one from M. 
Finally, the selected measure word is inserted into 
previously determined measure word slot in the 
SMT system?s output, yielding the final translation 
result. 
2.3 Measure word position identification 
To identify where to generate measure words in the 
SMT outputs, all positions after numerals are 
marked at first since measure words often follow 
numerals. For other cases in which measure words 
do not follow numerals (e.g., ??? /? /??? 
(many computers), where ??? is a measure word 
and ???? (computers) is its head word), we just 
mine the set of words which can be followed by 
measure words from training corpus.  Most of 
words in the set are pronouns such as ??? (this), 
??? (that) and ???? (several). In the SMT out-
put, the positions after these words are also identi-
fied as candidate positions to generate measure 
words.  
2.4 Candidate measure word generation 
To avoid high computation cost, the measure word 
candidate set only consists of those measure words 
which can form valid MW-HW collocations with 
their head words. We assume that all the surround-
ing words within a certain window size centered on 
the given position to generate a measure word are 
potential head words, and require that a measure 
word candidate must collocate with at least one of 
the surrounding words. Valid MW-HW colloca-
tions are mined from the training corpus and a sep-
arate lexicon resource.  
There is a possibility that the real head word is 
outside the window of given size. To address this 
problem, we also use a source window centered on 
the position ps, which is aligned to the target meas-
ure word position pt. The link between ps and pt 
can be inferred from SMT decoding result. Thus, 
the chance of capturing the best measure word in-
creases with the aid of words located in the source 
window. For example, given the window size of 10, 
although the target head word ???? (undertaking) 
in Figure 1 is located outside the target window, its 
corresponding source head word undertaking can 
be found in the source window. Based on this 
source head word, the best measure word ??? will 
be included into the candidate measure word set. 
This example shows how bilingual information can 
enrich the measure word candidate set. 
Another special word {NULL} is always in-
cluded in the measure word candidate set. {NULL} 
represents those measure words having a corres-
ponding English translation as mentioned in Sec-
tion 2.1. If {NULL} is selected, it means that we 
need not generate any measure word at the current 
position. Thus, no matter what kinds of measure 
words they are, we can handle the issue of measure 
word generation in a unified framework.  
2.5 Measure word selection model 
After obtaining the measure word candidate set M, 
a measure word selection model is employed to 
select the best one from M. Given the contextual 
information C in both source window and target 
91
 window, we model the measure word selection as 
finding the measure word m* with highest post-
erior probability given C: 
?? = argmax????(?|?)                  (1) 
To leverage the collocation knowledge between 
measure words and head words, we extend (1) by 
introducing a hidden variable h where H represents 
all candidate head words located within the target 
window: 
     ?? = argmax??? ? ?(?, ?|?)???  
           = argmax??? ? ?(?|?)?(?|?, ?)???   (2) 
In (2), ?(?|?) is the head word selection proba-
bility and is empirically estimated according to the 
position distribution of head words in Table 1. 
?(?|?, ?) is the conditional probability of m given 
both h and C. We use maximum entropy model to 
compute ?(?|?, ?): 
            ?(?|?, ?) = exp(? ?? ??(?,?)? )? exp(? ?? ??(??,?)? )????      (3) 
Based on the different features used in the com-
putation of ?(?|?, ?) , we can train two sub-
models ? a monolingual model (Mo-ME) which 
only uses monolingual (Chinese) features and a 
bilingual model (Bi-ME) which integrates bilingual 
features. The advantage of the Mo-ME model is 
that it can employ an unlimited monolingual target 
training corpora, while the Bi-ME model leverages 
rich features including both the source and target 
information and may improve the precision. Com-
pared to the Mo-ME model, the Bi-ME model suf-
fers from small scale of parallel training data. To 
leverage advantages of both models, we use a 
combined model Co-ME, by linearly combing the 
monolingual and bilingual sub-models: 
?? = argmax??????????  + (1 ? ?)??????  
where ? ? [0,1] is a free parameter that can be op-
timized on held-out data and it was set to 0.39 in 
our experiments. 
2.6 Features 
The computation of Formula (3) involves the fea-
tures listed in Table 2 where the Mo-ME model 
only employs target features and the Bi-ME model 
leverages both target features and source features.  
For target features, n-gram language model 
score is defined as the sum of log n-gram probabil-
ities within the target window after the measure 
word is filled into the measure word slot. The 
MW-HW collocation feature is defined to be a 
function f1 to capture the collocation between a 
measure word and a head word. For features of 
surrounding words, the feature function f2 is de-
fined as 1 if a certain word exists at a certain posi-
tion, otherwise 0. For example, f2(?,-2)=1 means 
the second word on the left is ???. f2(?,3)=1 
means the third word on the right is ???. For 
punctuation position feature function f3, the feature 
value is 1 when there is a punctuation following 
the measure word, which indicates the target head 
word may appear to the left of measure word. Oth-
erwise, it is 0. In practice, we can also ignore the 
position part, i.e., a word appears anywhere within 
the window is viewed as the same feature. 
 Target features Source features 
n-gram language model 
score 
MW-HW collocation
MW-HW collocation surrounding words 
surrounding words source head word 
punctuation position POS tags 
Table 2. Features used in our model 
For source language side features, MW-HW col-
location and surrounding words are used in a simi-
lar way as does with target features. The source 
head word feature is defined to be a function f4 to 
indicate whether a word ei is the source head word 
in English according to a parse tree of the source 
sentence. Similar to the definition of lexical fea-
tures, we also use a set of features based on POS 
tags of source language. 
3 Model Training and Application 
3.1 Training 
We parsed English and Chinese sentences to get 
training samples for measure word generation 
model. Based on the source syntax parse tree, for 
each measure word, we identified its head word by 
using a toolkit from (Chiang and Bikel, 2002) 
which can heuristically identify head words for 
sub-trees. For the bilingual corpus, we also per-
form word alignment to get correspondences be-
tween source and target words. Then, the colloca-
tion between measure words and head words and 
their surrounding contextual information are ex-
tracted to train the measure word selection models. 
According to word alignment results, we classify 
92
 measure words into two classes based on whether 
they have non-null translations. We map Chinese 
measure words having non-null translations to a 
unified symbol {NULL} as mentioned in Section 
2.4, indicating that we need not generate these kind 
of measure words since they can be translated from 
English.  
In our work, the Berkeley parser (Petrov and 
Klein, 2007) was employed to extract syntactic 
knowledge from the training corpus. We ran GI-
ZA++ (Och and Ney, 2000) on the training corpus 
in both directions with IBM model 4, and then ap-
plied the refinement rule described in (Koehn et al, 
2003) to obtain a many-to-many word alignment 
for each sentence pair. We used the SRI Language 
Modeling Toolkit (Stolcke, 2002) to train a five-
gram model with modified Kneser-Ney smoothing 
(Chen and Goodman, 1998). The Maximum Entro-
py training toolkit from (Zhang, 2006) was em-
ployed to train the measure word selection model. 
3.2 Measure word generation 
As mentioned in previous sections, we apply our 
measure word generation module into SMT output 
as a post-processing step. Given a translation from 
an SMT system, we first determine the position pt 
at which to generate a Chinese measure word. Cen-
tered on pt, a surrounding word window with spe-
cified size is determined. From translation align-
ments, the corresponding source position ps aligned 
to pt can be referred.  In the same way, a source 
window centered on ps is determined as well. Then, 
contextual information within the windows in the 
source and the target sentence is extracted and fed 
to the measure word selection model. Meanwhile, 
the candidate set is obtained based on words in 
both windows. Finally, each measure word in the 
candidate set is inserted to the position pt, and its 
score is calculated based on the models presented 
in Section 2.5. The measure word with the highest 
probability will be chosen.  
There are two reasons why we perform measure 
word generation for SMT systems as a post-
processing step. One is that in this way our method 
can be easily applied to any SMT system. The oth-
er is that we can leverage both source and target 
information during the measure word generation 
process. We do not integrate our measure word 
generation module into the SMT decoder since 
there is only little target contextual information 
available during SMT decoding. Moreover, as we 
will show in experiment section, a pre-processing 
method does not work well when only source in-
formation is available. 
4 Experiments 
4.1 Data 
In the experiments, the language model is a Chi-
nese 5-gram language model trained with the Chi-
nese part of the LDC parallel corpus and the Xin-
hua part of the Chinese Gigaword corpus with 
about 27 million words. We used an SMT system 
similar to Chiang (2005), in which FBIS corpus is 
used as the bilingual training data. The training 
corpus for Mo-ME model consists of the Chinese 
Peen Treebank and the Chinese part of the LDC 
parallel corpus with about 2 million sentences. The 
Bi-ME model is trained with FBIS corpus, whose 
size is smaller than that used in Mo-ME model 
training. 
We extracted both development and test data set 
from years of NIST Chinese-to-English evaluation 
data by filtering out sentence pairs not containing 
measure words. The development set is extracted 
from NIST evaluation data from 2002 to 2004, and 
the test set consists of sentence pairs from NIST 
evaluation data from 2005 to 2006. There are 759 
testing cases for measure word generation in our 
test data consisting of 2746 sentence pairs. We use 
the English sentences in the data sets as input to 
the SMT decoder, and apply our proposed method 
to generate measure words for the output from the 
decoder. Measure words in Chinese sentences of 
the development and test sets are used as refer-
ences. When there are more than one measure 
words acceptable at some places, we manually 
augment the references with multiple acceptable 
measure words. 
4.2 Baseline 
Our baseline is the SMT output where measure 
words are generated by a Hiero-like SMT decoder 
as discussed in Section 1. Due to noises in the Chi-
nese translations introduced by the SMT system, 
we cannot correctly identify all the positions to 
generate measure words. Therefore, besides preci-
sion we examine recall in our experiments. 
4.3 Evaluation over SMT output 
Table 3 and Table 4 show the precision and recall 
of our measure word generation method. From the 
93
 experimental results, the Mo-ME, Bi-ME and Co-
ME models all outperform the baseline. Compared 
with the baseline, the Mo-ME method takes advan-
tage of a large size monolingual training corpus 
and reduces the data sparseness problem. The ad-
vantage of the Bi-ME model is being able to make 
full use of rich knowledge from both source and 
target sentences. Also as shown in Table 3 and Ta-
ble 4, the Co-ME model always achieve the best 
results when using the same window size since it 
leverages the advantage of both the Mo-ME and 
the Bi-ME models. 
Wsize Baseline Mo-ME Bi-ME Co-ME
6  
 
54.82% 
64.29% 67.15%  67.66% 
8 64.93% 68.50%  69.00% 
10 64.72% 69.40% 69.58%
12 65.46% 69.40% 69.76%
14 65.61% 69.69%  70.03% 
Table 3. Precision over SMT output 
Wsize Baseline Mo-ME Bi-ME Co-ME
6  
 
45.61% 
51.48% 53.69%  54.09% 
8 51.98% 54.75%  55.14% 
10 51.81% 55.44% 55.58%
12 52.38% 55.44% 55.72%
14 52.50% 55.67%  55.93% 
Table 4. Recall over SMT output 
We can see that the Bi-ME model can achieve 
better results than the Mo-ME model in both recall 
and precision metrics although only a small sized 
bilingual corpus is used for Bi-ME model training. 
The reason is that the Mo-ME model cannot cor-
rectly handle the cases where head words are lo-
cated outside the target window. However, due to 
word order differences between English and Chi-
nese, when target head words are outside the target 
window, their corresponding source head words 
might be within the source window. The capacity 
of capturing head words is improved when both 
source and target windows are used, which demon-
strates that bilingual knowledge is useful for meas-
ure word generation. 
We compare the results for each model with dif-
ferent window sizes. Larger window size can lead 
to better results as shown in Table 3 and Table 4 
since more contextual knowledge is used to model 
measure word generation. However, enlarging the 
window size does not bring significant improve-
ments, The major reason is that even a small win-
dow size is already able to cover most of measure 
word collocations, as indicated by the position dis-
tribution of head words in Table 1.  
The quality of the SMT output also affects the 
quality of measure word generation since our me-
thod is performed in a post-processing step over 
the SMT output. Although translation errors de-
grade the measure word generation accuracy, we 
achieve about 15% improvement in precision and a 
10% increase in recall over baseline. We notice 
that the recall is relatively lower. Part of the reason 
is some positions to generate measure words are 
not successfully identified due to translation errors. 
In addition to precision and recall, we also evaluate 
the Bleu score (Papineni et al, 2002) changes be-
fore and after applying our measure word genera-
tion method to the SMT output. For our test data, 
we only consider sentences containing measure 
words for Bleu score evaluation. Our measure 
word generation step leads to a Bleu score im-
provement of 0.32 where the window size is set to 
10, which shows that it can improve the translation 
quality of an English-to-Chinese SMT system. 
4.4 Evaluation over reference data 
To isolate the impact of the translation errors in 
SMT output on the performance of our measure 
word generation model, we conducted another ex-
periment with reference bilingual sentences in 
which measure words in Chinese sentences are 
manually removed. This experiment can show the 
performance upper bound of our method without 
interference from an SMT system. Table 5 shows 
the results. Compared to the results in Table 3, the 
precision improvement in the Mo-ME model is 
larger than that in the Bi-ME model, which shows 
that noisy translation of the SMT system has more 
serious influence on the Mo-ME model than the 
Bi-ME model. This also indicates that source in-
formation without noises is helpful for measure 
word generation. 
Wsize Mo-ME Bi-ME Co-ME 
6 71.63% 74.92% 75.72% 
8 73.80% 75.48% 76.20% 
10 73.80% 74.76% 75.48% 
12 73.80% 75.24% 75.96% 
14 73.56% 75.48% 76.44% 
Table 5. Results over reference data 
94
 4.5 Impacts of features 
In this section, we examine the contribution of 
both target language based features and source 
language based features in our model. Table 6 and 
Table 7 show the precision and recall when using 
different features. The window size is set to 10. In 
the tables, Lm denotes the n-gram language model 
feature, Tmh denotes the feature of collocation be-
tween target head words and the candidate measure 
word, Smh denotes the feature of collocation be-
tween source head words and the candidate meas-
ure word, Hs denotes the feature of source head 
word selection, Punc denotes the feature of target 
punctuation position, Tlex denotes surrounding 
word features in translation, Slex denotes surround-
ing word features in source sentence, and Pos de-
notes Part-Of-Speech feature. 
Feature setting Precision Recall 
Baseline 54.82% 45.61% 
Lm 51.11% 41.24% 
+Tmh 61.43% 49.22% 
+Punc 62.54% 50.08% 
+Tlex 64.80% 51.87% 
Table 6. Feature contribution in Mo-ME model 
Feature setting Precision Recall 
Baseline 54.82% 45.61% 
Lm 51.11% 41.24% 
+Tmh+Smh 64.50% 51.64% 
+Hs 65.32% 52.26% 
+Punc 66.29% 53.10% 
+Pos 66.53% 53.25% 
+Tlex 67.50% 54.02% 
+Slex 69.52% 55.54% 
Table 7. Feature contribution in Bi-ME model 
The experimental results show that all the fea-
tures can bring incremental improvements. The 
method with only Lm feature performs worse than 
the baseline. However, with more features inte-
grated, our method outperforms the baseline, 
which indicates each kind of features we selected 
is useful for measure word generation. According 
to the results, the feature of MW-HW collocation 
has much contribution to reducing the selection 
error of measure words given head words. The 
contribution of Slex feature explains that other sur-
rounding words in source sentence are also helpful 
since head word determination in source language 
might be incorrect due to errors in English parse 
trees. Meanwhile, the contribution from Smh, Hs 
and Slex features demonstrates that bilingual 
knowledge can play an important role for measure 
word generation. Compared with lexicalized fea-
tures, we do not get much benefit from the Pos 
features. 
4.6 Error analysis 
We conducted an error analysis on 100 randomly 
selected sentences from the test data. There are 
four major kinds of errors as listed in Table 8. 
Most errors are caused by failures in finding posi-
tions to generate measure words. The main reason 
for this is some hint information used to identify 
measure word positions is missing in the noisy 
output of SMT systems. Two kinds of errors are 
introduced by incomplete head word and MW-HW 
collocation coverage, which can be solved by en-
larging the size of training corpus. There are also 
head word selection errors due to incorrect syntax 
parsing. 
Error type Ratio 
unseen head word  32.14% 
unseen MW-HW collocation 10.71% 
missing MW position 39.29% 
incorrect HW selection 10.71% 
others 7.14% 
Table 8. Error distribution 
4.7 Comparison with other methods 
In this section we compare our statistical methods 
with the pre-processing method and the rule-based 
methods for measure word generation in a transla-
tion task.  
In pre-processing method, only source language 
information is available. Given a source sentence, 
the corresponding syntax parse tree Ts is first con-
structed with an English parser. Then the pre-
processing method chooses the source head word 
hs based on Ts. The candidate measure word with 
the highest probability collocated with hs is se-
lected as the best result, where the measure word 
candidate set corresponding to each head word is 
mined over a bilingual training corpus in advance. 
We achieved precision 58.62% and recall 49.25%, 
which are worse than the results of our post-
processing based methods. The weakness of the 
pre-processing method is twofold. One problem is 
data sparseness with respect to collocations be-
95
 tween English head words and Chinese measure 
words. The other problem comes from the English 
head word selection error introduced by using 
source parse trees.  
We also compared our method with a well-
known rule-based machine translation system ? 
SYSTRAN3. We translated our test data with SY-
STRAN?s English-to-Chinese translation engine. 
The precision and recall are 63.82% and 51.09% 
respectively, which are also lower than our method.  
5 Related Work  
Most existing rule-based English-to-Chinese MT 
systems have a dedicated module handling meas-
ure word generation. In general a rule-based me-
thod uses manually constructed rule patterns to 
predict measure words. Like most rule based ap-
proaches, this kind of system requires lots of hu-
man efforts of experienced linguists and usually 
cannot easily be adapted to a new domain. The 
most relevant work based on statistical methods to 
our research might be statistical technologies em-
ployed to model issues such as morphology gener-
ation (Minkov et al, 2007). 
6 Conclusion and Future Work 
In this paper we propose a statistical model for 
measure word generation for English-to-Chinese 
SMT systems, in which contextual knowledge 
from both source and target sentences is involved. 
Experimental results show that our method not on-
ly achieves high precision and recall for generating 
measure words, but also improves the quality of 
English-to-Chinese SMT systems. 
In the future, we plan to investigate more fea-
tures and enlarge coverage to improve the quality 
of measure word generation, especially reduce the 
errors found in our experiments. 
Acknowledgements 
Special thanks to David Chiang, Stephan Stiller 
and the anonymous reviewers for their feedback 
and insightful comments. 
References 
Stanley F. Chen and Joshua Goodman. 1998. An Empir-
ical study of smoothing techniques for language 
                                                 
3 http://www.systransoft.com/ 
modeling. Technical Report TR-10-98, Harvard Uni-
versity Center for Research in Computing Technolo-
gy, 1998. 
David Chiang and Daniel M. Bikel. 2002. Recovering 
latent information in treebanks. Proceedings of COL-
ING '02, 2002.  
David Chiang. 2005. A hierarchical phrase-based mod-
el for statistical machine translation. In Proceedings 
of ACL 2005, pages 263-270. 
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. 
Statistical phrase-based translation. In Proceedings of 
HLT-NAACL 2003, pages 127-133.  
Einat Minkov, Kristina Toutanova, and Hisami Suzuki. 
2007. Generating complex morphology for machine 
translation. In Proceedings of 45th Annual Meeting 
of the ACL, pages 128-135. 
Franz J. Och and Hermann Ney. 2000. Improved statis-
tical alignment models. In Proceedings of 38th An-
nual Meeting of the ACL, pages 440-447.  
Franz J. Och and Hermann Ney. 2004. The alignment 
template approach to statistical machine translation. 
Computational Linguistics, 30:417-449. 
Kishore Papineni, Salim Roukos, ToddWard, and WeiJ-
ing Zhu. 2002. BLEU: a method for automatic evalu-
ation of machine translation. In Proceedings of 40th 
Annual Meeting of the ACL, pages 311-318. 
Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. In Proceedings of HLT-
NAACL, 2007. 
Andreas Stolcke. 2002. SRILM - an extensible language 
modeling toolkit. In Proceedings of International 
Conference on Spoken Language Processing, volume 
2, pages 901-904.  
Le Zhang. MaxEnt toolkit. 2006. http://homepages.inf. 
ed.ac.uk/s0450736/maxent_toolkit.html  
96
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 585?592,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Collaborative Decoding: Partial Hypothesis Re-ranking 
Using Translation Consensus between Decoders 
 
Mu Li1, Nan Duan2, Dongdong Zhang1, Chi-Ho Li1, Ming Zhou1 
        1Microsoft Research Asia                                        2Tianjin University 
              Beijing, China                                                     Tianjin, China 
{muli,v-naduan,dozhang,chl,mingzhou}@microsoft.com 
 
 
Abstract 
This paper presents collaborative decoding 
(co-decoding), a new method to improve ma-
chine translation accuracy by leveraging trans-
lation consensus between multiple machine 
translation decoders. Different from system 
combination and MBR decoding, which post-
process the n-best lists or word lattice of ma-
chine translation decoders, in our method mul-
tiple machine translation decoders collaborate 
by exchanging partial translation results. Us-
ing an iterative decoding approach, n-gram 
agreement statistics between translations of 
multiple decoders are employed to re-rank 
both full and partial hypothesis explored in 
decoding. Experimental results on data sets for 
NIST Chinese-to-English machine translation 
task show that the co-decoding method can 
bring significant improvements to all baseline 
decoders, and the outputs from co-decoding 
can be used to further improve the result of 
system combination. 
1 Introduction 
Recent research has shown substantial improve-
ments can be achieved by utilizing consensus 
statistics obtained from outputs of multiple ma-
chine translation systems. Translation consensus 
can be measured either at sentence level or at 
word level. For example, Minimum Bayes Risk 
(MBR) (Kumar and Byrne, 2004) decoding over 
n-best list tries to find a hypothesis with lowest 
expected loss with respect to all the other transla-
tions, which can be viewed as sentence-level 
consensus-based decoding. Word based methods 
proposed range from straightforward consensus 
voting (Bangalore et al, 2001; Matusov et al, 
2006) to more complicated word-based system 
combination model (Rosti et al, 2007; Sim et al, 
2007). Typically, the resulting systems take out-
puts of individual machine translation systems as 
input, and build a new confusion network for 
second-pass decoding. 
There have been many efforts dedicated to ad-
vance the state-of-the-art performance by com-
bining multiple systems? outputs. Most of the 
work focused on seeking better word alignment 
for consensus-based confusion network decoding 
(Matusov et al, 2006) or word-level system 
combination (He et al, 2008; Ayan et al, 2008). 
In addition to better alignment, Rosti et al 
(2008) introduced an incremental strategy for 
confusion network construction; and Hildebrand 
and Vogel (2008) proposed a hypotheses re-
ranking model for multiple systems? outputs with 
more features including word translation proba-
bility and n-gram agreement statistics. 
A common property of all the work mentioned 
above is that the combination models work on 
the basis of n-best translation lists (full hypo-
theses) of existing machine translation systems. 
However, the n-best list only presents a very 
small portion of the entire search space of a Sta-
tistical Machine Translation (SMT) model while 
a majority of the space, within which there are 
many potentially good translations, is pruned 
away in decoding. In fact, due to the limitations 
of present-day computational resources, a consi-
derable number of promising possibilities have to 
be abandoned at the early stage of the decoding 
process. It is therefore expected that exploring 
additional possibilities beyond n-best hypotheses 
lists for full sentences could bring improvements 
to consensus-based decoding. 
In this paper, we present collaborative decod-
ing (or co-decoding), a new SMT decoding 
scheme to leverage consensus information be-
tween multiple machine translation systems. In 
this scheme, instead of using a post-processing 
step, multiple machine translation decoders col-
laborate during the decoding process, and trans-
lation consensus statistics are taken into account 
to improve ranking not only for full translations, 
but also for partial hypotheses. In this way, we 
585
expect to reduce search errors caused by partial 
hypotheses pruning, maximize the contribution 
of translation consensus, and result in better final 
translations. 
We will discuss the general co-decoding mod-
el, requirements for decoders that enable colla-
borative decoding and describe the updated mod-
el structures. We will present experimental re-
sults on the data sets of NIST Chinese-to-English 
machine translation task, and demonstrate that 
co-decoding can bring significant improvements 
to baseline systems.  We also conduct extensive 
investigations when different settings of co-
decoding are applied, and make comparisons 
with related methods such as word-level system 
combination of hypothesis selection from mul-
tiple n-best lists.  
The rest of the paper is structured as follows. 
Section 2 gives a formal description of the co-
decoding model, the strategy to apply consensus 
information and hypotheses ranking in decoding. 
In Section 3, we make detailed comparison be-
tween co-decoding and related work such as sys-
tem combination and hypotheses selection out of 
multiple systems.  Experimental results and dis-
cussions are presented in Section 4. Section 5 
concludes the paper. 
2 Collaborative Decoding 
2.1 Overview 
Collaborative decoding does not present a full 
SMT model as other SMT decoders do such as 
Pharaoh (Koehn, 2004) or Hiero (Chiang, 2005). 
Instead, it provides a framework that accommo-
dates and coordinates multiple MT decoders. 
Conceptually, collaborative decoding incorpo-
rates the following four constituents:  
1. Co-decoding model. A co-decoding model 
consists of a set of member models, which 
are a set of augmented baseline models. We 
call decoders based on member models 
member decoders, and those based on base-
line models baseline decoders. In our work, 
any Maximum A Posteriori (MAP) SMT 
model with log-linear formulation (Och, 
2002) can be a qualified candidate for a 
baseline model. The requirement for a log-
linear model aims to provide a natural way to 
integrate the new co-decoding features. 
2. Co-decoding features. Member models are 
built by adding additional translation consen-
sus -based co-decoding features to baseline 
models. A baseline model can be viewed as a 
special case of member model with all co-
decoding feature values set to 0. Accordingly, 
a baseline decoder can be viewed as a special 
setting of a member decoder. 
3. Decoder coordinating. In co-decoding, each 
member decoder cannot proceed solely based 
on its own agenda. To share consensus statis-
tics with others, the decoding must be per-
formed in a coordinated way.  
4. Model training. Since we use multiple inter-
related decoders and introduce more features 
in member models, we also need to address 
the parameter estimation issue in the frame-
work of co-decoding. 
In the following sub-sections we first establish a 
general model for co-decoding, and then present 
details of feature design and decoder implemen-
tation, as well as parameter estimation in the co-
decoding framework. We leave the investigation 
of using specific member models to the experi-
ment section. 
2.2 Generic Collaborative Decoding Model 
For a given source sentence f, a member model 
in co-decoding finds the best translation ?? 
among the set of possible candidate translations 
?(?) based on a scoring function ?: 
?? = argmax???(?)?(?) (1) 
In the following, we will use ??  to denote the 
???  member decoder, and also use the notation 
??(?) for the translation hypothesis space of f 
determined by ?? . The ?
??  member model can 
be written as: 
??  ? = ?? (?, ?) + ??(?,??(?))
?,???
 (2) 
where ?? (?, ?) is the score function of the ?
??  
baseline model, and each ??(?,??(?)) is a par-
tial consensus score function with respect to ??  
and is defined over e and ?? ? :  
?? ?,?? ?  = ?? ,?  ??,?(?,?? ? ) 
?
 (3) 
where each ?? ,?(?,?? ? ) is a feature function 
based on a consensus measure between e and 
?? ? , and ??,?  is the corresponding feature 
weight. Feature index l ranges over all consen-
sus-based features in Equation 3. 
2.3 Decoder Coordination 
Before discussing the design and computation of 
translation consensus -based features, we first 
586
describe the multiple decoder coordination issue 
in co-decoding. Note that in Equation 2, though 
the baseline score function ??  ?, ?  can be 
computed inside each decoder, the case of 
??(?,??(?))  is more complicated. Because 
usually it is not feasible to enumerate the entire 
hypothesis space for machine translation, we ap-
proximate ?? ?  with n-best hypotheses by 
convention. Then there is a circular dependency 
between co-decoding features and ??(?) : on 
one hand, searching for n-best approximation of 
??(?) requires using Equation 2 to select top-
ranked hypotheses; while on the other hand, Eq-
uation 2 cannot be computed until every ??(?) 
is available.  
We address this issue by employing a boot-
strapping method, in which the key idea is that 
we can use baseline models? n-best hypotheses 
as seeds, and iteratively refine member models? 
n-best hypotheses with co-decoding. Similar to a 
typical phrase-based decoder (Koehn, 2004), we 
associate each hypothesis with a coverage vector 
c to track translated source words in it. We will 
use ??(?,?) for the set of hypotheses associated 
with c, and we also denote with ??(?) =
 ??(?,?)?  the set of all hypotheses generated 
by member decoder ??  in decoding. The co-
decoding process can be described as follows: 
1. For each member decoder ?? , perform de-
coding with a baseline model, and memorize 
all translation hypotheses generated during 
decoding in ??(?); 
2. Re-group translation hypotheses in ??(?) 
into a set of buckets  ?? ?,?  by the cover-
age vector c associated with each hypothesis; 
3. Use member decoders to re-decode source 
sentence ? with member models. For mem-
ber decoder ?? , consensus-based features of 
any hypotheses associated with coverage 
vector c are computed based on current set-
ting of ?? ?,?  for all s but k. New hypo-
theses generated by ??  in re-decoding are 
cached in ??
? (?); 
4. Update all ??(?) with ??
? (?); 
5. Iterate from step 2 to step 4 until a preset 
iteration limit is reached. 
In the iterative decoding procedure described 
above, hypotheses of different decoders can be 
mutually improved. For example, given two de-
coders ?1  and ?2  with hypotheses sets ?1  and 
?2 , improvements on ?1  enable ?2  to improve 
?2, and in turn ?1 benefits from improved ?2, 
and so forth. 
Step 2 is used to facilitate the computation of 
feature functions ?? ,?(?,?? ? ) , which require 
both e and every hypothesis in ?? ?   should be 
translations of the same set of source words. This 
step seems to be redundant for CKY-style MT 
decoders (Liu et al, 2006; Xiong et al, 2006; 
Chiang, 2005) since the grouping is immediately 
available from decoders because all hypotheses 
spanning the same range of source sentence have 
been stacked together in the same chart cell. But 
to be a general framework, this step is necessary 
for some state-of-the-art phrase-based decoders 
(Koehn, 2007; Och and Ney, 2004) because in 
these decoders, hypotheses with different cover-
age vectors can co-exist in the same bin, or hypo-
theses associated with the same coverage vector 
might appear in different bins.  
Note that a member model does not enlarge 
the theoretical search space of its baseline model, 
the only change is hypothesis scoring. By re-
running a complete decoding process, member 
model can be applied to re-score all hypotheses 
explored by a decoder. Therefore step 3 can be 
viewed as full-scale hypothesis re-ranking be-
cause the re-ranking scope is beyond the limited 
n-best hypotheses currently cached in ?? .  
In the implementation of member decoders, 
there are two major modifications compared to 
their baseline decoders. One is the support for 
co-decoding features, including computation of 
feature values and the use of augmented co-
decoding score function (Equation 2) for hypo-
thesis ranking and pruning. The other is hypothe-
sis grouping based on coverage vector and a me-
chanism to effectively access grouped hypothes-
es in step 2 and step 3. 
2.4 Co-decoding Features 
We now present the consensus-based feature 
functions  ?? ,?(?,?? ? ) introduced in Equation 
3. In this work all the consensus-based features 
have the following formulation: 
?? ,? ?,?? ?  =  ? ?? ?? ??(?, ??)
????? ? 
 (4) 
where e is a translation of f by decoder ?? (? ?
?), ? ?  is a translation in ?? ?  and ? ?? ??  is 
the posterior probability of translation ? ?  deter-
mined by decoder ??  given source sentence f. 
??(?, ??) is a consensus measure defined on e and 
??, by varying which different feature functions 
can be obtained.  
587
Referring to the log-linear model formulation, 
the translation posterior ? ?? ??  can be com-
puted as: 
? ?? ?? =
exp ??? ??  
 exp ??? ???  ??? ??? ? 
 (5) 
where ??(?) is the score function given in Equa-
tion 2, and  ? is a scaling factor following the 
work of Tromble et al (2008) 
To compute the consensus measures, we fur-
ther decompose each ?? ?, ??  into n-gram 
matching statistics between e and ??. Here we do 
not discriminate among different lexical n-grams 
and are only concerned with statistics aggrega-
tion of all n-grams of the same order. For each n-
gram of order n, we introduce a pair of comple-
mentary consensus measure functions ??+ ?, ??  
and ??? ?, ??  described as follows: 
??+ ?, ?
?  is the n-gram agreement measure 
function which counts the number of occurrences 
in ? ?of n-grams in e. So the corresponding fea-
ture value will be the expected number of occur-
rences in ?? ?  of all n-grams in e:  
??+ ?, ?? = ?(??
?+??1 , ??)
 ? ??+1
?=1
 
where ?(?,?)  is a binary indicator function ? 
? ??
?+??1 , ??  is 1 if the n-gram ??
?+??1 occurs in 
? ?  and 0 otherwise. 
??? ?, ?
?  is the n-gram disagreement meas-
ure function which is complementary to 
??+ ?, ?
? : 
??? ?, ?? =  1? ? ??
?+??1 , ??  
 ? ??+1
?=1
 
This feature is designed because ??+ ?, ?
?  
does not penalize long translation with low pre-
cision. Obviously we have the following: 
??+ ?, ?? + ??? ?, ?? =  ? ? ? + 1 
So if the weights of agreement and disagree-
ment features are equal, the disagreement-based 
features will be equivalent to the translation 
length features. Using disagreement measures 
instead of translation length there could be two 
potential advantages: 1) a length feature has been 
included in the baseline model and we do not 
need to add one; 2) we can scale disagreement 
features independently and gain more modeling 
flexibility. 
Similar to a language model score, n-gram 
consensus -based feature values cannot be 
summed up from smaller hypotheses. Instead, it 
must be re-computed when building each new 
hypothesis. 
2.5 Model Training 
We adapt the Minimum Error Rate Training 
(MERT) (Och, 2003) algorithm to estimate pa-
rameters for each member model in co-decoding. 
Let ??  be the feature weight vector for member 
decoder ?? , the training procedure proceeds as 
follows: 
1. Choose initial values for ?1 ,? ,??   
2. Perform co-decoding using all member de-
coders on a development set D with 
?1 ,? ,?? . For each decoder ?? , find a new 
feature weight vector ??
?  which optimizes 
the specified evaluation criterion L on D us-
ing the MERT algorithm based on the n-best 
list ??  generated by ?? : 
??
? = argmax? ? (?|?,??  ,?)) 
where T denotes the translations selected by 
re-ranking the translations in ??  using a 
new feature weight vector ? 
3. Let ?1 = ?1
? ,? ,?? = ??
?  and repeat step 2 
until convergence or a preset iteration limit is 
reached. 
 
Figure 1. Model training for co-decoding 
In step 2, there is no global criterion to optim-
ize the co-decoding parameters across member 
models. Instead, parameters of different member 
models are tuned to maximize the evaluation cri-
teria on each member decoder?s own n-best out-
put.  Figure 1 illustrates the training process of 
co-decoding with 2 member decoders. 
Source sentence
decoder
1
decoder
2
?1
MERT
?2
MERT
co-decoding
ref
1?? 2??
588
2.6 Output Selection 
Since there is more than one model in co-
decoding, we cannot rely on member model?s 
score function to choose one best translation 
from multiple decoders? outputs because the 
model scores are not directly comparable. We 
will examine the following two system combina-
tion -based solutions to this task: 
? Word-level system combination (Rosti et al, 
2007) of member decoders? n-best outputs  
? Hypothesis selection from combined n-best 
lists as proposed in Hildebrand  and Vogel 
(2008) 
3 Experiments 
In this section we present experiments to eva-
luate the co-decoding method. We first describe 
the data sets and baseline systems. 
3.1 Data and Metric 
We conduct our experiments on the test data 
from the NIST 2005 and NIST 2008 Chinese-to-
English machine translation tasks. The NIST 
2003 test data is used for development data to 
estimate model parameters. Statistics of the data 
sets are shown in Table 1. In our experiments all 
the models are optimized with case-insensitive 
NIST version of BLEU score and we report re-
sults using this metric in percentage numbers. 
 
Data set # Sentences # Words 
NIST 2003 (dev) 919 23,782 
NIST 2005 (test) 1,082 29,258 
NIST 2008 (test) 1,357 31,592 
Table 1: Data set statistics 
We use the parallel data available for the 
NIST 2008 constrained track of Chinese-to-
English machine translation task as bilingual 
training data, which contains 5.1M sentence 
pairs, 128M Chinese words and 147M English 
words after pre-processing. GIZA++ is used to 
perform word alignment in both directions with 
default settings, and the intersect-diag-grow me-
thod is used to generate symmetric word align-
ment refinement. 
The language model used for all models (in-
clude decoding models and system combination 
models described in Section 2.6) is a 5-gram 
model trained with the English part of bilingual 
data and xinhua portion of LDC English Giga-
word corpus version 3. 
3.2 Member Decoders 
We use three baseline decoders in the experi-
ments. The first one (SYS1) is re-implementation 
of Hiero, a hierarchical phrase-based decoder. 
Phrasal rules are extracted from all bilingual sen-
tence pairs, while rules with variables are ex-
tracted only from selected data sets including 
LDC2003E14, LDC2003E07, LDC2005T06 and 
LDC2005T10, which contain around 350,000 
sentence pairs, 8.8M Chinese words and 10.3M 
English words. The second one (SYS2) is a BTG 
decoder with lexicalized reordering model based 
on maximum entropy principle as proposed by 
Xiong et al (2006). We use all the bilingual data 
to extract phrases up to length 3. The third one 
(SYS3) is a string-to-dependency tree ?based 
decoder as proposed by Shen et al (2008). For 
rule extraction we use the same setting as in 
SYS1. We parsed the language model training 
data with Berkeley parser, and then trained a de-
pendency language model based on the parsing 
output. All baseline decoders are extended with 
n-gram consensus ?based co-decoding features 
to construct member decoders. By default, the 
beam size of 20 is used for all decoders in the 
experiments. We run two iterations of decoding 
for each member decoder, and hold the value of 
?  in Equation 5 as a constant 0.05, which is 
tuned on the test data of NIST 2004 Chinese-to-
English machine translation task. 
3.3 Translation Results 
We first present the overall results of co-
decoding on both test sets using the settings as 
we described. For member decoders, up to 4-
gram agreement and disagreement features are 
used. We also implemented the word-level sys-
tem combination (Rosti et al, 2007) and the hy-
pothesis selection method (Hildebrand and Vogel, 
2008). 20-best translations from all decoders are 
used in the experiments for these two combina-
tion methods. Parameters for both system com-
bination and hypothesis selection are also tuned 
on NIST 2003 test data. The results are shown in 
Table 2. 
 
 NIST 2005 NIST 2008 
SYS1 38.66/40.08 27.67/29.19 
SYS2 38.04/39.93 27.25/29.14 
SYS3 39.50/40.32 28.75/29.68 
Word-level Comb 40.45/40.85 29.52/30.35 
Hypo Selection 40.09/40.50 29.02/29.71 
Table 2: Co-decoding results on test data 
589
In the Table 2, the results of a member decod-
er and its corresponding baseline decoder are 
grouped together with the later one for the mem-
ber decoders. On both test sets, every member 
decoder performs significantly better than its 
baseline decoder (using the method proposed in 
Koehn (2004) for statistical significance test).  
We apply system combination methods to the 
n-best outputs of both baseline decoders and 
member decoders. We notice that we can achieve 
even better performance by applying system 
combination methods to member decoders? n-
best outputs. However, the improvement margins 
are smaller than those of baseline decoders on 
both test sets. This could be the result of less di-
versified outputs from co-decoding than those 
from baseline decoders. In particular, the results 
for hypothesis selection are only slightly better 
than the best system in co-decoding.  
We also evaluate the performance of system 
combination using different n-best sizes, and the 
results on NIST 2005 data set are shown in Fig-
ure 2, where bl- and co- legends denote combina-
tion results of baseline decoding and co-decoding 
respectively. From the results we can see that 
combination based on co-decoding?s outputs per-
forms consistently better than that based on base-
line decoders? outputs for all n-best sizes we ex-
perimented with. However, we did not observe 
any significant improvements for both combina-
tion schemes when n-best size is larger than 20. 
 
Figure 2. Performance of system combination 
with different sizes of n-best lists 
One interesting observation in Table 2 is that 
the performance gap between baseline decoders 
is narrowed through co-decoding. For example, 
the 1.5 points gap between SYS2 and SYS3 on 
NIST 2008 data set is narrowed to 0.5. Actually 
we find that the TER score between two member 
decoders? outputs are significantly reduced (as 
shown in Table 3), which indicates that the out-
puts become more similar due to the use of con-
sensus information. For example, the TER score 
between SYS2 and SYS3 of the NIST 2008 out-
puts are reduced from 0.4238 to 0.2665.  
 
 NIST 2005 NIST 2008 
SYS1 vs. SYS2 0.3190/0.2274 0.4016/0.2686 
SYS1 vs. SYS3 0.3252/0.1840 0.4176/0.2469 
SYS2 vs. SYS3 0.3498/0.2171 0.4238/0.2665 
Table 3: TER scores between co-decoding  
translation outputs 
In the rest of this section we run a series of 
experiments to investigate the impacts of differ-
ent factors in co-decoding. All the results are 
reported on NIST 2005 test set.  
We start with investigating the performance 
gain due to partial hypothesis re-ranking. Be-
cause Equation 3 is a general model that can be 
applied to both partial hypothesis and n-best (full 
hypothesis) re-scoring, we compare the results of 
both cases. Figure 3 shows the BLEU score 
curves with up to 1000 candidates used for re-
ranking. In Figure 3, the suffix p denotes results 
for partial hypothesis re-ranking, and f for n-best 
re-ranking only. For partial hypothesis re-
ranking, obtaining more top-ranked results re-
quires increasing the beam size, which is not af-
fordable for large numbers in experiments. We 
work around this issue by approximating beam 
sizes larger than 20 by only enlarging the beam 
size for the span covering the entire source sen-
tence. From Figure 3 we can see that all decoders 
can gain improvements before the size of candi-
date set reaches 100. When the size is larger than 
50, co-decoding performs consistently and sig-
nificantly better than the re-ranking results on 
any baseline decoder?s n-best outputs.  
 
Figure 3. Partial hypothesis vs. n-best re-ranking 
results on NIST 2005 test data 
Figure 4 shows the BLEU scores of a two-
system co-decoding as a function of re-decoding 
iterations. From the results we can see that the 
results for both decoders converge after two ite-
rations.  
In Figure 4, iteration 0 denotes decoding with 
baseline model. The setting of iteration 1 can be 
viewed as the case of partial co-decoding, in 
39.5
39.8
40.0
40.3
40.5
40.8
41.0
41.3
10 20 50 100 200
bl-comb
co-comb
bl-hyposel
co-hyposel
38.0
38.5
39.0
39.5
40.0
40.5
41.0
41.5
10 20 50 100 200 500 1000
SYS1f
SYS2f
SYS3f
SYS1p
SYS2p
SYS3p
590
which one decoder uses member model and the 
other keeps using baseline model. The results 
show that member models help each other: al-
though improvements can be made using a single 
member model, best BLEU scores can only be 
achieved when both member models are used as 
shown by the results of iteration 2. The results 
also help justify the independent parameter esti-
mation of member decoders described in Section 
2.5, since optimizing the performance of one de-
coder will eventually bring performance im-
provements to all member decoders. 
 
Figure 4. Results using incremental iterations  
in co-decoding 
Next we examine the impacts of different con-
sensus-based features in co-decoding. Table 4 
shows the comparison results of a two-system 
co-decoding using different settings of n-gram 
agreement and disagreement features. It is clear-
ly shown that both n-gram agreement and disa-
greement types of features are helpful, and using 
them together is the best choice. 
 SYS1 SYS2 
Baseline 38.66 38.04 
+agreement ?disagreement 39.36 39.02 
?agreement +disagreement  39.12 38.67 
+agreement +disagreement 39.68 39.61 
Table 4: Co-decoding with/without n-gram 
agreement and disagreement features 
In Table 5 we show in another dimension the 
impact of consensus-based features by restricting 
the maximum order of n-grams used to compute 
agreement statistics. 
 SYS1 SYS2 
1 38.75 38.27 
2  39.21 39.10 
3 39.48 39.25 
4 39.68 39.61 
5 39.52 39.36 
6 39.58 39.47 
Table 5: Co-decoding with varied n-gram agree-
ment and disagreement features 
From the results we do not observe BLEU im-
provement for ? > 4. One reason could be that 
the data sparsity for high-order n-grams leads to 
over fitting on development data. 
We also empirically investigated the impact of 
scaling factor ? in Equation 5. It is observed in 
Figure 5 that the optimal value is between 0.01 ~ 
0.1 on both development and test data.  
 
Figure 5. Impact of scaling factor ?  
4 Discussion 
Word-level system combination (system combi-
nation hereafter) (Rosti et al, 2007; He et al, 
2008) has been proven to be an effective way to 
improve machine translation quality by using 
outputs from multiple systems. Our method is 
different from system combination in several 
ways. System combination uses unigram consen-
sus only and a standalone decoding model irrele-
vant to single decoders. Our method uses agree-
ment information of n-grams, and consensus fea-
tures are integrated into decoding models. By 
constructing a confusion network, system com-
bination is able to generate new translations dif-
ferent from any one in the input n-best lists, 
while our method does not extend the search 
spaces of baseline decoding models. Member 
decoders only change the scoring and ranking of 
the candidates in the search spaces. Results in 
Table 2 show that these two approaches can be 
used together to obtain further improvements. 
The work on multi-system hypothesis selec-
tion of Hildebrand and Vogel (2008) bears more 
resemblance to our method in that both make use 
of n-gram agreement statistics. They also empiri-
cally show that n-gram agreement is the most 
important factor for improvement apart from 
language models.  
Lattice MBR decoding (Tromble et al, 2008) 
also uses n-gram agreement statistics. Their work 
focuses on exploring larger evidence space by 
using a translation lattice instead of the n-best list. 
They also show the connection between expected 
n-gram change and corpus Log-BLEU loss. 
37.5
38.0
38.5
39.0
39.5
40.0
0 1 2 3 4
SYS1
SYS2
38.0
38.5
39.0
39.5
40.0
0 0.01 0.03 0.05 0.1 0.2 0.5 1
Dev SYS1
Dev SYS2
Test SYS1
Test SYS2
591
5 Conclusion 
Improving machine translation with multiple sys-
tems has been a focus in recent SMT research. In 
this paper, we present a framework of collabora-
tive decoding, in which multiple MT decoders 
are coordinated to search for better translations 
by re-ranking partial hypotheses using aug-
mented log-linear models with translation con-
sensus -based features. An iterative approach is 
proposed to re-rank all hypotheses explored in 
decoding. Experimental results show that with 
collaborative decoding every member decoder 
performs significantly better than its baseline 
decoder. In the future, we will extend our method 
to use lattice or hypergraph to compute consen-
sus statistics instead of n-best lists. 
References  
Necip Fazil Ayan,  Jing Zheng, and Wen Wang. 2008. 
Improving alignments for better confusion net-
works for combining machine translation systems. 
In Proc. Coling, pages 33-40. 
Srinivas Bangalore, German Bordel and Giuseppe 
Riccardi. 2001. Computing consensus translation 
from multiple machine translation systems. In 
Proc. ASRU, pages 351-354. 
David Chiang. 2005. A hierarchical phrase-based 
model for statistical machine translation. In Proc. 
ACL, pages 263-270. 
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick 
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis for combining outputs from ma-
chine translation systems.  In Proc. EMNLP, pages 
98-107. 
Almut Silja Hildebrand and Stephan Vogel. 2008. 
Combination of machine translation systems via 
hypothesis selection from combined n-best lists. In 
8th AMTA conference, pages 254-261. 
Philipp Koehn, 2004. Statistical significance tests for 
machine translation evaluation. In Proc. EMNLP. 
Philipp Koehn, 2004. Pharaoh: a beam search decoder 
for phrase-based statistical machine translation 
model. In Proc. 6th AMTA Conference, pages 115-
124. 
Philipp Koehn, Hieu Hoang, Alexandra Brich, Chris 
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran, 
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra 
Constantin, Evan Herbst. 2007. Moses: open 
source toolkit for statistical machine translation. In 
Proc. ACL, demonstration session. 
Shankar Kumar and William Byrne 2004. Minimum 
Bayes-Risk Decoding for Statistical Machine 
Translation. In HLT-NAACL, pages 169-176. 
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine 
translation. In Proc. ACL-Coling, pages 609-616. 
Evgeny Matusov, Nicola Ueffi ng, and Hermann Ney. 
2006. Computing consensus translation from mul-
tiple machine translation systems using enchanced 
hypotheses alignment. In Proc. EACL, pages 33-
40. 
Franz Och and Hermann Ney. 2002. Discriminative 
training and maximum entropy models for statis-
tical machine translation. In Proc. ACL, pages 295-
302. 
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proc. ACL, pages 
160-167. 
Franz Och and Hermann Ney. 2004. The alignment 
template approach to statistical machine transla-
tion. Computational Linguistics, 30(4), pages 417-
449 
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang, 
Spyros Matsoukas, Richard Schwartz, and Bonnie 
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In HLT-NAACL, pages 
228-235 
Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas, 
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with 
application to machine translation system combina-
tion. In Proc. Of the Third ACL Workshop on Sta-
tistical Machine Translation, pages 183-186. 
K.C. Sim, W. Byrne, M. Gales, H. Sahbi, and P. 
Woodland. 2007. Consensus network decoding for 
statistical machine translation system combination. 
In ICASSP. 
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A 
new string-to-dependency machine translation al-
gorithm with a target dependency language model. 
In Proc. HLT-ACL, pages 577-585. 
Roy W. Tromble, Shankar Kumar, Franz Och, and 
Wolfgang Macherey. 2008. Lattice minimum 
bayes-risk decoding for statistical machine transla-
tion. In Proc. EMNLP, pages 620-629. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for 
statistical machine translation. In Proc. ACL, pages 
521-528. 
 
592
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 949?957,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Incremental HMM Alignment for MT System Combination
Chi-Ho Li
Microsoft Research Asia
49 Zhichun Road, Beijing, China
chl@microsoft.com
Xiaodong He
Microsoft Research
One Microsoft Way, Redmond, USA
xiaohe@microsoft.com
Yupeng Liu
Harbin Institute of Technology
92 Xidazhi Street, Harbin, China
ypliu@mtlab.hit.edu.cn
Ning Xi
Nanjing University
8 Hankou Road, Nanjing, China
xin@nlp.nju.edu.cn
Abstract
Inspired by the incremental TER align-
ment, we re-designed the Indirect HMM
(IHMM) alignment, which is one of the
best hypothesis alignment methods for
conventional MT system combination, in
an incremental manner. One crucial prob-
lem of incremental alignment is to align a
hypothesis to a confusion network (CN).
Our incremental IHMM alignment is im-
plemented in three different ways: 1) treat
CN spans as HMM states and define state
transition as distortion over covered n-
grams between two spans; 2) treat CN
spans as HMM states and define state tran-
sition as distortion over words in compo-
nent translations in the CN; and 3) use
a consensus decoding algorithm over one
hypothesis and multiple IHMMs, each of
which corresponds to a component trans-
lation in the CN. All these three ap-
proaches of incremental alignment based
on IHMM are shown to be superior to both
incremental TER alignment and conven-
tional IHMM alignment in the setting of
the Chinese-to-English track of the 2008
NIST Open MT evaluation.
1 Introduction
Word-level combination using confusion network
(Matusov et al (2006) and Rosti et al (2007)) is a
widely adopted approach for combining Machine
Translation (MT) systems? output. Word align-
ment between a backbone (or skeleton) translation
and a hypothesis translation is a key problem in
this approach. Translation Edit Rate (TER, Snover
et al (2006)) based alignment proposed in Sim
et al (2007) is often taken as the baseline, and
a couple of other approaches, such as the Indi-
rect Hidden Markov Model (IHMM, He et al
(2008)) and the ITG-based alignment (Karakos et
al. (2008)), were recently proposed with better re-
sults reported. With an alignment method, each
hypothesis is aligned against the backbone and all
the alignments are then used to build a confusion
network (CN) for generating a better translation.
However, as pointed out by Rosti et al (2008),
such a pair-wise alignment strategy will produce
a low-quality CN if there are errors in the align-
ment of any of the hypotheses, no matter how good
the alignments of other hypotheses are. For ex-
ample, suppose we have the backbone ?he buys a
computer? and two hypotheses ?he bought a lap-
top computer? and ?he buys a laptop?. It will be
natural for most alignment methods to produce the
alignments in Figure 1a. The alignment of hypoth-
esis 2 against the backbone cannot be considered
an error if we consider only these two translations;
nevertheless, when added with the alignment of
another hypothesis, it produces the low-quality
CN in Figure 1b, which may generate poor trans-
lations like ?he bought a laptop laptop?. While it
could be argued that such poor translations are un-
likely to be selected due to language model, this
CN does disperse the votes to the word ?laptop? to
two distinct arcs.
Rosti et al (2008) showed that this problem can
be rectified by incremental alignment. If hypoth-
esis 1 is first aligned against the backbone, the
CN thus produced (depicted in Figure 2a) is then
aligned to hypothesis 2, giving rise to the good CN
as depicted in Figure 2b.1 On the other hand, the
1Note that this CN may generate an incomplete sentence
?he bought a?, which is nevertheless unlikely to be selected
as it leads to low language model score.
949
Figure 1: An example bad confusion network due
to pair-wise alignment strategy
correct result depends on the order of hypotheses.
If hypothesis 2 is aligned before hypothesis 1, the
final CN will not be good. Therefore, the obser-
vation in Rosti et al (2008) that different order
of hypotheses does not affect translation quality is
counter-intuitive.
This paper attempts to answer two questions: 1)
as incremental TER alignment gives better perfor-
mance than pair-wise TER alignment, would the
incremental strategy still be better than the pair-
wise strategy if the TER method is replaced by
another alignment method? 2) how does transla-
tion quality vary for different orders of hypotheses
being incrementally added into a CN? For ques-
tion 1, we will focus on the IHMM alignment
method and propose three different ways of imple-
menting incremental IHMM alignment. Our ex-
periments will also try several orders of hypothe-
ses in response to question 2.
This paper is structured as follows. After set-
ting the notations on CN in section 2, we will
first introduce, in section 3, two variations of the
basic incremental IHMM model (IncIHMM1 and
IncIHMM2). In section 4, a consensus decoding
algorithm (CD-IHMM) is proposed as an alterna-
tive way to search for the optimal alignment. The
issues of alignment normalization and the order of
hypotheses being added into a CN are discussed in
sections 5 and 6 respectively. Experiment results
and analysis are presented in section 7.
Figure 2: An example good confusion network
due to incremental alignment strategy
2 Preliminaries: Notation on Confusion
Network
Before the elaboration of the models, let us first
clarify the notation on CN. A CN is usually de-
scribed as a finite state graph with many spans.
Each span corresponds to a word position and con-
tains several arcs, each of which represents an al-
ternative word (could be the empty symbol , ?) at
that position. Each arc is also associated with M
weights in an M -way system combination task.
Follow Rosti et al (2007), the i-th weight of an
arc is ?r 11+r , where r is the rank of the hypothe-
sis in the i-th system that votes for the word repre-
sented by the arc. This conception of CN is called
the conventional or compact form of CN. The net-
works in Figures 1b and 2b are examples.
On the other hand, as a CN is an integration
of the skeleton and all hypotheses, it can be con-
ceived as a list of the component translations. For
example, the CN in Figure 2b can be converted
to the form in Figure 3. In such an expanded or
tabular form, each row represents a component
translation. Each column, which is equivalent to
a span in the compact form, comprises the alter-
native words at a word position. Thus each cell
represents an alternative word at certain word po-
sition voted by certain translation. Each row is as-
signed the weight 11+r , where r is the rank of the
translation of some MT system. It is assumed that
all MT systems are weighted equally and thus the
950
Figure 3: An example of confusion network in tab-
ular form
rank-based weights from different system can be
compared to each other without adjustment. The
weight of a cell is the same as the weight of the
corresponding row. In this paper the elaboration
of the incremental IHMM models is based on such
tabular form of CN.
Let EI1 = (E1 . . . EI) denote the backbone CN,
and e?J1 = (e?1 . . . e?J) denote a hypothesis being
aligned to the backbone. Each e?j is simply a word
in the target language. However, each Ei is a span,
or a column, of the CN. We will also use E(k) to
denote the k-th row of the tabular form CN, and
Ei(k) to denote the cell at the k-th row and the
i-th column. W (k) is the weight for E(k), and
Wi(k) = W (k) is the weight for Ei(k). pi(k)
is the normalized weight for the cell Ei(k), such
that pi(k) = Wi(k)?
i Wi(k)
. Note that E(k) contains
the same bag-of-words as the k-th original trans-
lation, but may have different word order. Note
also that E(k) represents a word sequence with
inserted empty symbols; the sequence with all in-
serted symbols removed is known as the compact
form of E(k).
3 The Basic IncIHMM Model
A na??ve application of the incremental strategy to
IHMM is to treat a span in the CN as an HMM
state. Like He et al (2008), the conditional prob-
ability of the hypothesis given the backbone CN
can be decomposed into similarity model and dis-
tortion model in accordance with equation 1
p(e?J1 |EI1) =
?
aJ1
J?
j=1
[p(aj |aj?1, I)p(e?j |eaj )] (1)
The similarity between a hypothesis word e?j and
a span Ei is simply a weighted sum of the similar-
ities between e?j and each word contained in Ei as
equation 2:
p(e?j |Ei) =
?
Ei(k)?Ei
pi(k) ? p(e?j |Ei(k)) (2)
The similarity between two words is estimated in
exactly the same way as in conventional IHMM
alignment.
As to the distortion model, the incremental
IHMM model also groups distortion parameters
into a few ?buckets?:
c(d) = (1 + |d? 1|)?K
The problem in incremental IHMM is when to ap-
ply a bucket. In conventional IHMM, the transi-
tion from state i to j has probability:
p?(j|i, I) = c(j ? i)?I
l=1 c(l ? i)
(3)
It is tempting to apply the same formula to the
transitions in incremental IHMM. However, the
backbone in the incremental IHMM has a special
property that it is gradually expanding due to the
insertion operator. For example, initially the back-
bone CN contains the option ei in the i-th span and
the option ei+1 in the (i+1)-th span. After the first
round alignment, perhaps ei is aligned to the hy-
pothesis word e?j , ei+1 to e?j+2, and the hypothesis
word e?j+1 is left unaligned. Then the consequent
CN have an extra span containing the option e?j+1
inserted between the i-th and (i + 1)-th spans of
the initial CN. If the distortion buckets are applied
as in equation 3, then in the first round alignment,
the transition from the span containing ei to that
containing ei+1 is based on the bucket c(1), but
in the second round alignment, the same transition
will be based on the bucket c(2). It is therefore not
reasonable to apply equation 3 to such gradually
extending backbone as the monotonic alignment
assumption behind the equation no longer holds.
There are two possible ways to tackle this prob-
lem. The first solution estimates the transition
probability as a weighted average of different dis-
tortion probabilities, whereas the second solution
converts the distortion over spans to the distortion
over the words in each hypothesis E(k) in the CN.
3.1 Distortion Model 1: simple weighting of
covered n-grams
Distortion Model 1 shifts the monotonic alignment
assumption from spans of CN to n-grams covered
by state transitions. Let us illustrate this point with
the following examples.
In conventional IHMM, the distortion probabil-
ity p?(i + 1|i, I) is applied to the transition from
state i to i+1 given I states because such transition
951
jumps across only one word, viz. the i-th word of
the backbone. In incremental IHMM, suppose the
i-th span covers two arcs ea and ?, with probabili-
ties p1 and p2 = 1? p1 respectively, then the tran-
sition from state i to i+ 1 jumps across one word
(ea) with probability p1 and jumps across nothing
with probability p2. Thus the transition probabil-
ity should be p1 ? p?(i+ 1|i, I) + p2 ? p?(i|i, I).
Suppose further that the (i + 1)-th span covers
two arcs eb and ?, with probabilities p3 and p4 re-
spectively, then the transition from state i to i+ 2
covers 4 possible cases:
1. nothing (??) with probability p2 ? p4;
2. the unigram ea with probability p1 ? p4;
3. the unigram eb with probability p2 ? p3;
4. the bigram eaeb with probability p1 ? p3.
Accordingly the transition probability should be
p2p4p?(i|i, I) + p1p3p?(i+ 2|i, I) +
(p1p4 + p2p3)p?(i+ 1|i, I).
The estimation of transition probability can be
generalized to any transition from i to i? by ex-
panding all possible n-grams covered by the tran-
sition and calculating the corresponding probabil-
ities. We enumerate all possible cell sequences
S(i, i?) covered by the transition from span i to
i?; each sequence is assigned the probability
P i?i =
i??1?
q=i
pq(k).
where the cell at the i?-th span is on some row
E(k). Since a cell may represent an empty word,
a cell sequence may represent an n-gram where
0 ? n ? i? ? i (or 0 ? n ? i ? i? in backward
transition). We denote |S(i, i?)| to be the length of
n-gram represented by a particular cell sequence
S(i, i?). All the cell sequences S(i, i?) can be clas-
sified, with respect to the length of corresponding
n-grams, into a set of parameters where each ele-
ment (with a particular value of n) has the proba-
bility
P i?i (n; I) =
?
|S(i,i?)|=n
P i?i .
The probability of the transition from i to i? is:
p(i?|i, I) =
?
n
[P i?i (n; I) ? p?(i+ n|i, I)]. (4)
That is, the transition probability of incremental
IHMM is a weighted sum of probabilities of ?n-
gram jumping?, defined as conventional IHMM
distortion probabilities.
However, in practice it is not feasible to ex-
pand all possible n-grams covered by any transi-
tion since the number of n-grams grows exponen-
tially. Therefore a length limit L is imposed such
that for all state transitions where |i? ? i| ? L, the
transition probability is calculated as equation 4,
otherwise it is calculated by:
p(i?|i, I) = maxq p(i
?|q, I) ? p(q|i, I)
for some q between i and i?. In other words, the
probability of longer state transition is estimated
in terms of the probabilities of transitions shorter
or equal to the length limit.2 All the state transi-
tions can be calculated efficiently by dynamic pro-
gramming.
A fixed value P0 is assigned to transitions to
null state, which can be optimized on held-out
data. The overall distortion model is:
p?(j|i, I) =
{
P0 if j is null state
(1? P0)p(j|i, I) otherwise
3.2 Distortion Model 2: weighting of
distortions of component translations
The cause of the problem of distortion over CN
spans is the gradual extension of CN due to the
inserted empty words. Therefore, the problem
will disappear if the inserted empty words are re-
moved. The rationale of Distortion Model 2 is
that the distortion model is defined over the ac-
tual word sequence in each component translation
E(k).
Distortion Model 2 implements a CN in such a
way that the real position of the i-th word of the k-
th component translation can always be retrieved.
The real position of Ei(k), ?(i, k), refers to the
position of the word represented by Ei(k) in the
compact form of E(k) (i.e. the form without any
inserted empty words), or, if Ei(k) represents an
empty word, the position of the nearest preceding
non-empty word. For convenience, we also denote
by ??(i, k) the null state associated with the state
of the real word ?(i, k). Similarly, the real length
2This limit L is also imposed on the parameter I in distor-
tion probability p?(i?|i, I), because the value of I is growing
larger and larger during the incremental alignment process. I
is defined as L if I > L.
952
of E(k), L(k), refers to the number of non-empty
words of E(k).
The transition from span i? to i is then defined
as
p(i|i?) = 1?
k W (k)
?
k
[W (k) ? pk(i|i?)] (5)
where k is the row index of the tabular form CN.
Depending on Ei(k) and Ei?(k), pk(i|i?) is
computed as follows:
1. if both Ei(k) and Ei?(k) represent real
words, then
pk(i|i?) = p?(?(i, k)|?(i?, k), L(k))
where p? refers to the conventional IHMM
distortion probability as defined by equa-
tion 3.
2. if Ei(k) represents a real word but Ei?(k) the
empty word, then
pk(i|i?) = p?(?(i, k)|??(i?, k), L(k))
Like conventional HMM-based word align-
ment, the probability of the transition from a
null state to a real word state is the same as
that of the transition from the real word state
associated with that null state to the other real
word state. Therefore,
p?(?(i, k)|??(i?, k), L(k)) =
p?(?(i, k)|?(i?, k), L(k))
3. if Ei(k) represents the empty word but
Ei?(k) a real word, then
pk(i|i?) =
{
P0 if?(i, k) = ?(i?, k)
P0P?(i|i?; k) otherwise
where P?(i|i?; k) = p?(?(i, k)|?(i?, k), L(k)).
The second option is due to the constraint that
a null state is accessible only to itself or the
real word state associated with it. Therefore,
the transition from i? to i is in fact composed
of the first transition from i? to ?(i, k) and the
second transition from ?(i, k) to the null state
at i.
4. if both Ei(k) and Ei?(k) represent the empty
word, then, with similar logic as cases 2
and 3,
pk(i|i?) =
{
P0 if?(i, k) = ?(i?, k)
P0P?(i|i?; k) otherwise
4 Incremental Alignment using
Consensus Decoding over Multiple
IHMMs
The previous section describes an incremental
IHMM model in which the state space is based on
the CN taken as a whole. An alternative approach
is to conceive the rows (component translations)
in the CN as individuals, and transforms the align-
ment of a hypothesis against an entire network to
that against the individual translations. Each in-
dividual translation constitutes an IHMM and the
optimal alignment is obtained from consensus de-
coding over these multiple IHMMs.
Alignment over multiple sequential patterns has
been investigated in different contexts. For ex-
ample, Nair and Sreenivas (2007) proposed multi-
pattern dynamic time warping (MPDTW) to align
multiple speech utterances to each other. How-
ever, these methods usually assume that the align-
ment is monotonic. In this section, a consensus
decoding algorithm that searches for the optimal
(non-monotonic) alignment between a hypothesis
and a set of translations in a CN (which are already
aligned to each other) is developed as follows.
A prerequisite of the algorithm is a function
for converting a span index to the corresponding
HMM state index of a component translation. The
two functions ? and ?? s defined in section 3.2 are
used to define a new function:
??(i, k) =
{
??(i, k) if Ei(k) is null
?(i, k) otherwise
Accordingly, given the alignment aJ1 = a1 . . . aJ
of a hypothesis (with J words) against a CN
(where each aj is an index referring to the span
of the CN), we can obtain the alignment a?k =
??(a1, k) . . . ??(aJ , k) between the hypothesis and
the k-th row of the tabular CN. The real length
function L(k) is also used to obtain the number of
non-empty words of E(k).
Given the k-th row of a CN, E(k), an IHMM
?(k) is formed and the cost of the pair-wise align-
ment, a?k, between a hypothesis h and ?(k) is de-
fined as:
C(a?k;h, ?(k)) = ? logP (a?k|h, ?(k)) (6)
The cost of the alignment of h against a CN is then
defined as the weighted sum of the costs of the K
alignments a?k:
C(a;h,?) =
?
k
W (k)C(a?k;h, ?(k))
953
= ?
?
k
W (k) logP (a?k|h, ?(k))
where ? = {?(k)} is the set of pair-wise IHMMs,
and W (k) is the weight of the k-th row. The op-
timal alignment a? is the one that minimizes this
cost:
a? = argmaxa
?
k
W (k) logP (a?k|h, ?(k))
= argmaxa
?
k
W (k)[
?
j
[
logP (??(aj , k)|??(aj?1, k), L(k)) +
logP (ej |Ei(k))]]
= argmaxa
?
j
[
?
k
W (k) logP (??(aj , k)|??(aj?1, k), L(k)) +
?
k
W (k) logP (ej |Ei(k))]
= argmaxa
?
j
[logP ?(aj |aj?1) +
logP ?(ej |Eaj )]
A Viterbi-like dynamic programming algorithm
can be developed to search for a? by treating CN
spans as HMM states, with a pseudo emission
probability as
P ?(ej |Eaj ) =
K?
k=1
P (ej |Eaj (k))W (k)
and a pseudo transition probability as
P ?(j|i) =
K?
k=1
P (??(j, k)|??(i, k), L(k))W (k)
Note that P ?(ej |Eaj ) and P ?(j|i) are not true
probabilities and do not have the sum-to-one prop-
erty.
5 Alignment Normalization
After alignment, the backbone CN and the hypoth-
esis can be combined to form an even larger CN.
The same principles and heuristics for the con-
struction of CN in conventional system combina-
tion approaches can be applied. Our incremen-
tal alignment approaches adopt the same heuris-
tics for alignment normalization stated in He et al
(2008). There is one exception, though. All 1-
N mappings are not converted to N ? 1 ?-1 map-
pings since this conversion leads to N ? 1 inser-
tion in the CN and therefore extending the net-
work to an unreasonable length. The Viterbi align-
ment is abandoned if it contains an 1-N mapping.
The best alignment which contains no 1-N map-
ping is searched in the N-Best alignments in a way
inspired by Nilsson and Goldberger (2001). For
example, if both hypothesis words e?1 and e?2 are
aligned to the same backbone span E1, then all
alignments aj={1,2} = i (where i 6= 1) will be
examined. The alignment leading to the least re-
duction of Viterbi probability when replacing the
alignment aj={1,2} = 1 will be selected.
6 Order of Hypotheses
The default order of hypotheses in Rosti et al
(2008) is to rank the hypotheses in descending of
their TER scores against the backbone. This pa-
per attempts several other orders. The first one is
system-based order, i.e. assume an arbitrary order
of the MT systems and feeds all the translations
(in their original order) from a system before the
translations from the next system. The rationale
behind the system-based order is that the transla-
tions from the same system are much more similar
to each other than to the translations from other
systems, and it might be better to build CN by
incorporating similar translations first. The sec-
ond one is N-best rank-based order, which means,
rather than keeping the translations from the same
system as a block, we feed the top-1 translations
from all systems in some order of systems, and
then the second best translations from all systems,
and so on. The presumption of the rank-based or-
der is that top-ranked hypotheses are more reliable
and it seemed beneficial to incorporate more reli-
able hypotheses as early as possible. These two
kinds of order of hypotheses involve a certain de-
gree of randomness as the order of systems is arbi-
trary. Such randomness can be removed by impos-
ing a Bayes Risk order on MT systems, i.e. arrange
the MT systems in ascending order of the Bayes
Risk of their top-1 translations. These four orders
of hypotheses are summarized in Table 1. We also
tried some intuitively bad orders of hypotheses, in-
cluding the reversal of these four orders and the
random order.
7 Evaluation
The proposed approaches of incremental IHMM
are evaluated with respect to the constrained
Chinese-to-English track of 2008 NIST Open MT
954
Order Example
System-based 1:1 . . . 1:N 2:1 . . . 2:N . . . M:1 . . . M:N
N-best Rank-based 1:1 2:1 . . . M:1 . . . 1:2 2:2 . . . M:2 . . . 1:N . . . M:N
Bayes Risk + System-based 4:1 4:2 . . . 4:N . . . 1:1 1:2 . . . 1:N . . . 5:1 5:2 . . . 5:N
Bayes Risk + Rank-based 4:1 . . . 1:1 . . . 5:1 4:2 . . . 1:2 . . . 5:2 . . . 4:N . . . 1:N . . . 5:N
Table 1: The list of order of hypothesis and examples. Note that ?m:n? refers to the n-th translation from
the m-th system.
Evaluation (NIST (2008)). In the following sec-
tions, the incremental IHMM approaches using
distortion model 1 and 2 are named as IncIHMM1
and IncIHMM2 respectively, and the consensus
decoding of multiple IHMMs as CD-IHMM. The
baselines include the TER-based method in Rosti
et al (2007), the incremental TER method in Rosti
et al (2008), and the IHMM approach in He et
al. (2008). The development (dev) set comprises
the newswire and newsgroup sections of MT06,
whereas the test set is the entire MT08. The 10-
best translations for every source sentence in the
dev and test sets are collected from eight MT sys-
tems. Case-insensitive BLEU-4, presented in per-
centage, is used as evaluation metric.
The various parameters in the IHMM model are
set as the optimal values found in He et al (2008).
The lexical translation probabilities used in the
semantic similarity model are estimated from a
small portion (FBIS + GALE) of the constrained
track training data, using standard HMM align-
ment model (Och and Ney (2003)). The back-
bone of CN is selected by MBR. The loss function
used for TER-based approaches is TER and that
for IHMM-based approaches is BLEU. As to the
incremental systems, the default order of hypothe-
ses is the ascending order of TER score against the
backbone, which is the order proposed in Rosti
et al (2008). The default order of hypotheses
for our three incremental IHMM approaches is
N-best rank order with Bayes Risk system order,
which is empirically found to be giving the high-
est BLEU score. Once the CN is built, the final
system combination output can be obtained by de-
coding it with a set of features and decoding pa-
rameters. The features we used include word con-
fidences, language model score, word penalty and
empty word penalty. The decoding parameters are
trained by maximum BLEU training on the dev
set. The training and decoding processes are the
same as described by Rosti et al (2007).
Method dev test
best single system 32.60 27.75
pair-wise TER 37.90 30.96
incremental TER 38.10 31.23
pair-wise IHMM 38.52 31.65
incremental IHMM 39.22 32.63
Table 2: Comparison between IncIHMM2 and the
three baselines
7.1 Comparison against Baselines
Table 2 lists the BLEU scores achieved by
the three baseline combination methods and
IncIHMM2. The comparison between pairwise
and incremental TER methods justifies the supe-
riority of the incremental strategy. However, the
benefit of incremental TER over pair-wise TER is
smaller than that mentioned in Rosti et al (2008),
which may be because of the difference between
test sets and other experimental conditions. The
comparison between the two pair-wise alignment
methods shows that IHMM gives a 0.7 BLEU
point gain over TER, which is a bit smaller than
the difference reported in He et al (2008). The
possible causes of such discrepancy include the
different dev set and the smaller training set for
estimating semantic similarity parameters. De-
spite that, the pair-wise IHMM method is still a
strong baseline. Table 2 also shows the perfor-
mance of IncIHMM2, our best incremental IHMM
approach. It is almost one BLEU point higher than
the pair-wise IHMM baseline and much higher
than the two TER baselines.
7.2 Comparison among the Incremental
IHMM Models
Table 3 lists the BLEU scores achieved by
the three incremental IHMM approaches. The
two distortion models for IncIHMM approach
lead to almost the same performance, whereas
CD-IHMM is much less satisfactory.
For IncIHMM, the gist of both distortion mod-
955
Method dev test
IncIHMM1 39.06 32.60
IncIHMM2 39.22 32.63
CD-IHMM 38.64 31.87
Table 3: Comparison between the three incremen-
tal IHMM approaches
els is to shift the distortion over spans to the dis-
tortion over word sequences. In distortion model 2
the word sequences are those sequences available
in one of the component translations in the CN.
Distortion model 1 is more encompassing as it also
considers the word sequences which are combined
from subsequences from various component trans-
lations. However, as mentioned in section 3.1,
the number of sequences grows exponentially and
there is therefore a limit L to the length of se-
quences. In general the limit L ? 8 would ren-
der the tuning/decoding process intolerably slow.
We tried the values 5 to 8 for L and the variation
of performance is less than 0.1 BLEU point. That
is, distortion model 1 cannot be improved by tun-
ing L. The similar BLEU scores as shown in Ta-
ble 3 implies that the incorporation of more word
sequences in distortion model 1 does not lead to
extra improvement.
Although consensus decoding is conceptually
different from both variations of IncIHMM, it
can indeed be transformed into a form similar to
IncIHMM2. IncIHMM2 calculates the parameters
of the IHMM as a weighted sum of various proba-
bilities of the component translations. In contrast,
the equations in section 4 shows that CD-IHMM
calculates the weighted sum of the logarithm of
those probabilities of the component translations.
In other words, IncIHMM2 makes use of the sum
of probabilities whereas CD-IHMM makes use
of the product of probabilities. The experiment
results indicate that the interaction between the
weights and the probabilities is more fragile in the
product case than in the summation case.
7.3 Impact of Order of Hypotheses
Table 4 lists the BLEU scores on the test set
achieved by IncIHMM1 using different orders of
hypotheses. The column ?reversal? shows the im-
pact of deliberately bad order, viz. more than one
BLEU point lower than the best order. The ran-
dom order is a baseline for not caring about or-
der of hypotheses at all, which is about 0.7 BLEU
normal reversal
System 32.36 31.46
Rank 32.53 31.56
BR+System 32.37 31.44
BR+Rank 32.6 31.47
random 31.94
Table 4: Comparison between various orders of
hypotheses. ?System? means system-based or-
der; ?Rank? means N-best rank-based order; ?BR?
means Bayes Risk order of systems. The numbers
are the BLEU scores on the test set.
point lower than the best order. Among the orders
with good performance, it is observed that N-best
rank order leads to about 0.2 to 0.3 BLEU point
improvement, and that the Bayes Risk order of
systems does not improve performance very much.
In sum, the performance of incremental alignment
is sensitive to the order of hypotheses, and the op-
timal order is defined in terms of the rank of each
hypothesis on some system?s n-best list.
8 Conclusions
This paper investigates the application of the in-
cremental strategy to IHMM, one of the state-of-
the-art alignment methods for MT output com-
bination. Such a task is subject to the prob-
lem of how to define state transitions on a grad-
ually expanding CN. We proposed three differ-
ent solutions, which share the principle that tran-
sition over CN spans must be converted to the
transition over word sequences provided by the
component translations. While the consensus de-
coding approach does not improve performance
much, the two distortion models for incremental
IHMM (IncIHMM1 and IncIHMM2) give superb
performance in comparison with pair-wise TER,
pair-wise IHMM, and incremental TER. We also
showed that the order of hypotheses is important
as a deliberately bad order would reduce transla-
tion quality by one BLEU point.
References
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore 2008. Indirect-HMM-
based Hypothesis Alignment for Combining Out-
puts from Machine Translation Systems. Proceed-
ings of EMNLP 2008.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer 2008. Machine Translation
956
System Combination using ITG-based Alignments.
Proceedings of ACL 2008.
Evgeny Matusov, Nicola Ueffing and Hermann Ney.
2006. Computing Consensus Translation from Mul-
tiple Machine Translation Systems using Enhanced
Hypothesis Alignment. Proceedings of EACL.
Nishanth Ulhas Nair and T.V. Sreenivas. 2007. Joint
Decoding of Multiple Speech Patterns for Robust
Speech Recognition. Proceedings of ASRU.
Dennis Nilsson and Jacob Goldberger 2001. Sequen-
tially Finding the N-Best List in Hidden Markov
Models. Proceedings of IJCAI 2001.
NIST 2008. The NIST Open Machine
Translation Evaluation. www.nist.gov/
speech/tests/mt/2008/doc/
Franz J. Och and Hermann Ney 2003. A Systematic
Comparison of Various Statistical Alignment Mod-
els. Computational Linguistics 29(1):pp 19-51
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
Jing Zhu 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. Proceedings of
ACL 2002
Antti-Veikko I. Rosti, Spyros Matsoukas, and Richard
Schwartz 2007. Improved Word-level System Com-
bination for Machine Translation. Proceedings of
ACL 2007.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz 2008. Incremental Hypoth-
esis Alignment for Building Confusion Networks
with Application to Machine Translation System
Combination. Proceedings of the 3rd ACL Work-
shop on SMT.
Khe Chai Sim, William J. Byrne, Mark J.F. Gales,
Hichem Sahbi, and Phil C. Woodland 2007. Con-
sensus Network Decoding for Statistical Machine
Translation System Combination. Proceedings of
ICASSP vol. 4.
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea
Micciulla and John Makhoul 2006. A Study of
Translation Edit Rate with Targeted Human Anno-
tation. Proceedings of AMTA 2006
957
Proceedings of the Third Workshop on Statistical Machine Translation, pages 1?8,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
An Empirical Study in Source Word Deletion
for Phrase-based Statistical Machine Translation
Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou
Microsoft Research Asia
Beijing, China
chl, dozhang@microsoft.com
muli, mingzhou@microsoft.com
Hailei Zhang
Northeastern University of China
Shenyang, China
hailei.zh@gmail.com
Abstract
The treatment of ?spurious? words of source
language is an important problem but often
ignored in the discussion on phrase-based
SMT. This paper explains why it is impor-
tant and why it is not a trivial problem, and
proposes three models to handle spurious
source words. Experiments show that any
source word deletion model can improve a
phrase-based system by at least 1.6 BLEU
points and the most sophisticated model
improves by nearly 2 BLEU points. This
paper also explores the impact of training
data size and training data domain/genre on
source word deletion.
1 Introduction
It is widely known that translation is by no
means word-to-word conversion. Not only be-
cause sometimes a word in some language trans-
lates as more than one word in another language,
also every language has some ?spurious? words
which do not have any counterpart in other lan-
guages. Consequently, an MT system should be
able to identify the spurious words of the source
language and not translate them, as well as to gen-
erate the spurious words of the target language.
This paper focuses on the first task and studies
how it can be handled in phrase-based SMT.
An immediate reaction to the proposal of inves-
tigating source word deletion (henceforth SWD)
is: Is SWD itself worth our attention? Isn?t it a
trivial task that can be handled easily by existing
techniques? One of the reasons why we need to
pay attention to SWD is its significant improve-
ment to translation performance, which will be
shown by the experiments results in section 4.2.
Another reason is that SWD is not a trivial task.
While some researchers think that the spurious
words of a language are merely function words
or grammatical particles, which can be handled
by some simple heuristics or statistical means,
there are in fact some tricky cases of SWD which
need sophisticated solution. Consider the follow-
ing example in Chinese-to-English translation: in
English we have the subordinate clause ?accord-
ing to NP?, where NP refers to some source of
information. The Chinese equivalent of this
clause can sometimes be ?ACCORDING-TO/??
NP EXPRESS/,+?; that is, in Chinese we could
have a clause rather than a noun phrase following
the preposition ACCORDING-TO/??. There-
fore, when translating Chinese into English, the
content word EXPRESS/,+ should be consid-
ered spurious and not to be translated. Of course,
the verb EXPRESS/,+ is not spurious in other
contexts. It is an example that SWD is not only
about a few function words, and that the solu-
tion to SWD has to take context-sensitive factors
into account. Moreover, the solution needed for
such tricky cases seems to be beyond the scope
of current phrase-based SMT, unless we have a
very large amount of training data which cov-
ers all possible variations of the Chinese pattern
?ACCORDING-TO/?? NP EXPRESS/,+?.
Despite the obvious need for handling spuri-
ous source words, it is surprising that phrase-
based SMT, which is a major approach to SMT,
does not well address the problem. There are
two possible ways for a phrase-based system to
deal with SWD. The first one is to allow a source
1
language phrase to translate to nothing. How-
ever, no existing literature has mentioned such
a possibility and discussed the modifications re-
quired by such an extension. The second way is
to capture SWD within the phrase pairs in trans-
lation table. That is, suppose there is a foreign
phrase F? = (fAfBfC) and an English phrase
E? = (eAeC), where fA is aligned to eA and fC
to eC , then the phrase pair (F? , E?) tacitly deletes
the spurious word fB . Such a SWD mechanism
fails when data sparseness becomes a problem. If
the training data does not have any word sequence
containing fB , then the spurious fB cannot asso-
ciate with other words to form a phrase pair, and
therefore cannot be deleted tacitly in some phrase
pair. Rather, the decoder can only give a phrase
segmentation that treats fB itself as a phrase, and
this phrase cannot translate into nothing, as far
as the SMT training and decoding procedure re-
ported by existing literature are used. In sum, the
current mechanism of phrase-based SMT is not
capable of handling all cases of SWD.
In this paper, we will present, in section 3, three
SWD models and elaborate how to apply each
of them to phrase-based SMT. Experiment set-
tings are described in section 4.1, followed by the
report and analysis of experiment results, using
BLEU as evaluation metric, in section 4.2, which
also discusses the impact of training data size and
training data domain on SWD models. Before
making our conclusions, the effect of SWD on an-
other evaluation metric, viz. METEOR, is exam-
ined in section 5.
2 Literature Review
Research work in SMT seldom treats SWD as
a problem separated from other factors in trans-
lation. However, it can be found in differ-
ent SMT paradigms the mechanism of handling
SWD. As to the pioneering IBM word-based
SMT models (Brown et al, 1990), IBM mod-
els 3, 4 and 5 handle spurious source words by
considering them as corresponding to a particular
EMPTY word token on the English side, and by the
fertility model which allows the English EMPTY
to generate a certain number of foreign words.
As to the hierarchical phrase-based ap-
proach (Chiang, 2007), its hierarchical rules are
more powerful in SWD than the phrase pairs
in conventional phrase-based approach. For
instance, the ?ACCORDING-TO/?? NP EX-
PRESS/,+? example in the last section can be
handled easily by the hierarchical rule
X ?<?? X,+, according to X > .
In general, if the deletion of a source word
depends on some context cues, then the hier-
archical approach is, at least in principle, ca-
pable of handling it correctly. However, it is
still confronted by the same problem as the con-
ventional phrase-based approach regarding those
words whose ?spuriousness? does not depend on
any context.
3 Source Word Deletion Models
This section presents a number of solutions to the
problem of SWD. These solutions share the same
property that a specific empty symbol ? on the tar-
get language side is posited and any source word
is allowed to translate into ?. This symbol is in-
visible in every module of the decoder except the
translation model. That is, ? is not counted when
calculating language model score, word penalty
and any other feature values, and it is omitted in
the final output of the decoder. It is only used to
delete spurious source words and refine transla-
tion model scores accordingly.
It must be noted that in our approach phrases
comprising more than one source word are not al-
lowed to translate into ?. This constraint is based
on our subjective evaluation of alignment matrix,
which indicates that the un-alignment of a con-
tinuous sequence of two or more source words is
far less accurate than the un-alignment of a sin-
gle source word lying within aligned neighbors.
Consequently, in order to treat a source word as
spurious, the decoder must give a phrase segmen-
tation that treats the word itself as a phrase.
Another important modification to the phrase-
based architecture is a new feature added to the
log-linear model. The new feature, ?-penalty, rep-
resents how many source words translate into ?.
The purpose of this feature is the same as that
of the feature of word penalty. As many features
used in the log-linear model have values of log-
arithm of probability, candidate translations with
more words have, in general, lower scores, and
2
Model 1 P (?)
Model 2 P (?|f)
Model 3 PCRF (?|~F (f)
Table 1: Summary of the Three SWD Models
therefore the decoder has a bias towards shorter
translations. Word penalty (in fact, it should be
renamed as word reward) is used to neutralize
this bias. Similarly, the more source words trans-
late into ?, the shorter the translation will be,
and therefore the higher score the translation will
have. The ?-penalty is proposed to neutralize the
bias towards shorter translations.
The core of the solutions is the SWD model,
which calculates P (?|f), the probability distribu-
tion of translating some source word f to ?. Three
SWD models will be elaborated in the following
subsections. They differ from each other by the
conditions of the probability distribution, as sum-
marized in Table 1. Model 1 is a uniform prob-
ability distribution that does not take the source
word f into account. Model 2 is a simple proba-
bility distribution conditioned on the lexical form
of f only. Model 3 is a more complicated distribu-
tion conditioned on a feature vector of f , and the
distribution is estimated by the method of Condi-
tional Random Field.
3.1 Model 1: Uniform Probability
The first model assumes a uniform probability
of translation to ?. This model is inspired by
the HMM-based alignment model (Och and Ney,
2000a), which posits a probability P0 for align-
ment of some source word to the empty word
on the target language side, and weighs all other
alignment probabilities by the factor 1 ? P0. In
the same style, SWD model 1 posits a probability
P (?) for the translation of any source word to ?.
The probabilities of normal phrase pairs should
be weighed accordingly. For a source phrase
containing only one word, its weight is simply
P (??) = 1 ? P (?). As to a source phrase con-
taining more than one word, it implies that every
word in the phrase does not translate into ?, and
therefore the weighing factor P (??) should be mul-
tiplied as many times as the number of words in
the source phrase. In sum, for any phrase pair
< F? , E? >, its probability is
P (E?|F? ) =
{
P (?) ifE? = (?)
P (??)|F? |PT (E?|F? ) otherwise
where PT (E?|F? ) is the probability of the phrase
pair as registered in the translation table, and |F? |
is the length of the phrase F? . The estimation of
P (?) is done by MLE:
P (?) = number of unaligned source word tokensnumber of source word tokens .
3.2 Model 2: EMPTY as Normal Word
Model 1 assumes that every word is as likely to be
spurious as any other word. Definitely this is not
a reasonable assumption, since certain function
words and grammatical particles are more likely
to be spurious than other words. Therefore, in our
second SWD model the probability of translating
a source word f to ? is conditioned on f itself.
This probability, P (?|f), is in the same form as
the probability of a normal phrase pair, P (E?|F? ),
if we consider ? as some special phrase of the tar-
get language and f as a source language phrase
on its own. Thus P (?|f) can be estimated and
recorded in the same way as the probability of
normal phrase pairs. During the phase of phrase
enumeration, in addition to enumerating all nor-
mal phrase pairs, we also enumerate all unaligned
source words f and add phrase pairs of the form
< (f), (?) >. These special phrase pairs, TO-
EMPTY phrase pairs, are fed to the module of
phrase scoring along with the normal phrase pairs.
Both types of phrase pairs are then stored in the
translation table with corresponding phrase trans-
lation probabilities. It can be seen that, since the
probabilities of normal phrase pairs are estimated
in the same procedure as those of TO-EMPTY
phrase pairs, they do not need re-weighing as in
the case of SWD model 1.
3.3 Model 3: Context-sensitive Model
Although model 2 is much more informative than
model 1, it is still unsatisfactory if we consider
the problem of SWD as a problem of tagging.
The decoder can be conceived as if it carries out
a tagging task over the source language sentence:
each source word is tagged either as ?spurious? or
?non-spurious?. Under such a perspective, SWD
3
model 2 is merely a unigram tagging model, and
it uses only one feature template, viz. the lex-
ical form of the source word in hand. Such a
model can by no means encode any contextual
information, and therefore it cannot handle the
?ACCORDING-TO/?? NP EXPRESS/,+? ex-
ample in section 1.
An obvious solution to this limitation is a more
powerful tagging model augmented with context-
sensitive feature templates. Inspired by research
work like (Lafferty et al, 2001) and (Sha and
Pereira, 2003), our SWD model 3 uses first-order
Conditional Random Field (CRF) to tackle the
tagging task.1 The CRF model uses the follow-
ing feature templates:
1. the lexical form and the POS of the foreign
word f itself;
2. the lexical forms and the POSs of f?2, f?1,
f+1, and f+2, where f?2 and f?1 are the two
words to the left of f , and f+1 and f+2 are
the two words to the right of f ;
3. the lexical form and the POS of the head
word of f ;
4. the lexical forms and the POSs of the depen-
dent words of f .
The lexical forms are the major source of infor-
mation whereas the POSs are employed to allevi-
ate data sparseness. The neighboring words are
used to capture local context information. For ex-
ample, in Chinese there is often a comma after
verbs like ?said? or ?stated?, and such a comma
is not translated to any word or punctuation in
English. These spurious commas are therefore
identified by their immediate left neighbors. The
head and dependent words are employed to cap-
ture non-local context information found by some
dependency parser. For the ?ACCORDING-TO/?
? NP EXPRESS/,+? example in section 1,
the Chinese word ACCORDING-TO/?? is the
head word of EXPRESS/,+. The spurious to-
ken of EXPRESS/,+ in this pattern can be dis-
tinguished from the non-spurious tokens through
the feature template of head word.
1Maximum Entropy was also tried in our experiments but
its performance is not as good as CRF.
The training data for the CRF model comprises
the alignment matrices of the bilingual training
data for the MT system. A source word (token)
in the training data is tagged as ?non-spurious? if
it is aligned to some target word(s), otherwise it is
tagged as ?spurious?. The sentences in the train-
ing data are also POS-tagged and parsed by some
dependency parser, so that each word can be as-
signed values for the POS-based feature templates
as well as the feature templates of head word and
dependency words.
The trained CRF model can then be used to
augment the decoder to tackle the SWD problem.
An input source sentence should first be POS-
tagged and parsed for assigning feature values.
The probability for f being spurious, P (?|f), is
then calculated by the trained CRF model as
PCRF (spurious|~F (f)).
The probability for f being non-spurious is sim-
ply 1 ? P (?|f). For a normal phrase pair
< F? , E? > recorded in the translation table,
its phrase translation probability and the lexical
weight should be re-weighed by the probabilities
of non-spuriousness. The weighing factor is
?
fi?F?
(1? P (?|fi)),
since the translation of F? into E? means the de-
coder considers every word in F? as non-spurious.
4 Experiments
4.1 Experiment Settings
A series of experiments were run to compare the
performance of the three SWD models against the
baseline, which is the standard phrase-based ap-
proach to SMT as elaborated in (Koehn et al,
2003). The experiments are about Chinese-to-
English translation. The bilingual training data
is the one for NIST MT-2006. The GIGAWORD
corpus is used for training language model. The
development/test corpora are based on the test
sets for NIST MT-2005/6.
The alignment matrices of the training data are
produced by the GIZA++ (Och and Ney, 2000b)
word alignment package with its default settings.
The subsequent construction of translation table
was done in exactly the same way as explained
4
in (Koehn et al, 2003). For SWD model 2,
the phrase enumeration step is modified as de-
scribed in section 3.2. We used the Stanford
parser (Klein and Manning, 2003) with its default
Chinese grammar for its POS-tagging as well as
finding the head/dependent words of all source
words. The CRF toolkit used for model 3 is
CRF++2. The training data for the CRF model
should be the same as that for translation table
construction. However, since there are too many
instances (every single word in the training data
is an instance) with a huge feature space, no pub-
licly available CRF toolkit can handle the entire
training set of NIST MT-2006.3 Therefore, we
can use at most only about one-third of the NIST
training set (comprising the FBIS, B1, and T10
sections) for CRF training.
The decoder in the experiments is our re-
implementation of HIERO (Chiang, 2007), aug-
mented with a 5-gram language model and a re-
ordering model based on (Zhang et al, 2007).
Note that no hierarchical rule is used with the de-
coder; the phrase pairs used are still those used
in conventional phrase-based SMT. Note also that
the decoder does not translate OOV at all even
in the baseline case, and thus the SWD models
do not improve performance simply by removing
OOVs.
In order to test the effect of training data size on
the performance of the SWD models, three varia-
tions of training data were used:
FBIS Only the FBIS section of the NIST training
set is used as training data (for both transla-
tion table and the CRF model in model 3).
This section constitutes about 10% of the en-
tire NIST training set. The purpose of this
variation is to test the performance of each
model when very small amount of data are
available.
BFT Only the B1, FBIS, and T10 sections of the
NIST training set are used as training data.
These sections are about one-third of the en-
tire NIST training set. The purpose of this
2http://crfpp.sourceforge.net/
3Apart from CRF++, we also tried FLEX-
CRF (http://flexcrfs.sourceforge.net) and MALLET
(http://mallet.cs.umass.edu).
Data baseline model 1 model 2 model 3
FBIS 28.01 29.71 29.48 29.64
BFT 29.82 31.55 31.61 31.75
NIST 29.77 31.39 31.33 31.71
Table 2: BLEU scores in Experiment 1: NIST?05 as
dev and NIST?06 as test
variation is to test each model when medium
size of data are available.4
NIST All the sections of the NIST training set
are used. The purpose of this variation is to
test each model when a large amount of data
are available.
(Case-insensitive) BLEU-4 (Papineni et al,
2002) is used as the evaluation metric. In each
test in our experiments, maximum BLEU training
were run 10 times, and thus there are 10 BLEU
scores for the test set. In the following we will
report the mean scores only.
4.2 Experiment Results and Analysis
Table 2 shows the results of the first experiment,
which uses the NIST MT-2005 test set as develop-
ment data and the NIST MT-2006 test set as test
data. The most obvious observation is that any
SWD model achieves much higher BLEU score
than the baseline, as there is at least 1.6 BLEU
point improvement in each case, and in some case
the improvement of using SWD is nearly 2 BLEU
points. This clearly proves the importance of
SWD in phrase-based SMT.
The difference between the performance of the
various SWD models is much smaller. Yet there
are still some noticeable facts. The first one is
that model 1 gives the best result in the case of
using only FBIS as training data but it fails to
do so when more training data is available. This
phenomenon is not strange since model 2 and
model 3 are conditioned on more information and
therefore they need more training data.
The second observation is about the strength of
SWD model 3, which achieves the best BLEU
score in both the BFT and NIST cases. While
its improvement over models 1 and 2 is marginal
in the case of BFT, its performance in the NIST
4Note also that the BFT data set is the largest training
data that the CRF model in model 3 can handle.
5
case is remarkable. A suspicion to the strength of
model 3 is that in the NIST case both models 1
and 2 use the entire NIST training set for esti-
mating P (?), while model 3 uses only the BFT
sections to train its CRF model. It may be that
the BFT sections are more consistent with the test
data set than the other NIST sections, and there-
fore a SWD model trained on BFT sections only
is better than that trained on the entire NIST. This
conjecture is supported by the fact that in all four
settings the BLEU scores in the NIST case are
lower than those in the BFT case, which suggests
that other NIST sections are noisy. While it is im-
possible to test model 3 with the entire NIST, it is
possible to restrict the data for the estimation of
P (?|f) in model 1 to the BFT sections only and
check if such a restriction helps.5 We estimated
the uniform probability P (?) from only the BFT
sections and used it with the translation table con-
structed from the complete NIST training set. The
BLEU score thus obtained is 31.24, which is even
lower than the score (31.39) of the original case
of using the entire NIST for both translation table
and P (?|f) estimation. In sum, the strength of
model 3 is not simply due to the choice of train-
ing data.
The test set used in Experiment 1 distinguishes
itself from the development data and the training
data by its characteristics of combining text from
different genres. There are three sources of the
NIST MT-2006 test set, viz. ?newswire?, ?news-
group?, and ?broadcast news?, while our devel-
opment data and the NIST training set comprises
only newswire text and text of similar style. It is
an interesting question whether SWD only works
for some genres (say, newswire) but not for other
genres. In fact, it is dubious whether SWD fits the
test set to the same extent as it fits the develop-
ment set. That is, perhaps SWD contributes to the
improvement in Experiment 1 simply by improv-
ing the translation of the development set which is
composed of newswire text only, and SWD may
not benefit the translation of the test data at all.
In order to test this conjecture, we ran Experi-
ment 2, in which the SWD models were still ap-
plied to the development data during training, but
5Unfortunately this way does not work for model 2 as
the estimation of P (?|f) and the construction of translation
table are tied together.
Data model 1 model 2 model 3
FBIS 29.85 29.91 29.95
BFT 31.73 31.84 32.08
NIST 31.70 31.82 32.05
Table 3: BLEU scores in Experiment 2, which is the
same as Experiment 1 but no word is deleted for test
corpus. Note: the baseline scores are the same as the
baselines in Experiment 1 (Table 2).
all SWD models stopped working when translat-
ing the test data with the trained parameters. The
results are shown in Table 3. These results are
very discouraging if we compare each cell in Ta-
ble 3 against the corresponding cell in Table 2: in
all cases SWD seems harmful to the translation of
the test data. It is tempting to accept the conclu-
sion that SWD works for newswire text only.
To scrutinize the problem, we split up the test
data set into two parts, viz. the newswire sec-
tion and the non-newswire section, and ran ex-
periments separately. Table 4 shows the results
of Experiment 3, in which the development data
is still the NIST MT-2005 test set and the test
data is the newswire section of NIST MT-2006
test set. It is confirmed that if test data shares
the same genre as the training/development data,
then SWD does improve translation performance
a lot. It is also observed that more sophisticated
SWD models perform better when provided with
sufficient training data, and that model 3 exhibits
remarkable improvement when it comes to the
NIST case.
Of course, the figures in Table 5, which shows
the results of Experiment 4 where the non-
newswire section of NIST MT-2006 test set is
used as test data, still leave us the doubt that SWD
is useful for a particular genre only. After all, it
is reasonable to assume that a model trained from
data of a particular domain can give good perfor-
mance only to data of the same domain. On the
other hand, the language model is another cause
of the poor performance, as the GIGAWORD cor-
pus is also of the newswire style.
While we cannot prove the value of SWD with
respect to training data of other genres in the
mean time, we could test the effect of using de-
velopment data of other genres. In our last ex-
periment, the first halves of both the newswire
6
apply SWD for test set no SWD for test set
Data model 1 model 2 model 3 model 1 model 2 model 3
FBIS 30.81 30.81 30.68 29.23 29.61 29.46
BFT 33.57 33.74 33.71 31.88 31.87 32.25
NIST 33.65 34.01 34.42 32.14 32.59 32.87
Table 4: BLEU scores in Experiment 3, which is the same as Experiments 1 and 2 but only the newswire section
of NIST?06 test set is used. Note: the baseline scores are the same as the baselines in Experiment 1 (Table 2).
apply SWD for test set no SWD for test set
Data model 1 model 2 model 3 model 1 model 2 model 3
FBIS 29.19 28.86 29.16 30.07 29.67 30.08
BFT 30.62 30.64 30.86 31.66 31.83 32.00
NIST 30.34 30.10 30.46 31.50 31.45 31.66
Table 5: BLEU scores in Experiment 4, which is the same as Experiments 1 and 2 but only the non-newswire
section of NIST?06 test set is used. Note: the baseline scores are the same as the baselines in Experiment 1
(Table 2).
Data baseline model 1 model 2 model 3
FBIS 26.87 27.79 27.51 27.61
BFT 29.11 30.38 30.49 30.41
NIST 29.34 30.63 30.95 31.00
Table 6: BLEU scores in Experiment 5: which is the
same as Experiment 1 but uses half of NIST?06 as de-
velopment set and another half of NIST?06 as test set.
and non-newswire sections of NIST MT-2006 test
set are combined to form the new development
data, and the second halves of the two sections
are combined to form the new test data. The new
development data is therefore consistent with the
new test data. If SWD, or at least a SWD model
from newswire, is harmful to the non-newswire
section, which constitutes about 60% of the de-
velopment/test data, then it will be either that the
parameter training process minimizes the impact
of SWD, or that the SWD model will make the
parameter training process fail to search for good
parameter values. The consequence of either case
is that the baseline setting should produce similar
or even higher BLEU score than the settings that
employ some SWD model. Experiment results, as
shown in Table 6, illustrate that SWD is still very
useful even when both development and test sets
contain texts of different genres from the training
text. It is also observed, however, that the three
SWD models give rise to roughly the same BLEU
scores, indicating that the SWD training data do
not fit the test/development data very well as even
the more sophisticated models are not benefited
from more data.
5 Experiments using METEOR
The results in the last section are all evaluated us-
ing the BLEU metric only. It is dubious whether
SWD is useful regarding recall-oriented metrics
like METEOR (Banerjee and Lavie, 2005), since
SWD removes information in source sentences.
This suspicion is to certain extent confirmed by
our application of METEOR to the translation
outputs of Experiment 1 (c.f. Table 7), which
shows that all SWD models achieve lower ME-
TEOR scores than the baseline. However, SWD is
not entirely harmful to METEOR: if SWD is ap-
plied to parameter tuning only but not for the test
set, (i.e. Experiment 2), even higher METEOR
scores can be obtained. This puzzling observa-
tion may be because the parameters of the de-
coder are optimized with respect to BLEU score,
and SWD benefits parameter tuning by improv-
ing BLEU score. In future experiments, maxi-
mum METEOR training should be used instead
of maximum BLEU training so as to examine if
SWD is really useful for parameter tuning.
7
Experiment 1 Experiment 2
SWD for both dev/test SWD for dev only
Data baseline model 1 model 2 model 3 model 1 model 2 model 3
FBIS 50.07 47.90 49.83 49.34 51.58 51.08 51.17
BFT 52.47 50.55 51.89 52.10 54.72 54.43 54.30
NIST 52.12 49.86 50.97 51.59 54.14 53.82 54.01
Table 7: METEOR scores in Experiments 1 and 2
6 Conclusion and Future Work
In this paper, we have explained why the han-
dling of spurious source words is not a trivial
problem and how important it is. Three solu-
tions, with increasing sophistication, to the prob-
lem of SWD are presented. Experiment results
show that, in our setting of using NIST MT-2006
test set, any SWD model leads to an improvement
of at least 1.6 BLEU points, and SWD model 3,
which makes use of contextual information, can
improve up to nearly 2 BLEU points. If only
the newswire section of the test set is considered,
SWD model 3 is even more superior to the other
two SWD models.
The effect of training data size on SWD has
also been examined, and it is found that more
sophisticated SWD models do not outperform
unless they are provided with sufficient amount
of data. As to the effect of training data do-
main/genre on SWD, it is clear that SWD models
trained on text of certain genre perform the best
when applied to text of the same genre. While
it is infeasible for the time being to test if SWD
works well for non-newswire style of training
data, we managed to illustrate that SWD based on
newswire text still to certain extent benefits the
training and translation of non-newswire text.
In future, two extensions of our system are
needed for further examination of SWD. The first
one is already mentioned in the last section: max-
imum METEOR training should be implemented
in order to fully test the effect of SWD regard-
ing METEOR. The second extension is about the
weighing factor in models 1 and 3. The current
implementation assumes that all source words
in a normal phrase pair need to be weighed by
1? P (?). However, in fact some source words in
a source phrase are tacitly deleted (as explained
in the Introduction). Thus the word alignment in-
formation within phrase pairs need to be recorded
and the weighing of a normal phrase pair should
be done in accordance with such alignment infor-
mation.
References
Brown, P., J. Cocke, S. Della Pietra, V. Della Pietra,
F. Jelinek, J. Lafferty, R. Mercer, and P. Roossin.
1990. A Statistical Approach to Machine Transla-
tion Computational Linguistics, 16(2).
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. Pro-
ceedings of Workshop on Evaluation Measures for
MT and/or Summarization at ACL 2005.
David Chiang. 2007. Hierarchical Phrase-based
Translation. Computational Linguistics, 33(2).
Dan Klein and Christopher D. Manning. 2003. Ac-
curate Unlexicalized Parsing. Proceedings for ACL
2003.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical Phrase-based Translation. Proceedings
for HLT-NAACL 2003.
John Lafferty, Andrew McCallum, and Fernando
Pereira 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. Proceedings for 18th International
Conf. on Machine Learning.
Franz J. Och, and Hermann Ney. 2000. A comparison
of alignment models for statistical machine transla-
tion. Proceedings of COLING 2000.
Franz J. Och, and Hermann Ney. 2000. Improved
Statistical Alignment Models. Proceedings for ACL
2000.
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. Proceed-
ings for ACL 2002.
Fei Sha, Fernando Pereira. 2003. Shallow parsing
with conditional random fields. Proceedings of
NAACL 2003.
Dongdong Zhang, Mu Li, Chi-Ho Li and Ming
Zhou. 2007. Phrase Reordering Model Integrat-
ing Syntactic Knowledge for SMT. Proceedings for
EMNLP 2007.
8
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 291?298, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Learning What to Talk About in Descriptive Games
Hugo Zaragoza
Microsoft Research
Cambridge, United Kingdom
hugoz@microsoft.com
Chi-Ho Li
University of Sussex
Brighton, United Kingdom
C.H.Li@sussex.ac.uk
Abstract
Text generation requires a planning mod-
ule to select an object of discourse and its
properties. This is specially hard in de-
scriptive games, where a computer agent
tries to describe some aspects of a game
world. We propose to formalize this prob-
lem as a Markov Decision Process, in
which an optimal message policy can be
defined and learned through simulation.
Furthermore, we propose back-off poli-
cies as a novel and effective technique to
fight state dimensionality explosion in this
framework.
1 Introduction
Traditionally, text generation systems are decom-
posed into three modules: the application module
which manages the high-level task representation
(state information, actions, goals, etc.), the text plan-
ning module which chooses messages based on the
state of the application module, and the sentence
generation module which transforms messages into
sentences. The planning module greatly depends
on the characteristics of both the application and
the generation modules, solving issues in domain
modelling, discourse and sentence planning, and to
some degree lexical and feature selection (Cole et
al., 1997). In this paper we concentrate on one
of the most basic tasks that text planning needs to
solve: selecting the message content, or more sim-
ply, choosing what to talk about.
Work on text-generation often assumes that an
object or topic has been already chosen for discus-
sion. This is reasonable for many applications, but
in some cases choosing what to talk about can be
harder than choosing how to. This is the case in the
type of text generation applications that we are in-
terested in: generating descriptive messages in com-
puter games. In a modern computer game at any
given moment there may be an enormous number
of object properties that can be described, each with
varying importance and consequences. The outcome
of the game depends not only on the skill of the
player, but also on the quality of the descriptive mes-
sages produced. We refer to such situations as de-
scriptive games.
Our goal is to develop a strategy to choose the
most interesting descriptive messages that a particu-
lar talker may communicate to a particular listener,
given their context (i.e. their knowledge of the world
and of each-other). We refer to this as message plan-
ning.
Developing a general framework for planning is
very difficult because of the strong coupling be-
tween the planning and application modules. We
propose to frame message planning as a Markov De-
cision Process (MDP) which encodes the environ-
ment, the information available to the talker and lis-
tener, the consequences of their communicative and
non-communicative acts, and the constraints of the
text generation module. Furthermore we propose to
use Reinforcement Learning (RL) to learn the op-
timal message policy. We demonstrate the overall
principle (Section 2) and then develop in more de-
tail a computer game setting (Section 3).
291
One of the main weaknesses of RL is the problem
of state dimensionality explosion. This problem is
specially acute in message planning, since in typical
situations there can be hundreds of thousands of po-
tential messages. At the same time, the domain is
highly structured. We propose to exploit this struc-
ture using a form of the back-off smoothing princi-
ple on the state space (Section 4).
1.1 Related Work
Our problem setting can be seen as a generalisation
of the content selection problem in the generation of
referring expressions in NLG. In the standard set-
ting of this problem (see for example (van Deemter
and Krahmer, to appear)) an algorithm needs to se-
lect the distinguishing description of an object in a
scene. This description can be seen as a subset of
scene properties which i) uniquely identifies a given
target object, and ii) is optimal in some sense (min-
imal, psychologically plausible, etc.) van Deemter
and Krahmer show that most content selection algo-
rithms can be described as different cost functions
over a particular graph representation of the scene.
Minimising the cost of a subgraph leads to a distin-
guishing description.
Some aspects of our work generalise that of con-
tent selection: i) we consider the target object is un-
known, ii) we consider scenes (i.e. world states) that
are dynamic (i.e. they change over time) and reac-
tive (i.e. utterances change the world), and iii) we
consider listeners that have partial knowledge of the
scene. This has important consequences. For exam-
ple, the cost of a description cannot be directly eval-
uated; instead, we must play the game, that is, gener-
ate utterances and observe the rewards obtained over
time. Also identical word-states may lead to differ-
ent optimal messages, depending on the listener?s
partial knowledge. Other aspects of our work are
very simplistic compared to current work in con-
tent selection, for example with respect to the use
of negation and of properties that are boolean, rel-
ative or graded (van Deemter and Krahmer, to ap-
pear). We hope to incorporate these ideas into our
work soon.
Probabilistic dialogue policies have been previ-
ously proposed for spoken dialogue systems (SDS)
(see for example (Singh et al, 2002; Williams et
al., 2005) and references therein). However, work in
SDS focus mainly on coping with the noise and un-
certainty resulting from speech recognition and sen-
tence parsing. In this context MDPs are used to infer
features and plan communicative strategies (modal-
ity, confusion, initiative, etc.) In our work we do not
need to deal with uncertainty or parsing; our main
concern is in the selection of the message content.
In this sense our work is closer to (Henderson et al,
2005), where RL is used to train a SDS with very
many states encoding message content.
Finally, with respect to the state-explosion prob-
lem in RL, related work can be found in the areas of
multi-task learning and robot motion planning (Diet-
terich, 2000, and references therein). In these works
the main concern is identifying the features that are
relevant to specific sub-tasks, so that robots may
learn multiple loosely-coupled tasks without incur-
ring state-explosion. (Henderson et al, 2005) also
addresses this problem in the context of SDS and
proposes a semi-supervised solution. Our approach
is related to these works, but it is different in that
we assume that the feature structure is known in ad-
vance and has a very particular form amenable to a
form of back-off regularisation.
2 Message planning
Let us consider an environment comprising a world
with some objects and some agents, and some dy-
namics that govern their interaction. Agents can ob-
serve and memorize certain things about the world,
can carry out actions and communicate with other
agents. As they do so, they are rewarded or pun-
ished by the environment (e.g. if they find food, if
the complete some goal, if they run out of energy,
etc.)
The agents? actions are governed by a policy. We
will consider separately the physical action policy
(?), which decides which physical action to take
given the state of the agent, and the message action
policy (?), which decides when to communicate, to
whom, and what about. Our main concern in this
paper will be to learn an optimal ?. Before we de-
fine this goal more precisely, we will introduce some
notation.
A property is a set of attribute-value pairs. An
object is a set of properties, with (at least) attributes
Type and Location. A domain is a set of objects. Fur-
292
thermore, we say that s? is a sub-domain of s if s? can
be obtained by deleting property?value pairs from
s (while enforcing the condition that remaining ob-
jects must have Type and Location). Sub(s) is the set
containing s, all sub-domains of s, and the empty
domain ?.
A world state can be represented as a domain,
noted s
W
. Any partial view of the world state can
also be represented as a domain s ? Sub(s
W
). Sim-
ilarly the content of any descriptive message about
the world, noted m, can be represented as a partial
view of it. An agent is the tuple:
A :=
(
s
A
, ?
A
, {?
AA
?
, s
AA
?}
A
? 6=A
)
? s
A
? Sub(s
W
): knowledge that A has about
the state of the world.
? s
AA
? ? Sub(s
A
? s?
A
): knowledge that A
has about the knowledge that A? has about the
world.
? ?
a
:= P (c|s
A
) is the action policy of A, and c
is a physical action.
? ?
AA
? := P (m ? M(s
A
)|s
A
, s
AA
?) is the mes-
sage policy of A for sending messages to A?,
and M(s
A
) are all valid messages at state s
A
(discussed in Section 2.3).
When an agent A decides to send a message to A?,
it can use its knowledge of A? to choose messages
effectively. For example, A will prefer to describe
things that it knows A? does not know (i.e. not in
s
AA
?). This is the reason why the message policy
?
A
depends on both s
A
and s
AA
? . After a message is
sent (i.e. realised and uttered) the agent?s will update
their knowledge states s
A
? , s
A
?
A
and s
AA
? .
The question that we address in this paper is that
of learning an optimal message policy ?
AA
? .
2.1 Talker?s Markov Decision Process
We are going to formalize this problem as a stan-
dard Markov Decision Process (MDP). In general a
MDP (Sutton and Barto, 1998) is defined over some
set of states S := {s
i
}
i=1..K
and actions associated
to every state, A(s
i
) := {a
ij
}
j=1..N
i
. The envi-
ronment is governed by the state transition function
Pa
ss
?
:= P (s?|s, a). A policy determines the likeli-
hood of actions at a given state: ?(s) := P (a|s). At
each state transition a reward is generated from the
reward function Ra
ss
?
:= E{r|s, s?, a}.
MDPs allow us to define and find optimal poli-
cies which maximise the expected reward. Classical
MDPs assume that the different functions introduced
above are known and have some tractable analyti-
cal form. Reinforcement Learning (RL) in as ex-
tension of MDPs in which the environment function
Pa
ss
?
is unknown or complex, and so the optimal pol-
icy needs to be learned online by directly interacting
with the environment. There exist a number of algo-
rithms to solve a RL problem, such as Q-Learning
or SARSA (Sutton and Barto, 1998).
We can use a MDP to describe a full descrip-
tive game, in which several agents interact with the
world and communicate with each-other. To do so
we would need to consider composite states con-
taining s
W
, {s
A
}
A
, and
{
{s
AA
?}
A
? 6=A
}
A
. Simi-
larly, we need to consider composite policies con-
taining {?
A
}
A
and
{
(?
AA
?)
A
? 6=A
}
A
. Finally, we
would consider the many constrains in this model;
for example: only physical actions affect the state
of the world, only message actions affect believes,
and only believe states can affect the choice of the
agent?s actions.
MDPs provide us with a principled way to deal
with these elements and their relationships. How-
ever, dealing with the most general case results in
models that are very cumbersome and which hide
the conceptual simplicity of our approach. For this
reason, we will limit ourselves in this paper to one
of the simplest communication cases of interest: a
single all-knowing talker, and a single listener com-
pletely observed by the talker. We will discuss later
how this can be generalized.
2.2 The Talking God Setting
In the simplest case, an all-knowing agent A
0
sits in
the background, without taking any physical actions,
and uses its message policy (?
01
) to send messages
to a listener agent A
1
. The listener agent cannot talk
back, but can interact with the environment using
its physical action policy ?
1
. Rewards obtained by
A
1
are shared by both agents. We refer to this set-
ting as the talking God setting. Examples of such
situations are common in games, for example when
a computer character talks to its (computer) team-
293
w w?
s1 s?1
m0
a1
s s?
a
r
Figure 1: Talking God MDP.
mates, or when a mother-ship with full information
of the ground sends a small simple robot to do a task.
Another example would be that of a teacher talking
to a learner, except that the teacher may not have full
information of the learners head!
Since the talker is all-knowing, it follows that
s
0
= s
W
and s
01
= s
1
. Furthermore, since the
talker does not take physical actions, ?
0
does not
need to be defined. Similarly, since the listener does
not talk we do not need to define ?
10
or s
10
. This
case is depicted in Figure 1 as a graphical model.
By grouping states and actions (dotted lines) we can
see that this is can be modelled as a standard MDP.
If all the probability distributions are known analyt-
ically, or if they can be sampled, optimal physical
and message policies can be learnt (thick arrows).
Several generalizations of this model are possible.
A straight forward generalization is to consider more
than one listener agent. We can then choose to learn
a single policy for all, or individual policies for each
agent.
A second way to generalize the setting is to make
the listeners mind only partially observable to the
talker. In this case the talker continues to know the
entire world (s
0
= s
W
), but does not know ex-
actly what the listener knows (s
01
6= s
0
). This is
more realistic in situations in which the listener can-
not talk back to the talker, or in which the talkers
mind is not observable. However, to model this we
need a partially observable MDP (POMDP). Solv-
ing POMDPS is much harder than solving MDPs,
but there have been models proposed for dialogue
management (Williams et al, 2005).
In the more general case, the talker would have
partial knowledge of the world and of the listener,
and would itself act. In that case all agents are equal
and can communicate as they evolve in the envi-
ronment. The other agents minds are not directly
observable, but we obtain information about them
from their actions and their messages. This can all
be in principle modelled by POMDPs in a straight-
forward manner, although solving these models is
more involved. We are currently working towards
doing so.
Finally, we note that all the above cases have
dealt with worlds in which objects are static (i.e.
information does not become obsolete), agents do
not gain or communicate erroneous information, and
communication itself is non-ambiguous and loss-
less. This is a realistic scenario for text generation,
and for communication between computer agents in
games, but it is far removed from the spoken dia-
logue setting.
2.3 Generation Module and Valid Messages
Generating descriptive sentences of domains can be
done in a number of ways, from template to feature-
based systems (Cole et al, 1997). Our framework
does not depend on a particular choice of generation
module, and so we do not need to discuss this mod-
ule. However, our message policy is not decoupled
of the generation module; indeed, it would not make
sense to develop a planning module which plans
messages that cannot be realised! In our framework,
the generation module is seen simply as a fixed and
known filter over all possible the messages.
We formalize this by representing an agent?s gen-
eration module as a function ?
A
(m) mapping a mes-
sage m to a NL sentence, or to ? if the module can-
not fully realise m. The set of available messages
to an agent A in state s
A
is therefore: M(s
A
) :=
{m |m ? Sub(s
A
) , ?
A
(m) 6= ?}.
3 A Simple Game Example
In this section we will use a simple computer game
to demonstrate how the proposed framework can be
used to learn message policies.
The game evolves in a grid-world. A mother-
ship sends a scout, which will try to move from its
294
Figure 2: Example of a Simple Game Board.
starting position (top left corner) to a target (bot-
tom right). There are two types of objects on the
board, Type := {bomb, tree}, with a property Size :=
{big, small} in addition of Location. If a scout at-
tempts to move into a big tree, the move is blocked;
small trees have no effect. If a scout moves into
a bomb the scout is destroyed and a new one is
created at the starting position. Before every step
the mother-ship may send a message to the scout.
Then the scout moves one step (horizontal or ver-
tical) towards the target choosing the shortest path
which avoids hazards known by the scout (the A*
algorithm is used for this). Initially scouts have no
knowledge of the objects in the world; they gain this
knowledge by stepping into objects or by receiving
information from the mother-ship.
This is an instance of the talking god model dis-
cussed previously. The scout is the listener agent
(A
1
), and the mother-ship the talker (A
0
). The
scouts action policy ?
1
is fixed (as described above),
but we need to learn the message policy ?
01
.
Rewards are associated with the results of phys-
ical actions: a high positive reward (1000) is as-
signed to reaching the destination, a large negative
reward (-100) to stepping in a bomb, a medium neg-
ative reward (-10) to being blocked by a big tree, a
small negative reward to every step (-1). Further-
more, sending a message has a small negative re-
ward proportional to the number of attributes men-
tioned in the message (-2 per attribute, to discourage
the talker from sending useless information). The
message ? is given zero cost; this is done in order to
200 500 1000 1500 2000 2500 3000 3500 4000 4500 5000
250
300
350
400
450
500
550
600
650
Training Cycles
R
ew
ar
d
optimal properties
all properties
TYPE only
Figure 3: Simple Game Learning Results
State Best Action Learnt
(and possible sentence realisation)
{ TREE-BIG-LEFT } ?
-SILENCE-
{ BOMB-BIG-FRONT } BOMB-FRONT
There is a bomb in front of you
{ TREE-SMALL-LEFT, TREE-BIG-RIGHT
TREE-BIG-RIGHT } There is a big tree to your right
{ BOMB-BIG-FRONT,
BOMB-SMALL-LEFT, TREE-BIG-RIGHT
TREE-BIG-RIGHT, There is a big tree to your right
TREE-SMALL-BACK }
Table 1: Examples of learnt actions.
learn when not to talk.
Learning is done as follows. We designed five
maps of 11 ? 11 cells, each with approximately 15
bombs and 20 trees of varying sizes placed in strate-
gic locations to make the scouts task difficult (one
of these maps is depicted in Figure 2; an A* path
without any knowledge and one with full knowl-
edge of the board are shown as dotted and dashed ar-
rows respectively). A training epoch consists of ran-
domly drawing one of these maps and running a sin-
gle game until completion. The SARSA algorithm
is used to learn the message policy, with  = 0.1
and ? = 0.9. The states s
W
and s
1
are encoded
to represent the location of objects surrounding the
scout, relative to its direction (i.e. objects directly in
front of the agent always receive the same location
value). To speed up training, we only consider the 8
cells adjacent to the agent.
Figure 3 shows the results of these experiments.
For comparison, we note that completing the game
295
with a uniformly random talking policy results in an
average reward of less than ?3000 meaning that on
average more than 30 scouts die before the target is
reached. The dashed line indicates the reward ob-
tained during training for a policy which does not
use the size attribute, but only type and location.
This policy effectively learns that both bombs and
trees in front of the agent are to be communicated,
resulting in an average reward of approximately 400,
and reducing the average number of deaths to less
than 2. The solid line represents the results obtained
by a policy that is forced to use all attributes. De-
spite the increase in communication cost, this pol-
icy can distinguish between small and large trees,
and so it increases the overall reward two-fold. Fi-
nally, the dotted line represents the results obtained
by a policy that can choose whether to use or not the
size attribute. This policy proves to be even more
effective than the previous one; this means that it
has learnt to use the size attribute only when it is
necessary. Some optimal (state,action) pairs learnt
for this policy are shown in Table 1. The first three
show correctly learnt optimal actions. The last is an
example of a wrongly learnt action, due to the state
being rare.
These are encouraging results, since they demon-
strate in practice how optimal policies may be learnt
for message planning. However, it should be clear
form this example that, as we increase the number
of types, attributes and values, this approach will be-
come unfeasible. This is discussed in the next sec-
tion.
4 Back-Off Policies
One of the main problems when using RL in prac-
tical settings (and, more generally, using MDPs) is
the exponential growth of the state space, and con-
sequently of the learning time required. In our case,
if there are M attributes, and each attribute p
i
has
N(p
i
) values, then there are S =
?
M
i=1
N(p
i
) pos-
sible sub-domains, and up to 2S states in the state
space. This exponential growth, unless addressed,
will render MDP learning unfeasible.
NL domains are usually rich with structure, some
of it which is known a priori. This is the case in
text generation of descriptions for computer games,
where we have many sources of information about
the objects of discourse (i.e. world ontology, dy-
namics, etc.) We propose to tackle the problem of
state dimensionality explosion by using this struc-
ture explicitly in the design of hierarchical policies.
We do so by borrowing the back-off smoothing
idea from language models. This idea can be stated
as: train a set of probability models, ordered by their
specificity, and make predictions using the most spe-
cific model possible, but only if there is enough
training data to support its prediction; otherwise,
back-off to the next less-specific model available.
Formally, let us assume that for every state
s we can construct a sequence of K embedded
partial representations of increasing complexity,
(s
[1]
, . . . , s
[k]
, . . . , s
[K]
). Let us denote ??
[k]
a se-
quence of policies operating at each of the partial
representation levels respectively, and let each of
these policies have a confidence measurement c
k
(s)
indicating the quality of the prediction at each state.
Since k indicates increasingly complex, we require
that c
k
(s) ? c
k
?(s) if k < k?. Then, the most spe-
cific policy we can use at state s can be written as:
k
?
s
:= arg max
k
{k ? sign (c
k
(s) ? ?)} (1)
A back-off policy can be implemented by choosing,
at every state s the most specific policy available:
?(s) = ??
[k
?
s
]
(s
[k
?
s
]
) (2)
We can use a standard off-policy learning algo-
rithm (such as Q-learning or SARSA) to learn all the
policies simultaneously. At every step, we draw an
action using (2) and update all policies with the ob-
tained reward1. Initially, the learning will be driven
by high-level (simple) policies. More complex poli-
cies will kick-in progressively for those states that
are encountered more often.
In order to implement back-off policies for our
setting, we need to define a confidence function c
k
.
A simple confidence measure is the number of times
the state s
[k]
has been previously encountered. This
measure grows on average very quickly for small k
states and slowly for high k states. Nevertheless, re-
occurring similar states will have high visit counts
1An alternative view of back-off policies is to consider that a
single complete policy is being learnt, but that actions are being
drawn from regularised versions of this policy, where the regu-
larisation is a back-off model on the features. We show this in
Appendix I
296
0 1000 2000 3000 4000 5000
500
550
600
650
700
750
800
850
training epochs
A
ve
ra
ge
 T
ot
al
 R
ew
ar
d 
(1
00
 R
un
s)
Full State, without noise objects
Full State, with 40 noise objects
Simple State, without noise objects
Simple State, with 40 noise objects
Back?Off with 40 noise objects
Figure 4: Back-Off Policy Simulation Results.
for all k values. This is exactly the kind of behav-
iour we require.
Furthermore, we need to choose a set of repre-
sentations of increasing complexity. For example,
in the case of n-gram models it is natural to choose
as representations sequences of preceding words of
increasing size. There are many choices open to us
in our application domain. A natural choice is to or-
der attribute types by their importance to the task.
For example, at the simplest level of representation
objects can be represented only by their type, at a
second level by the type and colour, and at a third
level by all the attributes. This same technique could
be used to exploit ontologies and other sources of
knowledge. Another way to create levels of repre-
sentation of increasing detail is to consider different
perceptual windows. For example, at the simplest
level the agent can consider only objects directly in
front of it, since these are generally the most im-
portant when navigating. At a second level we may
consider also what is to the left and right of us, and
finally consider all surrounding cells. This could be
pursued even further by considering regions of in-
creasing size.
4.1 Simulation Results
We present here a series of experiments based on
the previous game setting, but further simplified to
pinpoint the effect of dimensionality explosion, and
how back-off policies can be used to mitigate it.
We modify the simple game of Section 3 as fol-
lows. First, we add a new object type, stone, and a
new property Colour := {red, green}. We let al trees
be green and big and all bombs red and small, and
furthermore we fix their location (i.e. we use one
map instead of five). Finally we change the world
behaviour so that an agent that steps into a bomb re-
ceives the negative reward but does not die, it contin-
ues until it reaches the target. All these changes are
done to reduce the variability of our learning base-
line.
At every game we generate 40 stones of random
location, size and colour. Stepping on stones has no
physical effect to the scout and it generates the same
reward as moving into an empty cell, but this is un-
known to the talker and will need to be learnt. These
stones are used as noise objects, which increase the
size of the state space. When there are no noise ob-
jects, the number of possible states is 38 ? 6.5K
(the actual number of states will be much smaller
since there is a single maze). Noise objects can take
2 ? 2 = 4 possible forms, so the total number of
states with noise objects is (3 + 4)8 ? 6M . Even
with such a simplistic example we can see how dras-
tic the state dimensionality problem is. Despite the
fact that the noise objects do not affect the reward
structure of our simple game, reinforcement learn-
ing will be drastically slowed down by them.
Simulation results2 are shown in Figure 4. First
let us look at the results obtained using the full state
representation used in Section 3 (noted Full State).
Solid and dotted lines represent runs obtained with
and without noise objects. First note that learning
without noise objects (dotted circles) occurs mostly
within the first few epochs and settles after 250
epochs. When noise objects are added (solid cir-
cles) learning greatly slows down, taking over 5K
epochs. This is a typical illustration of the effect that
the number of states has on the speed of learning.
An obvious way to limit the number of states is
to eliminate features. For comparison, we learned
a simple representation policy with states encod-
ing only the type of the object directly in front of
the agent, ignoring its colour and all other locations
(noted Simple State). Without noise, the performance
(dotted triangles) is only slightly worse than that of
the original policy. However, when noise objects
2Every 200 training epochs we run 100 validation epochs
with  = 0. Only the average validation rewards are plotted.
297
are added (solid triangles) the training is no longer
slowed down. In fact, with noise objects this policy
outperforms the original policy up to epoch 1000:
the performance lost in the representation is made
up by the speed of learning.
We set up a back-off policy with K = 3 as fol-
lows. We use the Simple representation at k = 1,
plus a second level of representation where we rep-
resent the colour as well as the type of the object in
front of the agent, and finally the Full representation
as the third level. As the c
k
function we use state
visit counts as discussed above and we set ? = 10.
Before reaching the full policy (level 3), this policy
should progressively learn to avoid bombs and trees
directly in front (level 1), then (level 2) not avoid
small trees directly in front. We plot the perfor-
mance of this back-off policy (stars) in Figure 4. We
see that it attains very quickly the performance of
the simple policy (in less than 200 epochs), but the
continues to increase in performance settling within
500 epochs with a performance superior to that of
the full state representation, and very close to that of
the policies operating in the noiseless world.
Despite the small scale of this study, our results
clearly suggest that back-off policies can be used
effectively to control state dimensionality explosion
when we have strong prior knowledge of the struc-
ture of the state space. Furthermore (and this may be
very important in real applications such as game de-
velopment) we find that back-off policies produce a
natural to feel to the errors incurred while learning,
since policies develop progressively in their com-
plexity.
5 Conclusion
We have developed a formalism to learn interac-
tively the most informative message content given
the state of the listener and the world. We formalised
this problem as a MDP and shown how RL may be
used to learn message policies even when the envi-
ronment dynamics are unknown. Finally, we have
shown the importance of tackling the problem of
state dimensionality explosion, and we have pro-
posed one method to do so which exploits explicit
a priori ontological knowledge of the task.
References
R. Cole, J. Mariani, H. Uszkoreit, A. Zaenen, and V. Zue.
1997. Survey of the State of the Art in Human Lan-
guage Technology. Cambridge University Press.
T. G. Dietterich. 2000. Hierarchical reinforcement learn-
ing with the MAXQ value function decomposition.
Journal of Artificial Intelligence Research, 13:227?
303.
J. Henderson, O. Lemon, and K. Georgila. 2005. Hybrid
reinforcement/supervised learning for dialogue poli-
cies from communicator data. In 4th IJCAI Workshop
on Knowledge and Reasoning in Practical Dialogue
Systems.
S. Singh, D. Litmanand, M. Kearns, and M. Walker.
2002. Optimizing dialogue management with re-
inforcement learning: Experiments with the njfun
system. Journal of Artificial Intelligence Research,
16:105?133.
R. S. Sutton and A. G. Barto. 1998. Reinforcement
Learning. MIT Press.
K. van Deemter and E. Krahmer. (to appear). Graphs and
booleans. In Computing Meaning, volume 3 of Stud-
ies in Linguistics and Philosophy. Kluwer Academic
Publishers.
J. D. Williams, P. Poupart, and S. Young. 2005. Fac-
tored partially observable markov decision processes
for dialogue management. In 4th IJCAI Workshop on
Knowledge and Reasoning in Practical Dialogue Sys-
tems.
6 Appendix I
We show here that the expected reward for a partial
policy ?
k
after an action a, noted Q?k(s, a), can be
obtained from the expected reward of the full pol-
icy Q?(s, a) and the conditional state probabilities
P (s|s
[k]
). We may use this to compute the expected
risk of any partial policy R?k(s) from the full policy.
Let T
k
(s) :=
{
s
? ? S | s?
[k]
= s
[k]
}
be the sub-
set of full states which map to the same value of s.
Given a state distribution P (s) we can define distri-
butions over partial states:
P (s
[k]
, s
[j]
) =
?
s
??T
k
(s)?T
j
(s)
P (s?) . (3)
Since
?
s
??T
k
(s)
P (s?|s
[k]
) = 1, we have
P (A|s
[k]
) =
?
s
??T
k
(s)
P (A|s?)P (s?|s
[k]
), and so:
Q
?
k(s, a) =
?
s
??T
k
(s)
P (s?|s
[k]
)Q?(s?, a) . (4)
298
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 474?482,
Beijing, August 2010
An Empirical Study on Web Mining of Parallel Data 
Gumwon Hong1, Chi-Ho Li2, Ming Zhou2 and Hae-Chang Rim1 
1Department of Computer Science & En-
gineering, Korea University 
{gwhong,rim}@nlp.korea.ac.kr 
2Natural Language Computing Group, 
Microsoft Research Asia 
{chl,mingzhou}@microsoft.com 
 
Abstract 
This paper 1  presents an empirical ap-
proach to mining parallel corpora. Con-
ventional approaches use a readily 
available collection of comparable, non-
parallel corpora to extract parallel sen-
tences. This paper attempts the much 
more challenging task of directly search-
ing for high-quality sentence pairs from 
the Web. We tackle the problem by 
formulating good search query using 
?Learning to Rank? and by filtering 
noisy document pairs using IBM Model 
1 alignment. End-to-end evaluation 
shows that the proposed approach sig-
nificantly improves the performance of 
statistical machine translation. 
1 Introduction 
Bilingual corpora are very valuable resources in 
NLP. They can be used in statistical machine 
translation (SMT), cross language information 
retrieval, and paraphrasing. Thus the acquisition 
of bilingual corpora has received much attention. 
Hansards, or parliamentary proceedings in 
more than one language, are obvious source of 
bilingual corpora, yet they are about a particular 
domain and therefore of limited use. Many re-
searchers then explore the Web. Some approach 
attempts to locate bilingual text within a web 
page (Jiang et al, 2009); some others attempt to 
collect web pages in different languages and 
decide the parallel relationship between the web 
pages by means of structural cues, like exist-
ence of a common ancestor web page, similarity 
between URLs, and similarity between the 
HTML structures (Chen and Nie, 2000; Resnik 
                                                 
1 This work has been done while the first author was visit-
ing Microsoft Research Asia. 
and Smith, 2003; Yang and Li, 2003; Shi et al, 
2006). The corpora thus obtained are generally 
of high quality and wide variety in domain, but 
the amount is still limited, as web pages that 
exhibit those structural cues are not abundant. 
Some other effort is to mine bilingual corpora 
by textual means only. That is, two pieces of 
text are decided to be parallel merely from the 
linguistic perspective, without considering any 
hint from HTML markup or website structure. 
These approaches (Zhao and Vogel, 2002; 
Utiyama and Isahara 2003; Fung and Cheung, 
2004; Munteanu and Marcu, 2005; Abdul-Rauf 
and Schwenk, 2009) share roughly the same 
framework: 
Phase 1: Document Pair Retrieval 
1) documents in some target language (TL) are 
stored in some database; 
2) each document in some source language (SL) 
is represented by some TL keywords; 
3) the TL keywords in (2) are used to assign 
some TL documents to a particular SL doc-
ument, using some information retrieval (IR) 
technique. For example, Munteanu and Mar-
cu (2005) apply the Lemur IR toolkit, 
Utiyama and Isahara (2003) use the BM25 
similarity measure, and Fung and Cheung 
(2004) use cosine similarity. Each TL docu-
ment pairs up with the SL document to form 
a candidate parallel document pair. 
Phase 2: Sentence Pair Extraction 
1) sentence pairs can be obtained by running 
sentence alignment over all candidate docu-
ment pairs (or a selection of them) (Zhao and 
Vogel, 2002; Utiyama and Isahara, 2003); 
2) sentence pairs can also be selected, by some 
classifier or reliability measure, from the 
candidate sentence pairs enumerated from 
the candidate document pairs (Munteanu and 
Marcu, 2005). 
Note that the primary interest of these ap-
proaches is sentence pairs rather than document 
474
pairs, partially because document pair retrieval 
is not accurate, and partially because the ulti-
mate purpose of these corpora is SMT training, 
which is based on sentence pairs. It is found that 
most of the sentence pairs thus obtained are not 
truly parallel; rather they are loose translations 
of each other or they carry partially similar mes-
sages. Such bilingual corpora are thus known as 
comparable corpora, while genuinely mutual 
translations constitute parallel corpora.  
Note also that all these comparable corpus 
mining approaches are tested on closed docu-
ment collections only. For example, Zhao and 
Vogel (2002), Utiyama and Isahara (2003), and 
Munteanu and Marcu (2005) all acquire their 
comparable corpora from a collection of news 
articles which are either downloaded from the 
Web or archived by LDC. The search of candi-
date document pairs in such a closed collection 
is easy in three ways:  
1) all the TL documents come from the same 
news agency and they are not mixed up with 
similar documents from other news agencies;  
2) all the TL documents are news text and they 
are not mixed up with text of other domains;  
3) in fact, the search in these approaches is 
made easier by applying tricks like date win-
dow. 
There is no evidence that these methods apply 
to corpus mining from an open document col-
lection (e.g. the entire Web) without search con-
straint. The possibility of open-ended text min-
ing is a crucial problem. 
This paper focuses on bilingual corpus min-
ing using only textual means. It attempts to an-
swer two questions: 
1) Can comparable corpus mining be applied to 
an open document collection, i.e., the Web? 
2) Can comparable corpus mining be adapted to 
parallel corpus mining? 
We give affirmation to both questions. For the 
first problem, we modify document pair 
retrieval so that there is no longer a closed set of 
TL documents. Instead we search for candidate 
TL documents for a particular SL document 
from the Web by means of some Web search 
engine. For the second problem, in Phase 2 we 
replace the sentence pair classifier by a 
document pair filter and a sentence alignment 
module. Based on end-to-end SMT experiments, 
we will show that 1) high quality bilingual 
corpora can be mined from the Web; 2) the very 
first key to Web-mining of bilingual corpus is 
the formulation of good TL keywords to 
represent a SL document; 3) a simple document 
pair filter using IBM Model 1 probabilities is 
able to identify parallel corpus out of noisy 
comparable text; and 4) Web-mined parallel 
corpus, despite its smaller size, improves SMT 
much more than Web-mined comparable corpus. 
2 Problem Setting 
Our ultimate goal is to mine from the Web 
training data for translation from Chinese (SL) 
to English (TL). As the first step, about 11,000 
Chinese web pages of news articles are crawled 
from some Chinese News sites. Then the task is 
to search for the English sentences correspond-
ing to those in the selected SL articles. These 
selected SL news articles all contain cue phrases 
like ???????? (according to foreign me-
dia), as these cue phrases suggest that the Chi-
nese articles are likely to have English counter-
parts. Moreover, each selected SL article has at 
least 500 words (empirically determined) since 
we assume that it is much easier to formulate 
reliable keywords from a long document than a 
short one. 
3 Document Pair Retrieval 
Conventional approaches to comparable corpus 
mining usually start with document pair retriev-
al, which assigns to each SL document a set of 
candidate TL documents. This step is essentially 
a preliminary search for candidate sentence 
pairs for further scrutiny in Phase 2. The target 
is to find document pairs which may contain 
many good sentence pairs, rather than to discard 
document pairs which may not contain good 
sentence pairs. Therefore, recall is much more 
emphasized than precision. 
Document pair retrieval in conventional ap-
proaches presumes a closed set of TL docu-
ments which some IR system can handle easily. 
In this paper we override this presumption and 
attempt a much more challenging retrieval task, 
viz. to search for TL documents among the Web, 
using the search engines of Google and Yahoo. 
Therefore we are subject to a much noisier data 
domain. The correct TL documents may not be 
indexed by the search engines at all, and even 
when the target documents are indexed, it re-
475
quires a more sophisticated formulation of que-
ries to retrieve them. 
In response to these challenges, we propose 
various kinds of queries (elaborated in the fol-
lowing subsections). Moreover, we merge the 
TL documents found by each query into a big 
collection, so as to boost up the recall. In case a 
query fails to retrieve any document, we itera-
tively drop a keyword in the query until some 
documents are found. On the other hand, alt-
hough the document pairs in question are of 
news domain, we use the general Google/Yahoo 
web search engines instead of the specific news 
search engines, because 1) the news search en-
gines keep only a few web pages for all pages 
about the same news event, and 2) we leave 
open possibility for correct TL documents to be 
found in non-news web pages.  
3.1 Simple Queries 
There are three baseline formulations of queries: 
1) Query of translations of SL TF-IDF-ranked 
keywords (QSL-TFIDF). This is the method 
proposed by Munteanu and Marcu (2005). 
All the words in a SL document are ranked 
by TF-IDF and the top-N words are selected. 
Each keyword is then translated into a few 
TL words by a statistically learned diction-
ary. In our experiments the dictionary is 
learned from NIST SMT training data.  
2) Query of TF-IDF-ranked machine translated 
keywords (QTL-TFIDF). It is assumed that a 
machine translation (MT) system is better at 
handling lexical ambiguity than simple dic-
tionary translation. Thus we propose to first 
translate the SL document into TL and ex-
tract the top-N TF-IDF-ranked words as 
query. In our experiments the MT system 
used is hierarchical phrase-based system 
(Chiang, 2007).2 
3) Query of named entities (QNE). Another 
way to tackle the drawback of QSL-TFIDF is to 
focus on named entities (NEs) only, since 
NEs often provide strong clue for identify-
ing correspondence between two languages. 
All NEs in a SL document are ranked by 
TF-IDF, and the top-N NEs are then trans-
lated (word by word) by dictionary. In our 
experiments we identify SL (Chinese) NEs 
                                                 
2 We also try online Google translation service, and the 
performance was roughly the same. 
implicitly found by the word segmentation 
algorithm stated in Gao et al (2003), and 
the dictionaries for translating NEs include 
the same one used for QSL-TFIDF, and the 
LDC  Chinese/English NE dictionary. For 
the NEs not covered by our dictionary, we 
use Google translation service as a back-up. 
A small-scale experiment is run to evaluate 
the merits of these queries. 300 Chinese news 
web pages in three different periods (each 100) 
are collected. For each Chinese text, each query 
(containing 10 keywords) is constructed and 
submitted to both Google and Yahoo Search, 
and top-40 returned English web pages for each 
search are kept. Note that the Chinese news ar-
ticles are not part of 11,000 pages in section 2. 
In fact, they do not only satisfy the requirement 
of length and cue phrases (described in section 
2), but they also have another property that they 
are translated from some English news articles 
(henceforth target pages) on the Web. Thus they 
are ideal data for studying the performance of 
document pair retrieval. 
To test the influence of translation quality in 
document pair retrieval, we also try ?oracle que-
ries?, i.e. queries formulated directly from the 
target pages:  
1) OQTFIDF. This is the query of the top-N TF-
IDF-ranked words from the target page. 
2) OQNE. This is the query of the top-N TF-
IDF-ranked NEs from the target web page. 
We define recall as the proportion of SL docu-
ments whose true target pages are found. The 
comparison between a retrieved page and the 
target page is done by Longest Common Subse-
quence (LCS) ratio, defined as the length of the 
longest common word sequence of two docu-
ments divided by the length of the longer of two 
documents. The threshold 0.7 is adopted as it is 
strict enough to distinguish parallel document 
pairs from non-parallel ones. 
Table 1 shows the recalls for various queries. 
It can be seen from Tests 6 and 7 that the largest 
recall, 85% (within top 40 search results), is 
achieved when the word distributions in the tar-
get web pages are known. In the real scenario 
where the true English word distribution is not 
known, the recalls achieved by the simple que-
ries are very unsatisfactory, as shown by Tests 1 
to 3. This clearly shows how challenging Web-
based mining of bilingual corpora is. Another 
challenge can be observed in comparing across 
476
columns, viz. it is much more difficult to re-
trieve outdated news document pairs. This im-
plies that bilingual news mining must be incre-
mentally carried out.  
Comparing Test 1 to Tests 2 and 3, it is obvi-
ous that QSL-TFIDF is not very useful in document 
pair retrieval. This confirms our hypothesis that 
suitable TL keywords are not likely to be ob-
tained by simple dictionary lookup. While the 
recalls by QTL-TFIDF are similar to those by QNE, 
the two queries contribute in different ways. 
Test 4 simply merges the Web search results in 
Tests 2 and 3. The significantly higher recalls in 
Test 4 imply that each of the two queries finds 
substantially different targets than each other. 
The comparison of Test 5 to Test 4 further con-
firms the weakness of QSL-TFIDF. 
The huge gap between the three simple que-
ries and the oracle queries shows that the quality 
of translation of keywords from SL to TL is a 
major obstacle. There are two problems in trans-
lation quality: 1) the MT system or dictionary 
cannot produce any translation for a SL word 
(let us refer to such TL keywords as ?Utopian 
translations?); 2) the MT system or dictionary 
produces an incorrect translation for a SL word. 
We can do very little for the Utopian transla-
tions, as the only solution is simply to use a bet-
ter MT system or a larger dictionary. On the 
contrary, it seems that the second problem can 
somewhat be alleviated, if we have a way to 
distinguish those terms that are likely to be cor-
rect translations from those terms that are not. 
In other words, it may be worthwhile to reorder 
candidate TL keywords by our confidence in its 
translation quality.  
Tests 8 and 9 in Table 1 show that this hy-
pothesis is promising. In both tests the TF-IDF-
based (Test 8) or the NE-based (Test 9) key-
words are selected from only those TL words 
that appear both in the target page and the ma-
chine translated text of the source page. In other 
words, we ensure that the keywords in the query 
must be correct translations. The recalls (espe-
cially the recalls by NE-based query in Test 9) 
are very close to the recalls by oracle queries. 
The conclusion is, even though we cannot pro-
duce the Utopian translations, document pair 
retrieval can be improved to a large extent by 
removing incorrect translations. Even an imper-
fect MT system or NE dictionary can help us 
achieve as good document pair retrieval recall 
as oracle queries.  
In the next subsection we will take this in-
sight into our bilingual data mining system, by 
selecting keywords which are likely to be cor-
rect translation.  
3.2 Re-ranked Queries 
Machine learning is applied to re-rank key-
words for a particular document. The re-ranking 
of keywords is based on two principles. The 
first one is, of course, the confidence on the 
translation quality. The more likely a keyword 
is a correct translation, the higher this keyword 
should be ranked. The second principle is the 
representativeness of document. The more rep-
resentative of the topic of the document where a 
keyword comes from, the higher this keyword 
should be ranked. The design of features should 
incorporate both principles.  
The representativeness of document is mani-
fested in the following features for each key-
word per each document: 
? TF: the term frequency. 
? IDF: the inverted document frequency. 
? TF-IDF: the product of TF and IDF. 
? Title word: it indicates whether a key-
word appears in the title of the document. 
? Bracketed word: it indicates whether a 
word is enclosed in a bracket in the 
source document. 
? Position of first appearance: the position 
where a keyword first appears in a doc-
ument, normalized by number of words 
in the document. 
ID Query Remote Near Recent 
1 QSL-TFIDF 7 6 8 
2 QTL-TFIDF 16 19 32 
3 QNE 16 21 38 
4 union(2,3) 27 31 48 
5 union(1,2,3) 28 31 48 
6 OQTFIDF 56 66 82 
7 OQNE 62 68 85 
8 OverlapTFIDF 52 51 74 
9 OverlapNE 55 62 83 
Table 1: Recall (%age) of simple queries. ?Remote? 
refers to news documents more than a year ago; 
?Near? refers to documents about 3 months ago; ?Re-
cent? refers to documents in the last two weeks. 
477
? NE types: it indicates whether a keyword 
is a person, organization, location, nu-
merical expression, or non NE. 
The confidence on translation quality is man-
ifested in the following features: 
? Translation source: it indicates whether 
the keyword (in TL) is produced by MT 
system, dictionary, or by both. 
? Original word: it indicates whether the 
keyword is originally written in English 
in the source document. Note that this 
feature also manifests the representative-
ness of a document. 
? Dictionary rank: if the keyword is a NE 
produced by dictionary, this feature indi-
cates the rank of the NE keyword among 
all translation options registered in the 
dictionary.  
It is difficult to definitely classify a TL key-
word into good or bad translation in absolute 
sense, and therefore we take the alternative of 
ranking TL keywords with respect to the two 
principles. The learning algorithm used is Rank-
ing SVM (Herbrich et al, 2000; Joachims, 
2006), which is a state-of-the-art method of the 
?Learning to rank? framework. 
The training dataset of the keyword re-ranker 
comprises 1,900 Chinese/English news docu-
ment pairs crawled from the Web3. This set is 
not part of 11,000 pages in section 2. These 
document pairs share the same properties as 
those 300 pairs used in Section 3.1. For each 
English/target document, we build a set TALL, 
which contains all words in the English docu-
ment, and also a set TNE, which is a subset of 
TALL such that all words in TNE are NEs in TALL. 
The words in both sets are ranked by TFIDF. 
On the other hand, for each Chinese/source 
document, we machine-translate it and then 
store the translated words into a set S, and we 
also add the dictionary translations of the source 
NEs into S. Note that S is composed of both 
good translations (appearing in the target docu-
ment) and bad translations (not appearing in the 
target document).  
Then there are two ways to assign labels to 
the words in S. In the first way of labeling 
(LALL), the label 3 is assigned to those words in 
S which are ranked among top 5 in TALL, label 2 
                                                 
3 We also attempt to add more training data for re-ranking 
but the performance remain the same. 
to those ranked among top 10 but not top 5 in 
TALL, 1 to those beyond top 10 but still in TALL, 
and 0 to those words which do not appear in 
TALL at all. The second way of labeling, LNE, is 
done in similar way with respect to TNE. Col-
lecting all training samples over all document 
pairs, we can train a model, MALL, based on la-
beling LALL, and another model MNE, based on 
labeling LNE. 
The trained models can then be applied to re-
rank the keywords of simple queries. In this 
case, a set STEST is constructed from the 300 
Chinese documents in similar way of construct-
ing S. We repeat the experiment in Section 3.1 
with two new queries: 
1) QRANK-TFIDF: the top N keywords from re-
ranking STEST by MALL; 
2) QRANK-NE: the top N keywords from rerank-
ing STEST by MNE. 
Again N is chosen as 10. 
The results shown in Table 2 indicate that, 
while the re-ranked queries still perform much 
poorer than oracle queries (Tests 6 and 7 in Ta-
ble 1), they show great improvement over the 
simple queries (Tests 1 to 5 in Table 1). The 
results also show that re-ranked queries based 
on NEs are more reliable than those based on 
common words. 
4 Sentence pair Extraction 
The document pairs obtained by the various 
queries described in Section 3 are used to pro-
duce sentence pairs as SMT training data. There 
are two different methods of extraction for cor-
pora of different nature. 
4.1 For Comparable Corpora 
Sentence pair extraction for comparable corpus 
is the same as that elaborated in Munteanu and 
Marcu (2005). All possible sentence pairs are 
enumerated from all candidate document pairs 
produced in Phase 1. These huge number of 
candidate sentence pairs are first passed to a 
coarse sentence pair filter, which discards very 
unlikely candidates by heuristics like sentence 
ID Query Remote Near Recent 
10 QRANK-TFIDF 18 20 29 
11 QRANK-NE 35 43 54 
12 union(10,11) 39 49 63 
Table 2: Recall (%age) of re-ranked queries. 
 
478
length ratio and percentage of word pairs regis-
tered in some dictionary. 
The remaining candidates are then given to a 
Maximum Entropy based classifier (Zhang, 
2004), which uses features based on alignment 
patterns produced by some word alignment 
model. In our experiment we use the HMM 
alignment model with the NIST SMT training 
dataset. The sentence pairs which are assigned 
as positive by the classifier are collected as the 
mined comparable corpus.  
4.2 For Parallel Corpora 
The sentence pairs obtained in Section 4.1 are 
found to be mostly not genuine mutual transla-
tions. Often one of the sentences contains some 
extra phrase or clause, or even conveys different 
meaning than the other. It is doubtful if the doc-
ument pairs from Phase 1 are too noisy to be 
processed by the sentence pair classifier. An 
alternative way for sentence pair extraction is to 
further filter the document pairs and discard any 
pairs that do not look like parallel.  
It is hypothesized that the parallel relation-
ship between two documents can be assimilated 
by the word alignment between them. The doc-
ument pair filter produces the Viterbi alignment, 
with the associated probability, of each docu-
ment pair based on IBM Model 1 (Brown et al, 
1993). The word alignment model (i.e. the sta-
tistical dictionary used by IBM Model 1) is 
trained on the NIST SMT training dataset. The 
probability of the Viterbi alignment of a docu-
ment pair is the sole basis on which we decide 
whether the pair is genuinely parallel. That is, 
an empirically determined threshold is used to 
distinguish parallel pairs from non-parallel ones. 
In our experiment, a very strict threshold is se-
lected so as to boost up the precision at the ex-
pense of recall. 
There are a few important details that enable 
the document pair filter succeed in identifying 
parallel text: 
1) Function words and other common words 
occur frequently and so any pair of common 
word occupies certain probability mass in 
an alignment model. These common words 
enable even non-parallel documents achieve 
high alignment probability. In fact, it is well 
known that the correct alignment of com-
mon words must take into account position-
al and/or structural factors, and it is benefi-
cial to a simple alignment model like IBM 
Model 1 to work on data without common 
words. Therefore, all words on a compre-
hensive stopword list must be removed 
from a document pair before word align-
ment. 
2) The alignment probability must be normal-
ized with respect to sentence length, so that 
the threshold applies to all documents re-
gardless of document length.  
Subjective evaluation on selected samples 
shows that most of the document pairs kept by 
the filter are genuinely parallel. Thus the docu-
ment pairs can be broken down into sentence 
pairs simply by a sentence alignment method. 
For the sentence alignment, our experiments use 
the algorithm in Moore (2002). 
5 Experiments 
It is a difficult task to evaluate the quality of 
automatically acquired bilingual corpora. As our 
ultimate purpose of mining bilingual corpora is 
to provide more and better training data for 
SMT, we evaluate the parallel and comparable 
corpora with respect to improvement in Bleu 
score (Papineni et al, 2002). 
5.1 Experiment Setup 
Our experiment starts with the 11,000 Chinese 
documents as described in Section 2. We use 
various combinations of queries in document 
pair retrieval (Section 3). Based on the candi-
date document pairs, we produce both compara-
ble corpora and parallel corpora using sentence 
pair extraction (Section 4). The corpora are then 
given to our SMT systems as training data. 
The SMT systems are our implementations of 
phrase-based SMT (Koehn et al, 2003) and hi-
erarchical phrase-based SMT (Chiang, 2007). 
The two systems employ a 5-gram language 
model trained from the Xinhua section of the 
Gigaword corpus. There are many variations of 
the bilingual training dataset. The B1 section of 
the NIST SMT training set is selected as the 
baseline bilingual dataset; its size is of the same 
order of magnitude as most of the mined corpo-
ra so that the comparison is fair. Each of the 
mined bilingual corpora is compared to that 
baseline dataset, and we also evaluate the per-
formance of the combination of each mined bi-
lingual corpus with the baseline set. 
479
The SMT systems learn translation knowledge 
(phrase table and rule table) in standard way. 
The parameters in the underlying log-linear 
model are trained by Minimum Error Rate 
Training (Och, 2003) on the development set of 
NIST 2003 test set. The quality of translation 
output is evaluated by case-insensitive BLEU4 
on NIST 2005 and NIST 2008 test sets4. 
5.2 Experimental result 
Table 3 lists the size of various mined parallel 
and comparable corpora against the baseline B1 
bilingual dataset. It is obvious that for a specific 
type of query in document pair retrieval, the 
parallel corpus is significantly smaller than the 
corresponding comparable corpus. 
The apparent explanation is that a lot of doc-
ument pairs are discarded due to the document 
                                                 
4 It is checked that there is no sentence in the test sets 
overlapping with any sentences in the mined corpus. 
pair filter. Note that the big difference in size of 
the two comparable corpora by single queries, 
i.e., QRANK-NE and M&M, verifies again that re-
ranked queries based on NEs are more reliable 
in sentence pair extraction. 
Table 4 lists the Bleu scores obtained by 
augmenting the baseline bilingual training set 
with the mined corpora. The most important 
observation is that, despite their smaller size, 
parallel corpora lead to no less, and often better, 
improvement in translation quality than compa-
rable corpora. That is especially true for the 
case where document pair retrieval is based on 
all five types of query5. The superiority of paral-
lel corpora confirms that, in Phase 2 (sentence 
pair extraction), quality is more important than 
quantity and thus the filtering of document 
pair/sentence pair must not be generous. 
On the other hand, sentence pair extraction 
for parallel corpora generally achieves the best 
result when all queries are applied in document 
pair retrieval. It is not sufficient to use the more 
sophisticated re-ranked queries. That means in 
Phase 1 quantity is more important and we must 
seek more ways to retrieve as many document 
pairs as possible. That also confirms the empha-
sis on recall in document pair retrieval.  
Looking into the performance of comparable 
corpora, it is observed that the M&M query 
does not effectively apply to Web mining of 
comparable corpora but the proposed queries do. 
Any of the proposed query leads to better result 
than the conventional method, i.e. M&M. 
Moreover, it can be seen that all four combina-
tions of proposed queries achieve similar per-
                                                 
5 QSL-TFIDF, QTL-TFIDF, QNE, QRANK-TFIDF, and QRANK-NE 
Queries SP 
extraction 
#SP #SL 
words 
#TL 
words 
Baseline: B1 in NIST 68K 1.7M 1.9M 
M&M comparable 43K 1.1M 1.2M 
QRANK-NE comparable 98K 2.7M 2.8M 
all simple comparable 98K 2.6M 2.9M 
all ranked comparable 115K 3.1M 3.3M 
all query comparable 135K 3.6M 4.0M 
QRANK-NE 
all simple 
parallel 
parallel 
66K 
52K 
1.9M 
1.5M 
1.8M 
1.4M 
all ranked parallel 73K 2.1M 2.0M 
all query parallel 90K 2.5M 2.4M 
Table 3: Statistics on corpus size. SP means sentence 
pair. ?all simple?, ?all ranked?, and ?all query? refer to 
the merge of the retrieval results of all simple queries, 
all re-ranked queries, and all simple and re-ranked que-
ries, respectively; M&M (after Munteanu and Marcu 
(2005)) refers to QSL-TFIDF.  
Bilingual Training Corpus 
Phrase-based SMT (PSMT) Hierarchical PSMT 
NIST 2005 NIST 2008 NIST 2005 NIST 2008 
B1 (baseline) 33.08 21.66 32.85 21.18 
B1+comparable(M&M) 33.51(+0.43) 22.71(+1.05) 32.99(+0.14) 22.11(+0.93) 
B1+comparable(QRANK-NE) 34.81(+1.73) 23.30(+1.64) 34.43(+1.58) 22.85(+1.67) 
B1+comparable(all simple) 34.74(+1.66) 23.48(+1.82) 34.28(+1.43) 23.18(+2.00) 
B1+comparable(all ranked) 34.79(+1.71) 23.48(+1.82) 34.37(+1.52) 23.06(+1.88) 
B1+comparable(all query) 34.74(+1.66) 23.19(+1.53) 34.46(+1.61) 23.12(+1.94) 
B1+parallel(QRANK-NE) 34.75(+1.67) 23.37(+1.71) 34.24(+1.39) 23.45(+2.27) 
B1+parallel(all simple) 34.99(+1.91) 23.96(+2.30) 34.94(+2.09) 23.35(+2.17) 
B1+parallel(all ranked) 34.76(+1.68) 23.41(+1.75) 34.54(+1.69) 23.59(+2.41) 
B1+parallel(all query) 35.40(+2.32) 23.47(+1.81) 35.27(+2.42) 23.61(+2.43) 
Table 4: Evaluation of translation quality improvement by mined corpora. The figures inside brackets refer 
to the improvement over baseline. The bold figures indicate the highest Bleu score in each column for 
comparable corpora and parallel corpora, respectively. 
480
formance. This illustrates a particular advantage 
of using a single re-ranked query, viz. QRANK-NE, 
because it significantly reduces the retrieval 
time and downloading space required for docu-
ment pair retrieval as it is the main bottleneck of 
whole process. 
Table 5 lists the Bleu scores obtained by re-
placing the baseline bilingual training set with 
the mined corpora. It is easy to note that transla-
tion quality drops radically by using mined bi-
lingual corpus alone. That is a natural conse-
quence of the noisy nature of Web mined data. 
We should not be too pessimistic about Web 
mined data, however. Comparing the Bleu 
scores for NIST 2005 test set to those for NIST 
2008 test set, it can be seen that the reduction of 
translation quality for the NIST 2008 set is 
much smaller than that for the NIST 2005 set. It 
is not difficult to explain the difference. Both 
the baseline B1 training set and the NIST 2005 
comprise news wire (in-domain) text only. Alt-
hough the acquisition of bilingual data also tar-
gets news text, the noisy mined corpus can nev-
er compete with the well prepared B1 dataset. 
On the contrary, the NIST 2008 test set contains 
a large portion of out-of-domain text, and so the 
B1 set does not gain any advantage over Web 
mined corpora. It might be that better and/or 
larger Web mined corpus achieves the same 
performance as manually prepared corpus.  
Note also that the reduction in Bleu score by 
each mined corpus is roughly the same as that 
by each other, while in general parallel corpora 
are slightly better than comparable corpora. 
6 Conclusion and Future Work 
In this paper, we tackle the problem of mining 
parallel sentences directly from the Web as 
training data for SMT. The proposed method 
essentially follows the corpus mining frame-
work by pioneer work like Munteanu and Mar-
cu (2005). However, unlike those conventional 
approaches, which work on closed document 
collection only, we propose different ways of 
formulating queries for discovering parallel 
documents over Web search engines. Using 
learning to rank algorithm, we re-rank keywords 
based on representativeness and translation 
quality. This new type of query significantly 
outperforms existing query formulation in re-
trieving document pairs. We also devise a doc-
ument pair filter based on IBM model 1 for 
handling the noisy result from document pair 
retrieval. Experimental results show that the 
proposed approach achieves substantial im-
provement in SMT performance. 
For mining news text, in future we plan to 
apply the proposed approach to other language 
pairs. Also, we will attempt to use meta-
information implied in SL document, such as 
?publishing date? or ?news agency name?, as 
further clue to the document pair retrieval. Such 
meta-information may likely to increase the 
precision of retrieval, which is important to the 
efficiency of the retrieval process. 
An important contribution of this work is to 
show the possibility of mining text other than 
news domain from the Web, which is another 
piece of future work. The difficulty of this task 
should not be undermined, however. Our suc-
cess in mining news text from the Web depends 
on the cue phrases available in news articles. 
These cue phrases more or less indicate the ex-
istence of corresponding articles in another lan-
guage. Therefore, to mine non-news corpus, we 
should carefully identify and select cue phrases.  
Bilingual Training Corpus 
Phrase-based SMT Hierarchical PSMT 
NIST 2005 NIST 2008 NIST 2005 NIST 2008 
B1 (baseline) 33.08 21.66 32.85 21.18 
comparable(M&M) 20.84(-12.24) 14.33(-7.33) 20.65(-12.20) 13.73(-7.45) 
comparable(QRANK-NE) 26.78(-6.30) 18.54(-3.12) 27.10(-5.75) 18.02(-3.16) 
comparable(all simple) 26.39(-6.69) 18.52(-3.14) 26.40(-6.45) 18.22(-2.96) 
comparable(all ranked) 27.36(-5.72) 18.89(-2.77) 27.40(-5.45) 18.72(-2.46) 
comparable(all query) 27.96(-5.12) 19.27(-2.39) 27.83(-5.02) 19.46(-1.72) 
parallel(QRANK-NE) 26.37(-6.71) 18.70(-2.96) 26.47(-6.38) 18.51(-2.67) 
parallel(all simple) 25.65(-7.43) 18.69(-2.97) 25.28(-7.57) 18.55(-2.63) 
parallel(all ranked) 26.86(-6.22) 18.94(-2.72) 27.10(-5.75) 18.78(-2.40) 
parallel(all query) 27.58(-5.50) 19.73(-1.93) 28.10(-4.75) 19.52(-1.66) 
Table 5: Evaluation of translation quality by mined corpora. 
481
References 
Abdul-Rauf, Sadaf and Holger Schwenk. 2009. Ex-
ploiting Comparable Corpora with TER and 
TERp. In Proceedings of ACL-IJCNLP 2009 
workshop on Building and Using Comparable 
Corpora, pages 46?54. 
Brown, Peter F., Vincent J. Della Pietra, Stephen A. 
Della Pietra, and Robert L. Mercer. 1993. The 
Mathematics of Statistical Machine Translation: 
Parameter Estimation. Computational Linguistics, 
19(2): 263-311. 
Chen, Jiang and Jian-Yun Nie. 2000. Automatic 
Construction of Parallel Chinese-English Corpus 
for Cross-Language Information Retrieval. In 
Proceedings of NAACL-ANLP, pages 21-28. 
Chiang, David. 2007. Hierarchical Phrase-based 
Translation. Computational Linguistics, 33(2): 
202-228. 
Fung, Pascale, and Percy Cheung. 2004. Mining very 
non-parallel corpora: Parallel sentence and lexi-
con extraction via bootstrapping and EM. In Pro-
ceedings of 2004 Conference on Empirical Meth-
ods in Natural Language Processing, pages 57-63. 
Gao, Jianfeng, Mu Li, and Changning Huang. 2003. 
Improved Source-Channel Models for Chinese 
Word Segmentation. In Proceedings of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics, pages 272-279. 
Herbrich, Ralf, Thore Graepel, and Klaus Obermayer. 
2000. Large margin rank boundaries for ordinal 
regression. In Advances in Large Margin Classifi-
ers, pages 115?132. MIT Press, Cambridge, MA. 
Jiang, Long, Shiquan Yang, Ming Zhou, Xiaohua 
Liu, and Qingsheng Zhu. 2009. Mining Bilingual 
Data from the Web with Adaptively Learnt Pat-
terns. In Proceedings of the 47th Annual Meeting 
of the Association for Computational Linguistics 
and 4th International Joint Conference on Natural 
Language Processing, pages 870-878. 
Joachims, Thorsten. 2006. Training Linear SVMs in 
Linear Time. In Proceedings of the 12th ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining, pages 217-226.  
Koehn, Philipp, Franz Och, and Daniel Marcu. 2003. 
Statistical Phrase-based Translation. In Proceed-
ings of conference combining Human Language 
Technology conference series and the North 
American Chapter of the Association for Compu-
tational Linguistics conference series, pages 48-
54. 
Moore, Robert. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora. In Proceedings 
of the 5th conference of the Association for Ma-
chine Translation in the Americas, pages 135?144. 
Munteanu, Dragos, and Daniel Marcu. 2005. Im-
proving Machine Translation Performance by Ex-
ploiting Non-Parallel Corpora. Computational 
Linguistics, 31(4): 477-504. 
Och, Franz J. 2003. Minimum Error Rate Training in 
Statistical Machine Translation. In Proceedings of 
the 41st Annual Meeting of the Association for 
Computational Linguistics, pages 160-167. 
Papineni, Kishore, Salim Roukos, Todd Ward, and 
Wei-Jing Zhu. 2002. BLEU: a Method for Auto-
matic Evaluation of Machine Translation. In Pro-
ceedings of the 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 311-
318. 
Resnik, Philip, and Noah Smith. 2003. The Web as a 
Parallel Corpus. Computational Linguistics, 29(3): 
349-380. 
Shi, Lei, Cheng Niu, Ming Zhou, and Jianfeng Gao. 
2006. A DOM Tree Alignment Model for Mining 
Parallel Data from the Web. In Proceedings of the 
21st International Conference on Computational 
Linguistics and the 44th Annual Meeting of the As-
sociation for Computational Linguistics, pages 
489-496. 
Utiyama, Masao, and Hitoshi Isahara. 2003. Reliable 
Measures for Aligning Japanese-English News 
Articles and Sentences. In Proceedings of the 41st 
Annual Meeting of the Association for Computa-
tional Linguistics, pages 72-79. 
Vogel, Stephan. 2003. Using noisy bilingual data for 
statistical machine translation. In Proceedings of 
the 10th Conference of the European Chapter of 
the Association for Computational Linguistics, 
pages 175-178. 
Yang, Christopher C., and Kar Wing Li. 2003. Au-
tomatic construction of English/Chinese parallel 
corpora. Journal of the American Society for In-
formation Science and Technology, 54(8):730?
742. 
Zhang, Le. 2004. Maximum Entropy Modeling 
Toolkit for Python and C++. 
http://homepages.inf.ed.ac.uk/s0450736/maxent_t
oolkit.html 
Zhao, Bing, and Stephan Vogel. 2002. Adaptive Par-
allel Sentences Mining from Web Bilingual News 
Collection. In Proceedings of IEEE international 
conference on data mining, pages 745-750. 
482
Coling 2010: Poster Volume, pages 730?738,
Beijing, August 2010
Improved Discriminative ITG Alignment using  
Hierarchical Phrase Pairs and Semi-supervised Training 
?Shujie Liu*, ?Chi-Ho Li and ?Ming Zhou 
? School of Computer Science and Technology 
Harbin Institute of Technology 
shujieliu@mtlab.hit.edu.cn 
?Microsoft Research Asia 
{chl, mingzhou}@microsoft.com  
 
Abstract 
While ITG has many desirable properties 
for word alignment, it still suffers from 
the limitation of one-to-one matching. 
While existing approaches relax this li-
mitation using phrase pairs, we propose a 
ITG formalism, which even handles units 
of non-contiguous words, using both 
simple and hierarchical phrase pairs. We 
also propose a parameter estimation me-
thod, which combines the merits of both 
supervised and unsupervised learning, 
for the ITG formalism. The ITG align-
ment system achieves significant im-
provement in both word alignment quali-
ty and translation performance. 
1 Introduction 
Inversion transduction grammar (ITG) (Wu, 
1997) is an adaptation of CFG to bilingual 
parsing. It does synchronous parsing of two 
languages with phrasal and word-level alignment 
as by-product. One of the merits of ITG is that it 
is less biased towards short-distance reordering 
compared with other word alignment models 
such as HMM. For this reason ITG has gained 
more and more attention recently in the word 
alignment community (Zhang et al, 2005; 
Cherry et al, 2006; Haghighi et al, 2009)1. 
The basic ITG formalism suffers from the ma-
jor drawback of one-to-one matching. This limi-
tation renders ITG unable to produce certain 
alignment patterns (such as many-to-many 
                                                 
* This work has been done while the first author was visit-
ing Microsoft Research Asia. 
alignment for idiomatic expression). For this 
reason those recent approaches to ITG alignment 
introduce the notion of phrase (or block), de-
fined as sequence of contiguous words, into the 
ITG formalism (Cherry and Lin, 2007; Haghighi 
et al, 2009; Zhang et al, 2008). However, there 
are still alignment patterns which cannot be cap-
tured by phrases. A simple example is connec-
tive in Chinese/English. In English, two clauses 
are connected by merely one connective (like 
"although", "because") but in Chinese we need 
two connectives (e.g. There is a sentence pattern 
"??    ??   ?     although   ", where    
and     are variables for clauses). The English 
connective should then be aligned to two non-
contiguous Chinese connectives, and such 
alignment pattern is not available in either word-
level or phrase-level ITG. As hierarchical 
phrase-based SMT (Chiang, 2007) is proved to 
be superior to simple phrase-based SMT, it is 
natural to ask, why don?t we further incorporate 
hierarchical phrase pairs (henceforth h-phrase 
pairs) into ITG? In this paper we propose a ITG 
formalism and parsing algorithm using h-phrase 
pairs. 
The ITG model involves much more parame-
ters. On the one hand, each phrase/h-phrase pair 
has its own probability or score. It is not feasible 
to learn these parameters through discrimina-
tive/supervised learning since the repertoire of 
phrase pairs is much larger than the size of hu-
man-annotated alignment set. On the other hand, 
there are also a few useful features which cannot 
be estimated merely by unsupervised learning 
like EM. Inspired by Fraser et al (2006), we 
propose a semi-supervised learning algorithm 
which combines the merits of both discrimina-
730
tive training (error minimization) and approx-
imate EM (estimation of numerous parameters). 
The ITG model augmented with the learning 
algorithm is shown by experiment results to im-
prove significantly both alignment quality and 
translation performance.  
In the following, we will explain, step-by-step, 
how to incorporate hierarchical phrase pairs into 
the ITG formalism (Section 2) and in ITG pars-
ing (Section 3). The semi-supervised training 
method is elaborated in Section 4. The merits of 
the complete system are illustrated with the ex-
periments described in Section 5. 
2 ITG Formalisms 
2.1 W-ITG : ITG with only word pairs 
The simplest formulation of ITG contains three 
types of rules: terminal unary rules  ?    , 
where e and f represent words (possibly a null 
word, ?) in the English and foreign language 
respectively, and the binary rules  ?       and 
 ?      , which refer to that the component 
English and foreign phrases are combined in the 
same and inverted order respectively. From the 
viewpoint of word alignment, the terminal unary 
rules provide the links of word pairs, whereas 
the binary rules represent the reordering factor. 
Note also that the alignment between two phrase 
pairs is always composed of the alignment 
between word pairs (c.f. Figure 1(a) and (b)). 
The Figure 1 also shows ITG can handle the 
cases where two languages share the same 
(Figure 1(a)) and different (Figure 1(b)) word 
order 
?? XX,?X  (b)
]f,e[?X  (c) f1Xf3][e1Xe3,?X  (d)
X][X,?X  ( )
e2
e1
f1 f2 f1 f2
e2
e1
e2
e1
f1 f2
e2
e1
f1 f2 f3
e3
 
Figure 1. Four ways in which ITG can analyze a 
multi-word span pair. 
Such a formulation has two drawbacks. First 
of all, the simple ITG leads to redundancy if 
word alignment is the sole purpose of applying 
ITG. For instance, there are two parses for three 
consecutive word pairs, viz.               
      and                   . The problem of re-
dundancy is fixed by adopting ITG normal form. 
The ITG normal form grammar as used in this 
paper is described in Appendix A. 
The second drawback is that ITG fails to 
produce certain alignment patterns. Its constraint 
that a word is not allowed to align to more than 
one word is indeed a strong limitation as no 
idiom or multi-word expression is allowed to 
align to a single word on the other side. 
Moreover, its reordering constraint makes it 
unable to produce the ?inside-out? alignment 
pattern (c.f. Figure 2). 
f1      f2      f3      f4
e1     e2      e3      e4 
Figure 2. An example of inside-out alignment. 
2.2 P-ITG : ITG with Phrase Pairs 
A single word in one language is not always on a 
par with a single word in another language. For 
example, the Chinese word "??" is equivalent 
to two words in English ("white house"). This 
problem is even worsened by segmentation er-
rors (i.e. splitting a single word into more than 
one word). The one-to-one constraint in W-ITG 
is a serious limitation as in reality there are al-
ways segmentation or tokenization errors as well 
as idiomatic expressions. Therefore, researches 
like Cherry and Lin (2007), Haghighi et al 
(2009) and Zhang et al (2009) tackle this prob-
lem by enriching ITG, in addition to word pairs, 
with pairs of phrases (or blocks). That is, a se-
quence of source language word can be aligned, 
as a whole, to one (or a sequence of more than 
one) target language word. 
These methods can be subsumed under the 
term phrase-based ITG (P-ITG), which enhances 
W-ITG by altering the definition of a terminal 
production to include phrases:   ?      (c.f. 
Figure 1(c)).    stands for English phrase and 
   stands for foreign phrase. As an example, if 
there is a simple phrase pair <white house, ?
731
?>, then it is transformed into the ITG rule 
 ?  white house   ?? . 
An important question is how these phrase 
pairs can be formulated. Marcu and Wong (2002) 
propose a joint probability model which searches 
the phrase alignment space, simultaneously 
learning translations lexicons for words and 
phrases without consideration of potentially sub-
optimal word alignments and heuristic for phrase 
extraction. This method suffers from computa-
tional complexity because it considers all possi-
ble phrases and all their possible alignments. 
Birch et al (2006) propose a better and more 
efficient method of constraining the search space 
which does not contradict a given high confi-
dence word alignment for each sentence. Our P-
ITG collects all phrase pairs which are consistent 
with a word alignment matrix produced by a 
simpler word alignment model. 
2.3 HP-ITG : P-ITG with H-Phrase pairs 
P-ITG is the first enhancement of ITG to capture 
the linguistic phenomenon that more than one 
word of a language may function as a single unit, 
so that these words should be aligned to a single 
unit of another language. But P-ITG can only 
treat contiguous words as a single unit, and 
therefore cannot handle the single units of non-
contiguous words. Apart from sentence 
connectives as mentioned in Section 1, there is 
also the example that the single word ?since? in 
English corresponds to two non-adjacent words "
?" and "??" as shown the following sentence 
pair: 
?  ???  ?? ? ? ?? ? ?? . 
I have been ill since last weekend . 
No matter whether it is P-ITG or phrase-based 
SMT, the very notion of phrase pair is not help-
ful because this example is simply handled by 
enumerating all possible contiguous sequences 
involving the words "?" and "??", and thus 
subject to serious data sparseness. The lesson 
learned from hierarchical phrase-based SMT is 
that the modeling of non-contiguous word se-
quence can be very simple if we allow rules in-
volving h-phrase pairs, like: 
  ?  since    ?   ??  
where   is a placeholder for substituting a 
phrase pair like "???/last weekend". 
H-phrase pairs can also perform reordering, as 
illustrated by the well-known example from 
Chiang (2007),  ?   have    with        ?    
?     , for the following bilingual sentence 
fragment: 
?  ??  ?  ?? 
have diplomatic relations with North Korea 
The potential of intra-phrase reordering may also 
help us to capture those alignment patterns like 
the ?inside-out? pattern. 
All these merits of h-phrase pairs motivate a 
ITG formalism, viz. hierarchical phrase-based 
ITG (HP-ITG), which employs not only simple 
phrase pairs but also hierarchical ones. The ITG 
grammar is enriched with rules of the format: 
 ?      where   and    refer to either a phrase 
or h-phrase (c.f. Figure 1(d)) pair in English and 
foreign language respectively 2 . Note that, al-
though the format of HP-ITG is similar to P-ITG, 
it is much more difficult to handle rules with h-
phrase pairs in ITG parsing, which will be elabo-
rated in the next section. 
It is again an important question how to for-
mulate the h-phrase pairs. Similar to P-ITG, the 
h-phrase pairs are obtained by extracting the h-
phrase pairs which are consistent with a word 
alignment matrix produced by some simpler 
word alignment model. 
3 ITG Parsing 
Based on the rules, W-ITG word alignment is 
done in a similar way to chart parsing (Wu, 
1997). The base step applies all relevant terminal 
unary rules to establish the links of word pairs. 
The word pairs are then combined into span 
pairs in all possible ways. Larger and larger span 
pairs are recursively built until the sentence pair 
is built. 
Figure 3(a) shows one possible derivation for 
a toy example sentence pair with three words in 
each sentence. Each node (rectangle) represents 
a pair, marked with certain phrase category, of 
                                                 
2 Haghighi et al (2009) impose some rules which look like 
h-phrase pairs, but their rules are essentially h-phrase pairs 
with at most one ? ? only, added with the constraint that 
each ? ? covers only one word. 
732
foreign span (F-span) and English span (E-span) 
(the upper half of the rectangle) and the asso-
ciated alignment hypothesis (the lower half). 
Each graph like Figure 3(a) shows only one de-
rivation and also only one alignment hypothesis. 
The various derivations in ITG parsing can be 
compactly represented in hypergraph (Klein et 
al., 2001) like Figure 3(b). Each hypernode (rec-
tangle) comprises both a span pair (upper half) 
and the list of possible alignment hypotheses 
(lower half) for that span pair. The hyperedges 
show how larger span pairs are derived from 
smaller span pairs. Note that hypernode may 
have more than one alignment hypothesis, since 
a hypernode may be derived through more than 
one hyperedge (e.g. the topmost hypernode in 
Figure 3(b)). Due to the use of normal form, the 
hypotheses of a span pair are different from each 
other. 
In the case of P-ITG parsing, each span pair 
does not only examine all possible combinations 
of sub-span pairs using binary rules, but also 
checks if the yield of that span pair is exactly the 
same as that phrase pair. If so, then this span pair 
is treated as a valid leaf node in the parse tree. 
Moreover, in order to enable the parse tree pro-
duce a complete word aligned matrix as by-
product, the alignment links within the phrase 
pair (which are recorded when the phrase pair is 
extracted from a word aligned matrix produced 
by a simpler model) are taken as an alternative 
alignment hypothesis of that span pair. 
In the case of HP-ITG parsing, an ITG rule 
like  ?  have    with      ?   ?     (ori-
ginated from the hierarchical rule like  ?  <?
    ?   , have    with   >), is processed in the 
following manner: 1) Each span pair checks if it 
contains the lexical anchors: "have", "with","?" 
and "?"; 2) each span pair checks if the remain-
ing words in its yield can form two sub-span 
pairs which fit the reordering constraint among 
   and    (Note that span pairs of any category 
in the ITG normal form grammar can substitute 
for    or   ). 3) If both conditions hold, then the 
span pair is assigned an alignment hypothesis 
which combines the alignment links among the 
lexical anchors and those links among the sub-
span pairs. 
C:[e3,e3]/[f3,f3]
{e3/f3}
C:[e1,e2]/[f1,f2]
{e1/f2,e1/f1,
e2/f1,e2/f2}
A:[e1,e3]/[f1,f3]
{e1/f2,e1/f1,e2/f1,e2/f2,e3/f3} ,
 {e1/f1,e1/f3,e3/f1,e3/f3,e2,f2}
{e2/f2}
e1Xe3/f1Xf3:
[e1Xe3]/[f1Xf3]
{e1/f3,e1/f1,
e3/f3,e3/f1}
C:[e2,e2]/[f2,f2]
(c) 
e1               e2              e3
f1                f2               f3
(a) (b) 
e1               e2              e3
f1                f2               f3
A?[C,C] A?[e1Xe3/f1Xf3,C]
 
Figure 4. Phrase/h-phrase in hypergraph. 
 Figure 4(c) shows an example how to use 
phrase pair and h-phrase pairs in hypergraph.  
Figure 4(a) and  Figure 4(b) refer to alignment 
matrixes which cannot be generated by W-ITG, 
because of the one-to-one assumption.  Figure 
4(c) shows how the span pair [e1,e3]/[f1,f3] can 
be generated in two ways: one is combining a 
phrase pair and a word pair directly, and the oth-
er way is replacing the X in the h-phrase pair 
with a word pair. Here we only show how h-
phrase pairs with one variable be used during the 
B:[e1,e2]/[f1,f2]
{e1/f2,e2/f1}
C:[e1,e1]/[f2,f2]
{e1/f2}
C:[e2,e2]/[f1,f1]
{e2/f1}
C:[e3,e3]/[f3,f3]
{e3/f3}
A:[e1,e3]/[f1,f3]
{e1/f2,e2/f1,e3/f3}
(a) 
C:[e2,e2]/[f2,f2]
{e2/f2}
C:[e1,e1]/[f1,f1]
{e1/f1}
C:[e3,e3]/[f3,f3]
{e3/f3}
C:[e2,e2]/[f1,f1]
{e2/f1}
C:[e1,e1]/[f2,f2]
{e1/f2}
B:[e1,e2]/[f1,f2]
{e1/f2}
A:[e1,e2]/[f1,f2]
{e2/f2}
A:[e1,e3]/[f1,f3]
{e1/f2,e2/f1,e3/f3} , 
{e1/f1,e2/f2,e3,f3}
(b)
B?<C,C> A?[C,C]
A?[A,C]A?[B,C]
 
Figure 3.  Example ITG parses in graph (a) and hypergraph (b). 
733
parsing, and h-phrase pairs with more than one 
variable can be used in a similar way. 
The original (unsupervised) ITG algorithm 
has complexity of O(n6). When extended to su-
pervised/discriminative framework, ITG runs 
even more slowly. Therefore all attempts to ITG 
alignment come with some pruning method. 
Zhang and Gildea (2005) show that Model 1 
(Brown et al, 1993) probabilities of the word 
pairs inside and outside a span pair are useful.  
Tic-tac-toe pruning algorithm (Zhang and Gildea, 
2005) uses dynamic programming to compute 
inside and outside scores for a span pair in O(n4). 
Tic-tac-toe pruning method is adopted in this 
paper. 
4 Semi-supervised Training 
The original formulation of ITG (W-ITG) is a 
generative model in which the ITG tree of a sen-
tence pair is produced by a set of rules. The pa-
rameters of these rules are trained by EM. Cer-
tainly it is difficult to add more non-independent 
features in such a generative model, and there-
fore Cherry et al (2006) and Haghighi et al 
(2009) used a discriminative model to incorpo-
rate features to achieve state-of-art alignment 
performance. 
4.1 HP-DITG : Discriminative HP-ITG 
We also use a discriminative model to assign 
score to an alignment candidate for a sentence 
pair (     ) as probability from a log-linear model 
(Liu et al, 2005; Moore, 2006): 
          
                    
                           
 (1) 
where each           is some feature about the 
alignment matrix, and each ? is the weight of the 
corresponding feature. The discriminative 
version of W-ITG, P-ITG, and HP-ITG are then 
called W-DITG, P-DITG, and HP-DITG 
respectively. 
There are two kinds of parameters in (1) to be 
learned. The first is the values of the features ?. 
Most features are indeed about the probabilities 
of the phrase/h-phrase pairs and there are too 
many of them to be trained from a labeled data 
set of limited size. Thus the feature values are 
trained by approximate EM. The other kind of 
parameters is feature weights ?, which are 
trained by an error minimization method. The 
discriminative training of ? and the approximate 
EM training of ? are integrated into a semi-
supervised training framework similar to EMD3 
(Fraser and Marcu, 2006). 
4.2 Discriminative Training of ? 
MERT (Och, 2003) is used to train feature 
weights ?. MERT estimates model parameters 
with the objective of minimizing certain measure 
of translation errors (or maximizing certain 
performance measure of translation quality) for a 
development corpus. Given an SMT system 
which produces, with model parameters   
 , the 
K-best candidate translations        
   for a 
source sentence   , and an error measure 
           of a particular candidate      with 
respect to the reference translation   , the 
optimal parameter values will be: 
   
        
  
 
             
   
 
   
  
       
  
 
                     
        
 
   
 
   
  
MERT for DITG applies the same equation 
for parameter tuning, with different interpreta-
tion of the components in the equation. Instead 
of a development corpus with reference transla-
tions, we have a collection of training samples, 
each of which is a sentence pair with annotated 
alignment result. The ITG parser outputs for 
each sentence pair a K-best list of alignment re-
sult        
   based on the current parameter 
values   
 . The MERT module for DITG takes 
alignment F-score of a sentence pair as the per-
formance measure. Given an input sentence pair 
and the reference annotated alignment, MERT 
aims to maximize the F-score of DITG-produced 
alignment.  
4.3 Approximate EM Training of ?  
Three kinds of features (introduced in section 
4.5 and 4.6) are calculated from training corpus 
given some initial alignment result: conditional 
probability of word pairs and two types of 
conditional probabilities for phrase/h-phrase. 
                                                 
3 For simplicity, we will also call our semi-supervised 
framework as EMD. 
734
The initial alignment result is far from perfect 
and so the feature values thus obtained are not 
optimized. There are too many features to be 
trained in supervised way. So, unsupervised 
training like EM is the best solution. 
When EM is applied to our model, the E-step 
corresponds to calculating the probability for all 
the ITG trees, and the M-step corresponds to re-
estimate the feature values. As it is intractable to 
handle all possible ITG trees, instead we use the 
Viterbi parse to update the feature values. In 
other words, the training is a kind of approx-
imate EM rather than EM. 
Word pairs are collected over Viterbi align-
ment and their conditional probabilities are esti-
mated by MLE. As to phrase/h-phrase, if they 
are handled in a similar way, then there will be 
data sparseness (as there are much fewer 
phrase/h-phrase pairs in Viterbi parse tree than 
needed for reliable parameter estimation). Thus, 
we collect all phrase/h-phrase pairs which are 
consistent with the alignment links. The condi-
tional probabilities are then estimated by MLE. 
4.4 Semi-supervised training 
Algorithm EMD (semi-supervised training) 
input development data dev, test data test, training 
data with initial alignment (train, align_train) 
output feature weights   and features . 
1: estimate initial features    with (train, align_train) 
2: get an initial weights    by MERT with the initial 
features   on dev. 
3: get the F-Measure    for          on test. 
4: for( =1;;  ++) 
5:  get the Viterbi alignment align_train for train 
using      and     
6:  estimate    with (train, align_train) 
7:  get new feature weights    by MERT with    
on dev. 
8:  get the F-Measure    for          on test. 
9:  if             then 
10:   break 
11: end for 
12: return      and     
Figure 5. Semi-supervised training for HP-DITG. 
The discriminative training (error minimiza-
tion) of feature weights   and the approximate 
EM learning of feature values  are integrated in 
a single semi-supervised framework. Given an 
initial estimation of  (estimated from an initial 
alignment matrix by some simpler word align-
ment model) and an initial estimation of  , the 
discriminative training process and the approx-
imate EM learning process are alternatively ite-
rated until there is no more improvement. The 
sketch of the semi-supervised training is shown 
in Figure 5. 
4.5 Features for word pairs 
The following features about alignment link are 
used in W-DITG: 
1) Word pair translation probabilities 
trained from HMM model (Vogel et al, 
1996) and IBM model 4 (Brown et al, 
1993). 
2) Conditional link probability (Moore, 
2006). 
3) Association score rank features (Moore et 
al., 2006). 
4) Distortion features: counts of inversion 
and concatenation. 
4.6 Features for phrase/h-phrase pairs 
For our HP-DITG model, the rule probabilities 
in both English-to-foreign and foreign-to-
English directions are estimated and taken as 
features, in addition to those features in W-
DITG, in the discriminative model of alignment 
hypothesis selection:  
1)           : The conditional probability of 
English phrase/h-phrase given foreign 
phrase/h-phrase. 
2)           : The conditional probability of 
foreign phrase/h-phrase given English 
phrase/h-phrase. 
The features are calculated as described in 
section 4.3. 
5 Evaluation 
Our experiments evaluate the performance of 
HP-DITG in both word alignment and transla-
tion in a Chinese-English setting, taking GI-
ZA++, BerkeleyAligner (henceforth BERK) 
(Haghighi, et al, 2009), W-ITG as baselines. 
Word alignment quality is evaluated by recall, 
precision, and F-measure, while translation per-
formance is evaluated by case-insensitive 
BLEU4. 
5.1 Experiment Data 
The small human annotated alignment set for 
discriminative training of feature weights is the 
same as that in Haghighi et al (2009). The 491 
735
sentence pairs in this dataset are adapted to our 
own Chinese word segmentation standard. 250 
sentence pairs are used as training data and the 
other 241 are test data. The large, un-annotated 
bilingual corpus for approximate EM learning of 
feature values is FBIS, which is also the training 
set for our SMT systems. 
In SMT experiments, our 5-gram language 
model is trained from the Xinhua section of the 
Gigaword corpus. The NIST?03 test set is used 
as our development corpus and the NIST?05 and 
NIST?08 test sets are our test sets.  We use two 
kinds of state-of-the-art SMT systems. One is a 
phrase-based decoder (PBSMT) with a MaxEnt-
based distortion model (Xiong, et al, 2006), and 
the other is an implementation of hierarchical 
phrase-based model (HPBSMT) (Chiang, 2007). 
The phrase/rule table for these two systems is 
not generated from the terminal node of HP-
DITG tree directly, but extracted from word 
alignment matrix (HP-DITG generated) using 
the same criterion as most phrase-based systems 
(Chiang, 2007). 
5.2 HP-DITG without EMD 
Our first experiment isolates the contribution of 
the various DITG alignment models from that of 
semi-supervised training. The feature values of 
the DITG models are estimated simply from 
IBM Model 4 using GIZA++. Apart from DITG, 
P-ITG, and HP-ITG as introduced in Section 2, 
we also include a variation, known as H-DITG, 
which covers h-phrase pairs but no simple 
phrase pairs at all. The experiment results are 
shown in Table 1. 
 Precision Recall F-Measure 
GIZA++ 0.826 0.807 0.816 
BERK 0.917 0.814 0.862 
W-DITG 0.912 0.745 0.820 
P-DITG 0.913 0.788 0.846 
H-DITG 0.913 0.781 0.842 
HP-DITG 0.915 0.795 0.851 
Table 1. Performance gains with features for 
HP-DITG. 
It is obvious that any form of ITG achieves 
better F-Measure than GIZA++. Without semi-
supervised training, however, our various DITG 
models cannot compete with BERK. Among the 
DITG models, it can be seen that precision is 
roughly the same in all cases, while W-ITG has 
the lowest recall, due to the limitation of one-to-
one matching. The improvement by (simple) 
phrase pairs is roughly the same as that by h-
phrase pairs. And it is not surprising that the 
combination of both kinds of phrases achieve the 
best result. 
Even HP-DITG does not achieve as high recall 
as BERK, it does produce promising alignment 
patterns that BERK fails to produce. For in-
stance, for the following sentence pair: 
?  ???  ?? ? ? ?? ? ?? . 
I have been ill since last weekend . 
Both GIZA++ and BERK produce the pattern 
in Figure 6(a), while HP-DITG produces the bet-
ter pattern in Figure 6(b) as it learns the h-phrase 
pair  since     ?   ?? . 
(b): HP-DITG
?           ???        ??
since          last       weekend
?           ???        ??
since          last       weekend
(a): BERK/Giza++
 
Figure 6. Partial alignment results. 
5.3 Alignment Quality of HP-DITG with 
EMD 
 Precision Recall F- Measure 
GIZA++ 0.826 0.807 0.816 
BERK 0.917 0.814 0.862 
EMD0 0.915 0.795 0.851 
EMD1 0.923 0.814 0.865 
EMD2 0.930 0.821 0.872 
EMD3 0.935 0.819 0.873 
Table 2. Semi-supervised Training Task on F-
Measure. 
The second experiment evaluates how the 
semi-supervised method of EMD improves HP-
DITG with respect to word alignment quality. 
The results are shown in Table 2. In the table, 
EMD0 refers to the HP-DITG model before any 
EMD training; EMD1 refers to the model after 
the first iteration of training, and so on. It is em-
pirically found that F-Measure is not improved 
after the third EMD iteration. 
It can be observed that EMD succeeds to help 
HP-DITG improves feature value and weight 
estimation iteratively. When semi-supervised 
736
training converges, the new HP-DITG model is 
better than before training by 2%, and better than 
BERK by 1%. 
5.4 Translation Quality of HP-DITG with 
EMD 
The third experiment evaluates the same 
alignment models in the last experiment but with 
respect to translation quality, measured by case-
insensitive BLEU4. The results are shown in 
Table 3. Note that the differences between 
EMD3 and the two baselines are statistically 
significant. 
 PBSMT HPBSMT 
05 08 05 08 
GIZA++ 33.43 23.89 33.59 24.39 
BERK 33.76 24.92 34.22 25.18 
EMD0 34.02 24.50 34.30 24.90 
EMD1 34.29 24.80 34.77 25.25 
EMD2 34.25 25.01 35.04 25.43 
EMD3 34.42 25.19 34.82 25.56 
Table 3. Semi-supervised Training Task on 
BLEU. 
It can be observed that EMD improves SMT 
performance in most iterations in most cases. 
EMD does not always improve BLEU score be-
cause the objective function of the discrimina-
tive training in EMD is about alignment F-
Measure rather than BLEU. And it is well 
known that the correlation between F-Measure 
and BLEU (Fraser and Marcu, 2007) is itself an 
intriguing problem. 
The best HP-DITG leads to more than 1 
BLEU point gain compared with GIZA++ on all 
datasets/MT models. Compared with BERK, 
EMD3 improves SMT performance significantly 
on NIST05 and slightly on NIST08. 
6 Conclusion and Future Work 
In this paper, we propose an ITG formalism 
which employs the notion of phrase/h-phrase 
pairs, in order to remove the limitation of one-to-
one matching. The formalism is proved to enable 
an alignment model to capture the linguistic fact 
that a single concept is expressed in several non-
contiguous words. Based on the formalism, we 
also propose a semi-supervised training method 
to optimize feature values and feature weights, 
which does not only improve the alignment qual-
ity but also machine translation performance 
significantly. Combining the formalism and 
semi-supervised training, we obtain better 
alignment and translation than the baselines of 
GIZA++ and BERK. 
A fundamental problem of our current frame-
work is that we fail to obtain monotonic incre-
ment of BLEU score during the course of semi-
supervised training. In the future, therefore, we 
will try to take the BLEU score as our objective 
function in discriminative training. That is to 
certain extent inspired by Deng et al (2008).  
Appendix A. The Normal Form Grammar 
Table 4 lists the ITG rules in normal form as 
used in this paper, which extend the normal form 
in Wu (1997) so as to handle the case of 
alignment to null. 
1     ?       
2     ?                                    
3     ?                         
     ?             
4     ?            
5     ?           
6    ?     
7     ?       ?     
8    ?                ?             
9    ?             ?          
Table 4. ITG Rules in Normal Form. 
In these rules,   is the Start symbol;   is the 
category for concatenating combination whereas 
  for inverted combination. Rules (2) and (3) are 
inherited from Wu (1997). Rules (4) divide the 
terminal category   into subcategories. Rule 
schema (6) subsumes all terminal unary rules for 
some English word   and foreign word  , and 
rule schemas (7) are unary rules for alignment to 
null. Rules (8) ensure all words linked to null are 
combined in left branching manner, while rules 
(9) ensure those words linked to null combine 
with some following, rather than preceding, 
word pair. (Note: Accordingly, all sentences 
must be ended by a special token      , other-
wise the last word(s) of a sentence cannot be 
linked to null.) If there are both English and for-
eign words linked to null, rule (5) ensures that 
those English words linked to null precede those 
foreign words linked to null. 
737
References 
Birch, Alexandra, Chris Callison-Burch, Miles Os-
borne and Phillipp Koehn. 2006. Constraining the 
Phrase-Based, Joint Probability Statistical Transla-
tion Model. Proceedings of the Workshop on Sta-
tistical Machine Translation. 
Brown, Peter F. Brown, Stephen A. Della Pietra, 
Vincent J. Della Peitra, Robert L. Mercer. 1993. 
The Mathematics of Statistical Machine Transla-
tion: Parameter Estimation. Computational Lin-
guistics, 19(2):263-311. 
Cherry, Colin and Dekang Lin. 2006. Soft Syntactic 
Constraints for Word Alignment through Discri-
minative Training. Proceedings of the 21st Interna-
tional Conference on Computational Linguistics 
and 44th Annual Meeting of the Association for 
Computational Linguistics.  
Cherry, Colin and Dekang Lin. 2007. Inversion 
Transduction Grammar for Joint Phrasal Transla-
tion Modeling. Proceedings of the Second Work-
shop on Syntax and Structure in Statistical Trans-
lation, Pages:17-24.  
Chiang, David. 2007. Hierarchical Phrase-based 
Translation. Computational Linguistics, 33(2). 
Deng, Yonggang, Jia Xu and Yuqing Gao. 2008. 
Phrase Table Training For Precision and Recall: 
What Makes a Good Phrase and a Good Phrase 
Pair?. Proceedings of the 7th International Confe-
rence on Human Language Technology Research 
and 46th Annual Meeting of the Association for 
Computational Linguistics, Pages:1017-1026. 
Fraser, Alexander, Daniel Marcu. 2006. Semi-
Supervised Training for StatisticalWord Align-
ment. Proceedings of the 21st International Confe-
rence on Computational Linguistics and 44th An-
nual Meeting of the Association for Computational 
Linguistics, Pages:769-776. 
Fraser, Alexander, Daniel Marcu. 2007. Measuring 
Word Alignment Quality for Statistical Machine 
Translation. Computational Linguistics, 33(3). 
Haghighi, Aria, John Blitzer, John DeNero, and Dan 
Klein. 2009. Better Word Alignments with Super-
vised ITG Models. Proceedings of the Joint Confe-
rence of the 47th Annual Meeting of the ACL and 
the 4th International Joint Conference on Natural 
Language, Pages: 923-931. 
Klein, Dan and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. Proceedings of the 7th In-
ternational Workshop on Parsing Technologies, 
Pages:17-19 
Liu, Yang, Qun Liu and Shouxun Lin. 2005. Log-
linear models for word alignment. Proceedings of 
the 43rd Annual Meeting of the Association for 
Computational Linguistics, Pages: 81-88. 
Marcu, Daniel, William Wong. 2002. A Phrase-Based, 
Joint Probability Model for Statistical Machine 
Translation. Proceedings of 2002 Conference on 
Empirical Methods in Natural Language 
Processing, Pages:133-139. 
Moore, Robert, Wen-tau Yih, and Andreas Bode. 
2006. Improved Discriminative Bilingual Word 
Alignment. Proceedings of the 44rd Annual Meet-
ing of the Association for Computational Linguis-
tics, Pages: 513-520. 
Och, Franz Josef. 2003. Minimum error rate training 
in statistical machine translation. Proceedings of 
the 41rd Annual Meeting of the Association for 
Computational Linguistics, Pages:160-167. 
Och, Franz Josef and Hermann Ney. 2004. The 
Alignment Template Approach to Statistical Ma-
chine Translation. Computational Linguistics, 
30(4) : 417-449. 
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in sta-
tistical translation. Proceedings of 16th Interna-
tional Conference on Computational Linguistics, 
Pages: 836-841. 
Wu, Dekai. 1997. Stochastic Inversion Transduction 
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3). 
Xiong, Deyi, Qun Liu and Shouxun Lin. 2006. Max-
imum entropy based phrase reordering model for 
statistical machine translation. Proceedings of the 
44rd Annual Meeting of the Association for Com-
putational Linguistics, Pages: 521-528. 
Zhang, Hao and Daniel Gildea. 2005. Stochastic Lex-
icalized Inversion Transduction Grammar for 
Alignment. Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics.  
Zhang, Hao, Chris Quirk, Robert Moore, and Daniel 
Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. 
Proceedings of the 46rd Annual Meeting of the As-
sociation for Computational Linguistics, Pages: 
314-323. 
738
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 854?862, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Re-training Monolingual Parser Bilingually for Syntactic SMT 
 
?
Shujie Liu*, ?Chi-Ho Li, ?Mu Li and ?Ming Zhou 
?School of Computer Science and Technology 
Harbin Institute of Technology, Harbin, China 
shujieliu@mtlab.hit.edu.cn 
?Microsoft Research Asia, Beijing, China 
{chl, muli, mingzhou}@microsoft.com 
 
 
Abstract 
The training of most syntactic SMT approaches 
involves two essential components, word 
alignment and monolingual parser. In the 
current state of the art these two components 
are mutually independent, thus causing 
problems like lack of rule generalization, and 
violation of syntactic correspondence in 
translation rules. In this paper, we propose two 
ways of re-training monolingual parser with the 
target of maximizing the consistency between 
parse trees and alignment matrices. One is 
targeted self-training with a simple evaluation 
function; the other is based on training data 
selection from forced alignment of bilingual 
data. We also propose an auxiliary method for 
boosting alignment quality, by symmetrizing 
alignment matrices with respect to parse trees. 
The best combination of these novel methods 
achieves 3 Bleu point gain in an IWSLT task 
and more than 1 Bleu point gain in NIST tasks. 
1 Introduction 
There are many varieties in syntactic statistical 
machine translation (SSMT). Apart from a few 
attempts to use synchronous parsing to produce the 
tree structure of both source language (SL) and 
target language (TL) simultaneously, most SSMT 
approaches make use of monolingual parser to 
produce the parse tree(s) of the SL and/or TL 
sentences, and then link up the information of the 
two languages through word alignment. In the 
current state of the art, word aligner and 
monolingual parser are trained and applied 
separately. On the one hand, an average word 
aligner does not consider the syntax information of 
both languages, and the output links may violate 
syntactic correspondence. That is, some SL words 
yielded by a SL parse tree node may not be traced 
to, via alignment links, some TL words with 
legitimate syntactic structure. On the other hand, 
parser design is a monolingual activity and its 
impact on MT is not well studied (Ambati, 2008). 
Many good translation rules may thus be filtered 
by a good monolingual parser. 
In this paper we will focus on the translation 
task from Chinese to English, and the string-to-tree 
SSMT model as elaborated in (Galley et al2006). 
There are two kinds of translation rules in this 
model, minimal rules, and composed rules, which 
are composition of minimal rules. The minimal 
rules are extracted from a special kind of nodes, 
known as frontier nodes, on TL parse tree. The 
concept of frontier node can be illustrated by 
Figure 1, which shows two partial bilingual 
sentences with the corresponding TL sub-trees and 
word alignment links. The TL words yielded by a 
TL parse node can be traced to the corresponding 
SL words through alignment links. In the diagram, 
each parse node is represented by a rectangle, 
showing the phrase label, span, and complement 
span respectively. The span of a TL node   is 
defined as the minimal contiguous SL string that 
covers all the SL words reachable from  . The 
complement span of   is the union of spans of all 
the nodes that are neither descendants nor 
ancestors of   (c.f. Galley et al2006) . A frontier 
node is a node of which the span and the 
complement span do not overlap with each other. 
In the diagram, frontier nodes are grey in color. 
Frontier node is the key in the SSMT model, as it 
identifies the bilingual information which is 
consistent with both the parse tree and alignment 
matrix. 
There are two major problems in the SSMT 
model. The first one is the violation of syntactic 
854
structure by incorrect alignment links, as shown by 
the two dashed links in Figure 1(a). These two 
incorrect links hinder the extraction of a good 
minimal rule ???            ? and that of a 
good composed rule ??? , ?   NP(DT(the), 
NN(herdsmen), POS('s)) ?. By and large, incorrect 
alignment links lead to translation rules that are 
large in size, few in number, and poor in 
generalization ability (Fossum et al008). The 
second problem is parsing error, as shown in 
Figure 1(b). The incorrect POS tagging of the word 
?lectures" causes a series of parsing errors, 
including the absence of the noun phrase 
?NP(NN(propaganda), NN(lectures))?. These 
parsing errors hinder the extraction of good rules, 
such as ? ? ?   NP(NN(propaganda), 
NN(lectures)) ?. 
Note that in Figure 1(a), the parse tree is correct, 
and the incorrect alignment links might be fixed if 
the aligner takes the parse tree into consideration. 
Similarly, in Figure 1(b) some parsing errors might 
be fixed if the parser takes into consideration the 
correct alignment links about ?propaganda? and 
?lecture?. That is, alignment errors and parsing 
might be fixed if word aligner and parser are not 
mutually independent.  
In this paper, we emphasize more on the 
correction of parsing errors by exploiting 
alignment information. The general approach is to 
re-train a parser with parse trees which are the 
most consistent with alignment matrices. Our first 
strategy is to apply the idea of targeted self-
training (Katz-Brown et al2011) with the simple 
evaluation function of frontier set size. That is to 
re-train the parser with the parse trees which give 
rise to the largest number of frontier nodes. The 
second strategy is to apply forced alignment 
(Wuebker et al2010) to bilingual data and select 
the parse trees generated by our SSMT system for 
re-training the parser. Besides, although we do not 
invent a new word aligner exploiting syntactic 
information, we propose a new method to 
symmetrize the alignment matrices of two 
directions by taking parse tree into consideration.  
6
1-6
NNS
6
1-6
POS
4
1-3,5-6
NNIN
3
1-2,4-6
lived
1    
 in
2 
     the
3  
 herdsmen
4 
?s
 5 
  yurts
6  
   at
 7 
   night
8  
??
1
           ??
2
          ?
3
         ??
4
        ?
5
       ??
6
 
2 3 null1 4 6 6 6 1
VDB
2
1,3-6
DT
null1
1-6
6
1-6
IN
1
2-6
NN
1
2-6
NP
4-6
1-3,6
NP
4-6
1-3,6
NP
3-6
1-6
PP
1-6
2-6
PP
1-6
----
VP
a
1
large
2 
number
3
 of
4 
people
5 
coming
6
 to
7
 listen
8
 to
9
 their
10
 propaganda
11
 lectures
12
?
1
               ?
2
            ??
3
           ??
4
                ?
5
                 ?
6
               ??
7
7 7 5 6 1 4 4null null null null 3
null
1-7
DT
7
1-7
JJ
7
1-7
NN
5
1-4,6-7
IN
6
1-5,7
NNS
1
3-7
VBG
null
1-7
TO
null
1-7
VB
null
1-7
TO
3
1,4-7
PRP
4
1-7
NN
4
1-7
VP
7
1-6
NP
3-4
1,4-7
NP
3-4
1,4-7
PP
3-4
1,4-7
VP
3-4
1,4-7
VP
1-4
4-7
VP
6
1-5,7
NP
1-6
4,5,7
NP
1-6
4,7
PP
1-7
4
NP
1-7
----
S
 
(a)  (b) 
Figure 1. Two example partial bilingual sentences with word alignment and syntactic tree for the 
target sentence. All the nodes in gray are frontier nodes. Example (a) contains two error links (in dash 
line), and the syntactic tree for the target sentence of example (b) is wrong. 
 
855
2 Parser Re-training Strategies 
Most monolingual parsers used in SSMT are 
trained upon certain tree bank. That is, a parser is 
trained with the target of maximizing the 
agreement between its decision on syntactic 
structure and that decision in the human-annotated 
parse trees. As mentioned in Section 1, 
monolingual syntactic structure is not necessarily 
suitable for translation, and sometimes the 
bilingual information in word alignment may help 
the parser find out the correct structure. Therefore, 
it is desirable if there is a way to re-train a parser 
with bilingual information. 
What is needed includes a framework of parser 
re-training, and a data selection strategy that 
maximizes the consistency between parse tree and 
alignment matrix. Our two solutions will be 
introduced in the next two subsections respectively. 
2.1 Targeted Self-Training with Frontier Set 
Based Evaluation (TST-FS) 
The first solution is based on targeted self-training 
(TST) (Katz-Brown et al2011). In standard self-
training, the top one parse trees produced by the 
current parser are taken as training data for the 
next round, and the training objective is still the 
correctness of monolingual syntactic structure. In 
targeted self-training, the training objective shifts 
to certain external evaluation function. For each 
sentence, the n-best parse trees from the current 
parser are re-ranked in accordance with this 
external evaluation function, and the top one of the 
re-ranked candidates is then selected as training 
data for the next round. The key of targeted self-
training is the definition of this external evaluation 
function. 
As shown by the example in Figure 1(b), an 
incorrect parse tree is likely to hinder the 
extraction of good translation rules, because the 
number of frontier nodes in the incorrect tree is in 
general smaller than that in the correct tree. 
Consider the example in Figure 2, which is about 
the same partial bilingual sentence as in Figure 
1(b). Although both parse trees do not have the 
correct syntactic structure, the tree in Figure 2 has 
more frontier nodes, leads to more valid translation 
rules, and is therefore more preferable.  
This example suggests a very simple external 
evaluation function, viz. the size of frontier set. 
Given a bilingual sentence, its alignment matrix, 
and the N-best parse trees of the TL sentence, we 
will calculate the number of frontier nodes for each 
parse tree, and re-rank the parse trees in its 
descending order. The new top one parse tree is 
selected as the training data for the next round of 
targeted self-training of the TL parser. In the 
following we will call this approach as targeted 
self-training with frontier set based evaluation 
(TST-FS). 
Note that the size of the N-best list should be 
kept small. It is because sometimes a parse tree 
with an extremely mistaken structure happens to 
have perfect match with the alignment matrix, 
thereby giving rise to nearest the largest frontier set 
size. It is empirically found that a 5-best list of 
parse trees is already sufficient to significantly 
improve translation performance. 
2.2 Forced Alignment-based Parser Re-
Training (FA-PR) 
If we doubt that the parse tree from a monolingual 
parser is not appropriate enough for translation 
purpose, then it seems reasonable to consider using 
the parse tree produced by an SSMT system to re-
train the parser. A na?ve idea is simply to run an 
SSMT system over some SL sentences and retrieve 
the by-product TL parse trees for re-training the 
monolingual parser. The biggest problem of this 
na?ve approach is that the translation by an MT 
system is often a 'weird' TL sentence, and thus the 
associated parse tree is of little use in improving 
the parser. 
Forced alignment (Wuebker et al2010) of 
bilingual data is a much more promising approach. 
7
1-6
NP
3-4
1,5-7
NP
3-4
1,5-7
PP
3-4
1,5-7
VP
6
1-5,7
NP
1-6
3-5,7
NP
VP
1
3-7
1-6
3-4,7
PP
3-4
1,5-7
VP
1-7
3-4
NP
a1large2 number3 of4 people5 coming6 to7 listen8 to9 their10 propaganda11 lectures12
?1               ?2            ??3           ??4                ?5                 ?6               ??7
7 7 5 6 1 4 4null null null null 3
null
1-7
DT
7
1-7
JJ
7
1-7
NN
5
1-4,6-7
IN
6
1-5,7
NNS
1
3-7
VBG
null
1-7
TO
null
1-7
VB
null
1-7
TO
3
1,4-7
PRP
4
1-7
NN
4
1-7
VP
 
Figure 2. The parse tree selected by TST-FS for 
the example in Figure 1(b) 
856
When applied to SSMT, given a bilingual sentence, 
it performs phrase segmentation of the SL side, 
parsing of the TL side, and word alignment of the 
bilingual sentence, using the full translation system 
as in decoding. It finds the best decoding path that 
generates the TL side of the bilingual sentence, and 
the parse tree of the TL sentence is also obtained as 
a by-product. The parse trees from forced 
alignment are suitable for re-training the 
monolingual parser.  
Here is the simple iterative re-training algorithm. 
First we have a baseline monolingual parser and 
plug it into an SSMT system. Then perform forced 
alignment, using the SSMT system, of some 
bilingual data and obtain the parse trees as new 
training data for the parser. The new parser can 
then be applied again to do the second round of 
forced alignment. This iteration of forced 
alignment followed by parser re-training is kept 
going until some stopping criterion is met. In the 
following we will call this approach as forced 
alignment based parser re-training (FA-PR). 
Algorithm 1  Forced Alignment Based Parser Re-
Training (FA-PR) 
? step1:      ;                . 
? step2: Use parser      to parse target 
sentences of training data, and build a 
SSMT system      . 
? step3: Perform forced alignment on training 
data with      to get parse trees 
         for target sentence of training 
data. 
? step4: Train a new parser          with 
         . 
? step5:                       . 
? Step6: Go to step 2, until performance of      
on development data drops, or a preset 
limit is reached. 
There are a few important implementation 
details of FA-PR. Forced alignment is guaranteed 
to obtain a parse tree if all translation rules are kept 
and no pruning is performed during decoding. Yet 
in reality an average MT system applies pruning 
during translation model training and decoding, 
and a lot of translation rules will then be discarded. 
In order to have more parse trees be considered by 
forced alignment, we keep all translation rules and 
relax pruning constraints in the decoder, viz. 
enlarge the stack size of each cell in the chart from 
50 to 150.  
Another measure to guarantee the existence of a 
decoding path in forced alignment is to allow part 
of a SL or TL sentence translate to null. Consider 
the example in Figure 1(b). We also add a null 
alignment for any span of the source and target 
sentences to handle the null translation scenario. It 
is easy to add a null translation candidate for a 
span of the source sentence during decoding, but 
not easy for target spans. For example, suppose the 
best translation candidate for the source span " ? 1  
NP ? 5 ? 6 ?? 7" is "a large number of people 
coming NP", and the best translation candidate for 
"? 2 ?? 3 ?? 4" is "their propaganda lectures", 
there is no combination of candidates from two n-
best translation lists which can match a sequence in 
the given target part, so we add a translation 
candidate ("to listen to ") generated from null, 
whose syntactic label can be any label (decided 
according to the translated context, which is 
?ADJP? here).  The feature weights for the added 
null alignment are set to be very small, so as to 
avoid the competition with the normal candidates. 
In order to generate normal trees with not so many 
null alignment sub-trees for the target sentence 
(such trees are not suitable for parser re-training), 
only target spans with less than 4 words can align 
to null, and such null-aligned sub-tree can only be 
added  no more than 3 times.  
With all the mentioned modification of the 
forced alignment, the partial target tree generated 
using forced alignment for the example in Figure 
1(b) is shown in Figure 3. We can see that even 
a1large2 number3 of4 people5 coming6 to7 listen8 to9 their10 propaganda11 lectures12
7
1-6
NP
4
1-3,5-7
NP
6
1-5,7
NP
5-6
1-4,7
PP
5-7
1-4
NP
null
1-7
ADJP
3-4
1,5-7
NP
?1               ?2            ??3           ??4                ?5                 ?6               ??7
7 7 5 6 1 4 4null null null null 3
null
1-7
DT
7
1-7
JJ
7
1-7
NN
5
1-4,6-7
IN
6
1-5,7
NNS
1
3-7
VBG
null
1-7
TO
null
1-7
VB
null
1-7
TO
3
1,4-7
PRP
4
1-7
NN
4
1-7
VP
3-4
1,5-7
NP
1-4
5-7
NP
 
Figure 3. The parse tree selected by FA-PR for the 
example in Figure 1(b) 
 
857
with an incorrect sub-tree, more useful rules can be 
extracted, compared with the baseline sub-tree and 
the sub-tree generated from TST-FS. 
3  Word Alignment Symmetrization 
The most widely used word aligners in MT, like 
HMM and IBM Models (Och and Ney, 2003), are 
directional aligners. Such aligner produces one set 
of alignment matrices for the SL-to-TL direction 
and another set for the TL-to-SL direction. 
Symmetrization refers to the combination of these 
two sets of alignment matrices.  
The most popular method of symmetrization is 
intersect-diag-grow (IDG). Given a bilingual 
sentence and its two alignment matrices     and 
     IDG starts with all the links in        . 
Then IDG considers each link in           
          in turn. A link is added if its addition 
does not make some phrase pairs overlap. 
Although IDG is simple and efficient, and has been 
shown to be effective in phrase-based SMT, it is 
problematic in SSMT, as illustrated by the example 
in section 1. 
3.1 Intersect-Diag-Syntactic-Grow (IDSG) 
We propose a new symmetrization method, 
Intersect-Diag-Syntactic-Grow (IDSG), which is 
an adaptation of IDG but also taking syntactic 
information in consideration. It is sketched in 
Algorithm 2.  
Algorithm 2 Intersect-Diag-Syntactic-Grow  
? step1: Generate all the candidate links        
using IDG. 
? step2: Select the one which can generate the 
biggest frontier set: 
        
         
                          
? step3: Add   to  , and repeat step 1, until no 
new link can be added. 
Like IDG, IDSG starts with all the links in 
        and its main task is to add links selected 
from                         . IDSG is 
also subject to the constraints of IDG. The new 
criterion in link selection in IDSG is specified in 
Step 2. Given a parse tree of the TL side of the 
bilingual sentence, in each iteration IDSG 
considers the change of frontier set size caused by 
the addition of each link in       . The link 
leading to the maximum number of frontier nodes 
is added (and removed from       ). This process 
continues until no more links can be added. 
In sum, IDSG add links in an order which take 
syntactic structure into consideration, and the link 
with the least violation of the syntactic structure is 
added first. 
For the example in Figure 1(a), IDSG succeeds 
in discarding the two incorrect links, and produces 
the final alignment and frontier set as shown in 
Figure 4. Note that IDSG still fails to produce the 
correct link (the3, ?? 4), since this link does not 
appear in        at all. 
3.2 Combining TST-FS/FA-PR and IDSG 
Parser re-training aims to improve a parser with 
alignment matrix while IDSG aims to improve 
alignment matrix with parse tree. It is reasonable to 
combine them, and there are two alternatives of the 
combination, depending on the order of application. 
That is, we could either improve alignment matrix 
by IDSG and then re-train parser with the better 
alignment, or re-train parser and then improve 
alignment matrix with better syntactic information. 
Either alternative can be arranged into an iterative 
training routine, but empirically it is found that 
only one round of parser re-training before or after 
only one round of IDSG is already enough. 
6
1-5
NNS
5
1-4,6
POS
4
1-3,5-6
NNIN
3
1-2,4-6
lived
1    
 in
2 
     the
3  
 herdsmen
4 
?s
 5 
  yurts
6  
   at
 7 
   night
8  
??
1
           ??
2
          ?
3
         ??
4
        ?
5
       ??
6
 
2 3 null1 4 5 6 1 1
VDB
2
1,3-6
DT
null1
1-6
1
1-6
IN
1
1-6
NN
1
1-6
NP
4-5
1-3,6
NP
4-6
1-3
NP
3-6
1-2
PP
1
2-6
PP
1-6
----
VP
 
Figure 4, the alignment generated by IDSG for the 
example in Figure 1(a) 
858
4 Experiment 
In this section, we conduct experiments on Chinese 
to English translation task to test our proposed 
methods of parser re-training and word alignment 
symmetrization.  The evaluation method is the case 
insensitive IBM BLEU-4 (Papineni et al2002). 
Significant testing is carried out using bootstrap re-
sampling method proposed by Koehn (2004) with 
a 95% confidence level. 
4.1 Parser and SMT Decoder 
The syntactic parser we used in this paper is 
Berkley parser, with the grammar trained on WSJ 
corpus, and the training method follows Petrov and 
Klein (2007). Our SMT decoder is an in-house 
implementation of string-to-tree decoder. The 
features we used are standard used features, such 
as translation probabilities, lexical weights, 
language model probabilities and distortion 
probability. The feature weights are tuned using 
the minimum error rate training (MERT) (Och, 
2003). 
4.2 Experiment Data Setting and Baselines 
We test our method with two data settings: one is 
IWSLT data set, the other is NIST data set. 
 dev8+dialog dev9 
Baseline 50.58 49.85 
Table 1. Baselines for IWSLT data set 
 NIST'03 NIST'05 NIST'08 
Baseline 37.57 36.44 24.87 
Table 2. Baselines for NIST data set 
Our IWSLT data is the IWSLT 2009 dialog task 
data set. The training data include the BTEC and 
SLDB training data. The training data contains 81k 
sentence pairs, 655k Chinese words and 806k 
English words. The language model is 5-gram 
language model trained with the English sentences 
in the training data. We use the combination of 
dev8 and dialog as development set, and dev9 as 
test set. The TL sentences of the training data with 
the selected/generated trees are used as the training 
data to re-train the parser. To get the baseline of 
this setting, we run IDG to combine the bi-
direction alignment generated by Giza++ (Och 
Ney, 2003), and run Berkeley parser (Petrov and 
Klein, 2007) to parse the target sentences. With the 
baseline alignments and syntactic trees, we extract 
rules and calculate features. The baseline results 
are shown in Table 1. 
For the NIST data set, the bilingual training data 
we used is NIST 2008 training set excluding the 
Hong Kong Law and Hong Kong Hansard. The 
training data contains 354k sentence pairs, 8M 
Chinese words and 10M English words, and is also 
the training data for our parser re-training. The 
language model is 5-gram language model trained 
with the Giga-Word corpus plus the English 
sentences in the training data. The development 
data to tune the feature weights of our decoder is 
NIST 2003 evaluation set, and test sets are NIST 
2005 and 2008 evaluation sets. The baseline for 
NIST data is got in a similar way with for IWSLT, 
which are shown in Table 2 . 
4.3 Results of TST-FS/ FA-PR 
The parser re-training strategies TST-FS and FA-
PR are tested with two baselines, one is the default 
parser without any re-training and another is 
standard self-training (SST). All three re-training 
approaches are based on the same bilingual 
datasets as used in translation model training. The 
MT performances on IWSLT and NIST by the four 
approaches are shown in Table 3 and 4 
respectively. 
It can be seen that just standard self-training 
does improve translation performance, as re-
training on the TL side of bilingual data is a kind 
of domain adaptation (from WSJ to IWSLT/NIST). 
But targeted self-training achieves more noticeable 
improvement, almost twice as much as standard 
self-training. This confirms the value of word 
alignment information in parser re-training. Finally, 
the even larger improvement of FA-PR than TST-
FS shows that merely increasing the number of 
frontier nodes is not enough.  Some frontier nodes 
are of poor quality, and the frontier nodes found in 
forced alignment are more suitable.  
It can also be seen that the improvement in 
IWSLT is larger than that in NIST. The first reason 
is that both WSJ and NIST are of the news domain 
and of formal writing style, whereas IWSLT is of 
the tourist domain and of colloquial style. 
Therefore any improvement from the default parser, 
which is trained on WSJ, is expected to be smaller 
in the NIST case. Another reason is that, since the 
859
IWSLT dataset is much smaller, the impact of 
more and better rules is more obvious.  
Note that the figures in Table 3 and 4 are about 
parser re-training for only one iteration. It is found 
that, more iteration do not lead to further 
significant improvement. The forced alignment of 
bilingual training data does not obtain a full 
decoding path for every bilingual sentence. It is 
because, although all translation rules are kept, 
there is still pruning during decoding. Only 64% of 
the IWSLT dataset and 53% of the NIST dataset 
can be successfully forced-aligned. In general, the 
longer the bilingual sentence, the less likely forced 
alignment is successful, and that is why a lower 
proportion of NIST can be forced-aligned. 
4.4  Symmetrization 
The new symmetrization method IDSG is 
compared with the baseline method IDG. 
 dev8+dialog dev9 # Rules 
IDG 50.58 49.85 515K 
IDSG 
52.71 
(+2.31) 
51.80 
(+2.05) 
626K 
Table 5. MT performance of symmetrization 
methods on IWSLT data set. The results in bold 
type are significantly better than the performance 
of IDG. 
 NIST'03 NIST'05 NIST'08 #Rules 
IDG 37.57 36.44 24.87 3,376K 
IDSG 
38.15 
(+0.58) 
37.07 
(+0.63) 
25.67 
(+0.80) 
4,109K 
Table 6. MT performance of symmetrization 
methods on NIST data. The results in bold type are 
significantly better than the performance of IDG. 
As shown by the results in Table 5 and 6, IDSG 
enlarges the set of translation rules by more than 
20%, thereby improving translation performance 
significantly. As in parser re-training, the 
improvement in the IWSLT task is larger than that 
in the NIST task. Again, it is because the IWSLT 
dataset is very small and so the effect of rule table 
size is more obvious. 
4.5 Methods combined 
As mentioned in section 3.2, parser re-training and 
the new symmetrization method can be combined 
in two different ways, depending on the order of 
application. Table 7 and 8 show the experiment 
results of combining FA-PR with IDSG. 
It can be seen that either way of the combination 
is better than using FA-PR or IDSG alone. Yet 
there is no significant difference between the two 
kinds of combination.  
The best result is a gain of more than 3 Bleu 
points on IWSLT and that of more than 1 Bleu 
point on NIST.  
5 Related Works 
There are a lot of attempts in improving word 
alignment with syntactic information (Cherry and 
Lin, 2006; DeNero and Klein, 2007; Hermjackob, 
2009) and in improving parser with alignment 
information (Burkett and Klein, 2008). Yet strictly 
speaking all these attempts aim to improve the 
 dev8+dialog dev9 # Rules 
Baseline 50.58 49.85 515K 
SST 
52.04 
(+1.46) 
51.26 
(+1.41) 
574K 
TST-FS 
52.75 
(+2.17) 
52.51 
(+2.66) 
572K 
FA-PR 
53.31 
(+2.73) 
52.8 
(+2.95) 
591K 
Table 3. MT performance of parser re-training 
strategies on IWSLT data set. The results in 
bold type are significantly better than the 
baseline. 
 NIST'03 NIST'05 NIST'08 #Rules 
Baseline 37.57 36.44 24.87 3,376K 
SST 
37.98 
(+0.41) 
36.79 
(+0.35) 
25.30 
(+0.43) 
3,462K 
TST-FS 
38.42 
(+0.85) 
37.39 
(+0.95) 
25.79 
(+0.92) 
3,642K 
FA-PR 
38.74 
(+1.17) 
37.69 
(+1.25) 
25.89 
(+1.02) 
3,976K 
Table 4. MT performance of parser re-training 
strategies on NIST data set. The results in bold 
type are significantly better than the baseline. 
860
 dev8+dialog dev9 
# 
Rules 
Baseline 50.58 49.85 515K 
IDSG 
52.71 
(+2.31) 
51.80 
(+2.05) 
626K 
FA-PR 
53.31 
(+2.73) 
52.8 
(+2.95) 
591K 
IDSG  then 
FA-PR 
53.64 
(3.06) 
53.32 
(+3.47) 
602K 
FA-PR then 
IDSG 
53.81 
(+3.23) 
53.26 
(+3.41) 
597K 
Table 7. MT performance of the new methods 
on IWSLT data set. The results in bold type 
are significantly better than the baseline. 
 NIST'03 NIST'05 NIST'08 #Rules 
Baseline 37.57 36.44 24.87 3,376K 
IDSG 
38.15 
(+0.58) 
37.07 
(+0.63) 
25.67 
(+0.80) 
4,109K 
FA-PR 
38.74 
(+1.17) 
37.69 
(+1.25) 
25.89 
(+1.02) 
3,976K 
IDSG 
then 
FA-PR 
38.97 
(+1.40) 
37.95 
(+1.51) 
26.74 
(+1.87) 
4,557K 
FA-PR 
then 
IDSG 
38.90 
(+1.33) 
37.94 
(+1.50) 
26.52 
(+1.65) 
4,478K 
Table 8. MT performance of the new methods 
on NIST data set. The results in bold type are 
significantly better than the baseline. 
parser/aligner itself rather than the translation 
model.  
To improve the performance of syntactic 
machine translation, Huang and Knight (2006) 
proposed a method incorporating a handful of 
relabeling strategies to modify the syntactic trees 
structures. Ambati and Lavie (2008) restructured 
target parse trees to generate highly isomorphic 
target trees that preserve the syntactic boundaries 
of constituents aligned in the original parse trees. 
Wang et al(2010) proposed to use re-structuring 
and re-labeling to modify the parser tree. The re-
structuring method uses a binarization method to 
enable the reuse of sub-constituent structures, and 
the linguistic and statistical re-labeling methods to 
handle the coarse nonterminal problem, so as to 
enhance generalization ability. Different from the 
previous work of modifying tree structures with 
post-processing methods, our methods try to learn 
a suitable grammar for string-to-tree SMT models, 
and directly produce trees which are consistent 
with word alignment matrices.  
Instead of modifying the parse tree to improve 
machine translation performance, many methods 
were proposed to modify word alignment by taking 
syntactic tree into consideration, including deleting 
incorrect word alignment links by a discriminative 
model (Fossum et al2008), re-aligning sentence 
pairs using EM method with the rules extracted 
with initial alignment (Wang et al2010), and 
removing ambiguous alignment of functional 
words with constraint from chunk-level 
information during rule extraction (Wu et al
2011). Unlike all these pursuits, to generate a 
consistent word alignment, our method modifies 
the popularly used IDG symmetrization method to 
make it suitable for string-to-tree rule extraction, 
and our method is much simpler and faster than the 
previous works.  
6 Conclusion  
In this paper we have attempted to improve SSMT 
by reducing the errors introduced by the mutual 
independence between monolingual parser and 
word aligner. Our major contribution is the 
strategies of re-training parser with the bilingual 
information in alignment matrices. Either of our 
proposals of targeted self-training with frontier set 
size as evaluation function and forced alignment 
based re-training is more effective than baseline 
parser or standard self-training of parser. As an 
auxiliary method, we also attempted to improve 
alignment matrices by a new symmetrization 
method.  
In future, we will explore more alternatives in 
integrating parsing information and alignment 
information, such as discriminative word 
alignment using a lot of features from parser. 
References  
Vamshi Ambati and Alon Lavie. 2008. Improving 
syntax driven translation models by re-structuring 
divergent and non-isomorphic parse tree structures. 
In Student Research Workshop of the Eighth 
Conference of the Association for Machine 
Translation in the Americas, pages 235-244. 
861
David Burkett and Dan Klein. 2008. Two languages are 
better than one (for syntactic parsing). In 
Proceedings of the Conference on Empirical 
Methods on Natural Language Processing, pages 
877-886. 
Colin Cherry and Dekang Lin. 2006. Soft syntactic 
constraints for word alignment through 
discriminative training. In Proceedings of the 21st 
International Conference on Computational 
Linguistics and 44th Annual Meeting of the 
Association for Computational Linguistics.  
John DeNero and Dan Klein. 2007. Tailing word 
alignment to syntactic machine translation. In 
Proceedings of the Association for Computational 
Linguistics, pages 17-24. 
Victoria Fossum, Kevin Knight, Steven Abney. 2008. 
Using syntax to improve word alignment precision 
for syntax-based machine translation. In Proceedings 
of the Third Workshop on Statistical Machine 
Translation, pages 44-52. 
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel 
Marcu, Steve Deneefe, Wei Wang and Ignacio 
Thayer. 2006. Scalable inference and training of 
context-rich syntactic translation models. In 
Proceedings of the 21st International Conference on 
Computational Linguistics and 44th Annual Meeting 
of the Association for Computational Linguistics, 
pages 961-968. 
Ulf Hermjackob. Improved word alignment with 
statistics and linguistic heuristics. In Proceedings of 
the Conference on Empirical Methods on Natural 
Language Processing, pages 229-237. 
Bryant Huang, Kevin Knight. 2006. Relabeling syntax 
trees to improve syntax-based machine translation 
quality. In Proceedings of the Human Technology 
Conference of the North American Chapter of the 
ACL, pages 240-247. 
Jason Katz-Brown, Slav Petrov, Ryan McDonald, Franz 
Och, David Talbot, Hiroshi Ichikawa, Masakazu 
Seno, Hideto Kazawa. 2011. Training a parser for 
machine translation reordering. In Proceedings of the 
Conference on Empirical Methods on Natural 
Language Processing, pages 183-192. 
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of the 
Conference on Empirical Methods on Natural 
Language Processing, pages 388-395. 
Wei Wang, Jonathan May, Kevin Knight, Daniel Marcu. 
2010. Re-structuring, re-labeling, and re-alignment 
for syntax-Based machine translation. Computational 
Linguistics, 36(2). 
Xianchao Wu, Takuya Matsuzaki and Jun'ichi Tsujii. 
2011. Effective use of function words for rule 
generalization in forest-based translation. In 
Proceedings of the Association for Computational 
Linguistics, pages 22-31. 
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of the 
Association for Computational Linguistics, pages 
160-167. 
Franz Josef Och and Hermann Ney. 2003. A systematic 
comparison of various statistical alignment models. 
Computational Linguistics, 29(1). 
Joern Wuebker, Arne Mauser and Hermann Ney. 2010. 
Training phrase translation models with leaving-one-
out. In Proceedings of the Association for 
Computational Linguistics, pages 475-484. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
the Association for Computational Linguistics, pages 
311-318. 
Slav Petrov and Dan Klein. 2007. Improved inference 
for unlexicalized parsing. In Proceedings of Human 
Language Technologies: The Annual Conference of 
the North American Chapter of the Association for 
Computational Linguistics, pages 404?411. 
 
862
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 316?324,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Discriminative Pruning for Discriminative ITG Alignment 
Shujie Liu?, Chi-Ho Li? and Ming Zhou? 
?School of Computer Science and Technology 
Harbin Institute of Technology, Harbin, China 
shujieliu@mtlab.hit.edu.cn 
?Microsoft Research Asia, Beijing, China 
{chl, mingzhou}@microsoft.com 
  
 
Abstract 
While Inversion Transduction Grammar (ITG) 
has regained more and more attention in recent 
years, it still suffers from the major obstacle of 
speed. We propose a discriminative ITG prun-
ing framework using Minimum Error Rate 
Training and various features from previous 
work on ITG alignment. Experiment results 
show that it is superior to all existing heuristics 
in ITG pruning. On top of the pruning frame-
work, we also propose a discriminative ITG 
alignment model using hierarchical phrase 
pairs, which improves both F-score and Bleu 
score over the baseline alignment system of 
GIZA++. 
1 Introduction 
Inversion transduction grammar (ITG) (Wu, 1997) 
is an adaptation of SCFG to bilingual parsing. It 
does synchronous parsing of two languages with 
phrasal and word-level alignment as by-product. 
For this reason ITG has gained more and more 
attention recently in the word alignment commu-
nity (Zhang and Gildea, 2005; Cherry and Lin, 
2006; Haghighi et al, 2009). 
A major obstacle in ITG alignment is speed. 
The original (unsupervised) ITG algorithm has 
complexity of O(n6). When extended to super-
vised/discriminative framework, ITG runs even 
more slowly. Therefore all attempts to ITG 
alignment come with some pruning method. For 
example, Haghighi et al (2009) do pruning based 
on the probabilities of links from a simpler 
alignment model (viz. HMM); Zhang and Gildea 
(2005) propose Tic-tac-toe pruning, which is 
based on the Model 1 probabilities of word pairs 
inside and outside a pair of spans. 
As all the principles behind these techniques 
have certain contribution in making good pruning 
decision, it is tempting to incorporate all these 
features in ITG pruning. In this paper, we pro-
pose a novel discriminative pruning framework 
for discriminative ITG. The pruning model uses 
no more training data than the discriminative ITG 
parser itself, and it uses a log-linear model to in-
tegrate all features that help identify the correct 
span pair (like Model 1 probability and HMM 
posterior). On top of the discriminative pruning 
method, we also propose a discriminative ITG 
alignment system using hierarchical phrase pairs.  
In the following, some basic details on the ITG 
formalism and ITG parsing are first reviewed 
(Sections 2 and 3), followed by the definition of 
pruning in ITG (Section 4). The ?Discriminative 
Pruning for Discriminative ITG? model (DPDI) 
and our discriminative ITG (DITG) parsers will 
be elaborated in Sections 5 and 6 respectively. 
The merits of DPDI and DITG are illustrated 
with the experiments described in Section 7.  
2 Basics of ITG 
The simplest formulation of ITG contains three 
types of rules: terminal unary rules ? ? ?/? , 
where ?  and ?  represent words (possibly a null 
word, ?) in the English and foreign language 
respectively, and the binary rules ? ?  ?,?  and 
? ?  ?,? , which refer to that the component 
English and foreign phrases are combined in the 
same and inverted order respectively. 
From the viewpoint of word alignment, the 
terminal unary rules provide the links of word 
pairs, whereas the binary rules represent the reor-
dering factor. One of the merits of ITG is that it 
is less biased towards short-distance reordering. 
Such a formulation has two drawbacks. First of 
all, it imposes a 1-to-1 constraint in word align-
ment. That is, a word is not allowed to align to 
more than one word. This is a strong limitation as 
no idiom or multi-word expression is allowed to 
align to a single word on the other side. In fact 
there have been various attempts in relaxing the 
1-to-1 constraint. Both ITG alignment 
316
approaches with and without this constraint will 
be elaborated in Section 6.  
Secondly, the simple ITG leads to redundancy 
if word alignment is the sole purpose of applying 
ITG. For instance, there are two parses for three 
consecutive word pairs, viz. [?/?? [?/?? ?/
??] ]  and [[?/?? ?/??] ?/??] . The problem of re-
dundancy is fixed by adopting ITG normal form. 
In fact, normal form is the very first key to speed-
ing up ITG. The ITG normal form grammar as 
used in this paper is described in Appendix A. 
3 Basics of ITG Parsing 
Based on the rules in normal form, ITG word 
alignment is done in a similar way to chart pars-
ing (Wu, 1997). The base step applies all relevant 
terminal unary rules to establish the links of word 
pairs. The word pairs are then combined into 
span pairs in all possible ways. Larger and larger 
span pairs are recursively built until the sentence 
pair is built.  
Figure 1(a) shows one possible derivation for a 
toy example sentence pair with three words in 
each sentence. Each node (rectangle) represents a 
pair, marked with certain phrase category, of for-
eign span (F-span) and English span (E-span) 
(the upper half of the rectangle) and the asso-
ciated alignment hypothesis (the lower half). 
Each graph like Figure 1(a) shows only one deri-
vation and also only one alignment hypothesis.  
The various derivations in ITG parsing can be 
compactly represented in hypergraph (Klein and 
Manning, 2001) like Figure 1(b). Each hypernode 
(rectangle) comprises both a span pair (upper half) 
and the list of possible alignment hypotheses 
(lower half) for that span pair. The hyperedges 
show how larger span pairs are derived from 
smaller span pairs. Note that a hypernode may 
have more than one alignment hypothesis, since a 
hypernode may be derived through more than one 
hyperedge (e.g. the topmost hypernode in Figure 
1(b)). Due to the use of normal form, the hypo-
theses of a span pair are different from each other.  
4 Pruning in ITG Parsing 
The ITG parsing framework has three levels of 
pruning: 
1) To discard some unpromising span pairs; 
2) To discard some unpromising F-spans 
and/or E-spans; 
3) To discard some unpromising alignment 
hypotheses for a particular span pair. 
The second type of pruning (used in Zhang et. 
al. (2008)) is very radical as it implies discarding 
too many span pairs. It is empirically found to be 
highly harmful to alignment performance and 
therefore not adopted in this paper.  
The third type of pruning is equivalent to mi-
nimizing the beam size of alignment hypotheses 
in each hypernode. It is found to be well handled 
by the K-Best parsing method in Huang and 
Chiang (2005). That is, during the bottom-up 
construction of the span pair repertoire, each span 
pair keeps only the best alignment hypothesis. 
Once the complete parse tree is built, the k-best 
list of the topmost span is obtained by minimally 
expanding the list of alignment hypotheses of 
minimal number of span pairs. 
The first type of pruning is equivalent to mi-
nimizing the number of hypernodes in a hyper-
graph. The task of ITG pruning is defined in this 
paper as the first type of pruning; i.e. the search 
for, given an F-span, the minimal number of E-
spans which are the most likely counterpart of 
that F-span.1 The pruning method should main-
tain a balance between efficiency (run as quickly 
as possible) and performance (keep as many cor-
rect span pairs as possible).  
                                                 
1 Alternatively it can be defined as the search of the minimal 
number of E-spans per F-span. That is simply an arbitrary 
decision on how the data are organized in the ITG parser.  
B:[e1,e2]/[f1,f2]
{e1/f2,e2/f1}
C:[e1,e1]/[f2,f2]
{e1/f2}
C:[e2,e2]/[f1,f1]
{e2/f1}
C:[e3,e3]/[f3,f3]
{e3/f3}
A:[e1,e3]/[f1,f3]
{e1/f2,e2/f1,e3/f3}
(a) 
C:[e2,e2]/[f2,f2]
{e2/f2}
C:[e1,e1]/[f1,f1]
{e1/f1}
C:[e3,e3]/[f3,f3]
{e3/f3}
C:[e2,e2]/[f1,f1]
{e2/f1}
C:[e1,e1]/[f2,f2]
{e1/f2}
B:[e1,e2]/[f1,f2]
{e1/f2}
A:[e1,e2]/[f1,f2]
{e2/f2}
A:[e1,e3]/[f1,f3]
{e1/f2,e2/f1,e3/f3} , 
{e1/f1,e2/f2,e3,f3}
(b)
B?<C,C> A?[C,C]
A?[A,C]A?[B,C]
 
Figure 1:  Example ITG parses in graph (a) and hypergraph (b). 
317
A na?ve approach is that the required pruning 
method outputs a score given a span pair. This 
score is used to rank all E-spans for a particular 
F-span, and the score of the correct E-span 
should be in general higher than most of the in-
correct ones.  
5 The DPDI Framework 
DPDI, the discriminative pruning model pro-
posed in this paper, assigns score to a span pair 
 ? , ?  as probability from a log-linear model: 
? ? ?  =
???(  ???? ? , ? ? )
 ???(  ????(? , ? ?))?? ???  
 (1) 
where each ??(? ,? )  is some feature about the 
span pair, and each ? is the weight of the corres-
ponding feature. There are three major questions 
to this model:  
1) How to acquire training samples? (Section 
5.1) 
2) How to train the parameters ? ? (Section 5.2) 
3) What are the features? (Section 5.3) 
5.1 Training Samples 
Discriminative approaches to word alignment use 
manually annotated alignment for sentence pairs. 
Discriminative pruning, however, handles not 
only a sentence pair but every possible span pair. 
The required training samples consist of various 
F-spans and their corresponding E-spans. 
Rather than recruiting annotators for marking 
span pairs, we modify the parsing algorithm in 
Section 3 so as to produce span pair annotation 
out of sentence-level annotation. In the base step, 
only the word pairs listed in sentence-level anno-
tation are inserted in the hypergraph, and the re-
cursive steps are just the same as usual.  
If the sentence-level annotation satisfies the 
alignment constraints of ITG, then each F-span 
will have only one E-span in the parse tree. How-
ever, in reality there are often the cases where a 
foreign word aligns to more than one English 
word. In such cases the F-span covering that for-
eign word has more than one corresponding E-
spans. Consider the example in Figure 2, where 
the golden links in the alignment annotation are 
?1/?1, ?2/?1, and ?3/?2; i.e. the foreign word 
?1 aligns to both the English words ?1 and ?2. 
Therefore the F-span  ?1,?1  aligns to the E-
span  ?1, ?1  in one hypernode and to the E-span 
 ?2, ?2  in another hypernode. When such situa-
tion happens, we calculate the product of the in-
side and outside probability of each alignment 
hypothesis of the span pair, based on the proba-
bilities of the links from some simpler alignment 
model2. The E-span with the most probable hypo-
thesis is selected as the alignment of the F-span.  
A?[C,C]
C
w
:
[e1,e1]/[f1,f1]
{e1/f1}
C
e
:
[e1]/?
C
w
:
[e2,e2]/[f1,f1]
C
e
:
[e2]/?
C
w
:
[e3,e3]/[f2,f2]
C:
[e1,e2]/[f1,f1]
{e2/f1}
C:
[e2,e3]/[f2,f2]
{e3/f2}
A:
[e1,e3]/[f1,f2]
{e1/f1,e3/f2},{e2/f1,e3/f2}
C? [C
e
,C
w
]
A?[C,C]
C? [C
e
,C
w
]
{e1/f1} {e1/f1}
(a) (b)
[f1,f1]
[e1,e1]
[e1,e2]
[e2,e2]
[f2,f2]
[e2,e3]
[e3,e3]
[f1,f2] [e1,e3]
Figure 2: Training sample collection. 
Table (b) lists, for the hypergraph in (a), the candidate 
E-spans for each F-span. 
It should be noted that this automatic span pair 
annotation may violate some of the links in the 
original sentence-level alignment annotation. We 
have already seen how the 1-to-1 constraint in 
ITG leads to the violation. Another situation is 
the ?inside-out? alignment pattern (c.f. Figure 3). 
The ITG reordering constraint cannot be satisfied 
unless one of the links in this pattern is removed. 
f1      f2      f3      f4
e1     e2      e3      e4 
Figure 3: An example of inside-out alignment 
The training samples thus obtained are positive 
training samples. If we apply some classifier for 
parameter training, then negative samples are 
also needed. Fortunately, our parameter training 
does not rely on any negative samples. 
5.2 MERT for Pruning 
Parameter training of DPDI is based on Mini-
mum Error Rate Training (MERT) (Och, 2003), a 
widely used method in SMT. MERT for SMT 
estimates model parameters with the objective of 
minimizing certain measure of translation errors 
(or maximizing certain performance measure of 
translation quality) for a development corpus. 
Given an SMT system which produces, with 
                                                 
2 The formulae of the inside and outside probability of a 
span pair will be elaborated in Section 5.3. The simpler 
alignment model we used is HMM. 
318
model parameters ?1
?, the K-best candidate trans-
lations ? (??; ?1
?) for a source sentence ??, and an 
error measure ?(?? , ??,?) of a particular candidate 
??,? with respect to the reference translation ?? , 
the optimal parameter values will be:  
? 1
? = ??????
?1
?
 ? ?? , ? ??; ?1
?  
?
?=1
  
 = ??????
?1
?
  ? ?? , ??,? ?(? ??; ?1
? , ??,?)
?
?=1
?
?=1
   
DPDI applies the same equation for parameter 
tuning, with different interpretation of the com-
ponents in the equation. Instead of a development 
corpus with reference translations, we have a col-
lection of training samples, each of which is a 
pair of F-span (??) and its corresponding E-span 
(??). These samples are acquired from some ma-
nually aligned dataset by the method elaborated 
in Section 5.1. The ITG parser outputs for each fs  
a K-best list of E-spans ? ??; ?1
?  based on the 
current parameter values ?1
?.  
The error function is based on the presence and 
the rank of the correct E-span in the K-best list:  
?  ?? , ? ??; ?1
?  =  
????? ??  ?? ?? ? ? ??; ?1
? 
???????      ?????????          
  
(2)    
where ???? ??  is the (0-based) rank of the cor-
rect E-span ?? in the K-best list  ? ??; ?1
? . If  ?? is 
not in the K-best list at all, then the error is de-
fined to be ???????, which is set as -100000 in 
our experiments. The rationale underlying this 
error function is to keep as many correct E-spans 
as possible in the K-best lists of E-spans, and 
push the correct E-spans upward as much as 
possible in the K-best lists. 
This new error measure leads to a change in 
details of the training algorithm. In MERT for 
SMT, the interval boundaries at which the per-
formance or error measure changes are defined 
by the upper envelope (illustrated by the dash 
line in Figure 4(a)), since the performance/error 
measure depends on the best candidate transla-
tion. In MERT for DPDI, however, the error 
measure depends on the correct E-span rather 
than the E-span leading to the highest system 
score. Thus the interval boundaries are the inter-
sections between the correct E-span and all other 
candidate E-spans (as shown in Figure 4(b)). The 
rank of the correct E-span in each interval can 
then be figured out as shown in Figure 4(c). Fi-
nally, the error measure in each interval can be 
calculated by Equation (2) (as shown in Figure 
4(d)).  All other steps in MERT for DPDI are the 
same as that for SMT. 
??
m
f
m
 
-index
loss
?
k
-8
-9
-10
-8
-9
-100,000
gold
??
m
f
m
?
k
(a)
(b)
(c)
(d)
?
k
?
k
 
Figure 4: MERT for DPDI 
Part (a) shows how intervals are defined for SMT and 
part (b) for DPDI. Part (c) obtains the rank of correct 
E-spans in each interval and part (d) the error measure. 
Note that the beam size (max number of E-spans) for 
each F-span is 10. 
5.3 Features 
The features used in DPDI are divided into three 
categories:  
1) Model 1-based probabilities. Zhang and Gil-
dea (2005) show that Model 1 (Brown et al, 
1993; Och and Ney., 2000) probabilities of 
the word pairs inside and outside a span pair 
( ??1 , ??2 /[??1 ,??2]) are useful. Hence these 
two features: 
a) Inside probability (i.e. probability of 
word pairs within the span pair): 
????  ??1,?2 ??1,?2 
=   
1
 ?2 ? ?1 
??1 ?? ?? 
?? ?1,?2 ?? ?1,?2 
 
b) Outside probability (i.e. probability of 
the word pairs outside the span pair): 
????  ??1,?2 ??1,?2 
=   
1
 ? ? ?2 + ?1 
??1 ?? ?? 
? ? ?1,?2 ?? ?1,?2 
   
where ? is the length of the foreign sen-
tence. 
2) Heuristics. There are four features in this cat-
egory. The features are explained with the 
319
example of Figure 5, in which the span pair 
in interest is  ?2, ?3 /[?1,?2]. The four links 
are produced by some simpler alignment 
model like HMM. The word pair  ?2/?1  is 
the only link in the span pair. The links 
?4/?2  and ?3/?3 are inconsistent with the 
span pair.3  
f1      f2      f3      f4
e1     e2      e3      e4
 
Figure 5: Example for heuristic features 
a) Link ratio: 
2?#?????
???? +????
 
where #?????  is the number of links in 
the span pair, and ???? and ???? are the 
length of the foreign and English spans 
respectively. The feature value of the ex-
ample span pair is (2*1)/(2+2)=0.5. 
b) inconsistent link ratio: 
2?#????? ?????
???? +????
  
where #??????????  is the number of links 
which are inconsistent with the phrase 
pair according to some simpler alignment 
model (e.g. HMM). The feature value of 
the example is (2*2)/(2+2) =1.0. 
c) Length ratio: 
????
????
? ????????   
where ????????  is defined as the average 
ratio of foreign sentence length to Eng-
lish sentence length, and it is estimated to 
be around 1.15 in our training dataset. 
The rationale underlying this feature is 
that the ratio of span length should not be 
too deviated from the average ratio of 
sentence length. The feature value for the 
example is |2/2-1.15|=0.15. 
d) Position Deviation: ???? ? ????    
where ????  refers to the position of the 
F-span in the entire foreign sentence, and 
it is defined as 
1
2?
 ?????? + ???? , 
??????  /????  being the position of the 
first/last word of the F-span in the for-
eign sentence. ????  is defined similarly. 
The rationale behind this feature is the 
monotonic assumption, i.e. a phrase of 
the foreign sentence usually occupies 
roughly the same position of the equiva-
lent English phrase. The feature value for 
                                                 
3
 An inconsistent link connects a word within the phrase pair 
to some word outside the phrase pair. C.f. Deng et al (2008) 
the example is |(1+2)/(2*4)-(2+3)/(2*4)| 
=0.25. 
3) HMM-based probabilities. Haghighi et al 
(2009) show that posterior probabilities from 
the HMM alignment model is useful for 
pruning. Therefore, we design two new fea-
tures by replacing the link count in link ratio 
and inconsistent link ratio with the sum of the 
link?s posterior probability. 
6 The DITG Models 
The discriminative ITG alignment can be con-
ceived as a two-staged process. In the first stage 
DPDI selects good span pairs. In the second stage 
good alignment hypotheses are assigned to the 
span pairs selected by DPDI. Two discriminative 
ITG (DITG) models are investigated. One is 
word-to-word DITG (henceforth W-DITG), 
which observes the 1-to-1 constraint on align-
ment. Another is DITG with hierarchical phrase 
pairs (henceforth HP-DITG), which relaxes the 1-
to-1 constraint by adopting hierarchical phrase 
pairs in Chiang (2007).  
Each model selects the best alignment hypo-
theses of each span pair, given a set of features. 
The contributions of these features are integrated 
through a log linear model (similar to Liu et al, 
2005; Moore, 2005) like Equation (1). The dis-
criminative training of the feature weights is 
again MERT (Och, 2003). The MERT module 
for DITG takes alignment F-score of a sentence 
pair as the performance measure. Given an input 
sentence pair and the reference annotated align-
ment, MERT aims to maximize the F-score of 
DITG-produced alignment. Like SMT (and un-
like DPDI), it is the upper envelope which de-
fines the intervals where the performance meas-
ure changes. 
6.1 Word-to-word DITG 
The following features about alignment link are 
used in W-DITG: 
1) Word pair translation probabilities trained 
from HMM model (Vogel, et.al., 1996) 
and IBM model 4 (Brown et.al., 1993; 
Och and Ney, 2000). 
2) Conditional link probability (Moore, 2005). 
3) Association score rank features (Moore et 
al., 2006). 
4) Distortion features: counts of inversion 
and concatenation. 
5) Difference between the relative positions 
of the words. The relative position of a 
word in a sentence is defined as the posi-
320
tion of the word divided by sentence 
length.  
6) Boolean features like whether a word in 
the word pair is a stop word.  
6.2 DITG with Hierarchical Phrase Pairs 
The 1-to-1 assumption in ITG is a serious limita-
tion as in reality there are always segmentation or 
tokenization errors as well as idiomatic expres-
sions. Wu (1997) proposes a bilingual segmenta-
tion grammar extending the terminal rules by 
including phrase pairs. Cherry and Lin (2007) 
incorporate phrase pairs in phrase-based SMT 
into ITG, and Haghighi et al (2009) introduce 
Block ITG (BITG), which adds 1-to-many or 
many-to-1 terminal unary rules.  
It is interesting to see if DPDI can benefit the 
parsing of a more realistic ITG. HP-DITG ex-
tends Cherry and Lin?s approach by not only em-
ploying simple phrase pairs but also hierarchical 
phrase pairs (Chiang, 2007). The grammar is 
enriched with rules of the format: ?? ? ?/? ? 
where ? ?  and ? ?  refer to the English and foreign 
side of the i-th (simple/hierarchical) phrase pair 
respectively.  
As example, if there is a simple phrase pair 
??  ????? ?????,? ?? , then it is trans-
formed into the ITG rule ?? "North Korea"/
"? ??". During parsing, each span pair does 
not only examine all possible combinations of 
sub-span pairs using binary rules, but also checks 
if the yield of that span pair is exactly the same as 
that phrase pair. If so, then the alignment links 
within the phrase pair (which are obtained in 
standard phrase pair extraction procedure) are 
taken as an alternative alignment hypothesis of 
that span pair.  
For a hierarchical phrase pair like 
??  ?1 ?? ?2 ,?2 ? ?1 , it is transformed into 
the ITG rule  ?? "?1 ?? ?2"/"?2 ? ?1"  during 
parsing, each span pair checks if it contains the 
lexical anchors "of" and "?", and if the remain-
ing words in its yield can form two sub-span 
pairs which fit the reordering constraint among 
?1 and ?2. (Note that span pairs of any category 
in the ITG normal form grammar can substitute 
for ?1 or ?2 .) If both conditions hold, then the 
span pair is assigned an alignment hypothesis 
which combines the alignment links among the 
lexical anchors (???? ??/?)  and those links 
among the sub-span pairs.  
HP-ITG acquires the rules from HMM-based 
word-aligned corpus using standard phrase pair 
extraction as stated in Chiang (2007). The rule 
probabilities and lexical weights in both English-
to-foreign and foreign-to-English directions are 
estimated and taken as features, in addition to 
those features in W-DITG, in the discriminative 
model of alignment hypothesis selection.  
7 Evaluation 
DPDI is evaluated against the baselines of Tic-
tac-toe (TTT) pruning (Zhang and Gildea, 2005) 
and Dynamic Program (DP) pruning (Haghighi et 
al., 2009; DeNero et al, 2009) with respect to 
Chinese-to-English alignment and translation. 
Based on DPDI, HP-DITG is evaluated against 
the alignment systems GIZA++ and BITG. 
7.1 Evaluation Criteria 
Four evaluation criteria are used in addition to 
the time spent on ITG parsing. We will first eva-
luate pruning regarding the pruning decisions 
themselves. That is, the first evaluation metric, 
pruning error rate (henceforth PER), measures 
how many correct E-spans are discarded. The 
major drawback of PER is that not all decisions 
in pruning would impact on alignment quality, 
since certain F-spans are of little use to the entire 
ITG parse tree.  
An alternative criterion is the upper bound on 
alignment F-score, which essentially measures 
how many links in annotated alignment can be 
kept in ITG parse. The calculation of F-score up-
per bound is done in a bottom-up way like ITG 
parsing. All leaf hypernodes which contain a cor-
rect link are assigned a score (known as hit) of 1. 
The hit of a non-leaf hypernode is based on the 
sum of hits of its daughter hypernodes. The max-
imal sum among all hyperedges of a hypernode is 
assigned to that hypernode. Formally,  
??? ? ? , ?  = 
???
?,?,? 1 ,? 1 ,? 2 ,? 2
(??? ? ? 1, ? 1  + ???[? 2, ? 2]) 
??? ??  ?, ?  =  
1      ??  ?, ? ? ?
0        ????????? 
  
??? ?? = 0;??? ?? = 0 
where ?,?,? are variables for the categories in 
ITG grammar, and ? comprises the golden links 
in annotated alignment. ?? , ?? , ??  are defined in 
Appendix A. 
Figure 6 illustrates the calculation of the hit 
score for the example in Section 5.1/Figure 2. 
The upper bound of recall is the hit score divided 
by the total number of golden links. The upper 
321
ID pruning beam size pruning/total time cost PER F-UB F-score 
1 DPDI 10 72??/3?03?? 4.9% 88.5% 82.5% 
2 TTT 10 58??/2?38?? 8.6% 87.5% 81.1% 
3 TTT 20 53??/6?55?? 5.2% 88.6% 82.4% 
4 DP -- 11??/6?01?? 12.1% 86.1% 80.5% 
Table 1: Evaluation of DPDI against TTT (Tic-tac-toe) and DP (Dynamic Program) for W-DITG 
ID pruning beam size pruning/total time cost PER F-UB F-score 
1 DPDI 10 72??/5?18?? 4.9% 93.9% 87.0% 
2 TTT 10 58??/4?51?? 8.6% 93.0% 84.8% 
3 TTT 20 53??/12?5?? 5.2% 94.0% 86.5% 
4 DP -- 11??/15?39?? 12.1% 91.4% 83.6% 
Table 2: Evaluation of DPDI against TTT (Tic-tac-toe) and DP (Dynamic Program) for HP-DITG. 
bound of precision, which should be defined as 
the hit score divided by the number of links pro-
duced by the system, is almost always 1.0 in 
practice. The upper bound of alignment F-score 
can thus be calculated as well.  
A?[C,C]
C
w
:
[e1,e1]/[f1,f1]
hit=1
C
e
:
[e1]/?
C
w
:
[e2,e2]/[f1,f1]
C
e
:
[e2]/?
C
w
:
[e3,e3]/[f2,f2]
C:
[e1,e2]/[f1,f1]
hit=max{0+ }=1
C:
[e2,e3]/[f2,f2]
hit=max{0+1}=1
A:
[e1,e3]/[f1,f2]
hit=max{1+1,1+1}=2
C? [C
e
,C
w
]
A?[C,C]
C? [C
e
,C
w
]
hit=1 hit=1hit=0 hit=0
 
Figure 6: Recall Upper Bound Calculation 
Finally, we also do end-to-end evaluation us-
ing both F-score in alignment and Bleu score in 
translation. We use our implementation of hierar-
chical phrase-based SMT (Chiang, 2007), with 
standard features, for the SMT experiments.  
7.2 Experiment Data 
Both discriminative pruning and alignment need 
training data and test data. We use the manually 
aligned Chinese-English dataset as used in Hag-
highi et al (2009). The 491 sentence pairs in this 
dataset are adapted to our own Chinese word 
segmentation standard. 250 sentence pairs are 
used as training data and the other 241 are test 
data. The corresponding numbers of F-spans in 
training and test data are 4590 and 3951 respec-
tively.  
In SMT experiments, the bilingual training da-
taset is the NIST training set excluding the Hong 
Kong Law and Hong Kong Hansard, and our 5-
gram language model is trained from the Xinhua 
section of the Gigaword corpus. The NIST?03 
test set is used as our development corpus and the 
NIST?05 and NIST?08 test sets are our test sets.  
7.3 Small-scale Evaluation 
The first set of experiments evaluates the perfor-
mance of the three pruning methods using the 
small 241-sentence set. Each pruning method is 
plugged in both W-DITG and HP-DITG. IBM 
Model 1 and HMM alignment model are re-
implemented as they are required by the three 
ITG pruning methods.  
The results for W-DITG are listed in Table 1. 
Tests 1 and 2 show that with the same beam size 
(i.e. number of E-spans per F-span), although 
DPDI spends a bit more time (due to the more 
complicated model), DPDI makes far less incor-
rect pruning decisions than the TTT. In terms of 
F-score upper bound, DPDI is 1 percent higher. 
DPDI achieves even larger improvement in ac-
tual F-score. 
To enable TTT achieving similar F-score or F-
score upper bound, the beam size has to be 
doubled and the time cost is more than twice the 
original (c.f. Tests 1 and 3 in Table 1) . 
The DP pruning in Haghighi et.al. (2009) per-
forms much poorer than the other two pruning 
methods. In fact, we fail to enable DP achieve the 
same F-score upper bound as the other two me-
thods before DP leads to intolerable memory 
consumption. This may be due to the use of dif-
ferent HMM model implementations between our 
work and Haghighi et.al. (2009).  
Table 2 lists the results for HP-DITG. Roughly 
the same observation as in W-DITG can be made. 
In addition to the superiority of DPDI, it can also 
be noted that HP-DITG achieves much higher F-
score and F-score upper bound. This shows that 
322
hierarchical phrase is a powerful tool in rectify-
ing the 1-to-1 constraint in ITG. 
Note also that while TTT in Test 3 gets rough-
ly the same F-score upper bound as DPDI in Test 
1, the corresponding F-score is slightly worse. A 
possible explanation is that better pruning not 
only speeds up the parsing/alignment process but 
also guides the search process to focus on the 
most promising region of the search space. 
7.4 Large-scale End-to-End Experiment 
ID Prun-
ing 
beam 
size 
time 
cost 
Bleu-
05 
Bleu-
08 
1 DPDI 10 1092h 38.57 28.31 
2 TTT 10 972h 37.96 27.37 
3 TTT 20 2376h 38.13 27.58 
4 DP -- 2068h 37.43 27.12 
Table 3:  Evaluation of DPDI against TTT and 
DP for HP-DITG  
ID WA-
Model 
F-Score Bleu-05 Bleu-08 
1 HMM 80.1% 36.91 26.86 
2 Giza++ 84.2% 37.70 27.33 
3 BITG 85.9% 37.92 27.85 
4 HP-DITG 87.0% 38.57 28.31 
Table 4:  Evaluation of DPDI against HMM, Gi-
za++ and BITG 
Table 3 lists the word alignment time cost and 
SMT performance of different pruning methods.  
HP-DITG using DPDI achieves the best Bleu 
score with acceptable time cost. Table 4 com-
pares HP-DITG to HMM (Vogel, et al, 1996), 
GIZA++ (Och and Ney, 2000) and BITG (Hag-
highi et al, 2009). It shows that HP-DITG (with 
DPDI) is better than the three baselines both in 
alignment F-score and Bleu score. Note that the 
Bleu score differences between HP-DITG and the 
three baselines are statistically significant (Koehn, 
2004). 
An explanation of the better performance by 
HP-DITG is the better phrase pair extraction due 
to DPDI. On the one hand, a good phrase pair 
often fails to be extracted due to a link inconsis-
tent with the pair. On the other hand, ITG prun-
ing can be considered as phrase pair selection, 
and good ITG pruning like DPDI guides the sub-
sequent ITG alignment process so that less links 
inconsistent to good phrase pairs are produced. 
This also explains (in Tables 2 and 3) why DPDI 
with beam size 10 leads to higher Bleu than TTT 
with beam size 20, even though both pruning me-
thods lead to roughly the same alignment F-score.  
8 Conclusion and Future Work 
This paper reviews word alignment through ITG 
parsing, and clarifies the problem of ITG pruning. 
A discriminative pruning model and two discri-
minative ITG alignments systems are proposed. 
The pruning model is shown to be superior to all 
existing ITG pruning methods, and the HP-DITG 
alignment system is shown to improve state-of-
the-art alignment and translation quality.  
The current DPDI model employs a very li-
mited set of features. Many features are related 
only to probabilities of word pairs. As the success 
of HP-DITG illustrates the merit of hierarchical 
phrase pair, in future we should investigate more 
features on the relationship between span pair 
and hierarchical phrase pair. 
Appendix A. The Normal Form Grammar 
Table 5 lists the ITG rules in normal form as 
used in this paper, which extend the normal form 
in Wu (1997) so as to handle the case of align-
ment to null. 
1  ?  ? ?|?|? 
2  ?  ?  ? ? | ? ? | ? ? | ?? | ? ? | ? ?  
3  ?  ?  ? ? | ? ? | ? ? | ? ?  
  ?  ?   ? ? | ? ?  
4  ?  ? ?? |??? |???  
5  ?  ?  ???  ???   
6 ??  ? ?/? 
7 ??   ? ?/?;?? ? ?/? 
8 ??? ? ??| ???  ?? ;??? ? ?? | ???  ??  
9 ??? ?  ???  ??  ;??? ?  ???  ??   
 
Table 5: ITG Rules in Normal Form 
In these rules, ? is the Start symbol; ? is the 
category for concatenating combination whereas 
? for inverted combination. Rules (2) and (3) are 
inherited from Wu (1997). Rules (4) divide the 
terminal category ?  into subcategories. Rule 
schema (6) subsumes all terminal unary rules for 
some English word ?  and foreign word ? , and 
rule schemas (7) are unary rules for alignment to 
null. Rules (8) ensure all words linked to null are 
combined in left branching manner, while rules 
(9) ensure those words linked to null combine 
with some following, rather than preceding, word 
pair. (Note: Accordingly, all sentences must be 
ended by a special token  ??? , otherwise the 
last word(s) of a sentence cannot be linked to 
null.) If there are both English and foreign words 
linked to null, rule (5) ensures that those English 
323
words linked to null precede those foreign words 
linked to null. 
References  
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Peitra, Robert L. Mercer. 1993. The Mathe-
matics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics, 
19(2):263-311. 
Colin Cherry and Dekang Lin. 2006. Soft Syntactic 
Constraints for Word Alignment through Dis-
criminative Training. In Proceedings of ACL-
COLING.  
Colin Cherry and Dekang Lin. 2007. Inversion 
Transduction Grammar for Joint Phrasal 
Translation Modeling. In Proceedings of SSST, 
NAACL-HLT, Pages:17-24.  
David Chiang. 2007. Hierarchical Phrase-based 
Translation. Computational Linguistics, 33(2). 
John DeNero, Mohit Bansal, Adam Pauls, and Dan 
Klein. 2009. Efficient Parsing for Transducer 
Grammars. In Proceedings of NAACL, Pag-
es:227-235. 
Alexander Fraser and Daniel Marcu. 2006. Semi-
Supervised Training for StatisticalWord 
Alignment. In Proceedings of ACL, Pages:769-
776. 
Aria Haghighi, John Blitzer, John DeNero, and Dan 
Klein. 2009. Better Word Alignments with Su-
pervised ITG Models. In Proceedings of ACL, 
Pages: 923-931. 
Liang Huang and David Chiang. 2005. Better k-best 
Parsing. In Proceedings of IWPT 2005, Pag-
es:173-180. 
Franz Josef Och and Hermann Ney. 2000. Improved 
statistical alignment models. In Proceedings of 
ACL. Pages: 440-447 
Franz Josef Och. 2003. Minimum error rate train-
ing in statistical machine translation. In Pro-
ceedings of ACL,  Pages:160-167. 
Dan Klein and Christopher D. Manning. 2001. Pars-
ing and Hypergraphs. In Proceedings of IWPT, 
Pages:17-19 
Philipp Koehn. 2004. Statistical Significance Tests 
for Machine Translation Evaluation. In Pro-
ceedings of EMNLP,  Pages: 388-395. 
Yang Liu, Qun Liu and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceed-
ings of ACL, Pages: 81-88. 
Robert Moore. 2005. A Discriminative Framework 
for Bilingual Word Alignment. In Proceedings of 
EMNLP 2005, Pages: 81-88. 
Robert Moore, Wen-tau Yih, and Andreas Bode. 2006. 
Improved Discriminative Bilingual Word 
Alignment. In Proceedings of ACL, Pages: 513-
520. 
Stephan Vogel, Hermann Ney, and Christoph Till-
mann. 1996. HMM-based word alignment in 
statistical translation. In Proceedings of COL-
ING, Pages: 836-841. 
Stephan Vogel. 2005. PESA: Phrase Pair Extrac-
tion as Sentence Splitting.  In Proceedings of MT 
Summit. 
Dekai Wu. 1997. Stochastic Inversion Transduc-
tion Grammars and Bilingual Parsing of Pa-
rallel Corpora. Computational Linguistics, 23(3). 
Hao Zhang and Daniel Gildea. 2005. Stochastic Lex-
icalized Inversion Transduction Grammar for 
Alignment. In Proceedings of ACL.  
Hao Zhang, Chris Quirk, Robert Moore, and Daniel 
Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous pars-
ing. In Proceedings of ACL, Pages: 314-323. 
 
324
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 302?310,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Learning Translation Consensus with Structured Label Propagation 
 
?Shujie Liu*, ?Chi-Ho Li, ?Mu Li and ?Ming Zhou
? Harbin Institute of Technology ?Microsoft Research Asia 
 Harbin, China Beijing, China 
shujieliu@mtlab.hit.edu.cn 
 
{chl, muli, mingzhou}@microsoft.com 
 
 
Abstract 
In this paper, we address the issue for 
learning better translation consensus in 
machine translation (MT) research, and 
explore the search of translation consensus 
from similar, rather than the same, source 
sentences or their spans. Unlike previous 
work on this topic, we formulate the 
problem as structured labeling over a much 
smaller graph, and we propose a novel 
structured label propagation for the task. 
We convert such graph-based translation 
consensus from similar source strings into 
useful features both for n-best output re-
ranking and for decoding algorithm. 
Experimental results show that, our method 
can significantly improve machine 
translation performance on both IWSLT 
and NIST data, compared with a state-of-
the-art baseline.  
1 Introduction 
Consensus in translation has? gained more and 
more attention in recent years. The principle of 
consensus can be sketched as ?a translation 
candidate is deemed more plausible if it is 
supported by other translation candidates.? The 
actual formulation of the principle depends on 
whether the translation candidate is a complete 
sentence or just a span of it, whether the candidate 
is the same as or similar to the supporting 
candidates, and whether the supporting candidates 
come from the same or different MT system.  
                                                          
? This work has been done while the first author was visiting 
Microsoft Research Asia. 
Translation consensus is employed in those 
minimum Bayes risk (MBR) approaches where the 
loss function of a translation is defined with 
respect to all other translation candidates. That is, 
the translation with the minimal Bayes risk is the 
one to the greatest extent similar to other 
candidates. These approaches include the work of 
Kumar and Byrne (2004), which re-ranks the n-
best output of a MT decoder, and the work of 
Tromble et al (2008) and Kumar et al (2009), 
which does MBR decoding for lattices and 
hypergraphs.  
Others extend consensus among translations 
from the same MT system to those from different 
MT systems. Collaborative decoding (Li et al, 
2009) scores the translation of a source span by its 
n-gram similarity to the translations by other 
systems. Hypothesis mixture decoding (Duan et al, 
2011) performs a second decoding process where 
the search space is enriched with new hypotheses 
composed out of existing hypotheses from multiple 
systems. 
All these approaches are about utilizing 
consensus among translations for the same (span 
of) source sentence. It should be noted that 
consensus among translations of similar source 
sentences/spans is also helpful for good candidate 
selection. Consider the examples in Figure 1. For 
the source (Chinese) span ??? ? ?? ? ? ?, 
the MT system produced the correct translation for 
the second sentence, but it failed to do so for the 
first one. If the translation of the first sentence 
could take into consideration the translation of the 
second sentence, which is similar to but not 
exactly the same as the first one, the final 
translation output may be improved. 
Following this line of reasoning, a 
discriminative learning method is proposed to 
constrain the translation of an input sentence using 
302
the most similar translation examples from 
translation memory (TM) systems (Ma et al, 
2011). A classifier is applied to re-rank the n-best 
output of a decoder, taking as features the 
information about the agreement with those similar 
translation examples. Alexandrescu and Kirchhoff 
(2009) proposed a graph-based semi-supervised 
model to re-rank n-best translation output. Note 
that these two attempts are about translation 
consensus for similar sentences, and about re-
ranking of n-best output. It is still an open question 
whether translation consensus for similar 
sentences/spans can be applied to the decoding 
process. Moreover, the method in Alexandrescu 
and Kirchhoff (2009) is formulated as a typical and 
simple label propagation, which leads to very large 
graph, thus making learning and search inefficient. 
(c.f. Section 3.) 
In this paper, we attempt to leverage translation 
consensus among similar (spans of) source 
sentences in bilingual training data, by a novel 
graph-based model of translation consensus. 
Unlike Alexandrescu and Kirchhoff (2009), we 
reformulate the task of seeking translation 
consensus among source sentences as structured 
labeling. We propose a novel label propagation 
algorithm for structured labeling, which is much 
more efficient than simple label propagation, and 
derive useful MT decoder features out of it. We 
conduct experiments with IWSLT and NIST data, 
and experimental results show that, our method 
can improve the translation performance 
significantly on both data sets, compared with a 
state-of-the-art baseline. 
2 Graph-based Translation Consensus 
Our MT system with graph-based translation 
consensus adopts the conventional log-linear 
model. For the source string ? , the conditional 
probability of a translation candidate ? is defined 
as: 
???|?? ? exp ?? ???????, ???? ?? ?exp?? ????????, ???? ????????? (1)
where ?  is the feature vector, ?  is the feature 
weights, and ????  is the set of translation 
hypotheses in the search space.  
Based on the commonly used features, two 
kinds of feature are added to equation (1), one is 
graph-based consensus features, which are about 
consensus among the translations of similar 
sentences/spans; the other is local consensus 
features, which are about consensus among the 
translations of the same sentence/span. We 
develop a structured label propagation method, 
which can calculate consensus statistics from 
translation candidates of similar source 
sentences/spans. 
In the following, we explain why the standard, 
simple label propagation is not suitable for 
translation consensus, and then introduce how the 
problem is formulated as an instance of structured 
labeling, with the proposed structured label 
propagation algorithm, in section 3. Before 
elaborating how the graph model of consensus is 
constructed for both a decoder and N-best output 
re-ranking in section 5, we will describe how the 
consensus features and their feature weights can be 
trained in a semi-supervised way, in section 4. 
3 Graph-based Structured Learning 
In general, a graph-based model assigns labels to 
instances by considering the labels of similar 
instances. A graph is constructed so that each 
instance is represented by a node, and the weight 
of the edge between a pair of nodes represents the 
similarity between them. The gist of graph-based 
model is that, if two instances are connected by a 
strong edge, then their labels tend to be the same 
(Zhu, 2005). 
 
IWSLT Chinese to English Translation Task 
Src ? ??? ?? ? ?? ? ? ? 
Ref Do you have any tea under five 
hundred dollars ? 
Best1 Do you have any less than five 
hundred dollars tea ? 
Src ? ?? ?? ? ?? ? ? . 
Ref I would like some tea under five 
hundred dollars . 
Best1 I would like tea under five hundred 
dollars . 
Figure 1. Two sentences from IWSLT 
(Chinese to English) data set. "Src" stands for 
the source sentence, and "Ref" means the 
reference sentence. "Best1" is the final output 
of the decoder. 
303
In MT, the instances are source sentences or 
spans of source sentences, and the possible labels 
are their translation candidates. This scenario 
differs from the general case of graph-based model 
in two aspects. First, there are an indefinite, or 
even intractable, number of labels. Each of them is 
a string of words rather than a simple category. In 
the following we will call these labels as structured 
labels (Berlett et al, 2004). Second, labels are 
highly ?instance-dependent?. In most cases, for any 
two different (spans of) source sentences, however 
small their difference is, their correct labels 
(translations) are not exactly the same. Therefore, 
the principle of graph-based translation consensus 
must be reformulated as, if two instances (source 
spans) are similar, then their labels (translations) 
tend to be similar (rather than the same). 
Note that Alexandrescu and Kirchhoff (2009) do 
not consider translation as structured labeling. In 
their graph, a node does not represent only a 
source sentence but a pair of source sentence and 
its candidate translation, and there are only two 
possible labels for each node, namely, 1 (this is a 
good translation pair) and 0 (this is not a good 
translation pair). Thus their graph-based model is a 
normal example of the general graph-based model. 
The biggest problem of such a perspective is 
inefficiency. An average MT decoder considers a 
vast amount of translation candidates for each 
source sentence, and therefore the corresponding 
graph also contains a vast amount of nodes, thus 
rendering learning over a large dataset is infeasible. 
3.1 Label Propagation for General Graph-
based Models 
A general graph-based model is iteratively trained 
by label propagation, in which ??,?, the probability 
of label l for the node ?, is updated with respect to 
the corresponding probabilities for ??s neighboring 
nodes ???? . In Zhu (2005), the updating rule is 
expressed in a matrix calculation. For convenience, 
the updating rule is expressed for each label here: 
??,???? ? ? ???, ????,??
??????
 
 
(2)
where ???, ??,  the propagating probability, is 
defined as: 
???, ?? ? ??,?? ??,?????????  
 
(3)
??,?  defines the weight of the edge, which is a 
similarity measure between nodes ? and ?. 
Note that the graph contains nodes for training 
instances, whose correct labels are known. The 
probability of the correct label to each training 
instance is reset to 1 at the end of each iteration. 
With a suitable measure of instance/node similarity, 
it is expected that an unlabeled instance/node will 
find the most suitable label from similar labeled 
nodes.  
3.2 Structured Label Propagation for Graph-
based Learning 
In structured learning like MT, different instances 
would not have the same correct label, and so the 
updating rule (2) is no longer valid, as the value of 
??,?   should not be calculated based on ??,? . Here 
we need a new updating rule so that ??,?  can be 
updated with respect to ??,?? , where in general 
? ? ??. 
Let us start with the model in Alexandrescu and 
Kirchhoff (2009). According to them, a node in the 
graph represents the pair of some source 
sentence/span ??  and its translation candidate ?? . 
The updating rule (for the label 1 or 0) is: 
???,????? ? ? ????, ??, ???, ????????,????
???,????????,??
??4? 
where ????, ?? is the set of neighbors of the node 
??, ?). 
When the problem is reformulated as structured 
labeling, each node represents the source 
sentence/span only, and the translation candidates 
become labels. The propagating probability 
????, ??, ???, ????? has to be reformulated 
accordingly. A natural way is to decompose it into 
a component for nodes and a component for labels. 
Assuming that the two components are 
independent, then: 
????, ??, ???, ???? ? ????, ??? ????, ????????????5? 
where ????, ??? is the propagating probability from source sentence/span ?? to ? , and ????, ??? is that from translation candidate  ?? to ?.  
The set of neighbors ????, ?? of a pair ??, ?? 
has also to be reformulated in terms of the set of 
neighbors ???? of a source sentence/span ?: 
????, ?? ? ????, ???|?? ? ????, ?? ? ????????6? 
304
where???????is?the?set?of?translation?candidates?
for?source???.?The new updating rule will then be:?
??,???? ? ? ????, ??? ????, ??????,???
???????,????????
?
? ? ? ????, ??? ????, ??????,???
???????????????
?
? ? ????, ??? ? ????, ??????,???
???????????????
???7? 
The new rule updates the probability of a 
translation ?  of a source sentence/span ? with 
probabilities of similar translations ??s  of some 
similar source sentences/spans ??s.  
Propagation probability ????, ??? is as defined in equation (3), and ????, ??? is defined given some similarity measure ?????, ??? between labels ? and 
??: 
????, ??? ? ???
??, ???
? ?????, ????????????? ? ?????????????8? 
Note that rule (2) is a special case of rule (7), 
when ?????, ??? is defined as: 
?????, ??? ? ?
1
0
???????????
???? ? ???;
?????????;
 
4 Features and Training 
The last section sketched the structured label 
propagation algorithm. Before elaborating the 
details of how the actual graph is constructed, we 
would like to first introduce how the graph-based 
translation consensus can be used in an MT system. 
4.1 Graph-based Consensus Features  
The probability as estimated in equation (7) is 
taken as a group of new features in either a 
decoder or an n-best output re-ranker. We will call 
these features collectively as graph-based 
consensus features (GC): 
????, ?? ?????????????????????????????????????????????????????????????????9??
log?? ? ????, ??? ? ????, ??????,??
???????????????
??
Recall that, ???? refers to source sentences/spans 
which are similar with ? , and ?????  refers to 
translation candidates of ?? . ???,??  is initialized 
with the translation posterior of ?? given ?? .The 
translation posterior is normalized in the n-best list. 
For the nodes representing the training sentence 
pairs, this posterior is fixed. ? ????, ???  is the propagating probability in equation (8), with the 
similarity measure ?????, ??? defined as the Dice 
co-efficient over the set of all n-grams in ?  and 
those in ??. That is, 
?????, ??? ? ????????????, ????????? 
where ??????? is the set of n-grams in string ?, and ??????, ?? is the Dice co-efficient over sets ? 
and ?: 
??????, ?? ? 2|? ? ?||?| ? |?| 
We take 1 ? ? ? 4  for similarity between 
translation candidates, thus leading to four features. 
The other propagating probability ????, ??? , as defined in equation (3),  takes symmetrical 
sentence level BLEU as similarity measure1: 
??,?? ?
1
2 ???? ???????, ?
?? ? ??? ????????, ??? 
where ??? ???????, ???  is defined as follows (Liang et al, 2006): 
??? ???????, ??? ?? ? ? ??????, ?
??
2?????
?
???
????10? 
where ? ? ??????, ???  is the IBM BLEU score 
computed over i-grams for hypothesis ? using ?? 
as reference. 
In theory we could use other similarity measures 
such as edit distance, string kernel. Here simple n-
gram similarity is used for the sake of efficiency. 
4.2 Other Features 
In addition to graph-based consensus features, we 
also propose local consensus features, defined over 
the n-best translation candidates as: 
????, ?? ? log ? ? ????|?? ?????, ???
???????
?? (11)
                                                          
1 BLEU is not symmetric, which means, different scores are 
obtained depending on which one is reference and which one 
is hypothesis. 
305
where ????|??? is translation posterior. Like ?? , 
there are four features with respect to the value of 
n in n-gram similarity measure. 
We also use other fundamental features, such as 
translation probabilities, lexical weights, distortion 
probability, word penalty, and language model 
probability. 
4.3 Training Method 
When graph-based consensus is applied to an MT 
system, the graph will have nodes for training data, 
development (dev) data, and test data (details in 
Section 5). There is only one label/translation for 
each training data node. For each dev/test data 
node, the possible labels are the n-best translation 
candidates from the decoder. Note that there is 
mutual dependence between the consensus graph 
and the decoder. On the one hand, the MT decoder 
depends on the graph for the GC features. On the 
other hand, the graph needs the decoder to provide 
the translation candidates as possible labels, and 
their posterior probabilities as initial values of 
various ??,? . Therefore, we can alternatively 
update graph-based consensus features and feature 
weights in the log-linear model. 
Algorithm 1 Semi-Supervised Learning 
??? ? 0; 
??=??????????, ????, ????; 
while not converged do 
 ?? ? ?????????????, ??????, ????, ?????, ???. 
 ????? ? ????????????. 
 ???? ? ?????????, ????, ?????? 
end while 
return last (???,???) 
Algorithm 1 outlines our semi-supervised 
method for such alternative training. The entire 
process starts with a decoder without consensus 
features. Then a graph is constructed out of all 
training, dev, and test data. The subsequent 
structured label propagation provides ??  feature 
values to the MT decoder. The decoder then adds 
the new features and re-trains all the feature 
weights?by Minimum Error Rate Training (MERT) 
(Och, 2003). The decoder with new feature 
weights then provides new n-best candidates and 
their posteriors for constructing another consensus 
graph, which in turn gives rise to next round of 
MERT. This alternation of structured label 
propagation and MERT stops when the BLEU 
score on dev data converges, or a pre-set limit (10 
rounds) is reached. 
5 Graph Construction 
A technical detail is still needed to complete the 
description of graph-based consensus, namely, 
how the actual consensus graph is constructed. We 
will divide the discussion into two sections 
regarding how the graph is used.  
5.1 Graph Construction for Re-Ranking 
When graph-based consensus is used for re-
ranking the n-best outputs of a decoder, each node 
in the graph corresponds to a complete sentence. A 
separate node is created for each source sentence 
in training data, dev data, and test data. For any 
node from training data (henceforth training node), 
it is labeled with the correct translation, and ??,? is 
fixed as 1. If there are sentence pairs with the same 
source sentence but different translations, all the 
translations will be assigned as labels to that 
source sentence, and the corresponding 
probabilities are estimated by MLE. There is no 
edge between training nodes, since we suppose all 
the sentences of the training data are correct, and it 
is pointless to re-estimate the confidence of those 
sentence pairs. 
Each node from dev/test data (henceforth test 
node) is unlabeled, but it will be given an n-best 
list of translation candidates as possible labels 
from a MT decoder. The decoder also provides 
translation posteriors as the initial confidences of 
 
1, e1 a1 c b
2, e1 a1 b c
3, e2 a1 b c
E A B C
1, f1 b c d1
2, f1 d1 b c
3, f2 d1 b c
e1 a1 m n e1 a1 b n e1 d1 b n
0.5
0.5
0.75 0.5
 
Figure 2. A toy graph constructed for re-ranking.  
306
the labels. A test node can be connected to training 
nodes and other test nodes. If the source sentences 
of a test node and some other node are sufficiently 
similar, a similarity edge is created between them. 
In our experiment we measure similarity by 
symmetrical sentence level BLEU of source 
sentences, and 0.3 is taken as the threshold for 
edge creation.  
Figure 2 shows a toy example graph. Each node 
is depicted as rectangle with the upper half 
showing the source sentence and the lower half 
showing the correct or possible labels. Training 
nodes are in grey while test nodes are in white. 
The edges between the nodes are weighted by the 
similarities between the corresponding source 
sentences.   
5.2 Graph Construction for Decoding 
Graph-based consensus can also be used in the 
decoding algorithm, by re-ranking the translation 
candidates of not only the entire source sentence 
but also every source span. Accordingly the graph 
does not contain only the nodes for source 
sentences but also the nodes for all source spans. It 
is needed to find the candidate labels for each 
source span. 
It is not difficult to handle test nodes, since the 
purpose of MT decoder is to get al possible 
segmentations of a source sentence in dev/test data, 
search for the translation candidates of each source 
span, and calculate the probabilities of the 
candidates. Therefore, the cells in the search space 
of a decoder can be directly mapped as test nodes 
in the graph. 
 Training nodes can be handled similarly, by 
applying forced alignment. Forced alignment 
performs phrase segmentation and alignment of 
each sentence pair of the training data using the 
full translation system as in decoding (Wuebker et 
al., 2010). In simpler term, for each sentence pair 
in training data, a decoder is applied to the source 
side, and all the translation candidates that do not 
match any substring of the target side are deleted. 
The cells of in such a reduced search space of the 
decoder can be directly mapped as training nodes 
in the graph, just as in the case of test nodes. Note 
that, due to pruning in both decoding and 
translation model training, forced alignment may 
fail, i.e. the decoder may not be able to produce 
target side of a sentence pair. In such case we still 
map the cells in the search space as training nodes. 
Note also that the shorter a source span is, the 
more likely it appears in more than one source 
sentence. All the translation candidates of the same 
source span in different source sentences are 
merged. 
Edge creation is the same as that in graph 
construction for n-best re-ranking, except that two 
nodes are always connected if they are about a 
span and its sub-span. This exception ensures that 
shorter spans can always receive propagation from 
longer ones, and vice versa.  
Figure 3 shows a toy example. There is one 
node for the training sentence "E A M N" and two 
nodes for the test sentences "E A B C" and "F D B 
C". All the other nodes represent spans. The node 
"M N" and "E A" are created according to the 
forced alignment result of the sentence "E A M N". 
As we see, the translation candidates for "M N" 
and "E A" are not the sub-strings from the target 
sentence of "E A M N". There are two kinds of 
edges. Dash lines are edges connecting nodes of a 
span and its sub-span, such as the one between "E 
A B C" and "E". Solid lines are edges connecting 
nodes with sufficient source side n-gram similarity, 
such as the one between "E A M N" and "E A B 
C". 
 
 
Figure 3. A toy example graph for decoding. 
Edges in dash line indicate relation between a 
span and its sub-span, whereas edges of solid 
line indicate source side similarity. 
307
6 Experiments and Results 
In this section, graph-based translation consensus 
is tested on the Chinese to English translation tasks. 
The evaluation method is the case insensitive IBM 
BLEU-4 (Papineni et al, 2002). Significant testing 
is carried out using bootstrap re-sampling method 
proposed by Koehn (2004) with a 95% confidence 
level. 
6.1 Experimental Data Setting and Baselines 
We test our method with two data settings: one is 
IWSLT data set, the other is NIST data set. Our 
baseline decoder is an in-house implementation of 
Bracketing Transduction Grammar (Dekai Wu, 
1997) (BTG) in CKY-style decoding with a lexical 
reordering model trained with maximum entropy 
(Xiong et al, 2006). The features we used are 
commonly used features as standard BTG decoder, 
such as translation probabilities, lexical weights, 
language model, word penalty and distortion 
probabilities.  
Our IWSLT data is the IWSLT 2009 dialog task 
data set. The training data include the BTEC and 
SLDB training data. The training data contains 81k 
sentence pairs, 655k Chinese words and 806 
English words. The language model is 5-gram 
language model trained with the target sentences in 
the training data. The test set is devset9, and the 
development set for MERT comprises both 
devset8 and the Chinese DIALOG set. The 
baseline results on IWSLT data are shown in Table 
1. 
 devset8+dialog devset9 
Baseline 48.79 44.73 
Table 1. Baselines for IWSLT data 
For the NIST data set, the bilingual training data 
we used is NIST 2008 training set excluding the 
Hong Kong Law and Hong Kong Hansard. The 
training data contains 354k sentence pairs, 8M 
Chinese words and 10M English words. The 
language model is 5-gram language model trained 
with the Giga-Word corpus plus the English 
sentences in the training data. The development 
data utilized to tune the feature weights of our 
decoder is NIST?03 evaluation set, and test sets are 
NIST?05 and NIST?08 evaluation sets. The 
baseline results on NIST data are shown in Table 2. 
 NIST'03 NIST'05 NIST'08 
Baseline 38.57 38.21 27.52 
Table 2. Baselines for NIST data 
6.2 Experimental Result 
Table 3 shows the performance of our consensus-
based re-ranking and decoding on the IWSLT data 
set. To perform consensus-based re-ranking, we 
first use the baseline decoder to get the n-best list 
for each sentence of development and test data, 
then we create graph using the n-best lists and 
training data as we described in section 5.1, and 
perform semi-supervised training as mentioned in 
section 4.3. As we can see from Table 3, our 
consensus-based re-ranking (G-Re-Rank) 
outperforms the baseline significantly, not only for 
the development data, but also for the test data.  
Instead of using graph-based consensus 
confidence as features in the log-linear model, we 
perform structured label propagation (Struct-LP) to 
re-rank the n-best list directly, and the similarity 
measures for source sentences and translation 
candidates are symmetrical sentence level BLEU 
(equation (10)). Using Struct-LP, the performance 
is significantly improved, compared with the 
baseline, but not as well as G-Re-Rank. 
devset8+dialog devset9
Baseline 48.79 44.73 
Struct-LP 49.86 45.54 
G-Re-Rank 50.66 46.52 
G-Re-Rank-GC 50.23 45.96 
G-Re-Rank-LC 49.87 45.84 
G-Decode 51.20 47.31 
G-Decode-GC 50.46 46.21 
G-Decode-LC 50.11 46.17 
Table 3. Consensus-based re-ranking and decoding 
for IWSLT data set. The results in bold type are 
significantly better than the baseline. 
We use the baseline system to perform forced 
alignment procedure on the training data, and 
create span nodes using the derivation tree of the 
forced alignment. We also saved the spans of the 
sentences from development and test data, which 
will be used to create the responding nodes for 
consensus-based decoding. In such a way, we 
create the graph for decoding, and perform semi-
308
supervised training to calculate graph-based 
consensus features, and tune the weights for all the 
features we used. In Table 3, we can see that our 
consensus-based decoding (G-Decode) is much 
better than baseline, and also better than 
consensus-based re-ranking method. That is 
reasonable since the neighbor/local similarity 
features not only re-rank the final n-best output, 
but also the spans during decoding. 
To test the contribution of each kind of features, 
we first remove all the local consensus features 
and perform consensus-based re-ranking and 
decoding (G-Re-Rank-GC and G-Decode-GC), 
and then we remove all the graph-based consensus 
features to test the contribution of local consensus 
features (G-Re-Rank-LC and G-Decode-LC). 
Without the graph-based consensus features, our 
consensus-based re-ranking and decoding is 
simplified into a consensus re-ranking and 
consensus decoding system, which only re-rank 
the candidates according to the consensus 
information of other candidates in the same n-best 
list.  
From Table 3, we can see, the G-Re-Rank-LC 
and G-Decode-LC improve the performance of 
development data and test data, but not as much as 
G-Re-Rank and G-Decode do. G-Re-Rank-GC and 
G-Decode-GC improve the performance of 
machine translation according to the baseline. G-
Re-Rank-GC does not achieve the same 
performance as G-Re-Rank-LC does. Compared 
with G-Decode-LC, the performance with G-
Decode-GC is much better.  
 NIST'03 NIST'05 NIST'08
Baseline 38.57 38.21 27.52 
Struct-LP 38.79 38.52 28.06 
G-Re-Rank 39.21 38.93 28.18 
G-Re-Rank-GC 38.92 38.76 28.21 
G-Re-Rank-LC 38.90 38.65 27.88 
G-Decode 39.62 39.17 28.76 
G-Decode-GC 39.42 39.02 28.51 
G-Decode-LC 39.17 38.70 28.20 
Table 4. Consensus-based re-ranking and decoding 
for NIST data set. The results in bold type are 
significantly better than the baseline. 
We also conduct experiments on NIST data, and 
results are shown in Table 4. The consensus-based 
re-ranking methods are performed in the same way 
as for IWSLT data, but for consensus-based 
decoding, the data set contains too many sentence 
pairs to be held in one graph for our machine. We 
apply the method of Alexandrescu and Kirchhoff 
(2009) to construct separate graphs for each 
development and test sentence without losing 
global connectivity information. We perform 
modified label propagation with the separate 
graphs to get the graph-based consensus for n-best 
list of each sentence, and the graph-based 
consensus will be recorded for the MERT to tune 
the weights. 
From Table 4, we can see that, Struct-LP 
improves the performance slightly, but not 
significantly. Local consensus features (G-Re-
Rank-LC and G-Decode-LC) improve the 
performance slightly. The combination of graph-
based and local consensus features can improve 
the translation performance significantly on SMT 
re-ranking. With graph-based consensus features, 
G-Decode-GC achieves significant performance 
gain, and combined with local consensus features, 
G-Decode performance is improved farther. 
7 Conclusion and Future Work 
In this paper, we extend the consensus method by 
collecting consensus statistics, not only from 
translation candidates of the same source 
sentence/span, but also from those of similar ones. 
To calculate consensus statistics, we develop a 
novel structured label propagation method for 
structured learning problems, such as machine 
translation. Note that, the structured label 
propagation can be applied to other structured 
learning tasks, such as POS tagging and syntactic 
parsing. The consensus statistics are integrated into 
the conventional log-linear model as features. The 
features and weights are tuned with an iterative 
semi-supervised method. We conduct experiments 
on IWSLT and NIST data, and our method can 
improve the performance significantly. 
In this paper, we only tried Dice co-efficient of 
n-grams and symmetrical sentence level BLEU as 
similarity measures. In the future, we will explore 
other consensus features and other similarity 
measures, which may take document level 
information, or syntactic and semantic information 
into consideration. We also plan to introduce 
feature to model the similarity of the source 
309
sentences, which are reflected by only one score in 
our paper, and optimize the parameters with CRF 
model. 
References  
Andrei Alexandrescu, Katrin Kirchhoff. 2009. Graph-
based learning for statistical machine translation. In 
Proceedings of Human Language Technologies and 
Annual Conference of the North American Chapter 
of the ACL, pages 119-127. 
Peter L. Bertlett, Michael Collins, Ben Taskar and 
David McAllester. 2004. Exponentiated gradient 
algorithms for large-margin structured classification. 
In Proceedings of Advances in Neural Information 
Processing Systems. 
John DeNero, David Chiang, and Kevin Knight. 2009. 
Fast consensus decoding over translation forests. In 
Proceedings of the Association for Computational 
Linguistics, pages 567-575. 
John DeNero, Shankar Kumar, Ciprian Chelba and 
Franz Och. 2010. Model combination for machine 
translation. In Proceedings of the North American 
Association for Computational Linguistics, pages 
975-983. 
Nan Duan, Mu Li, Dongdong Zhang, and Ming Zhou. 
2010. Mixture model-based minimum bayes risk 
decoding using multiple machine translation Systems. 
In Proceedings of the International Conference on 
Computational Linguistics, pages 313-321. 
Philipp Koehn. 2004. Statistical significance tests for 
machine translation evaluation. In Proceedings of the 
Conference on Empirical Methods on Natural 
Language Processing, pages 388-395. 
Shankar Kumar and William Byrne. 2004. Minimum 
bayes-risk decoding for statistical machine 
translation. In Proceedings of the North American 
Association for Computational Linguistics, pages 
169-176. 
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and 
Franz Och. 2009. Efficient minimum error rate 
training and minimum bayes-risk decoding for 
translation hypergraphs and lattices. In Proceedings 
of the Association for Computational Linguistics, 
pages 163-171. 
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and 
Ming Zhou. 2009. Collaborative decoding: partial 
hypothesis re-ranking using translation consensus 
between decoders. In Proceedings of the Association 
for Computational Linguistics, pages 585-592. 
Percy Liang, Alexandre Bouchard-Cote, Dan Klein, and 
Ben Taskar. 2006. An end-to-end discriminative 
approach to machine translation. In Proceedings of 
the International Conference on Computational 
Linguistics and the ACL, pages 761-768 
Yanjun Ma, Yifan He, Andy Way, Josef van Genabith. 
2011. Consistent translation using discriminative 
learning: a translation memory-inspired approach. In 
Proceedings of the Association for Computational 
Linguistics, pages 1239-1248. 
Franz Josef Och. 2003. Minimum error rate training in 
statistical machine translation. In Proceedings of the 
Association for Computational Linguistics, pages 
160-167. 
Kishore Papineni, Salim Roukos, Todd Ward and Wei-
jing Zhu. 2002. BLEU: a method for automatic 
evaluation of machine translation. In Proceedings of 
the Association for Computational Linguistics, pages 
311-318. 
Roy Tromble, Shankar Kumar, Franz Och, and 
Wolfgang Macherey. 2008. Lattice minimum bayes-
risk decoding for statistical machine translation. In 
Proceedings of the Conference on Empirical 
Methods on Natural Language Processing, pages 
620-629. 
Dekai Wu. 1997. Stochastic inversion transduction 
grammars and bilingual parsing of parallel corpora. 
Computational Linguistics, 23(3). 
Joern Wuebker, Arne Mauser and Hermann Ney. 2010. 
Training phrase translation models with leaving-one-
out. In Proceedings of the Association for 
Computational Linguistics, pages 475-484. 
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. 
Maximum entropy based phrase reordering model for 
statistical machine translation. In Proceedings of the 
Association for Computational Linguistics, pages 
521-528. 
Xiaojin Zhu. 2005. Semi-supervised learning with 
graphs. Ph.D. thesis, Carnegie Mellon University. 
CMU-LTI-05-192. 
 
 
310
