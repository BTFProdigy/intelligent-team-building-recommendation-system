Proceedings of NAACL HLT 2007, pages 372?379,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Applying Many-to-Many Alignments and Hidden Markov Models to
Letter-to-Phoneme Conversion
Sittichai Jiampojamarn, Grzegorz Kondrak and Tarek Sherif
Department of Computing Science,
University of Alberta,
Edmonton, AB, T6G 2E8, Canada
{sj,kondrak,tarek}@cs.ualberta.ca
Abstract
Letter-to-phoneme conversion generally
requires aligned training data of letters
and phonemes. Typically, the align-
ments are limited to one-to-one align-
ments. We present a novel technique of
training with many-to-many alignments.
A letter chunking bigram prediction man-
ages double letters and double phonemes
automatically as opposed to preprocess-
ing with fixed lists. We also apply
an HMM method in conjunction with
a local classification model to predict a
global phoneme sequence given a word.
The many-to-many alignments result in
significant improvements over the tradi-
tional one-to-one approach. Our system
achieves state-of-the-art performance on
several languages and data sets.
1 Introduction
Letter-to-phoneme (L2P) conversion requires a sys-
tem to produce phonemes that correspond to a given
written word. Phonemes are abstract representa-
tions of how words should be pronounced in natural
speech, while letters or graphemes are representa-
tions of words in written language. For example, the
phonemes for the word phoenix are [ f i n I k s ].
The L2P task is a crucial part of speech synthesis
systems, as converting input text (graphemes) into
phonemes is the first step in representing sounds.
L2P conversion can also help improve performance
in spelling correction (Toutanova and Moore, 2001).
Unfortunately, proper nouns and unseen words pre-
vent a table look-up approach. It is infeasible to con-
struct a lexical database that includes every word in
the written language. Likewise, orthographic com-
plexity of many languages prevents us from using
hand-designed conversion rules. There are always
exceptional rules that need to be added to cover a
large vocabulary set. Thus, an automatic L2P sys-
tem is desirable.
Many data-driven techniques have been proposed
for letter-to-phoneme conversion systems, including
pronunciation by analogy (Marchand and Damper,
2000), constraint satisfaction (Van Den Bosch and
Canisius, 2006), Hidden Markov Model (Taylor,
2005), decision trees (Black et al, 1998), and
neural networks (Sejnowski and Rosenberg, 1987).
The training data usually consists of written words
and their corresponding phonemes, which are not
aligned; there is no explicit information indicating
individual letter and phoneme relationships. These
relationships must be postulated before a prediction
model can be trained.
Previous work has generally assumed one-to-one
alignment for simplicity (Daelemans and Bosch,
1997; Black et al, 1998; Damper et al, 2005).
An expectation maximization (EM) based algo-
rithm (Dempster et al, 1977) is applied to train the
aligners. However, there are several problems with
this approach. Letter strings and phoneme strings
are not typically the same length, so null phonemes
and null letters must be introduced to make one-
to-one-alignments possible, Furthermore, two letters
frequently combine to produce a single phoneme
372
(double letters), and a single letter can sometimes
produce two phonemes (double phonemes).
To help address these problems, we propose an
automatic many-to-many aligner and incorporate it
into a generic classification predictor for letter-to-
phoneme conversion. Our many-to-many aligner
automatically discovers double phonemes and dou-
ble letters, as opposed to manually preprocessing
data by merging phonemes using fixed lists. To our
knowledge, applying many-to-many alignments to
letter-to-phoneme conversion is novel.
Once we have our many-to-many alignments, we
use that data to train a prediction model. Many
phoneme prediction systems are based on local pre-
diction methods, which focus on predicting an indi-
vidual phoneme given each letter in a word. Con-
versely, a method like pronunciation by analogy
(PbA) (Marchand and Damper, 2000) is considered
a global prediction method: predicted phoneme se-
quences are considered as a whole. Recently, Van
Den Bosch and Canisius (2006) proposed trigram
class prediction, which incorporates a constraint sat-
isfaction method to produce a global prediction for
letter-to-phoneme conversion. Both PbA and tri-
gram class prediction show improvement over pre-
dicting individual phonemes, confirming that L2P
systems can benefit from incorporating the relation-
ship between phonemes in a sequence.
In order to capitalize on the information found
in phoneme sequences, we propose to apply an
HMM method after a local phoneme prediction pro-
cess. Given a candidate list of two or more possible
phonemes, as produced by the local predictor, the
HMM will find the best phoneme sequence. Using
this approach, our system demonstrates an improve-
ment on several language data sets.
The rest of the paper is structured as follows.
We describe the letter-phoneme alignment methods
including a standard one-to-one alignment method
and our many-to-many approach in Section 2. The
alignment methods are used to align graphemes
and phonemes before the phoneme prediction mod-
els can be trained from the training examples. In
Section 3, we present a letter chunk prediction
method that automatically discovers double letters
in grapheme sequences. It incorporates our many-
to-many alignments with prediction models. In
Section 4, we present our application of an HMM
method to the local prediction results. The results
of experiments on several language data sets are dis-
cussed in Section 5. We conclude and propose future
work in Section 6.
2 Letter-phoneme alignment
2.1 One-to-one alignment
There are two main problems with one-to-one align-
ments:
1. Double letters: two letters map to one phoneme
(e.g. sh - [ S ], ph - [ f ]).
2. Double phonemes: one letter maps to two
phonemes (e.g. x - [ k s ], u - [ j u ]).
First, consider the double letter problem. In most
cases when the grapheme sequence is longer than
the phoneme sequence, it is because some letters are
silent. For example, in the word abode, pronounced
[ @ b o d ], the letter e produces a null phoneme (?).
This is well captured by one-to-one aligners. How-
ever, the longer grapheme sequence can also be gen-
erated by double letters; for example, in the word
king, pronounced [ k I N ], the letters ng together
produce the phoneme [ N ]. In this case, one-to-one
aligners using null phonemes will produce an in-
correct alignment. This can cause problems for the
phoneme prediction model by training it to produce
a null phoneme from either of the letters n or g.
In the double phoneme case, a new phoneme is
introduced to represent a combination of two (or
more) phonemes. For example, in the word fume
with phoneme sequence [ f j u m ], the letter u pro-
duces both the [ j ] and [ u ] phonemes. There
are two possible solutions for constructing a one-
to-one alignment in this case. The first is to cre-
ate a new phoneme by merging the phonemes [ j ]
and [ u ]. This requires constructing a fixed list of
new phonemes before beginning the alignment pro-
cess. The second solution is to add a null letter in
the grapheme sequence. However, the null letter not
only confuses the phoneme prediction model, but
also complicates the the phoneme generation phase.
For comparison with our many-to-many ap-
proach, we implement a one-to-one aligner based on
the epsilon scattering method (Black et al, 1998).
The method applies the EM algorithm to estimate
373
Algorithm 1: Pseudocode for a many-to-many
expectation-maximization algorithm.
Algorithm:EM-many2many
Input: xT , yV ,maxX,maxY
Output: ?
forall mapping operations z do
?(z) := 0
foreach sequence pair (xT , yV ) do
Expectation-many2many(xT , yV ,maxX,maxY, ?)
Maximization-Step(?)
the probability of mapping a letter l to a phoneme
p, P (l, p). The initial probability table starts by
mapping all possible alignments between letters and
phonemes for each word in the training data, in-
troducing all possible null phoneme positions. For
example, the word/phoneme-sequence pair abode
[ @ b o d ] has five possible positions where a null
phoneme can be added to make an alignment.
The training process uses the initial probability ta-
ble P (l, p) to find the best possible alignments for
each word using the Dynamic Time Warping (DTW)
algorithm (Sankoff and Kruskal, 1999). At each it-
eration, the probability table P (l, p) is re-calculated
based on the best alignments found in that iteration.
Finding the best alignments and re-calculating the
probability table continues iteratively until there is
no change in the probability table. The final proba-
bility table P (l, p) is used to find one-to-one align-
ments given graphemes and phonemes.
2.2 Many-to-Many alignment
We present a many-to-many alignment algorithm
that overcomes the limitations of one-to-one align-
ers. The training of the many-to-many aligner is
an extension of the forward-backward training of a
one-to-one stochastic transducer presented in (Ris-
tad and Yianilos, 1998). Partial counts are counts of
all possible mappings from letters to phonemes that
are collected in the ? table, while mapping probabil-
ities (initially uniform) are maintained in the ? table.
For each grapheme-/phoneme-sequence pair (x, y),
the EM-many2many function (Algorithm 1) calls the
Expectation-many2many function (Algorithm 2) to
collect partial counts. T and V are the lengths of x
and y respectively. The maxX and maxY variables
are the maximum lengths of subsequences used in
a single mapping operation for x and y. (For the
Algorithm 2: Pseudocode for a many-to-many
expectation algorithm.
Algorithm:Expectation-many2many
Input: xT , yV ,maxX,maxY, ?
Output: ?
? := Forward-many2many (xT , yV ,maxX,maxY )
? := Backward-many2many (xT , yV ,maxX,maxY )
if (?T,V = 0) then
return
for t = 0...T do
for v = 0...V do
if (t > 0 ?DELX) then
for i = 1...maxX st t? i ? 0 do
?(xtt?i+1, ?)+ =
?t?i,v?(x
t
t?i+1,?)?t,v
?T,V
if (v > 0 ?DELY ) then
for j = 1...maxY st v ? j ? 0 do
?(?, yvv?j+1)+ =
?t,v?j?(?,y
v
v?j+1)?t,v
?T,V
if (v > 0 ? t > 0) then
for i = 1...maxX st t? i ? 0 do
for j = 1...maxY st v ? j ? 0 do
?(xtt?i+1, y
v
v?j+1)+ =
?t?i,v?j?(x
t
t?i+1,y
v
v?j+1)?t,v
?T,V
task at hand, we set both maxX and maxY to 2.)
The Maximization-step function simply normalizes
the partial counts to create a probability distribution.
Normalization can be done over the whole table to
create a joint distribution or per grapheme to create
a conditional distribution.
The Forward-many2many function (Algorithm 3)
fills in the table ?, with each entry ?(t, v) being the
sum of all paths through the transducer that gen-
erate the sequence pair (xt1, yv1). Analogously, the
Backward-many2many function fills in ?, with each
entry ?(t, v) being the sum of all paths through the
transducer that generate the sequence pair (xTt , yVv ).
The constants DELX and DELY indicate whether
or not deletions are allowed on either side. In our
system, we allow letter deletions (i.e. mapping of
letters to null phoneme), but not phoneme deletions.
Expectation-many2many first calls the two func-
tions to fill the ? and ? tables, and then uses the
probabilities to calculate partial counts for every
possible mapping in the sequence pair. The par-
tial count collected at positions t and v in the se-
quence pair is the sum of all paths that generate the
sequence pair and go through (t, v), divided by the
sum of all paths that generate the entire sequence
pair (?(T, V )).
Once the probabilities are learned, the Viterbi
374
Algorithm 3: Pseudocode for a many-to-many
forward algorithm.
Algorithm:Forward-many2many
Input: (xT , yV ,maxX,maxY )
Output: ?
?0,0 := 1
for t = 0...T do
for v = 0...V do
if (t > 0 ? v > 0) then
?t,v = 0
if (t > 0 ?DELX) then
for i = 1...maxX st t? i ? 0 do
?t,v+ = ?(xtt?i+1, ?)?t?i,v
if (v > 0 ?DELY ) then
for j = 1...maxY st v ? j ? 0 do
?t,v+ = ?(?, yvv?j+1)?t,v?j
if (v > 0 ? t > 0) then
for i = 1...maxX st t? i ? 0 do
for j = 1...maxY st v ? j ? 0 do
?t,v+ = ?(xtt?i+1, y
v
v?j+1)?t?i,v?j
algorithm can be used to produce the most likely
alignment as in the following equations. Back point-
ers to maximizing arguments are kept at each step so
the alignment can be reconstructed.
?(0, 0) = 1 (1)
?(t, v) = max
1?i?maxX,
1?j?maxY
8
<
:
?(xtt?i+1, ?)?t?i,v
?(?, yvv?j+1)?t,v?j
?(xtt?i+1, y
v
v?j+1)?t?i,v?j
(2)
Given a set of words and their phonemes, align-
ments are made across graphemes and phonemes.
For example, the word phoenix, with phonemes [ f i
n I k s ], is aligned as:
ph oe n i x
| | | | |
f i n I ks
The letters ph are an example of the double let-
ter problem (mapping to the single phoneme [ f ]),
while the letter x is an example of the double
phoneme problem (mapping to both [ k ] and [ s ]
in the phoneme sequence). These alignments pro-
vide more accurate grapheme-to-phoneme relation-
ships for a phoneme prediction model.
3 Letter chunking
Our new alignment scheme provides more accu-
rate alignments, but it is also more complex ?
sometimes a prediction model should predict two
phonemes for a single letter, while at other times
the prediction model should make a prediction based
on a pair of letters. In order to distinguish between
these two cases, we propose a method called ?letter
chunking?.
Once many-to-many alignments are built across
graphemes and phonemes, each word contains a set
of letter chunks, each consisting of one or two let-
ters aligned with phonemes. Each letter chunk can
be considered as a grapheme unit that contains either
one or two letters. In the same way, each phoneme
chunk can be considered as a phoneme unit consist-
ing of one or two phonemes. Note that the double
letters and double phonemes are implicitly discov-
ered by the alignments of graphemes and phonemes.
They are not necessarily consistent over the train-
ing data but based on the alignments found in each
word.
In the phoneme generation phase, the system has
only graphemes available to predict phonemes, so
there is no information about letter chunk bound-
aries. We cannot simply merge any two letters that
have appeared as a letter chunk in the training data.
For example, although the letter pair sh is usually
pronounced as a single phoneme in English (e.g.
gash [ g ae S ]), this is not true universally (e.g.
gasholder [ g ae s h o l d @ r ]). Therefore, we im-
plement a letter chunk prediction model to provide
chunk boundaries given only graphemes.
In our system, a bigram letter chunking predic-
tion automatically discovers double letters based on
instance-based learning (Aha et al, 1991). Since the
many-to-many alignments are drawn from 1-0, 1-1,
1-2, 2-0, and 2-1 relationships, each letter in a word
can form a chunk with its neighbor or stand alone
as a chunk itself. We treat the chunk prediction as
a binary classification problem. We generate all the
bigrams in a word and determine whether each bi-
gram should be a chunk based on its context. Table 1
shows an example of how chunking prediction pro-
ceeds for the word longs. Letters li?2, li?1, li+1, and
li+2 are the context of the bigram li; chunk = 1 if
the letter bigram li is a chunk. Otherwise, the chunk
simply consists of an individual letter. In the exam-
ple, the word is decomposed as l|o|ng|s, which can
be aligned with its pronunciation [ l | 6 | N | z ]. If
the model happens to predict consecutive overlap-
ping chunks, only the first of the two is accepted.
375
li?2 li?1 li li+1 li+2 chunk
lo n g 0
l on g s 0
l o ng s 1
o n gs 0
Table 1: An example of letter chunking prediction.
4 Phoneme prediction
Most of the previously proposed techniques for
phoneme prediction require training data to be
aligned in one-to-one alignments. Those models
approach the phoneme prediction task as a classi-
fication problem: a phoneme is predicted for each
letter independently without using other predictions
from the same word. These local predictions assume
independence of predictions, even though there are
clearly interdependencies between predictions. Pre-
dicting each phoneme in a word without considering
other assignments may not satisfy the main goal of
finding a set of phonemes that work together to form
a word.
A trigram phoneme prediction with constraint sat-
isfaction inference (Van Den Bosch and Canisius,
2006) was proposed to improve on local predictions.
From each letter unit, it predicts a trigram class that
has the target phoneme in the middle surrounded by
its neighboring phonemes. The phoneme sequence
is generated in such a way that it satisfies the tri-
gram, bigram and unigram constraints. The over-
lapping predictions improve letter-to-phoneme per-
formance mainly by repairing imperfect one-to-one
alignments.
However, the trigram class prediction tends to be
more complex as it increases the number of tar-
get classes. For English, there are only 58 uni-
gram phoneme classes but 13,005 tri-gram phoneme
classes. The phoneme combinations in the tri-gram
classes are potentially confusing to the prediction
model because the model has more target classes in
its search space while it has access to the same num-
ber of local features in the grapheme side.
We propose to apply a supervised HMM method
embedded with local classification to find the most
likely sequence of phonemes given a word. An
HMM is a statistical model that combines the obser-
vation likelihood (probability of phonemes given let-
ters) and transition likelihood (probability of current
phoneme given previous phonemes) to predict each
phoneme. Our approach differs from a basic Hidden
Markov Model for letter-to-phoneme system (Tay-
lor, 2005) that formulates grapheme sequences as
observation states and phonemes as hidden states.
The basic HMM system for L2P does not provide
good performance on the task because it lacks con-
text information on the grapheme side. In fact, a
pronunciation depends more on graphemes than on
the neighboring phonemes; therefore, the transition
probability (language model) should affect the pre-
diction decisions only when there is more than one
possible phoneme that can be assigned to a letter.
Our approach is to use an instance-based learn-
ing technique as a local predictor to generate a set
of phoneme candidates for each letter chunk, given
its context in a word. The local predictor produces
confidence values for Each candidate phoneme. We
normalize the confidence values into values between
0 and 1, and treat them as the emission probabilities,
while the transition probabilities are derived directly
from the phoneme sequences in the training data.
The pronunciation is generated by considering
both phoneme prediction values and transition prob-
abilities. The optimal phoneme sequence is found
with the Viterbi search algorithm. We limit the size
of the context to n = 3 in order to avoid over-
fitting and minimize the complexity of the model.
Since the candidate set is from the classifier, the
search space is limited to a small number of can-
didate phonemes (1 to 5 phonemes in most cases).
The HMM postprocessing is independent of local
predictions from the classifier. Instead, it selects the
best phoneme sequence from a set of possible lo-
cal predictions by taking advantage of the phoneme
language model, which is trained on the phoneme
sequences in the training data.
5 Evaluation
We evaluated our approaches on CMUDict, Brulex,
and German, Dutch and English Celex cor-
pora (Baayen et al, 1996). The corpora (except
English Celex) are available as part of the Letter-
to-Phoneme Conversion PRONALSYL Challenge1.
1The PRONALSYL Challenge: http://www.
pascal-network.org/Challenges/PRONALSYL/.
376
Language Data set Number of words
English CMUDict 112,102
English Celex 65,936
Dutch Celex 116,252
German Celex 49,421
French Brulex 27,473
Table 2: Number of words in each data set.
For the English Celex data, we removed duplicate
words as well as words shorter than four letters. Ta-
ble 2 shows the number of words and the language
of each corpus.
For all of our experiments, our local classifier
for predicting phonemes is the instance-based learn-
ing IB1 algorithm (Aha et al, 1991) implemented
in the TiMBL package (Daelemans et al, 2004).
The HMM technique is applied as post process-
ing to the instance-based learning to provide a se-
quence prediction. In addition to comparing one-to-
one and many-to-many alignments, we also compare
our method to the constraint satisfaction inference
method as described in Section 4. The results are
reported in word accuracy rate based on the 10-fold
cross validation, with the mean and standard devia-
tion values.
Table 3 shows word accuracy performance across
a variety of methods. We show results comparing
the one-to-one aligner described in Section 2.1 and
the one-to-one aligner provided by the PRONAL-
SYL challenge. The PRONALSYS one-to-one
alignments are taken directly from the PRONAL-
SYL challenge, whose method is based on an EM
algorithm. For both alignments, we use instance-
based learning as the prediction model.
Overall, our one-to-one alignments outperform
the alignments provided by the data sets for all cor-
pora. The main difference between the PRONAL-
SYS one-to-one alignment and our one-to-one align-
ment is that our aligner does not allow a null letter
on the grapheme side. Consider the word abomina-
tion [ @ b 6 m I n e S @ n ]: the first six letters and
phonemes are aligned the same way by both align-
ers (abomin- [ @ b 6 m I n ]). However, the two
aligners produce radically different alignments for
the last five letters. The alignment provided by the
PRONALSYS one-to-one alignments is:
a t i o n
| | | | | | |
e S @ n
while our one-to-one alignment is:
a t i o n
| | | | |
e S @ n
Clearly, the latter alignment provides more informa-
tion on how the graphemes map to the phonemes.
Table 3 also shows that impressive improvements
for all evaluated corpora are achieved by using
many-to-many alignments rather than one-to-one
alignments (1-1 align vs. M-M align). The signif-
icant improvements, ranging from 2.7% to 7.6% in
word accuracy, illustrate the importance of having
more precise alignments. For example, we can now
obtain the correct alignment for the second part of
the word abomination:
a ti o n
| | | |
e S @ n
Instead of adding a null phoneme in the phoneme
sequence, the many-to-many aligner maps the letter
chunk ti to a single phoneme.
The HMM approach is based on the same hy-
pothesis as the constraint satisfaction inference
(CSInf) (Van Den Bosch and Canisius, 2006). The
results in Table 3 (1-1+CSInf vs. 1-1+HMM) show
that the HMM approach consistently improves per-
formance over the baseline system (1-1 align), while
the CSInf degrades performance on the Brulex data
set. For the CSInf method, most errors are caused
by trigram confusion in the prediction phase.
The results of our best system, which combines
the HMM method with the many-to-many align-
ments (M-M+HMM), are better than the results re-
ported in (Black et al, 1998) on both the CMU-
Dict and German Celex data sets. This is true even
though Black et al (1998) use explicit lists of letter-
phoneme mappings during the alignment process,
while our approach is a fully automatic system that
does not require any handcrafted list.
6 Conclusion and future work
We presented a novel technique of applying many-
to-many alignments to the letter-to-phoneme conver-
sion problem. The many-to-many alignments relax
377
Language Data set PRONALSYS 1-1 align 1-1+CsInf 1-1+HMM M-M align M-M+HMM
English CMUDict 58.3? 0.49 60.3? 0.53 62.9? 0.45 62.1? 0.53 65.1? 0.60 65.6? 0.72
English Celex ? 74.6? 0.80 77.8? 0.72 78.5? 0.76 82.2? 0.63 83.6? 0.63
Dutch Celex 84.3? 0.34 86.6? 0.36 87.5? 0.32 87.6? 0.34 91.1? 0.27 91.4? 0.24
German Celex 86.0? 0.40 86.6? 0.54 87.6? 0.47 87.6? 0.59 89.3? 0.53 89.8? 0.59
French Brulex 86.3? 0.67 87.0? 0.38 86.5? 0.68 88.2? 0.39 90.6? 0.57 90.9? 0.45
Table 3: Word accuracies achieved on data sets based on the 10-fold cross validation. PRONALSYS: one-
to-one alignments provided by the PRONALSYL challenge. 1-1 align: our one-to-one alignment method
described in Section 2.1. CsInf: Constraint satisfaction inference (Van Den Bosch and Canisius, 2006).
M-M align: our many-to-many alignment method. HMM: our HMM embedded with a local prediction.
the constraint assumptions of the traditional one-to-
one alignments. Letter chunking bigram prediction
incorporates many-to-many alignments into the con-
ventional phoneme prediction models. Finally, the
HMM technique yields global phoneme predictions
based on language models.
Impressive word accuracy improvements are
achieved when the many-to-many alignments are ap-
plied over the baseline system. On several languages
and data sets, using the many-to-many alignments,
word accuracy improvements ranged from 2.7% to
7.6%, as compared to one-to-one alignments. The
HMM cooperating with the local predictions shows
slight improvements when it is applied to the many-
to-many alignments. We illustrated that the HMM
technique improves the word accuracy more con-
sistently than the constraint-based approach. More-
over, the HMM can be easily incorporated into the
many-to-many alignment approach.
We are investigating the possibility of integrat-
ing syllabification information into our system. It
has been reported that syllabification can poten-
tially improve pronunciation performance in En-
glish (Marchand and Damper, 2005). We plan
to explore other sequence prediction approaches,
such as discriminative training methods (Collins,
2004), and sequence tagging with Support Vector
Machines (SVM-HMM) (Altun et al, 2003) to in-
corporate more features (context information) into
the phoneme generation model. We are also inter-
ested in applying our approach to other related areas
such as morphology and transliteration.
Acknowledgements
We would like to thank Susan Bartlett, Colin Cherry,
and other members of the Natural Language Pro-
cessing research group at University of Alberta for
their helpful comments and suggestions. This re-
search was supported by the Natural Sciences and
Engineering Research Council of Canada.
References
David W. Aha, Dennis Kibler, and Marc K. Albert. 1991.
Instance-based learning algorithms. Machine Learn-
ing, 6(1):37?66.
Yasemin Altun, Ioannis Tsochantaridis, and Thomas
Hofmann. 2003. Hidden Markov Support Vector Ma-
chines. In Proceedings of the 20th International Con-
ference on Machine Learning (ICML-2003).
Harald Baayen, Richard Piepenbrock, and Leon Gulikers.
1996. The CELEX2 lexical database. LDC96L14.
Alan W. Black, Kevin Lenzo, and Vincent Pagel. 1998.
Issues in building general letter to sound rules. In The
Third ESCA Workshop in Speech Synthesis, pages 77?
80.
Michael Collins. 2004. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
gauge Processing (EMNLP).
Walter Daelemans and Antal Van Den Bosch. 1997.
Language-independent data-oriented grapheme-to-
phoneme conversion. In Progress in Speech Synthesis,
pages 77?89. Springer, New York.
Walter Daelemans, Jakub Zavrel, Ko Van Der Sloot, and
Antal Van Den Bosch. 2004. TiMBL: Tilburg Mem-
ory Based Learner, version 5.1, reference guide. In
ILK Technical Report Series 04-02.
Robert I. Damper, Yannick Marchand, John DS.
Marsters, and Alexander I. Bazin. 2005. Aligning
text and phonemes for speech technology applications
using an EM-like algorithm. International Journal of
Speech Technology, 8(2):147?160, June.
378
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. In Journal of the Royal Statistical Society,
pages B:1?38.
Yannick Marchand and Robert I. Damper. 2000. A
multistrategy approach to improving pronunciation by
analogy. Computational Linguistics, 26(2):195?219,
June.
Yannick Marchand and Robert I. Damper. 2005. Can
syllabification improve pronunciation by analogy of
English? In Natural Language Engineering, pages
(1):1?25.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 20(5):522?532.
David Sankoff and Joseph Kruskal, 1999. Time Warps,
String Edits, and Macromolecules, chapter 2, pages
55?91. CSLI Publications.
Terrence J. Sejnowski and Charles R. Rosenberg. 1987.
Parallel networks that learn to pronounce English text.
In Complex Systems, pages 1:145?168.
Paul Taylor. 2005. Hidden Markov Models for grapheme
to phoneme conversion. In Proceedings of the 9th
European Conference on Speech Communication and
Technology 2005.
Kristina Toutanova and Robert C. Moore. 2001. Pro-
nunciation modeling for improved spelling correction.
In ACL ?02: Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, pages
144?151, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Antal Van Den Bosch and Sander Canisius. 2006.
Improved morpho-phonological sequence processing
with constraint satisfaction inference. Proceedings of
the Eighth Meeting of the ACL Special Interest Group
in Computational Phonology, SIGPHON ?06, pages
41?49, June.
379
Proceedings of ACL-08: HLT, pages 905?913,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Joint Processing and Discriminative Training for
Letter-to-Phoneme Conversion
Sittichai Jiampojamarn? Colin Cherry? Grzegorz Kondrak?
?Department of Computing Science ?Microsoft Research
University of Alberta One Microsoft Way
Edmonton, AB, T6G 2E8, Canada Redmond, WA, 98052
{sj,kondrak}@cs.ualberta.ca colinc@microsoft.com
Abstract
We present a discriminative structure-
prediction model for the letter-to-phoneme
task, a crucial step in text-to-speech process-
ing. Our method encompasses three tasks
that have been previously handled separately:
input segmentation, phoneme prediction,
and sequence modeling. The key idea is
online discriminative training, which updates
parameters according to a comparison of the
current system output to the desired output,
allowing us to train all of our components
together. By folding the three steps of a
pipeline approach into a unified dynamic
programming framework, we are able to
achieve substantial performance gains. Our
results surpass the current state-of-the-art on
six publicly available data sets representing
four different languages.
1 Introduction
Letter-to-phoneme (L2P) conversion is the task
of predicting the pronunciation of a word, repre-
sented as a sequence of phonemes, from its or-
thographic form, represented as a sequence of let-
ters. The L2P task plays a crucial role in speech
synthesis systems (Schroeter et al, 2002), and is
an important part of other applications, including
spelling correction (Toutanova and Moore, 2001)
and speech-to-speech machine translation (Engel-
brecht and Schultz, 2005).
Converting a word into its phoneme represen-
tation is not a trivial task. Dictionary-based ap-
proaches cannot achieve this goal reliably, due to
unseen words and proper names. Furthermore, the
construction of even a modestly-sized pronunciation
dictionary requires substantial human effort for each
new language. Effective rule-based approaches can
be designed for some languages such as Spanish.
However, Kominek and Black (2006) show that in
languages with a less transparent relationship be-
tween spelling and pronunciation, such as English,
Dutch, or German, the number of letter-to-sound
rules grows almost linearly with the lexicon size.
Therefore, most recent work in this area has focused
on machine-learning approaches.
In this paper, we present a joint framework for
letter-to-phoneme conversion, powered by online
discriminative training. By updating our model pa-
rameters online, considering only the current system
output and its feature representation, we are able to
not only incorporate overlapping features, but also to
use the same learning framework with increasingly
complex search techniques. We investigate two on-
line updates: averaged perceptron and Margin In-
fused Relaxed Algorithm (MIRA). We evaluate our
system on L2P data sets covering English, French,
Dutch and German. In all cases, our system outper-
forms the current state of the art, reducing the best
observed error rate by as much as 46%.
2 Previous work
Letter-to-phoneme conversion is a complex task, for
which a number of diverse solutions have been pro-
posed. It is a structure prediction task; both the input
and output are structured, consisting of sequences of
letters and phonemes, respectively. This makes L2P
a poor fit for many machine-learning techniques that
are formulated for binary classification.
905
The L2P task is also characterized by the exis-
tence of a hidden structure connecting input to out-
put. The training data consists of letter strings paired
with phoneme strings, without explicit links con-
necting individual letters to phonemes. The subtask
of inserting these links, called letter-to-phoneme
alignment, is not always straightforward. For ex-
ample, consider the word ?phoenix? and its corre-
sponding phoneme sequence [f i n I k s], where
we encounter cases of two letters generating a sin-
gle phoneme (ph?f), and a single letter generat-
ing two phonemes (x?k s). Fortunately, align-
ments between letters and phonemes can be discov-
ered reliably with unsupervised generative models.
Originally, L2P systems assumed one-to-one align-
ment (Black et al, 1998; Damper et al, 2005), but
recently many-to-many alignment has been shown
to perform better (Bisani and Ney, 2002; Jiampoja-
marn et al, 2007). Given such an alignment, L2P
can be viewed either as a sequence of classification
problems, or as a sequence modeling problem.
In the classification approach, each phoneme is
predicted independently using a multi-class classi-
fier such as decision trees (Daelemans and Bosch,
1997; Black et al, 1998) or instance-based learn-
ing (Bosch and Daelemans, 1998). These systems
predict a phoneme for each input letter, using the
letter and its context as features. They leverage the
structure of the input but ignore any structure in the
output.
L2P can also be viewed as a sequence model-
ing, or tagging problem. These approaches model
the structure of the output, allowing previously pre-
dicted phonemes to inform future decisions. The
supervised Hidden Markov Model (HMM) applied
by Taylor (2005) achieved poor results, mostly be-
cause its maximum-likelihood emission probabili-
ties cannot be informed by the emitted letter?s con-
text. Other approaches, such as those of Bisani and
Ney (2002) and Marchand and Damper (2000), have
shown that better performance can be achieved by
pairing letter substrings with phoneme substrings,
allowing context to be captured implicitly by these
groupings.
Recently, two hybrid methods have attempted
to capture the flexible context handling of
classification-based methods, while also mod-
eling the sequential nature of the output. The
constraint satisfaction inference (CSInf) ap-
proach (Bosch and Canisius, 2006) improves the
performance of instance-based classification (Bosch
and Daelemans, 1998) by predicting for each letter
a trigram of phonemes consisting of the previous,
current and next phonemes in the sequence. The
final output sequence is the sequence of predicted
phonemes that satisfies the most unigram, bigram
and trigram agreement constraints. The second
hybrid approach (Jiampojamarn et al, 2007) also
extends instance-based classification. It employs a
many-to-many letter-to-phoneme alignment model,
allowing substrings of letters to be classified into
substrings of phonemes, and introducing an input
segmentation step before prediction begins. The
method accounts for sequence information with
post-processing: the numerical scores of possible
outputs from an instance-based phoneme predictor
are combined with phoneme transition probabili-
ties in order to identify the most likely phoneme
sequence.
3 A joint approach
By observing the strengths and weaknesses of previ-
ous approaches, we can create the following priori-
tized desiderata for any L2P system:
1. The phoneme predicted for a letter should be
informed by the letter?s context in the input
word.
2. In addition to single letters, letter substrings
should also be able to generate phonemes.
3. Phoneme sequence information should be in-
cluded in the model.
Each of the previous approaches focuses on one
or more of these items. Classification-based ap-
proaches such as the decision tree system (Black
et al, 1998) and instance-based learning sys-
tem (Bosch and Daelemans, 1998) take into ac-
count the letter?s context (#1). By pairing letter sub-
strings with phoneme substrings, the joint n-gram
approach (Bisani and Ney, 2002) accounts for all
three desiderata, but each operation is informed only
by a limited amount of left context. The many-
to-many classifier of Jiampojamarn et al (2007)
also attempts to account for all three, but it adheres
906
 






	







	





	




 



	





 






	





	

 



	




Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 118?126,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
A Ranking Approach to Stress Prediction
for Letter-to-Phoneme Conversion
Qing Dou, Shane Bergsma, Sittichai Jiampojamarn and Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, T6G 2E8, Canada
{qdou,bergsma,sj,kondrak}@cs.ualberta.ca
Abstract
Correct stress placement is important in
text-to-speech systems, in terms of both
the overall accuracy and the naturalness of
pronunciation. In this paper, we formu-
late stress assignment as a sequence pre-
diction problem. We represent words as
sequences of substrings, and use the sub-
strings as features in a Support Vector Ma-
chine (SVM) ranker, which is trained to
rank possible stress patterns. The rank-
ing approach facilitates inclusion of arbi-
trary features over both the input sequence
and output stress pattern. Our system ad-
vances the current state-of-the-art, predict-
ing primary stress in English, German, and
Dutch with up to 98% word accuracy on
phonemes, and 96% on letters. The sys-
tem is also highly accurate in predicting
secondary stress. Finally, when applied in
tandem with an L2P system, it substan-
tially reduces the word error rate when
predicting both phonemes and stress.
1 Introduction
In many languages, certain syllables in words are
phonetically more prominent in terms of duration,
pitch, and loudness. This phenomenon is referred
to as lexical stress. In some languages, the loca-
tion of stress is entirely predictable. For example,
lexical stress regularly falls on the initial syllable
in Hungarian, and on the penultimate syllable in
Polish. In other languages, such as English and
Russian, any syllable in the word can be stressed.
Correct stress placement is important in text-
to-speech systems because it affects the accuracy
of human word recognition (Tagliapietra and Ta-
bossi, 2005; Arciuli and Cupples, 2006). How-
ever, the issue has often been ignored in previ-
ous letter-to-phoneme (L2P) systems. The sys-
tems that do generate stress markers often do not
report separate figures on stress prediction accu-
racy, or they only provide results on a single lan-
guage. Some only predict primary stress mark-
ers (Black et al, 1998; Webster, 2004; Demberg
et al, 2007), while those that predict both primary
and secondary stress generally achieve lower ac-
curacy (Bagshaw, 1998; Coleman, 2000; Pearson
et al, 2000).
In this paper, we formulate stress assignment as
a sequence prediction problem. We divide each
word into a sequence of substrings, and use these
substrings as features for a Support Vector Ma-
chine (SVM) ranker. For a given sequence length,
there is typically only a small number of stress
patterns in use. The task of the SVM is to rank
the true stress pattern above the small number of
acceptable alternatives. This is the first system
to predict stress within a powerful discriminative
learning framework. By using a ranking approach,
we enable the use of arbitrary features over the en-
tire (input) sequence and (output) stress pattern.
We show that the addition of a feature for the en-
tire output sequence improves prediction accuracy.
Our experiments on English, German, and
Dutch demonstrate that our ranking approach sub-
stantially outperforms previous systems. The
SVM ranker achieves exceptional 96.2% word ac-
curacy on the challenging task of predicting the
full stress pattern in English. Moreover, when
combining our stress predictions with a state-of-
the-art L2P system (Jiampojamarn et al, 2008),
we set a new standard for the combined prediction
of phonemes and stress.
The paper is organized as follows. Section 2
provides background on lexical stress and a task
definition. Section 3 presents our automatic stress
prediction algorithm. In Section 4, we confirm the
power of the discriminative approach with experi-
ments on three languages. Section 5 describes how
stress is integrated into L2P conversion.
118
2 Background and Task Definition
There is a long history of research into the prin-
ciples governing lexical stress placement. Zipf
(1929) showed that stressed syllables are of-
ten those with low frequency in speech, while
unstressed syllables are usually very common.
Chomsky and Halle (1968) proposed a set of
context-sensitive rules for producing English
stress from underlying word forms. Due to its
importance in text-to-speech, there is also a long
history of computational stress prediction sys-
tems (Fudge, 1984; Church, 1985; Williams,
1987). While these early approaches depend
on human definitions of vowel tensity, syllable
weight, word etymology, etc., our work follows
a recent trend of purely data-driven approaches to
stress prediction (Black et al, 1998; Pearson et al,
2000; Webster, 2004; Demberg et al, 2007).
In many languages, only two levels of stress
are distinguished: stressed and unstressed. How-
ever, some languages exhibit more than two levels
of stress. For example, in the English word eco-
nomic, the first and the third syllable are stressed,
with the former receiving weaker emphasis than
the latter. In this case, the initial syllable is said
to carry a secondary stress. Although each word
has only one primary stress, it may have any num-
ber of secondary stresses. Predicting the full stress
pattern is therefore inherently more difficult than
predicting the location of primary stress only.
Our objective is to automatically assign primary
and, where possible, secondary stress to out-of-
vocabulary words. Stress is an attribute of sylla-
bles, but syllabification is a non-trivial task in it-
self (Bartlett et al, 2008). Rather than assuming
correct syllabification of the input word, we in-
stead follow Webster (2004) in placing the stress
on the vowel which constitutes the nucleus of the
stressed syllable. If the syllable boundaries are
known, the mapping from the vowel to the cor-
responding syllable is straightforward.
We investigate the assignment of stress to two
related but different entities: the spoken word
(represented by its phonetic transcription), and
the written word (represented by its orthographic
form). Although stress is a prosodic feature, as-
signing stress to written words (?stressed orthog-
raphy?) has been utilized as a preprocessing stage
for the L2P task (Webster, 2004). This prepro-
cessing is motivated by two factors. First, stress
greatly influences the pronunciation of vowels in
English (c.f., allow vs. alloy). Second, since
phoneme predictors typically utilize only local
context around a letter, they do not incorporate the
global, long-range information that is especially
predictive of stress, such as penultimate syllable
emphasis associated with the suffix -ation. By tak-
ing stressed orthography as input, the L2P system
is able to implicitly leverage morphological infor-
mation beyond the local context.
Indicating stress on letters can also be help-
ful to humans, especially second-language learn-
ers. In some languages, such as Spanish, ortho-
graphic markers are obligatory in words with ir-
regular stress. The location of stress is often ex-
plicitly marked in textbooks for students of Rus-
sian. In both languages, the standard method of
indicating stress is to place an acute accent above
the vowel bearing primary stress, e.g., adio?s. The
secondary stress in English can be indicated with
a grave accent (Coleman, 2000), e.g., pre`ce?de.
In summary, our task is to assign primary and
secondary stress markers to stress-bearing vowels
in an input word. The input word may be either
phonemes or letters. If a stressed vowel is repre-
sented by more than one letter, we adopt the con-
vention of marking the first vowel of the vowel se-
quence, e.g., me?eting. In this way, we are able to
focus on the task of stress prediction, without hav-
ing to determine at the same time the exact sylla-
ble boundaries, or whether a vowel letter sequence
represents one or more spoken vowels (e.g., beat-
ing vs. be-at-i-fy).
3 Automatic Stress Prediction
Our stress assignment system maps a word, w, to a
stressed-form of the word, w?. We formulate stress
assignment as a sequence prediction problem. The
assignment is made in three stages:
(1) First, we map words to substrings (s), the ba-
sic units in our sequence (Section 3.1).
(2) Then, a particular stress pattern (t) is chosen
for each substring sequence. We use a sup-
port vector machine (SVM) to rank the possi-
ble patterns for each sequence (Section 3.2).
(3) Finally, the stress pattern is used to produce
the stressed-form of the word (Section 3.3).
Table 1 gives examples of words at each stage of
the algorithm. We discuss each step in more detail.
119
Word Substrings Pattern Word?
w ? s ? t ? w?
worker ? wor-ker ? 1-0 ? wo?rker
overdo ? ov-ver-do ? 2-0-1 ? o`verdo?
react ? re-ac ? 0-1 ? rea?ct
?bstr?kt ? ?b-r?k ? 0-1 ? ?bstr??kt
prisid ? ri-sid ? 2-1 ? pr?`s??d
Table 1: The steps in our stress prediction sys-
tem (with orthographic and phonetic prediction
examples): (1) word splitting, (2) support vector
ranking of stress patterns, and (3) pattern-to-vowel
mapping.
3.1 Word Splitting
The first step in our approach is to represent the
word as a sequence of N individual units: w ?
s = {s1-s2-...-sN}. These units are used to define
the features and outputs used by the SVM ranker.
Although we are ultimately interested in assigning
stress to individual vowels in the phoneme and let-
ter sequence, it is beneficial to represent the task in
units larger than individual letters.
Our substrings are similar to syllables; they
have a vowel as their nucleus and include con-
sonant context. By approximating syllables, our
substring patterns will allow us to learn recur-
rent stress regularities, as well as dependencies
between neighboring substrings. Since determin-
ing syllable breaks is a non-trivial task, we in-
stead adopt the following simple splitting tech-
nique. Each vowel in the word forms the nucleus
of a substring. Any single preceding or follow-
ing consonant is added to the substring unit. Thus,
each substring consists of at most three symbols
(Table 1).
Using shorter substrings reduces the sparsity of
our training data; words like cryer, dryer and fryer
are all mapped to the same form: ry-er. The
SVM can thus generalize from observed words to
similarly-spelled, unseen examples.
Since the number of vowels equals the num-
ber of syllables in the phonetic form of the word,
applying this approach to phonemes will always
generate the correct number of syllables. For let-
ters, splitting may result in a different number of
units than the true syllabification, e.g., pronounce
? ron-no-un-ce. This does not prevent the system
from producing the correct stress assignment after
the pattern-to-vowel mapping stage (Section 3.3)
is complete.
3.2 Stress Prediction with SVM Ranking
After creating a sequence of substring units, s =
{s1-s2-...-sN}, the next step is to choose an out-
put sequence, t = {t1-t2-...-tN}, that encodes
whether each unit is stressed or unstressed. We
use the number ?1? to indicate that a substring re-
ceives primary stress, ?2? for secondary stress, and
?0? to indicate no stress. We call this output se-
quence the stress pattern for a word. Table 1 gives
examples of words, substrings, and stress patterns.
We use supervised learning to train a system to
predict the stress pattern. We generate training
(s, t) pairs in the obvious way from our stress-
marked training words, w?. That is, we first ex-
tract the letter/phoneme portion, w, and use it
to create the substrings, s. We then create the
stress pattern, t, using w??s stress markers. Given
the training pairs, any sequence predictor can be
used, for example a Conditional Random Field
(CRF) (Lafferty et al, 2001) or a structured per-
ceptron (Collins, 2002). However, we can take
advantage of a unique property of our problem to
use a more expressive framework than is typically
used in sequence prediction.
The key observation is that the output space of
possible stress patterns is actually fairly limited.
Clopper (2002) shows that people have strong
preferences for particular sequences of stress, and
this is confirmed by our training data (Section 4.1).
In English, for example, we find that for each set
of spoken words with the same number of sylla-
bles, there are no more than fifteen different stress
patterns. In total, among 55K English training ex-
amples, there are only 70 different stress patterns.
In both German and Dutch there are only about
50 patterns in 250K examples.1 Therefore, for a
particular input sequence, we can safely limit our
consideration to only the small set of output pat-
terns of the same length.
Thus, unlike typical sequence predictors, we do
not have to search for the highest-scoring output
according to our model. We can enumerate the
full set of outputs and simply choose the highest-
scoring one. This enables a more expressive rep-
resentation. We can define arbitrary features over
the entire output sequence. In a typical CRF or
structured perceptron approach, only output fea-
tures that can be computed incrementally during
search are used (e.g. Markov transition features
that permit Viterbi search). Since search is not
1See (Dou, 2009) for more details.
120
needed here, we can exploit longer-range features.
Choosing the highest-scoring output from a
fixed set is a ranking problem, and we provide the
full ranking formulation below. Unlike previous
ranking approaches (e.g. Collins and Koo (2005)),
we do not rely on a generative model to produce
a list of candidates. Candidates are chosen in ad-
vance from observed training patterns.
3.2.1 Ranking Formulation
For a substring sequence, s, of length N , our task
is to select the correct output pattern from the set
of all length-N patterns observed in our training
data, a set we denote as TN . We score each possi-
ble input-output combination using a linear model.
Each substring sequence and possible output pat-
tern, (s, t), is represented with a set of features,
?(s, t). The score for a particular (s, t) combina-
tion is a weighted sum of these features, ???(s, t).
The specific features we use are described in Sec-
tion 3.2.2.
Let tj be the stress pattern for the jth training
sequence sj , both of length N . At training time,
the weights, ?, are chosen such that for each sj ,
the correct output pattern receives a higher score
than other patterns of the same length: ?u ?
TN ,u 6= tj,
? ??(sj, tj) > ? ??(sj ,u) (1)
The set of constraints generated by Equation 1
are called rank constraints. They are created sep-
arately for every (sj , tj) training pair. Essen-
tially, each training pair is matched with a set
of automatically-created negative examples. Each
negative has an incorrect, but plausible, stress pat-
tern, u.
We adopt a Support Vector Machine (SVM) so-
lution to these ranking constraints as described by
Joachims (2002). The learner finds the weights
that ensure a maximum (soft) margin separation
between the correct scores and the competitors.
We use an SVM because it has been successful in
similar settings (learning with thousands of sparse
features) for both ranking and classification tasks,
and because an efficient implementation is avail-
able (Joachims, 1999).
At test time we simply score each possible out-
put pattern using the learned weights. That is,
for an input sequence s of length N , we compute
? ??(s, t) for all t ? TN , and we take the highest
scoring t as our output. Note that because we only
Substring si, ti
si, i, ti
Context si?1, ti
si?1si, ti
si+1, ti
sisi+1, ti
si?1sisi+1, ti
Stress Pattern t1t2 . . . tN
Table 2: Feature Template
consider previously-observed output patterns, it is
impossible for our system to produce a nonsensi-
cal result, such as having two primary stresses in
one word. Standard search-based sequence pre-
dictors need to be specially augmented with hard
constraints in order to prevent such output (Roth
and Yih, 2005).
3.2.2 Features
The power of our ranker to identify the correct
stress pattern depends on how expressive our fea-
tures are. Table 2 shows the feature templates used
to create the features ?(s, t) for our ranker. We
use binary features to indicate whether each com-
bination occurs in the current (s,t) pair.
For example, if a substring tion is unstressed in
a (s, t) pair, the Substring feature {si, ti = tion,0}
will be true.2 In English, often the penultimate
syllable is stressed if the final syllable is tion.
We can capture such a regularity with the Con-
text feature si+1, ti. If the following syllable is
tion and the current syllable is stressed, the fea-
ture {si+1, ti = tion,1} will be true. This feature
will likely receive a positive weight, so that out-
put sequences with a stress before tion receive a
higher rank.
Finally, the full Stress Pattern serves as an im-
portant feature. Note that such a feature would
not be possible in standard sequence predictors,
where such information must be decomposed into
Markov transition features like ti?1ti. In a ranking
framework, we can score output sequences using
their full output pattern. Thus we can easily learn
the rules in languages with regular stress rules. For
languages that do not have a fixed stress rule, pref-
erences for particular patterns can be learned using
this feature.
2tion is a substring composed of three phonemes but we
use its orthographic representation here for clarity.
121
3.3 Pattern-to-Vowel Mapping
The final stage of our system uses the predicted
pattern t to create the stress-marked form of the
word, w?. Note the number of substrings created
by our splitting method always equals the number
of vowels in the word. We can thus simply map
the indicator numbers in t to markers on their cor-
responding vowels to produce the stressed word.
For our example, pronounce ? ron-no-un-ce,
if the SVM chooses the stress pattern, 0-1-0-
0, we produce the correct stress-marked word,
prono?unce. If we instead stress the third vowel, 0-
0-1-0, we produce an incorrect output, pronou?nce.
4 Stress Prediction Experiments
In this section, we evaluate our ranking approach
to stress prediction by assigning stress to spoken
and written words in three languages: English,
German, and Dutch. We first describe the data and
the various systems we evaluate, and then provide
the results.
4.1 Data
The data is extracted from CELEX (Baayen et al,
1996). Following previous work on stress predic-
tion, we randomly partition the data into 85% for
training, 5% for development, and 10% for test-
ing. To make results on German and Dutch com-
parable with English, we reduce the training, de-
velopment, and testing set by 80% for each. Af-
ter removing all duplicated items as well as abbre-
viations, phrases, and diacritics, each training set
contains around 55K words.
In CELEX, stress is labeled on syllables in the
phonetic form of the words. Since our objec-
tive is to assign stress markers to vowels (as de-
scribed in Section 2) we automatically map the
stress markers from the stressed syllables in the
phonetic forms onto phonemes and letters rep-
resenting vowels. For phonemes, the process is
straightforward: we move the stress marker from
the beginning of a syllable to the phoneme which
constitutes the nucleus of the syllable. For let-
ters, we map the stress from the vowel phoneme
onto the orthographic forms using the ALINE al-
gorithm (Dwyer and Kondrak, 2009). The stress
marker is placed on the first letter within the sylla-
ble that represents a vowel sound.3
3Our stand-off stress annotations for English, German,
and Dutch CELEX orthographic data can be downloaded at:
http://www.cs.ualberta.ca/?kondrak/celex.html.
System Eng Ger Dut
P+S P P P
SUBSTRING 96.2 98.0 97.1 93.1
ORACLESYL 95.4 96.4 97.1 93.2
TOPPATTERN 66.8 68.9 64.1 60.8
Table 3: Stress prediction word accuracy (%) on
phonemes for English, German, and Dutch. P:
predicting primary stress only. P+S: primary and
secondary.
CELEX also provides secondary stress annota-
tion for English. We therefore evaluate on both
primary and secondary stress (P+S) in English and
on primary stress assignment alone (P) for En-
glish, German, and Dutch.
4.2 Comparison Approaches
We evaluate three different systems on the letter
and phoneme sequences in the experimental data:
1) SUBSTRING is the system presented in Sec-
tion 3. It uses the vowel-based splitting
method, followed by SVM ranking.
2) ORACLESYL splits the input word into sylla-
bles according to the CELEX gold-standard,
before applying SVM ranking. The output
pattern is evaluated directly against the gold-
standard, without pattern-to-vowel mapping.
3) TOPPATTERN is our baseline system. It uses
the vowel-based splitting method to produce a
substring sequence of length N . Then it simply
chooses the most common stress pattern among
all the stress patterns of length N .
SUBSTRING and ORACLESYL use scores pro-
duced by an SVM ranker trained on the training
data. We employ the ranking mode of the popular
learning package SVMlight (Joachims, 1999). In
each case, we learn a linear kernel ranker on the
training set stress patterns and tune the parameter
that trades-off training error and margin on the de-
velopment set.
We evaluate the systems using word accuracy:
the percent of words for which the output form of
the word, w?, matches the gold standard.
4.3 Results
Table 3 provides results on English, German, and
Dutch phonemes. Overall, the performance of our
automatic stress predictor, SUBSTRING, is excel-
lent. It achieves 98.0% accuracy for predicting
122
System Eng Ger Dut
P+S P P P
SUBSTRING 93.5 95.1 95.9 91.0
ORACLESYL 94.6 96.0 96.6 92.8
TOPPATTERN 65.5 67.6 64.1 60.8
Table 4: Stress prediction word accuracy (%) on
letters for English, German, and Dutch. P: pre-
dicting primary stress only. P+S: primary and sec-
ondary.
primary stress in English, 97.1% in German, and
93.1% in Dutch. It also predicts both primary and
secondary stress in English with high accuracy,
96.2%. Performance is much higher than our base-
line accuracy, which is between 60% and 70%.
ORACLESYL, with longer substrings and hence
sparser data, does not generally improve perfor-
mance. This indicates that perfect syllabification
is unnecessary for phonetic stress assignment.
Our system is a major advance over the pre-
vious state-of-the-art in phonetic stress assign-
ment. For predicting stressed/unstressed syllables
in English, Black et al (1998) obtained a per-
syllable accuracy of 94.6%. We achieve 96.2%
per-word accuracy for predicting both primary and
secondary stress. Others report lower numbers
on English phonemes. Bagshaw (1998) obtained
65%-83.3% per-syllable accuracy using Church
(1985)?s rule-based system. For predicting both
primary and secondary stress, Coleman (2000)
and Pearson et al (2000) report 69.8% and 81.0%
word accuracy, respectively.
The performance on letters (Table 4) is also
quite encouraging. SUBSTRING predicts primary
stress with accuracy above 95% for English and
German, and equal to 91% in Dutch. Performance
is 1-3% lower on letters than on phonemes. On
the other hand, the performance of ORACLESYL
drops much less on letters. This indicates that
most of SUBSTRING?s errors are caused by the
splitting method. Letter vowels may or may not
represent spoken vowels. By creating a substring
for every vowel letter we may produce an incorrect
number of syllables. Our pattern feature is there-
fore less effective.
Nevertheless, SUBSTRING?s accuracy on letters
also represents a clear improvement over previ-
ous work. Webster (2004) reports 80.3% word
accuracy on letters in English and 81.2% in Ger-
man. The most comparable work is Demberg et al
 84
 86
 88
 90
 92
 94
 96
 98
 100
 10000  100000
W
or
d 
Ac
cu
ra
cy
 (%
)
Number of training examples
German
Dutch
English
Figure 1: Stress prediction accuracy on letters.
(2007), which achieves 90.1% word accuracy on
letters in German CELEX, assuming perfect letter
syllabification. In order to reproduce their strict
experimental setup, we re-partition the full set of
German CELEX data to ensure that no overlap of
word stems exists between the training and test
sets. Using the new data sets, our system achieves
a word accuracy of 92.3%, a 2.2% improvement
over Demberg et al (2007)?s result. Moreover, if
we also assume perfect syllabification, the accu-
racy is 94.3%, a 40% reduction in error rate.
We performed a detailed analysis to understand
the strong performance of our system. First of all,
note that an error could happen if a test-set stress
pattern was not observed in the training data; its
correct stress pattern would not be considered as
an output. In fact, no more than two test errors in
any test set were so caused. This strongly justi-
fies the reduced set of outputs used in our ranking
formulation.
We also tested all systems with the Stress Pat-
tern feature removed. Results were worse in all
cases. As expected, it is most valuable for pre-
dicting primary and secondary stress. On English
phonemes, accuracy drops from 96.2% to 95.3%
without it. On letters, it drops from 93.5% to
90.0%. The gain from this feature also validates
our ranking framework, as such arbitrary features
over the entire output sequence can not be used in
standard search-based sequence prediction.
Finally, we examined the relationship between
training data size and performance by plotting
learning curves for letter stress accuracy (Fig-
ure 1). Unlike the tables above, here we use the
123
full set of data in Dutch and German CELEX to
create the largest-possible training sets (255K ex-
amples). None of the curves are levelling off; per-
formance grows log-linearly across the full range.
5 Lexical stress and L2P conversion
In this section, we evaluate various methods of
combining stress prediction with phoneme gener-
ation. We first describe the specific system that we
use for letter-to-phoneme (L2P) conversion. We
then discuss the different ways stress prediction
can be integrated with L2P, and define the systems
used in our experiments. Finally, we provide the
results.
5.1 The L2P system
We combine stress prediction with a state-of-the-
art L2P system (Jiampojamarn et al, 2008). Like
our stress ranker, their system is a data-driven se-
quence predictor that is trained with supervised
learning. The score for each output sequence is
a weighted combination of features. The feature
weights are trained using the Margin Infused Re-
laxed Algorithm (MIRA) (Crammer and Singer,
2003), a powerful online discriminative training
framework. Like other recent L2P systems (Bisani
and Ney, 2002; Marchand and Damper, 2007; Ji-
ampojamarn et al, 2007), this approach does not
generate stress, nor does it consider stress when it
generates phonemes.
For L2P experiments, we use the same training,
testing, and development data as was used in Sec-
tion 4. For all experiments, we use the develop-
ment set to determine at which iteration to stop
training in the online algorithm.
5.2 Combining stress and phoneme
generation
Various methods have been used for combining
stress and phoneme generation. Phonemes can be
generated without regard to stress, with stress as-
signed as a post-process (Bagshaw, 1998; Cole-
man, 2000). Both van den Bosch (1997) and
Black et al (1998) argue that stress should be pre-
dicted at the same time as phonemes. They ex-
pand the output set to distinguish between stressed
and unstressed phonemes. Similarly, Demberg et
al. (2007) produce phonemes, stress, and syllable-
boundaries within a single joint n-gram model.
Pearson et al (2000) generate phonemes and stress
together by jointly optimizing a decision-tree
phoneme-generator and a stress predictor based on
stress pattern counts. In contrast, Webster (2004)
first assigns stress to letters, creating an expanded
input set, and then predicts both phonemes and
stress jointly. The system marks stress on let-
ter vowels by determining the correspondence be-
tween affixes and stress in written words.
Following the above approaches, we can expand
the input or output symbols of our L2P system to
include stress. However, since both decision tree
systems and our L2P predictor utilize only local
context, they may produce invalid global output.
One option, used by Demberg et al (2007), is to
add a constraint to the output generation, requiring
each output sequence to have exactly one primary
stress.
We enhance this constraint, based on the obser-
vation that the number of valid output sequences
is fairly limited (Section 3.2). The modified sys-
tem produces the highest-scoring sequence such
that the output?s corresponding stress pattern has
been observed in our training data. We call this
the stress pattern constraint. This is a tighter
constraint than having only one primary stress.4
Another advantage is that it provides some guid-
ance for the assignment of secondary stress.
Inspired by the aforementioned strategies, we
evaluate the following approaches:
1) JOINT: The L2P system?s input sequence is let-
ters, the output sequence is phonemes+stress.
2) JOINT+CONSTR: Same as JOINT, except it se-
lects the highest scoring output that obeys the
stress pattern constraint.
3) POSTPROCESS: The L2P system?s input is let-
ters, the output is phonemes. It then applies the
SVM stress ranker (Section 3) to the phonemes
to produce the full phoneme+stress output.
4) LETTERSTRESS: The L2P system?s input is
letters+stress, the output is phonemes+stress.
It creates the stress-marked letters by applying
the SVM ranker to the input letters as a pre-
process.
5) ORACLESTRESS: The same input/output as
LETTERSTRESS , except it uses the gold-
standard stress on letters (Section 4.1).
4In practice, the L2P system generates a top-N list, and
we take the highest-scoring output on the list that satisfies
the constraint. If none satisfy the constraint, we take the top
output that has only one primary stress.
124
System Eng Ger Dut
P+S P P P
JOINT 78.9 80.0 86.0 81.1
JOINT+CONSTR 84.6 86.0 90.8 88.7
POSTPROCESS 86.2 87.6 90.9 88.8
LETTERSTRESS 86.5 87.2 90.1 86.6
ORACLESTRESS 91.4 91.4 92.6 94.5
Festival 61.2 62.5 71.8 65.1
Table 5: Combined phoneme and stress predic-
tion word accuracy (%) for English, German, and
Dutch. P: predicting primary stress only. P+S:
primary and secondary.
Note that while the first approach uses only
local information to make predictions (features
within a context window around the current let-
ter), systems 2 to 5 leverage global information in
some manner: systems 3 and 4 use the predictions
of our stress ranker, while 2 uses a global stress
pattern constraint.5
We also generated stress and phonemes using
the popular Festival Speech Synthesis System6
(version 1.96, 2004) and report its accuracy.
5.3 Results
Word accuracy results for predicting both
phonemes and stress are provided in Table 5.
First of all, note that the JOINT approach,
which simply expands the output set, is 4%-
8% worse than all other comparison systems
across the three languages. These results clearly
indicate the drawbacks of predicting stress us-
ing only local information. In English, both
LETTERSTRESS and POSTPROCESS perform
best, while POSTPROCESS and the constrained
system are highest on German and Dutch. Results
using the oracle letter stress show that given
perfect stress assignment on letters, phonemes
and stress can be predicted very accurately, in all
cases above 91%.
We also found that the phoneme prediction ac-
curacy alone (i.e., without stress) is quite simi-
lar for all the systems. The gains over JOINT
on combined stress and phoneme accuracy are
almost entirely due to more accurate stress as-
signment. Utilizing the oracle stress on letters
markedly improves phoneme prediction in English
5This constraint could also help the other systems. How-
ever, since they already use global information, it yields only
marginal improvements.
6http://www.cstr.ed.ac.uk/projects/festival/
(from 88.8% to 91.4%). This can be explained by
the fact that English vowels are often reduced to
schwa when unstressed (Section 2).
Predicting both phonemes and stress is a chal-
lenging task, and each of our globally-informed
systems represents a major improvement over pre-
vious work. The accuracy of Festival is much
lower even than our JOINT approach, but the rel-
ative performance on the different languages is
quite similar.
A few papers report accuracy on the combined
stress and phoneme prediction task. The most di-
rectly comparable work is van den Bosch (1997),
which also predicts primary and secondary stress
using English CELEX data. However, the re-
ported word accuracy is only 62.1%. Three other
papers report word accuracy on phonemes and
stress, using different data sets. Pearson et al
(2000) report 58.5% word accuracy for predicting
phonemes and primary/secondary stress. Black et
al. (1998) report 74.6% word accuracy in English,
while Webster (2004) reports 68.2% on English
and 82.9% in German (all primary stress only).
Finally, Demberg et al (2007) report word accu-
racy on predicting phonemes, stress, and syllab-
ification on German CELEX data. They achieve
86.3% word accuracy.
6 Conclusion
We have presented a discriminative ranking ap-
proach to lexical stress prediction, which clearly
outperforms previously developed systems. The
approach is largely language-independent, appli-
cable to both orthographic and phonetic repre-
sentations, and flexible enough to handle multi-
ple stress levels. When combined with an exist-
ing L2P system, it achieves impressive accuracy
in generating pronunciations together with their
stress patterns. In the future, we will investigate
additional features to leverage syllabic and mor-
phological information, when available. Kernel
functions could also be used to automatically cre-
ate a richer feature space; preliminary experiments
have shown gains in performance using polyno-
mial and RBF kernels with our stress ranker.
Acknowledgements
This research was supported by the Natural
Sciences and Engineering Research Council of
Canada, the Alberta Ingenuity Fund, and the Al-
berta Informatics Circle of Research Excellence.
125
References
Joanne Arciuli and Linda Cupples. 2006. The pro-
cessing of lexical stress during visual word recog-
nition: Typicality effects and orthographic corre-
lates. Quarterly Journal of Experimental Psychol-
ogy, 59(5):920?948.
Harald Baayen, Richard Piepenbrock, and Leon Gu-
likers. 1996. The CELEX2 lexical database.
LDC96L14.
Paul C. Bagshaw. 1998. Phonemic transcription by
analogy in text-to-speech synthesis: Novel word
pronunciation and lexicon compression. Computer
Speech and Language, 12(2):119?142.
Susan Bartlett, Grzegorz Kondrak, and Colin Cherry.
2008. Automatic syllabification with structured
SVMs for letter-to-phoneme conversion. In ACL-
08: HLT, pages 568?576.
Maximilian Bisani and Hermann Ney. 2002. Investi-
gations on joint-multigram models for grapheme-to-
phoneme conversion. In ICSLP, pages 105?108.
Alan W Black, Kevin Lenzo, and Vincent Pagel. 1998.
Issues in building general letter to sound rules. In
The 3rd ESCA Workshop on Speech Synthesis, pages
77?80.
Noam Chomsky and Morris Halle. 1968. The sound
pattern of English. New York: Harper and Row.
Kenneth Church. 1985. Stress assignment in letter
to sound rules for speech synthesis. In ACL, pages
246?253.
Cynthia G. Clopper. 2002. Frequency of stress pat-
terns in English: A computational analysis. IULC
Working Papers Online.
John Coleman. 2000. Improved prediction of stress in
out-of-vocabulary words. In IEEE Seminar on the
State of the Art in Speech Synthesis.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?70.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and ex-
periments with perceptron algorithms. In EMNLP,
pages 1?8.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Vera Demberg, Helmut Schmid, and Gregor Mo?hler.
2007. Phonological constraints and morphologi-
cal preprocessing for grapheme-to-phoneme conver-
sion. In ACL, pages 96?103.
Qing Dou. 2009. An SVM ranking approach to stress
assignment. Master?s thesis, University of Alberta.
Kenneth Dwyer and Grzegorz Kondrak. 2009. Reduc-
ing the annotation effort for letter-to-phoneme con-
version. In ACL-IJCNLP.
Erik C. Fudge. 1984. English word-stress. London:
Allen and Unwin.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme
conversion. In NAACL-HLT 2007, pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In ACL-
08: HLT, pages 905?913.
Thorsten Joachims. 1999. Making large-scale Support
Vector Machine learning practical. In B. Scho?lkopf
and C. Burges, editors, Advances in Kernel Meth-
ods: Support Vector Machines, pages 169?184.
MIT-Press.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In KDD, pages 133?142.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional Random Fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML, pages 282?289.
Yannick Marchand and Robert I. Damper. 2007. Can
syllabification improve pronunciation by analogy of
English? Natural Language Engineering, 13(1):1?
24.
Steve Pearson, Roland Kuhn, Steven Fincke, and Nick
Kibre. 2000. Automatic methods for lexical stress
assignment and syllabification. In ICSLP, pages
423?426.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields.
In ICML, pages 736?743.
Lara Tagliapietra and Patrizia Tabossi. 2005. Lexical
stress effects in Italian spoken word recognition. In
The XXVII Annual Conference of the Cognitive Sci-
ence Society, pages 2140?2144.
Antal van den Bosch. 1997. Learning to pronounce
written words: A study in inductive language learn-
ing. Ph.D. thesis, Universiteit Maastricht.
Gabriel Webster. 2004. Improving letter-
to-pronunciation accuracy with automatic
morphologically-based stress prediction. In
ICSLP, pages 2573?2576.
Briony Williams. 1987. Word stress assignment in a
text-to-speech synthesis system for British English.
Computer Speech and Language, 2:235?272.
George Kingsley Zipf. 1929. Relative frequency as a
determinant of phonetic change. Harvard Studies in
Classical Philology, 15:1?95.
126
Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 114?115,
New York City, June 2006. c?2006 Association for Computational Linguistics
Biomedical Term Recognition
With the Perceptron HMM Algorithm
Sittichai Jiampojamarn and Grzegorz Kondrak and Colin Cherry
Department of Computing Science,
University of Alberta,
Edmonton, AB, T6G 2E8, Canada
{sj,kondrak,colinc}@cs.ualberta.ca
Abstract
We propose a novel approach to the iden-
tification of biomedical terms in research
publications using the Perceptron HMM
algorithm. Each important term is iden-
tified and classified into a biomedical con-
cept class. Our proposed system achieves
a 68.6% F-measure based on 2,000 train-
ing Medline abstracts and 404 unseen
testing Medline abstracts. The system
achieves performance that is close to the
state-of-the-art using only a small feature
set. The Perceptron HMM algorithm pro-
vides an easy way to incorporate many po-
tentially interdependent features.
1 Introduction
Every day, new scientific articles in the biomedi-
cal field are published and made available on-line.
The articles contain many new terms and names
involving proteins, DNA, RNA, and a wide vari-
ety of other substances. Given the large volume of
the new research articles, it is important to develop
systems capable of extracting meaningful relation-
ships between substances from these articles. Such
systems need to recognize and identify biomedical
terms in unstructured texts. Biomedical term recog-
nition is thus a step towards information extraction
from biomedical texts.
The term recognition task aims at locating
biomedical terminology in unstructured texts. The
texts are unannotated biomedical research publica-
tions written in English. Meaningful terms, which
may comprise several words, are identified in order
to facilitate further text mining tasks. The recogni-
tion task we consider here also involves term clas-
sification, that is, classifying the identified terms
into biomedical concepts: proteins, DNA, RNA, cell
types, and cell lines.
Our biomedical term recognition task is defined
as follows: given a set of documents, in each docu-
ment, find and mark each occurrence of a biomedi-
cal term. A term is considered to be annotated cor-
rectly only if all its composite words are annotated
correctly. Precision, recall and F-measure are deter-
mined by comparing the identified terms against the
terms annotated in the gold standard.
We believe that the biomedical term recogni-
tion task can only be adequately addressed with
machine-learning methods. A straightforward dic-
tionary look-up method is bound to fail because
of the term variations in the text, especially when
the task focuses on locating exact term boundaries.
Rule-based systems can achieve good performance
on small data sets, but the rules must be defined
manually by domain experts, and are difficult to
adapt to other data sets. Systems based on machine-
learning employ statistical techniques, and can be
easily re-trained on different data. The machine-
learning techniques used for this task can be divided
into two main approaches: the word-based methods,
which annotate each word without taking previous
assigned tags into account, and the sequence based
methods, which take other annotation decisions into
account in order to decide on the tag for the current
word.
We propose a biomedical term identification
114
system based on the Perceptron HMM algo-
rithm (Collins, 2004), a novel algorithm for HMM
training. It uses the Viterbi and perceptron algo-
rithms to replace a traditional HMM?s conditional
probabilities with discriminatively trained parame-
ters. The method has been successfully applied to
various tasks, including noun phrase chunking and
part-of-speech tagging. The perceptron makes it
possible to incorporate discriminative training into
the traditional HMM approach, and to augment it
with additional features, which are helpful in rec-
ognizing biomedical terms, as was demonstrated in
the ABTA system (Jiampojamarn et al, 2005). A
discriminative method allows us to incorporate these
features without concern for feature interdependen-
cies. The Perceptron HMM provides an easy and
effective learning algorithm for this purpose.
The features used in our system include the part-
of-speech tag information, orthographic patterns,
word prefix and suffix character strings. The ad-
ditional features are the word, IOB and class fea-
tures. The orthographic features encode the spelling
characteristics of a word, such as uppercase letters,
lowercase letters, digits, and symbols. The IOB and
class features encode the IOB tags associated with
biomedical class concept markers.
2 Results and discussion
We evaluated our system on the JNLPBA Bio-Entity
recognition task. The training data set contains
2,000 Medline abstracts labeled with biomedical
classes in the IOB style. The IOB annotation method
utilizes three types of tags: <B> for the beginning
word of a term, <I> for the remaining words of a
term, and <O> for non-term words. For the purpose
of term classification, the IOB tags are augmented
with the names of the biomedical classes; for ex-
ample, <B-protein> indicates the first word of
a protein term. The held-out set was constructed
by randomly selecting 10% of the sentences from
the available training set. The number of iterations
for training was determined by observing the point
where the performance on the held-out set starts to
level off. The test set is composed of new 404 Med-
line abstracts.
Table 1 shows the results of our system on all five
classes. In terms of F-measure, our system achieves
Class Recall Precision F-measure
protein 76.73 % 65.56 % 70.71 %
DNA 63.07 % 64.47 % 63.76 %
RNA 64.41 % 59.84 % 62.04 %
cell type 64.71 % 76.35 % 70.05 %
cell line 54.20 % 52.02 % 53.09 %
ALL 70.93 % 66.50 % 68.64 %
Table 1: The performance of our system on the test
set with respect to each biomedical concept class.
the average of 68.6%, which a substantial improve-
ment over the baseline system (based on longest
string matching against a lists of terms from train-
ing data) with the average of 47.7%, and over the
basic HMM system, with the average of 53.9%. In
comparison with the results of eight participants at
the JNLPBA shared tasks (Kim et al, 2004), our
system ranks fourth. The performance gap between
our system and the best systems at JNLPBA, which
achieved the average up to 72.6%, can be attributed
to the use of richer and more complete features such
as dictionaries and Gene ontology.
3 Conclusion
We have proposed a new approach to the biomedical
term recognition task using the Perceptron HMM al-
gorithm. Our proposed system achieves a 68.6% F-
measure with a relatively small number of features
as compared to the systems of the JNLPBA partici-
pants. The Perceptron HMM algorithm is much eas-
ier to implement than the SVM-HMMs, CRF, and
the Maximum Entropy Markov Models, while the
performance is comparable to those approaches. In
the future, we plan to experiment with incorporat-
ing external resources, such as dictionaries and gene
ontologies, into our feature set.
References
M. Collins. 2004. Discriminative training methods forhidden markov models: Theory and experiments withperceptron algorithms. In Proceedings of EMNLP.
S. Jiampojamarn, N. Cercone, and V. Keselj. 2005. Bi-ological named entity recognition using n-grams andclassification methods. In Proceedings of PACLING.
J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Collier.
2004. Introduction to the bio-entity recognition task atJNLPBA. In Proceedings of JNLPBA.
115
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 28?31,
Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLP
DIRECTL: a Language-Independent Approach to Transliteration
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou, Kenneth Dwyer, Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, T6G 2E8, Canada
{sj,abhargava,qdou,dwyer,kondrak}@cs.ualberta.ca
Abstract
We present DIRECTL: an online discrimi-
native sequence prediction model that em-
ploys a many-to-many alignment between
target and source. Our system incorpo-
rates input segmentation, target charac-
ter prediction, and sequence modeling in
a unified dynamic programming frame-
work. Experimental results suggest that
DIRECTL is able to independently dis-
cover many of the language-specific reg-
ularities in the training data.
1 Introduction
In the transliteration task, it seems intuitively im-
portant to take into consideration the specifics of
the languages in question. Of particular impor-
tance is the relative character length of the source
and target names, which vary widely depending on
whether languages employ alphabetic, syllabic, or
ideographic scripts. On the other hand, faced with
the reality of thousands of potential language pairs
that involve transliteration, the idea of a language-
independent approach is highly attractive.
In this paper, we present DIRECTL: a translit-
eration system that, in principle, can be applied to
any language pair. DIRECTL treats the transliter-
ation task as a sequence prediction problem: given
an input sequence of characters in the source lan-
guage, it produces the most likely sequence of
characters in the target language. In Section 2,
we discuss the alignment of character substrings
in the source and target languages. Our transcrip-
tion model, described in Section 3, is based on
an online discriminative training algorithm that
makes it possible to efficiently learn the weights
of a large number of features. In Section 4, we
provide details of alternative approaches that in-
corporate language-specific information. Finally,
in Section 5 and 6, we compare the experimental
results of DIRECTL with its variants that incor-
porate language-specific pre-processing, phonetic
alignment, and manual data correction.
2 Transliteration alignment
In the transliteration task, training data consist of
word pairs that map source language words to
words in the target language. The matching be-
tween character substrings in the source word and
target word is not explicitly provided. These hid-
den relationships are generally known as align-
ments. In this section, we describe an EM-based
many-to-many alignment algorithm employed by
DIRECTL. In Section 4, we discuss an alternative
phonetic alignment method.
We apply an unsupervised many-to-many align-
ment algorithm (Jiampojamarn et al, 2007) to the
transliteration task. The algorithm follows the ex-
pectation maximization (EM) paradigm. In the
expectation step shown in Algorithm 1, partial
counts ? of the possible substring alignments are
collected from each word pair (xT , yV ) in the
training data; T and V represent the lengths of
words x and y, respectively. The forward prob-
ability ? is estimated by summing the probabili-
ties of all possible sequences of substring pairings
from left to right. The FORWARD-M2M procedure
is similar to lines 5 through 12 of Algorithm 1, ex-
cept that it uses Equation 1 on line 8, Equation 2
on line 12, and initializes ?0,0 := 1. Likewise, the
backward probability ? is estimated by summing
the probabilities from right to left.
?t,v += ?(xtt?i+1, ?)?t?i,v (1)
?t,v += ?(xtt?i+1, yvv?j+1)?t?i,v?j (2)
The maxX and maxY variables specify the
maximum length of substrings that are permitted
when creating alignments. Also, for flexibility, we
allow a substring in the source word to be aligned
with a ?null? letter (?) in the target word.
28
Algorithm 1: Expectation-M2M alignment
Input: xT , yV ,maxX,maxY, ?
Output: ?
? := FORWARD-M2M (xT , yV ,maxX,maxY )1
? := BACKWARD-M2M (xT , yV ,maxX,maxY )2
if (?T,V = 0) then3
return4
for t = 0 . . . T , v = 0 . . . V do5
if (t > 0) then6
for i = 1 . . .maxX st t? i ? 0 do7
?(xtt?i+1, ?) +=
?t?i,v?(xtt?i+1,?)?t,v
?T,V8
if (v > 0 ? t > 0) then9
for i = 1 . . .maxX st t? i ? 0 do10
for j = 1 . . . maxY st v ? j ? 0 do11
?(xtt?i+1, yvv?j+1) +=
?t?i,v?j?(xtt?i+1,y
v
v?j+1)?t,v
?T,V12
In the maximization step, we normalize the par-
tial counts ? to the alignment probability ? using
the conditional probability distribution. The EM
steps are repeated until the alignment probability
? converges. Finally, the most likely alignment for
each word pair in the training data is computed
with the standard Viterbi algorithm.
3 Discriminative training
We adapt the online discriminative training frame-
work described in (Jiampojamarn et al, 2008) to
the transliteration task. Once the training data has
been aligned, we can hypothesize that the ith let-
ter substring xi ? x in a source language word
is transliterated into the ith substring yi ? y in
the target language word. Each word pair is rep-
resented as a feature vector ?(x,y). Our feature
vector consists of (1) n-gram context features, (2)
HMM-like transition features, and (3) linear-chain
features. The n-gram context features relate the
letter evidence that surrounds each letter xi to its
output yi. We include all n-grams that fit within
a context window of size c. The c value is deter-
mined using a development set. The HMM-like
transition features express the cohesion of the out-
put y in the target language. We make a first order
Markov assumption, so that these features are bi-
grams of the form (yi?1, yi). The linear-chain fea-
tures are identical to the context features, except
that yi is replaced with a bi-gram (yi?1, yi).
Algorithm 2 trains a linear model in this fea-
ture space. The procedure makes k passes over
the aligned training data. During each iteration,
the model produces the nmost likely output words
Y?j in the target language for each input word xj
in the source language, based on the current pa-
Algorithm 2: Online discriminative training
Input: Data {(x1,y1), (x2,y2), . . . , (xm,ym)},
number of iterations k, size of n-best list n
Output: Learned weights ?
? := ~01
for k iterations do2
for j = 1 . . .m do3
Y?j = {y?j1, . . . , y?jn} = argmaxy[? ? ?(xj ,y)]4
update ? according to Y?j and yj5
return ?6
rameters ?. The values of k and n are deter-
mined using a development set. The model param-
eters are updated according to the correct output
yj and the predicted n-best outputs Y?j , to make
the model prefer the correct output over the in-
correct ones. Specifically, the feature weight vec-
tor ? is updated by using MIRA, the Margin In-
fused Relaxed Algorithm (Crammer and Singer,
2003). MIRA modifies the current weight vector
?o by finding the smallest changes such that the
new weight vector ?n separates the correct and in-
correct outputs by a margin of at least ?(y, y?), the
loss for a wrong prediction. We define this loss to
be 0 if y? = y; otherwise it is 1 + d, where d is
the Levenshtein distance between y and y?. The
update operation is stated as a quadratic program-
ming problem in Equation 3. We utilize a function
from the SVMlight package (Joachims, 1999) to
solve this optimization problem.
min?n ? ?n ? ?o ?
subject to ?y? ? Y? :
?n ? (?(x,y) ? ?(x, y?)) ? ?(y, y?)
(3)
The argmax operation is performed by an exact
search algorithm based on a phrasal decoder (Zens
and Ney, 2004). This decoder simultaneously
finds the l most likely substrings of letters x that
generate the most probable output y, given the
feature weight vector ? and the input word xT .
The search algorithm is based on the following dy-
namic programming recurrence:
Q(0, $) = 0
Q(t, p) = max
p?,p,
t?maxX?t?<t
{? ? ?(xtt?+1, p?, p) +Q(t?, p?)}
Q(T+1, $) = max
p?
{? ? ?($, p?, $) +Q(T, p?)}
To find the n-best predicted outputs, the table
Q records the top n scores for each output sub-
string that has the suffix p substring and is gen-
erated by the input letter substring xt1; here, p? is
29
a sub-output generated during the previous step.
The notation ?(xtt?+1, p?, p) is a convenient way
to describe the components of our feature vector
?(x,y). The n-best predicted outputs Y? can be
discovered by backtracking from the end of the ta-
ble, which is denoted by Q(T + 1, $).
4 Beyond DIRECTL
4.1 Intermediate phonetic representation
We experimented with converting the original Chi-
nese characters to Pinyin as an intermediate repre-
sentation. Pinyin is the most commonly known
Romanization system for Standard Mandarin. Its
alphabet contains the same 26 letters as English.
Each Chinese character can be transcribed pho-
netically into Pinyin. Many resources for Pinyin
conversion are available online.1 A small percent-
age of Chinese characters have multiple pronunci-
ations represented by different Pinyin representa-
tions. For those characters (about 30 characters in
the transliteration data), we manually selected the
pronunciations that are normally used for names.
This preprocessing step significantly reduces the
size of target symbols from 370 distinct Chinese
characters to 26 Pinyin symbols which enables our
system to produce better alignments.
In order to verify whether the addition of
language-specific knowledge can improve the
overall accuracy, we also designed intermediate
representations for Russian and Japanese. We
focused on symbols that modify the neighbor-
ing characters without producing phonetic output
themselves: the two yer characters in Russian,
and the long vowel and sokuon signs in Japanese.
Those were combined with the neighboring char-
acters, creating new ?super-characters.?
4.2 Phonetic alignment with ALINE
ALINE (Kondrak, 2000) is an algorithm that
performs phonetically-informed alignment of two
strings of phonemes. Since our task requires
the alignment of characters representing different
writing scripts, we need to first replace every char-
acter with a phoneme that is the most likely to be
produced by that character.
We applied slightly different methods to the
test languages. In converting the Cyrillic script
into phonemes, we take advantage of the fact
that the Russian orthography is largely phonemic,
which makes it a relatively straightforward task.
1For example, http://www.chinesetopinyin.com/
In Japanese, we replace each Katakana character
with one or two phonemes using standard tran-
scription tables. For the Latin script, we simply
treat every letter as an IPA symbol (International
Phonetic Association, 1999). The IPA contains a
subset of 26 letter symbols that tend to correspond
to the usual phonetic value that the letter repre-
sents in the Latin script. The Chinese characters
are first converted to Pinyin, which is then handled
in the same way as the Latin script.
Similar solutions could be engineered for other
scripts. We observed that the transcriptions do not
need to be very precise in order for ALINE to pro-
duce high quality alignments.
4.3 System combination
The combination of predictions produced by sys-
tems based on different principles may lead to im-
proved prediction accuracy. We adopt the follow-
ing combination algorithm. First, we rank the in-
dividual systems according to their top-1 accuracy
on the development set. To obtain the top-1 pre-
diction for each input word, we use simple voting,
with ties broken according to the ranking of the
systems. We generalize this approach to handle n-
best lists by first ordering the candidate translitera-
tions according to the highest rank assigned by any
of the systems, and then similarly breaking ties by
voting and system ranking.
5 Evaluation
In the context of the NEWS 2009 Machine
Transliteration Shared Task (Li et al, 2009), we
tested our system on six data sets: from English to
Chinese (EnCh) (Li et al, 2004), Hindi (EnHi),
Russian (EnRu) (Kumaran and Kellner, 2007),
Japanese Katakana (EnJa), and Korean Hangul
(EnKo); and from Japanese Name to Japanese
Kanji (JnJk)2. We optimized the models? param-
eters by training on the training portion of the
provided data and measuring performance on the
development portion. For the final testing, we
trained the models on all the available labeled data
(training plus development data). For each data
set, we converted any uppercase letters to lower-
case. Our system outputs the top 10 candidate an-
swers for each input word.
Table 1 reports the performance of our system
on the development and final test sets, measured
in terms of top-1 word accuracy (ACC). For cer-
tain language pairs, we tested variants of the base
2http://www.cjk.org/
30
Task Model Dev Test
EnCh DIRECTL 72.4 71.7
INT(M2M) 73.9 73.4
INT(ALINE) 73.8 73.2
COMBINED 74.8 74.6
EnHi DIRECTL 41.4 49.8
DIRECTL+MC 42.3 50.9
EnJa DIRECTL 49.9 50.0
INT(M2M)? 49.6 49.2
INT(ALINE) 48.3 51.0
COMBINED? 50.6 50.5
EnKo DIRECTL 36.7 38.7
EnRu DIRECTL 80.2 61.3
INT(M2M) 80.3 60.8
INT(ALINE) 80.0 60.7
COMBINED? 80.3 60.8
JnJk DIRECTL 53.5 56.0
Table 1: Top-1 word accuracy on the development
and test sets. The asterisk denotes the results ob-
tained after the test reference sets were released.
system described in Section 4. DIRECTL refers
to our language-independent model, which uses
many-to-many alignments. The INT abbreviation
denotes the models operating on the language-
specific intermediate representations described in
Section 4.1. The alignment algorithm (ALINE or
M2M) is given in brackets.
In the EnHi set, many names consisted of mul-
tiple words: we assumed a one-to-one correspon-
dence between consecutive English words and
consecutive Hindi words. In Table 1, the results in
the first row (DIRECTL) were obtained with an au-
tomatic cleanup script that replaced hyphens with
spaces, deleted the remaining punctuation and nu-
merical symbols, and removed 43 transliteration
pairs with a disagreement between the number of
source and target words. The results in the sec-
ond row (DIRECTL+MC) were obtained when the
cases with a disagreement were individually ex-
amined and corrected by a Hindi speaker.
We did not incorporate any external resources
into the models presented in Table 1. In order
to emphasize the performance of our language-
independent approach, we consistently used the
DIRECTL model for generating our ?standard?
runs on all six language pairs, regardless of its rel-
ative performance on the development sets.
6 Discussion
DIRECTL, our language-independent approach to
transliteration achieves excellent results, espe-
cially on the EnCh, EnRu, and EnHi data sets,
which represent a wide range of language pairs
and writing scripts. Both the many-to-many
and phonetic alignment algorithms produce high-
quality alignments. The former can be applied di-
rectly to the training data without the need for an
intermediate representation, while the latter does
not require any training. Surprisingly, incorpo-
ration of language-specific intermediate represen-
tations does not consistently improve the perfor-
mance of our system, which indicates that DI-
RECTL may be able to discover the structures im-
plicit in the training data without additional guid-
ance. The EnHi results suggest that manual clean-
ing of noisy data can yield noticeable gains in ac-
curacy. On the other hand, a simple method of
combining predictions from different systems pro-
duced clear improvement on the EnCh set, but
mixed results on two other sets. More research on
this issue is warranted.
Acknowledgments
This research was supported by the Alberta Inge-
nuity, Informatics Circle of Research Excellence
(iCORE), and Natural Sciences of Engineering
Research Council of Canada (NSERC).
References
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
International Phonetic Association. 1999. Handbook
of the International Phonetic Association. Cam-
bridge University Press.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme
conversion. In Proc. HLT-NAACL, pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proc.
ACL, pages 905?913.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. Advances in kernel methods:
support vector learning, pages 169?184. MIT Press.
Grzegorz Kondrak. 2000. A new algorithm for the
alignment of phonetic sequences. In Proc. NAACL,
pages 288?295.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proc. SI-
GIR, pages 721?722.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source channel model for machine transliteration. In
Proc. ACL, pages 159?166.
Haizhou Li, A Kumaran, Min Zhang, and Vladimir
Pervouchine. 2009. Whitepaper of NEWS 2009
machine transliteration shared task. In Proc. ACL-
IJCNLP Named Entities Workshop.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
Proc. HLT-NAACL, pages 257?264.
31
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 697?700,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Integrating Joint n-gram Features
into a Discriminative Training Framework
Sittichai Jiampojamarn? and Colin Cherry? and Grzegorz Kondrak?
?Department of Computing Science ?National Research Council Canada
University of Alberta 1200 Montreal Road
Edmonton, AB, T6G 2E8, Canada Ottawa, ON, K1A 0R6, Canada
{sj,kondrak}@cs.ualberta.ca Colin.Cherry@nrc-cnrc.gc.ca
Abstract
Phonetic string transduction problems, such
as letter-to-phoneme conversion and name
transliteration, have recently received much
attention in the NLP community. In the past
few years, two methods have come to dom-
inate as solutions to supervised string trans-
duction: generative joint n-gram models, and
discriminative sequence models. Both ap-
proaches benefit from their ability to consider
large, flexible spans of source context when
making transduction decisions. However, they
encode this context in different ways, provid-
ing their respective models with different in-
formation. To combine the strengths of these
two systems, we include joint n-gram fea-
tures inside a state-of-the-art discriminative
sequence model. We evaluate our approach
on several letter-to-phoneme and translitera-
tion data sets. Our results indicate an improve-
ment in overall performance with respect to
both the joint n-gram approach and traditional
feature sets for discriminative models.
1 Introduction
Phonetic string transduction transforms a source
string into a target representation according to its
pronunciation. Two important examples of this task
are letter-to-phoneme conversion and name translit-
eration. In general, the problem is challenging be-
cause source orthography does not unambiguously
specify the target representation. When consider-
ing letter-to-phoneme, ambiguities and exceptions
in the pronunciation of orthography complicate con-
version. Transliteration suffers from the same ambi-
guities, but the transformation is further complicated
by restrictions in the target orthography that may not
exist in the source.
Joint n-gram models (Bisani and Ney, 2002;
Chen, 2003; Bisani and Ney, 2008) have been
widely applied to string transduction problems (Li et
al., 2004; Demberg et al, 2007; Jansche and Sproat,
2009). The power of the approach lies in building
a language model over the operations used in the
conversion from source to target. Crucially, this al-
lows the inclusion of source context in the generative
story. Smoothing techniques play an important role
in joint n-gram models, greatly affecting their per-
formance. Although joint n-gram models are capa-
ble of capturing context information in both source
and target, they cannot selectively use only source
or target information, nor can they consider arbitrary
sequences within their context window, as they are
limited by their back-off schedule.
Discriminative sequence models have also been
shown to perform extremely well on string transduc-
tion problems. These begin with a Hidden Markov
Model architecture, augmented with substring op-
erations and discriminative training. The primary
strength of these systems is their ability to include
rich indicator features representing long sequences
of source context. We will assume a specific in-
stance of discriminative sequence modeling, DI-
RECTL (Jiampojamarn et al, 2009), which achieved
the best results on several language pairs in the
NEWS Machine Transliteration Shared Task (Li et
al., 2009). The same system matches or exceeds the
performance of the joint n-gram approach on letter-
to-phoneme conversion (Jiampojamarn et al, 2008).
Its features are optimized by an online, margin-
697
based learning algorithm, specifically, the Margin
Infused Relaxed Algorithm, MIRA (Crammer and
Singer, 2003).
In this paper, we propose an approach that com-
bines these two different paradigms by formulating
the joint n-gram model as a new set of features in the
discriminative model. This leverages an advantage
of discriminative training, in that it can easily and
effectively incorporate arbitrary features. We eval-
uate our approach on several letter-to-phoneme and
transliteration data sets. Our results demonstrate an
improvement in overall performance with respect to
both the generative joint n-gram approach and the
original DIRECTL system.
2 Background
String transduction transforms an input string x into
the desired output string y. The input and output are
different representations of the same entity; for ex-
ample, the spelling and the pronunciation of a word,
or the orthographic forms of a word in two different
writing scripts.
One approach to string transduction is to view
it as a tagging problem where the input charac-
ters are tagged with the output characters. How-
ever, since sounds are often represented by multi-
character units, the relationship between the input
and output characters is often complex. This pre-
vents the straightforward application of standard
tagging techniques, but can be addressed by sub-
string decoders or semi-Markov models.
Because the relationship between x and y is hid-
den, alignments between the input and output char-
acters (or substrings) are often provided in a pre-
processing step. These are usually generated in an
unsupervised fashion using a variant of the EM al-
gorithm. Our system employs the many-to-many
alignment described in (Jiampojamarn et al, 2007).
We trained our system on these aligned examples by
using the online discriminative training of (Jiampo-
jamarn et al, 2009). At each step, the parameter
update is provided by MIRA.
3 Features
Jiampojamarn et al (2009) describe a set of indica-
tor feature templates that include (1) context features
(2) transition features and (3) linear-chain features.
context xi?c yi
. . .
xi+c yi
xi?cxi?c+1 yi
. . .
xi+c?1xi+c yi
. . . . . .
xi?c . . . xi+c yi
transition yi?1 yi
linear-chain xi?c yi?1 yi
. . .
xi+c yi?1 yi
xi?cxi?c+1 yi?1 yi
. . .
xi+c?1xi+c yi?1 yi
. . . . . .
xi?c . . . xi+c, yi?1 yi
joint n-gram xi+1?nyi+1?nxiyi
. . .
xi?1yi?1xiyi
xi+1?nyi+1?nxi+2?nyi+2?nxiyi
. . .
xi?2yi?2xi?1yi?1xiyi
. . . . . .
xi+1?nyi+1?n . . . xi?1yi?1xiyi
Table 1: Feature template
Table 1 summarizes these features and introduces
the new set of joint n-gram features.
The context features represent the source side ev-
idence that surrounds an input substring xi as it gen-
erates the target output yi. These features include
all possible n-grams that fit inside a source-side con-
text windows of size C, each conjoined with yi. The
transition features enforce the cohesion of the gen-
erated output with target-side bigrams. The linear-
chain features conjoin context and transition fea-
tures.
The set of feature templates described above
has been demonstrated to achieve excellent perfor-
mance. The context features express rich informa-
tion on the source side, but no feature template al-
lows target context beyond yi?1,yi. Target and
source context are considered jointly, but only in a
very limited fashion, as provided by the linear chain
features. Jiampojamarn et al (2008) report that con-
text features contribute the most to system perfor-
mance. They also report that increasing the Markov
order in the transition features from bigram to tri-
698
Figure 1: System accuracy as a function of the beam size
gram results in no significant improvement. Intu-
itively, the joint information of both source and tar-
get sides is important in string transduction prob-
lems. By integrating the joint n-gram features into
the online discriminative training framework, we en-
able the system to not only enjoy rich context fea-
tures and long-range dependency linear-chain fea-
tures, but we also take advantage of joint informa-
tion between source and target substring pairs, as
encoded by the joint n-gram template shown in the
bottom of Table 1.
An alternative method to incorporate a joint n-
gram feature would compute the generative joint n-
gram scores, and supply them as a real-valued fea-
ture to the model. As all of the other features in
the DIRECTL framework are indicators, the training
algorithm may have trouble scaling an informative
real-valued feature. Therefore, we represent these
joint n-gram features as binary features that indi-
cate whether the model has seen particular strings
of joint evidence in the previous n ? 1 operations
when generating yi from xi. In this case, the sys-
tem learns a distinct weight for each substring of the
joint n-gram.
In order to accommodate higher-order joint n-
grams, we replace the exact search algorithm of Ji-
ampojamarn et al (2008) with a beam search. Dur-
ing our development experiments, we observed no
significant decrease in accuracy after introducing
this approximation. Figure 1 shows the system per-
formance in terms of the word accuracy as a function
of the beam size on a development set. The perfor-
mance starts to converge quickly and shows no fur-
ther improvement for values grater than 20. In the
remaining experiments we set the beam size to 50.
We also performed development experiments
Figure 2: System accuracy as a function of n-gram size
with a version of the system that includes only joint
n-gram indicators. Figure 2 shows the word ac-
curacy with different values of n. The accuracy
reaches its maximum for n = 4, and actually falls
off for larger values of n. This anomaly is likely
caused by the model using its expanded expressive
power to memorize sequences of operations, overfit-
ting to its training data. Such overfitting is less likely
to happen in the generative joint n-gram model,
which smooths high-order estimates very carefully.
4 Experiments and Results
We evaluate our new approach on two string trans-
duction applications: (1) letter-to-phoneme conver-
sion and (2) name transliteration. For the letter-to-
phoneme conversion, we employ the English Celex,
NETtalk, OALD, CMUdict, and the French Brulex
data sets. In order to perform direct comparison with
the joint n-gram approach, we follow exactly the
same data splits as Bisani and Ney (2008). The train-
ing sizes range from 19K to 106K words. For the
transliteration task, we use three data sets provided
by the NEWS 2009 Machine Transliteration Shared
Task (Li et al, 2009): English-Russian (EnRu),
English-Chinese (EnCh), and English-Hindi (EnHi).
The training sizes range from 10K to 30K words.
We set n = 6 for the joint n-gram features; other pa-
rameters are set on the respective development sets.
Tables 2 and 3 show the performance of our new
system in comparison with the joint n-gram ap-
proach and DIRECTL. The results in the rightmost
column of Table 2 are taken directly from (Bisani
and Ney, 2008), where they were evaluated on the
same data splits. The results in the rightmost col-
umn of Table 3 are from (Jansche and Sproat, 2009),
which was the best performing system based on joint
699
Data set this work DIRECTL joint n-gram
Celex 89.23 88.54 88.58
CMUdict 76.41 75.41 75.47
OALD 85.54 82.43 82.51
NETtalk 73.52 70.18 69.00
Brulex 95.21 95.03 93.75
Table 2: Letter-to-phoneme conversion accuracy
Data set this work DIRECTL joint n-gram
EnRu 61.80 61.30 59.70
EnCh 74.17 73.34 64.60
EnHi 50.30 49.80 41.50
Table 3: Name transliteration accuracy
n-grams at NEWS 2009. We report all results in
terms of the word accuracy, which awards the sys-
tem only for complete matches between system out-
puts and the references.
Our full system outperforms both D IRECTL and
the joint n-gram approach in all data sets. This
shows the utility of adding joint n-gram features to
the DIRECTL system, and confirms an advantage of
discriminative approaches: strong competitors can
simply be folded into the model.
Comparing across tables, one can see that the gap
between the generative joint n-gram and the DI-
RECTL methods is much larger for the transliter-
ation tasks. This could be because joint n-grams
are a poor fit for transliteration, or the gap could
stem from differences between the joint n-gram im-
plementations used for the two tasks. Looking at
the improvements to DIRECTL from joint n-gram
features, we see further evidence that joint n-grams
are better suited to letter-to-phoneme than they are
to transliteration: letter-to-phoneme improvements
range from relative error reductions of 3.6 to 17.3,
while in transliteration, the largest reduction is 3.1.
5 Conclusion
We have presented a new set of joint n-gram features
for the DIRECTL discriminative sequence model.
The resulting system combines two successful ap-
proaches for string transduction ? D IRECTL and
the joint n-gram model. Joint n-gram indicator fea-
tures are efficiently trained using a large margin
method. We have shown that the resulting system
consistently outperforms both DIRECTL and strong
joint n-gram implementations in letter-to-phoneme
conversion and name transliteration, establishing a
new state-of-the-art for these tasks.
Acknowledgements
This research was supported by the Alberta Ingenu-
ity Fund and the Natural Sciences and Engineering
Research Council of Canada.
References
Maximilian Bisani and Hermann Ney. 2002. Investi-
gations on joint-multigram models for grapheme-to-
phoneme conversion. In Proc. ICSLP, pages 105?108.
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434?451.
Stanley F. Chen. 2003. Conditional and joint mod-
els for grapheme-to-phoneme conversion. In Proc.
Eurospeech-2003.
Koby Crammer and Yoram Singer. 2003. Ultraconserva-
tive online algorithms for multiclass problems. Jour-
nal of Machine Learning Research, 3:951?991.
Vera Demberg, Helmut Schmid, and Gregor Mo?hler.
2007. Phonological constraints and morphological
preprocessing for grapheme-to-phoneme conversion.
In Proc. ACL, pages 96?103.
Martin Jansche and Richard Sproat. 2009. Named entity
transcription with pair n-gram models. In Proc. ACL-
IJCNLP Named Entities Workshop, pages 32?35.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme con-
version. In Proc. HLT-NAACL, pages 372?379.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proc.
ACL, pages 905?913.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language independent approach to translitera-
tion. In Proc. ACL-IJCNLP Named Entities Workshop,
pages 28?31.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source channel model for machine transliteration. In
Proc. ACL, pages 159?166.
Haizhou Li, A Kumaran, Vladimir Pervouchine, and Min
Zhang. 2009. Report of NEWS 2009 machine translit-
eration shared task. In Proc. ACL-IJCNLP Named En-
tities Workshop, pages 1?18.
700
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 780?788,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Letter-Phoneme Alignment: An Exploration
Sittichai Jiampojamarn and Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, T6G 2E8, Canada
{sj,kondrak}@cs.ualberta.ca
Abstract
Letter-phoneme alignment is usually gen-
erated by a straightforward application of
the EM algorithm. We explore several al-
ternative alignment methods that employ
phonetics, integer programming, and sets
of constraints, and propose a novel ap-
proach of refining the EM alignment by
aggregation of best alignments. We per-
form both intrinsic and extrinsic evalua-
tion of the assortment of methods. We
show that our proposed EM-Aggregation
algorithm leads to the improvement of the
state of the art in letter-to-phoneme con-
version on several different data sets.
1 Introduction
Letter-to-phoneme (L2P) conversion (also called
grapheme-to-phoneme conversion) is the task of
predicting the pronunciation of a word given its
orthographic form by converting a sequence of
letters into a sequence of phonemes. The L2P
task plays a crucial role in speech synthesis sys-
tems (Schroeter et al, 2002), and is an important
part of other applications, including spelling cor-
rection (Toutanova and Moore, 2001) and speech-
to-speech machine translation (Engelbrecht and
Schultz, 2005). Many data-driven techniques have
been proposed for letter-to-phoneme conversion
systems, including neural networks (Sejnowski
and Rosenberg, 1987), decision trees (Black et al,
1998), pronunciation by analogy (Marchand and
Damper, 2000), Hidden Markov Models (Taylor,
2005), and constraint satisfaction (Bosch and Can-
isius, 2006).
Letter-phoneme alignment is an important step
in the L2P task. The training data usually consists
of pairs of letter and phoneme sequences, which
are not aligned. Since there is no explicit infor-
mation indicating the relationships between indi-
vidual letter and phonemes, these must be inferred
by a letter-phoneme alignment algorithm before
a prediction model can be trained. The quality
of the alignment affects the accuracy of L2P con-
version. Letter-phoneme alignment is closely re-
lated to transliteration alignment (Pervouchine et
al., 2009), which involves graphemes representing
different writing scripts. Letter-phoneme align-
ment may also be considered as a task in itself; for
example, in the alignment of speech transcription
with text in spoken corpora.
Most previous L2P approaches induce the align-
ment between letters and phonemes with the ex-
pectation maximization (EM) algorithm. In this
paper, we propose a number of alternative align-
ment methods, and compare them to the EM-
based algorithms using both intrinsic and extrin-
sic evaluations. The intrinsic evaluation is con-
ducted by comparing the generated alignments to
a manually-constructed gold standard. The extrin-
sic evaluation uses two different generation tech-
niques to perform letter-to-phoneme conversion
on several different data sets. We discuss the ad-
vantages and disadvantages of various methods,
and show that better alignments tend to improve
the accuracy of the L2P systems regardless of the
actual technique. In particular, one of our pro-
posed methods advances the state of the art in L2P
conversion. We also examine the relationship be-
tween alignment entropy and alignment quality.
This paper is organized as follows. In Sec-
tion 2, we enumerate the assumptions that the
alignment methods commonly adopt. In Section 3,
we review previous work that employs the EM ap-
proach. In Sections 4, 5 and 6, we describe alter-
native approaches based on phonetics, manually-
constructed constraints, and Integer Programming,
respectively. In Section 7, we propose an algo-
rithm to refine the alignments produced by EM.
Sections 8 and 9 are devoted to the intrinsic and
extrinsic evaluation of various approaches. Sec-
tion 10 concludes the paper.
780
2 Background
We define the letter-phoneme alignment task as
the problem of inducing links between units that
are related by pronunciation. Each link is an in-
stance of a specific mapping between letters and
phonemes. The leftmost example alignment of the
word accuse [@kjuz] below includes 1-1, 1-0, 1-
2, and 2-1 links. The letter e is considered to be
linked to special null phoneme.
Figure 1: Two alignments of accuse.
The following constraints on links are assumed
by some or all alignment models:
? the monotonicity constraint prevents links
from crossing each other;
? the representation constraint requires each
phoneme to be linked to at least one letter,
thus precluding nulls on the letter side;
? the one-to-one constraint stipulates that each
letter and phoneme may participate in at most
one link.
These constraints increasingly reduce the search
space and facilitate the training process for the
L2P generation models.
We refer to an alignment model that assumes all
three constraints as a pure one-to-one (1-1) model.
By allowing only 1-1 and 1-0 links, the align-
ment task is thus greatly simplified. In the sim-
plest case, when the number of letters is equal to
the number of phonemes, there is only one pos-
sible alignment that satisfies all three constraints.
When there are more letters than phonemes, the
search is reduced to identifying letters that must
be linked to null phonemes (the process referred
to as ?epsilon scattering? by Black et al (1998)).
In some words, however, one letter clearly repre-
sents more than one phoneme; for example, u in
Figure 1. Moreover, a pure 1-1 approach cannot
handle cases where the number of phonemes ex-
ceeds the number of letters. A typical solution to
overcome this problems is to introduce so-called
double phonemes by merging adjacent phonemes
that could be represented as a single letter. For
example, a double phoneme U would replace a se-
quence of the phonemes j and u in Figure 1. This
solution requires a manual extension of the set of
phonemes present in the data. By convention, we
regard the models that include a restricted set of
1-2 mappings as 1-1 models.
Advanced L2P approaches, including the joint
n-gram models (Bisani and Ney, 2008) and the
joint discriminative approach (Jiampojamarn et
al., 2007) eliminate the one-to-one constraint en-
tirely, allowing for linking of multiple letters to
multiple phonemes. We refer to such models as
many-to-many (M-M) models.
3 EM Alignment
Early EM-based alignment methods (Daelemans
and Bosch, 1997; Black et al, 1998; Damper et
al., 2005) were generally pure 1-1 models. The
1-1 alignment problem can be formulated as a dy-
namic programming problem to find the maximum
score of alignment, given a probability table of
aligning letter and phoneme as a mapping func-
tion. The dynamic programming recursion to find
the most likely alignment is the following:
Ci,j = max
?
?
?
Ci?1,j?1 + ?(xi, yj)
Ci?1,j + ?(xi, ?)
Ci,j?1 + ?(?, yj)
(1)
where ?(xi, ?) denotes a probability that a let-
ter xi aligns with a null phoneme and ?(?, yj) de-
notes a probability that a null letter aligns with a
phoneme yj . In practice, the latter probability is
often set to zero in order to enforce the represen-
tation constraint, which facilitates the subsequent
phoneme generation process. The probability ta-
ble ?(xi, yj) can be initialized by a uniform dis-
tribution and is iteratively re-computed (M-step)
from the most likely alignments found at each it-
eration over the data set (E-step). The final align-
ments are constructed after the probability table
converges.
M2M-aligner (Jiampojamarn et al, 2007) is a
many-to-many (M-M) alignment algorithm based
on EM that allows for mapping of multiple let-
ters to multiple phonemes. Algorithm 1 describes
the E-step of the many-to-many alignment algo-
rithm. ? represents partial counts collected over
all possible mappings between substrings of let-
ters and phonemes. The maximum lengths of let-
ter and phoneme substrings are controlled by the
781
Algorithm 1: Many-to-many alignment
Input: x, y,maxX,maxY, ?
Output: ?
? := FORWARD-M2M (x, y,maxX,maxY )1
? := BACKWARD-M2M (x, y,maxX,maxY )2
T = |x| + 1 , V = |y| + 13
if (?T,V = 0) then4
return5
for t = 1..T , v = 1..V do6
for i = 1..maxX st t ? i ? 0 do7
?(xtt?i+1, ?) +=
?t?i,v?(xtt?i+1,?)?t,v
?T,V8
for i = 1..maxX st t ? i ? 0 do9
for j = 1..maxY st v ? j ? 0 do10
?(xtt?i+1, yvv?j+1) +=
?t?i,v?j?(xtt?i+1,y
v
v?j+1)?t,v
?T,V11
maxX and maxY parameters. The forward prob-
ability ? is estimated by summing the probabilities
from left to right, while the backward probabil-
ity ? is estimated in the opposite direction. The
FORWARD-M2M procedure is similar to line 3 to
10 of Algorithm 1, except that it uses Equation 2
in line 8 and 3 in line 11. The BACKWARD-M2M
procedure is analogous to FORWARD-M2M.
?t,v += ?(xtt?i+1, ?)?t?i,v (2)
?t,v += ?(xtt?i+1, yvv?j+1)?t?i,v?j (3)
In M-step, the partial counts are normalized
by using a conditional distribution to create the
mapping probability table ?. The final many-to-
many alignments are created by finding the most
likely paths using the Viterbi algorithm based on
the learned mapping probability table. The source
code of M2M-aligner is publicly available.1
Although the many-to-many approach tends to
create relatively large models, it generates more
intuitive alignments and leads to improvement in
the L2P accuracy (Jiampojamarn et al, 2007).
However, since many links involve multiple let-
ters, it also introduces additional complexity in the
phoneme prediction phase. One possible solution
is to apply a letter segmentation algorithm at test
time to cluster letters according to the alignments
in the training data. This is problematic because
of error propagation inherent in such a process.
A better solution is to combine segmentation and
decoding using a phrasal decoder (e.g. (Zens and
Ney, 2004)).
1http://code.google.com/p/m2m-aligner/
4 Phonetic alignment
The EM-based approaches to L2P alignment treat
both letters and phonemes as abstract symbols.
A completely different approach to L2P align-
ment is based on the phonetic similarity between
phonemes. The key idea of the approach is to rep-
resent each letter by a phoneme that is likely to be
represented by the letter. The actual phonemes on
the phoneme side and the phonemes representing
letters on the letter side can then be aligned on the
basis of phonetic similarity between phonemes.
The main advantage of the phonetic alignment is
that it requires no training data, and so can be read-
ily be applied to languages for which no pronunci-
ation lexicons are available.
The task of identifying the phoneme that is most
likely to be represented by a given letter may seem
complex and highly language-dependent. For ex-
ample, the letter a can represent no less than 12
different English vowels. In practice, however, ab-
solute precision is not necessary. Intuitively, the
letters that had been chosen (often centuries ago)
to represent phonemes in any orthographic system
tend to be close to the prototype phoneme in the
original script. For example, the letter ?o? rep-
resented a mid-high rounded vowel in Classical
Latin and is still generally used to represent simi-
lar vowels.
The following simple heuristic works well for a
number of languages: treat every letter as if it were
a symbol in the International Phonetic Alphabet
(IPA). The set of symbols employed by the IPA in-
cludes the 26 letters of the Latin alphabet, which
tend to correspond to the phonemes that they rep-
resent in the Latin script. For example, the IPA
symbol [m] denotes a voiced bilabial nasal con-
sonant, which is the phoneme represented by the
letter m in most languages that utilize Latin script.
ALINE (Kondrak, 2000) performs phonetic
alignment of two strings of phonemes. It combines
a dynamic programming alignment algorithm with
an appropriate scoring scheme for computing pho-
netic similarity on the basis of multivalued fea-
tures. The example below shows the alignment of
the word sheath to its phonetic transcription [S iT].
ALINE correctly links the most similar pairs of
phonemes (s:S, e:i, t:T).2
2ALINE can also be applied to non-Latin scripts by re-
placing every grapheme with the IPA symbol that is phoneti-
cally closest to it.
782
s h e a t h
| | | | | |
S - i - T -
Since ALINE is designed to align phonemes
with phonemes, it does not incorporate the repre-
sentation constraint. In order to avoid the prob-
lem of unaligned phonemes, we apply a post-
processing algorithm, which also handles 1-2
links. The algorithm first attempts to remove 0-1
links by merging them with the adjacent 1-0 links.
If this is not possible, the algorithm scans a list of
valid 1-2 mappings, attempting to replace a pair of
0-1 and 1-1 links with a single 1-2 link. If this also
fails, the entire entry is removed from the training
set. Such entries often represent unusual foreign-
origin words or outright annotation errors. The
number of unaligned entries rarely exceeds 1% of
the data.
The post-processing algorithm produces an
alignment that contains 1-0, 1-1, and 1-2 links.
The list of valid 1-2 mappings must be prepared
manually. The length of such lists ranges from 1
for Spanish and German (x:[ks]) to 17 for English.
This approach is more robust than the double-
phoneme technique because the two phonemes are
clustered only if they can be linked to the corre-
sponding letter.
5 Constraint-based alignment
One of the advantages of the phonetic alignment
is its ability to rule out phonetically implausible
letter-phoneme links, such as o:p. We are in-
terested in establishing whether a set of allow-
able letter-phoneme mappings could be derived di-
rectly from the data without relying on phonetic
features.
Black et al (1998) report that constructing lists
of possible phonemes for each letter leads to L2P
improvement. They produce the lists in a ?semi-
automatic?, interactive manner. The lists constrain
the alignments performed by the EM algorithm
and lead to better-quality alignments.
We implement a similar interactive program
that incrementally expands the lists of possible
phonemes for each letter by refining alignments
constrained by those lists. However, instead of
employing the EM algorithm, we induce align-
ments using the standard edit distance algorithm
with substitution and deletion assigned the same
cost. In cases when there are multiple alternative
alignments that have the same edit distance, we
randomly choose one of them. Furthermore, we
extend this idea also to many-to-many alignments.
In addition to lists of phonemes for each letter (1-
1 mappings), we also construct lists of many-to-
many mappings, such as ee:i, sch:S, and ew:ju. In
total, the English set contains 377 mappings, of
which more than half are of the 2-1 type.
6 IP Alignment
The process of manually inducing allowable letter-
phoneme mappings is time-consuming and in-
volves a great deal of language-specific knowl-
edge. The Integer Programming (IP) framework
offers a way to induce similar mappings without a
human expert in the loop. The IP formulation aims
at identifying the smallest set of letter-phoneme
mappings that is sufficient to align all instances in
the data set.
Our IP formulation employs the three con-
straints enumerated in Section 2, except that the
one-to-one constraint is relaxed in order to identify
a small set of 1-2 mappings. We specify two types
of binary variables that correspond to local align-
ment links and global letter-phoneme mappings,
respectively. We distinguish three types of local
variables, X , Y , and Z, which correspond to 1-0,
1-1, and 1-2 links, respectively. In order to min-
imize the number of global mappings, we set the
following objective that includes variables corre-
sponding to 1-1 and 1-2 mappings:
minimize :
?
l,p
G(l, p) +
?
l,p1,p2
G(l, p1p2) (4)
We adopt a simplifying assumption that any let-
ter can be linked to a null phoneme, so no global
variables corresponding to 1-0 mappings are nec-
essary.
In the lexicon entry k, let lik be the letter at po-
sition i, and pjk the phoneme at position j. In or-
der to prevent the alignments from utilizing letter-
phoneme mappings which are not on the global
list, we impose the following constraints:
?i,j,kY (i, j, k) ? G(lik, pjk) (5)
?i,j,kZ(i, j, k) ? G(lik, pjkp(j+1)k) (6)
For example, the local variable Y (i, j, k) is set if
lik is linked to pjk. A corresponding global vari-
able G(lik, pjk) is set if the list of allowed letter-
phoneme mappings includes the link (lik, pjk).
Activating the local variable implies activating the
corresponding global variable, but not vice versa.
783
Figure 2: A network of possible alignment links.
We create a network of possible alignment links
for each lexicon entry k, and assign a binary vari-
able to each link in the network. Figure 2 shows an
alignment network for the lexicon entry k: wriggle
[r I g @ L]. There are three 1-0 links (level), three
1-1 links (diagonal), and one 1-2 link (steep). The
local variables that receive the value of 1 are the
following: X(1,0,k), Y(2,1,k), Y(3,2,k), Y(4,3,k),
X(5,3,k), Z(6,5,k), and X(7,5,k). The correspond-
ing global variables are: G(r,r), G(i,I), G(g,g), and
G(l,@L).
We create constraints to ensure that the link
variables receiving a value of 1 form a left-to-right
path through the alignment network, and that all
other link variables receive a value of 0. We ac-
complish this by requiring the sum of the links
entering each node to equal the sum of the links
leaving each node.
?i,j,k X(i, j, k) + Y (i, j, k) + Z(i, j, k) =
X(i + 1, j, k) + Y (i + 1, j + 1, k)
+Z(i + 1, j + 2, k)
We found that inducing the IP model with the
full set of variables gives too much freedom to the
IP program and leads to inferior results. Instead,
we first run the full set of variables on a subset of
the training data which includes only the lexicon
entries in which the number of phonemes exceeds
the number of letters. This generates a small set
of plausible 1-2 mappings. In the second pass, we
run the model on the full data set, but we allow
only the 1-2 links that belong to the initial set of
1-2 mappings induced in the first pass.
6.1 Combining IP with EM
The set of allowable letter-phoneme mappings can
also be used as an input to the EM alignment algo-
rithm. We call this approach IP-EM. After induc-
ing the minimal set of letter-phoneme mappings,
we constrain EM to use only those mappings with
the exclusion of all others. We initialize the prob-
ability of the minimal set with a uniform distribu-
tion, and set it to zero for other mappings. We train
the EM model in a similar fashion to the many-to-
many alignment algorithm presented in Section 3,
except that we limit the letter size to be one letter,
and that any letter-phoneme mapping that is not in
the minimal set is assigned zero count during the
E-step. The final alignments are generated after
the parameters converge.
7 Alignment by aggregation
During our development experiments, we ob-
served that the technique that combines IP with
EM described in the previous section generally
leads to alignment quality improvement in com-
parison with the IP alignment. Nevertheless, be-
cause EM is constrained not to introduce any new
letter-phoneme mappings, many incorrect align-
ments are still proposed. We hypothesized that in-
stead of pre-constraining EM, a post-processing of
EM?s output may lead to better results.
M2M-aligner has the ability to create precise
links involving more than one letter, such as ph:f.
However, it also tends to create non-intuitive links
such as se:z for the word phrase [f r e z], where e
is clearly a case of a ?silent? letter. We propose
an alternative EM-based alignment method that
instead utilizes a list of alternative one-to-many
alignments created with M2M-aligner and aggre-
gates 1-M links into M-M links in cases when
there is a disagreement between alignments within
the list. For example, if the list contains the two
alignments shown in Figure 3, the algorithm cre-
ates a single many-to-many alignment by merg-
ing the first pair of 1-1 and 1-0 links into a single
ph:f link. However, the two rightmost links are not
merged because there is no disagreement between
the two initial alignments. Therefore, the resulting
alignment reinforces the ph:f mapping, but avoids
the questionable se:z link.
p h r a s e p h r a s e
| | | | | | | | | | | |
f - r e z - - f r e z -
Figure 3: Two alignments of phrase.
In order to generate the list of best alignments,
we use Algorithm 2, which is an adaptation of the
standard Viterbi algorithm. Each cell Qt,v con-
tains a list of n-best scores that correspond to al-
784
Algorithm 2: Extracting n-best alignments
Input: x, y, ?
Output: QT,V
T = |x| + 1 , V = |y| + 11
for t = 1..T do2
Qt,v = ?3
for v = 1..V do4
for q ? Qt?1,v do5
append q ? ?(xt, ?) to Qt,v6
for j = 1..maxY st v ? j ? 0 do7
for q ? Qt?1,v?j do8
append q ? ?(xt, yvv?j+1) to Qt,v9
sort Qt,v10
Qt,v = Qt,v[1 : n]11
ternative alignments during the forward pass. In
line 9, we consider all possible 1-M links between
letter xt and phoneme substring yvv?j+1. At the
end of the main loop, we keep at most n best align-
ments in each Qt,v list.
Algorithm 2 yields n-best alignments in the
QT,V list. However, in order to further restrict
the set of high-quality alignments, we also dis-
card the alignments with scores below threshold
R with respect to the best alignment score. Based
on the experiments with the development set, we
set R = 0.8 and n = 10.
8 Intrinsic evaluation
For the intrinsic evaluation, we compared the gen-
erated alignments to gold standard alignments ex-
tracted from the the core vocabulary of the Com-
bilex data set (Richmond et al, 2009). Combilex
is a high quality pronunciation lexicon with ex-
plicit expert manual alignments. We used a sub-
set of the lexicon composed of the core vocabu-
lary containing 18,145 word-phoneme pairs. The
alignments contain 550 mappings, which include
complex 4-1 and 2-3 types.
Each alignment approach creates alignments
from unaligned word-phoneme pairs in an un-
supervised fashion. We distinguish between the
1-1 and M-M approaches. We report the align-
ment quality in terms of precision, recall and F-
score. Since the gold standard includes many links
that involve multiple letters, the theoretical up-
per bound for recall achieved by a one-to-one ap-
proach is 90.02%. However, it is possible to obtain
the perfect precision because we count as correct
all 1-1 links that are consistent with the M-M links
in the gold standard. The F-score corresponding
to perfect precision and the upper-bound recall is
94.75%.
Alignment entropy is a measure of alignment
quality proposed by Pervouchine et al (2009) in
the context of transliteration. The entropy indi-
cates the uncertainty of mapping between letter
l and phoneme p resulting from the alignment:
We compute the alignment entropy for each of the
methods using the following formula:
H = ?
?
l,p
P (l, p) logP (l|p) (7)
Table 1 includes the results of the intrinsic eval-
uation. (the two rightmost columns are discussed
in Section 9). The baseline BaseEM is an im-
plementation of the one-to-one alignment method
of (Black et al, 1998) without the allowable list.
ALINE is the phonetic method described in Sec-
tion 4. SeedMap is the hand-seeded method de-
scribed in Section 5. M-M-EM is the M2M-
aligner approach of Jiampojamarn et al (2007).
1-M-EM is equivalent to M-M-EM but with the
restriction that each link contains exactly one let-
ter. IP-align is the alignment generated by the
IP formulation from Section 6. IP-EM is the
method that combines IP with EM described in
Section 6.1. EM-Aggr is our final many-to-many
alignment method described in Section 7. Oracle
corresponds to the gold-standard alignments from
Combilex.
Overall, the M-M models obtain lower preci-
sion but higher recall and F-score than 1-1 models,
which is to be expected as the gold standard is de-
fined in terms of M-M links. ALINE produces the
most accurate alignments among the 1-1 methods,
with the precision and recall values that are very
close to the theoretical upper bounds. Its preci-
sion is particularly impressive: on average, only
one link in a thousand is not consistent with the
gold standard. In terms of word accuracy, 98.97%
words have no incorrect links. Out of 18,145
words, only 112 words contain incorrect links, and
further 75 words could not be aligned. The rank-
ing of the 1-1 methods is quite clear: ALINE fol-
lowed by IP-EM, 1-M-EM, IP-align, and BaseEM.
Among the M-M methods, EM-Aggr has slightly
better precision than M-M-EM, but its recall is
much worse. This is probably caused by the ag-
gregation strategy causing EM-Aggr to ?lose? a
significant number of correct links. In general, the
entropy measure does not mirror the quality of the
alignment.
785
Aligner Precision Recall F1 score Entropy L2P 1-1 L2P M-M
BaseEM 96.54 82.84 89.17 0.794 50.00 65.38
ALINE 99.90 89.54 94.44 0.672 54.85 68.74
1-M-EM 99.04 89.15 93.84 0.636 53.91 69.13
IP-align 98.30 88.49 93.14 0.706 52.66 68.25
IP-EM 99.31 89.40 94.09 0.651 53.86 68.91
M-M-EM 96.54 97.13 96.83 0.655 ? 68.52
EM-Aggr 96.67 93.39 95.00 0.635 ? 69.35
SeedMap 97.88 97.44 97.66 0.634 ? 68.69
Oracle 100.0 100.0 100.0 0.640 ? 69.35
Table 1: Alignment quality, entropy, and L2P conversion accuracy on the Combilex data set.
Aligner Celex-En CMUDict NETtalk OALD Brulex
BaseEM 75.35 60.03 54.80 67.23 81.33
ALINE 81.50 66.46 54.90 72.12 89.37
1-M-EM 80.12 66.66 55.00 71.11 88.97
IP-align 78.88 62.34 53.10 70.46 83.72
IP-EM 80.95 67.19 54.70 71.24 87.81
Table 2: L2P word accuracy using the TiMBL-based generation system.
9 Extrinsic evaluation
In order to investigate the relationship between
the alignment quality and L2P performance, we
feed the alignments to two different L2P systems.
The first one is a classification-based learning sys-
tem employing TiMBL (Daelemans et al, 2009),
which can utilize either 1-1 or 1-M alignments.
The second system is the state-of-the-art online
discriminative training for letter-to-phoneme con-
version (Jiampojamarn et al, 2008), which ac-
cepts both 1-1 and M-M types of alignment. Ji-
ampojamarn et al (2008) show that the online dis-
criminative training system outperforms a num-
ber of competitive approaches, including joint n-
grams (Demberg et al, 2007), constraint satisfac-
tion inference (Bosch and Canisius, 2006), pro-
nunciation by analogy (Marchand and Damper,
2006), and decision trees (Black et al, 1998). The
decoder module uses standard Viterbi for the 1-1
case, and a phrasal decoder (Zens and Ney, 2004)
for the M-M case. We report the L2P performance
in terms of word accuracy, which rewards only
the completely correct output phoneme sequences.
The data set is randomly split into 90% for training
and 10% for testing. For all experiments, we hold
out 5% of our training data to determine when to
stop the online training process.
Table 1 includes the results on the Combilex
data set. The two rightmost columns correspond
to our two test L2P systems. We observe that al-
though better alignment quality does not always
translate into better L2P accuracy, there is never-
theless a strong correlation between the two, espe-
cially for the weaker phoneme generation system.
Interestingly, EM-Aggr matches the L2P accuracy
obtained with the gold standard alignments. How-
ever, there is no reason to claim that the gold stan-
dard alignments are optimal for the L2P genera-
tion task, so that result should not be considered as
an upper bound. Finally, we note that alignment
entropy seems to match the L2P accuracy better
than it matches alignment quality.
Tables 2 and 3 show the L2P results on sev-
eral evaluation sets: English Celex, CMUDict,
NETTalk, OALD, and French Brulex. The train-
ing sizes range from 19K to 106K words. We fol-
low exactly the same data splits as in Bisani and
Ney (2008).
The TiMBL L2P generation method (Table 2)
is applicable only to the 1-1 alignment models.
ALINE produces the highest accuracy on four out
of six datasets (including Combilex). The perfor-
mance of IP-EM is comparable to 1-M-EM, but
not consistently better. IP-align does not seem to
measure up to the other algorithms.
The discriminative approach (Table 3) is flexi-
ble enough to utilize all kinds of alignments. How-
ever, the M-M models perform clearly better than
1-1 models. The only exception is NetTalk, which
786
Aligner Celex-En CMUDict NETTalk OALD Brulex
BaseEM 85.66 71.49 68.60 80.76 88.41
ALINE 87.96 75.05 69.52 81.57 94.56
1-M-EM 88.08 75.11 70.78 81.78 94.54
IP-EM 88.00 75.09 70.10 81.76 94.96
M-M-EM 88.54 75.41 70.18 82.43 95.03
EM-Aggr 89.11 75.52 71.10 83.32 95.07
joint n-gram 88.58 75.47 69.00 82.51 93.75
Table 3: L2P word accuracy using the online discriminative system.
Figure 4: L2P word accuracy vs. alignment en-
tropy.
can be attributed to the fact that NetTalk already
includes double-phonemes in its original formu-
lation. In general, the 1-M-EM method achieves
the best results among the 1-1 alignment methods,
Overall, EM-Aggr achieves the best word accuracy
in comparison to other alignment methods includ-
ing the joint n-gram results, which are taken di-
rectly from the original paper of Bisani and Ney
(2008). Except the Brulex and CMUDict data
sets, the differences between EM-Aggr and M-M-
EM are statistically significant according to Mc-
Nemar?s test at 90% confidence level.
Figure 4 contains a plot of alignment entropy
values vs. L2P word accuracy. Each point rep-
resent an application of a particular alignment
method to a different data sets. It appears that
there is only weak correlation between alignment
entropy and L2P accuracy. So far, we have been
unable to find either direct or indirect evidence that
alignment entropy is a reliable measure of letter-
phoneme alignment quality.
10 Conclusion
We investigated several new methods for gener-
ating letter-phoneme alignments. The phonetic
alignment is recommended for languages with lit-
tle or no training data. The constraint-based ap-
proach achieves excellent accuracy at the cost
of manual construction of seed mappings. The
IP alignment requires no linguistic expertise and
guarantees a minimal set of letter-phoneme map-
pings. The alignment by aggregation advances
the state-of-the-art results in L2P conversion. We
thoroughly evaluated the resulting alignments on
several data sets by using them as input to two dif-
ferent L2P generation systems. Finally, we em-
ployed an independently constructed lexicon to
demonstrate the close relationship between align-
ment quality and L2P conversion accuracy.
One open question that we would like to investi-
gate in the future is whether L2P conversion accu-
racy could be improved by treating letter-phoneme
alignment links as latent variables, instead of com-
mitting to a single best alignment.
Acknowledgments
This research was supported by the Alberta In-
genuity, Informatics Circle of Research Excel-
lence (iCORE), and Natural Science of Engineer-
ing Research Council of Canada (NSERC).
References
Maximilian Bisani and Hermann Ney. 2008. Joint-
sequence models for grapheme-to-phoneme conver-
sion. Speech Communication, 50(5):434?451.
Alan W. Black, Kevin Lenzo, and Vincent Pagel. 1998.
Issues in building general letter to sound rules. In
The Third ESCA Workshop in Speech Synthesis,
pages 77?80.
Antal Van Den Bosch and Sander Canisius. 2006.
Improved morpho-phonological sequence process-
ing with constraint satisfaction inference. Proceed-
ings of the Eighth Meeting of the ACL Special Inter-
est Group in Computational Phonology, SIGPHON
?06, pages 41?49.
787
Walter Daelemans and Antal Van Den Bosch. 1997.
Language-independent data-oriented grapheme-to-
phoneme conversion. In Progress in Speech Synthe-
sis, pages 77?89. New York, USA.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2009. TiMBL: Tilburg Mem-
ory Based Learner, version 6.2, Reference Guide.
ILK Research Group Technical Report Series no.
09-01.
Robert I. Damper, Yannick Marchand, John DS.
Marsters, and Alexander I. Bazin. 2005. Align-
ing text and phonemes for speech technology appli-
cations using an EM-like algorithm. International
Journal of Speech Technology, 8(2):147?160.
Vera Demberg, Helmut Schmid, and Gregor Mo?hler.
2007. Phonological constraints and morphologi-
cal preprocessing for grapheme-to-phoneme conver-
sion. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
96?103, Prague, Czech Republic.
Herman Engelbrecht and Tanja Schultz. 2005. Rapid
development of an afrikaans-english speech-to-
speech translator. In International Workshop of Spo-
ken Language Translation (IWSLT), Pittsburgh, PA,
USA.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and hidden markov models to letter-to-phoneme
conversion. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 372?
379, Rochester, New York, USA.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Pro-
ceedings of ACL-08: HLT, pages 905?913, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Grzegorz Kondrak. 2000. A new algorithm for the
alignment of phonetic sequences. In Proceedings of
NAACL 2000: 1st Meeting of the North American
Chapter of the Association for Computational Lin-
guistics, pages 288?295.
Yannick Marchand and Robert I. Damper. 2000. A
multistrategy approach to improving pronunciation
by analogy. Computational Linguistics, 26(2):195?
219.
Yannick Marchand and Robert I. Damper. 2006. Can
syllabification improve pronunciation by analogy of
English? Natural Language Engineering, 13(1):1?
24.
Vladimir Pervouchine, Haizhou Li, and Bo Lin. 2009.
Transliteration alignment. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
136?144, Suntec, Singapore, August. Association
for Computational Linguistics.
Korin Richmond, Robert A. J. Clark, and Sue Fitt.
2009. Robust LTS rules with the Combilex speech
technology lexicon. In Proceedings od Interspeech,
pages 1295?1298.
Juergen Schroeter, Alistair Conkie, Ann Syrdal, Mark
Beutnagel, Matthias Jilka, Volker Strom, Yeon-Jun
Kim, Hong-Goo Kang, and David Kapilow. 2002.
A perspective on the next challenges for TTS re-
search. In IEEE 2002 Workshop on Speech Synthe-
sis.
Terrence J. Sejnowski and Charles R. Rosenberg.
1987. Parallel networks that learn to pronounce En-
glish text. In Complex Systems, pages 1:145?168.
Paul Taylor. 2005. Hidden Markov Models for
grapheme to phoneme conversion. In Proceedings
of the 9th European Conference on Speech Commu-
nication and Technology.
Kristina Toutanova and Robert C. Moore. 2001. Pro-
nunciation modeling for improved spelling correc-
tion. In ACL ?02: Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 144?151, Morristown, NJ, USA.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
HLT-NAACL 2004: Main Proceedings, pages 257?
264, Boston, Massachusetts, USA.
788
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 39?47,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Transliteration Generation and Mining with Limited Training Resources
Sittichai Jiampojamarn, Kenneth Dwyer, Shane Bergsma, Aditya Bhargava,
Qing Dou, Mi-Young Kim, Grzegorz Kondrak
Department of Computing Science
University of Alberta
Edmonton, AB, T6G 2E8, Canada
{sj,dwyer,bergsma,abhargava,qdou,miyoung2,kondrak}@cs.ualberta.ca
Abstract
We present DIRECTL+: an online dis-
criminative sequence prediction model
based on many-to-many alignments,
which is further augmented by the in-
corporation of joint n-gram features.
Experimental results show improvement
over the results achieved by DIRECTL in
2009. We also explore a number of diverse
resource-free and language-independent
approaches to transliteration mining,
which range from simple to sophisticated.
1 Introduction
Many out-of-vocabulary words in statistical ma-
chine translation and cross-language information
retrieval are named entities. If the languages in
question use different writing scripts, such names
must be transliterated. Transliteration can be de-
fined as the conversion of a word from one writ-
ing script to another, which is usually based on the
phonetics of the original word.
DIRECTL+ is our current approach to name
transliteration which is an extension of the DI-
RECTL system (Jiampojamarn et al, 2009). We
augmented the feature set with joint n-gram fea-
tures which allow the discriminative model to uti-
lize long dependencies of joint information of
source and target substrings (Jiampojamarn et al,
2010). Experimental results suggest an improve-
ment over the results achieved by DIRECTL in
2009.
Transliteration mining aims at automatically
obtaining bilingual lists of names written in differ-
ent scripts. We explore a number of different ap-
proaches to transliteration mining in the context of
the NEWS 2010 Shared Task.1 The sole resource
that is provided for each language pair is a ?seed?
1http://translit.i2r.a-star.edu.sg/
news2010
dataset that contains 1K transliteration word pairs.
The objective is then to mine transliteration pairs
from a collection of Wikipedia titles/topics that are
given in both languages.
We explore a number of diverse resource-free
and language-independent approaches to translit-
eration mining. One approach is to bootstrap the
seed data by generating pseudo-negative exam-
ples, which are combined with the positives to
form a dataset that can be used to train a clas-
sifier. We are particularly interested in achiev-
ing good performance without utilizing language-
specific resources, so that the same approach can
be applied with minimal or no modifications to an
array of diverse language pairs.
This paper is divided in two main parts that cor-
respond to the two tasks of transliteration genera-
tion and transliteration mining.
2 Transliteration generation
The structure of this section is as follows. In Sec-
tion 2.1, we describe the pre-processing steps that
were applied to all datasets. Section 2.2 reviews
two methods for aligning the source and target
symbols in the training data. We provide details
on the DIRECTL+ systems in Section 2.3. In Sec-
tion 2.4, we discuss extensions of DIRECTL+ that
incorporate language-specific information. Sec-
tion 2.5 summarizes our results.
2.1 Pre-processing
For all generation tasks, we pre-process the pro-
vided data as follows. First, we convert all char-
acters in the source word to lower case. Then,
we remove non-alphabetic characters unless they
appear in both the source and target words. We
normalize whitespace that surrounds a comma, so
that there are no spaces before the comma and ex-
actly one space following the comma. Finally, we
separate multi-word titles into single words, using
whitespace as the separator. We assume a mono-
39
tonic matching and ignore the titles that have a dif-
ferent number of words on both sides.
We observed that in the ArAe task there are
cases where an extra space is added to the target
when transliterating from Arabic names to their
English equivalents; e.g., ?Al Riyad?, ?El Sayed?,
etc. In order to prevent the pre-processing from
removing too many title pairs, we allow non-equal
matching if the source title is a single word.
For the English-Chinese (EnCh) task, we con-
vert the English letter ?x? to ?ks? to facilitate bet-
ter matching with its Chinese targets.
During testing, we pre-process test data in the
same manner, except that we do not remove non-
alphabetic characters. After the pre-processing
steps, our system proposes 10-best lists for single
word titles in the test data. For multi-word titles,
we construct 10-best lists by ranking the combina-
tion scores of single words that make up the test
titles.
2.2 Alignment
In the transliteration tasks, training data consist
of pairs of names written in source and target
scripts without explicit character-level alignment.
In our experiments, we applied two different algo-
rithms to automatically generate alignments in the
training data. The generated alignments provide
hypotheses of substring mappings in the training
data. Given aligned training data, a transliteration
model is trained to generate names in the target
language given names in the source language.
The M2M-aligner (Jiampojamarn et al, 2007)
is based on the expectation maximization (EM)
algorithm. It allows us to create alignments be-
tween substrings of various lengths. We opti-
mized the maximum substring sizes for the source
and target based on the performance of the end
task on the development sets. We allowed empty
strings (nulls) only on the target side. We used the
M2M-aligner for all alignment tasks, except for
English-Pinyin alignment. The source code of the
M2M-aligner is publicly available.2
An alternative alignment algorithm is based on
the phonetic similarity of graphemes. The key idea
of this approach is to represent each grapheme by a
phoneme or a sequence of phonemes that is likely
to be represented by the grapheme. The sequences
of phonemes on the source side and the target
side can then be aligned on the basis of phonetic
2http://code.google.com/p/m2m-aligner/
b a r c - l a y
| | | | | | | |
b a - k u r - i
Figure 1: An alignment example.
similarity between phonemes. The main advan-
tage of the phonetic alignment is that it requires
no training data. We use the ALINE phonetic
aligner (Kondrak, 2000), which aligns two strings
of phonemes. The example in Figure 1 shows
the alignment of the word Barclay to its Katakana
transliteration ba-ku-ri. The one-to-one alignment
can then be converted to a many-to-many align-
ment by grouping the Japanese phonemes that cor-
respond to individual Katakana symbols.
2.3 DIRECTL+
We refer to our present approach to transliteration
as DIRECTL+. It is an extension of our DIRECTL
system (Jiampojamarn et al, 2009). It includes ad-
ditional ?joint n-gram? features that allow the dis-
criminative model to correlate longer source and
target substrings. The additional features allow
our discriminative model to train on information
that is present in generative joint n-gram models,
and additionally train on rich source-side context,
transition, and linear-chain features that have been
demonstrated to be important in the transliteration
task (Jiampojamarn et al, 2010).
Our model is based on an online discriminative
framework. At each training iteration, the model
generates an m-best list for each given source
name based on the current feature weights. The
feature weights are updated according to the gold-
standard answers and the generated m-best an-
swer lists using the Margin Infused Relaxed Algo-
rithm (MIRA) (Crammer and Singer, 2003). This
training process iterates over the training examples
until the model converges. For m-best and n-gram
parameters, we set m = 10 and n = 6 for all lan-
guage pairs. These parameters as well as others
were optimized on the development sets.
We trained our models directly on the data
that were provided by the organizers, with three
exceptions. In order to improve performance,
we gave special treatment to English-Korean
(EnKo), English-Chinese (EnCh), and English-
Hindi (EnHi). These special cases are described
in the next section.
40
2.4 Beyond DIRECTL+
2.4.1 Korean Jaso
A Korean syllable can be decomposed into two
or three components called Jaso: an initial con-
sonant, a middle vowel, and optionally a final con-
sonant. The Korean generation for EnKo involves
the following three steps: (1) English-to-Jaso gen-
eration, (2) correction of illegal Jaso sequences,
and (3) Jaso-to-Korean conversion.
In order to correct illegal Jaso sequences that
cannot be combined into Korean syllables in step
2, we consider both vowel and consonant rules.
A Korean vowel can be either a simple vowel or
a complex vowel that combines two simple vow-
els. We can use this information in order to replace
double vowels with one complex vowel. We also
use the silent consonant o (i-eung) when we need
to insert a consonant between double vowels. A
Korean vowel - (eu) is most commonly inserted
between two English consonants in transliteration.
In order to resolve three consecutive consonants, it
can be placed into the most probable position ac-
cording to the probability distribution of the train-
ing data.
2.4.2 Japanese Katakana
In the Japanese Katakana generation task, we re-
place each Katakana symbol with one or two let-
ters using standard romanization tables. This has
the effect of expressing the target side in Latin let-
ters, which facilitates the alignment. DIRECTL+
is trained on the converted data to generate the tar-
get from the source. A post-processing program
then attempts to convert the generated letters back
into Katakana symbols. Sequences of letters that
cannot be converted into Katakana are removed
from the output m-best lists and replaced by lower
scoring sequences that pass the back-conversion
filter. Otherwise, there is usually a single valid
mapping because most Katakana symbols are rep-
resented by single vowels or a consonant-vowel
pair. The only apparent ambiguity involves the
letter n, which can either stand by itself or clus-
ter with the following vowel letter. We resolve the
ambiguity by always assuming the latter case un-
less the letter n occurs at the end of the word.
2.4.3 Chinese Pinyin
Following (Jiampojamarn et al, 2009), we experi-
mented with converting the original Chinese char-
acters to Pinyin as an intermediate representation.
Pinyin is the most commonly known romanization
system for Standard Mandarin and many free tools
are available for converting Chinese characters to
Pinyin. Its alphabet contains the same 26 letters
as English. Each Chinese character can be tran-
scribed phonetically into Pinyin. A small percent-
age of Chinese characters have multiple pronunci-
ations, and are thus represented by different Pinyin
sequences. For those characters, we manually se-
lected the pronunciations that are normally used
for names. This pre-processing step significantly
reduces the size of the target symbols: from 370
distinct Chinese characters to 26 Pinyin symbols.
This allows our system to produce better align-
ments.
We developed three models: (1) trained on the
original Chinese characters, (2) trained on Pinyin,
and (3) the model that incorporates the phonetic
alignment described in Section 2.2. The combi-
nation of the predictions of the different systems
was performed using the following simple algo-
rithm (Jiampojamarn et al, 2009). First, we rank
the individual systems according to their top-1 ac-
curacy on the development set. To obtain the top-
1 prediction for each input word, we use simple
voting, with ties broken according to the ranking
of the systems. We generalize this approach to
handle n-best lists by first ordering the candidate
transliterations according to the rank assigned by
each individual system, and then similarly break-
ing ties by voting and using the ranking of the sys-
tems.
2.4.4 Language identification for Hindi
Bhargava and Kondrak (2010) apply support vec-
tor machines (SVMs) to the task of identifying
the language of names. The intuition here is that
language information can inform transliteration.
Bhargava and Kondrak (2010) test this hypothe-
sis on the NEWS 2009 English-Hindi transliter-
ation data by training language identification on
data manually tagged as being of either Indian or
non-Indian origin. It was found that splitting the
data disjointly into two sets and training separate
transliteration models yields no performance in-
crease due to the decreased size of the data for the
models.
We adopt this approach for the NEWS 2010
task, but here we do not use disjoint splits. In-
stead, we use the SVMs to generate probabilities,
and then we apply a threshold to these probabili-
ties to generate two datasets. For example, if we
set the threshold to be 0.05, then we determine the
41
probabilities of a given name being of Indian ori-
gin (phi) and of being of non-Indian origin (pen).
If phi < 0.05 then the name is excluded from the
Indian set, and if pen < 0.05 then the name is
excluded from the non-Indian set. Using the two
obtained non-disjoint sets, we then train a translit-
eration model for each set using DIRECTL+.
Since the two sets are not disjoint, we must de-
cide how to combine the two results. Given that a
name occurs in both sets, and both models provide
a ranked list of possible targets for that name, we
obtain a combined ranking using a linear combi-
nation over the mean reciprocal ranks (MRRs) of
the two lists. The weights used are phi and pen so
that the more likely a name is considered to be of
Indian origin, the more strongly the result from the
Indian set is considered relative to the result from
the non-Indian set.
2.5 Evaluation
In the context of the NEWS 2010 Machine
Transliteration Shared Task we tested our sys-
tem on all twelve datasets: from English to Chi-
nese (EnCh), Thai (EnTh), Hindi (EnHi), Tamil
(EnTa), Bangla (EnBa), Kannada (EnKa), Ko-
rean Hangul (EnKo), Japanese Katakana (EnJa),
Japanese Kanji (JnJk); and, in the opposite di-
rection, to English from Arabic (ArAe), Chi-
nese (ChEn), and Thai (ThEn). For all datasets,
we trained transliteration models on the provided
training and development sets without additional
resources.
Table 1 shows our best results obtained on the
datasets in terms of top-1 accuracy and mean F-
score. We also include the rank in standard runs
ordered by top-1 word accuracy. The EnCh re-
sult presented in the table refers to the output of
the three-system combination, using the combi-
nation algorithm described in Section 2.4.3. The
respective results for the three component EnCh
systems were: 0.357, 0.360, and 0.363. The
EnJa result in the table refers the system described
in Section 2.4.2 that applied specific treatment
to Japanese Katakana. Based on our develop-
ment results, this specific treatment improves as
much as 2% top-1 accuracy over the language-
independent model. The EnHi system that in-
corporates language identification obtained ex-
actly the same top-1 accuracy as the language-
independent model. However, the EnKo system
with Jaso correction produced the top-1 accu-
Task top-1 F-score Rank
EnCh 0.363 0.707 2
ChEn 0.137 0.740 1
EnTh 0.378 0.866 2
ThEn 0.352 0.861 2
EnHi 0.456 0.884 1
EnTa 0.390 0.891 2
EnKa 0.341 0.867 2
EnJa 0.398 0.791 1
EnKo 0.554 0.770 1
JnJk 0.126 0.426 1
ArAe 0.464 0.924 1
EnBa 0.395 0.877 2
Table 1: Transliteration generation results
racy of 0.554, which is a significant improvement
over 0.387 achieved by the language-independent
model.
3 Transliteration mining
This section is structured as follows. In Sec-
tion 3.1, we describe the method of extracting
transliteration candidates that serves as the input
to the subsequently presented mining approaches.
Two techniques for generating negative exam-
ples are discussed in Section 3.2. Our language-
independent approaches to transliteration mining
are described in Section 3.3, and a technique for
mining English-Chinese pairs is proposed in Sec-
tion 3.4. In Section 3.5, we address the issue of
overlapping predictions. Finally, Section 3.6 and
Section 3.7 summarize our results.
3.1 Extracting transliteration candidates
We cast the transliteration mining task as a bi-
nary classification problem. That is, given a word
in the source language and a word in the target
language, a classifier predicts whether or not the
pair constitutes a valid transliteration. As a pre-
processing step, we extract candidate translitera-
tions from the pairs of Wikipedia titles. Word seg-
mentation is performed based on sequences of one
or more spaces and/or punctuation symbols, which
include hyphens, underscores, brackets, and sev-
eral other non-alphanumeric characters. Apostro-
phes and single quotes are not used for segmenta-
tion (and therefore remain in a given word); how-
ever, all single quote-like characters are converted
into a generic apostrophe. Once an English ti-
tle and its target language counterpart have been
42
segmented into words, we form the candidate set
for this title as the cross product of the two sets
of words after discarding any words that contain
fewer than two characters.
After the candidates have been extracted, indi-
vidual words are flagged for certain attributes that
may be used by our supervised learner as addi-
tional features. Alternatively, the flags may serve
as criteria for filtering the list of candidate pairs
prior to classification. We identify words that are
capitalized, consist of all lowercase (or all capital)
letters, and/or contain one or more digits. We also
attempt to encode each word in the target language
as an ASCII string, and flag that word if the opera-
tion succeeds. This can be used to filter out words
that are written in English on both the source and
target side, which are not transliterations by defi-
nition.
3.2 Generating negative training examples
The main issue with applying a supervised learn-
ing approach to the NEWS 2010 Shared Task is
that annotated task-specific data is not available
to train the system. However, the seed pairs do
provide example transliterations, and these can be
used as positive training examples. The remaining
issue is how to select the negative examples.
We adopt two approaches for selecting nega-
tives. First, we generate all possible source-target
pairs in the seed data, and take as negatives those
pairs which are not transliterations but have a
longest common subsequence ratio (LCSR) above
0.58; this mirrors the approach used by Bergsma
and Kondrak (2007). The method assumes that
the source and target words are written in the same
script (e.g., the foreign word has been romanized).
A second possibility is to generate all seed pair-
ings as above, but then randomly select negative
examples, thus mirroring the approach in Klemen-
tiev and Roth (2006). In this case, the source and
target scripts do not need to be the same. Com-
pared with the LCSR technique, random sampling
in this manner has the potential to produce nega-
tive examples that are very ?easy? (i.e., clearly not
transliterations), and which may be of limited util-
ity when training a classifier. On the other hand, at
test time, the set of candidates extracted from the
Wikipedia data will include pairs that have very
low LCSR scores; hence, it can be argued that dis-
similar pairs should also appear as negative exam-
ples in the training set.
3.3 Language-independent approaches
In this section, we describe methods for transliter-
ation mining that can, in principle, be applied to a
wide variety of language pairs without additional
modification. For the purposes of the Shared Task,
however, we convert all source (English) words to
ASCII by removing diacritics and making appro-
priate substitutions for foreign letters. This is done
to mitigate sparsity in the relatively small seed sets
when training our classifiers.
3.3.1 Alignment-derived romanization
We developed a simple method of performing ro-
manization of foreign scripts. Initially, the seed set
of transliterations is aligned using the one-to-one
option of the M2M-aligner approach (Jiampoja-
marn et al, 2007). We allow nulls on both the
source and target sides. The resulting alignment
model contains pairs of Latin letters and foreign
script symbols (graphemes) sorted by their con-
ditional probability. Then, for each grapheme,
we select a letter (or a null symbol) that has the
highest conditional probability. The process pro-
duces an approximate romanization table that can
be obtained without any knowledge of the target
script. This method of romanization was used by
all methods described in the remainder of Sec-
tion 3.3.
3.3.2 Normalized edit distance
Normalized edit distance (NED) is a measure of
the similarity of two strings. We define a uniform
edit cost for each of the three operations: substitu-
tion, insertion, and deletion. NED is computed by
dividing the minimum edit distance by the length
of the longer string, and subtracting the resulting
fraction from 1. Thus, the extreme values of NED
are 1 for identical strings, and 0 for strings that
have no characters in common.
Our baseline method, NED+ is simply the NED
measure augmented with filtering of the candidate
pairs described in Section 3.1. In order to address
the issue of morphological variants, we also fil-
ter out the pairs in which the English word ends
in a consonant and the foreign word ends with a
vowel. With no development set provided, we set
the similarity thresholds for individual languages
on the basis of the average word length in the seed
sets. The values were 0.38, 0.48, 0.52, and 0.58
for Hindi, Arabic, Tamil, and Russian, respec-
tively, with the last number taken from Bergsma
and Kondrak (2007).
43
3.3.3 Alignment-based string similarity
NED selects transliteration candidates when the
romanized foreign strings have high character
overlap with their English counterparts. The mea-
sure is independent of the language pair. This
is suboptimal for several reasons. First of all,
phonetically unrelated words can share many in-
cidental character matches. For example, the
French word ?recettes? and the English word
?proceeds? share the letters r,c,e,e,s as a com-
mon subsequence, but the words are phonetically
unrelated. Secondly, many reliable, recurrent,
language-specific substring matches are prevalent
in true transliterations. These pairings may or may
not involve matching characters. NED can not
learn or adapt to these language-specific patterns.
In light of these drawbacks, researchers have
proposed string similarity measures that can learn
from provided example pairs and adapt the simi-
larity function to a specific task (Ristad and Yiani-
los, 1998; Bilenko and Mooney, 2003; McCallum
et al, 2005; Klementiev and Roth, 2006).
One particularly successful approach is by
Bergsma and Kondrak (2007), who use discrim-
inative learning with an improved feature repre-
sentation. The features are substring pairs that are
consistent with a character-level alignment of the
two strings. This approach strongly improved per-
formance on cognate identification, while varia-
tions of it have also proven successful in transliter-
ation discovery (Goldwasser and Roth, 2008). We
therefore adopted this approach for the translitera-
tion mining task.
We produce negative training examples using
the LCSR threshold approach described in Sec-
tion 3.2. For features, we extract from the aligned
word pairs all substring pairs up to a maximum
length of three. We also append characters mark-
ing the beginning and end of words, as described
in Bergsma and Kondrak (2007). For our clas-
sifier, we use a Support Vector Machine (SVM)
training with the very efficient LIBLINEAR pack-
age (Fan et al, 2008). We optimize the SVM?s
regularization parameter using 10-fold cross vali-
dation on the generated training data. At test time,
we apply our classifier to all the transliteration
candidates extracted from the Wikipedia titles,
generating transliteration pairs whenever there is
a positive classification.
3.3.4 String kernel classifier
The alignment-based classifier described in the
preceding section is limited to using substring fea-
tures that are up to (roughly) three or four letters
in length, due to the combinatorial explosion in the
number of unique features as the substring length
increases. It is natural to ask whether longer sub-
strings can be utilized to learn a more accurate pre-
dictor.
This question inspired the development of a sec-
ond SVM-based learner that uses a string kernel,
and therefore does not have to explicitly repre-
sent feature vectors. Our kernel is a standard n-
gram (or spectrum) kernel that implicitly embeds
a string in a feature space that has one co-ordinate
for each unique n-gram (see, e.g., (Shawe-Taylor
and Cristianini, 2004)). Let us denote the alphabet
over input strings as A. Given two input strings x
and x?, this kernel function computes:
k(x, x?) =
?
s?An
#(s, x)#(s, x?)
where s is an n-gram and #(a, b) counts the num-
ber of times a appears as a substring of b.
An extension of the n-gram kernel that we em-
ploy here is to consider all n-grams of length
1 ? n ? k, and weight each n-gram as a func-
tion of its length. In particular, we specify a value
? and weight each n-gram by a factor of ?n. We
implemented this kernel in the LIBSVM software
package (Chang and Lin, 2001). Optimal values
for k, ?, and the SVM?s regularization parame-
ter were estimated for each dataset using 5-fold
cross-validation. The values of (k, ?) that we ul-
timately used were: EnAr (3, 0.8), EnHi (8, 0.8),
EnRu (5, 1.2), and EnTa (5, 1.0).
Our input string representation for a candidate
pair is formed by first aligning the source and tar-
get words using M2M-aligner (Jiampojamarn et
al., 2007). Specifically, an alignment model is
trained on the seed examples, which are subse-
quently aligned and used as positive training ex-
amples. We then generate 20K negative examples
by random sampling (cf. Section 3.2) and apply
the alignment model to this set. Not all of these
20K word pairs will necessarily be aligned; we
randomly select 10K of the successfully aligned
pairs to use as negative examples in the training
set.
Each aligned pair is converted into an ?align-
ment string? by placing the letters that appear in
44
Word pair zubtsov z u b ov
Aligned pair z|u|b|t|s|o|v| z|u|b|| |o|v|
Align?t string zz|uu|bb|t|s |oo|vv
Table 2: An example showing how an alignment
string (the input representation for the string ker-
nel) is created from a word pair.
the same position in the source and target next to
one another, while retaining the separator charac-
ters (see Table 2). We also appended beginning
and end of word markers. Note that no romaniza-
tion of the target words is necessary for this pro-
cedure.
At test time, we apply the alignment model to
the candidate word pairs that have been extracted
from the train data, and retain all the successfully
aligned pairs. Here, M2M-aligner also acts as a
filter, since we cannot form alignment strings from
unaligned pairs ? these yield negative predictions
by default. We also filter out pairs that met any of
the following conditions: 1) the English word con-
sists of all all capital or lowercase letters, 2) the
target word can be converted to ASCII (cf. Sec-
tion 3.1), or 3) either word contains a digit.
3.3.5 Generation-based approach
In the mining tasks, we are interested in whether a
candidate pair (x, y) is a transliteration pair. One
approach is to determine if the generated translit-
erations of a source word y? = ?(x) and a target
word x? = ?(y) are similar to the given candi-
date pair. We applied DIRECTL+ to the mining
tasks by training transliteration generation models
on the provided seed data in forward and back-
ward transliteration directions, creating ?(x) and
?(y) models. We now define a transliteration
score function in Eq. 1. N(x?, x) is the normal-
ized edit distance between string x? and x, and w1
and w2 are combination weights to favor forward
and backward transliteration models.
S(x, y) = w1 ? N(y?, y) + w2 ? N(x?, x)w1 + w2
(1)
A candidate pair is considered a transliteration
pair if its S(x, y) > ? . Ideally, we would like
to optimize these parameters, ?, w1, w2 based on
a development set for each language pair. Unfor-
tunately, no development sets were provided for
the Shared Task. Therefore, following Bergsma
and Kondrak (2007), we adopt the threshold of
? = 0.58. We experimented with three sets of val-
ues for w1 and w2: (1, 0), (0.5, 0.5), and (0, 1).
Our final predictions were made using w0 = 0
and w1 = 1, which appeared to produce the best
results. Thus, only the backward transliteration
model was ultimately employed.
3.4 English-Chinese string matching
Due to the fact that names transliterated into Chi-
nese consist of multiple Chinese characters and
that the Chinese text provided in this shared task
is not segmented, we have to adopt a different ap-
proach to the English-Chinese mining task (Unlike
many other languages, there are no clear bound-
aries between Chinese words). We first train a
generation model using the seed data and then ap-
ply a greedy string matching algorithm to extract
transliteration pairs.
The generation model is built using the discrim-
inative training framework described in (Jiampoja-
marn et al, 2008). Two models are learned: one
is trained using English and Chinese characters,
while the other is trained on English and Pinyin (a
standard phonetic representation of Chinese char-
acters). In order to mine transliteration pairs from
Wikipedia titles, we first use the generation model
to produce transliterations for each English token
on the source side as both Chinese characters and
Pinyin. The generated Chinese characters are ul-
timately converted to Pinyin during string match-
ing. We also convert all the Chinese characters on
the target side to their Pinyin representations when
performing string matching.
The transliteration pairs are then mined by com-
bining two different strategies. First of all, we ob-
serve that most of the titles that contain a separa-
tion symbol ? ? ? on the target side are translit-
erations. In this case, the number of tokens on
both sides is often equal. Therefore, the mining
task can be formulated as a matching problem.
We use a competitive linking approach (Melamed,
2000) to find the best match. First, we select
links between all possible pairs if similarity of
strings on both sides is above a threshold (0.6 ?
length(Pinyin)). We then greedily extract the
pairs with highest similarity until the number of
unextracted segments on either side becomes zero.
The problem becomes harder when there is no
indication of word segmentation for Chinese. In-
stead of trying to segment the Chinese characters
first, we use an incremental string matching strat-
45
egy. For each token on the source side, the algo-
rithm calculates its similarity with all possible n-
grams (2 ? n ? L) on the target side, where L
is the length of the Chinese title (i.e., the number
of characters). If the similarity score of n-gram
with the highest similarity surpasses a threshold
(0.5 ? length(Pinyin)), the n-gram sequence is
proposed as a possible transliteration for the cur-
rent source token.
3.5 Resolving overlapping predictions
Given a set of candidate word pairs that have been
extracted from a given Wikipedia title according to
the procedure described in Section 3.1, our clas-
sifiers predict a class label for each pair inde-
pendently of the others. Pairs that receive neg-
ative predictions are discarded immediately and
are never reported as mined pairs. However, it
is sometimes necessary to arbitrate between pos-
itive predictions, since it is possible for a classifier
to mark as transliterations two or more pairs that
involve the same English word or the same target
word in the title. Clearly, mining multiple overlap-
ping pairs will lower the system?s precision, since
there is (presumably) at most one correct translit-
eration in the target language version of the title
for each English word.3
Our solution is to apply a greedy algorithm that
sorts the word pair predictions for a given title
in descending order according to the scores that
were assigned by the classifier. We make one pass
through the sorted list and report a pair of words as
a mined pair unless the English word or the target
language word has already been reported (for this
particular title).4
3.6 Results
In the context of the NEWS 2010 Shared Task
on Transliteration Generation we tested our sys-
tem on all five data sets: from English to Rus-
sian (EnRu), Hindi (EnHi), Tamil (EnTa), Arabic
(EnAr), and Chinese (EnCh). The EnCh set dif-
fers from the remaining sets in the lack of transpar-
ent word segmentation on the Chinese side. There
were no development sets provided for any of the
language pairs.
3On the other hand, mining all such pairs might improve
recall.
4A bug was later discovered in our implementation of this
algorithm, which had failed to add the words in a title?s first
mined pair to the ?already reported? list. This sometimes
caused up to two additional mined pairs per title to be re-
ported in the prediction files that were submitted.
Task System F P R
EnRu NED+ .875 .880 .869
BK-2007 .778 .684 .902
StringKernel* .811 .746 .889
DIRECTL+ .786 .778 .795
EnHi NED+ .907 .875 .941
BK-2007 .882 .883 .880
StringKernel .924 .954 .895
DIRECTL+ .904 .945 .866
EnTa NED+ .791 .916 .696
BK-2007 .829 .808 .852
StringKernel .914 .923 .906
DIRECTL+ .801 .919 .710
EnAr NED+ .800 .818 .783
BK-2007 .816 .834 .798
StringKernel* .827 .917 .753
DIRECTL+ .742 .861 .652
EnCh GreedyMatch .530 .698 .427
DIRECTL+ .009 .045 .005
Table 3: Transliteration mining results. An aster-
isk (*) indicates an unofficial result.
Table 3 shows the results obtained by our var-
ious systems on the final test sets, measured in
terms of F-score (F), precision (P), and recall
(R). The systems referred to as NED+, BK-2007,
StringKernel, DIRECTL+, and GreedyMatch are
described in Section 3.3.2, Section 3.3.3, Sec-
tion 3.3.4, Section 3.3.5, and Section 3.4 respec-
tively. The runs marked with an asterisk (*)
were produced after the Shared Task deadline, and
therefore are not included in the official results.
3.7 Discussion
No fixed ranking of the four approaches emerges
across the four alphabetic language pairs (all ex-
cept EnCh). However, StringKernel appears to be
the most robust, achieving the highest F-score on
three language pairs. This suggests that longer
substring features are indeed useful for classifying
candidate transliteration pairs. The simple NED+
method is a clear winner on EnRu, and obtains de-
cent scores on the remaining alphabetic language
pairs. The generation-based DIRECTL+ approach
ranks no higher than third on any language pair,
and it fails spectacularly on EnCh because of the
word segmentation ambiguity.
Finally, we observe that there are a number of
cases where the results for our discriminatively
trained classifiers, BK-2007 and StringKernel, are
46
not significantly better than those of the simple
NED+ approach. We conjecture that automatically
generating training examples is suboptimal for this
task. A more effective strategy may be to filter all
possible word pairs in the seed data to only those
with NED above a fixed threshold. We would then
apply the same threshold to the Wikipedia candi-
dates, only passing to the classifier those pairs that
surpass the threshold. This would enable a better
match between the training and test operation of
the system.
4 Conclusion
The results obtained in the context of the NEWS
2010 Machine Transliteration Shared Task con-
firm the effectiveness of our discriminative ap-
proaches to transliteration generation and mining.
Acknowledgments
This research was supported by the Alberta Inge-
nuity Fund, Informatics Circle of Research Excel-
lence (iCORE), and the Natural Sciences and En-
gineering Research Council of Canada (NSERC).
References
Shane Bergsma and Grzegorz Kondrak. 2007.
Alignment-based discriminative string similarity. In
Proc. ACL.
Aditya Bhargava and Grzegorz Kondrak. 2010. Lan-
guage identification of names with SVMs. In Proc.
NAACL-HLT.
Mikhail Bilenko and Raymond J. Mooney. 2003.
Adaptive duplicate detection using learnable string
similarity measures. In Proc. KDD.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.ntu.
edu.tw/?cjlin/libsvm.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLIN-
EAR: A library for large linear classification. JMLR,
9:1871?1874.
Dan Goldwasser and Dan Roth. 2008. Transliteration
as constrained optimization. In Proc. EMNLP.
Sittichai Jiampojamarn, Grzegorz Kondrak, and Tarek
Sherif. 2007. Applying many-to-many alignments
and Hidden Markov Models to letter-to-phoneme
conversion. In Proc. HLT-NAACL.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In Proc.
ACL.
Sittichai Jiampojamarn, Aditya Bhargava, Qing Dou,
Kenneth Dwyer, and Grzegorz Kondrak. 2009. Di-
recTL: a language-independent approach to translit-
eration. In NEWS ?09: Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration, pages 28?31.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2010. Integrating joint n-gram features
into a discriminative training framework. In Proc.
NAACL-HLT.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In Proc. HLT-NAACL.
Grzegorz Kondrak. 2000. A new algorithm for the
alignment of phonetic sequences. In Proc. NAACL,
pages 288?295.
Andrew McCallum, Kedar Bellare, and Fernando
Pereira. 2005. A conditional random field for
discriminatively-trained finite-state string edit dis-
tance. In Proc. UAI.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221?249.
Eric Sven Ristad and Peter N. Yianilos. 1998. Learn-
ing string-edit distance. IEEE Trans. Pattern Analy-
sis and Machine Intelligence, 20(5).
John Shawe-Taylor and Nello Cristianini. 2004. Ker-
nel Methods for Pattern Analysis. Cambridge Uni-
versity Press.
47
