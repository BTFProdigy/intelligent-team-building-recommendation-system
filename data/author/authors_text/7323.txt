Detecting Structural Metadata with Decision Trees and
Transformation-Based Learning
Joungbum Kim? and Sarah E. Schwarm? and Mari Ostendorf?
?Dept. of Electrical Engineering ?Dept. of Computer Science
University of Washington
Seattle, WA 98195. USA
{bummie,sarahs,mo}@ssli.ee.washington.edu
Abstract
The regular occurrence of disfluencies is a
distinguishing characteristic of spontaneous
speech. Detecting and removing such disflu-
encies can substantially improve the usefulness
of spontaneous speech transcripts. This pa-
per presents a system that detects various types
of disfluencies and other structural information
with cues obtained from lexical and prosodic
information sources. Specifically, combina-
tions of decision trees and language models are
used to predict sentence ends and interruption
points and, given these events, transformation-
based learning is used to detect edit disfluen-
cies and conversational fillers. Results are re-
ported on human and automatic transcripts of
conversational telephone speech.
1 Introduction
Automatic speech-to-text (STT) transcripts of sponta-
neous speech are often difficult to comprehend even with-
out the challenges arising from word recognition errors
introduced by imperfect STT systems (Jones et al, 2003).
Such transcripts lack punctuation that indicates clausal or
sentential boundaries, and they contain a number of dis-
fluencies that would not normally occur in written lan-
guage. Repeated words, hesitations such as ?um? and
?uh?, and corrections to a sentence in mid-stream are
a normal part of conversational speech. These disflu-
encies are handled easily by human listeners (Shriberg,
1994), but their existence makes transcripts of sponta-
neous speech ill-suited for most natural language pro-
cessing (NLP) systems developed for text, such as parsers
or information extraction systems. Similarly, the lack
of meaningful segmentation in automatically generated
speech transcripts makes them problematic to use in NLP
systems, most of which are designed to work at the sen-
tence level. Detecting and removing disfluencies and lo-
cating sentential unit boundaries in spontaneous speech
transcripts can improve their readability and make them
more suitable for NLP. Automatically annotating dis-
course markers and other conversational fillers is also
likely to be useful, since proper handling is needed to fol-
low the flow of conversation. Hence, the overall goal of
our work is to detect such structural information in con-
versational speech using features generated by currently
available speech processing systems and statistical ma-
chine learning tools.
This paper is organized as follows. In Section 2, we
describe the types of metadata that this work addresses,
followed by a discussion of related prior work in Sec-
tion 3. Section 4 describes the system architecture and
details the algorithms and features used by our system.
Section 5 discusses the experimental paradigm and re-
sults. Finally we provide a summary and directions for
future work in Section 6.
2 Structural Metadata
We consider three main types of structural metadata:
sentence-like units, conversational fillers and edit disflu-
encies. These structures were chosen primarily because
of the availability of annotated conversational speech data
from the Linguistic Data Consortium (Strassel, 2003) and
standard scoring tools (NIST, 2003).
2.1 Sentence Units
Conversational speech lacks the clear sentence bound-
aries of written text. Instead, we detect SUs (variously
referred to as sentence, semantic, and slash units), which
are linguistic units maximally equivalent to sentences
that are used to mark segmentation boundaries in con-
versational speech where utterances often end without
forming ?grammatical? sentences in the sense expected
in written text. SUs can be sub-categorized according
to their discourse role. In our data, annotations distin-
guish statement, question, backchannel, incomplete SU
and SU-internal clause boundaries. Here, we ignore the
SU-internal boundaries, and merge all but the incomplete
SU categories in characterizing SU events.
Table 1: Filled pauses and discourse markers to be de-
tected by our system.
Filled Pauses ah, eh, er, uh, um
Discourse Markers actually, anyway, basically, I
mean, let?s see, like, now, see,
so, well, you know, you see
Table 2: Examples of edit disfluencies.
Disfluency Example
Repetition (I was) + I was very interested...
(I was) + { uh} I was very interested...
Repair (I was) + she was very interested...
(I was) + { I mean } she was very...
Restart (I was very) + Did you hear the news?
2.2 Conversational Fillers
Conversational fillers include filled pauses (hesitation
sounds such as ?uh?, ?um? and ?er?), discourse mark-
ers (e.g. ?well?, ?you know?), and explicit editing terms.
Defining an all-inclusive set of English filled pauses and
discourse markers is a problematic task. Our system de-
tects only a limited set of filled pauses and discourse
markers, listed in Table 1, which cover a large majority of
cases (Strassel, 2003). An explicit editing term is a filler
occurring within an edit disfluency, described further be-
low. For example, the discourse marker I mean serves as
an explicit editing term in the following edit disfluency:
?I didn?t tell her that, I mean, I couldn?t tell her that he
was already gone.?
2.3 Edit Disfluencies
Edit disfluencies largely encompass three separate phe-
nomena: repetition, repair and restart (Shriberg, 1994).
A repetition occurs when a speaker repeats the most re-
cently spoken portion of an utterance to hold off the flow
of speech. A repair happens when the speaker attempts
to correct a mistake that he or she just made. Finally, in
a restart, the speaker abandons a current utterance com-
pletely and starts a new one.
Previous studies characterize edit disfluencies using
a structure with different segments (Shriberg, 1994;
Nakatani and Hirschberg, 1994). The first part of this
structure is called the reparandum, a string of words that
gets repeated or corrected. The reparandum is immedi-
ately followed by a non-lexical boundary event termed
the interruption point (IP). The IP marks the point where
the speaker interrupts a fluent utterance. Optionally, there
may be a filled pause or explicit editing term. The final
part of the edit disfluency structure is called the alter-
ation, which is a repetition or revised copy of the reparan-
dum. In the case of a restart, the alteration is empty. In
Table 2, reparanda are enclosed in parentheses, IPs are
represented by ?+?, optional fillers are in braces, and al-
terations are in boldface.
Annotation of complex edit disfluencies, where a dis-
fluency occurs within an alteration, can be difficult. The
data used here is annotated with a flattened structure
that treats these cases as simple disfluencies with mul-
tiple IPs (Strassel, 2003). IPs within a complex disflu-
ency are detected separately, and contiguous sequences
of edit words associated with these IPs are referred to as
a deletable region.
3 Previous Work
In an early study on automatic disfluency detection a
deterministic parser and correction rules were used to
clean up edit disfluencies (Hindle, 1983). However theirs
was not a truly automatic system as it relied on hand-
annotated ?edit signals? to locate IPs. Bear et al (1992)
explored pattern matching, parsing and acoustic cues and
concluded that multiple sources of information would be
needed to detect edit disfluencies. A decision-tree-based
system that took advantage of various acoustic and lexi-
cal features to detect IPs was developed in (Nakatani and
Hirschberg, 1994).
Shriberg et al (1997) applied machine prediction of
IPs with decision trees to the broader Switchboard corpus
by generating decision trees with a variety of prosodic
features. Stolcke et al (1998) then expanded the prosodic
tree model with a hidden event language model (LM)
to identify sentence boundaries, filled pauses and IPs in
different types of edit disfluencies. The hidden event
LM used in their work adapted Hidden Markov Model
(HMM) algorithms to an n-gram LM paradigm to repre-
sent non-lexical events such as IPs and sentence bound-
aries as hidden states. Liu et al (2003) built on this
framework and extended prosodic features and the hidden
event LM to predict edit IPs on both human transcripts
and STT system output. Their system also detected the
onset of the reparandum by employing rule-based pattern
matching once edit IPs have been detected.
Edit disfluency detection systems that rely exclusively
on word-based information have been presented by Hee-
man et al (Heeman et al, 1996) and Charniak and John-
son (Charniak and Johnson, 2001). Common to both of
these approaches is a focus on repeated or similar se-
quences of words and information about the words them-
selves and the length and similarity of the sequences.
Our approach is most similar to (Liu et al, 2003), since
we also detect boundary events such as IPs first and use
them as ?signals? when identifying the reparandum in
a later stage. The motivation to detect IPs first is that
Speech IP/SUPredictionProsodic and LexicalFeature Extraction
Word Boundary
Event Prediction
(DT/HE-LM)
Filler/Edit
Word Detection
(TBL)
Output
Figure 1: System Diagram
speech before an IP is fluent and is likely to be free of
any prosodic or lexical irregularities that can indicate the
occurrence of an edit disfluency. Like Liu et al, we use a
decision tree trained with prosodic features and a hidden
event language model for the IP detection task. However,
we incorporate SU detection in those models as well. We
use part-of-speech (POS) tags and pattern match features
in decision tree training whereas Liu et al (2003) devel-
oped language models for them. We explore three dif-
ferent methods of combining the hidden event language
model and the decision tree model, namely linear inter-
polation, joint tree-based modeling and an HMM-based
approach. Moreover, our system uses the transformation-
based learning algorithm rather than hand-crafted rules
for the second stage of edit region detection.
Another key difference between our system and most
previous work is the prediction target. Our system incor-
porates detecting word boundary events such as SUs and
IPs, locating onsets of edit regions, and identifying filled
pauses, discourse markers and explicit editing terms. We
believe that such a comprehensive detection scheme al-
lows our system to better model dependencies between
these events, which will lead to an improvement in the
overall detection performance.
4 System Description
4.1 Overall Architecture
As shown in Figure 1, our system detects disfluencies
in a two-step process. First, for each word boundary in
the given transcription, a decision tree predicts one of the
four boundary events IP, SU, ISU (incomplete SU), and
the null event. Then in the second stage, rules learned
via the transformation-based learning (TBL) algorithm
are applied to the data containing predicted boundary
events and other lexical information to identify edits and
fillers. Following edit region and filler prediction, the sys-
tem output was post-processed to eliminate edit region
predictions not associated with IP predictions as well as
IP predictions for which no edit region or filler was de-
tected. An analysis of post-processing alternatives con-
firmed that this strategy reduced insertion errors.
4.2 Detecting Boundary Events
In order to detect boundary events, we trained a CART-
style decision tree (Breiman et al, 1984) with various
prosodic and lexical features. Decision trees are well-
suited for this task because they provide a convenient way
to integrate both symbolic and numerical features in pre-
diction. Furthermore, a trained decision tree is highly ex-
plainable by its nature, which allows us to gain additional
insight into the utilities of and the interactions between
multiple information sources.
Prosodic features generated for decision tree training
included the following:
? Word and rhyme1 durations.
? Rhyme duration differences between two neighbor-
ing words.
? F0 statistics (minimum, mean, maximum, slope)
over a word.
? Differences in F0 statistics between two neighboring
words.
? Energy statistics over a word and its rhyme.
? Silence duration following a word.
? A flag indicating start and end of a speaker turn and
speaker overlap.
? Ordinal position of a word in a turn.
Energy and F0 features were generated with the Entropic
System ESPS/Waves package and the F0 stylization tool
developed in (So?nmez et al, 1998). Word and rhyme
duration were normalized by phone duration statistics
(mean and variance) calculated over all available training
data. F0 and energy features were normalized for each
individual speaker?s baseline. A turn boundary was hy-
pothesized for word boundaries with silences longer than
four seconds.
Since inclusion of features that do not contribute to
the classification of data can degrade the performance of
a decision tree, we selected only the prosodic features
whose exclusion from the training process led to a de-
crease in boundary event detection accuracy on the de-
velopment data by utilizing the leave-one-out method.
Lexical features consisted of POS tag groups, word and
POS tag pattern matches, and a flag indicating existence
1In our work, a rhyme was defined to contain the final vowel
of a word and any consonants following the final vowel.
of filler words to the right of the current word bound-
ary. The POS tag features were produced by first predict-
ing the tags with Ratnaparkhi?s Maximum Entropy Tag-
ger (Ratnaparkhi, 1996) and then clustered by hand into
a smaller number of groups based on their syntactic role.
The clustering was performed to speed up decision tree
training as well as to reduce the impact of tagger errors.
Word pattern match features were generated by com-
paring words over the range of up to four words across the
word boundary in consideration. Grouped POS tags were
compared in a similar way, but the range was limited to
at most two tags across the boundary since a wider com-
parison range would have resulted in far more matches
than would be useful due to the low number of available
POS tag groups. When words known to be identified fre-
quently as fillers existed after the boundary, they were
skipped and the range of pattern matching was extended
accordingly.
Another useful cue for boundary event detection is the
existence of word fragments. Since word fragments occur
when the speaker cuts short the word being spoken, they
are highly indicative of IPs. However currently available
STT systems do not recognize word fragments. As our
goal is to build an automatic detection system, our sys-
tem was not designed to use any features related to word
fragments. However, for a control case, we conducted
an experiment with reference transcripts using a single
?frag? word token to show the potential for improved per-
formance of a system capable of recognizing fragments.
In addition to the decision tree model, we also em-
ployed a hidden event language model to predict bound-
ary events. A hidden event LM is the same as a typical
n-gram LM except that it models non-lexical events in
the n-gram context by counting special non-word tokens
representing such events. The hidden event LM estimates
the joint distribution P (W,E) of words W and events E.
Once the model has been trained, a forward-backward al-
gorithm can be used to calculate P (E|W ), or the poste-
rior probability of an event given the preceding word se-
quence (Stolcke et al, 1998; Stolcke and Shriberg, 1996).
The SRI Language Modeling Toolkit (SRILM) (Stolcke,
2002) was used to train a trigram open-vocabulary lan-
guage model with Kneser-Ney discounting (Kneser and
Ney, 1995) on data that had boundary events (SU, ISU,
and IP) inserted in the word stream. Posterior probabil-
ities of boundary events for every word boundary were
then estimated with SRILM?s capability for computing
hidden event posteriors.
While the hidden event LM alone can be used to de-
tect boundary events, prior work has shown that it ben-
efits from also using prosodic cues, so we combined the
language model and the decision tree model in three dif-
ferent ways. In the first approach, which we call the joint
tree model, the boundary event posterior probability from
the hidden event LM is jointly modeled with other fea-
tures in the decision tree to make predictions about the
boundary events. In the second approach, referred to as
the linearly interpolated model, a decision is made based
on the combined posterior probability
?Ptree(E|A,W ) + (1 ? ?)PLM (E|W ),
where A corresponds to the acoustic-prosodic features
and the weighting factor ? can be chosen empirically to
maximize target performance, i.e. bias the prediction to-
ward the more accurate model. In the third approach,
the decision tree features, words and boundary events
are jointly modeled via an integrated HMM (Shriberg
et al, 2000). This approach augments the hidden event
LM by modeling decision tree features as emissions from
the HMM states represented by the word and boundary
event. Under this framework, the forward-backward al-
gorithm can again be used to determine posterior prob-
abilities of boundary events. Similar to the linearly in-
terpolated model, a weighting factor can be used to intro-
duce the desired bias to the combination model. The joint
tree model has the advantage that the (possibly) complex
interaction between lexical and prosodic cues can be cap-
tured. However, since the tree is trained on reference tran-
scriptions, it favors lexical cues, which are less reliable in
STT output. In the linearly interpolated and joint HMM
approaches, the relative weighting of the two knowledge
sources is estimated on the development test set for STT
output, so it is possible for prosodic cues to be given a
higher weight.
4.3 Edit and Filler Detection
After SUs and IPs have been marked, we use
transformation-based learning (TBL) to learn rules to
detect edit disfluencies and conversational fillers. TBL
is an automatic rule learning technique that has been
successfully applied to a variety of problems in natu-
ral language processing, including part-of-speech tag-
ging (Brill, 1995), spelling correction (Mangu and Brill,
1997), error correction in automatic speech recogni-
tion (Mangu and Padmanabhan, 2001), and named entity
detection (Kim and Woodland, 2000). We selected TBL
for our tagging-like metadata detection task since it has
been used successfully for these other tagging tasks.
TBL is an iterative technique for inducing rules from
training data. A TBL system consists of a baseline pre-
dictor, a set of rule templates, and an objective function
for scoring potential rules. After tagging the training data
using the baseline predictor, the system learns a list of
rules to correct errors in these predictions. At each iter-
ation, the system uses the rule templates to generate all
possible rules that correct at least one error in the training
data and selects the best rule according to the objective
function, commonly token error rate. The best rule is
Table 3: Example word and POS matches for TBL.
Word Match that IP that
POS Match the dog IP the cat
recorded and applied to the training data in preparation
for the next iteration. The standard stopping criterion for
rule learning is to stop when the score of the best rule falls
below a threshold value; statistical significance measures
have also been used (Mangu and Padmanabhan, 2001).
To tag new data, the rules are applied in the order in which
they were learned. This allows rules which are learned
later in the process to fine tune the effects of the earlier
rules. TBL produces concise, comprehensible rules, and
uses the entire corpus to train all of the rules. We used
Florian and Ngai?s Fast TBL system (fnTBL) (Ngai and
Florian, 2001) to train rules using disfluency annotated
conversational speech data.
The input to our TBL system consists of text divided
into utterances, with IPs and SUs inserted as if they were
extra words. (For simplicity, these special words are also
assigned ?IP? and ?SU? as part of speech tags.)
Our TBL system used the following types of features:
? Identity of the word.
? Part of speech (POS) and grouped part of speech
(GPOS) of the word (same as the decision tree).
? Is the word commonly used as: filled pause (FP),
backchannel (BC), explicit editing term (EET), dis-
course marker (DM)?
? Does this word/ POS/ GPOS match the word/ POS/
GPOS that is 1/2/3 positions to its right?
? Is this word at the beginning of a turn or utterance?
? Tag to be learned.
The ?tag? feature is the one we want the system to
learn. It is also used in templates that consider features
of neighboring words. The baseline predictor sets the tag
to its most common value, ?no disfluency,? for all words.
Other values of the tag are the three types of fillers (FP,
EET, DM) and edit. The objective function for our learner
is token error rate, and rule learning is stopped at a thresh-
old score of 5.
We generated a set of rule templates using these fea-
tures. The rule templates account for individual features
of the current word and/or its neighbors, the proximity
of potential FP/EET/DM terms, and matches between the
current word and nearby words, especially when in close
proximity to a boundary event or potential filler. Example
word and POS matches are shown in Table 3.
5 Experiments
5.1 Experimental Setup
For training our system and its components, we used two
different subsets of Switchboard, a corpus of conversa-
tional telephone speech (CTS) (Godfrey et al, 1992).
One of the data sets included 417 conversations (LDC1.3)
that were hand-annotated by the Linguistic Data Consor-
tium for disfluencies and SUs according to the V5 guide-
lines detailed in (Strassel, 2003). Another set of 1086
conversations from the Switchboard corpus was anno-
tated according to (Meteer et al, 1995) and is available as
part of the Treebank3 corpus (TB3). We used a version
of this set that contained annotations machine-mapped to
approximate the V5 annotation specification.
For development and testing of our system, we used
hand transcripts and STT system output for 72 conversa-
tions from Switchboard and the Fisher corpus, a recent
CTS data collection. Half of these conversations were
held out and used as development data (dev set), and the
other 36 conversations were used as test data (eval set).
The STT output, used only in testing, was from a state-of-
the-art large vocabulary conversational speech recognizer
developed by BBN. The word error rates for the STT out-
put were 27% on the dev set and 25% on the eval set.
To assess the performance of our overall system, dis-
fluencies and boundary events were predicted and then
evaluated by the scoring tools developed for the NIST
Rich Transcript evaluation task.
5.2 Boundary Event Prediction
Decision trees to predict boundary events were trained
and tested using the IND system developed by
NASA (Buntine and Caruan, 1991). All decision trees
were pruned by ten-fold cross validation. The LDC1.3
set2 with reference transcriptions was used to train the
trees3 and the dev set was used to evaluate their perfor-
mances.
Several decision trees with different combinations of
feature groups were trained to assess the usefulness of
different knowledge sources for boundary event detec-
tion. The tree was then used to predict the boundary
events on the reference transcription of the dev set. The
results are presented in Table 4. The inclusion of a spe-
cial token for fragments resulted in improved precision
and recall for SUs and IPs but, surprisingly, degraded per-
formance for ISUs. These results show that prosodic fea-
tures by themselves failed to detect ISUs and IPs, though
2Experiments combining the LDC1.3 set with the mapped
TB3 set were not as successful as LDC1.3 set alne for decision
tree training.
3While it might be better to train from automatic transcripts,
it is difficult to define target class labels in cases where there are
insertion errors or a sequence of several word errors.
Table 4: Impact of different features on boundary event prediction using the joint tree model on reference transcripts.
Features SU ISU IP
Recall Precision Recall Precision Recall Precision
Prosody Only 46.5 74.6 0 - 8.8 47.2
POS, Pattern, LM 77.3 79.6 30.0 53.3 64.4 77.4
Prosody, POS, Pattern, LM 81.5 80.4 36.5 69.7 66.1 78.7
All Above + Fragments 81.1 81.6 20.1 60.7 80.7 80.4
they lead to performance gains when combined with lex-
ical cues. Examination of the decision tree trained with
only the prosodic features revealed that pause duration
and turn information features were placed near the top of
the tree.
Use of lexical features brought substantial perfor-
mance improvement in all aspects, and classification ac-
curacy increased when features extracted from different
knowledge sources were combined. However, we ob-
served that a smaller number of prosodic features ended
up being used in the tree and they were placed at or near
leaf nodes as more lexical features were made available
for training. The importance of prosodic features is likely
to be much more apparent for STT data. The word errors
prevalent in the STT transcriptions will affect lexical fea-
tures far more severely than prosodic features, and there-
fore the prosodic features contribute to the robustness of
the overall system when lexical features become less re-
liable.
5.3 Edit and Filler Detection
After the prediction of boundary events, the rules learned
by the TBL system described in section 4.3 were applied
to detect fillers and edit regions. As with the decision
trees, we trained rules using the LDC1.3 data alone, and
combined with the mapped TB3 data, finding that the
combined dataset gave better results for TBL training.
Again we used only reference word transcripts but dis-
covered that training with SUs and IPs predicted by the
first stage of our system was more effective than using
reference boundary events.
It is difficult to formally assess the effectiveness of the
TBL module independently, and results for the entire sys-
tem are discussed in detail in the next section. Informal
inspection of the rules learned by the TBL system indi-
cates that, not surprisingly, word match features and the
presence of IPs are very important for the detection of
edit regions. The most commonly used features for iden-
tifying discourse markers are the identity or POS of the
current and/or neighboring words and the tag already as-
signed to neighboring words.
Table 5: Detection of boundary events and disfluencies
on STT output as scored by rt-eval.
Task % Corr % Del % Ins % SER
Filler 63.9 36.1 14.0 50.1
Edit 25.5 74.5 13.7 88.2
IP 49.6 50.5 16.3 66.8
SU 73.1 26.9 19.7 46.6
5.4 Overall System Results
The performance of our system was evaluated on the fall
2003 NIST Rich Transcription Evaluation test set (RT-
03F) using the rt-eval scoring tool (NIST, 2003), which
combines ISUs and SUs in a single category, and reports
results for detection of SUs, IPs, fillers, and edits with-
out differentiating subcategories of fillers and edits. This
tool produces a collection of results, including percentage
correct, deletions, insertions, and Slot Error Rate (SER),
similar to the word error rate measure used in speech
recognition. SER is defined as the number of insertions
and deletions divided by the number of reference items.
Note that scores are somewhat different from those in
Table 4, because of differences in scoring and metadata
alignment methods.
Figure 2: Detection of boundary events and disfluencies
on reference and STT transcripts (joint tree model).
Results of our system on the RT-03F task are shown in
Table 6: Percentage of missed IPs on the dev set.
Transcription % IPs after
fragments
% Other edit
IPs
Reference 81.7 37.6
STT 74.0 51.2
Table 5 for the joint tree version of the system as applied
to the STT transcription of the test data. SU detection
by our system is relatively good. IP detection is not as
successful, which also impacts edit detection.
Figure 2 contrasts the results of the joint tree model for
STT output with those obtained on reference data with
and without fragments. As expected, all error rates are
higher on STT output; IPs and fillers take the biggest hit.
Filler performance in particular seems to be affected by
recognition errors, which is not surprising, since misrec-
ognized words would likely not be on the target lists of
filled pauses and discourse markers. In particular, nearly
all missed and incorrectly inserted filled pauses are due
to recognition errors. Detection of discourse markers is
more challenging; fewer than half the errors on discourse
markers are due to recognition errors. Most non-STT-
related filler errors involved the words ?so? and ?like?
used as DMs, which are hard problems since the vast ma-
jority of the occurrences of these two words are not DMs.
It is also not surprising that improved IP detection on ref-
erence data contributes to a lower error rate for edits.
As expected, the inclusion of fragments improves per-
formance on IP and edit detection, where fragments fre-
quently occur. In LDC1.3, 17.2% of edit IPs have word
fragments occurring before them; 9.9% of edits consist
of just a single fragment. In the dev set, 35.5% of edit
IPs are associated with fragments. However, fragments
are rarely output by the STT system, so for most of our
work we chose to use the identical system for processing
reference and STT transcripts and did not include frag-
ments. IP detection performance was significantly worse
for those IPs associated with fragments, as shown in Ta-
ble 6. However, since fragments are often deleted or rec-
ognized as a full word, STT output actually ?helps? with
detection of IPs after fragments, apparently because the
POS tagger and hidden event LM tend to give unreliable
results on the reference transcripts near fragments.
Figure 3 compares the eval test set performances of the
different alternatives for incorporating the hidden event
LM posterior, i.e. inclusion in the decision tree, linear
interpolation and the joint HMM. For this experiment,
the interpolation weighting factor was selected empiri-
cally to maximize boundary event prediction accuracy on
the STT transcription of the dev set. The results of this
comparison are mixed: SU detection is better with the
joint tree model, but IP detection and consequently edit
Figure 3: Results for joint tree (JTM), linearly interpo-
lated (LIM) and joint HMM models on STT transcripts.
detection are better with the interpolation and HMM ap-
proaches. The degradation of SU detection performance
with the HMM is counter to findings in previous work
(Stolcke et al, 1998; Shriberg et al, 2000). This may
be due to differences in evaluation criteria, given that
the HMM approach typically had higher precision which
might benefit earlier word-based measures more. In addi-
tion, the difference in conclusions may be due to the fact
that the decision trees used here include lexical pattern
match features in addition to hidden event posteriors.
A problem in our system is the inability to predict more
than one label for a given word or boundary. Words la-
beled as both filler and edit account for only 0.5% of all
fillers and edits in the LDC1.3 training data, so it is prob-
ably not a significant problem. We also do not predict
boundaries as both SU and IP. In LDC1.3, these account
for 12.8% of SU boundaries, and are treated as simply SU
in training. This does not affect IPs for edits, but impacts
38.6% of IPs before fillers. By predicting a combined
SU-IP boundary in addition to isolated SUs and IPs, we
obtain a small reduction in SER for IPs but at the expense
of an increase in SU SER. However, separating prediction
of IPs after edit regions vs. before fillers also yields small
improvements in edit region precision and filler recall, re-
sulting in 3.3% and 0.8% relative reduction in filler and
edit SERs respectively for the joint HMM.
6 Conclusions
We have demonstrated a two-tiered system that detects
various types of disfluencies in spontaneous speech. In
the first tier, a decision tree model utilizes multiple
knowledge sources to predict interword boundary events.
Then the system employs a transformation-based learn-
ing algorithm to identify the extent and type of disflu-
encies. Experimental results show that the large vari-
ance and noise inherent in prosodic features makes them
much less effective than lexical features for reference
data; however, in the presence of word recognition errors
prevalent in automatic transcripts of spontaneous speech,
prosodic features have more value. Performance differ-
ences for the various score combination methods were
small, but combining decision tree and HE-LM scores
with a weight optimized on dev data is slightly better for
edit disfluencies. Transformation-based learning is an ef-
fective way to tag fillers and edit regions after boundary
events are tagged, but the best performance is obtained
when training with automatically predicted SU and IP
boundary events.
As this is a new task, error rates are relatively high
(though significantly better than chance), but this ap-
proach achieved competitive results on the Fall 2003
NIST Rich Transcription Evaluation, and there are many
directions for future improvements.
Acknowledgments
This work was supported by DARPA, no. MDA904-02-C-0437,
in a project led by BBN. The authors thank their colleagues at
BBN for providing recognizer output for the training and test
data, and colleagues at SRI for providing F0 conditioning tools
and mapped TB3 data. Any opinions, conclusions or recom-
mendations expressed in this material are those of the authors
and do not necessarily reflect the views of the sponsor or our
collaborators.
References
J. Bear et al, ?Integrating multiple knowledge sources for de-
tection and correction of repairs in human-computer dialog?,
Meeting of the ACL, pp. 56?63, 1992.
L. Breiman et al, Classification and Regression Trees Chap-
man and Hall, 1984.
E. Brill, ?Transformation-based error-driven learning and natu-
ral language: a case study in part of speech tagging?, Com-
putational Linguistics, 21(4), pp. 543?565, 1995.
W. Buntine and R. Caruan ?Introduction to IND and recursive
partitioning?, NASA Ames Research Center, TR. FIA-91-
28, 1991.
E. Charniak and M. Johnson, ?Edit detection and parsing for
transcribed speech?, Proc. NAACL, pp. 118?126, 2001.
J. J. Godfrey et al, ?SWITCHBOARD: Telephone speech cor-
pus for research and development?, Proc. ICASSP, v. I, pp.
517?520, 1992.
P. A. Heeman et al, ?Combining the detection and correction
of speech repairs?, Proc. ICSLP, v. 1, pp. 362?365, 1996.
C. Hemphill et al, ?The ATIS spoken language systems pi-
lot corpus?, Proc. of DARPA Speech and Natural Language
Workshop, pp. 96?101, 1990.
D. Hindle ?Deterministic parsing of syntactic nonfluencies?,
Meeting of the ACL, pp. 123?128, 1983.
D. Jones et al, ?Measuring the readability of automatic speech-
to-text transcripts?, Proc. Eurospeech, pp. 1585?1588, 2003.
J.-H. Kim and P. Woodland ?A rule-based named entity recog-
nition system for speech input?, Proc. ICSLP, pp.2757?
2760, 2001.
R. Kneser and H. Ney ?Improved backing-off for mgram lan-
guage modeling?, Proc. ICASSP, pp. 181?184, 1995.
Y. Liu, ?Automatic disfluency identification in conversa-
tional speech using multiple knowledge sources?, Proc. Eu-
rospeech, pp. 957?960, 2003.
L. Mangu and E. Brill, ?Automatic rule acquisition for spelling
correction?, Proc. Intl. Conf on Machine Learning, pp. 187?
194, 1997.
L. Mangu and M. Padmanabhan, ?Error corrective mechanisms
for speech recognition?, Proc. ICASSP, pp. 29?32, 2001.
M. Meteer et al, ?Dysfluency annotation stylebook for the
Switchboard corpus?, Distributed by the LDC, 1995.
C. Nakatani and J. Hirschberg ?A corpus-based study of re-
pair cues in spontaneous speech?, Journal of the Acoustical
Society of America, pp. 1603?1616, 1994.
G. Ngai and R. Florian ?Transformation-based learning in the
fast lane?, Proc. NAACL, pp. 40?47, 2001.
NIST, ?The Rich Transcription Fall 2003 (RT-
03F) evaluation plan,? http://www.nist.gov/
speech/tests/rt/rt2003/fall/docs/
rt03-fall-eval-plan-v9.pdf, 2003.
A. Ratnaparkhi, ?A maximum entropy part-of-speech tagger?,
Proc. Empirical Methods in Natural Language Processing
Conf., pp. 133?141, 1996.
E. Shriberg, Preliminaries to a theory of speech disfluencies,
PhD thesis, Department of Psychology, University of Cali-
fornia, Berkeley, 1994.
E. Shriberg et al, ?A prosody-only decision-tree model for dis-
fluency detection?, Proc. Eurospeech, pp. 2383?2386, 1997.
E. Shriberg et al, ?Prosody-based automatic segmentation of
speech into sentences and topics? Speech Communication,
32(1-2), pp. 127?154, 2000.
K. So?nmez et al, ?Modeling dynamic prosodic variation for
speaker verification,? Proc. Intl. Conf. on Spoken Language
Processing, v. 7, pp. 3189?3192, 1998.
A. Srivastava and F. Kubala ?Sentence boundary detection in
Arabic speech?, Proc. Eurospeech, pp. 949?952, 2003.
A. Stolcke and E. Shriberg ?Automatic linguistic segmenta-
tion of conversational speech?, Proc. ICSLP, v. 2, pp. 1005?
1008, 1996.
A. Stolcke et al, ?Automatic detection of sentence boundaries
and disfluencies based on recognized words,? Proc. ICSLP,
1998, v. 5, pp. 2247?2250.
A. Stolcke, ?SRILM - an extensible language modeling
toolkit?, Proc. ICSLP, v. 2, pp. 901-904, 2002.
S. Strassel, ?Simple metadata annotation specification version
5.0?, http://www.ldc.upenn.edu/Projects/
MDE/Guidelines/SimpleMDE\_V5.0.pdf, 2003.
Proceedings of the 43rd Annual Meeting of the ACL, pages 523?530,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Reading Level Assessment Using Support Vector Machines and
Statistical Language Models
Sarah E. Schwarm
Dept. of Computer Science and Engineering
University of Washington
Seattle, WA 98195-2350
sarahs@cs.washington.edu
Mari Ostendorf
Dept. of Electrical Engineering
University of Washington
Seattle, WA 98195-2500
mo@ee.washington.edu
Abstract
Reading proficiency is a fundamen-
tal component of language competency.
However, finding topical texts at an appro-
priate reading level for foreign and sec-
ond language learners is a challenge for
teachers. This task can be addressed with
natural language processing technology to
assess reading level. Existing measures
of reading level are not well suited to
this task, but previous work and our own
pilot experiments have shown the bene-
fit of using statistical language models.
In this paper, we also use support vector
machines to combine features from tradi-
tional reading level measures, statistical
language models, and other language pro-
cessing tools to produce a better method
of assessing reading level.
1 Introduction
The U.S. educational system is faced with the chal-
lenging task of educating growing numbers of stu-
dents for whom English is a second language (U.S.
Dept. of Education, 2003). In the 2001-2002 school
year, Washington state had 72,215 students (7.2% of
all students) in state programs for Limited English
Proficient (LEP) students (Bylsma et al, 2003). In
the same year, one quarter of all public school stu-
dents in California and one in seven students in
Texas were classified as LEP (U.S. Dept. of Edu-
cation, 2004). Reading is a critical part of language
and educational development, but finding appropri-
ate reading material for LEP students is often diffi-
cult. To meet the needs of their students, bilingual
education instructors seek out ?high interest level?
texts at low reading levels, e.g. texts at a first or sec-
ond grade reading level that support the fifth grade
science curriculum. Teachers need to find material
at a variety of levels, since students need different
texts to read independently and with help from the
teacher. Finding reading materials that fulfill these
requirements is difficult and time-consuming, and
teachers are often forced to rewrite texts themselves
to suit the varied needs of their students.
Natural language processing (NLP) technology is
an ideal resource for automating the task of selecting
appropriate reading material for bilingual students.
Information retrieval systems successfully find top-
ical materials and even answer complex queries in
text databases and on the World Wide Web. How-
ever, an effective automated way to assess the read-
ing level of the retrieved text is still needed. In
this work, we develop a method of reading level as-
sessment that uses support vector machines (SVMs)
to combine features from statistical language mod-
els (LMs), parse trees, and other traditional features
used in reading level assessment.
The results presented here on reading level as-
sessment are part of a larger project to develop
teacher-support tools for bilingual education instruc-
tors. The larger project will include a text simpli-
fication system, adapting paraphrasing and summa-
rization techniques. Coupled with an information
retrieval system, these tools will be used to select
and simplify reading material in multiple languages
for use by language learners. In addition to students
in bilingual education, these tools will also be use-
ful for those with reading-related learning disabili-
523
ties and adult literacy students. In both of these sit-
uations, as in the bilingual education case, the stu-
dent?s reading level does not match his/her intellec-
tual level and interests.
The remainder of the paper is organized as fol-
lows. Section 2 describes related work on reading
level assessment. Section 3 describes the corpora
used in our work. In Section 4 we present our ap-
proach to the task, and Section 5 contains experi-
mental results. Section 6 provides a summary and
description of future work.
2 Reading Level Assessment
This section highlights examples and features of
some commonly used measures of reading level and
discusses current research on the topic of reading
level assessment using NLP techniques.
Many traditional methods of reading level assess-
ment focus on simple approximations of syntactic
complexity such as sentence length. The widely-
used Flesch-Kincaid Grade Level index is based on
the average number of syllables per word and the
average sentence length in a passage of text (Kin-
caid et al, 1975) (as cited in (Collins-Thompson
and Callan, 2004)). Similarly, the Gunning Fog in-
dex is based on the average number of words per
sentence and the percentage of words with three or
more syllables (Gunning, 1952). These methods are
quick and easy to calculate but have drawbacks: sen-
tence length is not an accurate measure of syntactic
complexity, and syllable count does not necessar-
ily indicate the difficulty of a word. Additionally,
a student may be familiar with a few complex words
(e.g. dinosaur names) but unable to understand com-
plex syntactic constructions.
Other measures of readability focus on seman-
tics, which is usually approximated by word fre-
quency with respect to a reference list or corpus.
The Dale-Chall formula uses a combination of av-
erage sentence length and percentage of words not
on a list of 3000 ?easy? words (Chall and Dale,
1995). The Lexile framework combines measures
of semantics, represented by word frequency counts,
and syntax, represented by sentence length (Stenner,
1996). These measures are inadequate for our task;
in many cases, teachers want materials with more
difficult, topic-specific words but simple structure.
Measures of reading level based on word lists do not
capture this information.
In addition to the traditional reading level metrics,
researchers at Carnegie Mellon University have ap-
plied probabilistic language modeling techniques to
this task. Si and Callan (2001) conducted prelimi-
nary work to classify science web pages using uni-
gram models. More recently, Collins-Thompson and
Callan manually collected a corpus of web pages
ranked by grade level and observed that vocabulary
words are not distributed evenly across grade lev-
els. They developed a ?smoothed unigram? clas-
sifier to better capture the variance in word usage
across grade levels (Collins-Thompson and Callan,
2004). On web text, their classifier outperformed
several other measures of semantic difficulty: the
fraction of unknown words in the text, the number
of distinct types per 100 token passage, the mean log
frequency of the text relative to a large corpus, and
the Flesch-Kincaid measure. The traditional mea-
sures performed better on some commercial corpora,
but these corpora were calibrated using similar mea-
sures, so this is not a fair comparison. More impor-
tantly, the smoothed unigram measure worked better
on the web corpus, especially on short passages. The
smoothed unigram classifier is also more generaliz-
able, since it can be trained on any collection of data.
Traditional measures such as Dale-Chall and Lexile
are based on static word lists.
Although the smoothed unigram classifier outper-
forms other vocabulary-based semantic measures, it
does not capture syntactic information. We believe
that higher order n-gram models or class n-gram
models can achieve better performance by captur-
ing both semantic and syntactic information. This is
particularly important for the tasks we are interested
in, when the vocabulary (i.e. topic) and grade level
are not necessarily well-matched.
3 Corpora
Our work is currently focused on a corpus obtained
from Weekly Reader, an educational newspaper with
versions targeted at different grade levels (Weekly
Reader, 2004). These data include a variety of la-
beled non-fiction topics, including science, history,
and current events. Our corpus consists of articles
from the second, third, fourth, and fifth grade edi-
524
Grade Num Articles Num Words
2 351 71.5k
3 589 444k
4 766 927k
5 691 1M
Table 1: Distribution of articles and words in the
Weekly Reader corpus.
Corpus Num Articles Num Words
Britannica 115 277k
B. Elementary 115 74k
CNN 111 51k
CNN Abridged 111 37k
Table 2: Distribution of articles and words in the
Britannica and CNN corpora.
tions of the newspaper. We design classifiers to dis-
tinguish each of these four categories. This cor-
pus contains just under 2400 articles, distributed as
shown in Table 1.
Additionally, we have two corpora consisting of
articles for adults and corresponding simplified ver-
sions for children or other language learners. Barzi-
lay and Elhadad (2003) have allowed us to use their
corpus from Encyclopedia Britannica, which con-
tains articles from the full version of the encyclope-
dia and corresponding articles from Britannica El-
ementary, a new version targeted at children. The
Western/Pacific Literacy Network?s (2004) web site
has an archive of CNN news stories and abridged
versions which we have also received permission to
use. Although these corpora do not provide an ex-
plicit grade-level ranking for each article, broad cat-
egories are distinguished. We use these data as a
supplement to the Weekly Reader corpus for learn-
ing models to distinguish broad reading level classes
than can serve to provide features for more detailed
classification. Table 2 shows the size of the supple-
mental corpora.
4 Approach
Existing reading level measures are inadequate due
to their reliance on vocabulary lists and/or a superfi-
cial representation of syntax. Our approach uses n-
gram language models as a low-cost automatic ap-
proximation of both syntactic and semantic analy-
sis. Statistical language models (LMs) are used suc-
cessfully in this way in other areas of NLP such as
speech recognition and machine translation. We also
use a standard statistical parser (Charniak, 2000) to
provide syntactic analysis.
In practice, a teacher is likely to be looking for
texts at a particular level rather than classifying a
group of texts into a variety of categories. Thus
we construct one classifier per category which de-
cides whether a document belongs in that category
or not, rather than constructing a classifier which
ranks documents into different categories relative to
each other.
4.1 Statistical Language Models
Statistical LMs predict the probability that a partic-
ular word sequence will occur. The most commonly
used statistical language model is the n-gram model,
which assumes that the word sequence is an (n?1)th
order Markov process. For example, for the com-
mon trigram model where n = 3, the probability of
sequence w is:
P (w) = P (w1)P (w2|w1)
m
?
i=3
P (wi|wi?1, wi?2).
(1)
The parameters of the model are estimated using a
maximum likelihood estimate based on the observed
frequency in a training corpus and smoothed using
modified Kneser-Ney smoothing (Chen and Good-
man, 1999). We used the SRI Language Modeling
Toolkit (Stolcke, 2002) for language model training.
Our first set of classifiers consists of one n-gram
language model per class c in the set of possible
classes C. For each text document t, we can cal-
culate the likelihood ratio between the probability
given by the model for class c and the probabilities
given by the other models for the other classes:
LR = P (t|c)P (c)?
c? 6=c P (t|c?)P (c?)
(2)
where we assume uniform prior probabilities P (c).
The resulting value can be compared to an empiri-
cally chosen threshold to determine if the document
is in class c or not. For each class c, a language
model is estimated from a corpus of training texts.
525
In addition to using the likelihood ratio for classi-
fication, we can use scores from language models as
features in another classifier (e.g. an SVM). For ex-
ample, perplexity (PP ) is an information-theoretic
measure often used to assess language models:
PP = 2H(t|c), (3)
where H(t|c) is the entropy relative to class c of a
length m word sequence t = w1, ..., wm, defined as
H(t|c) = ? 1m log2 P (t|c). (4)
Low perplexity indicates a better match between the
test data and the model, corresponding to a higher
probability P (t|c). Perplexity scores are used as fea-
tures in the SVM model described in Section 4.3.
The likelihood ratio described above could also be
used as a feature, but we achieved better results us-
ing perplexity.
4.2 Feature Selection
Feature selection is a common part of classifier
design for many classification problems; however,
there are mixed results in the literature on feature
selection for text classification tasks. In Collins-
Thompson and Callan?s work (2004) on readabil-
ity assessment, LM smoothing techniques are more
effective than other forms of explicit feature selec-
tion. However, feature selection proves to be impor-
tant in other text classification work, e.g. Lee and
Myaeng?s (2002) genre and subject detection work
and Boulis and Ostendorf?s (2005) work on feature
selection for topic classification.
For our LM classifiers, we followed Boulis and
Ostendorf?s (2005) approach for feature selection
and ranked words by their ability to discriminate
between classes. Given P (c|w), the probability of
class c given word w, estimated empirically from
the training set, we sorted words based on their in-
formation gain (IG). Information gain measures the
difference in entropy when w is and is not included
as a feature.
IG(w) = ?
?
c?C
P (c) log P (c)
+ P (w)
?
c?C
P (c|w) log P (c|w)
+ P (w?)
?
c?C
P (c|w?) log P (c|w?).(5)
The most discriminative words are selected as fea-
tures by plotting the sorted IG values and keeping
only those words below the ?knee? in the curve, as
determined by manual inspection of the graph. In an
early experiment, we replaced all remaining words
with a single ?unknown? tag. This did not result
in an effective classifier, so in later experiments the
remaining words were replaced with a small set of
general tags. Motivated by our goal of represent-
ing syntax, we used part-of-speech (POS) tags as la-
beled by a maximum entropy tagger (Ratnaparkhi,
1996). These tags allow the model to represent pat-
terns in the text at a higher level than that of individ-
ual words, using sequences of POS tags to capture
rough syntactic information. The resulting vocabu-
lary consisted of 276 words and 56 POS tags.
4.3 Support Vector Machines
Support vector machines (SVMs) are a machine
learning technique used in a variety of text classi-
fication problems. SVMs are based on the principle
of structural risk minimization. Viewing the data as
points in a high-dimensional feature space, the goal
is to fit a hyperplane between the positive and neg-
ative examples so as to maximize the distance be-
tween the data points and the plane. SVMs were in-
troduced by Vapnik (1995) and were popularized in
the area of text classification by Joachims (1998a).
The unit of classification in this work is a single
article. Our SVM classifiers for reading level use the
following features:
? Average sentence length
? Average number of syllables per word
? Flesch-Kincaid score
? 6 out-of-vocabulary (OOV) rate scores.
? Parse features (per sentence):
? Average parse tree height
? Average number of noun phrases
? Average number of verb phrases
? Average number of ?SBAR?s.1
? 12 language model perplexity scores
The OOV scores are relative to the most common
100, 200 and 500 words in the lowest grade level
1SBAR is defined in the Penn Treebank tag set as a ?clause
introduced by a (possibly empty) subordinating conjunction.? It
is an indicator of sentence complexity.
526
(grade 2) 2. For each article, we calculated the per-
centage of a) all word instances (tokens) and b) all
unique words (types) not on these lists, resulting in
three token OOV rate features and three type OOV
rate features per article.
The parse features are generated using the Char-
niak parser (Charniak, 2000) trained on the standard
Wall Street Journal Treebank corpus. We chose to
use this standard data set as we do not have any
domain-specific treebank data for training a parser.
Although clearly there is a difference between news
text for adults and news articles intended for chil-
dren, inspection of some of the resulting parses
showed good accuracy.
Ideally, the language model scores would be for
LMs from domain-specific training data (i.e. more
Weekly Reader data.) However, our corpus is lim-
ited and preliminary experiments in which the train-
ing data was split for LM and SVM training were
unsuccessful due to the small size of the resulting
data sets. Thus we made use of the Britannica and
CNN articles to train models of three n-gram or-
ders on ?child? text and ?adult? text. This resulted
in 12 LM perplexity features per article based on
trigram, bigram and unigram LMs trained on Bri-
tannica (adult), Britannica Elementary, CNN (adult)
and CNN abridged text.
For training SVMs, we used the SVMlight toolkit
developed by Joachims (1998b). Using development
data, we selected the radial basis function kernel
and tuned parameters using cross validation and grid
search as described in (Hsu et al, 2003).
5 Experiments
5.1 Test Data and Evaluation Criteria
We divide the Weekly Reader corpus described in
Section 3 into separate training, development, and
test sets. The number of articles in each set is shown
in Table 3. The development data is used as a test
set for comparing classifiers, tuning parameters, etc,
and the results presented in this section are based on
the test set.
We present results in three different formats. For
analyzing our binary classifiers, we use Detection
Error Tradeoff (DET) curves and precision/recall
2These lists are chosen from the full vocabulary indepen-
dently of the feature selection for LMs described above.
Grade Training Dev/Test
2 315 18
3 529 30
4 690 38
5 623 34
Table 3: Number of articles in the Weekly Reader
corpus as divided into training, development and test
sets. The dev and test sets are the same size and each
consist of approximately 5% of the data for each
grade level.
measures. For comparison to other methods, e.g.
Flesch-Kincaid and Lexile, which are not binary
classifiers, we consider the percentage of articles
which are misclassified by more than one grade
level.
Detection Error Tradeoff curves show the tradeoff
between misses and false alarms for different thresh-
old values for the classifiers. ?Misses? are positive
examples of a class that are misclassified as neg-
ative examples; ?false alarms? are negative exam-
ples misclassified as positive. DET curves have been
used in other detection tasks in language processing,
e.g. Martin et al (1997). We use these curves to vi-
sualize the tradeoff between the two types of errors,
and select the minimum cost operating point in or-
der to get a threshold for precision and recall calcu-
lations. The minimum cost operating point depends
on the relative costs of misses and false alarms; it
is conceivable that one type of error might be more
serious than the other. After consultation with teach-
ers (future users of our system), we concluded that
there are pros and cons to each side, so for the pur-
pose of this analysis we weighted the two types of
errors equally. In this work, the minimum cost op-
erating point is selected by averaging the percent-
ages of misses and false alarms at each point and
choosing the point with the lowest average. Unless
otherwise noted, errors reported are associated with
these actual operating points, which may not lie on
the convex hull of the DET curve.
Precision and recall are often used to assess in-
formation retrieval systems, and our task is similar.
Precision indicates the percentage of the retrieved
documents that are relevant, in this case the per-
centage of detected documents that match the target
527
grade level. Recall indicates the percentage of the
total number of relevant documents in the data set
that are retrieved, in this case the percentage of the
total number of documents from the target level that
are detected.
5.2 Language Model Classifier
  1   2     5    10    20    40    60    80    90    1   
  2   
  5   
  10  
  20  
  40  
  60  
  80  
  90  
False Alarm probability (in %)
M
is
s 
pr
ob
ab
ilit
y 
(in
 %
)
grade 2
grade 3
grade 4
grade 5
Figure 1: DET curves (test set) for classifiers based
on trigram language models.
Figure 1 shows DET curves for the trigram LM-
based classifiers. The minimum cost error rates for
these classifiers, indicated by large dots in the plot,
are in the range of 33-43%, with only one over 40%.
The curves for bigram and unigram models have
similar shapes, but the trigram models outperform
the lower-order models. Error rates for the bigram
models range from 37-45% and the unigram mod-
els have error rates in the 39-49% range, with all but
one over 40%. Although our training corpus is small
the feature selection described in Section 4.2 allows
us to use these higher-order trigram models.
5.3 Support Vector Machine Classifier
By combining language model scores with other fea-
tures in an SVM framework, we achieve our best
results. Figures 2 and 3 show DET curves for this
set of classifiers on the development set and test
set, respectively. The grade 2 and 5 classifiers have
the best performance, probably because grade 3 and
4 must be distinguished from other classes at both
higher and lower levels. Using threshold values se-
lected based on minimum cost on the development
  1   2     5    10    20    40    60    80    90    1   
  2   
  5   
  10  
  20  
  40  
  60  
  80  
  90  
False Alarm probability (in %)
M
is
s 
pr
ob
ab
ilit
y 
(in
 %
)
grade 2
grade 3
grade 4
grade 5
Figure 2: DET curves (development set) for SVM
classifiers with LM features.
  1   2     5    10    20    40    60    80    90    1   
  2   
  5   
  10  
  20  
  40  
  60  
  80  
  90  
False Alarm probability (in %)
M
is
s 
pr
ob
ab
ilit
y 
(in
 %
)
grade 2
grade 3
grade 4
grade 5
Figure 3: DET curves (test set) for SVM classifiers
with LM features.
set, indicated by large dots on the plot, we calcu-
lated precision and recall on the test set. Results are
presented in Table 4. The grade 3 classifier has high
recall but relatively low precision; the grade 4 classi-
fier does better on precision and reasonably well on
recall. Since the minimum cost operating points do
not correspond to the equal error rate (i.e. equal per-
centage of misses and false alarms) there is variation
in the precision-recall tradeoff for the different grade
level classifiers. For example, for class 3, the oper-
ating point corresponds to a high probability of false
alarms and a lower probability of misses, which re-
sults in low precision and high recall. For operating
points chosen on the convex hull of the DET curves,
the equal error rate ranges from 12-25% for the dif-
528
Grade Precision Recall
2 38% 61%
3 38% 87%
4 70% 60%
5 75% 79%
Table 4: Precision and recall on test set for SVM-
based classifiers.
Grade Errors
Flesch-Kincaid Lexile SVM
2 78% 33% 5.5%
3 67% 27% 3.3%
4 74% 26% 13%
5 59% 24% 21%
Table 5: Percentage of articles which are misclassi-
fied by more than one grade level.
ferent grade levels.
We investigated the contribution of individual fea-
tures to the overall performance of the SVM clas-
sifier and found that no features stood out as most
important, and performance was degraded when any
particular features were removed.
5.4 Comparison
We also compared error rates for the best per-
forming SVM classifier with two traditional read-
ing level measures, Flesch-Kincaid and Lexile. The
Flesch-Kincaid Grade Level index is a commonly
used measure of reading level based on the average
number of syllables per word and average sentence
length. The Flesch-Kincaid score for a document is
intended to directly correspond with its grade level.
We chose the Lexile measure as an example of a
reading level classifier based on word lists.3 Lexile
scores do not correlate directly to numeric grade lev-
els, however a mapping of ranges of Lexile scores to
their corresponding grade levels is available on the
Lexile web site (Lexile, 2005).
For each of these three classifiers, Table 5 shows
the percentage of articles which are misclassified by
more than one grade level. Flesch-Kincaid performs
poorly, as expected since its only features are sen-
3Other classifiers such as Dale-Chall do not have automatic
software available.
tence length and average syllable count. Although
this index is commonly used, perhaps due to its sim-
plicity, it is not accurate enough for the intended
application. Our SVM classifier also outperforms
the Lexile metric. Lexile is a more general measure
while our classifier is trained on this particular do-
main, so the better performance of our model is not
entirely surprising. Importantly, however, our clas-
sifier is easily tuned to any corpus of interest.
To test our classifier on data outside the Weekly
Reader corpus, we downloaded 10 randomly se-
lected newspaper articles from the ?Kidspost? edi-
tion of The Washington Post (2005). ?Kidspost? is
intended for grades 3-8. We found that our SVM
classifier, trained on the Weekly Reader corpus, clas-
sified four of these articles as grade 4 and seven ar-
ticles as grade 5 (with one overlap with grade 4).
These results indicate that our classifier can gener-
alize to other data sets. Since there was no training
data corresponding to higher reading levels, the best
performance we can expect for adult-level newspa-
per articles is for our classifiers to mark them as the
highest grade level, which is indeed what happened
for 10 randomly chosen articles from standard edi-
tion of The Washington Post.
6 Conclusions and Future Work
Statistical LMs were used to classify texts based
on reading level, with trigram models being no-
ticeably more accurate than bigrams and unigrams.
Combining information from statistical LMs with
other features using support vector machines pro-
vided the best results. Future work includes testing
additional classifier features, e.g. parser likelihood
scores and features obtained using a syntax-based
language model such as Chelba and Jelinek (2000)
or Roark (2001). Further experiments are planned
on the generalizability of our classifier to text from
other sources (e.g. newspaper articles, web pages);
to accomplish this we will add higher level text as
negative training data. We also plan to test these
techniques on languages other than English, and in-
corporate them with an information retrieval system
to create a tool that may be used by teachers to help
select reading material for their students.
529
Acknowledgments
This material is based upon work supported by the National Sci-
ence Foundation under Grant No. IIS-0326276. Any opinions,
findings, and conclusions or recommendations expressed in this
material are those of the authors and do not necessarily reflect
the views of the National Science Foundation.
Thank you to Paul Heavenridge (Literacyworks), the Weekly
Reader Corporation, Regina Barzilay (MIT) and Noemie El-
hadad (Columbia University) for sharing their data and corpora.
References
R. Barzilay and N. Elhadad. Sentence alignment for monolin-
gual comparable corpora. In Proc. of EMNLP, pages 25?32,
2003.
C. Boulis and M. Ostendorf. Text classification by aug-
menting the bag-of-words representation with redundancy-
compensated bigrams. Workshop on Feature Selection in
Data Mining, in conjunction with SIAM conference on Data
Mining, 2005.
P. Bylsma, L. Ireland, and H. Malagon. Educating English Lan-
guage Learners in Washington State. Office of the Superin-
tendent of Public Instruction, Olympia, WA, 2003.
J.S. Chall and E. Dale. Readability revisited: the new Dale-
Chall readability formula. Brookline Books, Cambridge,
Mass., 1995.
E. Charniak. A maximum-entropy-inspired parser. In Proc. of
NAACL, pages 132?139, 2000.
C. Chelba and F. Jelinek. Structured Language Modeling.
Computer Speech and Language, 14(4):283-332, 2000.
S. Chen and J. Goodman. An empirical study of smoothing
techniques for language modeling. Computer Speech and
Language, 13(4):359?393, 1999.
K. Collins-Thompson and J. Callan. A language model-
ing approach to predicting reading difficulty. In Proc. of
HLT/NAACL, pages 193?200, 2004.
R. Gunning. The technique of clear writing. McGraw-Hill,
New York, 1952.
C.-W. Hsu et al A practical guide to support vector classi-
fication. http://www.csie.ntu.edu.tw/?cjlin/
papers/guide/guide.pdf, 2003. Accessed 11/2004.
T. Joachims. Text categorization with support vector machines:
learning with many relevant features. In Proc. of the Eu-
ropean Conference on Machine Learning, pages 137?142,
1998a.
T. Joachims. Making large-scale support vector machine learn-
ing practical. In Advances in Kernel Methods: Support Vec-
tor Machines. B. Scho?lkopf, C. Burges, A. Smola, eds. MIT
Press, Cambridge, MA, 1998b.
J.P. Kincaid, Jr., R.P. Fishburne, R.L. Rodgers, and
B.S. Chisson. Derivation of new readability formulas for
Navy enlisted personnel. Research Branch Report 8-75, U.S.
Naval Air Station, Memphis, 1975.
Y.-B. Lee and S.H. Myaeng. Text genre classification with
genre-revealing and subject-revealing features. In Proc. of
SIGIR, pages 145?150, 2002.
The Lexile framework for reading. http://www.lexile.
com, 2005. Accessed April 15, 2005.
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocki. The DET curve in assessment of detection
task performance. Proc. of Eurospeech, v. 4, pp. 1895-1898,
1997.
A. Ratnaparkhi. A maximum entropy part-of-speech tagger. In
Proc. of EMNLP, pages 133?141, 1996.
B. Roark. Probabilistic top-down parsing and language model-
ing. Computational Linguistics, 27(2):249-276, 2001.
L. Si and J.P. Callan. A statistical model for scientific readabil-
ity. In Proc. of CIKM, pages 574?576, 2001.
A.J. Stenner. Measuring reading comprehension with the Lex-
ile framework. Presented at the Fourth North American Con-
ference on Adolescent/Adult Literacy, 1996.
A. Stolcke. SRILM - an extensible language modeling toolkit.
Proc. ICSLP, v. 2, pp. 901-904, 2002.
U.S. Department of Education, National Center for Ed-
ucational Statistics. The condition of education.
http://nces.ed.gov/programs/coe/2003/
section1/indicator04.asp, 2003. Accessed June
18, 2004.
U.S. Department of Education, National Center for Educational
Statistics. NCES fast facts: Bilingual education/Limited
English Proficient students. http://nces.ed.gov/
fastfacts/display.asp?id=96, 2003. Accessed
June 18, 2004.
V. Vapnik. The Nature of Statistical Learning Theory. Springer,
New York, 1995.
The Washington Post. http://www.washingtonpost.
com, 2005. Accessed April 20, 2005.
Weekly Reader. http://www.weeklyreader.com,
2004. Accessed July, 2004.
Western/Pacific Literacy Network / Literacyworks. CNN
SF learning resources. http://literacynet.org/
cnnsf/, 2004. Accessed June 15, 2004.
530
