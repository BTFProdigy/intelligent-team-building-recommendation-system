Proceedings of the 12th Conference of the European Chapter of the ACL, pages 327?335,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Enhancing Unlexicalized Parsing Performance
using a Wide Coverage Lexicon, Fuzzy Tag-set Mapping,
and EM-HMM-based Lexical Probabilities
Yoav Goldberg1? Reut Tsarfaty2? Meni Adler1? Michael Elhadad1
1Department of Computer Science, Ben Gurion University of the Negev
{yoavg|adlerm|elhadad}@cs.bgu.ac.il
2Institute for Logic, Language and Computation, University of Amsterdam
R.Tsarfaty@uva.nl
Abstract
We present a framework for interfacing
a PCFG parser with lexical information
from an external resource following a dif-
ferent tagging scheme than the treebank.
This is achieved by defining a stochas-
tic mapping layer between the two re-
sources. Lexical probabilities for rare
events are estimated in a semi-supervised
manner from a lexicon and large unanno-
tated corpora. We show that this solu-
tion greatly enhances the performance of
an unlexicalized Hebrew PCFG parser, re-
sulting in state-of-the-art Hebrew parsing
results both when a segmentation oracle is
assumed, and in a real-word parsing sce-
nario of parsing unsegmented tokens.
1 Introduction
The intuition behind unlexicalized parsers is that
the lexicon is mostly separated from the syntax:
specific lexical items are mostly irrelevant for ac-
curate parsing, and can be mediated through the
use of POS tags and morphological hints. This
same intuition also resonates in highly lexicalized
formalism such as CCG: while the lexicon cate-
gories are very fine grained and syntactic in na-
ture, once the lexical category for a lexical item is
determined, the specific lexical form is not taken
into any further consideration.
Despite this apparent separation between the
lexical and the syntactic levels, both are usually es-
timated solely from a single treebank. Thus, while
?Supported by the Lynn and William Frankel Center for
Computer Sciences, Ben Gurion University
?Funded by the Dutch Science Foundation (NWO), grant
number 017.001.271.
?Post-doctoral fellow, Deutsche Telekom labs at Ben Gu-
rion University
PCFGs can be accurate, they suffer from vocabu-
lary coverage problems: treebanks are small and
lexicons induced from them are limited.
The reason for this treebank-centric view in
PCFG learning is 3-fold: the English treebank is
fairly large and English morphology is fairly sim-
ple, so that in English, the treebank does provide
mostly adequate lexical coverage1; Lexicons enu-
merate analyses, but don?t provide probabilities
for them; and, most importantly, the treebank and
the external lexicon are likely to follow different
annotation schemas, reflecting different linguistic
perspectives.
On a different vein of research, current POS tag-
ging technology deals with much larger quantities
of training data than treebanks can provide, and
lexicon-based unsupervised approaches to POS
tagging are practically unlimited in the amount
of training data they can use. POS taggers rely
on richer knowledge than lexical estimates de-
rived from the treebank, have evolved sophisti-
cated strategies to handle OOV and can provide
distributions p(t|w, context) instead of ?best tag?
only.
Can these two worlds be combined? We pro-
pose that parsing performance can be greatly im-
proved by using a wide coverage lexicon to sug-
gest analyses for unknown tokens, and estimating
the respective lexical probabilities using a semi-
supervised technique, based on the training pro-
cedure of a lexicon-based HMM POS tagger. For
many resources, this approach can be taken only
on the proviso that the annotation schemes of the
two resources can be aligned.
We take Modern Hebrew parsing as our case
study. Hebrew is a Semitic language with rich
1This is not the case with other languages, and also not
true for English when adaptation scenarios are considered.
327
morphological structure. This rich structure yields
a large number of distinct word forms, resulting in
a high OOV rate (Adler et al, 2008a). This poses
a serious problem for estimating lexical probabili-
ties from small annotated corpora, such as the He-
brew treebank (Sima?an et al, 2001).
Hebrew has a wide coverage lexicon /
morphological-analyzer (henceforth, KC Ana-
lyzer) available2, but its tagset is different than the
one used by the Hebrew Treebank. These are not
mere technical differences, but derive from dif-
ferent perspectives on the data. The Hebrew TB
tagset is syntactic in nature, while the KC tagset
is lexicographic. This difference in perspective
yields different performance for parsers induced
from tagged data, and a simple mapping between
the two schemes is impossible to define (Sec. 2).
A naive approach for combining the use of the
two resources would be to manually re-tag the
Treebank with the KC tagset, but we show this ap-
proach harms our parser?s performance. Instead,
we propose a novel, layered approach (Sec. 2.1),
in which syntactic (TB) tags are viewed as contex-
tual refinements of the lexicon (KC) tags, and con-
versely, KC tags are viewed as lexical clustering
of the syntactic ones. This layered representation
allows us to easily integrate the syntactic and the
lexicon-based tagsets, without explicitly requiring
the Treebank to be re-tagged.
Hebrew parsing is further complicated by the
fact that common prepositions, conjunctions and
articles are prefixed to the following word and
pronominal elements often appear as suffixes. The
segmentation of prefixes and suffixes can be am-
biguous and must be determined in a specific con-
text only. Thus, the leaves of the syntactic parse
trees do not correspond to space-delimited tokens,
and the yield of the tree is not known in advance.
We show that enhancing the parser with external
lexical information is greatly beneficial, both in an
artificial scenario where the token segmentation is
assumed to be known (Sec. 4), and in a more re-
alistic one in which parsing and segmentation are
handled jointly by the parser (Goldberg and Tsar-
faty, 2008) (Sec. 5). External lexical informa-
tion enhances unlexicalized parsing performance
by as much as 6.67 F-points, an error reduction
of 20% over a Treebank-only parser. Our results
are not only the best published results for pars-
ing Hebrew, but also on par with state-of-the-art
2http://mila.cs.technion.ac.il/hebrew/resources/lexicons/
lexicalized Arabic parsing results assuming gold-
standard fine-grained Part-of-Speech (Maamouri
et al, 2008).3
2 A Tale of Two Resources
Modern Hebrew has 2 major linguistic resources:
the Hebrew Treebank (TB), and a wide coverage
Lexicon-based morphological analyzer developed
and maintained by the Knowledge Center for Pro-
cessing Hebrew (KC Analyzer).
The Hebrew Treebank consists of sentences
manually annotated with constituent-based syn-
tactic information. The most recent version (V2)
(Guthmann et al, 2009) has 6,219 sentences, and
covers 28,349 unique tokens and 17,731 unique
segments4.
The KC Analyzer assigns morphological analy-
ses (prefixes, suffixes, POS, gender, person, etc.)
to Hebrew tokens. It is based on a lexicon of
roughly 25,000 word lemmas and their inflection
patterns. From these, 562,439 unique word forms
are derived. These are then prefixed (subject to
constraints) by 73 prepositional prefixes.
It is interesting to note that even with these
numbers, the Lexicon?s coverage is far from com-
plete. Roughly 1,500 unique tokens from the He-
brew Treebank cannot be assigned any analysis
by the KC Lexicon, and Adler et al(2008a) report
that roughly 4.5% of the tokens in a 42M tokens
corpus of news text are unknown to the Lexicon.
For roughly 400 unique cases in the Treebank, the
Lexicon provides some analyses, but not a correct
one. This goes to emphasize the productive nature
of Hebrew morphology, and stress that robust lex-
ical probability estimates cannot be derived from
an annotated resource as small as the Treebank.
Lexical vs. Syntactic POS Tags The analyses
produced by the KC Analyzer are not compatible
with the Hebrew TB.
The KC tagset (Adler et al, 2008b; Netzer et
al., 2007; Adler, 2007) takes a lexical approach to
POS tagging (?a word can assume only POS tags
that would be assigned to it in a dictionary?), while
the TB takes a syntactic one (?if the word in this
particular positions functions as an Adverb, tag it
as an Adverb, even though it is listed in the dictio-
nary only as a Noun?). We present 2 cases that em-
phasize the difference: Adjectives: the Treebank
3Our method is orthogonal to lexicalization and can be
used in addition to it if one so wishes.
4In these counts, all numbers are conflated to one canoni-
cal form
328
treats any word in an adjectivial position as an Ad-
jective. This includes also demonstrative pronouns
?? ??? (this boy). However, from the KC point of
view, the fact that a pronoun can be used to modify
a noun does not mean it should appear in a dictio-
nary as an adjective. The MOD tag: similarly,
the TB has a special POS-tag for words that per-
form syntactic modification. These are mostly ad-
verbs, but almost any Adjective can, in some cir-
cumstances, belong to that class as well. This cat-
egory is highly syntactic, and does not conform to
the lexicon based approach.
In addition, many adverbs and prepositions in
Hebrew are lexicalized instances of a preposition
followed by a noun (e.g., ?????, ?in+softness?,
softly). These can admit both the lexical-
ized and the compositional analyses. Indeed,
many words admit the lexicalized analyses in
one of the resource but not in the other (e.g.,
????? ?for+benefit? is Prep in the TB but only
Prep+Noun in the KC, while for ??? ?from+side?
it is the other way around).
2.1 A Unified Resource
While the syntactic POS tags annotation of the TB
is very useful for assigning the correct tree struc-
ture when the correct POS tag is known, there are
clear benefits to an annotation scheme that can be
easily backed by a dictionary.
We created a unified resource, in which every
word occurrence in the Hebrew treebank is as-
signed a KC-based analysis. This was done in a
semi-automatic manner ? for most cases the map-
ping could be defined deterministically. The rest
(less than a thousand instances) were manually as-
signed. Some Treebank tokens had no analyses
in the KC lexicon, and some others did not have
a correct analysis. These were marked as ?UN-
KNOWN? and ?MISSING? respectively.5
The result is a Treebank which is morpho-
logically annotated according to two different
schemas. On average, each of the 257 TB tags
is mapped to 2.46 of the 273 KC tags.6 While this
resource can serve as a basis for many linguisti-
cally motivated inquiries, the rest of this paper is
5Another solution would be to add these missing cases to
the KC Lexicon. In our view this act is harmful: we don?t
want our Lexicon to artificially overfit our annotated corpora.
6A ?tag? in this context means the complete morphologi-
cal information available for a morpheme in the Treebank: its
part of speech, inflectional features and possessive suffixes,
but not prefixes or nominative and accusative suffixes, which
are taken to be separate morphemes.
devoted to using it for constructing a better parser.
Tagsets Comparison In (Adler et al, 2008b),
we hypothesized that due to its syntax-based na-
ture, the Treebank morphological tagset is more
suitable than the KC one for syntax related tasks.
Is this really the case? To verify it, we simulate a
scenario in which the complete gold morpholog-
ical information is available. We train 2 PCFG
grammars, one on each tagged version of the Tree-
bank, and test them on the subset of the develop-
ment set in which every token is completely cov-
ered by the KC Analyzer (351 sentences).7 The
input to the parser is the yields and disambiguated
pre-terminals of the trees to be parsed. The parsing
results are presented in Table 1. Note that this sce-
nario does not reflect actual parsing performance,
as the gold information is never available in prac-
tice, and surface forms are highly ambiguous.
Tagging Scheme Precision Recall
TB / syntactic 82.94 83.59
KC / dictionary 81.39 81.20
Table 1: evalb results for parsing with Oracle
morphological information, for the two tagsets
With gold morphological information, the TB
tagging scheme is more informative for the parser.
The syntax-oriented annotation scheme of the
TB is more informative for parsing than the lexi-
cographic KC scheme. Hence, we would like our
parser to use this TB tagset whenever possible, and
the KC tagset only for rare or unseen words.
A Layered Representation It seems that learn-
ing a treebank PCFG assuming such a different
tagset would require a treebank tagged with the
alternative annotation scheme. Rather than assum-
ing the existence of such an alternative resource,
we present here a novel approach in which we
view the different tagsets as corresponding to dif-
ferent aspects of the morphosyntactic representa-
tion of pre-terminals in the parse trees. Each of
these layers captures subtleties and regularities in
the data, none of which we would want to (and
sometimes, cannot) reduce to the other. We, there-
fore, propose to retain both tagsets and learn a
fuzzy mapping between them.
In practice, we propose an integrated represen-
tation of the tree in which the bottommost layer
represents the yield of the tree, the surface forms
7For details of the train/dev splits as well as the grammar,
see Section 4.2.
329
are tagged with dictionary-based KC POS tags,
and syntactic TB POS tags are in turn mapped onto
the KC ones (see Figure 1).
TB: KC: Layered:
...
JJ-ZYTB
??
...
PRP-M-S-3-DEMKC
??
...
JJ-ZYTB
PRP-M-S-3-DEMKC
??
...
INTB
??????
...
INKC
?
...
NN-F-SKC
?????
...
INTB
INKC
?
NN-F-SKC
?????
Figure 1: Syntactic (TB), Lexical (KC) and
Layered representations
This representation helps to retain the informa-
tion both for the syntactic and the morphologi-
cal POS tagsets, and can be seen as capturing the
interaction between the morphological and syn-
tactic aspects, allowing for a seamless integra-
tion of the two levels of representation. We re-
fer to this intermediate layer of representation as
a morphosyntactic-transfer layer and we formally
depict it as p(tKC |tTB).
This layered representation naturally gives rise
to a generative model in which a phrase level con-
stituent first generates a syntactic POS tag (tTB),
and this in turn generates the lexical POS tag(s)
(tKC). The KC tag then ultimately generates the
terminal symbols (w). We assume that a morpho-
logical analyzer assigns all possible analyses to a
given terminal symbol. Our terminal symbols are,
therefore, pairs: ?w, t?, and our lexical rules are of
the form t? ?w, t?. This gives rise to the follow-
ing equivalence:
p(?w, tKC?|tTB) = p(tKC |tTB)p(?w, tKC?|tKC)
In Sections (4, 5) we use this layered gener-
ative process to enable a smooth integration of
a PCFG treebank-learned grammar, an external
wide-coverage lexicon, and lexical probabilities
learned in a semi-supervised manner.
3 Semi-supervised Lexical Probability
Estimations
A PCFG parser requires lexical probabilities
of the form p(w|t) (Charniak et al, 1996).
Such information is not readily available in
the lexicon. However, it can be estimated
from the lexicon and large unannotated cor-
pora, by using the well-known Baum-Welch
(EM) algorithm to learn a trigram HMM tagging
model of the form p(t1, . . . , tn, w1, . . . , wn) =
argmax
?
p(ti|ti?1, ti?2)p(wi|ti), and taking
the emission probabilities p(w|t) of that model.
In Hebrew, things are more complicated, as
each emission w is not a space delimited token, but
rather a smaller unit (a morphological segment,
henceforth a segment). Adler and Elhadad (2006)
present a lattice-based modification of the Baum-
Welch algorithm to handle this segmentation am-
biguity.
Traditionally, such unsupervised EM-trained
HMM taggers are thought to be inaccurate, but
(Goldberg et al, 2008) showed that by feeding the
EM process with sufficiently good initial proba-
bilities, accurate taggers (> 91% accuracy) can be
learned for both English and Hebrew, based on a
(possibly incomplete) lexicon and large amount of
raw text. They also present a method for automat-
ically obtaining these initial probabilities.
As stated in Section 2, the KC Analyzer (He-
brew Lexicon) coverage is incomplete. Adler
et al(2008a) use the lexicon to learn a Maximum
Entropy model for predicting possible analyses for
unknown tokens based on their orthography, thus
extending the lexicon to cover (even if noisily) any
unknown token. In what follows, we use KC Ana-
lyzer to refer to this extended version.
Finally, these 3 works are combined to create
a state-of-the-art POS-tagger and morphological
disambiguator for Hebrew (Adler, 2007): initial
lexical probabilities are computed based on the
MaxEnt-extended KC Lexicon, and are then fed
to the modified Baum-Welch algorithm, which is
used to fit a morpheme-based tagging model over
a very large corpora. Note that the emission prob-
abilities P (W |T ) of that model cover all the mor-
phemes seen in the unannotated training corpus,
even those not covered by the KC Analyzer.8
We hypothesize that such emission probabili-
ties are good estimators for the morpheme-based
P (T ? W ) lexical probabilities needed by a
PCFG parser. To test this hypothesis, we use it
to estimate p(tKC ? w) in some of our models.
4 Parsing with a Segmentation Oracle
We now turn to describing our first set of exper-
iments, in which we assume the correct segmen-
8P (W |T ) is defined also for words not seen during train-
ing, based on the initial probabilities calculation procedure.
For details, see (Adler, 2007).
330
tation for each input sentence is known. This is
a strong assumption, as the segmentation stage
is ambiguous, and segmentation information pro-
vides very useful morphological hints that greatly
constrain the search space of the parser. However,
the setting is simpler to understand than the one
in which the parser performs both segmentation
and POS tagging, and the results show some in-
teresting trends. Moreover, some recent studies on
parsing Hebrew, as well as all studies on parsing
Arabic, make this oracle assumption. As such, the
results serve as an interesting comparison. Note
that in real-world parsing situations, the parser is
faced with a stream of ambiguous unsegmented to-
kens, making results in this setting not indicative
of real-world parsing performance.
4.1 The Models
The main question we address is the incorporation
of an external lexical resource into the parsing pro-
cess. This is challenging as different resources fol-
low different tagging schemes. One way around
it is re-tagging the treebank according to the new
tagging scheme. This will serve as a baseline
in our experiment. The alternative method uses
the Layered Representation described above (Sec.
2.1). We compare the performance of the two ap-
proaches, and also compare them against the per-
formance of the original treebank without external
information.
We follow the intuition that external lexical re-
sources are needed only when the information
contained in the treebank is too sparse. There-
fore, we use treebank-derived estimates for reli-
able events, and resort to the external resources
only in the cases of rare or OOV words, for which
the treebank distribution is not reliable.
Grammar and Notation For all our experi-
ments, we use the same grammar, and change
only the way lexical probabilities are imple-
mented. The grammar is an unlexicalized
treebank-estimated PCFG with linguistically mo-
tivated state-splits.9
In what follows, a lexical event is a word seg-
ment which is assigned a single POS thereby func-
tioning as a leaf in a syntactic parse tree. A rare
9Details of the grammar: all functional information is re-
moved from the non-terminals, finite and non-finite verbs, as
well as possessive and other PPs are distinguished, definite-
ness structure of constituents is marked, and parent annota-
tion is employed. It is the same grammar as described in
(Goldberg and Tsarfaty, 2008).
(lexical) event is an event occurring less than K
times in the training data, and a reliable (lexical)
event is one occurring at least K times in the train-
ing data. We use OOV to denote lexical events ap-
pearing 0 times in the training data. count(?) is
a counting function over the training data, rare
stands for any rare event, and wrare is a specific
rare event. KCA(?) is the KC Analyzer function,
mapping a lexical event to a set of possible tags
(analyses) according to the lexicon.
Lexical Models
All our models use relative frequency estimated
probabilities for reliable lexical events: p(t ?
w|t) = count(w,t)count(t) . They differ only in their treat-
ment of rare (including OOV) events.
In our Baseline, no external resource is used.
We smooth for rare and OOV events using a per-
tag probability distribution over rare segments,
which we estimate using relative frequency over
rare segments in the training data: p(wrare|t) =
count(rare,t)
count(t) . This is the way lexical probabilities
in treebank grammars are usually estimated.
We experiment with two flavours of lexical
models. In the first, LexFilter, the KC Analyzer is
consulted for rare events. We estimate rare events
using the same per-tag distribution as in the base-
line, but use the KC Analyzer to filter out any in-
compatible cases, that is, we force to 0 the proba-
bility of any analysis not supported by the lexicon:
p(wrare|t) =
{
count(rare,t)
count(t) t ? KCA(wrare)
0 t /? KCA(wrare)
Our second flavour of lexical models, Lex-
Probs, the KC Analyzer is consulted to propose
analyses for rare events, and the probability of an
analysis is estimated via the HMM emission func-
tion described in Section 3, which we denote B:
p(wrare|t) = B(wrare, t)
In both LexFilter and LexProbs, we resort to
the relative frequency estimation in case the event
is not covered in the KC Analyzer.
Tagset Representations
In this work, we are comparing 3 different rep-
resentations: TB, which is the original Treebank,
KC which is the Treebank converted to use the KC
Analyzer tagset, and Layered, which is the layered
representation described above.
The details of the lexical models vary according
to the representation we choose to work with.
For the TB setting, our lexical rules are of the form
331
ttb ? w. Only the Baseline models are relevant
here, as the tagset is not compatible with that of
the external lexicon.
For the KC setting, our lexical rules are of the form
tkc ? w, and their probabilities are estimated as
described above. Note that this setting requires our
trees to be tagged with the new (KC) tagset, and
parsed sentences are also tagged with this tagset.
For the Layered setting, we use lexical rules of
the form ttb ? w. Reliable events are esti-
mated as usual, via relative frequency over the
original treebank. For rare events, we estimate
p(ttb ? w|ttb) = p(ttb ? tkc|ttb)p(tkc ? w|tkc),
where the transfer probabilities p(ttb ? tkc) are
estimated via relative frequencies over the layered
trees, and the emission probabilities are estimated
either based on other rare events (LexFilter) or
based on the semi-supervised method described in
Section 3 (LexProbs).
The layered setting has several advantages:
First, the resulting trees are all tagged with the
original TB tagset. Second, the training proce-
dure does not require a treebank tagged with the
KC tagset: Instead of learning the transfer layer
from the treebank we could alternatively base our
counts on a different parallel resource, estimate it
from unannotated data using EM, define it heuris-
tically, or use any other estimation procedure.
4.2 Experiments
We perform all our experiments on Version 2 of
the Hebrew Treebank, and follow the train/test/dev
split introduced in (Tsarfaty and Sima?an, 2007):
section 1 is used for development, sections 2-12
for training, and section 13 is the test set, which
we do not use in this work. All the reported re-
sults are on the development set.10 After removal
of empty sentences, we have 5241 sentences for
training, and 483 for testing. Due to some changes
in the Treebank11, our results are not directly com-
parable to earlier works. However, our baseline
models are very similar to the models presented
in, e.g. (Goldberg and Tsarfaty, 2008).
In order to compare the performance of the
model on the various tagset representations (TB
tags, KC tags, Layered), we remove from the test
set 51 sentences in which at least one token is
marked as not having any correct segmentation in
the KC Analyzer. This introduces a slight bias in
10This work is part of an ongoing work on a parser, and the
test set is reserved for final evaluation of the entire system.
11Normalization of numbers and percents, correcting of
some incorrect trees, etc.
favor of the KC-tags setting, and makes the test
somewhat easier for all the models. However, it
allows for a relatively fair comparison between the
various models.12
Results and Discussion
Results are presented in Table 2.13
Baseline
rare: < 2 rare: < 10
Prec Rec Prec Rec
TB 72.80 71.70 67.66 64.92
KC 72.23 70.30 67.22 64.31
LexFilter
rare: < 2 rare: < 10
Prec Rec Prec Rec
KC 77.18 76.31 77.34 76.20
Layered 76.69 76.40 76.66 75.74
LexProbs
rare: < 2 rare: < 10
Prec Rec Prec Rec
KC 77.29 76.65 77.22 76.36
Layered 76.81 76.49 76.85 76.08
Table 2: evalb results for parsing with a
segmentation Oracle.
As expected, all the results are much lower than
those with gold fine-grained POS (Table 1).
When not using any external knowledge (Base-
line), the TB tagset performs slightly better than
the converted treebank (KC). Note, however, that
the difference is less pronounced than in the gold
morphology case. When varying the rare words
threshold from 2 to 10, performance drops consid-
erably. Without external knowledge, the parser is
facing difficulties coping with unseen events.
The incorporation of an external lexical knowl-
edge in the form of pruning illegal tag assignments
for unseen words based on the KC lexicon (Lex-
Filter) substantially improves the results (? 72 to
? 77). The additional lexical knowledge clearly
improves the parser. Moreover, varying the rare
words threshold in this setting hardly affects the
parser performance: the external lexicon suffices
to guide the parser in the right direction. Keep-
ing the rare words threshold high is desirable, as it
reduces overfitting to the treebank vocabulary.
We expected the addition of the semi-
supervised p(t ? w) distribution (LexProbs) to
improve the parser, but found it to have an in-
significant effect. The correct segmentation seems
12We are forced to remove these sentences because of the
artificial setting in which the correct segmentation is given. In
the no-oracle setting (Sec. 5), we do include these sentences.
13The layered trees have an extra layer of bracketing
(tTB ? tKC ). We remove this layer prior to evaluation.
332
to remove enough ambiguity as to let the parser
base its decisions on the generic tag distribution
for rare events.
In all the settings with a Segmentation Oracle,
there is no significant difference between the KC
and the Layered representation. We prefer the lay-
ered representation as it provides more flexibility,
does not require trees tagged with the KC tagset,
and produces parse trees with the original TB POS
tags at the leaves.
5 Parsing without a Segmentation Oracle
When parsing real world data, correct token seg-
mentation is not known in advance. For method-
ological reasons, this issue has either been set-
aside (Tsarfaty and Sima?an, 2007), or dealt with
in a pipeline model in which a morphological dis-
ambiguator is run prior to parsing to determine the
correct segmentation. However, Tsarfaty (2006)
argues that there is a strong interaction between
syntax and morphological segmentation, and that
the two tasks should be modeled jointly, and not
in a pipeline model. Several studies followed this
line, (Cohen and Smith, 2007) the most recent of
which is Goldberg and Tsarfaty (2008), who pre-
sented a model based on unweighted lattice pars-
ing for performing the joint task.
This model uses a morphological analyzer to
construct a lattice over all possible morphologi-
cal analyses of an input sentence. The arcs of
the lattice are ?w, t? pairs, and a lattice parser
is used to build a parse over the lattice. The
Viterbi parse over the lattice chooses a lattice path,
which induces a segmentation over the input sen-
tence. Thus, parsing and segmentation are per-
formed jointly.
Lexical rules in the model are defined over the
lattice arcs (t? ?w, t?|t), and smoothed probabil-
ities for them are estimated from the treebank via
relative frequency over terminal/preterminal pairs.
The lattice paths themselves are unweighted, re-
flecting the intuition that all morphological anal-
yses are a-priori equally likely, and that their per-
spective strengths should come from the segments
they contain and their interaction with the syntax.
Goldberg and Tsarfaty (2008) use a data-driven
morphological analyzer derived from the treebank.
Their better models incorporated some external
lexical knowledge by use of an Hebrew spell
checker to prune some illegal segmentations.
In what follows, we use the layered represen-
tation to adapt this joint model to use as its mor-
phological analyzer the wide coverage KC Ana-
lyzer in enhancement of a data-driven one. Then,
we further enhance the model with the semi-
supervised lexical probabilities described in Sec 3.
5.1 Model
The model of Goldberg and Tsarfaty (2008) uses a
morphological analyzer to constructs a lattice for
each input token. Then, the sentence lattice is built
by concatenating the individual token lattices. The
morphological analyzer used in that work is data
driven based on treebank observations, and em-
ploys some well crafted heuristics for OOV tokens
(for details, see the original paper). Here, we use
instead a morphological analyzer which uses the
KC Lexicon for rare and OOV tokens.
We begin by adapting the rare vs. reliable events
distinction from Section 4 to cover unsegmented
tokens. We define a reliable token to be a token
from the training corpus, which each of its possi-
ble segments according to the training corpus was
seen in the training corpus at least K times.14 All
other tokens are considered to be rare.
Our morphological analyzer works as follows:
For reliable tokens, it returns the set of analyses
seen for this token in the treebank (each analysis
is a sequence of pairs of the form ?w, tTB?).
For rare tokens, it returns the set of analyses re-
turned by the KC analyzer (here, analyses are se-
quences of pairs of the form ?w, tKC?).
The lattice arcs, then, can take two possible
forms, either ?w, tTB? or ?w, tKC?.
Lexical rules of the form tTB ? ?w, tTB? are reli-
able, and their probabilities estimated via relative
frequency over events seen in training.
Lexical rules of the form tTB ? ?w, tKC?
are estimated in accordance with the transfer
layer introduced above: p(tTB ? ?w, tKC?) =
p(tKC |tTB)p(?w, tKC?|tKC).
The remaining question is how to estimate
p(?w, tKC?|tKC). Here, we use either the LexFil-
ter (estimated over all rare events) or LexProbs
(estimated via the semisupervised emission prob-
abilities)models, as defined in Section 4.1 above.
5.2 Experiments
As our Baseline, we take the best model of (Gold-
berg and Tsarfaty, 2008), run against the current
14Note that this is more inclusive than requiring that the
token itself is seen in the training corpus at least K times, as
some segments may be shared by several tokens.
333
version of the Treebank.15 This model uses the
same grammar as described in Section 4.1 above,
and use some external information in the form of a
spell-checker wordlist. We compare this Baseline
with the LexFilter and LexProbs models over the
Layered representation.
We use the same test/train splits as described in
Section 4. Contrary to the Oracle segmentation
setting, here we evaluate against all sentences, in-
cluding those containing tokens for which the KC
Analyzer does not contain any correct analyses.
Due to token segmentation ambiguity, the re-
sulting parse yields may be different than the gold
ones, and evalb can not be used. Instead, we use
the evaluation measure of (Tsarfaty, 2006), also
used in (Goldberg and Tsarfaty, 2008), which is
an adaptation of parseval to use characters instead
of space-delimited tokens as its basic units.
Results and Discussion
Results are presented in Table 3.
rare: < 2 rare: < 10
Prec Rec Prec Rec
Baseline 67.71 66.35 ? ?
LexFilter 68.25 69.45 57.72 59.17
LexProbs 73.40 73.99 70.09 73.01
Table 3: Parsing results for the joint parsing+seg
task, with varying external knowledge
The results are expectedly lower than with the
segmentation Oracle, as the joint task is much
harder, but the external lexical information greatly
benefits the parser also in the joint setting. While
significant, the improvement from the Baseline to
LexFilter is quite small, which is due to the Base-
line?s own rather strong illegal analyses filtering
heuristic. However, unlike the oracle segmenta-
tion case, here the semisupervised lexical prob-
abilities (LexProbs) have a major effect on the
parser performance (? 69 to ? 73.5 F-score), an
overall improvement of ? 6.6 F-points over the
Baseline, which is the previous state-of-the art for
this joint task. This supports our intuition that rare
lexical events are better estimated using a large
unannotated corpus, and not using a generic tree-
bank distribution, or sparse treebank based counts,
and that lexical probabilities have a crucial role in
resolving segmentation ambiguities.
15While we use the same software as (Goldberg and Tsar-
faty, 2008), the results reported here are significantly lower.
This is due to differences in annotation scheme between V1
and V2 of the Hebrew TB
The parsers with the extended lexicon were un-
able to assign a parse to about 10 of the 483 test
sentences. We count them as having 0-Fscore
in the table results.16 The Baseline parser could
not assign a parse to more than twice that many
sentences, suggesting its lexical pruning heuris-
tic is quite harsh. In fact, the unparsed sen-
tences amount to most of the difference between
the Baseline and LexFilter parsers.
Here, changing the rare tokens threshold has
a significant effect on parsing accuracy, which
suggests that the segmentation for rare tokens is
highly consistent within the corpus. When an un-
known token is encountered, a clear bias should
be taken toward segmentations that were previ-
ously seen in the same corpus. Given that that ef-
fect is remedied to some extent by introducing the
semi-supervised lexical probabilities, we believe
that segmentation accuracy for unseen tokens can
be further improved, perhaps using resources such
as (Gabay et al, 2008), and techniques for incor-
porating some document, as opposed to sentence
level information, into the parsing process.
6 Conclusions
We present a framework for interfacing a parser
with an external lexicon following a differ-
ent annotation scheme. Unlike other studies
(Yang Huang et al, 2005; Szolovits, 2003) in
which such interfacing is achieved by a restricted
heuristic mapping, we propose a novel, stochastic
approach, based on a layered representation. We
show that using an external lexicon for dealing
with rare lexical events greatly benefits a PCFG
parser for Hebrew, and that results can be further
improved by the incorporation of lexical probabil-
ities estimated in a semi-supervised manner using
a wide-coverage lexicon and a large unannotated
corpus. In the future, we plan to integrate this
framework with a parsing model that is specifi-
cally crafted to cope with morphologically rich,
free-word order languages, as proposed in (Tsar-
faty and Sima?an, 2008).
Apart from Hebrew, our method is applicable
in any setting in which there exist a small tree-
bank and a wide-coverage lexical resource. For
example parsing Arabic using the Arabic Tree-
bank and the Buckwalter analyzer, or parsing En-
glish biomedical text using a biomedical treebank
and the UMLS Specialist Lexicon.
16When discarding these sentences from the test set, result
on the better LexProbs model leap to 74.95P/75.56R.
334
References
M. Adler and M. Elhadad. 2006. An unsupervised
morpheme-based hmm for hebrew morphological
disambiguation. In Proc. of COLING/ACL2006.
Meni Adler, Yoav Goldberg, David Gabay, and
Michael Elhadad. 2008a. Unsupervised lexicon-
based resolution of unknown words for full morpho-
logical analysis. In Proc. of ACL 2008.
Meni Adler, Yael Netzer, David Gabay, Yoav Goldberg,
and Michael Elhadad. 2008b. Tagging a hebrew
corpus: The case of participles. In Proc. of LREC
2008.
Meni Adler. 2007. Hebrew Morphological Disam-
biguation: An Unsupervised Stochastic Word-based
Approach. Ph.D. thesis, Ben-Gurion University of
the Negev, Beer-Sheva, Israel.
Eugene Charniak, Glenn Carroll, John Adcock, An-
thony Cassandra, Yoshihiko Gotoh, Jeremy Katz,
Michael Littman, and John McCann. 1996. Taggers
for parsers. Artif. Intell., 85(1-2):45?57.
Shay B. Cohen and Noah A. Smith. 2007. Joint mor-
phological and syntactic disambiguation. In Pro-
ceedings of EMNLP-CoNLL-07, pages 208?217.
David Gabay, Ziv Ben Eliahu, and Michael Elhadad.
2008. Using wikipedia links to construct word seg-
mentation corpora. In Proc. of the WIKIAI-08 Work-
shop, AAAI-2008 Conference.
Yoav Goldberg and Reut Tsarfaty. 2008. A single gen-
erative model for joint morphological segmentation
and syntactic parsing. In Proc. of ACL 2008.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. Em can find pretty good hmm pos-taggers
(when given a good start). In Proc. of ACL 2008.
Noemie Guthmann, Yuval Krymolowski, Adi Milea,
and Yoad Winter. 2009. Automatic annotation of
morpho-syntactic dependencies in a modern hebrew
treebank. In Proc. of TLT.
Mohamed Maamouri, Ann Bies, and Seth Kulick.
2008. Enhanced annotation and parsing of the ara-
bic treebank. In INFOS 2008, Cairo, Egypt, March
27-29, 2008.
Yael Netzer, Meni Adler, David Gabay, and Michael
Elhadad. 2007. Can you tag the modal? you should!
In ACL07 Workshop on Computational Approaches
to Semitic Languages, Prague, Czech.
K. Sima?an, A. Itai, Y. Winter, A. Altman, and N. Nativ.
2001. Building a tree-bank of modern hebrew text.
Traitement Automatique des Langues, 42(2).
P. Szolovits. 2003. Adding a medical lexicon to an
english parser. In Proc. AMIA 2003 Annual Sympo-
sium.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morpholog-
ically rich languages. In Proc. of IWPT 2007.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
realizational parsing. In Proc. of CoLING, pages
889?896, Manchester, UK, August. Coling 2008.
Reut Tsarfaty. 2006. Integrated Morphological and
Syntactic Disambiguation for Modern Hebrew. In
Proceedings of ACL-SRW-06.
MS Yang Huang, MD Henry J. Lowe, PhD Dan Klein,
and MS Russell J. Cucina, MD. 2005. Improved
identification of noun phrases in clinical radiology
reports using a high-performance statistical natural
language parser augmented with the umls specialist
lexicon. J Am Med Inform Assoc, 12(3), May.
335
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 665?672,
Sydney, July 2006. c?2006 Association for Computational Linguistics
An Unsupervised Morpheme-Based HMM for Hebrew
Morphological Disambiguation
Meni Adler
Department of Computer Science
Ben Gurion University of the Negev
84105 Beer Sheva, Israel
adlerm@cs.bgu.ac.il
Michael Elhadad
Department of Computer Science
Ben Gurion University of the Negev
84105 Beer Sheva, Israel
elhadad@cs.bgu.ac.il
Abstract
Morphological disambiguation is the pro-
cess of assigning one set of morphologi-
cal features to each individual word in a
text. When the word is ambiguous (there
are several possible analyses for the word),
a disambiguation procedure based on the
word context must be applied. This paper
deals with morphological disambiguation
of the Hebrew language, which combines
morphemes into a word in both agglutina-
tive and fusional ways. We present an un-
supervised stochastic model ? the only re-
source we use is a morphological analyzer ?
which deals with the data sparseness prob-
lem caused by the affixational morphology
of the Hebrew language.
We present a text encoding method for
languages with affixational morphology in
which the knowledge of word formation
rules (which are quite restricted in He-
brew) helps in the disambiguation. We
adapt HMM algorithms for learning and
searching this text representation, in such
a way that segmentation and tagging can
be learned in parallel in one step. Results
on a large scale evaluation indicate that
this learning improves disambiguation for
complex tag sets. Our method is applicable
to other languages with affix morphology.
1 Introduction
Morphological disambiguation is the process of as-
signing one set of morphological features to each
individual word in a text, according to the word
context.
In this work, we investigate morphological dis-
ambiguation in Modern Hebrew. We explore unsu-
pervised learning method, which is more challeng-
ing than the supervised case. The main motivation
for this approach is that despite the development
?This work is supported by the Lynn and William
Frankel Center for Computer Sciences, and by the
Knowledge Center for Hebrew Processing, Israel Sci-
ence Ministry.
of annotated corpora in Hebrew1, there is still not
enough data available for supervised training. The
other reason, is that unsupervised methods can
handle the dynamic nature of Modern Hebrew, as
it evolves over time.
In the case of English, because morphology is
simpler, morphological disambiguation is generally
covered under the task of part-of-speech tagging.
The main morphological variations are embedded
in the tag name (for example, Ns and Np for
noun singular or plural). The tagging accuracy
of supervised stochastic taggers is around 96%-
97% (Manning and Schutze, 1999, 10.6.1). Meri-
aldo (1994) reports an accuracy of 86.6% for an un-
supervised word-based HMM, trained on a corpus
of 42,186 sentences (about 1M words), over a tag
set of 159 different tags. Elworthy (1994), in con-
trast, reports an accuracy of 75.49%, 80.87% and
79.12% for unsupervised word-based HMM trained
on parts of the LOB corpora, with a tagset of
134 tags. With good initial conditions, such as
good approximation of the tag distribution for each
word, Elworthy reports an improvement to 94.6%,
92.27% and 94.51% on the same data sets. Meri-
aldo, on the other hand, reports an improvement
to 92.6% and 94.4% for the case where 100 and
2000 sentences of the training corpus are manually
tagged.
Modern Hebrew is characterized by rich mor-
phology, with a high level of ambiguity. On aver-
age, in our corpus, the number of possible analyses
per word reached 2.4 (in contrast to 1.4 for En-
glish). In Hebrew, several morphemes combine into
a single word in both agglutinative and fusional
ways. This results in a potentially high number of
tags for each word.
In contrast to English tag sets whose sizes range
from 48 to 195, the number of tags for Hebrew,
based on all combinations of the morphological
attributes (part-of-speech, gender, number, per-
son, tense, status, and the affixes? properties2),
1The Knowledge Center for Hebrew processing is
developing such corpora: http://mila.cs.technion.ac.il/
2The list of morphological attributes is described in
(Yona and Wintner, 2005). An in-depth discussion of
the Hebrew word form is provided in (Allon, 1995, pp.
665
can grow theoretically to about 300,000 tags. In
practice, we found only 1,934 tags in a corpus of
news stories we gathered, which contains about 6M
words.
The large size of such a tag set (about 10 times
larger than the most comprehensive English tag
set) is problematic in term of data sparseness.
Each morphological combination appears rarely,
and more samples are required in order to learn
the probabilistic model.
In this paper, we hypothesize that the large set
of morphological features of Hebrew words, should
be modeled by a compact morpheme model, based
on the segmented words (into prefix, baseform, and
suffix). Our main result is that best performance
is obtained when learning segmentation and mor-
pheme tagging in one step, which is made possible
by an appropriate text representation.
2 Hebrew and Arabic Tagging -
Previous Work
Several works have dealt with Hebrew tagging in
the past decade. In Hebrew, morphological anal-
ysis requires complex processing according to the
rules of Hebrew word formation. The task of a
morphological analyzer is to produce all possible
analyses for a given word. Recent analyzers pro-
vide good performance and documentation of this
process (Yona and Wintner, 2005; Segal, 2000).
Morphological analyzers rely on a dictionary, and
their performance is, therefore, impacted by the oc-
currence of unknown words. The task of a morpho-
logical disambiguation system is to pick the most
likely analysis produced by an analyzer in the con-
text of a full sentence.
Levinger et al (1995) developed a context-free
method in order to acquire the morpho-lexical
probabilities, from an untagged corpus. Their
method handles the data sparseness problem by
using a set of similar words for each word, built
according to a set of rules. The rules produce vari-
ations of the morphological properties of the word
analyses. Their tests indicate an accuracy of about
88% for context-free analysis selection based on the
approximated analysis distribution. In tests we re-
produced on a larger data set (30K tagged words),
the accuracy is only 78.2%. In order to improve
the results, the authors recommend merging their
method together with other morphological disam-
biguation methods ? which is the approach we pur-
sue in this work.
Levinger?s morphological disambiguation sys-
tem (Levinger, 1992) combines the above approx-
imated probabilities with an expert system, based
on a manual set of 16 syntactic constraints . In
the first phase, the expert system is applied, dis-
24?86).
ambiguating 35% of the ambiguous words with an
accuracy of 99.6%. In order to increase the applica-
bility of the disambiguation, approximated proba-
bilities are used for words that were not disam-
biguated in the first stage. Finally, the expert sys-
tem is used again over the new probabilities that
were set in the previous stage. Levinger reports
an accuracy of about 94% for disambiguation of
85% of the words in the text (overall 80% disam-
biguation). The system was also applied to prune
out the least likely analyses in a corpus but with-
out, necessarily, selecting a single analysis for each
word. For this task, an accuracy of 94% was re-
ported while reducing 92% of the ambiguous anal-
yses.
Carmel and Maarek (1999) use the fact that
on average 45% of the Hebrew words are unam-
biguous, to rank analyses, based on the number
of disambiguated occurrences in the text, normal-
ized by the total number of occurrences for each
word. Their application ? indexing for an informa-
tion retrieval system ? does not require all of the
morphological attributes but only the lemma and
the PoS of each word. As a result, for this case,
75% of the words remain with one analysis with
95% accuracy, 20% with two analyses and 5% with
three analyses.
Segal (2000) built a transformation-based tag-
ger in the spirit of Brill (1995). In the first phase,
the analyses of each word are ranked according to
the frequencies of the possible lemmas and tags in
a training corpus of about 5,000 words. Selection
of the highest ranked analysis for each word gives
an accuracy of 83% of the test text ? which con-
sists of about 1,000 words. In the second stage,
a transformation learning algorithm is applied (in
contrast to Brill, the observed transformations are
not applied, but used for re-estimation of the word
couples probabilities). After this stage, the accu-
racy is about 93%. The last stage uses a bottom-
up parser over a hand-crafted grammar with 150
rules, in order to select the analysis which causes
the parsing to be more accurate. Segal reports an
accuracy of 95%. Testing his system over a larger
test corpus, gives poorer results: Lembersky (2001)
reports an accuracy of about 85%.
Bar-Haim et al (2005) developed a word seg-
menter and PoS tagger for Hebrew. In their archi-
tecture, words are first segmented into morphemes,
and then, as a second stage, these morphemes are
tagged with PoS. The method proceeds in two
sequential steps: segmentation into morphemes,
then tagging over morphemes. The segmentation
is based on an HMM and trained over a set of 30K
annotated words. The segmentation step reaches
an accuracy of 96.74%. PoS tagging, based on un-
supervised estimation which combines a small an-
notated corpus with an untagged corpus of 340K
666
Word Segmentation Tag Translation
bclm bclm PNN name of a human rights association (Betselem)
bclm bclm VB while taking a picture
bclm bcl-m cons-NNM-suf their onion
bclm b-cl-m P1-NNM-suf under their shadow
bclm b-clm P1-NNM in a photographer
bclm b-clm P1-cons-NNM in a photographer
bclm b-clm P1-h-NNM in the photographer
hn?im h-n?im P1-VBR that are moving
hn?im hn?im P1-h-JJM the lovely
hn?im hn?im VBP made pleasant
Table 1: Possible analyses for the words bclm hn?im
words by using smoothing technique, gives an ac-
curacy of 90.51%.
As noted earlier, there is as yet no large scale
Hebrew annotated corpus. We are in the process
of developing such a corpus, and we have devel-
oped tagging guidelines (Elhadad et al, 2005) to
define a comprehensive tag set, and assist human
taggers achieve high agreement. The results dis-
cussed above should be taken as rough approxima-
tions of the real performance of the systems, until
they can be re-evaluated on such a large scale cor-
pus with a standard tag set.
Arabic is a language with morphology quite sim-
ilar to Hebrew. Theoretically, there might be
330,000 possible morphological tags, but in prac-
tice, Habash and Rambow (2005) extracted 2,200
different tags from their corpus, with an average
number of 2 possible tags per word. As reported
by Habash and Rambow, the first work on Arabic
tagging which used a corpus for training and eval-
uation was the work of Diab et al (2004). Habash
and Rambow were the first to use a morphological
analyzer as part of their tagger. They developed a
supervised morphological disambiguator, based on
training corpora of two sets of 120K words, which
combines several classifiers of individual morpho-
logical features. The accuracy of their analyzer
is 94.8% ? 96.2% (depending on the test corpus).
An unsupervised HMM model for dialectal Ara-
bic (which is harder to be tagged than written
Arabic), with accurracy of 69.83%, was presented
by Duh and Kirchhoff (2005). Their supervised
model, trained on a manually annotated corpus,
reached an accuracy of 92.53%.
Arabic morphology seems to be similar to He-
brew morphology, in term of complexity and data
sparseness, but comparison of the performances
of the baseline tagger used by Habash and Ram-
bow ? which selects the most frequent tag for a
given word in the training corpus ? for Hebrew and
Arabic, shows some intriguing differences: 92.53%
for Arabic and 71.85% for Hebrew. Furthermore,
as mentioned above, even the use of a sophisti-
cated context-free tagger, based on (Levinger et
al., 1995), gives low accuracy of 78.2%. This might
imply that, despite the similarities, morphological
disambiguation in Hebrew might be harder than in
Arabic. It could also mean that the tag set used
for the Arabic corpora has not been adapted to the
specific nature of Arabic morphology (a comment
also made in (Habash and Rambow, 2005)).
We propose an unsupervised morpheme-based
HMM to address the data sparseness problem. In
contrast to Bar-Haim et al, our model combines
segmentation and morphological disambiguation,
in parallel. The only resource we use in this work is
a morphological analyzer. The analyzer itself can
be generated from a word list and a morphologi-
cal generation module, such as the HSpell wordlist
(Har?el and Kenigsberg, 2004).
3 Morpheme-Based Model for
Hebrew
3.1 Morpheme-Based HMM
The lexical items of word-based models are the
words of the language. The implication of this
decision is that both lexical and syntagmatic re-
lations of the model, are based on a word-oriented
tagset. With such a tagset, it must be possible to
tag any word of the language with at least one tag.
Let us consider, for instance, the Hebrew phrase
bclm hn?im3, which contains two words. The word
bclm has several possible morpheme segmentations
and analyses4 as described in Table 1. In word-
based HMM, we consider such a phrase to be gen-
erated by a Markov process, based on the word-
oriented tagset of N = 1934 tags/states and about
M = 175K word types. Line W of Table 2 de-
scribes the size of a first-order word-based HMM,
built over our corpus. In this model, we found 834
entries for the ? vector (which models the distri-
bution of tags in first position in sentences) out of
possibly N = 1934, about 250K entries for the A
matrix (which models the transition probabilities
from tag to tag) out of possibly N 2 ? 3.7M , and
about 300K entries for the B matrix (which models
3Transcription according to Ornan (2002).
4The tagset we use for the annotation follows the
guidelines we have developed (Elhadad et al, 2005).
667
States PI A A2 B B2
W 1934 834 250K 7M 300K 5M
M 202 145 20K 700K 130K 1.7M
Table 2: Model Sizes
the emission probabilities from tag to word) out of
possibly M ?N ? 350M . For the case of a second-
order HMM, the size of the A2 matrix (which mod-
els the transition probabilities from two tags to the
third one), grows to about 7M entries, where the
size of the B2 matrix (which models the emission
probabilities from two tags to a word) is about 5M.
Despite the sparseness of these matrices, the num-
ber of their entries is still high, since we model the
whole set of features of the complex word forms.
Let us assume, that the right segmentation for
the sentence is provided to us ? for example: b
clm hn?im ? as is the case for English text. In
such a way, the observation is composed of mor-
phemes, generated by a Markov process, based
on a morpheme-based tagset. The size of such a
tagset for Hebrew is about 200, where the size of
the ?,A,B,A2 and B2 matrices is reduced to 145,
16K, 140K, 700K, and 1.7M correspondingly, as
described in line M of Table 2 ? a reduction of
90% when compared with the size of a word-based
model.
The problem in this approach, is that ?someone?
along the way, agglutinates the morphemes of each
word leaving the observed morphemes uncertain.
For example, the word bclm can be segmented in
four different ways in Table 1, as indicated by the
placement of the ?-? in the Segmentation column,
while the word hn?im can be segmented in two dif-
ferent ways. In the next section, we adapt the pa-
rameter estimation and the searching algorithms
for such uncertain output observation.
3.2 Learning and Searching Algorithms
for Uncertain Output Observation
In contrast to standard HMM, the output observa-
tions of the above morpheme-based HMM are am-
biguous. We adapted Baum-Welch (Baum, 1972)
and Viterbi (Manning and Schutze, 1999, 9.3.2) al-
gorithms for such uncertain observation. We first
formalize the output representation and then de-
scribe the algorithms.
Output Representation The learning and
searching algorithms of HMM are based on the
output sequence of the underlying Markov pro-
cess. For the case of a morpheme-based model,
the output sequence is uncertain ? we don?t see the
emitted morphemes but the words they form. If,
for instance, the Markov process emitted the mor-
phemes b clm h n?im, we would see two words (bclm
hn?im) instead. In order to handle the output am-
biguity, we use static knowledge of how morphemes
are combined into a word, such as the four known
combinations of the word bclm, the two possible
combinations of the word hn?im, and their possi-
ble tags within the original words. Based on this
information, we encode the sentence into a struc-
ture that represents all the possible ?readings? of
the sentence, according to the possible morpheme
combinations of the words, and their possible tags.
The representation consists of a set of vectors,
each vector containing the possible morphemes and
their tags for each specific ?time? (sequential posi-
tion within the morpheme expansion of the words
of the sentence). A morpheme is represented by
a tuple (symbol, state, prev, next), where symbol
denotes a morpheme, state is one possible tag for
this morpheme, prev and next are sets of indexes,
denoting the indexes of the morphemes (of the pre-
vious and the next vectors) that precede and follow
the current morpheme in the overall lattice, repre-
senting the sentence. Fig. 2 describes the repre-
sentation of the sentence bclm hn?im. An emission
is denoted in this figure by its symbol, its state
index, directed edges from its previous emissions,
and directed edges to its next emissions.
In order to meet the condition of Baum-Eagon
inequality (Baum, 1972) that the polynomial
P (O|?) ? which represents the probability of an
observed sequence O given a model ? ? be homo-
geneous, we must add a sequence of special EOS
(end of sentence) symbols at the end of each path
up to the last vector, so that all the paths reach
the same length.
The above text representation can be used to
model multi-word expressions (MWEs). Consider
the Hebrew sentence: hw? ?wrk dyn gdwl, which can
be interpreted as composed of 3 units (he lawyer
great / he is a great lawyer) or as 4 units (he edits
law big / he is editing an important legal deci-
sion). In order to select the correct interpretation,
we must determine whether ?wrk dyn is an MWE.
This is another case of uncertain output observa-
tion, which can be represented by our text encod-
ing, as done in Fig. 1.
?wrk dyn 6 gdwl 19 EOS 17 EOS 17
dyn 6 gdwl 19?wrk 18
hw? 20
Figure 1: The sentence hw? ?wrk dyn gdwl
This representation seems to be expensive in
term of the number of emissions per sentence.
However, we observe in our data that most of the
words have only one or two possible segmentations,
and most of the segmentations consist of at most
one affix. In practice, we found the average number
of emissions per sentence in our corpus (where each
symbol is counted as the number of its predecessor
emissions) to be 455, where the average number
of words per sentence is about 18. That is, the
668
cost of operating over an ambiguous sentence rep-
resentation increases the size of the sentence (from
18 to 455), but on the other hand, it reduces the
probabilistic model by a factor of 10 (as discussed
above).
Morphological disambiguation over such a se-
quence of vectors of uncertain morphemes is similar
to words extraction in automatic speech recogni-
tion (ASR)(Jurafsky and Martin, 2000, chp. 5,7).
The states of the ASR model are phones, where
each observation is a vector of spectral features.
Given a sequence of observations for a sentence,
the encoding ? based on the lattice formed by the
phones distribution of the observations, and the
language model ? searches for the set of words,
made of phones, which maximizes the acoustic like-
lihood and the language model probabilities. In a
similar manner, the supervised training of a speech
recognizer combines a training corpus of speech
wave files, together with word-transcription, and
language model probabilities, in order to learn the
phones model.
There are two main differences between the typi-
cal ASR model and ours: (1) an ASR decoder deals
with one aspect - segmentation of the observations
into a set of words, where this segmentation can
be modeled at several levels: subphones, phones
and words. These levels can be trained individ-
ually (such as training a language model from a
written corpus, and training the phones model for
each word type, given transcripted wave file), and
then combined together (in a hierarchical model).
Morphological disambiguation over uncertain mor-
phemes, on the other hand, deals with both mor-
pheme segmentation and the tagging of each mor-
pheme with its morphological features. Model-
ing morpheme segmentation, within a given word,
without its morphology features would be insuf-
ficient. (2) The supervised resources of ASR are
not available for morphological disambiguation: we
don?t have a model of morphological features se-
quences (equivalent to the language model of ASR)
nor a tagged corpus (equivalent to the transcripted
wave files of ASR).
These two differences require a design which
combines the two dimensions of the problem, in or-
der to support unsupervised learning (and search-
ing) of morpheme sequences and their morpholog-
ical features, simultaneously.
Parameter Estimation We present a variation
of the Baum-Welch algorithm (Baum, 1972) which
operates over the lattice representation we have de-
fined above. The algorithm starts with a proba-
bilistic model ? (which can be chosen randomly
or obtained from good initial conditions), and at
each iteration, a new model ?? is derived in order to
better explain the given output observations. For a
given sentence, we define T as the number of words
in the sentence, and T? as the number of vectors of
the output representation O = {ot}, 1 ? t ? T? ,
where each item in the output is denoted by olt =
(sym, state, prev, next), 1 ? t ? T? , 1 ? l ? |ot|.
We define ?(t, l) as the probability to reach olt at
time t, and ?(t, l) as the probability to end the se-
quence from olt. Fig. 3 describes the expectation
and the maximization steps of the learning algo-
rithm for a first-order HMM. The algorithm works
in O(T? ) time complexity, where T? is the total num-
ber of symbols in the output sequence encoding,
where each symbol is counted as the size of its prev
set.
Searching for best state sequence The
searching algorithm gets an observation sequence
O and a probabilistic model ?, and looks for the
best state sequence that generates the observation.
We define ?(t, l) as the probability of the best state
sequence that leads to emission olt, and ?(t, l) as
the index of the emission at time t?1 that precedes
olt in the best state sequence that leads to it. Fig. 4
describes the adaptation of the Viterbi (Manning
and Schutze, 1999, 9.3.2) algorithm to our text rep-
resentation for first-order HMM, which works in
O(T? ) time.
4 Experimental Results
We ran a series of experiments on a Hebrew corpus
to compare various approaches to the full morpho-
logical disambiguation and PoS tagging tasks. The
training corpus is obtained from various newspa-
per sources and is characterized by the following
statistics: 6M word occurrences, 178,580 distinct
words, 64,541 distinct lemmas. Overall, the ambi-
guity level is 2.4 (average number of analyses per
word).
We tested the results on a test corpus, manually
annotated by 2 taggers according to the guidelines
we published and checked for agreement. The test
corpus contains about 30K words. We compared
two unsupervised models over this data set: Word
model [W], and Morpheme model [M]. We also
tested two different sets of initial conditions. Uni-
form distribution [Uniform]: For each word, each
analysis provided by the analyzer is estimated with
an equal likelihood. Context Free approximation
[CF]: We applied the CF algorithm of Levinger et
al.(1995) to estimate the likelihood of each analy-
sis.
Table 3 reports the results of full morphologi-
cal disambiguation. For each morpheme and word
models, three types of models were tested: [1]
First-order HMM, [2-] Partial second-order HMM -
only state transitions were modeled (excluding B2
matrix), [2] Second-order HMM (including the B2
matrix).
Analysis If we consider the tagger which selects
the most probable morphological analysis for each
669
clm 7
m 3
n?im 16
clm 10
cl 9
hn?im 14
hn?im 15
h 2
n?im 16
h 2
EOS 17
clm 8
hn?im 11
hn?im 12
m 4
hn?im 14
hn?im 15
h 2
hn?im 11
hn?im 12
EOS 17
hn?im 14
hn?im 15
hn?im 11
hn?im 12
EOS 17
n?im 16
EOS 17
bcl 6
b 1
bclm 5
b 0
Figure 2: Representation of the sentence bclm hn?im
Expectation
?(1, l) = piol1.statebol1.state,ol1.sym (1)
?(t, l) = bolt.state,olt.sym
?
l??olt.prev
?(t? 1, l?)aol?t?1.state,olt.state
?(T? , l) = 1 (2)
?(t, l) =
?
l??olt.next
aolt.state,ol?t+1.statebol?t+1.state,ol?t+1.sym?(t+ 1, l
?)
Maximization
?i =
?
l:ol1.state=i ?(1, l)?(1, l)
?
l ?(1, l)?(1, l)
(3)
a?i,j =
?T?
t=2
?
l:olt.state=j
?
l??olt.prev:ol
?
t?1.state=i
?(t? 1, l?)ai,jbj,olt.sym?(t, l)
?T??1
t=1
?
l:olt.state=i ?(t, l)?(t, l)
(4)
b?i,k =
?T?
t=1
?
l:olt.sym=k,olt.state=i ?(t, l)?(t, l)
?T?
t=1
?
l:olt.state=i ?(t, l)?(t, l)
(5)
Figure 3: The learning algorithm for first-order model
Initialization
?(1, l) = piol1.statebol1.state,ol1.sym (6)
Induction
?(t, l) = max
l??olt.prev
?(t? 1, l?)aol?t?1.state,olt.statebolt.state,olt.sym (7)
?(t, l) = argmax
l??olt.prev
?(t? 1, l?)aol?t?1.state,olt.statebolt.state,olt.sym (8)
Termination and path readout
X?T? = argmax1?l?|T? | ?(T? , l) (9)
X?t = ?(t+ 1, X?t+1)
P (X?) = max
1?l?|OT? |
?(T? , l) (10)
Figure 4: The searching algorithm for first-order model
670
Order Uniform CF
W 1 82.01 84.08
W 2- 80.44 85.75
W 2 79.88 85.78
M 1 81.08 84.54
M 2- 81.53 88.5
M 2 83.39 85.83
Table 3: Morphological Disambiguation
word in the text, according to Levinger et al (1995)
approximations, with accuracy of 78.2%, as the
baseline tagger, four steps of error reduction can
be identified. (1) Contextual information: The
simplest first-order word-based HMM with uniform
initial conditions, achieves error reduction of 17.5%
(78.2 ? 82.01). (2) Initial conditions: Error reduc-
tions in the range: 11.5% ? 37.8% (82.01 ? 84.08
for word model 1, and 81.53 ? 88.5 for morhpeme
model 2-) were achieved by initializing the various
models with context-free approximations. While
this observation confirms Elworthy (1994), the im-
pact of error reduction is much less than reported
there for English - about 70% (79 ? 94). The key
difference (beside the unclear characteristic of El-
worthy initial condition - since he made use of an
annotated corpus) is the much higher quality of the
uniform distribution for Hebrew. (3) Model order:
The partial second-order HMM [2-] produced the
best results for both word (85.75%) and morpheme
(88.5%) models over the initial condition. The full
second-order HMM [2] didn?t upgrade the accu-
racy of the partial second-order, but achieved the
best results for the uniform distribution morpheme
model. This is because the context-free approxima-
tion does not take into account the tag of the previ-
ous word, which is part of model 2. We believe that
initializing the morpheme model over a small set of
annotated corpus will set much stronger initial con-
dition for this model. (4) Model type: The main
result of this paper is the error reduction of the
morpheme model with respect to the word model:
about 19.3% (85.75 ? 88.5).
In addition, we apply the above models for the
simpler task of segmentation and PoS tagging, as
reported in Table 4. The task requires picking the
correct morphemes of each word with their correct
PoS (excluding all other morphological features).
The best result for this task is obtained with the
morpheme model 2: 92.32%. For this simpler task,
the improvement brought by the morpheme model
over the word model is less significant, but still
consists of a 5% error reduction.
Unknown words account for a significant
chunk of the errors. Table 5 shows the distribution
of errors contributed by unknown words (words
that cannot be analyzed by the morphological an-
alyzer). 7.5% of the words in the test corpus are
unknown: 4% are not recognized at all by the mor-
phological analyzer (marked as [None] in the ta-
Order Uniform CF
W 1 91.07 91.47
W 2- 90.45 91.93
W 2 90.21 91.84
M 1 89.23 91.42
M 2- 89.77 91.76
M 2 91.42 92.32
Table 4: Segmentation and PoS Tagging
ble), and for 3.5%, the set of analyses proposed by
the analyzer does not contain the correct analy-
sis [Missing]. We extended the lexicon to include
missing and none lexemes of the closed sets. In
addition, we modified the analyzer to extract all
possible segmentations of unknown words, with all
the possible tags for the segmented affixes, where
the remaining unknown baseforms are tagged as
UK. The model was trained over this set. In the
next phase, the corpus was automatically tagged,
according to the trained model, in order to form a
tag distribution for each unknown word, according
to its context and its form. Finally, the tag for
each unknown word were selected according to its
tag distribution. This strategy accounts for about
half of the 7.5% unknown words.
None Missing %
Proper name 26 36 62
Closed Set 8 5.6 13.6
Other 16.5 5.4 21.9
Junk 2.5 0 2.5
53 47 100
Table 5: Unknown Word Distribution
Table 6 shows the confusion matrix for known
words (5% and up). The key confusions can be at-
tributed to linguistic properties of Modern Hebrew:
most Hebrew proper names are also nouns (and
they are not marked by capitalization) ? which ex-
plains the PN/N confusion. The verb/noun and
verb/adjective confusions are explained by the na-
ture of the participle form in Hebrew (beinoni) ?
participles behave syntactically almost in an iden-
tical manner as nouns.
Correct Error %
proper name noun 17.9
noun verb 15.3
noun proper name 6.6
verb noun 6.3
adjective noun 5.4
adjective verb 5.0
Table 6: Confusion Matrix for Known Words
5 Conclusions and Future Work
In this work, we have introduced a new text encod-
ing method that captures rules of word formation
in a language with affixational morphology such as
Hebrew. This text encoding method allows us to
671
learn in parallel segmentation and tagging rules in
an unsupervised manner, despite the high ambigu-
ity level of the morphological data (average num-
ber of 2.4 analyses per word). Reported results on
a large scale corpus (6M words) with fully unsu-
pervised learning are 92.32% for PoS tagging and
88.5% for full morphological disambiguation.
In this work, we used the backoff smoothing
method, suggested by Thede and Harper (1999),
with an extension of additive smoothing (Chen,
1996, 2.2.1) for the lexical probabilities (B and B2
matrices). To complete this study, we are currently
investigating several smoothing techniques (Chen,
1996), in order to check whether the morpheme
model is critical for the data sparseness problem,
or whether it can be handled with smoothing over
a word model.
We are currently investigating two major meth-
ods to improve our results: first, we have started
gathering a larger corpus of manually tagged text
and plan to perform semi-supervised learning on
a corpus of 100K manually tagged words. Second,
we plan to improve the unknown word model, such
as integrating it with named entity recognition sys-
tem (Ben-Mordechai, 2005).
References
Emmanuel Allon. 1995. Unvocalized Hebrew Writ-
ing. Ben Gurion University Press. (in Hebrew).
Roy Bar-Haim, Khalil Sima?an, and Yoad Winter.
2005. Choosing an optimal architecture for seg-
mentation and pos-tagging of modern Hebrew.
In Proceedings of ACL-05 Workshop on Compu-
tational Approaches to Semitic Languages.
Leonard E. Baum. 1972. An inequality and asso-
ciated maximization technique in statistical es-
timation for probabilistic functions of a Markov
process. Inequalities, 3:1?8.
Na?ama Ben-Mordechai. 2005. Named entities
recognition in Hebrew. Master?s thesis, Ben Gu-
rion University of the Negev, Beer Sheva, Israel.
(in Hebrew).
Eric Brill. 1995. Transformation-based error-
driven learning and natural languge processing:
A case study in part-of-speech tagging. Compu-
tational Linguistics, 21:543?565.
David Carmel and Yoelle S. Maarek. 1999. Mor-
phological disambiguation for Hebrew search
systems. In Proceeding of NGITS-99.
Stanley F. Chen. 1996. Building Probabilistic
Models for Natural Language. Ph.D. thesis, Har-
vard University, Cambridge, MA.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky.
2004. Automatic tagging of Arabic text: From
raw text to base phrase chunks. In Proceeding
of HLT-NAACL-04.
Kevin Duh and Katrin Kirchhoff. 2005. Pos tag-
ging of dialectal Arabic: A minimally supervised
approach. In Proceedings of ACL-05 Workshop
on Computational Approaches to Semitic Lan-
guages.
Michael Elhadad, Yael Netzer, David Gabay, and
Meni Adler. 2005. Hebrew morphological tag-
ging guidelines. Technical report, Ben Gurion
University, Dept. of Computer Science.
David Elworthy. 1994. Does Baum-Welch re-
estimation help taggers? In Proceeding of
ANLP-94.
Nizar Habash and Owen Rambow. 2005. Arabic
tokenization, part-of-speech tagging and mor-
phological disambiguation in one fell swoop. In
Proceeding of ACL-05.
Nadav Har?el and Dan Kenigsberg. 2004. HSpell
- the free Hebrew spell checker and morphologi-
cal analyzer. Israeli Seminar on Computational
Linguistics, December 2004.
Daniel Jurafsky and James H. Martin. 2000.
Speech and language processing. Prentice-Hall.
Gennady Lembersky. 2001. Named entities recog-
nition; compounds: approaches and recognitions
methods. Master?s thesis, Ben Gurion Univer-
sity of the Negev, Beer Sheva, Israel. (in He-
brew).
Moshe Levinger, Uzi Ornan, and Alon Itai. 1995.
Learning morpholexical probabilities from an
untagged corpus with an application to Hebrew.
Computational Linguistics, 21:383?404.
Moshe Levinger. 1992. Morhphological disam-
biguation in hebrew. Master?s thesis, Technion,
Haifa, Israel. (in Hebrew).
Christopher D. Manning and Hinrich Schutze.
1999. Foundation of Statistical Language Pro-
cessing. MIT Press.
Bernard Merialdo. 1994. Tagging English text
with probabilistic model. Computatinal Linguis-
tics, 20:155?171.
Uzi Ornan. 2002. Hebrew in latin script.
Le?s?one?nu, LXIV:137?151. (in Hebrew).
Erel Segal. 2000. Hebrew morphological ana-
lyzer for Hebrew undotted texts. Master?s the-
sis, Technion, Haifa, Israel. (in Hebrew).
Scott M. Thede and Mary P. Harper. 1999. A
second-order hidden Markov model for part-of-
speech tagging. In Proceeding of ACL-99.
Shlomo Yona and Shuly Wintner. 2005. A finite-
state morphological grammar of Hebrew. In
Proceedings of ACL-05 Workshop on Computa-
tional Approaches to Semitic Languages.
672
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 689?696,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Noun Phrase Chunking in Hebrew 
Influence of Lexical and Morphological Features 
 
 
Yoav Goldberg  and  Meni Adler  and  Michael Elhadad 
Computer Science Department 
Ben Gurion University of the Negev 
P.O.B 653 Be'er Sheva 84105, Israel  
{yoavg,adlerm,elhadad}@cs.bgu.ac.il 
 
  
 
Abstract 
We present a method for Noun Phrase 
chunking in Hebrew. We show that the 
traditional definition of base-NPs as non-
recursive noun phrases does not apply in 
Hebrew, and propose an alternative defi-
nition of Simple NPs.  We review syntac-
tic properties of Hebrew related to noun 
phrases, which indicate that the task of 
Hebrew SimpleNP chunking is harder 
than base-NP chunking in English. As a 
confirmation, we apply methods known 
to work well for English to Hebrew data. 
These methods give low results (F from 
76 to 86) in Hebrew. We then discuss our 
method, which applies SVM induction 
over lexical and morphological features. 
Morphological features improve the av-
erage precision by ~0.5%, recall by ~1%, 
and F-measure by ~0.75, resulting in a 
system with average performance of 93% 
precision, 93.4% recall and 93.2 F-
measure.* 
1 Introduction 
Modern Hebrew is an agglutinative Semitic lan-
guage, with rich morphology.  Like most other 
non-European languages, it lacks NLP resources 
and tools, and specifically there are currently no 
available syntactic parsers for Hebrew.  We ad-
dress the task of NP chunking in Hebrew as a 
                                                 
*
 This work was funded by the Israel Ministry of Sci-
ence and Technology under the auspices of the 
Knowledge Center for Processing Hebrew.  Addi-
tional funding was provided by the Lynn and William 
Frankel Center for Computer Sciences.  
first step to fulfill the need for such tools.  We 
also illustrate how this task can successfully be 
approached with little resource requirements, and 
indicate how the method is applicable to other 
resource-scarce languages. 
NP chunking is the task of labelling noun 
phrases in natural language text. The input to this 
task is free text with part-of-speech tags.  The 
output is the same text with brackets around base 
noun phrases.  A base noun phrase is an NP 
which does not contain another NP (it is not re-
cursive).  NP chunking is the basis for many 
other NLP tasks such as shallow parsing, argu-
ment structure identification, and information 
extraction 
We first realize that the definition of base-NPs 
must be adapted to the case of Hebrew (and 
probably other Semitic languages as well) to cor-
rectly handle its syntactic nature.  We propose 
such a definition, which we call simple NPs and 
assess the difficulty of chunking such NPs by 
applying methods that perform well in English to 
Hebrew data.  While the syntactic problem in 
Hebrew is indeed more difficult than in English, 
morphological clues do provide additional hints, 
which we exploit using an SVM learning 
method.  The resulting method reaches perform-
ance in Hebrew comparable to the best results 
published in English. 
2 Previous Work 
Text chunking (and NP chunking in particular), 
first proposed by Abney (1991), is a well studied 
problem for English. The CoNLL2000 shared 
task (Tjong Kim Sang et al, 2000) was general 
chunking. The best result achieved for the shared 
task data was by Zhang et al(2002), who 
achieved NP chunking results of 94.39% preci-
sion, 94.37% recall and 94.38 F-measure using a 
689
generalized Winnow algorithm, and enhancing 
the feature set with the output of a dependency 
parser. Kudo and Matsumoto (2000) used an 
SVM based algorithm, and achieved NP chunk-
ing results of 93.72% precision, 94.02% recall 
and 93.87 F-measure for the same shared task 
data, using only the words and their PoS tags. 
Similar results were obtained using Conditional 
Random Fields on similar features (Sha and 
Pereira, 2003). 
The NP chunks in the shared task data are 
base-NP chunks ? which are non-recursive NPs, 
a definition first proposed by Ramshaw and 
Marcus (1995). This definition yields good NP 
chunks for English, but results in very short and 
uninformative chunks for Hebrew (and probably 
other Semitic languages). 
Recently, Diab et al(2004) used SVM based 
approach for Arabic text chunking.  Their chunks 
data was derived from the LDC Arabic TreeBank 
using the same program that extracted the chunks 
for the shared task.  They used the same features 
as Kudo and Matsumoto (2000), and achieved 
over-all chunking performance of 92.06% preci-
sion, 92.09% recall and 92.08 F-measure (The 
results for NP chunks alone were not reported).  
Since Arabic syntax is quite similar to Hebrew, 
we expect that the issues reported below apply to 
Arabic results as well. 
3 Hebrew Simple NP Chunks 
The standard definition of English base-NPs is 
any noun phrase that does not contain another 
noun phrase, with possessives treated as a special 
case, viewing the possessive marker as the first 
word of a new base-NP (Ramshaw and Marcus, 
1995).  To evaluate the applicability of this defi-
nition to Hebrew, we tested this definition on the 
Hebrew TreeBank (Sima?an et al 2001) pub-
lished by the Hebrew Knowledge Center. We 
extracted all base-NPs from this TreeBank, 
which is similar in genre and contents to the 
English one.  This results in extremely simple 
chunks.  
 
English 
BaseNPs 
Hebrew 
BaseNPs 
Hebrew 
SimpleNPs 
Avg # of words 2.17 1.39 2.49 
% length 1 30.95 63.32 32.83 
% length 2 39.35 35.48 32.12 
% length 3 18.68 0.83 14.78 
% length 4 6.65 0.16 9.47 
% length 5 2.70 0.16 4.56 
% length > 5 1.67 0.05 6.22 
Table 1.  Size of Hebrew and English NPs 
Table 1 shows the average number of words in a 
base-NP for English and Hebrew.  The Hebrew 
chunks are basically one-word groups around 
Nouns, which is not useful for any practical pur-
pose, and so we propose a new definition for He-
brew NP chunks, which allows for some nested-
ness. We call our chunks Simple NP chunks.  
3.1 Syntax of NPs in Hebrew 
One of the reasons the traditional base-NP defi-
nition fails for the Hebrew TreeBank is related to 
syntactic features of Hebrew ? specifically, 
smixut (construct state ? used to express noun 
compounds), definite marker and the expression 
of possessives. These differences are reflected to 
some extent by the tagging guidelines used to 
annotate the Hebrew Treebank and they result in 
trees which are in general less flat than the Penn 
TreeBank ones.  
Consider the example base noun phrase [The 
homeless people]. The Hebrew equivalent is 
(1)  	
  
 which by the non-recursive NP definition will be 
bracketed as: 
   	
  , or, loosely translating 
back to English: [the home]less [people].  
In this case, the fact that the bound-morpheme 
less appears as a separate construct state word 
with its own definite marker (ha-) in Hebrew 
would lead the chunker to create two separate 
NPs for a simple expression.  We present below 
syntactic properties of Hebrew which are rele-
vant to NP chunking. We then present our defini-
tion of Simple NP Chunks.  
 
Construct State: The Hebrew genitive case is 
achieved by placing two nouns next to each other. 
This is called ?noun construct?, or smixut. The 
semantic interpretation of this construct is varied 
(Netzer and Elhadad, 1998), but it specifically 
covers possession. The second noun can be 
treated as an adjective modifying the next noun. 
The first noun is morphologically marked in a 
form known as the construct form (denoted by 
const). The definite article marker is placed on 
the second word of the construction: 
(2)  
 beit sefer / house-[const] book 
 School 
(3)  
 beit ha-sefer / house-[const] the-book 
 The school 
 
The construct form can also be embedded: 
(4) 	


 
690
misrad ro$ ha-mem$ala  
Office-[const poss] head-[const] the-government 
The prime-minister?s office 
 
Possessive: the smixut form can be used to indi-
cate possession. Other ways to express posses-
sion include the possessive marker  - ?$el? / 
?of? - (5), or adding a possessive suffix on the 
noun (6). The various forms can be mixed to-
gether, as in (7): 
(5) 	
 
ha-bait $el-i / the-house of-[poss 1st person] 
My house 
(6)  
beit-i / house-[poss 1st person] 
My house 
(7) 	

	

  
misrad-o $el ro$ ha-mem$ala 
Office-[poss 3rd] of head-[const] the-government 
The prime minister office 
 
Adjective: Hebrew adjectives come after the 
noun, and agree with it in number, gender and 
definite marker: 
(8)  
ha-tapu?ah ha-yarok / the-Apple the-green 
The green apple 
 
Some aspects of the predicate structure in He-
brew directly affect the task of NP chunking, as 
they make the decision to ?split? NPs more or 
less difficult than in English. 
 
Word order and the preposition 'et': Hebrew 
sentences can be either in SVO or VSO form. In 
order to keep the object separate from the sub-
ject, definite direct objects are marked with the 
special preposition 'et', which has no analog in 
English.  
 
Possible null equative: The equative form in 
Hebrew can be null. Sentence (9) is a non-null 
equative, (10) a null equative, while (11) and 
(12) are predicative NPs, which look very similar 
to the null-equative form: 
 
(9) 	 
ha-bait hu gadol 
The-house is big 
The house is big 
 
(10) 	 
ha-bait gadol 
The-house big 
The house is big 
 
(11) 	 
bait gadol 
House big 
A big house 
(12) 	 
ha-bait ha-gadol 
The-house the-big 
The big house 
 
Morphological Issues: In Hebrew morphology, 
several lexical units can be concatenated into a 
single textual unit.  Most prepositions, the defi-
nite article marker and some conjunctions are 
concatenated as prefixes, and possessive pro-
nouns and some adverbs are concatenated as suf-
fixes.  The Hebrew Treebank is annotated over a 
segmented version of the text, in which prefixes 
and suffixes appear as separate lexical units.  On 
the other hand, many bound morphemes in Eng-
lish appear as separate lexical units in Hebrew.  
For example, the English morphemes re-, ex-, 
un-, -less, -like, -able, appear in Hebrew as sepa-
rate lexical units ? , 	, 

 , , , 
, . 
  
In our experiment, we use as input to the 
chunker the text after it has been morphologi-
cally disambiguated and segmented. Our 
analyzer provides segmentation and PoS tags 
with 92.5% accuracy and full morphology with 
88.5% accuracy (Adler and Elhadad, 2006). 
3.2 Defining Simple NPs 
Our definition of Simple NPs is pragmatic. We 
want to tag phrases that are complete in their 
syntactic structure, avoid the requirement of tag-
ging recursive structures that include full clauses 
(relative clauses for example) and in general, tag 
phrases that have a simple denotation. To estab-
lish our definition, we start with the most com-
plex NPs, and break them into smaller parts by 
stating what should not appear inside a Simple 
NP. This can be summarized by the following 
table: 
 
Outside SimpleNP Exceptions 
Prepositional Phrases 
Relative Clauses 
Verb Phrases 
Apposition1 
Some conjunctions 
(Conjunctions are 
marked according to the 
TreeBank guidelines)2. 
% related PPs are 
allowed:  

5% of the sales 
 
Possessive  - '$el' / 
'of' - is not consid-
ered a PP 
Table 2.   Definition of Simple NP chunks 
Examples for some Simple NP chunks resulting 
from that definition: 
 
                                                 
1
 Apposition structure is not annotated in the TreeBank. As 
a heuristic, we consider every comma inside a non conjunct-
ive NP which is not followed by an adjective or an adjective 
phrase to be marking the beginning of an apposition. 
2
 As a special case, Adjectival Phrases and possessive con-
junctions are considered to be inside the Simple NP.  
691
   	   
	

Proceedings of ACL-08: HLT, pages 728?736,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Unsupervised Lexicon-Based Resolution of Unknown Words for Full
Morphological Analysis
Meni Adler and Yoav Goldberg and David Gabay and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science?
POB 653 Be?er Sheva, 84105, Israel
{adlerm,goldberg,gabayd,elhadad}@cs.bgu.ac.il
Abstract
Morphological disambiguation proceeds in 2
stages: (1) an analyzer provides all possible
analyses for a given token and (2) a stochastic
disambiguation module picks the most likely
analysis in context. When the analyzer does
not recognize a given token, we hit the prob-
lem of unknowns. In large scale corpora, un-
knowns appear at a rate of 5 to 10% (depend-
ing on the genre and the maturity of the lexi-
con).
We address the task of computing the distribu-
tion p(t|w) for unknown words for full mor-
phological disambiguation in Hebrew. We in-
troduce a novel algorithm that is language in-
dependent: it exploits a maximum entropy let-
ters model trained over the known words ob-
served in the corpus and the distribution of
the unknown words in known tag contexts,
through iterative approximation. The algo-
rithm achieves 30% error reduction on dis-
ambiguation of unknown words over a com-
petitive baseline (to a level of 70% accurate
full disambiguation of unknown words). We
have also verified that taking advantage of a
strong language-specific model of morpholog-
ical patterns provides the same level of disam-
biguation. The algorithm we have developed
exploits distributional information latent in a
wide-coverage lexicon and large quantities of
unlabeled data.
?This work is supported in part by the Lynn and William
Frankel Center for Computer Science.
1 Introduction
The term unknowns denotes tokens in a text that can-
not be resolved in a given lexicon. For the task of
full morphological analysis, the lexicon must pro-
vide all possible morphological analyses for any
given token. In this case, unknown tokens can be
categorized into two classes of missing informa-
tion: unknown tokens are not recognized at all by
the lexicon, and unknown analyses, where the set
of analyses for a lexeme does not contain the cor-
rect analysis for a given token. Despite efforts on
improving the underlying lexicon, unknowns typi-
cally represent 5% to 10% of the number of tokens
in large-scale corpora. The alternative to continu-
ously investing manual effort in improving the lex-
icon is to design methods to learn possible analy-
ses for unknowns from observable features: their
letter structure and their context. In this paper, we
investigate the characteristics of Hebrew unknowns
for full morphological analysis, and propose a new
method for handling such unavoidable lack of in-
formation. Our method generates a distribution of
possible analyses for unknowns. In our evaluation,
these learned distributions include the correct anal-
ysis for unknown words in 85% of the cases, con-
tributing an error reduction of over 30% over a com-
petitive baseline for the overall task of full morpho-
logical analysis in Hebrew.
The task of a morphological analyzer is to pro-
duce all possible analyses for a given token. In
Hebrew, the analysis for each token is of the form
lexeme-and-features1: lemma, affixes, lexical cate-
1In contrast to the prefix-stem-suffix analysis format of
728
gory (POS), and a set of inflection properties (ac-
cording to the POS) ? gender, number, person, sta-
tus and tense. In this work, we refer to the mor-
phological analyzer of MILA ? the Knowledge Cen-
ter for Processing Hebrew2 (hereafter KC analyzer).
It is a synthetic analyzer, composed of two data re-
sources ? a lexicon of about 2,400 lexemes, and a
set of generation rules (see (Adler, 2007, Section
4.2)). In addition, we use an unlabeled text cor-
pus, composed of stories taken from three Hebrew
daily news papers (Aruts 7, Haaretz, The Marker),
of 42M tokens. We observed 3,561 different com-
posite tags (e.g., noun-sing-fem-prepPrefix:be) over
this corpus. These 3,561 tags form the large tagset
over which we train our learner. On the one hand,
this tagset is much larger than the largest tagset used
in English (from 17 tags in most unsupervised POS
tagging experiments, to the 46 tags of the WSJ cor-
pus and the about 150 tags of the LOB corpus). On
the other hand, our tagset is intrinsically factored as
a set of dependent sub-features, which we explicitly
represent.
The task we address in this paper is morphologi-
cal disambiguation: given a sentence, obtain the list
of all possible analyses for each word from the an-
alyzer, and disambiguate each word in context. On
average, each token in the 42M corpus is given 2.7
possible analyses by the analyzer (much higher than
the average 1.41 POS tag ambiguity reported in En-
glish (Dermatas and Kokkinakis, 1995)). In previ-
ous work, we report disambiguation rates of 89%
for full morphological disambiguation (using an un-
supervised EM-HMM model) and 92.5% for part of
speech and segmentation (without assigning all the
inflectional features of the words).
In order to estimate the importance of unknowns
in Hebrew, we analyze tokens in several aspects: (1)
the number of unknown tokens, as observed on the
corpus of 42M tokens; (2) a manual classification
of a sample of 10K unknown token types out of the
200K unknown types identified in the corpus; (3) the
number of unknown analyses, based on an annotated
corpus of 200K tokens, and their classification.
About 4.5% of the 42M token instances in the
Buckwalter?s Arabic analyzer (2004), which looks for any le-
gal combination of prefix-stem-suffix, but does not provide full
morphological features such as gender, number, case etc.
2http://mila.cs.technion.ac.il.html
training corpus were unknown tokens (45% of the
450K token types). For less edited text, such as ran-
dom text sampled from the Web, the percentage is
much higher ? about 7.5%. In order to classify these
unknown tokens, we sampled 10K unknown token
types and examined them manually. The classifica-
tion of these tokens with their distribution is shown
in Table 13. As can be seen, there are two main
classes of unknown token types: Neologisms (32%)
and Proper nouns (48%), which cover about 80%
of the unknown token instances. The POS distribu-
tion of the unknown tokens of our annotated corpus
is shown in Table 2. As expected, most unknowns
are open class words: proper names, nouns or adjec-
tives.
Regarding unknown analyses, in our annotated
corpus, we found 3% of the 100K token instances
were missing the correct analysis in the lexicon
(3.65% of the token types). The POS distribution of
the unknown analyses is listed in Table 2. The high
rate of unknown analyses for prepositions at about
3% is a specific phenomenon in Hebrew, where
prepositions are often prefixes agglutinated to the
first word of the noun phrase they head. We observe
the very low rate of unknown verbs (2%) ? which are
well marked morphologically in Hebrew, and where
the rate of neologism introduction seems quite low.
This evidence illustrates the need for resolution
of unknowns: The naive policy of selecting ?proper
name? for all unknowns will cover only half of the
errors caused by unknown tokens, i.e., 30% of the
whole unknown tokens and analyses. The other 70%
of the unknowns ( 5.3% of the words in the text in
our experiments) will be assigned a wrong tag.
As a result of this observation, our strategy is to
focus on full morphological analysis for unknown
tokens and apply a proper name classifier for un-
known analyses and unknown tokens. In this paper,
we investigate various methods for achieving full
morphological analysis distribution for unknown to-
kens. The methods are not based on an annotated
corpus, nor on hand-crafted rules, but instead ex-
ploit the distribution of words in an available lexicon
and the letter similarity of the unknown words with
known words.
3Transcription according to Ornan (2002)
729
Category Examples DistributionTypes Instances
Proper names ?asulin (family name) oileq`
?a?udi (Audi) ice`` 40% 48%
Neologisms ?agabi (incidental) iab`
tizmur (orchestration) xenfz 30% 32%
Abbreviation mz?p (DIFS) t"fnkb?t (security officer) h"aw 2.4% 7.8%
Foreign
presentacyah (presentation) divhpfxt
?a?ut (out) he``
right
3.8% 5.8%
Wrong spelling
?abibba?ah
.
ronah (springatlast) dpexg`aaia`
?idiqacyot (idication) zeivwici`
ryus?alaim (Rejusalem) milyeix
1.2% 4%
Alternative spelling ?opyynim (typical) mipiite`priwwilegyah (privilege ) diblieeixt 3.5% 3%
Tokenization ha?sap (the?threshold) sq"d
?al/17 (on/17) 71/lr 8% 2%
Table 1: Unknown Hebrew token categories and distribution.
Part of Speech Unknown Tokens Unknown Analyses Total
Proper name 31.8% 24.4% 56.2%
Noun 12.6% 1.6% 14.2%
Adjective 7.1% 1.7% 8.8%
Junk 3.0% 1.3% 4.3%
Numeral 1.1% 2.3% 3.4%
Preposition 0.3% 2.8% 3.1%
Verb 1.8% 0.4% 2.2%
Adverb 0.9% 0.9% 1.8%
Participle 0.4% 0.8% 1.2%
Copula / 0.8% 0.8%
Quantifier 0.3% 0.4% 0.7%
Modal 0.3% 0.4% 0.7%
Conjunction 0.1% 0.5% 0.6%
Negation / 0.6% 0.6%
Foreign 0.2% 0.4% 0.6%
Interrogative 0.1% 0.4% 0.5%
Prefix 0.3% 0.2% 0.5%
Pronoun / 0.5% 0.5%
Total 60% 40% 100%
Table 2: Unknowns Hebrew POS Distribution.
730
2 Previous Work
Most of the work that dealt with unknowns in the last
decade focused on unknown tokens (OOV). A naive
approach would assign all possible analyses for each
unknown token with uniform distribution, and con-
tinue disambiguation on the basis of a learned model
with this initial distribution. The performance of a
tagger with such a policy is actually poor: there are
dozens of tags in the tagset (3,561 in the case of He-
brew full morphological disambiguation) and only
a few of them may match a given token. Several
heuristics were developed to reduce the possibility
space and to assign a distribution for the remaining
analyses.
Weischedel et al (1993) combine several heuris-
tics in order to estimate the token generation prob-
ability according to various types of information ?
such as the characteristics of particular tags with
respect to unknown tokens (basically the distribu-
tion shown in Table 2), and simple spelling fea-
tures: capitalization, presence of hyphens and spe-
cific suffixes. An accuracy of 85% in resolving un-
known tokens was reported. Dermatas and Kokki-
nakis (1995) suggested a method for guessing un-
known tokens based on the distribution of the ha-
pax legomenon, and reported an accuracy of 66% for
English. Mikheev (1997) suggested a guessing-rule
technique, based on prefix morphological rules, suf-
fix morphological rules, and ending-guessing rules.
These rules are learned automatically from raw text.
They reported a tagging accuracy of about 88%.
Thede and Harper (1999) extended a second-order
HMM model with a C = ck,i matrix, in order to en-
code the probability of a token with a suffix sk to
be generated by a tag ti. An accuracy of about 85%
was reported.
Nakagawa (2004) combine word-level and
character-level information for Chinese and
Japanese word segmentation. At the word level, a
segmented word is attached to a POS, where the
character model is based on the observed characters
and their classification: Begin of word, In the
middle of a word, End of word, the character is a
word itself S. They apply Baum-Welch training over
a segmented corpus, where the segmentation of each
word and its character classification is observed, and
the POS tagging is ambiguous. The segmentation
(of all words in a given sentence) and the POS
tagging (of the known words) is based on a Viterbi
search over a lattice composed of all possible word
segmentations and the possible classifications of
all observed characters. Their experimental results
show that the method achieves high accuracy over
state-of-the-art methods for Chinese and Japanese
word segmentation. Hebrew also suffers from
ambiguous segmentation of agglutinated tokens into
significant words, but word formation rules seem to
be quite different from Chinese and Japanese. We
also could not rely on the existence of an annotated
corpus of segmented word forms.
Habash and Rambow (2006) used the
root+pattern+features representation of Arabic
tokens for morphological analysis and generation
of Arabic dialects, which have no lexicon. They
report high recall (95%?98%) but low precision
(37%?63%) for token types and token instances,
against gold-standard morphological analysis. We
also exploit the morphological patterns characteris-
tic of semitic morphology, but extend the guessing
of morphological features by using contextual
features. We also propose a method that relies
exclusively on learned character-level features and
contextual features, and eventually reaches the same
performance as the patterns-based approach.
Mansour et al (2007) combine a lexicon-based
tagger (such as MorphTagger (Bar-Haim et al,
2005)), and a character-based tagger (such as the
data-driven ArabicSVM (Diab et al, 2004)), which
includes character features as part of its classifica-
tion model, in order to extend the set of analyses
suggested by the analyzer. For a given sentence, the
lexicon-based tagger is applied, selecting one tag for
a token. In case the ranking of the tagged sentence is
lower than a threshold, the character-based tagger is
applied, in order to produce new possible analyses.
They report a very slight improvement on Hebrew
and Arabic supervised POS taggers.
Resolution of Hebrew unknown tokens, over a
large number of tags in the tagset (3,561) requires
a much richer model than the the heuristics used
for English (for example, the capitalization feature
which is dominant in English does not exist in He-
brew). Unlike Nakagawa, our model does not use
any segmented text, and, on the other hand, it aims
to select full morphological analysis for each token,
731
including unknowns.
3 Method
Our objective is: given an unknown word, provide
a distribution of possible tags that can serve as the
analysis of the unknown word. This unknown anal-
ysis step is performed at training and testing time.
We do not attempt to disambiguate the word ? but
only to provide a distribution of tags that will be dis-
ambiguated by the regular EM-HMM mechanism.
We examined three models to construct the distri-
bution of tags for unknown words, that is, whenever
the KC analyzer does not return any candidate anal-
ysis, we apply these models to produce possible tags
for the token p(t|w):
Letters A maximum entropy model is built for
all unknown tokens in order to estimate their tag
distribution. The model is trained on the known
tokens that appear in the corpus. For each anal-
ysis of a known token, the following features are
extracted: (1) unigram, bigram, and trigram letters
of the base-word (for each analysis, the base-word
is the token without prefixes), together with their
index relative to the start and end of the word. For
example, the n-gram features extracted for the word
abc are { a:1 b:2 c:3 a:-3 b:-2 c:-1
ab:1 bc:2 ab:-2 bc:-1 abc:1 abc:-1
} ; (2) the prefixes of the base-word (as a single
feature); (3) the length of the base-word. The class
assigned to this set of features, is the analysis of the
base-word. The model is trained on all the known
tokens of the corpus, each token is observed with its
possible POS-tags once for each of its occurrences.
When an unknown token is found, the model
is applied as follows: all the possible linguistic
prefixes are extracted from the token (one of the 76
prefix sequences that can occur in Hebrew); if more
than one such prefix is found, the token is analyzed
for each possible prefix. For each possible such
segmentation, the full feature vector is constructed,
and submitted to the Maximum Entropy model.
We hypothesize a uniform distribution among the
possible segmentations and aggregate a distribution
of possible tags for the analysis. If the proposed
tag of the base-word is never found in the corpus
preceded by the identified prefix, we remove this
possible analysis. The eventual outcome of the
model application is a set of possible full morpho-
logical analyses for the token ? in exactly the same
format as the morphological analyzer provides.
Patterns Word formation in Hebrew is based on
root+pattern and affixation. Patterns can be used to
identify the lexical category of unknowns, as well
as other inflectional properties. Nir (1993) investi-
gated word-formation in Modern Hebrew with a spe-
cial focus on neologisms; the most common word-
formation patterns he identified are summarized in
Table 3. A naive approach for unknown resolution
would add all analyses that fit any of these patterns,
for any given unknown token. As recently shown by
Habash and Rambow (2006), the precision of such
a strategy can be pretty low. To address this lack of
precision, we learn a maximum entropy model on
the basis of the following binary features: one fea-
ture for each pattern listed in column Formation of
Table 3 (40 distinct patterns) and one feature for ?no
pattern?.
Pattern-Letters This maximum entropy model is
learned by combining the features of the letters
model and the patterns model.
Linear-Context-based p(t|c) approximation
The three models above are context free. The
linear-context model exploits information about the
lexical context of the unknown words: to estimate
the probability for a tag t given a context c ? p(t|c)
? based on all the words in which a context occurs,
the algorithm works on the known words in the
corpus, by starting with an initial tag-word estimate
p(t|w) (such as the morpho-lexical approximation,
suggested by Levinger et al (1995)), and iteratively
re-estimating:
p?(t|c) =
?
w?W p(t|w)p(w|c)
Z
p?(t|w) =
?
c?C p(t|c)p(c|w)allow(t, w)
Z
where Z is a normalization factor, W is the set of
all words in the corpus, C is the set of contexts.
allow(t, w) is a binary function indicating whether t
is a valid tag for w. p(c|w) and p(w|c) are estimated
via raw corpus counts.
Loosely speaking, the probability of a tag given a
context is the average probability of a tag given any
732
Category Formation Example
Verb Template
?iCCeC ?ibh
.
en (diagnosed) oga`
miCCeC mih
.
zer (recycled) xfgn
CiCCen timren (manipulated) oxnz
CiCCet tiknet (programmed) zpkz
tiCCeC ti?arek (dated) jx`z
Participle Template
meCuCaca ms?wh
.
zar (reconstructed) xfgeyn
muCCaC muqlat
.
(recorded) hlwen
maCCiC malbin (whitening) oialn
Noun
Suffixation
ut h
.
aluciyut (pioneership) zeivelg
ay yomanay (duty officer) i`pnei
an ?egropan (boxer) otexb`
on pah
.
on (shack) oegt
iya marakiyah (soup tureen) diiwxn
it t
.
iyulit (open touring vehicle) zileih
a lomdah (courseware) dcnel
Template
maCCeC mas?neq (choke) wpyn
maCCeCa madgera (incubator) dxbcn
miCCaC mis?ap (branching) srqn
miCCaCa mignana (defensive fighting) dppbn
CeCeCa pelet
.
(output) hlt
tiCCoCet tiproset (distribution) zqextz
taCCiC tah
.
rit
.
(engraving) hixgz
taCCuCa tabru?ah (sanitation) d`exaz
miCCeCet micrepet (leotard) ztxvn
CCiC crir (dissonance) xixv
CaCCan bals?an (linguist) oyla
CaCeCet s?ah
.
emet (cirrhosis) zngy
CiCul t
.
ibu? (ringing) reaih
haCCaCa hanpas?a (animation) dytpd
heCCeC het?em (agreement) m`zd
Adjective
Suffixationb
i nora?i (awful) i`xep
ani yeh
.
idani (individual) ipcigi
oni t
.
elewizyonic (televisional) ipeifieelh
a?i yed
.
ida?i (unique) i`cigi
ali st
.
udentiali (student) il`ihpcehq
Template C1C2aC3C2aC3
d metaqtaq (sweetish) wzwzn
CaCuC rapus (flaccid ) qetx
Adverb Suffixation
ot qcarot (briefly) zexvw
it miyadit (immediately) zicin
Prefixation b bekeip (with fun) sika
aCoCeC variation: wzer ?wyeq (a copy).
bThe feminine form is made by the t and iya suffixes: ipcigi yeh
.
idanit (individual), dixvep nwcriya (Christian).
cIn the feminine form, the last h of the original noun is omitted.
dC1C2aC3C2oC3 variation: oehphw qt.ant.wn (tiny).
Table 3: Common Hebrew Neologism Formations.
733
Model Analysis Set MorphologicalDisambiguationCoverage Ambiguity Probability
Baseline 50.8% 1.5 0.48 57.3%
Pattern 82.8% 20.4 0.10 66.8%
Letter 76.7% 5.9 0.32 69.1%
Pattern-Letter 84.1% 10.4 0.25 69.8%
WordContext-Pattern 84.4% 21.7 0.12 66.5%
TagContext-Pattern 85.3% 23.5 0.19 64.9%
WordContext-Letter 80.7% 7.94 0.30 69.7%
TagContext-Letter 83.1% 7.8 0.22 66.9%
WordContext-Pattern-Letter 85.2% 12.0 0.24 68.8%
TagContext-Pattern-Letter 86.1% 14.3 0.18 62.1%
Table 4: Evaluation of unknown token full morphological analysis.
of the words appearing in that context, and similarly
the probability of a tag given a word is the averaged
probability of that tag in all the (reliable) contexts
in which the word appears. We use the function
allow(t, w) to control the tags (ambiguity class) al-
lowed for each word, as given by the lexicon.
For a given word wi in a sentence, we examine
two types of contexts: word context wi?1, wi+1,
and tag context ti?1, ti+1. For the case of word con-
text, the estimation of p(w|c) and p(c|w) is simply
the relative frequency over all the events w1, w2, w3
occurring at least 10 times in the corpus. Since the
corpus is not tagged, the relative frequency of the
tag contexts is not observed, instead, we use the
context-free approximation of each word-tag, in or-
der to determine the frequency weight of each tag
context event. For example, given the sequence
icnl ziznerl daebz tgubah l?umatit lmadai (a quite
oppositional response), and the analyses set pro-
duced by the context-free approximation: tgubah
[NN 1.0] l?umatit [] lmadai [RB 0.8, P1-NN 0.2].
The frequency weight of the context {NN RB} is
1 ? 0.8 = 0.8 and the frequency weight of the con-
text {NN P1-NN} is 1 ? 0.2 = 0.2.
4 Evaluation
For testing, we manually tagged the text which is
used in the Hebrew Treebank (consisting of about
90K tokens), according to our tagging guideline (?).
We measured the effectiveness of the three mod-
els with respect to the tags that were assigned to the
unknown tokens in our test corpus (the ?correct tag?),
according to three parameters: (1) The coverage of
the model, i.e., we count cases where p(t|w) con-
tains the correct tag with a probability larger than
0.01; (2) the ambiguity level of the model, i.e., the
average number of analyses suggested for each to-
ken; (3) the average probability of the ?correct tag?,
according to the predicted p(t|w). In addition, for
each experiment, we run the full morphology dis-
ambiguation system where unknowns are analyzed
according by the model.
Our baseline proposes the most frequent tag
(proper name) for all possible segmentations of the
token, in a uniform distribution. We compare the
following models: the 3 context free models (pat-
terns, letters and the combined patterns and letters)
and the same models combined with the word and
tag context models. Note that the context models
have low coverage (about 40% for the word context
and 80% for the tag context models), and therefore,
the context models cannot be used on their own. The
highest coverage is obtained for the combined model
(tag context, pattern, letter) at 86.1%.
We first show the results for full morphological
disambiguation, over 3,561 distinct tags in Table 4.
The highest coverage is obtained for the model com-
bining the tag context, patterns and letters models.
The tag context model is more effective because
it covers 80% of the unknown words, whereas the
word context model only covers 40%. As expected,
our simple baseline has the highest precision, since
the most frequent proper name tag covers over 50%
of the unknown words. The eventual effectiveness of
734
Model Analysis Set POS TaggingCoverage Ambiguity Probability
Baseline 52.9% 1.5 0.52 60.6%
Pattern 87.4% 8.7 0.19 76.0%
Letter 80% 4.0 0.39 77.6%
Pattern-Letter 86.7% 6.2 0.32 78.5%
WordContext-Pattern 88.7% 8.8 0.21 75.8%
TagContext-Pattern 89.5% 9.1 0.14 73.8%
WordContext-Letter 83.8% 4.5 0.37 78.2%
TagContext-Letter 87.1% 5.7 0.28 75.2%
WordContext-Pattern-Letter 87.8 6.5 0.32 77.5%
TagContext-Pattern-Letter 89.0% 7.2 0.25 74%
Table 5: Evaluation of unknown token POS tagging.
the method is measured by its impact on the eventual
disambiguation of the unknown words. For full mor-
phological disambiguation, our method achieves an
error reduction of 30% (57% to 70%). Overall, with
the level of 4.5% of unknown words observed in our
corpus, the algorithm we have developed contributes
to an error reduction of 5.5% for full morphological
disambiguation.
The best result is obtained for the model com-
bining pattern and letter features. However, the
model combining the word context and letter fea-
tures achieves almost identical results. This is an
interesting result, as the pattern features encapsulate
significant linguistic knowledge, which apparently
can be approximated by a purely distributional ap-
proximation.
While the disambiguation level of 70% is lower
than the rate of 85% achieved in English, it must
be noted that the task of full morphological disam-
biguation in Hebrew is much harder ? we manage
to select one tag out of 3,561 for unknown words as
opposed to one out of 46 in English. Table 5 shows
the result of the disambiguation when we only take
into account the POS tag of the unknown tokens.
The same models reach the best results in this case
as well (Pattern+Letters and WordContext+Letters).
The best disambiguation result is 78.5% ? still much
lower than the 85% achieved in English. The main
reason for this lower level is that the task in He-
brew includes segmentation of prefixes and suffixes
in addition to POS classification. We are currently
investigating models that will take into account the
specific nature of prefixes in Hebrew (which encode
conjunctions, definite articles and prepositions) to
better predict the segmentation of unknown words.
5 Conclusion
We have addressed the task of computing the distri-
bution p(t|w) for unknown words for full morpho-
logical disambiguation in Hebrew. The algorithm
we have proposed is language independent: it ex-
ploits a maximum entropy letters model trained over
the known words observed in the corpus and the dis-
tribution of the unknown words in known tag con-
texts, through iterative approximation. The algo-
rithm achieves 30% error reduction on disambigua-
tion of unknown words over a competitive baseline
(to a level of 70% accurate full disambiguation of
unknown words). We have also verified that tak-
ing advantage of a strong language-specific model
of morphological patterns provides the same level
of disambiguation. The algorithm we have devel-
oped exploits distributional information latent in a
wide-coverage lexicon and large quantities of unla-
beled data.
We observe that the task of analyzing unknown to-
kens for POS in Hebrew remains challenging when
compared with English (78% vs. 85%). We hy-
pothesize this is due to the highly ambiguous pattern
of prefixation that occurs widely in Hebrew and are
currently investigating syntagmatic models that ex-
ploit the specific nature of agglutinated prefixes in
Hebrew.
735
References
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.
Roy Bar-Haim, Khalil Sima?an, and Yoad Winter. 2005.
Choosing an optimal architecture for segmentation and
pos-tagging of modern Hebrew. In Proceedings of
ACL-05 Workshop on Computational Approaches to
Semitic Languages.
Tim Buckwalter. 2004. Buckwalter Arabic morphologi-
cal analyzer, version 2.0.
Evangelos Dermatas and George Kokkinakis. 1995. Au-
tomatic stochastic tagging of natural language texts.
Computational Linguistics, 21(2):137?163.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of Arabic text: From raw text to
base phrase chunks. In Proceeding of HLT-NAACL-
04.
Michael Elhadad, Yael Netzer, David Gabay, and Meni
Adler. 2005. Hebrew morphological tagging guide-
lines. Technical report, Ben-Gurion University, Dept.
of Computer Science.
Nizar Habash and Owen Rambow. 2006. Magead: A
morphological analyzer and generator for the arabic
dialects. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, pages 681?688, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Moshe Levinger, Uzi Ornan, and Alon Itai. 1995. Learn-
ing morpholexical probabilities from an untagged cor-
pus with an application to Hebrew. Computational
Linguistics, 21:383?404.
Saib Mansour, Khalil Sima?an, and Yoad Winter. 2007.
Smoothing a lexicon-based pos tagger for Arabic and
Hebrew. In ACL07 Workshop on Computational Ap-
proaches to Semitic Languages, Prague, Czech Repub-
lic.
Andrei Mikheev. 1997. Automatic rule induction for
unknown-word guessing. Computational Linguistics,
23(3):405?423.
Tetsuji Nakagawa. 2004. Chinese and Japanese word
segmentation using word-level and character-level in-
formation. In Proceedings of the 20th international
conference on Computational Linguistics, Geneva.
Raphael Nir. 1993. Word-Formation in Modern Hebrew.
The Open University of Israel, Tel-Aviv, Israel.
Uzi Ornan. 2002. Hebrew in Latin script. Le?s?one?nu,
LXIV:137?151. (in Hebrew).
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden Markov model for part-of-speech tag-
ging. In Proceeding of ACL-99.
R. Weischedel, R. Schwartz, J. Palmucci, M. Meteer, and
L. Ramshaw. 1993. Coping with ambiguity and un-
known words through probabilistic models. Computa-
tional Linguistics, 19:359?382.
736
Proceedings of ACL-08: HLT, pages 746?754,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
EM Can Find Pretty Good HMM POS-Taggers
(When Given a Good Start)?
Yoav Goldberg and Meni Adler and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yoavg,adlerm,elhadad}@cs.bgu.ac.il
Abstract
We address the task of unsupervised POS tag-
ging. We demonstrate that good results can be
obtained using the robust EM-HMM learner
when provided with good initial conditions,
even with incomplete dictionaries. We present
a family of algorithms to compute effective
initial estimations p(t|w). We test the method
on the task of full morphological disambigua-
tion in Hebrew achieving an error reduction of
25% over a strong uniform distribution base-
line. We also test the same method on the stan-
dard WSJ unsupervised POS tagging task and
obtain results competitive with recent state-of-
the-art methods, while using simple and effi-
cient learning methods.
1 Introduction
The task of unsupervised (or semi-supervised) part-
of-speech (POS) tagging is the following: given a
dictionary mapping words in a language to their pos-
sible POS, and large quantities of unlabeled text
data, learn to predict the correct part of speech for
a given word in context. The only supervision given
to the learning process is the dictionary, which in
a realistic scenario, contains only part of the word
types observed in the corpus to be tagged.
Unsupervised POS tagging has been traditionally
approached with relative success (Merialdo, 1994;
Kupiec, 1992) by HMM-based generative mod-
els, employing EM parameters estimation using the
Baum-Welch algorithm. However, as recently noted
?This work is supported in part by the Lynn and William
Frankel Center for Computer Science.
by Banko and Moore (2004), these works made use
of filtered dictionaries: dictionaries in which only
relatively probable analyses of a given word are pre-
served. This kind of filtering requires serious su-
pervision: in theory, an expert is needed to go over
the dictionary elements and filter out unlikely anal-
yses. In practice, counts from an annotated corpus
have been traditionally used to perform the filtering.
Furthermore, these methods require rather compre-
hensive dictionaries in order to perform well.
In recent work, researchers try to address these
deficiencies by using dictionaries with unfiltered
POS-tags, and testing the methods on ?diluted dic-
tionaries? ? in which many of the lexical entries are
missing (Smith and Eisner, 2005) (SE), (Goldwater
and Griffiths, 2007) (GG), (Toutanova and Johnson,
2008) (TJ).
All the work mentioned above focuses on unsu-
pervised English POS tagging. The dictionaries are
all derived from tagged English corpora (all recent
work uses the WSJ corpus). As such, the setting of
the research is artificial: there is no reason to per-
form unsupervised learning when an annotated cor-
pus is available. The problem is rather approached
as a workbench for exploring new learning methods.
The result is a series of creative algorithms, that have
steadily improved results on the same dataset: unsu-
pervised CRF training using contrastive estimation
(SE), a fully-bayesian HMM model that jointly per-
forms clustering and sequence learning (GG), and
a Bayesian LDA-based model using only observed
context features to predict tag words (TJ). These so-
phisticated learning algorithms all outperform the
traditional baseline of EM-HMM based methods,
746
while relying on similar knowledge: the lexical con-
text of the words to be tagged and their letter struc-
ture (e.g., presence of suffixes, capitalization and
hyphenation).1
Our motivation for tackling unsupervised POS
tagging is different: we are interested in develop-
ing a Hebrew POS tagger. We have access to a good
Hebrew lexicon (and a morphological analyzer), and
a fair amount of unlabeled training data, but hardly
any annotated corpora. We actually report results
on full morphological disambiguation for Hebrew, a
task similar but more challenging than POS tagging:
we deal with a tagset much larger than English (over
3,561 distinct tags) and an ambiguity level of about
2.7 per token as opposed to 1.4 for English. Instead
of inventing a new learning framework, we go back
to the traditional EM trained HMMs. We argue that
the key challenge to learning an effective model is
to define good enough initial conditions. Given suf-
ficiently good initial conditions, EM trained models
can yield highly competitive results. Such models
have other benefits as well: they are simple, robust,
and computationally more attractive.
In this paper, we concentrate on methods for de-
riving sufficiently good initial conditions for EM-
HMM learning. Our method for learning initial con-
ditions for the p(t|w) distributions relies on a mix-
ture of language specific models: a paradigmatic
model of similar words (where similar words are
words with similar inflection patterns), simple syn-
tagmatic constraints (e.g., the sequence V-V is ex-
tremely rare in English). These are complemented
by a linear lexical context model. Such models are
simple to build and test.
We present results for unsupervised PoS tagging
of Hebrew text and for the common WSJ English
test sets. We show that our method achieves state-of-
the-art results for the English setting, even with a rel-
atively small dictionary. Furthermore, while recent
work report results on a reduced English tagset of
17 PoS tags, we also present results for the complete
45 tags tagset of the WSJ corpus. This considerably
raises the bar of the EM-HMM baseline. We also
report state-of-the-art results for Hebrew full mor-
1Another notable work, though within a slightly differ-
ent framework, is the prototype-driven method proposed by
(Haghighi and Klein, 2006), in which the dictionary is replaced
with a very small seed of prototypical examples.
phological disambiguation.
Our primary conclusion is that the problem of
learning effective stochastic classifiers remains pri-
marily a search task. Initial conditions play a domi-
nant role in solving this task and can rely on linguis-
tically motivated approximations. A robust learn-
ing method (EM-HMM) combined with good initial
conditions based on a robust feature set can go a
long way (as opposed to a more complex learning
method). It seems that computing initial conditions
is also the right place to capture complex linguistic
intuition without fear that over-generalization could
lead a learner to diverge.
2 Previous Work
The tagging accuracy of supervised stochastic tag-
gers is around 96%?97% (Manning and Schutze,
1999). Merialdo (1994) reports an accuracy
of 86.6% for an unsupervised token-based EM-
estimated HMM, trained on a corpus of about 1M
words, over a tagset of 159 tags. Elworthy (1994), in
contrast, reports accuracy of 75.49%, 80.87%, and
79.12% for unsupervised word-based HMM trained
on parts of the LOB corpora, with a tagset of 134
tags. With (artificially created) good initial condi-
tions, such as a good approximation of the tag distri-
bution for each word, Elworthy reports an improve-
ment to 94.6%, 92.27%, and 94.51% on the same
data sets. Merialdo, on the other hand, reports an im-
provement to 92.6% and 94.4% for the case where
100 and 2,000 sentences of the training corpus are
manually tagged. Later, Banko and Moore (2004)
observed that earlier unsupervised HMM-EM re-
sults were artificially high due to use of Optimized
Lexicons, in which only frequent-enough analyses
of each word were kept. Brill (1995b) proposed
an unsupervised tagger based on transformation-
based learning (Brill, 1995a), achieving accuracies
of above 95%. This unsupervised tagger relied on
an initial step in which the most probable tag for
each word is chosen. Optimized lexicons and Brill?s
most-probable-tag Oracle are not available in realis-
tic unsupervised settings, yet, they show that good
initial conditions greatly facilitate learning.
Recent work on unsupervised POS tagging for
English has significantly improved the results on this
task: GG, SE and most recently TJ report the best re-
747
sults so far on the task of unsupervised POS tagging
of the WSJ with diluted dictionaries. With dictionar-
ies as small as 1249 lexical entries the LDA-based
method with a strong ambiguity-class model reaches
POS accuracy as high as 89.7% on a reduced tagset
of 17 tags.
While these 3 methods rely on the same feature
set (lexical context, spelling features) for the learn-
ing stage, the LDA approach bases its predictions
entirely on observable features, and excludes the tra-
ditional hidden states sequence.
In Hebrew, Levinger et al (1995) introduced the
similar-words algorithm for estimating p(t|w) from
unlabeled data, which we describe below. Our
method uses this algorithm as a first step, and refines
the approximation by introducing additional linguis-
tic constraints and an iterative refinement step.
3 Initial Conditions For EM-HMM
The most common model for unsupervised learning
of stochastic processes is Hidden Markov Models
(HMM). For the case of tagging, the states corre-
spond to the tags ti, and words wi are emitted each
time a state is visited. The parameters of the model
can be estimated by applying the Baum-Welch EM
algorithm (Baum, 1972), on a large-scale corpus of
unlabeled text. The estimated parameters are then
used in conjunction with Viterbi search, to find the
most probable sequence of tags for a given sentence.
In this work, we follow Adler (2007) and use a vari-
ation of second-order HMM in which the probability
of a tag is conditioned by the tag that precedes it and
by the one that follows it, and the probability of an
emitted word is conditioned by its tag and the tag
that follows it2. In all experiments, we use the back-
off smoothing method of (Thede and Harper, 1999),
with additive smoothing (Chen, 1996) for the lexical
probabilities.
We investigate methods to approximate the initial
parameters of the p(t|w) distribution, from which
we obtain p(w|t) by marginalization and Bayesian
inversion. We also experiment with constraining the
p(t|t?1, t+1) distribution.
2Technically this is not Markov Model but a Dependency
Net. However, bidirectional conditioning seem more suitable
for language tasks, and in practice the learning and inference
methods are mostly unaffected. See (Toutanova et al, 2003).
General syntagmatic constraints We set linguis-
tically motivated constraints on the p(t|t?1, t+1)
distribution. In our setting, these are used to force
the probability of some events to 0 (e.g., ?Hebrew
verbs can not be followed by the of preposition?).
Morphology-based p(t|w) approximation
Levinger et al (1995) developed a context-free
method for acquiring morpho-lexical probabilities
(p(t|w)) from an untagged corpus. The method is
based on language-specific rules for constructing a
similar words (SW) set for each analysis of a word.
This set is composed of morphological variations
of the word under the given analysis. For example,
the Hebrew token ??? can be analyzed as either a
noun (boy) or a verb (gave birth). The noun SW set
for this token is composed of the definiteness and
number inflections ????,?????,?????? (the boy, boys,
the boys), while the verb SW set is composed
of gender and tense inflections ????,???? (she/they
gave birth). The approximated probability of each
analysis is based on the corpus frequency of its SW
set. For the complete details, refer to the original
paper. Cucerzan and Yarowsky (2000) proposed
a similar method for the unsupervised estimation
of p(t|w) in English, relying on simple spelling
features to characterize similar word classes.
Linear-Context-based p(t|w) approximation
The method of Levinger et al makes use of Hebrew
inflection patterns in order to estimate context free
approximation of p(t|w) by relating a word to its
different inflections. However, the context in which
a word occurs can also be very informative with
respect to its POS-analysis (Schu?tze, 1995). We
propose a novel algorithm for estimating p(t|w)
based on the contexts in which a word occurs.3
The algorithm starts with an initial p(t|w) esti-
mate, and iteratively re-estimates:
p?(t|c) =
?
w?W p(t|w)p(w|c)
Z
p?(t|w) =
?
c?RELC p(t|c)p(c|w)allow(t, w)
Z
3While we rely on the same intuition, our use of context
differs from earlier works on distributional POS-tagging like
(Schu?tze, 1995), in which the purpose is to directly assign the
possible POS for an unknown word. In contrast, our algorithm
aims to improve the estimate for the whole distribution p(t|w),
to be further disambiguated by the EM-HMM learner.
748
where Z is a normalization factor, W is the set of
all words in the corpus, C is the set of all contexts,
andRELC ? C is a set of reliable contexts, defined
below. allow(t, w) is a binary function indicating
whether t is a valid tag for w. p(c|w) and p(w|c) are
estimated via raw corpus counts.
Intuitively, we estimate the probability of a tag
given a context as the average probability of a tag
given any of the words appearing in that context, and
similarly the probability of a tag given a word is the
averaged probability of that tag in all the (reliable)
contexts in which the word appears. At each round,
we define RELC , the set of reliable contexts, to be
the set of all contexts in which p(t|c) > 0 for at most
X different ts.
The method is general, and can be applied to dif-
ferent languages. The parameters to specify for each
language are: the initial estimation p(t|w), the esti-
mation of the allow relation for known and OOV
words, and the types of contexts to consider.
4 Application to Hebrew
In Hebrew, several words combine into a single to-
ken in both agglutinative and fusional ways. This
results in a potentially high number of tags for each
token. On average, in our corpus, the number of pos-
sible analyses per known word reached 2.7, with the
ambiguity level of the extended POS tagset in cor-
pus for English (1.41) (Dermatas and Kokkinakis,
1995).
In this work, we use the morphological analyzer
of MILA ? Knowledge Center for Processing He-
brew (KC analyzer). In contrast to English tagsets,
the number of tags for Hebrew, based on all com-
binations of the morphological attributes, can grow
theoretically to about 300,000 tags. In practice, we
found ?only? about 3,560 tags in a corpus of 40M
tokens training corpus taken from Hebrew news ma-
terial and Knesset transcripts. For testing, we man-
ually tagged the text which is used in the Hebrew
Treebank (Sima?an et al, 2001) (about 90K tokens),
according to our tagging guidelines.
4.1 Initial Conditions
General syntagmatic constraints We define 4
syntagmatic constraints over p(t|t?1, t+1): (1) a
construct state form cannot be followed by a verb,
preposition, punctuation, existential, modal, or cop-
ula; (2) a verb cannot be followed by the preposition
?? s?el (of), (3) copula and existential cannot be fol-
lowed by a verb, and (4) a verb cannot be followed
by another verb, unless one of them has a prefix, or
the second verb is an infinitive, or the first verb is
imperative and the second verb is in future tense.4
Morphology-Based p(t|w) approximation We
extended the set of rules used in Levinger et al , in
order to support the wider tagset used by the KC an-
alyzer: (1) The SW set for adjectives, copulas, exis-
tentials, personal pronouns, verbs and participles, is
composed of all gender-number inflections; (2) The
SW set for common nouns is composed of all num-
ber inflections, with definite article variation for ab-
solute noun; (3) Prefix variations for proper nouns;
(4) Gender variation for numerals; and (5) Gender-
number variation for all suffixes (possessive, nomi-
native and accusative).
Linear-Context-based p(t|w) approximation
For the initial p(t|w) we use either a uniform distri-
bution based on the tags allowed in the dictionary,
or the estimate obtained by using the modified
Levinger et al algorithm. We use contexts of the
form LR=w?1, w+1 (the neighbouring words). We
estimate p(w|c) and p(c|w) via relative frequency
over all the events w1, w2, w3 occurring at least
10 times in the corpus. allow(t, w) follows the
dictionary. Because of the wide coverage of the
Hebrew lexicon, we take RELC to be C (all
available contexts).
4.2 Evaluation
We run a series of experiments with 8 distinct ini-
tial conditions, as shown in Table 1: our baseline
(Uniform) is the uniform distribution over all tags
provided by the KC analyzer for each word. The
Syntagmatic initial conditions add the p(t|t?1, t+1)
constraints described above to the uniform base-
line. The Morphology-Based and Linear-Context
initial conditions are computed as described above,
while the Morph+Linear is the result of applying
the linear-context algorithm over initial values com-
puted by the Morphology-based method. We repeat
4This rule was taken from Shacham and Wintner(2007).
749
Initial Condition Dist Context-Free EM-HMMFull Seg+Pos Full Seg+Pos
Uniform 60 63.8 71.9 85.5 89.8
Syntagmatic Pair Constraints 60 / / 85.8 89.8Init-Trans 60 / / 87.9 91
Morpho-Lexical
Morph-Based 76.8 76.4 83.1 87.7 91.6
Linear-Context 70.1 75.4 82.6 85.3 89.6
Morph+Linear 79.8 79.0 85.5 88 92
PairConst+Morph
Morph-Based / / / 87.6 91.4
Linear-Context / / / 84.5 89.0
Morph+Linear / / / 87.1 91.5
InitTrans+Morph
Morph-Based / / / 89.2 92.3
Linear-Context / / / 87.7 90.9
Morph+Linear / / / 89.4 92.4
Table 1: Accuracy (%) of Hebrew Morphological
Disambiguation and POS Tagging over various initial
conditions
these last 3 models with the addition of the syntag-
matic constraints (Synt+Morph).
For each of these, we first compare the computed
p(t|w) against a gold standard distribution, taken
from the test corpus (90K tokens), according to the
measure used by (Levinger et al, 1995) (Dist). On
this measure, we confirm that our improved morpho-
lexical approximation improves the results reported
by Levinger et al from 74% to about 80% on a
richer tagset, and on a much larger test set (90K vs.
3,400 tokens).
We then report on the effectiveness of p(t|w) as
a context-free tagger that assigns to each word the
most likely tag, both for full morphological analy-
sis (3,561 tags) (Full) and for the simpler task of
token segmentation and POS tag selection (36 tags)
(Seg+Pos). The best results on this task are 80.8%
and 87.5% resp. achieved on the Morph+Linear ini-
tial conditions.
Finally, we test effectiveness of the initial con-
ditions with EM-HMM learning. We reach 88%
accuracy on full morphological and 92% accuracy
for POS tagging and word segmentation, for the
Morph+Linear initial conditions.
As expected, EM-HMM improves results (from
80% to 88%). Strikingly, EM-HMM improves the
uniform initial conditions from 64% to above 85%.
However, better initial conditions bring us much
over this particular local maximum ? with an error
reduction of 20%. In all cases, the main improve-
ment over the uniform baseline is brought by the
morphology-based initial conditions. When applied
on its own, the linear context brings modest im-
provement. But the combination of the paradigmatic
morphology-based method with the linear context
improves all measures.
A most interesting observation is the detrimental
contribution of the syntagmatic constraints we in-
troduced. We found that 113,453 sentences of the
corpus (about 5%) contradict these basic and ap-
parently simple constraints. As an alternative to
these common-sense constraints, we tried to use a
small seed of randomly selected sentences (10K an-
notated tokens) in order to skew the initial uniform
distribution of the state transitions. We initialize the
p(t|t?1, t+1) distribution with smoothed ML esti-
mates based on tag trigram and bigram counts (ig-
noring the tag-word annotations). This small seed
initialization (InitTrans) has a great impact on ac-
curacy. Overall, we reach 89.4% accuracy on full
morphological and 92.4% accuracy for POS tagging
and word segmentation, for the Morph+Linear con-
ditions ? an error reduction of more than 25% from
the uniform distribution baseline.
5 Application to English
We now apply the same technique to English semi-
supervised POS tagging. Recent investigations of
this task use dictionaries derived from the Penn WSJ
corpus, with a reduced tag set of 17 tags5 instead of
the original 45-tags tagset. They experiment with
full dictionaries (containing complete POS informa-
tion for all the words in the text) as well as ?diluted?
dictionaries, from which large portions of the vo-
cabulary are missing. These settings are very dif-
ferent from those used for Hebrew: the tagset is
much smaller (17 vs. ?3,560) and the dictionaries
are either complete or extremely crippled. However,
for the sake of comparison, we have reproduced the
same experimental settings.
We derive dictionaries from the complete WSJ
corpus6, and the exact same diluted dictionaries used
in SE, TJ and GG.
5ADJ ADV CONJ DET ENDPUNC INPUNC LPUNC
RPUNC N POS PRT PREP PRT TO V VBG VBN WH
6The dictionary derived from the WSJ data is very noisy:
many of the stop words get wrong analyses stemming from tag-
ging mistakes (for instance, the word the has 6 possible analyses
in the data-derived dictionary, which we checked manually and
found all but DT erroneous). Such noise is not expected in a real
world dictionary, and our algorithm is not designed to accomo-
date it. We corrected the entries for the 20 most frequent words
in the corpus. This step could probably be done automatically,
but we consider it to be a non-issue in any realistic setting.
750
Syntagmatic Constraints We indirectly incor-
porated syntagmatic constraints through a small
change to the tagset. The 17-tags English tagset
allows for V-V transitions. Such a construction is
generally unlikely in English. By separating modals
from the rest of the verbs, and creating an addi-
tional class for the 5 be verbs (am,is,are,was,were),
we made such transition much less probable. The
new 19-tags tagset reflects the ?verb can not follow
a verb? constraint.
Morphology-Based p(t|w) approximation En-
glish morphology is much simpler compared to that
of Hebrew, making direct use of the Levinger con-
text free approximation impossible. However, some
morphological cues exist in English as well, in par-
ticular common suffixation patterns. We imple-
mented our morphology-based context-free p(t|w)
approximation for English as a special case of the
linear context-based algorithm described in Sect.3.
Instead of generating contexts based on neighboring
words, we generate them using the following 5 mor-
phological templates:
suff=S The word has suffix S (suff=ing).
L+suff=W,S The word appears just after word W ,
with suffix S (L+suff=have,ed).
R+suff=S,W The word appears just before wordW ,
with suffix S (R+suff=ing,to)
wsuf=S1,S2 The word suffix is S1, the same stem is
seen with suffix S2 (wsuf=,s).
suffs=SG The word stem appears with the SG group
of suffixes (suffs=ed,ing,s).
We consider a word to have a suffix only if the
word stem appears with a different suffix somewhere
in the text. We implemented a primitive stemmer
for extracting the suffixes while preserving a us-
able stem by taking care of few English orthogra-
phy rules (handling, e.g., , bigger ? big er, nicer
? nice er, happily ? happy ly, picnicking ? pic-
nic ing). For the immediate context W in the tem-
plates L+suff,R+suff, we consider only the 20 most
frequent tokens in the corpus.
Linear-Context-based p(t|w) approximation
We expect the context based approximation to be
particularly useful in English. We use the following
3 context templates: LL=w?2,w?1, LR=w?1,w+1
and RR=w+1,w+2. We estimate p(w|c) and p(c|w)
by relative frequency over word triplets occurring at
least twice in the unannotated training corpus.
Combined p(t|w) approximation This approx-
imation combines the morphological and linear
context approximations by using all the above-
mentioned context templates together in the iterative
process.
For all three p(t|w) approximations, we take
RELC to be contexts containing at most 4 tags.
allow(t, w) follows the dictionary for known words,
and is the set of all open-class POS for unknown
words. We take the initial p(t|w) for each w to be
uniform over all the dictionary specified tags for w.
Accordingly, the initial p(t|w) = 0 for w not in the
dictionary. We run the process for 8 iterations.7
Diluted Dictionaries and Unknown Words
Some of the missing dictionary elements are as-
signed a set of possible POS-tags and corresponding
probabilities in the p(t|w) estimation process. Other
unknown tokens remain with no analysis at the
end of the initial process computation. For these
missing elements, we assign an ambiguity class by
a simple ambiguity-class guesser, and set p(t|w)
to be uniform over all the tags in the ambiguity
class. Our ambiguity-class guesser assigns for each
word the set of all open-class tags that appeared
with the word suffix in the dictionary. The word
suffix is the longest (up to 3 characters) suffix of the
word that also appears in the top-100 suffixes in the
dictionary.
Taggers We test the resulting p(t|w) approxima-
tion by training 2 taggers: CF-Tag, a context-free
tagger assigning for each word its most probable
POS according to p(t|w), with a fallback to the most
probable tag in case the word does not appear in
the dictionary or if ?t, p(t|w) = 0. EM-HMM,
a second-order EM-HMM initialized with the esti-
mated p(t|w).
Baselines As baseline, we use two EM-trained
HMM taggers, initialized with a uniform p(t|w) for
every word, based on the allowed tags in the dic-
tionary. For words not in the dictionary, we take
the allowed tags to be either all the open-class POS
7This is the first value we tried, and it seems to work fine.
We haven?t experimented with other values. The same applies
for the choice of 4 as the RELC threshold.
751
(uniform(oc)) or the allowed tags according to our
simple ambiguity-class guesser (uniform(suf)).
All the p(t|w) estimates and HMM models are
trained on the entire WSJ corpus. We use the same
24K word test-set as used in SE, TJ and GG, as well
as the same diluted dictionaries. We report the re-
sults on the same reduced tagsets for comparison,
but also include the results on the full 46 tags tagset.
5.1 Results
Table 2 summarizes the results of our experiments.
Uniform initialization based on the simple suffix-
based ambiguity class guesser yields big improve-
ments over the uniform all-open-class initialization.
However, our refined initial conditions always im-
prove the results (by as much as 40% error re-
duction). As expected, the linear context is much
more effective than the morphological one, espe-
cially with richer dictionaries. This seem to indi-
cate that in English the linear context is better at re-
fining the estimations when the ambiguity classes
are known, while the morphological context is in
charge of adding possible tags when the ambigu-
ity classes are not known. Furthermore, the bene-
fit of the morphology-context is bigger for the com-
plete tagset setting, indicating that, while the coarse-
grained POS-tags are indicated by word distribu-
tion, the finer distinctions are indicated by inflec-
tions and orthography. The combination of linear
and morphology contexts is always beneficial. Syn-
tagmatic constraints (e.g., separating be verbs and
modals from the rest of the verbs) constantly im-
prove results by about 1%. Note that the context-free
tagger based on our p(t|w) estimates is quite accu-
rate. As with the EM trained models, combining lin-
ear and morphological contexts is always beneficial.
To put these numbers in context, Table 3 lists
current state-of-the art results for the same task.
CE+spl is the Contrastive-Estimation CRF method
of SE. BHMM is the completely Bayesian-HMM
of GG. PLSA+AC, LDA, LDA+AC are the mod-
els presented in TJ, LDA+AC is a Bayesian model
with a strong ambiguity class (AC) component, and
is the current state-of-the-art of this task. The other
models are variations excluding the Bayesian com-
ponents (PLSA+AC) or the ambiguity class.
While our models are trained on the unannotated
text of the entire WSJ Treebank, CE and BHMM use
much less training data (only the 24k words of the
test-set). However, as noted by TJ, there is no reason
one should limit the amount of unlabeled data used,
and in addition other results reported in GG,SE show
that accuracy does not seem to improve as more un-
labeled data are used with the models. We also re-
port results for training our EM-HMM tagger on the
smaller dataset (the p(t|w) estimation is still based
on the entire unlabeled WSJ).
All the abovementioned models follow the as-
sumption that all 17 tags are valid for the unknown
words. In contrast, we restrict the set of allowed
tags for an unknown word to open-class tags. Closed
class words are expected to be included in a dictio-
nary, even a small one. The practice of allowing only
open-class tags for unknown words goes back a long
way (Weischedel et al, 1993), and proved highly
beneficial also in our case.
Notice that even our simplest models, in which
the initial p(t|w) distribution for each w is uniform,
already outperform most of the other models, and,
in the case of the diluted dictionaries, by a wide
margin. Similarly, given the p(t|w) estimate, EM-
HMM training on the smaller dataset (24k) is still
very competitive (yet results improve with more un-
labeled data). When we use our refined p(t|w) dis-
tribution as the basis of EM-HMM training, we get
the best results for the complete dictionary case.
With the diluted dictionaries, we are outperformed
only by LDA+AC. As we outperform this model in
the complete dictionary case, it seems that the ad-
vantage of this model is due to its much stronger
ambiguity class model, and not its Bayesian com-
ponents. Also note that while we outperform this
model when using the 19-tags tagset, it is slightly
better in the original 17-tags setting. It could be that
the reliance of the LDA models on observed surface
features instead of hidden state features is beneficial
avoiding the misleading V-V transitions.
We also list the performance of our best mod-
els with a slightly more realistic dictionary setting:
we take our dictionary to include information for all
words occurring in section 0-18 of the WSJ corpus
(43208 words). We then train on the entire unanno-
tated corpus, and test on sections 22-24 ? the stan-
dard train/test split for supervised English POS tag-
ging. We achieve accuracy of 92.85% for the 19-
tags set, and 91.3% for the complete 46-tags tagset.
752
Initial Conditions Full dict ? 2 dict ? 3 dict
(49206 words) (2141 words) (1249 words)
CF-Tag EM-HMM CF-Tag EM-HMM CF-Tag EM-HMM
Uniform(oc) 81.7 88.7 68.4 81.9 62.5 79.6
Uniform(suf) NA NA 76.8 83.4 76.9 81.6
17tags Morph-Cont 82.2 88.6 73.3 83.9 69.1 81.7
Linear-Cont 90.1 92.9 81.1 87.8 78.3 85.8
Combined-Cont 89.9 93.3 83.1 88.5 81.1 86.4
Uniform(oc) 79.9 91.0 66.6 83.4 60.7 84.7
Uniform(suf) NA NA 75.1 86.5 73.1 86.7
19tags Morph-Cont 80.5 89.2 71.5 86.5 67.5 87.1
Linear-Cont 88.4 93.7 78.9 89.0 76.3 86.9
Combined-Cont 88.0 93.8 81.1 89.4 79.2 87.4
Uniform(oc) 76.7 88.3 61.2 * 55.7 *
Uniform(suf) NA NA 64.2 81.9 60.3 79.8
46tags Morph-Cont 74.8 88.8 65.6 83.0 61.9 80.3
Linear-Cont 85.5 91.2 74.5 84.0 70.1 82.2
Combined-Cont 85.9 91.4 75.4 85.5 72.4 83.3
Table 2: Accuracy (%) of English POS Tagging over various initial conditions
Dict InitEM-HMM (24k) LDA LDA+AC PLSA+AC CE+spl BHMM
Full 93.8 (91.1) 93.4 93.4 89.7 88.7 87.3
? 2 89.4 (87.9) 87.4 91.2 87.8 79.5 79.6
? 3 87.4 (85.9) 85 89.7 85.9 78.4 71
Table 3: Comparison of English Unsupervised POS Tagging Methods
6 Conclusion
We have demonstrated that unsupervised POS tag-
ging can reach good results using the robust EM-
HMM learner when provided with good initial con-
ditions, even with incomplete dictionaries. We pre-
sented a general family of algorithms to compute ef-
fective initial conditions: estimation of p(t|w) rely-
ing on an iterative process shifting probabilities be-
tween words and their contexts. The parameters of
this process (definition of the contexts and initial es-
timations of p(t|w) can safely encapsulate rich lin-
guistic intuitions.
While recent work, such as GG, aim to use the
Bayesian framework and incorporate ?linguistically
motivated priors?, in practice such priors currently
only account for the fact that language related dis-
tributions are sparse - a very general kind of knowl-
edge. In contrast, our method allow the incorpora-
tion of much more fine-grained intuitions.
We tested the method on the challenging task
of full morphological disambiguation in Hebrew
(which was our original motivation) and on the stan-
dard WSJ unsupervised POS tagging task.
In Hebrew, our model includes an improved ver-
sion of the similar words algorithm of (Levinger et
al., 1995), a model of lexical context, and a small
set of tag ngrams. The combination of these knowl-
edge sources in the initial conditions brings an error
reduction of more than 25% over a strong uniform
distribution baseline. In English, our model is com-
petitive with recent state-of-the-art results, while us-
ing simple and efficient learning methods.
The comparison with other algorithms indicates
directions of potential improvement: (1) our initial-
conditions method might benefit the other, more so-
phisticated learning algorithms as well. (2) Our
models were designed under the assumption of a
relatively complete dictionary. As such, they are
not very good at assigning ambiguity-classes to
OOV tokens when starting with a very small dic-
tionary. While we demonstrate competitive results
using a simple suffix-based ambiguity-class guesser
which ignores capitalization and hyphenation infor-
mation, we believe there is much room for improve-
ment in this respect. In particular, (Haghighi and
Klein, 2006) presents very strong results using a
distributional-similarity module and achieve impres-
sive tagging accuracy while starting with a mere
116 prototypical words. Experimenting with com-
bining similar models (as well as TJ?s ambiguity
class model) with our p(t|w) distribution estimation
method is an interesting research direction.
753
References
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev, Beer-Sheva, Israel.
Michele Banko and Robert C. Moore. 2004. Part-of-
speech tagging in context. In Proceedings of Coling
2004, pages 556?561, Geneva, Switzerland, Aug 23?
Aug 27. COLING.
Leonard E. Baum. 1972. An inequality and associ-
ated maximization technique in statistical estimation
for probabilistic functions of a Markov process. In-
equalities, 3:1?8.
Eric Brill. 1995a. Transformation-based error-driven
learning and natural languge processing: A case study
in part-of-speech tagging. Computational Linguistics,
21:543?565.
Eric Brill. 1995b. Unsupervised learning of disam-
biguation rules for part of speech tagging. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora, pages
1?13, Somerset, New Jersey. Association for Compu-
tational Linguistics.
Stanley F. Chen. 1996. Building Probabilistic Models for
Natural Language. Ph.D. thesis, Harvard University,
Cambridge, MA.
Silviu Cucerzan and David Yarowsky. 2000. Language
independent, minimally supervised induction of lex-
ical probabilities. In ACL ?00: Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 270?277, Morristown, NJ,
USA. Association for Computational Linguistics.
Evangelos Dermatas and George Kokkinakis. 1995. Au-
tomatic stochastic tagging of natural language texts.
Computational Linguistics, 21(2):137?163.
David Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Proceeding of ANLP-94.
Sharon Goldwater and Thomas L. Griffiths. 2007.
A fully bayesian approach to unsupervised part-of-
speech tagging. In Proceeding of ACL 2007, Prague,
Czech Republic.
Aria Haghighi and Dan Klein. 2006. Prototype-driven
learning for sequence models. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 320?
327, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
J. Kupiec. 1992. Robust part-of-speech tagging using
hidden Markov model. Computer Speech and Lan-
guage, 6:225?242.
Moshe Levinger, Uzi Ornan, and Alon Itai. 1995. Learn-
ing morpholexical probabilities from an untagged cor-
pus with an application to Hebrew. Computational
Linguistics, 21:383?404.
Christopher D. Manning and Hinrich Schutze. 1999.
Foundation of Statistical Language Processing. MIT
Press.
Bernard Merialdo. 1994. Tagging English text
with probabilistic model. Computational Linguistics,
20:155?171.
Hinrich Schu?tze. 1995. Distributional part-of-speech
tagging. In Proceedings of the seventh conference
on European chapter of the Association for Computa-
tional Linguistics, pages 141?148, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Danny Shacham and Shuly Wintner. 2007. Morpho-
logical disambiguation of hebrew: A case study in
classifier combination. In Proceeding of EMNLP-07,
Prague, Czech.
Khalil Sima?an, Alon Itai, Alon Altman Yoad Winter,
and Noa Nativ. 2001. Building a tree-bank of mod-
ern Hebrew text. Journal Traitement Automatique des
Langues (t.a.l.). Special Issue on NLP and Corpus
Linguistics.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
354?362, Ann Arbor, Michigan, June.
Scott M. Thede and Mary P. Harper. 1999. A second-
order hidden Markov model for part-of-speech tag-
ging. In Proceeding of ACL-99.
Kristina Toutanova and Mark Johnson. 2008. A bayesian
lda-based model for semi-supervised part-of-speech
tagging. In J.C. Platt, D. Koller, Y. Singer, and
S. Roweis, editors, Advances in Neural Information
Processing Systems 20. MIT Press, Cambridge, MA.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In HLT-
NAACL.
R. Weischedel, R. Schwartz, J. Palmucci, M. Meteer, and
L. Ramshaw. 1993. Coping with ambiguity and un-
known words through probabilistic models. Computa-
tional Linguistics, 19:359?382.
754
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 57?64,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Can You Tag the Modal? You Should.
Yael Netzer and Meni Adler and David Gabay and Michael Elhadad
Ben Gurion University of the Negev
Department of Computer Science
POB 653 Be?er Sheva, 84105, Israel
{yaeln,adlerm,gabayd,elhadad}@cs.bgu.ac.il
Abstract
Computational linguistics methods are typ-
ically first developed and tested in English.
When applied to other languages, assump-
tions from English data are often applied
to the target language. One of the most
common such assumptions is that a ?stan-
dard? part-of-speech (POS) tagset can be
used across languages with only slight vari-
ations. We discuss in this paper a specific is-
sue related to the definition of a POS tagset
for Modern Hebrew, as an example to clar-
ify the method through which such varia-
tions can be defined. It is widely assumed
that Hebrew has no syntactic category of
modals. There is, however, an identified
class of words which are modal-like in their
semantics, and can be characterized through
distinct syntactic and morphologic criteria.
We have found wide disagreement among
traditional dictionaries on the POS tag at-
tributed to such words. We describe three
main approaches when deciding how to tag
such words in Hebrew. We illustrate the im-
pact of selecting each of these approaches
on agreement among human taggers, and on
the accuracy of automatic POS taggers in-
duced for each method. We finally recom-
mend the use of a ?modal? tag in Hebrew
and provide detailed guidelines for this tag.
Our overall conclusion is that tagset defini-
tion is a complex task which deserves appro-
priate methodology.
1 Introduction
In this paper we address one linguistic issue that was
raised while tagging a Hebrew corpus for part of
speech (POS) and morphological information. Our
corpus is comprised of short news stories. It in-
cludes roughly 1,000,000 tokens, in articles of typ-
ical length between 200 to 1000 tokens. The arti-
cles are written in a relatively simple style, with a
high token/word ratio. Of the full corpus, a sam-
ple of articles comprising altogether 100,000 tokens
was assembled at random and manually tagged for
part of speech. We employed four students as tag-
gers. An initial set of guidelines was first composed,
relying on the categories found in several dictionar-
ies and on the Penn treebank POS guidelines (San-
torini, 1995). Tagging was done using an automatic
tool1. We relied on existing computational lexicons
(Segal, 2000; Yona, 2004) to generate candidate tags
for each word. As many words from the corpus were
either missing or tagged in a non uniform manner in
the lexicons, we recommended looking up missing
words in traditional dictionaries. Disagreement was
also found among copyrighted dictionaries, both for
open and closed set categories. Given the lack of
a reliable lexicon, the taggers were not given a list
of options to choose from, but were free to tag with
whatever tag they found suitable. The process, al-
though slower and bound to produce unintentional
mistakes, was used for building a lexicon, and to
refine the guidelines and on occasion modify the
POS tagset. When constructing and then amending
the guidelines we sought the best trade-off between
1http://wordfreak.sourceforge.net
57
accuracy and meaningfulness of the categorization,
and simplicity of the guidelines, which is important
for consistent tagging.
Initially, each text was tagged by four different
people, and the guidelines were revised according
to questions or disagreements that were raised. As
the guidelines became more stable, the disagreement
rate decreased, each text was tagged by three peo-
ple only and eventually two taggers and a referee
that reviewed disagreements between the two. The
disagreement rate between any two taggers was ini-
tially as high as 20%, and dropped to 3% after a few
rounds of tagging and revising the guidelines.
Major sources of disagreements that were identi-
fied, include:
Prepositional phrases vs. prepositions In Hebrew,
formative letters ?      b,c,l,m2 ? can be attached
to a noun to create a short prepositional phrase. In
some cases, such phrases function as a preposition
and the original meaning of the noun is not clearly
felt. Some taggers would tag the word as a prepo-
sitional prefix + noun, while others tagged it as a
preposition, e.g., 
	 
 b?iqbot (following), that
can be tagged as 	 
 b-iqbot (in the footsteps
of).
Adverbial phrases vs. Adverbs the problem is simi-
lar to the one above, e.g.,  	  bdiyuq (exactly), can
be tagged as b-diyuq (with accuracy).
Participles vs. Adjectives as both categories can
modify nouns, it is hard to distinguish between
them, e.g,   Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 117?125,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Efficient Tree-based Approximation for Entailment Graph Learning
Jonathan Berant?, Ido Dagan?, Meni Adler?, Jacob Goldberger?
? The Blavatnik School of Computer Science, Tel Aviv University
? Department of Computer Science, Bar-Ilan University
? Faculty of Engineering, Bar-Ilan University
jonatha6@post.tau.ac.il
{dagan,goldbej}@{cs,eng}.biu.ac.il
adlerm@cs.bgu.ac.il
Abstract
Learning entailment rules is fundamental in
many semantic-inference applications and has
been an active field of research in recent years.
In this paper we address the problem of learn-
ing transitive graphs that describe entailment
rules between predicates (termed entailment
graphs). We first identify that entailment
graphs exhibit a ?tree-like? property and are
very similar to a novel type of graph termed
forest-reducible graph. We utilize this prop-
erty to develop an iterative efficient approxi-
mation algorithm for learning the graph edges,
where each iteration takes linear time. We
compare our approximation algorithm to a
recently-proposed state-of-the-art exact algo-
rithm and show that it is more efficient and
scalable both theoretically and empirically,
while its output quality is close to that given
by the optimal solution of the exact algorithm.
1 Introduction
Performing textual inference is in the heart of many
semantic inference applications such as Question
Answering (QA) and Information Extraction (IE). A
prominent generic paradigm for textual inference is
Textual Entailment (TUE) (Dagan et al, 2009). In
TUE, the goal is to recognize, given two text frag-
ments termed text and hypothesis, whether the hy-
pothesis can be inferred from the text. For example,
the text ?Cyprus was invaded by the Ottoman Em-
pire in 1571? implies the hypothesis ?The Ottomans
attacked Cyprus?.
Semantic inference applications such as QA and
IE crucially rely on entailment rules (Ravichandran
and Hovy, 2002; Shinyama and Sekine, 2006) or
equivalently inference rules, that is, rules that de-
scribe a directional inference relation between two
fragments of text. An important type of entailment
rule specifies the entailment relation between natu-
ral language predicates, e.g., the entailment rule ?X
invade Y ? X attack Y? can be helpful in inferring
the aforementioned hypothesis. Consequently, sub-
stantial effort has been made to learn such rules (Lin
and Pantel, 2001; Sekine, 2005; Szpektor and Da-
gan, 2008; Schoenmackers et al, 2010).
Textual entailment is inherently a transitive rela-
tion , that is, the rules ?x ? y? and ?y ? z? imply
the rule ?x ? z?. Accordingly, Berant et al (2010)
formulated the problem of learning entailment rules
as a graph optimization problem, where nodes are
predicates and edges represent entailment rules that
respect transitivity. Since finding the optimal set of
edges respecting transitivity is NP-hard, they em-
ployed Integer Linear Programming (ILP) to find the
exact solution. Indeed, they showed that applying
global transitivity constraints improves rule learning
comparing to methods that ignore graph structure.
More recently, Berant et al (Berant et al, 2011) in-
troduced a more efficient exact algorithm, which de-
composes the graph into connected components and
then applies an ILP solver over each component.
Despite this progress, finding the exact solution
remains NP-hard ? the authors themselves report
they were unable to solve some graphs of rather
moderate size and that the coverage of their method
is limited. Thus, scaling their algorithm to data sets
with tens of thousands of predicates (e.g., the extrac-
tions of Fader et al (2011)) is unlikely.
117
In this paper we present a novel method for learn-
ing the edges of entailment graphs. Our method
computes much more efficiently an approximate so-
lution that is empirically almost as good as the exact
solution. To that end, we first (Section 3) conjecture
and empirically show that entailment graphs exhibit
a ?tree-like? property, i.e., that they can be reduced
into a structure similar to a directed forest.
Then, we present in Section 4 our iterative ap-
proximation algorithm, where in each iteration a
node is removed and re-attached back to the graph in
a locally-optimal way. Combining this scheme with
our conjecture about the graph structure enables a
linear algorithm for node re-attachment. Section 5
shows empirically that this algorithm is by orders of
magnitude faster than the state-of-the-art exact al-
gorithm, and that though an optimal solution is not
guaranteed, the area under the precision-recall curve
drops by merely a point.
To conclude, the contribution of this paper is two-
fold: First, we define a novel modeling assumption
about the tree-like structure of entailment graphs and
demonstrate its validity. Second, we exploit this as-
sumption to develop a polynomial approximation al-
gorithm for learning entailment graphs that can scale
to much larger graphs than in the past. Finally, we
note that learning entailment graphs bears strong
similarities to related tasks such as Taxonomy In-
duction (Snow et al, 2006) and Ontology induction
(Poon and Domingos, 2010), and thus our approach
may improve scalability in these fields as well.
2 Background
Until recently, work on learning entailment rules be-
tween predicates considered each rule independently
of others and did not exploit global dependencies.
Most methods utilized the distributional similarity
hypothesis that states that semantically similar pred-
icates occur with similar arguments (Lin and Pan-
tel, 2001; Szpektor et al, 2004; Yates and Etzioni,
2009; Schoenmackers et al, 2010). Some meth-
ods extracted rules from lexicographic resources
such as WordNet (Szpektor and Dagan, 2009) or
FrameNet (Bob and Rambow, 2009; Ben Aharon et
al., 2010), and others assumed that semantic rela-
tions between predicates can be deduced from their
co-occurrence in a corpus via manually-constructed
patterns (Chklovski and Pantel, 2004).
Recently, Berant et al (2010; 2011) formulated
the problem as the problem of learning global entail-
ment graphs. In entailment graphs, nodes are predi-
cates (e.g., ?X attack Y?) and edges represent entail-
ment rules between them (?X invade Y ? X attack
Y?). For every pair of predicates i, j, an entailment
score wij was learned by training a classifier over
distributional similarity features. A positive wij in-
dicated that the classifier believes i? j and a nega-
tive wij indicated that the classifier believes i 9 j.
Given the graph nodes V (corresponding to the pred-
icates) and the weighting function w : V ? V ? R,
they aim to find the edges of a graph G = (V,E)
that maximize the objective
?
(i,j)?E wij under the
constraint that the graph is transitive (i.e., for every
node triplet (i, j, k), if (i, j) ? E and (j, k) ? E,
then (i, k) ? E).
Berant et al proved that this optimization prob-
lem, which we term Max-Trans-Graph, is NP-hard,
and so described it as an Integer Linear Program
(ILP). Let xij be a binary variable indicating the ex-
istence of an edge i ? j in E. Then, X = {xij :
i 6= j} are the variables of the following ILP for
Max-Trans-Graph:
argmax
X
?
i 6=j
wij ? xij (1)
s.t. ?i,j,k?V xij + xjk ? xik ? 1
?i,j?V xij ? {0, 1}
The objective function is the sum of weights over the
edges of G and the constraint xij + xjk ? xik ? 1
on the binary variables enforces that whenever xij=
xjk=1, then also xik = 1 (transitivity).
Since ILP is NP-hard, applying an ILP solver di-
rectly does not scale well because the number of
variables isO(|V |2) and the number of constraints is
O(|V |3). Thus, even a graph with?80 nodes (predi-
cates) has more than half a million constraints. Con-
sequently, in (Berant et al, 2011), they proposed a
method that efficiently decomposes the graph into
smaller components and applies an ILP solver on
each component separately using a cutting-plane
procedure (Riedel and Clarke, 2006). Although this
method is exact and improves scalability, it does
not guarantee an efficient solution. When the graph
does not decompose into sufficiently small compo-
nents, and the weights generate many violations of
118
transitivity, solving Max-Trans-Graph becomes in-
tractable. To address this problem, we present in
this paper a method for approximating the optimal
set of edges within each component and show that
it is much more efficient and scalable both theoreti-
cally and empirically.
Do and Roth (2010) suggested a method for a re-
lated task of learning taxonomic relations between
terms. Given a pair of terms, a small graph is con-
structed and constraints are imposed on the graph
structure. Their work, however, is geared towards
scenarios where relations are determined on-the-fly
for a given pair of terms and no global knowledge
base is explicitly constructed. Thus, their method
easily produces solutions where global constraints,
such as transitivity, are violated.
Another approximation method that violates tran-
sitivity constraints is LP relaxation (Martins et al,
2009). In LP relaxation, the constraint xij ? {0, 1}
is replaced by 0 ? xij ? 1, transforming the prob-
lem from an ILP to a Linear Program (LP), which
is polynomial. An LP solver is then applied on the
problem, and variables xij that are assigned a frac-
tional value are rounded to their nearest integer and
so many violations of transitivity easily occur. The
solution when applying LP relaxation is not a transi-
tive graph, but nevertheless we show for comparison
in Section 5 that our method is much faster.
Last, we note that transitive relations have been
explored in adjacent fields such as Temporal Infor-
mation Extraction (Ling and Weld, 2010), Ontol-
ogy Induction (Poon and Domingos, 2010), and Co-
reference Resolution (Finkel and Manning, 2008).
3 Forest-reducible Graphs
The entailment relation, described by entailment
graphs, is typically from a ?semantically-specific?
predicate to a more ?general? one. Thus, intuitively,
the topology of an entailment graph is expected to be
?tree-like?. In this section we first formalize this in-
tuition and then empirically analyze its validity. This
property of entailment graphs is an interesting topo-
logical observation on its own, but also enables the
efficient approximation algorithm of Section 4.
For a directed edge i ? j in a directed acyclic
graphs (DAG), we term the node i a child of node
j, and j a parent of i. A directed forest is a DAG
Xdisease be 
epidemic in 
 Ycountry 
Xdisease 
common in 
 Ycountry 
Xdisease 
occur in 
 Ycountry 
Xdisease 
frequent in 
 Ycountry 
Xdisease 
begin in 
 Ycountry 
be epidemic in 
common in 
frequent in 
occur in 
begin in 
be epidemic in 
common in 
 frequent in 
occur in 
begin in 
(a) 
(b) 
(c) 
Figure 1: A fragment of an entailment graph (a), its SCC
graph (b) and its reduced graph (c). Nodes are predicates
with typed variables (see Section 5), which are omitted in
(b) and (c) for compactness.
where all nodes have no more than one parent.
The entailment graph in Figure 1a (subgraph from
the data set described in Section 5) is clearly not a
directed forest ? it contains a cycle of size two com-
prising the nodes ?X common in Y? and ?X frequent in
Y?, and in addition the node ?X be epidemic in Y? has
3 parents. However, we can convert it to a directed
forest by applying the following operations. Any
directed graph G can be converted into a Strongly-
Connected-Component (SCC) graph in the follow-
ing way: every strongly connected component (a set
of semantically-equivalent predicates, in our graphs)
is contracted into a single node, and an edge is added
from SCC S1 to SCC S2 if there is an edge in G from
some node in S1 to some node in S2. The SCC graph
is always a DAG (Cormen et al, 2002), and if G is
transitive then the SCC graph is also transitive. The
graph in Figure 1b is the SCC graph of the one in
119
Xcountry annex  Yplace 
Xcountry invade  Yplace Yplace be part of Xcountry  
Figure 2: A fragment of an entailment graph that is not
an FRG.
Figure 1a, but is still not a directed forest since the
node ?X be epidemic in Y? has two parents.
The transitive closure of a directed graph G is
obtained by adding an edge from node i to node j
if there is a path in G from i to j. The transitive
reduction of G is obtained by removing all edges
whose absence does not affect its transitive closure.
In DAGs, the result of transitive reduction is unique
(Aho et al, 1972). We thus define the reduced graph
Gred = (Vred, Ered) of a directed graph G as the
transitive reduction of its SCC graph. The graph in
Figure 1c is the reduced graph of the one in Fig-
ure 1a and is a directed forest. We say a graph is a
forest-reducible graph (FRG) if all nodes in its re-
duced form have no more than one parent.
We now hypothesize that entailment graphs are
FRGs. The intuition behind this assumption is
that the predicate on the left-hand-side of a uni-
directional entailment rule has a more specific mean-
ing than the one on the right-hand-side. For instance,
in Figure 1a ?X be epidemic in Y? (where ?X? is a type
of disease and ?Y? is a country) is more specific than
?X common in Y? and ?X frequent in Y?, which are
equivalent, while ?X occur in Y? is even more gen-
eral. Accordingly, the reduced graph in Figure 1c
is an FRG. We note that this is not always the case:
for example, the entailment graph in Figure 2 is not
an FRG, because ?X annex Y? entails both ?Y be part
of X? and ?X invade Y?, while the latter two do not
entail one another. However, we hypothesize that
this scenario is rather uncommon. Consequently, a
natural variant of the Max-Trans-Graph problem is
to restrict the required output graph of the optimiza-
tion problem (1) to an FRG. We term this problem
Max-Trans-Forest.
To test whether our hypothesis holds empirically
we performed the following analysis. We sampled
7 gold standard entailment graphs from the data set
described in Section 5, manually transformed them
into FRGs by deleting a minimal number of edges,
and measured recall over the set of edges in each
graph (precision is naturally 1.0, as we only delete
gold standard edges). The lowest recall value ob-
tained was 0.95, illustrating that deleting a very
small proportion of edges converts an entailment
graph into an FRG. Further support for the prac-
tical validity of this hypothesis is obtained from
our experiments in Section 5. In these experiments
we show that exactly solving Max-Trans-Graph and
Max-Trans-Forest (with an ILP solver) results in
nearly identical performance.
An ILP formulation for Max-Trans-Forest is sim-
ple ? a transitive graph is an FRG if all nodes in
its reduced graph have no more than one parent. It
can be verified that this is equivalent to the following
statement: for every triplet of nodes i, j, k, if i ? j
and i ? k, then either j ? k or k ? j (or both).
Therefore, the ILP is formulated by adding this lin-
ear constraint to ILP (1):
?i,j,k?V xij+xik+(1? xjk)+(1? xkj) ? 3 (2)
We note that despite the restriction to FRGs, Max-
Trans-Forest is an NP-hard problem by a reduction
from the X3C problem (Garey and Johnson, 1979).
We omit the reduction details for brevity.
4 Sequential Approximation Algorithms
In this section we present Tree-Node-Fix, an efficient
approximation algorithm for Max-Trans-Forest, as
well as Graph-Node-Fix, an approximation for Max-
Trans-Graph.
4.1 Tree-Node-Fix
The scheme of Tree-Node-Fix (TNF) is the follow-
ing. First, an initial FRG is constructed, using some
initialization procedure. Then, at each iteration a
single node v is re-attached (see below) to the FRG
in a way that improves the objective function. This
is repeated until the value of the objective function
cannot be improved anymore by re-attaching a node.
Re-attaching a node v is performed by removing
v from the graph and connecting it back with a better
set of edges, while maintaining the constraint that it
is an FRG. This is done by considering all possible
edges from/to the other graph nodes and choosing
120
(a) 
d 
c 
v ? c v 
c 
d1 ? d2 
v 
? ? ? 
r1 r2 
v (b) (b?) (c) 
r3 
? 
Figure 3: (a) Inserting v into a component c ? Vred. (b)
Inserting v as a child of c and a parent of a subset of c?s
children in Gred. (b?) A node d that is a descendant but
not a child of c can not choose v as a parent, as v becomes
its second parent. (c) Inserting v as a new root.
the optimal subset, while the rest of the graph re-
mains fixed. Formally, let Sv?in =
?
i 6=v wiv ? xiv
be the sum of scores over v?s incoming edges and
Sv?out =
?
k 6=v wvk ? xvk be the sum of scores over
v?s outgoing edges. Re-attachment amounts to opti-
mizing a linear objective:
argmax
Xv
(Sv-in + Sv-out) (3)
where the variables Xv ? X are indicators for all
pairs of nodes involving v. We approximate a solu-
tion for (1) by iteratively optimizing the simpler ob-
jective (3). Clearly, at each re-attachment the value
of the objective function cannot decrease, since the
optimization algorithm considers the previous graph
as one of its candidate solutions.
We now show that re-attaching a node v is lin-
ear. To analyze v?s re-attachment, we consider the
structure of the directed forest Gred just before v is
re-inserted, and examine the possibilities for v?s in-
sertion relative to that structure. We start by defin-
ing some helpful notations. Every node c ? Vred
is a connected component in G. Let vc ? c be an
arbitrary representative node in c. We denote by
Sv-in(c) the sum of weights from all nodes in c and
their descendants to v, and by Sv-out(c) the sum of
weights from v to all nodes in c and their ancestors:
Sv-in(c) =
?
i?c
wiv +
?
k /?c
wkvxkvc
Sv-out(c) =
?
i?c
wvi +
?
k /?c
wvkxvck
Note that {xvck, xkvc} are edge indicators in G
and not Gred. There are two possibilities for re-
attaching v ? either it is inserted into an existing
component c ? Vred (Figure 3a), or it forms a new
component. In the latter, there are also two cases:
either v is inserted as a child of a component c (Fig-
ure 3b), or not and then it becomes a root in Gred
(Figure 3c). We describe the details of these 3 cases:
Case 1: Inserting v into a component c ? Vred.
In this case we add in G edges from all nodes in c
and their descendants to v and from v to all nodes in
c and their ancestors. The score (3) in this case is
s1(c) , Sv-in(c) + Sv-out(c) (4)
Case 2: Inserting v as a child of some c ? Vred.
Once c is chosen as the parent of v, choosing v?s
children in Gred is substantially constrained. A node
that is not a descendant of c can not become a child
of v, since this would create a new path from that
node to c and would require by transitivity to add a
corresponding directed edge to c (but all graph edges
not connecting v are fixed). Moreover, only a direct
child of c can choose v as a parent instead of c (Fig-
ure 3b), since for any other descendant of c, v would
become a second parent, and Gred will no longer be
a directed forest (Figure 3b?). Thus, this case re-
quires adding in G edges from v to all nodes in c and
their ancestors, and also for each new child of v, de-
noted by d ? Vred, we add edges from all nodes in
d and their descendants to v. Crucially, although the
number of possible subsets of c?s children in Gred is
exponential, the fact that they are independent trees
in Gred allows us to go over them one by one, and
decide for each one whether it will be a child of v
or not, depending on whether Sv-in(d) is positive.
Therefore, the score (3) in this case is:
s2(c) , Sv-out(c)+
?
d?child(c)
max(0, Sv-in(d)) (5)
where child(c) are the children of c.
Case 3: Inserting v as a new root in Gred. Similar
to case 2, only roots of Gred can become children of
v. In this case for each chosen root r we add in G
edges from the nodes in r and their descendants to
v. Again, each root can be examined independently.
Therefore, the score (3) of re-attaching v is:
s3 ,
?
r
max(0, Sv-in(r)) (6)
where the summation is over the roots of Gred.
It can be easily verified that Sv-in(c) and
Sv-out(c) satisfy the recursive definitions:
121
Algorithm 1 Computing optimal re-attachment
Input: FRG G = (V,E), function w, node v ? V
Output: optimal re-attachment of v
1: remove v and compute Gred = (Vred, Ered).
2: for all c ? Vred in post-order compute Sv-in(c) (Eq.
7)
3: for all c ? Vred in pre-order compute Sv-out(c) (Eq.
8)
4: case 1: s1 = maxc?Vred s1(c) (Eq. 4)
5: case 2: s2 = maxc?Vred s2(c) (Eq. 5)
6: case 3: compute s3 (Eq. 6)
7: re-attach v according to max(s1, s2, s3).
Sv-in(c) =
?
i?c
wiv +
?
d?child(c)
Sv-in(d), c ? Vred (7)
Sv-out(c) =
?
i?c
wvi + Sv-out(p), c ? Vred (8)
where p is the parent of c in Gred. These recursive
definitions allow to compute in linear time Sv-in(c)
and Sv-out(c) for all c (given Gred) using dynamic
programming, before going over the cases for re-
attaching v. Sv-in(c) is computed going over Vred
leaves-to-root (post-order), and Sv-out(c) is com-
puted going over Vred root-to-leaves (pre-order).
Re-attachment is summarized in Algorithm 1.
Computing an SCC graph is linear (Cormen et al,
2002) and it is easy to verify that transitive reduction
in FRGs is also linear (Line 1). Computing Sv-in(c)
and Sv-out(c) (Lines 2-3) is also linear, as explained.
Cases 1 and 3 are trivially linear and in case 2 we go
over the children of all nodes in Vred. As the reduced
graph is a forest, this simply means going over all
nodes of Vred, and so the entire algorithm is linear.
Since re-attachment is linear, re-attaching all
nodes is quadratic. Thus if we bound the number
of iterations over all nodes, the overall complexity is
quadratic. This is dramatically more efficient and
scalable than applying an ILP solver. In Section
5 we ran TNF until convergence and the maximal
number of iterations over graph nodes was 8.
4.2 Graph-node-fix
Next, we show Graph-Node-Fix (GNF), a similar
approximation that employs the same re-attachment
strategy but does not assume the graph is an FRG.
Thus, re-attachment of a node v is done with an
ILP solver. Nevertheless, the ILP in GNF is sim-
pler than (1), since we consider only candidate edges
v  
i  k  
v  
i  k  
v
i k
v  
i  k  
Figure 4: Three types of transitivity constraint violations.
involving v. Figure 4 illustrates the three types of
possible transitivity constraint violations when re-
attaching v. The left side depicts a violation when
(i, k) /? E, expressed by the constraint in (9) below,
and the middle and right depict two violations when
the edge (i, k) ? E, expressed by the constraints
in (10). Thus, the ILP is formulated by adding the
following constraints to the objective function (3):
?i,k?V \{v} if (i, k) /? E, xiv + xvk ? 1 (9)
if (i, k) ? E, xvi ? xvk, xkv ? xiv (10)
xiv, xvk ? {0, 1} (11)
Complexity is exponential due to the ILP solver;
however, the ILP size is reduced by an order of mag-
nitude to O(|V |) variables and O(|V |2) constraints.
4.3 Adding local constraints
For some pairs of predicates i, j we sometimes have
prior knowledge whether i entails j or not. We term
such pairs local constraints, and incorporate them
into the aforementioned algorithms in the following
way. In all algorithms that apply an ILP solver, we
add a constraint xij = 1 if i entails j or xij = 0 if i
does not entail j. Similarly, in TNF we incorporate
local constraints by settingwij =? orwij = ??.
5 Experiments and Results
In this section we empirically demonstrate that TNF
is more efficient than other baselines and its output
quality is close to that given by the optimal solution.
5.1 Experimental setting
In our experiments we utilize the data set released
by Berant et al (2011). The data set contains 10 en-
tailment graphs, where graph nodes are typed pred-
icates. A typed predicate (e.g., ?Xdisease occur in
Ycountry?) includes a predicate and two typed vari-
ables that specify the semantic type of the argu-
ments. For instance, the typed variable Xdisease can
be instantiated by arguments such as ?flu? or ?dia-
betes?. The data set contains 39,012 potential edges,
122
of which 3,427 are annotated as edges (valid entail-
ment rules) and 35,585 are annotated as non-edges.
The data set alo contains, for every pair of pred-
icates i, j in every graph, a local score sij , which is
the output of a classifier trained over distributional
similarity features. A positive sij indicates that the
classifier believes i? j. The weighting function for
the graph edges w is defined as wij = sij??, where
? is a single parameter controlling graph sparseness:
as ? increases, wij decreases and becomes nega-
tive for more pairs of predicates, rendering the graph
more sparse. In addition, the data set contains a set
of local constraints (see Section 4.3).
We implemented the following algorithms for
learning graph edges, where in all of them the graph
is first decomposed into components according to
Berant et als method, as explained in Section 2.
No-trans Local scores are used without transitiv-
ity constraints ? an edge (i, j) is inserted iffwij > 0.
Exact-graph Berant et al?s exact method (2011)
for Max-Trans-Graph, which utilizes an ILP solver1.
Exact-forest Solving Max-Trans-Forest exactly
by applying an ILP solver (see Eq. 2).
LP-relax Solving Max-Trans-Graph approxi-
mately by applying LP-relaxation (see Section 2)
on each graph component. We apply the LP solver
within the same cutting-plane procedure as Exact-
graph to allow for a direct comparison. This also
keeps memory consumption manageable, as other-
wise all |V |3 constraints must be explicitly encoded
into the LP. As mentioned, our goal is to present
a method for learning transitive graphs, while LP-
relax produces solutions that violate transitivity.
However, we run it on our data set to obtain empiri-
cal results, and to compare run-times against TNF.
Graph-Node-Fix (GNF) Initialization of each
component is performed in the following way: if the
graph is very sparse, i.e. ? ? C for some constantC
(set to 1 in our experiments), then solving the graph
exactly is not an issue and we use Exact-graph. Oth-
erwise, we initialize by applying Exact-graph in a
sparse configuration, i.e., ? = C.
Tree-Node-Fix (TNF) Initialization is done as in
GNF, except that if it generates a graph that is not an
FRG, it is corrected by a simple heuristic: for every
node in the reduced graph Gred that has more than
1We use the Gurobi optimization package in all experiments.
l
l
l
l
l
l
l
?0.8 ?0.6 ?0.4 ?0.2 0.0
10
50
100
500
500
0
500
00
?lambda
sec
l Exact?graphLP?relaxGNFTNF
Figure 5: Run-time in seconds for various ?? values.
one parent, we choose from its current parents the
single one whose SCC is composed of the largest
number of nodes in G.
We evaluate algorithms by comparing the set of
gold standard edges with the set of edges learned by
each algorithm. We measure recall, precision and
F1 for various values of the sparseness parameter
?, and compute the area under the precision-recall
Curve (AUC) generated. Efficiency is evaluated by
comparing run-times.
5.2 Results
We first focus on run-times and show that TNF is
efficient and has potential to scale to large data sets.
Figure 5 compares run-times2 of Exact-graph,
GNF, TNF, and LP-relax as ?? increases and the
graph becomes denser. Note that the y-axis is in
logarithmic scale. Clearly, Exact-graph is extremely
slow and run-time increases quickly. For ? = 0.3
run-time was already 12 hours and we were unable
to obtain results for ? < 0.3, while in TNF we easily
got a solution for any ?. When ? = 0.6, where both
Exact-graph and TNF achieve best F1, TNF is 10
times faster than Exact-graph. When ? = 0.5, TNF
is 50 times faster than Exact-graph and so on. Most
importantly, run-time for GNF and TNF increases
much more slowly than for Exact-graph.
2Run on a multi-core 2.5GHz server with 32GB of RAM.
123
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
0.0
0.2
0.4
0.6
0.8
1.0
recall
prec
isio
n
l
l
ll l
l
l
l l
l
l
l l l
l
l l l
l
l
l
l Exact?graphTNFNo?trans
Figure 6: Precision (y-axis) vs. recall (x-axis) curve.
Maximal F1 on the curve is .43 for Exact-graph, .41 for
TNF, and .34 for No-trans. AUC in the recall range 0-0.5
is .32 for Exact-graph, .31 for TNF, and .26 for No-trans.
Run-time of LP-relax is also bad compared to
TNF and GNF. Run-time increases more slowly than
Exact-graph, but still very fast comparing to TNF.
When ? = 0.6, LP-relax is almost 10 times slower
than TNF, and when ? = ?0.1, LP-relax is 200
times slower than TNF. This points to the difficulty
of scaling LP-relax to large graphs.
As for the quality of learned graphs, Figure 6 pro-
vides a precision-recall curve for Exact-graph, TNF
and No-trans (GNF and LP-relax are omitted from
the figure and described below to improve readabil-
ity). We observe that both Exact-graph and TNF
substantially outperform No-trans and that TNF?s
graph quality is only slightly lower than Exact-graph
(which is extremely slow). Following Berant et al,
we report in the caption the maximal F1 on the curve
and AUC in the recall range 0-0.5 (the widest range
for which we have results for all algorithms). Note
that compared to Exact-graph, TNF reduces AUC by
a point and the maximal F1 score by 2 points only.
GNF results are almost identical to those of TNF
(maximal F1=0.41, AUC: 0.31), and in fact for all
? configurations TNF outperforms GNF by no more
than one F1 point. As for LP-relax, results are just
slightly lower than Exact-graph (maximal F1: 0.43,
AUC: 0.32), but its output is not a transitive graph,
and as shown above run-time is quite slow. Last, we
note that the results of Exact-forest are almost iden-
tical to Exact-graph (maximal F1: 0.43), illustrating
that assuming that entailment graphs are FRGs (Sec-
tion 3) is reasonable in this data set.
To conclude, TNF learns transitive entailment
graphs of good quality much faster than Exact-
graph. Our experiment utilized an available data
set of moderate size; However, we expect TNF to
scale to large data sets (that are currently unavail-
able), where other baselines would be impractical.
6 Conclusion
Learning large and accurate resources of entailment
rules is essential in many semantic inference appli-
cations. Employing transitivity has been shown to
improve rule learning, but raises issues of efficiency
and scalability.
The first contribution of this paper is a novel mod-
eling assumption that entailment graphs are very
similar to FRGs, which is analyzed and validated
empirically. The main contribution of the paper is
an efficient polynomial approximation algorithm for
learning entailment rules, which is based on this
assumption. We demonstrate empirically that our
method is by orders of magnitude faster than the
state-of-the-art exact algorithm, but still produces an
output that is almost as good as the optimal solution.
We suggest our method as an important step to-
wards scalable acquisition of precise entailment re-
sources. In future work, we aim to evaluate TNF on
large graphs that are automatically generated from
huge corpora. This of course requires substantial ef-
forts of pre-processing and test-set annotation. We
also plan to examine the benefit of TNF in learning
similar structures, e.g., taxonomies or ontologies.
Acknowledgments
This work was partially supported by the Israel
Science Foundation grant 1112/08, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Community?s Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT). The first author has carried out
this research in partial fulfilment of the requirements
for the Ph.D. degree.
124
References
Alfred V. Aho, Michael R. Garey, and Jeffrey D. Ullman.
1972. The transitive reduction of a directed graph.
SIAM Journal on Computing, 1(2):131?137.
Roni Ben Aharon, Idan Szpektor, and Ido Dagan. 2010.
Generating entailment rules from framenet. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2011. Global learning of typed entailment rules. In
Proceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics.
Coyne Bob and Owen Rambow. 2009. Lexpar: A freely
available english paraphrase lexicon automatically ex-
tracted from framenet. In Proceedings of IEEE Inter-
national Conference on Semantic Computing.
Timothy Chklovski and Patrick Pantel. 2004. Verb
ocean: Mining the web for fine-grained semantic verb
relations. In Proceedings of Empirical Methods in
Natural Language Processing.
Thomas H. Cormen, Charles E. leiserson, Ronald L.
Rivest, and Clifford Stein. 2002. Introduction to Al-
gorithms. The MIT Press.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(4):1?17.
Quang Do and Dan Roth. 2010. Constraints based tax-
onomic relation classification. In Proceedings of Em-
pirical Methods in Natural Language Processing.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of Empirical Methods in Nat-
ural Language Processing.
J. R. Finkel and C. D. Manning. 2008. Enforcing transi-
tivity in coreference resolution. In Proceedings of the
46th Annual Meeting of the Association for Computa-
tional Linguistics.
Michael R. Garey and David S. Johnson. 1979. Comput-
ers and Intractability: A Guide to the Theory of NP-
Completeness. W. H. Freeman.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7(4):343?360.
Xiao Ling and Dan S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of the 24th AAAI Con-
ference on Artificial Intelligence.
Andre Martins, Noah Smith, and Eric Xing. 2009. Con-
cise integer linear programming formulations for de-
pendency parsing. In Proceedings of the 47th Annual
Meeting of the Association for Computational Linguis-
tics.
Hoifung Poon and Pedro Domingos. 2010. Unsuper-
vised ontology induction from text. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics.
Deepak Ravichandran and Eduard Hovy. 2002. Learning
surface text patterns for a question answering system.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics.
Sebastian Riedel and James Clarke. 2006. Incremental
integer linear programming for non-projective depen-
dency parsing. In Proceedings of Empirical Methods
in Natural Language Processing.
Stefan Schoenmackers, Jesse Davis, Oren Etzioni, and
Daniel S. Weld. 2010. Learning first-order horn
clauses from web text. In Proceedings of Empirical
Methods in Natural Language Processing.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between ne pairs. In
Proceedings of IWP.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive
information extraction using unrestricted relation dis-
covery. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference.
Rion Snow, Dan Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous ev-
idence. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of the
22nd International Conference on Computational Lin-
guistics.
Idan Szpektor and Ido Dagan. 2009. Augmenting
wordnet-based inference with argument mapping. In
Proceedings of TextInfer.
Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-
tura Coppola. 2004. Scaling web-based acquisition
of entailment relations. In Proceedings of Empirical
Methods in Natural Language Processing.
Alexander Yates and Oren Etzioni. 2009. Unsupervised
methods for determining object and relation synonyms
on the web. Journal of Artificial Intelligence Research,
34:255?296.
125
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 79?84,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Entailment-based Text Exploration
with Application to the Health-care Domain
Meni Adler
Bar Ilan University
Ramat Gan, Israel
adlerm@cs.bgu.ac.il
Jonathan Berant
Tel Aviv University
Tel Aviv, Israel
jonatha6@post.tau.ac.il
Ido Dagan
Bar Ilan University
Ramat Gan, Israel
dagan@cs.biu.ac.il
Abstract
We present a novel text exploration model,
which extends the scope of state-of-the-art
technologies by moving from standard con-
cept-based exploration to statement-based ex-
ploration. The proposed scheme utilizes the
textual entailment relation between statements
as the basis of the exploration process. A user
of our system can explore the result space of
a query by drilling down/up from one state-
ment to another, according to entailment re-
lations specified by an entailment graph and
an optional concept taxonomy. As a promi-
nent use case, we apply our exploration sys-
tem and illustrate its benefit on the health-care
domain. To the best of our knowledge this is
the first implementation of an exploration sys-
tem at the statement level that is based on the
textual entailment relation.
1 Introduction
Finding information in a large body of text is be-
coming increasingly more difficult. Standard search
engines output a set of documents for a given query,
but do not allow any exploration of the thematic
structure in the retrieved information. Thus, the need
for tools that allow to effectively sift through a target
set of documents is becoming ever more important.
Faceted search (Stoica and Hearst, 2007; Ka?ki,
2005) supports a better understanding of a target do-
main, by allowing exploration of data according to
multiple views or facets. For example, given a set of
documents on Nobel Prize laureates we might have
different facets corresponding to the laureate?s na-
tionality, the year when the prize was awarded, the
field in which it was awarded, etc. However, this
type of exploration is still severely limited insofar
that it only allows exploration by topic rather than
content. Put differently, we can only explore accord-
ing to what a document is about rather than what
a document actually says. For instance, the facets
for the query ?asthma? in the faceted search engine
Yippy include the concepts allergy and children, but
do not specify what are the exact relations between
these concepts and the query (e.g., allergy causes
asthma, and children suffer from asthma).
Berant et al (2010) proposed an exploration
scheme that focuses on relations between concepts,
which are derived from a graph describing textual
entailment relations between propositions. In their
setting a proposition consists of a predicate with two
arguments that are possibly replaced by variables,
such as ?X control asthma?. A graph that specifies
an entailment relation ?X control asthma ? X af-
fect asthma? can help a user, who is browsing doc-
uments dealing with substances that affect asthma,
drill down and explore only substances that control
asthma. This type of exploration can be viewed as
an extension of faceted search, where the new facet
concentrates on the actual statements expressed in
the texts.
In this paper we follow Berant et al?s proposal,
and present a novel entailment-based text explo-
ration system, which we applied to the health-care
domain. A user of this system can explore the re-
sult space of her query, by drilling down/up from
one proposition to another, according to a set of en-
tailment relations described by an entailment graph.
In Figure 1, for example, the user looks for ?things?
79
Figure 1: Exploring asthma results.
that affect asthma. She invokes an ?asthma? query
and starts drilling down the entailment graph to ?X
control asthma? (left column). In order to exam-
ine the arguments of a selected proposition, the user
may drill down/up a concept taxonomy that classi-
fies terms that occur as arguments. The user in Fig-
ure 1, for instance, drills down the concept taxon-
omy (middle column), in order to focus on Hor-
mones that control asthma, such as ?prednisone?
(right column). Each drill down/up induces a subset
of the documents that correspond to the aforemen-
tioned selections. The retrieved document in Fig-
ure 1 (bottom) is highlighted by the relevant propo-
sition, which clearly states that prednisone is often
given to treat asthma (and indeed in the entailment
graph ?X treat asthma? entails ?X control asthma?).
Our system is built over a corpus of documents,
a set of propositions extracted from the documents,
an entailment graph describing entailment relations
between propositions, and, optionally, a concept hi-
erarchy. The system implementation for the health-
care domain, for instance, is based on a web-crawled
health-care corpus, the propositions automatically
extracted from the corpus, entailment graphs bor-
rowed from Berant et al (2010), and the UMLS1
taxonomy. To the best of our knowledge this is the
first implementation of an exploration system, at the
proposition level, based on the textual entailment re-
lation.
2 Background
2.1 Exploratory Search
Exploratory search addresses the need of users to
quickly identify the important pieces of information
in a target set of documents. In exploratory search,
users are presented with a result set and a set of ex-
ploratory facets, which are proposals for refinements
of the query that can lead to more focused sets of
documents. Each facet corresponds to a clustering
of the current result set, focused on a more specific
topic than the current query. The user proceeds in
the exploration of the document set by selecting spe-
cific documents (to read them) or by selecting spe-
cific facets, to refine the result set.
1http://www.nlm.nih.gov/research/umls/
80
Early exploration technologies were based on a
single hierarchical conceptual clustering of infor-
mation (Hofmann, 1999), enabling the user to drill
up and down the concept hierarchies. Hierarchi-
cal faceted meta-data (Stoica and Hearst, 2007), or
faceted search, proposed more sophisticated explo-
ration possibilities by providing multiple facets and
a hierarchy per facet or dimension of the domain.
These types of exploration techniques were found to
be useful for effective access of information (Ka?ki,
2005).
In this work, we suggest proposition-based ex-
ploration as an extension to concept-based explo-
ration. Our intuition is that text exploration can
profit greatly from representing information not only
at the level of individual concepts, but also at the
propositional level, where the relations that link con-
cepts to one another are represented effectively in a
hierarchical entailment graph.
2.2 Entailment Graph
Recognizing Textual Entailment (RTE) is the task
of deciding, given two text fragments, whether the
meaning of one text can be inferred from another
(Dagan et al, 2009). For example, ?Levalbuterol
is used to control various kinds of asthma? entails
?Levalbuterol affects asthma?. In this paper, we use
the notion of proposition to denote a specific type
of text fragments, composed of a predicate with two
arguments (e.g., Levalbuterol control asthma).
Textual entailment systems are often based on en-
tailment rules which specify a directional inference
relation between two fragments. In this work, we
focus on leveraging a common type of entailment
rules, in which the left-hand-side of the rule (LHS)
and the right-hand-side of the rule (RHS) are propo-
sitional templates - a proposition, where one or both
of the arguments are replaced by a variable, e.g., ?X
control asthma? X affect asthma?.
The entailment relation between propositional
templates of a given corpus can be represented by an
entailment graph (Berant et al, 2010) (see Figure 2,
top). The nodes of an entailment graph correspond
to propositional templates, and its edges correspond
to entailment relations (rules) between them. Entail-
ment graph representation is somewhat analogous to
the formation of ontological relations between con-
cepts of a given domain, where in our case the nodes
correspond to propositional templates rather than to
concepts.
3 Exploration Model
In this section we extend the scope of state-of-the-
art exploration technologies by moving from stan-
dard concept-based exploration to proposition-based
exploration, or equivalently, statement-based explo-
ration. In our model, it is the entailment relation
between propositional templates which determines
the granularity of the viewed information space. We
first describe the inputs to the system and then detail
our proposed exploration scheme.
3.1 System Inputs
Corpus A collection of documents, which form
the search space of the system.
Extracted Propositions A set of propositions, ex-
tracted from the corpus document. The propositions
are usually produced by an extraction method, such
as TextRunner (Banko et al, 2007) or ReVerb (Fader
et al, 2011). In order to support the exploration
process, the documents are indexed by the proposi-
tional templates and argument terms of the extracted
propositions.
Entailment graph for predicates The nodes of
the entailment graph are propositional templates,
where edges indicate entailment relations between
templates (Section 2.2). In order to avoid circular-
ity in the exploration process, the graph is trans-
formed into a DAG, by merging ?equivalent? nodes
that are in the same strong connectivity component
(as suggested by Berant et al (2010)). In addition,
for clarity and simplicity, edges that can be inferred
by transitivity are omitted from the DAG. Figure 2
illustrates the result of applying this procedure to a
fragment of the entailment graph for ?asthma? (i.e.,
for propositional templates with ?asthma? as one of
the arguments).
Taxonomy for arguments The optional concept
taxonomy maps terms to one or more pre-defined
concepts, arranged in a hierarchical structure. These
terms may appear in the corpus as arguments of
predicates. Figure 3, for instance, illustrates a sim-
ple medical taxonomy, composed of three concepts
(medical, diseases, drugs) and four terms (cancer,
asthma, aspirin, flexeril).
81
Figure 2: Fragment of the entailment graph for ?asthma?
(top), and its conversion to a DAG (bottom).
3.2 Exploration Scheme
The objective of the exploration scheme is to support
querying and offer facets for result exploration, in
a visual manner. The following components cover
the various aspects of this objective, given the above
system inputs:
Querying The user enters a search term as a query,
e.g., ?asthma?. The given term induces a subgraph of
the entailment graph that contains all propositional
templates (graph nodes) with which this term ap-
pears as an argument in the extracted propositions
(see Figure 2). This subgraph is represented as a
DAG, as explained in Section 3.1, where all nodes
that have no parent are defined as the roots of the
DAG. As a starting point, only the roots of the DAG
are displayed to the user. Figure 4 shows the five
roots for the ?asthma? query.
Exploration process The user selects one of the
entailment graph nodes (e.g., ?associate X with
asthma?). At each exploration step, the user can
drill down to a more specific template or drill up to a
Figure 3: Partial medical taxonomy. Ellipses denote con-
cepts, while rectangles denote terms.
Figure 4: The roots of the entailment graph for the
?asthma? query.
more general template, by moving along the entail-
ment hierarchy. For example, the user in Figure 5,
expands the root ?associate X with asthma?, in order
to drill down through ?X affect asthma? to ?X control
Asthma?.
Selecting a propositional template (Figure 1, left
column) displays a concept taxonomy for the argu-
ments that correspond to the variable in the selected
template (Figure 1, middle column). The user can
explore these argument concepts by drilling up and
down the concept taxonomy. For example, in Fig-
ure 1 the user, who selected ?X control Asthma?,
explores the arguments of this template by drilling
down the taxonomy to the concept ?Hormone?.
Selecting a concept opens a third column, which
lists the terms mapped to this concept that occurred
as arguments of the selected template. For example,
in Figure 1, the user is examining the list of argu-
ments for the template ?X control Asthma?, which
are mapped to the concept ?Hormone?, focusing on
the argument ?prednisone?.
82
Figure 5: Part of the entailment graph for the ?asthma?
query, after two exploration steps. This corresponds to
the left column in Figure 1.
Document retrieval At any stage, the list of docu-
ments induced by the current selected template, con-
cept and argument is presented to the user, where
in each document snippet the relevant proposition
components are highlighted. Figure 1 (bottom)
shows such a retrieved document. The highlighted
extraction in the snippet, ?prednisone treat asthma?,
entails the proposition selected during exploration,
?prednisone control asthma?.
4 System Architecture
In this section we briefly describe system compo-
nents, as illustrated in the block diagram (Figure 6).
The search service implements full-text and
faceted search, and document indexing. The data
service handles data (e.g., documents) replication
for clients. The entailment service handles the logic
of the entailment relations (for both the entailment
graph and the taxonomy).
The index server applies periodic indexing of new
texts, and the exploration server serves the explo-
ration application on querying, exploration, and data
Figure 6: Block diagram of the exploration system.
access. The exploration application is the front-end
user application for the whole exploration process
described above (Section 3.2).
5 Application to the Health-care Domain
As a prominent use case, we applied our exploration
system to the health-care domain. With the advent
of the internet and social media, patients now have
access to new sources of medical information: con-
sumer health articles, forums, and social networks
(Boulos and Wheeler, 2007). A typical non-expert
health information searcher is uncertain about her
exact questions and is unfamiliar with medical ter-
minology (Trivedi, 2009). Exploring relevant infor-
mation about a given medical issue can be essential
and time-critical.
System implementation For the search service,
we used SolR servlet, where the data service is
built over FTP. The exploration application is im-
plemented as a web application.
Input resources We collected a health-care cor-
pus from the web, which contains more than 2M
sentences and about 50M word tokens. The texts
deal with various aspects of the health care domain:
answers to questions, surveys on diseases, articles
on life-style, etc. We extracted propositions from
the health-care corpus, by applying the method de-
scribed by Berant et al (2010). The corpus was
parsed, and propositions were extracted from depen-
dency trees according to the method suggested by
Lin and Pantel (2001), where propositions are de-
pendency paths between two arguments of a predi-
83
cate. We filtered out any proposition where one of
the arguments is not a term mapped to a medical
concept in the UMLS taxonomy.
For the entailment graph we used the 23 entail-
ment graphs published by Berant et al2. For the ar-
gument taxonomy we employed UMLS ? a database
that maps natural language phrases to over one mil-
lion unique concept identifiers (CUIs) in the health-
care domain. The CUIs are also mapped in UMLS
to a concept taxonomy for the health-care domain.
The web application of our system is
available at: http://132.70.6.148:
8080/exploration
6 Conclusion and Future Work
We presented a novel exploration model, which ex-
tends the scope of state-of-the-art exploration tech-
nologies by moving from standard concept-based
exploration to proposition-based exploration. Our
model combines the textual entailment paradigm
within the exploration process, with application to
the health-care domain. According to our model, it
is the entailment relation between propositions, en-
coded by the entailment graph and the taxonomy,
which leads the user between more specific and
more general statements throughout the search re-
sult space. We believe that employing the entail-
ment relation between propositions, which focuses
on the statements expressed in the documents, can
contribute to the exploration field and improve in-
formation access.
Our current application to the health-care domain
relies on a small set of entailment graphs for 23
medical concepts. Our ongoing research focuses on
the challenging task of learning a larger entailment
graph for the health-care domain. We are also in-
vestigating methods for evaluating the exploration
process (Borlund and Ingwersen, 1997). As noted
by Qu and Furnas (2008), the success of an ex-
ploratory search system does not depend simply on
how many relevant documents will be retrieved for a
given query, but more broadly on how well the sys-
tem helps the user with the exploratory process.
2http://www.cs.tau.ac.il/?jonatha6/
homepage_files/resources/HealthcareGraphs.
rar
Acknowledgments
This work was partially supported by the Israel
Ministry of Science and Technology, the PASCAL-
2 Network of Excellence of the European Com-
munity FP7-ICT-2007-1-216886, and the Euro-
pean Communitys Seventh Framework Programme
(FP7/2007-2013) under grant agreement no. 287923
(EXCITEMENT).
References
Michele Banko, Michael J Cafarella, Stephen Soderl,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
IJCAI, pages 2670?2676.
Jonathan Berant, Ido Dagan, and Jacob Goldberger.
2010. Global learning of focused entailment graphs.
In Proceedings of ACL, Uppsala, Sweden.
Pia Borlund and Peter Ingwersen. 1997. The develop-
ment of a method for the evaluation of interactive in-
formation retrieval systems. Journal of Documenta-
tion, 53:225?250.
Maged N. Kamel Boulos and Steve Wheeler. 2007. The
emerging web 2.0 social software: an enabling suite of
sociable technologies in health and health care educa-
tion. Health Information & Libraries, 24:2?23.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.
2009. Recognizing textual entailment: Rational, eval-
uation and approaches. Natural Language Engineer-
ing, 15(Special Issue 04):i?xvii.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP, pages 1535?1545. ACL.
Thomas Hofmann. 1999. The cluster-abstraction model:
Unsupervised learning of topic hierarchies from text
data. In Proceedings of IJCAI, pages 682?687.
Mika Ka?ki. 2005. Findex: search result categories help
users when document ranking fails. In Proceedings
of SIGCHI, CHI ?05, pages 131?140, New York, NY,
USA. ACM.
Dekang Lin and Patrick Pantel. 2001. Discovery of infer-
ence rules for question answering. Natural Language
Engineering, 7:343?360.
Yan Qu and George W. Furnas. 2008. Model-driven for-
mative evaluation of exploratory search: A study un-
der a sensemaking framework. Inf. Process. Manage.,
44:534?555.
Emilia Stoica and Marti A. Hearst. 2007. Automating
creation of hierarchical faceted metadata structures. In
Proceedings of NAACL HLT.
Mayank Trivedi. 2009. A study of search engines for
health sciences. International Journal of Library and
Information Science, 1(5):69?73.
84
