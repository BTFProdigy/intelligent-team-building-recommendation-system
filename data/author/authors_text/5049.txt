11
12
13
14
15
16
17
18
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 420?429,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Model-Based Aligner Combination Using Dual Decomposition
John DeNero
Google Research
denero@google.com
Klaus Macherey
Google Research
kmach@google.com
Abstract
Unsupervised word alignment is most often
modeled as a Markov process that generates a
sentence f conditioned on its translation e. A
similar model generating e from f will make
different alignment predictions. Statistical
machine translation systems combine the pre-
dictions of two directional models, typically
using heuristic combination procedures like
grow-diag-final. This paper presents a graph-
ical model that embeds two directional align-
ers into a single model. Inference can be per-
formed via dual decomposition, which reuses
the efficient inference algorithms of the direc-
tional models. Our bidirectional model en-
forces a one-to-one phrase constraint while ac-
counting for the uncertainty in the underlying
directional models. The resulting alignments
improve upon baseline combination heuristics
in word-level and phrase-level evaluations.
1 Introduction
Word alignment is the task of identifying corre-
sponding words in sentence pairs. The standard
approach to word alignment employs directional
Markov models that align the words of a sentence
f to those of its translation e, such as IBM Model 4
(Brown et al, 1993) or the HMM-based alignment
model (Vogel et al, 1996).
Machine translation systems typically combine
the predictions of two directional models, one which
aligns f to e and the other e to f (Och et al,
1999). Combination can reduce errors and relax
the one-to-many structural restriction of directional
models. Common combination methods include the
union or intersection of directional alignments, as
well as heuristic interpolations between the union
and intersection like grow-diag-final (Koehn et al,
2003). This paper presents a model-based alterna-
tive to aligner combination. Inference in a prob-
abilistic model resolves the conflicting predictions
of two directional models, while taking into account
each model?s uncertainty over its output.
This result is achieved by embedding two direc-
tional HMM-based alignment models into a larger
bidirectional graphical model. The full model struc-
ture and potentials allow the two embedded direc-
tional models to disagree to some extent, but reward
agreement. Moreover, the bidirectional model en-
forces a one-to-one phrase alignment structure, sim-
ilar to the output of phrase alignment models (Marcu
and Wong, 2002; DeNero et al, 2008), unsuper-
vised inversion transduction grammar (ITG) models
(Blunsom et al, 2009), and supervised ITG models
(Haghighi et al, 2009; DeNero and Klein, 2010).
Inference in our combined model is not tractable
because of numerous edge cycles in the model
graph. However, we can employ dual decomposi-
tion as an approximate inference technique (Rush et
al., 2010). In this approach, we iteratively apply the
same efficient sequence algorithms for the underly-
ing directional models, and thereby optimize a dual
bound on the model objective. In cases where our
algorithm converges, we have a certificate of opti-
mality under the full model. Early stopping before
convergence still yields useful outputs.
Our model-based approach to aligner combina-
tion yields improvements in alignment quality and
phrase extraction quality in Chinese-English exper-
iments, relative to typical heuristic combinations
methods applied to the predictions of independent
directional models.
420
2 Model Definition
Our bidirectional model G = (V,D) is a globally
normalized, undirected graphical model of the word
alignment for a fixed sentence pair (e,f). Each ver-
tex in the vertex set V corresponds to a model vari-
able Vi, and each undirected edge in the edge set D
corresponds to a pair of variables (Vi, Vj). Each ver-
tex has an associated potential function ?i(vi) that
assigns a real-valued potential to each possible value
vi of Vi.1 Likewise, each edge has an associated po-
tential function ?ij(vi, vj) that scores pairs of val-
ues. The probability under the model of any full as-
signment v to the model variables, indexed by V ,
factors over vertex and edge potentials.
P(v) ?
?
vi?V
?i(vi) ?
?
(vi,vj)?D
?ij(vi, vj)
Our model contains two directional hidden
Markov alignment models, which we review in Sec-
tion 2.1, along with additional structure that that we
introduce in Section 2.2.
2.1 HMM-Based Alignment Model
This section describes the classic hidden Markov
model (HMM) based alignment model (Vogel et al,
1996). The model generates a sequence of words f
conditioned on a word sequence e. We convention-
ally index the words of e by i and f by j. P(f |e)
is defined in terms of a latent alignment vector a,
where aj = i indicates that word position i of e
aligns to word position j of f .
P(f |e) =
?
a
P(f ,a|e)
P(f ,a|e) =
|f |?
j=1
D(aj |aj?1)M(fj |eaj ) . (1)
In Equation 1 above, the emission model M is
a learned multinomial distribution over word types.
The transition model D is a multinomial over tran-
sition distances, which treats null alignments as a
special case.
D(aj = 0|aj?1 = i) = po
D(aj = i
? 6= 0|aj?1 = i) = (1? po) ? c(i
? ? i) ,
1Potentials in an undirected model play the same role as con-
ditional probabilities in a directed model, but do not need to be
locally normalized.
where c(i? ? i) is a learned distribution over signed
distances, normalized over the possible transitions
from i. The parameters of the conditional multino-
mial M and the transition model c can be learned
from a sentence aligned corpus via the expectation
maximization algorithm. The null parameter po is
typically fixed.2
The highest probability word alignment vector
under the model for a given sentence pair (e,f) can
be computed exactly using the standard Viterbi al-
gorithm for HMMs in O(|e|2 ? |f |) time.
An alignment vector a can be converted trivially
into a set of word alignment links A:
Aa = {(i, j) : aj = i, i 6= 0} .
Aa is constrained to be many-to-one from f to e;
many positions j can align to the same i, but each j
appears at most once.
We have defined a directional model that gener-
ates f from e. An identically structured model can
be defined that generates e from f . Let b be a vector
of alignments where bi = j indicates that word po-
sition j of f aligns to word position i of e. Then,
P(e,b|f) is defined similarly to Equation 1, but
with e and f swapped. We can distinguish the tran-
sition and emission distributions of the two models
by subscripting them with their generative direction.
P(e,b|f) =
|e|?
j=1
Df?e(bi|bi?1)Mf?e(ei|fbi) .
The vector b can be interpreted as a set of align-
ment links that is one-to-many: each value i appears
at most once in the set.
Ab = {(i, j) : bi = j, j 6= 0} .
2.2 A Bidirectional Alignment Model
We can combine two HMM-based directional align-
ment models by embedding them in a larger model
2In experiments, we set po = 10
?6. Transitions from a null-
aligned state aj?1 = 0 are also drawn from a fixed distribution,
where D(aj = 0|aj?1 = 0) = 10?4 and for i? ? 1,
D(aj = i
?|aj?1 = 0) ? 0.8
(
?
?
?
?i??
|f|
|e|?j
?
?
?
)
.
With small po, the shape of this distribution has little effect on
the alignment outcome.
421
How are
you
?
?
How are
you
?
?
How are
you
a
1
c
11
b
2
b
2
c
22
(a)
c
22
(b)
a
2
a
3
b
1
c
12
c
13
c
21
c
22
c
23
a
1
a
2
a
3
b
1
c
21
(a)
c
21
(b)
c
23
(a)
c
23
(b)
c
13
(a)
c
13
(b)
c
12
(a)
c
12
(b)
c
11
(a)
c
11
(b)
c
22
(a)
a
1
a
2
a
3
c
21
(a)
c
23
(a)
c
13
(a)
c
12
(a)
c
11
(a)
Figure 1: The structure of our graphical model for a sim-
ple sentence pair. The variables a are blue, b are red, and
c are green.
that includes all of the random variables of two di-
rectional models, along with additional structure that
promotes agreement and resolves discrepancies.
The original directional models include observed
word sequences e and f , along with the two latent
alignment vectors a and b defined in Section 2.1.
Because the word types and lengths of e and f are
always fixed by the observed sentence pair, we can
define our model only over a and b, where the edge
potentials between any aj , fj , and e are compiled
into a vertex potential function ?(a)j on aj , defined
in terms of f and e, and likewise for any bi.
?(a)j (i) = Me?f (fj |ei)
?(b)i (j) = Mf?e(ei|fj)
The edge potentials between a and b encode the
transition model in Equation 1.
?(a)j?1,j(i, i
?) = De?f (aj = i
?|aj?1 = i)
?(b)i?1,i(j, j
?) = Df?e(bi = j
?|bi?1 = j)
In addition, we include in our model a latent
boolean matrix c that encodes the output of the com-
bined aligners:
c ? {0, 1}|e|?|f | .
This matrix encodes the alignment links proposed
by the bidirectional model:
Ac = {(i, j) : cij = 1} .
Each model node for an element cij ? {0, 1} is
connected to aj and bi via coherence edges. These
edges allow the model to ensure that the three sets
of variables, a, b, and c, together encode a coher-
ent alignment analysis of the sentence pair. Figure 1
depicts the graph structure of the model.
2.3 Coherence Potentials
The potentials on coherence edges are not learned
and do not express any patterns in the data. Instead,
they are fixed functions that promote consistency be-
tween the integer-valued directional alignment vec-
tors a and b and the boolean-valued matrix c.
Consider the assignment aj = i, where i = 0
indicates that word fj is null-aligned, and i ? 1 in-
dicates that fj aligns to ei. The coherence potential
ensures the following relationship between the vari-
able assignment aj = i and the variables ci?j , for
any i? ? [1, |e|].
? If i = 0 (null-aligned), then all ci?j = 0.
? If i > 0, then cij = 1.
? ci?j = 1 only if i? ? {i? 1, i, i+ 1}.
? Assigning ci?j = 1 for i? 6= i incurs a cost e??.
Collectively, the list of cases above enforce an intu-
itive correspondence: an alignment aj = i ensures
that cij must be 1, adjacent neighbors may be 1 but
incur a cost, and all other elements are 0.
This pattern of effects can be encoded in a poten-
tial function ?(c) for each coherence edge. These
edge potential functions takes an integer value i for
some variable aj and a binary value k for some ci?j .
?(c)(aj ,ci?j)
(i, k) =
?
????????????
????????????
1 i = 0 ? k = 0
0 i = 0 ? k = 1
1 i = i? ? k = 1
0 i = i? ? k = 0
1 i 6= i? ? k = 0
e?? |i? i?| = 1 ? k = 1
0 |i? i?| > 1 ? k = 1
(2)
Above, potentials of 0 effectively disallow certain
cases because a full assignment to (a,b, c) is scored
by the product of all model potentials. The poten-
tial function ?(c)(bi,cij? )
(j, k) for a coherence edge be-
tween b and c is defined similarly.
422
2.4 Model Properties
We interpret c as the final alignment produced by the
model, ignoring a and b. In this way, we relax the
one-to-many constraints of the directional models.
However, all of the information about how words
align is expressed by the vertex and edge potentials
on a and b. The coherence edges and the link ma-
trix c only serve to resolve conflicts between the di-
rectional models and communicate information be-
tween them.
Because directional alignments are preserved in-
tact as components of our model, extensions or
refinements to the underlying directional Markov
alignment model could be integrated cleanly into
our model as well, including lexicalized transition
models (He, 2007), extended conditioning contexts
(Brunning et al, 2009), and external information
(Shindo et al, 2010).
For any assignment to (a,b, c) with non-zero
probability, c must encode a one-to-one phrase
alignment with a maximum phrase length of 3. That
is, any word in either sentence can align to at most
three words in the opposite sentence, and those
words must be contiguous. This restriction is di-
rectly enforced by the edge potential in Equation 2.
3 Model Inference
In general, graphical models admit efficient, exact
inference algorithms if they do not contain cycles.
Unfortunately, our model contains numerous cycles.
For every pair of indices (i, j) and (i?, j?), the fol-
lowing cycle exists in the graph:
cij ? bi ? cij? ? aj? ?
ci?j? ? bi? ? ci?j ? aj ? cij
Additional cycles also exist in the graph through
the edges between aj?1 and aj and between bi?1
and bi. The general phrase alignment problem under
an arbitrary model is known to be NP-hard (DeNero
and Klein, 2008).
3.1 Dual Decomposition
While the entire graphical model has loops, there are
two overlapping subgraphs that are cycle-free. One
subgraph Ga includes all of the vertices correspond-
ing to variables a and c. The other subgraph Gb in-
cludes vertices for variables b and c. Every edge in
the graph belongs to exactly one of these two sub-
graphs.
The dual decomposition inference approach al-
lows us to exploit this sub-graph structure (Rush et
al., 2010). In particular, we can iteratively apply
exact inference to the subgraph problems, adjusting
their potentials to reflect the constraints of the full
problem. The technique of dual decomposition has
recently been shown to yield state-of-the-art perfor-
mance in dependency parsing (Koo et al, 2010).
3.2 Dual Problem Formulation
To describe a dual decomposition inference proce-
dure for our model, we first restate the inference
problem under our graphical model in terms of the
two overlapping subgraphs that admit tractable in-
ference. Let c(a) be a copy of c associated with Ga,
and c(b) with Gb. Also, let f(a, c(a)) be the un-
normalized log-probability of an assignment to Ga
and g(b, c(b)) be the unnormalized log-probability
of an assignment to Gb. Finally, let I be the index
set of all (i, j) for c. Then, the maximum likelihood
assignment to our original model can be found by
optimizing
max
a,b,c(a),c(b)
f(a, c(a)) + g(b, c(b)) (3)
such that: c(a)ij = c
(b)
ij ? (i, j) ? I .
The Lagrangian relaxation of this optimization
problem is L(a,b, c(a), c(b),u) =
f(a, c(a))+ g(b, c(b))+
?
(i,j)?I
u(i, j)(c(a)i,j ?c
(b)
i,j ) .
Hence, we can rewrite the original problem as
max
a,b,c(a),c(b)
min
u
L(a,b, c(a), c(b),u) .
We can form a dual problem that is an up-
per bound on the original optimization problem by
swapping the order of min and max. In this case,
the dual problem decomposes into two terms that are
each local to an acyclic subgraph.
min
u
?
?max
a,c(a)
?
?f(a, c(a)) +
?
i,j
u(i, j)c(a)ij
?
?
+ max
b,c(b)
?
?g(b, c(b))?
?
i,j
u(i, j)c(b)ij
?
?
?
? (4)
423
How are
you
?
?
How are
you
?
?
How are
you
a
1
c
11
b
2
b
2
c
22
(a)
c
22
(b)
a
2
a
3
b
1
c
12
c
13
c
21
c
22
c
23
a
1
a
2
a
3
b
1
c
21
(a)
c
21
(b)
c
23
(a)
c
23
(b)
c
13
(a)
c
13
(b)
c
12
(a)
c
12
(b)
c
11
(a)
c
11
(b)
c
22
(a)
a
1
a
2
a
3
c
21
(a)
c
23
(a)
c
13
(a)
c
12
(a)
c
11
(a)
Figure 2: Our combined model decomposes into two
acyclic models that each contain a copy of c.
The decomposed model is depicted in Figure 2.
As in previous work, we solve for the dual variable
u by repeatedly performing inference in the two de-
coupled maximization problems.
3.3 Sub-Graph Inference
We now address the problem of evaluating Equa-
tion 4 for fixed u. Consider the first line of Equa-
tion 4, which includes variables a and c(a).
max
a,c(a)
?
?f(a, c(a)) +
?
i,j
u(i, j)c(a)ij
?
? (5)
Because the graph Ga is tree-structured, Equa-
tion 5 can be evaluated in polynomial time. In fact,
we can make a stronger claim: we can reuse the
Viterbi inference algorithm for linear chain graph-
ical models that applies to the embedded directional
HMM models. That is, we can cast the optimization
of Equation 5 as
max
a
?
?
|f |?
j=1
De?f (aj |aj?1) ?M
?
j(aj = i)
?
? .
In the original HMM-based aligner, the vertex po-
tentials correspond to bilexical probabilities. Those
quantities appear in f(a, c(a)), and therefore will be
a part of M?j(?) above. The additional terms of Equa-
tion 5 can also be factored into the vertex poten-
tials of this linear chain model, because the optimal
How are
you
?
?
How are
you
?
?
How are
you
a
1
c
11
b
2
b
2
c
22
(a)
c
22
(b)
a
2
a
3
b
1
c
12
c
13
c
21
c
22
c
23
a
1
a
2
a
3
b
1
c
21
(a)
c
21
(b)
c
23
(a)
c
23
(b)
c
13
(a)
c
13
(b)
c
12
(a)
c
12
(b)
c
11
(a)
c
11
(b)
c
22
(a)
a
1
a
2
a
3
c
21
(a)
c
23
(a)
c
13
(a)
c
12
(a)
c
11
(a)
Figure 3: The tree-structured subgraph Ga can be mapped
to an equivalent chain-structured model by optimizing
over ci?j for aj = i.
choice of each cij can be determined from aj and the
model parameters. If aj = i, then cij = 1 according
to our edge potential defined in Equation 2. Hence,
setting aj = i requires the inclusion of the corre-
sponding vertex potential ?(a)j (i), as well as u(i, j).
For i? 6= i, either ci?j = 0, which contributes noth-
ing to Equation 5, or ci?j = 1, which contributes
u(i?, j)??, according to our edge potential between
aj and ci?j .
Thus, we can capture the net effect of assigning
aj and then optimally assigning all ci?j in a single
potential M?j(aj = i) =
?(a)j (i) + exp
?
?u(i, j) +
?
j?:|j??j|=1
max(0, u(i, j?)? ?)
?
?
Note that Equation 5 and f are sums of terms in
log space, while Viterbi inference for linear chains
assumes a product of terms in probability space,
which introduces the exponentiation above.
Defining this potential allows us to collapse the
source-side sub-graph inference problem defined
by Equation 5, into a simple linear chain model
that only includes potential functions M?j and ?
(a).
Hence, we can use a highly optimized linear chain
inference implementation rather than a solver for
general tree-structured graphical models. Figure 3
depicts this transformation.
An equivalent approach allows us to evaluate the
424
Algorithm 1 Dual decomposition inference algo-
rithm for the bidirectional model
for t = 1 to max iterations do
r ? 1t . Learning rate
c(a) ? argmax f(a, c(a)) +
?
i,j u(i, j)c
(a)
ij
c(b) ? argmax g(b, c(b))?
?
i,j u(i, j)c
(b)
ij
if c(a) = c(b) then
return c(a) . Converged
u? u + r ? (c(b) ? c(a)) . Dual update
return combine(c(a), c(b)) . Stop early
second line of Equation 4 for fixed u:
max
b,c(b)
?
?g(b, c(b)) +
?
i,j
u(i, j)c(b)ij
?
? . (6)
3.4 Dual Decomposition Algorithm
Now that we have the means to efficiently evalu-
ate Equation 4 for fixed u, we can define the full
dual decomposition algorithm for our model, which
searches for a u that optimizes Equation 4. We can
iteratively search for such a u via sub-gradient de-
scent. We use a learning rate 1t that decays with the
number of iterations t. The full dual decomposition
optimization procedure appears in Algorithm 1.
If Algorithm 1 converges, then we have found a u
such that the value of c(a) that optimizes Equation 5
is identical to the value of c(b) that optimizes Equa-
tion 6. Hence, it is also a solution to our original
optimization problem: Equation 3. Since the dual
problem is an upper bound on the original problem,
this solution must be optimal for Equation 3.
3.5 Convergence and Early Stopping
Our dual decomposition algorithm provides an infer-
ence method that is exact upon convergence.3 When
Algorithm 1 does not converge, the two alignments
c(a) and c(b) can still be used. While these align-
ments may differ, they will likely be more similar
than the alignments of independent aligners.
These alignments will still need to be combined
procedurally (e.g., taking their union), but because
3This certificate of optimality is not provided by other ap-
proximate inference algorithms, such as belief propagation,
sampling, or simulated annealing.
they are more similar, the importance of the combi-
nation procedure is reduced. We analyze the behav-
ior of early stopping experimentally in Section 5.
3.6 Inference Properties
Because we set a maximum number of iterations
n in the dual decomposition algorithm, and each
iteration only involves optimization in a sequence
model, our entire inference procedure is only a con-
stant multiple n more computationally expensive
than evaluating the original directional aligners.
Moreover, the value of u is specific to a sen-
tence pair. Therefore, our approach does not require
any additional communication overhead relative to
the independent directional models in a distributed
aligner implementation. Memory requirements are
virtually identical to the baseline: only u must be
stored for each sentence pair as it is being processed,
but can then be immediately discarded once align-
ments are inferred.
Other approaches to generating one-to-one phrase
alignments are generally more expensive. In par-
ticular, an ITG model requires O(|e|3 ? |f |3) time,
whereas our algorithm requires only
O(n ? (|f ||e|2 + |e||f |2)) .
Moreover, our approach allows Markov distortion
potentials, while standard ITG models are restricted
to only hierarchical distortion.
4 Related Work
Alignment combination normally involves selecting
some A from the output of two directional models.
Common approaches include forming the union or
intersection of the directional sets.
A? = Aa ? Ab
A? = Aa ? Ab .
More complex combiners, such as the grow-diag-
final heuristic (Koehn et al, 2003), produce align-
ment link sets that include all of A? and some sub-
set ofA? based on the relationship of multiple links
(Och et al, 1999).
In addition, supervised word alignment models
often use the output of directional unsupervised
aligners as features or pruning signals. In the case
425
that a supervised model is restricted to proposing
alignment links that appear in the output of a di-
rectional aligner, these models can be interpreted as
a combination technique (Deng and Zhou, 2009).
Such a model-based approach differs from ours in
that it requires a supervised dataset and treats the di-
rectional aligners? output as fixed.
Combination is also related to agreement-based
learning (Liang et al, 2006). This approach to
jointly learning two directional alignment mod-
els yields state-of-the-art unsupervised performance.
Our method is complementary to agreement-based
learning, as it applies to Viterbi inference under the
model rather than computing expectations. In fact,
we employ agreement-based training to estimate the
parameters of the directional aligners in our experi-
ments.
A parallel idea that closely relates to our bidi-
rectional model is posterior regularization, which
has also been applied to the word alignment prob-
lem (Grac?a et al, 2008). One form of posterior
regularization stipulates that the posterior probabil-
ity of alignments from two models must agree, and
enforces this agreement through an iterative proce-
dure similar to Algorithm 1. This approach also
yields state-of-the-art unsupervised alignment per-
formance on some datasets, along with improve-
ments in end-to-end translation quality (Ganchev et
al., 2008).
Our method differs from this posterior regulariza-
tion work in two ways. First, we iterate over Viterbi
predictions rather than posteriors. More importantly,
we have changed the output space of the model to
be a one-to-one phrase alignment via the coherence
edge potential functions.
Another similar line of work applies belief prop-
agation to factor graphs that enforce a one-to-one
word alignment (Cromie`res and Kurohashi, 2009).
The details of our models differ: we employ
distance-based distortion, while they add structural
correspondence terms based on syntactic parse trees.
Also, our model training is identical to the HMM-
based baseline training, while they employ belief
propagation for both training and Viterbi inference.
Although differing in both model and inference, our
work and theirs both find improvements from defin-
ing graphical models for alignment that do not admit
exact polynomial-time inference algorithms.
Aligner Intersection Union Agreement
Model |A?| |A?| |A?|/|A?|
Baseline 5,554 10,998 50.5%
Bidirectional 7,620 10,262 74.3%
Table 1: The bidirectional model?s dual decomposition
algorithm substantially increases the overlap between the
predictions of the directional models, measured by the
number of links in their intersection.
5 Experimental Results
We evaluated our bidirectional model by comparing
its output to the annotations of a hand-aligned cor-
pus. In this way, we can show that the bidirectional
model improves alignment quality and enables the
extraction of more correct phrase pairs.
5.1 Data Conditions
We evaluated alignment quality on a hand-aligned
portion of the NIST 2002 Chinese-English test set
(Ayan and Dorr, 2006). We trained the model on a
portion of FBIS data that has been used previously
for alignment model evaluation (Ayan and Dorr,
2006; Haghighi et al, 2009; DeNero and Klein,
2010). We conducted our evaluation on the first 150
sentences of the dataset, following previous work.
This portion of the dataset is commonly used to train
supervised models.
We trained the parameters of the directional mod-
els using the agreement training variant of the expec-
tation maximization algorithm (Liang et al, 2006).
Agreement-trained IBM Model 1 was used to ini-
tialize the parameters of the HMM-based alignment
models (Brown et al, 1993). Both IBM Model 1
and the HMM alignment models were trained for
5 iterations on a 6.2 million word parallel corpus
of FBIS newswire. This training regimen on this
data set has provided state-of-the-art unsupervised
results that outperform IBM Model 4 (Haghighi et
al., 2009).
5.2 Convergence Analysis
With n = 250 maximum iterations, our dual decom-
position inference algorithm only converges 6.2%
of the time, perhaps largely due to the fact that the
two directional models have different one-to-many
structural constraints. However, the dual decompo-
426
Model Combiner Prec Rec AER
union 57.6 80.0 33.4
Baseline intersect 86.2 62.7 27.2
grow-diag 60.1 78.8 32.1
union 63.3 81.5 29.1
Bidirectional intersect 77.5 75.1 23.6
grow-diag 65.6 80.6 28.0
Table 2: Alignment error rate results for the bidirectional
model versus the baseline directional models. ?grow-
diag? denotes the grow-diag-final heuristic.
Model Combiner Prec Rec F1
union 75.1 33.5 46.3
Baseline intersect 64.3 43.4 51.8
grow-diag 68.3 37.5 48.4
union 63.2 44.9 52.5
Bidirectional intersect 57.1 53.6 55.3
grow-diag 60.2 47.4 53.0
Table 3: Phrase pair extraction accuracy for phrase pairs
up to length 5. ?grow-diag? denotes the grow-diag-final
heuristic.
sition algorithm does promote agreement between
the two models. We can measure the agreement
between models as the fraction of alignment links
in the union A? that also appear in the intersection
A? of the two directional models. Table 1 shows
a 47% relative increase in the fraction of links that
both models agree on by running dual decomposi-
tion (bidirectional), relative to independent direc-
tional inference (baseline). Improving convergence
rates represents an important area of future work.
5.3 Alignment Error Evaluation
To evaluate alignment error of the baseline direc-
tional aligners, we must apply a combination pro-
cedure such as union or intersection to Aa and Ab.
Likewise, in order to evaluate alignment error for
our combined model in cases where the inference
algorithm does not converge, we must apply combi-
nation to c(a) and c(b). In cases where the algorithm
does converge, c(a) = c(b) and so no further combi-
nation is necessary.
We evaluate alignments relative to hand-aligned
data using two metrics. First, we measure align-
ment error rate (AER), which compares the pro-
posed alignment setA to the sure set S and possible
set P in the annotation, where S ? P .
Prec(A,P) =
|A ? P|
|A|
Rec(A,S) =
|A ? S|
|S|
AER(A,S,P) = 1?
|A ? S|+ |A ? P|
|A|+ |S|
AER results for Chinese-English are reported in
Table 2. The bidirectional model improves both pre-
cision and recall relative to all heuristic combination
techniques, including grow-diag-final (Koehn et al,
2003). Intersected alignments, which are one-to-one
phrase alignments, achieve the best AER.
Second, we measure phrase extraction accuracy.
Extraction-based evaluations of alignment better co-
incide with the role of word aligners in machine
translation systems (Ayan and Dorr, 2006). Let
R5(S,P) be the set of phrases up to length 5 ex-
tracted from the sure link set S and possible link set
P . Possible links are both included and excluded
from phrase pairs during extraction, as in DeNero
and Klein (2010). Null aligned words are never in-
cluded in phrase pairs for evaluation. Phrase ex-
traction precision, recall, and F1 for R5(A,A) are
reported in Table 3. Correct phrase pair recall in-
creases from 43.4% to 53.6% (a 23.5% relative in-
crease) for the bidirectional model, relative to the
best baseline.
Finally, we evaluated our bidirectional model in a
large-scale end-to-end phrase-based machine trans-
lation system from Chinese to English, based on
the alignment template approach (Och and Ney,
2004). The translation model weights were tuned for
both the baseline and bidirectional alignments using
lattice-based minimum error rate training (Kumar et
al., 2009). In both cases, union alignments outper-
formed other combination heuristics. Bidirectional
alignments yielded a modest improvement of 0.2%
BLEU4 on a single-reference evaluation set of sen-
tences sampled from the web (Papineni et al, 2002).
4BLEU improved from 29.59% to 29.82% after training
IBM Model 1 for 3 iterations and training the HMM-based
alignment model for 3 iterations. During training, link poste-
riors were symmetrized by pointwise linear interpolation.
427
As our model only provides small improvements in
alignment precision and recall for the union com-
biner, the magnitude of the BLEU improvement is
not surprising.
6 Conclusion
We have presented a graphical model that combines
two classical HMM-based alignment models. Our
bidirectional model, which requires no additional
learning and no supervised data, can be applied us-
ing dual decomposition with only a constant factor
additional computation relative to independent di-
rectional inference. The resulting predictions im-
prove the precision and recall of both alignment
links and extraced phrase pairs in Chinese-English
experiments. The best results follow from combina-
tion via intersection.
Because our technique is defined declaratively in
terms of a graphical model, it can be extended in a
straightforward manner, for instance with additional
potentials on c or improvements to the component
directional models. We also look forward to dis-
covering the best way to take advantage of these
new alignments in downstream applications like ma-
chine translation, supervised word alignment, bilin-
gual parsing (Burkett et al, 2010), part-of-speech
tag induction (Naseem et al, 2009), or cross-lingual
model projection (Smith and Eisner, 2009; Das and
Petrov, 2011).
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going be-
yond AER: An extensive analysis of word alignments
and their impact on MT. In Proceedings of the Asso-
ciation for Computational Linguistics.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Association for Computational Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics.
Jamie Brunning, Adria de Gispert, and William Byrne.
2009. Context-dependent alignment models for statis-
tical machine translation. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In Proceedings of the North American As-
sociation for Computational Linguistics and IJCNLP.
Fabien Cromie`res and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In Proceedings of
the European Chapter of the Association for Compu-
tational Linguistics and IJCNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of the Association for Computa-
tional Linguistics.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of the As-
sociation for Computational Linguistics.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
Proceedings of the Association for Computational Lin-
guistics.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Yonggang Deng and Bowen Zhou. 2009. Optimizing
word alignment combination for phrase table training.
In Proceedings of the Association for Computational
Linguistics.
Kuzman Ganchev, Joao Grac?a, and Ben Taskar. 2008.
Better alignments = better translations? In Proceed-
ings of the Association for Computational Linguistics.
Joao Grac?a, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In Proceedings of Neural Information Processing Sys-
tems.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of the Association for
Computational Linguistics.
Xiaodong He. 2007. Using word-dependent transition
models in HMM-based word alignment for statistical
machine. In ACL Workshop on Statistical Machine
Translation.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the North American Chapter of the Association
for Computational Linguistics.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
428
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Josef Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of the
Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. Journal of Ar-
tificial Intelligence Research.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.
Franz Josef Och, Christopher Tillman, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
Association for Computational Linguistics.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2010. Word alignment with synonym regularization.
In Proceedings of the Association for Computational
Linguistics.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the Conference on Computa-
tional linguistics.
429
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1395?1404,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Language-independent Compound Splitting with Morphological Operations
Klaus Macherey1 Andrew M. Dai2 David Talbot1 Ashok C. Popat1 Franz Och1
1Google Inc.
1600 Amphitheatre Pkwy.
Mountain View, CA 94043, USA
{kmach,talbot,popat,och}@google.com
2University of Edinburgh
10 Crichton Street
Edinburgh, UK EH8 9AB
a.dai@ed.ac.uk
Abstract
Translating compounds is an important prob-
lem in machine translation. Since many com-
pounds have not been observed during train-
ing, they pose a challenge for translation sys-
tems. Previous decompounding methods have
often been restricted to a small set of lan-
guages as they cannot deal with more complex
compound forming processes. We present a
novel and unsupervised method to learn the
compound parts and morphological operations
needed to split compounds into their com-
pound parts. The method uses a bilingual
corpus to learn the morphological operations
required to split a compound into its parts.
Furthermore, monolingual corpora are used to
learn and filter the set of compound part can-
didates. We evaluate our method within a ma-
chine translation task and show significant im-
provements for various languages to show the
versatility of the approach.
1 Introduction
A compound is a lexeme that consists of more than
one stem. Informally, a compound is a combina-
tion of two or more words that function as a single
unit of meaning. Some compounds are written as
space-separated words, which are called open com-
pounds (e.g. hard drive), while others are written
as single words, which are called closed compounds
(e.g. wallpaper). In this paper, we shall focus only
on closed compounds because open compounds do
not require further splitting.
The objective of compound splitting is to split a
compound into its corresponding sequence of con-
stituents. If we look at how compounds are created
from lexemes in the first place, we find that for some
languages, compounds are formed by concatenating
existing words, while in other languages compound-
ing additionally involves certain morphological op-
erations. These morphological operations can be-
come very complex as we illustrate in the following
case studies.
1.1 Case Studies
Below, we look at splitting compounds from 3 differ-
ent languages. The examples introduce in part the
notation used for the decision rule outlined in Sec-
tion 3.1.
1.1.1 English Compound Splitting
The word flowerpot can appear as a closed or open
compound in English texts. To automatically split
the closed form we have to try out every split point
and choose the split with minimal costs according to
a cost function. Let's assume that we already know
that flowerpot must be split into two parts. Then we
have to position two split points that mark the end of
each part (one is always reserved for the last charac-
ter position). The number of split points is denoted
by K (i.e. K = 2), while the position of split points
is denoted by n1 and n2. Since flowerpot consists of
9 characters, we have 8 possibilities to position split
point n1 within the characters c1, . . . , c8. The final
split point corresponds with the last character, that is,
n2 = 9. Trying out all possible single splits results
in the following candidates:
flowerpot ? f+ lowerpot
flowerpot ? fl+ owerpot
...
flowerpot ? flower+ pot
...
flowerpot ? flowerpo+ t
1395
If we associate each compound part candidate with
a cost that reflects how frequent this part occurs in a
large collection of English texts, we expect that the
correct split flower + pot will have the lowest cost.
1.1.2 German Compound Splitting
The previous example covered a casewhere the com-
pound is constructed by directly concatenating the
compound parts. While this works well for En-
glish, other languages require additional morpholog-
ical operations. To demonstrate, we look at the Ger-
man compound Verkehrszeichen (traffic sign) which
consists of the two nouns Verkehr (traffic) and Zei-
chen (sign). Let's assume that we want to split this
word into 3 parts, that is, K = 3. Then, we get the
following candidates.
Verkehrszeichen ? V+ e+ rkehrszeichen
Verkehrszeichen ? V+ er+ kehrszeichen
...
Verkehrszeichen ? Verkehr+ s+ zeichen
...
Verkehrszeichen ? Verkehrszeich+ e+ n
Using the same procedure as described before, we
can lookup the compound parts in a dictionary or de-
termine their frequency from large text collections.
This yields the optimal split points n1 = 7, n2 =
8, n3 = 15. The interesting part here is the addi-
tional s morpheme, which is called a linking mor-
pheme, because it combines the two compound parts
to form the compound Verkehrszeichen. If we have
a list of all possible linking morphemes, we can
hypothesize them between two ordinary compound
parts.
1.1.3 Greek Compound Splitting
The previous example required the insertion of a
linking morpheme between two compound parts.
We shall now look at a more complicated mor-
phological operation. The Greek compound
?????????? (cardboard box) consists of the two
parts ????? (paper) and ????? (box). Here, the
problem is that the parts ????? and ????? are not
valid words in Greek. To lookup the correct words,
we must substitute the suffix of the compound part
candidates with some other morphemes. If we allow
the compound part candidates to be transformed by
some morphological operation, we can lookup the
transformed compound parts in a dictionary or de-
termine their frequencies in some large collection of
Greek texts. Let's assume that we need only one split
point. Then this yields the following compound part
candidates:
?????????? ? ? + ?????????
?????????? ? ? + ????????? g2 : ? / ?
?????????? ? ? + ????????? g2 : ? / ?
...
?????????? ? ????? + ????? g1 : ? / ? ,
g2 : ? / ?...
?????????? ? ????????? + ? g1 : ? / ?
?????????? ? ????????? + ? g2 : ? / ?
Here, gk : s/t denotes the kth compound part which
is obtained by replacing string s with string t in the
original string, resulting in the transformed part gk.
1.2 Problems and Objectives
Our goal is to design a language-independent com-
pound splitter that is useful for machine translation.
The previous examples addressed the importance of
a cost function that favors valid compound parts ver-
sus invalid ones. In addition, the examples have
shown that, depending on the language, the morpho-
logical operations can become very complex. For
most Germanic languages like Danish, German, or
Swedish, the list of possible linking morphemes is
rather small and can be provided manually. How-
ever, in general, these lists can become very large,
and language experts who could provide such lists
might not be at our disposal. Because it seems in-
feasible to list the morphological operations explic-
itly, we want to find and extract those operations
automatically in an unsupervised way and provide
them as an additional knowledge source to the de-
compounding algorithm.
Another problem is how to evaluate the quality
of the compound splitter. One way is to compile
for every language a large collection of compounds
together with their valid splits and to measure the
proportion of correctly split compounds. Unfortu-
nately, such lists do not exist for many languages.
1396
While the training algorithm for our compound split-
ter shall be unsupervised, the evaluation data needs
to be verified by human experts. Since we are in-
terested in improving machine translation and to cir-
cumvent the problem of explicitly annotating com-
pounds, we evaluate the compound splitter within a
machine translation task. By decompounding train-
ing and test data of a machine translation system, we
expect an increase in the number of matching phrase
table entries, resulting in better translation quality
measured in BLEU score (Papineni et al, 2002).
If BLEU score is sensitive enough to measure the
quality improvements obtained from decompound-
ing, there is no need to generate a separate gold stan-
dard for compounds.
Finally, we do not want to split non-compounds
and named entities because we expect them to be
translated non-compositionally. For example, the
German wordDeutschland (Germany) could be split
into two parts Deutsch (German) + Land (coun-
try). Although this is a valid split, named entities
should be kept as single units. An example for a
non-compound is the German participle vereinbart
(agreed) which could be wrongly split into the parts
Verein (club) + Bart (beard). To avoid overly eager
splitting, we will compile a list of non-compounds in
an unsupervised way that serves as an exception list
for the compound splitter. To summarize, we aim to
solve the following problems:
? Define a cost function that favors valid com-
pound parts and rejects invalid ones.
? Learn morphological operations, which is im-
portant for languages that have complex com-
pound forming processes.
? Apply compound splitting to machine transla-
tion to aid in translation of compounds that have
not been seen in the bilingual training data.
? Avoid splitting non-compounds and named en-
tities as this may result in wrong translations.
2 Related work
Previous work concerning decompounding can be
divided into two categories: monolingual and bilin-
gual approaches.
Brown (2002) describes a corpus-driven approach
for splitting compounds in a German-English trans-
lation task derived from a medical domain. A large
proportion of the tokens in both texts are cognates
with a Latin or Greek etymological origin. While the
English text keeps the cognates as separate tokens,
they are combined into compounds in the German
text. To split these compounds, the author compares
both the German and the English cognates on a char-
acter level to find reasonable split points. The algo-
rithm described by the author consists of a sequence
of if-then-else conditions that are applied on the two
cognates to find the split points. Furthermore, since
the method relies on finding similar character se-
quences between both the source and the target to-
kens, the approach is restricted to cognates and can-
not be applied to split more complex compounds.
Koehn and Knight (2003) present a frequency-
based approach to compound splitting for German.
The compound parts and their frequencies are es-
timated from a monolingual corpus. As an exten-
sion to the frequency approach, the authors describe
a bilingual approach where they use a dictionary ex-
tracted from parallel data to find better split options.
The authors allow only two linking morphemes be-
tween compound parts and a few letters that can be
dropped. In contrast to our approach, those opera-
tions are not learned automatically, but must be pro-
vided explicitly.
Garera and Yarowsky (2008) propose an approach
to translate compounds without the need for bilin-
gual training texts. The compound splitting pro-
cedure mainly follows the approach from (Brown,
2002) and (Koehn and Knight, 2003), so the em-
phasis is put on finding correct translations for com-
pounds. To accomplish this, the authors use cross-
language compound evidence obtained from bilin-
gual dictionaries. In addition, the authors describe a
simple way to learn glue characters by allowing the
deletion of up to two middle and two end charac-
ters.1 More complex morphological operations are
not taken into account.
Alfonseca et al (2008b) describe a state-of-the-
art German compound splitter that is particularly ro-
bust with respect to noise and spelling errors. The
compound splitter is trained on monolingual data.
Besides applying frequency and probability-based
methods, the authors also take the mutual informa-
tion of compound parts into account. In addition, the
1However, the glue characters found by this procedure seem
to be biased for at least German and Albanian. A very frequent
glue morpheme like -es- is not listed, while glue morphemes
like -k- and -h- rank very high, although they are invalid glue
morphemes for German. Albanian shows similar problems.
1397
authors look for compound parts that occur in dif-
ferent anchor texts pointing to the same document.
All these signals are combined and the weights are
trained using a support vector machine classifier. Al-
fonseca et al (2008a) apply this compound splitter
on various other Germanic languages.
Dyer (2009) applies a maximum entropy model
of compound splitting to generate segmentation lat-
tices that serve as input to a translation system.
To train the model, reference segmentations are re-
quired. Here, we produce only single best segmen-
tations, but otherwise do not rely on reference seg-
mentations.
3 Compound Splitting Algorithm
In this section, we describe the underlying optimiza-
tion problem and the algorithm used to split a token
into its compound parts. Starting from Bayes' de-
cision rule, we develop the Bellman equation and
formulate a dynamic programming-based algorithm
that takes a word as input and outputs the constituent
compound parts. We discuss the procedure used to
extract compound parts from monolingual texts and
to learn themorphological operations using bilingual
corpora.
3.1 Decision Rule for Compound Splitting
Given a token w = c1, . . . , cN = cN1 consisting of a
sequence of N characters ci, the objective function
is to find the optimal number K? and sequence of split
points n?K?0 such that the subwords are the constituents
of the token, where2 n0 := 0 and nK := N :
w = cN1 ? (K?, n?K?0 ) =
= argmax
K,nK0
{
Pr(cN1 ,K, nK0 )
}
(1)
= argmax
K,nK0
{
Pr(K) ? Pr(cN1 , nK0 |K)
}
u argmax
K,nK0
{
p(K) ?
K
?
k=1
p(cnknk?1+1, nk?1|K)?
?p(nk|nk?1,K)} (2)
with p(n0) = p(nK |?) ? 1. Equation 2 requires that
token w can be fully decomposed into a sequence
2For algorithmic reasons, we use the start position 0 to rep-
resent a fictitious start symbol before the first character of the
word.
of lexemes, the compound parts. Thus, determin-
ing the optimal segmentation is sufficient for finding
the constituents. While this may work for some lan-
guages, the subwords are not valid words in general
as discussed in Section 1.1.3. Therefore, we allow
the lexemes to be the result of a transformation pro-
cess, where the transformed lexemes are denoted by
gK1 . This leads to the following refined decision rule:
w = cN1 ? (K?, n?K?0 , g?K?1 ) =
= argmax
K,nK0 ,gK1
{
Pr(cN1 ,K, nK0 , gK1 )
}
(3)
= argmax
K,nK0 ,gK1
{
Pr(K) ? Pr(cN1 , nK0 , gK1 |K)
}
(4)
u argmax
K,nK0 ,gK1
{
p(K) ?
K
?
k=1
p(cnknk?1+1, nk?1, gk|K)
? ?? ?
compound part probability
?
? p(nk|nk?1,K)
}
(5)
The compound part probability is a zero-order
model. If we penalize each split with a constant split
penalty ?, and make the probability independent of
the number of splits K, we arrive at the following
decision rule:
w = cN1 ? (K?, n?K?1 , g?K?1 )
= argmax
K,nK0 ,gK1
{
?K ?
K
?
k=1
p(cnknk?1+1, nk?1, gk)
}
(6)
3.2 Dynamic Programming
We use dynamic programming to find the optimal
split sequence. Each split infers certain costs that
are determined by a cost function. The total costs of
a decomposed word can be computed from the in-
dividual costs of the component parts. For the dy-
namic programming approach, we define the follow-
ing auxiliary function Q with nk = j:
Q(cj1) = max
nk0 ,gk1
{
?k ?
k
?
?=1
p(cn?n??1+1, n??1, g?)
}
that is, Q(cj1) is equal to the minimal costs (maxi-
mum probability) that we assign to the prefix string
cj1 where we have used k split points at positions nk1 .
This yields the following recursive equation:
Q(cj1) = maxnk,gk
{
? ? Q(cnk?11 )?
? p(cnknk?1+1, nk?1, gk)
}
(7)
1398
Algorithm 1 Compound splitting
Input: input word w = cN1
Output: compound parts
Q(0) = 0
Q(1) = ? ? ? = Q(N) = ?
for i = 0, . . . , N ? 1 do
for j = i + 1, . . . , N do
split-costs = Q(i) + cost(cji+1, i, gj) +
split-penalty
if split-costs < Q(j) then
Q(j) = split-costs
B(j) = (i, gj)
end if
end for
end for
with backpointer
B(j) = argmax
nk,gk
{
? ? Q(cnk?11 )?
? p(cnknk?1+1, nk?1, gk)
}
(8)
Using logarithms in Equations 7 and 8, we can inter-
pret the quantities as additive costs rather than proba-
bilities. This yields Algorithm 1, which is quadratic
in the length of the input string. By enforcing that
each compound part does not exceed a predefined
constant length `, we can change the second for loop
as follows:
for j = i + 1, . . . ,min(i + `,N) do
With this change, Algorithm 1 becomes linear in the
length of the input word, O(|w|).
4 Cost Function and Knowledge Sources
The performance of Algorithm 1 depends on
the cost function cost(?), that is, the probability
p(cnknk?1+1, nk?1, gk). This cost function incorpo-
rates knowledge about morpheme transformations,
morpheme positionswithin a compound part, and the
compound parts themselves.
4.1 Learning Morphological Operations using
Phrase Tables
Let s and t be strings of the (source) language al-
phabet A. A morphological operation s/t is a pair
of strings s, t ? A?, where s is replaced by t. With
the usual definition of the Kleene operator ?, s and
t can be empty, denoted by ?. An example for such
a pair is ?/es, which models the linking morpheme
es in the German compound Bundesagentur (federal
agency):
Bundesagentur ? Bund+ es+ Agentur .
Note that by replacing either s or t with ?, we can
model insertions or deletions of morphemes. The
explicit dependence on position nk?1 in Equation 6
allows us to determine if we are at the beginning,
in the middle, or at the end of a token. Thus, we
can distinguish between start, middle, or end mor-
phemes and hypothesize them during search.3 Al-
though not explicitly listed in Algorithm 1, we dis-
allow sequences of linking morphemes. This can
be achieved by setting the costs to infinity for those
morpheme hypotheses, which directly succeed an-
other morpheme hypothesis.
To learn the morphological operations involved
in compounding, we determine the differences be-
tween a compound and its compound parts. This can
be done by computing the Levenshtein distance be-
tween the compound and its compound parts, with
the allowable edit operations being insertion, dele-
tion, or substitution of one or more characters. If we
store the current and previous characters, edit opera-
tion and the location (prefix, infix or suffix) at each
position during calculation of the Levenshtein dis-
tance then we can obtain the morphological opera-
tions required for compounding. Applying the in-
verse operations, that is, replacing twith s yields the
operation required for decompounding.
4.1.1 Finding Compounds and their Parts
To learn the morphological operations, we need
compounds together with their compound parts. The
basic idea of finding compound candidates and their
compound parts in a bilingual setting are related to
the ideas presented in (Garera and Yarowsky, 2008).
Here, we use phrase tables rather than dictionaries.
Although phrase tablesmight containmore noise, we
believe that overall phrase tables cover more phe-
nomena of translations thanwhat can be found in dic-
tionaries. The procedure is as follows. We are given
a phrase table that provides translations for phrases
from a source language l into English and from En-
glish into l. Under the assumption that English does
not contain many closed compounds, we can search
3We jointly optimize over K and the split points nk, so we
know that cnKnK?1 is a suffix of w.
1399
the phrase table for those single-token source words
f in language l, which translate into multi-token En-
glish phrases e1, . . . , en for n > 1. This results
in a list of (f ; e1, . . . , en) pairs, which are poten-
tial compound candidates together with their English
translations. If for each pair, we take each token ei
from the English (multi-token) phrase and lookup
the corresponding translation for language l to get
gi, we should find entries that have at least some
partial match with the original source word f , if f
is a true compound. Because the translation phrase
table was generated automatically during the train-
ing of a multi-language translation system, there is
no guarantee that the original translations are cor-
rect. Thus, the bilingual extraction procedure is
subject to introduce a certain amount of noise. To
mitigate this, thresholds such as minimum edit dis-
tance between the potential compound and its parts,
minimum co-occurrence frequencies for the selected
bilingual phrase pairs and minimum source and tar-
get word lengths are used to reduce the noise at the
expense of finding fewer compounds. Those entries
that obey these constraints are output as triples of
form:
(f ; e1, . . . , en; g1, . . . , gn) (9)
where
? f is likely to be a compound,
? e1, . . . , en is the English translation, and
? g1, . . . , gn are the compound parts of f .
The following example for German illustrates the
process. Suppose that the most probable translation
for?berweisungsbetrag is transfer amount using the
phrase table. We then look up the translation back to
German for each translated token: transfer translates
to?berweisung and amount translates toBetrag. We
then calculate the distance between all permutations
of the parts and the original compound and choose
the one with the lowest distance and highest transla-
tion probability: ?berweisung Betrag.
4.2 Monolingual Extraction of Compound
Parts
The most important knowledge source required for
Algorithm 1 is a word-frequency list of compound
parts that is used to compute the split costs. The
procedure described in Section 4.1.1 is useful for
learning morphological operations, but it is not suffi-
cient to extract an exhaustive list of compound parts.
Such lists can be extracted frommonolingual data for
which we use language model (LM) word frequency
lists in combination with some filter steps. The ex-
traction process is subdivided into 2 passes, one over
a high-quality news LM to extract the parts and the
other over a web LM to filter the parts.
4.2.1 Phase 1: Bootstrapping pass
In the first pass, we generate word frequency lists de-
rived from news articles for multiple languages. The
motivation for using news articles rather than arbi-
trary web texts is that news articles are in general
less noisy and contain fewer spelling mistakes. The
language-dependent word frequency lists are filtered
according to a sequence of filter steps. These filter
steps include discarding all words that contain digits
or punctuations other than hyphen, minimum occur-
rence frequency, and a minimum length which we
set to 4. The output is a table that contains prelim-
inary compound parts together with their respective
counts for each language.
4.2.2 Phase 2: Filtering pass
In the second pass, the compound part vocabulary
is further reduced and filtered. We generate a LM
vocabulary based on arbitrary web texts for each lan-
guage and build a compound splitter based on the vo-
cabulary list that was generated in phase 1. We now
try to split every word of the web LM vocabulary
based on the compound splitter model from phase
1. For the compound parts that occur in the com-
pound splitter output, we determine how often each
compound part was used and output only those com-
pound parts whose frequency exceed a predefined
threshold n.
4.3 Example
Suppose we have the following word frequencies
output from pass 1:
floor 10k poll 4k
flow 9k pot 5k
flower 15k potter 20k
In pass 2, we observe the word flowerpot. With the
above list, the only compound parts used are flower
and pot. If we did not split any other words and
threshold at n = 1, our final list would consist of
flower and pot. This filtering pass has the advantage
of outputting only those compound part candidates
1400
which were actually used to split words from web
texts. The thresholding also further reduces the risk
of introducing noise. Another advantage is that since
the set of parts output in the first pass may contain a
high number of compounds, the filter is able to re-
move a large number of these compounds by exam-
ining relative frequencies. In our experiments, we
have assumed that compound part frequencies are
higher than the compound frequency and so remove
words from the part list that can themselves be split
and have a relatively high frequency. Finally, after
removing the low frequency compound parts, we ob-
tain the final compound splitter vocabulary.
4.4 Generating Exception Lists
To avoid eager splitting of non-compounds and
named entities, we use a variant of the procedure de-
scribed in Section 4.1.1. By emitting all those source
words that translate with high probability into single-
token English words, we obtain a list of words that
should not be split.4
4.5 Final Cost Function
The final cost function is defined by the following
components which are combined log-linearly.
? The split penalty ? penalizes each compound
part to avoid eager splitting.
? The cost for each compound part gk is com-
puted as ? logC(gk), where C(gk) is the un-
igram count for gk obtained from the news LM
word frequency list. Since we use a zero-order
model, we can ignore the normalization and
work with unigram counts rather than unigram
probabilities.
? Because Algorithm 1 iterates over the charac-
ters of the input token w, we can infer from the
boundaries (i, j) if we are at the start, in the
middle, or at the end of the token. Applying
a morphological operation adds costs 1 to the
overall costs.
Although the cost function is language dependent,
we use the same split penalty weight ? = 20 for all
languages except for German, where the split penalty
weight is set to 13.5.
5 Results
To show the language independence of the approach
within a machine translation task, we translate from
languages belonging to different language families
into English. The publicly available Europarl corpus
is not suitable for demonstrating the utility of com-
pound splitting because there are few unseen com-
pounds in the test section of the Europarl corpus.
The WMT shared translation task has a broader do-
main compared to Europarl but covers only a few
languages. Hence, we present results for German-
English using the WMT-07 data and cover other lan-
guages using non-public corporawhich contain news
as well as open-domain web texts. Table 1 lists the
various corpus statistics. The source languages are
grouped according to their language family.
For learning the morphological operations, we al-
lowed the substitution of at most 2 consecutive char-
acters. Furthermore, we only allowed at most one
morphological substitution to avoid introducing too
much noise. The found morphological operations
were sorted according to their frequencies. Those
which occurred less than 100 times were discarded.
Examples of extracted morphological operations are
given in Table 2. Because the extraction procedure
described in Section 4.1 is not purely restricted to the
case of decompounding, we found that many mor-
phological operations emitted by this procedure re-
flect morphological variations that are not directly
linked to compounding, but caused by inflections.
To generate the language-dependent lists of com-
pound parts, we used language model vocabulary
lists5 generated from news texts for different lan-
guages as seeds for the first pass. These lists were
filtered by discarding all entries that either con-
tained digits, punctuations other than hyphens, or se-
quences of the same characters. In addition, the in-
frequent entries were discarded as well to further re-
duce noise. For the second pass, we used the lists
generated in the first pass together with the learned
morphological operations to construct a preliminary
compound splitter. We then generated vocabulary
lists for monolingual web texts and applied the pre-
liminary compound splitter onto this list. The used
4Because we will translate only into English, this is not an
issue for the introductory example flowerpot.
5The vocabulary lists also contain the word frequencies. We
use the term vocabulary list synonymously for a word frequency
list.
1401
Family Src Language #Tokens Train src/trg #Tokens Dev src/trg #Tokens Tst src/trg
Germanic Danish 196M 201M 43, 475 44, 479 72, 275 74, 504
German 43M 45M 23, 151 22, 646 45, 077 43, 777
Norwegian 251M 255M 42, 096 43, 824 70, 257 73, 556
Swedish 201M 213M 42, 365 44, 559 70, 666 74, 547
Hellenic Greek 153M 148M 47, 576 44, 658 79, 501 74, 776
Uralic Estonian 199M 244M 34, 987 44, 658 57, 916 74, 765
Finnish 205M 246M 32, 119 44, 658 53, 365 74, 771
Table 1: Corpus statistics for various language pairs. The target language is always English. The source languages are
grouped according to their language family.
Language morpholog. operations
Danish -/?, s/?
German -/?, s/?, es/?, n/?, e/?, en/?
Norwegian -/?, s/?, e/?
Swedish -/?, s/?
Greek ?/?, ?/?, ?/?, ?/?, ?/?, ?/?
Estonian -/?, e/?, se/?
Finnish ?/n, n/?, ?/en
Table 2: Examples of morphological operations that were
extracted from bilingual corpora.
compound parts were collected and sorted according
to their frequencies. Those which were used at least
2 times were kept in the final compound parts lists.
Table 3 reports the number of compound parts kept
after each pass. For example, the Finnish news vo-
cabulary list initially contained 1.7M entries. After
removing non-alpha and infrequent words in the first
filter step, we obtained 190K entries. Using the pre-
liminary compound splitter in the second filter step
resulted in 73K compound part entries.
The finally obtained compound splitter was in-
tegrated into the preprocessing pipeline of a state-
of-the-art statistical phrase-based machine transla-
tion system that works similar to the Moses de-
coder (Koehn et al, 2007). By applying the com-
pound splitter during both training and decoding we
ensured that source language tokens were split in
the same way. Table 4 presents results for vari-
ous language-pairs with and without decompound-
ing. Both the Germanic and the Uralic languages
show significant BLEU score improvements of 1.3
BLEU points on average. The confidence inter-
vals were computed using the bootstrap resampling
normal approximation method described in (Noreen,
1989). While the compounding process for Ger-
manic languages is rather simple and requires only a
few linking morphemes, compounds used in Uralic
languages have a richer morphology. In contrast to
the Germanic and Uralic languages, we did not ob-
serve improvements for Greek. To investigate this
lack of performance, we turned off transliteration
and kept unknown source words in their original
script. We analyzed the number of remaining source
characters in the baseline system and the system us-
ing compound splitting by counting the number of
Greek characters in the translation output. The num-
ber of remaining Greek characters in the translation
output was reduced from 6, 715 in the baseline sys-
tem to 3, 624 in the systemwhich used decompound-
ing. In addition, a few other metrics like the number
of source words that consisted of more than 15 char-
acters decreased as well. Because we do not know
how many compounds are actually contained in the
Greek source sentences6 and because the frequency
of using compounds might vary across languages,
we cannot expect the same performance gains across
languages belonging to different language families.
An interesting observation is, however, that if one
language from a language family shows performance
gains, then there are performance gains for all the
languages in that family. We also investigated the ef-
fect of not using any morphological operations. Dis-
allowing all morphological operations accounts for
a loss of 0.1 - 0.2 BLEU points across translation
systems and increases the compound parts vocabu-
lary lists by up to 20%, which means that most of the
gains can be achieved with simple concatenation.
The exception lists were generated according to
the procedure described in Section 4.4. Since we
aimed for precision rather than recall when con-
structing these lists, we inserted only those source
6Quite a few of the remaining Greek characters belong to
rare named entities.
1402
Language initial vocab size #parts after 1st pass #parts after 2nd pass
Danish 918, 708 132, 247 49, 592
German 7, 908, 927 247, 606 45, 059
Norwegian 1, 417, 129 237, 099 62, 107
Swedish 1, 907, 632 284, 660 82, 120
Greek 877, 313 136, 436 33, 130
Estonian 742, 185 81, 132 36, 629
Finnish 1, 704, 415 190, 507 73, 568
Table 3: Number of remaining compound parts for various languages after the first and second filter step.
System BLEU[%] w/o splitting BLEU[%] w splitting ? CI 95%
Danish 42.55 44.39 1.84 (? 0.65)
German WMT-07 25.76 26.60 0.84 (? 0.70)
Norwegian 42.77 44.58 1.81 (? 0.64)
Swedish 36.28 38.04 1.76 (? 0.62)
Greek 31.85 31.91 0.06 (? 0.61)
Estonian 20.52 21.20 0.68 (? 0.50)
Finnish 25.24 26.64 1.40 (? 0.57)
Table 4: BLEU score results for various languages translated into English with and without compound splitting.
Language Split source translation
German no Die EU ist nicht einfach ein Freundschaftsclub. The EU is not just a Freundschaftsclub.
yes Die EU ist nicht einfach ein Freundschaft Club The EU is not simply a friendship club.
Greek no ?? ????? ??????????? ??????????; What ??????????? configuration?
yes ?? ????? ????? ?????? ??????????; What is pulse code modulation?
Finnish no Lis?vuodevaatteet ja pyyheliinat ovat kaapissa. Lis?vuodevaatteet and towels are in the closet.
yes Lis? Vuode Vaatteet ja pyyheliinat ovat kaapissa. Extra bed linen and towels are in the closet.
Table 5: Examples of translations into English with and without compound splitting.
words whose co-occurrence count with a unigram
translation was at least 1, 000 and whose translation
probability was larger than 0.1. Furthermore, we re-
quired that at least 70%of all target phrase entries for
a given source word had to be unigrams. All decom-
pounding results reported in Table 4 were generated
using these exception lists, which prevented wrong
splits caused by otherwise overly eager splitting.
6 Conclusion and Outlook
We have presented a language-independent method
for decompounding that improves translations for
compounds that otherwise rarely occur in the bilin-
gual training data. We learned a set of morpholog-
ical operations from a translation phrase table and
determined suitable compound part candidates from
monolingual data in a two pass process. This al-
lowed us to learn morphemes and operations for lan-
guages where these lists are not available. In addi-
tion, we have used the bilingual information stored
in the phrase table to avoid splitting non-compounds
as well as frequent named entities. All knowledge
sources were combined in a cost function that was
applied in a compound splitter based on dynamic
programming. Finally, we have shown this improves
translation performance on languages from different
language families.
The weights were not optimized in a systematic
way but set manually to their respective values. In
the future, the weights of the cost function should be
learned automatically by optimizing an appropriate
error function. Instead of using gold data, the devel-
opment data for optimizing the error function could
be collected without supervision using the methods
proposed in this paper.
1403
References
Enrique Alfonseca, Slaven Bilac, and Stefan Paries.
2008a. Decompounding query keywords from com-
pounding languages. In Proc. of the 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL): Human Language Technologies (HLT), pages
253--256, Columbus, Ohio, USA, June.
Enrique Alfonseca, Slaven Bilac, and Stefan Paries.
2008b. German decompounding in a difficult corpus.
In A. Gelbukh, editor, Lecture Notes in Computer Sci-
ence (LNCS): Proc. of the 9th Int. Conf. on Intelligent
Text Processing and Computational Linguistics (CI-
CLING), volume 4919, pages 128--139. Springer Ver-
lag, February.
Ralf D. Brown. 2002. Corpus-Driven Splitting of Com-
poundWords. In Proc. of the 9th Int. Conf. on Theoret-
ical andMethodological Issues inMachine Translation
(TMI), pages 12--21, Keihanna, Japan, March.
Chris Dyer. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proc. of
the Human Language Technologies (HLT): The An-
nual Conf. of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL), pages
406--414, Boulder, Colorado, June.
Nikesh Garera and David Yarowsky. 2008. Translating
Compounds by Learning Component Gloss Transla-
tion Models via Multiple Languages. In Proc. of the
3rd Internation Conference on Natural Language Pro-
cessing (IJCNLP), pages 403--410, Hyderabad, India,
January.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proc. of the 10th
Conf. of the European Chapter of the Association for
Computational Linguistics (EACL), volume 1, pages
187--193, Budapest, Hungary, April.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of the 44th
Annual Meeting of the Association for Computational
Linguistics (ACL), volume 1, pages 177--180, Prague,
Czech Republic, June.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley & Sons, Canada.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proc. of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 311--318, Philadel-
phia, Pennsylvania, July.
1404
