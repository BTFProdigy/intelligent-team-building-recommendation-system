Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 38?44,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Expanding the Range of Automatic Emotion Detection in 
Microblogging Text 
 
 
Jasy Liew Suet Yan 
School of Information Studies 
Syracuse University 
Syracuse, New York, USA 
jliewsue@syr.edu 
 
  
 
Abstract 
Detecting emotions on microblogging sites such as 
Twitter is a subject of interest among researchers in 
behavioral studies investigating how people react to 
different events, topics, etc., as well as among users 
hoping to forge stronger and more meaningful 
connections with their audience through social media. 
However, existing automatic emotion detectors are 
limited to recognize only the basic emotions. I argue 
that the range of emotions that can be detected in 
microblogging text is richer than the basic emotions, 
and restricting automatic emotion detectors to identify 
only a small set of emotions limits their practicality in 
real world applications. Many complex emotions are 
ignored by current automatic emotion detectors 
because they are not programmed to seek out these 
?undefined? emotions. The first part of my 
investigation focuses on discovering the range of 
emotions people express on Twitter using manual 
content analysis, and the emotional cues associated 
with each emotion. I will then use the gold standard 
data developed from the first part of my investigation 
to inform the features to be extracted from text for 
machine learning, and identify the emotions that 
machine learning models are able to reliably detect 
from the range of emotions which humans can 
reliably detect in microblogging text. 
1 Introduction 
The popularity of microblogging sites such as 
Twitter provide us with a new source of data to 
study how people interact and communicate with 
their social networks or the public. Emotion is a 
subject of interest among researchers in 
behavioral studies investigating how people react 
to different events, topics, etc., as well as among 
users hoping to forge stronger and more 
meaningful connections with their audience 
through social media. There is growing interest 
among researchers to study how emotions on 
social media affect stock market trends (Bollen, 
Mao, & Zeng, 2011), relate to fluctuations in 
social and economic indicators (Bollen, Pepe, & 
Mao, 2011), serve as a measure for the 
population?s level of happiness (Dodds & 
Danforth, 2010), and provide situational 
awareness for both the authorities and the public 
in the event of disasters (Vo & Collier, 2013).  
In order to perform large-scale analysis of 
emotion phenomena and social behaviors on 
social media, there is a need to first identify the 
emotions that are expressed in text as the 
interactions on these platforms are dominantly 
text-based. With the surging amount of 
emotional content on social media platforms, it is 
an impossible task to detect the emotions that are 
expressed in each message using manual effort. 
Automatic emotion detectors have been 
developed to deal with this challenge. However, 
existing applications still rely on simple keyword 
spotting or lexicon-based methods due to the 
absence of sufficiently large emotion corpora for 
training and testing machine learning models 
38
(Bollen, Pepe, et al., 2011; Dodds & Danforth, 
2010).  
Research in using machine learning 
techniques to process emotion-laden text is 
gaining traction among sentiment analysis 
researchers, but existing automatic emotion 
detectors are restricted to identify only a small 
set of emotions, thus limiting their practicality 
for capturing the richer range of emotions 
expressed on social media platforms. The current 
state-of-the-art of simply adopting the basic 
emotions described in the psychology literature 
as emotion categories in text, as favored by a 
majority of scholars, is too limiting. Ekman?s six 
basic emotions (happiness, sadness, fear, anger, 
disgust, and surprise) (Ekman, 1971) are 
common emotion categories imposed on both 
humans and computers tasked to detect emotions 
in text (Alm, Roth, & Sproat, 2005; Aman & 
Szpakowicz, 2007; Liu, Lieberman, & Selker, 
2003). It is important to note that most basic 
emotions such as the six from Ekman are derived 
from facial expressions that can be universally 
recognized by humans. Verbal expressions of 
emotion are different from non-verbal 
expressions of emotion. Emotions expressed in 
text are richer than the categories suggested by 
the basic emotions. Also, people from different 
cultures use various cues to express a myriad of 
emotions in text. 
By using a restricted set of emotion 
categories, many emotions not included as part 
of the basic set are ignored or worse still, force-
fitted into one of the available emotion 
categories. This introduces a greater level of 
fuzziness in the text examples associated with 
each emotion.  
Example [1]: ?My prayers go to family of Amb. 
Stevens & others affected by this tragedy. We 
must not allow the enemy to take another. 
http://t.co/X8xTzeE4? 
Example [1] is an obvious case of ?sympathy? 
as the writer is expressing his or her condolences 
to people affected by a tragedy. If ?sympathy? is 
not in the pre-defined list of emotion categories 
that humans can choose from, human annotators 
may label this instance as ?sadness?, which is not 
entirely accurate. These inaccuracies will then be 
propagated into the automatic emotion detector. 
While the basic emotions have been 
established as universal emotions (Ekman, 
1999), their usefulness in emotion detection in 
text is still unclear. How useful are the six basic 
emotions in detecting consumers? emotional 
reactions towards a product or service from 
microblogs? What if a company wishes to detect 
disappointment? The focus on only the basic 
emotions has resulted in a dearth of effort to 
build emotion detectors that are able to recognize 
a wider range of emotions, especially the 
complex ones. Complex emotions are not merely 
combinations of the basic ones. For example, 
none of the combinations of Ekman?s six basic 
emotions seem to represent ?regret? or 
?empathy?. Without human-annotated examples 
of complex emotions, automatic emotion 
detectors remain ignorant of these emotions 
simply because they are not programmed to seek 
out these ?undefined? emotions.    
There is a need to create automatic emotion 
detectors that can detect a richer range of 
emotions apart from the six basic emotions 
proposed by Ekman to deal with emotional 
content from social media platforms. A broader 
range of emotions will enable automatic emotion 
detectors to capture more fine-grained emotions 
that truly reflect actual human emotional 
experience. Limited research has been done so 
far to determine the full range of emotions which 
humans can reliably detect in text, as well as 
salient cues that can be used to identify distinct 
emotions in text. A crucial step to address this 
gap is to develop a gold standard corpus 
annotated with a richer set of emotions for 
machine learning models to learn from.   
My research goal is to first discover the range 
of emotions humans can reliably detect in 
microblogging text, and investigate specific cues 
humans rely on to detect each emotion. Is there a 
universal set of cues humans rely on to detect a 
particular emotion or do these cues differ across 
39
individuals? Using grounded theory, the first part 
of my investigation focuses on discovering the 
range of emotions from tweets collected from a 
popular microblogging site, Twitter, and the 
emotional cues associated with each emotion. 
Twitter offers a wealth of publicly available 
emotional content generated by a variety of users 
on numerous topics. The inherently social nature 
of interactions on Twitter also allows me to 
investigate social emotions apart from personal 
emotions. In the second part of my investigation, 
human annotations from the first part of my 
investigation will serve as gold standard data for 
machine learning experiments used to determine 
the emotions that automatic methods can reliably 
detect from the range of emotions that humans 
can reliably identify.     
2 Background 
Early research on automatic emotion detection in 
text is linked to subjectivity analysis (Wiebe, 
Wilson, Bruce, Bell, & Martin, 2004; Wiebe, 
Wilson, & Cardie, 2005). Emotion detection in 
text is essentially a form of sentiment 
classification task based on finer-grained 
emotion categories. Automatic emotion detection 
has been applied in the domain of emails (Liu et 
al., 2003), customer reviews (Rubin, Stanton, & 
Liddy, 2004), children?s stories (Alm et al., 
2005), blog posts (Aman & Szpakowicz, 2007), 
newspaper headlines (Strapparava & Mihalcea, 
2008), suicide notes (Pestian et al., 2012), and 
chat logs (Brooks et al., 2013). Early 
development of automatic emotion detectors 
focused only on the detection of Ekman?s six 
basic emotions: happiness, surprise, sadness, fear, 
disgust, and anger (Alm et al., 2005; Aman & 
Szpakowicz, 2007; Liu et al., 2003; Strapparava 
& Mihalcea, 2008). Plutchik?s model is an 
expansion of Ekman?s basic emotions through 
the addition of trust and anticipation in his eight 
basic emotions (Plutchik, 1962), while Izard?s 
ten basic emotions also include guilt and shame 
(Izard, 1971).  
 Scholars have only recently started to expand 
the categories for automatic emotion 
classification as noted in the 14 emotions that are 
pertinent in the domain of suicide notes (Pestian 
et al., 2012), and 13 top categories that are used 
for emotion classification out of 40 emotions that 
emerged from the scientific collaboration chat 
logs (Brooks et al., 2013; Scott et al., 2012). 
However, existing gold standard corpora are 
limited by the emotion categories that are most 
often specific to a particular domain. 
Furthermore, it is difficult to pinpoint the exact 
words, symbols or phrases serving as salient 
emotion indicators because existing gold 
standard data are manually annotated at the 
sentence or message level. 
 Using Twitter, scholars have explored 
different strategies to automatically harness large 
volumes of data automatically for emotion 
classification. Pak & Paroubek (2010) applied a 
method similar to Read (2005) to extract tweets 
containing happy emoticons to represent positive 
sentiment, and sad emoticons to represent 
negative sentiment. First, this limits the emotion 
classifier to detect only happiness and sadness. 
Second, the lack of clear distinctions between the 
concepts of sentiment and emotion is 
problematic because tweeters may express a 
negative emotion towards an entity which they 
hold a positive sentiment on, and vice versa. For 
example, a tweeter expressing sympathy to 
another person who has experienced an 
unfortunate event is expressing a negative 
emotion but the tweet contains an overall 
positive sentiment. Third, such a data collection 
method assumes that the emotion expressed in 
the text is the same as the emotion the emoticon 
represents, and does not take into account of 
cases where the emotion expressed in the text 
may not be in-sync with the emotion represented 
by the emoticon (e.g., sarcastic remarks).  
 Mohammad (2012) and Wang, Chen, 
Thirunarayan, & Sheth (2012) applied a slightly 
improved method to create a large corpus of 
readily-annotated tweets for emotion 
classification. Twitter allows the use of hashtags 
(words that begin with the # sign) as topic 
indicators. These scholars experimented with 
extracting tweets that contain a predefined list of 
40
emotion words appearing in the form of hashtags. 
Mohammad (2012) only extracted tweets with 
emotion hashtags corresponding to Ekman?s six 
basic emotions (#anger, #disgust, #fear, #joy, 
#sadness, and #surprise) while Wang et al. (2012) 
expanded the predefined hashtag list to include 
emotion words associated with an emotion 
category, as well as the lexical variants of these 
emotion words. Although this method allows 
researchers to take advantage of the huge amount 
of data available on Twitter to train machine 
learning models, little is known about the 
specific emotional cues that are associated with 
these emotion categories. Also, this data 
collection method is biased towards tweeters 
who choose to express their emotions explicitly 
in tweets. 
 Kim, Bak, & Oh (2012) proposed a semi-
supervised method using unannotated data for 
emotion classification. They first applied Latent 
Dirichlet Allocation (LDA) to discover topics 
from tweets, and then determined emotions from 
the discovered topics by calculating the 
pointwise mutual information (PMI) score for 
each emotion from a list of eight emotions given 
a topic. The evaluation of this method using a 
corpus of manually annotated tweets revealed 
that this automatic emotion detector only 
managed to correctly classify 30% of tweets 
from the test dataset. The gold standard corpus 
used for evaluation was developed through 
manual annotations using Amazon Mechanical 
Turk (AMT). Only 3% of the tweets received full 
agreement among five annotators. 
3 Defining Emotions In Text 
In everyday language, people refer to emotion as 
prototypes of common emotions such as 
happiness, sadness, and anger (Fehr & Russell, 
1984). In the scientific realm, emotion is 
generally defined as ?ongoing states of mind that 
are marked by mental, bodily or behavioral 
symptoms? (Parrott, 2001). Specifically, each 
emotion category (e.g., happiness, sadness, anger, 
etc.) is distinguishable by a set of mental, bodily 
or behavioral symptoms. When a person 
expresses emotion in text, these symptoms are 
encoded in written language (words, phrases and 
sentences). 
Emotion in text is conceptualized as emotion 
expressed by the writer of the text. Emotion 
expression consists of ?signs that people give in 
various emotional states?, usually with the 
intention to be potentially perceived or 
understood by the others (Cowie, 2009). People 
express their emotional states through different 
non-verbal (e.g., facial expression, vocal 
intonation, and gestures) and verbal (e.g., text, 
spoken words) manifestations. Emotion 
expression in text is a writer?s descriptions of his 
or her emotional experiences or feelings. It is 
important to note that emotion expression only 
provides a window into a person?s emotional 
state depending on what he or she chooses to 
reveal to the others. It may not be depictions of a 
person?s actual emotional state, which is a 
limitation to the study of emotion in text (Calvo 
& D?Mello, 2010). 
4 Research Questions 
Detecting emotions in microblog posts poses 
new challenges to existing automatic emotion 
detectors due to reasons described below: 
? Unlike traditional texts, tweets consist of 
short texts expressed within the limit of 
140 characters, thus the language used to 
express emotions differs from longer 
texts (e.g., blogs, news, and fairy tales). 
? The language tweeters use is typically 
informal. Automatic emotion detectors 
must be able to deal with the presence of 
abbreviations, acronyms, orthographic 
elements, and misspellings. 
? Emotional cues are not limited to only 
emotion words. Twitter features such as 
#hashtags (topics), @username, retweets, 
and other user profile metadata may 
serve as emotional cues. 
Using data from Twitter, a popular 
microblogging platform, I will develop an initial 
framework to study the richness of emotions 
41
expressed for personal, as well as for social 
purposes. My research investigation is guided by 
the research questions listed below:  
? What emotions can humans reliably 
detect in microblogging text? 
? What salient cues are associated with 
each emotion? 
? How can good features for machine 
learning be identified from the salient 
cues humans associate with each emotion? 
? What emotions in microblogging text can 
be reliably detected using current 
machine learning techniques? 
5 Proposed Methodology 
My research design consists of three phases: 1) 
small-scale inductive content analysis for code 
book development, 2) large-scale deductive 
content analysis for gold standard data 
development, and 3) the design of machine 
learning experiments for automatic emotion 
detection in text. 
5.1 Data Collection 
When sampling for tweets from Twitter, I will 
utilize three sampling strategies to ensure the 
variability of emotions being studied. First, I will 
collect a random sample of publicly-available 
tweets. This sampling strategy aims to create a 
sample that is representative of the population on 
Twitter but may not produce a collection of 
tweets with sufficient emotional content. The 
second sampling strategy is based on topics or 
events. To ensure that tweets are relevant to this 
investigation, tweets will be sampled based on 
hashtags of events likely to evoke text with 
emotional content. Topics will include politics, 
sports, products/services, festive celebrations, 
and disasters.  
The third sampling strategy is based on users. 
This sampling strategy allows me to explore the 
range of emotions expressed by different 
individuals based on different stimuli, and not 
biased towards any specific events. To make the 
manual annotation feasible, I plan to first identify 
the usernames of 1) active tweeters with a large 
number of followers (e.g., tweets from 
politicians) to ensure sufficient data for analysis, 
and 2) random tweeters to represent ?average? 
users of Twitter. I acknowledge that this 
sampling strategy may be limited to only certain 
groups of people, and may not be representative 
of all Twitter users but it offers a good start to 
exploring the range of emotions being expressed 
in individual streams of tweets.  
5.2 Phase 1 
To develop a coding scheme for emotion 
annotation, I will first randomly sample 1,000 
tweets each from the random, topic-based, and 
user-based datasets for open coding. I will work 
with a small group of coders to identify the 
emotion categories from a subset of the 1,000 
tweets. Coders will be given instructions to 
assign each tweet with only one emotion label 
(i.e., the best emotion tag to describe the overall 
emotion expressed by the writer in a tweet), 
highlight the specific cues associated with the 
emotion, as well as identify the valence and 
intensity of the emotion expressed in the tweet.  
To verify the grouping of the emotion tags, 
coders will be asked to perform a card sorting 
exercise to group emotion tags that are 
semantically similar in the same group. Based on 
the discovered emotion categories, nuanced 
colorations within each category may be detected 
from the valence and intensity codes.  
Coders will incrementally annotate more 
tweets (300 tweets per round) until a point of 
saturation is reached, where new emotion 
categories stop emerging from data. I will 
continuously meet with the coders to discuss 
disagreements until the expected inter-annotator 
agreement threshold for the final set of emotion 
categories is achieved.   
5.3 Phase 2 
Using the coding scheme developed from Phase 
1, I will obtain a larger set of manual annotations 
using Amazon Mechanical Turk (AMT). AMT 
allows me to collect manual annotations of 
42
emotions on a large-scale, thus enabling me to 
investigate if there are any differences as to what 
a larger crowd of people identify as emotion cues 
in tweets. Each tweet will be annotated by at 
least three coders. To ensure the quality of the 
manual annotations collected from AMT, 
workers on AMT will have to undergo a short 
training module explaining the coding scheme, 
and will have to pass a verification test before 
being presented with the actual tweets to be 
annotated. Inter-annotator agreement will be 
calculated, and the emotion categories that 
humans can reliably detect in text will be 
identified.  
5.4 Phase 3 
Detecting a single emotion label for each tweet 
can be defined as a multi-class classification 
problem. The corpus from Phase 2 will be used 
as training data, and the corpus from Phase 1 will 
be used as testing data for the machine learning 
model. An analysis of the emotional cues from 
Phase 1 and Phase 2 datasets is conducted to 
identify salient features to be used for machine 
learning. Support vector machines (SVM) have 
been shown to perform well in this problem 
space (Alm et al., 2005; Aman & Szpakowicz, 
2007; Brooks et al., 2013; Cherry, Mohammad, 
& de Bruijn, 2012) so I will run experiments 
using SVM, and compare the performance of the 
model against a baseline using simple lexical 
features (i.e., n-grams). 
6 Research Contributions 
Analyzing the emotional contents in tweets 
can expand the theoretical understanding of the 
range of emotions humans express on social 
media platforms like Twitter. From a natural 
language processing standpoint, it is also crucial 
for the community to gain clearer insights on the 
cues associated with each fine-grained emotion. 
On top of that, findings from the machine 
learning experiments will inform the community 
as to whether training the machine learning 
models based on data collected using usernames, 
instead of topic hashtags will reduce noise in the 
data, and improve the performance of automatic 
emotion detection in microblogging texts.  
The expected contributions of this research 
investigation are three-fold: 1) the construction 
of an emotion taxonomy and detailed annotation 
scheme that could provide a useful starting point 
for future research, 2) the creation of machine 
learning models that can detect a wider range of 
emotions in text in order to enable researchers to 
tap into this wealth of information provided by 
Twitter to study a greater multitude of behavioral 
and social phenomenon, and 3) findings on the 
range of emotions people express on Twitter can 
potentially help inform the design of social 
network platforms to be more emotion sensitive. 
References  
Alm, C. O., Roth, D., & Sproat, R. (2005). Emotions 
from text: Machine learning for text-based 
emotion prediction. In Proceedings of the 
Conference on Human Language Technology 
and Empirical Methods in Natural Language 
Processing (pp. 579?586). Stroudsburg, PA, 
USA. 
Aman, S., & Szpakowicz, S. (2007). Identifying 
expressions of emotion in text. In Text, 
Speech and Dialogue (pp. 196?205).  
Bollen, J., Mao, H., & Zeng, X. (2011). Twitter mood 
predicts the stock market. Journal of 
Computational Science, 2(1), 1?8.  
Bollen, J., Pepe, A., & Mao, H. (2011). Modeling 
public mood and emotion: Twitter sentiment 
and socio-economic phenomena. In 
Proceedings of the Fifth International AAAI 
Conference on Weblogs and Social Media 
(pp. 450?453).  
Brooks, M., Kuksenok, K., Torkildson, M. K., Perry, 
D., Robinson, J. J., Scott, T. J., ? Aragon, 
C. R. (2013). Statistical affect detection in 
collaborative chat. Presented at the 
Conference on Computer Supported 
Cooperative Work and Social Computing, 
San Antonio, TX.  
Calvo, R. A., & D?Mello, S. (2010). Affect detection: 
An interdisciplinary review of models, 
methods, and their applications. IEEE 
Transactions on Affective Computing, 1(1), 
18?37.  
Cherry, C., Mohammad, S. M., & de Bruijn, B. 
(2012). Binary classifiers and latent sequence 
43
models for emotion detection in suicide 
notes. Biomedical Informatics Insights, 5, 
147?154.  
Cowie, R. (2009). Perceiving emotion: Towards a 
realistic understanding of the task. 
Philosophical Transactions of the Royal 
Society of London B: Biological Sciences, 
364(1535), 3515?3525.  
Dodds, P. S., & Danforth, C. M. (2010). Measuring 
the happiness of large-scale written 
expression: Songs, blogs, and Presidents. 
Journal of Happiness Studies, 11(4), 441?
456.  
Ekman, P. (1971). Universals and cultural differences 
in facial expressions of emotion. Nebraska 
Symposium on Motivation, 19, 207?283. 
Ekman, P. (1999). Basic emotions. In Handbook of 
Cognition and Emotion (pp. 45?60). John 
Wiley & Sons, Ltd.  
Fehr, B., & Russell, J. A. (1984). Concept of emotion 
viewed from a prototype perspective. 
Journal of Experimental Psychology: 
General, 113(3), 464?486.  
Izard, C. E. (1971). The face of emotion (Vol. xii). 
East Norwalk,  CT,  US: Appleton-Century-
Crofts. 
Kim, S., Bak, J., & Oh, A. H. (2012). Do you feel 
what I feel? Social aspects of emotions in 
Twitter conversations. In International AAAI 
Conference on Weblogs and Social Media 
(ICWSM).  
Liu, H., Lieberman, H., & Selker, T. (2003). A model 
of textual affect sensing using real-world 
knowledge. In Proceedings of the 8th 
International Conference on Intelligent User 
Interfaces (pp. 125?132). 
Mohammad, S. M. (2012). #Emotional tweets. In 
Proceedings of the First Joint Conference on 
Lexical and Computational Semantics. 
Montreal, QC. 
Pak, A., & Paroubek, P. (2010). Twitter as a corpus 
for sentiment analysis and opinion mining. In 
Seventh International Conference on 
Language Resources and Evaluation 
(LREC).  
Parrott, W. G. (2001). Emotions in social psychology:  
Essential readings (Vol. xiv). New York,  
NY,  US: Psychology Press. 
Pestian, J. P., Matykiewicz, P., Linn-Gust, M., South, 
B., Uzuner, O., Wiebe, J., ? Brew, C. 
(2012). Sentiment analysis of suicide notes: 
A shared task. Biomedical Informatics 
Insights, 5(Suppl. 1), 3?16.  
Plutchik, R. (1962). The Emotions: Facts, theories, 
and a new model. New York: Random 
House. 
Read, J. (2005). Using emoticons to reduce 
dependency in machine learning techniques 
for sentiment classification. In Proceedings 
of the ACL Student Research Workshop (pp. 
43?48). Stroudsburg, PA, USA. 
Rubin, V. L., Stanton, J. M., & Liddy, E. D. (2004). 
Discerning emotions in texts. In The AAAI 
Symposium on Exploring Attitude and Affect 
in Text (AAAI-EAAT).  
Scott, T. J., Kuksenok, K., Perry, D., Brooks, M., 
Anicello, O., & Aragon, C. (2012). Adapting 
grounded theory to construct a taxonomy of 
affect in collaborative online chat. In 
Proceedings of the 30th ACM International 
Conference on Design of Communication 
(pp. 197?204). New York, USA. 
Strapparava, C., & Mihalcea, R. (2008). Learning to 
identify emotions in text. In Proceedings of 
the 2008 ACM Symposium on Applied 
Computing (pp. 1556?1560). New York, 
USA. 
Vo, B.-K. H., & Collier, N. (2013). Twitter emotion 
analysis in earthquake situations. 
International Journal of Computational 
Linguistics and Applications, 4(1), 159?173. 
Wang, W., Chen, L., Thirunarayan, K., & Sheth, A. P. 
(2012). Harnessing Twitter ?big data? for 
automatic emotion identification. In 2012 
International Conference on Privacy, 
Security, Risk and Trust (PASSAT), and 2012 
International Conference on Social 
Computing (SocialCom) (pp. 587?592).  
Wiebe, J. M., Wilson, T., Bruce, R., Bell, M., & 
Martin, M. (2004). Learning subjective 
language. Computational Linguistics, 30(3), 
277?308.  
Wiebe, J. M., Wilson, T., & Cardie, C. (2005). 
Annotating expressions of opinions and 
emotions in language. Language Resources 
and Evaluation, 39(2-3), 165?210.  
 
44
Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 44?48,
Baltimore, Maryland, USA, June 26, 2014. c?2014 Association for Computational Linguistics
 
 
Optimizing Features in Active Machine Learning for Complex 
Qualitative Content Analysis 
 
Jasy Liew Suet Yan 
School of Information Studies 
Syracuse University, USA 
jliewsue@syr.edu 
Nancy McCracken 
School of Information Studies 
Syracuse University, USA 
njmccrac@syr.edu 
Shichun Zhou 
College of Engineering and 
Computer Science 
Syracuse University, USA 
szhou02@syr.edu 
Kevin Crowston 
National Science  
Foundation, USA 
crowston@syr.edu 
 
 
Abstract 
We propose a semi-automatic approach for 
content analysis that leverages machine learn-
ing (ML) being initially trained on a small set 
of hand-coded data to perform a first pass in 
coding, and then have human annotators cor-
rect machine annotations in order to produce 
more examples to retrain the existing model 
incrementally for better performance. In this 
?active learning? approach, it is equally im-
portant to optimize the creation of the initial 
ML model given less training data so that the 
model is able to capture most if not all posi-
tive examples, and filter out as many negative 
examples as possible for human annotators to 
correct. This paper reports our attempt to op-
timize the initial ML model through feature 
exploration in a complex content analysis 
project that uses a multidimensional coding 
scheme, and contains codes with sparse posi-
tive examples. While different codes respond 
optimally to different combinations of fea-
tures, we show that it is possible to create an 
optimal initial ML model using only a single 
combination of features for codes with at 
least 100 positive examples in the gold stand-
ard corpus. 
1 Introduction 
Content analysis, a technique for finding evi-
dence of concepts of theoretical interest through 
text, is an increasingly popular technique social 
scientists use in their research investigations. In 
the process commonly known as ?coding?, social 
scientists often have to painstakingly comb 
through large quantities of natural language cor-
pora to annotate text segments (e.g., phrase, sen-
tence, and paragraphs) with codes exhibiting the 
concepts of interest (Miles & Huberman, 1994). 
Analyzing textual data is very labor-intensive, 
time-consuming, and is often limited to the capa-
bilities of individual researchers (W. Evans, 
1996). The coding process becomes even more 
demanding as the complexity of the project in-
creases especially in the case of attempting to 
apply a multidimensional coding scheme with a 
significant number of codes (D?nmez, Ros?, 
Stegmann, Weinberger, & Fischer, 2005). 
With the proliferation and availability of dig-
ital texts, it is challenging, if not impossible, for 
human coders to manually analyze torrents of 
text to help advance social scientists? understand-
ing of the practices of different populations of 
interest through textual data. Therefore, compu-
tational methods offer significant benefits to help 
augment human capabilities to explore massive 
amounts of text in more complex ways for theory 
generation and theory testing. Content analysis 
can be framed as a text classification problem, 
where each text segment is labeled based on a 
predetermined set of categories or codes.  
Full automation of content analysis is still far 
from being perfect (Grimmer & Stewart, 2013). 
The accuracy of current automatic approaches on 
the best performing codes in social science re-
search ranges from 60-90% (Broadwell et al., 
2013; Crowston, Allen, & Heckman, 2012; M. 
Evans, McIntosh, Lin, & Cates, 2007; Ishita, 
Oard, Fleischmann, Cheng, & Templeton, 2010; 
Zhu, Kraut, Wang, & Kittur, 2011). While the 
potential of automatic content analysis is promis-
ing, computational methods should not be 
viewed as a replacement for the role of the pri-
mary researcher in the careful interpretation of 
text. Rather, the computers? pattern recognition 
capabilities can be leveraged to seek out the most 
likely examples for each code of interest, thus 
reducing the amount of texts researchers have to 
read and process.  
We propose a semi-automatic method that 
promotes a close human-computer partnership 
for content analysis. Machine learning (ML) is 
used to perform the first pass of coding on the 
unlabeled texts. Human annotators then have to 
correct only what the ML model identifies as 
positive examples of each code. The initial ML 
44
  
model needs to learn only from a small set of 
hand-coded examples (i.e., gold standard data), 
and will evolve and improve as machine annota-
tions that are verified by human annotators are 
used to incrementally retrain the model. In con-
trast to conventional machine learning, this ?ac-
tive learning? approach will significantly reduce 
the amount of training data needed upfront from 
the human annotators. However, it is still equally 
important to optimize the creation of the initial 
ML model given less training data so that the 
model is able to capture most if not all positive 
examples, and filter out as many negative exam-
ples as possible for human annotators to correct.  
To effectively implement the active learning 
approach for coding qualitative data, we have to 
first understand the nature and complexity of 
content analysis projects in social science re-
search. Our pilot case study, an investigation of 
leadership behaviors exhibited in emails from a 
FLOSS development project (Misiolek, Crow-
ston, & Seymour, 2012), reveals that it is com-
mon for researchers to use a multidimensional 
coding scheme consisting of a significant number 
of codes in their research inquiry. Previous work 
has shown that not all dimensions in a multidi-
mensional coding scheme could be applied fully 
automatically with acceptable level of accuracy 
(D?nmez et al., 2005) but little is known if it is 
possible at all to train an optimal model for all 
codes using the same combination of features. 
Also, the distribution of codes is often times un-
even with some rarely occurring codes having 
only few positive examples in the gold standard 
corpus.  
This paper presents our attempt in optimiz-
ing the initial ML model through feature explora-
tion using gold standard data created from a mul-
tidimensional coding scheme, including codes 
that suffer from sparseness of positive examples. 
Specifically, our study is guided by two research 
questions: 
a) How can features for an initial machine 
learning model be optimized for all codes in 
a text classification problem based on multi-
dimensional coding schemes? Is it possible 
to train a one-size-fits-all model for all codes 
using a single combination of features?  
b) Are certain features better suited for codes 
with sparse positive examples? 
2 Machine Learning Experiments 
To optimize the initial machine learning model, 
we systematically ran multiple experiments using 
a gold standard corpus of emails from a 
free/libre/open-source software (FLOSS) devel-
opment project coded for leadership behaviors 
(Misiolek et al., 2012). The coding scheme con-
tained six dimensions: 1) social/relationship, 2) 
task process, 3) task substance, 4) dual process 
and substance, 5) change behaviors, and 6) net-
working. The number of codes for each dimen-
sion ranged from 1 to 14. There were a total of 
35 codes in the coding scheme. Each sentence 
could be assigned more than one code. Framing 
the problem as a multi-label classification task, 
we trained a binary classification model for each 
code using support vector machine (SVM) with 
ten-fold cross-validation. This gold standard cor-
pus consisted of 3,728 hand-coded sentences 
from 408 email messages.  
For the active learning setup, we tune the ini-
tial ML model for high recall since having the 
annotators pick out positive examples that have 
been incorrectly classified by the model is pref-
erable to missing machine-annotated positive 
examples to be presented to human annotators 
for verification (Liew, McCracken, & Crowston, 
2014). Therefore, the initial ML model with low 
precision is acceptable. 
 
Category Features 
Content Unigram, bigram, pruning, 
tagging, lowercase, stop-
words, stemming, part-of-
speech (POS) tags 
Syntactic Token count 
Orthographic Capitalization of first letter of 
a word, capitalization of entire 
word 
Word list Subjectivity words 
Semantic Role of sender (software de-
veloper or not) 
 
Table 1. Features for ML model. 
 
As shown in Table 1, we have selected gen-
eral candidate features that have proven to work 
well across various text classification tasks, as 
well as one semantic feature specific to the con-
text of FLOSS development projects. For content 
features, techniques that we have incorporated to 
reduce the feature space include pruning, substi-
tuting certain tokens with more generic tags, 
converting all tokens to lowercase, excluding 
stopwords, and stemming. Using the wrapper 
approach (Kohavi & John, 1997), the same clas-
sifier is used to test the prediction performance 
of various feature combinations listed in Table 1. 
45
  
Model SINGLE MULTIPLE 
Measure Mean Recall Mean Precision Mean Recall Mean Precision 
Overall 
All (35) 0.690 0.065 0.877 0.068 
Dimension 
Change (1) 0.917 0.011 1.000 0.016 
Dual Process and 
Substance (13) 
0.675 0.069 0.852 0.067 
Networking (1) 0.546 0.010 0.843 0.020 
Process (3) 0.445 0.006 0.944 0.024 
Relationship (14) 0.742 0.083 0.872 0.089 
Substance (3) 0.735 0.061 0.919 0.051 
 
Table 2. Comparison of mean recall and mean precision between SINGLE and MULTIPLE models.  
 
 
 
Figure 1. Recall and precision for each code (grouped by dimension). 
 
3 Results and Discussion 
We ran 343 experiments with different combina-
tions of the 13 features in Table 1. We first com-
pare the performance of the best one-size-fits-all 
initial machine learning model that produces the 
highest recall using a single combination of fea-
tures for all codes (SINGLE) with an ?ensemble? 
model that uses different combinations of fea-
tures to produce the highest recall for each code 
(MULTIPLE). The SINGLE model combines 
content (unigram + bigram + POS tags + lower-
case + stopwords) with syntactic, orthographic, 
and semantic features. None of the best feature 
combination for each code in the MULTIPLE 
model coincides with the feature combination in 
the SINGLE model. For example, the best fea-
ture combination for code ?Phatics/Salutations? 
consists of only 2 out of the 13 features (unigram 
+ bigram). 
The best feature combination for each code 
in the MULTIPLE model varies with only some 
regularity noted in a few codes within the Dual 
and Substance dimensions. However, these pat-
terns are not consistent across all codes in a sin-
gle dimension indicating that the pertinent lin-
guistic features for codes belonging to the same 
dimension may differ despite their conceptual 
similarities, and even fitting an optimal model 
for all codes within a single dimension may 
prove to be difficult especially when the distribu-
tion of codes is uneven, and positive examples 
for certain codes are sparse. There are also no 
consistent feature patterns observed from the 
codes with sparse positive examples in the 
MULTIPLE model. 
0
0.2
0.4
0.6
0.8
1
E
xternal M
onitoring
Inform
ing
Issue D
irective
C
orrection
O
ffer/P
rovide A
ssistance
A
pproval
R
equest/Invite
C
om
m
it/A
ssum
e R
esponsibility
C
onfirm
/C
larify
O
bjection/D
isagreem
ent
Q
uery/Q
uestion
U
pdate
S
uggest/R
ecom
m
end
E
xplanation
N
etw
orking/B
oundary S
panning
R
em
ind
P
rocedure
S
chedule
C
riticism
P
roactive Inform
ing
A
pology
C
onsulting
H
um
or
A
ppreciation
S
elf-disclosure
V
ocative
A
greem
ent
E
m
otional E
xpression
P
hatics/S
alutations
Inclusive R
eference
O
pinion/P
reference
A
cronym
/Jargon
G
enerate N
ew
 Idea
E
valuation/F
eedback
P
rovide Inform
ation
Best Recall (MULTIPLE) Best Recall (SINGLE) Precision (MULTIPLE) Precision (SINGLE)
Change Dual Networking Process Relationship Substance 
46
  
 
 
 
 
 
Figure 2. Recall and precision for each code (sorted by gold frequency) 
 
The comparison between the two models in 
Table 2 further demonstrates that the MULTI-
PLE model outperforms the SINGLE model both 
in the overall mean recall of all 35 codes, as well 
as the mean recall for each dimension. Figure 1 
(codes grouped by dimensions) illustrates that 
the feature combination on the SINGLE model is 
ill-suited for the Process codes, and half the Dual 
Process and Substance codes. Recall for each 
code for the SINGLE model are mostly below or 
at par with the recall for each code in the MUL-
TIPLE model. Thus, creating a one-size-fits-all 
initial model may not be optimal when training 
data is limited. Figure 2 (codes sorted based on 
gold frequency as shown beside the code names 
in the x-axis) exhibits that the SINGLE model is 
able to achieve similar recall to the MULTIPLE 
model for codes with over 100 positive examples 
in the training data. Precision for these codes are 
also higher compared to codes with sparse posi-
tive examples. This finding is promising because 
it implies that creating a one-size-fits-all initial 
ML model may be possible even for a multidi-
mensional coding scheme if there are more than 
100 positive examples for each code.  
4 Conclusion and Future Work 
We conclude that creating an optimal initial one-
size-fits-all ML model for all codes in a multi-
dimensional coding scheme using only a single 
feature combination is not possible when codes 
with sparse positive examples are present, and 
training data is limited, which may be common 
in real world content analysis projects in social 
science research. However, our findings also 
show that the potential of using a one-size-fits-all 
model increases when the size of positive exam-
ples for each code in the gold standard corpus are 
above 100. For social scientists who may not 
possess the technical skills needed for feature 
selection to optimize the initial ML model, this 
discovery confirms that we can create a ?canned? 
model using a single combination of features that 
would work well in text classification for a wide 
range of codes with the condition that research-
ers must be able to provide sufficient positive 
examples above a certain threshold to train the 
initial model. This would make the application of 
machine learning for qualitative content analysis 
more accessible to social scientists. 
The initial ML model with low precision 
means that the model is over-predicting. As a 
result, human annotators will have to correct 
more false positives in the machine annotations. 
For future work, we plan to experiment with dif-
ferent sampling strategies to pick the most ?prof-
itable? machine annotations to be corrected by 
human annotators. We will also work on design-
ing an interactive and adaptive user interface to 
promote greater understanding of machine learn-
ing outputs for our target users. 
0
0.2
0.4
0.6
0.8
1
R
em
ind (4)
G
enerate N
ew
 Idea (4)
C
riticism
 (5)
P
roactive Inform
ing (5)
Inform
ing (7)
P
rocedure (7)
E
xternal M
onitoring (8)
Issue D
irective (8)
S
chedule (8)
A
pology (8)
E
valuation/F
eedback (9)
C
orrection (11)
O
ffer/P
rovide A
ssistance (11)
A
pproval (12)
C
onsulting (13)
R
equest/Invite (14)
C
om
m
it/A
ssum
e R
esponsibility (17)
N
etw
orking/B
oundary S
panning (18)
A
cronym
/Jargon (19)
H
um
or (21)
A
ppreciation (23)
S
elf-disclosure (37)
V
ocative (41)
C
onfirm
/C
larify (45)
O
bjection/D
isagreem
ent (51)
A
greem
ent (67)
E
m
otional E
xpression (108)
Q
uery/Q
uestion (115)
P
hatics/S
alutations (116)
U
pdate (119)
S
uggest/R
ecom
m
end (138)
Inclusive R
eference (146)
O
pinion/P
reference (215)
P
rovide Inform
ation (312)
E
xplanation (327)
Best Recall (MULTIPLE) Best Recall (SINGLE) Precision (MULTIPLE) Precision (SINGLE)
47
  
Acknowledgments 
This material is based upon work supported 
by the National Science Foundation under Grant 
No. IIS-1111107. Kevin Crowston is supported 
by the National Science Foundation. Any opin-
ions, findings, and conclusions or recommenda-
tions expressed in this material are those of the 
author(s) and do not necessarily reflect the views 
of the National Science Foundation. The authors 
wish to thank Janet Marsden for assisting with 
the feature testing experiments, and gratefully 
acknowledge helpful suggestions by the review-
ers. 
References 
Broadwell, G. A., Stromer-Galley, J., Strzalkowski, 
T., Shaikh, S., Taylor, S., Liu, T., Boz, U., Elia, 
A., Jiao, L., Webb, N. (2013). Modeling Soci-
ocultural phenomena in discourse. Natural Lan-
guage Engineering, 19(02), 213?257.  
Crowston, K., Allen, E. E., & Heckman, R. (2012). 
Using natural language processing technology for 
qualitative data analysis. International Journal of 
Social Research Methodology, 15(6), 523?543.  
D?nmez, P., Ros?, C., Stegmann, K., Weinberger, A., 
& Fischer, F. (2005). Supporting CSCL with au-
tomatic corpus analysis technology. In Proceed-
ings of 2005 Conference on Computer Support 
for Collaborative Learning (pp. 125?134).  
Evans, M., McIntosh, W., Lin, J., & Cates, C. (2007). 
Recounting the courts? Applying automated con-
tent analysis to enhance empirical legal research. 
Journal of Empirical Legal Studies, 4(4), 1007?
1039.  
Evans, W. (1996). Computer-supported content anal-
ysis: Trends, tools, and techniques. Social Sci-
ence Computer Review, 14(3), 269?279.  
Grimmer, J., & Stewart, B. M. (2013). Text as data: 
The promise and pitfalls of automatic content 
analysis methods for political texts. Political 
Analysis, 21(3), 267?297.  
Ishita, E., Oard, D. W., Fleischmann, K. R., Cheng, 
A.-S., & Templeton, T. C. (2010). Investigating 
multi-label classification for human values. Pro-
ceedings of the American Society for Information 
Science and Technology, 47(1), 1?4.  
Liew, J. S. Y., McCracken, N., & Crowston, K. 
(2014). Semi-automatic content analysis of quali-
tative data. In iConference 2014 Proceedings (pp. 
1128?1132).  
Miles, M. B., & Huberman, A. M. (1994). Qualitative 
data analysis: An expanded sourcebook (2nd 
ed.). Sage.  
Misiolek, N., Crowston, K., & Seymour, J. (2012). 
Team dynamics in long-standing technology-
supported virtual teams. Presented at the Acade-
my of Management Annual Meeting, Organiza-
tional Behavior Division, Boston, MA. 
Zhu, H., Kraut, R. E., Wang, Y.-C., & Kittur, A. 
(2011). Identifying shared leadership in Wikipe-
dia. In Proceedings of the SIGCHI Conference on 
Human Factors in Computing Systems (pp. 
3431?3434). New York, NY, USA.  
 
 
 
48
Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 59?62,
Baltimore, Maryland, USA, June 27, 2014. c?2014 Association for Computational Linguistics
Design of an Active Learning System with Human Correction for Content Analysis  Nancy McCracken School of Information Studies Syracuse University, USA njmccrac@syr.edu 
Jasy Liew Suet Yan School of Information Studies Syracuse University, USA jliewsue@syr.edu 
Kevin Crowston National Science Foundation Syracuse University, USA crowston@syr.edu    Abstract 
Our research investigation focuses on the role of humans in supplying corrected examples in active learning cycles, an important aspect of deploying active learning in practice.  In this paper, we dis-cuss sampling strategies and sampling sizes in set-ting up an active learning system for human ex-periments in the task of content analysis, which involves labeling concepts in large volumes of text.  The cost of conducting comprehensive hu-man subject studies to experimentally determine the effects of sampling sizes and sampling sizes is high. To reduce those costs, we first applied an active learning simulation approach to test the ef-fect of different sampling strategies and sampling sizes on machine learning (ML) performance in order to select a smaller set of parameters to be evaluated in human subject studies. 1 Introduction Social scientists often use content analysis to understand the practices of groups by analyzing texts such as transcripts of interpersonal commu-nication. Content analysis is the process of iden-tifying and labeling conceptually significant fea-tures in text, referred to as ?coding? (Miles and Huberman, 1994). For example, researchers studying leadership might look for evidence of behaviors such as ?suggesting or recommending? or ?inclusive reference? expressed in email mes-sages. However, analyzing text is very labor-intensive, as the text must be read and under-stood by a human. Consequently, important re-search questions in the qualitative social sciences may not be addressed because there is too much data for humans to analyze in a reasonable time. A few researchers have tried automatic tech-niques on content analysis problems. For exam-ple, Crowston et al. (2012) manually developed a classifier to identify codes related to group maintenance behavior in free/libre open source software (FLOSS) teams. Others have applied machine-learning (ML) techniques. For example, Ishita et al. (2010) used ML to automatically 
classify sections of text within documents on ten human values taken from the Schwartz?s Value Inventory. Broadwell et al. (2012) developed models to classify sociolinguistic behaviors to infer social roles (e.g., leadership). On the best performing codes, these approaches achieve ac-curacies from 60?80%, showing the potential of automatic qualitative content analysis. However, these studies all limited their reports to a subset of codes used by the social scientists, due in part to the need for a large volume of training data.  The state-of-the-art ML approaches for con-tent analysis require researchers to obtain a large amount of annotated data upfront, which is often costly or impractical. An active learning ap-proach which uses human correction during the steps of active learning could potentially help produce a large amount of annotated data while minimizing the cost of human annotation effort.  Unlike other text annotation tasks, the code an-notation for content analysis requires significant cognitive effort, which may limit, or even nulli-fy, the benefits of active learning.   We are building an active machine learning system to semi-automate the process of content analysis, and are planning to study the human role in such machine learning systems.   
Manually code 
documents in 
ATLAS.ti
Export gold 
standard data into 
XML
Human Annotation Machine Annotation
Learn model
Apply model to 
additional 
documents
Human Correction
Manually correct 
model coding
Save corrected 
coding as silver data
 Figure 1. Active learning for semi-automatic content analysis.  As illustrated in Figure 1, the system design in-corporates building a classifier from an initial set of hand-coded examples and iteratively improv-
59
ing the model by having human annotators cor-rect new examples identified by the system  Little is yet known about the optimal number of machine annotations to be presented to human annotators for correction, and how the sample sizes of machine annotations affect ML perfor-mance. Also, existing active learning sampling strategies to pick out the most ?beneficial? ex-amples for human correction to be used in the next round of ML training have not been tested in the context of social science data, where con-cept codes may be multi-dimensional or hierar-chical, and the problem may be multi-label (one phrase or sentence in the annotated text has mul-tiple labels). Also, concept codes tend to be very sparse in the text, resulting in a classification problem that has both imbalance?the non-annotated pieces of text (negative examples) tend to be far more frequent that annotated text?and rarity, where there may not be enough examples of some codes to achieve a good classifier.  The cost of conducting comprehensive human subject studies to experimentally determine the effects of sampling sizes and sampling sizes is high. Therefore, we first applied an active learn-ing simulation approach to test the effect of dif-ferent sampling strategies and sampling sizes on machine learning (ML) performance. This allows the human subject studies to involve a smaller set of parameters to be evaluated. 2 Related Work For active learning in our system, we are using what is sometimes called pool-based active learning, where a large number of unlabeled ex-amples are available to be the pool of the next samples. This type of active learning has been well explored for text categorization tasks (Lewis and Gale, 1994; Tong and Koller 2000; Schohn and Cohn 2000). This approach often uses the method of uncertainty sampling to pick new samples from the pool, both with probability models to give the ?uncertainty? (Lewis and Gale, 1994) and with SVM models, where the margin numbers give the ?uncertainty? (Tong and Koller 2000; Schohn and Cohn 2000). While much of the research focus has been on the sam-pling method, some has also focused on the size of the sample, e.g. in Schohn and Cohn (2000), sample sizes of 4, 8, 16, and 32 were used, where the result was that smaller sizes gave a steeper learning curve with a greater classification cost, and the authors settled on a sample size of 8. For 
additional active learning references, see the Set-tles (2009) survey of active learning literature. This type of active learning has also been used in the context of human correction. One such system is described in Mandel et al. (2006), using active learning for music retrieval, where users were presented with up to 6 examples of songs to label. Another is the DUALLIST system described in Settles (2011) and Settles and Zhu (2012) where human experiments were carried out for text classification and other tasks.  While most active learning experiments focus on reduc-ing the number of examples to achieve an accu-rate model, there has been some effort to model the reduction of the cost of human time in anno-tation, where the human time is non-uniform per example.  Both the systems in Culotta and McCallum (2005) and in Clancy et al. (2012) for the task of named entity extraction, modeled hu-man cost in the context of sequential information extraction tasks.  However, one difference be-tween these systems and ours is that all of the tasks studied in these systems did not require annotators to have extensive training to annotate complex concept codes.  3 Problem We worked with a pilot project in which researchers are studying leadership in open source software groups by analyzing open source developer emails. After a year of part-time annotation by two annotators, the researchers developed a codebook that provides a definition and examples for 35 codes. The coders achieved an inter-annotator agreement (kappa) of about 80%, and annotated about 400 email threads, consisting of about 3700 sentences. We used these coded messages as the ?gold standard? data for our study. However, only 15 codes had more than 25 instances in the gold standard set. The most common code (?Explanation/Rationale/ Background?) occurred only 319 times.  In our pilot correction experiments, annota-tors tried correcting samples of sizes ranging from about 50 to about 400. Anecdotal evidence indicates that annotators liked to annotate sample sizes of about 100 in order to achieve good focus on a particular code definition at one time, but without getting stressed with too many examples.  Part of the required focus is that annotators need to refresh their memory on any particular code at the start of annotation, so switching frequently between different codes is cognitively taxing. This desired sample size contrasts with prior ac-
60
tive learning systems that employ much smaller sample sizes, in the range of 1 to 20.  We are currently in the process of setting up the human experiments to test our main research question of achieving an accurate model for con-tent analysis using a minimum of human effort. In this paper, we discuss two questions for active learning in order to have annotators cor-rect an acceptable number of machine annota-tions that are most likely to increase the perfor-mance of the ML model in each iteration. These are:  how do different sample sizes and different sampling strategies of machine annotations pre-sented to human annotators for correction in each round affect ML performance?  4 Active Learning Simulation Setup In a similar strategy to that of Clancy et al. (2012), we carried out a preliminary investiga-tion by conducting an active learning simulation on our gold standard data. The simulation starts with a small initial sample, and uses active learn-ing where we ?correct? the sample labels by tak-ing labels from the gold standard corpus. For our simulation experiments, we separated the gold standard data randomly into a training set of 90% of the examples, 3298 sentences, and a test set of 10%, 366 sentences.  In the experimental setup, we used a version of libSVM that was modified to produce num-bers of distance to the margin of the SVM classi-fication. We implemented the multi-label classi-fication by classifying each label separately where some sentences have the selected label and all others were counted as ?negative? labels. We used svm weights to handle the problem of imbalance in the negative examples. After exper-imentation with different combinations of fea-tures, we used a set of features that was best overall for the codes: unigram tokens lowercased and filtered by stop words, bigrams, orthographic features from capitalization, the token count, and the role of the sender of the email. For an initial sample, we randomly chose 3 positive and 3 negative examples from the de-velopment set to be the initial training set used for all experimental runs. We carried out experi-ments with a number of sample sizes, b, ranging over 5, 10, 20, 40, 50, 60, 80 and 100 instances. For experiments on methods used to select correction examples, we have chosen to experi-ment with sampling methods similar to those found in Lewis and Gale (1994) and Lewis (1995) using a random sampling method, where 
a new sample is chosen randomly from the re-maining examples in the development set, a rele-vance sampling method, where a new sample is chosen as the b number of most likely labeled candidates in the development set with the larg-est distance from the margin of the SVM classi-fication, and an uncertainty sampling method, where a new sample is chosen as the b number of candidates in the region of uncertainty on either side of the margin of the SVM classification. 5 Preliminary Results In this simulation experiment, the pool size is quite small (3664 examples) compared to the large amount of unlabeled data that is normally available for active learning, and would be avail-able for our system under actual use. We tested the active learning simulation on 8 codes. There was no clear winning sampling strategy out of the 3 we used in the simulation experiment but random sampling (5 out of 8 codes) appeared to be the one that most often produced the highest F?2 score in the shortest number of iterations. Figure 2 shows the F?2 score for each sampling strategy based on code ?Opinion/Preference? using sample sizes 5 and 100 respectively.  As for sampling sizes, we did not observe a large difference in the evolution of the F?2 score between the various sample sizes, and the learn-ing curves in Figure 2, shown for the sample siz-es of 5 and 100, are typical. This means that we should be able to use larger sample sizes for hu-man subject studies to achieve the same im-provements in performance as with the smaller sample sizes, and can carry out the experiments to relate the cost of human annotation with in-creases in performance. 
 
 Figure 2. Active ML performance for code Opinion/Preference. 
61
6 Conclusion and Future Work Our findings are inconclusive as we have yet to run the active learning simulations on all the codes. However, preliminary results are directing us towards using larger sample sizes and then experimenting with random and uncertainty sampling in the human subject studies.  From our experiments with the different codes, we found the performance on less fre-quent codes to be problematic as it is difficult for the active learning system to identify potential positive examples to improve the models. While the system performance may improve to handle such sparse cases, it may be better to modify the codebook instead. We plan to give the user feed-back on the performance of the codes at each iteration of the active learning and support modi-fications to the codebook, for example, the user may wish to drop some codes or collapse them according to some hierarchy. After all, if a code is not found in the text, it is hard to argue for its theoretical importance.  We are currently completing the design of the parameters of the active learning process for the human correction experiments on our pilot project with the codes about leadership in open source software groups. We will also be testing and undergoing further development of the user interface for the annotators.  Our next step will be to test the system on other projects with other researchers. We hope to gain more insight into what types of coding schemes and codes are easier to learn than oth-ers, and to be able to guide social scientists into developing coding schemes that are not only based on the social science theory but also useful in practice to develop an accurate classifier for very large amounts of digital text.  Acknowledgements:  This material is based upon work supported by the National Science Foundation under Grant No. IIS-1111107. Kevin Crowston is supported by the National Science Foundation. Any opin-ions, findings, and conclusions or recommenda-tions expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. The authors gratefully acknowledge helpful suggestions by the reviewers. 
Reference Broadwell, G. A., Stromer-Galley, J., Strzalkowski, T., Shaikh, S., Taylor, S., Liu, T., Boz, U., Elia, A., Jiao, L., & Webb, N. (2013). Modeling sociocul-tural phenomena in discourse. Natural Language Engineering, 19(02), 213?257.  Clancy, S., Bayer, S. and Kozierok, R. (2012)  ?Ac-tive Learning with a Human In The Loop,? Mitre Corporation.  Crowston, K., Allen, E. E., & Heckman, R. (2012). Using natural language processing technology for qualitative data analysis. International Journal of Social Research Methodology, 15(6), 523?543.  Culotta, A. and McCallum, A. (2005) ?Reducing La-beling Effort for Structured Prediction Tasks.? Ishita, E., Oard, D. W., Fleischmann, K. R., Cheng, A.-S., & Templeton, T. C. (2010). Investigating multi-label classification for human values. Pro-ceedings of the American Society for Information Science and Technology, 47(1), 1?4.  Miles, M. B., & Huberman, A. M. (1994). Qualitative data analysis: An expanded sourcebook. Sage Pub-lications.  Lewis, D. D., & Gale, W. A. (1994). A sequential algorithm for training text classifiers. In Proceed-ings of the 17th annual international ACM SIGIR conference on Research and development in infor-mation retrieval (pp. 3-12). Lewis, D. D. (1995). A sequential algorithm for train-ing text classifiers: Corrigendum and additional da-ta. In ACM SIGIR Forum (Vol. 29, No. 2, pp. 13-19). Mandel, M. I., Poliner, G. E., & Ellis, D. P. (2006). Support vector machine active learning for music retrieval. Multimedia systems, 12(1), 3-13.  Schohn, G., & Cohn, D. (2000). Less is more: Active learning with support vector machines. In Interna-tional Conference on Machine Learning (pp. 839-846).  Settles, B. (2010). Active learning literature survey. University of Wisconsin, Madison, 52, 55-66. Settles, B. (2011). Closing the loop: Fast, interactive semi-supervised annotation with queries on fea-tures and instances. In Proceedings of the Confer-ence on Empirical Methods in Natural Language Processing (pp. 1467-1478).  Settles, B., & Zhu, X. (2012). Behavioral factors in interactive training of text classifiers. In Proceed-ings of the 2012 Conference of the North American Chapter of the Association for Computational Lin-guistics: Human Language Technologies (pp. 563-567).   Tong, S., & Koller, D. (2002). Support vector ma-chine active learning with applications to text clas-sification. The Journal of Machine Learning Re-search, 2, 45-66.   
62
